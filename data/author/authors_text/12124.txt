Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 145?148,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Hidden Markov Tree Model in Dependency-based Machine Translation
?
Zden
?
ek
?
Zabokrtsk?y
Charles University in Prague
Institute of Formal and Applied Linguistics
zabokrtsky@ufal.mff.cuni.cz
Martin Popel
Charles University in Prague
Institute of Formal and Applied Linguistics
popel@matfyz.cz
Abstract
We would like to draw attention to Hid-
den Markov Tree Models (HMTM), which
are to our knowledge still unexploited in
the field of Computational Linguistics, in
spite of highly successful Hidden Markov
(Chain) Models. In dependency trees,
the independence assumptions made by
HMTM correspond to the intuition of lin-
guistic dependency. Therefore we suggest
to use HMTM and tree-modified Viterbi
algorithm for tasks interpretable as label-
ing nodes of dependency trees. In par-
ticular, we show that the transfer phase
in a Machine Translation system based
on tectogrammatical dependency trees can
be seen as a task suitable for HMTM.
When using the HMTM approach for
the English-Czech translation, we reach a
moderate improvement over the baseline.
1 Introduction
Hidden Markov Tree Models (HMTM) were intro-
duced in (Crouse et al, 1998) and used in appli-
cations such as image segmentation, signal classi-
fication, denoising, and image document catego-
rization, see (Durand et al, 2004) for references.
Although Hidden Markov Models belong to the
most successful techniques in Computational Lin-
guistics (CL), the HMTM modification remains to
the best of our knowledge unknown in the field.
The first novel claim made in this paper is that
the independence assumptions made by Markov
Tree Models can be useful for modeling syntactic
trees. Especially, they fit dependency trees well,
because these models assume conditional depen-
dence (in the probabilistic sense) only along tree
?
The work on this project was supported by the grants
MSM 0021620838, GAAV
?
CR 1ET101120503, and M
?
SMT
?
CR LC536. We thank Jan Haji?c and three anonymous review-
ers for many useful comments.
edges, which corresponds to intuition behind de-
pendency relations (in the linguistic sense) in de-
pendency trees. Moreover, analogously to applica-
tions of HMM on sequence labeling, HMTM can
be used for labeling nodes of a dependency tree,
interpreted as revealing the hidden states
1
in the
tree nodes, given another (observable) labeling of
the nodes of the same tree.
The second novel claim is that HMTMs are
suitable for modeling the transfer phase in Ma-
chine Translation systems based on deep-syntactic
dependency trees. Emission probabilities rep-
resent the translation model, whereas transition
(edge) probabilities represent the target-language
tree model. This decomposition can be seen as
a tree-shaped analogy to the popular n-gram ap-
proaches to Statistical Machine Translation (e.g.
(Koehn et al, 2003)), in which translation and lan-
guage models are trainable separately too. More-
over, given the input dependency tree and HMTM
parameters, there is a computationally efficient
HMTM-modified Viterbi algorithm for finding the
globally optimal target dependency tree.
It should be noted that when using HMTM, the
source-language and target-language trees are re-
quired to be isomorphic. Obviously, this is an un-
realistic assumption in real translation. However,
we argue that tectogrammatical deep-syntactic de-
pendency trees (as introduced in the Functional
Generative Description framework, (Sgall, 1967))
are relatively close to this requirement, which
makes the HMTM approach practically testable.
As for the related work, one can found a num-
ber of experiments with dependency-based MT
in the literature, e.g., (Boguslavsky et al, 2004),
(Menezes and Richardson, 2001), (Bojar, 2008).
However, to our knowledge none of the published
systems searches for the optimal target representa-
1
HMTM looses the HMM?s time and finite automaton in-
terpretability, as the observations are not organized linearly.
However, the terms ?state? and ?transition? are still used.
145
	
	 		
	
 Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125?129,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
English-Czech MT in 2008 ?
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin Popel,
Jan Pta?c?ek, Jan Rous?, Zdene?k ?Zabokrtsky?
Charles University, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz
{popel,jan.rous}@matfyz.cz
Abstract
We describe two systems for English-to-
Czech machine translation that took part
in the WMT09 translation task. One of
the systems is a tuned phrase-based system
and the other one is based on a linguisti-
cally motivated analysis-transfer-synthesis
approach.
1 Introduction
We participated in WMT09 with two very dif-
ferent systems: (1) a phrase-based MT based
on Moses (Koehn et al, 2007) and tuned for
English?Czech translation, and (2) a complex
system in the TectoMT platform ( ?Zabokrtsky? et
al., 2008).
2 Data
2.1 Monolingual Data
Our Czech monolingual data consist of (1)
the Czech National Corpus (CNC, versions
SYN200[056], 72.6%, Kocek et al (2000)), (2)
a collection of web pages downloaded by Pavel
Pecina (Web, 17.1%), and (3) the Czech mono-
lingual data provided by WMT09 organizers
(10.3%). Table 1 lists sentence and token counts
(see Section 2.3 for the explanation of a- and t-
layer).
Sentences 52 M
with nonempty t-layer 51 M
a-nodes (i.e. tokens) 0.9 G
t-nodes 0.6 G
Table 1: Czech monolingual training data.
? The work on this project was supported by the grants
MSM0021620838, 1ET201120505, 1ET101120503, GAUK
52408/2008, M?SMT ?CR LC536 and FP6-IST-5-034291-STP
(EuroMatrix).
2.2 Parallel Data
As the source of parallel data we use an internal
release of Czech-English parallel corpus CzEng
(Bojar et al, 2008) extended with some additional
texts. One of the added sections was gathered
from two major websites containing Czech sub-
titles to movies and TV series1. The matching of
the Czech and English movies is rather straight-
forward thanks to the naming conventions. How-
ever, we were unable to reliably determine the se-
ries number and the episode number from the file
names. We employed a two-step procedure to au-
tomatically pair the TV series subtitle files. For
every TV series:
1. We clustered the files on both sides to remove
duplicates
2. We found the best matching using a provi-
sional translation dictionary. This proved to
be a successful technique on a small sample
of manually paired test data. The process was
facilitated by the fact that the correct pairs of
episodes usually share some named entities
which the human translator chose to keep in
the original English form.
Table 2 lists parallel corpus sizes and the distri-
bution of text domains.
English Czech
Sentences 6.91 M
with nonempty t-layer 6.89 M
a-nodes (i.e. tokens) 61 M 50 M
t-nodes 41 M 33 M
Distribution: [%] [%]
Subtitles 68.2 Novels 3.3
Software Docs 17.0 Commentaries/News 1.5
EU (Legal) Texts 9.5 Volunteer-supplied 0.4
Table 2: Czech-English data sizes and sources.
1www.opensubtitles.org and titulky.com
125
2.3 Data Preprocessing using TectoMT
platform: Analysis and Alignment
As we believe that various kinds of linguistically
relevant information might be helpful in MT, we
performed automatic analysis of the data. The
data were analyzed using the layered annotation
scheme of the Prague Dependency Treebank 2.0
(PDT 2.0, Hajic? and others (2006)), i.e. we used
three layers of sentence representation: morpho-
logical layer, surface-syntax layer (called analyti-
cal (a-) layer), and deep-syntax layer (called tec-
togrammatical (t-) layer).
The analysis was implemented using TectoMT,
( ?Zabokrtsky? et al, 2008). TectoMT is a highly
modular software framework aimed at creating
MT systems (focused, but by far not limited to
translation using tectogrammatical transfer) and
other NLP applications. Numerous existing NLP
tools such as taggers, parsers, and named entity
recognizers are already integrated in TectoMT, es-
pecially for (but again, not limited to) English and
Czech.
During the analysis of the large Czech mono-
lingual data, we used Jan Hajic??s Czech tagger
shipped with PDT 2.0, Maximum Spanning Tree
parser (McDonald et al, 2005) with optimized set
of features as described in Nova?k and ?Zabokrtsky?
(2007), and a tool for assigning functors (seman-
tic roles) from Klimes? (2006), and numerous other
components of our own (e.g. for conversion of an-
alytical trees into tectogrammatical ones).
In the parallel data, we analyzed the Czech side
using more or less the same scenario as used for
the monolingual data. English sentences were an-
alyzed using (among other tools) Morce tagger
Spoustova? et al (2007) and Maximum Spanning
Tree parser.2
The resulting deep syntactic (tectogrammatical)
Czech and English trees are then aligned using T-
aligner?a feature based greedy algorithm imple-
mented for this purpose (Marec?ek et al, 2008). T-
aligner finds corresponding nodes between the two
given trees and links them. For deciding whether
to link two nodes or not, T-aligner makes use of
a bilingual lexicon of tectogrammatical lemmas,
morphosyntactic similarities between the two can-
didate nodes, their positions in the trees and other
similarities between their parent/child nodes. It
2In some previous experiments (e.g. ?Zabokrtsky? et al
(2008)), we used phrase-structure parser Collins (1999) with
subsequent constituency-dependency conversion.
also uses word alignment generated from surface
shapes of sentences by GIZA++ tool, Och and Ney
(2003). We use acquired aligned tectogrammatical
trees for training some models for the transfer.
As analysis of such amounts of data is obvi-
ously computationally very demanding, we run it
in parallel using Sun Grid Engine3 cluster of 40
4-CPU computers. For this purpose, we imple-
mented a rather generic tool that submits any Tec-
toMT pipeline to the cluster.
3 Factored Phrase-Based MT
We essentially repeat our experiments from last
year (Bojar and Hajic?, 2008): GIZA++ align-
ments4 on a-layer lemmas (a-layer nodes corre-
spond 1-1 to surface tokens), symmetrized using
grow-diag-final (no -and) heuristic5 .
Probably due to the domain difference (the test
set is news), including Subtitles in the parallel data
and Web in the monolingual data did not bring any
improvement that would justify the additional per-
formance costs. For most of the phrase-based ex-
periments, we thus used only 2.2M parallel sen-
tences (27M Czech and 32M English tokens) and
43M Czech sentences (694 M tokens).
In Table 3 below, we report the scores for the
following setups selected from about 50 experi-
ments we ran in total:
Moses T is a simple phrase-based translation (T)
with no additional factors. The translation is
performed on truecased word forms (i.e. sen-
tence capitalization removed unless the first
word seems to be a name). The 4-gram lan-
guage model is based on the 43M sentences.
Moses T+C is a factored setup with form-to-form
translation (T) and target-side morphological
coherence check following Bojar and Hajic?
(2008). The setup uses two language mod-
els: 4-grams of word forms and 7-grams of
morphological tags.
Moses T+C+C&T+T+G 84k is a setup desirable
from the linguistic point of view. Two in-
dependent translation paths are used: (1)
form?form translation with two target-side
checks (lemma and tag generated from the
target-side form) as a fine-grained baseline
3http://gridengine.sunsource.net/
4Default settings, IBM models and iterations: 153343.
5Later, we found out that the grow-diag-final-and heuris-
tic provides insignificantly superior results.
126
with the option to resort to (2) an independent
translation of lemma?lemma and tag?tag
finished by a generation step that combines
target-side lemma and tag to produce the fi-
nal target-side form.
We use three language models in this setup
(3-grams of forms, 3-grams of lemmas, and
10-grams of tags).
Due to the increased complexity of the setup,
we were able to train this model on 84k par-
allel sentences only (the Commentaries sec-
tion) and we use the target-side of this small
training data for language models, too.
For all the setups we perform standard MERT
training on the provided development set.6
4 Translation Setup Based on
Tectogrammatical Transfer
In this translation experiment, we follow the tradi-
tional analysis-transfer-synthesis approach, using
the set of PDT 2.0 layers: we analyze the input
English sentence up to the tectogrammatical layer
(through the morphological and analytical ones),
then perform the tectogrammatical transfer, and
then synthesize the target Czech sentence from its
tectogrammatical representation. The whole pro-
cedure consists of about 80 steps, so the following
description is necessarily very high level.
4.1 Analysis
Each sentence is tokenized (roughly according to
the Penn Treebank conventions), tagged by the En-
glish version of the Morce tagger Spoustova? et al
(2007), and lemmatized by our lemmatizer. Then
the dependency parser (McDonald et al, 2005) is
applied. Then the analytical trees resulting from
the parser are converted to the tectogrammatical
ones (i.e. functional words are removed, only
morphologically indispensable categories are left
with the nodes using a sequence of heuristic proce-
dures). Unlike in PDT 2.0, the information about
the original syntactic form is stored with each t-
node (values such as v:inf for an infinitive verb
form, v:since+fin for the head of a subor-
dinate clause of a certain type, adj:attr for
an adjective in attribute position, n:for+X for a
given prepositional group are distinguished).
6We used the full development set of 2k sentences for
?Moses T? and a subset of 1k sentences for the other two
setups due to time constraints.
One of the steps in the analysis of English is
named entity recognition using Stanford Named
Entity Recognizer (Finkel et al, 2005). The nodes
in the English t-layer are grouped according to the
detected named entities and they are assigned the
type of entity (location, person, or organization).
This information is preserved in the transfer of the
deep English trees to the deep Czech trees to al-
low for the appropriate capitalization of the Czech
translation.
4.2 Transfer
The transfer phase consists of the following steps:
? Initiate the target-side (Czech) t-trees sim-
ply by ?cloning? the source-side (English) t-
trees. Subsequent steps usually iterate over
all t-nodes. In the following, we denote a
source-side t-node as S and the correspond-
ing target-side node as T.
? Translate formemes using
two probabilistic dictionaries
(p(T.formeme|S.formeme, S.parent.lemma)
and p(T.formeme|S.formeme)) and a few
manual rules. The formeme translation
probability estimates were extracted from a
part of the parallel data mentioned above.
? Translate lemmas using a probabilistic dictio-
nary (p(T.lemma|S.lemma)) and a few rules
that ensure compatibility with the previously
chosen formeme. Again, this probabilistic
dictionary was obtained using the aligned
tectogrammatical trees from the parallel cor-
pus.
? Fill the grammatemes (deep-syntactic equiv-
alent of morphological categories) gender
(for denotative nouns) and aspect (for verbs)
according to the chosen lemma. We also
fix grammateme values where the English-
Czech grammateme correspondence is non-
trivial (e.g. if an English gerund expression is
translated to Czech as a subordinating clause,
the tense grammateme has to be filled). How-
ever, the transfer of grammatemes is defi-
nitely much easier task than the transfer of
formemes and lemmas.
4.3 Synthesis
The transfer step yields an abstract deep
syntactico-semantical tree structure. Firstly,
127
we derive surface morphological categories
from their deep counterparts taking care of their
agreement where appropriate and we also remove
personal pronouns in subject positions (because
Czech is a pro-drop language).
To arrive at the surface tree structure, auxil-
iary nodes of several types are added, including
(1) reflexive particles, (2) prepositions, (3) subor-
dinating conjunctions, (4) modal verbs, (5) ver-
bal auxiliaries, and (6) punctuation nodes. Also,
grammar-based node ordering changes (imple-
mented by rules) are performed: e.g. if an English
possessive attribute is translated using Czech gen-
itive, it is shifted into post-modification position.
After finishing the inflection of nouns, verbs,
adjectives and adverbs (according to the values of
morphological categories derived from agreement
etc.), prepositions may need to be vocalized: the
vowel -e or -u is attached to the preposition if the
pronunciation of prepositional group would be dif-
ficult otherwise.
After the capitalization of the beginning of each
sentence (and each named entity instance), we ob-
tain the final translation by flattening the surface
tree.
4.4 Preliminary Error Analysis
According to our observations most errors happen
during the transfer of lemmas and formemes.
Usually, there are acceptable translations of
lemma and formeme in respective n-best lists
but we fail to choose the best one. The sce-
nario described in Section 4.2 uses quite a
primitive transfer algorithm where formemes
and lemmas are translated separately in two
steps. We hope that big improvements could
be achieved with more sophisticated algo-
rithms (optimizing the probability of the whole
tree) and smoothed probabilistic models (such
as p(T.lemma|S.lemma, T.parent.lemma) and
p(T.formeme|S.formeme, T.lemma, T.parent.lemma)).
Other common errors include:
? Analysis: parsing (especially coordinations
are problematic with McDonald?s parser).
? Transfer: the translation of idioms and col-
locations, including named entities. In these
cases, the classical transfer at the t-layer
is not appropriate and utilization of some
phrase-based MT would help.
? Synthesis: reflexive particles, word order.
5 Experimental Results and Discussion
Table 3 reports lowercase BLEU and NIST scores
and preliminary manual ranks of our submissions
in contrast with other systems participating in
English?Czech translation, as evaluated on the
official WMT09 unseen test set. Note that auto-
matic metrics are known to correlate quite poorly
with human judgements, see the best ranking but
?lower scoring? PC Translator this year and also
in Callison-Burch et al (2008).
System BLEU NIST Rank
Moses T 14.24 5.175 -3.02 (4)
Moses T+C 13.86 5.110 ?
Google 13.59 4.964 -2.82 (3)
U. of Edinburgh 13.55 5.039 -3.24 (5)
Moses T+C+C&T+T+G 84k 10.01 4.360 -
Eurotran XP 09.51 4.381 -2.81 (2)
PC Translator 09.42 4.335 -2.77 (1)
TectoMT 07.29 4.173 -3.35 (6)
Table 3: Automatic scores and preliminary human
rank for English?Czech translation. Systems in
italics are provided for comparison only. Best re-
sults in bold.
Unfortunately, this preliminary evaluation sug-
gests that simpler models perform better, partly
because it is easier to tune them properly both
from computational point of view (e.g. MERT
not stable and prone to overfitting with more fea-
tures7), as well as from software engineering point
of view (debugging of complex pipelines of tools
is demanding). Moreover, simpler models run
faster: ?Moses T? with 12 sents/minute is 4.6
times faster than ?Moses T+C?. (Note that we have
not tuned either of the models for speed.)
While ?Moses T? is probably nearly identical
setup as Google and Univ. of Edinburgh use,
the knowledge of correct language-dependent to-
kenization and the use of relatively high quality
large language model data seems to bring moder-
ate improvements.
6 Conclusion
We described our experiments with a complex lin-
guistically motivated translation system and vari-
ous (again linguistically-motivated) setups of fac-
tored phrase-based translation. An automatic eval-
uation seems to suggest that simpler is better, but
we are well aware that a reliable judgement comes
only from human annotators.
7For ?Moses T+C+C&T+T+G?, we observed BLEU
scores on the test set varying by up to five points absolute
for various weight settings yielding nearly identical dev set
scores.
128
References
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143?146, Columbus, Ohio, June. Association for
Computational Linguistics.
Ondr?ej Bojar, Miroslav Jan??c?ek, Zdene?k ?Zabokrtsky?,
Pavel ?Ces?ka, and Peter Ben?a. 2008. CzEng 0.7:
Parallel Corpus with Community-Supplied Transla-
tions. In Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, May. ELRA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T0 1, Philadelphia.
Va?clav Klimes?. 2006. Analytical and Tectogrammat-
ical Analysis of a Natural Language. Ph.D. thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity, Prague, Czech Rep.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of European Machine Translation Confer-
ence (EAMT 08), pages 102?111, Hamburg, Ger-
many.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT
?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancou-
ver, British Columbia, Canada.
Va?clav Nova?k and Zdene?k ?Zabokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, ed-
itors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th I nternational Conference on
Text, Speech and Dialogue, Lecture Notes in Com-
puter Science, pages 92?98, Pilsen, Czech Repub-
lic. Springer Science+Business Media Deutschland
GmbH.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular Hybrid MT System
with Tectogrammatics Used as Transfer Layer. In
Proc. of the ACL Workshop on Statistical Machine
Translation, pages 167?170, Columbus, Ohio, USA.
129
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 517?527,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Coordination Structures in Dependency Treebanks
Martin Popel, David Marec?ek, Jan S?te?pa?nek, Daniel Zeman, Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics (U?FAL)
Malostranske? na?me?st?? 25, CZ-11800 Praha, Czechia
{popel|marecek|stepanek|zeman|zabokrtsky}@ufal.mff.cuni.cz
Abstract
Paratactic syntactic structures are noto-
riously difficult to represent in depen-
dency formalisms. This has painful con-
sequences such as high frequency of pars-
ing errors related to coordination. In other
words, coordination is a pending prob-
lem in dependency analysis of natural lan-
guages. This paper tries to shed some
light on this area by bringing a system-
atizing view of various formal means de-
veloped for encoding coordination struc-
tures. We introduce a novel taxonomy of
such approaches and apply it to treebanks
across a typologically diverse range of 26
languages. In addition, empirical obser-
vations on convertibility between selected
styles of representations are shown too.
1 Introduction
In the last decade, dependency parsing has grad-
ually been receiving visible attention. One of
the reasons is the increased availability of depen-
dency treebanks, be they results of genuine depen-
dency annotation projects or converted automat-
ically from previously existing phrase-structure
treebanks.
In both cases, a number of decisions have to be
made during the construction or conversion of a
dependency treebank. The traditional notion of
dependency does not always provide unambiguous
solutions, e.g. when it comes to attaching func-
tional words. Worse, dependency representation is
at a loss when it comes to representing paratactic
linguistic phenomena such as coordination, whose
nature is symmetric (two or more conjuncts play
the same role), as opposed to the head-modifier
asymmetry of dependencies.1
1We use the term modifier (or child) for all types of de-
pendent nodes including arguments.
The dominating solution in treebank design is to
introduce artificial rules for the encoding of coor-
dination structures within dependency trees using
the same means that express dependencies, i.e., by
using edges and by labeling of nodes or edges. Ob-
viously, any tree-shaped representation of a coor-
dination structure (CS) must be perceived only as
a ?shortcut? since relations present in coordination
structures form an undirected cycle, as illustrated
already by Tesnie`re (1959). For example, if a noun
is modified by two coordinated adjectives, there
is a (symmetric) coordination relation between the
two conjuncts and two (asymmetric) dependency
relations between the conjuncts and the noun.
However, as there is no obvious linguistic in-
tuition telling us which tree-shaped CS encoding
is better and since the degree of freedom has sev-
eral dimensions, one can find a number of distinct
conventions introduced in particular dependency
treebanks. Variations exist both in topology (tree
shape) and labeling. The main goal of this pa-
per is to give a systematic survey of the solutions
adopted in these treebanks.
Naturally, the interplay of dependency and co-
ordination links in a single tree leads to serious
parsing issues.2 The present study does not try to
decide which coordination style is the best from
the parsing point of view.3 However, we believe
that our survey will substantially facilitate experi-
ments in this direction in the future, at least by ex-
ploring and describing the space of possible can-
didates.
2CSs have been reported to be one of the most frequent
sources of parsing errors (Green and Z?abokrtsky?, 2012; Mc-
Donald and Nivre, 2007; Ku?bler et al, 2009; Collins, 2003).
Their impact on quality of dependency-based machine trans-
lation can also be substantial; as documented on an English-
to-Czech dependency-based translation system (Popel and
Z?abokrtsky?, 2009), 39% of serious translation errors which
are caused by wrong parsing have to do with coordination.
3There might be no such answer, as different CS conven-
tions might serve best for different applications or for differ-
ent parser architectures.
517
The rest of the paper is structured as follows.
Section 2 describes some known problems related
to CS. Section 3 shows possible ?styles? for rep-
resenting CS. Section 4 lists treebanks whose CS
conventions we studied. Section 5 presents empir-
ical observations on CS convertibility. Section 6
concludes the paper.
2 Related work
Let us first recall the basic well-known character-
istics of CSs.
In the simplest case of a CS, a coordinating
conjunction joins two (usually syntactically and
semantically compatible) words or phrases called
conjuncts. Even this simplest case is difficult to
represent within a dependency tree because, in the
words of Lombardo and Lesmo (1998): Depen-
dency paradigms exhibit obvious difficulties with
coordination because, differently from most lin-
guistic structures, it is not possible to characterize
the coordination construct with a general schema
involving a head and some modifiers of it.
Proper formal representation of CSs is further
complicated by the following facts:
? CSs with more than two conjuncts (multi-
conjunct CSs) exist and are frequent.
? Besides ?private? modifiers of individual
conjuncts, there are modifiers shared by
all conjuncts, such as in ?Mary came and
cried?. Shared modifiers may appear along-
side with private modifiers of particular con-
juncts.
? Shared modifiers can be coordinated, too:
?big and cheap apples and oranges?.
? Nested (embedded) coordinations are possi-
ble: ?John and Mary or Sam and Lisa?.
? Punctuation (commas, semicolons, three
dots) is frequently used in CSs, mostly with
multi-conjunct coordinations or juxtaposi-
tions which can be interpreted as CSs with-
out conjunctions (e.g. ?Don?t worry, be
happy!?).
? In many languages, comma or other punctu-
ation mark may play the role of the main co-
ordinating conjunction.
? The coordinating conjunction may be a mul-
tiword expression (?as well as?).
? Deficient CSs with a single conjunct exist.
? Abbreviations like ?etc.? comprise both the
conjunction and the last conjunct.
? Coordination may form very intricate struc-
tures when combined with ellipsis. For ex-
ample, a conjunct can be elided while its ar-
guments remain in the sentence, such as in
the following traditional example: ?I gave
the books to Mary and the records to Sue.?
? The border between paratactic and hypotactic
surface means of expressing coordination re-
lations is fuzzy. Some languages can use en-
clitics instead of conjunctions/prepositions,
e.g. Latin ?Senatus Populusque Romanus?.
Purely hypotactic surface means such as the
preposition in ?John with Mary? occur too.4
? Careful semantic analysis of CSs discloses
additional complications: if a node is mod-
ified by a CS, it might happen that it is
the node itself (and not its modifiers) what
should be semantically considered as a con-
junct. Note the difference between ?red and
white wine? (which is synonymous to ?red
wine and white wine?) and ?red and white
flag of Poland?. Similarly, ?five dogs and
cats? has a different meaning than ?five dogs
and five cats?.
Some of these issues were recognized already
by Tesnie`re (1959). In his solution, conjuncts are
connected by vertical edges directly to the head
and by horizontal edges to the conjunction (which
constitutes a cycle in every CS). Many different
models have been proposed since, out of which the
following are the most frequently used ones:
? MS = Mel?c?uk style used in the Meaning-
Text Theory (MTT): the first conjunct is the
head of the CS, with the second conjunct at-
tached as a dependent of the first one, third
conjunct under the second one, etc. Coor-
dinating conjunction is attached under the
penultimate conjunct, and the last conjunct
is attached under the conjunction (Mel?c?uk,
1988),
? PS = Prague Dependency Treebank (PDT)
style: all conjuncts are attached under the
coordinating conjunction (along with shared
modifiers, which are distinguished by a spe-
cial attribute) (Hajic? et al, 2006),
4As discussed by Stassen (2000), all languages seem to
have some strategy for expressing coordination. Some of
them lack the paratactic surface means (the so called WITH-
languages), but the hypotactic surface means are present al-
most always.
518
? SS = Stanford parser style:5 the first conjunct
is the head and the remaining conjuncts (as
well as conjunctions) are attached under it.
One can find various arguments supporting the
particular choices. MTT possesses a complex
set of linguistic criteria for identifying the gov-
ernor of a relation (see Mazziotta (2011) for an
overview), which lead to MS. MS is preferred in
a rule-based dependency parsing system of Lom-
bardo and Lesmo (1998). PS is advocated by
S?te?pa?nek (2006) who claims that it can represent
shared modifiers using a single additional binary
attribute, while MS would require a more complex
co-indexing attribute. An argumentation of Tratz
and Hovy (2011) follows a similar direction: We
would like to change our [MS] handling of coordi-
nating conjunctions to treat the coordinating con-
junction as the head [PS] because this has fewer
ambiguities than [MS]. . .
We conclude that the influence of the choice of
coordination style is a well-known problem in de-
pendency syntax. Nevertheless, published works
usually focus only on a narrow ad-hoc selection of
few coordination styles, without giving any sys-
tematic perspective.
Choosing a file format presents a different prob-
lem. Despite various efforts to standardize lin-
guistic annotation,6 no commonly accepted stan-
dard exists. The primitive format used for CoNLL
shared tasks is widely used in dependency parsing,
but its weaknesses have already been pointed out
(cf. Stran?a?k and S?te?pa?nek (2010)). Moreover, par-
ticular treebanks vary in their contents even more
than in their format, i.e. each treebank has its own
way of representing prepositions or different gran-
ularity of syntactic labels.
3 Variations in representing
coordination structures
Our analysis of variations in representing coordi-
nation structures is based on observations from a
set of dependency treebanks for 26 languages.7
5We use the already established MS-PS-SS distinction to
facilitate literature overview; as shown in Section 3, the space
of possible coordination styles is much richer.
6For example, TEI (TEI Consortium, 2013), PML (Hana
and S?te?pa?nek, 2012), SynAF (ISO 24615, 2010).
7The primary data sources are the following: Ancient
Greek: Ancient Greek Dependency Treebank (Bamman and
Crane, 2011), Arabic: Prague Arabic Dependency Tree-
bank 1.0 (Smrz? et al, 2008), Basque: Basque Dependency
Treebank (larger version than CoNLL 2007 generously pro-
In accordance with the usual conventions, we as-
sume that each sentence is represented by one de-
pendency tree, in which each node corresponds
to one token (word or punctuation mark). Apart
from that, we deliberately limit ourselves to CS
representations that have shapes of connected sub-
graphs of dependency trees.
We limit our inventory of means of expressing
CSs within dependency trees to (i) tree topology
(presence or absence of a directed edge between
two nodes, Section 3.1), and (ii) node labeling
(additional attributes stored insided nodes, Sec-
tion 3.2).8 Further, we expect that the set of pos-
sible variations can be structured along several di-
mensions, each of which corresponds to a certain
simple characteristic (such as choosing the left-
most conjunct as the CS head, or attaching shared
modifiers below the nearest conjunct). Even if it
does not make sense to create the full Cartesian
product of all dimensions because some values
cannot be combined, it allows to explore the space
of possible CS styles systematically.9
3.1 Topological variations
We distinguish the following dimensions of topo-
logical variations of CS styles (see Figure 1):
Family ? configuration of conjuncts. We di-
vide the topological variations into three main
groups, labeled as Prague (fP), Moscow (fM), and
vided by IXA Group) (Aduriz and others, 2003), Bulgarian:
BulTreeBank (Simov and Osenova, 2005), Czech: Prague
Dependency Treebank 2.0 (Hajic? et al, 2006), Danish: Dan-
ish Dependency Treebank (Kromann et al, 2004), Dutch:
Alpino Treebank (van der Beek and others, 2002), English:
Penn TreeBank 3 (Marcus et al, 1993), Finnish: Turku De-
pendency Treebank (Haverinen et al, 2010), German: Tiger
Treebank (Brants et al, 2002), Greek (modern): Greek De-
pendency Treebank (Prokopidis et al, 2005), Hindi, Ben-
gali and Telugu: Hyderabad Dependency Treebank (Husain
et al, 2010), Hungarian: Szeged Treebank (Csendes et al,
2005), Italian: Italian Syntactic-Semantic Treebank (Mon-
temagni and others, 2003), Latin: Latin Dependency Tree-
bank (Bamman and Crane, 2011), Persian: Persian Depen-
dency Treebank (Rasooli et al, 2011), Portuguese: Floresta
sinta?(c)tica (Afonso et al, 2002), Romanian: Romanian De-
pendency Treebank (Ca?la?cean, 2008), Russian: Syntagrus
(Boguslavsky et al, 2000), Slovene: Slovene Dependency
Treebank (Dz?eroski et al, 2006), Spanish: AnCora (Taule?
et al, 2008), Swedish: Talbanken05 (Nilsson et al, 2005),
Tamil: TamilTB (Ramasamy and Z?abokrtsky?, 2012), Turk-
ish: METU-Sabanci Turkish Treebank (Atalay et al, 2003).
8Edge labeling can be trivially converted to node labeling
in tree structures.
9The full Cartesian product of variants in Figure 1 would
result in topological 216 variants, but only 126 are applicable
(the inapplicable combinations are marked with ??? in Fig-
ure 1). Those 126 topological variants can be further com-
bined with labeling variants defined in Section 3.2.
519
Main family Prague family (code fP)[14 treebanks]
Moscow family (code fM)
[5 treebanks]
Stanford family (code fS)
[6 treebanks]
Choice of head
Head on left (code hL)
[10 treebanks]
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Head on right (code hR)
[14 treebanks]
Mixed head (code hM) [1 treebank] A mixture of hL and hR
Attachment of shared modifiers
Shared modifier
below the nearest conjunct
(code sN)
[15 treebanks]
Shared modifier below head
(code sH)
[11 treebanks]
dogs
dogsaaan, caaaaataaaaaarocaaaaaoc
on
dogs an, cct do
dogs
an, cct
orao 
o 
dogsaaan, caaaataaaarocaaaon
oc
Attachment of coordinating conjunction
Coordinating conjunction
below previous conjunct (code cP)
[2 treebanks]
?
dogs
and,, acst,,racs dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
below following conjunct (code cF)
[1 treebank]
?
dogssadn,
g c,
adn,tssrdn,dog dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
between two conjuncts (code cB)
[8 treebanks]
?
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Coordinating conjunction as the head (code cH)
is the only applicable style for the Prague family [14 treebanks] ? ?
Placement of punctuation
values pP [7 treebanks], pF [1 treebank] and pB [15 treebanks] are analogous to cP, cF and cB
(but applicable also to the Prague family)
Figure 1: Different coordination styles, variations in tree topology. Example phrase: ?(lazy) dogs, cats
and rats?. Style codes are described in Section 3.1.
Stanford (fS) families.10 This first dimension dis-
tinguishes the configuration of conjuncts: in the
Prague family, all the conjuncts are siblings gov-
erned by one of the conjunctions (or a punctuation
fulfilling its role); in the Moscow family, the con-
juncts form a chain where each node in the chain
depends on the previous (or following) node; in
the Stanford family, the conjuncts are siblings ex-
cept for the first (or last) conjunct, which is the
10Names are chosen purely as a mnemonic device, so that
Prague Dependency Treebank belongs to the Prague family,
Mel?c?uk style belongs to the Moscow family, and Stanford
parser style belongs to the Stanford family.
head.11
Choice of head ? leftmost or rightmost. In
the Prague family, the head can be either the left-
most12 (hL) or the rightmost (hR) conjunction or
punctuation. Similarly, in the Moscow and Stan-
ford families, the head can be either the leftmost
(hL) or the rightmost (hR) conjunct. A third op-
11Note that for CSs with just two conjuncts, fM and fS
may look exactly the same (depending on the attachment of
conjunctions and punctuation as described below).
12For simplicity, we use the terms left and right even if
their meaning is reversed for languages with right-to-left
writing systems such as Arabic or Persian.
520
tion (hM) is to mix hL and hR based on some cri-
terion, e.g. the Persian treebank uses hR for coor-
dination of verbs and hL otherwise. For the exper-
iments in Section 5, we choose the head which is
closer to the parent of the whole CS, with the mo-
tivation to make the edge between CS head and its
parent shorter, which may improve parser training.
Attachment of shared modifiers. Shared mod-
ifiers may appear before the first conjunct or after
the last one. Therefore, it seems reasonable to at-
tach shared modifiers either to the CS head (sH),
or to the nearest (i.e. first or last) conjunct (sN).
Attachment of coordinating conjunctions. In
the Moscow family, conjunctions may be either
part of the chain of conjuncts (cB), or they may be
put outside of the chain and attached to the previ-
ous (cP) or following (cF) conjunct. In the Stan-
ford family, conjunctions may be either attached
to the CS head (and therefore between conjuncts)
(cB), or they may be attached to the previous (cP)
or the following (cF) conjunct. The cB option in
both Moscow and Stanford families, treats con-
junctions in the same way as conjuncts (with re-
spect to topology only). In the Prague family, there
is just one option available (cH) ? one of the con-
junctions is the CS head while the others are at-
tached to it.
Attachment of punctuation. Punctuation to-
kens separating conjuncts (commas, semicolons
etc.) could be treated the same way as conjunc-
tions. However, in most treebanks it is treated
differently, so we consider it as well. The val-
ues pP, pF and pB are analogous to cP, cF and
cB except that punctuation may be also attached
to the conjunction in case of pP and pF (other-
wise, a comma before the conjunction would be
non-projectively attached to the member follow-
ing the conjunction).
The three established styles mentioned in Sec-
tion 2 can be defined in terms of the newly intro-
duced abbreviations: PS = fPhRsHcHpB, MS =
fMhLsNcBp?, and SS = fShLsNcBp?.13
3.2 Labeling variations
Most state-of-the-art dependency parsers can pro-
duce labeled edges. However, the parsers produce
only one label per edge. To fully capture CSs,
we need more than one label, because there are
several aspects involved (see the initial assump-
13The question marks indicate that the original Mel?c?uk
and Stanford parser styles ignore punctuation.
tions in Section 3): We need to identify the co-
ordinating conjunction (its POS tag might not be
enough), conjuncts, shared modifiers, and punctu-
ation that separates conjuncts. Besides that, there
should be a label classifying the dependency rela-
tion between the CS and its parent.
Some of the information can be retrieved from
the topology of the tree and the ?main label? of
each node, but not everything. The additional in-
formation can be attached to the main label, but
such approach obscures the logical structure.
In the Prague family, there are two possible
ways to label a conjunction and conjuncts:
Code dU (?dependency labeled at the upper
level of the CS?). The dependency relation of the
whole CS to its parent is represented by the label
of the conjunction, while the conjuncts are marked
with a special label for conjuncts (e.g. ccof in the
Hyderabad Dependency Treebank).
Code dL (?lower level?). The CS is represented
by a coordinating conjunction (or punctuation if
there is no conjunction) with a special label (e.g.
Coord in PDT). Subsequently, each conjunct has
its own label that reflects the dependency relation
towards the parent of the whole CS, therefore, con-
juncts of the same CS can have different labels,
e.g. ?Who[SUBJ] and why[ADV] did it??
Most Prague family treebanks use sH, i.e.
shared modifiers are attached to the head (coor-
dinating conjunction). Each child of the head has
to belong to one of three sets: conjuncts, shared
modifiers, and punctuation or additional conjunc-
tions. In PDT, conjuncts, punctuation and addi-
tional conjunctions are recognized by specific la-
bels. Any other children of the head are shared
modifiers.
In the Stanford and Moscow families, one of
the conjuncts is the head. In practice, it is never la-
beled as a conjunct explicitly, because the fact that
it is a conjunct can be deduced from the presence
of conjuncts among its children. Usually, the other
conjuncts are labeled as conjuncts; conjunctions
and punctuation also have a special label. This
type of labeling corresponds to the dU type.
Alternatively (as found in the Turkish treebank,
dL), all conjuncts in the Moscow chain have their
own dependency labels and the fact that they are
conjuncts follows from the COORDINATION la-
bels of the conjunction and punctuation nodes be-
tween them.
To represent shared modifiers in the Stan-
521
ford and Moscow families, an additional label
is needed again to distinguish between private
and shared modifiers since they cannot be distin-
guished topologically. Moreover, if nested CSs
are allowed, a binary label is not sufficient (i.e.
?shared? versus ?private?) because it also has to
indicate which conjuncts the shared modifier be-
longs to.14
We use the following binary flag codes for cap-
turing which CS participants are distinguished in
the annotation: m01 = shared modifiers anno-
tated; m10 = conjuncts annotated; m11 = both
annotated; m00 = neither annotated.
4 Coordination Structures in Treebanks
In this section, we identify the CS styles defined
in the previous section as used in the primary tree-
bank data sources; statistical observations (such
as the amount of annotated shared modifiers) pre-
sented here, as well as experiments on CS-style
convertibility presented in Section 5.2, are based
on the normalized shapes of the treebanks as con-
tained in the HamleDT 1.0 treebank collection
(Zeman et al, 2012).15
Some of the treebanks were downloaded indi-
vidually from the web, but most of them came
from previously published collections for depen-
dency parsing campaigns: six languages from
CoNLL-2006 (Buchholz and Marsi, 2006), seven
languages from CoNLL-2007 (Nivre et al, 2007),
two languages from CoNLL-2009 (Hajic? and oth-
ers, 2009), three languages from ICON-2010 (Hu-
sain et al, 2010). Obviously, there is a certain
risk that the CS-related information contained in
the source treebanks was slightly biased by the
properties of the CoNLL format upon conversion.
In addition, many of the treebanks were natively
dependency-based (cf. the 2nd column of Table 1),
but some were originally based on constituents
and thus specific converters to the CoNLL for-
mat had to be created (for instance, the Span-
ish phrase-structure trees were converted to de-
pendencies using a procedure described by Civit
et al (2006); similarly, treebank-specific convert-
ers have been used for other languages). Again,
14This is not needed in Prague family where shared modi-
fiers are attached to the conjunction provided that each shared
modifier is shared by conjuncts that form a full subtree to-
gether with their coordinating conjunctions; no exceptions
were found during the annotation process of the PDT.
15A subset of the treebanks whose license
terms permit redistribution is available directly at
http://ufal.mff.cuni.cz/hamledt/.
Danish Romanian
dogsa
n,  anctttr  attt,

ddog
san,n           cntnrnsn     c,n
Hungarian
dogsadnnnn,nnnn ctrdadnnnnrncgdasd
Figure 2: Annotation styles of a few treebanks do
not fit well into the multidimensional space de-
fined in Section 3.1.
there is some risk that the CS-related information
contained in treebanks resulting from such conver-
sions is slightly different from what was intended
in the very primary annotation.
There are several other languages (e.g. Esto-
nian or Chinese) which are not included in our
study, despite of the fact that constituency tree-
banks do exist for them. The reason is that the
choice of their CS style would be biased, because
no independent converters exist ? we would have
to convert them to dependencies ourselves. We
also know about several more dependency tree-
banks that we have not processed yet.
Table 1 shows 26 languages whose treebanks
we have studied from the viewpoint of their CS
styles. It gives the basic quantitative properties of
the treebanks, their CS style in terms of the tax-
onomy introduced in Section 3, as well as statis-
tics related to CSs: the average number of CSs per
100 tokens, the average number of conjuncts per
one CS, the average number of shared modifiers
per one CS,16 and the percentage of nested CSs
among all CSs. The reader can return to Figure
1 to see the basic statistics on the ?popularity? of
individual design decisions among the developers
of dependency treebanks or constituency treebank
converters.
CS styles of most treebanks are easily classifi-
able using the codes introduced in Section 3, plus
a few additional codes:
? p0 = punctuation was removed from the tree-
bank.
16All non-Prague family treebanks are marked sN and
m00 or m10, (i.e. shared modifiers not marked in the origi-
nal annotation, but attached to the head conjunct) because we
found no counterexamples (modifiers attached to a conjunct,
but not the nearest one). The HamleDT normalization proce-
dure contains a few heuristics to detect shared modifiers, but
it cannot recover the missing distinction reliably, so the num-
bers in the ?SMs/CJ? column are mostly underestimated.
522
Language Orig. Data Sents. Tokens Original CS CSs / CJs / SMs / Nested RT
type set style code 100 tok. CS CS CS[%] UAS
Ancient
Greek dep prim. 31 316 461 782 fP hR sH cH pB dL m11 6.54 2.17 0.16 10.3 97.86
Arabic dep C07 3 043 116 793 fP hL sH cH pB dL m00 3.76 2.42 0.13 10.6 96.69
Basque dep prim. 11 225 151 593 fP hR sN cH pP dU m00 3.37 2.09 0.03 5.1 99.32
Bengali dep I10 1 129 7 252 fP hR sH cH pP dU m11 4.87 1.71 0.05 24.1 99.97
Bulgarian phr C06 13 221 196 151 fS hL sN cB pB dU m10 2.99 2.19 0.00 0.0 99.74
Czech dep C07 25 650 437 020 fP hR sH cH pB dL m11 4.09 2.16 0.20 14.6 99.42
Danish dep C06 5 512 100 238 fS* hL sN cP pB dU m10 3.68 1.93 0.13 7.5 99.76
Dutch phr C06 13 735 200 654 fP hR sN cH pP dU m10 2.06 2.17 0.05 3.3 99.47
English phr C07 40 613 991 535 fP hR sH cH pB dU m10 2.07 2.33 0.05 6.3 99.84
Finnish dep prim. 4 307 58 576 fS hL sN cB pB dU m10 4.06 2.41 0.00 6.4 99.70
German phr C09 38 020 680 710 fM hL sN cP pP dU m10 2.79 2.09 0.01 0.0 99.73
Greek dep C07 2 902 70 223 fP hR sH cH pB dL m11 3.25 2.48 0.18 7.2 99.43
Hindi dep I10 3 515 77 068 fP hR sH cH pP dU m11 2.45 1.97 0.04 10.3 98.35
Hungarian phr C07 6 424 139 143 fT hX sN cX pX dL m00 2.37 1.90 0.01 2.2 99.84
Italian dep C07 3 359 76 295 fS hL sN cB pB dU m10 3.32 2.02 0.03 3.8 99.51
Latin dep prim. 3 473 53 143 fP hR sH cH pB dL m11 6.74 2.24 0.41 12.3 97.45
Persian dep prim. 12 455 189 572 fM*hM sN cB pP dU m00 4.18 2.10 0.18 3.7 99.82
Portuguese phr C06 9 359 212 545 fS hL sN cB pB dU m10 2.51 1.95 0.26 11.1 99.16
Romanian dep prim. 4 042 36 150 fP* hR sN cH p0 dU m10 1.80 2.00 0.00 0.0 100.00
Russian dep prim. 34 895 497 465 fM hL sN cB p0 dU m10 4.02 2.02 0.07 3.9 99.86
Slovene dep C06 1 936 35 140 fP hR sH cH pB dL m00 4.31 2.49 0.00 10.8 98.87
Spanish phr C09 15 984 477 810 fS hL sN cB pB dU m10 2.79 1.98 0.14 12.7 99.24
Swedish phr C06 11 431 197 123 fM hL sN cF pF dU m10 3.94 2.19 0.13 0.7 99.66
Tamil dep prim. 600 9 581 fP hR sH cH pB dL m11 1.66 2.46 0.22 3.8 99.67
Telugu dep I10 1 450 5 722 fP hR sH cH pP dU m11 3.48 1.59 0.06 5.0 100.00
Turkish dep C07 5 935 69 695 fM hR sN cB pB dL m10 3.81 2.04 0.00 34.3 99.23
Table 1: Overview of analyzed treebanks. prim. = primary source; C06?C09 = CoNLL 2006?2009;
I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in
nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip
experiment described in Section 5. Style codes are defined in Sections 3 and 4.
? fM* = Persian treebank uses a mix of fM and
fS: fS for coordination of verbs and fM oth-
erwise.
Figure 2 shows three other anomalies:
? fS* = Danish treebank employs a mixture of
fS and fM, where the last conjunct is attached
indirectly via the conjunction.
? fP* = Romanian treebank omits punctuation
tokens and multi-conjunct coordinations get
split.
? fT = Hungarian Szeged treebank uses
?Tesnie`re family? ? disconnected graphs for
CSs where conjuncts (and conjunction and
punctuation) are attached directly to the par-
ent of CS, and so the other style dimensions
are not applicable (hX, cX, pX).
5 Empirical Observations on
Convertibility of Coordination Styles
The various styles cannot represent the CS-related
information to the same extent. For example,
it is not possible to represent nested CSs in the
Moscow and Stanford families without signifi-
cantly changing the number of possible labels.17
The dL style (which is most easily applicable to
the Prague family) can represent coordination of
different dependency relations. This is again not
possible in the other styles without adding e.g. a
special ?prefix? denoting the relations.
We can see that the Prague family has a greater
expressive power than the other two families: it
can represent complex CSs using just one addi-
tional binary label, distinguishing between shared
modifiers and conjuncts. A similar additional label
is needed in the other styles to distinguish between
shared and private modifiers.
Because of the different expressive power, con-
verting a CS from one style to another may
lead to a loss of information. For example, as
17Mel?c?uk uses ?grouping? to nest CSs ? cf. related so-
lutions involving coindexing or bubble trees (Kahane, 1997).
However, these approaches were not used in any of the re-
searched treebanks. To combine grouping with shared modi-
fiers, each group in a tree should have a different identifier.
523
there is no way of representing shared modifiers
in the Moscow family without an additional at-
tribute, converting a CS with shared modifiers
from Prague to Moscow family makes the modi-
fiers private. When converting back, one can use
certain heuristics to handle the most obvious cases,
but sometimes the modifiers will stay private (very
often, the nature of a modifier depends on context
or is debatable even for humans, e.g. ?Young boys
and girls?).
5.1 Transformation algorithm
We developed an algorithm to transform one CS
style to another. Two subtasks must be solved by
the algorithm: identification of individual CSs and
their participants, and transforming of the individ-
ual CSs.
Obviously, the individual CSs cannot be trans-
formed independently because of coordination
nesting. For instance, when transforming a nested
coordination from the Prague style to the Moscow
style (e.g. to fMhL), the leftmost conjunct in the
inner (lower) coordination must climb up to be-
come the head of the inner CS, but then it must
climb up once again to become the head of the
outer (upper) CS too. This shows that inner CSs
must be transformed first.
We tackle this problem by a depth-first recur-
sion. When going down the tree, we only recog-
nize all the participants of the CSs, classify them
and gather them in a separate data structure (one
for each visited CS). The following four types
of CS participants are distinguished: coordinat-
ing conjunctions, conjuncts, shared modifiers, and
punctuations that separate conjuncts.18 No change
of the tree is performed during these descent steps.
When returning back from the recursion (i.e.,
when climbing from a node back up to its par-
ent), we test whether the abandoned node is the
topmost node of some CS. If so, then this CS is
transformed, which means that its participants are
rehanged and relabelled according the the target
CS style.
This procedure naturally guarantees that the in-
18Conjuncts are explicitly marked in most styles. Coordi-
nating conjunctions can be usually identified with the help of
dependency labels and POS tags. Punctuation separating con-
juncts can be detected with high accuracy using simple rules.
If shared modifiers are not annotated (code m00 or m10),
one can imagine rule-based heuristics or special classifiers
trained to distinguish shared modifiers. For the experiments
in this section, we use the HamleDT gold annotation attribute
is shared modifier.
ner CSs are transformed first and that all CSs are
transformed when the recursions returns to the
root.
5.2 Roundtrip experiment
The number of possible conversion directions ob-
viously grows quadratically with the number of
styles. So far, we limited ourselves only to con-
versions from/to the style of the HamleDT tree-
bank collection, which contains all the treebanks
under our study already converted into a com-
mon scheme. The common scheme is based
on the conventions of PDT, whose CS style is
fPhRsHcHpB.19
We selected nine styles (3 families times 3 head
choices) and transformed all the HamleDT scheme
treebanks to these nine styles and back, which we
call a roundtrip. Resulting averaged unlabeled at-
tachment scores (UAS, evaluated against the Ham-
leDT scheme) in the last column of Table 1 indi-
cate that the percentage of transformation errors
(i.e. tokens attached to a different parent after the
roundtrip) is lower than 1% for 20 out of the 26
languages.20 A manual inspection revealed two
main error sources. First, as noted above, the Stan-
ford and Moscow families have lower expressive
power than the Prague family, so naturally, the in-
verse transformation was ambiguous and the trans-
formation heuristics were not capable of identify-
ing the correct variant every time. Second, we also
encountered inconsistencies in the original tree-
banks (which we were not trying to fix in Ham-
leDT for now).
6 Conclusions and Future Work
We described a (theoretically very large) space of
possible representations of CSs within the depen-
dency framework. We pointed out a range of de-
tails that make CSs a really complex phenomenon;
anyone dealing with CSs in treebanking should
take these observations into account.
We proposed a taxonomy of those approaches
19As documented in Zeman et al (2012), the normalization
procedures used in HamleDT embrace many other phenom-
ena as well (not only those related to coordination), and in-
volve both structural transformation and dependency relation
relabeling.
20Table 1 shows that Latin and Ancient Greek treebanks
have on average more than 6 CSs per 100 tokens, more than
2 conjuncts per CS, and Latin has also the highest number of
shared modifiers per CS. Therefore the percentage of nodes
affected by the roundtrip is the highest for these languages
and the lower roundtrip UAS is not surprising.
524
that have been argued for in literature or employed
in real treebanks.
We studied 26 existing treebanks of different
languages. For each value of each dimension in
Figure 1, we found at least one treebank where the
value is used; even so, several treebanks take their
own unique path that cannot be clearly classified
under the taxonomy (the taxonomy could indeed
be extended, for the price of being less clearly ar-
ranged).
We discussed the convertibility between the var-
ious styles and implemented a universal tool that
transforms between any two styles of the taxon-
omy. The tool achieves a roundtrip accuracy close
to 100%. This is important because it opens the
door to easily switching coordination styles for
parsing experiments, phrase-to-dependency con-
version etc.
While the focus of this paper is to explore and
describe the expressive power of various annota-
tion styles, we did not address the learnability of
the styles by parsers. That will be a complemen-
tary point of view, and thus a natural direction of
future work for us.
Acknowledgments
We thank the providers of the primary data re-
sources. The work on this project was sup-
ported by the Czech Science Foundation grants
no. P406/11/1499 and P406/2010/0875, and by
research resources of the Charles University in
Prague (PRVOUK). This work has been using lan-
guage resources developed and/or stored and/or
distributed by the LINDAT-Clarin project of the
Ministry of Education of the Czech Republic
(project LM2010013). Further, we would like to
thank Jan Hajic?, Ondr?ej Dus?ek and four anony-
mous reviewers for many useful comments on the
manuscript of this paper.
References
Itzair Aduriz et al 2003. Construction of a Basque de-
pendency treebank. In Proceedings of the 2nd Work-
shop on Treebanks and Linguistic Theories.
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In LREC, pages 1968?1703.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank. In
Proceedings of the 4th Intern. Workshop on Linguis-
tically Interpreteted Corpora (LINC).
David Bamman and Gregory Crane. 2011. The An-
cient Greek and Latin dependency treebanks. In
Language Technology for Cultural Heritage, Theory
and Applications of Natural Language Processing,
pages 79?98. Springer Berlin Heidelberg.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency treebank for Russian: Concept, tools,
types of information. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 987?991. Association for Computational Lin-
guistics Morristown, NJ, USA.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Montserrat Civit, Maria Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: From constituents to
dependencies. In FinTAL, volume 4139 of Lec-
ture Notes in Computer Science, pages 141?152.
Springer.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged treebank. In
TSD, volume 3658 of Lecture Notes in Computer
Science, pages 123?131. Springer.
Mihaela Ca?la?cean. 2008. Data-driven dependency
parsing for Romanian. Master?s thesis, Uppsala
University, August.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k Z?abokrtsky?, and Andreja Z?ele. 2006.
Towards a Slovene dependency treebank. In LREC
2006, pages 1388?1391, Genova, Italy. European
Language Resources Association (ELRA).
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hy-
brid combination of constituency and dependency
trees into an ensemble dependency parser. In Pro-
ceedings of the Workshop on Innovative Hybrid Ap-
proaches to the Processing of Textual Data, pages
19?26, Avignon, France. Association for Computa-
tional Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova?-Raz??mova?. 2006. Prague Dependency
Treebank 2.0. CD-ROM, Linguistic Data Consor-
tium, LDC Catalog No.: LDC2006T01, Philadel-
phia.
525
Jan Hajic? et al 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in mul-
tiple languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
Jirka Hana and Jan S?te?pa?nek. 2012. Prague
markup language framework. In Proceedings of the
Sixth Linguistic Annotation Workshop, pages 12?
21, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics, Association for Computational
Linguistics.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking Finnish. In Proceedings of
the Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9), pages 79?90.
Samar Husain, Prashanth Mannem, Bharat Ambati,
and Phani Gadde. 2010. The ICON-2010 tools
contest on Indian language dependency parsing. In
Proceedings of ICON-2010 Tools Contest on Indian
Language Dependency Parsing, Kharagpur, India.
ISO 24615. 2010. Language resource management ?
Syntactic annotation framework (SynAF).
Sylvain Kahane. 1997. Bubble trees and syntactic
representations. In Proceedings of the 5th Meeting
of the Mathematics of the Language, DFKI, Saar-
brucken.
Matthias T. Kromann, Line Mikkelsen, and Stine Kern
Lynge. 2004. Danish dependency treebank.
Sandra Ku?bler, Erhard Hinrichs, Wolfgang Maier, and
Eva Klett. 2009. Parsing coordinations. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 406?414,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Vincenzo Lombardo and Leonardo Lesmo. 1998. Unit
coordination and gapping in dependency theory. In
Processing of Dependency-Based Grammars; pro-
ceedings of the workshop. COLING-ACL, Montreal.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Nicolar Mazziotta. 2011. Coordination of verbal de-
pendents in Old French: Coordination as a specified
juxtaposition or apposition. In Proceedings of In-
ternational Conference on Dependency Linguistics
(DepLing 2011).
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Simonetta Montemagni et al 2003. Building the Ital-
ian syntactic-semantic treebank. In Building and us-
ing Parsed Corpora, Language and Speech series,
pages 189?210, Dordrecht. Kluwer.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of the
NODALIDA Special Session on Treebanks.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
2007 Shared Task. EMNLP-CoNLL, June.
Martin Popel and Zdene?k Z?abokrtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Prokopis Prokopidis, Elina Desipri, Maria Koutsom-
bogera, Harris Papageorgiou, and Stelios Piperidis.
2005. Theoretical and practical issues in the con-
struction of a Greek dependency treebank. In Pro-
ceedings of the 4th Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 149?160.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2012.
Prague dependency style treebank for Tamil. In
Proceedings of LREC 2012, pages 23?25, I?stanbul,
Turkey. European Language Resources Association.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
Persian verbs: The first steps towards Persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231, Poznan?, Poland.
Kiril Simov and Petya Osenova. 2005. Extending
the annotation of BulTreeBank: Phase 2. In The
Fourth Workshop on Treebanks and Linguistic Theo-
ries (TLT 2005), pages 173?184, Barcelona, Decem-
ber.
Otakar Smrz?, Viktor Bielicky?, Iveta Kour?ilova?, Jakub
Kra?c?mar, Jan Hajic?, and Petr Zema?nek. 2008.
Prague Arabic dependency treebank: A word on the
million words. In Proceedings of the Workshop on
Arabic and Local Languages (LREC) 2008, pages
16?23, Marrakech, Morocco. European Language
Resources Association.
Leon Stassen. 2000. And-languages and with-
languages. Linguistic Typology, 4(1):1?54.
Jan S?te?pa?nek. 2006. Capturing a Sentence Struc-
ture by a Dependency Relation in an Annotated Syn-
tactical Corpus (Tools Guaranteeing Data Consis-
tence) (in Czech). Ph.D. thesis, Charles Univer-
526
sity in Prague, Faculty of Mathematics and Physics,
Prague, Czech Republic.
Pavel Stran?a?k and Jan S?te?pa?nek. 2010. Represent-
ing layered and structured data in the CoNLL-ST
format. In Alex Fang, Nancy Ide, and Jonathan
Webster, editors, Proceedings of the Second Inter-
national Conference on Global Interoperability for
Language Resources, pages 143?152, Hong Kong,
China. City University of Hong Kong, City Univer-
sity of Hong Kong.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel annotated cor-
pora for Catalan and Spanish. In LREC. European
Language Resources Association.
TEI Consortium. 2013. TEI P5: Guidelines for Elec-
tronic Text Encoding and Interchange.
Lucien Tesnie`re. 1959. Ele?ments de syntaxe struc-
turale. Paris.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP, pages 1257?1268, Edin-
burgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Leonoor van der Beek et al 2002. Chapter 5. The
Alpino dependency treebank. In Algorithms for Lin-
guistic Processing NWO PIONIER Progress Report,
Groningen, The Netherlands.
Daniel Zeman, David Marec?ek, Martin Popel,
Loganathan Ramasamy, Jan S?te?pa?nek, Zdene?k
Z?abokrtsky?, and Jan Hajic?. 2012. HamleDT: To
parse or not to parse? In Proceedings of LREC 2012,
pages 2735?2741, I?stanbul, Turkey. European Lan-
guage Resources Association.
527
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201?206,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Maximum Entropy Translation Model
in Dependency-Based MT Framework
David Marec?ek, Martin Popel, Zdene?k Z?abokrtsky?
Charles University in Prague, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{marecek,popel,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Maximum Entropy Principle has been
used successfully in various NLP tasks. In
this paper we propose a forward transla-
tion model consisting of a set of maxi-
mum entropy classifiers: a separate clas-
sifier is trained for each (sufficiently fre-
quent) source-side lemma. In this way
the estimates of translation probabilities
can be sensitive to a large number of fea-
tures derived from the source sentence (in-
cluding non-local features, features mak-
ing use of sentence syntactic structure,
etc.). When integrated into English-to-
Czech dependency-based translation sce-
nario implemented in the TectoMT frame-
work, the new translation model signif-
icantly outperforms the baseline model
(MLE) in terms of BLEU. The perfor-
mance is further boosted in a configuration
inspired by Hidden Tree Markov Mod-
els which combines the maximum entropy
translation model with the target-language
dependency tree model.
1 Introduction
The principle of maximum entropy states that,
given known constraints, the probability distri-
bution which best represents the current state of
knowledge is the one with the largest entropy.
Maximum entropy models based on this princi-
ple have been widely used in Natural Language
Processing, e.g. for tagging (Ratnaparkhi, 1996),
parsing (Charniak, 2000), and named entity recog-
nition (Bender et al, 2003). Maximum entropy
models have the following form
p(y|x) =
1
Z(x)
exp
?
i
?ifi(x, y)
where fi is a feature function, ?i is its weight, and
Z(x) is the normalizing factor
Z(x) =
?
y
exp
?
i
?ifi(x, y)
In statistical machine translation (SMT), trans-
lation model (TM) p(t|s) is the probability that the
string t from the target language is the translation
of the string s from the source language. Typical
approach in SMT is to use backward translation
model p(s|t) according to Bayes? rule and noisy-
channel model. However, in this paper we deal
only with the forward (direct) model.1
The idea of using maximum entropy for con-
structing forward translation models is not new. It
naturally allows to make use of various features
potentially important for correct choice of target-
language expressions. Let us adopt a motivat-
ing example of such a feature from (Berger et al,
1996) (which contains the first usage of maxent
translation model we are aware of): ?If house ap-
pears within the next three words (e.g., the phrases
in the house and in the red house), then dans might
be a more likely [French] translation [of in].?
Incorporating non-local features extracted from
the source sentence into the standard noisy-
channel model in which only the backward trans-
lation model is available, is not possible. This
drawback of the noisy-channel approach is typi-
cally compensated by using large target-language
n-gram models, which can ? in a result ? play a
role similar to that of a more elaborate (more con-
text sensitive) forward translation model. How-
ever, we expect that it would be more beneficial to
exploit both the parallel data and the monolingual
data in a more balance fashion, rather than extract
only a reduced amount of information from the
parallel data and compensate it by large language
model on the target side.
1A backward translation model is used only for pruning
training data in this paper.
201
A deeper discussion on the potential advantages
of maximum entropy approach over the noisy-
channel approach can be found in (Foster, 2000)
and (Och and Ney, 2002), in which another suc-
cessful applications of maxent translation models
are shown. Log-linear translation models (instead
of MLE) with rich feature sets are used also in
(Ittycheriah and Roukos, 2007) and (Gimpel and
Smith, 2009); the idea can be traced back to (Pap-
ineni et al, 1997).
What makes our approach different from the
previously published works is that
1. we show how the maximum entropy trans-
lation model can be used in a dependency
framework; we use deep-syntactic depen-
dency trees (as defined in the Prague Depen-
dency Treebank (Hajic? et al, 2006)) as the
transfer layer,
2. we combine the maximum entropy transla-
tion model with target-language dependency
tree model and use tree-modified Viterbi
search for finding the optimal lemmas label-
ing of the target-tree nodes.
The rest of the paper is structured as follows. In
Section 2 we give a brief overview of the trans-
lation framework TectoMT in which the experi-
ments are implemented. In Section 3 we describe
how our translation models are constructed. Sec-
tion 4 summarizes the experimental results, and
Section 5 contains a summary.
2 Translation framework
We use tectogrammatical (deep-syntactic) layer of
language representation as the transfer layer in the
presented MT experiments. Tectogrammatics was
introduced in (Sgall, 1967) and further elaborated
within the Prague Dependency Treebank project
(Hajic? et al, 2006). On this layer, each sentence
is represented as a tectogrammatical tree, whose
main properties (from the MT viewpoint) are fol-
lowing: (1) nodes represent autosemantic words,
(2) edges represent semantic dependencies (a node
is an argument or a modifier of its parent), (3) there
are no functional words (prepositions, auxiliary
words) in the tree, and the autosemantic words ap-
pear only in their base forms (lemmas). Morpho-
logically indispensable categories (such as number
with nouns or tense with verbs, but not number
with verbs as it is only imposed by agreement) are
stored in separate node attributes (grammatemes).
The intuition behind the decision to use tec-
togrammatics for MT is the following: we be-
lieve that (1) tectogrammatics largely abstracts
from language-specific means (inflection, agglu-
tination, functional words etc.) of expressing
non-lexical meanings and thus tectogrammatical
trees are supposed to be highly similar across lan-
guages,2 (2) it enables a natural transfer factor-
ization,3 (3) and local tree contexts in tectogram-
matical trees carry more information (especially
for lexical choice) than local linear contexts in the
original sentences.4
In order to facilitate transfer of sentence ?syn-
tactization?, we work with tectogrammatical nodes
enhanced with the formeme attribute (Z?abokrtsky?
et al, 2008), which captures the surface mor-
phosyntactic form of a given tectogrammatical
node in a compact fashion. For example, the
value n:pr?ed+4 is used to label semantic nouns
that should appear in an accusative form in a
prepositional group with the preposition pr?ed in
Czech. For English we use formemes such as
n:subj (semantic noun (SN) in subject position),
n:for+X (SN with preposition for), n:X+ago (SN
with postposition ago), n:poss (possessive form of
SN), v:because+fin (semantic verb (SV) as a sub-
ordinating finite clause introduced by because),
v:without+ger (SV as a gerund after without), adj:attr
(semantic adjective (SA) in attributive position),
adj:compl (SA in complement position).
We have implemented our experiments in the
TectoMT software framework, which already of-
fers tool chains for analysis and synthesis of Czech
and English sentences (Z?abokrtsky? et al, 2008).
The translation scenario proceeds as follows.
1. The input English text is segmented into sen-
tences and tokens.
2. The tokens are lemmatized and tagged with
Penn Treebank tags using the Morce tagger
(Spoustova? et al, 2007).
2This claim is supported by error analysis of output of
tectogrammatics-based MT system presented in (Popel and
Z?abok/rtsky?, 2009), which shows that only 8 % of translation
errors are caused by the (obviously too strong) assumption
that the tectogrammatical tree of a sentence and the tree rep-
resenting its translation are isomorphic.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
?denser?, especially when translating from/to a language with
rich inflection such as Czech.
4Recall the house-is-somewhere-around feature in the in-
troduction; again, the fact that we know the dominating (or
dependent) word should allow to construct a more compact
translation model, compared to n-gram models.
202
Figure 1: Intermediate sentence representations when translating the English sentence ?However, this
very week, he tried to find refuge in Brazil.?, leading to the Czech translation ?Pr?esto se tento pra?ve?
ty?den snaz?il naj??t u?toc?is?te? v Braz??lii.?.
3. Then the Maximum Spanning Tree parser
(McDonald et al, 2005) is applied and a
surface-syntax dependency tree (analytical
tree in the PDT terminology) is created for
each sentence (Figure 1a).
4. This tree is converted to a tectogrammatical
tree (Figure 1b). Each autosemantic word
with its associated functional words is col-
lapsed into a single tectogrammatical node,
labeled with lemma, formeme, and seman-
tically indispensable morphologically cate-
gories; coreference is also resolved. Collaps-
ing edges are depicted by wider lines in the
Figure 1a.
5. The transfer phase follows, whose most dif-
ficult part consists in labeling the tree with
target-side lemmas and formemes5 (changes
of tree topology are required relatively infre-
quently). See Figure 1c.
6. Finally, surface sentence shape (Figure 1d) is
synthesized from the tectogrammatical tree,
which is basically a reverse operation for the
5In this paper we focus on using maximum entropy
for translating lemmas, but it can be used for translating
formemes as well.
tectogrammatical analysis: adding punctua-
tion and functional words, spreading mor-
phological categories according to grammat-
ical agreement, performing inflection (using
Czech morphology database (Hajic?, 2004)),
arranging word order etc.
3 Training the two models
In this section we describe two translation mod-
els used in the experiments: a baseline translation
model based on maximum likelihood estimates
(3.2), and a maximum entropy based model (3.3).
Both models are trained using the same data (3.1).
In addition, we describe a target-language tree
model (3.4), which can be combined with both
the translation models using the Hidden Tree
Markov Model approach and tree-modified Viterbi
search, similarly to the approach of (Z?abokrtsky?
and Popel, 2009).
3.1 Data preprocessing common for both
models
We used Czech-English parallel corpus CzEng 0.9
(Bojar and Z?abokrtsky?, 2009) for training the
translation models. CzEng 0.9 contains about
8 million sentence pairs, and also their tectogram-
matical analyses and node-wise alignment.
203
We used only trees from training sections (about
80 % of the whole data), which contain around 30
million pairs of aligned tectogrammatical nodes.
From each pair of aligned tectogrammatical
nodes, we extracted triples containing the source
(English) lemma, the target (Czech) lemma, and
the feature vector.
In order to reduce noise in the training data,
we pruned the data in two ways. First, we dis-
regarded all triples whose lemma pair did not oc-
cur at least twice in the whole data. Second,
we computed forward and backward maximum
likelihood (ML) translation models (target lemma
given source lemma and vice versa) and deleted
all triples whose probability according to one of
the two models was lower than the threshold 0.01.
Then the forward ML translation model was
reestimated using only the remaining data.
For a given pair of aligned nodes, the feature
vector was of course derived only from the source-
side node or from the tree which it belongs to. As
already mentioned in the introduction, the advan-
tage of the maximum entropy approach is that a
rich and diverse set of features can be used, with-
out limiting oneself to linearly local context. The
following features (or, better to say, feature tem-
plates, as each categorical feature is in fact con-
verted to a number of 0-1 features) were used:
? formeme and morphological categories of the
given node,
? lemma, formeme and morphological cate-
gories of the governing node,
? lemmas and formemes of all child nodes,
? lemmas and formemes of the nearest linearly
preceding and following nodes.
3.2 Baseline translation model
The baseline TM is basically the ML translation
model resulting from the previous section, lin-
early interpolated with several translation models
making use of regular word-formative derivations,
which can be helpful for translating some less fre-
quent (but regularly derived) lemmas. For exam-
ple, one of the derivation-based models estimates
the probability p(zaj??mave?|interestingly) (possibly
unseen pair of deadjectival adverbs) by the value
of p(zaj??mavy?|interesting). More detailed descrip-
tion of these models goes beyond the scope of this
paper; their weights in the interpolation are very
small anyway.
3.3 MaxEnt translation model
The MaxEnt TM was created as follows:
1. training triples (source lemma, target lemma,
feature vector) were disregarded if the source
lemma was not seen at least 50 times (only
the baseline model will be used for such lem-
mas),
2. the remaining triples were grouped by the En-
glish lemma (over 16 000 groups),
3. due to computational issues, the maximum
number of triples in a group was reduced to
1000 by random selection,
4. a separate maximum entropy classifier
was trained for each group (i.e., one
classifier per source-side lemma) using
AI::MaxEntropy Perl module,6
5. due to the more aggressive pruning of the
training data, coverage of this model is
smaller than that of the baseline model; in or-
der not to loose the coverage, the two mod-
els were combined using linear interpolation
(1:1).
Selected properties of the maximum entropy
translation model (before the linear interpolation
with the baseline model) are shown in Figure 2.
We increased the size of the training data from
10 000 training triples up to 31 million and eval-
uated three relative quantities characterizing the
translation models:
? coverage - relative frequency of source lem-
mas for which the translation model offers at
least one translation,
? first - relative frequency of source lemmas for
which the target lemmas offered as the first
by the model (argmax) are the correct ones,
? oracle - relative frequency of source lemmas
for which the correct target lemma is among
the lemmas offered by the translation model.
As mentioned in Section 3.1, there are context
features making use both of local linear context
and local tree context. After training the MaxEnt
model, there are about 4.5 million features with
non-zero weight, out of which 1.1 million features
6http://search.cpan.org/perldoc?AI::
MaxEntropy
204
Figure 2: Three measures characterizing the Max-
Ent translation model performance, depending on
the training data size. Evaluated on aligned node
pairs from the dtest portion of CzEng 0.9.
are derived from the linear context and 2.4 million
features are derived from the tree context. This
shows that the MaxEnt translation model employs
the dependency structure intensively.
A preliminary analysis of feature weights seems
to support our intuition that the linear context
is preferred especially in the case of more sta-
ble collocations. For example, the most impor-
tant features for translating the lemma bare are
based on the lemma of the following noun: tar-
get lemma bosy? (barefooted) is preferred if the fol-
lowing noun on the source side is foot, while holy?
(naked, unprotected) is preferred if hand follows.
The contribution of dependency-based features
can be illustrated on translating the word drop.
The greatest weight for choosing kapka (a droplet)
as the translation is assigned to the feature captur-
ing the presence of a node with formeme n:of+X
among the node?s children. The greatest weights
in favor of odhodit (throw aside) are assigned to
features capturing the presence of words such as
gun or weapon, while the greatest weights in favor
of klesnout (to come down) are assigned to fea-
tures saying that there is the lemma percent or the
percent sign among the children.
Of course, the lexical choice is influenced also
by the governing lemmas, as can be illustrated
with the word native. One can find a high-
value feature for rodily? (native-born) saying that
the source-side parent is speaker; similarly for
mater?sky? (mother) with governing tongue, and
rodny? (home) with land.
Linear and tree features are occasionally used
simultaneously: there are high-valued positive
configuration BLEU NIST
baseline TM 10.44 4.795
MaxEnt TM 11.77 5.135
baseline TM + TreeLM 11.77 5.038
MaxEnt TM + TreeLM 12.58 5.250
Table 1: BLEU and NIST evaluation of four con-
figurations of our MT system; the WMT 2010 test
set was used.
weights for translating order as objednat (reserve,
give an order for st.) assigned both to tree-based
features saying that there are words such as pizza,
meal or goods and to linear features saying that the
very following word is some or two.
3.4 Target-language tree model
Although the MaxEnt TM captures some contex-
tual dependencies that are covered by language
models in the standard noisy-channel SMT, it may
still be beneficial to exploit target-language mod-
els, because these can be trained on huge mono-
lingual corpora. We use a target-language depen-
dency tree model differing from standard n-gram
model in two aspects:
? it uses tree context instead of linear context,
? it predicts tectogrammatical attributes (lem-
mas and formemes) instead of word forms.
In particular, our target-language tree model
(TreeLM) predicts the probability of node?s
lemma and formeme given its parent?s lemma and
formeme. The optimal (lemma and formeme) la-
beling is found by tree-modified Viterbi search;
for details see (Z?abokrtsky? and Popel, 2009).
4 Experiments
When included into the above described transla-
tion scenario, the MaxEnt TM outperforms the
baseline TM, be it used together with or with-
out TreeLM. The results are summarized in Ta-
ble 1. The improvement is statistically signif-
icant according to paired bootstrap resampling
test (Koehn, 2004). In the configuration without
TreeLM the improvement is greater (1.33 BLEU)
than with TreeLM (0.81 BLEU), which confirms
our hypothesis that MaxEnt TM captures some of
the contextual dependencies resolved otherwise by
language models.
205
5 Conclusions
We have introduced a maximum entropy transla-
tion model in dependency-based MT which en-
ables exploiting a large number of feature func-
tions in order to obtain more accurate translations.
The BLEU evaluation proved significant improve-
ment over the baseline solution based on the trans-
lation model with maximum likelihood estimates.
However, the performance of this system still be-
low the state of the art (which is around BLEU 16
for the English-to-Czech direction).
Acknowledgments
This research was supported by the grants
MSM0021620838, MS?MT C?R LC536, FP7-ICT-
2009-4-247762 (Faust), FP7-ICT-2007-3-231720
(EuroMatrix Plus), GA201/09/H057, and GAUK
116310. We thank two anonymous reviewers for
helpful comments.
References
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proceedings of CoNLL 2003, pages
148?151.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathemati-
cal Linguistics, 92:63?83.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the ACL conference, pages
132?139, San Francisco, USA.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52, Morristown, USA.
Association for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2009. Feature-
rich translation by quasi-synchronous lattice pars-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 219?228, Morristown, USA. Association for
Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Candace L. Sidner, Tanja
Schultz, Matthew Stone, and ChengXiang Zhai, edi-
tors, HLT-NAACL, pages 57?64. The Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, volume 4, pages 388?395.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT / EMNLP, pages 523?530, Vancouver,
Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295?302.
Kishore A. Papineni, Salim Roukos, and Todd R.
Ward. 1997. Feature-based language understand-
ing. In European Conference on Speech Commu-
nication and Technology (EUROSPEECH), pages
1435?1438, Rhodes, Greece, September.
Martin Popel and Zdene?k Z?abok/rtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In In Proceedings
of EMNLP?96, pages 133?142.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k Z?abokrtsky? and Martin Popel. 2009. Hidden
markov tree model in dependency-based machine
translation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 145?148, Sun-
tec, Singapore.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL, pages 167?170.
206
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 1?11,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Grain of Salt for the WMT Manual Evaluation?
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{bojar,popel}@ufal.mff.cuni.cz
ercegovcevic@hotmail.com
Omar F. Zaidan
Department of Computer Science
Johns Hopkins University
ozaidan@cs.jhu.edu
Abstract
The Workshop on Statistical Machine
Translation (WMT) has become one of
ACL?s flagship workshops, held annually
since 2006. In addition to soliciting pa-
pers from the research community, WMT
also features a shared translation task for
evaluating MT systems. This shared task
is notable for having manual evaluation as
its cornerstone. The Workshop?s overview
paper, playing a descriptive and adminis-
trative role, reports the main results of the
evaluation without delving deep into ana-
lyzing those results. The aim of this paper
is to investigate and explain some interest-
ing idiosyncrasies in the reported results,
which only become apparent when per-
forming a more thorough analysis of the
collected annotations. Our analysis sheds
some light on how the reported results
should (and should not) be interpreted, and
also gives rise to some helpful recommen-
dation for the organizers of WMT.
1 Introduction
The Workshop on Statistical Machine Translation
(WMT) has become an annual feast for MT re-
searchers. Of particular interest is WMT?s shared
translation task, featuring a component for man-
ual evaluation of MT systems. The friendly com-
petition is a source of inspiration for participating
teams, and the yearly overview paper (Callison-
Burch et al, 2010) provides a concise report of the
state of the art. However, the amount of interest-
ing data collected every year (the system outputs
? This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of
the Czech Republic), P406/10/P259, MSM 0021620838, and
DARPA GALE program under Contract No. HR0011-06-2-
0001. We are grateful to our students, colleagues, and the
three reviewers for various observations and suggestions.
and, most importantly, the annotator judgments)
is quite large, exceeding what the WMT overview
paper can afford to analyze with much depth.
In this paper, we take a closer look at the data
collected in last year?s workshop, WMT101, and
delve a bit deeper into analyzing the manual judg-
ments. We focus mainly on the English-to-Czech
task, as it included a diverse portfolio of MT sys-
tems, was a heavily judged language pair, and also
illustrates interesting ?contradictions? in the re-
sults. We try to explain such points of interest,
and analyze what we believe to be the positive and
negative aspects of the currently established eval-
uation procedure of WMT.
Section 2 examines the primary style of man-
ual evaluation: system ranking. We discuss how
the interpretation of collected judgments, the com-
putation of annotator agreement, and document
that annotators? individual preferences may render
two systems effectively incomparable. Section 3
is devoted to the impact of embedding reference
translations, while Section 4 and Section 5 discuss
some idiosyncrasies of other WMT shared tasks
and manual evaluation in general.
2 The System Ranking Task
At the core of the WMT manual evaluation is the
system ranking task. In this task, the annotator
is presented with a source sentence, a reference
translation, and the outputs of five systems over
that source sentence. The instructions are kept
minimal: the annotator is to rank the presented
translations from best to worst. Ties are allowed,
but the scale provides five rank labels, allowing the
annotator to give a total order if desired.
The five assigned rank labels are submitted at
once, making the 5-tuple a unit of annotation. In
the following, we will call this unit a block. The
blocks differ from each other in the choice of the
1http://www.statmt.org/wmt10
1
Language Pair Systems Blocks Labels Comparisons Ref ? others Intra-annot. ? Inter-annot. ?
German-English 26 1,050 5,231 10,424 0.965 0.607 0.492
English-German 19 1,407 6,866 13,694 0.976 0.560 0.512
Spanish-English 15 1,140 5,665 11,307 0.989 0.693 0.508
English-Spanish 17 519 2,591 5,174 0.935 0.696 0.594
French-English 25 837 4,156 8,294 0.981 0.722 0.452
English-French 20 801 3,993 7,962 0.917 0.636 0.449
Czech-English 13 543 2,691 5,375 0.976 0.700 0.504
English-Czech 18 1,395 6,803 13,538 0.959 0.620 0.444
Average 19 962 4,750 9,471 0.962 0.654 0.494
Table 1: Statistics on the collected rankings, quality of references and kappas across language pairs. In
general, a block yields a set of five rank labels, which yields a set of
(5
2
)
= 10 pairwise comparisons.
Due to occasional omitted labels, the Comparisons/Blocks ratio is not exactly 10.
source sentence and the choice of the five systems
being compared. A couple of tricks are introduced
in the sampling of the source sentences, to en-
sure that a large enough number of judgments is
repeated across different screens for meaningful
computation of inter- and intra-annotator agree-
ment. As for the sampling of systems, it is done
uniformly ? no effort is made to oversample or un-
dersample a particular system (or a particular pair
of systems together) at any point in time.
In terms of the interface, the evaluation utilizes
the infrastructure of Amazon?s Mechanical Turk
(MTurk)2, with each MTurk HIT3 containing three
blocks, corresponding to three consecutive source
sentences.
Table 1 provides a brief comparison of the vari-
ous language pairs in terms of number of MT sys-
tems compared (including the reference), number
of blocks ranked, the number of pairwise com-
parisons extracted from the rankings (one block
with 5 systems ranked gives 10 pairwise compar-
isons, but occasional unranked systems are ex-
cluded), the quality of the reference (the percent-
age of comparisons where the reference was better
or equal than another system), and the ? statistic,
which is a measure of agreement (see Section 2.2
for more details).4
We see that English-to-Czech, the language pair
on which we focus, is not far from the average in
all those characteristics except for the number of
collected comparisons (and blocks), making it the
second most evaluated language pair.
2http://www.mturk.com/
3?HIT? is an acronym for human intelligence task, which
is the MTurk term for a single screen presented to the anno-
tator.
4We only use the ?expert? annotations of WMT10, ignor-
ing the data collected from paid annotators on MTurk, since
they were not part of the official evaluation.
2.1 Interpreting the Rank Labels
The description in the WMT overview paper says:
?Relative ranking is our official evaluation met-
ric. [Systems] are ranked based on how frequently
they were judged to be better than or equal to
any other system.? (Emphasis added.) The WMT
overview paper refers to this measure as ?? oth-
ers?, with a variant of it called ?> others? that does
not reward ties.
We first note that this description is somewhat
ambiguous, and an uninformed reader might in-
terpret it in one of two different ways. For some
system A, each block in which A appears includes
four implicit pairwise comparisons (against the
other presented systems). How is A?s score com-
puted from those comparisons?
The correct interpretation is that A is re-
warded once for each of the four comparisons in
which A wins (or ties).5 In other words, A?s score
is the number of pairwise comparisons in which
A wins (or ties), divided by the total number of
pairwise comparisons involving A. We will use
?? others? (resp. ?> others?) to refer to this inter-
pretation, in keeping with the terminology of the
overview paper.
The other interpretation is that A is rewarded
only if A wins (or ties) all four comparisons. In
other words, A?s score is the number of blocks in
whichA wins (or ties) all comparisons, divided by
the number of blocks in which A appears. We will
use ?? all in block? (resp. ?> all in block?) to
refer to this interpretation.6
5Personal communication with WMT organizers.
6There is yet a third interpretation, due to a literal read-
ing of the description, where A is rewarded at most once per
block if it wins (or ties) any one of its four comparisons. This
is probably less useful: it might be good at identifying the
bottom tier of systems, but would fail to distinguish between
all other systems.
2
REF C
U
-B
O
JA
R
C
U
-T
E
C
T
O
E
U
R
O
T
R
A
N
S
O
N
L
IN
E
B
P
C
-T
R
A
N
S
U
E
D
IN
? others 95.9 65.6 60.1 54.0 70.4 62.1 62.2
> others 90.5 45.0 44.1 39.3 49.1 49.4 39.6
? all in block 93.1 32.3 30.7 23.4 37.5 32.5 28.1
> all in block 81.3 13.6 19.0 13.3 15.6 18.7 10.6
Table 2: Sentence-level ranking scores for the
WMT10 English-Czech language pair. The ??
others? and ?> others? scores reproduced here
exactly match numbers published in the WMT10
overview paper. A boldfaced score marks the best
system in a given row (besides the reference).
For quality control purposes, the WMT organiz-
ers embed the reference translations as a ?system?
alongside the actual entries (the idea being that an
annotator clicking randomly would be easy to de-
tect, since they would not consistently rank the
reference ?system? highly). This means that the
reference is as likely as any other system to ap-
pear in a block, and when the score for a system A
is computed, pairwise comparisons with the refer-
ence are included.
We use the publicly released human judgments7
to compute the scores of systems participating in
the English-Czech subtask, under both interpreta-
tions. Table 2 reports the scores, with our ?? oth-
ers? (resp. ?> others?) scores reproduced exactly
matching those reported in Table 21 of the WMT
overview paper. (For clarity, Table 2 is abbreviated
to include only the top six systems of twelve.)
Our first suggestion is that both measures could
be reported in future evaluations, since each tells
us something different. The first interpretation
gives partial credit for an MT system, hence distin-
guishing systems from each other at a finer level.
This is especially important for a language pair
with relatively few annotations, since ?? others?
would produce a larger number of data points (four
per system per block) than ?? all in block? (one
per system per block). Another advantage of the
official ?? others? is greater robustness towards
various factors like the number of systems in the
competition, the number of systems in one block
or the presence of the reference in the block (how-
ever, see Section 3).
As for the second interpretation, it helps iden-
tify whether or not a single system (or a small
group of systems) is strongly dominant over the
other systems. For the systems listed in Table 2,
7http://statmt.org/wmt10/results.html
-
10 0
 
10
 
20
 
30
 
40
 
50
 
60  1
0
 
20
 
30
 
40
 
50
 
60
 
70
 
80
>= All in Block
>= 
Othe
rs
Czec
h-En
glish
Engl
ish-C
zech
Engl
ish-F
renc
h
Engl
ish-G
erma
n
Engl
ish-S
pani
sh
Fren
ch-E
nglis
h
Germ
an-E
nglis
h
Span
ish-E
nglis
h
a*x
+b
Figure 1: ?? all in block? and ?? others? provide
very similar ordering of systems.
?> all in block? suggests its potential in the con-
text of system combination: CU-TECTO and PC-
TRANS win almost one fifth of the blocks in which
they appear, despite the fact that either a refer-
ence translation or a combination system already
appears alongside them. (See also Table 4 below.)
Also, note that if the ranking task were designed
specifically to cater to the ?? all in block? inter-
pretation, it would only have two ?rank? labels (ba-
sically, ?top? and ?non-top?). In that case, an-
notators would spend considerably less time per
block than they do now, since all they need to do
is identify the top system(s) per block, without dis-
tinguishing non-top systems from each other.
Even for those interested in distinguishing non-
state-of-the-art systems from each other, we point
out that the ?? all in block? interpretation ulti-
mately gives a system ordering that is very simi-
lar to that of the official ?? others? interpretation,
even for the lower-tier systems (Figure 1).
2.2 Annotator Agreement
The WMT10 overview paper reports inter- and
intra-annotator agreement over the pairwise com-
parisons, to show the validity of the evaluation
setup and the ?? others? metric. Agreement is
quantified using the following formula:
? =
P (A)? P (E)
1? P (E)
(1)
where P (A) is the proportion of times two anno-
tators are observed to agree, and P (E) is the ex-
pected proportion of times two annotators would
agree by chance. Note that ? has a value of at most
1, with higher ? values indicating higher rates of
agreement. The ? measure is more meaningful
3
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9 1  
0
 
5
 
10
 
15
 
20
 
25
 
30
 
35
Kappa
Sour
ce le
ngth
Intra
. inc
l. ref
.
Intra
. exc
l. ref
.
Inter
. inc
l. ref
.
Inter
. exc
l. ref
.
Mod
erate
 agre
eme
nt
Figure 2: Intra-/inter-annotator agreement
with/without references, across various source
sentence lengths (lengths of n and n + 1 are used
to plot the point at x = n). This figure is based on
all language pairs.
than reporting P (A) as is, since it takes into ac-
count, via P (E), how ?surprising? it is for annota-
tors to agree in the first place.
In the context of pairwise comparisons, an
agreement between two annotators occurs when
they compare the same pair of systems (S1,S2),
and both agree on their relative ranking: either
S1 > S2, S1 = S2, or S1 < S2. P (E) is then:
P (E) = P 2(S1>S2)+P 2(S1=S2)+P 2(S1<S2) (2)
In the WMT overview paper, all three cate-
gories are assumed equally likely, giving P (E) =
1
9 +
1
9 +
1
9 =
1
3 . For consistency with the WMT
overview paper, and unless otherwise noted, we
also use P (E) = 13 whenever a ? value is re-
ported. (Though see Section 2.2.2 for a discussion
about P (E).)
2.2.1 Observed Agreement for Different
Sentence Lengths
In Figure 2 we plot the ? values across different
source sentence lengths. We see that the inter-
annotator agreement (when excluding references)
is reasonably high only for sentences up to 10
words in length ? according to Landis and Koch
(1977), and as cited by the WMT overview paper,
not even ?moderate? agreement can be assumed if
? is less than 0.4. Another popular (and controver-
sial) rule of thumb (Krippendorff, 1980) is more
strict and says that ? < 0.67 is not suitable even
for tentative conclusions.
For this reason, and given that a majority of sen-
tences are indeed more than 10 words in length
(the median is 20 words), we suggest that future
evaluations either include fewer outputs per block,
or divide longer sentences into shorter segments
(e.g. on clause boundaries), so these segments are
more easily and reliably comparable. The latter
suggestions assumes word alignment as a prepro-
cessing and presenting the annotators the context
of the judged segment.
2.2.2 Estimating P (E), the Expected
Agreement by Chance
Several agreement measures (usually called kap-
pas) were designed based on the Equation 1 (see
Artstein and Poesio (2008) and Eugenio and Glass
(2004) for an overview and a discussion). Those
measures differ from each other in how to de-
fine the individual components of Equation 2, and
hence differ in what the expected agreement by
chance (P (E)) would be:8
? The S measure (Bennett et al, 1954) assumes
a uniform distribution over the categories.
? Scott?s pi (Scott, 1955) estimates the distribu-
tion empirically from actual annotation.
? Cohen?s ? (Cohen, 1960) estimates the dis-
tribution empirically as well, and further as-
sumes a separate distribution for each anno-
tator.
Given that the WMT10 overview paper assumes
that the three categories (S1 > S2, S1 = S2, and
S1 < S2) are equally likely, it is using the S mea-
sure version of Equation 1, though it does not ex-
plicitly say so ? it simply calls it ?the kappa coef-
ficient? (K).
Regardless of what the measure should be
called, we believe that the uniform distribution it-
self is not appropriate, even though it seems to
model a ?random clicker? adequately. In partic-
ular, and given the design of the ranking inter-
face, 13 is an overestimate of P (S1 = S2) for
a random clicker, and should in fact be 15 : each
system receives one of five rank labels, and for
two systems to receive the same rank label, there
are only five (out of 25) label pairs that satisfy
S1 = S2. Therefore, with P (S1 = S2) = 15 ,
8These three measures were later generalized to more than
two annotators (Fleiss, 1971; Bartko and Carpenter, 1976),
Thus, without loss of generality, our examples involve two
annotators.
4
?? Others? S pi
Inter incl. ref. 0.487 0.454excl. ref. 0.439 0.403
Intra incl. ref. 0.633 0.609excl. ref. 0.601 0.575
Table 3: Summary of two variants of kappa: S
(or K as it is reported in the WMT10 paper) and
our proposed Scott?s pi. We report inter- vs. intra-
annotator agreement and collected from all com-
parisons (?incl. ref.?) vs. collected only from
comparisons without the reference (?excl. ref.?)
because it is generally easier to agree that the ref-
erence is better than the other systems. This table
is based on all language pairs.
we have P (S1 > S2) = P (S1 < S2) = 25 , and
therefore P (E) = 0.36 rather than 0.333.
Taking the discussion a step further, we actually
advocate following the idea of Scott?s pi, whereby
the distribution of each category is estimated em-
pirically from the actual annotation, rather than
assuming a random annotator ? these frequencies
are easy to compute, and reflect a more meaning-
ful P (E).9
Under this interpretation, P (S1 = S2) is cal-
culated to be 0.168, reflecting the fraction of pair-
wise comparisons that correspond to a tie. (Note
that this further supports the claim that setting
P (S1 = S2) = 13 for a random clicker, as used
in the WMT overview paper, is an overestimate.)
This results in P (E) = 0.374, yielding, for in-
stance, pi = 0.454 for ?? others? inter-annotator
agreement, somewhat lower than ? = 0.487 (re-
ported in Table 3).
We do note that the difference is rather small,
and that our aim is to be mathematically sound
above all. Carefully defining P (E) would be im-
portant when comparing kappas across different
tasks with different P (E), or when attempting
to satisfy certain thresholds (as the cited 0.4 and
0.67). Furthermore, if one is interested in mea-
suring agreement for individual annotators, such
as identifying those who have unacceptably low
intra-annotator agreement, the question of P (E) is
quite important, since annotation behavior varies
noticeably from one annotator to another. A ?con-
servative? annotator who prefers to rank systems
as being tied most of the time would have a high
9We believe that P (E) should not reflect the chance that
two random annotators would agree, but the chance that two
actual annotators would agree randomly. The two sound sub-
tly related but are actually quite different.
P (E), whereas an annotator using ties moderately
would have a low P (E). Hence, two annotators
with equal agreement rates (P (A)) are not neces-
sarily equally proficient, since their P (E) might
differ considerably.10
2.3 The ? variant vs. the > variant
Even within the same interpretation of how sys-
tems could be scored, there is a question of
whether or not to reward ties. The overview paper
reports both variants of its measure, but does not
note that there are non-trivial differences between
the two orderings. Compare for example the ??
others? ordering vs. the ?> others? ordering of
CU-BOJAR and PC-TRANS (Table 2), showing an
unexpected swing of 7.9%:
? others > others
CU-BOJAR 65.6 45.0
PC-TRANS 62.1 49.4
CU-BOJAR seems better under the? variant, but
loses out when only strict wins are rewarded. The-
oretically, this could be purely due to chance, but
the total number of pairwise comparisons in ??
others? is relatively large (about 1,500 pairwise
comparisons for each system), and ought to can-
cel such effects.
A similar pattern could be seen under the ?all in
block? interpretation as well (e.g. for CU-TECTO
and ONLINEB). Table 4 documents this effect by
looking at how often a system is the sole winner
of a block. Comparing PC-TRANS and CU-BOJAR
again, we see that PC-TRANS is up there with CU-
TECTO and DCU-COMBO as the most frequent sole
winners, winning 71 blocks, whereas CU-BOJAR
is the sole winner of only 53 blocks. This is in
spite of the fact that PC-TRANS actually appeared
in slightly fewer blocks than CU-BOJAR (385 vs.
401).
One possible explanation is that the two vari-
ants (??? and ?>?) measure two subtly different
things about MT systems. Digging deeper into Ta-
ble 2?s values, we find that CU-BOJAR is tied with
another system 65.6 ? 45.0 = 20.4% of the time,
while PC-TRANS is tied with another system only
62.1? 49.4 = 12.7% of the time. So it seems that
PC-TRANS?s output is noticeably different from
another system more frequently than CU-BOJAR,
which reduces the number of times that annotators
10Who?s more impressive: a psychic who correctly pre-
dicts the result of a coin toss 50% of the time, or a psychic
who correctly predicts the result of a die roll 50% of the time?
5
Blocks Sole Winner
305 Reference
73 CU-TECTO
71 PC-TRANS
70 DCU-COMBO
57 RWTH-COMBO
54 ONLINEB
53 CU-BOJAR
46 EUROTRANS
41 UEDIN
41 UPV-COMBO
175 One of eight other systems
409 No sole winner
1395 Total English-to-Czech Blocks
Table 4: A breakdown of the 1,395 blocks for the
English-Czech task, according to which system (if
any) is the sole winner. On average, a system ap-
pears in 388 blocks.
mark PC-TRANS as tied with another system.11 In
that sense, the ??? ranking is hurting PC-TRANS,
since it does not benefit from its small number of
ties. On the other hand, the ?>? variant would not
reward CU-BOJAR for its large number of ties.
The ?? others? score may be artificially boosted
if several very similar systems (and therefore
likely to be ?tied?) take part in the evaluation.12
One possible solution is to completely disregard
ties and calculate the final score as winswins+losses . We
recommend to use this score instead of ?? others?
( wins+tieswins+ties+losses ) which is biased toward often tied
systems, and ?> others? ( winswins+ties+losses ) which is
biased toward systems with few ties.
2.4 Surprise? Does the Number of
Evaluations Affect a System?s Score?
When examining the system scores for the
English-Czech task, we noticed a surprising pat-
tern: it seemed that the more times a system is
sampled to be judged, the lower its ?? others?
score (?? all in block? behaving similarly). A
scatter plot of a system?s score vs. the number of
blocks in which it appears (Figure 3) makes the
pattern obvious.
We immediately wondered if the pattern holds
in other language pairs. We measured Pearson?s
correlation coefficient within each language pair,
reported in Table 5. As it turns out, English-
11Indeed, PC-TRANS is a commercial system (manually)
tuned over a long period of time and based on resources very
different from what other participants in WMT use.
12In the preliminary WMT11 results, this seems to hap-
pen to four Moses-like systems (UEDIN, CU-BOJAR, CU-
MARECEK and CU-TAMCHYNA) which have better ?? oth-
ers? score but worse ?> others? score than CU-TECTO.
Correlation of Block Count
Source Target vs. ?? Others?
English Czech -0.558
English Spanish -0.434
Czech English -0.290
Spanish English -0.240
English French -0.227
English German -0.161
French English -0.024
German English 0.146
Overall -0.092
Table 5: Pearson?s correlation between the num-
ber of blocks where a system was ranked and the
system?s ?? others? score. (The reference itself is
not included among the considered systems).
 
30
 
35
 
40
 
45
 
50
 
55
 
60
 
65
 
70
 
75
 
80  35
0
 
360
 
370
 
380
 
390
 
400
 
410
 
420
>= Others
Num
ber o
f judg
ments
cmu
-hea
field
-com
bo
cu-
bojar
cu-
tecto cu
-ze
ma
n
dcu
dcu-
com
bo
eur
otra
ns
koc
koc-
com
bo
onlin
eA
onlin
eB
pc-tr
ans
pots
dam
rwth
-com
bo
sfu
uedi
n
upv-
com
bo
a*x
+b
Figure 3: A plot of ?? others? system score vs.
times judged, for English-Czech.
Czech happened to be the one language pair where
the ?correlation? is strongest, with only English-
Spanish also having a somewhat strong correla-
tion. Overall, though, there is a consistent trend
that can be seen across the language pairs. Could
it really be the case that the more often a system is
judged, the worse its score gets?
Examining plots for the other language pairs
makes things a bit clearer. Consider for example
the plot for English-Spanish (Figure 4). As one
would hope, the data points actually come together
to form a cloud, indicating a lack of correlation.
The reason that a hint of a correlation exists is the
presence of two outliers in the bottom right cor-
ner. In other words, the very worst systems are,
indeed, the ones judged quite often. We observed
this pattern in several other language pairs as well.
The correlation naturally does not imply cau-
sation. We are still not sure how to explain the
artifact. A subtle possibility lies in the MTurk
interface: annotators have the choice to accept a
HIT or skip it before actually providing their la-
6
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  13
0
 
135
 
140
 
145
 
150
 
155
 
160
 
165
 
170
>= Others
Num
ber o
f judg
mentscamb
ridge
cmu
-hea
field
-com
bo
cu-
zem
an
dcu dfk
ijhu
koc
koc-
com
bo
onlin
eA
onlin
eB
rwth
-com
bo
sfu
uedi
n upb-
com
bo
upv
upv-
nnlm
a*x
+b
Figure 4: A plot of ?? others? system score vs.
times judged, for English-Spanish.
bels. It might be the case that some annotators are
more willing to accept HITs when there is an ob-
viously poor system (since that would make their
task somewhat easier), and who are more prone
to skipping HITs where the systems seem hard to
distinguish from each other. So there might be a
causation effect after all, but in the reverse order:
a system gets judged more often if it is a bad sys-
tem.13 A suggestion from the reviewers is to run a
pilot annotation with deliberate inclusion of a poor
system among the ranked ones.
2.5 Issues of Pairwise Judgments
The WMT overview paper also provides pairwise
system comparisons: each cell in Table 6 indicates
the percentage of pairwise comparisons between
the two systems where the system in the column
was ranked better (>) than the system in the row.
For instance, there are 81 ranking responses where
both CU-TECTO and CU-BOJAR were present and
indeed ranked14 among the 5 systems in the block.
In 37 (45.7%) of the cases, CU-TECTO was ranked
better, in 29 (35.8%), CU-BOJAR was ranked better
and there was a tie in the remaining 15 (18.5%)
cases. The ties are not explicitly shown in Table 6
but they are implied by the total of 100%. The cell
is in bold where there was a win in the pairwise
comparison, so 45.7 is bold in our example.
An interesting ?discrepancy? in Table 6 is that
CU-TECTO wins pairwise comparisons with CU-
BOJAR and UEDIN but it scores worse than them
in the official ?? others?, cf. Table 2. Simi-
larly, UEDIN outperformed ONLINEB in the pair-
13No pun intended!
14The users sometimes did not fill any rank for a system.
Such cases are ignored.
R
E
F
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
E
U
R
O
T
R
A
N
S
O
N
L
IN
E
B
P
C
-T
R
A
N
S
U
E
D
IN
REF - 4.3 4.3 5.1 3.8 3.6 2.3
CU-BOJAR 87.1 - 45.7 28.3 44.4 39.5 41.1
CU-TECTO 88.2 35.8 - 38.0 55.8 44.0 36.0
EUROTRANS 88.5 60.9 46.8 - 50.7 53.8 48.6
ONLINEB 91.2 31.1 29.1 32.8 - 43.8 39.3
PC-TRANS 88.0 45.3 42.9 28.6 49.3 - 36.6
UEDIN 94.3 39.3 44.2 31.9 32.1 49.5 -
Table 6: Pairwise comparisons extracted from
sentence-level rankings of the WMT10 English-
Czech News Task. Re-evaluated to reproduce the
numbers published in WMT10 overview paper.
Bold in column A and row B means that system
A is pairwise better than system B.
wise comparisons but it was ranked worse in both
> and ? official comparison.
In the following, we focus on the CU-BOJAR
(B) and CU-TECTO (T) pair because they are in-
teresting competitors on their own. They both use
the same parallel corpus for lexical mapping but
operate very differently: CU-BOJAR is based on
Moses while CU-TECTO transfers at a deep syn-
tactic layer and generates target text which is more
or less grammatically correct but suffers in lexical
choice.
2.5.1 Different Set of Sentences
The mismatch in the outcomes of ?? others? and
pairwise comparisons could be caused by different
set of sentences. The pairwise ranking is collected
from the set of blocks where both CU-BOJAR and
CU-TECTO appeared (and were indeed ranked).
Each of the systems however competes in other
blocks as well, which contributes to the official ??
others?.
The set of sentences underlying the comparison
is very different and more importantly that the ba-
sis for pairwise comparisons is much smaller than
the basis of the official ?? others? interpretation.
The outcome of the official interpretation however
depends on the random set of systems your system
was compared to. In our case, it is impossible to
distinguish, whether CU-TECTO had just bad luck
on sentences and systems it was compared to when
CU-BOJAR was not in the block and/or whether the
81 blocks do not provide a reliable picture.
2.5.2 Pairwise Judgments Unreliable
To complement WMT10 rankings for the two sys-
tems and avoid the possible lower reliability due
to 5-fold ranking instead of a targeted compari-
7
Author of B says:
both both
B>T T>B fine wrong Total
T
sa
ys
:
B>T 9 - 1 1 11
T>B 2 13 - 3 18
both fine 2 - 2 3 7
both wrong 10 5 1 11 27
Total 23 18 4 18 63
Table 7: Additional annotation of 63 CU-BOJAR
(B) vs. CU-TECTO (T) sentences by two annota-
tors.
Better Both
Annotator B T fine wrong
A 24 23 5 11
C 10 12 5 36
D 32 20 2 9
M 11 18 7 27
O 23 18 4 18
Z 25 27 2 9
Total 125 118 25 110
Table 8: Blurry picture of pairwise rankings of
CU-BOJAR vs. CU-TECTO. Wins in bold.
son, we asked the main authors of both CU-BOJAR
and CU-TECTO to carry out a blind pairwise com-
parison on the exact set of 63 sentences appearing
across the 81 blocks in which both systems were
ranked. As the totals in Table 7 would suggest,
each author unwittingly recognized his system and
slightly preferred it. The details however reveal a
subtler reason for the low agreement: one of the
annotators was less picky about MT quality and
accepted 10+5 sentences completely rejected by
the other annotator. In total, these two annotators
agreed on 9 + 13 + 2 + 11 = 35 (56%) of cases
and their pairwise ? is 0.387.
A further annotation of these 63 sentences by
four more people completes the blurry picture:
the pairwise ? for each pair of our five annota-
tors ranges from 0.242 to 0.615 with the aver-
age 0.407?0.106. The multi-annotator ? (Fleiss,
1971) is 0.394 and all six annotators agree on a
single label only in 24% of cases. The agree-
ment is not better even if we merge the categories
?Both fine? and ?Both wrong? into a single one:
The pairwise ? ranges from 0.212 to 0.620 with
the average 0.405?0.116, the multi-annotator ? is
0.391. Individual annotations are given in Table 8.
Naturally, the set of these 63 sentences is not a
representative sample. Even if one of the systems
SRC It?s not completely ideal.
REF Nen?? to u?plne? idea?ln??. Ranks
PC-TRANS To nen?? u?plne? idea?ln??. 2 5
CU-BOJAR To nen?? u?plne? idea?ln??. 5 4
Table 9: Two rankings by the same annotator.
SRC FCC awarded a tunnel in Slovenia for 64 million
REF FCC byl pr?ide?len tunel ve Slovinsku za 64 milionu?
Gloss FCC was awarded a tunnel in Slovenia for 64 million
HYP1 FCC pr?ide?lil tunel ve Slovinsku za 64 milio?nu?
HYP2 FCC pr?ide?lila tunel ve Slovinsku za 64 milionu?
Gloss FCC awardedmasc/fem a tunnel in Slovenia for 64 million
Figure 5: A poor reference translation confuses
human judges. The SRC and REF differ in the ac-
tive/passive form, attributing completely different
roles to ?FCC?.
actually won, such an observation could not have
been generalized to other test sets. The purpose
of the exercise was to check whether we are at all
able to agree which of the systems translates this
specific set of sentences better. As it turns out,
even a simple pairwise ranking can fail to pro-
vide an answer because different annotators sim-
ply have different preferences.
Finally, Table 9 illustrates how poor the
WMT10 rankings can be. The exact same string
produced by two systems was ranked differently
each time ? by the same annotator. (The hypothe-
sis is a plausible translation, only the information
structure of the sentence is slightly distorted so the
translation may not fit well it the surrounding con-
text.)
3 The Impact of the Reference
Translation
3.1 Bad Reference Translations
Figure 5 illustrates the impact of poor reference
translation on manual ranking as carried out in
Section 2.5.2. Of our six independent annotations,
three annotators marked the hypotheses as ?both
fine? given the match with the source and three
annotators marked them as ?both wrong? due to
the mismatch with the reference. Given the con-
struction of the WMT test set, this particular sen-
tence comes from a Spanish original and it was
most likely translated directly to both English and
Czech.
8
Correlation of
Source Target Reference vs. ?? others?
Spanish English 0.341
English French 0.164
French English 0.098
German English 0.088
Czech English -0.041
English Czech -0.145
English Spanish -0.411
English German -0.433
Overall -0.107
Table 10: Pearson?s correlation of the relative per-
centage of blocks where the reference was in-
cluded in the ranking and the final ?? others?
of the system (the reference itself is not included
among the considered systems).
 
25
 
30
 
35
 
40
 
45
 
50
 
55
 
60
 
65
 
70
 
75  0.1
9
 
0.2
 
0.21
 
0.22
 
0.23
 
0.24
 
0.25
 
0.26
>= Others
Rela
tive 
pres
ence
 of th
e ref
eren
ce
cmu
-hea
field
-com
bo
cu-
zem
an
dfki
fbk
jhu
kit
koc
koc-
com
bo
lims
i
liu
onlin
eA
onlin
eB
rwth
rwth
-com
bo sfu
uedi
n
upps
ala
upv-
com
bo
a*x
+b
Figure 6: Correlation of the presence of the ref-
erence and the official ?? others? for English-
German evaluation.
3.2 Reference Can Skew Pairwise
Comparisons
The exact set of competing systems in each 5-fold
ranking in WMT10 evaluation is random. The ??
others? however is affected by this: a system may
suffer more losses if often compared to the refer-
ence, and similarly it may benefit from being com-
pared to a poor competitor.
To check this, we calculate the correlation be-
tween the relative presence of the reference among
the blocks where a system was judged and the
system?s official ?? others? score. Across lan-
guage, there is almost no correlation (Pearson?s
coefficient: ?0.107). However, for some language
pairs, the correlation is apparent, as listed in Ta-
ble 10. Negative correlation means: the more of-
ten the system was compared along with the refer-
ence, the worse the score of the system.
Figure 6 plots the extreme case of English-
German evaluation.
Source Target Min Avg?StdDev Max
English Czech 40 65?19 115
English French 40 66?17 110
English German 10 40?16 80
English Spanish 30 54?15 85
Czech English 5 38?13 60
French English 5 37?15 70
German English 10 32?12 65
Spanish English 35 56?11 70
Table 11: The number of post-edits per system for
each language pair to complement Figure 3 (page
12) of the WMT10 overview paper.
4 Other WMT10 Tasks
4.1 Blind Post-Editing Unreliable
WMT often carries out one more type of manual
evaluation: ?Editing the output of systems without
displaying the source or a reference translation,
and then later judging whether edited translations
were correct.? (Callison-Burch et al, 2010). We
call the evaluation ?blind post-editing? for short.
We feel that blind post-editing is more infor-
mative than system ranking. First, it constitutes
a unique comprehensibility test, and after all, MT
should aim at comprehensible output in the first
place. Second, blind post-editing can be further
analyzed to search for specific errors in system
output, see Bojar (2011) for a preliminary study.
Unfortunately, the amount of post-edits col-
lected in WMT10 varied a lot across systems and
language pairs. Table 11 provides the minimum,
average and maximum number of post-edits of
outputs of a particular MT system. We see that
e.g. while English-to-Czech has many judgments
of this kind per system, Czech-to-English is one of
the worst supported directions.
It is not surprising that conclusions based on 5
observations can be extremely deceiving. For in-
stance CU-BOJAR seems to produce 60% of out-
puts comprehensible (and thus wins in Figure 3 on
page 12 in the WMT overview paper), far better
than CMU. This is not in line with the ranking re-
sults where both rank equally (Table 5 on page 10
in the WMT overview paper). In fact, CU-BOJAR
was post-edited 5 times and 3 of these post-edits
were acceptable while CMU was post-edited 30
times and 5 of these post-edits were acceptable.
4.2 A Remark on System Combination Task
One results of WMT10 not observed in previous
years was that system combinations indeed per-
formed better than individual systems. Previous
9
Dev Set Test Set
Sententes 455 2034 Diff
GOOGLE 17.32?1.25 16.76?0.60 ?
BOJAR 16.00?1.15 16.90?0.61 ?
TECTOMT 11.48?1.04 13.19?0.58 ?
PC-TRANS 10.24?0.92 10.84?0.46 ?
EUROTRAN 9.64?0.92 11.04?0.48 ?
Table 12: BLEU scores of sample five systems in
English-to-Czech combination task.
years failed to show this clearly, because Google
Translate used to be included among the combined
systems, making it hard to improve. In WMT10,
Google Translate was excluded from system com-
bination task (except for translations involving
Czech, where it was accidentally included).
Our Table 12 provides an additional explanation
why the presence of Google among combined sys-
tems leads to inconclusive results. While the test
set was easier (based on BLEU) than the develop-
ment set for most systems, it was much harder for
Google. All system combinations were thus likely
to overfit and select Google n-grams most often.
Without access to Google powerful language mod-
els, the combination systems were likely to under-
perform Google in final fluency of the output.
5 Further Issues of Manual Evaluation
We have already seen that the comprehensibility
test by blind post-editing provides a different pic-
ture of the systems than the official ranking. Berka
et al (2011) introduced a third ?quiz-based evalu-
ation?. The quiz-like evaluation used the English-
to-Czech WMT10 systems, applied to different
texts: short text snippets were translated and an-
notators were asked to answer three yes/no ques-
tions complementing each snippet. The order of
the systems was rather different from the official
WMT10 results: CU-TECTO won the quiz-based
evaluation despite being the fourth in WMT10.
Because the texts were different in WMT10 and
the quiz-based evaluation, we asked a small group
of annotators to apply the ranking technique on the
text snippets. While not exactly comparable to the
WMT10 ranking, the WMT10 ranking was con-
firmed: CU-TECTO was again among the lowest-
scoring systems and Google won the ranking.
Bojar (2011) applies the error-flagging manual
evaluation by Vilar et al (2006) to four systems
of WMT09 English-to-Czech task. Again, the
overall order of the systems is somewhat differ-
ent when ranked by the number of errors flagged.
Mireia Farru?s and Fonollosa (2010) use a coarser
but linguistically motivated error classification for
Catalan-Spanish and suggest that differences in
ranking are caused by annotators treating some
types of errors as more serious.
In short, different types of manual evaluations
lead to different results even when identical sys-
tems and texts are evaluated.
6 Conclusion
We took a deeper look at the results of the WMT10
manual evaluation, and based on our observations,
we have some recommendations for future evalu-
ations:
? We propose to use a score which ignores
ties instead of the official ?? others? metric
which rewards ties and ?> others? which pe-
nalizes ties. Another score, ?? all in block?,
could help identify which systems are more
dominant.
? Inter-annotator agreement decreases dramat-
ically with sentence length; we recommend
including fewer sentences per block, at least
for longer sentences.
? We suggest agreement be measured based on
an empirical estimate of P (E), or at least us-
ing a more correct random clicking P (E) =
0.36.
? There is evidence of a negative correlation
between the number of times a system is
judged and its score; we recommend a deeper
analysis of this issue.
? We recommend the reference be sampled at
a lower rate than other systems, so as to play
a smaller role in the evaluation. We also rec-
ommend better quality control over the pro-
duction of the references.
And to the readers of the WMT overview paper,
we point out:
? Pairwise comparisons derived from 5-fold
rankings are sometimes unreliable. Even a
targeted pairwise comparison of two systems
can shed little light as to which is superior.
? The acceptability of post-edits is sometimes
very unreliable due to the low number of ob-
servations.
10
References
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
John J. Bartko and William T. Carpenter. 1976. On the
methods and theory of reliability. Journal of Ner-
vous and Mental Disease, 163(5):307?317.
E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited questioning. Pub-
lic Opinion Quarterly, 18(3):303?308.
Jan Berka, Martin C?erny?, and Ondr?ej Bojar. 2011.
Quiz-Based Evaluation of Machine Translation.
Prague Bulletin of Mathematical Linguistics, 95:77?
86, March.
Ondr?ej Bojar. 2011. Analyzing Error Types in
English-Czech Machine Translation. Prague Bul-
letin of Mathematical Linguistics, 95:63?76, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational lin-
guistics, 30(1):95?101.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA. Chapter 12.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Jose? B. Marin?o Mireia Farru?s, Marta R. Costa-jussa`
and Jose? A. R. Fonollosa. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th Annual
Conference of the Euoropean Association for Ma-
chine Translation (EAMT?10), pages 167?173, May.
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opin-
ion Quarterly, 19(3):321?325.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In International Conference on Lan-
guage Resources and Evaluation, pages 697?702,
Genoa, Italy, May.
11
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 433?439,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Influence of Parser Choice on Dependency-Based MT
Martin Popel, David Marec?ek, Nathan Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{popel,marecek,green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Accuracy of dependency parsers is one of the
key factors limiting the quality of dependency-
based machine translation. This paper deals
with the influence of various dependency pars-
ing approaches (and also different training
data size) on the overall performance of an
English-to-Czech dependency-based statisti-
cal translation system implemented in the
Treex framework. We also study the relation-
ship between parsing accuracy in terms of un-
labeled attachment score and machine transla-
tion quality in terms of BLEU.
1 Introduction
In the last years, statistical n-gram models domi-
nated the field of Machine Translation (MT). How-
ever, their results are still far from perfect. Therefore
we believe it makes sense to investigate alternative
statistical approaches. This paper is focused on an
analysis-transfer-synthesis translation system called
TectoMT whose transfer representation has a shape
of a deep-syntactic dependency tree. The system has
been introduced by Z?abokrtsky? et al (2008). The
translation direction under consideration is English-
to-Czech.
It has been shown by Popel (2009) that the current
accuracy of the dependency parser employed in this
translation system is one of the limiting factors from
the viewpoint of its output quality. In other words,
the parsing phase is responsible for a large portion
of translation errors. The biggest source of trans-
lation errors in the referred study was (and prob-
ably still is) the transfer phase, however the pro-
portion has changed since and the relative impor-
tance of the parsing phase has grown, because the
tranfer phase errors have already been addressed by
improvements based on Hidden Markov Tree Mod-
els for lexical and syntactic choice as shown by
Z?abokrtsky? and Popel (2009), and by context sensi-
tive translation models based on maximum entropy
as described by Marec?ek et al (2010).
Our study proceeds along two directions. First,
we train two state-of-the-art dependency parsers on
training sets with varying size. Second, we use
five parsers based on different parsing techniques.
In both cases we document the relation between
parsing accuracy (in terms of Unlabeled Attachment
Score, UAS) and translation quality (estimated by
the well known BLEU metric).
The motivation behind the first set of experiments
is that we can extrapolate the learning curve and try
to predict how new advances in dependency parsing
can affect MT quality in the future.
The second experiment series is motivated by
the hypothesis that parsers based on different ap-
proaches are likely to have a different distribution
of errors, even if they can have competitive perfor-
mance in parsing accuracy. In dependency parsing
metrics, all types of incorrect edges typically have
the same weight,1 but some incorrect edges can be
more harmful than others from the MT viewpoint.
For instance, an incorrect attachment of an adverbial
node is usually harmless, while incorrect attachment
of a subject node might have several negative conse-
1This issue has been tackled already in the parsing literature;
for example, some authors disregard placement of punctuation
nodes within trees in the evaluation (Zeman, 2004).
433
quences such as:
? unrecognized finiteness of the governing verb,
which can lead to a wrong syntactization on the
target side (an infinitive verb phrase instead of
a finite clause),
? wrong choice of the target-side verb form (be-
cause of unrecognized subject-predicate agree-
ment),
? missing punctuation (because of wrongly rec-
ognized finite clause boundaries),
? wrong placement of clitics (because of wrongly
recognized finite clause boundaries),
? wrong form of pronouns (personal and posses-
sive pronouns referring to the clause?s subject
should have reflexive forms in Czech).
Thus it is obvious that the parser choice is im-
portant and that it might not be enough to choose a
parser, for machine translation, only according to its
UAS.
Due to growing popularity of dependency syntax
in the last years, there are a number of dependency
parsers available. The present paper deals with
five parsers evaluated within the translation frame-
work: three genuine dependency parsers, namely the
parsers described in (McDonald et al, 2005), (Nivre
et al, 2007), and (Zhang and Nivre, 2011), and two
constituency parsers (Charniak and Johnson, 2005)
and (Klein and Manning, 2003), whose outputs were
converted to dependency structures by Penn Con-
verter (Johansson and Nugues, 2007).
As for the related literature, there is no published
study measuring the influence of dependency parsers
on dependency-based MT to our knowledge.2
The remainder of this paper is structured as fol-
lows. The overall translation pipeline, within which
the parsers are tested, is described in Section 2. Sec-
tion 3 lists the parsers under consideration and their
main features. Section 4 summarizes the influence
of the selected parsers on the MT quality in terms of
BLEU. Section 5 concludes.
2However, the parser bottleneck of the dependency-based
MT approach was observed also by other researchers (Robert
Moore, personal communication).
2 Dependency-based Translation in Treex
We have implemented our experiments in the Treex
software framework (formerly TectoMT, introduced
by Z?abokrtsky? et al (2008)), which already offers
tool chains for analysis and synthesis of Czech and
English sentences.
We use the tectogrammatical (deep-syntactic)
layer of language representation as the transfer layer
in the presented MT experiments. Tectogrammat-
ics was introduced by Sgall (1967) and further
elaborated within the Prague Dependency Treebank
project (Hajic? et al, 2006). On this layer, each
sentence is represented as a tectogrammatical tree,
whose main properties (from the MT viewpoint) are
the following:
1. nodes represent autosemantic words,
2. edges represent semantic dependencies (a node
is an argument or a modifier of its parent),
3. there are no functional words (prepositions,
auxiliary words) in the tree, and the autose-
mantic words appear only in their base forms
(lemmas). Morphologically indispensable cat-
egories (such as number with nouns or tense
with verbs, but not number with verbs as it is
only imposed by agreement) are stored in sep-
arate node attributes (grammatemes).
The intuitions behind the decision to use tec-
togrammatics for MT are the following: we be-
lieve that (1) tectogrammatics largely abstracts from
language-specific means (inflection, agglutination,
functional words etc.) of expressing non-lexical
meanings and thus tectogrammatical trees are sup-
posed to be highly similar across languages, (2)
it enables a natural transfer factorization,3 (3) and
local tree contexts in tectogrammatical trees carry
more information (especially for lexical choice) than
local linear contexts in the original sentences.
The translation scenario is outlined in the rest of
this section.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
?denser?, especially when translating from/to a language with
rich inflection such as Czech.
434
2.1 Analysis
The input English text is segmented into sentences
and tokens. The tokens are lemmatized and tagged
with Penn Treebank tags using the Morce tagger
(Spoustova? et al, 2007). Then one of the studied
dependency parsers is applied and a surface-syntax
dependency tree (analytical tree in the PDT termi-
nology) is created for each sentence.
This tree is converted to a tectogrammatical tree.
Each autosemantic word with its associated func-
tional words is collapsed into a single tectogram-
matical node, labeled with a lemma, formeme,4 and
semantically indispensable morphologically cate-
gories; coreference is also resolved.
2.2 Transfer
The transfer phase follows, whose most difficult part
consists especially in labeling the tree with target-
side lemmas and formemes. There are also other
types of changes, such as node addition and dele-
tion. However, as shown by Popel (2009), changes
of tree topology are required relatively infrequently
due to the language abstractions on the tectogram-
matical layer.
Currently, translation models based on Maxi-
mum Entropy classifiers are used both for lemmas
and formemes (Marec?ek et al, 2010). Tree label-
ing is optimized using Hidden Tree Markov Mod-
els (Z?abokrtsky? and Popel, 2009), which makes
use of target-language dependency tree probabilistic
model.
All models used in the transfer phase are trained
using training sections of the Czech-English parallel
corpus CzEng 0.9 (Bojar and Z?abokrtsky?, 2009).
2.3 Synthesis
Finally, surface sentence shape is synthesized from
the tectogrammatical tree, which is basically the
reverse operation of the tectogrammatical analy-
sis. It consists of adding punctuation and functional
4Formeme captures the morphosyntactic means which are
used for expressing the tectogrammatical node in the surface
sentence shape. Examples of formeme values: v:that+fin ?
finite verb in a subordinated clause introduced with conjunction
that, n:sb ? semantic noun in a subject position, n:for+X ?
semantic noun in a prepositional group introduced with prepo-
sition for, adj:attr ? semantic adjective in an attributive po-
sition.
words, spreading morphological categories accord-
ing to grammatical agreement, performing inflection
(using Czech morphology database (Hajic?, 2004)),
arranging word order etc.
The difference from the analysis phase is that
there is not very much space for optimization in the
synthesis phase. In other words, final sentence shape
is determined almost uniquely by the tectogrammat-
ical tree (enriched with formemes) resulting from
the transfer phase. However, if there are not enough
constraints for a unique choice of a surface form of
a lemma, then a unigram language model is used for
the final decision. The model was trained using 500
million words from the Czech National Corpus.5
3 Involved Parsers
We performed experiments with parsers from
three families: graph-based parsers, transition-
based parsers, and phrase-structure parsers (with
constituency-to-dependency postprocessing).
3.1 Graph-based Parser
In graph-based parsing, we learn a model for scoring
graph edges, and we search for the highest-scoring
tree composed of the graph?s edges. We used Max-
imum Spanning Tree parser (Mcdonald and Pereira,
2006) which is capable of incorporating second or-
der features (MST for short).
3.2 Transition-based Parsers
Transition-based parsers utilize the shift-reduce al-
gorithm. Input words are put into a queue and
consumed by shift-reduce actions, while the out-
put parser is gradually built. Unlike graph-based
parsers, transition-based parsers have linear time
complexity and allow straightforward application of
non-local features.
We included two transition-based parsers into our
experiments:
? Malt ? Malt parser introduced by Nivre et al
(2007) 6
5http://ucnk.ff.cuni.cz
6We used stackeager algorithm, liblinear learner, and
the enriched feature set for English (the same configu-
ration as in pretrained English models downloadable at
http://maltparser.org.
435
? ZPar ? Zpar parser7 which is basically an al-
ternative implementation of the Malt parser,
employing a richer set of non-local features as
described by Zhang and Nivre (2011).
3.3 CFG-based Tree Parsers
Another option how to obtain dependency trees is
to apply a constituency parser, recognize heads in
the resulting phrase structures and apply a recur-
sive algorithm for converting phrase-structure trees
into constituency trees (the convertibility of the two
types of syntactic structures was studied already by
Gaifman (1965)).
We used two constituency parsers:
? Stanford ? The Stanford parser (Klein and
Manning, 2003),8
? CJ ? a MaxEnt-based parser combined with
discriminative reranking (Charniak and John-
son, 2005).9
Before applying the parsers on the text, the system
removes all spaces within tokens. For instance U. S.
becomes U.S. to restrict the parsers from creating
two new tokens. Tokenization built into both parsers
is bypassed and the default tokenization in Treex is
used.
After parsing, Penn Converter introduced by Jo-
hansson and Nugues (2007) is applied, with the
-conll2007 option, to change the constituent
structure output, of the two parsers, into CoNLL de-
pendency structure. This allows us to keep the for-
mats consistent with the output of both MST and
MaltParser within the Treex framework.
There is an implemented procedure for cre-
ating tectogrammatical trees from the English
phrase structure trees described by Kuc?erova? and
Z?abokrtsky? (2002). Using the procedure is more
straightforward, as it does not go through the
CoNLL-style trees; English CoNLL-style trees dif-
fer slightly from the PDT conventions (e.g. in at-
taching auxiliary verbs) and thus needs additional
7http://sourceforge.net/projects/zpar/ (version 0.4)
8Only the constituent, phrase based, parsed output is used in
these experiments.
9We are using the default settings from the August 2006 ver-
sion of the software.
postprocessing for our purposes. However, we de-
cided to stick to Penn Converter, so that the similar-
ity of the translation scenarios is maximized for all
parsers.
3.4 Common Preprocessing: Shallow Sentence
Chunking
According to our experience, many dependency
parsers have troubles with analyzing sentences that
contain parenthesed or quoted phrases, especially if
they are long.
We use the assumption that in most cases the con-
tent of parentheses or quotes should correspond to
a connected subgraph (subtree) of the syntactic tree.
We implemented a very shallow sentence chunker
(SentChunk) which recognizes parenthesed word
sequences. These sequences can be passed to a
parser first, and be parsed independently of the rest
of the sentence. This was shown to improve not only
parsing accuracy of the parenthesed word sequence
(which is forced to remain in one subtree), but also
the rest of the sentence.10
In our experiments, SentChunk is used only
in combination with the three genuine dependency
parsers.
4 Experiments and Evaluation
4.1 Data for Parsers? Training and Evaluation
The dependency trees needed for training the parsers
and evaluating their UAS were created from the
Penn Treebank data (enriched first with internal
noun phrase structure applied via scripts provided
by Vadas and Curran (2007)) by Penn Converter (Jo-
hansson and Nugues, 2007) with the -conll2007
option (PennConv for short).
All the parsers were evaluated on the same data ?
section 23.
All the parsers were trained on sections 02?21,
except for the Stanford parser which was trained
on sections 01?21. We were able to retrain the
parser models only for MST and Malt. For the
other parsers we used pretrained models available on
the Internet: CJ?s default model ec50spfinal,
Stanford?s wsjPCFG.ser.gz model, and
10Edge length is a common feature in dependency parsers, so
?deleting? parenthesed words may give higher scores to correct
dependency links that happened to span over the parentheses.
436
ZPar?s english.tar.gz. The model of ZPar
is trained on data converted to dependencies using
Penn2Malt tool,11 which selects the last member of
a coordination as the head. To be able to compare
ZPar?s output with the other parsers, we postpro-
cessed it by a simple ConjAsHead code that con-
verts this style of coordinations to the one used in
CoNLL2007, where the conjuction is the head.
4.2 Reference Translations Used for Evaluation
Translation experiments were evaluated using refer-
ence translations from the new-dev2009 data set,
provided by the organizors of shared translation task
with the Workshop on Statistical Machine Transla-
tion.
4.3 Influence of Parser Training Data Size
We trained a sequence of parser models for MST and
Malt, using a roughly exponentially growing se-
quence of Penn Treebank subsets. The subsets are
contiguous and start from the beginning of section
02. The results are collected in Tables 1 and 2.12
#tokens UAS BLEU NIST
100 0.362 0.0579 3.6375
300 0.509 0.0859 4.3853
1000 0.591 0.0995 4.6548
3000 0.623 0.1054 4.7972
10000 0.680 0.1130 4.9695
30000 0.719 0.1215 5.0705
100000 0.749 0.1232 5.1193
300000 0.776 0.1257 5.1571
990180 0.793 0.1280 5.1915
Table 1: The effect of training data size on parsing accu-
racy and on translation performance with MST.
The trend of the relation between the training data
size and BLEU is visible also in Figure 1. It is ob-
vious that increasing the training data has a positive
effect on the translation quality. However, the pace
of growth of BLEU is sublogarithmic, and becomes
unconvincing above 100,000 training tokens. It in-
dicates that given one of the two parsers integrated
11http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
12To our knowledge, the best system participating in the
shared task reaches BLEU 17.8 for this translation direction.
#tokens UAS BLEU NIST
100 0.454 0.0763 4.0555
300 0.518 0.0932 4.4698
1000 0.591 0.1042 4.6769
3000 0.616 0.1068 4.7472
10000 0.665 0.1140 4.9100
30000 0.695 0.1176 4.9744
100000 0.723 0.1226 5.0504
300000 0.740 0.1238 5.1005
990180 0.759 0.1253 5.1296
Table 2: The effect of training data size on parsing accu-
racy and on translation performance with Malt.
 
0.05
 
0.06
 
0.07
 
0.08
 
0.09 0.1
 
0.11
 
0.12
 
0.13
 
100
 
100
0
 
100
00
 
100
000
 
1e+
06
BLEU
train
ing 
toke
ns
MS
T Mal
t
Figure 1: The effect of parser training data size of BLEU
with Malt and MST parsers.
into our translation framework, increasing the parser
training data alone would probably not lead to a sub-
stantial improvement of the translation performance.
4.4 Influence of Parser Choice
Table 3 summarizes our experiments with the five
parsers integrated into the tectogrammatical transla-
tion pipeline. Two configurations (with and without
SentChunk) are listed for the genuine dependency
parsers. The relationship between UAS and BLEU
for (the best configurations of) all five parsers is de-
picted also in Figure 2.
Additionally, we used paired bootstrap 95% con-
fidence interval testing (Zhang et al, 2004), to check
which BLEU differences are significant. For the
five compared parser (with SentChunk if appli-
cable), only four comparisons are not significant:
MST-CJ, MST-Stanford, Malt-Stanford,
and CJ-Stanford.
437
Parser Training data Preprocessing Postprocessing UAS BLEU NIST TER
MST PennTB + PennConv SentChunk ? 0.793 0.1280 5.192 0.735
MST PennTB + PennConv ? ? 0.794 0.1236 5.149 0.739
Malt PennTB + PennConv SentChunk ? 0.760 0.1253 5.130 0.740
Malt PennTB + PennConv ? ? 0.761 0.1214 5.088 0.744
Zpar PennTB + Penn2Malt SentChunk ConjAsHead 0.793 0.1176 5.039 0.749
Zpar PennTB + Penn2Malt ? ConjAsHead 0.792 0.1127 4.984 0.754
CJ PennTB ? PennConv 0.904 0.1284 5.189 0.737
Stanford PennTB ? PennConv 0.825 0.1277 5.137 0.740
Table 3: Dependency parsers tested in the translation pipeline.
 
0.1
 
0.10
5
 
0.11
 
0.11
5
 
0.12
 
0.12
5
 
0.13
 
0.13
5
 
0.14
 
0.14
5
 
0.15
 
0.74
 
0.76
 
0.78
 
0.8
 
0.82
 
0.84
 
0.86
 
0.88
 
0.9
 
0.92
BLEU
UAS
MS
T Mal
t
Zpa
r
Sta
nfor
d CJ
Figure 2: Unlabeled Attachment Score versus BLEU.
Even if BLEU grows relatively smoothly with
UAS for different parsing models of the same parser,
one can see that there is no obvious relation be-
tween UAS and BLEU accross all parsers. MST and
Zpar have the same UAS but quite different BLEU,
whereas MST and CJ have very similar BLEU but
distant UAS. It confirms the original hypothesis that
it is not only the overall UAS, but also the parser-
specific distribution of errors what matters.
4.5 Influence of Shallow Sentence Chunking
Table 3 confirms that parsing the contents paren-
theses separately from the rest of the sentence
(SentChunk) has a positive effect with all three
dependency parsers. Surprisingly, even if the effect
on UAS is negligible, the improvement is almost
half of BLEU point which is significant for all the
three parsers.
4.6 Discussion on Result Comparability
We tried to isolate the effects of the properties of
selected parsers, however, the separation from other
influencing factors is not perfect due to several tech-
nical issues:
? So far, we were not able to retrain the models
for all parsers ourselves and therefore their pre-
trained models (one of them based on slightly
different Penn Treebank division) must have
been used.
? Some parsers make their own choice of POS
tags within the parsed sentences, while other
parsers require the sentences to be tagged al-
ready on their input.
? The trees in the CzEng 0.9 parallel treebank
were created using MST. CzEng 0.9 was used
for training translation models used in the
transfer phase of the translation scenario; thus
these translation models might compensate for
some MST?s errors, which might handicap other
parsers. So far we were not able to reparse 8
million sentence pairs in CzEng 0.9 by all stud-
ied parsers.
5 Conclusions
This paper is a study of how the choice of a de-
pendency parsing technique influences the quality of
English-Czech dependency-based translation. Our
main observations are the following. First, BLEU
grows with the increasing amount of training depen-
dency trees, but only in a sublogarithmic pace. Sec-
ond, what seems to be quite effective for translation
438
is to facilitate the parsers? task by dividing the sen-
tences into smaller chunks using parenthesis bound-
aries. Third, if the parsers are based on different
approaches, their UAS does not correlate well with
their effect on the translation quality.
Acknowledgments
This research was supported by the
grants MSM0021620838, GAUK 116310,
GA201/09/H057, and by the European Com-
mission?s 7th Framework Program (FP7) under
grant agreements n? 238405 (CLARA), n? 247762
(FAUST), and n? 231720 (EuroMatrix Plus).
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathematical
Linguistics, 92:63?83.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
Association for Computational Linguistics, ACL ?05,
pages 173?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Haim Gaifman. 1965. Dependency systems and phrase-
structure systems. Information and Control, pages
304?337.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of Association for Computational Lin-
guistics, pages 423?430.
Ivona Kuc?erova? and Zdene?k Z?abokrtsky?. 2002. Trans-
forming Penn Treebank Phrase Trees into (Praguian)
Tectogrammatical Dependency Trees. The Prague
Bulletin of Mathematical Linguistics, (78):77?94.
David Marec?ek, Martin Popel, and Zdene?k Z?abokrtsky?.
2010. Maximum entropy translation model in
dependency-based MT framework. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 201?201, Uppsala,
Sweden. Association for Computational Linguistics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT / EMNLP, pages 523?530, Vancouver, Canada.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Martin Popel. 2009. Ways to Improve the Quality of
English-Czech Machine Translation. Master?s thesis,
Institute of Formal and Applied Linguistics, Charles
University, Prague, Czech Republic.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing, ACL
2007, pages 67?74, Praha.
David Vadas and James Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Zdene?k Z?abokrtsky? and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 145?148, Suntec, Sin-
gapore.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceedings
of the 3rd Workshop on Statistical Machine Transla-
tion, ACL, pages 167?170.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph.D. thesis, Faculty of Mathematics
and Physics, Charles University in Prague.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In To
appear in the Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores: How much improvement
do we need to have a better system. In Proceedings of
LREC, volume 4, pages 2051?2054.
439
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267?274,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Formemes in English-Czech Deep Syntactic MT ?
Ondr?ej Du?ek, Zdene?k ?abokrtsk?, Martin Popel,
Martin Majli?, Michal Nov?k, and David Marec?ek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?me?st? 25, Prague
{odusek,zabokrtsky,popel,majlis,mnovak,marecek}@ufal.mff.cuni.cz
Abstract
One of the most notable recent improve-
ments of the TectoMT English-to-Czech trans-
lation is a systematic and theoretically sup-
ported revision of formemes?the annotation
of morpho-syntactic features of content words
in deep dependency syntactic structures based
on the Prague tectogrammatics theory. Our
modifications aim at reducing data sparsity,
increasing consistency across languages and
widening the usage area of this markup.
Formemes can be used not only in MT, but in
various other NLP tasks.
1 Introduction
The cornerstone of the TectoMT tree-to-tree ma-
chine translation system is the deep-syntactic lan-
guage representation following the Prague tec-
togrammatics theory (Sgall et al, 1986), and its ap-
plication in the Prague Dependency Treebank (PDT)
2.01 (Hajic? et al, 2006), where each sentence is
analyzed to a dependency tree whose nodes corre-
spond to content words. Each node has a number
of attributes, but the most important (and difficult)
for the transfer phase are lemma?lexical informa-
tion, and formeme?surface morpho-syntactic infor-
? This research has been supported by the grants
FP7-ICT-2009-4-247762 (FAUST), FP7-ICT-2009-4-249119
(Metanet), LH12093 (Kontakt II), DF12P01OVV022 (NAKI),
201/09/H057 (Czech Science Foundation), GAUK 116310, and
SVV 265 314. This work has been using language resources de-
veloped and/or stored and/or distributed by the LINDAT-Clarin
project of the Ministry of Education of the Czech Republic
(project LM2010013).
1http://ufal.mff.cuni.cz/pdt2.0
mation, including selected auxiliary words (Pt?c?ek
and ?abokrtsk?, 2006; ?abokrtsk? et al, 2008).
This paper focuses on formemes?their definition
and recent improvements of the annotation, which
has been thoroughly revised in the course of prepa-
ration of the CzEng 1.0 parallel corpus (Bojar et al,
2012b), whose utilization in TectoMT along with the
new formemes version has brought the greatest ben-
efit to our English-Czech MT system in the recent
year. However, the area of possible application of
formemes is not limited to MT only or to the lan-
guage pair used in our system; the underlying ideas
are language-independent.
We summarize the development of morpho-
syntactic annotations related to formemes (Sec-
tion 2), provide an overview of the whole TectoMT
system (Section 3), then describe the formeme an-
notation (Section 4) and our recent improvements
(Section 5), as well as experimental applications, in-
cluding English-Czech MT (Section 6). The main
asset of the formeme revision is a first systematic re-
organization of the existing practical aid, providing
it with a solid theoretical base, but still bearing its
intended applications in mind.
2 Related Work
Numerous theoretical approaches had been made
to morpho-syntactic description, mainly within va-
lency lexicons, starting probably with the work by
Helbig and Schenkel (1969). Perhaps the best one
for Czech is PDT-VALLEX (Hajic? et al, 2003), list-
ing all possible subtrees corresponding to valency
arguments (Ure?ov?, 2009). ?abokrtsk? (2005)
gives an overview of works in this field.
267
This kind of information has been most exploited
in structural MT systems, employing semantic re-
lations (Menezes and Richardson, 2001) or surface
tree substructures (Quirk et al, 2005; Marcu et al,
2006). Formemes, originally developed for Natural
Language Generation (NLG) (Pt?c?ek and ?abokrt-
sk?, 2006), have been successfully applied to MT
within the TectoMT system. Our revision of for-
meme annotation aims to improve the MT perfor-
mance, keeping other possible applications in mind.
3 The TectoMT English-Czech Machine
Translation System
The TectoMT system is a structural machine trans-
lation system with deep transfer, first introduced
by ?abokrtsk? et al (2008). It currently supports
English-to-Czech translation. Its analysis stage
follows the Prague tectogrammatics theory (Sgall,
1967; Sgall et al, 1986), proceeding over two layers
of structural description, from shallow (analytical)
to deep (tectogrammatical) (see Section 3.1).
The transfer phase of the system is based on Max-
imum Entropy context-sensitive translation models
(Marec?ek et al, 2010) and Hidden Tree Markov
Models (?abokrtsk? and Popel, 2009). It is factor-
ized into three subtasks: lemma, formeme and gram-
matemes translation (see Sections 3.2 and 3.3).
The subsequent generation phase consists of rule-
based components that gradually change the deep
target language representation into a shallow one,
which is then converted to text (cf. Section 6.1).
The version of TectoMT submitted to WMT122
builds upon the WMT11 version. Several rule-based
components were slightly refined. However, most of
the effort was devoted to creating a better and bigger
parallel treebank?CzEng 1.03 (Bojar et al, 2012b),
and re-training the statistical components on this re-
source. Apart from bigger size and improved filter-
ing, one of the main differences between CzEng 0.9
(Bojar and ?abokrtsk?, 2009) (used in WMT11) and
CzEng 1.0 (used in WMT12) is the revised annota-
tion of formemes.
2http://www.statmt.org/wmt12
3http://ufal.mff.cuni.cz/czeng
3.1 Layers of structural analysis
There are two distinct structural layers used in the
TectoMT system:
? Analytical layer. A surface syntax layer, which
includes all tokens of the sentence, organized
into a labeled dependency tree. The labels cor-
respond to surface syntax functions.
? Tectogrammatical layer. A deep syntax/se-
mantic layer describing the linguistic meaning
of the sentence. Its dependency trees include
only content words as nodes, assigning to each
of them a deep lemma (t-lemma), a semantic
role label (functor), and other deep linguistic
features (grammatemes), such as semantic part-
of-speech, person, tense or modality.
The analytical layer can be obtained using differ-
ent dependency parsers (Popel et al, 2011); the tec-
togrammatical representation is then created by rule-
based modules from the analytical trees.
In contrast to the original PDT annotation,
the TectoMT tectogrammatical layer also includes
formemes describing the surface morpho-syntactic
realization of the nodes (cf. also Section 3.3).
3.2 Transfer: Translation Factorization and
Symmetry
Using the tectogrammatical representation in struc-
tural MT allows separating the problem of translat-
ing a sentence into relatively independent simpler
subtasks: lemma, functors, and grammatemes trans-
lation (Bojar et al, 2009; ?abokrtsk?, 2010). Since
topology changes to deep syntax trees are rare in MT
transfer, each of these three subtasks allows a vir-
tually symmetric source-target one-to-one mapping,
thus simplifying the initial n-to-m mapping of word
phrases or surface subtrees.
?abokrtsk? et al (2008) obviated the need for
transfer via functors (i.e. semantic role detection)
by applying a formeme transfer instead. While
formeme values are much simpler to obtain by au-
tomatic processing, this approach preserved the ad-
vantage of symmetric one-to-one value translation.
Moreover, translations of a given source morpho-
syntactic construction usually follow a limited num-
ber of patterns in the target language regardless of
268
their semantic functions, e.g. a finite clause will
most often be translated as a finite clause.
3.3 Motivation for the Introduction of
Formemes
Surface-oriented formemes have been introduced
into the semantics-oriented tectogrammatical layer,
as it proves beneficial to combine the deep syntax
trees, smaller in size and more consistent across lan-
guages, with the surface morphology and syntax to
provide for a straightforward transition to the surface
level (?abokrtsk?, 2010).
The three-fold factorization of the transfer phase
(see Section 3.2) helps address the data sparsity is-
sue faced by today?s MT systems. As the translation
of lemmas and their morpho-syntactic forms is sepa-
rated, combinations unseen in the training data may
appear on the output.
To further reduce data sparsity, only minimal in-
formation needed to reconstruct the surface form is
stored in formemes; morphological categories deriv-
able from elsewhere, i.e. morphological agreement
or grammatemes, are discarded.
4 Czech and English Formemes in
TectoMT
A formeme is a concise description of relevant
morpho-syntactic features of a node in a tectogram-
matical tree (deep syntactic tree whose nodes usu-
ally correspond to content words). The general
shape of revised Czech and English formemes, as
implemented within the Treex4 NLP framework
(Popel and ?abokrtsk?, 2010) for the TectoMT sys-
tem, consists of three main parts:
1. Syntactic part-of-speech.5 The number of syn-
tactic parts-of-speech is very low, as only con-
tent words are used on the deep layer and the
categories of pronouns and numerals have been
divided under nouns and adjectives accord-
ing to syntactic behavior (?evc??kov?-Raz?mov?
and ?abokrtsk?, 2006). The possible values are
v for verbs, n for nouns, adj for adjectives,
and adv for adverbs.
4http://ufal.mff.cuni.cz/treex/,
https://metacpan.org/module/Treex
5Cf. Section 5.2 for details.
2. Subordinate conjunction/preposition. Applies
only to formemes of prepositional phrases and
subordinate clauses introduced by a conjunc-
tion and contains the respective conjunction or
preposition; e.g. if, on or in_case_of.
3. Form. This part represents the morpho-
syntactic form of the node in question and de-
pends on the part-of-speech (see Table 1).
The two or three parts are concatenated into
a human-readable string to facilitate usage in
hand-written rules as well as statistical systems
(?abokrtsk?, 2010), producing values such as
v:inf, v:if+fin or n:into+X. Formeme val-
ues of nodes corresponding to uninflected words are
atomic.
Formemes are detected by rule-based modules op-
erating on deep and surface trees. Example deep
syntax trees annotated with formemes are shown in
Fig. 1. A listing of all possible formeme values is
given in Table 1.
Verbal formemes remain quite consistent in both
languages, except for the greater range of forms in
English (Czech uses adjectives or nouns instead of
gerunds and verbal attributes). Nominal formemes
differ more significantly: Czech is a free-word order
language with rich morphology, where declension
is important to syntactic relations?case is therefore
included in formemes. As English makes its syntac-
tic relations visible rather with word-order than with
morphology, English formemes indicate the syntac-
tic position instead. The same holds for adjecti-
val complements to verbs. Posession is expressed
mostly using nouns in English and adjectives in
Czech, which is also reflected in formemes.
5 Recent Markup Improvements
Our following markup innovations address several
issues found in the previous version and aim to adapt
the range of values more accurately to the intended
applications.
5.1 General Form Changes
The relevant preposition and subordinate conjunc-
tion nodes had been selected based on their depen-
dency labels; we use a simple part-of-speech tag fil-
ter instead in order to minimize the influence of pars-
ing errors and capture more complex prepositions,
269
Figure 1: An example English and Czech deep sentence structure annotated with formemes (in typewriter font).
Formeme Language Definition
v:(P+)fin both Verbs as heads of finite clauses
v:rc both Verbs as heads of relative clauses
v:(P+)inf both Infinitive clauses; typically with the particle to in English?
v:(P+)ger EN Gerunds, e.g. I like reading (v:ger), but I am tired of arguing (v:of+ger).
v:attr EN Present or past participles (i.e. -ing or -ed forms) in the attributive syntactic
position, e.g. Striking (v:attr) teachers hate bored (v:attr) students.
n:[1..7] CS Bare nouns; the numbers indicate morphological case?
n:X CS Bare nouns that cannot be inflected
n:subj EN Nouns in the subject position (i.e. in front of the main verb of the clause)
n:obj EN Nouns in the object position (i.e. following the verb with no preposition)
n:obj1, n:obj2 EN Nouns in the object position; distinguishing the two objects of ditransitive
verbs (e.g. give, consider)
n:adv EN Nouns in an adverbial position, e.g. The sales went up by 1 % last month
n:P+X EN Prepositional phrases
n:P+[1..7] CS Prepositional phrases; the preposition surface form is combined with the re-
quired case?
n:attr both Nominal attributes, e.g. insurance company or president Smith in English
and prezident Smith in Czech
n:poss EN English possessive pronouns and nouns with the ?s suffix
adj:attr both Adjectival attributes (Czech inflection forms need not be stored thanks to
congruency with the parent noun)
adj:compl EN Direct adjectival complements to verbs
adj:[1..7] CS Direct adjectival complements to verbs (morphological case must be stored
in Czech, as it is determined by valency)
adj:poss CS Czech possesive adjectives and pronouns; a counterpart to English n:poss
adv both Adverbs (not inflected, can take no prepositions etc.)
x both Coordinating conjunctions, other uninflected words
drop both Deep tree nodes which do not appear on the surface (e.g. pro-drop pronouns)
?I.e. infinitives as head of clauses, not infinitives as parts of compound verb forms with finite auxiliary verbs.
?Numbers are traditionally used to mark morphological case in Czech; 1 stands for nominative, 2 for genitive etc.
?Since many prepositions may govern multiple cases in Czech, the case number is necessary.
Table 1: A listing of all possible formeme values, indicating their usage in Czech, English or both languages. ?P+?
denotes the (lowercased) surface form of a preposition or a subordinate conjunction. Round brackets denote optional
parts, square brackets denote a set of alternatives.
270
e.g. in case of. Our revision also allows combining
prepositions with all English gerunds and infinitives,
preventing a loss of important data.
We also use the lowercased surface form in the
middle formeme part instead of lemmas to allow for
a more straightforward surface form generation.
5.2 Introducing Syntactic Part-of-Speech
Formemes originally contained the semantic part-of-
speech (sempos) (Raz?mov? and ?abokrtsk?, 2006)
as their first part. We replaced it with a syntac-
tic part-of-speech (syntpos), since it proved compli-
cated to assign sempos reliably by a rule-based mod-
ule and morpho-syntactic behavior is more relevant
to formemes than semantics.
The syntpos is assigned in two steps:
1. A preliminary syntpos is selected, using our
categorization based on the part-of-speech tag
and lemma.
2. The final syntpos is selected according to the
syntactic position of the node, addressing nom-
inal usage of adjectives and cardinal numerals
(see Sections 5.4 and 5.5).
5.3 Capturing Czech Nominal Attributes
Detecting the attributive usage of nouns is straight-
forward for English, where any noun depending di-
rectly on another noun is considered an attribute.
In Czech, one needs to distinguish case-congruent
attributes from others that have a fixed case. We
aimed at assigning the n:attr formeme only in the
former case and thus replaced the original method
based on word order with a less error-prone one
based on congruency and named entity recognition.
5.4 Numerals: Distinguishing Usage and
Correcting Czech Case
The new formemes now distinguish adjectival and
nominal usage of cardinal numerals (cf. also Sec-
tion 5.2), e.g. the number in 5 potatoes is now as-
signed the adj:attr formeme, whereas Apollo 11
is given n:attr. The new situation is analogous
in Czech, with nominal usages of numerals having
their morphological case marked in formemes.
To reduce data sparsity in the new formemes ver-
sion, we counter the inconsistent syntactic behavior
of Czech cardinal numerals, where 1-4 behave like
The word ban?n is in genitive (n:2), but would have an ac-
cusative (n:4) form if the numeral behaved like an adjective.
Figure 2: Case correction with numerals in Czech.
adjectives but other numerals behave like nouns and
shift their semantically governing noun to the po-
sition of a genitive attribute. An example of this
change is given in Fig. 2.
5.5 Adjectives: Nominal Usage and Case
The new formemes address the usage of adjectives
in the syntactic position of nouns (cf. Section 5.2),
which occurs only rarely, thus preventing sparse val-
ues, namely in these syntactic positions:
? The subject. We replaced the originally as-
signed adj:compl value, which was impos-
sible to tell from adjectival objects, with the
formeme a noun would have in the same po-
sition, e.g. in the sentence Many of them were
late, the subject many is assigned n:subj.
? Prepositional phrases. Syntactic behavior of
adjectives is identical to nouns here; we thus
assign them the formeme values a noun would
receive in the same position, e.g. n:of+X in-
stead of adj:of+X in He is one of the best at
school.
In Czech, we detect nominal usage of adjectives
in verbal direct objects as well, employing large-
coverage valency lexicons (Lopatkov? et al, 2008;
Hajic? et al, 2003).
Instead of assigning the compl value in Czech,
our formemes revision includes the case of adjecti-
val complements, which depends on the valency of
the respective verb.
5.6 Mutual Information Across Languages
The changes described above have been motivated
not only by theoretical linguistic description of the
languages in question, but also by the intended us-
age within the TectoMT translation system. Instead
271
of retraining the translation model after each change,
we devised a simpler and faster estimate to measure
the asset of our innovations: using Mutual Informa-
tion (MI) (Manning and Sch?tze, 1999, p. 66) of
formemes in Czech and English trees.
We expect that an inter-language MI increase will
lead to lower noise in formeme-to-formeme transla-
tion dictionary (Bojar et al, 2009, cf. Section 3.2),
thus achieving higher MT output quality.
Using the analysis pipeline from CzEng1.0, we
measured the inter-language MI on sentences from
the Prague Czech-English Dependency Treebank
(PCEDT) 2.0 (Bojar et al, 2012a). The overall re-
sults show an MI increase from 1.598 to 1.687 (Bo-
jar et al, 2012b). Several proposed markup changes
have been discarded as they led to an inter-language
MI drop; e.g. removing the v:rc relative clause
formeme or merging the v:attr and adj:attr
values in English.
6 Experimental Usage
We list here our experiments with the newly de-
veloped annotation: an NLG experiment aimed at
assessing the impact of formemes on the synthesis
phase of the TectoMT system, and the usage in the
English-Czech MT as a whole.
6.1 Czech Synthesis
The synthesis phase of the TectoMT system relies
heavily on the information included in formemes, as
its rule-based blocks use solely formemes and gram-
mar rules to gradually change a deep tree node into
a surface subtree.
To directly measure the suitability of our changes
for the synthesis stage of the TectoMT system, we
used a Czech-to-Czech round trip?deep analysis of
Czech PDT 2.0 development set sentences using the
CzEng 1.0 pipeline (Bojar et al, 2012b), followed
directly by the synthesis part of the TectoMT sys-
tem. The results were evaluated using the BLEU
metric (Papineni et al, 2002) with the original sen-
tences as reference; they indicate a higher suitability
of the new formemes for deep Czech synthesis (see
Table 2).
6.2 English-Czech Machine Translation
To measure the influence of the presented formeme
revision on the translation quality, we compared
Version BLEU
Original formemes 0.6818
Revised formemes 0.7092
Table 2: A comparison of formeme versions in Czech-to-
Czech round trip.
Version BLEU
Original formemes 0.1190
Revised formemes 0.1199
Table 3: A comparison of formeme versions in English-
to-Czech TectoMT translation on the WMT12 test set.
two translation scenarios?one using the origi-
nal formemes and the second using the revised
formemes in the formeme-to-formeme translation
model. Due to time reasons, we were able to
train both translation models only on 1/2 of the
CzEng 1.0 training data.
The results in Table 3 demonstrate a slight6 BLEU
gain when using the revised formemes version. The
gain is expected to be greater if several rule-based
modules of the transfer phase are adapted to the re-
visions.
7 Conclusion and Further Work
We have presented a systematic and theoretically
supported revision of a surface morpho-syntactic
markup within a deep dependency annotation sce-
nario, designed to facilitate the TectoMT transfer
phase. Our first practical experiments proved the
merits of our innovations in the tasks of Czech syn-
thesis and deep structural MT as a whole. We have
also experimented with formemes in the functor as-
signment (semantic role labelling) task and gained
moderate improvements (ca. 1-1.5% accuracy).
In future, we intend to tune the rule-based parts
of our MT transfer for the new version of formemes
and examine further possibilities of data sparsity re-
duction (e.g. by merging synonymous formemes).
We are also planning to create formeme annotation
modules for further languages to widen the range of
language pairs used in the TectoMT system.
6Significant at 90% level using pairwise bootstrap resam-
pling test (Koehn, 2004).
272
References
O. Bojar and Z. ?abokrtsk?. 2009. CzEng 0.9: Large
Parallel Treebank with Rich Annotation. Prague Bul-
letin of Mathematical Linguistics, 92.
O. Bojar, D. Marec?ek, V. Nov?k, M. Popel, J. Pt?c?ek,
J. Rou?, and Z. ?abokrtsk?. 2009. English-Czech MT
in 2008. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 125?129. As-
sociation for Computational Linguistics.
O. Bojar, J. Hajic?, E. Hajic?ov?, J. Panevov?, P. Sgall,
S. Cinkov?, E. Fuc??kov?, M. Mikulov?, P. Pajas,
J. Popelka, J. Semeck?, J. ?indlerov?, J. ?te?p?nek,
J. Toman, Z. Ure?ov?, and Z. ?abokrtsk?. 2012a.
Announcing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of LREC 2012, Istanbul,
Turkey, May. ELRA, European Language Resources
Association. In print.
O. Bojar, Z. ?abokrtsk?, O. Du?ek, P. Galu?c??kov?,
M. Majli?, D. Marec?ek, J. Mar??k, M. Nov?k,
M. Popel, and A. Tamchyna. 2012b. The Joy of Par-
allelism with CzEng 1.0. In Proceedings of LREC
2012, Istanbul, Turkey, May. ELRA, European Lan-
guage Resources Association. In print.
J. Hajic?, J. Panevov?, Z. Ure?ov?, A. B?mov?,
V. Kol?rov?, and P. Pajas. 2003. PDT-VALLEX: Cre-
ating a large-coverage valency lexicon for treebank an-
notation. In Proceedings of The Second Workshop on
Treebanks and Linguistic Theories, volume 9, pages
57?68.
J. Hajic?, J. Panevov?, E. Hajic?ov?, P. Sgall, P. Pajas,
J. ?te?p?nek, J. Havelka, M. Mikulov?, Z. ?abokrtsk?,
and M. ?evc??kov?-Raz?mov?. 2006. Prague Depen-
dency Treebank 2.0. CD-ROM LDC2006T01, LDC,
Philadelphia.
G. Helbig and W. Schenkel. 1969. W?rterbuch zur
Valenz und Distribution deutscher Verben. VEB Bib-
liographisches Institut, Leipzig.
P. Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
M. Lopatkov?, Z. ?abokrtsk?, V. Kettnerov?, and
K. Skwarska. 2008. Valenc?n? slovn?k c?esk?ch sloves.
Karolinum, Prague.
C.D. Manning and H. Sch?tze. 1999. Foundations of
statistical natural language processing. MIT Press.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntacti-
fied target language phrases. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 44?52. Association for
Computational Linguistics.
D. Marec?ek, M. Popel, and Z. ?abokrtsk?. 2010. Maxi-
mum entropy translation model in dependency-based
MT framework. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and Met-
rics (MATR), pages 201?206. Association for Compu-
tational Linguistics.
A. Menezes and S. D. Richardson. 2001. A best-first
alignment algorithm for automatic extraction of trans-
fer mappings from bilingual corpora. In Proceed-
ings of the workshop on Data-driven methods in ma-
chine translation - Volume 14, DMMT ?01, pages 1?8,
Stroudsburg, PA. Association for Computational Lin-
guistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
M. Popel and Z. ?abokrtsk?. 2010. TectoMT: modular
NLP framework. Advances in Natural Language Pro-
cessing, pages 293?304.
M. Popel, D. Marec?ek, N. Green, and Z. ?abokrtsk?.
2011. Influence of parser choice on dependency-based
MT. In Chris Callison-Burch, Philipp Koehn, Christof
Monz, and Omar Zaidan, editors, Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 433?439, Edinburgh, UK. Association for Com-
putational Linguistics.
J. Pt?c?ek and Z. ?abokrtsk?. 2006. Synthesis of
Czech sentences from tectogrammatical trees. In Text,
Speech and Dialogue, pages 221?228. Springer.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 271?279. Association for Computational
Linguistics.
M. Raz?mov? and Z. ?abokrtsk?. 2006. Annotation
of grammatemes in the Prague Dependency Treebank
2.0. In Proceedings of the LREC 2006 Workshop on
Annotation Science, pages 12?19.
M. ?evc??kov?-Raz?mov? and Z. ?abokrtsk?. 2006. Sys-
tematic parameterized description of pro-forms in the
Prague Dependency Treebank 2.0. In J. Hajic? and
J. Nivre, editors, Proceedings of the Fifth Workshop on
Treebanks and Linguistic Theories (TLT), pages 175?
186, Prague.
P. Sgall, E. Hajic?ov?, J. Panevov?, and J. Mey. 1986. The
meaning of the sentence in its semantic and pragmatic
aspects. Springer.
P. Sgall. 1967. Generativn? popis jazyka a c?esk? dekli-
nace. Academia, Prague.
Z. Ure?ov?. 2009. Building the PDT-VALLEX valency
lexicon. In On-line proceedings of the fifth Corpus
Linguistics Conference. University of Liverpool.
273
Z. ?abokrtsk?, J. Pt?c?ek, and P. Pajas. 2008. Tec-
toMT: highly modular MT system with tectogrammat-
ics used as transfer layer. In Proceedings of the Third
Workshop on Statistical Machine Translation, StatMT
?08, pages 167?170, Stroudsburg, PA. Association for
Computational Linguistics.
Z. ?abokrtsk?. 2005. Valency Lexicon of Czech Verbs.
Ph.D. thesis, Charles University in Prague.
Z. ?abokrtsk?. 2010. From Treebanking to Machine
Translation. Habilitation thesis, Charles University in
Prague.
Z. ?abokrtsk? and M. Popel. 2009. Hidden Markov
Tree Model in Dependency-based Machine Transla-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 145?148, Suntec, Singa-
pore.
274
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39?48,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Parallel Features in Parsing of Machine-Translated Sentences for
Correction of Grammatical Errors ?
Rudolf Rosa, Ondr?ej Dus?ek, David Marec?ek, and Martin Popel
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{rosa,odusek,marecek,popel}@ufal.mff.cuni.cz
Abstract
In this paper, we present two dependency
parser training methods appropriate for pars-
ing outputs of statistical machine transla-
tion (SMT), which pose problems to standard
parsers due to their frequent ungrammatical-
ity. We adapt the MST parser by exploiting
additional features from the source language,
and by introducing artificial grammatical er-
rors in the parser training data, so that the
training sentences resemble SMT output.
We evaluate the modified parser on DEP-
FIX, a system that improves English-Czech
SMT outputs using automatic rule-based cor-
rections of grammatical mistakes which re-
quires parsed SMT output sentences as its in-
put. Both parser modifications led to im-
provements in BLEU score; their combina-
tion was evaluated manually, showing a sta-
tistically significant improvement of the trans-
lation quality.
1 Introduction
The machine translation (MT) quality is on a steady
rise, with mostly statistical systems (SMT) dominat-
ing the area (Callison-Burch et al, 2010; Callison-
Burch et al, 2011). Most MT systems do not employ
structural linguistic knowledge and even the state-
of-the-art MT solutions are unable to avoid making
serious grammatical errors in the output, which of-
ten leads to unintelligibility or to a risk of misinter-
pretations of the text by a reader.
?This research has been supported by the EU Seventh
Framework Programme under grant agreement n? 247762
(Faust), and by the grants GAUK116310 and GA201/09/H057.
This problem is particularly apparent in target lan-
guages with rich morphological inflection, such as
Czech. As Czech often conveys the relations be-
tween individual words using morphological agree-
ment instead of word order, together with the word
order itself being relatively free, choosing the cor-
rect inflection becomes crucial.
Since the output of phrase-based SMT shows fre-
quent inflection errors (even in adjacent words) due
to each word belonging to a different phrase, a
possible way to address the grammaticality prob-
lem is a combination of statistical and structural ap-
proach, such as SMT output post-editing (Stymne
and Ahrenberg, 2010; Marec?ek et al, 2011).
In this paper, we focus on improving SMT output
parsing quality, as rule-based post-editing systems
rely heavily on the quality of SMT output analy-
sis. Parsers trained on gold standard parse trees of-
ten fail to produce the expected result when applied
to SMT output with grammatical errors. This is
partly caused by the fact that when parsing highly in-
flected free word-order languages the parsers have to
rely on morphological agreement, which, as stated
above, is often erroneous in SMT output.
Training a parser specifically by creating a man-
ually annotated treebank of MT systems? outputs
would be very expensive, and the application of such
treebank to other MT systems than the ones used
for its generation would be problematic. We address
this issue by two methods of increasing the quality
of SMT output parsing:
? a different application of previous works on
bitext parsing ? exploiting additional features
from the source language (Section 3), and
39
? introducing artificial grammatical errors in the
target language parser training data, so that the
sentences resemble the SMT output in some
ways (Section 4). This technique is, to our
knowledge, novel with regards to its applica-
tion to SMT and the statistical error model.
We test these two techniques on English-Czech
MT outputs using our own reimplementation of the
MST parser (McDonald et al, 2005) named RUR1
parser. and evaluate their contribution to the SMT
post-editing quality of the DEPFIX system (Marec?ek
et al, 2011), which we outline in Section 5. We
describe the experiments carried out and present the
most important results in Section 6. Section 7 then
concludes the paper and indicates more possibilities
of further improvements.
2 Related Work
Our approach to parsing with parallel features is
similar to various works which seek to improve the
parsing accuracy on parallel texts (?bitexts?) by us-
ing information from both languages. Huang et
al. (2009) employ ?bilingual constraints? in shift-
reduce parsing to disambiguate difficult syntac-
tic constructions and resolve shift-reduce conflicts.
Chen et al (2010) use similar subtree constraints to
improve parser accuracy in a dependency scenario.
Chen et al (2011) then improve the method by ob-
taining a training parallel treebank via SMT. In re-
cent work, Haulrich (2012) experiments with a setup
very similar to ours: adding alignment-projected
features to an originally monolingual parser.
However, the main aim of all these works is to im-
prove the parsing accuracy on correct parallel texts,
i.e. human-translated. This paper applies similar
methods, but with a different objective in mind ? in-
creasing the ability of the parser to process ungram-
matical SMT output sentences and, ultimately, im-
prove rule-based SMT post-editing.
Xiong et al (2010) use SMT parsing in translation
quality assessment, providing syntactic features to a
classifier detecting erroneous words in SMT output,
yet they do not concentrate on improving parsing ac-
curacy ? they employ a link grammar parser, which
1The abbreviation ?RUR? parser stands for ?Rudolph?s Uni-
versal Robust? parser.
is robust, but not tuned specifically to process un-
grammatical input.
There is also another related direction of research
in parsing of parallel texts, which is targeted on pars-
ing under-resourced languages, e.g. the works by
Hwa et al (2005), Zeman and Resnik (2008), and
McDonald et al (2011). They address the fact that
parsers for the language of interest are of low qual-
ity or even non-existent, whereas there are high-
quality parsers for the other language. They ex-
ploit common properties of both languages and de-
lexicalization. Zhao et al (2009) uses information
from word-by-word translated treebank to obtain ad-
ditional training data and boost parser accuracy.
This is different from our situation, as there ex-
ist high performance parsers for Czech (Buchholz
and Marsi, 2006; Nivre et al, 2007; Hajic? et al,
2009). Boosting accuracy on correct sentences is
not our primary goal and we do not intend to re-
place the Czech parser by an English parser; instead,
we aim to increase the robustness of an already ex-
isting Czech parser by adding knowledge from the
corresponding English source, parsed by an English
parser.
Other works in bilingual parsing aim to parse the
parallel sentences directly using a grammar formal-
ism fit for this purpose, such as Inversion Trans-
duction Grammars (ITG) (Wu, 1997). Burkett et
al. (2010) further include ITG parsing with word-
alignment in a joint scenario. We concentrate here
on using dependency parsers because of tools and
training data availability for the examined language
pair.
Regarding treebank adaptation for parser robust-
ness, Foster et al (2008) introduce various kinds of
artificial errors into the training data to make the fi-
nal parser less sensitive to grammar errors. How-
ever, their approach concentrates on mistakes made
by humans (such as misspellings, word repetition or
omission etc.) and the error models used are hand-
crafted. Our work focuses on morphology errors of-
ten encountered in SMT output and introduces sta-
tistical error modelling.
3 Parsing with Parallel Features
This section describes our SMT output parsing setup
with features from analyzed source sentences. We
40
explain our motivation for the inclusion of parallel
features in Section 3.1, then provide an account of
the parsers used (including our RUR parser) in Sec-
tion 3.2, and finally list all the monolingual and par-
allel features included in the parser training (in Sec-
tions 3.3 and 3.4, respectively).
3.1 Motivation
An advantage of SMT output parsing over general
dependency parsing is that one can also make use of
source ? English sentences in our case. Moreover,
although SMT output is often in many ways ungram-
matical, source is usually grammatical and therefore
easier to process (in our case especially to tag and
parse). This was already noticed in Marec?ek et al
(2011), who use the analysis of source sentence to
provide additional information for the DEPFIX rules,
claiming it to be more reliable than the analysis of
SMT output sentence.
We have carried this idea further by having de-
vised a simple way of making use of this information
in parsing of the SMT output sentences: We parse
the source sentence first and include features com-
puted over the parsed source sentence in the set of
features used for parsing SMT output. We first align
the source and SMT output sentences on the word
level and then use alignment-wise local features ?
i.e. for each SMT output word, we add features com-
puted over its aligned source word, if applicable (cf.
Section 3.4 for a listing).
3.2 Parsers Used
We have reimplemented the MST parser (McDonald
et al, 2005) in order to provide for a simple insertion
of the parallel features into the models.
We also used the original implementation of the
MST parser by McDonald et al (2006) for com-
parison in our experiments. To distinguish the two
variants used, we denote the original MST parser
as MCD parser,2 and the new reimplementation as
RUR parser.
We trained RUR parser in a first-order non-
projective setting with single-best MIRA. Depen-
dency labels are assigned in a second stage by a
2MCD uses k-best MIRA, does first- and second-order
parsing, both projectively and non-projectively, and can be
obtained from http://sourceforge.net/projects/
mstparser.
MIRA-based labeler, which has been implemented
according to McDonald (2006) and Gimpel and Co-
hen (2007).
We used the Prague Czech-English Dependency
Treebank3 (PCEDT) 2.0 (Bojar et al, 2012) as the
training data for RUR parser ? a parallel treebank
created from the Penn Treebank (Marcus et al,
1993) and its translation into Czech by human trans-
lators. The dependency trees on the English side
were converted from the manually annotated phrase-
structure trees in Penn Treebank, the Czech trees
were created automatically using MCD. Words of
the Czech and English sentences were aligned by
GIZA++ (Och and Ney, 2003).
We apply RUR parser only for SMT output pars-
ing; for source parsing, we use MCD parser trained
on the English CoNLL 2007 data (Nivre et al,
2007), as the performance of this parser is sufficient
for this task.
3.3 Monolingual Features
The set of monolingual features used in RUR parser
follows those described by McDonald et al (2005).
For parsing, we use the features described below.
The individual features are computed for both the
parent node and the child node of an edge and con-
joined in various ways. The coarse morphological
tag and lemma are provided by the Morc?e tagger
(Spoustova? et al, 2007).
? coarse morphological tag ? Czech two-letter
coarse morphological tag, as described in
(Collins et al, 1999),4
? lemma ? morphological lemma,
? context features: preceding coarse morpholog-
ical tag, following coarse morphological tag
? coarse morphological tag of a neighboring
node,
? coarse morphological tags in between ? bag of
coarse morphological tags of nodes positioned
between the parent node and the child node,
3http://ufal.mff.cuni.cz/pcedt
4The first letter is the main POS (12 possible values), the
second letter is either the morphological case field if the main
POS displays case (i.e. for nouns, adjectives, pronouns, numer-
als and prepositions; 7 possible values), or the detailed POS if
it does not (22 possible values).
41
? distance ? signed bucketed distance of the par-
ent and the child node in the sentence (in # of
words), using buckets 1, 2, 3, 4, 5 and 11.
To assign dependency labels, we use the same
set as described above, plus the following features
(called ?non-local? by McDonald (2006)), which
make use of the knowledge of the tree structure.
? is first child, is last child ? a boolean indicating
whether the node appears in the sentence as the
first/last one among all the child nodes of its
parent node,
? child number ? the number of syntactic chil-
dren of the current node.
3.4 Parallel Features
Figure 1: Example sentence for parallel features illustra-
tion (see Table 1).
In RUR parser we use three types of parallel fea-
tures, computed for the parent and child node of an
edge, which make use of the source English nodes
aligned to the parent and child node.
? aligned tag: morphological tag following the
Penn Treebank Tagset (Marcus et al, 1993) of
the English node aligned to the Czech node
Feature Feature value on
parent node child node
word form jel Martin
aligned tag VBD NNP
aligned dep. label Pred Sb
aligned edge existence true
word form jel autem
aligned tag VBD NN
aligned dep. label Pred Adv
aligned edge existence false
word form do zahranic???
aligned tag ? RB
aligned dep. label ? Adv
aligned edge existence ?
word form #root# .
aligned tag #root# .
aligned dep. label AuxS AuxK
aligned edge existence true
Table 1: Parallel features for several edges in Figure 1.
? aligned dependency label: dependency label of
the English node aligned to the Czech node in
question, according to the PCEDT 2.0 label set
(Bojar et al, 2012)
? aligned edge existence: a boolean indicating
whether the English node aligned to the Czech
parent node is also the parent of the English
node aligned to the Czech child node
The parallel features are conjoined with the
monolingual coarse morphological tag and lemma
features in various ways.
If there is no source node aligned to the parent
or child node, the respective feature cannot be com-
puted and is skipped.
An example of a pair of parallel sentences is given
in Figure 1 with the corresponding values of parallel
features for several edges in Table 1.
4 Worsening Treebanks to Simulate Some
of the SMT Frequent Errors
Addressing the issue of great differences between
the gold standard parser training data and the actual
analysis input (SMT output), we introduced artificial
inconsistencies into the training treebanks, in order
to make the parsers more robust in the face of gram-
mar errors made by SMT systems. We have concen-
42
trated solely on modeling incorrect word flection,
i.e. the dependency trees retained their original cor-
rect structures and word lemmas remained fixed, but
the individual inflected word forms have been modi-
fied according to an error model trained on real SMT
output. We simulate thus, with respect to morphol-
ogy, a treebank of parsed MT output sentences.
In Section 4.1 we describe the steps we take to
prepare the worsened parser training data. Sec-
tion 4.2 contains a description of our monolingual
greedy alignment tool which is needed during the
process to map SMT output to reference transla-
tions.
4.1 Creating the Worsened Parser Training
Data
The whole process of treebank worsening consists
of five steps:
1. We translated the English side of PCEDT5 to
Czech using SMT (we chose the Moses sys-
tem (Koehn et al, 2007) for our experiments)
and tagged the resulting translations using the
Morc?e tagger (Spoustova? et al, 2007).
2. We aligned the Czech side of PCEDT, now
serving as a reference translation, to the SMT
output using our Monolingual Greedy Aligner
(see Section 4.2).
3. Collecting the counts of individual errors, we
estimated the Maximum Likelihood probabili-
ties of changing a correct fine-grained morpho-
logical tag (of a word from the reference) into
a possibly incorrect fine-grained morphological
tag of the aligned word (from the SMT output).
4. The tags on the Czech side of PCEDT were
randomly sampled according to the estimated
?fine-grained morphological tag error model?.
In those positions where fine-grained morpho-
logical tags were changed, new word forms
were generated using the Czech morphological
generator by Hajic? (2004).6
5This approach is not conditioned by availability of parallel
treebanks. Alternatively, we might translate any text for which
reference translations are at hand. The model learned in the
third step would then be applied (in the fourth step) to a different
text for which parse trees are available.
6According to the ?fine-grained morphological tag error
We use the resulting ?worsened? treebank to train
our parser described in Section 3.2.
4.2 The Monolingual Greedy Aligner
Our monolingual alignment tool, used in treebank
worsening to tie reference translations to MT out-
put (see Section 4.1), scores all possible alignment
links and then greedily chooses the currently highest
scoring one, creating the respective alignment link
from word A (in the reference) to word B (in the
SMT output) and deleting all scores of links from A
or to B, so that one-to-one alignments are enforced.
The process is terminated when no links with a score
higher than a given threshold are available; some
words may thus remain unaligned.
The score is computed as a linear combination of
the following four features:
? word form (or lemma if available) similar-
ity based on Jaro-Winkler distance (Winkler,
1990),
? fine-grained morphological tag similarity,
? similarity of the relative position in the sen-
tence,
? and an indication whether the word following
(or preceding) A was already aligned to the
word following (or preceding) B.
Unlike bilingual word aligners, this tool needs no
training except for setting weights of the four fea-
tures and the threshold.7
5 The DEPFIX System
The DEPFIX system (Marec?ek et al, 2011) applies
various rule-based corrections to Czech-English
SMT output sentences, especially of morphological
agreement. It also employs the parsed source sen-
tences, which must be provided on the input together
with the SMT output sentences.
The corrections follow the rules of Czech gram-
mar, e.g. requiring that the clause subject be in the
model?, about 20% of fine-grained morphological tags were
changed. In 4% of cases, no word form existed for the new
fine-grained morphological tag and thus it was not changed.
7The threshold and weights were set manually using just ten
sentence pairs. The resulting alignment quality was found suf-
ficient, so no additional weights tuning was performed.
43
nominative case or enforcing subject-predicate and
noun-attribute agreements in morphological gender,
number and case, where applicable. Morphological
properties found violating the rules are corrected and
the corresponding word forms regenerated.
The source sentence parse, word-aligned to the
SMT output using GIZA++ (Och and Ney, 2003),
is used as a source of morpho-syntactic information
for the correction rules. An example of a correction
rule application is given in Figure 2.
Some
people
came
later
Atr
Sb
Pred
Advplpl
.AuxK
p?i?liPredpl
N?kte??
lid?
p?i?el
pozd?ji
Atr
Sb
Pred
Advsg, mpl
.AuxK
Figure 2: Example of fixing subject-predicate agreement.
The Czech word pr?is?el [he came] has a wrong morpho-
logical number and gender. Adapted from Marec?ek et al
(2011).
The system is implemented within the
TectoMT/Treex NLP framework (Popel and
Z?abokrtsky?, 2010). Marec?ek et al (2011) feed the
DEPFIX system with analyses by the MCD parser
trained on gold-standard treebanks for parsing of
English source sentences as well as Czech SMT
output.
6 Experiments and Results
We evaluate RUR parser indirectly by using it in the
DEPFIX system and measuring the performance of
the whole system. This approach has been chosen
instead of direct evaluation of the SMT output parse
trees, as the task of finding a correct parse tree of
a possibly grammatically incorrect sentence is not
well defined and considerably difficult to do.
We used WMT10, WMT11 and WMT12 En-
glish to Czech translation test sets, newssyscomb-
test2010, newssyscombtest2011 and news-
test2012,8 (denoted as WMT10, WMT11 and
8http://www.statmt.org/wmt10,
WMT12) for the automatic evaluation. The data sets
include the source (English) text, its reference trans-
lation and translations produced by several MT sys-
tems. We used the outputs of three SMT systems:
GOOGLE,9 UEDIN (Koehn et al, 2007) and BOJAR
(Bojar and Kos, 2010).
For the manual evaluation, two sets of 1000 ran-
domly selected sentences from WMT11 and from
WMT12 translated by GOOGLE were used.
6.1 Automatic Evaluation
Table 2 shows BLEU scores (Papineni et al, 2002)
for the following setups of DEPFIX:
? SMT output: output of an SMT system without
applying DEPFIX
? MCD: parsing with MCD
? RUR: parsing with RUR (Section 3.2)
? RUR+PARA: parsing with RUR using parallel
features (Section 3.4)
? RUR+WORS: parsing with RUR trained on
worsened treebank (Section 4)
? RUR+WORS+PARA: parsing with RUR
trained on worsened treebank and using
parallel features
It can be seen that both of the proposed ways of
adapting the parser to parsing of SMT output of-
ten lead to higher BLEU scores of translations post-
processed by DEPFIX, which suggests that they both
improve the parsing accuracy.
We have computed 95% confidence intervals
on 1000 bootstrap samples, which showed that
the BLEU score of RUR+WORS+PARA was sig-
nificantly higher than that of MCD and RUR
parser in 4 and 3 cases, respectively (results
where RUR+WORS+PARA achieved a significantly
higher score are marked with ?*?). On the other
hand, the score of neither RUR+WORS+PARA nor
RUR+WORS and RUR+PARA was ever signifi-
cantly lower than the score of MCD or RUR parser.
This leads us to believe that the two proposed meth-
ods are able to produce slightly better SMT output
parsing results.
http://www.statmt.org/wmt11,
http://www.statmt.org/wmt12
9http://translate.google.com
44
Test set WMT10 WMT11 WMT12
SMT system BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDIN
SMT output *15.85 *16.57 *15.91 *16.88 *20.26 *17.80 14.36 16.25 *15.54
MCD 16.09 16.95 *16.35 *17.02 20.45 *18.12 14.35 16.32 *15.65
RUR 16.08 *16.85 *16.29 17.03 20.42 *18.09 14.37 16.31 15.66
RUR+PARA 16.13 *16.90 *16.35 17.05 20.47 18.19 14.35 16.31 15.72
RUR+WORS 16.12 16.96 *16.45 17.06 20.53 18.21 14.40 16.31 15.71
RUR+WORS+PARA 16.13 17.03 16.54 17.12 20.53 18.25 14.39 16.30 15.74
Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and
UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and
processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores
marked with ?*? on the same data.
6.2 Manual Evaluation
Performance of RUR+WORS+PARA setup was man-
ually evaluated by doing a pairwise comparison with
other setups ? SMT output, MCD and RUR parser.
The evaluation was performed on both the WMT11
(Table 4) and WMT12 (Table 5) test set. 1000 sen-
tences from the output of the GOOGLE system were
randomly selected and processed by DEPFIX, using
the aforementioned SMT output parsers. The anno-
tators then compared the translation quality of the
individual variants in differing sentences, selecting
the better variant from a pair or declaring two vari-
ants ?same quality? (indefinite). They were also pro-
vided with the source sentence and a reference trans-
lation. The evaluation was done as a blind test, with
the sentences randomly shuffled.
The WMT11 test set was evaluated by two inde-
pendent annotators. (The WMT12 test set was eval-
uated by one annotator only.) The inter-annotator
agreement and Cohen?s kappa coefficient (Cohen
and others, 1960), shown in Table 3, were computed
both including all annotations (?with indefs?), and
disregarding sentences where at least one of the an-
notators marked the difference as indefinite (?with-
out indefs?) ? we believe a disagreement in choos-
ing the better translation to be more severe than a
disagreement in deciding whether the difference in
quality of the translations allows to mark one as be-
ing better.
For both of the test sets, RUR+WORS+PARA sig-
nificantly outperforms both MCD and RUR base-
line, confirming that a combination of the proposed
modifications of the parser lead to its better perfor-
mance. Statistical significance of the results was
RUR+WORS+PARA with indefs without indefs
compared to IAA Kappa IAA Kappa
SMT output 77% 0.54 92% 0.74
MCD 79% 0.66 95% 0.90
RUR 75% 0.60 94% 0.85
Table 3: Inter-annotator agreement on WMT11 data set
translated by GOOGLE
confirmed by a one-sided pairwise t-test, with the
following differences ranking: RUR+WORS+PARA
better = 1, baseline better = -1, indefinite = 0.
6.3 Inspection of Parser Modification Benefits
For a better understanding of the benefits of using
our modified parser, we inspected a small number of
parse trees, produced by RUR+WORS+PARA, and
compared them to those produced by RUR.
In many cases, the changes introduced by
RUR+WORS+PARA were clearly positive. We
provide two representative examples below.
Subject Identification
Czech grammar requires the subject to be in nom-
inative case, but this constraint is often violated in
SMT output and a parser typically fails to identify
the subject correctly in such situations. By wors-
ening the training data, we make the parser more ro-
bust in this respect, as the worsening often switches
the case of the subject; by including parallel fea-
tures, especially the aligned dependency label fea-
ture, RUR+WORS+PARA parser can often identify
the subject as the node aligned to the source subject.
45
Out of the differing sentences
Annotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinite
count percent count percent count percent
SMT output 422 301 71% 79 19% 42 10%
A MCD 211 120 57% 65 31% 26 12%
RUR 217 123 57% 64 29% 30 14%
SMT output 422 284 67% 69 16% 69 16%
B MCD 211 107 51% 56 26% 48 23%
RUR 217 118 54% 53 24% 46 21%
Table 4: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT11 data set
translated by GOOGLE, evaluated by two independent annotators.
Out of the differing sentences
Annotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinite
count percent count percent count percent
SMT output 420 270 64% 88 21% 62 15%
A MCD 188 86 45% 64 34% 38 20%
RUR 187 96 51% 57 30% 34 18%
Table 5: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT12 data set
translated by GOOGLE.
Governing Noun Identification
A parser for Czech typically relies on morpho-
logical agreement between an adjective and its gov-
erning noun (in morphological number, gender and
case), which is often violated in SMT output. Again,
RUR+WORS+PARA is more robust in this respect,
aligned edge existence now being the crucial feature
for the correct identification of this relation.
7 Conclusions and Future Work
We have studied two methods of improving the pars-
ing quality of Machine Translation outputs by pro-
viding additional information to the parser.
In Section 3, we propose a method of integrat-
ing additional information known at runtime, i.e.
the knowledge of the source sentence (source), from
which the sentence being parsed (SMT output) has
been translated. This knowledge is provided by
extending the parser feature set with new features
from the source sentence, projected through word-
alignment.
In Section 4, we introduce a method of utilizing
additional information known in the training phase,
namely the knowledge of the ways in which SMT
output differs from correct sentences. We provide
this knowledge to the parser by adjusting its training
data to model some of the errors frequently encoun-
tered in SMT output, i.e. incorrect inflection forms.
We have evaluated the usefulness of these two
methods by integrating them into the DEPFIX rule-
based MT output post-processing system (Marec?ek
et al, 2011), as MT output parsing is crucial for the
operation of this system. When used with our im-
proved parsing, the DEPFIX system showed better
performance both in automatic and manual evalua-
tion on outputs of several, including state-of-the-art,
MT systems.
We believe that the proposed methods of improv-
ing MT output parsing can be extended beyond their
current state. The parallel features used in our setup
are very few and very simple; it thus remains to
be examined whether more elaborate features could
help utilize the additional information contained in
the source sentence to a greater extent. Modeling
other types of SMT output inconsistencies in parser
training data is another possible step.
We also believe that the methods could be adapted
for use in other applications, e.g. automatic classifi-
cation of translation errors, confidence estimation or
multilingual question answering.
46
References
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar, Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?,
Petr Sgall, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing Prague Czech-English Dependency Treebank 2.0.
In Proceedings of LREC 2012, Istanbul, Turkey, May.
ELRA, European Language Resources Association.
In print.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 127?135. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 21?29. Association for Computational Lin-
guistics.
Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshi-
masa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro
Torisawa, and Haizhou Li. 2011. SMT helps bitext
dependency parsing. In EMNLP, pages 73?83. ACL.
Jacob Cohen et al 1960. A coefficient of agreement for
nominal scales. Educational and psychological mea-
surement, 20(1):37?46.
Michael Collins, Lance Ramshaw, Jan Hajic?, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 505?512,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jennifer Foster, Joachim Wagner, and Josef Van Gen-
abith. 2008. Adapting a WSJ-trained parser to gram-
matically noisy text. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 221?224. Association for Computa-
tional Linguistics.
Kevin Gimpel and Shay Cohen. 2007. Discriminative
online algorithms for sequence labeling- a comparative
study.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, et al 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Associa-
tion for Computational Linguistics.
Jan Hajic?. 2004. Disambiguation of rich inflection: com-
putational morphology of Czech. Karolinum.
Martin Haulrich. 2012. Data-Driven Bitext Dependency
Parsing and Alignment. Ph.D. thesis.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 3-Volume 3, pages 1222?1231. Association for
Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11:311?325, September.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
47
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Comput. Lin-
guist., 19:313?330, June.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Chris Callison-Burch,
Philipp Koehn, Christof Monz, and Omar Zaidan, edi-
tors, Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 426?432, Edinburgh, UK.
Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
62?72. Association for Computational Linguistics.
Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency parsing.
Ph.D. thesis, Philadelphia, PA, USA. AAI3225503.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), June.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Sara Stymne and Lars Ahrenberg. 2010. Using a gram-
mar checker for evaluation and postprocessing of sta-
tistical machine translation. In Proceedings of LREC,
pages 2175?2181.
William E. Winkler. 1990. String comparator met-
rics and enhanced decision rules in the Fellegi-Sunter
model of record linkage. In Proceedings of the Section
on Survey Research Methods (American Statistical As-
sociation), pages 354?359.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 604?611. Association for Computational
Linguistics.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. NLP for
Less Privileged Languages, page 35.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
55?63. Association for Computational Linguistics.
48
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141?147,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
PhraseFix: Statistical Post-Editing of TectoMT
Petra Galu?c??kov?, Martin Popel, and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?me?st? 25, Prague, Czech Republic
{galuscakova,popel,bojar}@ufal.mff.cuni.cz
Abstract
We present two English-to-Czech systems
that took part in the WMT 2013 shared
task: TECTOMT and PHRASEFIX. The
former is a deep-syntactic transfer-based
system, the latter is a more-or-less stan-
dard statistical post-editing (SPE) applied
on top of TECTOMT. In a brief survey, we
put SPE in context with other system com-
bination techniques and evaluate SPE vs.
another simple system combination tech-
nique: using synthetic parallel data from
TECTOMT to train a statistical MT sys-
tem (SMT). We confirm that PHRASEFIX
(SPE) improves the output of TECTOMT,
and we use this to analyze errors in TEC-
TOMT. However, we also show that ex-
tending data for SMT is more effective.
1 Introduction
This paper describes two submissions to the
WMT 2013 shared task:1 TECTOMT ? a deep-
syntactic tree-to-tree system and PHRASEFIX ?
statistical post-editing of TECTOMT using Moses
(Koehn et al, 2007). We also report on exper-
iments with another hybrid method where TEC-
TOMT is used to produce additional (so-called
synthetic) parallel training data for Moses. This
method was used in CU-BOJAR and CU-DEPFIX
submissions, see Bojar et al (2013).
2 Overview of Related Work
The number of approaches to system combination
is enormous. We very briefly survey those that
form the basis of our work reported in this paper.
2.1 Statistical Post-Editing
Statistical post-editing (SPE, see e.g. Simard et al
(2007), Dugast et al (2009)) is a popular method
1http://www.statmt.org/wmt13
for improving outputs of a rule-based MT sys-
tem. In principle, SPE could be applied to any
type of first-stage system including a statistical
one (Oflazer and El-Kahlout, 2007; B?chara et al,
2011), but most benefit could be expected from
post-editing rule-based MT because of the com-
plementary nature of weaknesses and advantages
of rule-based and statistical approaches.
SPE is usually done with an off-the-shelf SMT
system (e.g. Moses) which is trained on output of
the first-stage system aligned with reference trans-
lations of the original source text. The goal of SPE
is to produce translations that are better than both
the first-stage system alone and the second-stage
SMT trained on the original training data.
Most SPE approaches use the reference trans-
lations from the original training parallel corpus
to train the second-stage system. In contrast,
Simard et al (2007) use human-post-edited first-
stage system outputs instead. Intuitively, the lat-
ter approach achieves better results because the
human-post-edited translations are closer to the
first-stage output than the original reference trans-
lations. Therefore, SPE learns to perform the
changes which are needed the most. However, cre-
ating human-post-edited translations is laborious
and must be done again for each new (version of
the) first-stage system in order to preserve its full
advantage over using the original references.2
Rosa et al (2013) have applied SPE on
English?Czech SMT outputs. They have used
the approach introduced by B?chara et al (2011),
but no improvement was achieved. However, their
rule-based post-editing were found helpful.
Our SPE setting (called PHRASEFIX) uses
TECTOMT as the first-stage system and Moses as
the second-stage system. Ideally, TECTOMT pre-
2If more reference translations are available, it would be
beneficial to choose such references for training SPE which
are most similar to the first-stage outputs. However, in our
experiments only one reference is available.
141
serves well-formed syntactic sentence structures,
and the SPE (Moses) fixes low fluency wordings.
2.2 MT Output Combination
An SPE system is trained to improve the output
of a single first-stage system. Sometimes, more
(first-stage) systems are available, and we would
like to combine them. In MT output selection,
for each sentence one system?s translation is se-
lected as the final output. In MT output combi-
nation, the final translation of each sentence is a
combination of phrases from several systems. In
both approaches, the systems are treated as black
boxes, so only their outputs are needed. In the
simplest setting, all systems are supposed to be
equally good/reliable, and the final output is se-
lected by voting, based on the number of shared n-
grams or language model scores. The number and
the identity of the systems to be combined there-
fore do not need to be known in advance. More so-
phisticated methods learn parameters/weights spe-
cific for the individual systems. These methods
are based e.g. on confusion networks (Rosti et al,
2007; Matusov et al, 2008) and joint optimization
of word alignment, word order and lexical choice
(He and Toutanova, 2009).
2.3 Synthetic Data Combination
Another way to combine several first-stage sys-
tems is to employ a standard SMT toolkit, e.g.
Moses. The core of the idea is to use the n first-
stage systems to prepare synthetic parallel data
and include them in the training data for the SMT.
Corpus Combination (CComb) The easiest
method is to use these n newly created paral-
lel corpora as additional training data, i.e. train
Moses on a concatenation of the original paral-
lel sentences (with human-translated references)
and the new parallel sentences (with machine-
translated pseudo-references).
Phrase Table Combination (PTComb) An-
other method is to extract n phrase tables in
addition to the original phrase table and ex-
ploit the Moses option of multiple phrase tables
(Koehn and Schroeder, 2007). This means that
given the usual five features (forward/backward
phrase/lexical log probability and phrase penalty),
we need to tune 5 ? (n+1) features. Because such
MERT (Och, 2003) tuning may be unstable for
higher n, several methods were proposed where
the n+1 phrase tables are merged into a single one
(Eisele et al, 2008; Chen et al, 2009). Another is-
sue of phrase table combination is that the same
output can be achieved with phrases from several
phrase tables, leading to spurious ambiguity and
thus less diversity in n-best lists of a given size
(see Chen et al (2009) for one possible solution).
CComb does not suffer from the spurious ambi-
guity issue, but it does not allow to tune special
features for the individual first-stage systems.
In our experiments, we use both CComb and
PTComb approaches. In PTComb, we use TEC-
TOMT as the only first-stage system and Moses as
the second-stage system. We use the two phrase
tables separately (the merging is not needed; 5 ? 2
is still a reasonable number of features in MERT).
In CComb, we concatenate English?Czech par-
allel corpus with English??synthetic Czech? cor-
pus translated from English using TECTOMT. A
single phrase table is created from the concate-
nated corpus.
3 TECTOMT
TECTOMT is a linguistically-motivated tree-to-
tree deep-syntactic translation system with trans-
fer based on Maximum Entropy context-sensitive
translation models (Marec?ek et al, 2010) and
Hidden Tree Markov Models (?abokrtsk? and
Popel, 2009). It employs some rule-based compo-
nents, but the most important tasks in the analysis-
transfer-synthesis pipeline are based on statistics
and machine learning. There are three main rea-
sons why it is a suitable candidate for SPE and
other hybrid methods.
? TECTOMT has quite different distribution
and characteristics of errors compared to
standard SMT (Bojar et al, 2011).
? TECTOMT is not tuned for BLEU using
MERT (its development is rather driven by hu-
man inspection of the errors although different
setups are regularly evaluated with BLEU as an
additional guidance).
? TECTOMT uses deep-syntactic dependency
language models in the transfer phase, but it
does not use standard n-gram language mod-
els on the surface forms because the current syn-
thesis phase supports only 1-best output.
The version of TECTOMT submitted to WMT
2013 is almost identical to the WMT 2012 version.
Only a few rule-based components (e.g. detection
of surface tense of English verbs) were refined.
142
Corpus Sents TokensCzech English
CzEng 15M 205M 236M
tmt(CzEng) 15M 197M 236M
Czech Web Corpus 37M 627M ?
WMT News Crawl 25M 445M ?
Table 1: Statistics of used data.
4 Common Experimental Setup
All our systems (including TECTOMT) were
trained on the CzEng (Bojar et al, 2012) par-
allel corpus (development and evaluation sub-
sets were omitted), see Table 1 for statistics.
We translated the English side of CzEng with
TECTOMT to obtain ?synthetic Czech?. This
way we obtained a new parallel corpus, denoted
tmt(CzEng), with English? synthetic Czech sen-
tences. Analogically, we translated the WMT
2013 test set (newstest2013) with TECTOMT and
obtained tmt(newstest2013). Our baseline SMT
system (Moses) trained on CzEng corpus only was
then also used for WMT 2013 test set transla-
tion, and we obtained smt(newstest2013). For all
MERT tuning, newstest2011 was used.
4.1 Alignment
All our parallel data were aligned with GIZA++
(Och and Ney, 2003) and symmetrized with
the ?grow-diag-final-and? heuristics. This ap-
plies also to the synthetic corpora tmt(CzEng),
tmt(newstest2013),3 and smt(newstest2013).
For the SPE experiments, we decided to base
alignment on (genuine and synthetic Czech) lem-
mas, which could be acquired directly from the
TECTOMT output. For the rest of the experiments,
we approximated lemmas with just the first four
lowercase characters of each (English and Czech)
token.
4.2 Language Models
In all our experiments, we used three language
models on truecased forms: News Crawl as pro-
vided by WMT organizers,4 the Czech side of
CzEng and the Articles section of the Czech Web
3Another possibility was to adapt TECTOMT to output
source-to-target word alignment, but GIZA++ was simpler to
use also due to different internal tokenization in TECTOMT
and our Moses pipeline.
4The deep-syntactic LM of TECTOMT was trained only
on this News Crawl data ? http://www.statmt.org/
wmt13/translation-task.html (sets 2007?2012).
BLEU 1-TER
TECTOMT 14.71?0.53 35.61?0.60
PHRASEFIX 17.73?0.54 35.63?0.65
Filtering 14.68?0.50 35.47?0.57
Mark Reliable Phr. 17.87?0.55 35.57?0.66
Mark Identities 17.87?0.57 35.85?0.68
Table 2: Comparison of several strategies of SPE.
Best results are in bold.
Corpus (Spoustov? and Spousta, 2012).
We used SRILM (Stolcke, 2002) with modified
Kneser-Ney smoothing. We trained 5-grams on
CzEng; on the other two corpora, we trained 7-
grams and pruned them if the (training set) per-
plexity increased by less than 10?14 relative. The
domain of the pruned corpora is similar to the test
set domain, therefore we trained 7-grams on these
corpora. Adding CzEng corpus can then increase
the results only very slightly ? training 5-grams on
CzEng is therefore sufficient and more efficient.
Each of the three LMs got its weight as-
signed by MERT. Across the experiments, Czech
Web Corpus usually gained the largest portion of
weights (40?17% of the total weight assigned to
language models), WMT News Crawl was the sec-
ond (32?15%), and CzEng was the least useful
(15?7%), perhaps due to its wide domain mixture.
5 SPE Experiments
We trained a base SPE system as described in Sec-
tion 2.1 and dubbed it PHRASEFIX.
First two rows of Table 2 show that the first-
stage TECTOMT system (serving here as the base-
line) was significantly improved in terms of BLEU
(Papineni et al, 2002) by PHRASEFIX (p < 0.001
according to the paired bootstrap test (Koehn,
2004)), but the difference in TER (Snover et
al., 2006) is not significant.5 The preliminary
results of WMT 2013 manual evaluation show
only a minor improvement: TECTOMT=0.476
vs. PHRASEFIX=0.484 (higher means better, for
details on the ranking see Callison-Burch et al
(2012)).
5The BLEU and TER results reported here slightly differ
from the results shown at http://matrix.statmt.
org/matrix/systems_list/1720 because of differ-
ent tokenization and normalization. It seems that statmt.org
disables the --international-tokenization
switch, so e.g. the correct Czech quotes (?word?) are not
tokenized, hence the neighboring tokens are never counted
as matching the reference (which is tokenized as " word ").
143
Despite of the improvement, PHRASEFIX?s
phrase table (synthetic Czech ? genuine Czech)
still contains many wrong phrase pairs that worsen
the TECTOMT output instead of improving it.
They naturally arise in cases where the genuine
Czech is a too loose translation (or when the
English-Czech sentence pair is simply misaligned
in CzEng), and the word alignment between gen-
uine and synthetic Czech struggles.
Apart from removing such garbage phrase pairs,
it would also be beneficial to have some control
over the SPE. For instance, we would like to gen-
erally prefer the original output of TECTOMT ex-
cept for clear errors, so only reliable phrase pairs
should be used. We examine several strategies:
Phrase table filtering. We filter out all phrase
pairs with forward probability ? 0.7 and all sin-
gleton phrase pairs. These thresholds were set
based on our early experiments. Similar filtering
was used by Dugast et al (2009).
Marking of reliable phrases. This strategy is
similar to the previous one, but the low-frequency
phrase pairs are not filtered-out. Instead, a special
feature marking these pairs is added. The subse-
quent MERT of the SPE system selects the best
weight for this indicator feature. The frequency
and probability thresholds for marking a phrase
pair are the same as in the previous case.
Marking of identities A special feature indicat-
ing the equality of the source and target phrase in
a phrase pair is added. In general, if the output
of TECTOMT matched the reference, then such
output was probably good and does not need any
post-editing. These phrase pairs should be perhaps
slightly preferred by the SPE.
As apparent from Table 2, marking either reli-
able phrases or identities is useful in our SPE set-
ting in terms of BLEU score. In terms of TER
measure, marking the identities slightly improves
PHRASEFIX. However, none of the improvements
is statistically significant.
6 Data Combination Experiments
We now describe experiments with phrase table
and corpus combination. In the training step, the
source-language monolingual corpus that serves
as the basis of the synthetic parallel data can
be:
? the source side of the original parallel training
corpus (resulting in tmt(CzEng)),
? a huge source-language monolingual corpus for
which no human translations are available (we
have not finished this experiment yet),
? the source side of the test set (resulting in
tmt(newstest2013) if translated by TECTOMT
or smt(newstest2013) if translated by baseline
configuration of Moses trained on CzEng), or
? a combination of the above.
There is a trade-off in the choice: the source
side of the test set is obviously most useful for
the given input, but it restricts the applicability (all
systems must be installed or available online in the
testing time) and speed (we must wait for the slow-
est system and the combination).
So far, in PTComb we tried adding the full
synthetic CzEng (?CzEng + tmt(CzEng)?), adding
the test set (?CzEng + tmt(newstest2013)? and
?CzEng + smt(newstest2013)?), and adding both
(?CzEng + tmt(CzEng) + tmt(newstest2013)?). In
CComb, we concatenated CzEng and full syn-
thetic CzEng (?CzEng + tmt(CzEng)?).
There are two flavors of PTComb: either the
two phrase tables are used both at once as alter-
native decoding paths (?Alternative?), where each
source span is equipped with translation options
from any of the tables, or the synthetic Czech
phrase table is used only as a back-off method if a
source phrase is not available in the primary table
(?Back-off?). The back-off model was applied to
source phrases of up to 5 tokens.
Table 3 summarizes our results with phrase ta-
ble and corpus combination. We see that adding
synthetic data unrelated to the test set does bring
only a small benefit in terms of BLEU in the case
of CComb, and we see a small improvement in
TER in two cases. Adding the (synthetic) transla-
tion of the test set helps. However, adding trans-
lated source side of the test set is helpful only if
it is translated by the TECTOMT system. If our
baseline system is used for this translation, the re-
sults even slightly drop.
Somewhat related experiments for pivot lan-
guages by Galu?c??kov? and Bojar (2012) showed
a significant gain when the outputs of a rule-based
system were added to the training data of Moses.
In their case however, the genuine parallel corpus
was much smaller than the synthetic data. The
benefit of unrelated synthetic data seems to van-
ish with larger parallel data available.
144
Training Data for Moses Decoding Type BLEU 1-TER
baseline: CzEng ? 18.52?0.57 36.41?0.66
tmt(CzEng) ? 15.96?0.53 33.67?0.63
CzEng + tmt(CzEng) CComb 18.57?0.57 36.47?0.64
CzEng + tmt(CzEng) PTComb Alternative 18.42?0.58 36.47?0.65
CzEng + tmt(CzEng) PTComb Back-off 18.38?0.57 36.25?0.65
CzEng + tmt(newstest2013) PTComb Alternative 18.68?0.57 37.00?0.65
CzEng + smt(newstest2013) PTComb Alternative 18.46?0.54 36.59?0.65
CzEng + tmt(CzEng) + tmt(newstest2013) PTComb Alternative 18.85?0.58 37.03?0.66
Table 3: Comparison of several strategies used for Synthetic Data Combination (PTComb ? phrase table
combination and CComb ? corpus combination).
BLEU Judged better
SPE 17.73?0.54 123
PTComb 18.68?0.57 152
Table 4: Automatic (BLEU) and manual (number
of sentences judged better than the other system)
evaluation of SPE vs. PTComb.
7 Discussion
7.1 Comparison of SPE and PTComb
Assuming that our first-stage system, TECTOMT,
guarantees the grammaticality of the output (sadly
often not quite true), we see SPE and PTComb
as two complementary methods that bring in the
goods of SMT but risk breaking the grammati-
cality. Intuitively, SPE feels less risky, because
one would hope that the post-edits affect short se-
quences of words and not e.g. the clause structure.
With PTComb, one relies purely on the phrase-
based model and its well-known limitations with
respect to grammatical constraints.
Table 4 compares the two approaches empir-
ically. For SPE, we use the default PHRASE-
FIX; for PTComb, we use the option ?CzEng +
tmt(newstest2013)?. The BLEU scores are re-
peated.
We ran a small manual evaluation where three
annotators judged which of the two outputs was
better. The identity of the systems was hidden,
but the annotators had access to both the source
and the reference translation. Overall, we col-
lected 333 judgments over 120 source sentences.
Of the 333 judgments, 17 marked the two systems
as equally correct, and 44 marked the systems as
incomparably wrong. Across the remaining 275
non-tying comparisons, PTComb won ? 152 vs.
123.
We attribute the better performance of PTComb
to the fact that, unlike SPE, it has direct access to
the source text. Also, the risk of flawed sentence
structure in PTComb is probably not too bad, but
this can very much depend on the language pair.
English?Czech translation does not need much
reordering in general.
Based on the analysis of the better marked re-
sults of the PTComb system, the biggest problem
is the wrong selection of the word and word form,
especially for verbs. PTComb also outperforms
SPE in processing of frequent phrases and sub-
ordinate clauses. This problem could be solved
by enhancing fluency in SPE or by incorporat-
ing more training data. Another possibility would
be to modify TECTOMT system to produce more
than one-best translation as the correct word or
word form may be preserved in sequel transla-
tions.
7.2 Error Analysis of TECTOMT
While SPE seems to perform worse, it has a
unique advantage: it can be used as a feedback
for improving the first stage system. We can either
inspect the filtered SPE phrase table or differences
in translated sentences.
After submitting our WMT 2013 systems, this
comparison allowed us to spot a systematic error
in TECTOMT tagging of latin-origin words:
source pancreas
TECTOMT slinivek [plural]
PHRASEFIX slinivky [singular] br?i?n?
The part-of-speech tagger used in TECTOMT in-
correctly detects pancreas as plural, and the wrong
morphological number is used in the synthesis.
PHRASEFIX correctly learns that the plural form
slinivek should be changed to singular slinivky,
which has also a higher language model score.
Moreover, PHRASEFIX also learns that the trans-
145
lation of pancreas should be two words (br?i?n?
means abdominal). TECTOMT currently uses a
simplifying assumption of 1-to-1 correspondence
between content words, so it is not able to produce
the correct translation in this case.
Another example shows where PHRASEFIX
recovered from a lexical gap in TECTOMT:
source people who are strong-willed
TECTOMT lid? , kter?? jsou siln? willed
PHRASEFIX lid? , kter?? maj? silnou vu?li
TECTOMT?s primary translation model considers
strong-willed an OOV word, so a back-off dictio-
nary specialized for hyphen compounds is used.
However, this dictionary is not able to translate
willed. PHRASEFIX corrects this and also the
verb jsou = are (the correct Czech translation is
maj? silnou vu?li = have a strong will).
Finally, PHRASEFIX can also break things:
source You won?t be happy here
TECTOMT Nebudete ?t?astn? tady
PHRASEFIX Vy tady ?t?astn? [you here happy]
Here, PHRASEFIX damaged the translation by
omitting the negative verb nebudete = you won?t.
8 Conclusion
Statistical post-editing (SPE) and phrase table
combination (PTComb) can be seen as two com-
plementary approaches to exploiting the mutual
benefits of our deep-transfer system TECTOMT
and SMT.
We have shown that SPE improves the results of
TECTOMT. Several variations of SPE have been
examined, and we have further improved SPE re-
sults by marking identical and reliable phrases us-
ing a special feature. However, SMT still out-
performs SPE according to BLEU and TER mea-
sures. Finally, employing PTComb, we have im-
proved the baseline SMT system by utilizing ad-
ditional data translated by the TECTOMT system.
A small manual evaluation suggests that PTComb
is on average better than SPE, though in about one
third of sentences SPE was judged better. In our
future experiments, we plan to improve SPE by
applying techniques suited for monolingual align-
ment, e.g. feature-based aligner considering word
similarity (Rosa et al, 2012) or extending the par-
allel data with vocabulary identities to promote
alignment of the same word form (Dugast et al,
2009). Marking and filtering methods for SPE also
deserve a deeper study. As for PTComb, we plan
to combine several sources of synthetic data (in-
cluding a huge source-language monolingual cor-
pus).
Acknowledgements
This research is supported by the grants
GAUK 9209/2013, FP7-ICT-2011-7-288487
(MosesCore) of the European Union and SVV
project number 267 314. We thank the two
anonymous reviewers for their comments.
References
Hanna B?chara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. MT Summit XIII, pages 308?315.
Ondr?ej Bojar, Milo? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proc. of WMT, pages 1?11,
Edinburgh, Scotland. ACL.
Ondr?ej Bojar, Zdene?k ?abokrtsk?, Ondr?ej Du?ek, Pe-
tra Galu?c??kov?, Martin Majli?, David Marec?ek, Jir??
Mar??k, Michal Nov?k, Martin Popel, and Ale? Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928, Istanbul,
Turkey. ELRA.
Ondr?ej Bojar, Rudolf Rosa, and Ale? Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proc. of WMT.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proc. of WMT, Montreal,
Canada. ACL.
Yu Chen, Michael Jellinghaus, Andreas Eisele,
Yi Zhang, Sabine Hunsicker, Silke Theison, Chris-
tian Federmann, and Hans Uszkoreit. 2009. Com-
bining Multi-Engine Translations with Moses. In
Proc. of WMT, pages 42?46, Athens, Greece. ACL.
Lo?c Dugast, Jean Senellart, and Philipp Koehn.
2009. Statistical Post Editing and Dictionary Ex-
traction: Systran/Edinburgh Submissions for ACL-
WMT2009. In Proc. of WMT, pages 110?114,
Athens, Greece. ACL.
Andreas Eisele, Christian Federmann, Herv? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to Integrate Multi-
ple Rule-Based Machine Translation Engines into a
Hybrid System. In Proc. of WMT, pages 179?182,
Columbus, Ohio. ACL.
Petra Galu?c??kov? and Ondr?ej Bojar. 2012. Improving
SMT by Using Parallel Data of a Closely Related
Language. In Proc. of HLT, pages 58?65, Amster-
dam, Netherlands. IOS Press.
146
Xiaodong He and Kristina Toutanova. 2009. Joint Op-
timization for Machine Translation System Combi-
nation. In Proc. of EMNLP, pages 1202?1211, Sin-
gapore. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. In Proc. of WMT, pages 224?227, Prague,
Czech Republic. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL, pages 177?180, Prague, Czech Re-
public. ACL.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of
EMNLP, Barcelona, Spain.
David Marec?ek, Martin Popel, and Zdene?k ?abokrt-
sk?. 2010. Maximum entropy translation model
in dependency-based MT framework. In Proc. of
MATR, pages 201?206. ACL.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose B.
Marino, Matthias Paulik, Salim Roukos, Holger
Schwenk, and Hermann Ney. 2008. System Combi-
nation for Machine Translation of Spoken and Writ-
ten Language. IEEE, 16(7):1222?1237.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
Sapporo, Japan.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proc. of WMT, pages 25?32. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Stroudsburg, PA, USA. ACL.
Rudolf Rosa, Ondr?ej Du?ek, David Marec?ek, and Mar-
tin Popel. 2012. Using Parallel Features in Parsing
of Machine-Translated Sentences for Correction of
Grammatical Errors. In Proc. of SSST, pages 39?48,
Jeju, Republic of Korea. ACL.
Rudolf Rosa, David Marec?ek, and Ale? Tamchyna.
2013. Deepfix: Statistical Post-editing of Statistical
Machine Translation Using Deep Syntactic Analy-
sis. Sofia, Bulgaria. ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple
Machine Translation Systems. In Proc. of NAACL,
pages 228?235, Rochester, New York. ACL.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical Phrase-Based Post-Editing. In
Proc. of NAACL, pages 508?515, Rochester, New
York. ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of Association for Machine Translation in
the Americas, pages 223?231.
Johanka Spoustov? and Miroslav Spousta. 2012. A
High-Quality Web Corpus of Czech. In Proc. of
LREC, Istanbul, Turkey. ELRA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of ICSLP, pages
257?286.
Zdene?k ?abokrtsk? and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proc. of IJCNLP, pages 145?148,
Suntec, Singapore.
147
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 195?200,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
CUNI in WMT14: Chimera Still Awaits Bellerophon
Ale
?
s Tamchyna, Martin Popel, Rudolf Rosa, Ond
?
rej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, Prague, Czech Republic
surname@ufal.mff.cuni.cz
Abstract
We present our English?Czech and
English?Hindi submissions for this
year?s WMT translation task. For
English?Czech, we build upon last year?s
CHIMERA and evaluate several setups.
English?Hindi is a new language pair for
this year. We experimented with reverse
self-training to acquire more (synthetic)
parallel data and with modeling target-side
morphology.
1 Introduction
In this paper, we describe translation systems sub-
mitted by Charles University (CU or CUNI) to the
Translation task of the Ninth Workshop on Statis-
tical Machine Translation (WMT) 2014.
In ?2, we present our English?Czech systems,
CU-TECTOMT, CU-BOJAR, CU-DEPFIX and CU-
FUNKY. The systems are very similar to our sub-
missions (Bojar et al., 2013) from last year, the
main novelty being our experiments with domain-
specific and document-specific language models.
In ?3, we describe our experiments with
English?Hindi translation, which is a translation
pair new both to us and to WMT. We unsuccess-
fully experimented with reverse self-training and a
morphological-tags-based language model, and so
our final submission, CU-MOSES, is only a basic
instance of Moses.
2 English?Czech
Our submissions for English?Czech build upon
last year?s successful CHIMERA system (Bojar
et al., 2013). We combine several different ap-
proaches:
? factored phrase-based Moses model (?2.1),
? domain-adapted language model (?2.2),
? document-specific language models (?2.3),
? deep-syntactic MT system TectoMT (?2.4),
? automatic post-editing system Depfix (?2.5).
We combined the approaches in several ways
into our four submissions, as made clear by Ta-
ble 1. CU-TECTOMT is the stand-alone TectoMT
translation system, while the other submissions
are Moses-based, using TectoMT indirectly to pro-
vide an additional phrase-table. CU-BOJAR uses
a factored model and a domain-adapted language
model; in CU-DEPFIX, Depfix post-processing is
added; and CU-FUNKY also employs document-
specific language models.
C
U
-
T
E
C
T
O
M
T
C
U
-
B
O
J
A
R
C
U
-
D
E
P
F
I
X
C
U
-
F
U
N
K
Y
TectoMT (?2.4) D D D D
Factored Moses (?2.1) D D D
Adapted LM (?2.2) D D D
Document-specific LMs (?2.3) D
Depfix (?2.5) D D
Table 1: EN?CS systems submitted to WMT.
2.1 Our Baseline Factored Moses System
Our baseline translation system (denoted ?Base-
line? in the following) is similar to last year ? we
trained a factored Moses model on the concatena-
tion of CzEng (Bojar et al., 2012) and Europarl
(Koehn, 2005), see Table 2. We use two fac-
tors: tag, which is the part-of-speech tag, and stc,
which is ?supervised truecasing?, i.e. the surface
form with letter case set according to the lemma;
see (Bojar et al., 2013). Our factored Moses sys-
tem translates from English stc to Czech stc | tag
in one translation step.
Our basic language models are identical to last
year?s submission. We added an adapted language
195
Tokens [M]
Corpus Sents [M] English Czech
CzEng 1.0 14.83 235.67 205.17
Europarl 0.65 17.61 15.00
Table 2: English?Czech parallel data.
Corpus Sents [M] Tokens [M]
CzEng 1.0 14.83 205.17
CWC Articles 36.72 626.86
CNC News 28.08 483.88
CNA 47.00 830.32
Newspapers 64.39 1040.80
News Crawl 24.91 444.84
Total 215.93 3631.87
Table 3: Czech monolingual data.
model which we describe in the following section.
Tables 3 and 4 show basic data about the language
models. Aside from modeling surface forms, our
language models also capture morphological co-
herence to some degree.
2.2 Adapted Language Model
We used the 2013 News Crawl to create a language
model adapted to the domain of the test set (i.e.
news domain) using data selection based on infor-
mation retrieval (Tamchyna et al., 2012). We use
the Baseline system to translate the source sides of
WMT test sets 2012?2014. The translations then
constitute a ?query corpus? for Lucene.
1
For each
sentence in the query corpus, we use Lucene to
retrieve 20 most similar sentences from the 2013
News Crawl. After de-duplication, we obtained a
monolingual corpus of roughly 250 thousand sen-
tences and trained an additional 6-gram language
model on this data.
Domain Factor Order Sents Tokens ARPA.gz Trie
[M] [M] [GB] [GB]
General stc 4 201.31 3430.92 28.2 11.8
General stc 7 24.91 444.84 13.1 8.1
General tag 10 14.83 205.17 7.2 3.0
News stc 6 0.25 4.73 0.2 ?
Table 4: Czech LMs used in CU-BOJAR. The last
small model is described in ?2.2.
1
http://lucene.apache.org
2.3 Document-Specific Language Models
CU-FUNKY further extends the idea described in
?2.2. Taking advantage of document IDs which
are included in WMT development and test data,
we split our dev- (WMT 13) and test-set (WMT
14) into documents. We translate each document
with the Baseline system and use Lucene to re-
trieve 10,000 most similar target-side sentences
from News Crawl 2013 for each document sen-
tence.
Using this procedure, we obtain a corpus for
each document. On average, the corpora con-
tain roughly 208 thousand sentences after de-
duplication. Each corpus then serves as the
training data for the document-specific language
model.
We implemented an alternative to
moses-parallel.perl which splits the
input corpus based on document IDs and runs a
separate Moses instance/job for each document.
Moreover, it allows to modify the Moses config-
uration file according to document ID. We use
this feature to plant the correct document-specific
language model to each job.
In tuning, our technique only adds one weight.
In each split, the weight corresponds to a differ-
ent language model. The optimizer then hope-
fully averages the utility of this document-specific
LM across all documents. The same weight is ap-
plied also in the test set translation, exchanging the
document-specific LM file.
2.4 TectoMT Deep-Syntactic MT System
TectoMT
2
was one of the three key components
in last year?s CHIMERA. It is a linguistically-
motivated tree-to-tree deep-syntactic translation
system with transfer based on Maximum Entropy
context-sensitive translation models (Mare?cek et
al., 2010) and Hidden Tree Markov Models
(
?
Zabokrtsk?y and Popel, 2009). It is trained on
the WMT-provided data: CzEng 1.0 (parallel data)
and News Crawl (2007?2012 Czech monolingual
sets).
We maintain the same approach to combining
TectoMT with Moses as last year ? we translate
WMT test sets from years 2007?2014 and use
them as additional synthetic parallel training data ?
a corpus consisting of the test set source side (En-
glish) and TectoMT output (synthetic Czech). We
then use the standard extraction pipeline to create
2
http://ufal.mff.cuni.cz/tectomt/
196
an additional phrase table from this corpus. The
translated data overlap completely both with our
development and test data for Moses so that tuning
can assign an appropriate weight to the synthetic
phrase table.
2.5 Depfix Automatic Post-Editing
As in the previous years, we used Depfix (Rosa,
2013) to post-process the translations. Depfix is
an automatic post-editing system which is mainly
rule-based and uses various linguistic tools (tag-
gers, parsers, morphological generators, etc.) to
detect and correct errors, especially grammatical
ones. The system was slightly improved since last
year, and a new fixing rule was added for correct-
ing word order in noun clusters translated as geni-
tive constructions.
In English, a noun can behave as an adjective,
as in ?according to the house owners?, while in
Czech, this is not possible, and a genitive construc-
tion has to be used instead, similarly to ?according
to the owners of the house? ? the modifier is in the
genitive morphological case and follows the noun.
However, SMT systems translating into Czech do
not usually focus much on word reordering, which
leads to non-fluent or incomprehensible construc-
tions, such as ?podle domu
gen
vlastn??k?u
gen
? (ac-
cording to-the-house of-the-owners). Fortunately,
such cases are easy to distinguish with the help
of a dependency parser and a morphological tag-
ger ? genitive modifiers usually do not precede the
head but follow it (unless they are parts of named
entities), so we can safely switch the word order
to the correct one: ?podle vlastn??k?u
gen
domu
gen
?
(according to-the-owners of-the-house).
2.6 Results
We report scores of automatic metrics as shown in
the submission system,
3
namely (case-sensitive)
BLEU (Papineni et al., 2002) and TER (Snover
et al., 2006). The results, summarized in Ta-
ble 5, show that CU-FUNKY is the most success-
ful of our systems according to BLEU, while
the simpler CU-DEPFIX wins in TER. The re-
sults of manual evaluation suggest that CU-DEPFIX
(dubbed CHIMERA) remains the best performing
English?Czech system.
In comparison to other English?Czech sys-
tems submitted to WMT 2014, CU-FUNKY ranked
as the second in BLEU, and CU-DEPFIX ranked
3
http://matrix.statmt.org/
as the second in TER; the winning system, ac-
cording to both of these metrics, was UEDIN-
UNCONSTRAINED.
System BLEU TER Manual
CU-DEPFIX 21.1 0.670 0.373
UEDIN-UNCONSTRAINED 21.6 0.667 0.357
CU-BOJAR 20.9 0.674 0.333
CU-FUNKY 21.2 0.675 0.287
GOOGLE TRANSLATE 20.2 0.687 0.168
CU-TECTOMT 15.2 0.716 -0.177
CU-BOJAR +full 2013 news 20.7 0.677 ?
Table 5: Scores of automatic metrics and results of
manual evaluation for our systems. The table also
lists the best system according to automatic met-
rics and Google Translate as the best-performing
commercial system.
Our analysis of CU-FUNKY suggests that it is
not the best performing system on average (de-
spite achieving the highest BLEU scores from our
submissions), but that it is rather the most volatile
system. Some sentences were obviously improved
compared to CU-BOJAR but most got degraded es-
pecially in adequacy. We are well aware of the
many shortcomings our current implementation
has, the most severe of which lie in the sentence
selection by Lucene. For instance, we do not use
any stopwords or keyword detection methods, and
also pretending that each sentence in our monolin-
gual corpus is a ?document? for the information
retrieval system is far from ideal.
We also evaluated a version of CU-BOJAR which
uses not only the adapted LM but also an addi-
tional LM trained on the full 2013 News Crawl
data (see ?CU-BOJAR +full 2013 news? in Table 5)
but found no improvement compared to using just
the adapted model (trained on a subset of the data).
3 English?Hindi
English-Hindi is a new language pair this
year. We submitted an unconstrained system for
English?Hindi translation.
We used HindEnCorp (Bojar et al., 2014) as the
sole source of parallel data (nearly 276 thousand
sentence pairs, around 3.95 million English tokens
and 4.09 million Hindi tokens).
Given that no test set from previous years was
available and that the size of the development set
provided by WMT organizers was only 500 sen-
tence pairs, we held out the first 5000 sentence
pairs of HindEnCorp for this purpose. Our de-
velopment set then consisted of the 500 provided
197
Corpus Sents [M] Tokens [M]
NewsCrawl 1.27 27.27
HindEnCorp 0.28 4.09
HindMonoCorp 43.38 945.43
Total 44.93 976.80
Table 6: Hindi monolingual data.
sentences plus 1500 sentence pairs from HindEn-
Corp. The remaining 3500 sentence pairs taken
from HindEnCorp constituted our test set.
As for monolingual data, we used the News
Crawl corpora provided for the task and the new
monolingual HindMonoCorp, which makes our
submission unconstrained. Table 6 shows statis-
tics of our monolingual data.
We tagged and lemmatized the English data us-
ing Mor?ce (Spoustov?a et al., 2007) and the Hindi
data using Siva Reddy?s POS tagger.
4
3.1 Baseline System
The baseline system was eventually our best-
performing one. Its design is completely straight-
forward ? it uses one phrase table trained on
all parallel data (we translate from ?supervised-
truecased? English into Hindi forms) and one 5-
gram language model trained on all monolingual
data. We used KenLM (Heafield et al., 2013) for
estimating the model as the data was rather large
(see Table 6).
We used GIZA++ (Och and Ney, 2000) as
our word alignment tool. We experimented with
several coarser representations to make the final
alignment more reliable. Table 7 shows the re-
sults. The factor ?stem4? refers to simply taking
the first four characters of each word. For lem-
mas, we used the outputs of the tools mentioned
above. However, lemmas as output by the Hindi
tagger were not much coarser than surface forms
? the ratio between the number of types is merely
1.11 ? so we also tried ?stemming? the lemmas
(lemma4). Of these variants, stem4-stem4 align-
ment worked best and we used it for the rest of our
experiments.
3.2 Reverse Self-Training
Bojar and Tamchyna (2011) showed a simple tech-
nique for improving translation quality in situa-
tions where there is only a small amount of par-
4
http://sivareddy.in/downloads#hindi_
tools
English Hindi BLEU
stem4 stem4 22.96?1.17
lemma lemma4 22.59?1.17
lemma lemma 22.41?1.20
Table 7: Comparison of different factor combina-
tions for word alignment.
allel data available but where there is a sufficient
quantity of target-side monolingual texts. The so-
called ?reverse self-training? uses a factored sys-
tem trained in the opposite direction to translate
the large monolingual data into the source lan-
guage. The translation (in the source language,
i.e. English in our case) and the original target-
side data (Hindi) can be used as additional syn-
thetic parallel data. The authors recommend creat-
ing a separate phrase table from it and combining
the two translation models as alternatives in the
log-linear model (letting tuning weigh their impor-
tance).
The factored setup of the reverse system
(Hindi?English) is essential ? alternative decod-
ing paths with a back-off to a coarser representa-
tion (e.g. stems) on the source side (Hindi) give
the system the ability to generalize beyond surface
forms observed in the training data. The main aim
of this technique is to learn new forms of known
words.
The technique is thus aimed at translating into a
morphologically richer language than the source.
Indeed, the authors showed that if the target lan-
guage has considerably more word types than the
source, the gains achieved by reverse self-training
are higher. In this respect, English?Hindi is not
an ideal candidate given that the ratio we observed
is only 1.2.
The choice of back-off representation is impor-
tant. We measure the vocabulary reduction of
several options and summarize the results in Ta-
ble 8. E.g. for stem4, the vocabulary size is
roughly 30% compared to the number of surface
word forms.
Bojar and Tamchyna (2011) achieved the best
results using ?nosuf3? (?suffix trimming?, i.e. cut-
ting of the last 3 characters of each word); how-
ever, they experimented with European languages
and the highest reduction of vocabulary reported
in the paper is to roughly one half. In our case, the
vocabulary is reduced much more, so we opted for
a more conservative back-off, namely ?nosuf2?.
198
Back-off % of vocab. size
stem4 30.21
lemma4 32.36
nosuf3 36.36
nosuf2 50.76
stem5 53.48
lemma5 57.47
lemma 90.09
Table 8: Options for back-off factors in reverse
self-training and the percentage of their vocabu-
lary size compared to surface forms.
We translated roughly 2 million sentences from
the Hindi monolingual data, focusing on news
to maintain a domain match with the WMT test
set. However, adding the synthetic phrase table
did not bring any improvement and in fact, the
BLEU score dropped to 22.37?1.17 (baseline is
22.96?1.17).
We can attribute the failure of reverse self-
training to the nature of the language pair at hand.
While Hindi has some synthetic properties (e.g.
future tense of verbs or inflection of adjectives are
marked by suffixes), its inflectional morphemes
are realized mainly by post-positions which are
separated from their head-words. Overlooking this
essential property, we attempted to use reverse
self-training but our technique could contribute
only very little.
3.3 Target-Side Morphology
We also experimented with a setup that tradition-
ally works very well for English?Czech trans-
lation: using a high-order language model on
morphological tags to explicitly model target-side
morphological coherence in translation. We used
the same monolingual data as for the baseline lan-
guage model; however, the order of our morpho-
logical language model was set to 10.
This setup also brought no improvement over
the baseline ? in fact, the BLEU score dropped
even further to 22.27?1.14.
4 Conclusion
We presented our contributions to the Translation
task of WMT 2014.
As we have focused on English?Czech trans-
lation for many years, we have developed sev-
eral complex and well-performing systems for it
? an adaptation of the phrase-based Moses sys-
tem, a linguistically-motivated syntax-based Tec-
toMT system, and an automatic post-editing Dep-
fix system. We combine the individual systems
using a very simple yet effective method and the
combined system called CHIMERA confirmed its
state-of-the-art performance.
For English?Hindi translation, which was a
new task for us, we managed to get competitive
results by using a baseline Moses setup, but were
unable to improve upon those by employing ad-
vanced techniques that had proven to be effective
for other translation directions.
Acknowledgments
This research was supported by the grants FP7-
ICT-2013-10-610516 (QTLeap), FP7-ICT-2011-
7-288487 (MosesCore), SVV 260 104. and
GAUK 1572314. This work has been using lan-
guage resources developed, stored and distributed
by the LINDAT/CLARIN project of the Ministry
of Education, Youth and Sports of the Czech Re-
public (project LM2010013).
References
Ond?rej Bojar and Ale?s Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Proc.
of WMT, pages 330?336. ACL.
Ond?rej Bojar, Zden?ek
?
Zabokrtsk?y, Ond?rej Du?sek, Pe-
tra Galu?s?c?akov?a, Martin Majli?s, David Mare?cek, Ji?r??
Mar?s??k, Michal Nov?ak, Martin Popel, and Ale?s Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928. ELRA.
Ondrej Bojar, Rudolf Rosa, and Ale?s Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, pages 90?96.
Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, Pavel
Stra?n?ak, V??t Suchomel, Ale?s Tamchyna, and Daniel
Zeman. 2014. HindEnCorp ? Hindi-English and
Hindi-only Corpus for Machine Translation. Reyk-
jav??k, Iceland. European Language Resources Asso-
ciation.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79?86.
David Mare?cek, Martin Popel, and Zden?ek
?
Zabokrtsk?y.
2010. Maximum entropy translation model in
199
dependency-based MT framework. In Proc. of WMT
and MetricsMATR, pages 201?206. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proc. of ACL,
pages 440?447, Hong Kong. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Stroudsburg, PA, USA. ACL.
Rudolf Rosa. 2013. Automatic post-editing of phrase-
based machine translation outputs. Master?s thesis,
Charles University in Prague, Faculty of Mathemat-
ics and Physics, Praha, Czechia.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Drahom??ra Spoustov?a, Jan Haji?c, Jan Votrubec, Pavel
Krbec, and Pavel Kv?eto?n. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Ale?s Tamchyna, Petra Galu?s?c?akov?a, Amir Kamran,
Milo?s Stanojevi?c, and Ond?rej Bojar. 2012. Select-
ing Data for English-to-Czech Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, WMT ?12, pages 374?
381, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Zden?ek
?
Zabokrtsk?y and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proc. of ACL-IJCNLP Short Papers,
pages 145?148.
200
