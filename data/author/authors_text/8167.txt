Proceedings of the Linguistic Annotation Workshop, pages 57?60,
Prague, June 2007. c?2007 Association for Computational Linguistics
Querying multimodal annotation: A concordancer for GeM
Martin Thomas
Centre for Translation Studies
University of Leeds
UK, LS2 9JT
m.thomas@leeds.ac.uk
Abstract
This paper presents a multimodal corpus of
comparable pack messages and the concor-
dancer that has been built to query it. The
design of the corpus and its annotation is
introduced. This is followed by a descrip-
tion of the concordancer?s interface, imple-
mentation and concordance display. Finally,
some ideas for future work are outlined.
1 Introduction
This paper introduces a multimodal concordancer1
that has been developed to investigate variation be-
tween messages on fast-moving consumer goods
packaging from China, Taiwan and the UK. The
need to develop such a concordancer arises from the
fact that these pack messages are themselves mul-
timodal. While they communicate through what
Twyman (1985) calls the visual channel, messages
are realized using a combination of three modes
(verbal, schematic, pictorial). Moreover, the verbal
components of visual messages are modulated and
segmented through typography (Waller, 1987).
It is assumed that this multimodality will have
complex implications for cross-linguistic variation
within the genre of pack messages. The specific na-
ture of these implications is not yet known, but vari-
ation in the construal of textual meaning and cohe-
sion would seem to offer a good starting point for
investigation. However, using purely linguistic an-
notation and a monomodal concordancer to analyze
such material could reveal only part of the picture.
1http://corpus.leeds.ac.uk/?martin/
An existing annotation scheme, developed by the
Genre and Multimodality (GeM) project2, is well-
suited to my needs. In addition to information about
their verbal and visual realization, the scheme pro-
vides a mechanism for encoding the rhetorical rela-
tions between message components.
However, existing tools for multimodal analysis
do not support simultaneous investigation of verbal,
visual and rhetorical phenomena. While Baldry?s
(2004) multimodal concordancer supports multilay-
ered analysis of video data, his approach does not
support the segmentation of still visual layouts, let
alone consideration of specific typographical real-
izations. From an altogether different perspective,
the database developed as part of the Typographic
Design for Children3 project does allow access to
such typographic information, but does not relate
this directly to the linguistic realization of messages.
Their multimodal realization makes pack mes-
sages a rich testing ground for the new concordancer
and Chinese and English offer great potential for
looking at multimodal cross-linguistic variation. Ty-
pographic resources are constrained by the writing
system of a given language: Chinese offers variety
in reading directions and a consistent footprint for
each character; English offers a range of case dis-
tinctions and a predictable reading direction.
2 Corpus design
I take each pack as a text: through the messages
by which it is realized, it ?functions as a unity with
respect to its environment? (Halliday and Hasan,
2http://www.purl.org/net/gem/
3http://www.kidstype.org/
57
1976). In the corpus, each text constitutes a record.
Each record consists of a set of files. These include
the transcribed and annotated pack messages, and
photographs of each pack face. In the future, pack
metadata will be added to describe the product cat-
egory to which the pack belongs, the product name,
brand owner, variety and so on. I will also record
the location and date of purchase of each sample.
This will support query constraints at the level of
the record (e.g. packs of a certain size) and will fa-
cilitate comparisons across time as well as across lo-
cales, or markets.
Packs are represented in the corpus in an un-
opened state. As far as possible, every message
on each face of the pack which is visible in this
state is recorded. There are good reasons for this.
Sinclair (1991) makes the point that the differences
across specific parts of a text may constitute regu-
larity within a genre. In the context of investigation
into cross-linguistic variation within a single genre,
this observation seems particularly apt.
The selection of packs for inclusion in the corpus
will be made in cooperation with an industrial part-
ner. Packs will be selected from product categories
in which the partner is active, or seeks to participate,
in all three locales. A combination of popular local
brands as well as locally established global brands
will be selected. Thus the packs will be comparable
commercially as well as in terms of the communica-
tive functions that they perform.
3 Corpus annotation
The GeM scheme is described comprehensively by
Henschel (2003). It implements stand-off annota-
tion in four XML layers. The base layer segments
the document. The resulting base units are cross-
referenced by layers which describe layout, rhetori-
cal structure and navigation.
Within the layout layer, there are three main sec-
tions: layout segmentation (each layout unit con-
tains one or more base units), realization informa-
tion and a description of the layout structure of the
document. These components allow a comprehen-
sive picture of the typographic realization of the
messages to be built, from details such as font fam-
ily and colour to information about the composition
of each pack and the location, spacing and framing
of chunks of layout units.
Rhetorical relations between annotated units are
expressed in terms of Rhetorical Structure Theory
(Mann and Thompson, 1987). In the GeM imple-
mentation, RST has been extended to accommodate
the graphical elements found in multimodal texts.
RST annotation provides a way to identify patterns
in the construction of messages and to make com-
parisons across the corpus. It might be that more
RST relations of a specific type, e.g. elaboration,
are found in messages from a particular locale. Such
observations might support or contest claims, such
as that packs from developing markets convention-
ally carry more information about how to use the
product. In combination with the layout layer it will
also be possible to look for patterns in the choice of
semiotic mode used to realize messages involving
specific types of relation, such as evidence.
In sum, the aim of the annotation is not to support
low-level lexicogrammatical analysis, but rather to
facilitate the uncovering of patterns in the linguistic
and typographical realization of pack messages and
to relate these to semantic values expressed in terms
of RST relations. Such patterns may reflect local de-
sign conventions and language-dependent strategies
for ensuring textual cohesion.
So far annotation has begun with several UK and
Taiwan packs. All annotation has been performed
manually and has proved costly in terms of time. In
future it is hoped that at least some annotations may
be generated through the conversion of digital copies
of designs obtained directly from brand owners.
The pilot annotations have identified a number of
ways in which the GeM scheme will need to be ex-
tended to accommodate the genre of pack messages
and important aspects of Chinese typography: the
lists of colours and font families enumerated in the
DTD are not sufficiently extensive or delicate and
there is no mechanism in the layout annotation layer
to record the orientation and reading direction of
text.
4 The prototype concordancer
4.1 Design aims and system overview
The concordancer is an established tool for linguis-
tic analysis. Concordance lines, which show in-
stances of a key word in their immediate contexts,
58
Figure 1: Multimodal concordancer interface
have proved useful in uncovering patterns of usage
and variation that may not be apparent either from
reading individual texts or from consulting reference
resources, such as dictionaries and grammars.
My aim was to develop a similar tool to support
multimodal analysis. Such a tool should be able
to combine questions relating to the verbal compo-
nents of messages with those relating to the typo-
graphic resources through which they are realized. It
should do this in such a way that queries can easily
be built and modified. To this end, a user interface is
needed. Finally, the concordancer should be usable
without the need for local installation of specialist
client software.
In order to meet these requirements, I adopted
a web-based client-server model. The user inter-
face is shown in Figure 1. The concordancer is
implemented in Perl as a CGI script. XPath ex-
pressions are used to identify matches from among
the XML-annotated packs and to handle cross-
references across annotation layers.
Using the concordancer interface to build a query
is a process of moving from the general to the spe-
cific. By default, all constraints are relaxed: submit-
ting a query with these selections will return every
annotated message in the corpus. More usefully, se-
lections can be made to constrain the set of records
searched and the linguistic, typographic, and picto-
rial realization properties of messages to match.
4.2 Search criteria
The search criteria are grouped into high- and low-
level selections. I will introduce the high-level se-
lections first.
Locale and category selections control the set of
records to be processed.
Given the notion of generic regularity in the dif-
ferences between different parts of texts, it seemed
sensible to allow queries to be constrained by pack
face. Looking at the front of a shampoo bottle might
be seen as akin to looking at the abstract of an aca-
demic paper. This is a step towards implementing
more specific constraints about the on-pack position
of messages. The pack face constraint, as with most
of the remaining selections, is implemented in an
XPath expression. The remaining high-level selec-
tions constrain the type of encoded element to in-
clude in the search.
The first group of low-level selections relate to
specific font properties.
The colours used to realize messages are de-
scribed in the corpus using hexadecimal RGB
triplets. While this affords precision in annotation, it
also means that some calculation is required to sup-
port searching. The current approach is to take any
colour selected by the user from the menu and calcu-
late the distance between this and the RGB value for
each candidate match. If this distance falls within
the tolerance specified by the user, the colour is con-
sidered to match. Thus a search for greenmaymatch
RGB values representing various hues.
Finally, all matching layout units are cross-
referenced with the base units that they realize. If the
user specified a pattern to match (a string or regular
expression), this is tested against the string value of
the base unit.
4.3 Concordance display
The final options on the interface control the dis-
play of the resulting concordance. In the pilot an-
notations, an English gloss for each Chinese pack
message is recorded as an XML comment. These
glosses may be reproduced in the concordance. The
other display options control whether to display the
base unit preceding and/or following the match.
Figure 2 shows the results of a query generated
from the selections shown in Figure 1. This is a
search for verbal messages on the front of packs
which are realized in a large font. Unsurprisingly,
in each case, this returns the product name which is
conventionally salient.
Details about the search query are given above the
59
Figure 2: Multimodal concordance example
concordance. Depending on the specific query, this
may include selections for locale and product cat-
egory, the XPath expression which identifies candi-
date layout realization units, the colour selection and
the search string or regular expression.
Information relating to each match is then dis-
played. As in a traditional concordancer, matches
are presented together with the context in which they
are found. Optionally, this context includes the pre-
ceding and following base units. Moreover, the no-
tion of context is extended to include the visual en-
vironment in which each match is found. The colour
used on-pack to realize the matching message is re-
used in the presentation of the match. A thumbnail
image of the pack face on which the match is found
is also presented, as is information about the typo-
graphic realization of the match, taken from the lay-
out annotation. Links are provided to high resolu-
tion photographs and to each annotation layer for the
pack from which the match is retrieved.
The display of the thumbnail is a step towards
a more specific indication of the position of each
match on the pack. In the future, I hope to use in-
formation from the layout annotation to generate a
visual representation of the layout chunk in which
each match is found.
The number of matches found is given below the
concordance.
5 Conclusions and future work
The prototype concordancer is rather slow: it takes
just under a minute to process and print every unit
in the pilot corpus and the time taken will increase
as more packs are added. But it works. It has also
been tested with files taken from the original GeM
corpus. Once they have been renamed, following
the conventions used by the concordancer, the legacy
files integrate seamlessly into the new corpus.
As noted above, there is scope for further devel-
opment in a number of areas. The pilot corpus needs
to be populated with more packs. The GeM annota-
tion scheme requires modification in certain details.
It might also be useful to add an annotation layer to
record translations of the string values of base units
rather than using XML comments for this.
As for the concordancer, support for queries based
on the rhetorical relations between message compo-
nents is the next major step. Other planned function-
ality includes the generation of typographically real-
ized layout chunks which contain query matches and
the calculation of collocation statistics which may be
compared across sets of records.
Finally, more work is needed to see whether the
concordancer is useful for the kind of analytical
work it has been developed to support.
References
Anthony P. Baldry. 2004. Phase and transition, type and
instance: patterns in media texts seen through a multi-
modal concordancer. In Kay O?Halloran, editor, Mul-
timodal discourse analysis: Systemic-functional per-
spectives. Continuum, London.
M.A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in
English. Longman, London.
Renate Henschel, 2003. GeM Annotation Manual Ver-
sion 2. GeM Project.
William Mann and Sandra Annear Thompson. 1987.
Rhetorical structure theory: A theory of text organiza-
tion. Technical report, Information Sciences Institute,
Los Angeles.
John Sinclair. 1991. Corpus, concordance, collocation.
Oxford University Press, Oxford.
Michael Twyman. 1985. Using pictorial language:
A discussion of the dimensions of the problem. In
Thomas Walker and Robert Duffy, editors, Designing
Usable Texts, chapter 11. Academic Press, Orlando,
Florida.
Robert Waller. 1987. The Typographic Contribution to
Language. Ph.D. thesis, University of Reading.
60
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101?112,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Design of a hybrid high quality machine translation system 
Kurt Eberle 
Johanna Gei? 
Mireia Ginest?-Rosell 
Bogdan Babych  
Anthony Hartley 
Reinhard Rapp  
Lingenio GmbH Serge Sharoff 
Karlsruher Stra?e 10 
69 126 Heidelberg, Germany 
Martin Thomas 
Centre for Translation Studies 
University of Leeds 
 Leeds, LS2 9JT, UK 
[k.eberle,j.geiss,m.ginesti-rosell] 
@lingenio.de 
[B.Babych,A.Hartley,R.Rapp, 
S.Sharoff,M.Thomas]@leeds.ac.uk  
  
 
 
Abstract 
This paper gives an overview of the 
ongoing FP7 project HyghTra (2010 ? 
2014). The HyghTra project is conducted 
in a partnership between academia and 
industry involving the University of Leeds 
and Lingenio GmbH (company). It adopts a 
hybrid and bootstrapping approach to the 
enhancement of MT quality by applying 
rule-based analysis and statistical 
evaluation techniques to both parallel and 
comparable corpora in order to extract 
linguistic information and enrich the lexical 
and syntactic resources of the underlying 
(rule-based) MT system that is used for 
analysing the corpora. The project places 
special emphasis on the extension of 
systems to new language pairs and 
corresponding rapid, automated creation of 
high quality resources. The techniques are 
fielded and evaluated within an existing 
commercial MT environment. 
1 Motivation 
Statistical Machine Translation (SMT) has been 
around for about 20 years, and for roughly half of 
this time SMT and the 'traditional' Rule-based 
Machine Translation (RBMT) have been seen as 
competing paradigms. During the last decade 
however, there is a trend and growing interest in 
combining the two methodologies. In our approach 
these two approaches are viewed as 
complementary. 
Advantages of SMT are low cost and robustness, 
but definite disadvantages of (pure) SMT are that it 
needs huge amounts of data, which for many 
language pairs are not available and are unlikely to 
become available in the future. Also, SMT tends to 
disregard important classificatory knowledge (such 
as morphosyntactic, categorical and lexical class 
features), which can be provided and used 
relatively easily within non-statistical 
representations.  
On the other hand, advantages of RBMT are that 
its (grammar and lexical) rules and information are 
understandable by humans and can be exploited for 
a lot of applications outside of translation 
(dictionaries, text understanding, dialogue systems, 
etc.).  
The slot grammar approach used in Lingenio 
systems (cf.  McCord 1989, Eberle 2001) is a 
prime example of such linguistically rich 
representations that can be used for a number of 
different applications. Fig.1 shows this by a 
visualization of (an excerpt of) the entry for the 
ambiguous German verb einstellen in the database 
that underlies (a)  the Lingenio translation 
products, where it links up with corresponding set 
of the transfer rules, and (b) Lingenio?s dictionary 
product TranslateDict, which is primarily intended 
for human translators.   
 
101
 
 
Fig 1 a) data base entry einstellen 
('translation' represents links between SL and T entries) 
 
 
 
 
Fig 1 b) product entry einstellen 
 
The obvious disadvantages of RBMT are high cost, 
weaknesses in dealing with incorrect input and in 
making correct choices with respect to ambiguous 
words, structures, and transfer equivalents. 
SMT output is often surprisingly good with respect 
to short distance collocations, but often misses 
correct choices are missed in cases where 
selectional restrictions take effect on distant words. 
RBMT output is generally good if the parser 
assigns the correct analysis to a sentence and  if the 
target words can be correctly chosen from the set 
of alternatives. However, in the presence of 
ambiguous words and structures, and where 
linguistic information is lacking, the decisions may 
be wrong. 
Given the complementarity of SMT and RBMT 
and their very different strengths and weaknesses, 
we take a view that an optimized MT architecture 
must comprise elements of both paradigms. The 
key issue therefore lies in the identification of such 
elements and how to connect them to each other. 
We propose a specific type of hybrid translation ? 
hybrid high quality translation (HyghTra), where 
core RBMT systems are created and enhanced by a 
range of reliable statistical techniques. 
 
2 Development Methodology 
Many hybrid systems described in the literature 
have attempted to put some analytical abstraction 
on top of an SMT kernel.1 In our view this is not 
the best option because, according to the 
underlying philosophy, SMT is linguistically 
ignorant at the beginning and learns all linguistic 
rules automatically from corpora. However, the 
extracted information is typically represented in 
huge data sets which are not readable by humans in 
a natural way. This means that this type of 
architecture does not easily provide interfaces for 
incorporating linguistic knowledge in a canonical 
and simple way. 
Thus we approach the problem from the other end, 
, integrating information derived from corpora 
using statistical methods into RBMT systems. 
Provided the underlying RBMT systems are 
linguistically sound and sufficiently modular in 
structure, we believe this to have greater potential 
for generating high quality output. 
We currently use and carry out the following work 
plan: 
 
(I) Creation of MT systems  
(with rule-based core MT information and 
statistical extension and training): 
(a) We start out with declarative analysis and 
generation components of the considered 
languages, and with basic bilingual dictionaries 
connecting to one another the entries of relatively 
small vocabularies comprising the most frequent 
words of each language in a given translation pair 
(cf. Fig 1 a). 
(b) Having completed this phase, we extend the 
dictionaries and train the analysis-, transfer- and 
generation-components of the rule-based core 
systems using monolingual and bilingual corpora.  
 
                                                           
1 A prominent early example is Frederking and 
colleagues (Frederking & Nirenburg, 1994). For an 
overview of  hybrid MT till the late nineties see Streiter 
et al (1999). More recent  approaches include Groves & 
Way (2006a, 2006b). Commercial implementations 
include AppTek (http://www.apptek.com) and Language 
Weaver (http://www.languageweaver.com). An ongoing 
MT important project investigating hybrid methods is 
EuroMatrixPlus (http://www.euromatrixplus.net/) 
102
(II) Error detection and improvement cycle:  
(a) We automatically discover the most frequent 
problematic grammatical constructions and 
multiword expressions for commercial RBMT and 
SMT systems using automatic construction-based 
evaluation as proposed in (Babych and Hartley, 
2009) and develop a framework for fixing 
corresponding grammar rules and extending 
grammatical coverage of the systems in a semi-
automatic way. This shortens development time for 
commercial MT and contributes to yielding 
significantly higher translation quality. 
 
(III) Extension to other languages: 
Structural similarity and translation by pivot 
languages is used to obtain extension to further 
languages: 
High-quality translation between closely related 
languages (e.g., Russian and Ukrainian or 
Portuguese and Spanish) can be achieved with 
relatively simple resources (using linguistic 
similarity, but also homomorphism assumptions 
with respect to parallel text, if available), while 
greater efforts are put into ensuring better-quality 
translation between more distant languages (e.g. 
German and Russian). According to our prior 
research (Babych et al, 2007b) the pipeline 
between languages of different similarity results in 
improved translation quality for a larger number of 
language pairs (e.g., MT from Portuguese or 
Ukrainian into German is easier if there are high-
quality analysis and transfer modules for Spanish 
and Russian into German (respectively). Of course, 
(III) draws heavily on the detailed analysis and MT 
systems that the industrial partner in HyghTra 
provides for a number of languages. 
 
In the following sections we give more details of 
the work currently done with regard to (I) and with 
regard to parts of (II): the creation of a new MT 
system following the strategy sketched. We cannot 
go further into detail with (II) and (III) here, which 
will become a priority for future research. 
3 Creation of a new system 
Early pilot studies covering some aspects of the 
strategy described here (using information from 
pivot languages and similarity) showed promising 
results (Rapp, 1999; Rapp & Mart?n Vide, 2007; 
see also Koehn & Knight, 2002). 
We expect that the proposed semi-automatic 
creation of a new MT system as sketched above 
will work best if one of the two languages involved 
is already 'known' by modules to which the system 
has access. Against the background of the pipeline 
approach mentioned above in (III), this means that 
we assume an analysis and translation system that 
continuously grows by 'learning' new languages 
where 'learning' is facilitated by information about 
the languages already 'known' and by exploiting 
similarity assumptions ? and, of course, by being 
fed with information prepared and provided by the 
human 'companion' of the system. 
From this perspective, we assume the following 
steps of extending the system (with work done by 
the 'companion' and work done by the system) 
 
1. Acquire parallel and comparable corpora. 
2. Define a core of the morphology of the new 
language and compile a basic dictionary for the 
most frequent words and translations. 
Morphological representations and features for 
new languages are derived both manually and 
automatically, as proposed in (Babych et al, 
2012 (in preparation)). 
3. Using established alignment technology (e.g. 
Giza++) and parallel corpora, generate a first 
extension of this dictionary. 
4. Expand the dictionary of step 3 using 
comparable corpora as proposed in a study by 
Rapp (1999). This is applicable mainly to single 
word units. 
5. Expand coverage of multiword-units using 
novel technology. 
6. Cross-validate the new dictionary with respect 
to available ones by transitivity. 
7. Integrate the new dictionary into the new MT 
system as developing from reusing components 
and adding new components as in 8. 
8. Complete morphology and spell out declarative 
analysis and generation grammar for the new 
language. 
9. Automatically evaluate the translations of the 
most frequent grammatical constructions and 
multiword expressions in a machine-translated 
corpus, prioritising support for these 
constructions with a type of risk-assessment 
framework proposed in Babych and Hartley 
(2008). 
10. Extend support for high-priority constructions 
semi-automatically by mining correct 
103
translations from parallel corpora. 
11. Train and evaluate the new grammar and 
transfer of the new MT system using the new 
dictionary on the basis of available parallel 
corpora. 
 
The following sections give an overview of the 
different steps. 
Step 1: Acquire parallel and comparable 
corpora 
As our parallel corpus, we use the Europarl. The 
size of the current version is up to 40 million 
words per language, and several of the languages 
we are currently considering are covered. Also, we 
make use of other parallel corpora such as the 
Canadian Hansards (Proceedings of the Canadian 
Parliament) for the English?French language pair. 
For non-EU Languages (mainly Russian), we 
intend to conduct a pilot study to establish the 
feasibility of retrieving parallel corpora from the 
web, a problem for which various approaches have 
been proposed (Resnik, 1999; Munteanu & Marcu, 
2005; Wu & Fung, 2005).  
In addition to the parallel corpora, we will need 
large monolingual corpora in the future (at least 
200 million words) for each of the six languages. 
Here, we intend to use newspaper corpora 
supplemented with text collections downloadable 
from the web.  
The corpora are stored in a database that allows 
for assigning analyses of different depth and nature 
to the sentences and for alignment between the 
sentences and their analyses. The architecture of 
this database and the corresponding analysis and 
evaluation frontend is described in (Eberle et al
2010, 2012). Section Results contains examples of 
such representations. 
Step 2: Compile a basic dictionary for the most 
frequent words 
A prerequisite of the suggested hybrid approach 
with rule-based kernel is to define morphological 
classifications for the new language(s). This is 
done exploiting similarities to the classifications as 
available for the existing languages. Currently, this 
has been carried out for Dutch (on the basis of 
German) and for Spanish (on the basis of 
French/other Romance languages). The most 
frequent words (the basic vocabulary of a 
language) are typically also the most ambiguous 
ones. Since the Lingenio systems are lexically 
driven transfer systems (cf. Eberle 2001), we 
define (a) structural conditions,  which inform the 
choice of the possible target words (single words 
or multiword expressions) and (b)restructuring 
conditions, as necessary (cf. Fig 1 a:  attributes 
'transfer conditions' and 'structural change'). In 
order to ensure quality this must be done by human 
lexicographers and therefore costly for a large 
dictionary. However, we manually create only very 
small basic dictionaries and extend these (semi-
automatically) step 3 and those which follow. 
Some important morphosyntactic features of the 
language are derived from a monolingual corpus 
annotated with publicly available part-of-speech 
taggers and lemmatisers. However, these tools 
often do not explicitly represent linguistic features 
needed for the generation stage in RBMT. In 
(Babych et al, 2012) we propose a systematic 
approach to recovering such missing generation-
oriented representations from grammar models and 
statistical combinatorial properties of annotated 
features. 
Step 3: Generating dictionary extensions from 
parallel corpora 
Based on parallel corpora, dictionaries can be 
derived using established techniques of automatic 
sentence alignment and word alignment. For 
sentence alignment, the length-based Gale & 
Church aligner (1993) can be used, or ? 
alternatively ? Dan Melamed?s GSA-algorithm 
(Geometric Sentence Alignment; Melamed, 1999).  
For segmentation of text we use corresponding 
Lingenio-tools (unpublished).2 
For word alignment Giza++ (Och & Ney, 2003) is 
the standard tool. Given a word alignment, the 
extraction of a (SMT) dictionary is relatively 
straightforward. With the exception of sentence 
segmentation, these algorithms are largely 
language independent and can be used for all of the 
languages that we consider. We did this for a 
number of language pairs on the basis of the 
                                                           
2  If these cannot be applied because of  lack of 
information about a language, we intend to use the 
algorithm by Kiss & Strunk (2006). An open-source 
implementation of parts of the Kiss & Strunk algorithm 
is available from Patrick Tschorn at 
http://www.denkselbst.de/sentrick/index.html. 
104
Europarl-texts considered (as stored in our 
database). In order to optimize the results we use 
the dictionaries of step 1 as set of cognates (cf. 
Simard at al 1992, Gough & Way 2004), as well as 
other words easily obtainable from the internet that 
can be used for this purpose (like company names 
and other named entities with cross-language 
identity and terminology translations). Using the 
morphology component of the new language and 
the categorial information from the transfer 
relation, we compute the basic forms of the 
inflected words found. Later, we intend to further 
improve the accuracy of word alignment by 
exploiting chunk type syntactic information of the 
narrow context of the words (cf. Eberle & Rapp 
2008). An early stage variant of this is already used 
in Lingenio products. The corresponding function 
AutoLearn<word> extracts new word relations on 
the basis of existing dictionaries and (partial) 
syntactic analyses. (Fig 2 gives an example). 
 
 
 
 
 
 
 
 
 
 
Fig 2 AutoLearn<word>: new entries using 
transfer links and syntactic analysis 
 
Given the relatively small size of the available 
parallel corpora, we expect that the automatically 
generated dictionaries will comprise about 20,000 
entries each (This corresponds to first results on 
the basis of German?English). This is far too 
small for a serious general purpose MT system. 
Note that, in comparison, the English?German 
dictionary used in the current Lingenio MT 
product comprises more than 480,000 keywords 
and phrases. 
Step 4: Expanding dictionaries using 
comparable corpora (word equations) 
In order to expand the dictionaries using a set of 
monolingual comparable corpora, the basic 
approach pioneered by Fung & McKeown (1997) 
and Rapp (1995, 1999) is to be further developed 
and refined in the second phase of the project as to 
obtain a practical tool that can be used in an 
industrial context. 
The basic assumption underlying the approach 
is that across languages there is a correlation 
between the co-occurrences of words that are 
translations of each other. If ? for example ? in a 
text of one language two words A and B co-occur 
more often than expected by chance, then in a text 
of another language those words that are 
translations of A and B should also co-occur more 
frequently than expected. It is further assumed that 
a small dictionary (as generated in step 2) is 
available at the beginning, and that the aim is to 
expand this basic lexicon. Using a corpus of the 
target language, first a co-occurrence matrix is 
computed whose rows are all word types occurring 
in the corpus and whose columns are all target 
words appearing in the basic lexicon. Next a word 
of the source language is considered whose 
translation is to be determined. Using the source-
language corpus, a co-occurrence vector for this 
word is computed. Then all known words in this 
vector are translated to the target language. As the 
basic lexicon is small, only some of the 
translations are known. All unknown words are 
discarded from the vector and the vector positions 
are sorted in order to match the vectors of the 
target-language matrix. Using standard measures 
for vector similarity, the resulting vector is 
compared to all vectors in the co-occurrence 
matrix of the target language. The vector with the 
highest similarity is considered to be the 
translation of our source-language word. 
From a previous pilot study (Rapp, 1999) it can 
be expected that this methodology achieves an 
accuracy in the order of 70%, which means that 
only a relatively modest amount of manual post-
editing is required.  
The automatically generated results are 
improved and the amount of post-editing is 
reduced by exploiting sense (disambiguation) 
information as available from the analysis 
component for the 'known' language of the new 
language pair.. Also we try to exploit categorial 
and underspecified syntactic information of the 
contexts of the words similar to what has been 
suggested for improving word alignment in the 
previous step (see also Fig.2). Also, as the frequent 
words are already covered by the basic lexicon 
(whose production from parallel corpora on the 
basis of a manually compiled kernel does not show 
 
105
an ambiguity problem of similar significance), and 
as experience shows that most low frequency 
words in a full-size lexicon tend to be 
unambiguous, the ambiguity problem is reduced 
further for the words investigated and extracted by 
this comparison method. 
Step 5: Expanding dictionaries using 
comparable corpora (multiword units) 
In order to account for technical terms, idioms, 
collocations, and typical short phrases, an 
important feature of an MT lexicon is a high 
coverage of multiword units. Very recent work 
conducted at the University of Leeds (Sharoff et 
al., 2006) shows that dictionary entries for such 
multiword units can be derived from comparable 
corpora if a dictionary of single words is available. 
It could even be shown that this methodology can 
be superior to deriving multiword-units from 
parallel corpora (Babych et al, 2007). This is a 
major breakthrough as comparable corpora are far 
easier to acquire than parallel corpora. It even 
opens up the possibility of building domain-
specific dictionaries by using texts from different 
domains. 
The outline of the algorithm is as follows: 
? Extract collocations from a corpus of the 
source language (Smadja, 1993) 
? To translate a collocation, look up all its 
words using any dictionary 
? Generate all possible permutations 
(sequences) of the word translations 
? Count the occurrence frequencies of these 
sequences in a corpus of the target 
language and test for significance 
? Consider the most significant sequence to 
be the translation of the source language 
collocation 
Of course, in later steps of the project, we will 
experiment on filtering these sequences by 
exploiting structural knowledge similarly to what 
was described in the two previous steps. This can 
be obtained on the basis of the declarative analysis 
component of the new language which is 
developed in parallel. 
Step 6: Cross-validate dictionaries 
The combination of the corpus-based methods for 
automatic dictionary generation as described in 
steps 3 to 5 will lead to high coverage dictionaries 
as the availability of very large monolingual 
corpora is no major problem for our languages. 
However, as all steps are error prone, it can be 
expected that a considerable number of dictionary 
entries (e.g. 50%) are not correct. To facilitate (but 
not eliminate) the manual verification of the 
dictionary, we will  perform an automatic cross-
check which utilizes the dictionaries? property of 
transitivity. What we mean by this is that if we 
have two dictionaries, one translating from 
language A to language B, the other from language 
B to language C, then we can also translate from 
language A to C by use of the intermediate 
language (or interlingua) B. That is, the property of 
transitivity, although having some limitations due 
to ambiguity problems, can be exploited to 
automatically generate a raw dictionary for A to C. 
Lingenio  has some experience with this method 
having exploited it for extending and improving its 
English ? French dictionaries using French ? 
German and German ? English. 
As the corpus-based approach (steps 3 to 5) 
allows us to also generate this type of dictionary  
via comparable corpora, we have two different 
ways to generate a dictionary for a particular 
language pair. This means that we can validate one 
with the other. Furthermore, with increasing 
number of language pairs created, there are more 
and more languages that can serve as interlingua or 
'pivot': This, step by step, gives an increasing 
potential for mutual cross-validation.  
Specific attention will be paid to automating as 
far as possible the creation of selectional 
restrictions to be assigned to the transfer relations 
of the new dictionaries in all steps of dictionary 
creation (2?6). We will try to do this on the basis 
of the analysis components as available for the 
languages considered: These are: a completely 
worked out analysis component for the 'old' 
language, a declarative (chunk parsing) component 
for the new one (compare the two following steps 
for this).  
Step 7: Integrate dictionaries in existing 
machine translation systems 
Lingenio has a relatively rich infrastructure for 
automatic importation of various kinds of lexical 
information into the database used by the analyses 
and translation systems. If necessary the 
information on hand (for instance from 
conventional dictionaries of publishing houses) is 
106
completed and normalized during or before 
importation. This may be executed completely 
automatically ? by using the existing analyses 
components and resources respectively as 
databases ? or interactively ? by asking the 
lexicographer for additional information, if needed. 
For example, there may be a list of multiword 
expressions to be imported into the database. In 
order to have available correct syntactic and 
semantic information for these expressions, they 
are analysed by the parser of the corresponding 
language. From the analysis found, the information 
necessary to describe the new lemma in the lexicon 
with respect to semantic type and syntactic 
structure is obtained. The same information is used 
to automatically create correct restructuring 
constraints for translation relations which use the 
new lemma as target. If the parser does not find a 
sound syntactic description, for example because 
some basic information or the expression is 
missing in the lexical database, the lexicographer is 
asked for the missing information or is handed 
over the expression to code it manually.  
Using these tools importation of new lexical 
information, as provided in the previous steps, is 
considerably accelerated.  
Step 8: Compile rule bases for new language 
pairs 
Although experience clearly shows that 
construction and maintenance of the dictionaries is 
by far the most expensive task in (rule-based) 
Machine Translation, the grammars (analysis and 
generation) must of course be developed and 
maintained also. Lingenio has longstanding 
experience with the development of grammars, 
dictionaries and all other components of RBMT.  
The used grammar formalism (slot grammar, 
cf. McCord 1991) is unification based and its 
structuring focuses on dependency, where phrases 
are analysed into heads and grammatical roles ? so 
called (complement and adjunct) slots.  
The grammar formalism and basic rule types 
are designed in a very general way in order to 
allow good portability from one language to 
another such that spelling out the declarative part 
of a grammar does not take very much time (2-4 
person months approx. for relatively similar 
languages like Romance languages according to 
our experience). The portation of linguistic rules to 
new languages is also facilitated by the modular 
design with clearly defined interfaces that make it 
relatively straightforward to integrate information 
from corpora. 
Given a parallel corpus as acquired in step 1, 
the following procedure defines grammar develop-
ment:  
 
1. Define a declarative grammar for the new 
language and train this grammar on the parallel 
-corpus according to the following steps: 
2. Use a chunk parser for the grammar on the 
basis of an efficient part-of-speech tagger for 
the new language.  
3. Combine the chunk analyses of the sentence, 
according to suggestions for packed syntactic 
structures (cf. Schiehlen 2001 and others) and 
underspecified representation structures 
respectively (cf. Eberle, 2004, and others), 
such that the result represents a disjunction of 
the possible analyses of the sentence. 
4. Filter the alternatives of the representation by 
using mapping constraints between source and 
target sentence as can be computed from the 
lexical transfer relations and the structural 
analysis of the sentence. For instance, if we 
know, as in the example of the last section, that 
in the source sentence there is a relative clause 
with lexical elements A, B, . . . modifying a 
head H and that there are translations TH, TA, 
TB, . . . of H, A, B,. . . , in the target sentence 
which, among other possibilities, can be 
supposed to stand in a similar structural 
relation there, then we prefer this relation to 
the competing structural possibilities. (Fig. 3 in 
section results shows the corresponding 
selection for a German-Spanish example in the 
project database). 
5. For each of the remaining structural 
possibilities of the thus revised underspecified 
representation, take its lexical material and 
underspecified structuring as a context for its 
successful firing. For instance, if the 
possibility is left that O is the direct object of 
VP, where VP is an underspecified verbal 
phrase and O an underspecified nominal 
phrase (i.e. where details of the substructuring 
are not spelled out), take the sentence as a 
reference for direct object complementation 
and O and VP as contexts which accept this 
complementation. 
107
6. Develop more abstract conditions from the 
conditions learned according to (5) and 
integrate the different cases. 
7. Tune the results using standard methods of 
corpus-based linguistics. Among other things 
this means: Distinguish between training and 
test corpora, adjust weights according to the 
results of test runs, etc. 
 
The basic idea of the proposed learning procedure 
is similar to that used with respect to learning 
lexical transfer relations: Do not define the 
statistical model for the ?ignorant? state, where the 
surface items of the bilingual corpora are 
considered. Instead, define it for appropriate 
maximally abstract analyses of the sentences 
(which, of course, must be available 
automatically), because, then, much smaller sets of 
data will do. Here, the important question is: What 
is the most abstract level of representation that can 
be reached automatically and which shows reliable 
results? We think that it is the level of 
underspecified syntactic description as used in the 
procedure above. 
The result of training the grammar is a set of 
rules which assign weights and contexts to each 
filler rule of the declarative grammar and thus 
allow to estimate how likely it is that a particular 
rule is applied in a particular context in comparison 
with other rules (Fig. 4 and 5 in section results 
give an overview of the relevance of  grammar 
rules and their triggering conditions w.r.t. 
German).  
We mentioned that the task of translating texts 
into each other does not presuppose that each 
ambiguity in a source sentence is resolved. On the 
contrary, translation should be ambiguity 
preserving (cf. Kay, Gawron & Norvig 1994, 
compare the example above). It is obvious that 
underspecified syntactic representations as 
suggested here are also especially suited for 
preserving ambiguities appropriately.  
Step 9: Automatically evaluate translations of 
the most frequent grammatical constructions 
and multiword expressions in a machine-
translated corpus 
In a later work package of the project, we will run 
a large parallel corpus through available 
(competitive) MT engines, which will be enhanced 
by automatic dictionaries developed during the 
previous stages. On the source-language side of the 
corpus we will automatically generate lists of 
frequent multiword expressions (MWEs) and 
grammatical constructions using the methodology 
proposed in (Sharoff et al, 2006). For each of the 
identified MWEs and constructions we will 
generate a parallel concordance using open-source 
CSAR architecture developed by the Leeds team 
(Sharoff, 2006). The concordance will be 
generated by running queries to the sentence-
aligned parallel corpora and will return lists of 
corresponding sentences from gold-standard 
human translations and corresponding sentences 
generated by MT. Each of these concordances will 
be automatically evaluated using standard MT 
evaluation metrics, such as BLEU. Under these 
settings parallel concordances will be used as 
standard MT evaluation corpora in an automated 
MT evaluation scenario. 
Normally BLEU gives reliable results for MT 
corpora over 7000 words. However, in (Babych 
and Hartley, 2009; Babych and Hartley, 2008) we 
demonstrated that if the corpus is constructed in 
this controlled way, where evaluated fragments of 
sentences are selected as local contexts for specific 
multiword expressions or grammatical 
constructions, then BLEU scores have another 
?island of stability? for much smaller corpora, 
which now may consist of only five or more 
aligned concordance lines. This concordance-based 
evaluation scenario gives correct predictions of 
translation quality for the local context of each of 
the evaluated expressions. 
The scores for the evaluated MWEs and 
constructions will be put in a risk-assessment 
framework, where we will balance the frequency 
of constructions and their translation quality. The 
top priority receive the most frequent expressions 
that are the most problematic ones for a particular 
MT engine, i.e., with queries with lowest BLEU 
scores for their concordances. This framework will 
allow MT developers to work down the priority list 
and correct or extend coverage for those 
constructions which will have the biggest impact 
on MT quality. 
Step 10: Extend support for high-priority 
constructions semi-automatically by mining 
correct translations from parallel corpora 
At this stage we will automate the procedure of 
correcting errors and extending coverage for 
108
problematic MWEs and grammatical 
constructions, identified in Step 9. For this we will 
exploit alignment between source-language 
sentences and gold-standard human translations. In 
the target human translations we will identify 
linguistically-motivated multiword expressions, 
e.g., using part-of-speech patterns or tf-idf 
distribution templates (Babych et al, 2007) and 
run standard alignment tools (e.g., GIZA++) for 
finding the most probable candidate MWEs that 
correspond to the problematic source-language 
expressions. Source and target MWEs paired in 
this way will form the basis for automatically-
generated grammar rules. The rules will normally 
generalise several pairs of MWEs, and may be 
underspecified for certain lexical or morphological 
features. Later such rules will be manually checked 
and corrected by language specialists in MT 
development teams that work on specific 
translation directions. 
This procedure will allow to speed up the grammar 
development procedure for large-scale MT projects 
and will focus on grammatical constructions with 
the highest impact on MT quality, establishing 
them as a top priority for MT developers. In 
HyghTra and with respect to the languages 
considered there, this procedure will be integrated 
into the grammar development and optimization of 
step 8, in particular it will be related to step 4 of 
the procedure sketched there. With regard to 
integration, we aim at an interleaved architecture in 
the long run.  
Step 11: Bootstrap the system 
In Step 11, the new grammar and the transfer of 
the new MT system and the new dictionary may be 
mutually trained further using the steps before and 
applying the system to additional corpora. 
 
4 Results 
Declarative slot grammars for Dutch and Spanish 
have been developed using the patterns of German 
and French ? where declarative  means that there 
has been used no relevant semantic or other 
information in order to spell out weighting or 
filters for rule application -- the only constraint 
being morphosyntactic accessibility. The necessary 
morphological information has been adapted 
similarly from the corresponding model languages. 
The basic dictionaries have been compiled 
manually (Dutch) or extracted from a conventional 
electronic dictionary (translateDict Spanish).  
For a subset of the Spanish corpus (reference 
sentences of the grammar, parts of the open source 
Leeds corpus (Sharoff, 2006), and Europarl), 
syntactic analyses have been computed and stored 
in the database. As the number of analyses grows 
extremely with the length of sentences, only 
relatively short sentences (up to 15 words)  have 
been considered. These analyses are currently 
compared to the analyses of the German 
translations of the corresponding sentences (one 
translation per sentence), which are taken as a kind 
of 'gold' standard as the German analysis 
component (as part of the translation products) has 
proven to be sufficiently reliable. On the basis of 
the comparison a preference on the competitive 
analyses of the Spanish sentence is entailed and 
used for defining a statistical evaluation 
component for the Spanish grammar. Fig.3 shows 
the corresponding representations in the database 
for the sentence Aumenta la demana de energ?a 
el?ctrica por la ola de calor3  and its translation die 
Nachfrage nach Strom steigt wegen der 
Hitzewelle/the demand for electricity increases 
because of the heat-wave. 
 
 
 
 
 
 
 
 
 
 
Fig.3 Selection of analyses via correspondences 
(prefer first Spanish analysis because of subj-congruity) 
 
The analyses are associated with the corresponding 
creation protocols, which are structured lists whose 
items describe, via the identifiers, which rule has 
been applied when and to what structures in the 
process of creating the analysis. From the selection 
of a best analysis for a sentence, we can entail the 
circumstances under which the application of 
particular rules are preferred. This has been carried 
                                                           
3 Sentence taken from the online newspaper El D?a de 
Concepci?n del Uruguay 
 
 
109
out - not yet for the 'new' language Spanish, but for 
the 'known' language German, in order to obtain a 
measure about how correctly the existing grammar 
evaluation component can be replaced by the 
results of the corresponding statistical study.  
 
Fig.4  Frequency of applications of rules 
 
 
 cluster 
applications 
similarity feas  mod feas head 
383, 384,.. 0,86 sent, ... emosentaffv,.. 
557,558,566,.. 0,68 denselb,.. gebv, ... 
 
Fig.5  Preliminary constraints related to grammar 
rule clusters 
 
Fig.4 shows the distribution of rule usages within 
the training set of analyses (of approx.30.000 
sentences). 390 different rules were used with a 
total of 133708 rule applications. The subject rule 
(383) and the noun determiner rule (46) the most 
used rules (35% of all applications). Fig 5. 
illustrates the preliminary results of a clustering 
algorithm where different rule applications are 
grouped into clusters and the key features of the 
head and modifier phrases for each cluster are 
extracted. 
Currently, we try to determine further and tare 
the linguistic features and the weighting which 
models best the evaluation for German. (The gold 
standard that is used in this test is the set of 
analyses mentioned above). The investigations are 
not yet completed, but preliminary results on the 
basis of the morphosyntactic and semantic 
properties of the neighboring elements are 
promising. After consolidation, the findings will be 
transferred to Spanish on the basis of the selection 
procedure illustrated in Fig. 3. The next step of 
grammar training in the immediate future will 
consist of  changing the focus to underspecified 
analyses as described in step 8 
5 Conclusions 
The project tries to make state-of-the-art statistical 
methods available for dictionary development and 
grammar development for a rule-based dominated 
industrial setting and to exploit such methods 
there.  
With regard to SMT dictionary creation, it goes 
beyond the current state of the art as it also aims at 
developing and applying algorithms for the semi-
automatic generation of bilingual dictionaries from 
unrelated monolingual (i.e., comparable) corpora 
of the source and the target language, instead of 
using relatively literally translated (i.e., parallel) 
texts only. Comparable corpora are far easier to 
obtain than parallel corpora. Therefore the 
approach offers a solution to the serious data 
acquisition bottleneck in SMT. This approach is 
also more cognitively plausible than previous 
suggestions on this topic, since human bilinguality 
is normally not based on memorizing parallel texts. 
Our suggestion models human capacity to translate 
texts using linguistic knowledge acquired from 
monolingual data, so it also exemplifies many 
more features of a truly self-learning MT system 
(shared also by a human translator).  
In addition, the proposal suggests a new 
method for spelling out grammars and parsers for 
languages by splitting grammars into declarative 
kernels and trainable decision algorithms and by 
exploiting cross-linguistic knowledge for 
optimizing the results of the corresponding parsers.   
For developing different components and 
dictionaries for the system a bootstrapping 
architecture is suggested that uses the acquired 
lexical information for training the grammar of the 
new language, which in turn uses the 
(underspecified) parser results for optimizing the 
lexical information in the corresponding translation 
dictionaries. We expect that the suggested methods 
significantly improve translation quality and 
reduce the costs of creating new language pairs for 
Machine Translation. The preliminary results 
obtained so far in the project appear promising. 
6 Acknowledgments 
This research is supported by a Marie Curie IAPP 
project taking place within the 7th European 
Community Framework Programme (Grant 
agreement no.: 251534) 
110
7 References 
Armstrong, S.; Kempen, M.; McKelvie, D.; Petitpierre, D.; 
Rapp, R.; Thompson, H. (1998). Multilingual Corpora 
for Cooperation. Proceedings of the 1st International 
Conference on Linguistic Resources and Evaluation 
(LREC), Granada, Vol. 2, 975?980. 
Babych, B., Hartley, A., Sharoff S.; Mudraya, O. (2007). 
Assisting Translators in Indirect Lexical Transfer. 
Proceedings of the 45th Annual Meeting of the ACL.  
Babych, B., Anthony Hartley, & Serge Sharoff (2007b) 
Translating from under-resourced languages: 
comparing direct transfer against pivot translation. 
Proceedings of MT Summit XI, 10-14 September 
2007, Copenhagen, Denmark, 29-35 
Babych, B. & Hartley, A. (2008). Automated MT Evaluation 
for Error Analysis: Automatic Discovery of Potential 
Translation Errors for Multiword Expressions. ELRA 
Workshop on Evaluation ?Looking into the Future of 
Evaluation: When automatic metrics meet task-based  
and performance-based approaches?. Marrakech, 
Morocco 27 May 2008. Proceedings of LREC?08. 
Babych, B. and Hartley, A. (2009). Automated error analysis 
for multiword expressions: using BLEU-type scores 
for automatic discovery of potential translation errors. 
Linguistica Antverpiensia, New Series (8/2009): 
Journal of translation and interpreting studies. Special 
Issue on Evaluation of Translation Technology. 
Babych, B., Babych, S. and Eberle, K. (2012). Deriving 
generation-oriented MT resources from corpora: case 
study and evaluation of de/het classification for Dutch 
Noun (in preparation) 
Baroni, M.; Bernardini, S. (2004). BootCaT: Bootstrapping 
corpora and terms from the web. Proceedings of 
LREC 2004.  
Callison-Burch, C., Miles Osborne, & Philipp Koehn: Re-
evaluating the role of BLEU in machine translation 
research. EACL-2006: 11th Conference of the 
European Chapter of the Association for 
Computational Linguistics, Trento, Italy, April 3-7, 
2006; pp.249-256  
Charniak, E.; Knight, K.; Yamada, K. (2003). Syntax-based 
language models for statistical machine translation". 
Proceedings of MT Summit IX. 
Eberle, Kurt (2001). FUDR-based MT, head switching and the 
lexicon. Proceedings of the the eighth Machine 
Translation Summit, Santiage de Compostela.  
Eberle, Kurt (2004). Flat underspecified representation and its 
meaning for a fragment of German. 
Habilitationsschrift, Universit?t Stuttgart. 
Eberle, K.; Rapp, R. (2008). Rapid Construction of 
Explicative Dictionaries Using Hybrid Machine 
Translation. In: Storrer, A.;  Geyken, A.; Siebert, A.; 
W?rzner, K._M (eds.) Text Resources and Lexical 
Knowledge: Selected Papers from the 9th Conference 
on Natural Language Processing KONVENS 2008. 
Berlin: Mouton de Gruyter..  
Eckart,K., Eberle, K.; Heid, U. (2010) An infrastructure for 
more reliable corpus analysis. Proceedings of the 
Workshop on Web Services and Processing Pipelines 
in HLT of LREC-2010 , Valetta. 
Eberle, K.; Eckart,K., Heid, U.,Haselbach, B. (2012) A 
tool/database interface for multi-level analyses. 
Proceedings of LREC-2012 , Istanbul. 
Frederking, R.; Nirenburg, S.; Farwell, D.;  Helmreich, S.; 
Hovy, E.; Knight, K.; Beale, S.; Domashnev, C.; 
Attardo, D.; Grannes, D.; Brown, R. (1994). Integrated 
Translation from Multiple Sources within the Pangloss 
MARK II Machine Translation System. Proceedings 
of Machine Translation of the Americas, 73?80. 
Frederking, Robert and Sergei Nirenburg (1994). Three heads 
are better than one. In: Proceedings of ANLP-94, 
Stuttgart, Germany.  
Fung, P.; McKeown, K. (1997). Finding terminology 
translations from non-parallel corpora. Proceedings of 
the 5th Annual Workshop on Very Large Corpora, 
Hong Kong: August 1997, 192-202.  
Gale, W.A.; Church, K.W. (1993). A progrm for aligning 
sentences in bilingual corpora. Computational 
Linguistics, 19(1), 75?102. 
Gonz?lez, J.; Antonio L. Lagarda, Jos? R. Navarro, Laura 
Eliodoro, Adri? Gim?nez, Francisco Casacuberta, Joan 
M. de Val and Ferran Fabregat (2004). SisHiTra: A 
Spanish-to-Catalan hybrid machine translation system. 
Berlin: Springer LNCS. 
Gough, N., Way, A. (2004). Example-Based Controlled 
Translation. Proceedings of the Ninth Workshop of the 
European Association for Machine Translation, 
Valetta, Malta.  
Groves, D. & Way, A. (2006b). Hybridity in MT: Experiments 
on the Europarl Corpus. In Proceedings of the 11th 
Conference of the European Association for Machine 
Translation, Oslo, Norway, 115?124. 
Groves, D.; Way, A. (2006a). Hybrid data-driven models of 
machine translation. Machine Translation, 19(3?4). 
Special Issue on Example-Based Machine Translation. 
301?323. 
Habash, N.; Dorr, B. (2002). Handling translation 
divergences: Combining statistical and symbolic 
techniques in generation-heavy machine translation. 
Proceedings of AMTA-2002, Tiburon, California, 
USA. 
Kiss, T.; Strunk, J. (2006): Unsupervised multilingual 
sentence boundary detection. Computational 
Linguistics 32(4), 485?525. 
Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical 
Machine Translation. Proceedings of MT Summit X, 
Phuket, Thailand 
Koehn, P.; Knight, K. (2002). Learning a translation lexicon 
from monolingual corpora. In: Proceedings of ACL-02 
Workshop on Unsupervised Lexical Acquisition, 
Philadelphia PA. 
Language Industry Monitor (1992). Statistical methods 
gaining ground. In: Language Industry Monitor, 
September/October 1992 issue. 
111
McCord, M. (1989). A new version of the machine translation 
system LMT.  Journal of Literary and Linguistic 
Computing, 4, 218?299. 
McCord, M. (1991). The slot grammar system.  In: Wedekind, 
J., Rohrer, C.(eds): Unification in Grammar, MIT-
Press. 
Melamed, I. Dan (1999). Bitext maps and aligment via pattern 
recognition. Computational Linguistics, 25(1), 107?
130. 
Munteanu, D.S.; Marcu, D. (2005). Improving machine 
translation performance by exploiting non-parallel 
corpora. Computational Linguistics, 31(4), 477?504. 
Och, F.J.; Ney, H. (2002). Discriminative trainig and 
maximum entropy models for statistical machine 
translation. Proceedings of the  Annual Meeting of the 
Association for Computational Linguistics, 
Philadelphia, PA, 295?302.  
Och, F.J.; Ney, H. (2003). A systematic comparison of various 
statistical alignment models. Computational 
Linguistics, 29(1), 19?51. 
Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. (2002). BLEU: A 
method for automatic evaluation of machine 
translation. In: Proceedings of the 40th Annual 
Meeting of the ACL, Philadelphia, PA, 311?318. 
Rapp, R. (1995). Identifying word translations in non-parallel 
texts. In: Proceedings of the 33rd Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, 1995, 320?322 
Rapp, R. (1999). Automatic identification of word translations 
from unrelated English and German corpora. In: 
Proceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics 1999, College 
Park, Maryland. 519?526. 
Rapp, R. (2004). A freely available automatically generated 
thesaurus of related words. In: Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation (LREC), Lisbon, Vol. II, 
395?398. 
Rapp, R.; Martin Vide, C. (2007). Statistical machine 
translation without parallel corpora. In: Georg Rehm, 
Andreas Witt, Lothar Lemnitzer (eds.): Data 
Structures for Linguistic Resources and Applications. 
Proceedings of the Biennial GLDV Conference 2007. 
T?bingen: Gunter Narr. 231?240 
Resnik, R. (1999). Mining the web for bilingual text. 
Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics. 
Sato, S.; Nagao, M. (1990). Toward memory-based 
translation. Proceedings of COLING 1990, 247?252. 
Schiehlen, M. (2001) Syntactic Underspecification. In: Special 
Research Area 340 ? Final report, University of 
Stuttgart.  
Sharoff, S. (2006) Open-source corpora: using the net to fish 
for linguistic data. In International Journal of Corpus 
Linguistics 11(4), 435?462.  
Sharoff, S.; Babych, B.; Hartley, A. (2006). Using comparable 
corpora to solve problems difficult for human 
translators. In: Proceedings of COLING/ACL 2006, 
739?746.  
Sharoff, S. (2006). A uniform interface to large-scale 
linguistic resources. In Proceedings of the Fifth 
Language Resources and Evaluation Conference, 
LREC-2006, Genoa. 
Simard, M., Foster, G., Isabelle, P. (1992). Using Cognates to 
Align Sentences in Bilingual Corpora. Proceeedings of 
the International Conference on Theoretical and 
Methodological Issues, Montr?al. 
Smadja, F. (1993). Retrieving collocations from text: Xtract. 
Computational Linguistics, 19(1), 143?177. 
Streiter, O., Carl, M., Haller, J. (eds)(1999). Hybrid 
Approaches to Machine Translation. IAI working 
papers 36. 
Streiter, O.; Carl, M.; Iomdin, L.L.: 2000, A Virtual 
Translation Machine for Hybrid Machine Translation'. 
In: Proceedings of the Dialogue'2000 International 
Seminar in Computational Linguistics and 
Applications. Tarusa, Russia.  
Streiter, O.; Iomdin, L.L. (2000). Learning Lessons from 
Bilingual Corpora: Benefits for Machine Translation. 
International Journal of Corpus Linguistics, 5(2), 199?
230. 
Thurmair, G. (2005). Hybrid architectures for machine 
translation systems. Language Resources and 
Evaluation, 39 (1), 91?108. 
Thurmair, G. (2006). Using corpus information to improve 
MT quality. Proceedings of the LR4Trans-III 
Workshop, LREC, Genova. 
Thurmair, G. (2007) Automatic evaluation in MT system 
production. MT Summit XI Workshop: Automatic 
procedures in MT evaluation, 11 September 2007, 
Copenhagen, Denmark, 
Veronis, Jean (2006). Technologies du Langue. Actualit?s ? 
Comentaires ? R?flexions. Translation. Systran or 
Reverso? 
http://aixtal.blogspot.com/2006/01/translation-systran-
or-reverso.html  
Wu, D., Fung, P. (2005). Inversion transduction grammar 
constraints for mining parallel sentences from quasi-
comparable corpora. Second International Joint 
Conference on Natural Language Processing 
(IJCNLP-2005). Jeju, Korea. 
 
112
LAW VIII - The 8th Linguistic Annotation Workshop, pages 82?86,
Dublin, Ireland, August 23-24 2014.
Multiple views as aid to linguistic annotation error analysis
Marilena Di Bari
University of Leeds
mlmdb@leeds.ac.uk
Serge Sharoff
University of Leeds
s.sharoff@leeds.ac.uk
Martin Thomas
University of Leeds
m.thomas@leeds.ac.uk
Abstract
This paper describes a methodology for supporting the task of annotating sentiment in natural
language by detecting borderline cases and inconsistencies. Inspired by the co-training strategy,
a number of machine learning models are trained on different views of the same data. The predic-
tions obtained by these models are then automatically compared in order to bring to light highly
uncertain annotations and systematic mistakes. We tested the methodology against an English
corpus annotated according to a fine-grained sentiment analysis annotation schema (SentiML).
We detected that 153 instances (35%) classified differently from the gold standard were accept-
able and further 69 instances (16%) suggested that the gold standard should have been improved.
1 Introduction
This work pertains to the phase of testing the reliability of human annotation. The strength of our
approach relies on the fact that we use multiple supervised machine learning classifiers and analyse their
predictions in parallel to automatically identify disagreements. Those, in fact, ultimately lead to the
discovery of borderline cases in the annotation, an expensive task in terms of time when carried out
manually.
Predictions with a number of different labels are manually analysed, since they may indicate inconsis-
tencies in the annotation and cases difficult to annotate. Conversely, cases with high agreement suggest
that the annotation schema is reliable. On the one hand, the analysis of those disagreements, in conjunc-
tion with the gold annotations, provides fresh insights about the efficacy of the features provided to the
classifiers for the learning phase. On the other hand, when all the classifiers agree on a wrong annotation,
it is a strong signal of ambiguity in the annotation schema and/or guidelines.
In Section 2 we briefly introduce the data to which we apply the methodology described in Section 3.
In Section 4 we report results. In Section 5 we mention studies related to ours and in Section 6 we draw
conclusions and identify steps for future work.
2 Data
We tested our methodology on the SentiML corpus (Di Bari et al., 2013) for which the annotation
guidelines, as well as the original and annotated texts, are publicly available
1
. The corpus consists of
307 English sentences (6987 tokens), taken from political speeches, TED talks (Cettolo et al., 2012), and
news items from the MPQA opinion corpus (Wilson, 2008).
The aim of its annotation is to encapsulate opinions in pairs, by marking the role that each word takes
(modifier or target). For example, in
?More of you have lost your homes and even more are watching your home values plummet?
there would be two pairs: modifier ?lost? and target ?homes?, and modifier ?values? and target ?plum-
met?. Such two pairs are called appraisal groups.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://corpus.leeds.ac.uk/marilena/SentiML
82
Figure 1: Example of dependency tree. Dependency trees provide features for the machine learning step.
For each of these elements several features are annotated that are believed to improve the task of
sentiment analysis. The study presented here relates to the automatic identification of modifiers and
targets.
3 Methodology
To test our methodology we selected a corpus for which various types of linguistic information related
to appraisal groups were annotated. We started with the identification of modifiers and targets, since this
represents the base of all the other levels of annotation.
To test the reliability of annotation we set 10% of our annotated corpus aside, and performed the
machine learning part of the study on the remaining 90% of our corpus.
The first step consists of preparing the features for the machine learning phase. The optimal set to
model the annotation task varies from problem to problem. We used the following:
? Word features, representing the ordinal identifier, word form, lemma and POS tag of each word.
? Contextual features, representing the lemma and POS tags of the preceding and succeeding words.
? Dependency-based features, representing the reference to the word on which the current token de-
pends in the dependency tree (head) along with its lemma, POS tag and relation type (see Fig-
ure 1) (Nivre, 2005).
? Number of linked modifiers, representing the number of adjectives and adverbs linked to the current
word in the dependency tree.
? Role, representing the predicted role (modifier or target) of the current token in conveying sentiment.
The predictions are computed using fixed syntactic rules.
? Gazetteer-based sentiment. We used the NRC Word-Emotion Association Lexicon (Mohammad,
2011) to represent the a-priori sentiment of each word, i.e. regardless of its context.
Once the features are ready, two or more feature partitions (called views in the co-training strategy)
have to be defined in order to be as orthogonal as possible (Abney, 2007). We opted for a linguistically-
grounded dichotomy: lexical features (word features, role and gazetteer-based sentiment) versus syntac-
tic features (contextual and dependency-based features, number of linked modifiers). The training and
test sets are split accordingly.
At this point, machine learning classifiers are chosen. These need to be confidence-rated, i.e. able to
provide a confidence rate for each prediction. In our experiments we selected Na??ve Bayes, Radial Basis
Function Network and Logistic Regression
2
. These models rely on very different strategies, which makes
the analysis more reliable. We discarded Support Vector Machines since in our preliminary experiments
they achieved high precision (a range between 0.60 and 0.77 across modifiers and targets), but very
low recall (a range between 0.05 and 0.06 across modifiers and targets), which resulted in a very low
F-measure (a range between 0.09 and 0.11 across modifiers and targets).
A model for each combination of view and classifier is then produced and tested on the test set. We
performed a 10-fold cross-validation. In the test phase, we opted for a numerical threshold of 0.67 to
consider the predictions reliable. A prediction with a confidence lower than the threshold is considered
uncertain.
For each instance we obtained six predictions, which potentially differ from one another. The agree-
ment score is calculated for each class in order to identify the most frequent prediction.
2
In each case we used the implementation provided by WEKA (http://www.cs.waikato.ac.nz/
?
ml/weka/).
83
Feature set Classifier
Modifier Target
Precision Recall F
?=1
Precision Recall F
?=1
Lexical
Na??ve Bayes 0.71 0.10 0.48 0.82 0.12 0.43
RBF Network 0.52 0.56 0.54 0.51 0.59 0.55
Logistic regression 0.59 0.42 0.49 0.61 0.48 0.54
Syntactic
Na??ve Bayes 0.46 0.48 0.47 0.82 0.12 0.43
RBF Network 0.49 0.35 0.40 0.55 0.50 0.53
Logistic regression 0.58 0.22 0.32 0.60 0.41 0.49
Table 1: Performance of the classifiers trained on two views, lexical and syntactic. Experiments have
been performed using 10-fold cross-validation.
At this point, only the predictions different from the gold annotations are considered: the higher the
agreement score, the more the instance is interesting in the context of our analysis.
The final step consists of manually investigating such cases to shed light on the errors. In this experi-
ment we opted for the use of a simple protocol based on the following classification schema:
? W (wrong), where the classifiers disagree with the gold annotation, which we judge to be correct.
? A (ambiguous), where the classifiers disagree with the gold annotation and we judge both to be
valid. In such cases, the guidelines need to be clearer or the annotation method could have been
simpler.
? M (to modify), where we judge that the gold annotation is incorrect.
This approach has the advantage of yielding a much reduced subset of instances to be examined man-
ually, with respect to the full set.
4 Results
Table 1 shows the performances of the six models obtained from the training of each combination of
view and classifier, mentioned in Section 3. F-measures for modifiers range between 0.32 and 0.54
for modifiers, and 0.43 and 0.55 for targets. Overall, the RBF Network trained on the lexical view
performs best. However, there is no huge difference in general in performances between the lexical and
the syntactic feature sets, which is good in the light of data sparseness.
Performance on the the empty class (no category assigned) was exceptionally good, as 76% was pre-
dicted out of the gold 77%, whereas the performance on the modifiers was 4% out of the gold 12% and
the performance on the targets was 5% out of the gold 11%. Although the annotation allows each token
to be simultaneously annotated as modifier and target, we have not reported the performances for the MT
class as the cases were not significant. Finally, there was a 15% of cases in which the classifiers were not
confident.
In relation to the manual classification of errors (see final paragraph of Section 3) we found that, out
of the total test instances (2066), in 436 cases the most predicted class differed from the gold standard:
the label W was assigned 214 times (49%), the label A was assigned 153 times (35%), the label M
was assigned 69 times (16%). W was mostly assigned when the modifier or the target was correctly
identified, but not its counterpart in the pair (e.g., ?way forward?, ?blame society?, ?wrong side?). It was
also assigned when a word was correctly identified as evoking sentiment (e.g., ?destroy?, ?flourish?),
but only the first of two or more targets was identified (e.g., ?women and children?, ?the city and the
country?).
A was assigned when an adverb was annotated as modifier (e.g., ?through corruption?, ?seize gladly?,
?tragically reminded?): these are cases in which human annotators decide to include the adverb if it is
regarded as important for the sentiment. Other cases in which the label has been used is with compound
modifiers (e.g., ?face to face?, ?in the face of?), phrasal verbs (e.g., ?turn back?, ?carried forth?, ?came
forth?) and difficult couples to link (e.g., ?instruments with which we meet them? [challenges]). Finally,
this label was also used in cases in which the prediction was sensible, but considered less accurate than
the gold one (e.g., in ?enjoy relative plenty?, the gold standard was ?enjoy plenty? and the classifiers
84
predicted ?relative plenty?).
M was assigned when another modifier had been wrongly annotated by the annotator, instead of mod-
ifying the value of the force of the current one (e.g., in ?much more?, only ?more?? should have been
annotated with high force), in the case of couples with no sentiment (e.g., ?future generations?, ?different
form?), of couples not previously identified (e.g., ?stairway filled with smoke?, ?icy river?) or couples
that could have been annotated in an easier way (e.g., ?provoke us to step up and do something?, ?image
resonates with us?).
5 Related work
Evaluating the reliability of human annotation is a challenging and widely studied task (Pustejovsky
and Stubbs, 2012). The standard solution is the measurement of an inter-annotator agreement (IAA)
coefficient according to a variety of formulae that depend on the characteristics of the annotation set-
ting (Artstein and Poesio, 2008).
For example, in the case of Wilson (2008) and Read and Carroll (2007), it was useful to understand
inconsistencies in the selection of the span for attitudes and targets. Since this represents only one of
the commonly recognized challenges, some studies have focused on practically testing a methodological
framework for schema development for fine-grained and quality semantic annotations. (Bayerl et al.,
2003).
Our approach varies from the standard procedure in ways similar to that of Snow et al. (2008). For
each expert annotator (six in total) they trained a system using only the judgements provided by these
annotators, and then created a test set using the average of the responses of the remaining five labellers on
that set. This resulted in six independent expert-trained systems. The difference with our methodology
is that we trained six independent classifiers, but based on judgements of only one human annotator, and
compared the average of the responses of six classifiers with the gold standard.
Jin et al. (2009) also used the strategy of selecting the labelled sentences agreed upon by their classi-
fiers and achieved good performances in the task of identifying opinion sentences.
Finally, our methodology is also similar to one of those mentioned by Yu (2014). The author used the
traditional co-training strategy, i.e. providing a small pool of unlabelled data to two classifiers with con-
fidence rates, in order to obtain automatically labelled examples that would be added to an initial set of
labelled ones. Subsequently, this final large set is used to train the the two classifiers and a combination
of them (constructed by multiplying their predictions) is eventually the one used to label new docu-
ments. Five strategies were applied to obtain the views: (a) using unigrams and bigrams as features, (b)
randomly splitting the feature set in two, (c) using two different supervised learning algorithms because
they would provide useful examples to each other since based on different learning assumptions; (d)
randomly splitting the training set, and (e) applying a character-based language model (CLM) and a bag-
of-words model (BOW). We extended the third strategy by using three classifiers and two different views
for each of them, and by applying this to the task of annotation validation rather than semi-supervised
learning.
6 Conclusions
In this paper we have presented a methodology that makes use of multiple classifiers (based on different
views) in order to detect inconsistent annotations and borderline cases. In our test set, we found that
in 35% of the wrongly classified cases the predictions were different but acceptable, and in the 16% of
them the predictions suggested that the gold standard was wrong. On the other hand, the data resulting
from such procedure related to non-disagreeing predictions can be regarded as expression of either the
efficacy of the annotation schema and guidelines or the features used for the machine learning step.
Our next goal is to improve the performances of the classifiers over the instances that were incorrectly
handled, currently accounting for the 26% in our test set. We will also test the same methodology over
the extraction of the link between targets and modifiers (appraisal groups). The machine learning models,
the datasets and the error analysis are publicly available in order to ensure reproducibility
3
.
3
http://corpus.leeds.ac.uk/marilena/SentiML/LAW2014_error_analysis.zip
85
References
Steven Abney. 2007. Semisupervised Learning for Computational Linguistics. Chapman & Hall/CRC, 1st edition.
Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Comput. Linguist.,
34(4):555?596, December.
Petra S. Bayerl, Harald L?ungen, Ulrike Gut, and Karsten I. Paul. 2003. Methodology for reliable schema de-
velopment and evaluation of manual annotations. In Proceedings of the Workshop on Knowledge Markup and
Semantic Annotation at the Second International Conference on Knowledge Capture (K-CAP 2003, pages 17?
23.
Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit
3
: Web inventory of transcribed and translated
talks. In Proceedings of the 16
th
Conference of the European Association for Machine Translation (EAMT),
pages 261?268, Trento, Italy, May.
Marilena Di Bari, Serge Sharoff, and Martin Thomas. 2013. SentiML: Functional annotation for multilingual sen-
timent analysis. In DH-case 2013: Collaborative Annotations in Shared Environments: metadata, vocabularies
and techniques in the Digital Humanities, ACM International Conference Proceedings.
Wei Jin, Hung H. Ho, and Rohini K. Srihari. 2009. Opinionminer: a novel machine learning system for web opin-
ion mining and extraction. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge
discovery and data mining, KDD ?09, pages 1195?1204, New York, NY, USA. ACM.
Saif Mohammad. 2011. From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.
In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences,
and Humanities, pages 105?114, Portland, OR, USA, June.
Joakim Nivre. 2005. Dependency grammar and dependency parsing. Technical report, V?axj?o University.
James Pustejovsky and Amber Stubbs. 2012. Natural Language Annotation for Machine Learning. Oreilly and
Associate Series. O?Reilly Media, Incorporated.
Jonathon Read, David Hope, and John Carroll. 2007. Annotating expressions of appraisal in English. In Proceed-
ings of the Linguistic Annotation Workshop, LAW ?07, pages 93?100, Stroudsburg, PA, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast?but is it good?:
Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP ?08, pages 254?263, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity,
and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh.
Ning Yu. 2014. Exploring co-training strategies for opinion detection. Journal of the Asssociation for Information
Science and Technology.
86
