Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 811?818,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Comparison of Alternative Parse Tree Paths for Labeling Semantic Roles 
 Reid Swanson and Andrew S. Gordon Institute for Creative Technologies University of Southern California 13274 Fiji Way, Marina del Rey, CA 90292 USA swansonr@ict.usc.edu, gordon@ict.usc.edu     Abstract The integration of sophisticated infer-ence-based techniques into natural lan-guage processing applications first re-quires a reliable method of encoding the predicate-argument structure of the pro-positional content of text. Recent statisti-cal approaches to automated predicate-argument annotation have utilized parse tree paths as predictive features, which encode the path between a verb predicate and a node in the parse tree that governs its argument. In this paper, we explore a number of alternatives for how these parse tree paths are encoded, focusing on the difference between automatically generated constituency parses and de-pendency parses. After describing five al-ternatives for encoding parse tree paths, we investigate how well each can be aligned with the argument substrings in annotated text corpora, their relative pre-cision and recall performance, and their comparative learning curves. Results in-dicate that constituency parsers produce parse tree paths that can more easily be aligned to argument substrings, perform better in precision and recall, and have more favorable learning curves than those produced by a dependency parser. 1 Introduction A persistent goal of natural language processing research has been the automated transformation of natural language texts into representations that unambiguously encode their propositional content in formal notation. Increasingly, first-order predicate calculus representations of 
textual meaning have been used in natural lanugage processing applications that involve automated inference. For example, Moldovan et al (2003) demonstrate how predicate-argument formulations of questions and candidate answer sentences are unified using logical inference in a top-performing question-answering application. The importance of robust techniques for predicate-argument transformation has motivated the development of large-scale text corpora with predicate-argument annotations such as PropBank (Palmer et al, 2005) and FrameNet (Baker et al, 1998). These corpora typically take a pragmatic approach to the predicate-argument representations of sentences, where predicates correspond to single word triggers in the surface form of the sentence (typically verb lemmas), and arguments can be identified as substrings of the sentence. Along with the development of annotated corpora, researchers have developed new techniques for automatically identifying the arguments of predications by labeling text segments in sentences with semantic roles. Both Gildea & Jurafsky (2002) and Palmer et al (2005) describe statistical labeling algorithms that achieve high accuracy in assigning semantic role labels to appropropriate constituents in a parse tree of a sentence. Each of these efforts employed the use of parse tree paths as predictive features, encoding the series of up and down transitions through a parse tree to move from the node of the verb (predicate) to the governing node of the constituent (argument). Palmer et al (2005) demonstrate that utilizing the gold-standard parse trees of the Penn tree-bank (Marcus et al, 1993) to encode parse tree paths yields significantly better labeling accuracy than when using an automatic syntactical parser, namely that of Collins (1999). 
811
Parse tree paths (between verbs and arguments that fill semantic roles) are particularly interest-ing because they symbolically encode the rela-tionship between the syntactic and semantic as-pects of verbs, and are potentially generalized across other verbs within the same class (Levin, 1993). However, the encoding of individual parse tree paths for predicates is wholly depend-ent on the characteristics of the parse tree of a sentence, for which competing approaches could be taken.  The research effort described in this paper fur-ther explores the role of parse tree paths in iden-tifying the argument structure of verb-based predications. We are particularly interested in exploring alternatives to the constituency parses that were used in previous research, including parsing approaches that employ dependency grammars. Specifically, our aim is to answer four important questions: 1. How can parse tree paths be encoded when employing different automated constituency parsers, i.e. Charniak (2000), Klein & Manning (2003), or a dependency parser (Lin, 1998)? 2. Given that each of these alternatives creates a different formulation of the parse tree of a sen-tence, which of them encodes branches that are easiest to align with substrings that have been annotated with semantic role information? 3. What is the relative precision and recall per-formance of parse tree paths formulated using these alternative automated parsing techniques, and do the results vary depending on argument type? 4. How many examples of parse tree paths are necessary to provide as training examples in or-der to achieve high labeling accuracy when em-ploying each of these parsing alternatives? Each of these four questions is addressed in the four subsequent sections of this paper, fol-lowed by a discussion of the implications of our findings and directions for future work.  2 Alternative Parse Tree Paths Parse tree paths were introduced by Gildea & Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and arguments in the parse tree of a sentence. Predi-cates are typically assumed to be specific target words (usually verbs), and arguments are as-sumed to be a span of words in the sentence that are governed by a single node in the parse tree. A parse tree path can be described as a sequence of transitions up and down a parse tree from the 
target word to the governing node, as exempli-fied in Figure 1. The encoding of the parse tree path feature is dependent on the syntactic representation that is produced by the parser. This, in turn, is depend-ant on the training corpus used to build the parser, and the conditioning factors in its prob-ability model. As result, encodings of parse tree paths can vary greatly depending on the parser that is used, yielding parse tree paths that vary in their ability to generalize across sentences. In this paper we explore the characteristics of parse tree paths with respect to different ap-proaches to automated parsing. We were particu-larly interested in comparing traditional constitu-ency parsing (as exemplified in Figure 1) with dependency parsing, specifically the Minipar system built by Lin (1998). Minipar is increas-ingly being used in semantics-based nlp applica-tions (e.g. Pantel & Lin, 2002). Dependency parse trees differ from constituency parses in that they represent sentence structures as a set of de-pendency relationships between words, typed asymmetric binary relationships between head words and modifying words. Figure 2 depicts the output of Minipar on an example sentence, where each node is a word or an empty node along with the word lemma, its part of speech, and the relationship type to its governing node. Our motivation for exploring the use of Mini-par in for the creation of parse tree paths can be seen by comparing Figure 1 and Figure 2, where 
 Figure 1: An example parse tree path from the predicate ate to the argument NP He, rep-resented as VB?VP?S?NP.   
 Figure 2. An example dependency parse, with a parse tree path from the predicate ate to the argument He. 
812
the Minipar path is both shorter and simpler for the same predicate-argument relationship, and could be encoded in various ways that take ad-vantage of the additional semantic and lexical information that is provided. To compare traditional constituency parsing with dependency parsing, we evaluated the accu-racy of argument labeling using parse tree paths generated by two leading constituency parsers and three variations of parse tree paths generated by Minipar, as follows:  Charniak: We used the Charniak parser (2000) to extract parse tree paths similar to those found in Palmer et al (2005), with some slight modifications. In cases where the last node in the path was a non-branching pre-terminal, we added the lexical information to the path node. In addi-tion, our paths led to the lowest governing node, rather than the highest. For example, the parse tree path for the argument in Figure 1 would be  encoded as:  VB?VP?S?NP?PRP:he  Stanford: We also used the Stanford parser developed by Klein & Manning (2003), with the same path encoding as the Charniak parser.  Minipar A: We used three variations of parse tree path encodings based on Lin?s dependency parser, Minipar (1998). Minipar A is the first and most restrictive path encoding, where each is annotated with the entire information output by Minpar at each node. A typical path might be: ate:eat,V,i?He:he,N,s  Minipar B: A second parse tree path encoding was generated from Minipar parses that relaxes some of the constraints used in Minpar A. In-stead of using all the information contained at a node, in Minipar B we only encode a path with its part of speech and relational information. For example: V,i?N,s  Minipar C: As the converse to Minipar A we also tried one other Minipar encoding. As in Minipar A, we annotated the path with all the information output, but instead of doing a direct string comparison during our search, we consid-ered two paths matching when there was a match between either the word, the stem, the part of speech, or the relation. For example, the follow-ing two parse tree paths would be considered a match, as both include the relation i. 
ate:eat,V,i?He:he,N,s was:be,VBE,i?He:he,N,s  We explored other combinations of depend-ency relation information for Minipar-derived parse tree paths, including the use of the deep relations. However, results obtained using these other combinations were not notably different from those of the three base cases listed above, and are not included in the evaluation results re-ported in this paper. 3 Aligning arguments to parse trees nodes in a training / testing corpus We began our investigation by creating a training and testing corpus of 400 sentences each contain-ing an inflection of one of four target verbs (100 each), namely believe, think, give, and receive. These sentences were selected at random from the 1994-07 section of the New York Times gi-gaword corpus from the Linguistic Data Consor-tium. These four verbs were chosen because of the synonymy among the first two, and the re-flexivity of the second two, and because all four have straightforward argument structures when viewed as predicates, as follows:  predicate: believe arg0: the believer arg1: the thing that is believed  predicate: think arg0: the thinker arg1: the thing that is thought  predicate: give arg0: the giver arg1: the thing that is given arg2: the receiver  predicate: receive arg0: the receiver arg1: the thing that is received arg2: the giver  This corpus of sentences was then annotated with semantic role information by the authors of this paper. All annotations were made by assign-ing start and stop locations for each argument in the unparsed text of the sentence. After an initial pilot annotation study, the following annotation policy was adopted to overcome common dis-agreements: (1) When the argument is a noun and it is part of a definite description then in-
813
clude the entire definite description. (2) Do not include complementizers such as ?that? in ?be-lieve that? in an argument. (3) Do include prepo-sitions such as ?in? in ?believe in?. (4) When in doubt, assume phrases attach locally. Using this policy, an agreement of 92.8% was achieved among annotators for the set of start and stop locations for arguments. Examples of semantic role annotations in our corpus for each of the four predicates are as follows:  1. [Arg0Those who excavated the site in 1907] believe [Arg1 it once stood two or three stories high.] 2. Gus is in good shape and [Arg0 I] think [Arg1 he's happy as a bear.] 3. If successful, [Arg0 he] will give [Arg1 the funds] to [Arg2 his Vietnamese family.]  4. [Arg0 The Bosnian Serbs] have received [Arg1 military and economic support] from [Arg2 Ser-bia.] The next step was to parse the corpus of 400 sentences using each of three automated parsing systems (Charniak, Stanford, and Minipar), and align each of the annotated arguments with its closest matching branch in the resulting parse trees. Given the differences in the parsing models used by these three systems, each yield parse tree nodes that govern different spans of text in the sentence. Often there exists no parse tree node that governs a span of text that exactly matches the span of an argument in the annotated corpus. Accordingly, it was necessary to identify the closest match possible for each of the three pars-ing systems in order to encode parse tree paths for each. We developed a uniform policy that would facilitate a fair comparison between pars-ing techniques. Our approach was to identify a single node in a given parse tree that governed a string of text with the most overlap with the text of the annotated argument. Each of the parsing methods tokenizes the input string differently, so in order to simplify the selection of the govern-ing node with the most overlap, we made this selection based on lowest minimum edit distance (Levenshtein distance). All three of these different parsing algorithms produced single governing nodes that overlapped well with the human-annotated corpus. However, it appeared that the two constituency parsers pro-duced governing nodes that were more closely aligned, based on minimum edit distance. The Charniak parser aligned best with the annotated text, with an average of 2.40 characters for the lowest minimum edit distance (standard de-viation = 8.64). The Stanford parser performed 
slightly worse (average = 2.67, standard devia-tion = 8.86), while distances were nearly two times larger for Minipar (average = 4.73, standard deviation = 10.44).  In each case, the most overlapping parse tree node was treated as correct for training and test-ing purposes.  4 Comparative Performance Evaluation In order to evaluate the comparative performance of the parse tree paths for each of the five encod-ings, we divided the corpus in to equal-sized training and test sets (50 training and 50 test ex-amples for each of the four predicates). We then constructed a system that identified the parse tree paths for each of the 10 arguments in the training sets, and applied them to the sentences in each corresponding test sets. When applying the 50 training parse tree paths to any one of the 50 test sentences for a given predicate-argument pair, a set of zero or more candidate answer nodes were returned. For the purpose of calculating precision and recall scores, credit was given when the cor-rect answer appeared in this set. Precision scores were calculated as the number of correct answers found divided by the number of all candidate answer nodes returned. Recall scores were calcu-lated as the number of correct answers found di-vided by the total number of correct answers possible. F-scores were calculated as the equally-weighted harmonic mean of precision and recall.  Our calculation of recall scores represents the best-possible performance of systems using only these types of parse-tree paths. This level of per-formance could be obtained if a system could always select the correct answer from the set of candidates returned. However, it is also informa-tive to estimate the performance that could be achieved by randomly selecting among the can-didate answers, representing a lower-bound on performance. Accordingly, we computed an ad-justed recall score that awarded only fractional credit in cases where more than one candidate answer was returned (one divided by the set size). Adjusted recall is the sum of all of these adjusted credits divided by the total number of correct answers possible. Figure 3 summarizes the comparative recall, precision, f-score, and adjusted recall perform-ance for each of the five parse tree path formula-tions. The Charniak parser achieved the highest overall scores (precision=.49, recall=.68, f-score=.57, adjusted recall=.48), followed closely 
814
by the Stanford parser (precision=.47, recall=.67, f-score=.55, adjusted recall=.48). Our expectation was that the short, semanti-cally descriptive parse tree paths produced by Minipar would yield the highest performance. However, these results indicate the opposite; the constituency parsers produce the most accurate parse tree paths. Only Minipar C offers better recall (0.71) than the constituency parsers, but at the expense of extremely low precision. Minipar A offers excellent precision (0.62), but with ex-tremely low recall. Minipar B provides a balance between recall and precision performance, but falls short of being competitive with the parse tree paths generated by the two constituency parsers, with an f-score of .44. We utilized the Sign Test in order to deter-mine the statistical significance of these differ-ences. Rank orderings between pairs of systems were determined based on the adjusted credit that each system achieved for each test sentence. Sig-nificant differences were found between the per-formance of every system (p<0.05), with the ex-ception of the Charniak and Stanford parsers. Interestingly, by comparing weighted values for each test example, Minipar C more frequently scores higher than Minipar A, even though the 
sum of these scores favors Minipar A. In addition to overall performance, we were interested in determining whether performance varied depending on the type of the argument that is being labeled. In assigning labels to argu-ments in the corpus, we followed the general principles set out by Palmer et al (2005) for la-beling arguments arg0, arg1 and arg2. Across each of our four predicates, arg0 is the agent of the predication (e.g. the person that has the belief or is doing the giving), and arg1 is the thing that is acted upon by the agent (e.g. the thing that is believed or the thing that is given). Arg2 is used only for the predications based on the verbs give and receive, where it is used to indicate the other party of the action.  Our interest was in determining whether these five approaches yielded different results depend-ing on the semantic type of the argument. Fig-ure 4 presents the f-scores for each of these en-codings across each argument type.  Results indicate that the Charniak and Stan-ford parsers continue to produce parse tree paths that outperform each of the Minipar-based ap-proaches. In each approach argument 0 is the easiest to identify. Minipar A retains the general trends of Charniak and Stanford, with argument 
 Figure 3. Precision, recall, f-scores, and adjusted recall for five parse tree path types 
 Figure 4. Comparative f-scores for arguments 0, 1, and 2 for five parse tree path types  
815
1 easier to identify than argument 2, while Mini-par B and C show the reverse. The highest f-scores for argument 0 were achieved Stanford (f=.65), while Charniak achieved the highest scores for argument 1 (f=.55) and argument 2 (f=.49). 5 Learning Curve Comparisons The creation of large-scale text corpora with syn-tactic and/or semantic annotations is difficult, expensive, and time consuming. The PropBank effort has shown that producing this type of cor-pora is considerably easier once syntactic analy-sis has been done, but substantial effort and re-sources are still required. Better estimates of total costs could be made if it was known exactly how many annotations are necessary to achieve ac-ceptable levels of performance. Accordingly, we investigated the learning curves of precision, re-call, f-score, and adjusted recall achieved using the five different parse tree path encodings. For each encoding approach, learning curves were created by applying successively larger subsets of the training parse tree paths to each of the items in the corresponding test set. Precision, recall, f-scores, and adjusted recall were com-puted as described in the previous section, and identical subsets of sentences were used across parsers, in one-sentence increments. Individual learning curves for each of the five approaches are given in Figures 5, 6, 7, 8, and 9. Figure 10 presents a comparison of the f-score learning curves for all five of the approaches.  In each approach, the precision scores slowly degrade as more training examples are provided, due to the addition of new parse tree paths that yield additional candidate answers. Conversely, the recall scores of each system show their great-est gains early, and then slowly improve with the addition of more parse tree paths. In each ap-proach, the recall scores (estimating best-case performance) have the same general shape as the adjusted recall scores (estimating the lower-bound performance). The divergence between these two scores increases with the addition of more training examples, and is more pronounced in systems employing parse tree paths with less specific node information. The comparative f-score curves presented in Figure 10 indicate that Minipar B is competitive with Charniak and Stanford when only a small number of training examples is available. There is some evidence here that the performance of Minipar A would continue to improve with the addition of more 
training data, suggesting that this approach might be well-suited for applications where lots of training data is available.  6 Discussion Annotated corpora of linguistic phenomena en-able many new natural language processing ap-plications and provide new means for tackling difficult research problems. Just as the Penn Treebank offers the possibility of developing systems capable of accurate syntactic parsing, corpora of semantic role annotations open up new possibilities for rich textual understanding and integrated inference. In this paper, we compared five encodings of parse tree paths based on two constituency pars-ers and a dependency parser. Despite our expec-tations that the semantic richness of dependency parses would yield paths that outperformed the others, we discovered that parse tree paths from Charniak?s constituency parser performed the best overall. In applications where either preci-sion or recall is the only concern, then Minipar-derived parse tree paths would yield the best re-sults. We also found that the performance of all of these systems varied across different argument types.    In contrast to the performance results reported by Palmer et al (2005) and Gildea & Jurafsky (2002), our evaluation was based solely on parse tree path features. Even so, we were able to ob-tain reasonable levels of performance without the use of additional features or stochastic methods. Learning curves indicate that the greatest gains in performance can be garnered from the first 10 or so training examples. This result has implica-tions for the development of large-scale corpora of semantically annotated text. Developers should distribute their effort in order to maxi-mize the number of predicate-argument pairs with at least 10 annotations.  An automated semantic role labeling system could be constructed using only the parse tree path features described in this paper, with esti-mated performance between our recall scores and our adjusted recall scores. There are several ways to improve on the random selection approach used in the adjusted recall calculation. For exam-ple, one could simply select the candidate answer with the most frequent parse tree path.  The results presented in this paper help inform the design of future automated semantic role la-beling systems that improve on the best-performing systems available today (Gildea &  
816
    
 Figure 5. Charniak learning curves    
 Figure 6. Stanford learning curves    
 Figure 7. Minipar A learning curves        
    
 Figure 8. Minipar B learning curves    
 Figure 9. Minipar C learning curves    
 Figure 10. Comparative F-score curves   
817
Jurafsky, 2002; Moschitti et al, 2005). We found that different parse tree paths encode different types of linguistic information, and exhibit dif-ferent characteristics in the tradeoff between pre-cision and recall. The best approaches in future systems will intelligently capitalize on these dif-ferences in the face of varying amounts of train-ing data.  In our own future work, we are particularly in-terested in exploring the regularities that exist among parse tree paths for different predicates. By identifying these regularities, we believe that we will be able to significantly reduce the total number of annotations necessary to develop lexi-cal resources that have broad coverage over natu-ral language.  Acknowledgments The project or effort depicted was sponsored by the U. S. Army Research, Development, and En-gineering Command (RDECOM). The content or information does not necessarily reflect the posi-tion or the policy of the Government, and no of-ficial endorsement should be inferred. References Baker, Collin, Charles J. Fillmore and John B. Lowe. 1998. The Berkeley FrameNet Project, In Proceed-ings of COLING-ACL, Montreal. Charniak, Eugene. 2000. A maximum-entropy-inspired parser, In Proceedings NAACL. Collins, Michael. 1999. Head-Driven Statistical Mod-els for Natural Language Parsing, PhD thesis, Uni-versity of Pennsylvania. Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles, Computational Linguis-tics, 28(3):245--288. Klein, Dan and Christopher Manning. 2003. Accurate Unlexicalized Parsing, In Proceedings of the 41st Annual Meeting of the Association for Computa-tional Linguistics, 423-430. Levin, Beth. 1993. English Verb Classes and Alterna-tions: A Preliminary Investigation. Chicago, IL: University of Chicago Press. Lin, Dekang. 1998. Dependency-Based Evaluation of MINIPAR, In Proceedings of the Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation, Granada, Spain. Marcus, Mitchell P., Beatrice Santorini and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank, Computa-tional Linguistics, 19(35):313-330 
Moldovan, Dan I., Christine Clark, Sanda M. Hara-bagiu & Steven J. Maiorano. 2003. COGEX: A Logic Prover for Question Answering, HLT-NAACL. Moschitti, A., Giuglea, A., Coppola, B. & Basili, R. 2005. Hierarchical Semantic Role Labeling. In Proceedings of the 9th Conference on Computa-tional Natural Language Learning (CoNLL 2005 shared task), Ann Arbor(MI), USA. Palmer, Martha, Dan Gildea and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles, Computational Linguistics. Pantel, Patrick and Dekang Lin. 2002. Document clustering with committees, In Proceedings of SIGIRO2, Tampere, Finland.  
818
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 192?199,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generalizing Semantic Role Annotations  
Across Syntactically Similar Verbs 
 
 
Andrew S. Gordon Reid Swanson 
Institute for Creative Technologies Institute for Creative Technologies 
University of Southern California University of Southern California 
Marina del Rey, CA 90292 USA Marina del Rey, CA 90292 USA 
gordon@ict.usc.edu swansonr@ict.usc.edu 
 
 
 
 
Abstract 
Large corpora of parsed sentences with 
semantic role labels (e.g. PropBank) pro-
vide training data for use in the creation 
of high-performance automatic semantic 
role labeling systems. Despite the size of 
these corpora, individual verbs (or role-
sets) often have only a handful of in-
stances in these corpora, and only a 
fraction of English verbs have even a sin-
gle annotation. In this paper, we describe 
an approach for dealing with this sparse 
data problem, enabling accurate semantic 
role labeling for novel verbs (rolesets) 
with only a single training example. Our 
approach involves the identification of 
syntactically similar verbs found in Prop-
Bank, the alignment of arguments in their 
corresponding rolesets, and the use of 
their corresponding annotations in Prop-
Bank as surrogate training data. 
1 Generalizing Semantic Role Annotations 
A recent release of the PropBank (Palmer et al, 
2005) corpus of semantic role annotations of Tree-
bank parses contained 112,917 labeled instances of 
4,250 rolesets corresponding to 3,257 verbs, as 
illustrated by this example for the verb buy. 
 
[arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 from 
Jerry] [arg3 for $1000]. 
 
Annotations similar to these have been used to cre-
ate automated semantic role labeling systems 
(Pradhan et al, 2005; Moschitti et al, 2006) for 
use in natural language processing applications that 
require only shallow semantic parsing. As with all 
machine-learning approaches, the performance of 
these systems is heavily dependent on the avail-
ability of adequate amounts of training data. How-
ever, the number of annotated instances in 
PropBank varies greatly from verb to verb; there 
are 617 annotations for the want roleset, only 7 for 
desire, and 0 for any sense of the verb yearn. Do 
we need to keep annotating larger and larger cor-
pora in order to generate accurate semantic label-
ing systems for verbs like yearn? 
A better approach may be to generalize the data 
that exists already to handle novel verbs. It is rea-
sonable to suppose that there must be a number of 
verbs within the PropBank corpus that behave 
nearly exactly like yearn in the way that they relate 
to their constituent arguments. Rather than annotat-
ing new sentences that contain the verb yearn, we 
could simply find these similar verbs and use their 
annotations as surrogate training data. 
This paper describes an approach to generalizing 
semantic role annotations across different verbs, 
involving two distinct steps. The first step is to 
order all of the verbs with semantic role annota-
tions according to their syntactic similarity to the 
target verb, followed by the second step of aligning 
argument labels between different rolesets. To 
evaluate this approach we developed a simple 
automated semantic role labeling algorithm based 
on the frequency of parse-tree paths, and then 
compared its performance when using real and sur-
rogate training data from PropBank. 
192
2 Parse Tree Paths 
A key concept in understanding our approach to 
both automated semantic role annotation and gen-
eralization is the notion of a parse tree path. Parse 
tree paths were used for semantic role labeling by 
Gildea and Jurafsky (2002) as descriptive features 
of the syntactic relationship between predicates 
and their arguments in the parse tree of a sentence. 
Predicates are typically assumed to be specific tar-
get words (verbs), and arguments are assumed to 
be spans of words in the sentence that are domi-
nated by nodes in the parse tree. A parse tree path 
can be described as a sequence of transitions up 
from the target word then down to the node that 
dominates the argument span (e.g. Figure 1). 
 
 
Figure 1: An example parse tree path from the 
predicate ate to the argument NP He, represented 
as VBVPSNP 
 
Parse tree paths are particularly interesting for 
automated semantic role labeling because they 
generalize well across syntactically similar sen-
tences. For example, the parse tree path in Figure 1 
would still correctly identify the ?eater? argument 
in the given sentence if the personal pronoun ?he? 
were swapped with a markedly different noun 
phrase, e.g. ?the attendees of the annual holiday 
breakfast.? 
3 A Simple Semantic Role Labeler 
To explore issues surrounding the generalization of 
semantic role annotations across verbs, we began 
by authoring a simple automated semantic role la-
beling algorithm that assigns labels according to 
the frequency of the parse tree paths seen in train-
ing data. To construct a labeler for a specific role-
set, training data consisting of parsed sentences 
with role-labeled parse tree constituents are ana-
lyzed to identify all of the parse tree paths between 
predicates and arguments, which are then tabulated 
and sorted by frequency. For example, Table 1 lists 
the 10 most frequent pairs of arguments and parse 
tree paths for the want.01 roleset in a recent release 
of PropBank. 
 
Count Argument Parse tree path 
189 ARG0 VBPVPSNP  
159 ARG1 VBPVPS  
125 ARG0 VBZVPSNP  
110 ARG1 VBZVPS  
102 ARG0 VBVPVPSNP  
98 ARG1 VBVPS  
96 ARG0 VBDVPSNP  
79 ARGM VBVPVPRB  
76 ARG1 VBDVPS  
43 ARG1 VBPVPNP  
Table 1. Top 10 most frequent parse tree paths for 
arguments of the PropBank want.01 roleset, based 
on 617 annotations 
  
To automatically assign role labels to an unla-
beled parse tree, each entry in the table is consid-
ered in order of highest frequency. Beginning from 
the target word in the sentence (e.g. wants) a check 
is made to determine if the entry includes a possi-
ble parse tree path in the parse tree of the sentence. 
If so, then the constituent is assigned the role label 
of the entry, and all subsequent entries in the table 
that have the same argument label or lead to sub-
constituents of the labeled node are invalidated. 
Only subsequent entries that assign core arguments 
of the roleset (e.g. ARG0, ARG1) are invalidated, 
allowing for multiple assignments of non-core la-
bels (e.g. ARGM) to a test sentence. In cases 
where the path leads to more than one node in a 
sentence, the leftmost path is selected. This process 
then continues down the list of valid table entries, 
assigning additional labels to unlabeled parse tree 
constituents, until the end of the table is reached. 
This approach also offers a simple means of 
dealing with multiple-constituent arguments, 
which occasionally appear in PropBank data. In 
these cases, the data is listed as unique entries in 
the frequency table, where each of the parse tree 
paths to the multiple constituents are listed as a set. 
The labeling algorithm will assign the argument of 
the entry only if all parse tree paths in the set are 
present in the sentence. 
The expected performance of this approach to 
semantic role labeling was evaluated using the 
PropBank data using a leave-one-out cross-
validation experimental design. Precision and re-
call scores were calculated for each of the 3,086 
193
rolesets with at least two annotations. Figure 2 
graphs the average precision, recall, and F-score 
for rolesets according to the number of training 
examples of the roleset in the PropBank corpus. 
An additional curve in Figure 2 plots the percent-
age of these PropBank rolesets that have the given 
amount of training data or more. For example, F-
scores above 0.7 are first reached with 62 training 
examples, but only 8% of PropBank rolesets have 
this much training data available. 
 
 
Figure 2. Performance of our semantic role label-
ing approach on PropBank rolesets 
4 Identifying Syntactically Similar Verbs 
A key part of generalizing semantic role annota-
tions is to calculate the syntactic similarity be-
tween verbs. The expectation here is that verbs that 
appear in syntactically similar contexts are going 
to behave similarly in the way that they relate to 
their arguments. In this section we describe a fully 
automated approach to calculating the syntactic 
similarity between verbs. 
Our approach is strictly empirical; the similarity 
of verbs is determined by examining the syntactic 
contexts in which they appear in a large text cor-
pus. Our approach is analogous to previous work 
in extracting collocations from large text corpora 
using syntactic information (Lin, 1998). In our 
work, we utilized the GigaWord corpus of English 
newswire text (Linguistic Data Consortium, 2003), 
consisting of nearly 12 gigabytes of textual data. 
To prepare this corpus for analysis, we extracted 
the body text from each of the 4.1 million entries 
in the corpus and applied a maximum-entropy al-
gorithm to identify sentence boundaries (Reynar 
and Ratnaparkhi, 1997). 
Next we executed a four-step analysis process 
for each of the 3,257 verbs in the PropBank cor-
pus. In the first step, we identified each of the sen-
tences in the prepared GigaWord corpus that 
contained any inflection of the given verb. To 
automatically identify all verb inflections, we util-
ized the English DELA electronic dictionary 
(Courtois, 2004), which contained all but 21 of the 
PropBank verbs (for which we provided the inflec-
tions ourselves), with old-English verb inflections 
removed. We extracted GigaWord sentences con-
taining these inflections by using the GNU grep 
program and a template regular expression for each 
inflection list. The results of these searches were 
collected in 3,257 files (one for each verb). The 
largest of these files was for inflections of the verb 
say (15.9 million sentences), and the smallest was 
for the verb namedrop (4 sentences). 
The second step was to automatically generate 
syntactic parse trees for the GigaWord sentences 
found for each verb. It was our original intention to 
parse all of the found sentences, but we found that 
the slow speed of contemporary syntactic parsers 
made this impractical. Instead, we focused our ef-
forts on the first 100 sentences found for each of 
the 3,257 verbs with 100 or fewer tokens: a total of 
324,461 sentences (average of 99.6 per verb). For 
this task we utilized the August 2005 release of the 
Charniak parser with the default speed/accuracy 
settings (Charniak, 2000), which required roughly 
360 hours of processor time on a 2.5 GHz 
PowerPC G5. 
The third step was to characterize the syntactic 
context of the verbs based on where they appeared 
within the parse trees. For this purpose, we utilized 
parse tree paths as a means of converting tree 
structures into a flat, feature-vector representation. 
For each sentence, we identified all possible parse 
tree paths that begin from the verb inflection and 
terminate at a constituent that does not include the 
verb inflection. For example, the syntactic context 
of the verb in Figure 1 can be described by the fol-
lowing five parse tree paths: 
1. VBVPSNP 
2. VBVPSNPPRP 
3. VBVPNP 
4. VBVPNPDT 
5. VBVPNPNN 
Possible parse tree paths were identified for 
every parsed sentence for a given verb, and the 
frequencies of each unique path were tabulated 
194
into a feature vector representation. Parse tree 
paths where the first node was not a Treebank part-
of-speech tag for a verb were discarded, effectively 
filtering the non-verb homonyms of the set of in-
flections. The resulting feature vectors were nor-
malized by dividing the values of each feature by 
the number of verb instances used to generate the 
parse tree paths; the value of each feature indicates 
the proportion of observed inflections in which the 
parse tree path is possible. As a representative ex-
ample, 95 verb forms of abandon were found in 
the first 100 GigaWord sentences containing any 
inflection of this verb. For this verb, 4,472 possible 
parse tree paths were tabulated into 3,145 unique 
features, 2501 of which occurred only once. 
The fourth step was to compute the distance be-
tween a given verb and each of the 3,257 feature 
vector representations describing the syntactic con-
text of PropBank verbs. We computed and com-
pared the performance of a wide variety of possible 
vector-based distance metrics, including Euclidean, 
Manhattan, and Chi-square (with un-normalized 
frequency counts), but found that the ubiquitous 
cosine measure was least sensitive to variations in 
sample size between verbs. To facilitate a com-
parative performance evaluation (section 6), pair-
wise cosine distance measures were calculated 
between each pair of PropBank verbs and sorted 
into individual files, producing 3,257 lists of 3,257 
verbs ordered by similarity. 
Table 2 lists the 25 most syntactically similar 
pairs of verbs among all PropBank verbs. There 
are a number of notable observations in this list. 
First is the extremely high similarity between bind 
and bound. This is partly due to the fact that they 
share an inflection (bound is the irregular past 
tense form of bind), so the first 100 instances of 
GigaWord sentences for each verb overlap signifi-
cantly, resulting in overlapping feature vector rep-
resentations. Although this problem appears to be 
restricted to this one pair of verbs, it could be 
avoided in the future by using the part-of-speech 
tag in the parse tree to help distinguish between 
verb lemmas. 
A second observation of Table 2 is that several 
verbs appear multiple times in this list, yielding 
sets of verbs that all have high syntactic similarity. 
Three of these sets account for 19 of the verbs in 
this list: 
1. plunge, tumble, dive, jump, fall, fell, dip 
2. assail, chide, lambaste 
3. buffet, embroil, lock, superimpose, whip-
saw, pluck, whisk, mar, ensconce 
The appearance of these sets suggests that our 
method of computing syntactic similarity could be 
used to identify distinct clusters of verbs that be-
have in very similar ways. In future work, it would 
be particularly interesting to compare empirically-
derived verb clusters to verb classes derived from 
theoretical considerations (Levin, 1993), and to the 
automated verb classification techniques that use 
these classes (Joanis and Stevenson, 2003). 
A third observation of Table 2 is that the verb 
pairs with the highest syntactic similarity are often 
synonyms, e.g. the cluster of assail, chide, and 
lambaste. As a striking example, the 14 most syn-
tactically similar verbs to believe (in order) are 
think, guess, hope, feel, wonder, theorize, fear, 
reckon, contend, suppose, understand, know, 
doubt, and suggest ? all mental action verbs. This 
observation further supports the distributional hy-
pothesis of word similarity and corresponding 
technologies for identifying synonyms by similar-
ity of lexical-syntactic context (Lin, 1998). 
   
Verb pairs (instances) Cosine 
bind (83) bound (95) 0.950 
plunge (94) tumble (87) 0.888 
dive (36) plunge (94) 0.867 
dive (36) tumble (87) 0.866 
jump (79) tumble (87) 0.865 
fall (84) fell (102) 0.859 
intersperse (99) perch (81) 0.859 
assail (100) chide (98) 0.859 
dip (81) fell (102) 0.858 
buffet (72) embroil (100) 0.856 
embroil (100) lock (73) 0.856 
embroil (100) superimpose (100) 0.856 
fell (102) jump (79) 0.855 
fell (102) tumble (87) 0.855 
embroil (100) whipsaw (63) 0.850 
pluck (100) whisk (99) 0.849 
acquit (100) hospitalize (99) 0.849 
disincline (70) obligate (94) 0.848 
jump (79) plunge (94) 0.848 
dive (36) jump (79) 0.847 
assail (100) lambaste (100) 0.847 
festoon (98) strew (100) 0.846 
mar (78) whipsaw (63) 0.846 
pluck (100) whipsaw (63) 0.846 
ensconce (101) whipsaw (63) 0.845 
Table 2. Top 25 most syntactically similar pairs of 
the 3257 verbs in PropBank. Each verb is listed 
with the number of inflection instances used to 
calculate the cosine measurement. 
195
5 Aligning Arguments Across Rolesets 
The second key aspect of our approach to general-
izing annotations is to make mappings between the 
argument roles of the novel target verb and the 
roles used for a given roleset in the PropBank cor-
pus. For example, if we?d like to apply the training 
data for a roleset of the verb desire in PropBank to 
a novel roleset for the verb yearn, we need to know 
that the desirer corresponds to the yearner, the de-
sired to the yearned-for, etc. In this section, we 
describe an approach to argument alignment that 
involves the application of the semantic role label-
ing approach described in section 3 to a single 
training example for the target verb. 
To simplify the process of aligning argument la-
bels across rolesets, we make a number of assump-
tions. First, we only consider cases where two 
rolesets have exactly the same number of argu-
ments. The version of the PropBank corpus that we 
used in this research contained 4250 rolesets, each 
with 6 or fewer roles (typically two or three). Ac-
cordingly, when attempting to apply PropBank 
data to a novel roleset with a given argument count 
(e.g. two), we only consider the subset of Prop-
Bank data that labels rolesets with exactly the same 
count. 
Second, our approach requires at least one fully-
annotated training example for the target roleset. A 
fully-annotated sentence is one that contains a la-
beled constituent in its parse tree for each role in 
the roleset. As an illustration, the example sentence 
in section 1 (for the roleset buy.01) would not be 
considered a fully-annotated training example, as 
only four of the five arguments of the PropBank 
buy.01 roleset are present in the sentence (it is 
missing a benefactor, as in ?Chuck bought his 
mother a car from Jerry for $1000?). 
In both of these simplifying requirements, we 
ignore role labels that may be assigned to a sen-
tence but that are not defined as part of the roleset, 
specifically the ARGM labels used in PropBank to 
label standard proposition modifiers (e.g. location, 
time, manner).  
Our approach begins with a list of verbs ordered 
by their calculated syntactic similarity to the target 
verb, as described in section 4 of this paper. We 
subsequently apply two steps that transform this 
list into an ordered set of rolesets that can be 
aligned with the roles used in one or more fully-
annotated training examples of the target verb. In 
describing these two steps, we use instigate as an 
example target verb. Instigate already appears in 
the PropBank corpus as a two-argument roleset, 
but it has only a single training example: 
 
[arg0 The Mahatma, or "great souled one,"] 
[instigate.01 instigated] [arg1 several campaigns of 
passive resistance against the British 
government in India]. 
 
The syntactic similarity of instigate to all Prop-
Bank verbs was calculated in the manner described 
in the previous section. This resulting list of 3,180 
entries begins with the following fourteen verbs: 
orchestrate, misrepresent, summarize, wreak, rub, 
chase, refuse, embezzle, harass, spew, thrash, un-
earth, snub, and erect. 
The first step is to replace each of the verbs in 
the ordered list with corresponding rolesets from 
PropBank that have the same number of roles as 
the target verb. As an example, our target roleset 
for the verb instigate has two arguments, so each 
verb in the ordered list is replaced with the set of 
corresponding rolesets that also have two argu-
ments, or removed if no two-argument rolesets 
exist for the verb in the PropBank corpus. The or-
dered list of verbs for instigate is transformed into 
an ordered list of 2,115 rolesets with two argu-
ments, beginning with the following five entries: 
orchestrate.01, chase.01, unearth.01, snub.01, and 
erect.01.  
The second step is to identify the alignments be-
tween the arguments of the target roleset and each 
of the rolesets in the ordered list. Beginning with 
the first roleset on the list (e.g. orchestrate.01), we 
build a semantic role labeler (as described in sec-
tion 3) using its available training annotations from 
the PropPank corpus. We then apply this labeler to 
the single, fully-annotated example sentence for 
the target verb, treating it as if it were a test exam-
ple of the same roleset. We then check to see if any 
of the core (numbered) role labels overlap with the 
annotations that are provided. In cases where an 
annotated constituent of the target test sentence is 
assigned a label from the source roleset, then the 
roleset mappings are noted along with the entry in 
the ordered list. If no mappings are found, the role-
set is removed from the ordered list. 
For example, the roleset for orchestrate.01 con-
tains two arguments (ARG0 and ARG1) that corre-
spond to the ?conductor, manager? and the ?things 
196
being coordinated or managed?. This roleset is 
used for only three sentence annotations in the 
PropBank corpus. Using these annotations as train-
ing data, we build a semantic role labeler for this 
roleset and apply it to the annotated sentence for 
instigate.01, treating it as if it were a test sentence 
for the roleset orchestrate.01. The labeler assigns 
the orchestrate.01 label ARG1 to the same con-
stituent labeled ARG1 in the test sentence, but fails 
to assign a label to the other argument constituent 
in the test sentence. Therefore, a single mapping is 
recorded in the ordered list of rolesets, namely that 
ARG1 of orchestrate.01 can be mapped to ARG1 
of instigate.01. 
After all of the rolesets are considered, we are 
left with a filtered list of rolesets with their argu-
ment mappings, ordered by their syntactic similar-
ity to the target verb. For the roleset instigate.01, 
this list consists of 789 entries, beginning with the 
following 5 mappings. 
1. orchestrate.01, 1:1 
2. chase.01, 0:0, 1:1  
3. unearth.01, 0:0, 1:1  
4. snub.01, 1:1  
5. erect.01, 0:0, 1:1  
Given this list, arbitrary amounts of PropBank 
annotations can be used as surrogate training data 
for the instigate.01 roleset, beginning at the top of 
the list. To utilize surrogate training data in our 
semantic role labeling approach (Section 3), we 
combine parse tree path information for a selected 
portion of surrogate training data into a single list 
sorted by frequency, and apply these files to test 
sentences as normal.  
Although we use an existing PropBank roleset 
(instigate.01) as an example in this section, this 
approach will work for any novel roleset where 
one fully-annotated training example is available. 
For example, arbitrary amounts of surrogate Prop-
Bank data can be found for the novel verb yearn by 
1) searching for sentences with the verb yearn in 
the GigaWord corpus, 2) calculating the syntactic 
similarity between yearn and all PropBank verbs 
as described in Section 4, 3) aligning the argu-
ments in a single fully-annotated example of yearn 
with ProbBank rolesets with the same number of 
arguments using the method described in this sec-
tion, and 4) selecting arbitrary amounts of Prop-
Bank annotations to use as surrogate training data, 
starting from the top of the resulting list. 
6 Evaluation 
We conducted a large-scale evaluation to deter-
mine the performance of our semantic role labeling 
algorithm when using variable amounts of surro-
gate training data, and compared these results to 
the performance that could be obtained using vari-
ous amounts of real training data (as described in 
section 3). Our hypothesis was that learning-curves 
for surrogate-trained labelers would be somewhat 
less steep, but that the availability of large-amounts 
of surrogate training data would more than make 
up for the gap.  
To test this hypothesis, we conducted an evalua-
tion using the PropBank corpus as our testing data 
as well as our source for surrogate training data. As 
described in section 5, our approach requires the 
availability of at least one fully-annotated sentence 
for a given roleset. Only 28.5% of the PropBank 
annotations assign labels for each of the numbered 
arguments in their given roleset, and only 2,858 of 
the 4,250 rolesets used in PropBank annotations 
(66.5%) have at least one fully-annotated sentence. 
Of these, 2,807 rolesets were for verbs that ap-
peared at least once in our analysis of the Giga-
Word corpus (Section 4). Accordingly, we 
evaluated our approach using the annotations for 
this set of 2,807 rolesets as test data. For each of 
these rolesets, various amounts of surrogate train-
ing data were gathered from all 4,250 rolesets rep-
resented in PropBank, leaving out the data for 
whichever roleset was being tested. 
For each of the target 2,807 rolesets, we gener-
ated a list of semantic role mappings ordered by 
syntactic similarity, using the methods described in 
sections 4 and 5. In aligning arguments, only a sin-
gle training example from the target roleset was 
used, namely the first annotation within the Prop-
Bank corpus where all of the rolesets arguments 
were assigned. Our approach failed to identify any 
argument mappings for 41 of the target rolesets, 
leaving them without any surrogate training data to 
utilize. Of the remaining 2,766 rolesets, the num-
ber of mapped rolesets for a given target ranged 
from 1,041 to 1 (mean = 608, stdev = 297). 
For each of the 2,766 target rolesets with aligna-
ble roles, we gathered increasingly larger amounts 
of surrogate training data by descending the or-
dered list of mappings translating the PropBank 
data for each entry according to its argument map-
pings. Then each of these incrementally larger sets 
197
of training data was then used to build a semantic 
role labeler as described in section 3. The perform-
ance of each of the resulting labelers was then 
evaluated by applying it to all of the test data 
available for target roleset in PropBank, using the 
same scoring methods described in section 3. The 
performance scores for each labeler were recorded 
along with the total number of surrogate training 
examples used to build the labeler. 
Figure 3 presents the performance result of our 
semantic role labeling approach using various 
amounts of surrogate training data. Along with 
precision, recall, and F-score data, Figure 3 also 
graphs the percentage of PropBank rolesets for 
which a given amount of training data had been 
identified using our approach, of the 2,858 rolesets 
with at least one fully-annotated training example. 
For instance, with 120 surrogate annotations our 
system achieves an F-score above 0.5, and we 
identified this much surrogate training data for 
96% of PropBank rolesets with at least one fully-
annotated sentence. This represents 64% of all 
PropBank rolesets that are used for annotation. 
Beyond 120 surrogate training examples, F-scores 
remain around 0.6 before slowly declining after 
around 700 examples. 
 
 
Figure 3. Performance of our semantic role label-
ing approach on PropBank rolesets using various 
amounts of surrogate training data 
  
Several interesting comparisons can be made be-
tween the results presented in Figure 3 and those in 
Figure 2, where actual PropBank training data is 
used instead of surrogate training data. First, the 
precision obtained with surrogate training data is 
roughly 10% lower than with real data. Second, the 
recall performance of surrogate data performs 
similar to real data at first, but is consistently 10% 
lower than with real data after the first 50 training 
examples. Accordingly, F-scores for surrogate 
training data are 10% lower overall.  
Even though the performance obtained using 
surrogate training data is less than with actual data, 
there is abundant amounts of it available for most 
PropBank rolesets. Comparing the ?% of rolesets? 
plots in Figures 2 and 3, the real value of surrogate 
training data is apparent. Figure 2 suggests that 
over 20 real training examples are needed to 
achieve F-scores that are consistently above 0.5, 
but that less than 20% of PropBank rolesets have 
this much data available. In contrast, 64% of all 
PropBank rolesets can achieve this F-score per-
formance with the use of surrogate training data. 
This percentage increases to 96% if every Prop-
Bank roleset is given at least one fully annotated 
sentence, where all of its numbered arguments are 
assigned to constituents.  
In addition to supplementing the real training 
data available for existing PropBank rolesets, these 
results predict the labeling performance that can be 
obtained by applying this technique to a novel 
roleset with one fully-annotated training example, 
e.g. for the verb yearn. Using the first 120 surro-
gate training examples and our simple semantic 
role labeling approach, we would expect F-scores 
that are above 0.5, and that using the first 700 
would yield F-scores around 0.6. 
7 Discussion 
The overall performance of our semantic role la-
beling approach is not competitive with leading 
contemporary systems, which typically employ 
support vector machine learning algorithms with 
syntactic features (Pradhan et al, 2005) or syntac-
tic tree kernels (Moschitti et al, 2006). However, 
our work highlights a number of characteristics of 
the semantic role labeling task that will be helpful 
in improving performance in future systems. Parse 
tree paths features can be used to achieve high pre-
cision in semantic role labeling, but much of this 
precision may be specific to individual verbs. By 
generalizing parse tree path features only across 
syntactically similar verbs, we have shown that the 
drop in precision can be limited to roughly 10%. 
The approach that we describe in this paper is 
not dependent on the use of PropBank rolesets; any 
large corpus of semantic role annotations could be 
198
generalized in this manner. In particular, our ap-
proach would be applicable to corpora with frame-
specific role labels, e.g. FrameNet (Baker et al, 
1998). Likewise, our approach to generalizing 
parse tree path feature across syntactically similar 
verbs may improve the performance of automated 
semantic role labeling systems based on FrameNet 
data. Our work suggests that feature generalization 
based on verb-similarity may compliment ap-
proaches to generalization based on role-similarity 
(Gildea and Jurafsky, 2002; Baldewein et al, 
2004). 
There are a number of improvements that could 
be made to the approach described in this paper. 
Enhancements to the simple semantic role labeling 
algorithm would improve the alignment of argu-
ments across rolesets, which would help align role-
sets with greater syntactic similarity, as well as 
improve the performance obtained using the surro-
gate training data in assigning semantic roles.  
This research raises many questions about the 
relationship between syntactic context and verb 
semantics. An important area for future research 
will be to explore the correlation between our dis-
tance metric for syntactic similarity and various 
quantitative measures of semantic similarity 
(Pedersen, et al, 2004). Particularly interesting 
would be to explore whether different senses of a 
given verb exhibited markedly different profiles of 
syntactic context. A strong syntactic/semantic cor-
relation would suggest that further gains in the use 
of surrogate annotation data could be gained if syn-
tactic similarity was computed between rolesets 
rather than their verbs. However, this would first 
require accurate word-sense disambiguation both 
for the test sentences as well as for the parsed cor-
pora used to calculate parse tree path frequencies. 
Alternatively, parse tree path profiles associated 
with rolesets may be useful for word sense disam-
biguation, where the probability of a sense is com-
puted as the likelihood that an ambiguous verb's 
parse tree paths are sampled from the distributions 
associated with each verb sense. These topics will 
be the focus of our future work in this area. 
Acknowledgments 
The project or effort depicted was or is sponsored 
by the U.S. Army Research, Development, and 
Engineering Command (RDECOM), and that the 
content or information does not necessarily reflect 
the position or the policy of the Government, and 
no official endorsement should be inferred. 
References  
Baker, C., Fillmore, C., and Lowe, J. 1998. The Ber-
keley FrameNet Project, In Proceedings of COLING-
ACL, Montreal. 
Baldewein, U., Erk, K., Pado, S., and Prescher, D. 2004. 
Semantic role labeling with similarity-based gener-
alization using EM-based clustering. Proceedings of 
Senseval-3, Barcelona. 
Charniak, E. 2000. A maximum-entropy-inspired 
parser, Proceedings NAACL-ANLP, Seattle. 
Courtois, B. 2004. Dictionnaires ?lectroniques DELAF 
anglais et fran?ais. In C. Lecl?re, E. Laporte, M. Piot 
and M. Silberztein (eds.) Syntax, Lexis and Lexicon-
Grammar: Papers in Honour of Maurice Gross. Am-
sterdam: John Benjamins. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics 28:3, 
245-288. 
Joanis, E. and Stevenson, S. 2003. A general feature 
space for automatic verb classification. Proceedings 
EACL, Budapest. 
Levin, B. 1993. English Verb Classes and Alterna-tions: 
A Preliminary Investigation. Chicago, IL: University 
of Chicago Press. 
Lin, D. 1998. Automatic Retrieval and Clustering of 
Similar Words. COLING-ACL, Montreal. 
Linguistic Data Consortium. 2003. English Gigaword. 
Catalog number LDC2003T05. Available from LDC 
at http://www.ldc.upenn.edu. 
Moschitti, A., Pighin, D. and Basili, R. 2006. Semantic 
Role Labeling via Tree Kernel joint inference. Pro-
ceedings of CoNLL, New York. 
Palmer, M., Gildea, D., and Kingsbury, P. 2005. The 
Proposition Bank: An Annotated Corpus of Semantic 
Roles. Computational Linguistics 31(1):71-106. 
Pedersen, T., Patwardhan, S. and Michelizzi, J. 2004. 
WordNet::Similarity - Measuring the Relatedness of 
Concepts. Proceedings NAACL-04, Boston, MA. 
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., and 
Jurafsky, D. 2005. Semantic role labeling using dif-
ferent syntactic views. Proceedings ACL-2005, Ann 
Arbor, MI. 
Reynar, J. and Ratnaparkhi, A. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries. 
Proceedings of ANLP, Washington, D.C. 
199
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 369?379,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Unsupervised Induction of Contingent Event Pairs from Film Scenes
Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina, Reid Swanson and Marilyn A. Walker
Natural Language and Dialogue Systems Lab
Department of Computer Science, University of California, Santa Cruz
Santa Cruz, CA, 95064
{zhu, elahe, mlarissa, reid, maw}@soe.ucsc.edu
Abstract
Human engagement in narrative is partially
driven by reasoning about discourse relations
between narrative events, and the expectations
about what is likely to happen next that results
from such reasoning. Researchers in NLP
have tackled modeling such expectations from
a range of perspectives, including treating it as
the inference of the CONTINGENT discourse
relation, or as a type of common-sense causal
reasoning. Our approach is to model likeli-
hood between events by drawing on several of
these lines of previous work. We implement
and evaluate different unsupervised methods
for learning event pairs that are likely to be
CONTINGENT on one another. We refine event
pairs that we learn from a corpus of film scene
descriptions utilizing web search counts, and
evaluate our results by collecting human judg-
ments of contingency. Our results indicate that
the use of web search counts increases the av-
erage accuracy of our best method to 85.64%
over a baseline of 50%, as compared to an av-
erage accuracy of 75.15% without web search.
1 Introduction
Human engagement in narrative is partially driven
by reasoning about discourse relations between nar-
rative events, and the expectations about what is
likely to happen next that results from such reason-
ing (Gerrig, 1993; Graesser et al, 1994; Lehnert,
1981; Goyal et al, 2010). Thus discourse relations
are one of the primary means to structure narrative
in genres as diverse as weblogs, search queries, sto-
ries, film scripts and news articles (Chambers and
Jurafsky, 2009; Manshadi et al, 2008; Gordon and
Swanson, 2009; Gordon et al, 2011; Beamer and
Girju, 2009; Riaz and Girju, 2010; Do et al, 2011).
DOUGLAS QUAIL and his wife KRISTEN, are
asleep in bed.
Gradually the room lights brighten. the clock chimes
and begins speaking in a soft, feminine voice.
They don?t budge. Shortly, the clock chimes again.
Quail?s wife stirs. Maddeningly, the clock chimes a
third time.
CLOCK (continuing)Tick, tock ?.
Quail reaches out and shuts the clock off. Then he sits
up in bed.
He swings his legs out from under the covers and sits
on the edge of the bed. He puts on his glasses and sits,
lost in thought.
He is a good-looking but conventional man in his early
thirties. He seems rather in awe of his wife, who is
attractive and rather off-hand towards him.
Kirsten pulls on her robe, lights a cigarette, sits fishing
for her slippers.
Figure 1: Opening Scene from Total Recall
Recent work in NLP has tackled the inference of
relations between events from a broad range of per-
spectives: (1) as inference of a discourse relations
(e.g. the Penn Discourse Treebank (PDTB) CON-
TINGENT relation and its specializations); (2) as a
type of common sense reasoning; (3) as part of text
understanding to support question-answering; and
(4) as way of learning script-like or plot-like knowl-
edge structures. All these lines of work aim to model
narrative understanding, i.e. to enable systems to in-
fer which events are likely to have happened even
though they have not been mentioned in the text
(Schank et al, 1977), and which events are likely
to happen in the future. Such knowledge has prac-
tical applications in commonsense reasoning, infor-
369
mation retrieval, question answering, narrative un-
derstanding and inferring discourse relations.
We model this likelihood between events by
drawing on the PTDB?s general definition of the
CONTINGENT relation, which encapsulates relations
elsewhere called CAUSE, CONDITION and ENABLE-
MENT (Prasad et al, 2008a; Lin et al, 2010; Pitler et
al., 2009; Louis et al, 2010). Our aim in this paper
is to implement and evaluate a range of different un-
supervised methods for learning event pairs that are
likely to be CONTINGENT on one another.
We first utilize a corpus of scene descriptions
from films because they are guaranteed to have an
explicit narrative structure. Moreover, screenplay
scene descriptions tend to be told in temporal or-
der (Beamer and Girju, 2009; Gordon and Swan-
son, 2009), which makes them a good resource for
learning about contingencies between events. In
addition, scenes in film represent many typical se-
quences from real life, while providing a rich source
of event clusters related to battles, love and mys-
tery. We carry out separate experiments for the ac-
tion movie genre and the romance movie genre. For
example, in the scene from Total Recall, from the
action movie genre (See Fig. 1), we might learn that
the event of sits up is CONTINGENT on the event
of clock chimes. The subset of the corpus we
use comprises 123,869 total unique event pairs.
We produce initial scalar estimates of poten-
tial CONTINGENCY between events using four
previously defined measures of distributional co-
occurrence. We then refine these estimates through
web searches that explicitly model the patterns of
narrative event sequences that were previously ob-
served to be likely within a particular genre. There
are several advantages of this method: (1) events in
the same genre tend to be more similar than events
across genres, so less data is needed to estimate
co-occurrence; (2) film scenes are typically nar-
rated via simple tenses in the correct temporal order,
which allows the ordering of events to contribute to
the estimation of the CONTINGENCY relation; (3)
The web counts focus on validating event pairs al-
ready deemed to be likely to be CONTINGENT in
the smaller, more controlled, film scene corpus. To
test our method, we conduct perceptual experiments
with human subjects on Mechanical Turk by asking
them to select which of two pairs of events are the
most likely. For example, given the scene from To-
tal Recall in Fig. 1, Mechanical Turkers are asked
to select whether the sequential event pair clock
chimes, sits up is more likely than clock
chimes followed by a randomly selected event
from the action film genre. Our experimental data
and annotations are available at http://nlds.
soe.ucsc.edu/data/EventPairs.
Sec. 2 describes our experimental method in de-
tail. Sec. 3 describes how we set up our evaluation
experiments and the results. We show that none of
the methods from previous work perform better on
our data than 75.15% average accuracy as measured
by human perceptions of CONTINGENCY. But after
web search refinement, we achieve an average accu-
racy of 85.64%. We delay a more detailed compari-
son to previous work to Sec. 4 where we summarize
our results and compare previous work to our own.
2 Experimental Method
Our method uses a combination of estimating
the likelihood of a CONTINGENT relation between
events in a corpus of film scenes (Walker et al,
2012b), with estimates then revised through web
search. Our experiments are based on two sub-
sets of 862 film screen plays collected from the
IMSDb website using its ontology of film genres
(Walker et al, 2012b): a set of action movies of 115
screenplays totalling 748 MB, and a set of romance
movies of 71 screenplays totalling 390 MB. Fig. 1
provided an example scene from the action movie
genre from the IMSDb corpus.
We assume that the relation we are aiming to learn
is the PDTB CONTINGENT relation, which is de-
fined as a relation that exists when one of the sit-
uations described in the text spans that are identi-
fied as the two arguments of the relation, i.e. Arg1
and Arg2, causally influences the other (Prasad et
al., 2008b). As Girju notes, it is notoriously dif-
ficult to define causality without making the defi-
nition circular, but we follow Beamer and Girju?s
work in assuming that if events A, B are causally
related then B should occur less frequently when it
is not preceded by A and that B?A should be much
less frequent than A? B. We assume that both the
CAUSE and CONDITION subtypes of the CONTIN-
GENCY relation will result in pairs of events that are
likely to occur together and in a particular order. In
particular we assume that the subtypes of the PDTB
taxonomy of Contingency.Cause.Reason and Con-
tingency.Cause.Result are the most likely to occur
together as noted in previous work. Other related
370
work has made use of discourse connectives or dis-
course taggers (implicit discourse relations) to pro-
vide additional evidence of CONTINGENCY (Do et
al., 2011; Gordon et al, 2011; Chiarcos, 2012; Pitler
et al, 2009; Lin et al, 2010), but we do not because
the results have been mixed. In particular these dis-
course taggers are trained on The Wall Street Journal
(WSJ) and are unlikely to work well on our data.
We define an event as a verb lemma with its sub-
ject and object. Two events are considered equal if
they have the same verb. We do not believe word
ambiguities to be a primary concern, and previous
work also defines events to be the same if they have
the same surface verb, in some cases with a restric-
tion that the dependency relations should also be
the same (Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Do et al, 2011; Riaz and Girju,
2010; Manshadi et al, 2008). Word sense ambigu-
ities are also reduced in specific genres (Action and
Romance) of film scenes.
Our method for estimating the likelihood of a
CONTINGENT relations between events consists of
four steps:
1. TEXT PROCESSING: We use Stanford
CoreNLP to annotate the corpus docu-
ment by document and store the annotated text
in XML format (Sec. 2.1);
2. COMPUTE EVENT REPRESENTATIONS: We
form intermediate artifacts such as events, pro-
tagonists and event pairs from the annotated
documents. Each event has its arguments (sub-
ject and object). We calculate the frequency of
the event across the relevant genre (Sec. 2.2);
3. CALCULATE CONTINGENCY MEASURES: We
define 4 different measures of contingency and
calculate each one separately using the results
from Steps 1 and 2 above. We call each re-
sult a PREDICTED CONTINGENT EVENT PAIR
(PCEP). All measures return scalar values that
we use to rank the PCEPs (Sec. 2.3);
4. WEB SEARCH REFINEMENT: We select the top
100 event pairs calculated by each contingency
measure, and construct a RANDOM EVENT
PAIR (REP) for each PCEP that preserves the
first element of the PCEP, and replaces the sec-
ond element with another event selected ran-
domly from within the same genre. We then
define web search patterns for both PCEP and
REPs and compare the counts (Sec. 2.4).
2.1 Text Processing
We first separate our screen plays into two sets of
documents, one for the action genre and one for the
romance genre. Because we are interested in the
event descriptions that are part of the scene descrip-
tions, we excise the dialog from each screen play.
Then using the Stanford CoreNLP pipeline, we an-
notate the film scene files. Annotations include tok-
enization, lemmatization, named entity recognition,
parsing and coreference resolution.
We extract the events by keeping all tokens whose
POS tags begin with VB. We then use the depen-
dency parse to find the subject and object of each
verb (if any), considering only nsubj, agent,
dobj, iobj, nsubjpass. We keep the orig-
inal tokens of the subject and the object for further
processing.
2.2 Compute Event Representations
Given the results of Step 1 we start by generalizing
the subject and object stored with each event by sub-
stituting tokens with named entities if there are any
named entities tagged. Otherwise we generalize the
subjects and the objects using their lemmas. For ex-
ample, person UNLOCK door, as illustrated in
Table 1.
We then integrate all the subjects and objects
across all film scene files, keeping a record of the
frequency of each subject and object. For example,
[person (115), organization (14),
door (3)] UNLOCK [door (127),
person (5), bars (2)]. The most frequent
subject and object are selected as representative ar-
guments for the event. We then count the frequency
of each event across all the film scene files.
Within each film scene file, we count adjacent
events as potential CONTINGENT event pairs. Two
event pairs are defined as equal if they have the same
verbs in the same order. We also count the frequency
of each event pair.
2.3 Calculate Contingency Measures
We calculate four different measures of CONTIN-
GENCY based on previous work using the results
of Steps 1 and 2 (Sec. 2.1 and Sec. 2.2). These
measures are pointwise mutual information, causal
potential, bigram probability and protagonist-based
371
causal potential as described in detail below. We
calculate each measure separately by genre for the
action and romance genres of the film corpus.
Pointwise Mutual Information. The majority of
related work uses pointwise mutual information
(PMI) in some form or another (Chambers and Ju-
rafsky, 2008; Chambers and Jurafsky, 2009; Riaz
and Girju, 2010; Do et al, 2011). Given a set of
events (a verb and its collected set of subjects and
objects), we calculate the PMI using the standard
definition:
pmi(e1, e2) = log
P (e1, e2)
P (e1)P (e2)
(1)
in which e1 and e2 are two events. P (e1) is the
probability that event e1 occur in the corpus:
P (e1) =
count(e1)
?
x count(ex)
(2)
where count(e1) is the count of how many times
event e1 occurs in the corpus, and
?
x count(ex) is
the count of all the events in the corpus. The nu-
merator is the probability that the two events occur
together in the corpus:
P (e1, e2) =
count(e1, e2)
?
x
?
y count(ex, ey)
(3)
in which count(e1, e2) is the number of times the
two events e1 and e2 occur together in the corpus
regardless of their order. Only adjacent events in
each document are paired up. PMI is a symmetric
measurement for the relationship between two
events. The order of the events does not matter.
Causal Potential. Beamer and Girju proposed a
measure called Causal Potential (CP) based on pre-
vious work in philosophy and logic, along with an
annotation test for causality. An annotator decid-
ing whether event A causes event B asks herself the
following questions, where answering yes to both
means the two events are causally related:
? Does event A occur before (or simultaneously)
with event B?
? Keeping constant as many other states of affairs
of the world in the given text context as possi-
ble, does modifying event A entail predictably
modifying event B?
As Beamer & Girju note, this annotation test is
objective, and it is simple to execute mentally. It
only assumes that the average person knows a lot
about how things work in the world and can reliably
answer these questions. CP is then defined below,
where the arrow notation means ordered bigrams,
i.e. event e1 occurs before event e2:
?(e1, e2) = pmi(e1, e2) + log
P (e1 ? e2)
P (e2 ? e1)
(4)
where pmi(e1, e2) = log
P (e1, e2)
P (e1)P (e2)
The causal potential consists of two terms: the
first is pair-wise mutual information (PMI) and
the second is relative ordering of bigrams. PMI
measures how often events occur as a pair; whereas
relative ordering counts how often event order
occurs in the bigram. If there is no ordering of
events, the relative ordering is zero. We smooth
unseen event pairs by setting their frequency equal
to 1 to avoid zero probabilities. For CP as with PMI,
we restrict these calculations to adjacent events.
Column CP of Table 1 below provides sample
values for the CP measure.
Probabilistic Language Models. Our third method
models event sequences using statistical language
models (Manshadi et al, 2008). A language model
estimates the probability of a sequence of words us-
ing a sample corpus. To identify contingent event
sequences, we apply a bigram model which esti-
mates the probability of observing the sequence of
two words w1 and w2 as follows:
P (w1, w2) ?= P (w2|w1) =
count(w1, w2)
count(w1)
(5)
Here, the words are events. Each verb is a single
event and each film scene is treated as a sequence of
verbs. For example, consider the following sentence
from Total Recall:
Quail and Kirsten sit at a small table,
eating breakfast.
This sentence is represented as the sequence of its
two verbs: sit, eat. We estimate the probability
of verb bigrams using Equation 5 and hypothesize
that the verb sequences with higher probability are
372
Row # Causal Potential Pair CP PCEP Search pat-
tern
NumHits Random Pair REP Search pat-
tern
NumHits
1 person KNOW person -
person MEAN what
2.18 he knows * means 415M person KNOW person -
person PEDDLE papers
he knows * ped-
dles
2
2 person COME - person
REST head
2.12 he comes * rests 158M person COME - person
GLANCE window
he comes *
glances
41
3 person SLAM person -
person SHUT door
2.11 he slams * shuts 11 person SLAM person -
person CHUCKLE
he slams * chuck-
les
0
4 person UNLOCK door -
person ENTER room
2.11 he unlocks * en-
ters
80 person UNLOCK door -
person ACT shot
he unlocks * acts 0
5 person SLOW person -
person STOP person
2.10 he slows * stops 697K person SLOW person -
eyes RIVET eyes
he slows * rivets 0
6 person LOOK window -
person WONDER thing
2.06 he looks * won-
ders
342M person LOOK window -
person EDGE hardness
he looks * edges 98
7 person TAKE person -
person LOOK window
2.01 he takes * looks 163M person TAKE person -
person CATCH person
he takes * catches 311M
8 person MANAGE smile -
person GET person
2.01 he manages * gets 80M person MANAGE smile
- person APPROACH
person
he manages * ap-
proaches
16
9 person DIVE escape -
person SWIM way
2.00 he dives * swims 1.5M person DIVE escape -
gun JAM person
he dives * jams 6
10 person STAGGER person
- person DROP person
2.00 he staggers *
drops
33 person STAGGER per-
son - plain WHEEL per-
son
he staggers *
wheels
1
11 person SHOOT person -
person FALL feet
1.99 he shoots * falls 55.7M person SHOOT person -
person PREVENT per-
son
he shoots * pre-
vents
6
12 person SQUEEZE person
- person SHUT door
1.87 he squeezes *
shuts
5 person SQUEEZE per-
son - person MARK per-
son
he squeezes *
marks
1
13 person SEE person - per-
son GO
1.87 he sees * goes 184M person SEE person - im-
age QUIVER hips
he sees * quivers 2
Table 1: Sample web search patterns and values used in web search refinement algorithm from action genre
more likely to be contingent. We apply a threshold
of 20 for count(w1, w2) to avoid infrequent and
uncommon bigrams.
Protagonist-based Models. We also used a method
of generating event pairs based not only on the con-
secutive events in text but on their protagonist. This
is based on the assumption that the agent, or protag-
onist, will tend to perform actions that further her
own goals, and are thus causally related. We called
this method protagonist-based because all events
were partitioned into multiple sets where each set of
events has one protagonist. This method is roughly
based on previous work using chains of discourse
entities to induce narrative schemas (Chambers and
Jurafsky, 2009).
Events that share one protagonist were extracted
from text according to co-referring mentions pro-
vided by the Stanford CoreNLP toolkit.1 A man-
ual examination of coreference results on a sample
of movie scripts suggests that the accuracy is only
around 60%: most of the time the same entity (in its
1http://nlp.stanford.edu/software/corenlp.shtml
nominal and pronominal forms) was not recognized
and was assigned as a new entity.
We preserve the order of events based on their tex-
tual order assuming as above that film scripts tend
to preserve temporal order. An ordered event pair
is generated if both events share a protagonist. We
further filter event pairs by eliminating those whose
frequency is less than 5 to filter insignificant and rare
event pairs. This also tends to catch errors generated
by the Stanford parser.
CP was then calculated accordingly to Equa-
tion 4. To calculate the PMI part of CP, we combine
the frequencies of event pairs in both orders.
2.4 Web Search Refinement
The final step of our method is WEB SEARCH RE-
FINEMENT. Our hypothesis is that using the film
corpus within a particular genre to do the initial esti-
mates of contingency takes advantage of genre prop-
erties such as similar events and narration of scenes
in chronological order. However the film corpus is
necessarily small, and we can augment the evidence
for a particular contingent relation by defining spe-
cific narrative sequence patterns and collecting web
373
counts.
Recall that PCEP stands for predicted contin-
gent event pair and that REP stands for random
event pair. We first select the top 100 event pairs
calculated by each CONTINGENCY measure, and
construct a RANDOM EVENT PAIR (REP) for each
PCEP that preserves the first element of the PCEP,
and replaces the second element with another event
selected randomly from within the same genre. We
then define web search patterns for both PCEP and
REPs and compare the counts. PCEPs should be fre-
quent in web search and REPs should be infrequent.
Our web refinement procedure is:
? For each event pair, PCEPs and REPs, create a
Google search pattern as illustrated by Table 1,
and described in more detail below.
? Search for the exact match in Google gen-
eral web search using incognito browsing and
record the estimated count of results returned;
? Remove all the PCEP/REP pairs with PCEP
Google search count less than 100: highly con-
tingent events should be frequent in a general
web search;
? Remove all PCEP/REP pairs with REP Google
search count greater than 100: events that are
not contingent on one another should not be
frequent in a general web search.
The motivation for this step is to provide addi-
tional evidence for or against the contingency of a
pair of events. Table 1 shows a selection of the top
100 PCEPs learned using the Causal potential (CP)
Metric, the web search patterns that are automati-
cally derived from the PCEPs (Column 4), the REPs
that were constructed for each PCEP (Column 6),
the web search patterns that were automatically de-
rived from the REPs (Column 7). Column 5 shows
the results of web search hits for the PCEP patterns
and Column 8 shows the results of web search hits
for the REP patterns. These hit counts were then
used in refining our estimates of CONTINGENCY for
the learned patterns as described above.
Note that the web search patterns do not aim to
find every possible match of the targeted CONTIN-
GENT relation that could possibly occur. Instead,
they are generalizations of the instances of PCEPs
that we found in the films corpus that are targeted at
finding hits that are the most likely to occur in nar-
rative sequences. Narrative sequences are most re-
liably signalled by use of the historical present tense,
e.g. as instantiated in the search patterns He knows
in Row 1 and He comes in Row 2 of Table 1 (Swan-
son and Gordon, 2012; Beamer and Girju, 2009;
Labov and Waletzky, 1997). In addition, we use
the ?*? operator in Google Search to limit search
to pairs of events reported in the historical present
tense, that are ?near? one another, and in a particular
sequence. We don?t care whether the events are in
the same utterance or in sequential utterances, thus
for the second verb (event) we do not include a sub-
ject pronoun he. These search patterns are not in-
tended to match the original instances in the film
corpus and in general they are unlikely to match
those instances.
For example, consider the search patterns and
results shown in Row 1 of Table 1. The
PCEP is (person KNOW person, person
MEAN what). The REP is (person KNOW
person, person PEDDLE papers). Our
prediction is that the REP should be much less likely
in web search counts and the results validate that
predication. A paired t-test over the 100 top PCEP
pairs for the CP measure comparing the hit counts
for the PCEP pairs vs. the REP pairs was highly
significant (p < .00001). However, consider Row
7. Even though in general the PCEP pairs are more
likely (as measured by the paired t-test compar-
ing web search counts for PCEPs vs REPs), there
are cases where the REP is highly likely as shown
by the REP (person take person, person
CATCH person) in Row 7. Alternatively there are
cases where the web search counts provide evidence
against one of the PCEPs. Consider Rows 3, 4, 10
and 12. In all of these cases the web counts NumHits
for the PCEP number in the tens.
After the web search refinement, we retain the
PCEP/REP pairs with initially high PCEP estimates,
for which we found good evidence for contingency
and for randomness, e.g. Row 1 and 2 in Table 1.
We use 100 as a threshold because most of the time
the estimate result count from Google is either a
very large number (millions) or a very small num-
ber (tens), as illustrated by the NumHits columns in
Table 1.
We experimented with different types of patterns
with a development set of PCEPs before we settled
on the search pattern template shown in Table 1. We
374
decided to use third person rather than first person
patterns, because first person patterns are only one
type of narrative (Swanson and Gordon, 2012). We
also decided to utilize event patterns without typical
objects, such as head in person REST head in
Row 2 of Table 1. We do not have any evidence that
this is the optimal search pattern template because
we did not systematically try other types of search
patterns.
3 Evaluation and Results
While other work uses a range of methods for evalu-
ating accuracy, to our knowledge our work is the first
to use human judgments from Mechanical Turk to
evaluate the accuracy of the learned PCEPs. We first
describe the evaluation setup in Sec. 3.1 and then re-
port the results in Sec. 3.2
3.1 Mechanical Turk Contingent Pair
Evaluations
We used three different types of HITs (Human Intel-
ligence Tasks) on Mechanical Turk for our evalua-
tion. Two of the HITS are in Fig. 2 and Fig. 3. The
differences in the different types of HITS involve:
(1) whether the arguments of events were given in
the HIT, as in Fig. 2 and (2): whether the Turkers
were told that the order of the events mattered, as
in Fig. 3. We initially thought that providing the
arguments to the events as shown in Fig. 2 would
help Turkers to reason about which event was more
likely. We tested this hypothesis only in the action
genre for the Causal Potential Measure. For CP, Bi-
gram and Protag the order of events always matters.
For the PMI task, the order of the events doesn?t
matter because PMI is a symmetric measure. Fig. 2
illustrates the instructions that were given with the
HIT when the event order doesn?t matter. In all the
other cases, the instructions that were given with the
HIT are those in Fig. 3 where the Turkers are in-
structed to pay attention to the order of the events.
For all types of HITS, for all measures of CON-
TINGENCY, we set up the task as a choice over
two alternatives, where for each predicted contin-
gent pair (PCEP), we generate a random event pair
(REP), with the first event the same and the second
one randomly chosen from all the events in the same
film genre. The REPs are constructed the same way
as we construct REPs for web search refinement,
as illustrated by Table 1. This is illustrated in both
Fig. 2 and Fig. 3. For all types of HITS, we ask 15
Turkers from a pre-qualified group to select which
pair (the PCEP or the REP) is more likely to oc-
cur together. Thus, the framing of these Mechani-
cal Turk tasks only assumes that the average person
knows how the world works; we do not ask them to
explicitly reason about causality as other work does
(Beamer and Girju, 2009; Gordon et al, 2011; Do et
al., 2011).
For each measure of CONTINGENCY, we take 100
event pairs with highest PCEP scores, and put them
in 5 HITs with twenty items per HIT. Previous work
has shown that for many common NLP tasks, 7
Turkers? average score can match expert annotations
(Snow et al, 2008). However, we use 15 Turkers
because we had no gold-standard data and because
we were not sure how difficult the task is. It is
clearly subjective. To calculate the accuracy of each
method, we computed the average correlation coef-
ficient between each pair of raters and eliminate the
5 lowest scoring workers. We then used the percep-
tions of the 10 remaining workers to calculate accu-
racy as # of correct answers / total # of answers.
In general, deciding when a MTurk worker is un-
reliable when the data is subjective is a difficult
problem. In the future we plan to test other solu-
tions to measuring annotator reliability as proposed
in related work (Callison-Burch, 2009; Snow et al,
2008; Karger et al, 2011; Dawid and Skene, 1979;
Welinder et al, 2010; Liu et al, 2012).
3.2 Results
We report our results in terms of overall accuracy.
Because the Mechanical Turk task is a choose-
one question rather than a binary classification,
Precision = Recall in our experimental results:
True Positive = Number of Correct Answers
True Negative = Number of Correct Answers
False Positive = Number of Incorrect Answers
False Positive = Number of Incorrect Answers
Precision =
True Positive
True Positive + False Positive
Recall =
True Positive
True Positive + False Negative
The accuracies of all the methods are shown in
Table 2. The results of using event arguments
(person KNOW person) in the Mechanical Turk
evaluation task (i.e. Fig. 2) is given in Rows 1 and
2 of Table 2. The accuracies for Rows 1 and 2 are
375
Figure 2: Mechanical Turk HIT with event arguments provided. This HIT also illustrates instructions where Turkers
are told that the order of the events does not matter.
Row
#
Contingency Estimation Method Action
Acc%
Romance
Acc%
Average
Acc%
1 CP with event arguments 69.30 NA 69.30
2 CP with event arguments + Web search 77.57 NA 77.57
3 CP no args 75.20 75.10 75.15
4 CP no args +Web Search 87.67 83.61 85.64
5 PMI no args 68.70 79.60 74.15
6 PMI no args +Web Search 72.11 88.52 80.32
7 Bigram no args 67.10 66.50 66.80
8 Bigram no args +Web Search 72.40 70.14 71.27
9 Protag CP no args 65.40 68.20 66.80
10 Protag CP no args +Web Search 76.59 64.10 70.35
Table 2: Evaluation results for the top 100 event pairs using all methods.
considerably lower than when the PCEPs are tested
without arguments. Comparing Rows 1 and 2 with
Rows 3 and 4 suggests that even if the arguments
provide extra information that help to ground the
type of event, in some cases these constraints on
events may mislead the Turkers or make the eval-
uation task more difficult. There is an over 10% in-
crease in CP + Web search accuracy for the task that
omits the event arguments (i.e. Fig. 3 as can be seen
by comparing Row 2 with Row 4. Thus omitting the
arguments of events in evaluations actually appears
to allow Turkers to make better judgments.
In addition, Table 2 shows clearly that for ev-
ery single method, accuracy is improved by refin-
ing the initial estimates of contingency using the
narrative-based web search patterns. Web search in-
376
Figure 3: Mechanical Turk HIT for evaluation with no event arguments provided. This HIT also illustrates instructions
where Turkers are told that the order of the events does matter.
creases the accuracy of almost all evaluation tasks,
with increases ranging from 3.45% to 12.5% when
averaged over both film genres (Column 5 Average
Acc%). The best performing method for the Ac-
tion genre is CP+Web Search at 87.67%, while the
best performing method for the Romance genre is
PMI+Web search at 88.52%. However PMI+Web
Search does not beat CP+Web Search on average
over both genres we tested, even though the Me-
chanical Turk HIT for CP specifies that the order of
the events matters: a more stringent criterion. Also
overall the CP+WebSearch method achieves a very
high 85.64% average accuracy.
It is also interesting to note the variation across
the different methods. For example, while it is
well known that PMI typically requires very large
corpora to make good estimates, the PMI method
without web search refinement has an initially high
accuracy of 79.60% for the romance genre, while
only achieving 68.70% for action. Perhaps this dif-
ference arises because the romance genre is more
highly causal, or because situations are more struc-
tured in romance, providing better estimates with a
small corpus. However even in this case of romance
with PMI, adding web search refinement provides
an almost 10% increase in absolute accuracy to the
highest accuracy of any combination, i.e. 88.52%.
There is also an interesting case of Protag CP for
the romance genre where web search refinement ac-
tually decreases accuracy by 4.1%. In future work
we plan to examine more genres from the film cor-
pus and also examine the role of corpus size in more
detail.
4 Discussion and Future Work
We induced event pairs using several methods from
previous work with similar aims but widely different
problem formulations and evaluation methods. We
used a verb-rich film scene corpus where events are
normally narrated in temporal order. We used Me-
chanical Turk to evaluate the learned pairs of CON-
TINGENT events using human perceptions. In the
first stage drawing on previous measures of distribu-
tional co-occurrence, we achieved an overall average
accuracy of around 70%, over a 50% baseline. We
then implemented a novel method of defining narra-
tive sequence patterns using the Google Search API,
and used web counts to further refine our estimates
of the contingency of the learned event pairs. This
increased the overall average accuracy to around
77%, which is 27% above the baseline. Our results
indicate that the use of web search counts increases
the average accuracy of our Causal Potential-based
method to 85.64% as compared to an average accu-
racy of 75.15% without web search. To our knowl-
edge this is the highest accuracy achieved in tasks of
377
this kind to date.
Previous work on recognition of the PDTB CON-
TINGENT relation has used both supervised and un-
supervised learning, and evaluation typically mea-
sures precision and recall against a PDTB annotated
corpus (Do et al, 2011; Pitler et al, 2009; Zhou et
al., 2010; Chiarcos, 2012; Louis et al, 2010). We
use an unsupervised approach and measure accuracy
using human perceptions. Other work by Girju and
her students defined a measure called causal poten-
tial and then used film screen plays to learn a knowl-
edge base of causal pairs of events. They evaluate
the pairs by asking two trained human annotators to
label whether occurrences of those pairs in their cor-
pus are causally related (Beamer and Girju, 2009;
Riaz and Girju, 2010). We also make use of their
causal potential measure. Work on commonsense
causal reasoning aims to learn causal relations be-
ween pairs of events using a range of methods ap-
plied to a large corpus of weblog narratives (Gordon
et al, 2011; Gordon and Swanson, 2009; Manshadi
et al, 2008). One form of evaluation aimed to pre-
dict the last event in a sequence (Manshadi et al,
2008), while more recent work uses the learned pairs
to improve performance on the COPA SEMEVAL
task (Gordon et al, 2011).
Related work on SCRIPT LEARNING induces
likely sequences of temporally ordered events in
news, rather than CONTINGENCY or CAUSALITY
(Chambers and Jurafsky, 2008; Chambers and Ju-
rafsky, 2009). Chambers & Jurafsky also evaluate
against a corpus of existing documents, by leaving
one event out of a document (news story), and then
testing the system?s ability to predict the missing
event. To our knowledge, our method is the first
to augment distributional semantics measures from
a corpus with web search data. We are also the first
to evaluate the learned event pairs with a human per-
ceptual evaluation with native speakers.
We hypothesize that there are several advantages
to our method: (1) events in the same genre tend to
be more similar than events across genres, so less
data is needed to estimate co-occurrence; (2) film
scenes are typically narrated via simple tenses in the
correct temporal order, which allows the ordering
of events to contribute to estimates of the CONTIN-
GENCY relation; (3) The web counts focus on vali-
dating event pairs already deemed to be likely to be
CONTINGENT in the smaller, more controlled, film
scence corpus.
Our work capitalizes on event sequences narrated
in temporal order as a cue to causality. We expect
this approach to generalize to other domains where
these properties hold, such as fables, personal stories
and news articles. We do not expect this technique
to generalize without further refinements to genres
frequently told out of temporal order or when events
are not mentioned consecutively in the text, for ex-
ample in certain types of fiction.
In future work we want to explore in more de-
tail the differences in performance of the different
contingency measures. For example, previous work
would suggest that the the higher the measure is, the
more likely the two events are to be contingent on
one another. To date, while we have only tested the
top 100, we have not found that the bottom set of
20 are less accurate than the top set of 20. This
could be due to corpus size, or the measures them-
selves, or noise from parser accuracy etc. As shown
in Table 2, web search refinement is able to eliminate
most noise in event pairs, but we would still aim to
achieve a better understanding of the circumstances
which lead particular methods to work better.
In future work we also want to explore ways of in-
ducing larger event structures than event pairs, such
as the causal chains, scripts, or narrative schemas of
previous work.
Acknowledgments
We would like to thank Yan Li for setting up au-
tomatic search query. We also thank members of
NLDS for their discussions and suggestions, espe-
cially Stephanie Lukin, Rob Abbort, and Grace Lin.
References
B. Beamer and R. Girju. 2009. Using a bigram event
model to predict causal potential. In Computational
Linguistics and Intelligent Text Processing, p. 430?
441. Springer.
C. Callison-Burch. 2009. Fast, cheap, and creative: eval-
uating translation quality using amazon?s mechanical
turk. In Proc. of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 1 -
Volume 1, p. 286?295. Association for Computational
Linguistics.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. Proc. of ACL-08:
HLT, p. 789?797.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
378
Proc. of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 2-Volume 2, p. 602?610.
C. Chiarcos. 2012. Towards the unsupervised acquisi-
tion of discourse relations. In Proc. of the 50th Annual
Meeting of the Association for Computational Linguis-
tics: Short Papers-Volume 2, p. 213?217. Association
for Computational Linguistics.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries C (Applied Statistics), 28(1):20?28.
Q. X. Do, Y. S. Chan, and D. Roth. 2011. Minimally
supervised event causality identification. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 294?303. Association for Com-
putational Linguistics.
R.J. Gerrig. 1993. Experiencing narrative worlds: On
the psychological activities of reading. Yale Univ Pr.
A. Gordon and R. Swanson. 2009. Identifying personal
stories in millions of weblog entries. In Third Interna-
tional Conference on Weblogs and Social Media, Data
Challenge Workshop.
A. Gordon, Cosmin Bejan, and Kenji Sagae. 2011. Com-
monsense causal reasoning using millions of personal
stories. In Twenty-Fifth Conference on Artificial Intel-
ligence (AAAI-11).
A. Goyal, E. Riloff, and H. Daume? III. 2010. Automat-
ically producing plot unit representations for narrative
text. In Proc. of the 2010 Conference on Empirical
Methods in Natural Language Processing, p. 77?86.
Association for Computational Linguistics.
A. C. Graesser, M. Singer, and T. Trabasso. 1994. Con-
structing inferences during narrative text comprehen-
sion. Psychological review, 101(3):371.
D. R. Karger, S. Oh, and D. Shah. 2011. Iterative
learning for reliable crowdsourcing systems. In John
Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett,
Fernando C. N. Pereira, and Kilian Q. Weinberger, ed-
itors, NIPS, p. 1953?1961.
W. Labov and J. Waletzky. 1997. Narrative analysis:
Oral versions of personal experience.
W. G. Lehnert. 1981. Plot units and narrative summa-
rization. Cognitive Science, 5(4):293?331.
Z. Lin, M.-Y. Kan, and H. T Ng. 2010. A pdtb-styled
end-to-end discourse parser. In Proc. of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Q. Liu, J. Peng, and A. Ihler. 2012. Variational inference
for crowdsourcing. In Advances in Neural Information
Processing Systems 25, p. 701?709.
A. Louis, A. Joshi, R. Prasad, and A. Nenkova. 2010.
Using entity features to classify implicit relations. In
Proc. of the 11th Annual SIGdial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
M. Manshadi, R. Swanson, and A. S Gordon. 2008.
Learning a probabilistic model of event sequences
from internet weblog stories. In Proc. of the 21st
FLAIRS Conference.
E. Pitler, A. Louis, and A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in text.
In Proc. of the 47th Meeting of the Association for
Computational Linguistics.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008a. The penn discourse
treebank 2.0. In Proc. of the 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008), p. 2961?2968.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008b. The Penn Discourse
TreeBank 2.0. In Proc. of 6th International Confer-
ence on Language Resources and Evaluation (LREC
2008).
M. Riaz and R. Girju. 2010. Another look at causal-
ity: Discovering scenario-specific contingency rela-
tionships with no supervision. In Semantic Computing
(ICSC), 2010 IEEE Fourth International Conference
on, p. 361?368. IEEE.
R. Schank and R. Abelson. 1977. Scripts Plans Goals.
Lea.
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing, p. 254?263. Association for Com-
putational Linguistics.
R. Swanson and A. S. Gordon. 2012. Say anything:
Using textual case-based reasoning to enable open-
domain interactive storytelling. ACM Transactions on
Interactive Intelligent Systems (TiiS), 2(3):16.
M. A. Walker, G. Lin, and J. Sawyer. 2012b. An anno-
tated corpus of film dialogue for learning and charac-
terizing character style. In Language Resources and
Evaluation Conference, LREC2012.
P. Welinder, S. Branson, S. Belongie, and P. Perona.
2010. The multidimensional wisdom of crowds. In
Advances in Neural Information Processing Systems
23, p. 2424?2432.
Z.-M. Zhou, Y. Xu, Z.Y. Niu, M. Lan, J. Su, , and
C. L. Tan. 2010. Predicting discourse connectives for
implicit discourse relation recognition. In In Coling
2010: Posters, p. 1507?1514.
379
Proceedings of the SIGDIAL 2014 Conference, pages 171?180,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Identifying Narrative Clause Types in Personal Stories
Reid Swanson, Elahe Rahimtoroghi, Thomas Corcoran and Marilyn A. Walker
Natural Language and Dialog Systems Lab
University of California Santa Cruz
Santa Cruz, CA 95064, USA
{reid,elahe,maw}@soe.ucsc.edu, tcorcora@ucsc.edu
Abstract
This paper describes work on automatically
identifying categories of narrative clauses
in personal stories written by ordinary peo-
ple about their daily lives and experiences.
We base our approach on Labov & Walet-
zky?s theory of oral narrative which catego-
rizes narrative clauses into subtypes, such as
ORIENTATION, ACTION and EVALUATION.
We describe an experiment where we an-
notate 50 personal narratives from weblogs
and experiment with methods for achieving
higher annotation reliability. We use the re-
sulting annotated corpus to train a classi-
fier to automatically identify narrative cat-
egories, achieving a best average F-score of
.658, which rises to an F-score of .767 on
the cases with the highest annotator agree-
ment. We believe the identified narrative
structure will enable new types of compu-
tational analysis of narrative discourse.
1 Introduction
Sharing personal experiences by storytelling is
a fundamental aspect of human social behavior
(Fivush et al., 2005; Fivush and Nelson, 2004;
Habermas and Bluck, 2000; Bamberg, 2006;
Thorne, 2004; Bohanek et al., 2008; Thorne and
Nam, 2009; McLean and Thorne, 2003; Pratt and
Fiese, 2004). Humans appear to be wired to en-
gage with information that is narratively structured
(Gerrig, 1993; Bamberg, 2006; Bruner, 1991), and
telling stories provides a critical developmental and
societal function, by serving as a means to reinforce
community value systems and to define individual
identity (Thorne and Shapiro, 2011; Thorne et al.,
2007). This has led some theorists to claim that ?the
stories they tell? is the defining aspect of both indi-
viduals and cultures.
Unlike any prior time in human history, personal
narratives about many life experiences are being
told online, and are widely available in social me-
dia sources such as weblogs. A personal narrative
about an arrest is shown in Fig. 1, and one about a
protest is in Fig. 4. Narratives such as these provide
a valuable resource for learning a wealth of com-
monsense knowledge about people, the types of ac-
tivities they engage in, and the attitudes they hold.
They are also well suited to learning about causal
and temporal relationships between events because
narrative interpretation explicitly depends on the co-
herence of these relationships (Graesser et al., 1994;
Elson, 2012; Gordon et al., 2011; Hu et al., 2013).
This paper applies and tests a narrative clause la-
beling scheme to personal narratives. Our scheme
is derived from Labov & Waletzky?s (henceforth
L&W) theory of oral narrative (Labov, 1997; Labov
and Waletzky, 1967). L&W?s theory distinguishes
(1) clauses that indicate causal relationships (AC-
TION), from (2) clauses that provide traits or prop-
erties of the setting or characters (ORIENTATION),
from (3) clauses describing the story characters?
emotional reactions to the events (EVALUATION).
We adopt L&W?s theory for three reasons. First,
we believe that the narrative structure of personal
narratives posted on weblogs will be more similar
to oral narrative than they are to classical stories.
Second, we believe that any narrative discourse ty-
pology must at least distinguish ACTION, from ORI-
ENTATION, and EVALUATION. Third, personal sto-
ries found on the web are often noisy and diffi-
cult to interpret; they do not always clearly follow
well defined narrative conventions. A deep analysis
171
# Category Story Clause
1 Orientation
Now, on with this week?s story...
2 Orientation
The last month has been hectic.
3 Orientation
Turbo charged.
4 Orientation
Lot?s of work because I was learning from Tim, my partner in crime.
5 Orientation
This hasn?t been helped by the intense pressure in town due to the political transition coming to an end.
6 Orientation
This week things started alright and on schedule.
7 Action
But I managed to get myself arrested by the traffic police (rouleage) early last Wednesday.
8 Action
After yelling excessively at their outright corrupted methods
9 Action
and asking incessently for what law I actually broke,
10 Action
they managed to bring me in at the police HQ.
11 Action
I was drawing too much of a curious crowd for the authorities.
12 Action
In about half an hour at police HQ I had charmed every one around.
13 Action
I had prepared my ?gift? as they wished.
14 Evaluation
Decision witheld, they decided that I neednt to bother,
15 Evaluation
they liked me too much.
16 Evaluation
I should go free.
17 Action
I even managed to meet famous Raus, the big chief.
18 Evaluation
He was too happy to let me go when he realized I was no one.
19 Action
But then, a Major at his side noticed my Visa was expired.
20 Evaluation
Damn!
21 Orientation
My current Visa is being renewed in my other passport at Immigration?s.
22 Evaluation
Fuck.
23 Evaluation
In custody, for real.
Figure 1: An excerpt from an example story from our corpus annotated with the L&W categories.
and annotation scheme, such as the one employed
by DramaBank (Elson and McKeown, 2010; Elson,
2012) that extends theories of narrative structure and
plot units (Stein et al., 2000; Lehnert, 1981), offers
many advantages. However, acquiring this level of
analysis on user generated content, such as blog sto-
ries, is resource intensive.
Research on computational models of narrative
structure typically focus on inferring the causal and
temporal relationships between events (Goyal et al.,
2010; Chambers and Jurafsky, 2009; Riaz and Girju,
2010; Beamer and Girju, 2009; Do et al., 2011;
Manshadi et al., 2008; Gordon et al., 2011; Hu et al.,
2013). Yet L&W point out that stories are not just
about the events that occur. In fact, L&W say that
stories that are only about events are boring. Current
methods for inferring narrative structure, including
our own (Hu et al., 2013), do not distinguish event
clauses from other narrative clause types. But note
that actions only constitute about one third of the
clauses in the narratives in Fig. 1 and Fig. 4.
Sec. 2 provides more detail about L&W?s the-
ory. Sec. 3 describes the annotation experiments
and efforts to improve annotation reliability. Sec. 4
presents experiments on learning to automatically
classify L&W categories, where we examine the the
most predictive features, and the effect of annotator
agreement on classification accuracy. We achieve a
best average F-score of .658, which rises to an F-
score of .767 on the cases with the highest annotator
agreement. We analyze the types of errors the clas-
sifier makes in Sec. 5.1 and conclude in Sec. 6.
2 Labov & Waletzky?s Theory of
Narrative Discourse
L&W?s theory of oral narrative defines a story as
a series of ACTION clauses (events), of which at
least two must be temporally joined (e.g., clauses
7-13 in Fig. 1 and clauses 7-11 in Fig. 4) (Labov
and Waletzky, 1967; Labov, 1997) Stories also con-
tain ORIENTATIONS (setting the scene, describing
the characters), e.g. utterances 1-6 in Fig. 1. An
orientation clause introduces the time and place of
the events of the story, and identifies the participants
of the story and their initial behavior. To properly
understand narrative structure, orientations need to
be identified as a separate type of utterance distinct
from events. L&W define two other structural types
called ABSTRACT and CODA. The ABSTRACT is an
initial narrative clause summarizing the entire se-
quence of events. A CODA is final clause which
returns the narrative to the time of speaking, indi-
cating the end of the narrative. The CODA often pro-
vides the ?moral? of the story.
The final element of a story according to L&W
is EVALUATION, which L&W identify as essen-
tial to every story. According to L&W, evaluation
gives the reason for telling the story, or the point of
the story: without EVALUATION there is no story,
merely a boring recitation of events. L&W state
that the EVALUATION clauses may also provide in-
formation on the consequences of the events as they
172
relate to the goals and desires of the participants,
and can be used to describe the events that did not
occur, may have occurred, or could occur in the fu-
ture in the story. Clauses 14-16 and 18 in Fig. 1
provide the narrator?s evaluation of the transpiring
events as well as introducing possible but unreal-
ized alternative timelines. In theories of narrative
identity (McAdams, 2003; Thorne, 2004), evalua-
tion performs two additional functions: (1) it ex-
presses the tellers opinion and in doing so reflects
the value system of that person and their commu-
nity; (2) it constructs and maintains relations be-
tween the teller and the listener. Clauses 20 and 22
illustrate these functions where the narrator directly
reveals his affective response to the prior events.
3 Dataset
Corpus of Personal Stories. Previous work (Gor-
don and Swanson, 2009) showed that about 5% of
all weblog entries are personal stories describing an
event in the author?s daily life. They developed an
automatic classifier for identifying personal narra-
tives from a random sample of 5,000 posts from
a corpus of 44 million entries available as part of
the ICWSM 2010 dataset challenge (Burton et al.,
2009). 229 of these posts were manually labeled as
personal stories. Our experiments are based on 50
of these 229 stories.
Annotation Process. L&W?s theory applies to sub-
sentence discourse units in a narrative. It is an
open question what level of phrasal granularity is
appropriate to apply to written narratives. Here, we
treat each independent clause as the basic unit of
discourse and manually segment each story in our
dataset using this definition. This results in a collec-
tion of 1,602 independent clauses. We then divided
the 50 stories into 4 groups and annotated them in
batches among 3 annotators in order to refine our
annotation guidelines and process. This dataset is
freely available at https://nlds.soe.ucsc.edu/lw.
Previous work has applied L&W?s theory to Ae-
sop?s fables and achieved high levels of interanno-
tator agreement and extremely high machine learn-
ing accuracies (Rahimtoroghi et al., 2013). How-
ever personal narratives clearly provide a more chal-
lenging context for annotation. There was a high
level of disagreement after the initial round of an-
notation. We found at least 6 primary sources of
disagreements:
? Clauses of more than one category are common
with rising action and evaluation, e.g. a clause
containing elements of orientation, action, and
evaluation: After leaving the apartment at 6:45
AM, flying 2 hours, taking a cab to Seattle, and
then driving seven hours up to Whistler including
a border crossing, it?s safe to say that I felt pretty
much like a dick with legs.
? Actions that are implied but not explicitly stated
in the text.
? Stative descriptions of the world as a result of an
action that are not intuitively orientation.
? Stative descriptions of the world that are localized
to a specific place in the narrative, which is prob-
lematic to L&W?s definition of orientation.
? Subjective clauses in scene setting are usually ori-
entation, but are lexically similar to evaluation.
? Disambiguating the functional purpose of clauses
that describe actions, but may be intended to set
the scene as opposed to the rising action.
? Disambiguating the functional purpose of subjec-
tive language in the description of an event or
state, e.g., The gigantic tree outside my window,
The radiant blue of the sky.
After several rounds of annotation we stabilized
on a labeling scheme that hierarchically extends
the original L&W categories, along with annotation
guidelines that annotators could use to disambiguate
recurring problematic cases. The final set of ex-
tended category labels along with two reduced hi-
erarchical mappings are shown in Table 1.
STATIVE-LOCAL CONTEXT is a category for dis-
tinguishing stative descriptions of the world, that are
not intuitively orientation. For example:
? I saw the sign I expected to turn south on Hwy
138. The traffic sign pointed to Sutherlin and
Roseburg,
The clause in italics is a stative that describes the
sign seen in the previous action. It is clearly not an
action or evaluation, but is not intuitively an orien-
tation, because it is so locally dependent.
STATIVE-IMPLIED ACTIONS are clauses, which
do not explicitly mention an action or event, but im-
ply one that is necessary to maintain the causal or
temporal coherence of the remaining story. For ex-
ample: After that, we decided to walk some more.
In the context of the story it is necessary to know
that they actually did walk some more in order to
interpret the other actions described in the narrative.
Implied actions are often passive constructions that
describe a state of the world that could only be true
173
Label Set ? Labels
Extended 0.582 ? Story Orient Action Eval Local Context Implied Action Consequence
Stative 0.597 ? Story Orient Action Eval Stative Stative Stative
L&W 0.630 ? Story Orient Action Eval Orient Action Eval
Table 1: The extended L&W label categories and two reduced hierarchical mappings.
if an action had taken place. For example: We were
at the convention center in about 10 minutes.
STATIVE-CONSEQUENCE is a category that de-
scribes the state of the world that has resulted as
a consequence of an action, but does not directly
evaluate the goals, intentions or desires of the par-
ticipants. For example, clause 23 in Fig. 1.
Using this extended label set we were able to
achieve an inter-annotator agreement between the
3 annotators of 0.582 using Fleiss? ? on assign-
ing categories to clauses. We also mapped the full
set of labels to a smaller subsets to see if the finer
grained distinctions helped improve reliability on
more coarse grained labeling schemes. The ex-
tended labels we included were generally different
types of stative descriptions of the world, which
were all mapped to a single category for the Stative
label set. Finally, we mapped each extended label
to an original L&W category that we thought best
fit the original definitions. When mapping back to
these reduced label sets we were able to increase the
? to 0.597 for the stative set and 0.630 for the orig-
inal L&W categories. This result indicates that we
can achieve higher reliability by ensuring that the
annotators think carefully about particular kinds of
distinctions between different stative clauses.
Gold standard labels were selected based on a sim-
ple majority of the annotator assignments. When no
annotators agreed on a label, one of the selected la-
bels was chosen at random. Once completed there
were 424 action clauses, 702 evaluations, 26 not sto-
ries, 306 orientation, 17 stative consequences, 12
implied actions and 115 local contexts. Note that
EVALUATION and ORIENTATION clauses that would
not be distinguished from ACTION by previous work
constitute two thirds of the clauses.
4 Experiments
The triply annotated dataset described above was
used as training and test data for experiments on
learning to automatically label narrative clauses. 40
narratives were randomly selected to be used as
training and development data and the remaining
10 narratives for test data. The average story in
the training data had 29.3 clauses with the shortest
Feature Set Description
Linguistic Parts of Speech, Number of Charac-
ters in post, Average Word Length,
Unigrams, Bigrams
Lexical and Senti-
ment Categories
LIWC counts and frequencies, nega-
tion
Story Position First Clause, Last Clause, Position in
the story binned into ten story regions
Table 2: Feature Sets for L&W Classification.
story consisting of 4 and the maximum consisting
of 100. The average story in the test data had 43
clauses with the shortest story consisting of 4 and
the maximum consisting of 114.
To derive feature representations of each type of
narrative clause we started with the features pre-
sented in (Rahimtoroghi et al., 2013). We refined
these by examining L&W?s descriptions of distin-
guishing features of each category. Table 2 summa-
rizes the features we automatically extracted from
all narrative clauses in the weblogs.
First, we used the Stanford Parser to distinguish
independent and dependent clauses and kept track
separately of features that occurred in both types
of clause. This is because L&W state that the
unit of analysis should be an independent clause
with its subordinate clauses, but we felt that these
were exactly the cases that often caused difficul-
ties during annotation. However distinguishing
between the features occurring in the two clause
types would allow us to determine if and when
the features of the subordinate clause were rele-
vant or more informative for automatic classifica-
tion. Then, within both dependent and independent
clauses, we distinguished the part-of-speech of the
main verb (POS), whether the clause contained a
negation (Negate), lexical semantic categories from
LIWC (Pennebaker et al., 2001), dependency rela-
tions (DEP), lexical unigrams (STEM), and whether
the verb was one of a class of verbs that are likely to
be stative.
We also developed a set of features describing the
relative position of the clause in the story (Bin-
Position, FirstClause, LastClause), because differ-
ent story regions are often associated with different
174
Feature Gain Act Ori Eval ?
POS:IND-VBD 0.128 0.084 0.002 0.031 0.011
BinPosition0 0.076 0.017 0.042 0.014 0.003
FirstClause 0.044 0.010 0.019 0.011 0.003
POS:IND-VBZ 0.042 0.029 0.008 0.002 0.003
IND-Negate 0.040 0.025 0.000 0.013 0.002
IND-Copula 0.039 0.030 0.004 0.005 0.001
POS:IND-: 0.036 0.001 0.000 0.002 0.033
IND-FirstPerson 0.035 0.017 0.004 0.002 0.013
IND-LIWC Motion 0.034 0.021 0.003 0.006 0.004
POS:IND-VBP 0.033 0.023 0.001 0.007 0.002
Table 3: The 10 most highly correlated features with
each label and cumulatively over all the labels using
mutual information and information gain.
clause types. For example, in Fig. 1 and Fig. 4, the
beginning of the story contains more ORIENTATION
clauses, while ACTION clauses are concentrated in
the middle of the story. The EVALUATION clauses
typically occur part-way through the story where
they provide the narrator?s reaction to story events.
See Table 2.
In total there were 3,510 unique binary valued fea-
tures extracted from our training dataset. We used
mutual information to find the features that had the
highest correlation with each category and the in-
formation gain over all the labels. The 10 highest
valued features are in Table 3, e.g. the top feature is
when the part-of-speech (POS) of the main verb in
the independent clause (IND) is past tense (VBD).
This feature encoding was used for machine learn-
ing experiments with classification algorithms from
Mallet (McCallum, 2002): Naive Bayes (NB) (Wit-
ten and Frank, 2005), Confidence Weighted Linear
Classifier (CWLC) (Dredze et al., 2008), Maximum
Entropy (ME)(Witten and Frank, 2005) and a se-
quential classifier (CRF) (Lafferty et al., 2001).
5 Evaluation and Results
We evaluate the performance of our classifiers with
experiments using the 50 annotated stories. Us-
ing the 40 stories in the training set we calculated
the information gain for each feature (see Table 3).
For each subset of the highest valued features (in
the range of 2
2
-2
12
), we performed a 10-fold cross-
validation on the training data and assessed the per-
formance of each classifier to find the right number
of features. Within each fold of the cross-validation
we also perform a simple grid search for the optimal
hyper-parameters of the model (e.g., the prior in the
ME and CRF models) using only the data within the
training fold.
The feature selection experimental results are
Extended Stative L&W
Classifier # F1 # F1 # F1
CRF 2
7
0.61 2
9
0.61 2
7
0.65
CWLC 2
11
0.67 2
11
0.68
?
2
11
0.73
?
ME 2
11
0.67 2
10
0.68
?
2
10
0.73
?
NB 2
9
0.68
?
2
9
0.70
?
2
10
0.76
?
Table 4: The optimal number of features found for
each model and the average F-score obtained using
a 10-fold cross-validation on the training data.
shown in Table 4. We report the optimal number
of features and the corresponding macro F-score,
weighted by the relative frequency of each category,
for each algorithm and label set. For all algorithms,
performance increases for label sets with higher lev-
els of abstraction. The Naive Bayes and CRF mod-
els perform better with a small subset of the fea-
tures, while the ME and CWLC algorithms use a
much larger subset. Surprisingly the sequential clas-
sifier has the lowest F-score and Naive Bayes per-
forms the best. A
?
indicates a significant improve-
ment over CRF at p < 0.05 using a two-sided t-test.
No other differences were significant.
Using the optimal number of features obtained
from this search we trained a model for each algo-
rithm using the entire training dataset and selecting
the hyper-parameters as before. We applied these
models to the unseen test data and evaluated the per-
formance of each classifier as applied to the entire
set of clauses and to individual narratives.
We first computed the precision, recall and F-score
aggregated over all the clauses in the test set. Ta-
ble 5 summarizes the results for each classifier and
label set. The left hand side of the table shows the
macro precision, recall and F-score weighted by the
relative frequency of each label. The right hand
side of the table shows the F-score of each indi-
vidual label separately. On this evaluation, Naive
Bayes outperforms all other methods on all label
sets. Overall, precision and recall are relatively bal-
anced achieving a maximum F-score of 0.689 when
the labels are mapped back to the original L&W cat-
egories. The CRF does surprisingly well consider-
ing its poor performance during the feature selection
search. The classifiers perform the poorest on orien-
tation clauses and the best on evaluation clauses.
As mentioned above, the annotation task is highly
subjective, requiring interpreting the narrative and
the author?s intention, which prevents us from ob-
taining high levels of inter-rater agreement. Because
of the noise in the annotations, the standard evalua-
175
Overall Per Label
L&W Stative
Label Set Model Prec Rec F1 ? Ori Eva Act ? Imp Loc Con
Extended
CRF 0.568 0.626 0.593 0.419 0.532 0.727 0.651 0.000 0.000 0.041 0.000
CWLC 0.567 0.616 0.582 0.398 0.377 0.763 0.612 0.000 0.000 0.087 0.000
ME 0.597 0.649 0.614 0.450 0.496 0.767 0.667 0.000 0.000 0.085 0.000
NB 0.625 0.656 0.623 0.459 0.478 0.781 0.650 0.000 0.000 0.174 0.000
Stative
CRF 0.563 0.591 0.574 0.370 0.412 0.695 0.628 0.000 0.235
CWLC 0.572 0.621 0.587 0.403 0.417 0.760 0.614 0.000 0.077
ME 0.610 0.644 0.611 0.441 0.464 0.759 0.673 0.000 0.118
NB 0.650 0.667 0.638 0.477 0.496 0.779 0.676 0.000 0.226
L&W
CRF 0.650 0.665 0.656 0.468 0.556 0.742 0.640 0.000
CWLC 0.624 0.647 0.632 0.424 0.480 0.747 0.609 0.000
ME 0.681 0.700 0.688 0.517 0.580 0.780 0.670 0.000
NB 0.687 0.705 0.689 0.514 0.565 0.780 0.687 0.000
Table 5: The performance of each of classifier on the test set when all clauses are aggregated together.
Agreement Total # Prec Rec F1 ? Ori Eva Act ?
1 of 3 15 0.333 0.400 0.339 0.069 0.000 0.625 0.333 0.000
2 of 3 146 0.597 0.610 0.580 0.405 0.472 0.700 0.622 0.000
3 of 3 269 0.770 0.773 0.767 0.607 0.667 0.824 0.746 0.000
All 430 0.687 0.705 0.689 0.514 0.565 0.780 0.687 0.000
Adjusted 430 0.646 0.660 0.643 0.447 0.516 0.745 0.623 0.000
Table 6: Performance measures for different levels of agreement among the annotators.
tion metrics may hide information about the ability
of the classifiers to learn from our feature set. For
example, the best performing classifier (NB) incor-
rectly labeled 127 clauses out of 430 possible in the
test set. However, 44 of these errors agreed with
at least one annotator, but were counted as entirely
incorrect in the previous evaluations.
To address these concerns we also evaluated the
performance of the the best performing classifier
based on the level of agreement of each instance us-
ing two different approaches. See Table 6. The first
approach was inspired by the approach in (Louis
and Nenkova, 2011) where the clauses in the test
set are binned based on the number of annotators
who agreed with the gold standard label. The per-
formance is then calculated for each bin. The first
three rows of Table 6 show the performance for the
different levels of agreement in the dataset. There
were only 15 clauses in the test set where there was
no agreement at all. It is unsurprising that when
the annotators could not agree on a label the sys-
tem performed near chance levels. However, when
all three annotators agreed on the gold standard
label the F-score improved to 0.767. As a compar-
ison, the F-score of the entire test set was 0.689 as
shown in the row labeled All.
Our second approach is based on the proposal of
Tetreault et al. (Tetreault et al., 2013). They intro-
Label Set Model Min Max Mean ? CI
Extended
CRF 0.333 0.763 0.540 ? 0.080
CWLC 0.276 0.763 0.582 ? 0.099
ME 0.333 0.753 0.572 ? 0.088
NB 0.333 0.741 0.573 ? 0.093
Stative
CRF 0.298 0.762 0.521 ? 0.099
CWLC 0.345 0.758 0.591 ? 0.090
ME 0.333 0.753 0.562 ? 0.098
NB 0.333 0.758 0.582 ? 0.088
L&W
CRF 0.333 0.837 0.609 ? 0.097
CWLC 0.458 0.877 0.658 ? 0.081
ME 0.333 0.830 0.649 ? 0.095
NB 0.333 0.851 0.647 ? 0.096
Table 7: Summary statistics of the F-score, with
95% confidence intervals, when evaluating stories.
duce a modification to the standard precision, re-
call and F-scores that takes into account the level
of agreement of each instance, where the values of
true-positives and false-negatives are assigned frac-
tional counts based on the proportion of annotators
who assigned that label. The final row of Table 6
provides the results using these adjusted values.
We also investigated the performance of the classi-
fiers when evaluating each story separately. Table 7
summarizes these results. Each classifier was ap-
plied to the clauses of the 10 narratives in the test
set and the F-score was computed for each narrative
individually. The table shows the minimum, maxi-
mum and average F-score with 95% confidence in-
176
ll l
l l
l l l
l l l
0.50
0.55
0.60
0.65
0.70
0.75
250 500 750 1000# Training Clauses
F?Sc
ore
Model
l CRFCWLCMaxEntNB
Figure 2: Learning curves of the Naive Bayes clas-
sifier using the optimal number of features.
tervals over the 10 narratives.
The CWLC performed the best on this test and
the performance of all the algorithms generally im-
proved using the higher-level label sets. The results
also show that there is a high variance in perfor-
mance between stories, with a minimum F-score of
0.458 and a maximum of 0.877 for the CWLC on
the L&W label set. This indicates that some clauses
are ambiguous and difficult to label, but also that
some stories are more difficult to classify.
To assess whether more annotated data could im-
prove performance, we ran a series of learning
curves in Fig. 2. Only the training data was used
for these experiments. The curves were created by
randomly sampling 90% of the data for training and
10% for testing. A model was trained, using the
same process as above, on successively larger sub-
sets of the data and applied to the 10% held out
clauses. This process was repeated 10 times and the
mean F-Score is reported. In nearly all cases, the
performance of classifiers is still increasing when
all of the data is used indicating that we have not
exhausted the expressive power of our features and
more annotated data would be useful. However, we
also see we can reach about 93% of our maximum
performance with only a few hundred examples. We
plan to increase the size of our annotated data set in
future work.
5.1 Error Analysis
Our results to date indicate that we achieve an over-
all F-score of 0.689, and that our classifiers are most
accurate for the evaluation and action categories.
See Table 6. Fig. 3 presents a confusion matrix
0036
0521242
096826
01712183EvaluationAction
Orientation
Not Story
Evaluation Action Orientation Not StoryGold
Pred
icted
Figure 3: Confusion matrix for the best classifier.
showing the frequency of predicted labels against
the gold standard labels for the Naive Bayes classi-
fier on the L&W label set. With the exception of not
story there are cases of confusion between all cate-
gories. However, in the vast majority of cases both
action and orientation are confused with evaluation
and the classifier overpredicts evaluation.
We also categorized errors for the Naive Bayes
classifier into the the 4 sources of errors in the pre-
dictions shown in Table 8. The most common errors
involved clauses with lexical INDICATORS that are
highly correlated with one category, but in the con-
text and interpretation of the story actually function
as a different type. For example, unfortunately,
could and n?t are all strong indicators of evaluation,
but in this case the primary function of the clause
is to set the scene for the rest of the story, i.e., ori-
entation. The interpretation of these clauses is clear
to a human, despite the lexical items misleading the
classifier.
Another source of error is when the function of the
clause in the narrative is ambiguous (PURPOSE in
Table 8). While there may be some misleading lex-
ical indicators in these clauses, there were often no
strongly correlated words, such as the adjectives and
modal verbs in EVALUATIONS. The distinction in
these cases is that the primary function of the clause
within the story is unclear, even to a human reader.
Unsurprisingly, most of the examples in this cate-
gory had low inter-rater agreement.
Some of the clauses contain figurative language
or complex constructions that require a significant
amount of world knowledge and INFERENCE to in-
terpret. For example, understanding the INFERENCE
clause in Table 8 requires recognizing the metaphor
about rabbit food in order to identify the subjective
evaluation the narrator is making.
There are also cases of clauses that contain MULTI-
PLE categories, at least partially because of the gran-
ularity of our segmentation. In the example in Ta-
ble 8 a new character, Alejandrio, is introduced and
a rising action is described, trekking to the waterfall.
177
Error Type Freq Gold Pred Example
Indicators 57 Ori Eva So, unfortunately I couldn?t make the Gamesindustry.biz party tonight.
Purpose 20 Ori Eva I know it is a remarkable haircut because on the way home a handsome young Mo-
roccan man nearly died to tell me how beautiful I was.
Inference 6 Eva Ori That?s that rabbit food that all of those Northeastern Kerry voters...
Multiple 4 Act Ori We trekked to a waterfall in the park with the help of Alejandrio a 65 year old
Honduran guy who not only walked quicker than us but also carried all the water.
Unclear 39 Ori Eva We have diners out east,
Not Story 7 Not Act scroll down to the Hobbit post,
Table 8: Several common sources of errors with a prototypical example.
Our annotation guidelines instructed us to prefer ac-
tions in these types of clauses, however, both ORI-
ENTATION and ACTION are present in this situation.
There were also 39 clauses that were labeled in-
correctly that had no clear reason (UNCLEAR) for
being mislabelled. We also explicitly excluded the
7 clauses marked not part of the story.
The types of errors described above are not mutu-
ally exclusive and in some cases are causally related.
For example, the purpose of a clause may be am-
biguous because it contains conflicting lexical indi-
cators. Similarly, a clause containing multiple cate-
gories will likely have strong lexical indicators from
each of these categories so that the classification al-
gorithms cannot disambiguate among possible la-
bels. This might be improved by more data, more
sophisticated semantic features, or possibly an anal-
ysis focused on discourse relations, such as those in
the PDTB (Louis et al., 2010; Prasad et al., 2008),
or Elson?s STORY INTENTION GRAPH (Rishes et al.,
2013; Elson and McKeown, 2010; Elson, 2012).
6 Discussion
This paper describes work on categorization of nar-
rative clauses based on Labov & Waletzky?s theory
of oral narrative, applied to personal narratives writ-
ten by ordinary people. We show that we can auto-
matically classify narrative clauses with these cate-
gories achieving an overall F-score of 0.689, which
is substantially higher than a random (0.250) or ma-
jority class (0.437) baseline, which increases to an
F-score of .767 on the cases where all three annota-
tors agreed. The learning curves plotted in Fig. 2
clearly suggest that more training data would be
beneficial before we investigate more complex fea-
tures and learning algorithms.
We believe the ability to automatically perform this
type of simple narrative analysis will enable and
improve many other types of deeper narrative un-
derstanding (Rahimtoroghi et al., 2014; Hu et al.,
2013). For example, causal and temporal relation-
ship extraction methods that focus only on clauses in
the same action sequence be more accurate, because
they exclude disconnected events from the orienta-
tion or evaluation sections. This type of analysis
will also enable new methods for learning attitudes
and values of societal groups based on the different
affective evaluations that are provided in response
to action clauses.
Our results also highlight several properties of the
data. Performance is different for results by story
rather than over all clauses. This indicates that
some stories are more difficult to classify than oth-
ers and that ambiguous clauses are not uniformly
distributed but are likely to be correlated with par-
ticular authors or writing styles. In other work, we
have started to investigate whether we can automat-
ically rate the temporal coherence of personal narra-
tives (Ryan et al., 2014). We can use this to identify
stories with utterances that are likely to be difficult
to classify because of the poor quality of the narra-
tive input. These cases are unlikely to have usable
narrative structure.
Acknowledgments
This research was supported by NSF Grants IIS
#1002921 and IIS #123855. The content of this
publication does not necessarily reflect the position
or policy of the government, and no official en-
dorsement should be inferred.
Appendix A
See Fig. 4 for an additional example labelled with
L&W Categories.
178
#Category
Story Clause
1
Abstract
Today was a very eventful work day.
2
Orientation
Today was the start of the G20 summit.
3
Orientation
It happens every year
4
Orientation
and it is where 20 of the leaders of the world come together to talk about how to run their
governments effectively and what not.
5
Orientation
Since there are so many leaders coming together their are going to be a lot of people who
have different views on how to run the government they follow so they protest.
6
Orientation
This week things started alright and on schedule.
7
Action
There was a protest that happened along the street where I work
8
Action
and at first it looked peaceful until a bunch of people started rebelling
9
Action
and creating a riot.
10
Action
Police cars were burned
11
Action
and things were thrown at cops.
12
Orientation
Police were in full riot gear to alleviate the violence.
13
Action
As things got worse tear gas and bean bag bullets were fired at the rioters
14
Action
while they smash windows of stores.
15
Evaluation
And this all happened right in front of my store
16
Evaluation
which was kind of scary
17
Evaluation
but it was kind of interesting
18
Coda
since I?ve never seen a riot before.
Figure 4: A personal narrative about a protest, with narrative categories of Labov & Waletzky, 1967.
References
Michael Bamberg. 2006. Stories: Big or small: Why do
we care? Narrative inquiry, 16(1):139?147.
Brandon Beamer and Roxana Girju. 2009. Using a bi-
gram event model to predict causal potential. In Com-
putational Linguistics and Intelligent Text Processing,
p. 430?441. Springer.
Jennifer G Bohanek, Kelly A Marin, and Robyn Fivush.
2008. Family narratives, self, and gender in early
adolescence. The Journal of Early Adolescence,
28(1):153?176.
Jerome Bruner. 1991. The narrative construction of re-
ality. Critical Inquiry, 18:1?21.
Kevin Burton, Akshay Java, and Ian Soboroff. 2009.
The ICWSM 2009 spinn3r dataset. In Proc. of the Third
Annual Conf. on Weblogs and Social Media.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proc. of the 47th Annual Meeting of the ACL, p. 602?
610.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing, p. 294?303.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
Proc. of the 25th international conference on Machine
learning, p. 264?271. ACM.
D.K. Elson and K.R. McKeown. 2010. Building a bank
of semantically encoded narratives. In Proc. of the Sev-
enth International Conf. on Language Resources and
Evaluation (LREC 2010).
David K Elson. 2012. Detecting story analogies from
annotations of time, action and agency. In Proc. of
the LREC 2012 Workshop on Computational Models of
Narrative, Istanbul, Turkey.
Robyn Fivush and Katherine Nelson. 2004. Culture and
language in the emergence of autobiographical mem-
ory. Psychological Science, 15(9):573?577.
Robyn Fivush, Jennifer G Bohanek, and Marshall Duke.
2005. The intergenerational self: Subjective perspec-
tive and family history. Individual and collective self-
continuity. Mahwah, NJ: Erlbaum.
R.J. Gerrig. 1993. Experiencing narrative worlds: On
the psychological activities of reading.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Third
International Conf. on Weblogs and Social Media, Data
Challenge Workshop.
Andrew Gordon, Cosmin Bejan, and Kenji Sagae. 2011.
Commonsense causal reasoning using millions of per-
sonal stories. In Twenty-Fifth Conf. on Artificial Intelli-
gence (AAAI-11).
Amit Goyal, Ellen Riloff, and Hal Daum?e III. 2010. Au-
tomatically producing plot unit representations for nar-
rative text. In Proc. of the 2010 Conf. on Empirical
Methods in Natural Language Processing, p. 77?86.
Arthur C Graesser, Murray Singer, and Tom Trabasso.
1994. Constructing inferences during narrative text
comprehension. Psychological review, 101(3):371.
179
T. Habermas and S. Bluck. 2000. Getting a life: the
emergence of the life story in adolescence. Psychol
Bull, 126(5):748?69.
Zhichao Hu, Elahe Rahimtoroghi, Larissa Munishkina,
Reid Swanson, and Marilyn A Walker. 2013. Unsu-
pervised induction of contingent event pairs from film
scenes. In Proc. of Conf. on Empirical Methods in Nat-
ural Language Processing, p. 370?379.
W. Labov and J. Waletzky. 1967. Narrative analysis:
Oral versions of personal experience. In J. Helm, ed.,
Essays on the Verbal and Visual Arts, p. 12?44.
W. Labov. 1997. Some further steps in narrative analy-
sis. Journal of narrative and life history, 7:395?415.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data.
Wendy G Lehnert. 1981. Plot units and narrative sum-
marization. Cognitive Science, 5(4):293?331.
Annie Louis and Ani Nenkova. 2011. Automatic identi-
fication of general and specific sentences by leveraging
discourse annotations. In International Joint Conf. on
Natural Language Processing, p. 605?613.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify im-
plicit relations. In Proc. of the 11th Annual SIGdial
Meeting on Discourse and Dialogue.
Mehdi Manshadi, Reid Swanson, and Andrew S Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Proc. of the
21st FLAIRS Conf. .
Dan P McAdams. 2003. Identity and the life story. Au-
tobiographical memory and the construction of a nar-
rative self: Developmental and cultural perspectives,
9:187?207.
Andrew Kachites McCallum. 2002. MAL-
LET: a machine learning for language toolkit.
http://mallet.cs.umass.edu.
K.C. McLean and A. Thorne. 2003. Adolescents? self-
defining memories about relationships. Developmental
Psychology, (39):635?645.
J. W. Pennebaker, M. E. Francis, and R. J. Booth. 2001.
Inquiry and Word Count: LIWC 2001.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie Web-
ber. 2008. The Penn Discourse TreeBank 2.0. In Proc.
of 6th International Conf. on Language Resources and
Evaluation (LREC 2008).
Michael W Pratt and Barbara H Fiese. 2004. Families,
Stories, and the Life Course: An Ecological Context.
Elahe Rahimtoroghi, Reid Swanson, and Marilyn A.
Walker. 2013. Evaluation, orientation, and action in in-
teractive storytelling. In Proc. of Intelligent Narrative
Technologies 6.
Elahe Rahimtoroghi, Thomas Corcoran, Reid Swanson,
Marilyn A. Walker, Kenji Sagae, and Andrew S. Gor-
don. 2014. Minimal narrative annotation schemes
and their applications. In Proc. of Intelligent Narrative
Technologies 7.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contingency
relationships with no supervision. In Semantic Com-
puting (ICSC), p. 361?368. IEEE.
Elena Rishes, Stephanie Lukin, David K. Elson, and
Marilyn A. Walker. 2013. Generating dierent story
tellings from semantic representations of narrative. In
Int. Conf. on Interactive Digital Storytelling, ICIDS?13.
James Owen Ryan, Marilyn A. Walker, and Noah
Wardrip-Fruin. 2014. Toward recombinant dialogue
in interactive narrative. In 7th Workshop on Intelligent
Narrative Technologies.
Nancy L. Stein, Tom Trabasso, and Maria D. Liwag.
2000. A goal appraisal theory of emotional understand-
ing: Implications for development and learning. In M.
Lewis and J. M. Haviland-Jones, ed, Handbook of emo-
tions (2nd ed.), p. 436?457.
Joel Tetreault, Martin Chodorow, and Nitin Madnani.
2013. Bucking the trend: improved evaluation and an-
notation practices for esl error detection systems. Lan-
guage Resources and Evaluation, p. 1?27.
A. Thorne and V. Nam. 2009. The storied construction
of personality. In Kitayama S. and Cohen D., ed, Hand-
book of Cultural Psychology, p. 491?505.
A. Thorne and L. A. Shapiro. 2011. Testing, testing:
Everyday storytelling and the construction of adoles-
cent identity. Adolescent Vulnerabilities and Opportu-
nities: Developmental and Constructivist Perspectives,
38:117.
A. Thorne, N. Korobov, and E. M. Morgan. 2007. Chan-
neling identity: A study of storytelling in conversations
between introverted and extraverted friends. Journal of
research in personality, 41(5):1008?1031.
Avril Thorne. 2004. Putting the person into social iden-
tity. Human Development, 47(6):361?365.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques. Mor-
gan Kaufmann, San Francisco, CA.
180
