Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422?1426,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Elephant: Sequence Labeling for Word and Sentence Segmentation
Kilian Evang*, Valerio Basile*, Grzegorz Chrupa?a? and Johan Bos*
*University of Groningen, Oude Kijk in ?t Jatstraat 26, 9712 EK Groningen, The Netherlands
?Tilburg University, PO Box 90153, 5000 LE Tilburg, The Netherlands
*{k.evang, v.basile, johan.bos}@rug.nl ?g.chrupala@uvt.nl
Abstract
Tokenization is widely regarded as a solved
problem due to the high accuracy that rule-
based tokenizers achieve. But rule-based
tokenizers are hard to maintain and their
rules language specific. We show that high-
accuracy word and sentence segmentation can
be achieved by using supervised sequence la-
beling on the character level combined with
unsupervised feature learning. We evalu-
ated our method on three languages and ob-
tained error rates of 0.27 ? (English), 0.35 ?
(Dutch) and 0.76 ? (Italian) for our best mod-
els.
1 An Elephant in the Room
Tokenization, the task of segmenting a text into
words and sentences, is often regarded as a solved
problem in natural language processing (Dridan and
Oepen, 2012), probably because many corpora are
already in tokenized format. But like an elephant in
the living room, it is a problem that is impossible to
overlook whenever new raw datasets need to be pro-
cessed or when tokenization conventions are recon-
sidered. It is moreover an important problem, be-
cause any errors occurring early in the NLP pipeline
affect further analysis negatively. And even though
current tokenizers reach high performance, there are
three issues that we feel haven?t been addressed sat-
isfactorily so far:
? Most tokenizers are rule-based and therefore
hard to maintain and hard to adapt to new do-
mains and new languages (Silla Jr. and Kaest-
ner, 2004);
? Word and sentence segmentation are often seen
as separate tasks, but they obviously inform
each other and it could be advantageous to view
them as a combined task;
? Most tokenization methods provide no align-
ment between raw and tokenized text, which
makes mapping the tokenized version back
onto the actual source hard or impossible.
In short, we believe that regarding tokenization,
there is still room for improvement, in particular on
the methodological side of the task. We are partic-
ularly interested in the following questions: Can we
use supervised learning to avoid hand-crafting rules?
Can we use unsupervised feature learning to reduce
feature engineering effort and boost performance?
Can we use the same method across languages? Can
we combine word and sentence boundary detection
into one task?
2 Related Work
Usually the text segmentation task is split into word
tokenization and sentence boundary detection. Rule-
based systems for finding word and sentence bound-
aries often are variations on matching hand-coded
regular expressions (Grefenstette, 1999; Silla Jr. and
Kaestner, 2004; Jurafsky and Martin, 2008; Dridan
and Oepen, 2012).
Several unsupervised systems have been proposed
for sentence boundary detection. Kiss and Strunk
(2006) present a language-independent, unsuper-
vised approach and note that abbreviations form a
major source of ambiguity in sentence boundary
detection and use collocation detection to build a
high-accuracy abbreviation detector. The resulting
system reaches high accuracy, rivalling handcrafted
rule-based and supervised systems. A similar sys-
tem was proposed earlier by Mikheev (2002).
Existing supervised learning approaches for sen-
tence boundary detection use as features tokens pre-
ceding and following potential sentence boundary,
part of speech, capitalization information and lists
of abbreviations. Learning methods employed in
1422
these approaches include maximum entropy models
(Reynar and Ratnaparkhi, 1997) decision trees (Ri-
ley, 1989), and neural networks (Palmer and Hearst,
1997).
Closest to our work are approaches that present
token and sentence splitters using conditional ran-
dom fields (Tomanek et al, 2007; Fares et al, 2013).
However, these previous approaches consider tokens
(i.e. character sequences) as basic units for labeling,
whereas we consider single characters. As a con-
sequence, labeling is more resource-intensive, but it
also gives us more expressive power. In fact, our ap-
proach kills two birds with one stone, as it allows us
to integrate token and sentence boundaries detection
into one task.
3 Method
3.1 IOB Tokenization
IOB tagging is widely used in tasks identifying
chunks of tokens. We use it to identify chunks of
characters. Characters outside of tokens are labeled
O, inside of tokens I. For characters at the beginning
of tokens, we use S at sentence boundaries, other-
wise T (for token). This scheme offers some nice
features, like allowing for discontinuous tokens (e.g.
hyphenated words at line breaks) and starting a new
token in the middle of a typographic word if the to-
kenization scheme requires it, as e.g. in did|n?t. An
example is given in Figure 1.
It didn?t matter if the faces were male,
SIOTIITIIOTIIIIIOTIOTIIOTIIIIOTIIIOTIIITO
female or those of children. Eighty-
TIIIIIOTIOTIIIIOTIOTIIIIIIITOSIIIIIIO
three percent of people in the 30-to-34
IIIIIOTIIIIIIOTIOTIIIIIOTIOTIIOTIIIIIIIO
year old age range gave correct responses.
TIIIOTIIOTIIOTIIIIOTIIIOTIIIIIIOTIIIIIIIIT
Figure 1: Example of IOB-labeled characters
3.2 Datasets
In our experiments we use three datasets to compare
our method for different languages and for different
domains: manually checked English newswire texts
taken from the Groningen Meaning Bank, GMB
(Basile et al, 2012), Dutch newswire texts, com-
prising two days from January 2000 extracted from
the Twente News Corpus, TwNC (Ordelman et al,
2007), and a random sample of Italian texts from the
PAISA` corpus (Borghetti et al, 2011).
Table 1: Datasets characteristics.
Name Language Domain Sentences Tokens
GMB English Newswire 2,886 64,443
TNC Dutch Newswire 49,537 860,637
PAI Italian Web/various 42,674 869,095
The data was converted into IOB format by infer-
ring an alignment between the raw text and the seg-
mented text.
3.3 Sequence labeling
We apply the Wapiti implementation (Lavergne et
al., 2010) of Conditional Random Fields (Lafferty
et al, 2001), using as features the output label of
each character, combined with 1) the character it-
self, 2) the output label on the previous character, 3)
characters and/or their Unicode categories from con-
text windows of varying sizes. For example, with a
context size of 3, in Figure 1, features for the E in
Eighty-three with the output label S would be E/S,
O/S, /S, i/S, Space/S, Lowercase/S. The intuition
is that the 31 existing Unicode categories can gen-
eralize across similar characters whereas character
features can identify specific contexts such as abbre-
viations or contractions (e.g. didn?t). The context
window sizes we use are 0, 1, 3, 5, 7, 9, 11 and 13,
centered around the focus character.
3.4 Deep learning of features
Automatically learned word embeddings have been
successfully used in NLP to reduce reliance on man-
ual feature engineering and boost performance. We
adapt this approach to the character level, and thus,
in addition to hand-crafted features we use text
representations induced in an unsupervised fashion
from character strings. A complete discussion of
our approach to learning text embeddings can be
found in (Chrupa?a, 2013). Here we provide a brief
overview.
Our representations correspond to the activation
of the hidden layer in a simple recurrent neural
(SRN) network (Elman, 1990; Elman, 1991), imple-
mented in a customized version of Mikolov (2010)?s
RNNLM toolkit. The network is sequentially pre-
sented with a large amount of raw text and learns to
1423
predict the next character in the sequence. It uses the
units in the hidden layer to store a generalized rep-
resentation of the recent history. After training the
network on large amounts on unlabeled text, we run
it on the training and test data, and record the activa-
tion of the hidden layer at each position in the string
as it tries to predict the next character. The vector of
activations of the hidden layer provides additional
features used to train and run the CRF. For each of
the K = 10 most active units out of total J = 400
hidden units, we create features (f(1) . . . f(K)) de-
fined as f(k) = 1 if sj(k) > 0.5 and f(k) = 0 oth-
erwise, where sj(k) returns the activation of the kth
most active unit. For training the SRN only raw text
is necessary. We trained on the entire GMB 2.0.0
(2.5M characters), the portion of TwNC correspond-
ing to January 2000 (43M characters) and a sample
of the PAISA` corpus (39M characters).
4 Results and Evaluation
In order to evaluate the quality of the tokenization
produced by our models we conducted several ex-
periments with different combinations of features
and context sizes. For these tests, the models are
trained on an 80% portion of the data sets and tested
on a 10% development set. Final results are obtained
on a 10% test set. We report both absolute number
of errors and error rates per thousand (?).
4.1 Feature sets
We experiment with two kinds of features at the
character level, namely Unicode categories (31 dif-
ferent ones), Unicode character codes, and a combi-
nation of them. Unicode categories are less sparse
than the character codes (there are 88, 134, and 502
unique characters for English, Dutch and Italian, re-
spectively), so the combination provide some gener-
alization over just character codes.
Table 2: Error rates obtained with different feature sets.
Cat stands for Unicode category, Code for Unicode char-
acter code, and Cat-Code for a union of these features.
Error rates per thousand (?)
Feature set English Dutch Italian
Cat-9 45 (1.40) 1,403 (2.87) 1,548 (2.67)
Code-9 6 (0.19) 782 (1.60) 692 (1.20)
Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)
From these results we see that categories alone
perform worse than only codes. For English there is
no gain from the combination over using only char-
acter codes. For Dutch and Italian there is an im-
provement, although it is only significant for Ital-
ian (p = 0.480 and p = 0.005 respectively, bino-
mial exact test). We use this feature combination in
the experiments that follow. Note that these models
are trained using a symmetrical context of 9 charac-
ters (four left and four right of the current character).
In the next section we show performance of models
with different window sizes.
4.2 Context window
We run an experiment to evaluate how the size of the
context in the training phase impacts the classifica-
tion. In Table 4.2 we show the results for symmetri-
cal windows ranging in size from 1 to 13.
Table 3: Using different context window sizes.
Error rates per thousand (?)
Feature set English Dutch Italian
Cat-Code-1 273 (8.51) 4,924 (10.06) 9,108 (15.86)
Cat-Code-3 118 (3.68) 3,525 (7.20) 2,013 (3.51)
Cat-Code-5 20 (0.62) 930 (1.90) 788 (1.37)
Cat-Code-7 10 (0.31) 778 (1.60) 667 (1.16)
Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)
Cat-Code-11 9 (0.28) 761 (1.56) 692 (1.21)
Cat-Code-13 8 (0.25) 751 (1.54) 670 (1.17)
4.3 SRN features
We also tested the automatically learned features de-
rived from the activation of the hidden layer of an
SRN language model, as explained in Section 3.
We combined these features with character code and
Unicode category features in windows of different
sizes. The results of this test are shown in Table 4.
The first row shows the performance of SRN fea-
tures on their own. The following rows show the
combination of SRN features with the basic feature
sets of varying window size. It can be seen that aug-
menting the feature sets with SRN features results
in large reductions of error rates. The Cat-Code-1-
SRN setting has error rates comparable to Cat-Code-
9.
The addition of SRN features to the two best
previous models, Cat-Code-9 and Cat-Code-13, re-
duces the error rate by 83% resp. 81% for Dutch,
1424
and by 24% resp. 26% for Italian. All these dif-
ferences are statistically significant according to the
binomial test (p < 0.001). For English, there are too
few errors to detect a statistically significant effect
for Cat-Code-9 (p = 0.07), but for Cat-Code-13 we
find p = 0.016.
Table 4: Results obtained using different context window
sizes and addition of SRN features.
Error rates per thousand (?)
Feature set English Dutch Italian
SRN 24 (0.75) 276 (0.56) 738 (1.28)
Cat-Code-1-SRN 7 (0.21) 212 (0.43) 549 (0.96)
Cat-Code-3-SRN 4 (0.13) 165 (0.34) 507 (0.88)
Cat-Code-5-SRN 3 (0.10) 136 (0.28) 476 (0.83)
Cat-Code-7-SRN 1 (0.03) 111 (0.23) 497 (0.86)
Cat-Code-9-SRN 2 (0.06) 135 (0.28) 497 (0.86)
Cat-Code-11-SRN 2 (0.06) 132 (0.27) 468 (0.81)
Cat-Code-13-SRN 1 (0.03) 142 (0.29) 496 (0.86)
In a final step, we selected the best models based
on the development sets (Cat-Code-7-SRN for En-
glish and Dutch, Cat-Code-11-SRN for Italian), and
checked their performance on the final test set. This
resulted in 10 errors (0.27 ?) for English (GMB
corpus), 199 errors (0.35 ?) for Dutch (TwNC cor-
pus), and 454 errors (0.76 ?) for Italian (PAISA`
corpus).
5 Discussion
It is interesting to examine what kind of errors the
SRN features help avoid. In the English and Dutch
datasets many errors are caused by failure to rec-
ognize personal titles and initials or misparsing of
numbers. In the Italian data, a large fraction of er-
rors is due to verbs with clitics, which are written as
a single word, but treated as separate tokens. Table 5
shows examples of errors made by a simpler model
that are fixed by adding SRN features. Table 6 shows
the confusion matrices for the Cat-Code-7 and Cat-
Code-7-SRN sets on the Dutch data. The mistake
most improved by SRN features is T/I with 89% er-
ror reduction (see also Table 5). The is also the most
common remaining mistake.
A comparison with other approaches is hard be-
cause of the difference in datasets and task defini-
tion (combined word/sentence segmentation). Here
we just compare our results for sentence segmenta-
tion (sentence F1 score) with Punkt, a state-of-the-
Table 5: Positive impact of SRN features.
Ms. Hughes will joi
Cat-Code-7 SIIOSIIIIIOTIIIOTII
Cat-Code-7-SRN SIIOTIIIIIOTIIIOTII
$ 3.9 trillion by t
Cat-Code-7 TOTTIOTIIIIIIIOTIOT
Cat-Code-7-SRN TOTIIOTIIIIIIIOTIOT
bleek 0,4 procent
Cat-Code-11 OTIIIIOTTIOTIIIIIIO
Cat-Code-11-SRN OTIIIIOTIIOTIIIIIIO
toebedeeld: 6,2. In
Cat-Code-11 TIIIIIIIIITOTTITOSI
Cat-Code-11-SRN TIIIIIIIIITOTIITOSI
prof. Teulings het
Cat-Code-11 TIIITOSIIIIIIIOTIIO
Cat-Code-11-SRN TIIIIOTIIIIIIIOTIIO
per costringerlo al
Cat-Code-11 TIIOTIIIIIIIIIIIOTI
Cat-Code-11-SRN TIIOTIIIIIIIIITIOTI
Table 6: Confusion matrix for Dutch development set.
Predicted, Cat-Code-7 Predicted, Cat-Code-7-SRN
Gold I O S T I O S T
I 328128 0 2 469 328546 0 0 53
O 0 75234 0 0 0 75234 0 0
S 4 0 4323 18 1 0 4332 12
T 252 0 33 80828 35 0 10 81068
art sentence boundary detection system (Kiss and
Strunk, 2006). With its standard distributed mod-
els, Punkt achieves 98.51% on our English test set,
98.87% on Dutch and 98.34% on Italian, compared
with 100%, 99.54% and 99.51% for our system. Our
system benefits here from its ability to adapt to a new
domain with relatively little (but annotated) training
data.
6 What Elephant?
Word and sentence segmentation can be recast as a
combined tagging task. This way, tokenization is
cast as a supervised learning task, causing a shift of
labor from writing rules to manually correcting la-
bels. Learning this task with CRF achieves high ac-
curacy.1 Furthermore, our tagging method does not
lose the connection between original text and tokens.
In future work, we plan to broaden the scope of
this work to other steps in document preparation,
1All software needed to replicate our experiments is
available at http://gmb.let.rug.nl/elephant/
experiments.php
1425
such as normalization of punctuation, and their in-
teraction with segmentation. We further plan to test
our method on a wider range of datasets, allowing a
more direct comparison with other approaches. Fi-
nally, we plan to explore the possibility of a statis-
tical universal segmentation model for mutliple lan-
guages and domains.
In a famous scene with a live elephant on stage,
the comedian Jimmy Durante was asked about it by
a policeman and surprisedly answered: ?What ele-
phant?? We feel we can say the same now as far as
tokenization is concerned.
References
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istanbul,
Turkey.
Claudia Borghetti, Sara Castagnoli, and Marco Brunello.
2011. I testi del web: una proposta di classificazione
sulla base del corpus PAISA`. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica,
pages 147?170. Carocci, Roma.
Grzegorz Chrupa?a. 2013. Text segmentation with
character-level text embeddings. In ICML Workshop
on Deep Learning for Audio, Speech and Language
Processing, Atlanta, USA.
Rebecca Dridan and Stephan Oepen. 2012. Tokeniza-
tion: Returning to a long solved problem ? a survey,
contrastive experiment, recommendations, and toolkit
?. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 2:
Short Papers), pages 378?382, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179?211.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical structure.
Machine learning, 7(2):195?225.
Murhaf Fares, Stephan Oepen, and Zhang Yi. 2013. Ma-
chine learning for high-quality tokenization - replicat-
ing variable tokenization schemes. In A. Gelbukh, ed-
itor, CICLING 2013, volume 7816 of Lecture Notes in
Computer Science, pages 231?244, Berlin Heidelberg.
Springer-Verlag.
Gregory Grefenstette. 1999. Tokenization. In Hans van
Halteren, editor, Syntactic Wordclass Tagging, pages
117?133. Kluwer Academic Publishers, Dordrecht.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing. An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice Hall, 2nd edition.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational
Linguistics, 32(4):485?525.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML-01, pages 282?289.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 504?513, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Andrei Mikheev. 2002. Periods, capitalized words, etc.
Computational Linguistics, 28(3):289?318.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Interspeech.
Roeland Ordelman, Franciska de Jong, Arjan van Hessen,
and Hendri Hondorp. 2007. TwNC: a multifaceted
Dutch news corpus. ELRA Newsleter, 12(3/4):4?7.
David D. Palmer and Marti A. Hearst. 1997. Adap-
tive multilingual sentence boundary disambiguation.
Computational Linguistics, 23(2):241?267.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the Fifth Conference
on Applied Natural Language Processing, pages 16?
19, Washington, DC, USA. Association for Computa-
tional Linguistics.
Michael D. Riley. 1989. Some applications of tree-based
modelling to speech and language. In Proceedings of
the workshop on Speech and Natural Language, HLT
?89, pages 339?352, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Carlos N. Silla Jr. and Celso A. A. Kaestner. 2004. An
analysis of sentence boundary detection systems for
English and Portuguese documents. In Fifth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, volume 2945 of Lecture
Notes in Computer Science, pages 135?141. Springer.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on condi-
tional random fields. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 49?57, Melbourne, Australia.
1426
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 92?96,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A platform for collaborative semantic annotation
Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen
{v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl
Center for Language and Cognition Groningen (CLCG)
University of Groningen, The Netherlands
Abstract
Data-driven approaches in computational
semantics are not common because there
are only few semantically annotated re-
sources available. We are building a
large corpus of public-domain English texts
and annotate them semi-automatically with
syntactic structures (derivations in Com-
binatory Categorial Grammar) and seman-
tic representations (Discourse Representa-
tion Structures), including events, thematic
roles, named entities, anaphora, scope, and
rhetorical structure. We have created a
wiki-like Web-based platform on which a
crowd of expert annotators (i.e. linguists)
can log in and adjust linguistic analyses in
real time, at various levels of analysis, such
as boundaries (tokens, sentences) and tags
(part of speech, lexical categories). The
demo will illustrate the different features of
the platform, including navigation, visual-
ization and editing.
1 Introduction
Data-driven approaches in computational seman-
tics are still rare because there are not many
large annotated resources that provide empiri-
cal information about anaphora, presupposition,
scope, events, tense, thematic roles, named en-
tities, word senses, ellipsis, discourse segmenta-
tion and rhetorical relations in a single formal-
ism. This is not surprising, as it is challenging and
time-consuming to create such a resource from
scratch.
Nevertheless, our objective is to develop a
large annotated corpus of Discourse Representa-
tion Structures (Kamp and Reyle, 1993), com-
prising most of the aforementioned phenomena:
the Groningen Meaning Bank (GMB). We aim to
reach this goal by:
1. Providing a wiki-like platform supporting
collaborative annotation efforts;
2. Employing state-of-the-art NLP software for
bootstrapping semantic analysis;
3. Giving real-time feedback of annotation ad-
justments in their resulting syntactic and se-
mantic analysis;
4. Ensuring kerfuffle-free dissemination of
our semantic resource by considering only
public-domain texts for annotation.
We have developed the wiki-like platform from
scratch simply because existing annotation sys-
tems, such as GATE (Dowman et al 2005), NITE
(Carletta et al 2003), or UIMA (Hahn et al
2007), do not offer the functionality required for
deep semantic annotation combined with crowd-
sourcing.
In this description of our platform, we motivate
our choice of data and explain how we manage it
(Section 2), we describe the complete toolchain
of NLP components employed in the annotation-
feedback process (Section 3), and the Web-based
interface itself is introduced, describing how lin-
guists can adjust boundaries of tokens and sen-
tences, and revise tags of named entities, parts of
speech and lexical categories (Section 4).
2 Data
The goal of the Groningen Meaning Bank is to
provide a widely available corpus of texts, with
deep semantic annotations. The GMB only com-
prises texts from the public domain, whose dis-
tribution isn?t subject to copyright restrictions.
Moreover, we include texts from various genres
and sources, resulting in a rich, comprehensive
92
corpus appropriate for use in various disciplines
within NLP.
The documents in the current version of the
GMB are all in English and originate from four
main sources: (i) Voice of America (VOA), an on-
line newspaper published by the US Federal Gov-
ernment; (ii) the Manually Annotated Sub-Corpus
(MASC) from the Open American National Cor-
pus (Ide et al 2010); (iii) country descriptions
from the CIA World Factbook (CIA) (Central In-
telligence Agency, 2006), in particular the Back-
ground and Economy sections, and (iv) a col-
lection of Aesop?s fables (AF). All these docu-
ments are in the public domain and are thus redis-
tributable, unlike for example the WSJ data used
in the Penn Treebank (Miltsakaki et al 2004).
Each document is stored with a separate file
containing metadata. This may include the lan-
guage the text is written in, the genre, date of
publication, source, title, and terms of use of the
document. This metadata is stored as a simple
feature-value list.
The documents in the GMB are categorized
with different statuses. Initially, newly added doc-
uments are labeled as uncategorized. As we man-
ually review them, they are relabeled as either
accepted (document will be part of the next sta-
ble version, which will be released in regular in-
tervals), postponed (there is some difficulty with
the document that can possibly be solved in the
future) or rejected (something is wrong with the
document form, i.e., character encoding, or with
the content, e.g., it contains offensive material).
Currently, the GMB comprises 70K English
text documents (Table 1), corresponding to 1,3
million sentences and 31,5 million tokens.
Table 1: Documents in the GMB, as of March 5, 2012
Documents VOA MASC CIA AF All
Accepted 4,651 34 515 0 5,200
Uncategorized 61,090 0 0 834 61,924
Postponed 2,397 339 3 1 2,740
Rejected 184 27 4 0 215
Total 68,322 400 522 835 70,079
3 The NLP Toolchain
The process of building the Groningen Meaning
Bank takes place in a bootstrapping fashion. A
chain of software is run, taking the raw text docu-
ments as input. The output of this automatic pro-
cess is in the form of several layers of stand-off
annotations, i.e., files with links to the original,
raw documents.
We employ a chain of NLP components that
carry out, respectively, tokenization and sentence
boundary detection, POS tagging, lemmatization,
named entity recognition, supertagging, parsing
using the formalism of Combinatory Categorial
Grammar (Steedman, 2001), and semantic and
discourse analysis using the framework of Dis-
course Representation Theory (DRT) (Kamp and
Reyle, 1993) with rhetorical relations (Asher,
1993).
The lemmatizer used is morpha (Minnen et al
2001), the other steps are carried out by the C&C
tools (Curran et al 2007) and Boxer (Bos, 2008).
3.1 Bits of Wisdom
After each step in the toolchain, the intermediate
result may be automatically adjusted by auxiliary
components that apply annotations provided by
expert users or other sources. These annotations
are represented as ?Bits of Wisdom? (BOWs): tu-
ples of information regarding, for example, token
and sentence boundaries, tags, word senses or dis-
course relations. They are stored in a MySQL
database and can originate from three different
sources: (i) explicit annotation changes made by
experts using the Explorer Web interface (see Sec-
tion 4); (ii) an annotation game played by non-
experts, similar to ?games with a purpose? like
Phrase Detectives (Chamberlain et al 2008) and
Jeux de Mots (Artignan et al 2009); and (iii) ex-
ternal NLP tools (e.g. for word sense disambigua-
tion or co-reference resolution).
Since BOWs come from various sources, they
may contradict each other. In such cases, a judge
component resolves the conflict, currently by pre-
ferring the most recent expert BOW. Future work
will involve the application of different judging
techniques.
3.2 Processing Cycle
The widely known open-source tool GNU make
is used to orchestrate the toolchain while avoid-
ing unnecessary reprocessing. The need to rerun
the toolchain for a document arises in three sit-
uations: a new BOW for that document is avail-
able; a new, improved version of one of the com-
ponents is available; or reprocessing is forced by
a user via the ?reprocess? button in the Web inter-
face. A continually running program, the ?updat-
93
Figure 1: A screenshot of the web interface, displaying a tokenised document.
ing daemon?, is responsible for calling make for
the right document at the right time. It checks the
database for new BOWs or manual reprocessing
requests in very short intervals to ensure immedi-
ate response to changes experts make via the Web
interface. It also updates and rebuilds the compo-
nents in longer intervals and continuously loops
through all documents, remaking them with the
newest versions of the components. The number
of make processes that can run in parallel is con-
figurable; standard techniques of concurrent pro-
gramming are used to prevent more than one make
process from working simultaneously on the same
document.
4 The Expert Interface
We developed a wiki-like Web interface, called
the GMB Explorer, that provides users access to
the Groningen Meaning Bank. It fulfills three
main functions: navigation and search through the
documents, visualization of the different levels of
annotation, and manual correction of the annota-
tions. We will discuss these functions below.
4.1 Navigation and Search
The GMB Explorer allows navigation through the
documents of the GMB with their stand-off an-
notations (Figure 1). The default order of docu-
ments is based on their size in terms of number
of tokens. It is possible to apply filters to restrict
the set of documents to be shown: showing only
documents from a specific subcorpus, or specifi-
cally showing documents with/without warnings
generated by the NLP toolchain.
The Explorer interface comes with a built-in
search engine. It allows users to pose single- or
multi-word queries. The search results can then
be restricted further by looking for a specific lex-
ical category or part of speech. A more advanced
search system that is based on a semantic lexicon
with lexical information about all levels of anno-
tation is currently under development.
4.2 Visualization
The different visualization options for a document
are placed in tabs: each tab corresponds to a spe-
cific layer of annotation or additional informa-
tion. Besides the raw document text, users can
view its tokenized version, an interactive deriva-
tion tree per sentence, and the semantic represen-
tation of the entire discourse in graphical DRS
format. There are three further tabs in the Ex-
plorer: a tab containing the warnings produced by
the NLP pipeline (if any), one containing the Bits
of Wisdom that have been collected for the docu-
ment, and a tab with the document metadata.
The sentences view allows the user to show or
hide sub-trees per sentence and additional infor-
mation such as POS-tags, word senses, supertags
and partial, unresolved semantics. The deriva-
tions are shown using the CCG notation, gener-
ated by XSLT stylesheets applied to Boxer?s XML
output. An example is shown in Figure 2.
The discourse view shows a fully resolved
semantic representation in the form of a DRS with
Figure 2: An example of a CCG derivation as shown
in GMB Explorer.
94
Figure 3: An example of the semantic representations
in the GMB, with DRSs representing discourse units.
rhetorical relations. Clicking on discourse units
switches the visualization between text and se-
mantic representation. Figure 3 shows how DRSs
are visualized in the Web interface.
4.3 Editing
Some of the tabs in the Explorer interface have an
?edit? button. This allows registered users to man-
ually correct certain types of annotations. Cur-
rently, the user can edit the tokenization view and
on the derivation view. Clicking ?edit? in the to-
kenization view gives an annotator the possibility
to add and remove token and sentence boundaries
in a simple and intuitive way, as Figure 4 illus-
trates. This editing is done in real-time, following
the WYSIWYG strategy, with tokens separated
by spaces and sentences separated by new lines.
In the derivation view, the annotator can change
part-of-speech tags and named entity tags by se-
lecting a tag from a drop-down list (Figure 5).
Figure 4: Tokenization edit mode. Clicking on the
red ??? removes a sentence boundary after the token;
clicking on the green ?+? adds a sentence boundary.
Figure 5: Tag edit mode, showing derivation with par-
tial DRSs and illustrating how to adjust a POS tag.
As the updating daemon is running continu-
ally, the document is immediately reprocessed af-
ter editing so that the user can directly view the
new annotation with his BOW taken into account.
Re-analyzing a document typically takes a few
seconds, although for very large documents it can
take longer. It is also possible to directly rerun
the NLP toolchain on a specific document via the
?reprocess? button, in order to apply the most re-
cent version of the software components involved.
The GMB Explorer shows a timestamp of the last
processing for each document.
We are currently working on developing new
editing options, which allow users to change dif-
ferent aspects of the semantic representation, such
as word senses, thematic roles, co-reference and
scope.
5 Demo
In the demo session we show the functionality of
the various features in the Web-based user inter-
face of the GMB Explorer, which is available on-
line via: http://gmb.let.rug.nl.
We show (i) how to navigate and search
through all the documents, including the refine-
ment of search on the basis of the lexical cate-
gory or part of speech, (ii) the operation of the dif-
ferent view options, including the raw, tokenized,
derivation and semantics view of each document,
and (iii) how adjustments to annotations can be re-
alised in the Web interface. More concretely, we
demonstrate how boundaries of tokens and sen-
tences can be adapted, and how different types of
tags can be changed (and how that affects the syn-
tactic, semantic and discourse analysis).
In sum, the demo illustrates innovation in the
way changes are made and how they improve the
linguistic analysis in real-time. Because it is a
web-based platform, it paves the way for a collab-
orative annotation effort. Currently it is actively
in use as a tool to create a large semantically an-
notated corpus for English texts: the Groningen
Meaning Bank.
95
References
Guillaume Artignan, Mountaz Hascoe?t, and Mathieu
Lafourcade. 2009. Multiscale visual analysis of
lexical networks. In 13th International Confer-
ence on Information Visualisation, pages 685?690,
Barcelona, Spain.
Nicholas Asher. 1993. Reference to Abstract Objects
in Discourse. Kluwer Academic Publishers.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
J. Carletta, S. Evert, U. Heid, J. Kilgour, J. Robert-
son, and H. Voormann. 2003. The NITE XML
toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instru-
ments, and Computers, 35(3):353?363.
Central Intelligence Agency. 2006. The CIA World
Factbook. Potomac Books.
John Chamberlain, Massimo Poesio, and Udo Kr-
uschwitz. 2008. Addressing the Resource Bottle-
neck to Create Large-Scale Annotated Texts. In
Johan Bos and Rodolfo Delmonte, editors, Seman-
tics in Text Processing. STEP 2008 Conference Pro-
ceedings, volume 1 of Research in Computational
Semantics, pages 375?380. College Publications.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&C and Boxer. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33?36, Prague,
Czech Republic.
Mike Dowman, Valentin Tablan, Hamish Cunning-
ham, and Borislav Popov. 2005. Web-assisted an-
notation, semantic indexing and search of television
and radio news. In Proceedings of the 14th Interna-
tional World Wide Web Conference, pages 225?234,
Chiba, Japan.
U. Hahn, E. Buyko, K. Tomanek, S. Piao, J. Mc-
Naught, Y. Tsuruoka, and S. Ananiadou. 2007.
An annotation type system for a data-driven NLP
pipeline. In Proceedings of the Linguistic Annota-
tion Workshop, pages 33?40, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: a community resource for and by the
people. In Proceedings of the ACL 2010 Confer-
ence Short Papers, pages 68?73, Stroudsburg, PA,
USA.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The Penn Discourse Tree-
bank. In In Proceedings of LREC 2004, pages
2237?2240.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of en-
glish. Journal of Natural Language Engineering,
7(3):207?223.
Mark Steedman. 2001. The Syntactic Process. The
MIT Press.
96
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 301?309,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UGroningen: Negation detection with Discourse Representation Structures
Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen
{v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl
Center for Language and Cognition Groningen (CLCG)
University of Groningen, The Netherlands
Abstract
We use the NLP toolchain that is used to con-
struct the Groningen Meaning Bank to address
the task of detecting negation cue and scope,
as defined in the shared task ?Resolving the
Scope and Focus of Negation?. This toolchain
applies the C&C tools for parsing, using the
formalism of Combinatory Categorial Gram-
mar, and applies Boxer to produce seman-
tic representations in the form of Discourse
Representation Structures (DRSs). For nega-
tion cue detection, the DRSs are converted
to flat, non-recursive structures, called Dis-
course Representation Graphs (DRGs). DRGs
simplify cue detection by means of edge la-
bels representing relations. Scope detection
is done by gathering the tokens that occur
within the scope of a negated DRS. The re-
sult is a system that is fairly reliable for cue
detection and scope detection. Furthermore, it
provides a fairly robust algorithm for detect-
ing the negated event or property within the
scope.
1 Introduction
Nothing is more home to semantics than the phe-
nomenon of negation. In classical theories of mean-
ing all states of affairs are divided in two truth val-
ues, and negation plays a central role to determine
which truth value is at stake for a given sentence.
Negation lies at the heart of deductive inference, of
which consistency checking (searching for contra-
dictions in texts) is a prime example in natural lan-
guage understanding.
It shouldn?t therefore come as a surprise that
detecting negation and adequately representing its
scope is of utmost importance in computational se-
mantics. In this paper we present and evaluate a sys-
tem that transforms texts into logical formulas ? us-
ing the C&C tools and Boxer (Bos, 2008) ? in the
context of the shared task on recognising negation
in English texts (Morante and Blanco, 2012).
We will first sketch the background and the basics
of the formalism that we employ in our analysis of
negation (Section 2). In Section 3 we explain how
we detect negation cues and scope. Finally, in Sec-
tion 4 we present the results obtained in the shared
task, and we discuss them in Section 5.
2 Background
The semantic representations that are used in this
shared task on detecting negation in texts are con-
structed by means of a pipeline of natural language
processing components, of which the backbone is
provided by the C&C tools and Boxer (Curran et
al., 2007). This tool chain is currently in use semi-
automatically for constructing a large semantically
annotated corpus, the Groningen Meaning Bank
(Basile et al, 2012).
The C&C tools are applied for tagging the data
with part-of-speech and super tags and for syntactic
parsing, using the formalism of Combinatory Cate-
gorial Grammar, CCG (Steedman, 2001). The out-
put of the parser, CCG derivations, form the in-
put of Boxer, producing formal semantic representa-
tions in the form of Discourse Representation Struc-
tures (DRSs), the basic meaning-carrying structures
in the framework of Discourse Representation The-
ory (Kamp and Reyle, 1993). DRT is a widely ac-
cepted formal theory of natural language meaning
that has been used to study a wide range of linguistic
301
<<
IPRPNP?v0. ( x1
person(x1)
 ? (v0 @ x1) )
>
sawVBD(S[dcl]\NP)/NP?v0. ?v1. ?v2. (v1 @ ?v3. (v0 @ ?v4. ( e5
see(e5)agent(e5, v3)patient(e5, v4)
 ; (v2 @ e5) ) ) )
<
nothingDTNP?v0. ? ( x1
thing(x1)
 ; (v0 @ x1) )
*
suspiciousJJS[adj]\NP?v0. ?v1. (v0 @ ?v2. ( suspicious(v2)  ; (v1 @ v2) ) )
suspiciousNP\NP?v0. ?v1. (v0 @ ?v2. ( suspicious(v2)  ; (v1 @ v2)) )
nothing suspiciousNP?v0. ? ( x1
thing(x1)suspicious(x1)
 ; (v0 @ x1) )
saw nothing suspiciousS[dcl]\NP?v0. ?v1. (v0 @ ?v2. ? ( x3 e4
thing(x3)suspicious(x3)see(e4)agent(e4, v2)patient(e4, x3)
 ; (v1 @ e4) ) )
I saw nothing suspiciousS[dcl]?v0. ( x1
person(x1)
 ? ? ( x2 e3
thing(x2)suspicious(x2)see(e3)agent(e3, x1)patient(e3, x2)
 ; (v0 @ e3) ) )
..S[dcl]\S[dcl]?v0.v0
I saw nothing suspicious .S[dcl]?v0. ( x1
person(x1)
 ? ? ( x2 e3
thing(x2)suspicious(x2)see(e3)agent(e3, x1)patient(e3, x2)
 ; (v0 @ e3) ) )
Figure 1: CCG derivation and unresolved semantics for the sentence ?I saw nothing suspicious?
phenomena, such as anaphoric pronouns, temporal
relations (Kamp and Reyle, 1993), presuppositions
(Van der Sandt, 1992), abstract anaphora and rhetor-
ical relations (Asher, 1993; Asher and Lascarides,
2003).
A DRS contains two parts: a set of of discourse
referents, and a set of conditions. Negation is repre-
sented in a condition by a unary operator in DRT. As
an example, Figure 1 shows the derivation for one
sentence as produced by the pipeline, illustrating
how lexical semantic entries are used to construct
a DRS for a whole sentence, guided by the syntac-
tic parse tree. Here, machinery of the ?-calculus is
employed to deal with variable renaming when re-
quired.
DRSs are recursive structures by nature. They can
be produced in several formats (in Prolog or XML)
and translated into first-order formulas. The rep-
resentations can also be generated as a set of tu-
ples, forming together a directed graph equivalent
to the original DRS, where discourse referents and
symbols are nodes and predicates and relations are
viewed as labelled edges. These ?flat? Discourse
Representation Graphs, DRGs, are often more suit-
able for certain processing tasks. The tuples also
hold additional information, mapping DRS condi-
tions to surface tokens. This mapping is important
in tasks where surface realisation plays a role. We
also use it in this shared task to get back from com-
plex structures to a flat, token-based annotation of
scope.
3 Method
The shared task aims at detecting negation in text ?
systems are supposed to label tokens that are in the
scope of negation, and also identify the token that
triggered the negation. The basic idea of our method
was to run the existing Boxer system for semantic
analysis, then traverse the produced DRSs, and, on
encountering an embbeded negated DRS, output the
302
tokens associated with this negation, as well as the
token triggering it.
As this isn?t what Boxer is usually asked to do,
it required some bookkeeping adjustments. Boxer?s
anaphora resolution feature was turned off because
it is not necessary for the task and would lead
to unwanted inclusion of antecedents into negation
scopes. Also, its features for representing tense in-
formation and rhetorical relations were not used.
The rest of this section pays a closer look at how
negation cues are detected and how scope is as-
signed to tokens. We address the issues of trans-
lating a formal representation such as DRS into the
format required by the shared task ? a represen-
tation more oriented at the surface form. We sub-
mitted two runs of our system, which both used the
C&C tools and Boxer. For the second run, we added
some postprocessing steps that tune the result to-
wards a higher performance, especially on scope de-
tection. While these postprocessing steps improve
performance, many of them may be specific to the
genre and style of the texts used in the shared task.
3.1 Cue detection
Since Boxer has been developed as a system to
generate full semantic representations, its lexicon
implicitly contains a list of negation cues: those
words giving rise to semantic representations of the
form ?B, where B is the DRS representing the
meaning of the scope of the negation. Key examples
here are determiners and noun phrases (no, none, no-
one), and verb phrase negation (not).
However, negation detection is not the primary
function of Boxer, as it is part of the larger aim of
providing interpretable semantic representation for
English texts, and doing so robustly. So for the cur-
rent task, after investigating the development data
made available by the organisers, Boxer?s lexicon
was revised at a few points to account for particu-
lar negation cues that Boxer originally did not de-
tect. This included the detection of never as negation
cue, as well as words with a negative prefix or suffix
(e.g. inadequate, motionless). These affix negations
were detected using an automatically generated list
of negatively affixed nouns, adjectives and adverbs
from WordNet (Fellbaum, 1998). The list was cre-
ated by means of an algorithm that returns all nouns,
adjectives and adverbs in WordNet that start with
one of a, an, dis, in, il, im, ir, non, non-, un, or end
with one of less, lessness, lessly, and have a direct
antonym such that the lemma form equals the stem
of the affixed negation (i.e., without the affix).
On the other hand, not everything that introduces
a negated DRS in Boxer is a typical negation cue.
A case in point is the quantifier all, which up un-
til the shared task received a semantics similar to
?P?Q.??x(P (x)??Q(x)) in Boxer?s lexicon. As
a consequence, Boxer predicted all to be a nega-
tion cue trigger, in contrast to the shared task gold
standard data. Such instances were replaced by log-
ically equivalent representations (in the case of all:
?P?Q.?x(P (x) ? Q(x))).
In order to obtain the tokens that triggered the
negated DRS, Boxer?s DRG output was used. Oc-
currences of predicates, relations and connectives in
the DRG output carry explicit associations with the
tokens in the input whose lexical entries they come
from. For basic cue detection, the system annotates
as a negation cue those tokens (or affixes) associated
with the connective? (represented in the DRG as the
relation subordinates:neg). Example (1) shows a
part of the DRG?s tuple format that represents the
negation cue ?no?. Argument structure tuples (la-
beled concept and instance) are also shown, cor-
responding to a noun in the negation scope, as in
?no problem?. The first and third columns represent
nodes of the DRG graph (both discourse units in this
example), the second column represents the label of
the edge between the nodes, and the fourth column
shows the token associated with the relation (if any).
(1)
... ... ... ...
k1 subordinates:neg k6 no
k6 concept c1:problem
c1:problem instance k6:x1 problem
... ... ... ...
In this case, the token ?no? is detected as negation
cue because it is associated with the relation subor-
dinates:neg.
In the case of affix negation, ideally only the af-
fix should be associated with the negation tuple, and
the stem with a corresponding instance tuple. How-
ever, since the last column contains tokens, this does
not easily fit into the format. We therefore associate
the whole affix-negated token with the negation tu-
ple and use separate tuples for affix and stem in order
to preserve the information which part of the word
303
is the cue and which part is in the scope of the nega-
tion. The resulting three tuples from a sentence con-
taining the word ?injustice? are shown in the follow-
ing example:
(2)
... ... ... ...
k3 subordinates:neg k4 injustice
k4 concept c2:in:71
k4 concept c3:justice:1
... ... ... ...
The target nodes of the two argument structure tu-
ples (labeled concept because ?injustice? is a noun)
are labeled with the relevant part of the affix-negated
word, and a special ID to indicate the presence of
a prefix or suffix. This information is used by the
script producing the token-based result format. Al-
though multi-word cues, such as neither...nor and on
the contrary, were not correctly predicted as such by
Boxer, no effort was made to include them. Due to
the token-based detection approach, the cue detec-
tion algorithm would have to be severly complicated
to include these cases as one negation cue. Because
of the relatively small frequency of multi-word cues,
we decided not to include special processing steps to
account for them.
The second run includes some postprocessing
steps implemented on top of the basic output. Since
Boxer is not designed to deal with dialogue, inter-
jections were originally ignored as negation cues.
Therefore, the postprocessing script added the word
?no? as a negation cue (with empty scope) when it
occurred as an interjection (tagged ?UH?). It also ex-
cluded negations with the cue ?no? when occurring
as part of the expression ?no doubt? and not imme-
diately preceded by a verb with the lemma ?have?
or ?be? as in ?I have no doubt that...?, which is to be
annotated as a negation. High precision and recall
for cue detection on the training data suggested that
no further processing steps were worth the effort.
3.2 Scope detection
The tokens in the scope of a negation are deter-
mined on the basis of the detected negation cue. It
is associated with the negation connective of some
negated DRS ?B, so the system annotates as scope
all the tokens (and stems in the case of affix nega-
tion) directly or indirectly associated with predicates
and relations inside B. This includes tokens di-
rectly associated with predicates that appear within
the negated DRS, as well as those predicates outside
of the negated DRS whose discourse referent occurs
within the negation scope as the second argument of
a thematic role relation.
<P
RN?v0. <P(
x <1 Np
vNN Np(
ers.o <1(
vnvRs)s0nv <1(
?oN.e Np@><P(
VrNBN Np@><1(
Figure 2: DRS for the sentence ?I saw nothing suspi-
cious?
An example is given in Figure 2, where e.g. the
tokens see and suspicious are associated, respec-
tively, with see(e4) and suspicious(x3). Although
the predicate person(x2) associated with the pro-
noun I occurs outside of the negated DRS, its refer-
ent occurs as an argument within the negated DRS
in Agent(e4, x2) and therefore it is taken to be part
of the scope of the negation. The desired scope is
thus detected, containing the tokens I, saw and sus-
picious.
Again, in the second run some postprocessing
steps were implemented to improve performance.
We observed that the scopes in the manually anno-
tated data were usually continuous, except for nega-
tion cues within them. However, the scopes pro-
duced by the DRS algorithm contained many ?gaps?
between the tokens of the detected scope, due to an
intrinsic feature of the DRS representation. Conven-
tionally, DRSs only explicitly contain content words
(i.e. nouns, verbs, adjectives, adverbs), while func-
tion words, such as determiners, modals and auxil-
iary verbs, are represented e.g. as structural proper-
ties or temporal features, or not at all, as in the case
of the infinitival to. Thus, when retrieving the sur-
face representation of the negated scopes from the
DRSs, not all structural properties can be directly as-
sociated with a surface token and thus not all tokens
required for the scope are retrieved. Because in the
gold standard annotation these function words were
considered part of the negation scope, we designed
an ad hoc mechanism to include them, namely filling
all the gaps that occur in the negation scope (leaving
304
out the negation cue). For the same reason, deter-
miners immediately preceding the detected scopes
were added in postprocessing. Finally, conjunc-
tions were removed from the beginning of negation
scopes, because they were sometimes wrongly rec-
ognized by our pipeline as adverbs.
3.3 Negated event/property detection
Although not among our main goals, we also ad-
dressed the issue of detecting the ?negated event or
property? in negation scopes within factual state-
ments. This is done using a heuristic algorithm that
uses the detected scope, as well as the syntax tree
provided as part of the data.
Since the scope is provided as a set of tokens, the
first step is to identify what we call the scope con-
stituent, i.e. a constituent in the syntax tree that cor-
responds to the scope. This is done by going through
the tokens in the scope from left to right and de-
termining for each token the largest constituent that
starts with this token. The first constituent found in
this way the category of whose root is one of SBAR,
S and VP is taken to be the scope constituent.
In the second step, the scope VP is determined
as the first VP encountered when doing a pre-order,
left-to-right traversal of the scope constituent. The
first verb directly dominated by this VP node deter-
mines how the process continues: (i) For non-factual
modals (e.g. may, must, should), no event/property
is annotated. (ii) For futurity modals (e.g. would,
will, shall), the negated event/property is determined
recursively by taking the first embedded VP as the
new scope VP. (iii) For forms of the verb be, the
algorithm first looks for the head of an embedded
ADJP or NP. If one is found, this is annotated as a
negated property. Otherwise, the verb is assumed to
be a passive auxiliary and the negated event/property
is again determined recursively on the basis of the
first embedded VP. (iv) In all other cases, the verb
itself is annotated as the negated event.
To limit the undesired detection of negated
events/properties outside of factual statements, the
algorithm is not applied to any sentence that con-
tains a question mark.
4 Results
Here we discuss our results on the Shared Task as
compared to the gold standard annotations provided
by (Morante and Daelemans, 2012). The output of
our two runs will be discussed with respect to Task 1.
The first run includes the results of our system with-
out postprocessing steps and in the second run the
system is augmented with the postprocessing steps,
as discussed in Section 3.
During the process of evaluating the results of the
training data, an issue with the method of evaluation
was discovered. In the first version of the evaluation
script precision was calculated using the standard
formula: tptp+fp . However, partial matches are ex-
cluded from this calculation (they are only counted
as false negatives), which means that in the case
of scopes(cue match), precision is calculated as the
number of exact scope matches (true positives) di-
vided by the number of exact scope matches plus
the number of completely wrong instances with no
overlap (false positives). As precision is a measure
for calculating correctly detected instances among
all detected instances, it seems that partial matches
should also be taken into account as detected in-
stance. Therefore, we proposed a new evaluation
method (B): tpsystem , where system includes all de-
tected negations of the current system (including
partial matches). However, this measure may be too
strict as it penalizes a system harder for outputting a
partially correct scope than for outputting no scope
at all.1 This choice between two evils seems to in-
dicate that precision is too simple a measure for tar-
gets where partial matches are possible. Therefore,
in our evaluation of scope detection, we will focus
on the scope tokens measure where there are no par-
tial matches. For cue and negated event/property de-
tection, we use the stricter, but more meaningful B
version. The difference here is almost negligible be-
cause these targets typically have just one token.
4.1 Run 1 (without postprocessing)
Table 1 shows the results of the basic system with-
out postprocessing, with the most important results
for our system highlighted. As we can see, the
basic system performs well on cue detection (F1=
1This was pointed out by an anonymous reviewer.
305
Table 1: Results of the first run (without postprocessing)
Task gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 264 261 219 33 45 86.90 82.95 84.88
Scopes(cue match): 249 261 32 37 217 46.38 12.85 20.12
Scopes(no cue match): 249 261 32 37 217 46.38 12.85 20.12
Scope tokens(no cue match): 1805 1821 1269 552 536 69.69 70.30 69.99
Negated(no cue match): 173 169 89 76 82 53.94 52.05 52.98
Full negation: 264 261 20 33 244 37.74 7.58 12.62
Cues B: 264 261 219 33 45 83.91 82.95 83.43
Scopes B (cue match): 249 261 32 37 217 12.26 12.85 12.55
Scopes B (no cue match): 249 261 32 37 217 12.26 12.85 12.55
Negated B (no cue match): 173 169 89 76 82 52.66 52.05 52.35
Full negation B: 264 261 20 33 244 7.66 7.58 7.62
Table 2: Results of the second run (with postprocessing)
Task gold system tp fp fn precision (%) recall (%) F1 (%)
Cues: 264 261 224 28 40 88.89 84.85 86.82
Scopes(cue match): 249 256 102 32 147 76.12 40.96 53.26
Scopes(no cue match): 249 256 102 32 147 76.12 40.96 53.26
Scope tokens(no cue match): 1805 2146 1485 661 320 69.20 82.27 75.17
Negated(no cue match): 173 201 111 85 59 56.63 65.29 60.65
Full negation: 264 261 72 28 192 72.00 27.27 39.56
Cues B: 264 261 224 28 40 85.82 84.85 85.33
Scopes B (cue match): 249 256 102 32 147 39.84 40.96 40.39
Scopes B (no cue match): 249 256 102 32 147 39.84 40.96 40.39
Negated B (no cue match): 173 201 111 85 59 55.22 65.29 59.83
Full negation B: 264 261 72 28 192 27.59 27.27 27.43
83.43%), and reasonably well on the detection of
scope tokens (F1= 69.99%).
Note that the results for Scopes(cue match) and
Scopes(no cue match) are the same for our system.
Since we make use of token-based cue detection, the
only cases of partial cue detection are instances of
multi-word cues, which, as discussed above, were
not accounted for in our system. In these cases, the
part of the cue that is not detected has a large chance
of becoming part of the scope of the cue that is de-
tected due to collocation. So, we hypothesize that
Scopes(cue match) and Scopes(no cue match) are the
same because in all cases of partial cue detection, the
scope incorrectly contains part of the gold-standard
cue, which affects both measures negatively.
There is a large discrepancy between the detec-
tion of scope tokens and the detection of com-
plete scopes, as the latter is low on both precision
(12.26%) and recall (12.85%). The relatively high
precision and recall for scope tokens (69.69% and
70.30%, respectively) suggests that there are many
cases of partial scope detection, i.e. cases where the
scope is either under- or overdetected with respect
to the gold standard scope. Since the postprocessing
steps for scope detection were developed to reduce
exactly this under- and overdetection, we expect that
the results for the second run are significantly better.
The same holds for negated/event property detection
(F1= 52.98%) since it uses the results from scope
detection.
4.2 Run 2 (with postprocessing)
Table 2 reports the results of the extended system,
which extends the basic system with postprocessing
steps for cue detection and especially for scope de-
tection. The postprocessing steps indeed result in
higher precision and recall for all tasks, except for
Scope tokens, which shows a negligible decrease in
precision (from 69.69% to 69.20%). This suggests
that there are more cases of overdetected scopes
than underdetected scopes, because the number of
wrongly detected tokens (false positives) increased
while the number of undetected scope tokens (false
negatives) decreased. This is probably due to the
gap-filling mechanism that was implemented as a
postprocessing step for scope detection, generaliz-
306
ing that all scopes should be continuous. We will
elaborate more on this point in the discussion in Sec-
tion 5.
As expected, the detection of complete scopes
shows the highest increase in F1 score (from 12.55%
to 40.39%). This indicates that the postprocessing
steps effectively targeted the weak points of the ba-
sic system.
While there are no postprocessing steps for
negated event or property detection, the F1 score for
this task also increases (from 52.35% to 59.83%),
as expected, due to the improvement in scope detec-
tion.
5 Discussion
Overall, we can say that both of our systems perform
well on cue detection, with a small increase when in-
cluding the postprocessing steps. This was expected
since the postprocessing for cue detection targeted
only two specific types of cues, namely, interjections
and occurrences of ?no doubt?. The scope detection
benefits consideraby from adding the postprocessing
steps, as was their main goal. In the final results of
the shared task, run 2 of our system ended second
out of five in the open track, while run 1 was ranked
last. We will here discuss some points that deserve
special attention.
5.1 Affix Negation
As discussed above, affix negations received a spe-
cial treatment because they were not originally de-
tected as negation cues in Boxer. In the DRS, the
token containing the affixed negation cue is associ-
ated with two predicates, representing the negative
affix and the negated stem. The algorithm secures
that only the affix is annotated as the negation cue
and that the negated stem is annotated as part of the
scope. An example of a sentence containing affix
negation is shown in (3) (cardboard 31).2
(3) a. [You do yourself an] in[justice]. gold
b. You do yourself an in[justice]. run1
c. You do yourself [an] in[justice]. run2
2In the following, boldfaced tokens represent the negation
cues, brackets embed the scope and underlining signifies the
negated event or property (subscripts added in case of multiple
negation cues).
Table 3: Results of negated event/property detection on
gold standard cue and scope annotation
Task prec.(%) rec.(%) F1(%)
Negated (no cue match): 64.06 76.88 69.89
Negated B (no cue match): 59.71 76.88 67.22
Note that in neither of the runs the complete scope
from the gold standard is detected, although post-
processing increases the recall of scope tokens by
adding the determiner ?an? to the scope of the nega-
tion. However, examples like this are not unambigu-
ous with respect to their negation scope. For ex-
ample, the sentence in (3) can be interpreted in two
ways: ?It is not the case that you do yourself (some-
thing that is) justice? and ?It is the case that you do
yourself (something that is) not justice?. While the
gold standard annotation assumes the former, wide-
scope reading, our system predicts the narrow scope
reading for the negation. The narrow scope read-
ing can be motivated by means of Grice?s Maxim of
Manner (Grice, 1975); the choice of an affix nega-
tion instead of a verbal negation signals a narrow
scope, because in case a wide scope negation is in-
tended, a verbal negation would be more perspicu-
ous. Thus, the difference in the output of our sys-
tem and the gold standard annotation is in this case
caused by a different choice in disambiguating nega-
tion scope, rather than by a shortcoming of the de-
tection algorithm.
5.2 Negated event/property detection
Although correct detection of the negated event or
property was not our prime concern, the results
obtained with our algorithm were quite promising.
Among the systems participating in the closed track
of task 1, our extended system is ranked third out
of seven for negated event/property detection even
though the performance on scope detection is lower
than all of the other systems in this track. Since
negated event/property detection depends on the de-
tected scope, it seems that our heuristic algorithm
for detecting the negated event/property is very ro-
bust against noisy input. The performance of the de-
tection algorithm on the gold-standard annotation of
scopes is shown in Table 3. Although we cannot
compare these results to the performance of other
systems on the gold standard data, it should be noted
307
that the results shown here are unmatched by the
test results of any other system. It would therefore
be worthwile for future work to refine the negated
event/property detection algorithm outlined here.
5.3 Postprocessing
The results for the two versions of our system
showed that the postprocessing steps implemented
in the extended system improved the results consid-
erably, especially for scope detection. Example (4)
(cardboard 62) shows the effect of postprocessing on
the detection of scopes for negative interjections.
(4) a. ?No1, [I saw]2 nothing2.? gold
b. ?[No], [I saw] nothing.? run1
c. ?[No1, I saw]2 nothing2.? run2
In Run 1, the system correctly detects the cue ?noth-
ing? and the event ?saw?, although the detected
scope is too wide due to an error in the output of
the parser we used. In Run 2, postprocessing also
correctly recognizes the interjection ?no? as a nega-
tion cue. Gap filling in this case makes the scope
overdetection worse by also adding the comma to
the scope. A similar case of the overdetection of
scope is shown in (5) (cardboard 85).
(5) a. [The box] is a half-pound box of honey-
dew tobacco and [does] not [help us in
any way]. gold
b. [The box] is a half-pound box of hon-
eydew tobacco and does not [help us in
any way]. run1
c. [The box is a half-pound box of honey-
dew tobacco and does] not [help us in
any way]. run2
Note that in Run 1 the first part of the coordinated
structure is correctly excluded from the scope of
the negation, but the auxiliary ?does? is incorrectly
not counted as scope. The gap-filling mechanism
then adds the intermediary part to the scope of the
negation, resulting in an increase in recall for scope
tokens detection (since ?does? is now part of the
scope) but a lower precision because of the overgen-
eration of the coordinated part.
Nevertheless, the increased precision and recall
for scope detection can mainly be ascribed to the
gap-filling mechanism implemented in the postpro-
cessing steps for scope detection. As discussed
above, the presence of gaps in the original output
is due to the non-sequential nature of the DRT rep-
resentation and the fact that function words are not
directly associated with any element in the represen-
tations. This suggests that future work on surface re-
alisation from DRSs should focus on translating the
structural properties of DRSs into function words.
5.4 Differences between texts
We noted that there was a difference between the
performance on text 1 (The Adventure of the Red
Circle) and text 2 (The Adventure of the Cardboard
Box). The results for text 2 were overall higher than
the results for text 1 (except for a 1% decline in re-
call for Full negation). There was a higher scope
precision for text 2 and after the postprocessing steps
an even larger difference was found for scope de-
tection (15% versus 44% increase in F1 score for
Scopes). We hypothesize that this difference may be
due to a higher number of multiword expressions in
text 1 (7 vs. 2) and to the fact that text 1 seems to
have more scopes containing gaps. This latter ob-
servation is supported by the fact that gap filling re-
sults in more overgeneration (more false positives),
which is reflected in the ratios of false positives in
text 1 (38%) and text 2 (27%). Thus, while the post-
processing steps improve performance, they seem to
be genre and style dependent. This motivates further
development of the ?clean?, theoretically motivated
version of our system in order to secure domain-
independent broad coverage of texts, which is the
goal of the Groningen Meaning Bank project.
6 Conclusion
Participating in this shared task on negation detec-
tion gave us a couple of interesting insights into our
natural language processing pipeline that we are de-
veloping in the context of the Groningen Meaning
Bank. It also showed that it is not easy to trans-
fer the information about negation from a formal,
logical representation of scope to a theory-neutral
surface-oriented approach. The results were in line
with what we expected beforehand, with the highest
loss appearing in the awkward translation from one
formalism to another.
308
References
Nicholas Asher and Alex Lascarides. 2003. Logics of
conversation. Studies in natural language processing.
Cambridge University Press.
Nicholas Asher. 1993. Reference to Abstract Objects in
Discourse. Kluwer Academic Publishers.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically an-
notated corpus. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?12). To appear.
Johan Bos. 2008. Wide-Coverage Semantic Analysis
with Boxer. In J. Bos and R. Delmonte, editors, Se-
mantics in Text Processing. STEP 2008 Conference
Proceedings, volume 1 of Research in Computational
Semantics, pages 277?286. College Publications.
James Curran, Stephen Clark, and Johan Bos. 2007. Lin-
guistically Motivated Large-Scale NLP with C&C and
Boxer. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 33?36, Prague, Czech Republic.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
H. P. Grice. 1975. Logic and conversation. In P. Cole
and J. L. Morgan, editors, Syntax and Semantics: Vol.
3: Speech Acts, pages 41?58. Academic Press, San
Diego, CA.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and DRT. Kluwer,
Dordrecht.
Roser Morante and Eduardo Blanco. 2012. *SEM 2012
Shared Task: Resolving Scope and Focus of Negation.
In Proceedings of the First Joint Conference on Lexi-
cal and Computational Semantics (*SEM 2012), Mon-
treal, Canada. To appear.
Roser Morante and Walter Daelemans. 2012.
ConanDoyle-neg: Annotation of negation in Conan
Doyle stories. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?12). To appear.
Mark Steedman. 2001. The Syntactic Process. The MIT
Press.
Rob Van der Sandt. 1992. Presupposition Projection as
Anaphora Resolution. Journal of Semantics, 9:333?
377.
309
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 100?107,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Sentiment analysis on Italian tweets
Valerio Basile
University of Groningen
v.basile@rug.nl
Malvina Nissim
University of Bologna
malvina.nissim@unibo.it
Abstract
We describe TWITA, the first corpus of Italian
tweets, which is created via a completely au-
tomatic procedure, portable to any other lan-
guage. We experiment with sentiment anal-
ysis on two datasets from TWITA: a generic
collection and a topic-specific collection. The
only resource we use is a polarity lexicon,
which we obtain by automatically matching
three existing resources thereby creating the
first polarity database for Italian. We observe
that albeit shallow, our simple system captures
polarity distinctions matching reasonably well
the classification done by human judges, with
differences in performance across polarity val-
ues and on the two sets.
1 Introduction
Twitter is an online service which lets subscribers
post short messages (?tweets?) of up to 140 charac-
ters about anything, from good-morning messages
to political stands.
Such micro texts are a precious mine for grasping
opinions of groups of people, possibly about a spe-
cific topic or product. This is even more so, since
tweets are associated to several kinds of meta-data,
such as geographical coordinates of where the tweet
was sent from, the id of the sender, the time of the
day ? information that can be combined with text
analysis to yield an even more accurate picture of
who says what, and where, and when. The last years
have seen an enormous increase in research on de-
veloping opinion mining systems of various sorts
applying Natural Language Processing techniques.
Systems range from simple lookups in polarity or
affection resources, i.e. databases where a polarity
score (usually positive, negative, or neutral) is asso-
ciated to terms, to more sophisticated models built
through supervised, unsupervised, and distant learn-
ing involving various sets of features (Liu, 2012).
Tweets are produced in many languages, but most
work on sentiment analysis is done for English (even
independently of Twitter). This is also due to the
availability of tools and resources. Developing sys-
tems able to perform sentiment analysis for tweets in
a new language requires at least a corpus of tweets
and a polarity lexicon, both of which, to the best of
our knowledge, do not exist yet for Italian.
This paper offers three main contributions in this
respect. First, we present the first of corpus of tweets
for Italian, built in such a way that makes it possi-
ble to use the exact same strategy to build similar
resources for other languages without any manual
intervention (Section 2). Second, we derive a polar-
ity lexicon for Italian, organised by senses, also us-
ing a fully automatic strategy which can replicated
to obtain such a resource for other languages (Sec-
tion 3.1). Third, we use the lexicon to automatically
assign polarity to two subsets of the tweets in our
corpus, and evaluate results against manually anno-
tated data (Sections 3.2?3.4).
2 Corpus creation
We collected one year worth of tweets, from Febru-
ary 2012 to February 2013, using the Twitter fil-
ter API1 and a language recognition strategy which
1https://dev.twitter.com/docs/api/1/
post/statuses/filter
100
we describe below. The collection, named TWITA,
consists of about 100 million tweets in Italian en-
riched with several kinds of meta-information, such
as the time-stamp, geographic coordinates (when-
ever present), and the username of the twitter. Addi-
tionally, we used off-the-shelf language processing
tools to tokenise all tweets and tag them with part-
of-speech information.
2.1 Language detection
One rather straightforward way of creating a corpus
of language-specific tweets is to retrieve tweets via
the Twitter API which are matched with strongly
language-representative words. Tjong Kim Sang
and Bos (2012) compile their list of highly typ-
ical Dutch terms manually to retrieve Dutch-only
tweets. While we also use a list of strongly repre-
sentative Italian words, we obtain such list automat-
ically. This has the advantage of making the proce-
dure more objective and fully portable to any other
language for which large reference corpora are avail-
able. Indeed, we relied on frequency information de-
rived from ItWac, a large corpus of Italian (Baroni et
al., 2009), and exploited Google n-grams to rule out
cross-language homographs. For boosting precision,
we also used the publicly available language recog-
nition software langid.py (Lui and Baldwin, 2012).
The details of the procedure are given below:
1. extract the 1.000 most frequent lemmas from
ItWaC;
2. extract tweets matched by the selected repre-
sentative words and detect the language using a
freely available software;2
3. filter out the terms in the original list which
have high frequency in a conflicting language.
Frequency is obtained from Google N-grams;
4. use high frequency terms in the resulting
cleaner list to search the Twitter API.
The 20 top terms which were then used to match
Italian-only tweets are: vita Roma forza alla quanto
amore Milano Italia fare grazie della anche peri-
odo bene scuola dopo tutto ancora tutti fatto. In the
2Doing so, we identify other languages that share charac-
ter sequences with Italian. The large majority of tweets in the
first search were identified as Portuguese, followed by English,
Spanish and then Italian.
extraction, we preserved metadata about user, time,
and geographical coordinates whenever available.
Both precision and recall of this method are hard
to assess. We cannot know how many tweets that
are in fact Italian we?re actually missing, but the
amount of data we can in any case collect is so high
that the issue is not so relevant.3 Precision is more
important, but manual checking would be too time-
consuming. We inspected a subset of 1,000 tweets
and registered a precision of 99.7% (three very short
tweets were found to be in Spanish). Considering
that roughly 2.5% of the tweets also include the ge-
ographical coordinates of the device used to send the
message, we assessed an approximate precision in-
directly. We plotted a one million tweets randomly
chosen from our corpus and obtained the map shown
in Figure 1 (the map is clipped to the Europe area for
better identifiability). We can see that Italy is clearly
outlined, indicating that precision, though not quan-
tifiable, is likely to be satisfactory.
Figure 1: Map derived by plotting geo-coordinates of
tweets obtained via our language-detection procedure.
2.2 Processing
The collected tweets have then been enriched with
token-level, POS-tags, and lemma information.
Meta-information was excluded from processing.
So for POS-tagging and lemmatisation we substi-
tuted hashtags, mentions (strings of the form @user-
3This is because we extract generic tweets. Should one want
to extract topic-specific tweets, a more targeted list of charac-
terising terms should be used.
101
name referring to a specific user) and URLs with a
generic label. All the original information was re-
inserted after processing. The tweets were tokenised
with the UCTO rule-based tokeniser4 and then POS-
tagged using TreeTagger (Schmid, 1994) with the
provided Italian parameter file. Finally, we used the
morphological analyser morph-it! (Zanchetta and
Baroni, 2005) for lemmatisation.
3 Sentiment Analysis
The aim of sentiment analysis (or opinion mining) is
detecting someone?s attitude, whether positive, neu-
tral, or negative, on the basis of some utterance or
text s/he has produced. While a first step would be
determining whether a statement is objective or sub-
jective, and then only in the latter case identify its
polarity, it is often the case that only the second task
is performed, thereby also collapsing objective state-
ments and a neutral attitude.
In SemEval-2013?s shared task on ?Sentiment
Analysis in Twitter?5 (in English tweets), which is
currently underway, systems must detect (i) polar-
ity of a given word in a tweet, and (ii) polarity of
the whole tweet, in terms of positive, negative, or
neutral. This is also what we set to do for Italian.
We actually focus on (ii) in the sense that we do not
evaluate (i), but we use and combine each word?s
polarity to obtain the tweet?s overall polarity.
Several avenues have been explored for polar-
ity detection. The simplest route is detecting the
presence of specific words which are known to ex-
press a positive, negative or neutral feeling. For
example, O?Connor et al (2010) use a lexicon-
projection strategy yielding predictions which sig-
nificantly correlate with polls regarding ratings of
Obama. While it is clear that deeper linguistic anal-
ysis should be performed for better results (Pang and
Lee, 2008), accurate processing is rather hard on
texts such as tweets, which are short, rich in abbrevi-
ations and intra-genre expressions, and often syntac-
tically ill-formed. Additionally, existing tools for the
syntactic analysis of Italian, such as the DeSR parser
(Attardi et al, 2009), might not be robust enough for
processing such texts.
Exploiting information coming from a polarity
4http://ilk.uvt.nl/ucto/
5www.cs.york.ac.uk/semeval-2013/task2/.
lexicon, we developed a simple system which as-
signs to a given tweet one of three possible values:
positive, neutral or negative. The only input to the
system is the prior polarity coded in the lexicon per
word sense. We experiment with several ways of
combining all the polarities obtained for each word
(sense) in a given tweet. Performance is evaluated
against manually annotated tweets.
3.1 Polarity lexicon for Italian
Most polarity detection systems make use, in some
way, of an affection lexicon, i.e. a language-specific
resource which assigns a negative or positive prior
polarity to terms. Such resources have been built by
hand or derived automatically (Wilson et al, 2005;
Wiebe and Mihalcea, 2006; Esuli and Sebastiani,
2006; Taboada et al, 2011, e.g.). To our knowl-
edge, there isn?t such a resource already available
for Italian. Besides hand-crafting, there have been
proposals for creating resources for new languages
in a semi-automatic fashion, using manually anno-
tated sets of seeds (Pitel and Grefenstette, 2008),
or exploiting twitter emoticons directly (Pak and
Paroubek, 2011). Rather than creating a new po-
larity lexicon from scratch, we exploit three exist-
ing resources, namely MultiWordNet (Pianta et al,
2002), SentiWordNet (Esuli and Sebastiani, 2006;
Baccianella et al, 2010), and WordNet itself (Fell-
baum, 1998) to obtain an annotated lexicon of senses
for Italian. Basically, we port the SentiWordNet an-
notation to the Italian portion of MultiWordNet, and
we do so in a completely automatic fashion.
Our starting point is SentiWordNet, a version
of WordNet where the independent values positive,
negative, and objective are associated to 117,660
synsets, each value in the zero-one interval. Mul-
tiWordNet is a resource which aligns Italian and En-
glish synsets and can thus be used to transfer polar-
ity information associated to English synsets in Sen-
tiWordNet to Italian synsets. One obstacle is that
while SentiWordNet refers to WordNet 3.0, Multi-
WordNet?s alignment holds for WordNet 1.6, and
synset reference indexes are not plainly carried over
from one version to the next. We filled this gap using
an automatically produced mapping between synsets
of Wordnet versions 1.6 and 3.0 (Daud et al, 2000),
making it possible to obtain SentiWordNet annota-
tion for the Italian synsets of MultiWordNet. The
102
coverage of our resource is however rather low com-
pared to the English version, and this is due to the
alignment procedure which must exploit an earlier
version of the resource. The number of synsets is
less than one third of that of SentiWordNet.
3.2 Polarity assignment
Given a tweet, our system assigns a polarity score to
each of its tokens by matching them to the entries in
SentiWordNet. Only matches of the correct POS are
allowed. The polarity score of the complete tweet is
given by the sum of the polarity scores of its tokens.
Polarity is associated to synsets, and the same
term can occur in more than one synset. One option
would be to perform word sense disambiguation and
only pick the polarity score associated with the in-
tended sense. However, the structure of tweets and
the tools available for Italian do not make this op-
tion actually feasible, although we might investigate
it in the future. As a working solution, we compute
the positive and negative scores for a term occurring
in a tweet as the means of the positive and negative
scores of all synsets to which the lemma belongs to
in our lexical resource. The resulting polarity score
of a lemma is the difference between its positive and
negative scores. Whenever a lemma is not found in
the database, it is given a polarity score of 0.
One underlying assumption to this approach is
that the different senses of a given word have simi-
lar sentiment scores. However, because this assump-
tion might not be true in all cases, we introduce the
concept of ?polypathy?, which is the characterising
feature of a term exhibiting high variance of polarity
scores across its synsets. The polypathy of a lemma
is calculated as the standard deviation of the polar-
ity scores of the possible senses. This information
can be used to remove highly polypathic words from
the computation of the polarity of a complete tweet,
for instance by discarding the tokens with a poly-
pathy higher than a certain threshold. In particular,
for the experiments described in this paper, a thresh-
old of 0.5 has been empirically determined. To give
an idea, among the most polypathic words in Senti-
WordNet we found weird (.62), stunning (.61), con-
flicting (.56), terrific (.56).
Taboada et al (2011) also use SentiWordNet for
polarity detection, either taking the first sense of a
term (the most frequent in WordNet) or taking the
average across senses, as we also do ? although
we also add the polypathy-aware strategy. We can-
not use the first-sense strategy because through the
alignment procedure senses are not ranked accord-
ing to frequency anymore.
3.3 Gold standard
For evaluating the system performance we created
two gold standard sets, both annotated by three inde-
pendent native-speakers, who were given very sim-
ple and basic instructions and performed the anno-
tation via a web-based interface. The value to be
assigned to each tweet is one out of positive, neu-
tral, or negative. As mentioned, the neutral value
includes both objective statements as well as subjec-
tive statements where the twitter?s position is neutral
or equally positive and negative at the same time (see
also (Esuli and Sebastiani, 2007)).
All data selected for annotation comes from
TWITA. The first dataset consists of 1,000 ran-
domly selected tweets. The second dataset is topic-
oriented, i.e. we randomly extracted 1,000 tweets
from all those containing a given topic. Topic-
oriented, or target-dependent (Jiang et al, 2011),
classification involves detecting opinions about a
specific target rather than detecting the more gen-
eral opinion expressed in a given tweet. We identify
a topic through a given hashtag, and in this experi-
ment we chose the tag ?Grillo?, the leader of an Ital-
ian political movement. While in the first set the an-
notators were asked to assign a polarity value to the
message of the tweet as a whole, in the second set
the value was to be assigned to the author?s opinion
concerning the hashtag, in this case Beppe Grillo.
This is a relevant distinction, since it can happen
that the tweet is, say, very negative about someone
else while being positive or neutral about Grillo at
the same time. For example, the tweet in (1), ex-
presses a negative opinion about Vendola, another
Italian politician, but is remaining quite neutral to-
wards Grillo, the target of the annotation exercise.
(1) #Vendola da` del #populista a #Grillo e` una
barzelletta o ancora non si e` accorto che il
#comunismo e` basato sul populismo?
Thus, in the topic-specific set we operate a more
subtle distinction when assigning polarity, some-
103
thing which should make the task simpler for a hu-
man annotator while harder for a shallow system.
As shown in Table 1, for both sets the annotators
detected more than half of the tweets as neutral, or
they were disagreeing ? without absolute majority,
a tweet is considered neutral; however these cases
account for only 7.7% in the generic set and 6.9% in
the topic-specific set.
Table 1: Distribution of the tags assigned by the absolute
majority of the raters
set positive negative neutral
generic 94 301 605
topic-specific 293 145 562
Inter-annotator agreement was measured via Fleiss?
Kappa across three annotators. On the generic set,
we found an agreement of Kappa = 0.321, while
on the topic-specific set we found Kappa = 0.397.
This confirms our expectation that annotating topic-
specific tweets is actually an easier task. We might
also consider using more sophisticated and fine-
grained sentiment annotation schemes which have
proved to be highly reliable in the annotation of En-
glish data (Su and Markert, 2008a).
3.4 Evaluation
We ran our system on both datasets described in Sec-
tion 3.3, using all possible variations of two parame-
ters, namely all combinations of part-of-speech tags
and the application of the threshold scheme, as dis-
cussed in Section 3.2. We measure overall accuracy
as well as precision, recall, and f-score per polar-
ity value. In Tables 2 and 3, we report best scores,
and indicate in brackets the associated POS combi-
nation. For instance, in Table 2, we can read that the
recall of 0.701 for positive polarity is obtained when
the system is run without polypathy threshold and
using nouns, verbs, and adjectives (nva).
We can draw several observations from these re-
sults. First, a fully automatic approach that lever-
ages existing lexical resources performs better than
a wild guess. Performance is boosted when highly
polypathic words are filtered out.
Second, while the system performs well at recog-
nising especially neutral but also positive polarity,
it is really bad at detecting negative polarity. Es-
pecially in the topic-specific set, the system assigns
Table 2: Best results on the generic set. In brackets POS
combination: (n)oun, (v)erb, (a)djective, adve(r)b.
without polypathy threshold, best accuracy: 0.505 (a)
positive negative neutral
best precision 0.440 (r) 0.195 (v) 0.664 (nar)
best recall 0.701 (nva) 0.532 (var) 0.669 (a)
best F-score 0.485 (nvar) 0.262 (vr) 0.647 (a)
with polypathy threshold, best accuracy: 0.554 (r)
positive negative neutral
best precision 0.420 (r) 0.233 (v) 0.685 (nar)
best recall 0.714 (nvar) 0.457 (var) 0.785 (r)
best F-score 0.492 (nar) 0.296 (vr) 0.698 (r)
Table 3: Best results on the topic-specific set. In brackets
POS combination: (n)oun, (v)erb, (a)djective, adve(r)b.
without polypathy threshold, best accuracy: 0.487 (r)
positive negative neutral
best precision 0.164 (a) 0.412 (a) 0.617 (nar)
best recall 0.593 (nva) 0.150 (nr) 0.724 (a)
best f-score 0.251 (nv) 0.213 (nr) 0.637 (a)
with polypathy threshold, best accuracy: 0.514 (r)
positive negative neutral
best precision 0.163 (nvar) 0.414 (a) 0.623 (nar)
best recall 0.593 (nvar) 0.106 (nar) 0.829 (r)
best f-score 0.256 (nvar) 0.166 (nar) 0.676 (r)
too many positive labels in place of negative ones,
causing at the same time positive?s precision and
negative?s recall to drop. We believe there are two
explanations for this. The first one is the ?positive-
bias? of SentiWordNet, as observed by Taboada et
al. (2011), which causes limited performance in the
identification of negative polarity. The second one
is that we do not use any syntactic clues, such as for
detecting negated statements. Including some strat-
egy for dealing with this should improve recognition
of negative opinions, too.
Third, the lower performance on the topic-specific
dataset confirms the intuition that this task is harder,
mainly because we operate a more subtle distinc-
tion when assigning a polarity label as we refer to
one specific subject. Deeper linguistic analysis, such
as dependency parsing, might help, as only certain
words would result as related to the intended target
while others wouldn?t.
As far as parts of speech are concerned, there
is a tendency for adverbs to be good indicators to-
wards overall accuracy, and best scores are usually
obtained exploiting adjectives and/or adverbs.
104
4 Related work
We have already discussed some related work con-
cerning corpus creation, the development of an
affection lexicon, and the use of such polarity-
annotated resources for sentiment analysis (Sec-
tion 3). As for results, because this is the first experi-
ment on detecting polarity in Italian tweets, compar-
ing performance is not straightforward. Most work
on sentiment analysis in tweets is on English, and al-
though there exist relatively complex systems based
on statistical models, just using information from a
polarity resource is rather common. Su and Markert
(2008b) test SentiWordNet for assigning a subjec-
tivity judgement to word senses on a gold standard
corpus, observing an accuracy of 75.3%. Given that
SentiWordNet is the automatic expansion over a set
of manually annotated seeds, at word-level, this can
be considered as an upper bound in sense subjectiv-
ity detection. Taboada et al (2011) offer a survey of
lexicon-based methods which are evaluated on ad-
jectives only, by measuring overall accuracy against
a manually annotated set of words. Using Senti-
WordNet in a lexicon-projection fashion yields an
accuracy of 61.47% under best settings. These are
however scores on single words rather than whole
sentences or microtexts.
Considering that we assign polarity to tweets
rather than single words, and that in the creation of
our resource via automatic alignment we lose more
than two thirds of the original synsets (see Sec-
tion 3.1), our results are promising. They are also
not that distant from results reported by Agarwal et
al. (2011), whose best system, a combination of un-
igrams and the best set of features, achieves an ac-
curacy of 60.50% on a three-way classification like
ours, evaluated against a manually annotated set of
English tweets. Best f-scores reported for positive,
negative, and neutral are comprised between 59%
and 62%. Similar results are obtained by Pak and
Paroubek (2010), who train a classifier on automati-
cally tagged data, and evaluate their model on about
200 English tweets. Best reported f-score on a three-
way polarity assignment is just over 60%.
5 Conclusions and future work
We have presented the first corpus of Italian tweets
obtained in a completely automatic fashion, the first
polarity lexicon for Italian, and the first experiment
on sentiment analysis on Italian tweets using these
two resources. Both the corpus and the lexicon are
as of now unique resources for Italian, and were pro-
duced in a way which is completely portable to other
languages. In compliance with licensing terms of the
sources we have used, our resources are made avail-
able for research purposes after reviewing.
Simply projecting the affection lexicon, using two
different polarity scoring methods, we experimented
with detecting a generic sentiment expressed in a mi-
crotext, and detecting the twitter?s opinion on a spe-
cific topic. As expected, we found that topic-specific
classification is harder for an automatic system as it
must discern what is said about the topic itself and
what is said more generally or about another entity
mentioned in the text.
Indeed, this contribution can be seen as a first
step towards polarity detection in Italian tweets. The
information we obtain from SentiWordNet and the
ways we combine it could obviously be used as fea-
ture in a learning setting. Other sources of infor-
mation, to be used in combination with our polarity
scores or integrated in a statistical model, are the so-
called noisy labels, namely strings (such as emoti-
cons or specific hashtags (Go et al, 2009; Davi-
dov et al, 2010)) that can be taken as positive or
negative polarity indicators as such. Speriosu et al
(2011) have shown that training a maximum entropy
classier using noisy labels as class predictors in the
training set yields an improvement of about three
percentage points over a lexicon-based prediction.
Another important issue to deal with is figurative
language. During manual annotation we have en-
countered many cases of irony or sarcasm, which is a
phenomenon that must be obviously tackled. There
have been attempts at identifying it automatically in
the context of tweets (Gonza?lez-Iba?n?ez et al, 2011),
and we plan to explore this issue in future work.
Finally, the co-presence of meta and linguistic
information allows for a wide range of linguistic
queries and statistical analyses on the whole of the
corpus, also independently of sentiment informa-
tion, of course. For example, correlations between
parts-of-speech and polarity have been found (Pak
and Paroubek, 2010), and one could expect also
correlations with sentiment and time of the day, or
month of the year, and so on.
105
Acknowledgments
We would like to thank Manuela, Marcella e Silvia
for their help with annotation, and the reviewers for
their useful comments. All errors remain our own.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, LSM ?11, pages 30?38,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Giuseppe Attardi, Felice Dell?Orletta, Maria Simi, and
Joseph Turian. 2009. Accurate dependency parsing
with a stacked multilayer perceptron. In Proceeding
of Evalita 2009, LNCS. Springer.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Nicoletta Calzolari et al, editor, Proceedings of LREC
2010.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Jordi Daud, Llus Padr, and German Rigau. 2000. Map-
ping wordnets using structural information. In 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?2000)., Hong Kong.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 241?249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pagerank-
ing wordnet synsets: An application to opinion min-
ing. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
424?431, Prague, Czech Republic, June. Association
for Computational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
sentiment analysis using distant supervision. http:
//cs.wmich.edu/?tllake/fileshare/
TwitterDistantSupervision09.pdf.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 581?586, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 151?160, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In ACL (Sys-
tem Demonstrations), pages 25?30. The Association
for Computer Linguistics.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM 2010, Washington, DC, USA, May
23-26. The AAAI Press.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari et al, editor, Proceedings of the
International Conference on Language Resources and
Evaluation, LREC 2010, 17-23 May 2010, Valletta,
Malta. European Language Resources Association.
Alexander Pak and Patrick Paroubek. 2011. Twitter for
sentiment analysis: When language resources are not
available. 23rd International Workshop on Database
and Expert Systems Applications, 0:111?115.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In Proceedings of the First In-
ternational Conference on Global WordNet, pages 21?
25.
Guillaume Pitel and Gregory Grefenstette. 2008. Semi-
automatic building method for a multidimensional af-
fect dictionary for a new language. In Proceedings of
LREC 2008.
106
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First workshop on
Unsupervised Learning in NLP, pages 53?63, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Fangzhong Su and Katja Markert. 2008a. Eliciting sub-
jectivity and polarity judgements on word senses. In
Proceedings of COLING 2008 Workshop on Human
Judgements in Computational Linguistics, Manch-
ester, UK.
Fangzhong Su and Katja Markert. 2008b. From words
to senses: A case study of subjectivity recognition. In
Donia Scott and Hans Uszkoreit, editors, Proceedings
of COLING 2008, Manchester, UK, pages 825?832.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-based
methods for sentiment analysis. Comput. Linguist.,
37(2):267?307, June.
Erik Tjong Kim Sang and Johan Bos. 2012. Predicting
the 2011 dutch senate election results with twitter. In
Proceedings of the Workshop on Semantic Analysis in
Social Media, pages 53?60, Avignon, France, April.
Association for Computational Linguistics.
Janyce Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Nicoletta Calzolari, Claire Cardie,
and Pierre Isabelle, editors, ACL. The Association for
Computer Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing Conference, 6-8 October, Van-
couver, British Columbia, Canada.
Eros Zanchetta and Marco Baroni. 2005. Morph-it! a
free corpus-based morphological resource for the ital-
ian language. In Proceedings of Corpus Linguistics
2005.
107
Proceedings of the 14th European Workshop on Natural Language Generation, pages 1?9,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Aligning Formal Meaning Representations with Surface Strings for
Wide-coverage Text Generation
Valerio Basile Johan Bos
{v.basile,johan.bos}@rug.nl
Center for Language and Cognition Groningen (CLCG)
University of Groningen, The Netherlands
Abstract
Statistical natural language generation
from abstract meaning representations
presupposes large corpora consisting of
text?meaning pairs. Even though such
corpora exist nowadays, or could be con-
structed using robust semantic parsing, the
simple alignment between text and mean-
ing representation is too coarse for de-
veloping robust (statistical) NLG systems.
By reformatting semantic representations
as graphs, fine-grained alignment can be
obtained. Given a precise alignment at the
word level, the complete surface form of
a meaning representations can be deduced
using a simple declarative rule.
1 Introduction
Surface Realization is the task of producing flu-
ent text from some kind of formal, abstract rep-
resentation of meaning (Reiter and Dale, 2000).
However, while it is obvious what the output of
a natural language generation component should
be, namely text, there is little to no agreement
on what its input formalism should be (Evans et
al., 2002). Since open-domain semantic parsers
are able to produce formal semantic representa-
tions nowadays (Bos, 2008; Butler and Yoshi-
moto, 2012), it would be natural to see generation
as a reversed process, and consider such seman-
tic representations as input of a surface realization
component.
The idea of using large text corpora annotated
with formal semantic representations for robust
generation has been presented recently (Basile and
Bos, 2011; Wanner et al, 2012). The need for for-
mal semantic representations as a basis for NLG
was expressed already much earlier by Power
(1999), who derives semantic networks enriched
with scope information from knowledge represen-
tations for content planning. In this paper we take
a further step towards the goal of generating text
from deep semantic representations, and consider
the issue of aligning the representations with sur-
face strings that capture their meaning.
First we describe the basic idea of align-
ing semantic representations (logical forms) with
surface strings in a formalism-independent way
(Section 2). Then we apply our method to a
well-known and widely-used semantic formalism,
namely Discourse Representation Theory (DRT),
first demonstrating how to represent Discourse
Representation Structures (DRSs) as graphs (Sec-
tion 3) and showing that the resulting Discourse
Representation Graphs (DRGs) are equivalent to
DRSs but are more convenient to fulfill word-
level alignment (Section 4). Finally, in Section 5
we present a method that generates partial surface
strings for each discourse referent occurring in the
semantic representation of a text, and composes
them into a complete surface form. All in all, we
think this would be a first and important step in
surface realization from formal semantic represen-
tations.
2 Aligning Logic with Text
Several different formal semantic representations
have been proposed in the literature, and although
they might differ in various aspects, they also have
a lot in common. Many semantic representations
(or logical forms as they are sometimes referred
to) are variants of first-order logic and share basic
building blocks such as entities, properties, and re-
lations, complemented with quantifiers, negation
and further scope operators.
A simple snapshot of a formal meaning repre-
sentation is the following (with symbols composed
out of WordNet (Fellbaum, 1998) synset identi-
fiers to abstract away from natural language):
blue#a#1(x) ? cup#n#1(x)
How could this logical form be expressed in nat-
ural language? Or put differently, how could we
1
realize the variable x in text? As simple as it is, x
describes ?a blue cup?, or if your target language
is Italian, ?una tazza blu?, or variants hereof, e.g.
?every blue cup? (if x happens to be bound by uni-
versally quantified) or perhaps as ?una tazza az-
zurra?, using a different adjective to express blue-
ness. This works for simple examples, but how
does it scale up to larger and more complex se-
mantic representations?
In a way, NLG can be viewed as a machine
translation (MT) task, but unlike translating from
one natural language into another, the task is here
to translate a formal (unambiguous) language into
a natural language like English or Italian. Current
statistical MT techniques are based on large paral-
lel corpora of aligned source and target text. In this
paper we introduce a method for precise alignment
of formal semantic representations and text, with
the purpose of creating a large corpus that could
be used in NLG research, and one that opens the
way for statistical approaches, perhaps similar to
those used in MT.
Broadly speaking, alignments between seman-
tic representations and surface strings can be made
in three different ways. The simplest strategy, but
also the least informative, is to align a semantic
representation with a sentence or complete text
without further information on which part of the
representation produces what part of the surface
form. This might be enough to develop statisti-
cal NLG systems for small sentences, but proba-
bly does not scale up to handle larger texts. Alter-
natively, one could devise more complex schemes
that allow for a more fine-grained alignment be-
tween parts of the semantic representation and sur-
face strings (words and phrases). Here there are
two routes to follow, which we call the minimal
and maximal alignment.
In maximal alignment, each single piece of the
semantic representation corresponds to the words
that express that part of the meaning. Possible
problems with this approach are that perhaps not
every bit of the semantic representation corre-
sponds to a surface form, and a single word could
also correspond to various pieces in the seman-
tic representation. This is an interesting option
to explore, but in this paper we present the al-
ternative approach, minimal alignment, which is
a method where every word in the surface string
points to exactly one part of the semantic repre-
sentation. We think this alignment method forms
a better starting point for the development of a
statistical NLG component. With sufficient data
in the form of aligned texts with semantic repre-
sentations, these alignments can be automatically
learned, thus creating a model to generate surface
forms from abstract, logical representations.
However, aligning semantic representations
with words is a difficult enterprise, primarily be-
cause formal semantic representations are not flat
like a string of words and often form complex
structures. To overcome this issue we represent
formal semantic representations as a set of tu-
ples. For instance, returning to our earlier exam-
ple representation for ?blue cup?, we could repre-
sent part of it by the tuples ?blue#a#1,arg,x? and
?cup#n#1,arg,x?. For convenience we can display
this as a graph (Figure 1).
x
b l u e # a # 1
c u p # n # 1
Figure 1: Logical form graph.
Note that in this example several tuples are not
shown for clarity (such as conjunction and the
quantifier). We show below that we can indeed
represent every bit of semantic information in this
format without sacrificing the capability of align-
ment with the text. The important thing now is to
show how alignments between tuples and words
can be realized, which is done by adding an ele-
ment to each tuple denoting the surface string, for
instance ?cup#n#1,arg,x,?tazza??, as in Figure 2.
x
b l u e # a # 1
"blue"
 
"a"
c u p # n # 1
"cup" x
b l u e # a # 1
"blu"
 
"una"
c u p # n # 1
"tazza"
Figure 2: Logical form graphs aligned with sur-
face forms in two languages.
We can further refine the alignment by saying
something about the local order of surface expres-
sions. Again, this is done by adding an element
to the tuple, in this case one that denotes the local
order of a logical term. We will make this clear by
continuing with our example, where we add word
order encoded as numerical indices in the tuple,
e.g. ?cup#n#1,arg,x,?tazza?,2?, as Figure 3 shows.
From these graphs we can associate the term
2
xb l u e # a # 1
"blue"
2
 
"a" 1
c u p # n # 1
"cup" 3 x
b l u e # a # 1
"blu"
3
 
"una" 1
c u p # n # 1
"tazza" 2
Figure 3: Encoding local word order.
x with the surface strings ?a blue cup? and ?una
tazza blu?. But the way we express local or-
der is not limited to words and can be employed
for partial phrases as well, if one adopts a neo-
Davidsonian event semantics with explicit the-
matic roles. This can be achieved by using the
same kind of numerical indices already used for
the alignment of words. The example in Figure 4
shows how to represent an event ?hit? with its the-
matic roles, preserving their relative order. We call
surface forms ?partial? or ?incomplete? when they
contain variables, and ?complete? when they only
contain tokens. The corresponding partial surface
form is then ?y hit z?, where y and z are place-
holders for surface strings.
x
y
z
agen t
1h i t # v # 1 "hit" 2
t h eme
3
Figure 4: Graph for a neo-Davidsonian structure.
This is the basic idea of aligning surface strings
with parts of a deep semantic representation. Note
that precise alignment is only possible for words
with a lexical semantics that include first-order
variables. For words that introduce scope oper-
ators (negation particles, coordinating conjuncts)
we can?t have the cake and eat it: specifying the
local order with respect to an entity or event vari-
able directly and at the same time associating it
with an operator isn?t always possible. To solve
this we introduce surface tuples that complement a
semantic representation to facilitate perfect align-
ment. We will explain this in more detail in the
following sections.
3 Discourse Representation Graphs
The choice of semantic formalism should ideally
be independent from the application of natural
language generation itself, to avoid bias and spe-
cific tailoring the semantic representation to one?s
(technical) needs. Further, the formalism should
have a model-theoretic backbone, to ensure that
the semantic representations one works with actu-
ally have an interpretation, and can consequently
be used in inference tasks using, for instance, auto-
mated deduction for first-order logic. Given these
criteria, a good candidate is Discourse Represen-
tation Theory, DRT (Kamp and Reyle, 1993), that
captures the meaning of texts in the form of Dis-
course Representation Structures (DRSs).
DRSs are capable of effectively representing
the meaning of natural language, covering many
linguistic phenomena including pronouns, quanti-
fier scope, negation, modals, and presuppositions.
DRSs are recursive structures put together by logi-
cal and non-logical symbols, as in predicate logic,
and in fact can be translated into first-order logic
formulas (Muskens, 1996). The way DRSs are
nested inside each other give DRT the ability to
explain the behaviour of pronouns and presuppo-
sitions (Van der Sandt, 1992).
Aligning DRSs with texts with fine granularity
is hard because words and phrases introduce dif-
ferent kinds of semantic objects in a DRS: dis-
course referents, predicates, relations, but also
logical operators such as negation, disjunction and
implication that introduce embedded DRSs. A
precise alignment of a DRS with its text on the
level of words is therefore a non-trivial task.
To overcome this issue, we apply the idea pre-
sented in the previous section to DRSs, making all
recursion implicit by representing them as directed
graphs. We call a graph representing a DRS a
Discourse Representation Graph (DRG, in short).
DRGs encode the same information as DRSs, but
are expressed as a set of tuples. Essentially, this
is done by reification over DRSs ? every DRSs
gets a unique label, and the arity of DRS condi-
tions increases by one for accommodating a DRS
label. This allows us to reformulate a DRS as a set
of tuples.
A DRS is an ordered pair of discourse refer-
ents (variables over entities) and DRS-conditions.
DRS-conditions are basic (representing properties
or relations) or complex (to handle negation and
disjunction). To reflect these different constructs,
we distinguish three types of tuples in DRGs:
? ?K,referent,X? means that X is a discourse
referent in K (referent tuples);
? ?K,condition,C? means that C is a condition
3
?x1 e1
customer(x1)
pay(e1)
agent(e1,x1)
k1 unary ?
? scope k2
k2 referent e1
k2 referent x1
k2 event pay
k2 concept customer
k2 role agent
customer instance x1
pay instance e1
agent internal e1
agent external x1
k1 ?unary k2
e 1
referent
x1referent
pay
even t
cus tomer
concept
ag en trolescope
instance
instance
internal
external
Figure 5: DRS and corresponding DRG (in tuples and in graph format) for ?A customer did not pay.?
in K (condition tuples), with various sub-
types: concept, event, relation, role, named,
cardinality, attribute, unary, and binary;
? ?C,argument,A? means that C is a condition
with argument A (argument tuples), with
the sub-types internal, external, instance,
scope, antecedent, and consequence.
With the help of a concrete example, it is easy to
see that DRGs have the same expressive power as
DRSs. Consider for instance a DRS with negation,
before and after labelling it (Figure 6):
x y
r(x,y)
?
z
p(x)
s(z,y)
K1:
x y
c1:r(x,y)
c2:?K2:
z
c3:p(x)
c4:s(z,y)
Figure 6: From DRS to DRG: labelling.
Now, from the labelled DRS we can derive the
following three referent tuples: ?K1,referent,x?,
?K1,referent,y?, and ?K2,referent,z?; the follow-
ing four condition tuples: ?K1,relation,c1:r?,
?K1,unary,c2:??, ?K2,concept,c3:p?, and
?K2,relation,c4:s?; and the following argu-
ment tuples: ?c1:r,internal,x?, ?c1:r,external,y?,
?c2:?,scope,K2?, ?c3:p,instance,x?,
?c4:s,internal,z?, and ?c4:s,external,y?. From
these tuples, it is straightforward to recreate
a labelled DRS, and by dropping the labels
subsequently, the original DRS resurfaces again.
For the sake of readability we sometimes leave
out labels in examples throughout this paper. In
addition, we also show DRGs in graph-like pic-
tures, where the tuples that form a DRG are the
edges, and word-alignment information attached
at the tuple level is shown as labels on the graph
edges, as in Figure 9. In such graphs, nodes repre-
senting discourse referents are square shaped, and
nodes representing conditions are oval shaped.
Note that labelling conditions is crucial to dis-
tinguish between syntactically equivalent condi-
tions occurring in different (embedded) DRSs.
Unlike Power?s scoped semantic network for
DRSs, we don?t make the assumption that condi-
tions appear in the DRS in which their discourse
referents are introduced (Power, 1999). The ex-
ample in Figure 6 illustrates that this assumption
is not sound: the condition p(x) is in a different
DRS than where its discourse referent x is intro-
duced. Further note that our reification proce-
dure yields ?flatter? representations than similar
formalisms (Copestake et al, 1995; Reyle, 1993),
and this makes it more convenient to align surface
strings with DRSs with a high granularity, as we
will show below.
4 Word-Aligned DRGs
In this section we show how the alignment be-
tween surface text and its logical representation is
realized by adding information of the tuples that
make up a DRG. This sounds more straightfor-
ward than it is. For some word classes this is in-
deed easy to do. For others we need additional
machinery in the formalism. Let?s start with the
straightforward cases. Determiners are usually as-
sociated with referent tuples. Content words, such
as nouns, verbs, adverbs and adjectives, are typ-
ically directly associated with one-place relation
symbols, and can be naturally aligned with argu-
ment tuples. Verbs are assigned to instance tu-
ples linking its event condition; likewise, nouns
are typically aligned to instance tuples which link
discourse referents to the concepts they express;
adjectives are aligned to tuples of attribute con-
ditions. Finally, words expressing relations (such
as prepositions), are attached to the external ar-
gument tuple linking the relation to the discourse
referent playing the role of external argument.
Although the strategy presented for DRG?text
4
alignment is intuitive and straightforward to im-
plement, there are surface strings that don?t corre-
spond to something explicit in the DRS. To this
class belong punctuation symbols, and semanti-
cally empty words such as (in English) the infiniti-
val particle, pleonastic pronouns, auxiliaries, there
insertion, and so on. Furthermore, function words
such as ?not?, ?if?, and ?or?, introduce semantic
material, but for the sake of surface string gener-
ation could be better aligned with the event that
they take the scope of. To deal with all these cases,
we extend DRGs with surface tuples of the form
?K,surface,X?, whose edges are decorated with the
required surface strings. Figure 7 shows an exam-
ple of a DRG extended with such surface tuples.
k1 unary ?
? scope k2
k2 referent e1
k2 referent x1 1 A
k2 event pay
k2 concept customer
k2 role agent
customer instance x1 2 customer
pay instance e1 4 pay
agent internal e1 1
agent external x1
k2 surface e1 2 did
k2 surface e1 3 not
k2 surface e1 5 .
Figure 7: Word-aligned DRG for ?A customer did
not pay.? All alignment information (including
surface tuples) is highlighted.
Note that surface tuples don?t have any influ-
ence on the meaning of the original DRS ? they
just serve for the purpose of alignment of the re-
quired surface strings. Also note in Figure 7 the
indices that were added to some tuples. They serve
to express the local order of surface information.
Following the idea sketched in Section 2, the to-
tal order of words is transformed into a local rank-
ing of edges relative to discourse referents. This is
possible because the tuples that have word tokens
aligned to them always have a discourse referent
as third element (the head of the directed edge, in
terms of graphs). We group tuples that share the
same discourse referent and then assign indices re-
flecting the relative order of how these tuples are
realized in the original text.
Illustrating this with our example in Figure 7,
we got two discourse referents: x1 and e1. The
discourse referent x1 is associated with three tu-
ples, of which two are indexed (with indices 1
and 2). Generating the surface string for x1 suc-
ceeds by traversing the edges in the order speci-
fied, resulting in [A,customer] for x1. The refer-
ent e1 associates with six tuples, of which four
are indexed (with indices 1?4). The order of
these tuples would yield the partial surface string
[x1,did,not,pay,.] for e1.
Note that the manner in which DRSs are con-
structed during analysis ensures that all discourse
referents are linked to each other by taking the
transitive closure of all binary relations appearing
in a DRS, and therefore we can reconstruct the to-
tal order from composing the local orders. In the
next section we explain how this is done.
5 Surface Composition
In this section we show in detail how sur-
face strings can be generated from word-aligned
DRGs. It consists of two subsequent steps. First,
a surface form is associated with each discourse
referent. Secondly, surface forms are put together
in a bottom-up fashion, to generate the complete
output. During the composition, all of the dis-
course referents are associated with their own sur-
face representation. The surface form associated
with the discourse unit that contains all other dis-
course units is then the text aligned with the origi-
nal DRG.
Surface forms of discourse referents are lists of
tokens and other discourse referents. Recall that
the order of the elements of a discourse referent?s
surface form is reflected by the local ordering of
tuples, as explained in the previous section, and
tuples with no index are simply ignored when re-
constructing surface strings.
The surface form is composed by taking each
tuple belonging to a specific discourse referent, in
the correct order, and adding the tokens aligned
with the tuple to a list representing the surface
string for that discourse referent. An important
part of this process is that binary DRS relations,
represented in the DRG by a pair of internal and
external argument tuple, are followed unidirec-
tionally: if the tuple is of the internal type, then
the discourse referent on the other end of the re-
lation (i.e. following its external tuple edge) is
added to the list. Surface forms for embedded
DRSs include the discourse referents of the events
5
k1 : e4
x1 : Michelle e1 : x1 thinks p1
e1 : Michelle thinks p1
p1 : that e2
x2 : Obama e2 : x2 smokes .
e2 : Obama smokes .
p1 : that Obama smokes .
e1 : Michelle thinks that Obama smokes .
k1 : Michelle thinks that Obama smokes .
Figure 8: Surface composition of embedded structures.
they contain.
Typically, discourse units contain exactly one
event (the main event of the clause). Phenomena
such as gerunds (e.g. ?the laughing girl?) and rel-
ative clauses (e.g. ?the man who smokes?) may
introduce more than one event in a discourse unit.
To ensure correct order and grouping, we borrow
a technique from description logic (Horrocks and
Sattler, 1999) and invert roles in DRGs. Rather
than representing ?the laughing girl? as [girl(x) ?
agent(e,x) ? laugh(e)], we represent it as [girl(x)
? agent?1(x,e) ? laugh(e)], making use of R(x,y)
? R?1(y,x) to preserve meaning. This ?trick? en-
sures that we can describe the local order of noun
phrases with relative clauses and alike.
To wrap things up, the composition operation is
used to derive complete surface forms for DRGs.
Composition puts together two surface forms,
where one of them is complete, and one of them
is incomplete. It is formally defined as follows:
?1 : ? ?2 : ?1?1?2
?2 : ?1??2
(1)
where ?1 and ?2 are discourse referents, ? is a list
of tokens, and ?1 and ?2 are lists of word tokens
and discourse referents. In the example from Fig-
ure 7, the complete surface form for the discourse
unit k1 is derived by means of composition as for-
mulated in (1) as follows:
k2 : e1
x1 : A customer e1 : x1 did not pay
e1 : A customer did not pay .
k2 : A customer did not pay .
The procedure for generation described here is
reminiscent of the work of (Shieber, 1988) who
also employs a deductive approach. In particular
our composition operation can be seen as a simpli-
fied completion.
Going back to the example in Section 4, sub-
stituting the value of x1 in the incomplete sur-
face form of e1 produces the surface string
[A,customer,did,not,pay,.] for e1.
6 Selected Phenomena
We implemented a first prototype using our align-
ment and realization method and tested it on ex-
amples taken from the Groningen Meaning Bank,
a large annotated corpus of texts paired with DRSs
(Basile et al, 2012). Naturally, we came across
phenomena that are notoriously hard to analyze.
Most of these we can handle adequately, but some
we can?t currently account for and require further
work.
6.1 Embedded Clauses
In the variant of DRT that we are using, propo-
sitional arguments of verbs introduce embedded
DRSs associated with a discourse referent. This
is a good test for our surface realization formal-
ism, because it would show that it is capable of re-
cursively generating embedded clauses. Figure 9
shows the DRG for the sentence ?Michelle thinks
that Obama smokes.?
k1
x1
referent
e 1referent
p1
referent
"that"
1
subordinates:prop
t h ink
even t
michelle
n amed
Agent
role
Theme
role
x2
e 2
referent
referent
punctuat ion
"." 3
Pat ien t
role
smoke
even t
obaman amed
instance
"thinks" 2
instance
"Michelle" 1
ex t
int
1
int 3
ex t
ex t
int
1
ins tance
"smokes"
2
instance
"Obama"
1
Figure 9: Word-aligned DRG for the sentence
?Michelle thinks that Obama smokes.?
Here the surface forms of two discourse units
(main and embedded) are generated. In order to
generate the complete surface form, first the em-
bedded clause is generated, and then composed
6
with the incomplete surface form of the main
clause. As noted earlier, during the composition
process, the complete surface form for each dis-
course referent is generated (Figure 8), showing
a clear alignment between the entities of the se-
mantic representation and the surface forms they
represent.
6.2 Coordination
Coordination is another good test case for a lin-
guistic formalism. Consider for instance ?Sub-
sistence fishing and commercial trawling occur
within refuge waters?, where two noun phrases are
coordinated, giving rise to either a distributive (in-
troducing two events in the DRS) or a collective
interpretation (introducing a set formation of dis-
course referents in the DRS). We can account for
both interpretations (Figure 10).
Note that, interestingly, using the distributive
interpretation DRG as input to the surface realiza-
tion component could result, depending on how
words are aligned, in a surface form ?fishing oc-
curs and trawling occurs?, or as ?fishing and trawl-
ing occur?.
6.3 Long-Distance Dependencies
Cases of extraction, for instance with WH-
movement, could be problematic to capture with
our formalism. This is in particular an issue when
extraction crosses more than one clause boundary,
as in ?Which car does Bill believe John bought?.
Even though these cases are rare in the real world,
a complete formalism for surface realization must
be able to deal with such cases. The question is
whether this is a separate generation task in the
domain of syntax (White et al, 2007), or whether
the current formalism needs to be adapted to cover
such long-distance dependencies. Another range
of complications are caused by discontinuous con-
stituents, as in the Dutch sentence ?Ik heb kaart-
jes gekocht voor Berlijn? (literally: ?I have tick-
ets bought for Berlin?), where the prepositional
phrase ?voor Berlijn? is an argument of the noun
phrase ?kaartjes?. In our formalism the only align-
ment possible would result in the sentence ?Ik
heb kaartjes voor Berlijn gekocht?, which is ar-
guably a more fluent realization of the sentence,
but doesn?t correspond to the original text. If one
uses the original text as gold standard, this could
cause problems in evaluation. (One could also
benefit from this deficiency, and use it to generate
more than one gold standard surface string. This
is something to explore in future work.)
6.4 Control Verbs
In constructions like ?John wants to swim?, the
control verb ?wants? associates its own subject
with the subject of the infinitival clause that it has
as argument. Semantically, this is realized by vari-
able binding. Generating an appropriate surface
form for semantic representation with controlled
variables is a challenge: a naive approach would
generate ?John wants John to swim?. One possi-
ble solution is to add another derivation rule for
surface composition dedicated to deal with cases
where a placeholder variable occurs in more than
one partial surface form, substituting a null string
for a variable following some heuristic rules. A
second, perhaps more elegant solution is to inte-
grate a language model into the surface composi-
tion process.
7 Related work
Over the years, several systems have emerged that
aim at generate surface forms from different kind
of abstract input representations. An overview of
the state-of-the-art is showcased by the submis-
sions to the Surface Realization Shared Task (Belz
et al, 2012). Bohnet et al (2010), for instance,
employ deep structures derived from the CoNLL
2009 shared task, essentially sentences annotated
with shallow semantics, lemmata and dependency
trees; as the authors state, these annotations are not
made with generation in mind, and they necessi-
tate complex preprocessing steps in order to derive
syntactic trees, and ultimately surface forms. The
format presented in this work has been especially
developed with statistical approaches in mind.
Nonetheless, there is very little work on ro-
bust, wide-scale generation from DRSs, surpris-
ingly perhaps given the large body of theoretical
research carried out in the framework of Discourse
Representation Theory, and practical implemen-
tations and annotated corpora of DRSs that are
nowadays available (Basile et al, 2012). This is
in contrast to the NLG work in the framework of
Lexical Functional Grammar (Guo et al, 2011).
Flat representation of semantic representa-
tions, like the DRGs that we present, have also
been put forward to facilitate machine translation
(Schiehlen et al, 2000) and for evaluation pur-
poses (Allen et al, 2008), and semantic parsing
7
k1
e 1referent
e 2
referent
surface
"and" 1
x1
referent
x2
referent
t h eme
role
t h eme
role
occureven t
occureven t
fishingconcept
t rawling
concept
internal 1
external
internal
2
external
ins tance
"occur"
2
instance
"occur"
3
instance
"fishing" 1
instance
"trawling" 1
k1
e 1
referent
x1
referent
x2
referent
x3surface"and" 2
referent
supe r s e trelation
supe r s e t
relation
occur
even t
fishing
concept
t rawling
concept
t h emerole
external
internal 1
external
internal
3
ins tance
"occur" 2
instance
"fishing" 1
instance
"trawling" 1
internal 1
external
Figure 10: Analysis of NP coordination, in a distributive (left) and a collective interpretation (right).
(Le and Zuidema, 2012) just because they?re eas-
ier and more efficient to process. Packed seman-
tic representations (leaving scope underspecified)
also resemble flat representations (Copestake et
al., 1995; Reyle, 1993) and can be viewed as
graphs, however they show less elaborated reifica-
tion than the DRGs presented in this paper, and are
therefore less suitable for precise alignment with
surface strings.
8 Conclusion
We presented a formalism to align logical forms,
in particular Discourse Representation Structures,
with surface text strings. The resulting graph rep-
resentations (DRGs), make recursion implicit by
reification over nested DRSs. Because of their
?flat? structure, DRGs can be precisely aligned
with the text they represent at the word level. This
is key to open-domain statistical Surface Real-
ization, where words are learned from abstract,
syntactic or semantic, representations, but also
useful for other applications such as learning se-
mantic representations directly from text (Le and
Zuidema, 2012). The actual alignment between
the tuples that form a DRG and the surface forms
they represent is not trivial, and requires to make
several choices.
Given the alignment with text, we show that it
is possible to directly generate surface forms from
automatically generated word-aligned DRGs. To
do so, a declarative procedure is presented, that
generates complete surface forms from aligned
DRGs in a compositional fashion. The method
works in a bottom-up way, using discourse ref-
erents as starting points, then generating a sur-
face form for each of them, and finally compos-
ing all of the surface form together into a com-
plete text. We are currently building a large corpus
of word-aligned DRSs, and are investigating ma-
chine learning methods that could automatically
learn the alignments.
Surprisingly, given that DRT is one of the best
studied formalisms in formal semantics, there isn?t
much work on generation from DRSs so far. The
contribution of this paper presents a method to
align DRSs with surface strings, that paves the
way for robust, statistical methods for surface gen-
eration from deep semantic representations.
8
References
James F. Allen, Mary Swift, and Will de Beaumont.
2008. Deep Semantic Analysis of Text. In Johan
Bos and Rodolfo Delmonte, editors, Semantics in
Text Processing. STEP 2008 Conference Proceed-
ings, volume 1 of Research in Computational Se-
mantics, pages 343?354. College Publications.
Valerio Basile and Johan Bos. 2011. Towards generat-
ing text from discourse representation structures. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation (ENLG), pages 145?150,
Nancy, France.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner,
and Michael White. 2012. The surface realisation
task: Recent developments and future plans. In Bar-
bara Di Eugenio, Susan McRoy, Albert Gatt, Anja
Belz, Alexander Koller, and Kristina Striegnitz, ed-
itors, INLG, pages 136?140. The Association for
Computer Linguistics.
Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-
cia Burga. 2010. Broad coverage multilingual
deep sentence generation with a stochastic multi-
level realizer. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 98?106.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Alastair Butler and Kei Yoshimoto. 2012. Banking
meaning representations from treebanks. Linguis-
tic Issues in Language Technology - LiLT, 7(1):1?
?A?S?22.
Ann Copestake, Dan Flickinger, Rob Malouf, Susanne
Riehemann, and Ivan Sag. 1995. Translation us-
ing Minimal Recursion Semantics. In Proceedings
of the Sixth International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 15?32, University of Leuven, Belgium.
Roger Evans, Paul Piwek, and Lynne Cahill. 2002.
What is NLG? In Proceedings of the Second Inter-
national Conference on Natural Language Genera-
tion, pages 144?151.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2011. Dependency-based n-gram models for gen-
eral purpose sentence realisation. Natural Language
Engineering, 17:455?483.
Ian Horrocks and Ulrike Sattler. 1999. A descrip-
tion logic with transitive and inverse roles and role
hierarchies. Journal of logic and computation,
9(3):385?410.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Phong Le and Willem Zuidema. 2012. Learning com-
positional semantics for open domain semantic pars-
ing. Forthcoming.
Reinhard Muskens. 1996. Combining Montague Se-
mantics and Discourse Representation. Linguistics
and Philosophy, 19:143?186.
R. Power. 1999. Controlling logical scope in text gen-
eration. In Proceedings of the 7th European Work-
shop on Natural Language Generation, Toulouse,
France.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Uwe Reyle. 1993. Dealing with Ambiguities by Un-
derspecification: Construction, Representation and
Deduction. Journal of Semantics, 10:123?179.
Michael Schiehlen, Johan Bos, and Michael Dorna.
2000. Verbmobil interface terms (vits). In Wolfgang
Wahlster, editor, Verbmobil: Foundations of Speech-
to-Speech Translation. Springer.
Stuart M. Shieber. 1988. A uniform architecture for
parsing and generation. In Proceedings of the 12th
conference on Computational linguistics - Volume
2, COLING ?88, pages 614?619, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rob A. Van der Sandt. 1992. Presupposition Projec-
tion as Anaphora Resolution. Journal of Semantics,
9:333?377.
Leo Wanner, Simon Mille, and Bernd Bohnet. 2012.
Towards a surface realization-oriented corpus anno-
tation. In Proceedings of the Seventh International
Natural Language Generation Conference, INLG
?12, pages 22?30, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michael White, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface real-
ization with CCG. In Proceedings of the Workshop
on Using Corpora for NLG: Language Generation
and Machine Translation (UCNLG+MT).
9
