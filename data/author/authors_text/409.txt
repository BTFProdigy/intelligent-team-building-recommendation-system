Part of Speech Tagging in Context 
Michele BANKO and Robert C. MOORE 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 USA 
{mbanko, bobmoore}@microsoft.com 
 
Abstract 
We present a new HMM tagger that exploits 
context on both sides of a word to be tagged, and 
evaluate it in both the unsupervised and supervised 
case. Along the way, we present the first 
comprehensive comparison of unsupervised 
methods for part-of-speech tagging, noting that 
published results to date have not been comparable 
across corpora or lexicons. Observing that the 
quality of the lexicon greatly impacts the accuracy 
that can be achieved by the algorithms, we present 
a method of HMM training that improves accuracy 
when training of lexical probabilities is unstable. 
Finally, we show how this new tagger achieves 
state-of-the-art results in a supervised, non-training 
intensive framework. 
1 Introduction 
The empiricist revolution in computational 
linguistics has dramatically shifted the accepted 
boundary between what kinds of knowledge are 
best supplied by humans and what kinds are best 
learned from data, with much of the human-
supplied knowledge now being in the form of 
annotations of data.  As we look to the future, we 
expect that relatively unsupervised methods will 
grow in applicability, reducing the need for 
expensive human annotation of data. 
With respect to part-of-speech tagging, we 
believe that the way forward from the relatively 
small number of languages for which we can 
currently identify parts of speech in context with 
reasonable accuracy will make use of unsupervised 
methods that require only an untagged corpus and 
a lexicon of words and their possible parts of 
speech.  We believe this based on the fact that such 
lexicons exist for many more languages (in the 
form of conventional dictionaries) than extensive 
human-tagged training corpora exist for. 
Unsupervised part-of-speech tagging, as defined 
above, has been attempted using a variety of 
learning algorithms (Brill 1995, Church, 1988, 
Cutting et. al. 1992, Elworthy, 1994 Kupiec 1992, 
Merialdo 1991).  While this makes unsupervised 
part-of-speech tagging a relatively well-studied 
problem, published results to date have not been 
comparable with respect to the training and test 
data used, or the lexicons which have been made 
available to the learners.  
In this paper, we provide the first comprehensive 
comparison of methods for unsupervised part-of-
speech tagging.  In addition, we explore two new 
ideas for improving tagging accuracy.  First, we 
explore an HMM approach to tagging that uses 
context on both sides of the word to be tagged, 
inspired by previous work on building 
bidirectionality into graphical models (Lafferty et. 
al. 2001, Toutanova et. al. 2003).   Second we 
describe a method for sequential unsupervised 
training of tag sequence and lexical probabilities in 
an HMM, which we observe leads to improved 
accuracy over simultaneous training with certain 
types of models. 
In section 2, we provide a brief description of 
the methods we evaluate and review published 
results. Section 3 describes the contextualized 
variation on HMM tagging that we have explored. 
In Section 4 we provide a direct comparison of 
several unsupervised part-of-speech taggers, which 
is followed by Section 5, in which we present a 
new method for training with suboptimal lexicons. 
In section 6, we revisit our new approach to HMM 
tagging, this time, in the supervised framework. 
2 Previous Work 
A common formulation of an unsupervised part-of-
speech tagger takes the form of a hidden Markov 
model (HMM), where the states correspond to 
part-of-speech tags, ti, and words, wi, are emitted 
each time a state is visited. The training of HMM?
based taggers involves estimating lexical 
probabilities, P(wi|ti), and tag sequence 
probabilities, P(ti | ti-1 ... ti-n). The ultimate goal of 
HMM training is to find the model that maximizes 
the probability of a given training text, which can 
be done easily using the forward-backward, or 
Baum-Welch algorithm (Baum et al1970, Bahl, 
Jelinek and Mercer, 1983). These model 
probabilities are then used in conjunction with the 
Viterbi algorithm (Viterbi, 1967) to find the most 
probable sequence of part-of-speech tags for a 
given sentence. 
When estimating tag sequence probabilities, an 
HMM tagger, such as that described in Merialdo 
(1991), typically takes into account a history 
consisting of the previous two tags -- e.g. we 
compute  P(ti | ti-1, ti-2). Kupiec (1992) describes a 
modified trigram HMM tagger in which he 
computes word classes for which lexical 
probabilities are then estimated, instead of 
computing probabilities for individual words. 
Words contained within the same equivalence 
classes are those which possess the same set of 
possible parts of speech. 
Another highly-accurate method for part-of-
speech tagging from unlabelled data is Brill?s 
unsupervised transformation-based learner (UTBL) 
(Brill, 1995). Derived from his supervised 
transformation-based tagger (Brill, 1992), UTBL 
uses information from the distribution of 
unambiguously tagged data to make informed 
labeling decisions in ambiguous contexts. In 
contrast to the HMM taggers previously described, 
which make use of contextual information coming 
from the left side only, UTBL considers both left 
and right contexts. 
Reported tagging accuracies for these methods 
range from 87% to 96%, but are not directly 
comparable. Kupiec?s HMM class-based tagger, 
when trained on a sample of 440,000 words of the 
original Brown corpus, obtained a test set accuracy 
of 95.7%. Brill assessed his UTBL tagger using 
350,000 words of the Brown corpus for training, 
and found that 96% of words in a separate 
200,000-word test set could be tagged correctly.  
Furthermore, he reported test set accuracy of 
95.1% for the UTBL tagger trained on 120,000 
words of Penn Treebank and tested on a separate 
test set of 200,000 words taken from the same 
corpus.  Finally, using 1 million words from the 
Associated Press for training, Merialdo?s trigram 
tagger was reported to have an accuracy of 86.6%. 
This tagger was assessed using a tag set other than 
that which is employed by the Penn Treebank.  
Unfortunately none of these results can be 
directly compared to the others, as they have used 
different, randomized and irreproducible splits of 
training and test data (Brill and Kupiec), different 
tag sets (Merialdo) or different corpora altogether.  
The HMM taggers we have discussed so far are 
similar in that they use condition only on left 
context when estimating probabilities of tag 
sequences. Recently, Toutanova et al (2003) 
presented a supervised conditional Markov Model 
part-of-speech tagger (CMM) which exploited 
information coming from both left and right 
contexts.  Accuracy on the Penn Treebank using 
two tags to the left as features in addition to the 
current tag was 96.10%. When using tag to the left 
and tag to the right as features in addition to the 
current tag, accuracy improved to 96.55%.  
Lafferty et al (2001) also compared the 
accuracies of several supervised part-of-speech 
tagging models, while examining the effect of 
directionality in graphical models. Using a 50%-
50% train-test split of the Penn Treebank to assess 
HMMs, maximum entropy Markov models 
(MEMMs) and conditional random fields (CRFs), 
they found that CRFs, which make use of 
observation features from both the past and future, 
outperformed HMMs which in turn outperformed 
MEMMs. 
3 Building More Context into HMM Tagging 
In a traditional HMM tagger, the probability of 
transitioning into a state representing tag ti is 
computed based on the previous two tags ti-1 and ti-
2, and the probability of a word wi is conditioned 
only on the current tag ti. This formulation ignores 
dependencies that may exist between a word and 
the part-of-speech tags of the words which precede 
and follow it. For example, verbs which 
subcategorize strongly for a particular part-of-
speech but can also be tagged as nouns or 
pronouns (e.g. ?thinking that?) may benefit from 
modeling dependencies on future tags. 
To model this relationship, we now estimate the 
probability of a word wi based on tags ti-1 and ti-+1. 
This change in structure, which we will call a 
contextualized HMM, is depicted in Figure 1. This 
type of structure is analogous to context-dependent 
phone models used in acoustic modeling for 
speech recognition (e.g.Young, 1999, Section 4.3). 
 
3.1 Model Definition 
In order to build both left and right-context into an 
HMM part-of-speech tagger, we reformulate the 
 
 
 
Figure 1: Graphical Structure of Traditional 
HMM Tagger (top) and Contextualized HMM 
Tagger (bottom) 
trigram HMM model traditionally described as 
?
=
???? ?=
n
i
iiiiiiiii twtwtpttwtwwpTWp
1
111111 )..|()...|(),(
 
by replacing the approximation: 
 
)|()..|(
)|()...|(
1211
1111
????
??
=
=
iiiiiiii
iiiii
tttptwtwtp
twptwtwwp
 
 
with the approximation: 
)|()..|(
)|()...|(
1211
111111
????
+???
=
=
iiiiiiii
iiiiiii
tttptwtwtp
tttwptwtwwp
 
 
Given that we are using an increased context size 
during the estimation of lexical probabilities, thus 
fragmenting the data, we have found it desirable to 
smooth these estimates, for which we use a 
standard absolute discounting scheme (Ney, Essen 
and Knesser, 1994). 
4 Unsupervised Tagging: A Comparison 
4.1 Corpora and Lexicon Construction 
For our comparison of unsupervised tagging 
methods, we implemented the HMM taggers 
described in Merialdo (1991) and Kupiec (1992), 
as well as the UTBL tagger described in Brill 
(1995). We also implemented a version of the 
contextualized HMM using the type of word 
classes utilized in the Kupiec model. The 
algorithms were trained and tested using version 3 
of the Penn Treebank, using the training, 
development, and test split described in Collins 
(2002) and also employed by Toutanova et al 
(2003) in testing their supervised tagging 
algorithm. Specifically, we allocated sections 00-
18 for training, 19-21 for development, and 22-24 
for testing. To avoid the problem of unknown 
words, each learner was provided with a lexicon 
constructed from tagged versions of the full 
Treebank. We did not begin with any estimates of 
the likelihoods of tags for words, but only the 
knowledge of what tags are possible for each word 
in the lexicon, i.e., something we could obtain 
from a manually-constructed dictionary. 
4.2 The Effect of Lexicon Construction on 
Tagging Accuracy 
To our surprise, we found initial tag accuracies of 
all methods using the full lexicon extracted from 
the Penn Treebank to be significantly lower than 
previously reported. We discovered this was due to 
several factors.  
One issue we noticed which impacted tagging 
accuracy was that of a frequently occurring word  
(a) The/VB Lyneses/NNP ,/, of/IN Powder/NNP 
Springs/NNP ,/, Ga./NNP ,/, have/VBP 
filed/VBN suit/NN in/IN Georgia/NNP 
state/NN court/NN against/IN Stuart/NNP 
James/NNP ,/, *-1/-NONE- alleging/VBG 
fraud/NN ./. 
(b) Last/JJ week/NN CBS/NNP Inc./NNP 
cancelled/VBD ``/`` The/NNP People/NNP 
Next/NNP Door/NNP ./. ''/'' 
(c) a/SYM -/: Discounted/VBN rate/NN ./. 
Figure 2:  Manually-Tagged Examples 
being mistagged during Treebank construction, as 
shown in the example in Figure 2a. Since we are 
not starting out with any known estimates for 
probabilities of tags given a word, the learner 
considers this tag to be just as likely as the word?s 
other, more probable, possibilities. In another, 
more frequently occurring scenario, human 
annotators have chosen to tag all words in multi-
word names, such as titles, with the proper-noun 
tag, NNP (Figure 2b). This has the effect of adding 
noise to the set of tags for many closed-class 
words. 
Finally, we noticed that a certain number of 
frequently occurring words (e.g. a, to, of) are 
sometimes labeled with infrequently occurring tags 
(e.g.  SYM, RB), as exemplified in Figure 2c. In the 
case of the HMM taggers, where we begin with 
uniform estimates of both the state transition 
probabilities and the lexical probabilities, the 
learner finds it difficult to distinguish between 
more and less probable tag assignments. 
We later discovered that previous 
implementations of UTBL involved limiting which 
possible part of speech assignments were placed 
into the lexicon1, which was not explicitly detailed 
in the published reports.  We then simulated, in a 
similar fashion, the construction of higher quality 
lexicons by using relative frequencies of tags for 
each word from the tagged Treebank to limit 
allowable word-tag assignments.  That is, tags that 
appeared the tag of a particular word less than X% 
of the time were omitted from the set of possible 
tags for that word.  We varied this threshold until 
accuracy did not significantly change on our set of 
heldout data. The effect of thresholding tags based 
on relative frequency in the training set is shown 
for our set of part-of-speech taggers in the curve in 
Figure 3. As shown in Table 1, the elimination of 
noisy possible part-of-speech assignments raised 
accuracy back into the realm of previously 
published results. The best test set accuracies for 
the learners in the class of HMM taggers are  
                                                     
1
 Eric Brill, Personal Communication 
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 0.1 0.2 0.3
Threshold
Ta
g 
A
cc
u
ra
c
y
Merialdo Trigram
Contextual Trigram
Kupiec Trigram
UTBL
 
Figure 3:  The effect of lexicon construction on 
unsupervised part-of-speech taggers 
 
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0 1 2 3 4 5
Iteration
Ta
g 
A
cc
u
ra
cy
Contextual Trigram
Kupiec Trigram
Merialdo Trigram
 Figure 4: Test Accuracy of HMMs using 
Optimzed Lexicons 
 
 
plotted against the number of training iterations in 
Figure 4. 
5 Unsupervised Training With Noisy 
Lexicons 
While placing informed limitations on the tags that 
can be included in a lexicon can dramatically 
improve results, it is dependent on some form of 
supervision ? either from manually tagged data or 
by a human editor who post-filters an 
automatically constructed list. In the interest of 
being as unsupervised as possible, we sought to 
find a way to cope with the noisy aspects of the  
unfiltered lexicon described in the previous 
section. 
We suspected that in order to better control the 
training of lexical probabilities, having a stable 
model of state transition probabilities would be of 
help. We stabilized this model in two ways. 
 
 
 
 Unfiltered 
Lexicon 
Optimized 
Lexicon 
Merialdo HMM 71.9 93.9 
Contextualized 
HMM 76.9 94.0 
Kupiec HMM 77.1 95.9 
UTBL 77.2 95.9 
Contextualized 
HMM with Classes 77.2 95.9 
Table 1: Tag Accuracy of Unsupervised POS 
Taggers 
 
5.1 Using Unambiguous Tag Sequences To 
Initialize Contextual Probabilities 
First, we used our unfiltered lexicon along with our 
tagged corpus to extract non-ambiguous tag 
sequences. Specifically, we looked for trigrams in 
which all words contained at most one possible 
part-of-speech tag. We then used these n-grams 
and their counts to bias the initial estimates of state 
transitions in the HMM taggers. This approach is 
similar to that described in Ratnaparhki (1998), 
who used unambiguous phrasal attachments to 
train an unsupervised prepositional phrase 
attachment model. 
5.2 HMM Model Training Revised 
Second, we revised the training paradigm for 
HMMs, in which lexical and transition 
probabilities are typically estimated 
simultaneously. We decided to train the transition 
model probabilities first, keeping the lexical 
probabilities constant and uniform. Using the 
estimates initially biased by the method previously 
mentioned, we train the transition model until it 
reaches convergence on a heldout set. We then use 
this model, keeping it fixed, to train the lexical 
probabilities, until they eventually converge on 
heldout data. 
5.3 Results 
We implemented this technique for the Kupiec, 
Merialdo and Contextualized HMM taggers. From 
our training data, we were able to extract data for 
on the order of 10,000 unique unambiguous tag 
sequences which were then be used for better 
initializing the state transition probabilities. As 
shown in Table 2, this method improved tagging 
accuracy of the Merialdo and contextual taggers 
over traditional simultaneous HMM training, 
reducing error by 0.4 in the case of Merialdo and 
0.7 for the contextual HMM part-of-speech tagger.  
 HMM Tagger 
Simultaneous 
Model    
Training 
Sequential 
Model  
Training 
Merialdo 93.9 94.3 
Contextualized 94.0 94.7 
Kupiec 95.9 95.9 
Table 2: Effects of HMM Training on Tagger 
Accuracy 
In this paradigm, tagging accuracy of the Kupiec 
HMM did not change. 
6 Contextualized Tagging with Supervision 
As one more way to assess the potential benefit 
from using left and right context in an HMM 
tagger, we tested our tagging model in the 
supervised framework, using the same sections of 
the Treebank previously allocated for unsupervised 
training, development and testing. In addition to 
comparing against a baseline tagger, which always 
chooses a word?s most frequent tag, we 
implemented and trained a version of a standard 
HMM trigram tagger. For further comparison, we 
evaluated these part of speech taggers against 
Toutanova et als supervised dependency-network 
based tagger, which currently achieves the highest 
accuracy on this dataset to date. The best result for 
this tagger, at 97.24%, makes use of both lexical 
and tag features coming from the left and right 
sides of the target. We also chose to examine this 
tagger?s results when using only <ti, t i-1, t i+1> as 
feature templates, which represents the same 
amount of context built into our contextualized 
tagger.  
As shown in Table 3, incorporating more 
context into an HMM when estimating lexical 
probabilities improved accuracy from 95.87% to 
96.59%, relatively reducing error rate by 17.4%. 
With the contextualized tagger we witness a small 
improvement in accuracy over the current state of 
the art when using the same amount of context. It 
is important to note that this accuracy can be 
obtained without the intensive training required by 
Toutanova et. al?s log-linear models. This result 
falls only slightly below the full-blown training-
intensive dependency-based conditional model. 
7 Conclusions 
We have presented a comprehensive evaluation of 
several methods for unsupervised part-of-speech 
tagging, comparing several variations of hidden 
Markov model taggers and unsupervised 
transformation-based learning using the same 
corpus and same lexicons.  We discovered that the  
 
 
Supervised Tagger Test Accuracy 
Baseline 92.19 
Standard HMM 95.87 
Contextualized HMM 96.59 
Dependency  
Using LR tag features 96.55 
Dependency  
Best Feature Set 97.24 
Table 3: Comparison of Supervised Taggers 
quality of the lexicon made available to 
unsupervised learner made the greatest difference 
to tagging accuracy. Filtering the possible part-of-
speech assignments contained in a basic lexicon 
automatically constructed from the commonly-
used Penn Treebank improved results by as much 
as 22%. This finding highlights the importance of 
the need for clean dictionaries whether they are 
constructed by hand or automatically when we 
seek to be fully unsupervised. 
In addition, we presented a variation on HMM 
model training in which the tag sequence and 
lexical probabilities are estimated in sequence. 
This helped stabilize training when estimation of 
lexical probabilities can be noisy. 
Finally, we experimented with using left and 
right context in the estimation of lexical 
probabilities, which we refer to as a contextualized 
HMM. Without supervision, this new HMM 
structure improved results slightly compared to a 
simple trigram tagger as described in Merialdo, 
which takes into account only the current tag in 
predicting the lexical item.  With supervision, this 
model achieves state of the art results without the 
lengthy training procedure involved in other high-
performing models. In the future, we will consider 
making an increase the context-size, which helped 
Toutanova et al (2003). 
8 Acknowledgements 
The authors wish to thank Gideon Mann for 
performing some initial experiments with a 
publicly available implementation of UTBL, and 
Eric Brill for discussions regarding his work on 
unsupervised transformation based learning. 
 
References  
L.R. Bahl, F. Jelinek, and R. Mercer. 1983. A 
maximum likelihood approach to continuous 
speech recognition. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 
5(2):179--190. 
L.E. Baum, T. Petrie, G. Soules, and N. Weiss.  A 
maximization technique in the statistical analysis 
of probabilistic functions of Markov chains. 
Annals of Mathematical Statistics, 41:164-171. 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings of the Third Conference 
on Applied Natural Language Processing, ACL. 
Trento, Italy. 
E. Brill. 1995. Unsupervised learning of 
disambiguation rules for part of speech tagging. 
In Proceedings of the Third Workshop on Very 
Large Corpora, Cambridge, MA. 
K. Church. 1998. A stochastic parts program and 
noun phrase parser for unrestricted text. In 
Second Conference on Applied Natural 
Language Processing, ACL. 
M. Collins. 2002. Discriminative training methods 
for hidden Markov models: theory and 
experiments with perceptron algorithms. In 
Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, 
Philadelphia, PA. 
D. Cutting, J. Kupiec, J. Pedersen and P. Sibun. 
1992. A practical part-of-speech tagger. In Third 
Conference on Applied Natural Language 
Processing. ACL. 
D. Elworthy. 1994. Does Baum-Welch re-
estimation help taggers. In Proceedings of the 
Fourth Conference on Applied Natural 
Language Processing, ACL. 
J. Kupiec. 1992. Robust part-of-speech tagging 
using a hidden Markov model. Computer Speech 
and Language 6. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In 
Proceedings of ICML-01, pages 282-289. 
B. Merialdo. 1991. Tagging English text with a 
probabilistic model. In Proceedings of ICASSP. 
Toronto, pp. 809-812. 
H. Ney, U. Essen and R. Kneser. 1994. On 
structuring probabilistic dependencies in 
stochastic language modeling. Computer, Speech 
and Language, 8:1-38. 
A. Ratnaparkhi. 1998. Unsupervised statistical 
models for prepositional phrase attachment. In 
Proceedings of the Seventeenth International 
Conference on Computational Linguistics. 
Montreal,  Canada. 
K. Toutanova, D. Klein, C. Manning, and Y. 
Singer. 2003. Feature-Rich Part-of-Speech 
Tagging with a Cyclic Dependency Network. In 
Proceedings of HLT-NAACL. pp. 252-259. 
A.J. Viterbi. 1967. Error bounds for convolutional 
codes and an asymptotically optimal decoding 
algorithm. IEEE Transactions on Information 
Theory, 13:260--269.  
S. Young. 1999. Acoustic modelling for large 
vocabulary continuous speech recognition. 
Computational Models of Speech Pattern 
Processing: Proc NATO Advance Study Institute. 
K. Ponting, Springer-Verlag: 18-38. 
 
Mitigating the Paucity-of-Data Problem: Exploring the
Effect of Training Corpus Size on Classifier Performance
for Natural Language Processing
Michele Banko and Eric Brill
Microsoft Research
1 Microsoft Way
Redmond, WA 98052 USA
{mbanko, brill}@microsoft.com
ABSTRACT
In this paper, we discuss experiments applying machine learning
techniques to the task of confusion set disambiguation, using three
orders of magnitude more training data than has previously been
used for any disambiguation-in-string-context problem. In an
attempt to determine when current learning methods will cease to
benefit from additional training data, we analyze residual errors
made by learners when issues of sparse data have been
significantly mitigated. Finally, in the context of our results, we
discuss possible directions for the empirical natural language
research community.
Keywords
Learning curves, data scaling, very large corpora, natural language
disambiguation.
1. INTRODUCTION
A significant amount of work in empirical natural language
processing involves developing and refining machine learning
techniques to automatically extract linguistic knowledge from on-
line text corpora. While the number of learning variants for
various problems has been increasing, the size of training sets
such learning algorithms use has remained essentially unchanged.
For instance, for the much-studied problems of part of speech
tagging, base noun phrase labeling and parsing, the Penn
Treebank, first released in 1992, remains the de facto training
corpus. The average training corpus size reported in papers
published in the ACL-sponsored Workshop on Very Large
Corpora was essentially unchanged from the 1995 proceedings to
the 2000 proceedings. While the amount of available on-line text
has been growing at an amazing rate over the last five years (by
some estimations, there are currently over 500 billion readily
accessible words on the web), the size of training corpora used by
our field has remained static.
Confusable word set disambiguation, the problem of choosing the
correct use of a word given a set of words with which it is
commonly confused, (e.g. {to, too, two}, {your, you?re}), is a
prototypical problem in NLP. At some level, this task is identical
to many other natural language problems, including word sense
disambiguation, determining lexical features such as pronoun case
and determiner number for machine translation, part of speech
tagging, named entity labeling, spelling correction, and some
formulations of skeletal parsing. All of these problems involve
disambiguating from a relatively small set of tokens based upon a
string context. Of these disambiguation problems, lexical
confusables possess the fortunate property that supervised training
data is free, since the differences between members of a confusion
set are surface-apparent within a set of well-written text.
To date, all of the papers published on the topic of confusion set
disambiguation have used training sets for supervised learning of
less than one million words. The same is true for most if not all of
the other disambiguation-in-string-context problems. In this
paper we explore what happens when significantly larger training
corpora are used. Our results suggest that it may make sense for
the field to concentrate considerably more effort into enlarging
our training corpora and addressing scalability issues, rather than
continuing to explore different learning methods applied to the
relatively small extant training corpora.
2. PREVIOUS WORK
2.1 Confusion Set Disambiguation
Several methods have been presented for confusion set
disambiguation. The more recent set of techniques includes
multiplicative weight-update algorithms [4], latent semantic
analysis [7], transformation-based learning [8], differential
grammars [10], decision lists [12], and a variety of Bayesian
classifiers [2,3,5]. In all of these papers, the problem is
formulated as follows: Given a specific confusion set (e.g. {to,
two, too}), all occurrences of confusion set members in the test
set are replaced by some marker. Then everywhere the system
sees this marker, it must decide which member of the confusion
set to choose. Most learners that have been applied to this
problem use as features the words and part of speech tags
appearing within a fixed window, as well as collocations
surrounding the ambiguity site; these are essentially the same
features as those used for the other disambiguation-in-string-
context problems.
2.2 Learning Curves for NLP
A number of learning curve studies have been carried out for
different natural language tasks. Ratnaparkhi [12] shows a
learning curve for maximum-entropy parsing, for up to roughly
one million words of training data; performance appears to be
asymptoting when most of the training set is used. Henderson [6]
showed similar results across a collection of parsers.
Figure 1 shows a learning curve we generated for our task of
word-confusable disambiguation, in which we plot test
classification accuracy as a function of training corpus size using
a version of winnow, the best-performing learner reported to date
for this well-studied task [4]. This curve was generated by training
on successive portions of the 1-million word Brown corpus and
then testing on 1-million words of Wall Street Journal text for
performance averaged over 10 confusion sets. The curve might
lead one to believe that only minor gains are to be had by
increasing the size of training corpora past 1 million words.
While all of these studies indicate that there is likely some (but
perhaps limited) performance benefit to be obtained from
increasing training set size, they have been carried out only on
relatively small training corpora. The potential impact to be felt by
increasing the amount of training data by any signifcant order has
yet to be studied.
0.70
0.72
0.74
0.76
0.78
0.80
0.82
100,000 400,000 700,000 1,000,000
Training Corpus Size (words)
Te
st
Ac
cu
ra
cy
Figure 1: An Initial Learning Curve for Confusable
Disambiguation
3. EXPERIMENTS
This work attempts to address two questions ? at what point will
learners cease to benefit from additional data, and what is the
nature of the errors which remain at that point. The first question
impacts how best to devote resources in order to improve natural
language technology. If there is still much to be gained from
additional data, we should think hard about ways to effectively
increase the available training data for problems of interest. The
second question allows us to study failures due to inherent
weaknesses in learning methods and features rather than failures
due to insufficient data.
Since annotated training data is essentially free for the problem of
confusion set disambiguation, we decided to explore learning
curves for this problem for various machine learning algorithms,
and then analyze residual errors when the learners are trained on
all available data. The learners we used were memory-based
learning, winnow, perceptron,1 transformation-based learning, and
decision trees. All learners used identical features2 and were used
out-of-the-box, with no parameter tuning. Since our point is not
to compare learners we have refrained from identifying the
learners in the results below.
We collected a 1-billion-word training corpus from a variety of
English texts, including news articles, scientific abstracts,
government transcripts, literature and other varied forms of prose.
Using this collection, which is three orders of magnitude greater
than the largest training corpus previously used for this task, we
trained the five learners and tested on a set of 1 million words of
Wall Street Journal text.3
In Figure 2 we show learning curves for each learner, for up to
one billion words of training data.4 Each point in the graph
reflects the average performance of a learner over ten different
confusion sets which are listed in Table 1. Interestingly, even out
to a billion words, the curves appear to be log-linear. Note that
the worst learner trained on approximately 20 million words
outperforms the best learner trained on 1 million words. We see
that for the problem of confusable disambiguation, none of our
learners is close to asymptoting in performance when trained on
the one million word training corpus commonly employed within
the field.
Table 1: Confusion Sets
{accept, except} {principal, principle}
{affect, effect} {then, than}
{among, between} {their, there}
{its, it?s} {weather, whether}
{peace, piece} {your, you?re}
The graph in Figure 2 demonstrates that for word confusables, we
can build a system that considerably outperforms the current best
results using an incredibly simplistic learner with just slightly
more training data. In the graph, Learner 1 corresponds to a
trivial memory-based learner. This learner simply keeps track of
all <wi-1, wi+1>, < wi-1> and <wi+1> counts for all occurrences of
the confusables in the training set. Given a test set instance, the
learner will first check if it has seen <wi-1,wi+1> in the training set.
If so, it chooses the confusable word most frequently observed
with this tuple. Otherwise, the learner backs off to check for the
frequency of <wi-1>; if this also was not seen then it will back off
to <wi+1>, and lastly, to the most frequently observed confusion-
1 Thanks to Dan Roth for making both Winnow and Perceptron
available.
2 We used the standard feature set for this problem. For details
see [4].
3 The training set contained no text from WSJ.
4 Learner 5 could not be run on more than 100 million words of
training data.
set member as computed from the training corpus. Note that with
10 million words of training data, this simple learner outperforms
all other learners trained on 1 million words.
Many papers in empirical natural language processing involve
showing that a particular system (only slightly) outperforms
others on one of the popular standard tasks. These comparisons
are made from very small training corpora, typically less than a
million words. We have no reason to believe that any
comparative conclusions drawn on one million words will hold
when we finally scale up to larger training corpora. For instance,
our simple memory based learner, which appears to be among the
best performers at a million words, is the worst performer at a
billion. The learner that performs the worst on a million words of
training data significantly improves with more data.
Of course, we are fortunate in that labeled training data is easy to
locate for confusion set disambiguation. For many natural
language tasks, clearly this will not be the case. This reality has
sparked interest in methods for combining supervised and
unsupervised learning as a way to utilize the relatively small
amount of available annotated data along with much larger
collections of unannotated data [1,9]. However, it is as yet
unclear whether these methods are effective other than in cases
where we have relatively small amounts of annotated data
available.
4. RESIDUAL ERRORS
After eliminating errors arising from sparse data and examining
the residual errors the learners make when trained on a billion
words, we can begin to understand inherent weaknesses in
ourlearning algorithms and feature sets. Sparse data problems can
always be reduced by buying additional data; the remaining
problems truly require technological advances to resolve them.
We manually examined a sample of errors classifiers made when
trained on one billion words and classified them into one of four
categories: strongly misleading features, ambiguous context,
sparse context and corpus error. In the paragraphs that follow, we
define the various error types, and discuss what problems remain
even after a substantial decrease in the number of errors attributed
to the problem of sparse data.
Strongly Misleading Features
Errors arising from strongly misleading features occur when
features which are strongly associated with one class appear in the
context of another. For instance, in attempting to characterize the
feature set of weather (vs. its commonly-confused set member
whether), according to the canonical feature space used for this
problem we typically expect terms associated with atmospheric
conditions, temperature or natural phenomena to favor use of
weather as opposed to whether. Below is an example which
illustrates that such strong cues are not always sufficient to
accurately disambiguate between these confusables. In such cases,
a method for better weighing features based upon their syntactic
context, as opposed to using a simple bag-of-words model, may
be needed.
Example: On a sunny day whether she swims or not depends on
the temperature of the water.
0.75
0.80
0.85
0.90
0.95
1.00
1 10 100 1000
Sizeof TrainingCorpus (Millions of Words)
Test Accuracy
Learner 1
Learner 2
Learner 3
Learner 4
Learner 5
Figure 2. Learning Curves for Confusable Disambiguation
Ambiguous Context
Errors can also arise from ambiguous contexts. Such errors are
made when feature sets derived from shallow local contexts are
not sufficient to disambiguate among members of a confusable
set. Long-range, complex dependencies, deep semantic
understanding or pragmatics may be required in order to draw a
distinction among classes. Included in this class of problems are
so-called ?garden-path? sentences, in which ambiguity causes an
incorrect parse of the sentence to be internally constructed by the
reader until a certain indicator forces a revision of the sentence
structure.
Example 1: It's like you're king of the hill.
Example 2: The transportation and distribution departments
evaluate weather reports at least four times a day to determine if
delivery schedules should be modified.
Sparse Context
Errors can also be a result of sparse contexts. In such cases, an
informative term appears, but the term was not seen in the training
corpus. Sparse contexts differ from ambiguous contexts in that
with more data, such cases are potentially solvable using the
current feature set. Sparse context problems may also be lessened
by attributing informative lexical features to a word via clustering
or other analysis.
Example: It's baseball's only team-owned spring training site.
Corpus Error
Corpus errors are attributed to cases in which the test corpus
contains an incorrect use of a confusable word, resulting in
incorrectly evaluating the classification made by a learner. In a
well-edited test corpus such as the Wall Street Journal, errors of
this nature will be minimal.
Example: If they don't find oil, its going to be quite a letdown.
Table 2 shows the distribution of error types found after learning
with a 1-billion-word corpus. Specifically, the sample of errors
studied included instances that one particular learner, winnow,
incorrectly classified when trained on one billion words. It is
interesting that more than half of the errors were attributed to
sparse context. Such errors could potentially be corrected were
the learner to be trained on an even larger training corpus, or if
other methods such as clustering were used.
The ambiguous context errors are cases in which the feature space
currently utilized by the learners is not sufficient for
disambiguation; hence, simply adding more data will not help.
Table 2: Distribution of Error Types
Error Type Percent Observed
Ambiguous Context 42%
Sparse Context 57%
Misleading Features 0%
Corpus Error 1%
5. A BILLION-WORD TREEBANK?
Our experiments demonstrate that for confusion set
disambiguation, system performance improves with more data, up
to at least one billion words. Is it feasible to think of ever having
a billion-word Treebank to use as training material for tagging,
parsing, named entity recognition, and other applications?
Perhaps not, but let us run through some numbers.
To be concrete, assume we want a billion words annotated with
part of speech tags at the same level of accuracy as the original
million word corpus.5 If we train a tagger on the existing corpus,
the na?ve approach would be to have a person look at every single
tag in the corpus, decide whether it is correct, and make a change
if it is not. In the extreme, this means somebody has to look at
one billion tags. Assume our automatic tagger has an accuracy of
95% and that with reasonable tools, a person can verify at the rate
of 5 seconds per tag and correct at the rate of 15 seconds per tag.
This works out to an average of 5*.95 + 15*.05 = 5.5 seconds
spent per tag, for a total of 1.5 million hours to tag a billion
words. Assuming the human tagger incurs a cost of $10/hour, and
assuming the annotation takes place after startup costs due to
development of an annotation system have been accounted for, we
are faced with $15 million in labor costs. Given the cost and labor
requirements, this clearly is not feasible. But now assume that we
could do perfect error identification, using sample selection
techniques. In other words, we could first run a tagger over the
billion-word corpus and using sample selection, identify all and
only the errors made by the tagger. If the tagger is 95% accurate,
we now only have to examine 5% of the corpus, at a correction
cost of 15 seconds per tag. This would reduce the labor cost to $2
million for tagging a billion words. Next, assume we had a way
of clustering errors such that correcting one tag on average had
the effect of correcting 10. This reduces the total labor cost to
$200k to annotate a billion words, or $20k to annotate 100
million. Suppose we are off by an order of magnitude; then with
the proper technology in place it might cost $200k in labor to
annotate 100 million additional words.
As a result of the hypothetical analysis above, it is not absolutely
infeasible to think about manually annotating significantly larger
corpora. Given the clear benefit of additional annotated data, we
should think seriously about developing tools and algorithms that
would allow us to efficiently annotate orders of magnitude more
data than what is currently available.
6. CONCLUSIONS
We have presented learning curves for a particular natural
language disambiguation problem, confusion set disambiguation,
training with more than a thousand times more data than had
previously been used for this problem. We were able significantly
reduce the error rate, compared to the best system trained on the
standard training set size, simply by adding more training data.
5 We assume an annotated corpus such as the Penn Treebank
already exists, and our task is to significantly grow it.
Therefore, we are only taking into account the marginal cost of
additional annotated data, not start-up costs such as style
manual design.
We see that even out to a billion words the learners continue to
benefit from additional training data.
It is worth exploring next whether emphasizing the acquisition of
larger training corpora might be the easiest route to improved
performance for other natural language problems as well.
7. REFERENCES
[1] Brill, E. Unsupervised Learning of Disambiguation Rules
for Part of Speech Tagging. In Natural Language Processing
Using Very Large Corpora, 1999.
[2] Gale, W. A., Church, K. W., and Yarowsky, D. (1993). A
method for disambiguating word senses in a large corpus.
Computers and the Humanities, 26:415--439.
[3] Golding, A. R. (1995). A Bayesian hybrid method for
context-sensitive spelling correction. In Proc. 3rd Workshop
on Very Large Corpora, Boston, MA.
[4] Golding, A. R. and Roth, D. (1999), A Winnow-Based
Approach to Context-Sensitive Spelling Correction. Machine
Learning, 34:107--130.
[5] Golding, A. R. and Schabes, Y. (1996). Combining trigram-
based and feature-based methods for context-sensitive
spelling correction. In Proc. 34th Annual Meeting of the
Association for Computational Linguistics, Santa Cruz, CA.
[6] Henderson, J. Exploiting Diversity for Natural Language
Parsing. PhD thesis, Johns Hopkins University, August 1999.
[7] Jones, M. P. and Martin, J. H. (1997). Contextual spelling
correction using latent semantic analysis. In Proc. 5th
Conference on Applied Natural Language Processing,
Washington, DC.
[8] Mangu, L. and Brill, E. (1997). Automatic rule acquisition
for spelling correction. In Proc. 14th International
Conference on Machine Learning. Morgan Kaufmann.
[9] Nigam, K, McCallum, A, Thrun, S and Mitchell, T. Text
Classification from Labeled and Unlabeled Documents using
EM. Machine Learning. 39(2/3). pp. 103-134. 2000.
[10] Powers, D. (1997). Learning and application of differential
grammars. In Proc. Meeting of the ACL Special Interest
Group in Natural Language Learning, Madrid.
[11] Ratnaparkhi, Adwait. (1999) Learning to Parse Natural
Language with Maximum Entropy Models. Machine
Learning, 34, 151-175.
[12] Yarowsky, D. (1994). Decision lists for lexical ambiguity
resolution: Application to accent restoration in Spanish and
French. In Proc. 32nd Annual Meeting of the Association for
Computational Linguistics, Las Cruces, NM.
Using N-Grams to Understand the Nature of Summaries 
 
 
Michele Banko and Lucy Vanderwende 
One Microsoft Way 
Redmond, WA 98052 
{mbanko, lucyv}@microsoft.com 
 
 
Abstract 
Although single-document summarization is a 
well-studied task, the nature of multi-
document summarization is only beginning to 
be studied in detail.  While close attention has 
been paid to what technologies are necessary 
when moving from single to multi-document 
summarization, the properties of human-
written multi-document summaries have not 
been quantified.  In this paper, we empirically 
characterize human-written summaries 
provided in a widely used summarization 
corpus by attempting to answer the questions: 
Can multi-document summaries that are 
written by humans be characterized as 
extractive or generative?  Are multi-document 
summaries less extractive than single-
document summaries?  Our results suggest that 
extraction-based techniques which have been 
successful for single-document summarization 
may not be sufficient when summarizing 
multiple documents. 
1 Introduction 
The explosion of available online text has made it 
necessary to be able to present information in a succinct, 
navigable manner. The increased accessibility of 
worldwide online news sources and the continually 
expanding size of the worldwide web place demands on 
users attempting to wade through vast amounts of text.  
Document clustering and multi-document summarization 
technologies working in tandem promise to ease some of 
the burden on users when browsing related documents. 
Summarizing a set of documents brings about 
challenges that are not present when summarizing a 
single document. One might expect that a good multi-
document summary will present a synthesis of multiple 
views of the event being described over different 
documents, or present a high-level view of an event that 
is not explicitly reflected in any single document. A 
useful multi-document summary may also indicate the 
presence of new or distinct information contained within 
a set of documents describing the same topic (McKeown 
et. al., 1999, Mani and Bloedorn, 1999). To meet these 
expectations, a multi-document summary is required to 
generalize, condense and merge information coming 
from multiple sources. 
Although single-document summarization is a well-
studied task (see Mani and Maybury, 1999 for an 
overview), multi-document summarization is only 
recently being studied closely (Marcu & Gerber 2001). 
While close attention has been paid to multi-document 
summarization technologies (Barzilay et al 2002, 
Goldstein et al2000), the inherent properties of human-
written multi-document summaries have not yet been 
quantified. In this paper, we seek to empirically 
characterize ideal multi-document summaries in part by 
attempting to answer the questions: Can multi-document 
summaries that are written by humans be characterized 
as extractive or generative?  Are multi-document 
summaries less extractive than single-document 
summaries? Our aim in answering these questions is to 
discover how the nature of multi-document summaries 
will impact our system requirements. 
We have chosen to focus our experiments on the data 
provided for summarization evaluation during the 
Document Understanding Conference (DUC). While we 
recognize that other summarization corpora may exhibit 
different properties than what we report, the data 
prepared for DUC evaluations is widely used, and 
continues to be a powerful force in shaping directions in 
summarization research and evaluation.  
In the following section we describe previous work 
related to investigating the potential for extractive 
summaries. Section 3 describes a new approach for 
assessing the degree to which a summary can be 
described as extractive, and reports our findings for both 
single and multiple document summarization tasks. We 
conclude with a discussion of our findings in Section 4. 
2 Related Work 
Jing (2002) previously examined the degree to which 
single-document summaries can be characterized as 
extractive. Based on a manual inspection of 15 human-
written summaries, she proposes that for the task of 
single-document summarization, human summarizers 
use a ?cut-and-paste? approach in which six main 
operations are performed: sentence reduction, sentence 
combination, syntactic transformation, reordering, 
lexical paraphrasing, and generalization or specification.  
The first four operations are reflected in the construction 
of an HMM model that can be used to decompose human 
summaries.  According to this model, 81% of summary 
sentences contained in a corpus of 300 human-written 
summaries of news articles on telecommunications were 
found to fit the cut-and-paste method, with the rest 
believed to have been composed from scratch.1  
Another recent study (Lin and Hovy, 2003) 
investigated the extent to which extractive methods may 
be sufficient for summarization in the single-document 
case.  By computing a performance upper-bound for 
pure sentence extraction, they found that state-of-the-art 
extraction-based systems are still 15%-24%2  away from 
this limit, and 10% away from average human 
performance. While this sheds light on how much gain 
can be achieved by optimizing sentence extraction 
methods for single-document summarization, to our 
knowledge, no one has assessed the potential for 
extraction-based systems when attempting to summarize 
multiple documents. 
3 Using N-gram Sequences to 
Characterize Summaries 
Our approach to characterizing summaries is much 
simpler than what Jing has described and is based on the 
following idea: if human-written summaries are 
extractive, then we should expect to see long spans of 
text that have been lifted from the source documents to 
form a summary.  
Note that this holds under the assumptions made by 
Jing?s model of operations that are performed by human 
summarizers.  In the examples of operations given by 
Jing, we notice that long n-grams are preserved 
(designated by brackets), even in the operations mostly 
likely to disrupt the original text: 
 
                                                          
1
  Jing considers a sentence to have been generated from 
scratch if fewer than half of the words were composed of 
terms coming from the original document. 
2
 The range in potential gains is due to possible 
variations in summary length. 
Sentence Reduction:  
Document sentence: When it arrives sometime next 
year in new TV sets, the V-chip will give parents a new 
and potentially revolutionary device to block out 
programs they don?t want their children to see. 
Summary sentence: [The V-chip will give parents a] 
[device to block out programs they don?t want their 
children to see.] 
 
Syntactic Transformation: 
Document sentence: Since annoy.com enables 
visitors to send unvarnished opinions to political and 
other figures in the news, the company was concerned 
that its activities would be banned by the statute. 
Summary sentence: [Annoy.com enables visitors to 
send unvarnished opinions to political and other figures 
in the news] and feared the law could put them out of 
business. 
 
Sentence Combination: 
Document sentence 1: But it also raises serious 
questions about the privacy of such highly personal 
information wafting about the digital world. 
Document sentence 2: The issue thus fits squarely 
into the broader debate about privacy and security on the 
Internet, whether it involves protecting credit card 
numbers or keeping children from offensive information. 
Summary sentence: [But it also raises] the issue of 
[privacy of such] [personal information] and this issue 
hits the nail on the head [in the broader debate about 
privacy and security on the Internet.] 
3.1 Data and Experiments 
For our experiments we used data made available from 
the 2001 Document Understanding Conference (DUC), 
an annual large-scale evaluation of summarization 
systems sponsored by the National Institute of Standards 
and Technology (NIST).  In this corpus, NIST has 
gathered documents describing 60 events, taken from the 
Associated Press, Wall Street Journal, FBIS San Jose 
Mercury, and LA Times newswires. An event is 
described by between 3 and 20 separate (but not 
necessarily unique) documents; on average a cluster 
contains 10 documents. Of the 60 available clusters, we 
used the portion specifically designated for training, 
which contains a total of 295 documents distributed over 
30 clusters. 
As part of the DUC 2001 summarization corpus, 
NIST also provides four hand-written summaries of 
different lengths for every document cluster, as well as 
100-word summaries of each document. Since we 
wished to collectively compare single-document 
summaries against multi-document summaries, we used 
the 100-word multi-document summaries for our 
analysis.  It is important to note that for each cluster, all 
summaries (50, 100, 200 and 400-word multi-document 
and 100-word per-document) have been written by the 
same author. NIST used a total of ten authors, each 
providing summaries for 3 of the 30 topics. The 
instructions provided did not differ per task; in both 
single and multi-document scenarios, the authors were 
directed to use complete sentences and told to feel free to 
use their own words (Over, 2004). 
To compare the text of human-authored multi-
document summaries to the full-text documents 
describing the events, we automatically broke the 
documents into sentences, and constructed a minimal 
tiling of each summary sentence. Specifically, for each 
sentence in the summary, we searched for all n-grams 
that are present in both the summary and the documents, 
placing no restrictions on the potential size of an n-gram. 
We then covered each summary sentence with the n-
grams, optimizing to use as few n-grams as possible (i.e. 
favoring n-grams that are longer in length). For this 
experiment, we normalized the data by converting all 
terms to lowercase and removing punctuation. 
3.2  Results 
On average, we found the length of a tile to be 4.47 for 
single-document summaries, compared with 2.33 for 
multi-document summaries. We discovered that 61 out 
of all 1667 hand-written single-document summary 
sentences exactly matched a sentence in the source 
document, however we did not find any sentences for 
which this was the case when examining multi-document 
summaries.  
We also wanted to study how many sentences are  
fully tiled by phrases coming from exactly one sentence 
in the document corpus, and found that while no 
sentences from the multi-document summaries matched 
this criteria, 7.6% of sentences in the single-document 
summaries could be tiled in this manner.  When trying to 
tile sentences with tiles coming from only one document 
sentence, we found that we could tile, on average,  93% 
of a  single-document sentence in that manner, compared 
to an average of 36% of a multi-document sentence.  
This suggests that for multi-document summarization, 
we are not seeing any instances of what can be 
considered  single-sentence compression. Table 1 
summarizes the findings we have presented in this 
section. 
 SingleDoc MultiDoc 
Average Tile Size 
(words) 4.47 2.33 
Max Tile Size 
(words) 38 24 
Exact sentence 
matches 3.7% 0% 
Complete tiling 
from single 
sentence  
7.6% 0% 
Table 1. Comparison of Summary Tiling 
Figure 1 shows the relative frequency with which a 
summary sentence is optimally tiled using tile-sizes up to  
25 words in length in both the single and multi-
document scenarios. The data shows that the relative 
frequency with which a single-document summary 
sentence is optimally tiled using n-grams containing 3 or 
more words is consistently higher compared to the multi-
document case. Not shown on the histogram (due to 
insufficient readability) is that we found 379 tiles (of 
approximately 86,000) between 25 and 38 words long 
covering sentences from single-document summaries. 
No tiles longer than 24 words were found for multi-
document summaries. 
In order to test whether tile samples coming from 
tiling of single-document summaries and multi-
document summaries are likely to have come from the 
same underlying population, we performed two one-
tailed unpaired t-tests, in one instance assuming equal 
variances, and in the other case asssuming the variances 
were unequal.  For these statistical significance tests, we 
randomly sampled 100 summary sentences from each 
task, and extracted the lengths of the n-grams found via 
minmal tiling. This resulted in the creation of a sample 
of 551 tiles for single-document sentences and 735 tiles 
for multi-document sentences. 
For both tests (performed with ?=0.05), the P-values 
were low enough (0.00033 and 0.000858, respectively) 
to be able to reject the null hypothesis that the average 
tile length coming from single-document summaries is 
the same as the average tile length found in multi-
document summaries. We chose to use a one-tailed P-
value because based on our experiments we already 
suspected that the single-document tiles had a larger 
mean. 
 
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
Tile Length
Singledoc Multidoc
 Figure 1. Comparison of Relative Frequencies of 
Optimal Tile Lengths 
4 Conclusions and Future Work 
Our experiments show that when writing multi-
document summaries, human summarizers do not appear 
to be cutting and pasting phrases in an extractive fashion.  
On average, they are borrowing text around the bigram 
level, instead of extracting long sequences of words or 
full sentences as they tend to do when summarizing a 
single document. The extent to which human 
summarizers form extractive summaries during single 
and multi-document summarization was found to be 
different at a level which is statistically significant.  
These findings are additionally supported by the fact that 
automatic n-gram-based evaluation measures now being 
used to assess predominately extractive multi-document 
summarization systems correlate strongly with human 
judgments when restricted to the usage of unigrams and 
bigrams, but correlate weakly when longer n-grams are 
factored into the equation (Lin & Hovy, 2003). In the 
future, we wish to apply our method to other corpora, 
and to explore the extent to which different 
summarization goals, such as describing an event or 
providing a biography, affect the degree to which 
humans employ rewriting as opposed to extraction. 
Despite the unique requirements for multi-document 
summarization, relatively few systems have crossed over 
into employing generation and reformulation (McKeown 
& Radev, 1995, Nenkova, et al 2003). For the most part, 
summarization systems continue to be based on sentence 
extraction methods. Considering that humans appear to 
be generating summary text that differs widely from 
sentences in the original documents, we suspect that 
approaches which make use of generation and 
reformulation techniques may yield the most promise for 
multi-document summarization.  We would like to 
empirically quantify to what extent current 
summarization systems reformulate text, by applying the 
techniques presented in this paper to system output. 
Finally, the potential impact of our findings with 
respect to recent evaluation metrics should not be 
overlooked. Caution must be given when employing 
automatic evaluation metrics based on the overlap of n-
grams between human references and system summaries.  
When reference summaries do not contain long n-grams 
drawn from the source documents, but are instead 
generated in the author?s own words, the use of a large 
number of reference summaries becomes more critical.  
Acknowledgements 
The authors wish to thank Eric Ringger for providing 
tools used to construct the tiles, and Bob Moore for 
suggestions pertaining to our experiments. 
References 
Regina Barzilay, Noemie Elhadad, Kathleen McKeown. 
2002. "Inferring Strategies for Sentence Ordering in 
Multidocument Summarization." JAIR, 17:35-55. 
Jade Goldstein, Vibhu Mittal, Mark Kantrowitz and 
Jaime Carbonell, 2000. Multi-Document 
Summarization by Sentence Extraction. In the 
Proceedings of the ANLP/NAACL Workshop on 
Automatic Summarization. Seattle, WA 
Hongyan Jing. 2002. Using Hidden Markov Modeling to 
Decompose Human-Written Summaries. 
Computational Linguistics 28(4): 527-543. 
Chin-Yew Lin, and E.H. Hovy 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of 2003 
Language Technology Conference (HLT-NAACL 
2003), Edmonton, Canada. 
Chin-Yew Lin and Eduard.H. Hovy. 2003. The Potential 
and Limitations of Sentence Extraction for 
Summarization. In Proceedings of the Workshop on 
Automatic Summarization post-conference workshop 
of HLT-NAACL-2003. Edmonton, Canada. 
Daniel Marcu and Laurie Gerber. 2001. An Inquiry into 
the Nature of Multidocument Abstracts, Extracts, and 
Their Evaluation. Proceedings of the NAACL-2001 
Workshop on Automatic Summarization 
Inderjeet Mani and Eric Bloedorn. 1999. Summarizing 
similarities and differences among related documents. 
Information Retrieval, 1, pp. 35-67. 
Inderjeet Mani and Mark Maybury (Eds.). 1999. 
Advances in Automatic Text Summarization. MIT 
Press, Cambridge MA. 
Kathy McKeown, Judith Klavans, Vasileios 
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin. 
1999. Towards multidocument summarization by 
reformulation: Progress and prospects. In Proceedings 
of AAAI. 
Kathleen R. McKeown and Dragomir R. Radev. 
Generating summaries of multiple news articles. 
1995. In Proceedings, 18th Annual International 
ACM SIGIR Conference on Research and 
Development in Information Retrieval, pages 74-82. 
Ani Nenkova, Barry Schiffman, Andrew Schlaiker, 
Sasha Blair-Goldensohn, Regina Barzilay, Sergey 
Sigelman, Vasileios Hatzivassiloglou, and Kathleen 
McKeown. 2003. Columbia at the Document 
Understanding Conference. Document Understanding 
Conference 2003. 
Paul Over. 2004. Personal communication.
 
Scaling to Very Very Large Corpora for  
Natural Language Disambiguation 
Michele Banko and Eric Brill 
Microsoft Research 
1 Microsoft Way 
Redmond, WA 98052 USA 
{mbanko,brill}@microsoft.com 
 
Abstract 
The amount of readily available on-line 
text has reached hundreds of billions of 
words and continues to grow.  Yet for 
most core natural language tasks, 
algorithms continue to be optimized, 
tested and compared after training on  
corpora consisting of only one million 
words or less.  In this paper, we 
evaluate the performance of different 
learning methods on a prototypical 
natural language disambiguation task, 
confusion set disambiguation, when 
trained on orders of magnitude more 
labeled data than has previously been 
used.  We are fortunate that for this 
particular application, correctly labeled 
training data is free. Since this will 
often not be the case, we examine 
methods for effectively exploiting very 
large corpora when labeled data comes 
at a cost. 
1 Introduction 
Machine learning techniques, which 
automatically learn  linguistic information from 
online text corpora, have been applied to a 
number of natural language problems 
throughout the last decade.  A large percentage 
of papers published in this area involve 
comparisons of different learning approaches 
trained and tested with commonly used corpora.  
While the amount of available online text has 
been increasing at a dramatic rate, the size of 
training corpora typically used for learning has 
not.  In part, this is due to the standardization of 
data sets used within the field, as well as the 
potentially large cost of annotating data for 
those learning methods that rely on labeled text. 
The empirical NLP community has put 
substantial effort into evaluating performance of 
a large number of machine learning methods 
over fixed, and relatively small, data sets.  Yet 
since we now have access to significantly more 
data, one has to wonder what conclusions that 
have been drawn on small data sets may carry 
over when these learning methods are trained 
using much larger corpora.   
In this paper, we present a study of the 
effects of data size on machine learning for 
natural language disambiguation. In particular, 
we study the problem of selection among 
confusable words, using orders of magnitude 
more training data than has ever been applied to 
this problem.  First we show learning curves for 
four different machine learning algorithms.  
Next, we consider the efficacy of voting, sample 
selection and partially unsupervised learning 
with large training corpora, in hopes of being 
able to obtain the benefits that come from 
significantly larger training corpora without 
incurring too large a cost. 
2 Confusion Set Disambiguation 
Confusion set disambiguation is the problem of 
choosing the correct use of a word, given a set 
of words with which it is commonly confused.  
Example confusion sets include: {principle , 
principal}, {then, than}, {to,two,too}, and 
{weather,whether}. 
 Numerous methods have been presented 
for confusable disambiguation. The more recent 
set of techniques includes mult iplicative weight-
update algorithms (Golding and Roth, 1998), 
latent semantic analysis (Jones and Martin, 
1997), transformation-based learning (Mangu 
and Brill, 1997), differential grammars (Powers, 
1997), decision lists (Yarowsky, 1994), and a 
variety of Bayesian classifiers (Gale et al, 1993, 
Golding, 1995, Golding and Schabes, 1996).  In 
all of these approaches, the problem is 
formulated as follows:  Given a specific 
confusion set (e.g. {to,two,too}), all occurrences 
of confusion set members in the test set are 
replaced by a marker;  everywhere the system 
sees this marker, it must decide which member 
of the confusion set to choose.   
 Confusion set disambiguation is one of a 
class of natural language problems involving 
disambiguation from a relatively small set of 
alternatives based upon the string context in 
which the ambiguity site appears.  Other such 
problems include word sense disambiguation, 
part of speech tagging and some formulations of 
phrasal chunking.  One advantageous aspect of 
confusion set disambiguation, which allows us 
to study the effects of large data sets on 
performance, is that labeled training data is 
essentially free, since the correct answer is 
surface apparent in any collection of reasonably 
well-edited text.  
 
3 Learning Curve Expe riments 
This work was partially motivated by the desire 
to develop an improved grammar checker.  
Given a fixed amount of time, we considered 
what would be the most effective way to focus 
our efforts in order to attain the greatest 
performance improvement.  Some possibilities 
included modifying standard learning 
algorithms, exploring new learning techniques, 
and using more sophisticated features.  Before 
exploring these somewhat expensive paths, we 
decided to first see what happened if we simply 
trained an existing method with much more 
data.  This led to the exploration of learning 
curves for various machine learning algorithms : 
winnow1, perceptron, na?ve Bayes, and a very 
simple memory-based learner.  For the first 
three learners, we used the standard collection of 
features employed for this problem: the set of 
words within a window of the target word, and 
collocations containing words and/or parts of 
                                                                 
1 Thanks to Dan Roth for making both Winnow and 
Perceptron available. 
speech.  The memory-based learner used only 
the word before and word after as features. 
 
0.70
0.75
0.80
0.85
0.90
0.95
1.00
0.1 1 10 100 1000
Millions of Words
T
e
s
t
 
A
c
c
u
r
a
c
y
Memory-Based
Winnow
Perceptron
Na?ve Bayes
 
Figure 1. Learning Curves for Confusion Set 
Disambiguation 
 
 We collected a 1-billion-word training 
corpus from a variety of English texts, including 
news articles, scientific abstracts, government 
transcripts, literature and other varied forms of 
prose.  This training corpus is three orders of 
magnitude greater than the largest training 
corpus previously used for this problem.  We 
used 1 million words of Wall Street Journal text 
as our test set, and no data from the Wall Street 
Journal was used when constructing the training 
corpus. Each learner was trained at several 
cutoff points in the training corpus, i.e. the first 
one million words, the first five million words, 
and so on, until all one billion words were used 
for training. In order to avoid training biases that 
may result from merely concatenating the 
different data sources to form a larger training 
corpus, we constructed each consecutive 
training corpus by probabilistically sampling 
sentences from the different sources weighted 
by the size of each source. 
 In Figure 1, we show learning curves for 
each learner, up to one billion words of training 
data.  Each point in the graph is the average 
performance over ten confusion sets for that size 
training corpus.  Note that the curves appear to 
be log-linear even out to one billion words. 
 Of course for many problems, additional 
training data has a non-zero cost.  However, 
these results suggest that we may want to 
reconsider the trade-off between spending time 
and money on algorithm development versus 
spending it on corpus development.  At least for 
the problem of confusable disambiguation, none 
of the learners tested is close to asymptoting in 
performance at the training corpus size 
commonly employed by the field. 
 Such gains in accuracy, however, do not 
come for free.  Figure 2 shows the size of 
learned representations as a function of training 
data size.  For some applications, this is not 
necessarily a concern.  But for others, where 
space comes at a premium, obtaining the gains 
that come with a billion words of training data 
may not be viable without an effort made to 
compress information.  In such cases, one could 
look at numerous methods for compressing data 
(e.g. Dagan and Engleson, 1995, Weng, et al 
1998). 
4 The Efficacy of Voting 
Voting has proven to be an effective technique 
for improving classifier accuracy for many 
applications, including part-of-speech tagging 
(van Halteren, et al 1998), parsing (Henderson 
and Brill, 1999), and word sense disambiguation 
(Pederson, 2000).  By training a set of classifiers 
on a single training corpus and then combining 
their outputs in classification, it is often possible 
to achieve a target accuracy with less labeled 
training data than would be needed if only one 
classifier  was being used.  Voting can be 
effective in reducing both the bias of a particular 
training corpus and the bias of a specific learner.  
When a training corpus is very small, there is 
much more room for these biases to surface and 
therefore for voting to be effective.  But does 
voting still offer performance gains when 
classifiers are trained on much larger corpora? 
 The complementarity between two 
learners was defined by Brill and Wu (1998) in 
order to quantify the percentage of time when 
one system is wrong, that another system is 
correct, and therefore providing an upper bound 
on combination accuracy. As training size 
increases significantly, we would expect 
complementarity between classifiers to decrease.  
This is due in part to the fact that a larger 
training corpus will reduce the data set variance 
and any bias arising from this.  Also, some of 
the differences between classifiers might be due 
to how they handle a sparse training set.   
1
10
100
1000
10000
100000
1000000
1 10 100 1000
Millions of Words
Winnow
Memory-Based
 
Figure 2. Representation Size vs. Training 
Corpus Size 
 
 
As a result of comparing a sample of 
two learners as a function of increasingly large 
training sets, we see in Table 1 that 
complementarity does indeed decrease as 
training size increases. 
 
Training Size (words) Complementarity(L1,L2) 
106 0.2612 
107 0.2410 
108 0.1759 
109 0.1612 
Table 1. Complementarity 
 
 Next we tested whether this decrease in 
complementarity meant that voting loses its 
effectiveness as the training set increases.  To 
examine the impact of voting when using a 
significantly larger training corpus, we ran 3 out 
of the 4 learners on our set of 10 confusable 
pairs, excluding the memory-based learner.  
Voting was done by combining the normalized 
score each learner assigned to a classification 
choice.  In Figure 3, we show the accuracy 
obtained from voting, along with the single best 
learner accuracy at each training set size.  We 
see that for very small corpora, voting is 
beneficial, resulting in better performance than 
any single classifier.  Beyond 1 million words, 
little is gained by voting, and indeed on the 
largest training sets voting actually hurts 
accuracy. 
 
0.80
0.85
0.90
0.95
1.00
0.1 1 10 100 1000
Millions of words
T
e
s
t
 
A
c
c
u
r
a
c
y
Best
Voting
 
Figure 3. Voting Among Classifiers 
5 When Annotated Data Is Not Free 
While the observation that learning curves are 
not asymptoting even with orders of magnitude 
more training data than is currently used is very 
exciting, this result may have somewhat limited 
ramifications.  Very few problems exist for 
which annotated data of this size is available for 
free.  Surely we cannot reasonably expect that 
the manual annotation of one billion words 
along with corresponding parse trees will occur 
any time soon (but see (Banko and Brill 2001) 
for a discussion that this might not be 
completely infeasible).  Despite this pitfall, there 
are techniques one can use to try to obtain the 
benefits of considerably larger training corpora 
without incurring significant additional costs.  In 
the sections that follow, we study two such 
solutions: active learning and unsupervised 
learning. 
5.1    Active Learning 
Active learning involves intelligently selecting a 
portion of samples for annotation from a pool of  
as-yet unannotated training samples.  Not all 
samples in a training set are equally useful.  By 
concentrating human annotation efforts on the 
samples of greatest utility to the machine 
learning algorithm, it may be possible to attain 
better performance for a fixed annotation cost 
than if samples were chosen randomly for 
human annotation. 
Most active learning approaches work 
by first training a seed learner (or family of 
learners) and then running the learner(s) over a 
set of unlabeled samples.   A sample is 
presumed to be more useful for training the 
more uncertain its classification label is.  
Uncertainty can be judged by the relative 
weights assigned to different labels by a single 
classifier (Lewis and Catlett, 1994).  Another 
approach, committee-based sampling, first 
creates a committee of classifie rs and then 
judges classification uncertainty according to 
how much the learners differ among label 
assignments. For example, Dagan and Engelson 
(1995) describe a committee-based sampling 
technique where a part of speech tagger is 
trained using an annotated seed corpus.  A 
family of taggers is then generated by randomly 
permuting the tagger probabilities, and the 
disparity among tags output by the committee 
members is used as a measure of classification 
uncertainty.   Sentences for human annotation 
are drawn, biased to prefer those containing high 
uncertainty instances. 
 While active learning has been shown to 
work for a number of tasks, the majority of 
active learning experiments in natural language 
processing have been conducted using very 
small seed corpora and sets of unlabeled 
examples.  Therefore, we wish to explore 
situations where we have, or can afford, a non-
negligible sized training corpus (such as for 
part-of-speech tagging) and have access to very 
large amounts of unlabeled data.  
 We can use bagging (Breiman, 1996), a 
technique for generating a committee of 
classifiers, to assess the label uncertainty of a 
potential training instance.  With bagging, a 
variant of the original training set is constructed 
by randomly sampling sentences with 
replacement from the source training set in order 
to produce N new training sets of size equal to 
the original. After the N models have been 
trained and run on the same test set, their 
classifications for each test sentence can be 
compared for classification agreement.  The 
higher the disagreement between classifiers, the 
more useful it would be to have an instance
0%
1%
10%
100%
0.95 0.96 0.97 0.98 0.99 1.00
Test Accuracy
T
r
a
i
n
i
n
g
 
D
a
t
a
 
U
s
e
d
Sequential
Sampling from 5M
Sampling from 10M
Sampling from 100M
 
Figure 4.  Active Learning with Large Corpora 
manually labeled. 
 We used the na?ve Bayes classifier, 
creating 10 classifiers each trained on bags 
generated from an initial one million words of 
labeled training data.  We present the active 
learning algorithm we used below.  
 
 
Initialize: Training data consists of X words 
correctly labeled 
Iterate : 
1) Generate a committee of classifiers using 
bagging on the training set  
 
2) Run the committee on unlabeled portion of 
the training set 
3) Choose M instances from the unlabeled set 
for labeling - pick the M/2 with the greatest 
vote entropy and then pick another M/2 
randomly ? and add to training set 
  
 
We initially tried selecting the M most 
uncertain examples, but this resulted in a sample 
too biased toward the difficult instances.  
Instead we pick half of our samples for 
annotation randomly and the other half from 
those whose labels we are most uncertain of, as 
judged by the entropy of the votes assigned to 
the instance by the committee.  This is, in effect, 
biasing our sample toward instances the 
classifiers are most uncertain of. 
We show the results from sample 
selection for confusion set disambiguation in 
Figure 4.  The line labeled "sequential" shows 
test set accuracy achieved for different 
percentages of the one billion word training set, 
where training instances are taken at random.  
We ran three active learning experiments, 
increasing the size of the total unlabeled training 
corpus from which we can pick samples to be 
annotated.  In all three cases, sample selection 
outperforms sequential sampling.  At the 
endpoint of each training run in the graph, the 
same number of samples has been annotated for 
training.  However, we see that the larger the 
pool of candidate instances for annotation is, the 
better the resulting accuracy.  By increasing the 
pool of unlabeled training instances for active 
learning, we can improve accuracy with only a 
fixed additional annotation cost. Thus it is 
possible to benefit from the availability of 
extremely large corpora without incurring the 
full costs of annotation, training time, and 
representation size.  
5.2 Weakly Supervised Learning 
While the previous section shows that we can 
benefit from substantially larger training corpora 
without needing significant additional manual 
annotation, it would be ideal if we could 
improve classification accuracy using only our 
seed annotated corpus and the large unlabeled 
corpus, without requiring any additional hand 
labeling.  In this section we turn to unsupervised 
learning in an attempt to achieve this goal. 
Numerous approaches have been explored for 
exploiting situations where some amount of 
annotated data is available  and a much larger 
amount of data exists unannotated, e.g. 
Marialdo's HMM part-of-speech tagger training 
(1994), Charniak's parser retraining experiment 
(1996), Yarowsky's seeds for word sense 
disambiguation (1995) and Nigam et als (1998) 
topic classifier learned in part from unlabelled 
documents.  A nice discussion of this general 
problem can be found in Mitchell (1999). 
 The question we want to answer is 
whether there is something to be gained by 
combining unsupervised and supervised learning 
when we scale up both the seed corpus and the 
unlabeled corpus significantly.  We can again 
use a committee of bagged classifiers, this time 
for unsupervised learning.  Whereas with active 
learning we want to choose the most uncertain 
instances for human annotation, with 
unsupervised learning we want to choose the 
instances that have the highest probability of 
being correct for automatic labeling and 
inclusion in our labeled training data. 
In Table 2, we show the test set 
accuracy (averaged over the four most 
frequently occurring confusion pairs) as a 
function of the number of classifiers that agree 
upon the label of an instance. For this 
experiment, we trained a collection of 10 na?ve 
Bayes classifiers, using bagging on a 1-million-
word seed corpus.  As can be seen, the greater 
the classifier agreement, the more likely it is that 
a test sample has been correctly labeled. 
 
Classifiers 
 In Agreement 
Test  
Accuracy 
10 0.8734 
9 0.6892 
8 0.6286 
7 0.6027 
6 0.5497 
5 0.5000 
Table 2. Committee Agreement vs. Accuracy 
 
Since the instances in which all bags agree have 
the highest probability of being correct, we 
attempted to automatically grow our labeled 
training set using the 1-million-word labeled 
seed corpus along with the collection of na?ve 
Bayes classifiers described above. All instances 
from the remainder of the corpus on which all 
10 classifiers agreed were selected, trusting the 
agreed-upon label. The classif iers were then 
retrained using the labeled seed corpus plus the 
new training material collected automatically 
during the previous step.  
 In Table 3 we show the results from 
these unsupervised learning experiments for two 
confusion sets.  In both cases we gain from 
unsupervised training compared to using only 
the seed corpus, but only up to a point.  At this 
point, test set accuracy begins to decline as 
additional training instances are automatically 
harvested.  We are able to attain improvements 
in accuracy for free using unsupervised learning, 
but unlike our learning curve experiments using 
correctly labeled data, accuracy does not 
continue to improve with additional data.   
 {then, than} {among, between} 
 Test  
Accuracy 
% Total 
Training Data 
Test  
Accuracy 
% Total  
Training Data 
106-wd labeled seed corpus 0.9624 0.1 0.8183 0.1 
seed+5x106 wds, unsupervised 0.9588 0.6 0.8313 0.5 
seed+107 wds, unsupervised 0.9620 1.2 0.8335 1.0 
seed+108 wds, unsupervised 0.9715 12.2 0.8270 9.2 
seed+5x108 wds, unsupervised 0.9588 61.1 0.8248 42.9 
109 wds, supervised 0.9878 100 0.9021 100 
Table 3. Committee-Based Unsupervised Learning 
 
 Charniak (1996) ran an experiment in 
which he trained a parser on one million words 
of parsed data, ran the parser over an additional 
30 million words, and used the resulting parses 
to reestimate model probabilities.  Doing so 
gave a small improvement over just using the 
manually parsed data.  We repeated this 
experiment with our data, and show  the 
outcome in Table 4.  Choosing only the labeled 
instances most likely to be correct as judged by 
a committee of classifiers results in higher 
accuracy than using all instances classified by a 
model trained with the labeled seed corpus. 
 
 Unsupervised:    
All Labels 
Unsupervised: 
Most Certain Labels 
 {then, than} 
107  words 0.9524 0.9620 
108  words 0.9588 0.9715 
5x108 words 0.7604 0.9588 
 {among, between} 
107  words 0.8259 0.8335 
108  words 0.8259 0.8270 
5x108 words 0.5321 0.8248 
Table 4. Comparison of Unsupervised Learning 
Methods 
 In applying unsupervised learning to 
improve upon a seed-trained method, we 
consistently saw an improvement in 
performance followed by a decline. This is 
likely due to eventually having reached a point 
where the gains from additional training data are 
offset by the sample bias in mining these 
instances.  It may be possible to combine active 
learning with unsupervised learning as a way to 
reduce this sample bias and gain the benefits of 
both approaches. 
6 Conclusions  
In this paper, we have looked into what happens 
when we begin to take advantage of the large 
amounts of text that are now readily available. 
We have shown that for a prototypical natural 
language classification task,  the performance of 
learners can benefit significantly from much 
larger training sets.  We have also shown that 
both active learning and unsupervised learning 
can be used to attain at least some of the 
advantage that comes with additional training 
data, while minimizing the cost of additional 
human annotation. We propose that a logical 
next step for the research community would be 
to direct efforts towards increasing the size of 
annotated training collections, while 
deemphasizing the focus on comparing different 
learning techniques trained only on small 
training corpora.  While it is encouraging that 
there is a vast amount of on-line text, much 
work remains to be done if we are to learn how 
best to exploit this resource to improve natural 
language processing. 
References 
Banko, M. and Brill, E. (2001). Mitigating the 
Paucity of Data Problem. Human Language 
Technology. 
Breiman L., (1996). Bagging Predictors, Machine 
Learning 24 123-140. 
Brill, E. and Wu, J. (1998). Classifier combination 
for improved lexical disambiguation. In 
Proceedings of the 17th International Conference 
on Computational Linguistics. 
Charniak, E. (1996). Treebank Grammars , 
Proceedings AAAI-96 , Menlo Park, Ca. 
Dagan, I. and Engelson, S. (1995). Committee-based 
sampling for training probabilistic classifiers. In 
Proc. ML-95, the 12th Int. Conf. on Machine 
Learning. 
Gale, W. A., Church, K. W., and Yarowsky, D. 
(1993). A method for disambiguating word senses 
in a large corpus. Computers and the Humanities, 
26:415--439. 
Golding, A. R. (1995). A Bayesian hybrid method for 
context-sensitive spelling correction. In Proc. 3rd 
Workshop on Very Large Corpora, Boston, MA. 
Golding, A. R. and Roth, D.(1999),  A Winnow-
Based Approach to Context-Sensitive Spelling 
Correction.  Machine Learning, 34:107--130. 
Golding, A. R. and Schabes, Y. (1996). Combining 
trigram-based and feature-based methods for 
context-sensitive spelling correction. In Proc. 34th 
Annual Meeting of the Association for 
Computational Linguistics, Santa Cruz, CA. 
Henderson, J. C. and Brill, E. (1999). Exploiting 
diversity in natural language processing: 
combining parsers. In 1999 Joint Sigdat 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora. 
ACL, New Brunswick NJ. 187-194. 
Jones, M. P. and Martin, J. H. (1997). Contextual 
spelling correction using latent semantic analysis. 
In Proc. 5th Conference on Applied Natural 
Language Processing, Washington, DC. 
Lewis , D. D., & Catlett, J. (1994). Heterogeneous 
uncertainty sampling. Proceedings of the Eleventh 
International Conference on Machine Learning 
(pp. 148--156). New Brunswick, NJ: Morgan 
Kaufmann. 
Mangu, L. and Brill, E. (1997). Automatic rule 
acquisition for spelling correction. In Proc. 14th 
International Conference on Machine Learning. 
Morgan Kaufmann. 
Merialdo, B. (1994). Tagging English text with a 
probabilistic model. Computational Linguistics, 
20(2):155--172. 
Mitchell, T. M. (1999), The role of unlabeled data in 
supervised learning , in Proceedings of the Sixth 
International Colloquium on Cognitive Science, 
San Sebastian, Spain. 
Nigam, N., McCallum, A., Thrun, S., and  Mitchell, 
T. (1998). Learning to classify text from labeled 
and unlabeled documents. In Proceedings of the 
Fifteenth National Conference on Artificial 
Intelligence. AAAI Press.. 
Pedersen, T. (2000). A simple approach to building 
ensembles of naive bayesian classifiers for word 
sense disambiguation. In Proceedings of the First 
Meeting of the North American Chapter of the 
Association for Computational Linguistics May 1-
3, 2000, Seattle, WA 
Powers, D. (1997). Learning and application of 
differential grammars. In Proc. Meeting of the 
ACL Special Interest Group in Natural Language 
Learning, Madrid. 
van Halteren, H. Zavrel, J. and Daelemans, W. 
(1998). Improving data driven wordclass tagging 
by system combination. In COLING-ACL'98, 
pages 491497, Montreal, Canada. 
Weng, F., Stolcke, A, & Sankar, A (1998). Efficient 
lattice representation and generation . Proc. Intl. 
Conf. on Spoken Language Processing, vol. 6, pp. 
2531-2534. Sydney, Australia. 
Yarowsky, D. (1994). Decision lists for lexical 
ambiguity resolution: Application to accent 
restoration in Spanish and French . In Proc. 32nd 
Annual Meeting of the Association for 
Computational Linguistics, Las Cruces, NM.  
Yarowsky, D. (1995) Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 
Cambridge, MA, pp. 189-196, 1995. 
 
An Analysis of the AskMSR Question-Answering System 
Eric Brill, Susan Dumais and Michele Banko 
Microsoft Research 
One Microsoft Way 
Redmond, Wa. 98052 
{brill,sdumais,mbanko}@microsoft.com 
 
 
Abstract 
We describe the architecture of the 
AskMSR question answering system and 
systematically evaluate contributions of 
different system components to accuracy.    
The system differs from most question 
answering systems in its dependency on 
data redundancy rather than sophisticated 
linguistic analyses of either questions or 
candidate answers.    Because a wrong an-
swer is often worse than no answer, we 
also explore strategies for predicting 
when the question answering system is 
likely to give an incorrect answer. 
1 Introduction 
Question answering has recently received attention 
from the information retrieval, information extrac-
tion, machine learning, and natural language proc-
essing communities (AAAI, 2002; ACL-ECL, 
2002; Voorhees and Harman, 2000, 2001).   The 
goal of a question answering system is to retrieve 
answers to questions rather than full documents or 
best-matching passages, as most information re-
trieval systems currently do.   The TREC Question 
Answering Track, which has motivated much of 
the recent work in the field, focuses on fact-based, 
short-answer questions such as ?Who killed Abra-
ham Lincoln?? or ?How tall is Mount Everest??   
In this paper we describe our approach to short 
answer tasks like these, although the techniques we 
propose are more broadly applicable. 
Most question answering systems use a va-
riety of linguistic resources to help in understand-
ing the user?s query and matching sections in 
documents.  The most common linguistic resources 
include: part-of-speech tagging, parsing, named 
entity extraction, semantic relations, dictionaries, 
WordNet, etc. (e.g., Abney et al, 2000; Chen et al 
2000; Harabagiu et al, 2000; Hovy et al, 2000; 
Pasca et al, 2001; Prager et al, 2000).  We chose 
instead to focus on the Web as a gigantic data re-
pository with tremendous redundancy that can be 
exploited for question answering.  We view our 
approach as complimentary to more linguistic ap-
proaches, but have chosen to see how far we can 
get initially by focusing on data per se as a key 
resource available to drive our system design.  Re-
cently, other researchers have also looked to the 
web as a resource for question answering (Buch-
holtz, 2001; Clarke et al, 2001; Kwok et al, 
2001). These systems typically perform complex 
parsing and entity extraction for both queries and 
best matching Web pages, and maintain local 
caches of pages or term weights.  Our approach is 
distinguished from these in its simplicity and effi-
ciency in the use of the Web as a large data re-
source. 
Automatic QA from a single, small infor-
mation source is extremely challenging, since there 
is likely to be only one answer in the source to any 
user?s question.   Given a source, such as the 
TREC corpus, that contains only a relatively small 
number of formulations of answers to a query, we 
may be faced with the difficult task of mapping 
questions to answers by way of uncovering com-
plex lexical, syntactic, or semantic relationships 
between question string and answer string.  The 
need for anaphor resolution and synonymy, the 
presence of alternate syntactic formulations and 
indirect answers all make answer finding a poten-
tially challenging task.  However, the greater the 
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 257-264.
                         Proceedings of the Conference on Empirical Methods in Natural
     
Question Rewrite Query <Search Engine>
Collect Summaries, 
Mine N-grams
Filter N-GramsTile N-Grams N-Best Answers
Where is the Louvre
Museum located?
?+the Louvre Museum +is located?
?+the Louvre Museum +is +in?
?+the Louvre Museum +is near?
?+the Louvre Museum +is?
Louvre AND Museum AND nearin Paris France 59%
museums          12%
hostels              10%
Figure 1. System Architecture
answer redundancy in the source data collection, 
the more likely it is that we can find an answer that 
occurs in a simple relation to the question.  There-
fore, the less likely it is that we will need to solve 
the aforementioned difficulties facing natural lan-
guage processing systems. 
In this paper, we describe the architecture of 
the AskMSR Question Answering System and 
evaluate contributions of different system compo-
nents to accuracy.    Because a wrong answer is 
often worse than no answer, we also explore 
strategies for predicting when the question answer-
ing system is likely to give an incorrect answer. 
2 System Architecture 
As shown in Figure 1, the architecture of our sys-
tem can be described by four main steps: query-
reformulation, n-gram mining, filtering, and n-
gram tiling. In the remainder of this section, we 
will briefly describe these components. A more 
detailed description can be found in [Brill et al, 
2001]. 
2.1 Query Reformulation 
Given a question, the system generates a number 
of weighted rewrite strings which are likely sub-
strings of declarative answers to the question. For 
example, ?When was the paper clip invented?? is 
rewritten as ?The paper clip was invented?.  We 
then look through the collection of documents in 
search of such patterns.  Since many of these string 
rewrites will result in no matching documents, we 
also produce less precise rewrites that have a much 
greater chance of finding matches.  For each query, 
we generate a rewrite which is a backoff to a sim-
ple ANDing of all of the non-stop words in the 
query.   
The rewrites generated by our system are 
simple string-based manipulations. We do not use 
a parser or part-of-speech tagger for query refor-
mulation, but do use a lexicon for a small percent-
age of rewrites, in order to determine the possible 
parts-of-speech of a word as well as its morpho-
logical variants.   Although we created the rewrite 
rules and associated weights manually for the cur-
rent system, it may be possible to learn query-to-
answer reformulations and their weights (e.g., 
Agichtein et al, 2001; Radev et al, 2001). 
2.2 N-Gram Mining 
Once the set of query reformulations has been gen-
erated, each rewrite is formulated as a search en-
gine query and sent to a search engine from which 
page summaries are collected and analyzed.  From 
the page summaries returned by the search engine, 
n-grams are collected as possible answers to the 
question.   For reasons of efficiency, we use only 
the page summaries returned by the engine and not 
the full-text of the corresponding web page.   
The returned summaries contain the query 
terms, usually with a few words of surrounding 
context.  The summary text is processed in accor-
dance with the patterns specified by the rewrites. 
Unigrams, bigrams and trigrams are extracted and 
subsequently scored according to the weight of the 
query rewrite that retrieved it.  These scores are 
summed across all summaries containing the n-
gram (which is the opposite of the usual inverse 
document frequency component of docu-
ment/passage ranking schemes).  We do not count 
frequency of occurrence within a summary (the 
usual tf component in ranking schemes).  Thus, the 
final score for an n-gram is based on the weights 
associated with the rewrite rules that generated it 
and the number of unique summaries in which it 
occurred. 
2.3 N-Gram Filtering 
Next, the n-grams are filtered and reweighted ac-
cording to how well each candidate matches the 
expected answer-type, as specified by a handful of 
handwritten filters.  The system uses filtering in 
the following manner. First, the query is analyzed 
and assigned one of seven question types, such as 
who-question, what-question, or how-many-
question.  Based on the query type that has been 
assigned, the system determines what collection of 
filters to apply to the set of potential answers found 
during the collection of n-grams.  The candidate n-
grams are analyzed for features relevant to the fil-
ters, and then rescored according to the presence of 
such information.   
A collection of 15 simple filters were devel-
oped based on human knowledge about question 
types and the domain from which their answers can 
be drawn.  These filters used surface string fea-
tures, such as capitalization or the presence of dig-
its, and consisted of handcrafted regular expression 
patterns. 
2.4 N-Gram Tiling 
Finally, we applied an answer tiling algorithm, 
which both merges similar answers and assembles 
longer answers from overlapping smaller answer 
fragments.  For example, "A B C" and "B C D" is 
tiled into "A B C D." The algorithm proceeds 
greedily from the top-scoring candidate - all sub-
sequent candidates (up to a certain cutoff) are 
checked to see if they can be tiled with the current 
candidate answer. If so, the higher scoring candi-
date is replaced with the longer tiled n-gram, and 
the lower scoring candidate is removed. The algo-
rithm stops only when no n-grams can be further 
tiled. 
 
3 Experiments 
For experimental evaluations we used the first 500 
TREC-9 queries (201-700) (Voorhees and Harman, 
2000).  We used the patterns provided by NIST for 
automatic scoring.  A few patterns were slightly 
modified to accommodate the fact that some of the 
answer strings returned using the Web were not 
available for judging in TREC-9.   We did this in a 
very conservative manner allowing for more spe-
cific correct answers (e.g., Edward J. Smith vs. 
Edward Smith) but not more general ones (e.g., 
Smith vs. Edward Smith), and also allowing for 
simple substitutions (e.g., 9 months vs. nine 
months).  There also are substantial time differ-
ences between the Web and TREC databases (e.g., 
the correct answer to Who is the president of Bo-
livia? changes over time), but we did not modify 
the answer key to accommodate these time differ-
ences, because it would make comparison with 
earlier TREC results impossible.  These changes 
influence the absolute scores somewhat but do not 
change relative performance, which is our focus 
here.   
All runs are completely automatic, starting 
with queries and generating a ranked list of 5 can-
didate answers.  For the experiments reported in 
this paper we used Google as a backend because it 
provides query-relevant summaries that make our 
n-gram mining efficient.  Candidate answers are a 
maximum of 50 bytes long, and typically much 
shorter than that.  We report the Mean Reciprocal 
Rank (MRR) of the first correct answer, the Num-
ber of Questions Correctly Answered (NAns), and 
the proportion of Questions Correctly Answered 
(%Ans).   
3.1 Basic System Performance 
Using our current system with default settings we 
obtain a MRR of 0.507 and answers 61% of the 
queries correctly (Baseline, Table 1).  The average 
answer length was 12 bytes, so the system is re-
turning short answers, not passages.   Although it 
is impossible to compare our results precisely with 
TREC-9 groups, this is very good performance and 
would place us near the top of 50-byte runs for 
TREC-9.   
3.2 Contributions of Components 
Table 1 summarizes the contributions of the differ-
ent system components to this overall perform-
ance.  We report summary statistics as well as 
percent change in performance when components 
are removed (%Drop MRR). 
 
Query Rewrites: 
As described earlier, queries are transformed to 
successively less precise formats, with a final 
backoff to simply ANDing all the non-stop query 
terms.  More precise queries have higher weights 
associated with them, so n-grams found in these 
responses are given priority.   If we set al the re-
write weights to be equal, MRR drops from 0.507 
to 0.489, a drop of 3.6%.   Another way of looking 
at the importance of the query rewrites is to exam-
ine performance where the only rewrite the system 
uses is the backoff AND query.   Here the drop is 
more substantial, down to 0.450 which represents a 
drop of 11.2%.    
Query rewrites are one way in which we 
capitalize on the tremendous redundancy of data 
on the web ? that is, the occurrence of multiple 
linguistic formulations of the same answers in-
creases the chances of being able to find an answer 
that occurs within the context of a simple pattern 
match with the query.   Our simple rewrites help 
compared to doing just AND matching. Soubbotin 
and Soubbotin (2001) have used more specific 
regular expression matching to good advantage and 
we could certainly incorporate some of those ideas 
as well. 
MRR NAns %Ans
%Drop
MRR
Baseline 0.507 307 61.4% 0.0%
Query Rewrite:
  Same Weight All Rewrites 0.489 298 59.6% 3.6%
  AND-only query 0.450 281 56.2% 11.2%
Filter N-Gram:
  Base, NoFiltering 0.416 268 53.6% 17.9%
  AND, NoFiltering 0.338 226 45.2% 33.3%
Tile N-Gram:
  Base, NoTiling 0.435 277 55.4% 14.2%
  AND, NoTiling 0.397 251 50.2% 21.7%
Combinations:
  Base, NoTiling NoFiltering 0.319 233 46.6% 37.1%
  AND, NoTiling NoFiltering 0.266 191 38.2% 47.5%
Table 1.  Componential analysis of the AskMSR QA system.
 
N-Gram Filtering: 
Unigrams, bigrams and trigrams are extracted from 
the (up to) 100 best-matching summaries for each 
rewrite, and scored according the weight of the 
query rewrite that retrieved them.  The score as-
signed to an n-gram is a weighted sum across the 
summaries containing the n-grams, where the 
weights are those associated with the rewrite that 
retrieved a particular summary.   The best-scoring 
n-grams are then filtered according to seven query 
types.   For example the filter for the query How 
many dogs pull a sled in the Iditarod? prefers a 
number, so candidate n-grams  like dog race, run, 
Alaskan, dog racing, many mush move down the 
list and pool of 16 dogs (which is a correct answer) 
moves up.  Removing the filters decreases MRR 
by 17.9% relative to baseline (down to 0.416).  Our 
simple n-gram filtering is the most important indi-
vidual component of the system. 
 
N-Gram Tiling: 
Finally, n-grams are tiled to create longer answer 
strings.   This is done in a simple greedy statistical 
manner from the top of the list down.   Not doing 
this tiling decreases performance by 14.2% relative 
to baseline (down to 0.435).    The advantages 
gained from tiling are two-fold.  First, with tiling 
substrings do not take up several answer slots, so 
the three answer candidates: San, Francisco, and 
San Francisco, are conflated into the single answer 
candidate: San Francisco.  In addition, longer an-
swers can never be found with only trigrams, e.g., 
light amplification by stimulted emission of radia-
tion can only be returned by tiling these shorter n-
grams into a longer string.   
 
Combinations of Components: 
Not surprisingly, removing all of our major com-
ponents except the n-gram accumulation (weighted 
sum of occurrences of unigrams, bigrams and tri-
grams) results in substantially worse performance 
than our full system, giving an MRR of 0.266, a 
decrease of 47.5%.    The simplest entirely statisti-
cal system with no linguistic knowledge or proc-
essing employed, would use only AND queries, do 
no filtering, but do statistical tiling.   This system 
uses redundancy only in summing n-gram counts 
across summaries.  This system has MRR 0.338, 
which is a 33% drop from the best version of our 
system, with all components enabled.   Note, how-
ever, that even with absolutely no linguistic proc-
essing, the performance attained is still very rea-
sonable performance on an absolute scale, and in 
fact only one TREC-9 50-byte run achieved higher 
accuracy than this. 
To summarize, we find that all of our process-
ing components contribute to the overall accuracy 
of the question-answering system.  The precise 
weights assigned to different query rewrites seems 
relatively unimportant, but the rewrites themselves 
do contribute considerably to overall accuracy.    
N-gram tiling turns out to be extremely effective, 
serving in a sense as a ?poor man?s named-entity 
recognizer?.  Because of the effectiveness of our 
tiling algorithm over large amounts of data, we do 
not need to use any named entity recognition com-
ponents.  The component that identifies what filters 
to apply over the harvested n-grams, along with the 
actual regular expression filters themselves, con-
tributes the most to overall performance. 
4 Component Problems 
Above we described how components contributed 
to improving the performance of the system.  In 
this section we look at what components errors are 
attributed to.  In Table 2, we show the distribution 
of error causes, looking at those questions for 
which the system returned no correct answer in the 
top five hypotheses. 
 
Problem % of Errors 
Units 23 
Time 20 
Assembly 16 
Correct 14 
Beyond Paradigm 12 
Number Retrieval 5 
Unknown Problem 5 
Synonymy 2 
Filters  2 
Table 2.  Error Attribution 
 
The biggest error comes from not knowing 
what units are likely to be in an answer given a 
question (e.g. How fast can a Corvette go ? xxx 
mph).  Interestingly, 34% of our errors (Time and 
Correct) are not really errors, but are due to time 
problems or cases where the answer returned is 
truly correct but not present in the TREC-9 answer 
key.  16% of the failures come from the inability of 
our n-gram tiling algorithm to build up the full 
string necessary to provide a correct answer.   
Number retrieval problems come from the fact 
that we cannot query the search engine for a num-
ber without specifying the number.  For example, a 
good rewrite for the query How many islands does 
Fiji have would be ? Fiji has <NUM> islands ?, 
but we are unable to give this type of query to the 
search engine.  Only 12% of the failures we clas-
sify as being truly outside of the system?s current 
paradigm, rather than something that is either al-
ready correct or fixable with minor system en-
hancements. 
5 Knowing When We Don?t Know 
Typically, when deploying a question answering 
system, there is some cost associated with return-
ing incorrect answers to a user.  Therefore, it is 
important that a QA system has some idea as to 
how likely an answer is to be correct, so it can 
choose not to answer rather than answer incor-
rectly.  In the TREC QA track, there is no distinc-
tion made in scoring between returning a wrong 
answer to a question for which an answer exists 
and returning no answer.  However, to deploy a 
real system, we need the capability of making a 
trade-off between precision and recall, allowing 
the system not to answer a subset of questions, in 
hopes of attaining high accuracy for the questions 
which it does answer. 
Most question-answering systems use 
hand-tuned weights that are often combined in an 
ad-hoc fashion into a final score for an answer hy-
pothesis (Harabagiu et al, 2000; Hovy et al, 2000; 
Prager et al, 2000; Soubbotin & Soubbotin, 2001; 
Brill et. al., 2001).   Is it still possible to induce a 
useful precision-recall (ROC) curve when the sys-
tem is not outputting meaningful probabilities for 
answers?  We have explored this issue within the 
AskMSR question-answering system.   
Ideally, we would like to be able to deter-
mine the likelihood of answering correctly solely 
from an analysis of the question.  If we can deter-
mine we are unlikely to answer a question cor-
rectly, then we need not expend the time, cpu 
cycles and network traffic necessary to try to an-
swer that question.   
We built a decision tree to try to predict 
whether the system will answer correctly, based on 
a set of features extracted from the question string: 
word unigrams and bigrams, sentence length 
(QLEN), the number of capitalized words in the 
sentence, the number of stop words in the sentence 
(NUMSTOP), the ratio of the number of nonstop 
words to stop words, and the length of longest 
word (LONGWORD).  We use a decision tree be-
cause we also wanted to use this as a diagnostic 
tool to indicate what question types we need to put 
further developmental efforts into.  The decision 
tree built from these features is shown in Figure 2. 
The first split of the tree asks if the word ?How? 
appears in the question.  Indeed, the system per-
forms worst on ?How? question types.  We do best 
on short ?Who? questions with a large number of 
stop words. 
 
 
Figure 2.  Learning When We Don't Know -- Us-
ing Only Features from Query 
 
We can induce an ROC curve from this 
decision tree by sorting the leaf nodes from the 
highest probability of being correct to the lowest.  
Then we can gain precision at the expense of recall 
by not answering questions in the leaf nodes that 
have the highest probability of error.  The result of 
doing this can be seen in Figures 3 and 4, the line 
labeled ?Question Features?.  The decision tree 
was trained on Trec 9 data.  Figure 3 shows the 
results when applied to the same training data, and 
Figure 4 shows the results when testing on Trec 10 
data.  As we can see, the decision tree overfits the 
training data and does not generalize sufficiently to 
give useful results on the Trec 10 (test) data. 
Next, we explored how well answer cor-
rectness correlates with answer score in our sys-
tem.  As discussed above, the final score assigned 
to an answer candidate is a somewhat ad-hoc score 
based upon the number of retrieved passages the n-
gram occurs in, the weight of the rewrite used to 
retrieve each passage, what filters apply to the n-
gram, and the effects of merging n-grams in an-
swer tiling.  In Table 3, we show the correlation 
coefficient calculated between whether a correct 
answer appears in the top 5 answers output by the 
system and (a) the score of the system?s first 
ranked answer and (b) the score of the first ranked 
answer minus the score of the second ranked an-
swer.  A correlation coefficient of 1 indicates 
strong positive association, whereas a correlation 
of ?1 indicates strong negative association. We see 
that there is indeed a correlation between the 
scores output by the system and the answer accu-
racy, with the correlation being tighter when just 
considering the score of the first answer. 
 
 Correlation 
Coefficient 
Score #1 .363 
Score #1 ? Score #2 .270 
Table 3 . Do answer scores correlate with correct-
ness? 
 
Because a number of answers returned by 
our system are correct but scored wrong according 
to the TREC answer key because of time mis-
matches, we also looked at the correlation, limiting 
ourselves to Trec 9 questions that were not time-
sensitive.  Using this subset of questions, the corre-
lation coefficient between whether a correct an-
swer appears in the system?s top five answers, and 
the score of the #1 answer, increases from .363 to 
.401.  In Figure 3 and 4, we show the ROC curve 
induced by deciding when not to answer a question 
based on the score of the first ranked answer (the 
line labeled ?score of #1 answer?).  Note that the 
score of the top ranked answer is a significantly 
better predictor of accuracy than what we attain by 
considering features of the question string, and 
gives consistent results across two data sets. 
Finally, we looked into whether other at-
tributes were indicative of the likelihood of answer 
correctness.  For every question, a set of snippets is 
gathered.  Some of these snippets come from AND 
queries and others come from more refined exact 
string match rewrites.  In Table 4, we show MRR 
as a function of the number of non-AND snippets 
retrieved.  For instance, when all of the snippets 
come from AND queries, the resulting MRR was 
found to be only .238.  For questions with 100 to 
400 snippets retrieved from exact string match re-
writes, the MRR was .628. 
 
NumQ MRR
0 91 0.238
1 to 10 80 0.405
11 to 100 153 0.612
100 to 400 175 0.628
NumNon-AND 
Passages
 
Table 4 . Accuracy vs. Number of Passages Re-
trieved From Non-AND Rewrites 
 
We built a decision tree to predict whether 
a correct answer appears in the top 5 answers, 
based on all of the question-derived features de-
scribed earlier, the score of the number one rank-
ing answer, as well as a number of additional 
features describing the state of the system in proc-
essing a particular query.  Some of these features 
include: the total number of matching passages 
retrieved, the number of non-AND matching pas-
sages retrieved, whether a filter applied, and the 
weight of the best rewrite rule for which matching 
passages were found.   We show the resulting deci-
sion tree in Figure 5, and resulting ROC curve con-
structed from this decision tree, in Figure 3 and 4 
(the line labeled ?All Features?).  In this case, the 
decision tree does give a useful ROC curve on the 
test data (Figure 4), but does not outperform the 
simple technique of using the ad hoc score of the 
best answer returned by the system.  Still, the deci-
sion tree has proved to be a useful diagnostic in 
helping us understand the weaknesses of our sys-
tem. 
 
ROC Curve for QA
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.5 1
Recall
Pr
ec
isi
on
All Features
Score of #1
Answer
Question
Features
 
 
Figure 3. Three different precision/recall trade-
offs, trained on Trec 9 and tested on Trec 9. 
 
Trec 10 ROC Curve
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 0.5 1
Recall
Pr
ec
isi
on
Score of #1
Answer
Question
Features
All Features
 
Figure 4. Three different precision/recall trade-
offs, trained on Trec 9 and tested on Trec 10. 
6 Conclusions 
We have presented a novel approach to question-
answering and carefully analyzed the contributions 
of each major system component, as well as ana-
lyzing what factors account for the majority of er-
rors made by the AskMSR question answering 
system.   In addition, we have demonstrated an 
approach to learning when the system is likely to 
answer a question incorrectly, allowing us to reach 
any desired rate of accuracy by not answering 
some portion of questions.  We are currently ex-
ploring whether these techniques can be extended 
beyond short answer QA to more complex cases of 
information access.   
 
  
Figure 5.  Learning When We Don't Know -- Us-
ing All Features 
 
References 
AAAI Spring Symposium Series Mining answers from 
text and knowledge bases (2002). 
S. Abney, M. Collins and A. Singhal (2000).  Answer 
extraction.  In Proceedings of ANLP 2000. 
ACL-EACL Workshop on Open-domain question an-
swering.  (2002). 
E. Agichtein, S. Lawrence and L. Gravano (2001).  
Learning search engine specific query transforma-
tions for question answering.  In Proceedings of 
WWW10. 
E. Brill, J. Lin, M. Banko, S. Dumais and A. Ng (2001). 
Data-intensive question answering.  In Proceedings 
of the Tenth Text Retrieval Conference (TREC 2001). 
S. Buchholz (2001).  Using grammatical relations, an-
swer frequencies and the World Wide Web for TREC 
question answering.   To appear in Proceedings of 
the Tenth Text REtrieval Conference (TREC 2001). 
J. Chen, A. R. Diekema, M. D. Taffet, N. McCracken, 
N. E. Ozgencil, O. Yilmazel, E. D. Liddy (2001).  
Question answering: CNLP at the TREC-10 question 
answering track.  To appear in Proceedings of the 
Tenth Text REtrieval Conference (TREC 2001). 
C. Clarke, G. Cormack and T. Lyman (2001).  Exploit-
ing redundancy in question answering.   In Proceed-
ings of SIGIR?2001. 
C. Clarke, G. Cormack and T. Lynam (2001).   Web 
reinforced question answering.  To appear in Pro-
ceedings of the Tenth Text REtrieval Conference 
(TREC 2001). 
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. 
Surdeanu, R. Bunescu, R. Girju, V. Rus and P. 
Morarescu (2000).  FALCON: Boosting knowledge 
for question answering.  In Proceedings of the Ninth 
Text REtrieval Conference (TREC-9). 
E. Hovy, L. Gerber, U. Hermjakob, M. Junk and C. Lin 
(2000).  Question answering in Webclopedia.  In 
Proceedings of the Ninth Text REtrieval Conference 
(TREC-9). 
E. Hovy, U. Hermjakob and C. Lin (2001).  The use of 
external knowledge in factoid QA.  To appear in 
Proceedings of the Tenth Text REtrieval Conference 
(TREC 2001). 
C. Kwok, O. Etzioni and D. Weld (2001).  Scaling ques-
tion answering to the Web.  In Proceedings of 
WWW?10. 
M. A. Pasca and S. M. Harabagiu (2001).  High per-
formance question/answering.  In Proceedings of 
SIGIR?2001. 
J. Prager, E. Brown, A. Coden and D. Radev (2000).  
Question answering by predictive annotation.  In 
Proceedings of SIGIR?2000. 
D. R. Radev, H. Qi, Z. Zheng, S. Blair-Goldensohn, Z. 
Zhang, W. Fan and J. Prager (2001). Mining the web 
for answers to natural language questions. In ACM 
CIKM 2001: Tenth International Conference on In-
formation and Knowledge Management.  
M. M. Soubbotin and S. M. Soubbotin (2001).  Patterns 
and potential answer expressions as clues to the right 
answers.  To appear in Proceedings of the Tenth Text 
REtrieval Conference (TREC 2001). 
E. Voorhees and D. Harman, Eds. (2000).  Proceedings 
of the Ninth Text REtrieval Conference (TREC-9). 
E. Voorhees and D. Harman, Eds. (2001).  Proceedings 
of the Tenth Text REtrieval Conference (TREC 
2001). 
 
NAACL HLT Demonstration Program, pages 25?26,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
TextRunner: Open Information Extraction on the Web
Alexander Yates
Michael Cafarella
Michele Banko
Oren Etzioni
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195-2350
{ayates,banko,hastur,mjc,etzioni,soderlan}@cs.washington.edu
Matthew Broadhead
Stephen Soderland
1 Introduction
Traditional information extraction systems
have focused on satisfying precise, narrow,
pre-specified requests from small, homoge-
neous corpora. In contrast, the TextRunner
system demonstrates a new kind of informa-
tion extraction, called Open Information Ex-
traction (OIE), in which the system makes a
single, data-driven pass over the entire cor-
pus and extracts a large set of relational
tuples, without requiring any human input.
(Banko et al, 2007) TextRunner is a fully-
implemented, highly scalable example of OIE.
TextRunner?s extractions are indexed, al-
lowing a fast query mechanism.
Our first public demonstration of the Text-
Runner system shows the results of perform-
ing OIE on a set of 117 million web pages. It
demonstrates the power of TextRunner in
terms of the raw number of facts it has ex-
tracted, as well as its precision using our novel
assessment mechanism. And it shows the abil-
ity to automatically determine synonymous re-
lations and objects using large sets of extrac-
tions. We have built a fast user interface for
querying the results.
2 Previous Work
The bulk of previous information extraction
work uses hand-labeled data or hand-crafted
patterns to enable relation-specific extraction
(e.g., (Culotta et al, 2006)). OIE seeks to
avoid these requirements for human input.
Shinyama and Sekine (Shinyama and
Sekine, 2006) describe an approach to ?un-
restricted relation discovery? that does away
with many of the requirements for human in-
put. However, it requires clustering of the doc-
uments used for extraction, and thus scales in
quadratic time in the number of documents.
It does not scale to the size of the Web.
For a full discussion of previous work, please
see (Banko et al, 2007), or see (Yates and Et-
zioni, 2007) for work relating to synonym res-
olution.
3 Open IE in TextRunner
OIE presents significant new challenges for in-
formation extraction systems, including
Automation of relation extraction, which in
traditional information extraction uses hand-
labeled inputs.
Corpus Heterogeneity on the Web, which
makes tools like parsers and named-entity tag-
gers less accurate because the corpus is differ-
ent from the data used to train the tools.
Scalability and efficiency of the system.
Open IE systems are effectively restricted to
a single, fast pass over the data so that they
can scale to huge document collections.
In response to these challenges, Text-
Runner includes several novel components,
which we now summarize (see (Banko et al,
2007) for details).
1. Single Pass Extractor
The TextRunner extractor makes a sin-
gle pass over all documents, tagging sen-
tences with part-of-speech tags and noun-
phrase chunks as it goes. For each pair of noun
phrases that are not too far apart, and subject
to several other constraints, it applies a clas-
sifier described below to determine whether or
not to extract a relationship. If the classifier
25
deems the relationship trustworthy, a tuple of
the form t = (ei, rj , ek) is extracted, where
ei, ek are entities and rj is the relation between
them. For example, TextRunner might ex-
tract the tuple (Edison, invented, light bulbs).
On our test corpus (a 9 million document sub-
set of our full corpus), it took less than 68
CPU hours to process the 133 million sen-
tences. The process is easily parallelized, and
took only 4 hours to run on our cluster.
2. Self-Supervised Classifier
While full parsing is too expensive to apply to
the Web, we use a parser to generate training
examples for extraction. Using several heuris-
tic constraints, we automatically label a set
of parsed sentences as trustworthy or untrust-
worthy extractions (positive and negative ex-
amples, respectively). The classifier is trained
on these examples, using features such as the
part of speech tags on the words in the re-
lation. The classifier is then able to decide
whether a sequence of POS-tagged words is a
correct extraction with high accuracy.
3. Synonym Resolution
Because TextRunner has no pre-defined re-
lations, it may extract many different strings
representing the same relation. Also, as with
all information extraction systems, it can ex-
tract multiple names for the same object. The
Resolver system performs an unsupervised
clustering of TextRunner?s extractions to
create sets of synonymous entities and rela-
tions. Resolver uses a novel, unsupervised
probabilistic model to determine the probabil-
ity that any pair of strings is co-referential,
given the tuples that each string was extracted
with. (Yates and Etzioni, 2007)
4. Query Interface
TextRunner builds an inverted index of
the extracted tuples, and spreads it across a
cluster of machines. This architecture sup-
ports fast, interactive, and powerful relational
queries. Users may enter words in a relation or
entity, and TextRunner quickly returns the
entire set of extractions matching the query.
For example, a query for ?Newton? will return
tuples like (Newton, invented, calculus). Users
may opt to query for all tuples matching syn-
onyms of the keyword input, and may also opt
to merge all tuples returned by a query into
sets of tuples that are deemed synonymous.
4 Experimental Results
On our test corpus of 9 million Web doc-
uments, TextRunner extracted 7.8 million
well-formed tuples. On a randomly selected
subset of 400 tuples, 80.4% were deemed cor-
rect by human reviewers.
We performed a head-to-head compari-
son with a state-of-the-art traditional in-
formation extraction system, called Know-
ItAll. (Etzioni et al, 2005) On a set of ten
high-frequency relations, TextRunner found
nearly as many correct extractions as Know-
ItAll (11,631 to 11,476), while reducing the
error rate of KnowItAll by 33% (18% to
12%).
Acknowledgements
This research was supported in part by NSF
grants IIS-0535284 and IIS-0312988, DARPA
contract NBCHD030010, ONR grant N00014-
05-1-0185 as well as gifts from Google, and
carried out at the University of Washington?s
Turing Center.
References
M. Banko, M. J. Cafarella, S. Soderland,
M. Broadhead, and O. Etzioni. 2007. Open In-
formation Extraction from the Web. In IJCAI.
A. Culotta, A. McCallum, and J. Betz. 2006. Inte-
grating Probabilistic Extraction Models and Re-
lational Data Mining to Discover Relations and
Patterns in Text. In HLT-NAACL.
O. Etzioni, M. Cafarella, D. Downey, S. Kok,
A. Popescu, T. Shaked, S. Soderland, D. Weld,
and A. Yates. 2005. Unsupervised Named-
Entity Extraction from the Web: An Experi-
mental Study. Artificial Intelligence, 165(1):91?
134.
Y. Shinyama and S. Sekine. 2006. Preemptive
Information Extraction Using Unrestricted Re-
lation Discovery. In HLT-NAACL.
A. Yates and O. Etzioni. 2007. Unsupervised Res-
olution of Objects and Relations on the Web. In
NAACL-HLT.
26
Proceedings of ACL-08: HLT, pages 28?36,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The Tradeoffs Between Open and Traditional Relation Extraction
Michele Banko and Oren Etzioni
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195, USA
banko,etzioni@cs.washington.edu
Abstract
Traditional Information Extraction (IE) takes
a relation name and hand-tagged examples of
that relation as input. Open IE is a relation-
independent extraction paradigm that is tai-
lored to massive and heterogeneous corpora
such as theWeb. An Open IE system extracts a
diverse set of relational tuples from text with-
out any relation-specific input. How is Open
IE possible? We analyze a sample of English
sentences to demonstrate that numerous rela-
tionships are expressed using a compact set
of relation-independent lexico-syntactic pat-
terns, which can be learned by an Open IE sys-
tem.
What are the tradeoffs between Open IE and
traditional IE? We consider this question in
the context of two tasks. First, when the
number of relations is massive, and the rela-
tions themselves are not pre-specified, we ar-
gue that Open IE is necessary. We then present
a new model for Open IE called O-CRF and
show that it achieves increased precision and
nearly double the recall than the model em-
ployed by TEXTRUNNER, the previous state-
of-the-art Open IE system. Second, when the
number of target relations is small, and their
names are known in advance, we show that
O-CRF is able to match the precision of a tra-
ditional extraction system, though at substan-
tially lower recall. Finally, we show how to
combine the two types of systems into a hy-
brid that achieves higher precision than a tra-
ditional extractor, with comparable recall.
1 Introduction
Relation Extraction (RE) is the task of recognizing
the assertion of a particular relationship between two
or more entities in text. Typically, the target relation
(e.g., seminar location) is given to the RE system as
input along with hand-crafted extraction patterns or
patterns learned from hand-labeled training exam-
ples (Brin, 1998; Riloff and Jones, 1999; Agichtein
and Gravano, 2000). Such inputs are specific to the
target relation. Shifting to a new relation requires a
person to manually create new extraction patterns or
specify new training examples. This manual labor
scales linearly with the number of target relations.
In 2007, we introduced a new approach to the
RE task, called Open Information Extraction (Open
IE), which scales RE to the Web. An Open IE sys-
tem extracts a diverse set of relational tuples without
requiring any relation-specific human input. Open
IE?s extraction process is linear in the number of
documents in the corpus, and constant in the num-
ber of relations. Open IE is ideally suited to corpora
such as the Web, where the target relations are not
known in advance, and their number is massive.
The relationship between standard RE systems
and the new Open IE paradigm is analogous to the
relationship between lexicalized and unlexicalized
parsers. Statistical parsers are usually lexicalized
(i.e. they make parsing decisions based on n-gram
statistics computed for specific lexemes). However,
Klein and Manning (2003) showed that unlexical-
ized parsers are more accurate than previously be-
lieved, and can be learned in an unsupervised man-
ner. Klein and Manning analyze the tradeoffs be-
28
tween the two approaches to parsing and argue that
state-of-the-art parsing will benefit from employing
both approaches in concert. In this paper, we exam-
ine the tradeoffs between relation-specific (?lexical-
ized?) extraction and relation-independent (?unlexi-
calized?) extraction and reach an analogous conclu-
sion.
Is it, in fact, possible to learn relation-independent
extraction patterns? What do they look like? We first
consider the task of open extraction, in which the
goal is to extract relationships from text when their
number is large and identity unknown. We then con-
sider the targeted extraction task, in which the goal
is to locate instances of a known relation. How does
the precision and recall of Open IE compare with
that of relation-specific extraction? Is it possible to
combine Open IE with a ?lexicalized? RE system
to improve performance? This paper addresses the
questions raised above and makes the following con-
tributions:
? We present O-CRF, a new Open IE system that
uses Conditional Random Fields, and demon-
strate its ability to extract a variety of rela-
tions with a precision of 88.3% and recall of
45.2%. We compare O-CRF to O-NB, the ex-
traction model previously used by TEXTRUN-
NER (Banko et al, 2007), a state-of-the-art
Open IE system. We show that O-CRF achieves
a relative gain in F-measure of 63% over O-NB.
? We provide a corpus-based characterization of
how binary relationships are expressed in En-
glish to demonstrate that learning a relation-
independent extractor is feasible, at least for the
English language.
? In the targeted extraction case, we compare the
performance of O-CRF to a traditional RE sys-
tem and find that without any relation-specific
input, O-CRF obtains the same precision with
lower recall compared to a lexicalized extractor
trained using hundreds, and sometimes thou-
sands, of labeled examples per relation.
? We present H-CRF, an ensemble-based extrac-
tor that learns to combine the output of the
lexicalized and unlexicalized RE systems and
achieves a 10% relative increase in precision
with comparable recall over traditional RE.
The remainder of this paper is organized as fol-
lows. Section 2 assesses the promise of relation-
independent extraction for the English language by
characterizing how a sample of relations is ex-
pressed in text. Section 3 describes O-CRF, a new
Open IE system, as well as R1-CRF, a standard RE
system; a hybrid RE system is then presented in Sec-
tion 4. Section 5 reports on our experimental results.
Section 6 considers related work, which is then fol-
lowed by a discussion of future work.
2 The Nature of Relations in English
How are relationships expressed in English sen-
tences? In this section, we show that many rela-
tionships are consistently expressed using a com-
pact set of relation-independent lexico-syntactic pat-
terns, and quantify their frequency based on a sam-
ple of 500 sentences selected at random from an IE
training corpus developed by (Bunescu andMooney,
2007).1 This observation helps to explain the suc-
cess of open relation extraction, which learns a
relation-independent extraction model as described
in Section 3.1.
Previous work has noted that distinguished re-
lations, such as hypernymy (is-a) and meronymy
(part-whole), are often expressed using a small num-
ber of lexico-syntactic patterns (Hearst, 1992). The
manual identification of these patterns inspired a
body of work in which this initial set of extraction
patterns is used to seed a bootstrapping process that
automatically acquires additional patterns for is-a or
part-whole relations (Etzioni et al, 2005; Snow et
al., 2005; Girju et al, 2006), It is quite natural then
to consider whether the same can be done for all bi-
nary relationships.
To characterize how binary relationships are ex-
pressed, one of the authors of this paper carefully
studied the labeled relation instances and produced
a lexico-syntactic pattern that captured the relation
for each instance. Interestingly, we found that 95%
of the patterns could be grouped into the categories
listed in Table 1. Note, however, that the patterns
shown in Table 1 are greatly simplified by omitting
the exact conditions under which they will reliably
produce a correct extraction. For instance, while
many relationships are indicated strictly by a verb,
1For simplicity, we restrict our study to binary relationships.
29
Simplified
Relative Lexico-Syntactic
Frequency Category Pattern
37.8 Verb E1 Verb E2
X established Y
22.8 Noun+Prep E1 NP Prep E2
X settlement with Y
16.0 Verb+Prep E1 Verb Prep E2
X moved to Y
9.4 Infinitive E1 to Verb E2
X plans to acquire Y
5.2 Modifier E1 Verb E2 Noun
X is Y winner
1.8 Coordinaten E1 (and|,|-|:) E2 NP
X-Y deal
1.0 Coordinatev E1 (and|,) E2 Verb
X , Y merge
0.8 Appositive E1 NP (:|,)? E2
X hometown : Y
Table 1: Taxonomy of Binary Relationships: Nearly 95%
of 500 randomly selected sentences belongs to one of the
eight categories above.
detailed contextual cues are required to determine,
exactly which, if any, verb observed in the context
of two entities is indicative of a relationship between
them. In the next section, we show how we can use a
Conditional Random Field, a model that can be de-
scribed as a finite state machine with weighted tran-
sitions, to learn a model of how binary relationships
are expressed in English.
3 Relation Extraction
Given a relation name, labeled examples of the re-
lation, and a corpus, traditional Relation Extraction
(RE) systems output instances of the given relation
found in the corpus. In the open extraction task, re-
lation names are not known in advance. The sole
input to an Open IE system is a corpus, along with
a small set of relation-independent heuristics, which
are used to learn a general model of extraction for
all relations at once.
The task of open extraction is notably more diffi-
cult than the traditional formulation of RE for sev-
eral reasons. First, traditional RE systems do not
attempt to extract the text that signifies a relation in
a sentence, since the relation name is given. In con-
trast, an Open IE system has to locate both the set of
entities believed to participate in a relation, and the
salient textual cues that indicate the relation among
them. Knowledge extracted by an open system takes
the form of relational tuples (r, e1, . . . , en) that con-
tain two or more entities e1, . . . , en, and r, the name
of the relationship among them. For example, from
the sentence, ?Microsoft is headquartered in beau-
tiful Redmond?, we expect to extract (is headquar-
tered in, Microsoft, Redmond). Moreover, following
extraction, the system must identify exactly which
relation strings r correspond to a general relation of
interest. To ensure high-levels of coverage on a per-
relation basis, we need, for example to deduce that
? ?s headquarters in?, ?is headquartered in? and ?is
based in? are different ways of expressing HEAD-
QUARTERS(X,Y).
Second, a relation-independent extraction process
makes it difficult to leverage the full set of features
typically used when performing extraction one re-
lation at a time. For instance, the presence of the
words company and headquarters will be useful in
detecting instances of the HEADQUARTERS(X,Y)
relation, but are not useful features for identifying
relations in general. Finally, RE systems typically
use named-entity types as a guide (e.g., the second
argument to HEADQUARTERS should be a LOCA-
TION). In Open IE, the relations are not known in
advance, and neither are their argument types.
The unique nature of the open extraction task has
led us to develop O-CRF, an open extraction sys-
tem that uses the power of graphical models to iden-
tify relations in text. The remainder of this section
describes O-CRF, and compares it to the extraction
model employed by TEXTRUNNER, the first Open
IE system (Banko et al, 2007). We then describe
R1-CRF, a RE system that can be applied in a typi-
cal one-relation-at-a-time setting.
3.1 Open Extraction with Conditional Random
Fields
TEXTRUNNER initially treated Open IE as a clas-
sification problem, using a Naive Bayes classifier to
predict whether heuristically-chosen tokens between
two entities indicated a relationship or not. For the
remainder of this paper, we refer to this model as
O-NB. Whereas classifiers predict the label of a sin-
gle variable, graphical models model multiple, in-
30
K a f k a
E N T O E N TO E N T B 	 R E L I 	 R E L
, P r
a g u ea w
r i t
e
r b o r n i n
Figure 1: Relation Extraction as Sequence Labeling: A
CRF is used to identify the relationship, born in, between
Kafka and Prague
terdependent variables. Conditional Random Fields
(CRFs) (Lafferty et al, 2001), are undirected graphi-
cal models trained to maximize the conditional prob-
ability of a finite set of labels Y given a set of input
observations X . By making a first-order Markov as-
sumption about the dependencies among the output
variables Y , and arranging variables sequentially in
a linear chain, RE can be treated as a sequence la-
beling problem. Linear-chain CRFs have been ap-
plied to a variety of sequential text processing tasks
including named-entity recognition, part-of-speech
tagging, word segmentation, semantic role identifi-
cation, and recently relation extraction (Culotta et
al., 2006).
3.1.1 Training
As with O-NB, O-CRF?s training process is self-
supervised. O-CRF applies a handful of relation-
independent heuristics to the PennTreebank and ob-
tains a set of labeled examples in the form of rela-
tional tuples. The heuristics were designed to cap-
ture dependencies typically obtained via syntactic
parsing and semantic role labelling. For example,
a heuristic used to identify positive examples is the
extraction of noun phrases participating in a subject-
verb-object relationship, e.g., ?<Einstein> received
<the Nobel Prize> in 1921.? An example of a
heuristic that locates negative examples is the ex-
traction of objects that cross the boundary of an ad-
verbial clause, e.g. ?He studied <Einstein?s work>
when visiting <Germany>.?
The resulting set of labeled examples are de-
scribed using features that can be extracted without
syntactic or semantic analysis and used to train a
CRF, a sequence model that learns to identify spans
of tokens believed to indicate explicit mentions of
relationships between entities.
O-CRF first applies a phrase chunker to each doc-
ument, and treats the identified noun phrases as can-
didate entities for extraction. Each pair of enti-
ties appearing no more than a maximum number of
words apart and their surrounding context are con-
sidered as possible evidence for RE. The entity pair
serves to anchor each end of a linear-chain CRF, and
both entities in the pair are assigned a fixed label of
ENT. Tokens in the surrounding context are treated
as possible textual cues that indicate a relation, and
can be assigned one of the following labels: B-REL,
indicating the start of a relation, I-REL, indicating
the continuation of a predicted relation, or O, indi-
cating the token is not believed to be part of an ex-
plicit relationship. An illustration is given in Fig-
ure 1.
The set of features used by O-CRF is largely
similar to those used by O-NB and other state-
of-the-art relation extraction systems, They in-
clude part-of-speech tags (predicted using a sepa-
rately trained maximum-entropy model), regular ex-
pressions (e.g.detecting capitalization, punctuation,
etc.), context words, and conjunctions of features
occurring in adjacent positions within six words to
the left and six words to the right of the current
word. A unique aspect of O-CRF is that O-CRF
uses context words belonging only to closed classes
(e.g. prepositions and determiners) but not function
words such as verbs or nouns. Thus, unlike most RE
systems, O-CRF does not try to recognize semantic
classes of entities.
O-CRF has a number of limitations, most of which
are shared with other systems that perform extrac-
tion from natural language text. First, O-CRF only
extracts relations that are explicitly mentioned in
the text; implicit relationships that could inferred
from the text would need to be inferred from O-
CRF extractions. Second, O-CRF focuses on rela-
tionships that are primarily word-based, and not in-
dicated solely from punctuation or document-level
features. Finally, relations must occur between en-
tity names within the same sentence.
O-CRF was built using the CRF implementation
provided by MALLET (McCallum, 2002), as well
as part-of-speech tagging and phrase-chunking tools
available from OPENNLP.2
2http://opennlp.sourceforge.net
31
3.1.2 Extraction
Given an input corpus, O-CRF makes a single pass
over the data, and performs entity identification us-
ing a phrase chunker. The CRF is then used to label
instances relations for each possible entity pair, sub-
ject to the constraints mentioned previously.
Following extraction, O-CRF applies the RE-
SOLVER algorithm (Yates and Etzioni, 2007) to find
relation synonyms, the various ways in which a re-
lation is expressed in text. RESOLVER uses a prob-
abilistic model to predict if two strings refer to the
same item, based on relational features, in an unsu-
pervised manner. In Section 5.2 we report that RE-
SOLVER boosts the recall of O-CRF by 50%.
3.2 Relation-Specific Extraction
To compare the behavior of open, or ?unlexicalized,?
extraction to relation-specific, or ?lexicalized? ex-
traction, we developed a CRF-based extractor under
the traditional RE paradigm. We refer to this system
as R1-CRF.
Although the graphical structure of R1-CRF is the
same as O-CRF R1-CRF differs in a few ways. A
given relation R is specified a priori, and R1-CRF is
trained from hand-labeled positive and negative in-
stances of R. The extractor is also permitted to use
all lexical features, and is not restricted to closed-
class words as is O-CRF. Since R is known in ad-
vance, if R1-CRF outputs a tuple at extraction time,
the tuple is believed to be an instance of R.
4 Hybrid Relation Extraction
Since O-CRF and R1-CRF have complementary
views of the extraction process, it is natural to won-
der whether they can be combined to produce a
more powerful extractor. In many machine learn-
ing settings, the use of an ensemble of diverse clas-
sifiers during prediction has been observed to yield
higher levels of performance compared to individ-
ual algorithms. We now describe an ensemble-based
or hybrid approach to RE that leverages the differ-
ent views offered by open, self-supervised extraction
in O-CRF, and lexicalized, supervised extraction in
R1-CRF.
4.1 Stacking
Stacked generalization, or stacking, (Wolpert,
1992), is an ensemble-based framework in which the
goal is learn a meta-classifier from the output of sev-
eral base-level classifiers. The training set used to
train the meta-classifier is generated using a leave-
one-out procedure: for each base-level algorithm, a
classifier is trained from all but one training example
and then used to generate a prediction for the left-
out example. The meta-classifier is trained using the
predictions of the base-level classifiers as features,
and the true label as given by the training data.
Previous studies (Ting and Witten, 1999; Zenko
and Dzeroski, 2002; Sigletos et al, 2005) have
shown that the probabilities of each class value as
estimated by each base-level algorithm are effective
features when training meta-learners. Stacking was
shown to be consistently more effective than voting,
another popular ensemble-based method in which
the outputs of the base-classifiers are combined ei-
ther through majority vote or by taking the class
value with the highest average probability.
4.2 Stacked Relation Extraction
We used the stacking methodology to build an
ensemble-based extractor, referred to as H-CRF.
Treating the output of an O-CRF and R1-CRF as
black boxes, H-CRF learns to predict which, if any,
tokens found between a pair of entities (e1, e2), in-
dicates a relationship. Due to the sequential nature
of our RE task, H-CRF employs a CRF as the meta-
learner, as opposed to a decision tree or regression-
based classifier.
H-CRF uses the probability distribution over the
set of possible labels according to each O-CRF and
R1-CRF as features. To obtain the probability at
each position of a linear-chain CRF, the constrained
forward-backward technique described in (Culotta
andMcCallum, 2004) is used. H-CRF also computes
the Monge Elkan distance (Monge and Elkan, 1996)
between the relations predicted by O-CRF and R1-
CRF and includes the result in the feature set. An
additional meta-feature utilized by H-CRF indicates
whether either or both base extractors return ?no re-
lation? for a given pair of entities. In addition to
these numeric features, H-CRF uses a subset of the
base features used by O-CRF and R1-CRF. At each
32
O-CRF O-NB
Category P R F1 P R F1
Verb 93.9 65.1 76.9 100 38.6 55.7
Noun+Prep 89.1 36.0 51.3 100 9.7 55.7
Verb+Prep 95.2 50.0 65.6 95.2 25.3 40.0
Infinitive 95.7 46.8 62.9 100 25.5 40.6
Other 0 0 0 0 0 0
All 88.3 45.2 59.8 86.6 23.2 36.6
Table 2: Open Extraction by Relation Category. O-CRF
outperforms O-NB, obtaining nearly double its recall and
increased precision. O-CRF?s gains are partly due to its
lower false positive rate for relationships categorized as
?Other.?
given position i between e1 and e2, the presence of
the word observed at i as a feature, as well as the
presence of the part-of-speech-tag at i.
5 Experimental Results
The following experiments demonstrate the benefits
of Open IE for two tasks: open extraction and tar-
geted extraction.
Section 5.1, assesses the ability of O-CRF to lo-
cate instances of relationships when the number of
relationships is large and their identity is unknown.
We show that without any relation-specific input, O-
CRF extracts binary relationships with high precision
and a recall that nearly doubles that of O-NB.
Sections 5.2 and 5.3 compare O-CRF to tradi-
tional and hybrid RE when the goal is to locate in-
stances of a small set of known target relations. We
find that while single-relation extraction, as embod-
ied by R1-CRF, achieves comparatively higher lev-
els of recall, it takes hundreds, and sometimes thou-
sands, of labeled examples per relation, for R1-
CRF to approach the precision obtained by O-CRF,
which is self-trained without any relation-specific
input. We also show that the combination of unlex-
icalized, open extraction in O-CRF and lexicalized,
supervised extraction in R1-CRF improves precision
and F-measure compared to a standalone RE system.
5.1 Open Extraction
This section contrasts the performance of O-CRF
with that of O-NB on an Open IE task, and shows
that O-CRF achieves both double the recall and in-
creased precision relative to O-NB. For this exper-
iment, we used the set of 500 sentences3 described
in Section 2. Both IE systems were designed and
trained prior to the examination of the sample sen-
tences; thus the results on this sentence sample pro-
vide a fair measurement of their performance.
While the TEXTRUNNER system was previously
found to extract over 7.5 million tuples from a cor-
pus of 9 million Web pages, these experiments are
the first to assess its true recall over a known set of
relational tuples. As reported in Table 2, O-CRF ex-
tracts relational tuples with a precision of 88.3% and
a recall of 45.2%. O-CRF achieves a relative gain
in F1 of 63.4% over the O-NB model employed by
TEXTRUNNER, which obtains a precision of 86.6%
and a recall of 23.2%. The recall of O-CRF nearly
doubles that of O-NB.
O-CRF is able to extract instances of the four
most frequently observed relation types ? Verb,
Noun+Prep, Verb+Prep and Infinitive. Three of the
four remaining types ? Modifier, Coordinaten and
Coordinatev ? which comprise only 8% of the sam-
ple, are not handled due to simplifying assumptions
made by both O-CRF and O-NB that tokens indicat-
ing a relation occur between entity mentions in the
sentence.
5.2 O-CRF vs. R1-CRF Extraction
To compare performance of the extractors when a
small set of target relationships is known in ad-
vance, we used labeled data for four different re-
lations ? corporate acquisitions, birthplaces, inven-
tors of products and award winners. The first two
datasets were collected from the Web, and made
available by Bunescu and Mooney (2007). To aug-
ment the size of our corpus, we used the same tech-
nique to collect data for two additional relations, and
manually labelled positive and negative instances by
hand over all collections. For each of the four re-
lations in our collection, we trained R1-CRF from
labeled training data, and ran each of R1-CRF and
O-CRF over the respective test sets, and compared
the precision and recall of all tuples output by each
system.
Table 3 shows that from the start, O-CRF achieves
a high level of precision ? 75.0% ? without any
3Available at http://www.cs.washington.edu/research/
knowitall/hlt-naacl08-data.txt
33
O-CRF R1-CRF
Relation P R P R Train Ex
Acquisition 75.6 19.5 67.6 69.2 3042
Birthplace 90.6 31.1 92.3 64.4 1853
InventorOf 88.0 17.5 81.3 50.8 682
WonAward 62.5 15.3 73.6 52.8 354
All 75.0 18.4 73.9 58.4 5930
Table 3: Precision (P) and Recall (R) of O-CRF and R1-
CRF.
O-CRF R1-CRF
Relation P R P R Train Ex
Acquisition 75.6 19.5 67.6 69.2 3042?
Birthplace 90.6 31.1 92.3 53.3 600
InventorOf 88.0 17.5 81.3 50.8 682?
WonAward 62.5 15.3 65.4 61.1 50
All 75.0 18.4 70.17 60.7 >4374
Table 4: For 4 relations, a minimum of 4374 hand-tagged
examples is needed for R1-CRF to approximately match
the precision of O-CRF for each relation. A ??? indicates
the use of all available training data; in these cases, R1-
CRF was unable to match the precision of O-CRF.
relation-specific data. Using labeled training data,
the R1-CRF system achieves a slightly lower preci-
sion of 73.9%.
Exactly how many training examples per relation
does it take R1-CRF to achieve a comparable level
of precision? We varied the number of training ex-
amples given to R1-CRF, and found that in 3 out of
4 cases it takes hundreds, if not thousands of labeled
examples for R1-CRF to achieve acceptable levels
of precision. In two cases ? acquisitions and inven-
tions ? R1-CRF is unable to match the precision of
O-CRF, even with many labeled examples. Table 4
summarizes these findings.
Using labeled data, R1-CRF obtains a recall of
58.4%, compared to O-CRF, whose recall is 18.4%.
A large number of false negatives on the part of O-
CRF can be attributed to its lack of lexical features,
which are often crucial when part-of-speech tagging
errors are present. For instance, in the sentence, ?Ya-
hoo To Acquire Inktomi?, ?Acquire? is mistaken for
a proper noun, and sufficient evidence of the exis-
tence of a relationship is absent. The lexicalized R1-
CRF extractor is able to recover from this error; the
presence of the word ?Acquire? is enough to recog-
R1-CRF Hybrid
Relation P R F1 P R F1
Acquisition 67.6 69.2 68.4 76.0 67.5 71.5
Birthplace 93.6 64.4 76.3 96.5 62.2 75.6
InventorOf 81.3 50.8 62.5 87.5 52.5 65.6
WonAward 73.6 52.8 61.5 75.0 50.0 60.0
All 73.9 58.4 65.2 79.2 56.9 66.2
Table 5: A hybrid extractor that uses O-CRF improves
precision for all relations, at a small cost to recall.
nize the positive instance, despite the incorrect part-
of-speech tag.
Another source of recall issues facing O-CRF is
its ability to discover synonyms for a given relation.
We found that while RESOLVER improves the rela-
tive recall of O-CRF by nearly 50%, O-CRF locates
fewer synonyms per relation compared to its lexical-
ized counterpart. With RESOLVER, O-CRF finds an
average of 6.5 synonyms per relation compared to
R1-CRF?s 16.25.
In light of our findings, the relative tradeoffs of
open versus traditional RE are as follows. Open IE
automatically offers a high level of precision without
requiring manual labor per relation, at the expense
of recall. When relationships in a corpus are not
known, or their number is massive, Open IE is es-
sential for RE.When higher levels of recall are desir-
able for a small set of target relations, traditional RE
is more appropriate. However, in this case, one must
be willing to undertake the cost of acquiring labeled
training data for each relation, either via a computa-
tional procedure such as bootstrapped learning or by
the use of human annotators.
5.3 Hybrid Extraction
In this section, we explore the performance of H-
CRF, an ensemble-based extractor that learns to per-
form RE for a set of known relations based on the
individual behaviors of O-CRF and R1-CRF.
As shown in Table 5, the use of O-CRF as part
of H-CRF, improves precision from 73.9% to 79.2%
with only a slight decrease in recall. Overall, F1
improved from 65.2% to 66.2%.
One disadvantage of a stacking-based hybrid sys-
tem is that labeled training data is still required. In
the future, we would like to explore the development
of hybrid systems that leverage Open IE methods,
34
like O-CRF, to reduce the number of training exam-
ples required per relation.
6 Related Work
TEXTRUNNER, the first Open IE system, is part
of a body of work that reflects a growing inter-
est in avoiding relation-specificity during extrac-
tion. Sekine (2006) developed a paradigm for ?on-
demand information extraction? in order to reduce
the amount of effort involved when porting IE sys-
tems to new domains. Shinyama and Sekine?s ?pre-
emptive? IE system (2006) discovers relationships
from sets of related news articles.
Until recently, most work in RE has been carried
out on a per-relation basis. Typically, RE is framed
as a binary classification problem: Given a sentence
S and a relation R, does S assert R between two
entities in S? Representative approaches include
(Zelenko et al, 2003) and (Bunescu and Mooney,
2005), which use support-vector machines fitted
with language-oriented kernels to classify pairs of
entities. Roth and Yih (2004) also described a
classification-based framework in which they jointly
learn to identify named entities and relations.
Culotta et al (2006) used a CRF for RE, yet
their task differs greatly from open extraction. RE
was performed from biographical text in which the
topic of each document was known. For every en-
tity found in the document, their goal was to pre-
dict what relation, if any, it had relative to the page
topic, from a set of given relations. Under these re-
strictions, RE became an instance of entity labeling,
where the label assigned to an entity (e.g. Father) is
its relation to the topic of the article.
Others have also found the stacking framework to
yield benefits for IE. Freitag (2000) used linear re-
gression to model the relationship between the con-
fidence of several inductive learning algorithms and
the probability that a prediction is correct. Over
three different document collections, the combined
method yielded improvements over the best individ-
ual learner for all but one relation. The efficacy of
ensemble-based methods for extraction was further
investigated by (Sigletos et al, 2005), who experi-
mented with combining the outputs of a rule-based
learner, a Hidden Markov Model and a wrapper-
induction algorithm in five different domains. Of a
variety ensemble-based methods, stacking proved to
consistently outperform the best base-level system,
obtaining more precise results at the cost of some-
what lower recall. (Feldman et al, 2005) demon-
strated that a hybrid extractor composed of a statis-
tical and knowledge-based models outperform either
in isolation.
7 Conclusions and Future Work
Our experiments have demonstrated the promise of
relation-independent extraction using the Open IE
paradigm. We have shown that binary relationships
can be categorized using a compact set of lexico-
syntactic patterns, and presented O-CRF, a CRF-
based Open IE system that can extract different re-
lationships with a precision of 88.3% and a recall of
45.2%4. Open IE is essential when the number of
relationships of interest is massive or unknown.
Traditional IE is more appropriate for targeted ex-
traction when the number of relations of interest is
small and one is willing to incur the cost of acquir-
ing labeled training data. Compared to traditional
IE, the recall of our Open IE system is admittedly
lower. However, in a targeted extraction scenario,
Open IE can still be used to reduce the number of
hand-labeled examples. As Table 4 shows, numer-
ous hand-labeled examples (ranging from 50 for one
relation to over 3,000 for another) are necessary to
match the precision of O-CRF.
In the future, O-CRF?s recall may be improved
by enhancements to its ability to locate the various
ways in which a given relation is expressed. We also
plan to explore the capacity of Open IE to automati-
cally provide labeled training data, when traditional
relation extraction is a more appropriate choice.
Acknowledgments
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, ONR grant N00014-
08-1-0431 as well as gifts from Google, and carried
out at the University of Washington?s Turing Center.
Doug Downey, Stephen Soderland and Dan Weld
provided helpful comments on previous drafts.
4The TEXTRUNNER Open IE system now indexes extrac-
tions found by O-CRF from millions of Web pages, and is lo-
cated at http://www.cs.washington.edu/research/textrunner
35
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In
Procs. of the Fifth ACM International Conference on
Digital Libraries.
M. Banko, M. Cararella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Procs. of IJCAI.
S. Brin. 1998. Extracting Patterns and Relations from the
WorldWideWeb. InWebDBWorkshop at 6th Interna-
tional Conference on Extending Database Technology,
EDBT?98, pages 172?183, Valencia, Spain.
R. Bunescu and R. Mooney. 2005. Subsequence kernels
for relation extraction. In In Procs. of Neural Informa-
tion Processing Systems.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Proc. of ACL.
A. Culotta and A. McCallum. 2004. Confidence es-
timation for information extraction. In Procs of
HLT/NAACL.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrat-
ing probabilistic extraction models and data mining
to discover relations and patterns in text. In Procs of
HLT/NAACL, pages 296?303.
P. Domingos. 1996. Unifying instance-based and rule-
based induction. Machine Learning, 24(2):141?168.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
R. Feldman, B. Rosenfeld, and M. Fresko. 2005. Teg - a
hybrid approach to information extraction. Knowledge
and Information Systems, 9(1):1?18.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine Learning,
39(2-3):169?202.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1).
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Procs. of
ICML.
A. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
A. E. Monge and C. P. Elkan. 1996. The field matching
problem: Algorithms and applications. In Procs. of
KDD.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-level Boot-strapping.
In Procs. of AAAI-99, pages 1044?1049.
D. Roth and W. Yih. 2004. A linear progamming formu-
lation for global inference in natural language tasks.
In Procs. of CoNLL.
S. Sekine. 2006. On-demand information extraction. In
Proc. of COLING.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Proc. of the HLT-NAACL.
G. Sigletos, G. Paliouras, C. D. Spyropoulos, andM. Hat-
zopoulos. 2005. Combining infomation extraction
systems using voting and stacked generalization. Jour-
nal of Machine Learning Research, 6:1751,1782.
R. Snow, D. Jurafsky, and A. Ng. 2005. Learning syn-
tactic patterns for automatic hypernym discovery. In
Advances in Neural Information Processing Systems
17. MIT Press.
K.M. Ting and I. H. Witten. 1999. Issues in stacked gen-
eralization. Artificial Intelligence Research, 10:271?
289.
D. Wolpert. 1992. Stacked generalization. Neural Net-
works, 5(2):241?260.
A. Yates and O. Etzioni. 2007. Unsupervised resolu-
tion of objects and relations on the web. In Procs of
NAACL/HLT.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. JMLR, 3:1083?1106.
B. Zenko and S. Dzeroski. 2002. Stacking with an ex-
tended set of meta-level attributes and mlr. In Proc. of
ECML.
36
Headline Generation Based on Statistical Translation
Michele Banko
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218
banko@cs.jhu.edu
Vibhu O. Mittal
Just Research
4616 Henry Street
Pittsburgh, PA 15213
mittal@justresearch.com
Michael J. Witbrock
Lycos Inc.
400-2 Totten Pond Road
Waltham, MA 023451
mwitbrock@lycos.com
Abstract
Extractive summarization techniques
cannot generate document summaries
shorter than a single sentence, some-
thing that is often required. An ideal
summarization system would under-
stand each document and generate an
appropriate summary directly from the
results of that understanding. A more
practical approach to this problem re-
sults in the use of an approximation:
viewing summarization as a problem
analogous to statistical machine trans-
lation. The issue then becomes one of
generating a target document in a more
concise language from a source docu-
ment in a more verbose language. This
paper presents results on experiments
using this approach, in which statisti-
cal models of the term selection and
term ordering are jointly applied to pro-
duce summaries in a style learned from
a training corpus.
1 Introduction
Generating effective summaries requires the abil-
ity to select, evaluate, order and aggregate items
of information according to their relevance to
a particular subject or for a particular purpose.
Most previous work on summarization has fo-
cused on extractive summarization: selecting text
spans - either complete sentences or paragraphs
? from the original document. These extracts are
Vibhu Mittal is now at Xerox PARC, 3333 Coyote
Hill Road, Palo Alto, CA 94304, USA. e-mail: vmit-
tal@parc.xerox.com; Michael Witbrock?s initial work on
this system was performed whilst at Just Research.
then arranged in a linear order (usually the same
order as in the original document) to form a sum-
mary document. There are several possible draw-
backs to this approach, one of which is the fo-
cus of this paper: the inability to generate co-
herent summaries shorter than the smallest text-
spans being considered ? usually a sentence, and
sometimes a paragraph. This can be a problem,
because in many situations, a short headline style
indicative summary is desired. Since, in many
cases, the most important information in the doc-
ument is scattered across multiple sentences, this
is a problem for extractive summarization; worse,
sentences ranked best for summary selection of-
ten tend to be even longer than the average sen-
tence in the document.
This paper describes an alternative approach to
summarization capable of generating summaries
shorter than a sentence, some examples of which
are given in Figure 1. It does so by building sta-
tistical models for content selection and surface
realization. This paper reviews the framework,
discusses some of the pros and cons of this ap-
proach using examples from our corpus of news
wire stories, and presents an initial evaluation.
2 Related Work
Most previous work on summarization focused
on extractive methods, investigating issues such
as cue phrases (Luhn, 1958), positional indi-
cators (Edmundson, 1964), lexical occurrence
statistics (Mathis et al, 1973), probabilistic mea-
sures for token salience (Salton et al, 1997), and
the use of implicit discourse structure (Marcu,
1997). Work on combining an information ex-
traction phase followed by generation has also
been reported: for instance, the FRUMP sys-
tem (DeJong, 1982) used templates for both in-
1: time -3.76 Beam 40
2: new customers -4.41 Beam 81
3: dell computer products -5.30 Beam 88
4: new power macs strategy -6.04 Beam 90
5: apple to sell macintosh users -8.20 Beam 86
6: new power macs strategy on internet -9.35 Beam 88
7: apple to sell power macs distribution strategy -10.32 Beam 89
8: new power macs distribution strategy on internet products -11.81 Beam 88
9: apple to sell power macs distribution strategy on internet -13.09 Beam 86
Figure 1: Sample output from the system for a variety of target summary lengths from a single
input document.
formation extraction and presentation. More
recently, summarizers using sophisticated post-
extraction strategies, such as revision (McKeown
et al, 1999; Jing and McKeown, 1999; Mani et
al., 1999), and sophisticated grammar-based gen-
eration (Radev and McKeown, 1998) have also
been presented.
The work reported in this paper is most closely
related to work on statistical machine transla-
tion, particularly the ?IBM-style? work on CAN-
DIDE (Brown et al, 1993). This approach
was based on a statistical translation model that
mapped between sets of words in a source lan-
guage and sets of words in a target language, at
the same time using an ordering model to con-
strain possible token sequences in a target lan-
guage based on likelihood. In a similar vein,
a summarizer can be considered to be ?translat-
ing? between two languages: one verbose and the
other succinct (Berger and Lafferty, 1999; Wit-
brock and Mittal, 1999). However, by definition,
the translation during summarization is lossy, and
consequently, somewhat easier to design and ex-
periment with. As we will discuss in this paper,
we built several models of varying complexity;1
even the simplest one did reasonably well at sum-
marization, whereas it would have been severely
deficient at (traditional) translation.
1We have very recently become aware of related work
that builds upon more complex, structured models ? syn-
tax trees ? to compress single sentences (Knight and Marcu,
2000); our work differs from that work in (i) the level of
compression possible (much more) and, (ii) accuracy possi-
ble (less).
3 The System
As in any language generation task, summariza-
tion can be conceptually modeled as consisting
of two major sub-tasks: (1) content selection, and
(2) surface realization. Parameters for statistical
models of both of these tasks were estimated from
a training corpus of approximately 25,000 1997
Reuters news-wire articles on politics, technol-
ogy, health, sports and business. The target docu-
ments ? the summaries ? that the system needed
to learn the translation mapping to, were the head-
lines accompanying the news stories.
The documents were preprocessed before
training: formatting and mark-up information,
such as font changes and SGML/HTML tags, was
removed; punctuation, except apostrophes, was
also removed. Apart from these two steps, no
other normalization was performed. It is likely
that further processing, such as lemmatization,
might be useful, producing smaller and better lan-
guage models, but this was not evaluated for this
paper.
3.1 Content Selection
Content selection requires that the system learn a
model of the relationship between the appearance
of some features in a document and the appear-
ance of corresponding features in the summary.
This can be modeled by estimating the likelihood
of some token appearing in a summary given that
some tokens (one or more, possibly different to-
kens) appeared in the document to be summa-
rized. The very simplest, ?zero-level? model for
this relationship is the case when the two tokens
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 2 4 6 8 10 12
Pr
op
or
tio
n 
of
 d
oc
um
en
ts
Length in words
Summary lengths
headlines
Figure 2: Distribution of Headline Lengths for
early 1997 Reuters News Stories.
in the document and the summary are identical.
This can be computed as the conditional proba-
bility of a word occurring in the summary given
that the word appeared in the document:
 
	

 	 