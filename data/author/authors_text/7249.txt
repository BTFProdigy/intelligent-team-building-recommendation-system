Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 716?723, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Context and Learning in Novelty Detection
Barry Schiffman and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, N.Y.
{bschiff,kathy}@cs.columbia.edu
Abstract
We demonstrate the value of using con-
text in a new-information detection sys-
tem that achieved the highest precision
scores at the Text Retrieval Conference?s
Novelty Track in 2004. In order to de-
termine whether information within a sen-
tence has been seen in material read pre-
viously, our system integrates information
about the context of the sentence with
novel words and named entities within the
sentence, and uses a specialized learning
algorithm to tune the system parameters.
1 Introduction
New-information detection addresses two important
problems in a society awash in more digital infor-
mation than people can exploit. A novelty detection
system could help people who are tracking an event
in the news, where numerous sources present simi-
lar material. It could also provide a way to organize
summaries by focusing on the most recent informa-
tion, much like an automated bulletin service.
We envision that many types of users would find
such a system valuable. Certainly analysts, busi-
ness people, and anyone interested in current events,
would benefit from being able to track news stories
automatically, without repetition. Different news or-
ganizations report on the same event, often working
hard to make their reports look different from one
another, whether or not they have new material to
report. Our system would help readers to zero in
on new information. In addition, a focus on new
information provides a way of organizing a general
summary.
Our approach is unique in representing and main-
taining the focus in discourse. The idea stems from
the fact that novelty often comes in bursts, which
is not surprising since the articles are composed of
some number of smaller, coherent segments. Each
segment is started by some kind of introductory pas-
sage, and that is where we expect to find the novel
words. Novel words are identified by comparing
the current sentence?s words against a table of all
words seen in the inputs to that point. They let us
know whether the entire segment is likely to con-
tain more novel material. Subsequent passages are
likely to continue the novel discussion whether or
not they contain novel words. They may contain
pronomial references or other anaphoric references
to the novel entity. Our long-term goal is to inte-
grate the approach described in this paper into our
larger new-information detector, a system that per-
forms a more complicated syntactic analysis of the
input texts and employs machine learning to classify
passages as new or old.
Meanwhile, we tested our focus-based approach
at the Novelty Track at the Text Retrieval Confer-
ence (TREC) in 2004. The Novelty Tracks in 2003
and in 2004 were divided into four tasks; Task 1
and Task 3 incorporate retrieval, requiring submis-
sions to locate the relevant sentences before filter-
ing them for novelty. Tasks 2 and 4 are novelty de-
tection alone, using the relevant sentences selected
by humans as input. Since our interest is in nov-
716
elty detection, we chose to concentrate on Task 21
Our TREC submission was also designed to test a
specialized learning mechanism we implemented to
target either high precision or high recall.
In all, the problem of novelty detection is decep-
tively difficult. We were struck by the difficulty that
all groups in the Novelty Track in 2002 and 2003
had in obtaining high precision scores. Submissions
that classify a very large proportion of the input
sentences as novel reached the highest F-measure
scores by getting high recall scores, but failed to
achieve any substantial compression of material for
users. Given that our goal is to generate an up-
date summary, we focused on improving precision
and increasing compression, removing as many false
positives as possible.
The next section discusses the Novelty Track and
the approaches others have tried; Section 3 details
our system, and Section 4 presents the experiments.
2 Novelty Track
Much of the work in new-information detection has
been done for the TREC Novelty Track. The task
is related to first story detection, which is defined
on whole documents rather than on passages within
documents. In Task 1 of the Novelty Track, a system
is given about 25 documents on a topic and asked to
find all sentences relevant to the topic. In Task 2,
the inputs are the set of relevant sentences, so that
the program does not see the entire documents. The
program must scan the sentences in order and output
all that contain new information, that is information
not seen in the previous input sentences.
2.1 Related Work
At the recent TREC, Dublin City University did well
by comparing the words in a sentence against the
accumulated words in all previous sentences (Blott
et al, 2004). Their runs varied the way in which
the words were weighted with frequency and inverse
document frequency. Like our system, theirs follows
from the intuition that words that are new to a dis-
cussion are evidence of novelty. But our system dis-
1Task 4 was similar to Task 2, in that both have the human
annotations as input. For Task 2, participants only get the anno-
tations, but in Task 4, they also receive the novel sentences from
the first five documents as input. We felt that we would learn as
much from the one task as from both.
tinguishes between several kinds of words, includ-
ing common nouns, named persons, named organi-
zation, etc. Our system also incorporates a mecha-
nism for looking at the context of the sentence.
Both the Dublin system and ours are preceded by
the University of Iowa?s approach at TREC 2003. It
based novelty decisions on a straightforward count
of new named entities and noun phrases in a sen-
tence (Eichmann et al, 2003). In 2004, the Iowa sys-
tem (Eichmann et al, 2004) tried several embellish-
ments, one using synonyms in addition to the words
for novelty comparisons, and one using word-sense
disambiguation. These two runs were above average
in F-measure and about average in precision.
The University of Massachusetts system (Abdul-
Jaleel et al, 2004) mixed a vector-space model with
cosine similarity and a count of previously unseen
named entities. Their system resembled one of two
baseline methods that we submitted without our fo-
cus feature. Their submission used a similarity
threshold that was tuned experimentally, while ours
was learned automatically. In earlier work with the
TREC 2002 data, UMass (Allan et al, 2003) com-
pared a number of sentence-based models ranging
in complexity from a count of new words and cosine
distance, to a variety of sophisticated models based
on KL divergence with different smoothing strate-
gies and a ?core mixture model? that considered the
distribution of the words in the sentence with the
distributions in a topic model and a general English
model.
A number of groups have experimented with
matrix-based methods. In 2003, a group from the
University of Maryland and the Center for Com-
puting Sciences (Conroy et al, 2003) used three
techniques that used QR decomposition and sin-
gular value decomposition. The University of
Maryland, Baltimore County, worked with cluster-
ing algorithms and singular value decomposition
in sentence-sentence similarity matrices (Kallurkar
et al, 2003). In 2004, Conroy (Conroy, 2004)
tested Maximal Marginal Relevance (Goldstein et
al., 2000) as well as QR decomposition.
The information retrieval group at Tsinghua Uni-
versity used a pooling technique, grouping similar
sentences into clusters in order to capture sentences
that partially match two or more other sentences(Ru
et al, 2004). They said they had found difficulties
717
with sentence-by-sentence comparisons.
2.2 Precision
At all three Novelty Track evaluations, from 2002 to
2004, it is clear that high precision is much harder
to obtain than high recall. Trivial baselines ? such
as accept all sentences as novel ? have proved to be
difficult to beat by very much. This one-line algo-
rithm automatically obtains 100% recall and preci-
sion equal to the proportion of novel sentences in
the input. In 2003, when 66% of the relevant sen-
tences were novel, the mean precision score was
0.6352 and the median was 0.7. In 2004, 41% of the
relevant sentences were novel, and the average pre-
cision dropped to 0.46. The median precision was
also 0.46. Meanwhile, average recall scores across
all submissions actually rose to 0.861 in 2004, com-
pared with 0.795 in 2003. In terms of a real world
system, this means that as the number of target sen-
tences shrank, the number of sentences in the aver-
age program output rose. Likewise, a trivial system
could guarantee no errors by returning nothing, but
this would have no value.
2.3 Sentences
Normally, in Information Retrieval tasks, stricter
thresholds result in higher precision, and looser
thresholds, higher recall. In that way, a system can
target its results to a user?s needs. But in new-
information detection, this rule of thumb fails at
some point as thresholds become stricter. Recall
does fall, but precision does not rise. In other words,
there seems to be a ceiling for precision.
Several participants noted that their simpler
strategies produced the best results. For example,
in 2003, the Chinese Academy of Sciences (Sun et
al., 2003), noted that word overlap was surprisingly
strong as a similarity measure. As we have seen
above, the Iowa approach of counting nouns was in-
corporated by a few others for 2004, including us.
This strategy compares words in a sentence against
all previous seen words and thus, avoids comput-
ing pairwise similarity between all sentences. Al-
2One group appeared to have submitted a large number of ir-
relevant sentences in its submission, since it obtained relatively
high recall scores, but very low precision scores, causing the
average to drop below 0.66. The average precision of all other
groups is about 0.7.
most all participants performed such pairwise com-
parisons of systems.
A sentence-by-sentence comparison is clearly not
the optimal operation for establishing novelty. Sen-
tences with a large amount of overlap can express
very different thoughts. In the extreme, a single
word change can reverse the meaning of two sen-
tences: accept and reject. This phenomenon led the
Tsinghua University group to remark, ?many sen-
tences with an overlap of nearly 1 are real novel
ones.? (Ru et al, 2004).
On the other hand, it?s not hard to find cases where
realizations of equivalent statements take many dif-
ferent surface forms ? with different choices of
words and different syntactic structures. The data in
the Novelty Task is drawn from three news services
and clustered into fairly cohesive sets. The news
writers consciously try to avoid echoing each other,
and over time, echoing themselves. Sentences such
as these have low word overlap, but are not novel.
For this reason, we turned to a strategy of classifying
each sentence Si against the cumulative background
of all the words in all preceding sentences S1...i?1.
3 System
The system described in this paper was built with the
Novelty Track in mind. The goal was to look at ways
to consider longer spans of text than a sentence, and
to avoid sentence by sentence comparisons.
In the Novelty track, the relevant sentences are
presented in natural order, i.e. by the date of the
document they came from, and then by their loca-
tion in the document. The key characteristics of our
program include:
? For each relevant sentence, our program cal-
culates a sum of novel terms, which are terms
that have not been previously seen. The terms
are weighted according to their category, like
person, location, common noun or verb. The
weights are learned automatically.
? For the entire set, the program maintains a fo-
cus variable, which indicates whether the pre-
vious sentence is novel or old. Thresholds de-
termine whether to continue or shift the focus.
These are also learned automatically.
718
All input documents are fed in parallel into a
named-entity recognizer, which marks persons, or-
ganizations, locations, part-of-speech tags for com-
mon nouns, and into a finite-state parser, which is
used only to identify sentences beginning with sub-
ject pronouns. The output from the two preprocess-
ing modules are merged and sent to the classifier.
The classifier reads a configuration file that con-
tains a set of weights that were learned on the 2003
Novelty Track data to apply to different classes of
words that have not been previously seen.
For each sentence, the system adds up the amount
of novelty from the weighted terms in a sentence and
compares that to a learned threshold; it classifies the
sentence as novel if it exceeds the threshold. It also
stores the classification in a focus variable. If the
novelty threshold is not met, the system performs a
series of tests described below, and possibly classi-
fies some sentences with few content words as novel,
depending on the status of the focus variable. Our
algorithm enumerates all cases of changes in focus,
and tests these in the order that allows the system
to make the decision it can be most confident about
first. Thus, when we find a named entity new to the
discussion, we can be pretty sure that we have found
a novel sentence. We can classify that sentence as
new without regard to what preceded it. But, when
we find a sentence devoid of high-content words,
like ?She said the idea sounded good,? the system
uses the classification of the previous sentence. If
the antecedents to she or idea are novel, then this
sentence must also be novel. The series of learned
thresholds are imposed in a cascade to maximize the
number of correct decisions over the training cases,
in hopes the values will also cover unseen cases.
Thus, the classifier puts each sentence through the
tests below, using the learned thresholds and weights
described in Section 3.1. If any test succeeds, the
system goes on to the next sentence.
1. If there is a sufficient concentration of novel
words, classify the sentence as novel A suffi-
cient concentration occurs when the sum of the
weights of the novel content words (including
named entities) exceeds a threshold, Tnovel. If
the previous focus was old, this indicates the
focus has shifted to a novel segment.
2. If there is a lack of novel words, classify the
sentence as old This is computed by compar-
ing the sum of the weights of the already-seen
content words to a separate threshold, Told. If
the previous focus was novel, this means the
focus has shifted to an old segment.
3. For any remaining sentences, the classification
is based on context:
(a) If the sentence does not have a sufficient
number of content words, use the classifi-
cation in the focus variable. This adds the
sums of both new and old content words
and compares that to a threshold, Tkeep.
(b) If the first noun phrase is a third person
personal pronoun, use the classification in
the focus variable. Pronouns are known
to signal that the same focus continues
(Grosz and Sidner, 1986).
(c) If the sentence has not met any of the
above tests but has a minimum number of
content words, shift the focus. If all tests
above fail and there are a minimum num-
ber of content words, with a sum of Tshift
shift the focus.
4. Default This rarely occurs but the default is to
continue the focus, whether novel or old.
We examined the 2003 Novelty Track data and
found that more than half the novel sentences ap-
pear in sequences of consecutive sentences (See Ta-
ble 1). This circumstance creates an opportunity to
make principled classifications on some sentences
that have few, if any, clearly novel words, but con-
tinue a new segment. The use of a focus variable
handles these cases.
3.1 Learning
In all, the system uses 11 real-valued parameters,
weights and thresholds, and we wanted to learn op-
timal values for these. In particular, we wanted to be
able to target either high recall or high precision, As
we noted above, precision was much more difficult,
and for a summarization task, much more important.
To learn the optimal values for the parameters, we
opted to use an ad hoc algorithm. The main advan-
tage in doing so was when considering instance i,
the program can reference the classifications made
719
Length of Run Count
1 1338
2 421
3 132
4 72
5 43
6 22
7 11
8 2
9 3
10 3
11 2
12 2
15 2
17 1
Table 1: Novelty often comes in bursts. This table
shows that 1,338 of the novel sentences in the 2003
evaluation were singletons, and not a part of a run of
novel sentences. Meanwhile, 1,526 of the sentences
were part of runs of 2, 3 or 4 sentences.
for instance i ? 1, i ? 2, and possibly all the way
back to instance 1, because the classification for in-
stance i partly depends on the classification of pre-
vious instances. Not only do many standard super-
vised learning methods assume conditional indepen-
dence, but they also do not provide access to the on-
line classifications during learning. We constructed
a randomized hill-climbing. The learner is struc-
tured like a neural net, but the weight adjustments
are chosen at random as they are in genetic algo-
rithms (See Figure 1). The evaluation, or fitness
function, is the Novelty Track score itself, and the
training data was the 2003 Novelty Track data.
Changes to the hypothesis are selected at random
and evaluated. If the change does not hurt results, it
is accepted. Otherwise the program backtracks and
chooses another weight to update. We required the
new configuration to produce a score greater than or
equal to the previous one before we accepted it. The
choice of which weight to update is made at ran-
dom, in an effort to avoid local minima in the search
space, but with an important restriction: the previous
n choices are kept in a history list, which is checked
to avoid re-use. This list is updated at each iteration.
The configurations usually converge well within 100
iterations.
1. Initialize weights, history
Weights take random values
2. Run the system using current weight set
3. If current score >= previous best
Update previous best
4. Otherwise
Undo move
5. Update history
6. Choose next weight to change
7. Go to step 2
Figure 1: The learning algorithm uses a randomized
hill climbing approach with backtracking
3.2 Bias Adjustment
In training on the 2003 data, the biggest problem
was to find a way to deal with the large percentage
of novel sentences. About 65% of the instances are
positive, so that a random system achieves a rela-
tively high F-measure by increasing the number of
sentences it calls novel ? until recall reaches 1.0.
Another strategy would be to choose only the sen-
tences in the first document, achieving a high pre-
cision ? more than 90% of the relevant sentences in
the first document for each topic were called novel.
In the Novelty Track the F-measure was set to
give equal weight to precision and recall, but we
wanted to be able to coax the learner to give greater
weight to either precision or by adjusting the F-
measure computation:
F = 1?
prec +
(1??)
recall
? is a number between 0 and 1. The closer it gets
to 1, the more the formula favors precision.
We chose whether to emphasize precision or re-
call by altering the value of ?. At the most extreme,
we set ? at 0.9 for the largest emphasis on precision.
When emphasizing recall, we left ? at 0.5.
The design was motivated by the need to explore
the problem more fully and inform the algorithm for
deciding novelty as much as to find optimal param-
eters for the values. Thus, we wanted to be able
to record all the steps the learner made through the
search space, and to save the intermediate states. At
times, the learner would settle into a configuration
720
that produced a trivial solution, and we could choose
one of the intermediate configurations that produced
a more reasonable score.
3.3 Vector-Space Module
In addition to the system which integrates novel
word features with focus tracking, we also imple-
mented a vector-space approach as a baseline ? the
Cosine run. We tested the vector-space system alone
to contrast it with the focus system, but we also
tested a version which integrated the vector-space
system with the focus system.
Our vector-space module assigns all non-stop-
words a value of 1, and uses the cosine distance met-
ric to compute similarity.
Cos(u, v) = u ? v?u? ? ?v?
and
Novel(si)
?
?
?
?
?
True if Cos(si, sj) < T,
for j = 1 . . . i ? 1
False otherwise
As each sentence is scanned, its similarity is com-
puted with all previous sentences and the maximum
similarity is compared to a threshold T . If that max-
imum exceeds T , it is considered novel. We chose
the value of T after trials on the 2003 Novelty Track
data. It was set at 0.385, resulting in a balanced sys-
tem that matched the results of one of the strongest
performers at the TREC evaluations that year.
On the 2003 data, when we set T at .9, we found
that we had a precision of .71 and a recall of 0.98,
indicating that about 6% of the sentences were quite
similar to some preceding sentence (See Figure 2).
After that, each point of precision was very costly in
terms of recall. Our experience was mirrored by the
participants at TREC 2003 and again at TREC 2004.
We considered this vector-space model to be our
baseline. We also tried it in combination with the
Recall run explained above. Because both the Re-
call and Cosine runs produced a relatively large out-
put and because they used different methods, we
thought the intersection would result in higher pre-
cision, though with some loss of recall.
In practice, the range of recall was much greater
than precision. Judging from the experiences of the
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
R
ec
al
l
Precision
Precision, recall tradeoff
Cos <= .10
Cos <= .20
Cos <= .30
Cos <= .40
Cos > .40
(Novelty = 1 - Cos)
Figure 2: The precision and recall scores of a
vector-space model with cosine similarity at differ-
ent thresholds, on the TREC 2003 data. Making the
test for novelty stricter fails to improve precision but
has a drastic effect on recall.
participants at TREC and our own exploratory ex-
periments, it was difficult to push precision above
0.80 with the TREC 2003 data, and above 0.50 with
the TREC 2004 data.
4 Experiments
4.1 Results from TREC 2004
Our results are encouraging, especially since the
configurations that were oriented toward higher pre-
cision, indeed, achieved the best precision scores
in the evaluation, with our best precision run about
20% higher in precision than the best of all the runs
by other groups (See Figure 3.) Meanwhile, our
recall-oriented run was one of eight runs that were in
a virtual tie for achieving the top f-measure. These
eight runs were within 0.01 of one another in the
measure.
Our five submitted runs were:
Prec1 aimed at moderately high precision, with rea-
sonable recall.
Prec2 aimed at high precision, with little attention
to recall.
Recall weighted precision and recall equally.
Cosine a baseline of a standard vector-space model
with a cosine similarity metric.
721
Figure 3: The graph shows all 54 submission in
Task 2 for the Novelty Track, with our five submis-
sions labeled. Our precision-oriented runs were well
ahead of all others in precision, while our recall-
oriented run was in a large group that reached about
0.5 precision with relatively high recall.
Combo a composite submission using the intersec-
tion of Recall and Cosine.
Table 2 shows the numbers of our performance of
our five submissions. Prec1 had an F-score close
to the average of 0.577 for all systems, while Prec2
was 50% ahead of random selection in accuracy.
Both our Combo system and our baseline Cosine
were above average in F-measure. Our emphasis on
precision is justified in a number of ways, although
the official yardstick was the F-measure.
An analysis of the system?s behavior under the
different parameters showed that the precision-
oriented runs, in particular Prec1, valued verbs and
common nouns more than named entities in decid-
ing novelty. The precision-oriented runs also bene-
fited more from the focus variable, with their scores
about 5% higher in terms of F-measure than they
were without it. The pronoun test, however, was
rarely used, firing less than 1% of the time.
We note that we are developing novelty detection
for summarization, where compression of the report
is valuable. Table 2 shows the lengths of our re-
turns. It is impossible to compare these precisely
with other systems, because the averages given by
NIST are averages of the scores for each of the 50
sets, and we do not have the breakdown of the num-
bers by set for any submissions but our own. How-
ever, we can estimate the size of the other output by
considering average precision and recall as if they
were computed over the total number of sentences in
all 50 sets. This computation shows an average out-
put for all participants of about 6,500 sentences and
a median of 6,981 ? out of a total of 8,343 sentences.
However, this total includes some amount of header
material, not only the headline, but the document ID
and other identifiers, the date and some shorthand
messages from the wire services to its clients. In
addition, a number of the sets had near perfect du-
plicate articles. This is in sharp contrast with typi-
cal summaries. At the 2004 Document Understand-
ing Conference, the typical input cluster contained
more than 4,000 words, and the task required that
this be reduced to 100 words. We contend there is
little value in a system that does no more than weed
out very few sentences, even though they might have
achieved high F-measures.
Second, our experience, and the results of other
groups, shows that high precision is harder than high
recall. In all three years of the Novelty Track, pre-
cision scores tended to hover in a narrow band just
above what one would get by mechanically labeling
all sentences as novel.
5 Conclusion
The success of our use of context in the TREC
Novelty Track led us to incorporate the idea into a
larger system. This system identifies clauses within
sentences that express new information and tries to
identify semantic equivalents. It is being developed
as part of a multi-document summarizer that pro-
duces topical updates for users.
In addition, the work here suggests three direc-
tions for future work:
? Adapt the features used here to some of the
newer probabilistic formalisms, like condi-
tional random fields.
? Try full segmentation of the input documents
rather than treat the sentences as a sequence.
? Try to identify all nominal references to canon-
ical forms.
With this experimental system, we obtained the
the top precision scores in the Novelty Track, and
722
Run-Id Precision Recall F-meas Output length
Prec1 0.57 0.58 0.562 3276
Prec2 0.61 0.45 0.506 2372
Recall 0.51 0.82 0.611 5603
Cosine 0.49 0.81 0.599 5537
Combo 0.53 0.73 0.598 4578
Choose All 0.41 1.000 0.581 8343
Average All Runs 0.46 0.86 0.577 6500
Table 2: Comparison of results of our five runs, compared to a random selection of sentences, and the overall
average F-scores by all 55 submissions.
we obtained the program settings to do this auto-
matically. High precision is very difficult to obtain,
and every point in precision costs too much in recall.
Further exploration is needed to determine whether
linguistic knowledge will help, and whether state-
of-the-art tools are powerful enough to improve per-
formance.
Beyond new-information detection, the idea of
tracking context with a surface means like the focus
variable is worth exploring in other tasks, including
summarization and question-answering.
References
Nasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fer-
nando Diaz, Leah Larkey, Xiaoyan Li, Donald Met-
zler, Mark D. Smucker, Trevor Strohman, Howard Tur-
tle, and Courtney Wade. 2004. Umass at trec 2004:
Notebook. In The Thirteenth Text Retrieval Confer-
ence (TREC 2004) Notebook.
James Allan, Courtney Wade, and Alvaro Bolivar. 2003.
Retrieval and novelty detection at the sentence level.
In Proceedings of the ACM SIGIR conference on re-
search and development in information retrieval.
Stephen Blott, Oisin Boydell, Fabrice Camous, Paul Fer-
guson, Georgina Gaughan, Cathal Gurrin, Noel Mur-
phy, Noel O?Connor, Alan F. Smeaton, Barry Smyth,
and Peter Wilkins. 2004. Experiments in terabyte
searching, genomic retrieval and novelty detection for
trec-2004. In The Thirteenth Text Retrieval Confer-
ence (TREC 2004) Notebook.
John M Conroy, Daniel M. Dunlavy, and Dianne P.
O?Leary. 2003. From trec to duc to trec again. In
TREC Notebook Proceedings.
John M. Conroy. 2004. A hidden markov model for
trec?s novelty task. In The Thirteenth Text Retrieval
Conference (TREC 2004) Notebook.
David Eichmann, Padmini Srinivasan, Marc Light,
Hudong Wang, Xin Ying Qiu, Robert J. Arens, and
Aditya Sehgal. 2003. Experiments in novelty, genes
and questions at the university of iowa. In TREC Note-
book Proceedings.
David Eichmann, Yi Zhang, Shannon Bradshaw,
Xin Ying Qiu, Li Zhou, Padmini Srinivasan,
Aditya Kumar Sehgal, and Hudon Wong. 2004. Nov-
elty, question answering and genomics: The univer-
sity of iowa response. In The Thirteenth Text Retrieval
Conference (TREC 2004) Notebook.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. In Proceedings of ANLP/NAACL-
2000 Workshop on Automatic Summarization.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention, and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Srikanth Kallurkar, Yongmei Shi, R. Scott Cost, Charles
Nicholas, Akshay Java, Christopher James, Sowjanya
Rajavaram, Vishal Shanbhag, Sachin Bhatkar, and
Drew Ogle. 2003. Umbc at trec 12. In TREC Note-
book Proceedings.
R. Ohgaya, A. Shimmura, and T. Takagi. 2003. Meiji
university web and novelty track experiments at trec
2003. In TREC Notebook Proceedings.
Liyun Ru, Le Zhao, Min Zhang, and Shaoping Ma. 2004.
Improved feature selection and redundancy computing
? thuir at trec 2004 novelty track. In The Thirteenth
Text Retrieval Conference (TREC 2004) Notebook.
Ian Soboroff. 2004. Draft overview of the trec 2004 nov-
elty track. In The Thirteenth Text Retrieval Conference
(TREC 2004) Notebook.
Jian Sun, Wenfeng pan, Huaping Zhang, Zhe Yang, Bin
Wang, Gang Zhang, and Xueqi Cheng. 2003. Trec-
2003 novelty and web track at ict. In TREC Notebook
Proceedings.
723
Exper iments  in Automated  Lex icon  Bu i ld ing  for Text  Search ing  
Barry Schiffman and Kathleen R. McKeown 
Department of Computer Science 
Columbia University 
New York, NY 10027, USA 
{ bschiff,kathy} @)cs.columbia.edu 
Abstract 
This paper describes experiment's in the automat'ic 
construction of lexicons that would be useflfl in 
searching large document collect'ions tot text frag~ 
ments tinct address a specific inibrmation eed, such 
as an answer to a quest'ion. 
1 Introduct ion 
In develot)ing a syst'em to find answers in text to 
user questions, we mmovered a major obstacle: Doe- 
mnent sentences t'hat contained answers dkl not of_ 
ten use the same expressions as the question. While 
an:;wers in documents and questiolts llse terms that' 
are relat'e(l to each other, a system that sear(:hes for 
answers based on the quesl:ion wording will often 
fail. 3.b address t'his probleln, we develol)ed tech- 
niques to al,tomatically build a lexicon of associated 
terms t'hat can be used to hell) lind al)lIrol/riate bext' 
seglllent,s. 
The mismatch })et'ween (tuestion an(l doctlttlent 
wording was I)rought home to us in an analysis of a 
testbed of question/answer l/airs. \~Ze had a collec- 
tion of newswire articles about the Clinton impeach- 
ment t'() use as a small-scale corl)uS fin' development 
of ;_t system. V~Ze asked several )eol)le to 1)ose ques- 
tions about this well-known t'opic, but we (lid not 
make the corpus availal)le to our cont'ril)utors. \~Ze 
wanted to avoid quest'ions that tracked t'he terminol- 
ogy in t'he corlms too (:losely to s innl late quest'ions 
t'o a real-world syst'em. The result was a set of ques- 
tions that  used language that' rarely nmtched t'he 
phrasing in the. corl)us. \,Ve had expected t'hat' we 
would be able to make most of these lexical connec- 
tions with the hel l) of V~rordnet (Miller, 1990). 
For example, consider a simple quest'ion al)out tes- 
timony: "Did Secret Service agents give testimony 
about' Bill Clinton?" There is no reason t'o expect 
that' the answer would appear 1)aldly st'ated as "Se- 
cret Service. agents dkl testi(y ..." What  we need 
to know is what' testimony is about', where it: occurs, 
who gives it. The answer would lie likely to be found 
in a passage ment'ioning juries, or 1)roseeut'ors, like 
these tbund in our Clinton corl)uS: 
Starr immediately brought Secret Service 
employees before tim grand jury for ques- 
tioning. 
Prosecutors repeat'edly asked Secret Ser- 
viee 1)ersonnel to rel)eat' gossil) they may 
have heard. 
Yet, tile V~ordnet synsets fbr "testinlony" offer: 
"evidence, assertion, averment alia asseveration," 
not a very hell)tiff selection here. -Wordnet hyper- 
nyms become general quickly: "declarat'ion," indi- 
cat'ion" and " infor lnat ion" are only one st, eli u 1) in 
t'lle hierarehy. Following these does not lead us into 
a courtroom. 
We asked our cont'ril)ut'ors for a second round of 
questions, but this time made the corpus available 
to them, exl)laining t'hat we wanted to be sure the 
answers were contained in t'he collection of articles. 
'J'he result was a set of questions that' mueh more 
closely matched t'he wording in the corpus. This was~ 
in t'aet, what' the 1999 DARPA question-answering 
(:oml)et'ition did in order t'o ensure that their ques- 
tions couhl be answered (Singhal, 1!199). The sec- 
trod quest ion-answer ing  conference adopted a new 
approach to gathering questions and verifying sepa- 
rately that' they a.re answerable. 
Our intuition is t'hat if we can lind the tyl)ical 
lexical neighborhoods of concept's, we can efficiently 
locate a concept described in a query or a question 
without needing to know the precise way the answer 
is phrased and without relying on a cost'ly, hand- 
built concept' hierarchy. 
The example above illustrat'es the 1)oint. Tes- 
t imony is given 1) 3, wit'nesses, defendant's, eyewit- 
nesses. It is solicited by 1)rosecutors, counsels, 
lawyers. It is heard by judges, juries at trials, hear- 
ings, and recorded in depositions and transcripts. 
What' we wanted was a complete description of t'he 
world of testimony - the who, what, when and 
where of the word. Or, in other words, the "meta- 
aboutness" of terms. 
To this end, we exl)erimented /tSitlg shallow lin- 
guist.k: techniques t'o gat'her and analyze word co- 
occurrence data in various configurat'ions. Unlike 
previous collocation research, we were int'erested 
in an expansive set' of relationships between words 
719 
rather than a specific relationship. More important, 
we felt that the information we needed could be de- 
rived from an analysis that crossed clause and sen- 
tence boundaries. We hyl)othesized that news ar- 
ticles would be coherent so that the sequences of 
sentences and clauses would be linked conceptually. 
We exanfined the nouns in a number of configura- 
tions - paragraphs, entences, clauses and sequences 
of clauses - and obtained tile strongest results from 
configurations that count co-occurrences across the 
surface subjects of sequences of two to six clauses. 
Exl)eriments with multi-clause configurations were 
generally more accurate in a variety of experiments. 
In the next section, we briefly review related re- 
search. In section 3 we describe our experiments. 
In section 4, we discuss the problem of evaluation, 
and look ahead to future directions in the concluding 
sections. 
2 Re la ted  Work 
There has been a large body of work ill the collec- 
tion of co-occurrence data from a broad spectrum of 
perspectives, fi'om information retrieval to the devel- 
opnlent of statistical methods for investigating word 
similarity and classification. Our efforts fall some- 
where in tile middle. 
Compared with document retrieval tasks, we are 
more closely focused on the words themselves and 
on specific concepts than on document "aboutness." 
Jing and Croft (1994) exanfined words and phrases 
in paragraph units, and found that the association 
data improves retrieval performance. Callan (1994) 
compared paragraph units and fixed windows of text 
in examining passage-level retrieval. 
In the question-answering context, Morton (1999) 
collected document co-occurrence statistics to un- 
cover 1)art-whole and synonymy relationships to use 
in a question-answering system. The key differ- 
ence here was that co-occurrence was considered on 
a whole-docmnent basis. Harabagiu and Maiorano 
(1999) argued that indexing in question answering 
should be based on 1)aragraphs. 
One recent al)proach to automatic lexicon build- 
ing has used seed words to lmild up larger sets of 
semmltically similar words in one or nlore categories 
(Riloff and Shepherd, 1997). In addition, Strza- 
lkowski and Wang (1996) used a bootstrapping tech- 
nique to identify types of references, and Riloff and 
Jones (1999) adapted bootstrapping techniques to 
lexicon building targeted to information extraction. 
In the same vein, researchers at Brown Univer- 
sity (Caraballo and Charniak, 1999)~ (Berland and 
Charniak, 1999), (Caraballo, 1999) and (Roark and 
Charniak, 1998) focused on target constructions, in
particular complex noun t)hrases, and searched for 
information ot only on identifying classes of nouns, 
lint also hypernyms, noun specificity and meronymy. 
We have a diflbrent perspective than these lines of 
inquiry. They were specifying various semantic rela- 
tionships and seeking ways to collect similar pairs. 
We. have a less restrictive focus and are relying on 
surface syntactic information about clauses. 
For more than a decade, a variety of statistical 
techniques have been developed and refilled. Tile 
focus of much of this work was to develop the 
methods themselves. Church and Hanks (1989) ex- 
plored tile use of mutual information statistics in 
ranking co-occurrences within five-word windows. 
Smadja (1992) gathered co-occurrences within five- 
word windows to find collocations, particularly in 
specific domains. Hindle (1990) classified nouns 
on the basis of co-occurring patterns of subject- 
verb and verb-object pairs. Hatzivassiloglou and 
MeKeown (1993) clustered adjectives into semantic 
classes, and Pereira et al (1993) clustered nouns on 
their appearance ill verb-object pairs. We are try- 
ing to be less restrictive in learning multiple salient 
relationshil)s between words rather than seeldng a 
particular elationship. 
Ill a way, our idea is the mirror image of Barzilay 
and Elhadad (1997), who used Wordnet to identify 
lexical chains that would coincide with cohesive text 
segments. We assunmd that documents are cohesive 
and that co-occurrence l)atterns call uncover word 
relationships. 
3 Experiments 
Tile focus of onr experiment was on units of text in 
which the constituents must fit together in order for 
the discourse to be coherent. We made the assump- 
tion that the documents in our corpus were coherent 
and reasoned that if we had enough text, covering 
a broad range of topics, we could pick out domain- 
independent associations. For example, testimony 
can be about virtually anything, since anything can 
wind up in a court dispute. But over a large enough 
collection of text, the terms that directly relate to 
tile "who," "what" and "where" of testimony per 
se should appear in segments with testimony more 
frequently than chance. 
These associations do not necessarily appear in a 
dictionary or thesaurus. When huntans explain all 
unfamiliar word, they often use scenarios and analo- 
gies. 
We divided the experiments in two groups: one 
group that looks at co-occurrences within a single 
unit, and another that looks at a sequence of units. 
In the first group of experinmnts, we considered 
paragraphs, sentences and clauses, each with and 
without prepositional phrases. 
? Single paragraphs with/without PP 
? Single sentences with/without PP 
? Single clauses with/without PP 
720 
\]in the second group, we considered two clauses 
and sequences of subject 110un phrases from two to 
six chmses. Ill this group, we had: 
,, Two clauses with/without Pl) 
,, A sequence of subject NPs fl'onl 2 clauses 
A sequence of subject NPs Dora 3 clauses 
,, A sequence of subject NPs from 4 clauses 
? A sequence of subject NPs fi'om 5 clauses 
,, A sequence of subject NPs from 6 clauses 
The intuition for the second groul) is that a topic 
flows from one granmm.tical unit to another so that 
the salient nouns, l)articularly the surface subjects, 
in successive clauses should reveal the associations 
we are seeldng. 
'\[lo illustrate the method, consider the three-clause 
configuration: Say that ~vordi apl)ears in clausc,~. 
We maintain a table of all word pairs and increment 
the entries for O,,o,'(h , ',,,o,'d~ ), where , ,0 ,% is a sub- 
ject noun in cla'usc,~, clauscn+~, or ell'use,+2. No 
effort was made to resolve pronomial references, and 
these were skipped. 
We used nollnS Olfly' because l)reliminary tests 
showed that pairings between ouns seemed to stand 
out. V~Te included tokens that were tagged as 1)roper 
nall leS when they also have have con ln lon  n lean ings .  
For example, consider the Linguistic Data Consor- 
l;ium at the University of Pennsylvania. Data, Con- 
sortium and University wouM be on tile list used to 
build the table of nmtchul)s with other nouns, \])lit 
l)emlsylvania would not. V~To also collected noun 
modifiers as well as head nouns as they can carry 
more information than the surface heads, such as 
"business group", '".science class" or "crinm scene." 
The corpus consisted of all tile general-interest ar- 
ticles from the New York Tinms newswire in 1996 
in the North American News Corlms , and (lid not 
include either st)orts or l)usiness news. We tirst re- 
moved dul)licate articles. The data fl'om 1996 was 
too slmrse for the sequence-of-subjects ontigura- 
lions. '\]'o l)alance the expcrinmnts better, we added 
another year's worth of newswire articles, from 1995, 
tbr the sequence-of subject configurations sothat we 
had more than one million matchups for each con- 
figuration (Table 1). 
The I)roeess is flflly automatic, requiring no su- 
1)ervision or training examples. The corpus was 
tagged with a decision-tree tagger (Schmid, 1994) 
and parsed with a finite-state parser (Abney, 1996) 
using a specially written context-fi'ee-grannnar that 
focused on locating clause boundaries. The gram- 
mar also identified extended noun l)hrases in tile sub- 
ject position, verb l)hrases and other noun l)hrases 
and prepositional 1)hrases. The nouns in the tagged, 
parsed corl)uS were reduced to their syntactic roots 
(removing l)lurals from nouns) with a lookup table 
created t'rom Wordnet (Miller, 1990) and CELEX 
(1995). We. performed this last step mainly to ad- 
dress the sparse data problem. There were a sub- 
stantial nunfl)er of paMngs that occurred only once. 
We elinfinated from considerat;ion all such single- 
tons, although it did not al)peal to have much etfect 
on the overall outcome. 
Confi.q Matchups 
Para +pp 6.5 million 
Sent 1.7 million 
Sent +pp 4 million 
1 Clause 1.1 million 
1 Clause +pp 2.8 million 
2 Clause 1.9 million 
2 Clause +I)P 5 nfillion 
Subj 2 Clause 1.1 million* 
Subj 3 Clause 1.6 million* 
Subj 4 Clause 2.1 million* 
Subj 5 Clause 2 .6m~ 
Subj 6 Clause 3.1 million* 
'lhble 1: Nmnl)er of matchut)s ibund; tile "*" de- 
notes the inclusion of 1995 data 
There were about 1.2 million paragraphs, 2.2 mil- 
lion sentences and 3.4 million clauses in the selected 
portions of the 1996 COl'pus. The total number of 
words was 57 million. Table 2 shows the nmnl)er of 
distinct nouns. 
I I All Extracted 
No l)ps 74,500 
W/pps 91,700 
Subjs 51,000 
Counts > 1. 
44,400 
53,900 
30,800 
Td)le 2: Distinct Nouns, 1996 Data 
To score the nmtchups in our initial exlmriments , 
we used the Dice Coeliicient, which l)roduces values 
i'ronl 0 to 1, to measure the association between pairs 
of words and then produced an ordered association 
list fl'om the co-occurrence table, ranked according 
to the scores of the entries. 
2 ? f, '~q(wo,.,h n ,oo ,%)  
score,, = frcq(wordi) + frcq(wordj) 
One 1)roblem was immediately al)parent: The 
quality of tile association lists wxried greatly. Tile 
scoring was doing an acceptable job in ranking the 
words within each list, but tile scores varied greatly 
from one list to another. Our initial strategy was 
to choose a cutoff, which we set at 21 tbr each list, 
and we tried several alternatives to weed out weak 
associations. 
721 
In one method, we filtered the association lists 
by cross-referencing, removing from the association 
list for wordi any wordj that failed to reciprocate 
and to give a high rank to wordi on its association 
list. Another similar approach was to try to con> 
bine evidence fl'om different experiments by taking 
the results fl'om two configurations into considera- 
tion. A third strategy was to calculate the mutual 
information between the target word and the other 
words on its association list. 
scorc,,i = p(xy) * log \p(z)p(y) ( (xy) ) 
Using the mutual information computation pro- 
vided an way of using a single measure that was able 
to compare matchups across lists. We set a threshold 
of lxl.0 -6 for all matchups. Thus these association 
lists vary in length, depending on the distributions 
for the words, allowing them to grow up to 40, while 
some ended up with only one or two words. 
4 Evaluation 
The evaluation of a system like ours is problematic. 
The judgments we made to determine correctness 
were not only highly subjective but time-consunfing. 
We had 12 large lexicons fl'om the different config- 
urations. We had chosen a random sample of 10 
percent of the 2,700 words that occurred at least 
100 times in tile corpus, and manually constructed 
an answer key, which ended up with ahnost 30,000 
entries. 
From the resulting 270 words, we discarded 15 of 
those that coincided with common names of peo- 
ple, such as "carter," which could refer to the for- 
mer American president, Chris Carter (creator of 
tile television show "X-Files"), among others. We 
thought it better to delay making decisions on how 
to handle such cases, especially since it would require 
distinguishing one Carter fl'om another. Such words 
presented several difficulties. Unless the individuals 
involved were well-known, it was often impossible to 
distinguish whether the system was making errors 
or whether the resulting descriptive terms were in- 
tbrmative. 
Tables 3 and 4 show an example from the answer 
key tbr the word "faculty." 
The overall results from the first stage of the pro- 
cess, before the cross-referencing filter are shown in 
Table 5, ranging from 73% to 80% correct. The con- 
figurations that included prepositional phrases and 
those that used sequences of subject noun phrases 
outperformed the configurations that relied on suh- 
jects and objects in a single grammatical unit. These 
differences were statistically significant, with p < 
0.01 in all eases. 
The overall results after cross-referencing, in Ta- 
ble 6, showed improvements of 5 to 10 percentage 
enrollment hiring adnfinistrator 
journalism alumnus student 
school union math 
engineering curriculum trustee 
group seminar thesis 
tenure stair department 
mathematician educator member 
ivy arts college 
chancellor report senate 
activism university el,airman 
professor teaching law 
regent doctorate mtministration 
academic committee semester 
board camI)us undergraduate 
salary council research 
president adviser mathematics 
course advisor sociology 
dean study science 
teacher cannon provost 
vote 
Table 3: Answer Key for Faculty: OK 
load tratllcway unrest 
architecture diversity hurdle 
shield minority revision 
disburse percent woman 
clement 
Table 4: Answer Key ff)r Faculty: Wrong 
points, while the effect of the number of matchups 
was diminished. Here, the subject-sequence onfig- 
urations showed a distinct advantage. While more 
noise might be expected when a large segment of text; 
is considered, these results support the notion that 
the nnderlying coherence of a discourse can be recov- 
ered with the prol)er selection of linguistic features. 
The improvements in each configuration over the 
corresponding configuration in the first stage were 
all statistically significant, with p < 0.01. Likewise, 
the edge the sequence-of subjects configurations had 
over tile other configurations, was also statistically 
significant. 
The results fl'om combining the evidence from dif- 
ferent configurations, in Table 7, showed a much 
higher accnrae> but a sharp drop in the total nnm- 
ber of associated words found. The most fl'uitful 
pairs of experiments were those that combined dis- 
tinct approaches, for example, tile five-subject con- 
figuration with either fifll paragraphs or with sen- 
tences with prepositional phrases. It will remain 
unclear until we conduct a task-based evaluation 
whether the smaller number of associations will be 
harnfful. 
The final experiment, computing the mutual in- 
formation statistic tbr the matchul)s of a key word 
with co-occurring words was perhaps the most ill- 
teresting because it gave us the ability to apply a 
722 
(Jontig OK Wrong l)ct OK 
Para +l/ l)  3832 1054 78 
,qent 3773 1270 75 
Sent +Pl) 3973 1070 79 
\] Clause 3652 1371 73 
\] Clauses q-l)l) 3935 1108 78 
"! Clauses 3695 1328 74 
"! Clauses -t-l)l) 3983 1018 80 
Subj 2 CI 3877 1139 77 
Sul)j 3 CI 3899 1117 78 
Subj 4 CI 3!)(/5 :1082 78 
Sul)j 5 C1 390d 1076 78 
Sul)j 6 CI 3909 1066 7!) 
Table 5: Results 13efore Cross I loferencing 
Contig ()K Wrong Pet ()K 
Para q-Pl) 3651} 73/1 83 
Sent 3328 742 82 
Sent -bpp 3751 8:18 82 
:1 Clause 3067 748 80 
1 Clauses +1)I / 3659 826 82 
2 Cbmses 3048 55d 85 
2 Clauses +pp 3232 60d 8d 
Subj 2 CI 2910 450 87 
t-;ul~j 3 CI 3020 4d() 87 
Subj 4 CI 3050 d28 88 
l~tll).j 5 (J\] ;1:12t3 dd2 88 
Subj 6 C1 3237 dd9 88 
' lhble 6: l{esults After Cross Referencing 
single threshold across different key words, saving 
the effort of performing the cross-retbrencing calm> 
lations and providing a deeper assorl:ment in SOllle 
C~lSeS. lilt lnost of the configurations, lltlltllPl illfOr- 
mat.ion gave 118 lllore \Vol'ds, and greater ln'ecision 
at; the sanle time, but nlost of all, gave us a reason- 
able threshold to apply throughout  he exlicrinlent. 
Whi le the accuracies in most of the configurations 
were close to one another,  those that  used only sin- 
g\]e units tended to be weaker than the mult i -c lause 
units. Note that  the paragraI)h contiguration was 
tested with far more data  than any of the others. 
Our system maD~s no eth)rt to aeCOllnt for lexi- 
cal aml)iguil;y. The uses we intend for our lexicon 
should provide some insulat ion from the ett'ects of 
polysemy, since searches will be conducted on a nun> 
l)er of terms, which should converge to one meaning. 
It is clear that  in lists for key words with mult i -  
ple senses, the donfinant sense where there is one, 
al)pears much lnore frequently, such as "faculty ," 
where the meaning of "teacher" is more t:'re(tuent 
than the meaning of "al)ility." F igure \] shows the 
top 21 words in the sequence-otCsix subjects,  beibre 
the cross-referencing ii lter was applied. Twenty of 
the 21. entries were scored aeceptal)le. 
After the cross-referencing is applied, doctorate,, 
education and revision were elinfinated. 
Contig OK \?rong Pcl; OK 
l~ara 2003 183 92 
Sent 1962 222 90 
Sent-t- 2033 213 91 
1 Clause 1791 218 89 
1 C lause+ 2004 198 91 
2 Clause 2028 277 88 
2 C lause+ 2:129 24,1 90 
Tal)le 7: Results of coml)ining evidence; all configu- 
rat ions were combined with the sequence of six sub- 
jects 
Conlig OK "Wrong Pet OK 
Para +pp 4923 807 86 
Sent 5193 990 84 
Sent +Pl) 4876 775 86 
1 Claus(; 52!)!/ 1233 81 
1 Clauses-t-l)l) 5047 878 85 
2 Clauses 5025 928 84 
2 Clauses -I-Pl) d668 728 87 
Subj 2 C1 5229 939 85 
Subj 3 C1 5187 860 85 
Subj ~1 C1 5119 808 86 
Subj 5 C1 500"{ 76d 87 
Subj 6 CI 4!)80 736 87 
Table 8: l lesults with mutual  information 
The results from the single clause configuration 
(Figure 2) were almost as strong, with three erroFs, 
and a fair amount of overlap between the two. 
The word "admiral" was more difficult %r the ex- 
\])erilllellt ilSilig the l)ice coefficient. The. list shows 
some of l.he confusion arising from our strate.~y Oll 
prot)er nouns. Admiral  would be expected to oc- 
cur with many proper ll~tnles, i l lcluding some that  
axe st)elled liD; common 11o1111.q, bi l l  the list h)r the 
single clause q pp conf iguratkm presented a lmzzling 
list (F igure 3). 
The sparseness of the data  is also al)lmrent, but it 
was the dog reDxenees that  al)peared quite strange 
at a ghulce: Inspection of the. articles showed that  
they callle froln all a.rticle on the pets of famous 
people. Note that the dogs did not al)l)ear in top 
ranks of the sequence of subjects  configuration in 
the Dice exper iment (Figure 4), nor were they in the 
results t'rom the experiments with cross-referencing, 
combining evidence and mutua l  information. 
After cross-reR;rencing, the much-shorter list for 
the Sub j-6 configuration had "aviator",  "break-up",  
' ;commander",  "decoration",  "equal-ot)portunity",  
"tleet", "merino", "navf ' ,  "pearl",  "promotion", 
"rear" ~ alia "short". 
' l 'he combined-evidence list contained only eight 
words: "navy", "short", "aviator",  "merino", "dis- 
honor",  "decoration",  "sul)" and "break-ul)". 
Using the mutual  intbr lnat ion scoring, the list 
in the Subj-6 configuration tbr admiral  had only 
723 
faculty trustee(51) 0.053; carat)us(d1) (/.045; 
college(ll3) 0.034; member(369) 0.028; profes- 
sor(102) 0.028; university(203) 0.027; student(206) 
0.025; regent(19) 0.025; tenure(15) 0.025; ctmncel- 
lor(28) 0.023; administrator(34) 0.023; provost(12) 
0.023; dean(27) 0.021; ahmmus(13) 0.021; math(12) 
0.017; revision(8) 0.013; salary(13) 0.013; so- 
ciology(7) 0.013; educator(l l) 0.012; doctorate(6) 
0.011.; teaching(9) 0.011; 
Figure 1: Tile top-ranked matchups for "fac- 
ulty" from the Subj-6-Clause configuration be- 
fore cross-referencing. The nmnbers in paren- 
theses are the number of matchups and the real 
umnbers following are the scores. Errors are in 
bold 
faculty trustee(31) 0.033; meml)er(266) 0.025; ad- 
nfinistrator(31) 0.023; college(42) 0.012; dean(15) 
0.012; tenure(8) 0.011; ivy(6) 0.011; staff(a3) 0.01; 
semester(6) 0.01; regent(7) 0.01; salary(12) 0.01; 
math(7) 0.008; professor(a1) 0.008; load(6) 0.007; 
curricuhun(5) 0.006; revision(4) 0.006; minor- 
ity( l l )  0.006; 
Figure 2: The top-ranked matchups for "fac- 
ulty" under the single clause confignration. Er- 
rors are in bold. 
nine words: "navy", "general", "commander", 
"vice", "promotion", "officer", "fleet", "military" 
and "smith." 
Finally, the even-sparser mutual information list 
for the paragraph configuration lists only "navy" 
and "suicide." 
5 Conc lus ion  
Our results are encouraging. We were able to deci- 
pher a broad type of word association, and showed 
that our method of searching sequences of subjects 
outperformed the snore traditional approaches in 
finding collocations. We believe we can use tiffs tech- 
nique to build a large-scale l xicon to help in diffi- 
cult information retrieval and information extraction 
tasks like question answering. 
The most interesting aspect of" this work lies in 
the system's ability to look across several clauses 
and strengthen tile connections between associated 
words. We are able to deal with input that con- 
tains numerous errors from the tagging and shallow 
parsing processes. Local context has been studied 
extensively in recent years with sophisticated statis- 
tical tools and the availability of enormous amounts 
of text in digital form. Perhaps we can expand this 
perspective to look at a window of perhaps everal 
sentences by extracting the correct linguistic units in 
order to explore a large range of language processing 
problems. 
admiral- navy(all) 0.027; ayMon(d) 0.024; cheat- 
ing(5) 0.02; gallantry(3) 0.016; chow(4) 0.015; ser- 
vice,nan(d) 0.013; short(3) 0.013; wardroom(2) 
0.012; american(2) 0.012; enos(2) 0.012; self- 
assessment(2) 0.(/11; merino(2) 0.011; ocelot(2) 
0.011; wolfhound(2)0.011; igloo(2)0.011; pa- 
prika(2) 0.011; spaniel(2) 0.01; medal(8) 0.01; 
awe(a) 0.01; pedigree(2) 0.009; te,'rier(2) 0.009; 
Figure 3: Top-ranked matchups for "adnfiral" 
under the clause +pp configuration. 
admiral - navy(88) 0.071; short(7) 0.03; promo- 
tion(ll) 0.027; hal)l)iness(8) 0.026; fleet(ll) 0.024; 
aviator(5) 0.022; mnbition(8) 0.019; merino(3) 
0.019; dishonor(3)0.018; rear(4)0.018; deco- 
ration(4) 0.015; sub(a) 0.013; airman(3) 0.013; 
graveses(2) 0.012; submariner(2) 0.012; equal- 
opportunity(2) 0.012; break-up(2) 0.012; comman- 
der(18) 0.012; pearl(7) 0.012; l)rophccy(d) 0.01.2; 
torturer(2) 0.012; 
Figure 4: The list for admiral fi'om the Sub j-6 
contiguration. 
6 Future  Work  
? We will have the scoring key itself evaluated by 
people who are not involved in tile research. 
? ~re are planning to conduct ask-based evalua- 
tion in question answering. 
? We are considering deploying a named entity 
module to provide sonic classification of which 
proper nouns should be counted and which 
should not. 
? We 1)lan to experiment with ways to incorpo- 
rate using examining verbs and making use of 
surface objects in the configurations with se- 
quences of clauses, as well as strengthen the fi- 
nite state grammar. 
? We will explore using tile system to extract bi- 
ographic information. 
Acknowledgments  
This material is based upon work supported by tile 
National Science Foundation under grants Nos. IIS- 
96-19124 and IRI-96-18797, and work jointly sup- 
ported by the National Science Foundation and the 
National Library of Medicine under grant No. IIS- 
98-17434. Any opinions, findings, and conclusions 
or recmmnendations expressed in this material are 
those of tile authors and do not necessarily reflect 
the views of the National Science Foundation. 
724 
References  
Steven Abney. 1996. Partial parsing via finite-state 
cascades. In Proceedin9s of th, e ESSLLI '95 Robust 
Parsin9 Workshop. 
Regina Barzilay and Michael Elhadad. 1997. Using 
lexical chains tbr text smmnarization. In Pwcced- 
ings of the Ntelligent Scalable Text b'ummariza- 
tion Workshop. ACL. 
Matthew Berland and Eugene Charniak. 1999. 
Finding parts in very large corpora. 'l.bchnical Re- 
port TR CS99-02, Brown University. 
James P. Callan. 1994. Passage-level vidence in 
document retrieval. In Proceedin9s of the Seven- 
teenth Annual Intcunational A CM SIGIR Confer- 
ence, Dublin, Ireland. ACM. 
Sharon Caraballo and Eugene Charniak. 1999. De- 
termining the speciticity of nouns from text. In 
P~vceedinfls of Co~@rcnce on E,mpi~eal Methods 
in Nat'u'ral Langua9e Processing. 
Sharon Caraballo. 1999. Automatic acquisition of 
a hylmrnym-labeled noun hierarchy from text. In 
Pwceedings of th, e 37th Annual Meeting of the As- 
sociation for Comp'utational Linguistics, June. 
CELEX, 19!)5. Tit(; CELEX lezical database 
Dutch,, English, Ge.rntan. Centr for Lexical hffor- 
mation, Max Planck Institute for Psycholinguis- 
ties, Nijmegen. 
Kenneth W. Church and Patrk:k Itanks. 1989. Word 
association orms, mutual infornmtion and lexi- 
cography. In Proceedings of th.e 27th. nteetin9 of 
the ACL. 
S&nda ~/\[. Ilara,bagiu anti S|;even J. Maiorano. 1999. 
Finding answers in large collectkms of texts: Para- 
graph indexiltg -t- adductive inference. In Q'aes- 
tion Answering Systema'. AAAI, November. 
Vasileios Hatziw~ssiloglou and Kathleen R. McKe- 
own. 1993. 'lbwards the automatic identification 
of adjectival scales: Clustering adjectives accord- 
ing to meaning. In P~vceedin9 s of th, c 31st Annual 
Meeting of th, e A CL. 
Donald Hindle. 1990. Noun classitication ti'om 
predicate-argument structures. In PTvceedin9s of 
the 28th Annual Meeting of the A CL. 
Yufcng Jing and W. Bruce Croft. 1994. An associa- 
tion thesaurus for information retrieval, tech. rep. 
no 94-17. 2bchnical report, Amherst: University 
of Massachusetts, Center for Intelligent hfforma- 
tion Retrieval. 
G. Millet'. 1990. Wordnet: An on-line lexical 
database. International . ournal of Lezicoqraphy. 
Thomas S. Morton. 1999. Using coreibrence tor 
question answering. In P~vccedings of thc Work- 
shop on Coreference and Its Applications, l)ages 
85-89, College Park, Maryland, June. Associa- 
tion for Computational Linguisties, Association 
for Computation Linguistics. 
Fernando Pereira, Naffali Tishby, and Lillian Lee. 
1993. Distributional clustering of english words. 
In Pwcecdings of the 31st Annual Meeting of the 
ACL. 
Ellen Rilotf and Pmsie Jones. 1999. Learning die- 
tionarics for intbrmation extraction by multi- 
level bootstral)ping. In Proceedings of the Six- 
teenth Na, tional Co~@rencc on Artificial Intelli- 
gence. AAAI. 
Ellen I{ilotf and Jessica Shepherd. 1997. A corpus- 
based approach for building semantic lexicons. In 
Proceedings of the Second Conference on Empir~i - 
cal Meth, ods in Natural Langua9 c Processing. 
Brian lloark and Eugene Charniak. 1998. Noun- 
phrasae co-occurrence statistics for semi- 
automatic semantk: lexicon construction. In 
P~vcccdings of thc 36th Annual Meetin9 of the 
Association for Computational Linguistics and 
the 17th htternational Conference on Computa- 
tion Linguistics. 
Hehnut Schmid. 1994. Probabilistic part-of speech 
tagging using decision trees. In Proceedings of the 
International Cor@rence on New Methods in Lan- 
9ua.qe Proecssin9. 
Amit Singhal. 1999. Question and answer track 
home page. WWW. 
Frank Smadja. 1992. Retrieving collocations fi'om 
text: Xtract. Comp'll, tational Linguistics, Special 
Issue. 
Tomek Strzalkowski and Jin V~rang. 1996. A self- 
learning universal concept spotter. In lhvceedinfls 
of th, e International 6'm@renee on Computational 
Linfluisties (Colin 9 199@. 
725 
Inferring Temporal Ordering of Events in News 
Inderjeet Mani  
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102 
imani@mitre.org 
Barry Schiffman 
Columbia University 
1214 Amsterdam Avenue 
New York, NY 10027 
bschiff@cs.columbia.
edu 
Jianping Zhang 
The MITRE Corporation 
7515 Colshire Drive 
McLean, VA 22102 
 jzhang@mitre.org 
 
 
 
 
Abstract 
This paper describes a domain-independent, 
machine-learning based approach to tempo-
rally anchoring and ordering events in news. 
The approach achieves 84.6% accuracy in 
temporally anchoring events and 75.4% accu-
racy in partially ordering them.  
1 
2 
Introduction 
Practical NLP applications such as text summariza-
tion and question-answering place increasing demands 
on the processing of temporal information. In multi-
document summarization of news, it is important to 
know the relative order of events so as to correctly 
merge and present information. In question-answering, 
one would like to be able to ask when an event occurs, 
or what events occurred prior to a particular event. Such 
capabilities presuppose an ability to infer the temporal 
order of events in discourse.  
A number of different knowledge sources appear to 
be involved in inferring event ordering (Lascarides and 
Asher 1993), including tense and aspect (1), temporal 
adverbials (2), and world knowledge (3).  
(1) Max entered the room. He had drunk/was drink-
ing the wine. 
(2) A drunken man died in the central Phillipines 
when he put a firecracker under his armpit. 
(3) U. N. Secretary- General Boutros Boutros-Ghali 
Sunday opened a meeting of ....Boutros-Ghali ar-
rived in Nairobi from South Africa, ? 
As (Bell 1999) has pointed out, the temporal struc-
ture of news is dictated by perceived news value rather 
than chronology. Thus, the latest news is often pre-
sented first, instead of events being described in order of 
occurrence (the latter ordering is called the narrative 
convention).  
This paper describes a domain-independent ap-
proach to temporally anchoring and ordering events in 
news. The approach is motivated by a pilot experiment 
with 8 subjects providing news event-ordering judg-
ments which revealed that the narrative convention ap-
plied only 47% of the time in ordering the events in 
successive past-tense clauses. Our approach involves 
mixed-initiative corpus annotation, with automatic tag-
ging to identify clause structure, tense, aspect, and tem-
poral adverbials, as well as tagging of reference times 
and anchoring of events with respect to reference times.  
We report on machine learning results from event-time 
anchoring  judgments. 
Linguistic Processing 
The time expression tagger TempEx (Mani and Wil-
son 2000) tags and assigns values to temporal expres-
sions, both ?absolute? expressions like ?June 1, 2001? 
and relative expressions like ?Monday?. It was cited in 
(Mani and Wilson 2000) as achieving a .83 F-measure 
against hand-annotated data. Inter-annotator reliability 
across 5 annotators on 193 TDT2-documents was .79F 
for extent and .86F for time values, with  TempEx scor-
ing .76F (extent) and .82F (value) on these documents. 
The clause tagger (CLAUSE-IT) identifies top-level 
clauses (C), top-level clauses with gapped subjects 
(GC), e.g., ?<C>He returned the book</C> <GC>and 
went home</GC>?, relative clauses (RC), and comple-
ment clauses (CO), which include all non-finite clauses.  
Our pilot experiment also revealed that the propor-
tion of clauses with explicit time expressions (TIMEX2) 
is approximately 25%, suggesting that anchoring the 
events to just the explicit times wouldn?t be sufficient. 
The system accordingly computes a reference time 
(Reichenbach 1947) value (tval)  for each clause, de-
fined to be either the time value of an explicit temporal 
expression mentioned in the clause, or, when the ex-
plicit time expression is absent, an implicit time value 
inferred from context.  
To generate this tval   feature, the simple algorithm 
in Figure 1 was used. The system also anchors the 
event?s time with respect to the tval (at, before, or after) 
when the tval is an explicit reference time. This feature 
is called anchor-explicit. All in all, the features shown 
in Table 1 were computed for each clause. 
 
Set initial tval to document-creation-date. 
For each clause: 
1. If clause has explicit time, then set its tval to it. 
2. If clause-type is relative clause, assume its tval 
is inaccessible to later discourse. 
3. If clause verb is of type reporting verb, set tval 
to document-creation-date. 
4. If clause is inside quotes inherit tval from em-
bedding clause. 
5. Otherwise, pick most recent tval. 
 
Figure 1: Algorithm for Computing  
Reference Time (tval) 
 
CTYPE: clause is a regular clause, 
complement clause, or relative clause  
CINDEX: subclause index  
PARA: paragraph number  
SENT: sentence number  
SCONJ: subordinating conjunction 
(e.g., while, since, before)  
TPREP: preposition in a TIMEX2  PP 
TIMEX2: string in the TIMEX2 tag  
TMOD:  temporal modifier not at-
tached to a TIMEX2, (e.g., after [an 
altercation])  
QUOTE: number of words in quotes  
REPVERB-P: reporting verb in clause 
STATIVE-P: stative verb in clause  
ACCOMP-P:  accomplishment verb  
ASPECTSHIFT: shift in aspect from 
previous clause  
G-ASPECT: grammatical aspect 
{progessive, perfect,nil} 
TENSE: tense of clause {past, pre-
sent, future, nil} 
TENSESHIFT: shift in tense from 
previous clause  
ANCHOR_EXPLICIT: {<, >, =, un-
def} 
TVAL: reference time for clause, i.e., 
a time value  
 
Table 1: Linguistic Features for each Clause1,2 
                                                          
3 
1 The statives and accomplishments were computed from 
UMaryland?s LCS lexicon, based on (Dorr and Olsen 1997) 
Learning Anchoring Rules 
A human unconnected with our project corrected the 
tval, based on a set of annotation guidelines, on a sam-
ple of 2069 clauses extracted at random from the North 
American News Corpus. She also anchored the event?s 
time with respect to the tval (AT, BEF, AFT, or unde-
fined). This feature (not a machine feature) is called 
anchors.  
The corrections showed that the algorithm in Figure 
1 was right on tval for 1231 out of 2069, giving an accu-
racy of 59%.  Tracking the sequence of corrected tvals 
revealed that the tval of the previous clause was kept 
65.75% of the time, that it reverted to some other previ-
ous tval 22.99% of the time, and that it shifted to a new 
tval 11.26% of the times. Most of the errors in com-
puted tvals had to do with the tval being assigned erro-
neously the document date rather than reverting to a 
non-immediately previous tval. Finally, the anchor-
explicit relation is correct 83.8% of the time; however, 
just guessing ?at? for the explicit anchor will get an 
accuracy of 90.2%.  
 
 ANCHORS TVAL-
MOVES 
MAJORITY (AT) 76.9 (KEEP) 
65.75 
C5.0 Rules 80.2 (?1.8) 71.8 (?0.5) 
 
Table 2: Accuracy of Anchoring Rules 
 
We then used this training data to train a statistical 
classifier, C5.0 Rules (Quinlan 1997), to learn (1) an-
chors relation rules and (2) rules for tracking the tval 
moves (keep, revert, shift) across successive clauses. 
The accuracy of anchors rules as well as tval change 
rules are shown in Table 2. It can be seen that accuracy 
of machine learning here is significantly better than the 
majority class. The tval, tense, and tense shift play a 
useful role in anchoring, revealing that the tval is a use-
ful abstraction. Here are some of the rules learnt (here te 
is the clause index, assumed to stand for the event time 
of the clause): 
If no sconj and no tmod and no tprep and tval-class 
=day then anchors(AT, te,, tval) 80.4% accurate 
(156 examples). 
If tense is present and no sconj and tval-
class=month then anchors(AT, te,, tval) 77.8 (7). 
If  tense is present perfect and no sconj, then  
                                                                                           
See  www.umiacs.umd.edu/ ~bonnie/ LCS_ Data-
base_Documentation.html. 
2 Since the TIMEX2 and tval values form an open class, they 
were automatically grouped into classes based on the granular-
ity of the time expression, namely, {time-of-day, day, week, 
month, year, or non-specific}. 
anchors(BEF, te,, tval) 83 (4). 
If tense shift is present2past and no explicit time and 
no sconj, then anchors(AT, te,, tval) 90 (30) 
4 Partially Ordering Links 
Based on the best machine-learned rules for the 
anchors relation, anchors tuples are generated for each 
document. The tvals in the document?s anchor tuples are 
also partially ordered, yielding tuples consisting of or-
dered pairs of tvals. The two sets of tuples are then used 
to provide a partial ordering of events in the document, 
in the form of links tuples: links(R, ei, ej), where ei and 
ej are the events corresponding to clauses i and j, and R 
is in {at, bef, aft, or undefined}. One of the authors 
evaluated the partial ordering for accuracy, on seven 
documents3. The results of this evaluation are shown in 
Table 3. #Correct-anchor is the number of the anchors 
tuples correctly classified and #total is the total number 
of anchors tuples classified. Link Recall is the percent-
age of human generated links tuples (723 in all) that are 
correctly identified by machine learned rules. Link Pre-
cision is the percentage of the machine generated links 
tuples that are correct.  
 
#Cl
aus
es 
#Wo
rds 
#correct-
anchor  /  
#total-
anchor 
 Link 
Recall 
Link 
Precision 
40 525 15/18  
(83.3%) 
44/65 
(67.7%) 
53/63 
(84.1%) 
18 335 12/13 
(92.3%) 
59/59 
(100%) 
59/62 
(95.2%) 
27 509 17/22 
(77.2%) 
23/40 
(57.5%) 
23/58 
(39.7%) 
38 617 21/27 
(77.8%) 
94/172 
(54.7%) 
94/190 
(49.5%) 
22 296 11/12 
(91.7%) 
39/42 
(92.9%) 
39/49 
(79.6%) 
14 242 6/7 
(85.7%) 
6/6 
(100%) 
6/7 
(85.7%) 
35 447 28/31 
(90.3%) 
297/339 
(87.6%) 
289/335 
(86.3%) 
194 2971 110/130 
(84.6%) 
562/723  
(77.7%) 
563/764  
(73.7%) 
 
Table 3: Document-Level  Accuracy 
of Learnt Rules 
5 
                                                          
Conclusion 
3 Note that the na?ve algorithm for tval is only 59% correct. 
While improvements to the na?ve algorithm are clearly possi-
ble based on the corrected tval, to adequately test the machine 
learnt rules we use the corrected tval. 
Overall, our approach achieves 84.6% accuracy in 
anchoring events and 75.4% F-measure in partially or-
dering them. These numbers compare favorably with the 
previous literature: (Filatova and Hovy 2001) obtained 
82% accuracy on anchoring for a single type of 
event/topic on 172 clauses, while (Mani and Wilson 
2000) obtained accuracy of 59.4% on anchoring over 
663 verb contexts. Our approach is also distinct in its 
use of human experimentation, machine learning and 
the variety of linguistically motivated features (includ-
ing temporal adverbials) that are brought to bear.  
Future work will examine the role of aspectual fea-
tures, learning from skewed distributions dominated by 
AT (an overwhelming majority of news events occur at 
the reference times), and the incorporation of unsuper-
vised learning methods.  
References 
A. Bell. News Stories as Narratives. In A. Jaworski 
and N. Coupland, The Discourse Reader, Routledge, 
1999, 236-251. 
E. Filatova, and E. Hovy. Assigning Time-Stamps to 
Event-Clauses. Workshop on Temporal and Spatial In-
formation Processing, ACL?2001, Toulouse, 88-95. 
A. Lascarides and  N. Asher. Temporal Relations, 
Discourse Structure, and Commonsense Entailment. 
1993. Linguistics and Philosophy 16, 437-494. 
I. Mani and G. Wilson. Robust Temporal Processing 
of News. ACL'2000, 69-76.   
R. Quinlan. 1997. C5.0. www.rulequest.com. 
H. Reichenbach. The tenses of verbs. In H. Reichen-
bach, Elements of Symbolic Logic. Macmillan, 1947, 
Section 51, 287-298. 
Columbia?s Newsblaster: New Features and Future Directions
Kathleen McKeown, Regina Barzilay, John Chen, David Elson, David Evans,
Judith Klavans, Ani Nenkova, Barry Schiffman and Sergey Sigelman
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, New York, N.Y. 10027
kathy@cs.columbia.edu
Abstract
Columbia?s Newsblaster tracking and summa-
rization system is a robust system that clus-
ters news into events, categorizes events into
broad topics and summarizes multiple articles
on each event. Here we outline our most cur-
rent work on tracking events over days, produc-
ing summaries that update a user on new infor-
mation about an event, outlining the perspec-
tives of news coming from different countries
and clustering and summarizing non-English
sources.
1 Introduction
Columbia?s Newsblaster1 provide news updates on a
daily basis from news published on the Internet; it crawls
news sites, categorizes stories into six broad areas, groups
news into stories on the same event, and generates a sum-
mary of the multiple articles describing each event. In ad-
dition to demonstrating the robustness of current summa-
rization and tracking technology, Newsblaster also serves
as a research environment in which we explore new di-
rections and problems. Currently, we are exploring the
tasks of multilingual summarization where input sources
are drawn frommultiple languages and a summary is gen-
erated in English on the same event (Figure 1), tracking
events across days and generating summaries that update
the user on what is new, and editing generated summaries
to improve fluency and accuracy. Our focus here is on
editing references to people, improving coherency of the
summary and ensuring that references are accurate. Edit-
ing is particularly important as we add multilingual capa-
bilities, given the errors inherent in machine translation.
1http://newsblaster.cs.columbia.edu
2 Multilingual Tracking and
Summarization
The multilingual version of Columbia Newsblaster is
built upon the English version of Columbia Newsblaster,
sharing the same structure and components. To add mul-
tilingual capability, the system first crawls web sites in
foreign languages, and stores both the language and en-
coding for the files. To extract the article text from the
HTML pages, we use a new article extraction component
using language-independent statistical features computed
over text blocks along with a machine learning compo-
nent to classify text blocks as one of ?Article Text?, ?Ti-
tle?, ?Image?, ?Image Caption?, or ?Other?. The article
extraction component has been trained and tested on En-
glish, Japanese, and Russian data, but is also being suc-
cessfully applied to French, Spanish, German, and Ital-
ian data. We plan to train the article extractor on other
languages (Chinese, Arabic, Korean, Spanish, German,
French, etc.) in the near future.
To cluster multilingual documents with English doc-
uments, we use the existing Newsblaster English doc-
ument clustering module. Non-English documents are
translated for clustering after the article extraction phase.
We use simple and fast document translation techniques
for clustering if available, since we potentially process
thousands of documents for a language for each run. We
have developed simple dictionary lookup techniques for
translation for clustering for Japanese and Russian; for
other languages we use an interface to the Systran trans-
lation system via Babelfish. We plan on adding Arabic
translation to the system in the near future.
Summarization is performed using the same summa-
rization strategies in Newsblaster. We are experimenting
with different methods for improving summary quality
when translation of text is noisy. For example, when an
input cluster contains both English and foreign sources,
we weight the English higher in cases where we deter-
mine it is representative of both the English and foreign
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 15-16
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Multilingual Version
input documents. We are also experimenting with meth-
ods for determining similarity across documents using
different levels of translation.
3 Different Perspectives
When news media report on international issues, they re-
flect the perspectives of their own countries. In the past,
Newsblaster has included all international sources as in-
put to its summaries. Recently, we have added a feature
of ?international perspectives? to the system. In addition
to the universal summary for a particular event, which
includes all international sources, Newsblaster now gen-
erates separate summaries for each country, which may
illustrate unique biases or disagree on facts. The News-
blaster interface allows users to view any pair of sum-
maries side by side to compare different perspectives.
4 Summary Rewrite
Newsblaster also currently includes a module for rewrit-
ing summaries to achieve better readability. References
to people are rewritten so that the first mention includes
the person?s full name and a selected description and later
mentions are restricted to last name only. In addition to
improving readability, the rewritten version of the sum-
mary is usually shorter than the version before rewrite,
since multiple verbose descriptions of the same entity are
discarded. These changes can be seen when comparing
the summary sentence with the original document via a
link from the summary using a proxy.
5 Event Tracking and Updates
Newsblaster currently identifies events within a single
day; a new set of clusters is generated each day. We have
designed a new module for tracking events across days,
allowing the system to relate stories published on one day
to closely related stories on other days. In this way, the
user can more easily track events of interest as they un-
fold. The typical approach for tracking events across days
represents each event as one monolithic set of stories. We
have focused instead on a model where events on one day
can divide into related sub-events on the next day. For ex-
ample, a set of stories about the start of the Iraq war is an
event that can branch into multiple sets of stories, each set
representing a different facet of the war. We are currently
determining an appropriate evaluation of this approach as
well as investigating different possible interfaces.
If a user is tracking events across days, it is more useful
to have a summary that provides updates on what is new
as opposed to a summary of similarities across all days.
We have built a prototype update summarizer that scans
new articles extracted by the system and compares these
new articles with a background cluster on the same event.
The summarizer will provide the user with a summary of
only important new developments. As the tracking mod-
ule locates new articles, it will pass these to the update
summarizer, which will determine what, if anything, has
changed. This summarizer uses more syntactic and se-
mantic information about the articles to determine nov-
elty than is used in our other summarization strategies and
thus, efficiency is a challenge. We will demo these com-
ponents in a separately fromNewsblaster as they have not
yet been integrated in the development version.
Proceedings of NAACL HLT 2007, pages 532?539,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Question Answering using Integrated Information Retrieval and
Information Extraction
Barry Schiffman and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, NY 10027
bschiff,kathy@cs.columbia.edu
Ralph Grishman
Department of Computer Science
New York University
New York, NY 10003
grishman@cs.nyu.edu
James Allan
University of Massachusetts
Department of Computer Science
Amherst, MA 01003
allan@cs.umass.edu
Abstract
This paper addresses the task of provid-
ing extended responses to questions re-
garding specialized topics. This task is an
amalgam of information retrieval, topical
summarization, and Information Extrac-
tion (IE). We present an approach which
draws on methods from each of these ar-
eas, and compare the effectiveness of this
approach with a query-focused summa-
rization approach. The two systems are
evaluated in the context of the prosecution
queries like those in the DARPA GALE
distillation evaluation.
1 Introduction
As question-answering systems advance from han-
dling factoid questions to more complex requests,
they must be able to determine how much informa-
tion to include while making sure that the informa-
tion selected is indeed relevant. Unlike factoid ques-
tions, there is no clear criterion that defines the kind
of phrase that answers the question; instead, there
may be many phrases that could make up an answer
and it is often unclear in advance, how many. As
system developers, our goal is to yield high recall
without sacrificing precision.
In response to questions about particular events of
interest that can be enumerated in advance, it is pos-
sible to perform a deeper semantic analysis focusing
on the entities, relations, and sub-events of interest.
On the other hand, the deeper analysis may be error-
ful and will also not always provide complete cov-
erage of the information relevant to the query. The
challenge, therefore, is to blend a shallower, robust
approach with the deeper approach in an effective
way.
In this paper, we show how this can be achieved
through a synergistic combination of information re-
trieval and information extraction. We interleave in-
formation retrieval (IR) and response generation, us-
ing IR in high precision mode in the first stage to
return a small number of documents that are highly
likely to be relevant. Information extraction of enti-
ties and events within these documents is then used
to pinpoint highly relevant sentences and associated
words are selected to revise the query for a sec-
ond pass of retrieval, improving recall. As part of
this process, we approximate the relevant context by
measuring the proximity of the target name in the
query and extracted events.
Our approach has been evaluated in the frame-
work of the DARPA GALE1 program. One of the
GALE evaluations involves responding to questions
based on a set of question templates, ranging from
broad questions like ?Provide information on X?,
where X is an organization, to questions focused on
particular classes of events. For the experiments pre-
sented here, we used the GALE program?s prosecu-
tion class of questions. These are given in the fol-
lowing form: ?Describe the prosecution of X for Y,?
where X is a person and Y is a crime or charge. Our
results show that we are able to achieve higher accu-
1Global Autonomous Language Exploitation
532
racy with a system that exploits the justice events
identified by IE than with an approach based on
query-focused summarization alone.
In the following sections, we first describe the
task and then review related work in question-
answering. Section 3 details our procedure for find-
ing answers as well as performing the information
retrieval and information extraction tasks. Section 4
compares the results of the two approaches. Finally,
we present our conclusion and plans for future work.
1.1 The Task
The language of the question immediately raises the
question of what is meant by prosecution. Unlike a
question such as ?When was X born??, which is ex-
pected to be answered by a clear, concrete phrase,
the prosecution question asks for a much greater
range of material. The answer is in no way limited
to the statements and activities of the prosecuting at-
torney, although these would certainly be part of a
comprehensive answer.
In the GALE relevance guidelines2 , the answer
can include many facets of the case:
? Descriptions of the accused?s involvement in
the crime.
? Descriptions of the activities, motivations, and
involvement in the crime.
? Descriptions of the person as long as they are
related to the trial.
? Information about the defense of the suspect.
? Information about the sentencing of the person.
? Information about similar cases involving the
person.
? Information about the arrest of the person and
statements made by him or her.
? Reactions of people involved in the trial, as
well as statements by officials or reactions by
the general public.
2BAE Systems Advanced Information Technologies, ?Rele-
vance Guidelines for Distillation Evaluation for GALE: Global
Autonomous Language Exploitation?, Version 2.2, January 25,
2007
The guidelines also provide a catchall instruction
to ?include reported information believed to be rele-
vant to the case, but deemed inadmissible in a court
of law.?
It is easy to see that the use of a few search terms
alone will be insufficient to locate a comprehensive
answer.
We took a broad view of the question type and
consider that any information about the investiga-
tion, accusation, pursuit, capture, trial and punish-
ment of the individual, whether a person or organi-
zation, would be desireable in the answer.
1.2 Overview
The first step in our procedure sends a query tai-
lored to this question type to the IR system to ob-
tain a small number of high-quality documents with
which we can determine what name variations are
used in the corpus and estimate how many docu-
ments contain references to the individual. In the
future we will expand the type of information we
want to glean from this small set of documents. A
secondary search is issued to find additional docu-
ments that refer to the individual, or individuals.
Once we have the complete document retrieval,
the foundation for finding these types of events
lies in the Proteus information extraction compo-
nent (Grishman et al, 2005). We employ an IE sys-
tem trained for the tasks of the 2005 Automatic Con-
tent Extraction evaluation, which include entity and
event extraction. ACE defines a number of general
event types, including justice events, which cover in-
dictments, accusations, arrests, trials, and sentenc-
ings. The union of all these specific categories gives
us many of the salient events in a criminal justice
case from beginning to end. The program uses the
events, as well as the entities, to help identify the
passages that respond to the question.
The selection of sentences is based on the as-
sumption that the co-occurrence of the target indi-
vidual and a judicial event indicates that the target
is indeed involved in the event, but these two do not
necesssarily occur in the same sentence.
2 Related Work
A large body of work in question-answering has fol-
lowed from the opening of the Text Retrieval Con-
533
ference?s Q&A track in 1999. The task started as a
group of factoid questions and expanded from there
into more sophisticated problems. TREC provides
a unique testbed of question-answer pairs for re-
searchers and this data has been influential in fur-
thering progress.
In TREC 2006, there was a new secondary task
called ?complex, interactive Question Answering,?
(Dang et al, 2006) which is quite close to the GALE
problem, though it incorporated interaction to im-
prove results. Questions are posed in a canonical
form plus a narrative elaborating on the kind of in-
formation requested. An example question (from the
TREC guidelines) asks, ?What evidence is there for
transport of [drugs] from [Bonaire] to the [United
States]?? Our task is most similar to the fully-
automatic baseline runs of the track, which typically
took the form of passage retrieval with query ex-
pansion (Oard et al, 2006) or synonym processing
(Katz et al, 2006), and not the deeper processing
employed in this work.
Within the broader QA task, the other question
type is closest to the requirements in GALE, but it
is too open ended. In TREC, the input for other
questions is the name or description of the target,
and the response is supposed to be all information
that did not fit in the answers to the previous ques-
tions. While a few GALE questions have similar in-
put, most, including the prosecution questions, pro-
vide more detail about the topic in question.
A number of systems have used techniques in-
spired by information extraction. One of the top sys-
tems in the other questions category at the 2004 and
2005 evaluations generated lexical-syntactic pat-
terns and semantic patterns (Schone et al, 2004).
But they build these patterns from the question. In
our task, we took advantage of the structured ques-
tion format to make use of extensive work on the
semantics of selected domains. In this way we
hope to determine whether we can obtain better per-
formance by adding more sophisticated knowledge
about these domains. The Language Computer Cor-
poration (LCC) has long experimented with incorpo-
rating information extraction techniques. Recently,
in its system for the other type questions at TREC
2005, LCC developed search patterns for 33 target
classes (Harabagiu et al, 2005). These patterns were
learned with features from WordNet, stemming and
named entity recognition.
More and more systems are exploiting the size
and redundancy of the Web to help find answers.
Some obtain answers from the Web and then
project the answer back to the test corpus to find
a supporting document (Voorhees and Dang, 2005).
LCC used ?web boosting features? to add to key
words (Harabagiu et al, 2005). Rather than go to
the Web and enhance the question terms, we made
a beginning at examining the corpus for specific bits
of information, in this prototype, to determine alter-
native realizations of names.
3 Implementation
As stated above, the system takes a query in the
XML format required by the GALE program. The
query templates allow users to amplify their requests
by specifying a timeframe for the information and/or
a locale. In addition, there are provisions for en-
tering synonyms or alternate terms for either of the
main arguments, i.e. the accused and the crime, and
for related but less important terms.
Since this system is a prototype written especially
for the GALE evaluation in July 2006, we paid close
attention to the way example questions were given,
as well as to the evaluation corpus, which consisted
of more than 600,000 short news articles. The goal
in GALE was to offer comprehensive results to the
user, providing all snippets, or segments of texts,
that responded to the information request. This re-
quired us to develop a strategy that balanced pre-
cision against recall. A system that reported only
high-confidence answers was in danger of having no
answers or far fewer answers than other systems,
while a system that allowed lower confidence an-
swers risked producing answers with a great deal of
irrelevant material. Another way to look at this bal-
ancing act was that it was necessary for a system to
know when to quit. For this reason, we sought to
obtain a good estimate of the number of documents
we wanted to scan for answers.
Answer selection focused first on the name of the
suspect, which was always given in the query tem-
plate. In many of the training cases, the suspect was
in the news only because of a criminal charge against
him; and in most, the charge specified was the only
accusation reported in the news. Both location and
534
date constraints seemed to be largely superfluous,
and so we ignored these. But we did have a mecha-
nism for obtaining supplementary answers keyed to
the brief description of the crime and other related
words
The first step in the process is to request a seed
collection of 10 documents from the IR system.
This number was established experimentally. The
IR query combines terms tailored to the prosecution
template and the specific template parameters for a
particular question. The 10 documents returned are
then examined to produce a list of name variations
that substantially match the name as rendered in the
query template. The IR system is then asked for the
number of times that the name appears in the cor-
pus. This figure is adjusted by the frequency per
document in the seed collection and a new query is
submitted, set to obtain the N documents in which
we expect to find the target?s name.
3.1 Information Retrieval
The goal of the information retrieval component of
the system was to locate relevant documents that the
summarization system could then use to construct an
answer. All search, whether high-precision or high-
recall, was performed using the Indri retrieval sys-
tem 3 (Strohman et al, 2005).
Indri provides a powerful query language that
is used here to combine numerous aspects of the
query. The Indri query regarding Saddam Hus-
sein?s prosecution for crimes against humanity in-
cludes the following components: source restric-
tions, prosecution-related words, mentions of Sad-
dam Hussein, justice events, dependence model
phrases (Metzler and Croft, 2005) regarding the
crime, and a location constraint.
The first part of the query located references to
prosecutions by looking for the keywords prosecu-
tion, defense, trial, sentence, crime, guilty, or ac-
cuse, all of which were determined on training data
to occur in descriptions of prosecutions. These
words were important to have in documents for them
to be considered relevant, but the individual?s name
and the description of the crime were far more im-
portant (by a factor of almost 19 to 1).
The more heavily weighted part of the query,
3http://lemurproject.org/indri
then, was a ?justice event? marker found using in-
formation extraction (Section 3.2) and the more de-
tailed description of that event based on phrases ex-
tracted from the crime (here crimes against human-
ity). Those phrases give more probability of rele-
vance to documents that use more terms from the
crime. It also included a location constraint (here,
Iraq) that boosted documents referring to that lo-
cation. And it captured user-provided equivalent
words such as Saddam Hussein being a synonym for
former President of Iraq.
The most complex part of the query handled ref-
erences to the individual. The extraction system had
annotated all person names throughout the corpus.
We used the IR system to index all names across
all documents and used Indri to retrieve any name
forms that matched the individual. As a result, we
were able to find references to Saddam, Hussein,
and so on. This task could have also been accom-
plished with cross-document coreference technol-
ogy but our approach appeared to compensate for
incorrectly translated names slightly better than the
coreference system we had available at the time. For
example, Present rust Hussein was one odd form
that was matched by our simple approach.The final query looked like the following:
#filreq( #syn( #1(AFA).source ... #1(XIE).source )
#weight(
0.05 #combine( prosecution defense trial sentence
crime guilty accuse )
0.95 #combine(
#any:justice
#weight(1.0 #combine(humanity against crimes)
1.0 #combine(
#1(against humanity)
#1(crimes against)
#1(crimes against humanity))
1.0 #combine
#uw8(against humanity)
#uw8(crimes humanity)
#uw8(crimes against)
#uw12(crimes against humanity)))
Iraq
#syn( #1(saddam hussein)
#1(former president iraq))
#syn( #equals( entity 126180 ) ...))))
The actual query is much longer because it con-
tains 100 possible entities and numerous sources.
The processing is described in more detail else-
where (Kumaran and Allan, 2007).
3.2 Information Extraction
The Proteus system produces the full range of anno-
tations as specified for the ACE 2005 evaluation, in-
cluding entities, values, time expressions, relations,
535
and events. We focus here on the two annotations,
entities and events, most relevant to our question-
answering task. The general performance on entity
and event detection in news articles is within a few
percentage points of the top-ranking systems from
the evaluation.
The extraction engine identifies seven semantic
classes of entities mentioned in a document, of
which the most frequent are persons, organizations,
and GPE?s (geo-political entities ? roughly, regions
with a government). Each entity will have one or
more mentions in the document; these mentions in-
clude names, nouns and noun phrases, and pro-
nouns. Text processing begins with an HMM-based
named entity tagger, which identifies and classifies
the names in the document. Nominal and pronomi-
nal mentions are identified either with a chunker or
a full Penn-Treebank parser. A rule-based coref-
erence component identifies coreference relations,
forming entities from the mentions. Finally, a se-
mantic classifier assigns a class to each entity based
on the type of the first named mention (if the entity
includes a named mention) or the head of the first
nominal mention (using statistics gathered from the
ACE training corpus).
The ACE annotation guidelines specify 33 differ-
ent event subtypes, organized into 8 major types.
One of the major types is justice events, which in-
clude arrest, charge, trial, appeal, acquit, convict,
sentence, fine, execute, release, pardon, sue, and ex-
tradite subtypes. In parallel to entities, the event
tagger first identifies individual event mentions and
then uses event coreference to form events. For the
ACE evaluation, an annotated corpus of approxi-
mately 300,000 words is used to train the event tag-
ger.
For each event mention in the corpus, we collect
the trigger word (the main word indicating the event)
and a pattern recording the path from the trigger
to each event argument. These paths are recorded
in two forms: as the sequence of heads of maxi-
mal constituents between the trigger and the argu-
ment, and as the sequence of predicate-argument re-
lations connecting the trigger to the argument4 . In
4These predicate argument relations are based on a repre-
sentation called GLARF (Grammatical-Logical Argument Rep-
resentation Framework), which incorporates deep syntactic re-
lations and the argument roles from PropBank and NomBank.
addition, a set of maximum-entropy classifiers are
trained: to distinguish events from non-events, to
classify events by type and subtype, to distinguish
arguments from non-arguments, and to classify ar-
guments by argument role. In tagging new data, we
first match the context of each instance of a trig-
ger word against the collected patterns, thus iden-
tifying some arguments. The argument classifier is
then used to collect additional arguments within the
sentence. Finally, the event classifier (which uses
the proposed arguments as features) is used to re-
ject unlikely events. The patterns provide somewhat
more precise matching, while the argument classi-
fiers improve recall, yielding a tagger with better
performance than either strategy separately.
3.3 Answer Generation
Once the final batch of documents is received,
the answer generator module selects candidate pas-
sages. The names, with alternate renderings, are lo-
cated through the entity mentions by the IE system.
All sentences that contain a justice event and that
fall within a mention of a target by no more than
n sentences, where n is a settable parameter, which
was put at 5 for this evaluation, form the core of the
system?s answer.
The tactic takes the place of topic segmentation,
which we used for other question types in GALE
that did not have the benefit of the sophisticated
event recognition offered by the IE system. Segmen-
tation is used to give users sufficient context in the
answer without needing a means of identifying dif-
ficult definite nominal resolution cases that are not
handled by extraction.
In order to increase recall, in keeping with the
need for a comprehensive answer in the GALE eval-
uation, we added sentences that contain the name of
the target in documents that have justice events and
sentences that contain words describing the crime.
However, we imposed a limitation on the growth of
the answer size. When the target individual is well-
known, he or she will be mentioned in numerous
contexts, reducing the likelihood that this additional
mention will be relevant. Thus, when the size of the
answer grew too rapidly, we stopped including these
additional sentences, and produced sentences only
from the justice events. The threshold for triggering
this shift was 200 sentences.
536
3.4 Summarization
As a state-of-the-art baseline, we used a generic
multidocument summarization system that has been
tested in numerous contexts. It is, indeed, the
backup answer generator for several question types,
including the prosecution questions, in our GALE
system, and has been been tested in the topic-based
tasks of the 2005 and 2006 Document Understand-
ing Conferences.
A topic statement is formed by collapsing the
template arguments into one list, e.g., ?saddam hus-
sein crimes against humanity prosecution?, and the
answer generation module proceeds by using a hy-
brid approach that combines top-down strategies
based on syntactic patterns, alongside a suite of
summarization methods which guide content in a
bottom-up manner that clusters and combines the
candidate sentences (Blair-Goldensohn and McKe-
own, 2006).
4 Evaluation
The results of our evaluation are shown in Table 1.
We increased the number of test questions over the
number used in the official GALE evaluation and we
used only previously unseen questions. Documents
for the baseline system were selected without use of
the event annotations from Proteus.
We paired the 25 questions for judges, so that both
the system?s answer and the baseline answer were
assigned to the same person. We provided explicit
instructions on the handling on implicit references,
allowing the judges to use the context of the ques-
tion and other answer sentences to determine if a
sentence was relevant ? following the practice of the
GALE evaluation.
Our judges were randomly assigned questions
and asked whether the snippets, which in our case
were individual sentences, were relevant or not;
they could respond Relevant, Not Relevant or Don?t
Know. In cases where references were unclear, the
judges were asked to choose Don?t Know and these
were removed from the scoring.5
5In the GALE evaluation, the snippets are broken down by
hand into nuggets ? discrete pieces of information ? and the
answers are scored on that basis. However, we scored our re-
sponses on the basis of snippets (sentences) only, as it is much
more efficient, and therefore more feasible to repeat in the fu-
ture.
Our system using IE event detection and en-
tity tracking outperformed the summarization-based
baseline, with average precision of 68% compared
with 57%. Moreover, the specialized system sus-
tained that level of precision although it returned a
much larger number of snippets, totaling 2,086 over
the 25 questions, compared with 363 for the base-
line system. We computed a relative recall score, us-
ing the union of the sentences found by the systems
and judged relevant as the ground truth. For recall,
the specialized system scored an average 89% ver-
sus 17% for the baseline system. Computing an F-
measure weighting precision and recall equally, the
specialized system outperformed the baseline sys-
tem 75% to 23%. The difference in relative recall
and F-measure are both statisticaly significant under
a two-tailed, paired t-test, with p < 0.001.
5 Conclusion and Future Work
Our results show that the specialized system statis-
tically outperforms the baseline, a well-tested query
focused summarization approach, on precision. The
specialized system produced a much larger answer
on average (Table 1). Moreover, our answer gener-
ator seemed to adapt well to information in the cor-
pus. Of the six cases where it returned fewer than
10 sentences, the baseline found no additional sen-
tences four times (Questions B006, B011, B015 and
B022). We regard this as an important property in
the question-answering task.
A major challenge is to ascertain whether the
mention of the target is indeed involved in the rec-
ognized justice event. Our event recognition system
was developed within the ACE program and only
seeks to assigns roles within the local context of a
single sentence. We currently use a threshold to con-
sider whether an entity mention is reliable, but we
will experiment with ways to measure the likelihood
that a particular sentence is about the prosecution or
some other issue. We are planning to obtain vari-
ous pieces of information from additional secondary
queries to the search engine. Within the GALE pro-
gram, we are limited to the defined corpus, but in the
general case, we could add more varied resources.
In addition, we are working to produce answers
using text generation, to bring more sophisticated
summarization techniques to make a better presen-
537
QID System with IE Baseline System
Precision Recall F-meas Count Precision Recall F-meas Count
B001 0.728 0.905 0.807 92 0.818 0.122 0.212 11
B002 0.713 0.906 0.798 108 0.889 0.188 0.311 18
B003 0.770 0.942 0.848 148 0.875 0.058 0.109 8
B004 0.930 0.879 0.904 86 1.000 0.154 0.267 14
B005 0.706 0.923 0.800 34 0.400 0.231 0.293 15
B006 1.000 1.000 1.000 3 0.000 0.000 0.000 17
B007 0.507 1.000 0.673 73 0.421 0.216 0.286 19
B008 0.791 0.909 0.846 201 0.889 0.091 0.166 18
B009 0.759 0.960 0.848 158 0.941 0.128 0.225 17
B010 1.000 0.828 0.906 24 0.500 0.276 0.356 16
B011 0.500 1.000 0.667 6 0.000 0.000 0.000 18
B012 0.338 0.714 0.459 74 0.765 0.371 0.500 17
B013 0.375 0.900 0.529 120 0.700 0.280 0.400 20
B014 0.571 0.800 0.667 7 0.062 0.200 0.095 16
B015 0.500 1.000 0.667 2 0.000 0.000 0.000 10
B016 1.000 0.500 0.667 5 0.375 0.600 0.462 16
B017 1.000 1.000 1.000 13 0.125 0.077 0.095 7
B018 0.724 0.993 0.837 199 0.875 0.048 0.092 8
B019 0.617 0.954 0.749 201 0.684 0.100 0.174 19
B020 0.923 0.727 0.814 26 0.800 0.364 0.500 15
B021 0.562 0.968 0.711 162 0.818 0.096 0.171 11
B022 0.667 1.000 0.800 6 0.000 0.000 0.000 18
B023 0.684 0.950 0.795 196 0.778 0.050 0.093 9
B024 0.117 0.636 0.197 60 0.714 0.455 0.556 7
B025 0.610 0.943 0.741 82 0.722 0.245 0.366 18
Aver 0.684 0.893 0.749 83 0.566 0.174 0.229 14
Table 1: The table compares results of our answer generator combining the Indri and the Proteus ACE sys-
tem, against the focused-summarization baseline. This experiment is over 25 previously unseen questions.
The differences between the two systems are statistically significant (p < 0.001) for recall and f-measure by
a two-tailed, paired t-test. A big difference between the two systems is that the answer generator produces
a total of 2,086 answer sentences while sustaining an average precision of 0.684. In only three cases, does
the precision fall below 0.5. In contrast, the baseline system produced only 362, one-sixth the number of
answer sentences. While its average precision was not significantly worse than the answer-generator?s, its
precision varied widely, failing to find any correct sentences four times.
538
tation than an unordered list of sentences.
Finally, we will look into applying the techniques
used here on other topics. The first test would rea-
sonably be Conflict events, for which the ACE pro-
gram has training data. But ultimately, we would
like to adapt our system to arbitrary topic areas.
Acknowledgements
This material is based in part upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).
References
Sasha Blair-Goldensohn and Kathleen McKeown. 2006.Integrating rhetorical-semantic relation models forquery-focused summarization. In Proceedings of 6th
Document Understanding Conference (DUC2006).
Hoa Trang Dang, Jimmy Lin, and Diane Kelly. 2006.Overview of the TREC 2006 question answering track.In Proceedings TREC. Forthcoming.
Ralph Grishman, David Westbrook, and Adam Meyers.2005. NYU?s english ACE 2005 system descrip-tion. In ACE 05 Evaluation Workshop. On-line athttp://nlp.cs.nyu.edu/publication.
Sanda Harabagiu, Dan Moldovan, Christine Clark,Mitchell Bowden, Andrew Hickl, and Patrick Wang.2005. Employing two question answering systems in
TREC 2005. In Proceedings of the Fourteenth Text
Retrieval Conference.
B. Katz, G. Marton, G. Borchardt, A. Brownell,S. Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,
F. Mora, S. Stiller, O. Uzuner, and A. Wilcox.2006. External knowledge sources for question an-swering. In Proceedings of TREC. On-line at
http://www.trec.nist.gov.
Giridhar Kumaran and James Allan. 2007. Informationretrieval techniques for templated queries. In Proceed-
ings of RIAO. Forthcoming.
D. Metzler and W.B. Croft. 2005. A Markov random
field model for term dependencies. In Proceedings of
ACM SIGIR, pages 472?479.
D. Oard, T. Elsayed, J. Wang, Y. Wu, P. Zhang, E. Abels,
J. Lin, and D. Soergel. 2006. Trec 2006 at maryland:Blog, enterprise, legal and QA tracks. In Proceedings
of TREC. On-line at http://www.trec.nist.gov.
Patrick Schone, Gary Ciany, Paul McNamee, James
Mayeld, Tina Bassi, and Anita Kulman. 2004. Ques-tion answering with QACTIS at TREC-2004. In Pro-
ceedings of the Thirteenth Text Retrieval Conference.
T. Strohman, D. Metzler, H. Turtle, and W.B. Croft.
2005. Indri: A language-model based search enginefor complex queries (extended version). Technical Re-port IR-407, CIIR, UMass Amherst.
Ellen M. Voorhees and Hoa Trang Dang. 2005.
Overview of the TREC 2005 question answering track.In Proceedings of the Fourteenth Text Retrieval Con-
ference.
539
Producing Biographical Summaries: Combining Linguistic
Knowledge with Corpus Statistics1
Barry Schiffman
Columbia University
1214 Amsterdam Avenue
New York, NY 10027, USA
Bschiff@cs.columbia.edu
Inderjeet Mani2
The MITRE Corporation
11493 Sunset Hills Road
Reston, VA 20190, USA
imani@mitre.org
Kristian J. Concepcion
The MITRE Corporation
11493 Sunset Hills Road
Reston, VA 20190, USA
kjc9@mitre.org
                                                       
1
 This work has been funded by DARPA?s Translingual Information Detection, Extraction, and Summarization (TIDES)
research program, under contract number DAA-B07-99-C-C201 and ARPA Order H049.
2
 Also at the Department of Linguistics, Georgetown University, Washington, D. C. 20037.
Abstract
We describe a biographical multi-
document summarizer that summarizes
information about people described in
the news.  The summarizer uses corpus
statistics along with linguistic
knowledge to select and merge
descriptions of people from a document
collection, removing redundant
descriptions. The summarization
components have been extensively
evaluated for coherence, accuracy, and
non-redundancy of the descriptions
produced.
1 Introduction
The explosion of the World Wide Web has
brought with it a vast hoard of information, most
of it relatively unstructured. This has created a
demand for new ways of managing this often
unwieldy body of dynamically changing
information. The goal of automatic text
summarization is to take a partially-structured
source text, extract information content from it,
and present the most important content in a
condensed form in a manner sensitive to the
needs of the user and task (Mani and Maybury
1999). Summaries can be ?generic?, i.e., aimed
at a broad audience, or topic-focused, i.e.,
tailored to the requirements of a particular user
or group of users. Multi-Document
Summarization (MDS) is, by definition, the
extension of single-document summarization to
collections of related documents. MDS can
potentially help the user to see at a glance what a
collection is about, or to examine similarities
and differences in the information content in the
collection.
Specialized multi-document
summarization systems can be constructed for
various applications; here we discuss a
biographical summarizer. Biographies can, of
course, be long, as in book-length biographies,
or short, as in an author?s description on a book
jacket. The nature of descriptions in the
biography can vary, from physical
characteristics (e.g., for criminal suspects) to
scientific or other achievements (e.g., a
speaker?s biography). The crucial point here is
that facts about a person?s life are selected,
organized, and presented so as to meet the
compression and task requirements.
 While book-quality biographies are out
of reach of computers, many other kinds can be
synthesized by sifting through large quantities of
on-line information, a task that is tedious for
humans to carry out. We report here on the
development of a biographical MDS summarizer
that summarizes information about people
described in the news. Such a summarizer is of
interest, for example, to analysts who want to
automatically construct a dossier about a person
over time.
Rather than determining in advance
what sort of information should go into a
biography, our approach is more data-driven,
relying on discovering how people are actually
described in news reports in a collection. We use
corpus statistics from a background corpus along
with linguistic knowledge to select and merge
descriptions from a document collection,
removing redundant descriptions. The focus here
is on synthesizing succinct descriptions.  The
problem of assembling these descriptions into a
coherent narrative is not a focus of our paper;
the system currently uses canned text methods to
produce output text containing these
descriptions. Obviously, the merging of
descriptions should take temporal information
into account; this very challenging issue is also
not addressed here.
To give a clearer idea of the system?s output,
here are some examples of biographies produced
by our system (the descriptions themselves are
underlined, the rest is canned text). The
biographies contain descriptions of the salient
attributes and activities of people in the corpus,
along with lists of their associates. These short
summaries illustrate the extent of compression
provided. The first two summaries are of a
collection of 1300 wire service news documents
on the Clinton impeachment proceedings
(707,000 words in all, called the ?Clinton?
corpus). In this corpus, there are 607 sentences
mentioning Vernon Jordan by name, from which
the system extracted 82 descriptions expressed
as appositives (78) and relative clauses (4),
along with 65 descriptions consisting of
sentences whose deep subject is Jordan. The 4
relative clauses are duplicates of one another:
?who helped Lewinsky find a job?.  The 78
appositives fall into just 2 groups: ?friend? (or
equivalent descriptions, such as ?confidant?),
?adviser? (or equivalent such as ?lawyer?). The
sentential descriptions are filtered in part based
on the presence of verbs like ?testify, ?plead?, or
?greet? that are strongly associated with the
head noun of the appositive, namely ?friend?.
The target length can be varied to produce
longer summaries.
Vernon Jordan is a presidential friend and a
Clinton adviser. He is 63 years old. He helped
Ms. Lewinsky find a job. He testified  that Ms.
Monica Lewinsky said  that she had
conversations  with the president,  that she
talked  to the president. He has numerous
acquaintances, including Susan Collins, Betty
Currie, Pete Domenici, Bob Graham,  James
Jeffords and Linda Tripp.
1,300 docs, 707,000 words (Clinton corpus) 607
Jordan sentences, 78 extracted appositives, 2
groups: friend, adviser.
Henry Hyde is a Republican chairman of House
Judiciary Committee and a prosecutor in Senate
impeachment trial. He will lead the Judiciary
Committee's impeachment review. Hyde urged
his colleagues  to heed  their consciences ,  ?the
voice  that whispers  in our ear ,  ?duty,  duty,
duty.??
Clinton corpus, 503 Hyde sentences, 108
extracted appositives, 2 groups: chairman,
impeachment prosecutor.
Victor Polay  is the Tupac Amaru rebels' top
leader,  founder and the organization's
commander-and-chief. He was arrested  again
in  1992  and is serving  a life sentence. His
associates include  Alberto Fujimori, Tupac
Amaru Revolutionary, and Nestor Cerpa.
73 docs, 38,000 words, 24 Polay sentences, 10
extracted appositives, 3 groups: leader, founder
and commander-in-chief.   
2 Producing biographical descriptions
2.1 Preprocessing
Each document in the collection to be
summarized is processed by a sentence
tokenizer, the Alembic part-of-speech tagger
(Aberdeen et al 1995), the Nametag named
entity tagger  (Krupka 1995) restricted to people
names, and the CASS parser (Abney 1996).  The
tagged sentences are further analyzed by a
cascade of finite state machines leveraging
patterns with lexical and syntactic information,
to identify constructions such as pre- and post-
modifying appositive phrases, e.g., ?Presidential
candidate George Bush?, ?Bush, the presidential
candidate?, and relative clauses, e.g., ?Senator
..., who is running for re-election this Fall,?.
These appositive phrases and relative clauses
capture descriptive information which can
correspond variously to a person?s age,
occupation, or some role a person played in an
incident. In addition, we also extract sentential
descriptions in the form of sentences whose
(deep) subjects are person names.
2.2 Cross-document coreference
The classes of person names identified within
each document are then merged across
documents in the collection using a cross-
document coreference program from the
Automatic Content Extraction (ACE) research
program (ACE 2000), which compares names
across documents based on similarity of a
window of words surrounding each name, as
well as specific rules having to do with different
ways of abbreviating a person?s name (Mani and
MacMillan 1995). The end result of this process
is that for each distinct person, the set of
descriptions found for that person in the
collection are grouped together.
2.3 Appositives
2.3.1 Introduction
The appositive phrases usually provide
descriptions of attributes of a person. However,
the preprocessing component described in
Section 2.1 does produce errors in appositive
extraction, which are filtered out by syntactic
and semantic tests. The system also filters out
redundant descriptions, both duplicate
descriptions as well as similar ones. These
filtering methods are discussed next.
2.3.2 Pruning Erroneous  and Duplicate
Appositives
The appositive descriptions are first pruned to
record only one instance of an appositive phrase
which has multiple repetitions, and descriptions
whose head does not appear to refer to a person.
The latter test relies on a person typing program
which uses semantic information from WordNet
1.6 (Miller 1995) to test whether the head of the
description is a person. A given string is judged
as a person if a threshold percentage ?1  (set to
35% in our work) of senses of the string are
descended from the synset for Person in
WordNet. For example, this picks out ?counsel?
as a person, but ?accessory? as a non-person.
2.3.3 Merging Similar Appositives
The pruning of erroneous and duplicate
descriptions still leaves a large number of
redundant appositive descriptions across
documents. The system compares each pair of
appositive descriptions of a person, merging
them based on corpus frequencies of the
description head stem, syntactic information,
and semantic information based on the
relationship between the heads in WordNet. The
descriptions are merged if they have the same
head stem, or if both heads have a common
parent below Person in WordNet (in the latter
case the head which is more frequent in the
corpus is chosen as the merged head), or if one
head subsumes the other under Person in
WordNet (in which case the more general head
is chosen).
When the heads of descriptions are
merged, the most frequent modifying phrase that
appears in the corpus with the selected head is
used. When a person ends up with more than
one description, the modifiers are checked for
duplication, with distinct modifiers being
conjoined together, so that ?Wisconsin
lawmaker? and ?Wisconsin democrat? yields
?Wisconsin lawmaker and Democrat?.
Prepositional phrase variants of descriptions are
also merged here, so that ?chairman of the
Budget Committee? and ?Budget Committee
Chairman? are merged. Modifiers are dropped
but their original order is preserved for the sake
of fluency.
2.3.4 Appositive Description Weighting
The system then weights the appositives for
inclusion in a summary. A person?s appositives
are grouped into equivalence classes, with a
single head noun being chosen for each
equivalence class, with a weight for that class
based on the corpus frequency of the head noun.
The system then picks descriptions in decreasing
order of class weight until either the
compression rate is achieved or the head noun is
no longer in the top ?2 % most frequent
descriptions (?2 is set to 90% in our work). Note
that the summarizer refrains from choosing a
subsuming term from WordNet that is not
present in the descriptions, preferring to not risk
inventing new descriptions, instead confining
itself to cutting and pasting of actual words used
in the document.
2.4 Relative Clause Weighting
Once the relative clauses have been pruned for
duplicates, the system weights the appositive
clauses for inclusion in a summary. The
weighting is based on how often the relative
clause?s main verb is strongly associated with a
(deep) subject in a large corpus, compared to its
total number of appearances in the corpus. The
idea here is to weed out ?promiscuous? verbs
that are weakly associated with lots of subjects.
The corpus statistics are derived from the
Reuters portion of the North American News
Text Corpus (called ?Reuters? in this paper) --
nearly three years of wire service news reports
containing 105.5 million words.
Examples of verbs in the Reuters corpus
which show up as promiscuous include ?get?,
?like?, ?give?, ?intend?, ?add?, ?want?, ?be?,
?do?, ?hope?, ?think?, ?make?, ?dream?,
?have?, ?say?, ?see?, ?tell?, ?try?. In a test,
detailed below in Section 4.2, this feature fired
40 times in 184 trials.
To compute strong associations, we
proceed as follows. First, all subject-verb pairs
are extracted from the Reuters corpus with a
specially developed finite state grammar and the
CASS parser. The head nouns and main verbs
are reduced to their base forms by changing
plural endings and tense markers for the verbs.
Also included are ?gapped? subjects, such as the
subject of ?run? in ?the student promised to run
the experiment?; in this example, both pairs
?student-promise? and ?student-run? are
recorded. Passive constructions are also
recognized and the object of the by-PP
following the verb is taken as the deep subject.
Strength of association between subject i and
verb j is measured using mutual information
(Church and Hanks 1990):
)ln(),(
ji
ij
tftf
tfNjiMI
?
?
= .
Here tfij is the maximum frequency of
subject-verb pair ij in the Reuters corpus, tfi is
the frequency of subject head noun i in the
corpus, tfj is the frequency of verb j in the
corpus, and N is the number of terms in the
corpus. The associations are only scored for tf
counts greater than 4, and a threshold ?3  (set to
log score > -21 in our work) is used for a strong
association.
The relative clauses are thus filtered
initially (Filter 1) by excluding those whose
main verbs are highly promiscuous.  Next, they
are filtered (Filter 2) based on various syntactic
features, as well as the number of proper names
and pronouns. Finally, the relative clauses are
scored conventionally (Filter 3) by summing the
within-document relative term frequency of
content terms in the clause (i.e., relative to the
number of terms in the document), with an
adjustment for sentence length (achieved by
dividing by the total number of content terms in
the clause).
3 Sentential Descriptions
These descriptions are the relatively large set of
sentences which have a person name as a (deep)
subject. We filter them based on whether their
main verb is strongly associated with either of
the head nouns of the appositive descriptions
found for that person name (Filter 4). The
intuition here is that particular occupational
roles will be strongly associated with particular
verbs. For example, politicians vote and elect,
executives resign and appoint, police arrest and
shoot; so, a summary of information about a
policeman may include an arresting and
shooting event he was involved with. (The verb-
occupation association isn?t manifest in relative
clauses because the latter are too few in
number).
A portion of the results of doing this is
shown in Table 1.  The results for ?executive?
are somewhat loose, whereas for ?politician?
and ?police?, the associations seem tighter, with
the associated verbs meeting our intuitions.
All sentences which survive Filter 4 are
extracted and then scored, just as relative clauses
are, using Filter 1 and Filter 3. Filter 4 alone
provides a high degree of compression; for
example, it reduces a total of 16,000 words in
the combined sentences that include Vernon
Jordan' s name in the Clinton corpus to 578
words in 12 sentences; sentences up to the target
length can be selected from these based on
scores from Filter 1 and then Filter 3.
However, there are several difficulties with
these sentences. First, we are missing a lot of
them due to the fact that we do not as yet handle
pronominal subjects which are coreferential with
the proper name. Second, these sentences
contain lots of dangling anaphors, which will
need to be resolved. Third, there may be
redundancy between the sentential descriptions,
on one hand, and the appositive and relative
clause descriptions, on the other. Finally, the
entire sentence is extracted, including any
subordinate clauses, although we are working on
refinements involving sentence compaction. As
a result, we believe that more work is required
before the sentential descriptions can be fully
integrated into the biographies.
executive police politician
reprimand
16.36 shoot 17.37 clamor 16.94
conceal 17.46 raid 17.65 jockey 17.53
bank 18.27 arrest 17.96 wrangle 17.59
foresee 18.85 detain 18.04 woo 18.92
conspire 18.91 disperse 18.14 exploit 19.57
convene 19.69 interrogate18.36 brand 19.65
plead 19.83 swoop 18.44 behave 19.72
sue 19.85 evict 18.46 dare 19.73
answer 20.02 bundle 18.50 sway 19.77
commit 20.04 manhandle18.59 criticize 19.78
worry 20.04 search 18.60 flank 19.87
accompany
20.11
confiscate
18.63
proclaim
19.91
own 20.22 apprehend18.71 annul 19.91
witness 20.28 round 18.78 favor 19.92
testify 20.40 corner 18.80 denounce20.09
shift 20.42 pounce 18.81 condemn20.10
target 20.56 hustle 18.83 prefer 20.14
lie 20.58 nab 18.83 wonder 20.18
expand 20.65 storm 18.90 dispute 20.18
learn 20.73 tear 19.00 interfere 20.37
shut 20.80 overpower19.09 voice 20.38
Table 1. Verbs strongly associated with
particular classes of people in the Reuters
corpus (negative log scores).
4 Evaluation
4.1 Overview
Methods for evaluating text summarization can
be broadly classified into two categories
(Sparck-Jones and Galliers 1996). The first, an
extrinsic evaluation, tests the summarization
based on how it affects the completion of some
other task, such as comprehension, e.g., (Morris
et al 1992), or relevance assessment (Brandow
et al 1995) (Jing et al 1998) (Tombros and
Sanderson 1998) (Mani et al 1998). An intrinsic
evaluation, on the other hand, can involve
assessing the coherence of the summary
(Brandow et al 1995) (Saggion and Lapalme
2000).
Another intrinsic approach involves
assessing the informativeness of the summary,
based on to what extent key information from
the source is preserved in the system summary at
different levels of compression (Paice and Jones
1993), (Brandow et al 1995). Informativeness
can also be assessed in terms of how much
information in an ideal (or ?reference?) summary
is preserved in the system summary, where the
summaries being compared are at similar levels
of compression  (Edmundson 1969).
We have carried out a number of intrinsic
evaluations of the accuracy of components
involved in the summarization process, as well
as the succinctness, coherence and
informativeness of the descriptions. As this is a
MDS system, we also evaluate the non-
redundancy of the descriptions, since similar
information may be repeated across documents.
4.2 Person Typing Evaluation
The component evaluation tests how accurately
the tagger can identify whether a head noun in a
description is appropriate as a person description
The evaluation uses the WordNet 1.6 SEMCOR
semantic concordance, which has files from the
Brown corpus whose words have semantic tags
(created by WordNet' s creators) indicating
WordNet sense numbers. Evaluation on 6,000
sentences with almost 42,000 nouns compares
people tags generated by the program with
SEMCOR tags, and provided the following
results: right = 41,555, wrong = 1,298, missing
= 0, yielding Precision, Recall, and F-Measure
of 0.97.
4.3 Relative Clause Extraction Evaluation
This component evaluation tests the well-
formedness of the extracted relative clauses. For
this evaluation, we used the Clinton corpus. The
relative clause is judged correct if it has the right
extent, and the correct coreference index
indicating which person the relative clause
description pertains to. The judgments are based
on 36 instances of relative clauses from 22
documents. The results show 28 correct relative
clauses found, plus 4 spurious finds, yielding
Precision of 0.87, Recall of 0.78, and F-measure
of .82. Although the sample is small, the results
are very promising.
4.4 Appositive Merging Evaluation
This component evaluation tests the system?s
ability to accurately merge appositive
descriptions. The score is based on an automatic
comparison of the system?s merge of system-
generated appositive descriptions against a
human merge of them. We took all the names
that were identified in the Clinton corpus and
ran the system on each document in the corpus.
We took the raw descriptions that the system
produced before merging, and wrote a brief
description by hand for each person who had
two or more raw descriptions. The hand-written
descriptions were not done with any reference to
the automatically merged descriptions nor with
any reference to the underlying source material.
The hand-written descriptions were then
compared with the final output of the system
(i.e., the result after merging). The comparison
was automatic, measuring similarity among
vectors of content words (i.e., stop words such
as articles and prepositions were removed).
Here is an example to further clarify the
strict standard of the automatic evaluation
(words scored correct are underlined):
System: E. Lawrence Barcella is a Washington
lawyer, Washington white-collar defense lawyer,
former federal prosecutor
System Merge: Washington white-collar defense
lawyer
Human Merge: a Washington lawyer and former
federal prosecutor
Automatic Score: Correct=2; Extra-Words=2;
Missed-Words=3
Thus, although ?lawyer? and
?prosecutor? are synonymous in WordNet, the
automatic scorer doesn?t know that, and so
?prosecutor? is penalized as an extra word.
The evaluation was carried out over the
entire Clinton corpus, with descriptions
compared for 226 people who had more than
one description. 65 out of the 226 descriptions
were Correct (28%), with a further 32 cases
being semantically correct ?obviously similar?
substitutions which the automatic scorer missed
(giving an adjusted accuracy of 42%). As a
baseline, a merging program which performed
just a string match scored 21% accuracy. The
major problem areas were errors in coreference
(e.g., Clinton family members being put in the
same coreference class), lack of good
descriptions for famous people (news articles
tend not to introduce such people), and parsing
limitations (e.g., ?Senator Clinton? being parsed
erroneously as an NP in ?The Senator Clinton
disappointed??). Ultimately, of course,
domain-independent systems like ours are
limited semantically in merging by the lack of
world knowledge, e.g., knowing that Starr' s
chief lieutenant can be a prosecutor.
4.5 Description Coherence and
Informativeness Evaluation
To assess the coherence and informativeness of
the relative clause descriptions3, we asked  4
subjects who were unaware of our research to
judge descriptions generated by our system from
the Clinton corpus. For each relative clause
description, the subject was given the
description, a person name to whom that
description pertained, and a capsule description
consisting of merged appositives created by the
system. The subject was asked to assess (a) the
coherence of the relative clause description in
terms of its succinctness (was it a good length?)
and its comprehensibility (was it and
understandable by itself or in conjunction with
the capsule?), and (b) its informativeness in
terms of whether it was an accurate description
(does it conflict with the capsule or with what
you know?) and whether it was non-redundant
(is it distinct or does it repeat what is in the
capsule?).
 The subjects marked 87% of the
descriptions as accurate, 96% as non-redundant,
and 65% as coherent. A separate 3-subject inter-
                                                       
3
 Appositives are not assessed in this way as few errors of
coherence or informativeness were noticed in the
appositive extraction.
annotator agreement study, where all subjects
judged the same 46 decisions, showed that all
three subjects agreed on 82% of the accuracy
decisions, 85% of the non-redundancy decisions
and 82% of the coherence decisions.
5 Learning  to Produce Coherent
Descriptions
5.1 Overview
To learn rules for coherence for extracting
sentential descriptions, we used the examples
and judgments we obtained for coherence in the
evaluation of relative clause descriptions in
Section 4.5. Our focus was on features that
might relate to content and specificity: low verb
promiscuity scores, presence of proper names,
pronouns, definite and indefinite clauses. The
entire list is as follows:
badend:
boolean. is there an impossible
end, indicating a bad extraction (
... Mr.)?
bestverb:
continuous. use the verb
promiscuity threshhold ?3 to
find the score of the most non-
promiscuous verb in the clause
classes
(label):
boolean. accept the clause,
reject the clause
count
pronouns:
continuous. number of personal
pronouns
count
proper:
continuous. number of nouns
tagged as NP
hasobject: continuous. how many np'sfollow the verb?
haspeople: continuous. how many "name"
constituents are found?
has
possessive:
continuous. how many
possessive pronouns are there?
hasquote: boolean. is there a quotation?
hassubc: boolean. is there a subordinate
clause?
isdefinite: continuous. how many definiteNP's are there?
repeater: boolean. is the subject's name
repeated, or is there no subject?
timeref: boolean. is there a time
reference?
withquit: is there a ?quit? or ?resign?
verb?
withsay: boolean. is there a ?say? verb inthe clause?
5.2 Accuracy of Learnt Descriptions
Table 2 provides information on different
learning methods. The results are for a ten-fold
cross-validation on 165 training vectors and 19
test vectors, measured in terms of Predictive
Accuracy (percentage test vectors correctly
classified).
Tool Accuracy
Barry?s Rules .69
MC4 Decision Tree .69
C4.5Rules .67
Ripper .62
Naive Bayes .62
Majority Class (coherent) .60
Table 2.  Accuracy of Different Description
Learners on Clinton corpus
The best learning methods are comparable
with rules created by hand by one of the authors
(Barry?s rules). In the learners, the bestverb
feature is used heavily in tests for the negative
class, whereas in Barry?s Rules it occurs in tests
for the positive class.
6 Related Work
Our work on measuring subject-verb
associations has a different focus from the
previous work. (Lee and Pereira 1999), for
example, examined verb-object pairs. Their
focus was on a method that would improve
techniques for gathering statistics where there
are a multitude of sparse examples. We are
focusing on the use of the verbs for the specific
purpose of finding associations that we have
previously observed to be strong, with a view
towards selecting a clause or sentence, rather
than just to measure similarity. We also try to
strengthen the numbers by dealing with ?gapped?
constructions.
While there has been plenty of work on
extracting named entities and relations between
them, e.g., (MUC-7 1998), the main previous
body of work on biographical summarization is
that of (Radev and McKeown 1998). The
fundamental differences in our work are as
follows: (1) We extract not only appositive
phrases, but also clauses at large based on
corpus statistics; (2) We make heavy use of
coreference, whereas they don?t use coreference
at all; (3) We focus on generating succinct
descriptions by removing redundancy and
merging, whereas they categorize descriptions
using WordNet, without a focus on succinctness.
7 Conclusion
This research has described and evaluated
techniques for producing a novel kind of
summary called biographical summaries. The
techniques use syntactic analysis and semantic
type-checking (from WordNet), in combination
with a variety of corpus statistics. Future
directions could include improved sentential
descriptions as well as further intrinsic and
extrinsic evaluations of the summarizer as a
whole (i.e., including canned text).
References
 J. Aberdeen, J. Burger, D. Day, L. Hirschman,
P. Robinson, and M. Vilain. 1995. ?MITRE:
Description of the Alembic System system as used
for MUC-6?. In Proceedings of the Sixth Message
Understanding Conference (MUC-6), Columbia,
Maryland.
 S. Abney. 1996. ?Partial parsing Via Finite-State
Cascades?. Proceedings of the ESSLLI '96 Robust
Parsing Workshop.
Automatic Context Extraction Program.
http://www.nist.gov/speech/tests/ace/index.htm
R. Brandow, K. Mitze, and L. Rau. 1995. ?Automatic
condensation of electronic publications by
sentence selection.? Information Processing and
Management 31(5): 675-685. Reprinted in
Advances in Automatic Text Summarization, I.
Mani and M.T. Maybury (eds.), 293-303.
Cambridge, Massachusetts: MIT Press.
K. W. Church and P. Hanks. 1990. ?Word association
norms, mutual information, and lexicography?.
Computational Linguistics 16(1): 22-29.
H. P. Edmundson. 1969. ?New methods in automatic
abstracting?.  Journal of the Association for
Computing Machinery 16 (2): 264-285. Reprinted
in Advances in Automatic Text Summarization, I.
Mani and M.T. Maybury (eds.), 21-42.
Cambridge, Massachusetts: MIT Press.
G. Krupka. 1995. ?SRA: Description of the SRA
system as used for MUC-6?. In Proceedings of the
Sixth Message Understanding Conference (MUC-
6), Columbia, Maryland.
L. Lee and F. Pereira. 1999. ?Distributional
Similarity Models: Clustering vs. Nearest
Neighbors?. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, 33-40.
I. Mani and T. MacMillan. 1995. ?Identifying
Unknown Proper Names in Newswire Text?. In
Corpus Processing for Lexical Acquisition, B.
Boguraev and J. Pustejovsky (eds.), 41-73.
Cambridge, Massachusetts: MIT Press.
I. Mani and M. T. Maybury. (eds.). 1999. Advances
in Automatic Text Summarization. Cambridge,
Massachusetts: MIT Press.
G. Miller. 1995. ?WordNet: A Lexical Database for
English?. Communications of the Association For
Computing Machinery (CACM) 38(11): 39-41.
A. Morris, G. Kasper, and D. Adams. 1992. ?The
Effects and Limitations of Automatic Text
Condensing on Reading Comprehension
Performance?. Information Systems Research 3(1):
17-35. Reprinted in Advances in Automatic Text
Summarization, I. Mani and M.T. Maybury (eds.),
305-323. Cambridge, Massachusetts: MIT Press.
 MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference, DARPA.
C. D. Paice and P. A. Jones. 1993. ?The
Identification of Important Concepts in Highly
Structured Technical Papers.? In Proceedings of
the 16th International Conference on Research
and Development in Information Retrieval
(SIGIR'93), 69-78.
D. R. Radev and K. McKeown. 1998. ?Generating
Natural Language Summaries from Multiple On-
Line Sources?. Computational Linguistics 24(3):
469-500.
H. Saggion and G. Lapalme. 2000. ?Concept
Identification and Presentation in the Context of
Technical Text Summarization?. In Proceedings of
the Workshop on Automatic Summarization, 1-10.
K. Sparck-Jones and J. Galliers. 1996. Evaluating
Natural Language Processing Systems: An
Analysis and Review.  Lecture Notes in Artificial
Intelligence 1083. Berlin: Springer.
A. Tombros and M. Sanderson. 1998.?Advantages of
query biased summaries in information retrieval?.
In Proceedings of the 21st International
Conference on Research and Development in
Information Retrieval (SIGIR'98), 2-10.
