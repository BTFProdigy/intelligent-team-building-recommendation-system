Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 44?52, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems
Lushan Han, Abhay Kashyap, Tim Finin
Computer Science and
Electrical Engineering
University of Maryland, Baltimore County
Baltimore MD 21250
{lushan1,abhay1,finin}@umbc.edu
James Mayfield and Jonathan Weese
Human Language Technology
Center of Excellence
Johns Hopkins University
Baltimore MD 21211
mayfield@jhu.edu, jonny@cs.jhu.edu
Abstract
We describe three semantic text similarity
systems developed for the *SEM 2013 STS
shared task and the results of the correspond-
ing three runs. All of them shared a word sim-
ilarity feature that combined LSA word sim-
ilarity and WordNet knowledge. The first,
which achieved the best mean score of the 89
submitted runs, used a simple term alignment
algorithm augmented with penalty terms. The
other two runs, ranked second and fourth, used
support vector regression models to combine
larger sets of features.
1 Introduction
Measuring semantic text similarity has been a re-
search subject in natural language processing, infor-
mation retrieval and artificial intelligence for many
years. Previous efforts have focused on compar-
ing two long texts (e.g., for document classification)
or a short text with a long text (e.g., Web search),
but there are a growing number of tasks requiring
computing the semantic similarity between two sen-
tences or other short text sequences. They include
paraphrase recognition (Dolan et al, 2004), Twitter
tweets search (Sriram et al, 2010), image retrieval
by captions (Coelho et al, 2004), query reformula-
tion (Metzler et al, 2007), automatic machine trans-
lation evaluation (Kauchak and Barzilay, 2006) and
schema matching (Han et al, 2012).
There are three predominant approaches to com-
puting short text similarity. The first uses informa-
tion retrieval?s vector space model (Meadow, 1992)
in which each text is modeled as a ?bag of words?
and represented using a vector. The similarity be-
tween two texts is then computed as the cosine
similarity of the vectors. A variation on this ap-
proach leverages web search results (e.g., snip-
pets) to provide context for the short texts and en-
rich their vectors using the words in the snippets
(Sahami and Heilman, 2006). The second approach
is based on the assumption that if two sentences or
other short text sequences are semantically equiva-
lent, we should be able to align their words or ex-
pressions. The alignment quality can serve as a
similarity measure. This technique typically pairs
words from the two texts by maximizing the sum-
mation of the word similarity of the resulting pairs
(Mihalcea et al, 2006). The third approach com-
bines different measures and features using machine
learning models. Lexical, semantic and syntactic
features are computed for the texts using a variety
of resources and supplied to a classifier, which then
assigns weights to the features by fitting the model
to training data (Saric et al, 2012).
For evaluating different approaches, the 2013 Se-
mantic Textual Similarity (STS) task asked auto-
matic systems to compute sentence similarity ac-
cording to a scale definition ranging from 0 to 5,
with 0 meaning unrelated and 5 semantically equiv-
alent (Agirre et al, 2012; Agirre et al, 2013). The
example sentence pair ?The woman is playing the
violin? and ?The young lady enjoys listening to the
guitar? is scored as only 1 and the pair ?The bird is
bathing in the sink? and ?Birdie is washing itself in
the water basin? is given a score of 5.
The vector-space approach tends to be too shallow
for the task, since solving it well requires discrimi-
nating word-level semantic differences and goes be-
44
yond simply comparing sentence topics or contexts.
Our first run uses an align-and-penalize algorithm,
which extends the second approach by giving penal-
ties to the words that are poorly aligned. Our other
two runs use a support vector regression model to
combine a large number of general and domain spe-
cific features. An important and fundamental feature
used by all three runs is a powerful semantic word
similarity model based on a combination of Latent
Semantic Analysis (LSA) (Deerwester et al, 1990;
Landauer and Dumais, 1997) and knowledge from
WordNet (Miller, 1995).
The remainder of the paper proceeds as follows.
Section 2 presents the hybrid word similarity model.
Section 3 describes the align-and-penalize approach
used for the PairingWords run. In Section 4 we de-
scribe the SVM approach used for the Galactus and
Saiyan runs. Section 5 discusses the results and is
followed by a short conclusion.
2 Semantic Word Similarity Model
Our word similarity model was originally developed
for the Graph of Relations project (UMBC, 2013a)
which maps informal queries with English words
and phrases for an RDF linked data collection into
a SPARQL query. For this, we wanted a metric
in which only the semantics of a word is consid-
ered and not its lexical category. For example, the
verb ?marry? should be semantically similar to the
noun ?wife?. Another desiderata was that the met-
ric should give highest scores and lowest scores in
its range to similar and non-similar words, respec-
tively. In this section, we describe how we con-
structed the model by combining LSA word simi-
larity and WordNet knowledge.
2.1 LSA Word Similarity
LSA Word Similarity relies on the distributional hy-
pothesis that words occurring in the same contexts
tend to have similar meanings (Harris, 1968).
2.1.1 Corpus Selection and Processing
In order to produce a reliable word co-occurrence
statistics, a very large and balanced text corpus is
required. After experimenting with several cor-
pus choices including Wikipedia, Project Gutenberg
e-Books (Hart, 1997), ukWaC (Baroni et al, 2009),
Reuters News stories (Rose et al, 2002) and LDC
gigawords, we selected the Web corpus from the
Stanford WebBase project (Stanford, 2001). We
used the February 2007 crawl, which is one of the
largest collections and contains 100 million web
pages from more than 50,000 websites. The Web-
Base project did an excellent job in extracting tex-
tual content from HTML tags but still has abun-
dant text duplications, truncated text, non-English
text and strange characters. We processed the collec-
tion to remove undesired sections and produce high
quality English paragraphs. We detected paragraphs
using heuristic rules and only retrained those whose
length was at least two hundred characters. We elim-
inated non-English text by checking the first twenty
words of a paragraph to see if they were valid En-
glish words. We used the percentage of punctuation
characters in a paragraph as a simple check for typi-
cal text. We removed duplicated paragraphs using a
hash table. Finally, we obtained a three billion words
corpus of good quality English, which is available at
(Han and Finin, 2013).
2.1.2 Word Co-Occurrence Generation
We performed POS tagging and lemmatization on
the WebBase corpus using the Stanford POS tagger
(Toutanova et al, 2000). Word/term co-occurrences
are counted in a moving window of a fixed size
that scans the entire corpus1. We generated two co-
occurrence models using window sizes ?1 and ?4
because we observed different natures of the models.
?1 window produces a context similar to the depen-
dency context used in (Lin, 1998a). It provides a
more precise context but only works for comparing
words within the same POS. In contrast, a context
window of ?4 words allows us to compute semantic
similarity between words with different POS.
Our word co-occurrence models were based on
a predefined vocabulary of more than 22,000 com-
mon English words and noun phrases. We also
added to it more than 2,000 verb phrases extracted
from WordNet. The final dimensions of our word
co-occurrence matrices are 29,000 ? 29,000 when
words are POS tagged. Our vocabulary includes
only open-class words (i.e. nouns, verbs, adjectives
and adverbs). There are no proper nouns in the vo-
cabulary with the only exception of country names.
1We used a stop-word list consisting of only the three arti-
cles ?a?, ?an? and ?the?.
45
Word Pair ?4 model ?1 model
1. doctor NN, physician NN 0.775 0.726
2. car NN, vehicle NN 0.748 0.802
3. person NN, car NN 0.038 0.024
4. car NN, country NN 0.000 0.016
5. person NN, country NN 0.031 0.069
6. child NN, marry VB 0.098 0.000
7. wife NN, marry VB 0.548 0.274
8. author NN, write VB 0.364 0.128
9. doctor NN, hospital NN 0.473 0.347
10. car NN, driver NN 0.497 0.281
Table 1: Ten examples from the LSA similarity model
2.1.3 SVD Transformation
Singular Value Decomposition (SVD) has been
found to be effective in improving word similar-
ity measures (Landauer and Dumais, 1997). SVD
is typically applied to a word by document ma-
trix, yielding the familiar LSA technique. In
our case we apply it to our word by word ma-
trix. In literature, this variation of LSA is some-
times called HAL (Hyperspace Analog to Lan-
guage) (Burgess et al, 1998).
Before performing SVD, we transform the raw
word co-occurrence count fij to its log frequency
log(fij + 1). We select the 300 largest singular val-
ues and reduce the 29K word vectors to 300 dimen-
sions. The LSA similarity between two words is de-
fined as the cosine similarity of their corresponding
word vectors after the SVD transformation.
2.1.4 LSA Similarity Examples
Ten examples obtained using LSA similarity are
given in Table 1. Examples 1 to 6 illustrate that the
metric has a good property of differentiating simi-
lar words from non-similar words. Examples 7 and
8 show that the ?4 model can detect semantically
similar words even with different POS while the ?1
model yields much worse performance. Example 9
and 10 show that highly related but not substitutable
words can also have a strong similarity but the ?1
model has a better performance in discriminating
them. We call the ?1 model and the ?4 model
as concept similarity and relation similarity respec-
tively since the ?1 model has a good performance
on nouns and the ?4 model is good at computing
similarity between relations regardless of POS of
words, such as ?marry to? and ?is the wife of?.
2.2 Combining with WordNet Knowledge
Statistical word similarity measures have limita-
tions. Related words can have similarity scores as
high as what similar words get, as illustrated by
?doctor? and ?hospital? in Table 1. Word similar-
ity is typically low for synonyms having many word
senses since information about different senses are
mashed together (Han et al, 2013). By using Word-
Net, we can reduce the above issues.
2.2.1 Boosting LSA similarity using WordNet
We increase the similarity between two words if any
of the following relations hold.
? They are in the same WordNet synset.
? One word is the direct hypernym of the other.
? One word is the two-link indirect hypernym of
the other.
? One adjective has a direct similar to relation
with the other.
? One adjective has a two-link indirect similar to
relation with the other.
? One word is a derivationally related form of the
other.
? One word is the head of the gloss of the other
or its direct hypernym or one of its direct hy-
ponyms.
? One word appears frequently in the glosses of
the other and its direct hypernym and its direct
hyponyms.
We use the algorithm described in (Collins, 1999)
to find a word gloss header. We require a minimum
LSA similarity of 0.1 between the two words to filter
out noisy data when extracting WordNet relations.
We define a word?s ?significant senses? to deal
with the problem of WordNet trivial senses. The
word ?year?, for example, has a sense ?a body of
students who graduate together? which makes it a
synonym of the word ?class?. This causes problems
because ?year? and ?class? are not similar, in gen-
eral. A sense is significant, if any of the following
conditions are met: (i) it is the first sense; (ii) its
WordNet frequency count is not less than five; or
(iii) its word form appears first in its synset?s word
46
form list and it has a WordNet sense number less
than eight.
We assign path distance of zero to the category
1, path distance of one to the category 2, 4 and 6,
and path distance of two to the other categories. The
new similarity between word x and y by combining
LSA similarity and WordNet relations is shown in
the following equation
sim?(x, y) = simLSA(x, y) + 0.5e??D(x,y) (1)
where D(x, y) is the minimal path distance between
x and y. Using the e??D(x,y) to transform simple
shortest path length has been demonstrated to be
very effective according to (Li et al, 2003). The pa-
rameter ? is set to be 0.25, following their experi-
mental results. The ceiling of sim?(x, y) remains
1.0 and we simply cut the excess.
2.2.2 Dealing with words of many senses
For a word w with many WordNet senses (currently
ten or more), we use its synonyms with fewer senses
(at most one third of that of w) as its substitutions in
computing similarity with another word. Let Sx and
Sy be the sets of all such substitutions of the words
x and y respectively. The new similarity is obtained
using Equation 2.
sim(x, y) = max( max
sx?Sx?{x}
sim?(sx, y),
max
sy?Sy?{y}
sim?(x, sy)) (2)
An online demonstration of a similar model
developed for the GOR project is available
(UMBC, 2013b), but it lacks some of this version?s
features.
3 Align-and-Penalize Approach
First we hypothesize that STS similarity between
two sentences can be computed using
STS = T ? P ? ? P ?? (3)
where T is the term alignments score, P ? is the
penalty for bad term alignments and P ?? is the
penalty for syntactic contradictions led by the align-
ments. However P ?? had not been fully implemented
and was not used in our STS submissions. We show
it here just for completeness.
3.1 Aligning terms in two sentences
We start by applying the Stanford POS tagger to tag
and lemmatize the input sentences. We use our pre-
defined vocabulary, POS tagging data and simple
regular expressions to recognize multi-word terms
including noun and verb phrases, proper nouns,
numbers and time. We ignore adverbs with fre-
quency count larger than 500, 000 in our corpus and
stop words with general meaning.
Equation 4 shows our aligning function g which
finds the counterpart of term t ? S in sentence S?.
g(t) = argmax
t??S?
sim?(t, t?) (4)
sim?(t, t?) is a wrapper function over sim(x, y) in
Equation 2 that uses the relation similarity model.
It compares numerical and time terms by their val-
ues. If they are equal, 1 is returned; otherwise 0.
sim?(t, t?) provides limited comparison over pro-
nouns. It returns 1 between subject pronouns I, we,
they, he, she and their corresponding object pro-
nouns. sim?(t, t?) also outputs 1 if one term is the
acronym of the other term, or if one term is the head
of the other term, or if two consecutive terms in a
sentence match a single term in the other sentence
(e.g. ?long term? and ?long-term?). sim?(t, t?) fur-
ther adds support for matching words2 not presented
in our vocabulary using a simple string similarity al-
gorithm. It computes character bigram sets for each
of the two words without using padding characters.
Dice coefficient is then applied to get the degree of
overlap between the two sets. If it is larger than two
thirds, sim?(t, t?) returns a score of 1; otherwise 0.
g(t) is direction-dependent and does not achieve
one-to-one mapping. This property is useful in mea-
suring STS similarity because two sentences are of-
ten not exact paraphrases of one another. Moreover,
it is often necessary to align multiple terms in one
sentence to a single term in the other sentence, such
as when dealing with repetitions and anaphora or,
e.g., mapping ?people writing books? to ?writers?.
Let S1 and S2 be the sets of terms in two input
sentences. We define term alignments score T as the
following equation shows.
?
t?S1 sim
?(t, g(t))
2 ? |S1|
+
?
t?S2 sim
?(t, g(t))
2 ? |S2|
(5)
2We use the regular expression ?[A-Za-z][A-Za-z]*? to
identify them.
47
3.2 Penalizing bad term alignments
We currently treat two kinds of alignments as ?bad?,
as described in Equation 6. For the set Bi, we have
an additional restriction that neither of the sentences
has the form of a negation. In defining Bi, we used
a collection of antonyms extracted from WordNet
(Mohammad et al, 2008). Antonym pairs are a spe-
cial case of disjoint sets. The terms ?piano? and ?vi-
olin? are also disjoint but they are not antonyms. In
order to broaden the set Bi we will need to develop
a model that can determine when two terms belong
to disjoint sets.
Ai =
{
?t, g(t)? |t ? Si ? sim?(t, g(t)) < 0.05
}
Bi = {?t, g(t)? |t ? Si ? t is an antonymof g(t)}
i ? {1, 2} (6)
We show how we compute P ? in Equation 7.
PAi =
?
?t,g(t)??Ai (sim
?(t, g(t)) +wf (t) ? wp(t))
2 ? |Si|
PBi =
?
?t,g(t)??Bi (sim
?(t, g(t)) + 0.5)
2 ? |Si|
P ? = PA1 + PB1 + PA2 + PB2 (7)
The wf (t) and wp(t) terms are two weighting func-
tions on the term t. wf (t) inversely weights the log
frequency of term t and wp(t) weights t by its part of
speech tag, assigning 1.0 to verbs, nouns, pronouns
and numbers, and 0.5 to terms with other POS tags.
4 SVM approach
We used the scores from the align-and-penalize ap-
proach along with several other features to learn a
support vector regression model. We started by ap-
plying the following preprocessing steps.
? The sentences were tokenized and POS-tagged
using NLTK?s (Bird, 2006) default Penn Tree-
bank based tagger.
? Punctuation characters were removed from the
tokens except for the decimal point in numbers.
? All numbers written as words were converted
into numerals, e.g., ?2.2 million? was replaced
by ?2200000? and ?fifty six? by ?56?.
? All mentions of time were converted into mil-
itary time, e.g., ?5:40pm? was replaced by
?1740? and ?1h30am? by ?0130?.
? Abbreviations were expanded using a compiled
list of commonly used abbreviations.
? About 80 stopwords were removed.
4.1 Ngram Matching
The sentence similarities are derived as a function of
the similarity scores of their corresponding paired
word ngrams. These features closely resemble the
ones used in (Saric et al, 2012). For our system, we
used unigrams, bigrams, trigrams and skip-bigrams,
a special form of bigrams which allow for arbitrary
distance between two tokens.
An ngram from the first sentence is exclusively
paired with an ngram from the second which has the
highest similarity score. Several similarity metrics
are used to generate different features. For bigrams,
trigrams and skip-bigrams, the similarity score for
two ngrams is computed as the arithmetic mean of
the similarity scores of the individual words they
contain. For example, for the bigrams ?he ate? and
?she spoke?, the similarity score is the average of the
similarity scores between the words ?he? and ?she?
and the words ?ate? and ?spoke?.
The ngram overlap of two sentences is defined
as ?the harmonic mean of the degree to which
the second sentence covers the first and the de-
gree to which the first sentence covers the second?
(Saric et al, 2012). Given sets S1 and S2 containing
ngrams from sentences 1 and 2, and sets P1 and P2
containing their paired ngrams along with their sim-
ilarity scores, the ngram overlap score for a given
ngram type is computed using the following equa-
tion.
HM
(
?
n?P1 w(n).sim(n)
?
n?S1 w(n)
,
?
n?P2 w(n).sim(n)
?
n?S2 w(n)
)
(8)
In this formula, HM is the harmonic mean, w(n) is
the weight assigned for the given ngram and sim(n)
is the similarity score of the paired word.
By default, all the ngrams are assigned a uniform
weight of 1. But since different words carry differ-
ent amount of information, e.g. ?acclimatize? vs.
?take?, ?cardiologist? vs. ?person?, we also use in-
formation content as weights. The information con-
tent of a word is as defined in (Saric et al, 2012).
ic(w) = ln
(
?
w??C freq(w
?
)
freq(w)
)
(9)
48
Here C is the set of words in the corpus and freq(w)
is the frequency of a word in the corpus. The
weight of an ngram is the sum of its constituent word
weights. We use refined versions of Google ngram
frequencies (Michel et al, 2011) from (Mem, 2008)
and (Saric et al, 2012) to get the information con-
tent of the words. Words not in this list are assigned
the average weight.
We used several word similarity metrics for
ngram matching apart from the similarity metric de-
scribed in section 2. Our baseline similarity metric
was an exact string match which assigned a score
of 1 if two tokens contained the same sequence of
characters and 0 otherwise. We also used NLTK?s
library to compute WordNet based similarity mea-
sures such as Path Distance Similarity, Wu-Palmer
Similarity (Wu and Palmer, 1994) and Lin Similar-
ity (Lin, 1998b). For Lin Similarity, the Semcor cor-
pus was used for the information content of words.
4.2 Contrast Scores
We computed contrast scores between two sen-
tences using three different lists of antonym pairs
(Mohammad et al, 2008). We used a large list con-
taining 3.5 million antonym pairs, a list of about
22,000 antonym pairs from Wordnet and a list of
50,000 pairs of words with their degree of contrast.
Contrast scores between two sentences were derived
as a function of the number of antonym pairs be-
tween them similar to equation 8 but with negative
values to indicate contrast scores.
4.3 Features
We constructed 52 features from different combina-
tions of similarity metrics, their parameters, ngram
types (unigram, bigram, trigram and skip-bigram)
and ngram weights (equal weight vs. information
content) for all sentence pairs in the training data.
? We used scores from the align-and-penalize ap-
proach directly as a feature.
? Using exact string match over different ngram
types and ngram weights, we extracted eight
features (4 ? 4). We also developed four addi-
tional features (2 ? 2) by includin stopwords in
bigrams and trigrams, motivated by the nature
of MSRvid dataset.
? We used the LSA boosted similarity metric in
three modes: concept similarity, relation simi-
larity and mixed mode, which used the concept
model for nouns and relation model for verbs,
adverbs and adjectives. A total of 24 features
were extracted (4 ? 2 ? 3).
? For Wordnet-based similarity measures, we
used uniform weights for Path and Wu-Palmer
similarity and used the information content of
words (derived from the Semcor corpus) for
Lin similarity. Skip bigrams were ignored and
a total of nine features were produced (3 ? 3).
? Contrast scores used three different lists of
antonym pairs. A total of six features were ex-
tracted using different weight values (3 ? 2).
4.4 Support Vector Regression
The features described in 4.3 were used in dif-
ferent combinations to train several support vec-
tor regression (SVR) models. We used LIBSVM
(Chang and Lin, 2011) to learn the SVR models and
ran a grid search provided by (Saric et al, 2012) to
find the optimal values for the parameters C , g and
p. These models were then used to predict the scores
for the test sets.
The Galactus system was trained on all of STS
2012 data and used the full set of 52 features. The
FnWN dataset was handled slightly differently from
the others. We observed that terms like ?frame? and
?entity? were used frequently in the five sample sen-
tence pairs and treated them as stopwords. To ac-
commodate the vast difference in sentence lengths,
equation 8 was modified to compute the arithmetic
mean instead of the harmonic mean.
The Saiyan system employed data-specific train-
ing and features. The training sets were subsets of
the supplied STS 2012 dataset. More specifically,
the model for headlines was trained on 3000 sen-
tence pairs from MSRvid and MSRpar, SMT used
1500 sentence pairs from SMT europarl and SMT
news, while OnWN used only the 750 OnWN sen-
tence pairs from last year. The FnWN scores were
directly used from the Align-and-Penalize approach.
None of the models for Saiyan used contrast fea-
tures and the model for SMT also ignored similarity
scores from exact string match metric.
49
5 Results and discussion
Table 2 presents the official results of our three runs
in the 2013 STS task. Each entry gives a run?s Pear-
son correlation on a dataset as well as the rank of the
run among all 89 runs submitted by the 35 teams.
The last row shows the mean of the correlations and
the overall ranks of our three runs.
We tested performance of the align-and-penalize
approach on all of the 2012 STS datasets. It ob-
tained correlation values of 0.819 on MSRvid, 0.669
on MSRpar, 0.553 on SMTeuroparl, 0.567 on SMT-
news and 0.722 on OnWN for the test datasets, and
correlation values of 0.814 on MSRvid, 0.707 on
MSRpar and 0.646 on SMTeuroparl for the training
datasets. The performance of the approach without
using the antonym penalty is also tested, producing
correlation scores of 0.795 on MSRvid, 0.667 on
MSRpar, 0.554 on SMTeuroparl, 0.566 on SMTnew
and 0.727 on OnWN, for the test datasets, and 0.794
on MSRvid, 0.707 on MSRpar and 0.651 on SM-
Teuroparl for the training datasets. The average of
the correlation scores on all eight datasets with and
without the antonym penalty is 0.6871 and 0.6826,
respectively. Since the approach?s performance was
only slightly improved when the antonym penalty
was used, we decided to not include this penalty in
our PairingWords run in the hope that its simplicity
would make it more robust.
During development, our SVM approach
achieved correlations of 0.875 for MSRvid, 0.699
for MSRpar, 0.559 for SMTeuroparl, 0.625 for
SMTnews and 0.729 for OnWN on the 2012 STS
test data. Models were trained on their respective
training sets while SMTnews used SMTeuroparl and
OnWN used all the training sets. We experimented
with different features and training data to study
their influence on the performance of the models.
We found that the unigram overlap feature, based on
boosted LSA similarity and weighted by informa-
tion content, could independently achieve very high
correlations. Including more features improved the
accuracy slightly and in some cases added noise.
The difficulty in selecting data specific features and
training for novel datasets is indicated by Saiyan?s
contrasting performance on headlines and OnWN
datasets. The model used for Headlines was trained
on data from seemingly different domains (MSRvid,
Dataset Pairing Galactus Saiyan
Headlines (750 pairs) 0.7642 (3) 0.7428 (7) 0.7838 (1)
OnWN (561 pairs) 0.7529 (5) 0.7053 (12) 0.5593 (36)
FNWN (189 pairs) 0.5818 (1) 0.5444 (3) 0.5815 (2)
SMT (750 pairs) 0.3804 (8) 0.3705 (11) 0.3563 (16)
Weighted mean 0.6181 (1) 0.5927 (2) 0.5683 (4)
Table 2: Performance of our three systems on the four
test sets.
MSRpar) while OnWN was trained only on OnWN
from STS 2012. When the model for headlines
dataset was used to predict the scores for OnWN,
the correlation jumped from 0.55 to 0.71 indicating
that the earlier model suffered from overfitting.
Overfitting is not evident in the performance of
PairingWords and Galactus, which have more con-
sistent performance over all datasets. The relatively
simple PairingWords system has two advantages: it
is faster, since the current Galactus requires comput-
ing a large number of features; and its performance
is more predictable, since training is not needed thus
eliminating noise induced from diverse training sets.
6 Conclusion
We described three semantic text similarity systems
developed for the *SEM 2013 STS shared task and
the results of the corresponding three runs we sub-
mitted. All of the systems used a lexical similarity
feature that combined POS tagging, LSA word sim-
ilarity and WordNet knowledge.
The first run, which achieved the best mean score
out of all 89 submissions, used a simple term align-
ment algorithm augmented with two penalty met-
rics. The other two runs, ranked second and fourth
out of all submissions, used support vector regres-
sion models based on a set of more than 50 addi-
tional features. The runs differed in their feature
sets, training data and procedures, and parameter
settings.
Acknowledgments
This research was supported by AFOSR award
FA9550-08-1-0265 and a gift from Microsoft.
50
References
[Agirre et al2012] Eneko Agirre, Mona Diab, Daniel Cer,
and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task
6: a pilot on semantic textual similarity. In Proceed-
ings of the First Joint Conference on Lexical and Com-
putational Semantics, pages 385?393. Association for
Computational Linguistics.
[Agirre et al2013] Eneko Agirre, Daniel Cer, Mona Diab,
Aitor Gonzalez-Agirre, and Weiwei Guo. 2013. *sem
2013 shared task: Semantic textual similarity, includ-
ing a pilot on typed-similarity. In *SEM 2013: The
Second Joint Conference on Lexical and Computa-
tional Semantics. Association for Computational Lin-
guistics.
[Baroni et al2009] M. Baroni, S. Bernardini, A. Fer-
raresi, and E. Zanchetta. 2009. The wacky wide
web: A collection of very large linguistically pro-
cessed web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
[Bird2006] Steven Bird. 2006. Nltk: the natural lan-
guage toolkit. In Proceedings of the COLING/ACL on
Interactive presentation sessions, COLING-ACL ?06,
pages 69?72, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Burgess et al1998] C. Burgess, K. Livesay, and K. Lund.
1998. Explorations in context space: Words, sen-
tences, discourse. Discourse Processes, 25:211?257.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Systems and
Technology, 2:27:1?27:27.
[Coelho et al2004] T.A.S. Coelho, Pa?vel Pereira Calado,
Lamarque Vieira Souza, Berthier Ribeiro-Neto, and
Richard Muntz. 2004. Image retrieval using multiple
evidence ranking. IEEE Trans. on Knowl. and Data
Eng., 16(4):408?417.
[Collins1999] Michael John Collins. 1999. Head-driven
statistical models for natural language parsing. Ph.D.
thesis, University of Pennsylvania.
[Deerwester et al1990] Scott Deerwester, Susan T. Du-
mais, George W. Furnas, Thomas K. Landauer, and
Richard Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(6):391?407.
[Dolan et al2004] Bill Dolan, Chris Quirk, and Chris
Brockett. 2004. Unsupervised construction of
large paraphrase corpora: exploiting massively paral-
lel news sources. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, COL-
ING ?04. Association for Computational Linguistics.
[Ganitkevitch et al2013] Juri Ganitkevitch, Ben-
jamin Van Durme, and Chris Callison-Burch. 2013.
PPDB: The paraphrase database. In HLT-NAACL
2013.
[Han and Finin2013] Lushan Han and Tim Finin. 2013.
UMBC webbase corpus. http://ebiq.org/r/351.
[Han et al2012] Lushan Han, Tim Finin, and Anupam
Joshi. 2012. Schema-free structured querying of db-
pedia data. In Proceedings of the 21st ACM interna-
tional conference on Information and knowledge man-
agement, pages 2090?2093. ACM.
[Han et al2013] Lushan Han, Tim Finin, Paul McNamee,
Anupam Joshi, and Yelena Yesha. 2013. Improving
Word Similarity by Augmenting PMI with Estimates
of Word Polysemy. IEEE Transactions on Knowledge
and Data Engineering, 25(6):1307?1322.
[Harris1968] Zellig Harris. 1968. Mathematical Struc-
tures of Language. Wiley, New York, USA.
[Hart1997] M. Hart. 1997. Project gutenberg electronic
books. http://www.gutenberg.org/wiki/Main Page.
[Kauchak and Barzilay2006] David Kauchak and Regina
Barzilay. 2006. Paraphrasing for automatic evalua-
tion. In HLT-NAACL ?06, pages 455?462.
[Landauer and Dumais1997] T. Landauer and S. Dumais.
1997. A solution to plato?s problem: The latent se-
mantic analysis theory of the acquisition, induction,
and representation of knowledge. In Psychological
Review, 104, pages 211?240.
[Li et al2003] Y. Li, Z.A. Bandar, and D. McLean.
2003. An approach for measuring semantic similar-
ity between words using multiple information sources.
IEEE Transactions on Knowledge and Data Engineer-
ing, 15(4):871?882.
[Lin1998a] Dekang Lin. 1998a. Automatic retrieval and
clustering of similar words. In Proc. 17th Int. Conf. on
Computational Linguistics, pages 768?774, Montreal,
CN.
[Lin1998b] Dekang Lin. 1998b. An information-
theoretic definition of similarity. In Proceedings of the
Fifteenth International Conference on Machine Learn-
ing, ICML ?98, pages 296?304, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
[Meadow1992] Charles T. Meadow. 1992. Text Informa-
tion Retrieval Systems. Academic Press, Inc.
[Mem2008] 2008. Google word frequency counts.
http://bit.ly/10JdTRz.
[Metzler et al2007] Donald Metzler, Susan Dumais, and
Christopher Meek. 2007. Similarity measures for
short segments of text. In Proceedings of the 29th
European conference on IR research, pages 16?27.
Springer-Verlag.
[Michel et al2011] Jean-Baptiste Michel, Yuan K. Shen,
Aviva P. Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hoiberg,
Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
51
Martin A. Nowak, and Erez L. Aiden. 2011. Quan-
titative analysis of culture using millions of digitized
books. Science, 331(6014):176?182, January 14.
[Mihalcea et al2006] Rada Mihalcea, Courtney Corley,
and Carlo Strapparava. 2006. Corpus-based and
knowledge-based measures of text semantic similarity.
In Proceedings of the 21st national conference on Ar-
tificial intelligence, pages 775?780. AAAI Press.
[Miller1995] G.A. Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):41.
[Mohammad et al2008] Saif Mohammad, Bonnie Dorr,
and Graeme Hirst. 2008. Computing word-pair
antonymy. In Proc. Conf. on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-2008), October.
[Rose et al2002] Tony Rose, Mark Stevenson, and Miles
Whitehead. 2002. The reuters corpus volume 1 - from
yesterdays news to tomorrows language resources. In
In Proceedings of the Third International Conference
on Language Resources and Evaluation, pages 29?31.
[Sahami and Heilman2006] Mehran Sahami and Timo-
thy D. Heilman. 2006. A web-based kernel function
for measuring the similarity of short text snippets. In
Proceedings of the 15th international conference on
World Wide Web, WWW ?06, pages 377?386. ACM.
[Saric et al2012] Frane Saric, Goran Glavas, Mladen
Karan, Jan Snajder, and Bojana Dalbelo Basic. 2012.
Takelab: systems for measuring semantic text simi-
larity. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics, pages 441?
448. Association for Computational Linguistics.
[Sriram et al2010] Bharath Sriram, Dave Fuhry, Engin
Demir, Hakan Ferhatosmanoglu, and Murat Demirbas.
2010. Short text classification in twitter to improve
information filtering. In Proceedings of the 33rd in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 841?842.
ACM.
[Stanford2001] Stanford. 2001. Stanford WebBase
project. http://bit.ly/WebBase.
[Toutanova et al2000] Kristina Toutanova, Dan
Klein, Christopher Manning, William Mor-
gan, Anna Rafferty, and Michel Galley. 2000.
Stanford log-linear part-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
[UMBC2013a] UMBC. 2013a. Graph of relations
project. http://ebiq.org/j/95.
[UMBC2013b] UMBC. 2013b. Semantic similarity
demonstration. http://swoogle.umbc.edu/SimService/.
[Wu and Palmer1994] Z. Wu and M. Palmer. 1994. Verb
semantic and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-1994), pages 133?138, Las
Cruces (Mexico).
52
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416?423,
Dublin, Ireland, August 23-24, 2014.
Meerkat Mafia: Multilingual and Cross-Level
Semantic Textual Similarity Systems
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman,
Taneeya Satyapanich, Sunil Gandhi and Tim Finin
University of Maryland, Baltimore County
Baltimore, MD 21250 USA
{abhay1,lushan1,ryus,jsleem1,taneeya1,sunilga1,finin}@umbc.edu
Abstract
We describe UMBC?s systems developed
for the SemEval 2014 tasks on Multi-
lingual Semantic Textual Similarity (Task
10) and Cross-Level Semantic Similarity
(Task 3). Our best submission in the
Multilingual task ranked second in both
English and Spanish subtasks using an
unsupervised approach. Our best sys-
tems for Cross-Level task ranked second
in Paragraph-Sentence and first in both
Sentence-Phrase and Word-Sense subtask.
The system ranked first for the Phrase-
Word subtask but was not included in the
official results due to a late submission.
1 Introduction
We describe the semantic text similarity systems
we developed for two of the SemEval tasks for the
2014 International Workshop on Semantic Evalu-
ation. We developed systems for task 3, Cross-
Level Semantic Similarity (Jurgens et al., 2014),
and task 10, Multilingual Semantic Textual Simi-
larity (Agirre et al., 2014). A key component in
all the systems was an enhanced version of the
word similarity system used in our entry (Han et
al., 2013b) in the 2013 SemEval Semantic Textual
Similarity task.
Our best system in the Multilingual Semantic
Textual Similarity task used an unsupervised ap-
proach and ranked second in both the English and
Spanish subtasks. In the Cross-Level Semantic
Similarity task we developed a number of new al-
gorithms and used new linguistic data resources.
In this task, our best systems ranked second in
the Paragraph-Sentence task, first in the Sentence-
Phrase task and first in the Word-Sense task. The
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
system ranked first for the Phrase-Word task but
was not included in the official results due to a late
submission.
The remainder of the paper proceeds as follows.
Section 2 describes our word similarity model and
it?s wrapper to deal with named entities and out
of vocabulary words. Sections 3 and 4 describe
how we extended the word similarity model for
the specific tasks. Section 5 presents the results
we achieved on these tasks along with instances
where the system failed. Section 6 highlights our
future plans for improving the system.
2 Semantic Word Similarity Model
2.1 LSA Word Similarity Model
Our word similarity model is a revised version of
the one we used in the 2013 *SEM semantic text
similarity task. This was in turn derived from
a system developed for the Graph of Relations
project (UMBC, 2013b). For SemEval, we wanted
a measure that considered a word?s semantics but
not its lexical category, e.g., the verb ?marry?
should be semantically similar to the noun ?wife?.
An online demonstration of a similar model de-
veloped for the GOR project is available (UMBC,
2013a), but it lacks some of this version?s features.
LSA-based word similarity. LSA Word Simi-
larity relies on the distributional hypothesis that
words occurring in the same context tend to have
similar meanings (Harris, 1968). LSA relies on the
fact that words that are semantically similar (e.g.,
cat and feline or nurse and doctor) are more likely
to occur near one another in text. Thus evidence
for word similarity can be computed from a statis-
tical analysis of a large text corpus.
We extracted raw word co-occurrence statis-
tics from a portion of the 2007 crawl of the Web
corpus from the Stanford WebBase project (Stan-
ford, 2001). We processed the collection to re-
move some undesirable elements (text duplica-
416
Word pair ?4 model ?1 model
1 doctor NN, physician NN 0.775 0.726
2 car NN, vehicle NN 0.748 0.802
3 person NN, car NN 0.038 0.024
4 car NN, country NN 0.000 0.016
5 person NN, country NN 0.031 0.069
6 child NN, marry VB 0.098 0.000
7 wife NN, marry VB 0.548 0.274
8 author NN, write VB 0.364 0.128
9 doctor NN, hospital NN 0.473 0.347
10 car NN, driver NN 0.497 0.281
Table 1: Examples from the LSA similarity model.
tions, truncated text, non-English text and strange
characters) and produced a three billion word cor-
pus of high quality English, which is available on-
line (Han and Finin, 2013).
We performed POS tagging and lemmatiza-
tion on the corpus using the Stanford POS tag-
ger (Toutanova et al., 2000). Word/term co-
occurrences were counted in a moving window
of a fixed size that scans the entire corpus. We
generated two co-occurrence models using win-
dow sizes ?1 and ?4 because we observed differ-
ent natures of the models. ?1 window produces
a context similar to the dependency context used
in (Lin, 1998). It provides a more precise con-
text but is only good for comparing words within
the same POS. This is because words of different
POS are typically surrounded by words in differ-
ent syntactic forms. In contrast, a context window
of ?4 words allows us to compute semantic simi-
larity between words with different POS.
Examples from our LSA similarity model are
given in Table 1. Pairs 1 to 6 illustrate that the
measure has a good property of differentiating
similar words from non-similar words. Examples
7 and 8 show that the ?4 model can detect se-
mantically similar words even with different POS
while the ?1 model yields poor results. The pairs
in 9 and 10 show that highly related, but not sub-
stitutable, words may have a strong similarity and
that the ?1 model is better at detecting them.
Our word co-occurrence models were based on
a predefined vocabulary of more than 22,000 com-
mon English words and noun phrases. We also
added to it more than 2,000 verb phrases extracted
from WordNet. The final dimensions of our word
co-occurrence matrices are 29,000? 29,000 when
words are POS tagged. Our vocabulary includes
only open-class words, i.e., nouns, verbs, adjec-
tives and adverbs. There are no proper nouns in
the vocabulary with the only exception of country
names.
Singular Value Decomposition (SVD) has been
found to be effective in improving word similar-
ity measures (Landauer and Dumais, 1997). SVD
is typically applied to a word by document matrix,
yielding the familiar LSA technique. In our case,
we apply it to our word by word matrix (Burgess et
al., 1998). Before performing SVD, we transform
the raw word co-occurrence count f
ij
to its log fre-
quency log(f
ij
+1). We select the 300 largest sin-
gular values and reduce the 29K word vectors to
300 dimensions. The LSA similarity between two
words is defined as the cosine similarity of their
corresponding word vectors after the SVD trans-
formation. See (Han et al., 2013b; Lushan Han,
2014) for examples and more information on the
LSA model.
Statistical word similarity measures have limi-
tations. Related words can have similarity scores
as high as what similar words get, e.g., ?doctor?
and ?hospital?. Word similarity is typically low
for synonyms that have many word senses since
information about different senses are mashed to-
gether (Han et al., 2013a). To address these issues,
we augment the similarity between two words us-
ing knowledge from WordNet, for example, in-
creasing the score if they are in the same WordNet
synset or if one is a direct or two link hypernym
of the other. See (Han et al., 2013b) for further
details.
2.2 Word Similarity Wrapper
Our word similarity model is restricted to the vo-
cabulary size which only comprises open class
words. For words outside of the vocabulary, we
can only rely on their lexical features and deter-
mine equivalence (which we score as 0 or 1, since
a continuous scale makes little sense in this sce-
nario). An analysis of the previous STS datasets
show that out-of-vocabulary words account for
about 25 ? 45% of the total words. Datasets like
MSRpar and headlines lie on the higher end of this
spectrum due to the high volume of proper nouns.
In the previous version, we computed a charac-
ter bigram overlap score given by
characterBigramScore =
|A ?B|
|A ?B|
where A and B are the set of bigrams from the first
and second word respectively. We compare this
417
against a preset threshold (0.8) to determine equiv-
alence. While this is reasonable for named enti-
ties, it is not the best approach for other classes.
Named Entities. The wrapper is extended to
handle all classes of named entities that are in-
cluded in Stanford CoreNLP (Finkel et al., 2005).
We use heuristic rules to compute the similarity
between two numbers or two dates. To handle
named entity mentions of people, locations and or-
ganizations, we supplement our character bigram
overlap method with the DBpedia Lookup service
(Mendes et al., 2011). For each entity mention, we
select the DBpedia entity with the most inlinks,
which serves as a good estimate of popularity or
significance (Syed et al., 2010). If the two named
entity mentions map to identical DBpedia entities,
we lower our character bigram overlap threshold
to 0.6.
OOV words. As mentioned earlier, when deal-
ing with out-of-vocabulary words, we only have
its lexical features. A straightforward approach is
to simply get more context for the word. Since
our vocabulary is limited, we need to use external
dictionaries to find the word. For our system, we
use Wordnik (Davidson, 2013), which is a compi-
lation of several dictionaries including The Amer-
ican Heritage Dictionary, Wikitionary and Word-
Net. Wordnik provides a REST API to access sev-
eral attributes for a given word such as it?s defini-
tions, examples, related words etc. For out of vo-
cabulary words, we simply retrieve the word pair?s
top definitions and supply it to our existing STS
system (UMBC, 2013a) to compute its similarity.
As a fallback, in case the word is absent even in
Wordnik, we resort to our character bigram over-
lap measure.
3 Multilingual Semantic Text Similarity
3.1 English STS
For the 2014 STS-English subtask we submitted
three runs. They all used a simple term alignment
strategy to compute sentence similarities. The first
run was an unsupervised approach that used the
basic word-similarity model for term alignment.
The next two used a supervised approach to com-
bine the scores from the first run with alignment
scores using the enhanced word-similarity wrap-
per. The two runs differed in their training.
Align and Penalize Approach. The pairingWord
run was produced by the same Align-and-Penalize
system (Han et al., 2013b) that we used in the
2013 STS task with only minor changes. The
biggest change is that we included a small list
of disjoint concepts (Han et al., 2013b) that are
used in the penalization phase, such as {piano, vi-
olin} and {dog, cat}. The disjoint concepts were
manually collected from the MSRvid dataset pro-
vided by the 2012 STS task because we still lack a
reliable general method to automatically produce
them. The list only contains 23 pairs, which can
be downloaded at (dis, 2014).
We also slightly adjusted our stopword list.
We removed a few words that appear in the trial
datasets of 2013 STS task (e.g., frame) but we did
not add any new stopwords for this year?s task. All
the changes are small and we made them only in
the hope that they can slightly improve our system.
Unlike machine learning methods that require
manually selecting an appropriate trained model
for a particular test dataset, our unsupervised
Align-and-Penalize system is applied uniformly
to all six test datasets in 2014 STS task, namely,
deft-forum, deft-news, headlines, images, OnWN
and tweet-news. It achieves the second best rank
among all submitted runs.
Supervised Machine Learning. Our second and
third runs used machine learning approaches sim-
ilar to those we developed for the 2013 STS task
but with significant changes in both pre-processing
and the features extracted.
The most significant pre-processing change was
the use of Stanford coreNLP (Finkel et al., 2005)
tool for tokenization, part-of-speech tagging and
identifying named entity mentions. For the tweet-
news dataset we also removed the hashtag symbol
(?#?) prior to applying the Stanford tools. We use
only open class words and named entity mentions
and remove all other tokens.
We align tokens between two sentences based
on the updated word similarity wrapper that was
described in Section 2.2. We use information
content from Google word frequencies for word
weights similar to our approach last year. The
alignment process is a many-to-one mapping sim-
ilar to the Align and Penalize approach and two
tokens are only aligned if their similarity is greater
than 0.1. The sentence similarity score is then
computed as the average of the scores of their
aligned tokens. This score, along with the Align
and Penalize approach score, are used as features
to train support vector regression (SVR) models.
418
We use an epsilon SVR with a radial basis kernel
function and use a grid search to get the optimal
parameter values for cost, gamma and epsilon. We
use datasets from the previous STS tasks as train-
ing data and the two submitted runs differ in the
choice of their training data.
The first approach, named Hulk, is an attempt
to use a generic model trained on a large data set.
The SVR model uses a total of 3750 sentence pairs
(1500 from MSRvid, 1500 from MSRpar and 750
from headlines) for training. Datasets like SMT
were excluded due to poor quality.
The second approach, named Super Saiyan,
is an attempt at domain specific training. For
OnWN, we used 1361 sentence pairs from previ-
ous OnWN dataset. For Images, we used 1500
sentence pairs from MSRvid dataset. The others
lacked any domain specific training data so we
used a generic training dataset comprising 5111
sentence pairs from MSRvid, MSRpar, headlines
and OnWN datasets.
3.2 Spanish STS
As a base-line for this task we first considered
translating the Spanish sentences to English and
running the same systems explained for the En-
glish Subtask (i.e., pairingWord and Hulk). The
results obtained applying this approach to the pro-
vided training data gave a correlation of 0.777 so,
we selected this approach (with some improve-
ments) for the competition.
Translating the sentences. For the automatic
translation of the sentences from Spanish to En-
glish we used the Google Translate API
1
, a
free, multilingual machine-translation product by
Google. Google Translate presents very accurate
translations for European languages by using sta-
tistical machine translation (Brown et al., 1990)
where the translations are generated on the basis of
statistical models derived from bilingual text cor-
pora. In fact, Google used as part of this corpora
200 billion words from United Nations documents
that are typically published in all six official UN
languages, including English and Spanish.
In the experiments performed with the trial data
we manually evaluated the quality of the trans-
lations (one of the authors is a native Spanish
speaker). The overall translation was very accu-
rate but some statistical anomalies, incorrect trans-
lations due to the abundance of a specific sense of
1
http://translate.google.com
I1: Las costas o costa de un mar, lago o extenso r??o es la
tierra a lo largo del borde de estos.
T11: Costs or the cost of a sea, lake or wide river is the
land along the edge of these.
T12: Coasts or the coast of a sea, lake or wide river is the
land along the edge of these.
T13: Coasts or the coast of a sea, lake or wide river is the
land along the border of these.
...
Figure 1: Three of the English translations for the
Spanish sentence I1.
a word in the training set, appeared.
On one hand, some homonym words are
wrongly translated. For example, the Spanish sen-
tence ?Las costas o costa de un mar [...]? was
translated to ?Costs or the cost of a sea [...]?.
The Spanish word costa has two different senses:
?coast? (the shore of a sea or ocean) and ??cost?
(the property of having material worth). On the
other hand, some words are translated preserving
their semantics but with a slightly different mean-
ing. For example, the Spanish sentence ?Un coj??n
es una funda de tela [...]? was correctly translated
to ?A cushion is a fabric cover [...]?. However,
the Spanish sentence ?Una almohada es un coj??n
en forma rectangular [...]? was translated to ?A
pillow is a rectangular pad [...]?
2
.
Dealing with statistical anomalies. The afore-
mentioned problem of statistical machine transla-
tion caused a slightly adverse effect when comput-
ing the similarity of two English (translated from
Spanish) sentences with the systems explained in
Section 3.1. Therefore, we improved the direct
translation approach by taking into account the
different possible translations for each word in a
Spanish sentence. For that, our system used the in-
formation provided by the Google Translate API,
that is, all the possible translations for every word
of the sentence along with a popularity value. For
each Spanish sentence the system generates all its
possible translations by combining the different
possible translations of each word. For example,
Figure 1 shows three of the English sentences gen-
erated for a given Spanish sentence from the trial
data.
As a way of controlling the combinatorial ex-
plosion of this step, especially for long sentences,
we limited the maximum number of generated
2
Notice that both Spanish sentences used the term coj??n
that should be translated as cushion (the Spanish word for
pad is almohadilla).
419
sentences for each Spanish sentence to 20 and
we only selected words with a popularity greater
than 65. We arrived at the popularity threshold
through experimentation on every sentence in the
trial data set. After this filtering, our input for
the ?news? and ?wikipedia? tests went from 480
and 324 pairs of sentences to 5756 and 1776 pairs,
respectively.
Given a pair of Spanish sentences, I1
and I2, and the set of possible translations
generated by our system for each sentence,
T
I1
= {T
11
, T
12
, T
13
, . . . , T
1n
} and T
I2
=
{T
21
, T
22
, . . . , T
2m
}, we compute the similarity
between them by using the following formula:
SimSPA(I1, I2) =
n
?
i=1
m
?
j=1
SimENG(T
1i
, T
2j
)
n ?m
where SimENG(x, y) computes the similarity of
two English sentences using our existing STS sys-
tem (Han et al., 2013b).
For the final competition we submitted three
runs. The first (Pairing in Table 3) used the
pairingWord system with the direct translation of
the Spanish sentences to English. The second
run (PairingAvg in Table 3) used the formula for
SimSPA(x, y) based on SimENG(x, y) with
the pairingWord system. Finally, the third one
(Hulk in Table 3) used the Hulk system with the
direct translation.
4 Cross Level Similarity
4.1 Sentence to Paragraph/Phrase
We used the three systems developed for the En-
glish sentence similarity subtask and described in
Section 3.1 for both the sentence to paragraph and
sentence to phrase subtasks, producing three runs.
The model for Hulk remained the same (trained
on 3750 sentence pairs from MSRvid, MSRpar
and headlines dataset) but the SuperSaiyan sys-
tem, which is the domain specific approach, used
the given train and trial text pairs (about 530) for
the respective subtasks as training to generate task
specific models.
4.2 Phrase to Word
In our initial experiments, we directly computed
the phrase-word pair similarity using our English
STS. This yielded a very low correlation of 0.239
for the training set, primarily due to the absence of
these phrases and words in our vocabulary. To ad-
dress this issue, we used external sources to obtain
more contextual information and extracted several
features.
Dictionary features. We used Wordnik as a dic-
tionary resource and retrieved definitions and us-
age examples for the word. We then used our
English STS system to measure the similarity be-
tween these and the given phrase to extract two
features.
Web search features. These features were based
on the hypothesis that if a word and phrase have
similar meanings, then a web search that combines
the word and phrase should return similar docu-
ments when compared to a web search for each
individually.
We implemented this idea by comparing results
of three search queries: the word alone, the phrase
alone, and the word and phrase together.
Using the Bing Search API (BIN, 2014), we re-
trieved the top five results for each search, indexed
them with Lucene (Hatcher et al., 2004), and ex-
tracted term frequency vectors for each of the three
search result document sets. For the phrase ?spill
the beans? and word ?confess?, for example, we
built a Lucene index for the set of documents re-
trieved by a Bing search for ?spill the beans?, ?con-
fess?, and ?spill the beans confess?. We calculated
the similarity of pairs of search result sets using
the cosine similarity (1) of their term frequency
vectors.
CosineSimilarity =
n
?
i=1
V 1
i
? V 2
i
?
n
?
i=1
(V 1
i
)
2
?
?
n
?
i=1
(V 2
i
)
2
(1)
We calculated the mean and minimum sim-
ilarity of pairs of results for the phrase and
phrase+word searches. These features were ex-
tracted from the provided training set and used in
conjunction with the dictionary features to train
an SVM regression model to predict similarity
scores.
We observed this method can be problematic
when a word or phrase has multiple meanings.
For example, ?spill the beans? relates to ?confess-
ing? but it is also the name of a coffee shop and
a soup shop. A mix of these pages do get re-
turned by Bing and reduces the accuracy of our re-
sults. However, we found that this technique often
strengthens evidence of similarity enough that it
improves our overall accuracy when used in com-
bination with our dictionary features.
420
Dante#n#1: an Italian poet famous for writing the
Divine Comedy that describes a journey through Hell and
purgatory and paradise guided by Virgil and his idealized
Beatrice
writer#n#1: writes books or stories or articles or the like
professionally for pay
generator#n#3: someone who originates or causes or
initiates something, ?he was the generator of several
complaints?
author#v#1: be the author of, ?She authored this play?
Figure 2: The WordNet sense for Dante#n#1 and
the three author#n senses.
4.3 Word to Sense
For this subtask, we used external resources to re-
trieve more contextual information. For a given
word, we retrieved its synonym set from WordNet
along with their corresponding definitions. We re-
trieved the WordNet definition for the word sense
as well. For example, given a word-sense pair
(author#n, Dante#n#1), we retrieved the synset of
author#n (writer.n.01, generator.n.03, author.v.01)
along with their WordNet definitions and the sense
definition of Dante#n#1. Figure 2 shows the
WordNet data for this example.
By pairing every combination of the word?s
synset and their corresponding definitions with the
sense?s surface form and definition, we created
four features. For each feature, we used our En-
glish STS system to compare their semantic sim-
ilarity and kept the maximum score as feature?s
value.
We found that about 10% of the training
dataset?s words fell outside of WordNet?s vocab-
ulary. Examples of missing words included many
informal or ?slang? words like kegger, crackberry
and post-season. To address this, we used Word-
nik to retrieve the word?s top definition and com-
puted its similarity with the sense. This reduced
the out-of-vocabulary words to about 2% for the
training data. Wordnik thus gave us two addi-
tional features: the maximum semantic similarity
score of word-sense using Wordnik?s additional
definitions for all words and for just the out-of-
vocabulary words. We used these features to train
an SVM regression model with the provided train-
ing set to predict similarity scores.
Dataset Pairing Hulk SuperSaiyan
deft-forum 0.4711 (9) 0.4495 (15) 0.4918 (4)
deft-news 0.7628 (8) 0.7850 (1) 0.7712 (3)
headlines 0.7597 (8) 0.7571 (9) 0.7666 (2)
images 0.8013 (7) 0.7896 (10) 0.7676 (18)
OnWN 0.8745 (1) 0.7872 (18) 0.8022 (12)
tweet-news 0.7793 (2) 0.7571 (7) 0.7651 (4)
Weighted Mean 0.7605 (2) 0.7349 (6) 0.7410 (5)
Table 2: Performance of our three systems on the
six English test sets.
Dataset Pairing PairingAvg Hulk
Wikipedia 0.6682 (12) 0.7431 (6) 0.7382 (8)
News 0.7852 (12) 0.8454 (1) 0.8225 (6)
Weighted Mean 0.7380 (13) 0.8042 (2) 0.7885 (5)
Table 3: Performance of our three systems on the
two Spanish test sets.
5 Results
Multilingual Semantic Text Similarity. Table
2 shows the system performance for the English
STS task. Our best performing system ranked
second
3
, behind first place by only 0.0005.
It employs an unsupervised approach with no
training data required. The supervised systems
that handled named entity recognition and out-
of-vocabulary words performed slightly better on
datasets in the news domain but still suffered from
noise due to diverse training datasets.
Table 3 shows the performance for the Spanish
subtask. The best run achieved a weighted correla-
tion of 0.804, behind first place by only 0.003. The
Hulk system was similar to the Pairing run and
used only one translation per sentence. The per-
formance boost could be attributed to large num-
ber of named entities in the News and Wikipedia
datasets.
Cross Level Similarity. Table 4 shows our per-
formance in the Cross Level Similarity tasks. The
Paragraph-Sentence and Sentence-Phrase yielded
good results (ranked second and first respectively)
with our English STS system because of sufficient
amount of textual information. The correlation
scores dropped as the granularity level of the text
got finer.
The Phrase-Word run achieved a correlation of
0.457, the highest for the subtask. However, an
incorrect file was submitted prior to the deadline
3
An incorrect file for ?deft-forum? dataset was submitted.
The correct version had a correlation of 0.4896 instead of
0.4710. This would have placed it at rank 1 overall.
421
Wordnik BingSim Score
ID S1 S2 Baseline Definitions Example Sim Avg Min SVM GS Error
Idiomatic-212 spill the beans confess 0 0 0 0.0282 0.1516 0.1266 0.5998 4.0 3.4002
Idiomatic-292 screw the pooch mess up 0 0.04553 0.0176 0.0873 0.4238 0.0687 0.7185 4.0 3.2815
Idiomatic-273 on a shoogly peg insecure 0 0.0793 0 0.0846 0.3115 0.1412 0.8830 4.0 3.1170
Slang-115 wacky tabaccy cannabis 0 0 0 0.0639 0.4960 0.1201 0.5490 4.0 3.4510
Slang-26 pray to the porcelain god vomiting 0 0 0 0.0934 0.5275 0.0999 0.6452 4.0 3.3548
Slang-79 rock and roll commence 0 0.2068 0.0720 0.0467 0.5106 0.0560 0.8820 4.0 3.1180
Newswire-160 exercising rights under canon law lawyer 0.0044 0.6864 0.0046 0.3642 0.4990 0.2402 3.5562 0.5 3.0562
Table 5: Examples where our algorithm performed poorly and the scores for individual features.
Dataset Pairing Hulk SuperSaiyan WordExpand
Para.-Sent. 0.794 (10) 0.826 (4) 0.834 (2)
Sent.-Phrase 0.704 (14) 0.705 (13) 0.777 (1)
Phrase-Word 0.457 (1)
Word-Sense 0.389 (1)
Table 4: Performance of our systems on the four
Cross-Level Subtasks.
Figure 3: Average error with respect to category.
which meant that this was not included in the of-
ficial results. Figure 3 shows the average error
(measured as the average deviation from the gold
standard) across different categories for phrase to
word subtask. Our performance is slightly worse
for slang and idiomatic categories when compared
to others which is due to two reasons: (i) the se-
mantics of idioms is not compositional, reducing
the effectiveness of a distributional similarity mea-
sure and (ii) dictionary-based features often failed
to find definitions and/or examples of idioms. Ta-
ble 5 shows some of the words where our algo-
rithm performed poorly and their scores for indi-
vidual features.
The Word-Sense run ranked first in the sub-
task with a correlation score of 0.389. Table 6
shows some of the word-sense pairs where the
system performed poorly. Our system only used
Wordnik?s top definition which was not always the
right one to use to detect the similarity. For ex-
ample, the first definition of cheese#n is ?a solid
food prepared from the pressed curd of milk? but
there is a latter, less prominent one, which is
ID word sense key sense number predicted gold
80 cheese#n moolah%1:21:00:: moolah#n#1 0.78 4
377 bone#n chalk%1:07:00:: chalk#n#2 1.52 4
441 wasteoid#n drug user%1:18:00:: drug user#n#1 0.78 3
Table 6: Examples where our system performed
poorly.
?money?. A second problem is that some words,
like wasteoid#n, were absent even in Wordnik.
Using additional online lexical resources to in-
clude more slangs and idioms, like the Urban Dic-
tionary (Urb, 2014), could address these issues.
However, care must be taken since the quality of
some content is questionable. For example, the
Urban Dictionary?s first definition of ?program-
mer? is ?An organism capable of converting caf-
feine into code?.
6 Conclusion
We described our submissions to the Multilingual
Semantic Textual Similarity (Task 10) and Cross-
Level Semantic Similarity (Task 3) tasks for the
2014 International Workshop on Semantic Eval-
uation. Our best runs ranked second in both En-
glish and Spanish subtasks for Task 10 while rank-
ing first in Sentence-Phrase, Phrase-Word, Word-
Sense tasks and second in Paragraph-Sentence
subtasks for Task 3. Our success is attributed to
a powerful word similarity model based on LSA
word similarity and WordNet knowledge. We
used new linguistic resources like Wordnik to im-
prove our existing system for the Phrase-Word and
Word-Sense tasks and plan to include other re-
sources like ?Urban dictionary? in the future.
Acknowledgements
This research was supported by awards 1228198,
1250627 and 0910838 from the U.S. National Sci-
ence Foundation.
422
References
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
2014. BING search API. http://bing.com/developers-
/s/APIBasics.html.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Fredrick Jelinek, John D Laf-
ferty, Robert L Mercer, and Paul S Roossin. 1990.
A statistical approach to machine translation. Com-
putational linguistics, 16(2):79?85.
Curt Burgess, Kay Livesay, and Kevin Lund. 1998.
Explorations in context space: Words, sentences,
discourse. Discourse Processes, 25(2-3):211?257.
Sara Davidson. 2013. Wordnik. The Charleston Advi-
sor, 15(2):54?58.
2014. Disjoint concept pairs. http://semanticweb-
archive.cs.umbc.edu/disjointConcepts.txt.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating non-local
information into information extraction systems by
gibbs sampling. In 43rd Annual Meeting of the ACL,
pages 363?370.
Lushan Han and Tim Finin. 2013. UMBC webbase
corpus. http://ebiq.org/r/351.
Lushan Han, Tim Finin, Paul McNamee, Anupam
Joshi, and Yelena Yesha. 2013a. Improving Word
Similarity by Augmenting PMI with Estimates of
Word Polysemy. IEEE Trans. on Knowledge and
Data Engineering, 25(6):1307?1322.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013b.
UMBC EBIQUITY-CORE: Semantic Textual
Similarity Systems. In 2nd Joint Conf. on Lexical
and Computational Semantics. ACL, June.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York, USA.
Erik Hatcher, Otis Gospodnetic, and Michael McCand-
less. 2004. Lucene in action. Manning Publications
Greenwich, CT.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-Level Semantic Similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. 17th Int. Conf. on Compu-
tational Linguistics, pages 768?774, Montreal, CN.
Lushan Han. 2014. Schema Free Querying of Seman-
tic Data. Ph.D. thesis, University of Maryland, Bal-
timore County.
Pablo N Mendes, Max Jakob, Andr?es Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In 7th Int. Conf. on
Semantic Systems, pages 1?8. ACM.
Stanford. 2001. Stanford WebBase project.
http://bit.ly/WebBase.
Zareen Syed, Tim Finin, Varish Mulwad, and Anupam
Joshi. 2010. Exploiting a Web of Semantic Data for
Interpreting Tables. In Proceedings of the Second
Web Science Conference, April.
Kristina Toutanova, Dan Klein, Christopher Manning,
William Morgan, Anna Rafferty, and Michel Gal-
ley. 2000. Stanford log-linear part-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
UMBC. 2013a. Semantic similarity demonstration.
http://swoogle.umbc.edu/SimService/.
UMBC. 2013b. Umbc graph of relations project.
http://ebiq.org/j/95.
2014. Urban dictionary. http://urbandictionary.com/.
423
