Finding Errors Automatically in
Semantically Tagged Dialogues
John Aberdeen, Christine Doran, Laurie Damianos,
Samuel Bayer and Lynette Hirschman
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730 USA
+1.781.271.2000
{aberdeen,cdoran,laurie,sam,lynette}@mitre.org
ABSTRACT
We describe a novel method for detecting errors in task-based
human-computer (HC) dialogues by automatically deriving
them from semantic tags. We examined 27 HC dialogues from
the DARPA Communicator air travel domain, comparing user
inputs to system responses to look for slot value
discrepancies, both automatically and manually. For the
automatic method, we labeled the dialogues with semantic tags
corresponding to "slots" that would be filled in "frames" in
the course of the travel task. We then applied an automatic
algorithm to detect errors in the dialogues. The same dialogues
were also manually tagged (by a different annotator) to label
errors directly. An analysis of the results of the two tagging
methods indicates that it may be possible to detect errors
automatically in this way, but our method needs further work
to reduce the number of false errors detected. Finally, we
present a discussion of the differing results from the two
tagging methods.
Keywords
Dialogue, Error detection, DARPA Communicator.
1. INTRODUCTION
In studying the contrasts between human-computer (HC) and
human-human (HH) dialogues [1] it is clear that many HC
dialogues are plagued by disruptive errors that are rarely seen
in HH dialogues. A comparison of HC and HH dialogues may
help us understand such errors. Conversely, the ability to
detect errors in dialogues is critical to understanding the
differences between HC and HH communication.
Understanding HC errors is also crucial to improving HC
interaction, making it more robust, trustworthy and efficient.
The goal of the work described in this paper is to provide an
annotation scheme that allows automatic calculation of
misunderstandings and repairs, based on semantic information
presented at each turn. If we represent a dialogue as a sequence
of pairs of partially-filled semantic frames (one for the user?s
utterances, and one for the user?s view of the system state), we
can annotate the accumulation and revision of information in
the paired frames.  We hypothesized that, with such a
representation, it would be straightforward to detect when the
two views of the dialogue differ (a misunderstanding), where
the difference originated (source of error), and when the two
views reconverge (correction). This would be beneficial
because semantic annotation often is used for independent rea-
sons, such as measurements of concepts per turn [8],
information bit rate [9], and currently active concepts [10].
Given this, if our hypothesis is correct, then by viewing
semantic annotation as a representation of filling slots in user
and system frames, it should be possible to detect errors
automatically with little or no additional annotation.
2. SEMANTIC TAGGING
We tagged 27 dialogues from 4 different systems that
participated in a data collection conducted by the DARPA
Communicator  program in the summer of 2000. These are
dialogues between paid subjects and spoken language
dialogue systems operating in the air travel domain. Each
dialogue was labeled with semantic tags by one annotator. We
focused on just the surface information available in the
dialogues, to minimize inferences made by the annotator.
The semantic tags may be described along two basic
dimensions: slot and type. The slot dimension describes the
items in a semantic frame that are filled over the course of a
dialogue, such as DEPART_CITY and AIRLINE (see Table 1 for
the complete list).
The type dimension describes whether the tag is a PROMPT, a
FILL, or an OFFER. This type dimension is critical to semantic
analysis since it allows one to describe the effect a tag has on
slots in the frame. PROMPTs are attempts to gather values to
fill slots, e.g., "what city do you want to fly to". FILLs are
actual slot fills, e.g., "I?d like to fly to San Francisco". OFFERs
represent actual flight information based on previous slot
FILLs, e.g., "there is a 9:45 flight to San Francisco on Delta".
However, OFFERs often do not exactly match slot FILLs (e.g.,
the user requests a flight at 9:30, but the closest match flight
is at 9:45), and thus must be distinguished from FILLs.
In addition to the two basic dimensions of slot and type, each
tag takes a leg attribute to indicate which leg of a trip is being
discussed. There is also an initial USER_ID slot which has two
types (PROMPT_USER_ID and FILL_USER_ID), but no leg
attribute.
Our semantic tag set alo includes two special tags, YES and
NO, for annotating responses to offers and yes/no questions.
Finally, we have two tags, PROMPT_ERASE_ FRAMES and
FILL_ERASE_FRAMES, for annotating situations where the
frames are erased and the dialogue is restarted (e.g., the user
says "start over"). Figure 1 shows part of a sample dialogue
with semantic tags. Our semantic tagset is summarized in Table
1.
Table 1. Semantic Tagset
PROMPT FILL OFFER
DEPART_CITY X X X
ARRIVE_CITY X X X
DEPART_AIRPORT X X X
ARRIVE_AIRPORT X X X
DATE X X X
DEPART_TIME X X X
ARRIVE_TIME X X X
AIRLINE X X X
USER_ID X X
ERASE_FRAMES X X
YES (single bare tag)
NO (single bare tag)
3. ERROR DETECTION
To provide a baseline for comparison to an algorithm that
detects errors automatically, we had an annotator (not the same
person who did the semantic tagging described above)
manually tag the problem areas. This annotator marked four
items:
(1) occurrence: where the problem first occurs in the
dialogue (e.g. where the user says the item which the
system later incorporates incorrectly)
(2) detection: where the user could first be aware that
there is a problem (e.g. where the system reveals its
mistake)
(3) correction attempt: where the user attempts to repair
the error
(4) correction detection: where the user is first able to
detect that the repair has succeeded
We next developed an algorithm for automatically finding
errors in our semantically tagged dialogues. In this phase of
the research, we concentrated on deriving an automatic method
for assigning the first two of the four error categories,
occurrence and detection (in a later phase we plan to develop
automatic methods for correction attempt and correction
detection). First, the algorithm derives the turn-by-turn frame
states for both the user's utterances and the system's utterances
(i.e., what the user heard the system say), paying special
attention to confirmation tags such as YES or deletion tags
like FILL_ERASE_FRAMES. Then, the algorithm compares
patterns of user and system events to hypothesize errors.
Occurences and detections are hypothesized for three types of
errors: hallucinations (system slot fill without user slot fill),
mismatches (system slot fill does not match user slot fill), and
prompts after fills (system prompt after user slot fill).
Figure 2 shows a sample dialogue that illustrates several error
types. Utterance S12 shows a prompt after fill error ? the user
has already supplied (in utterance U11) the information the
system is requesting. In utterance U13 the user supplies
contradictory information, and the system catches this and
tries to resolve it in utterances S14 and S16. Next a mismatch
error is illustrated ? the user specifies ARRIVE_CITY in
utterance U17, and the system shows that it has misrecognized
Figure 1. Portion of a Sample Dialogue Labeled with Semantic Tags
 S0: System said: Welcome to our travel planning system.
<prompt_user_id>Please say your PIN number</prompt_user_id>.
U1: User said: My PIN is <fill_user_id>four four four four</fill_user_id>.
 S2: Sys tem said: What are your travel plans?
U3: User said: I want to go to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.
 S4: System said: Traveling to <fill_arrive_city leg=?1?>San Francisco</fill_arrive_city>.
<prompt_depart_city leg=?1?>Where are you departing from?</prompt_depart_city>
U5: User said: <fill_depart_city leg=?1?>Boston</fill_depart_city>.
 S6: System said: <prompt_date leg=?1?>When would you like to leave</prompt_date>
<fill_depart_city leg=?1?>Boston</fill_depart_city>?
U7: User said: <fill_date leg=?1?>July twenty-fifth</fill_date> at
<fill_depart_time leg=?1?>12 noon</fill_depart_time>.
 S8: System said: I have a <offer_airline leg=?1?>Delta</offer_airline> flight that leaves
<offer_depart_city leg=?1?>Boston</offer_depart_city> at
<offer_depart_time leg=?1?>11:10AM</offer_depart_time> on
<offer_date leg=?1?>July twenty-fifth</offer_date> and arrives in
<offer_arrive_city leg=?1?>San Francisco</offer_arrive_city> at
<offer_arrive_time>5:19PM</offer_arrive_time>. Is that OK?
it in utterance S18. The user attempts to correct this
misrecognition in utterance U21, and as can be seen from
utterance S22, the system again has misrecognized the user?s
utterance.
Below we describe the results from running the automatic
algorithm on our 27 semantically tagged dialogues.
4. RESULTS
In the 27 dialogues considered, a total of 131 items were
flagged by one or both of the methods as error items (60 occur,
71 detect). A breakdown of these errors and which method
found them is in Table 2.
Table 2. Unique Errors Identified
# errors found by: Occur Detect Total
Both Methods 14 23 37
Automatic Only 28 38 66
Manual Only 18 10 28
Totals 60 71 131
As can be seen in Table 2 the automatic method flagged many
more items as errors than the manual method.
Table 3. Error Judgements
Occur Detect
E NE Q E NE Q
Auto 48% 40% 12% 52% 38% 10%
Man 84% 13% 3% 82% 15% 3%
We carefully examined each of the items flagged as errors by
the two methods. Three judges (the semantic tagging
annotator, the manual error tagging annotator, and a third
person who did not participate in the annotation) determined
which of the errors found by each of the two methods were real
errors (E), not real errors (NE), or questionable (Q). For
calculations in the present analysis, we used E as the baseline
of real errors, rather than E+Q. Table 3 shows the judgements
made for both the automatic and manual method, which are
discussed in the next section. It is important to note that
human annotators do not perform this task perfectly, with error
rates of 13% and 15%. This is also shown in the precision and
recall numbers for the two methods in Table 4.
Table 4. Precision & Recall
Occur DetectPrecision
& Recall P R P R
Automatic 0.48 0.57 0.52 0.84
Manual 0.84 0.77 0.82 0.71
5. ANALYSIS
The automatic method flagged 40 items as errors that the
judges determined were not errors (17 occur, 23 detect). These
40 false errors can be classified as follows:
A. 10 were due to bugs in the algorithm or source data
B. 19 were false errors that can be eliminated with non-
trivial changes to the semantic tagset and/or algorithm
C. 3 were false errors that could not be eliminated
without the ability to make inferences about world
knowledge
D. 8 were due to mistakes made by the semantic
annotator
One example of the 19 false errors above in B is when the first
user utterance in a dialogue is a bare location, it is unclear
whether the user intends it to be a departure or arrival location.
Our semantic tagset currently has no tags for ambiguous
situations such as these. Adding underspecified tags to our
tagset (and updating the automatic algorithm appropriately)
would solve this problem. Another example is a situation
where a system was legitimately asking for clarification about
a slot fill, but the algorithm flagged it as prompting for keys
that had already been filled. This could be fixed by adding a
CLARIFY element to the type dimension (currently PROMPT,
FILL, and OFFER). We believe that making these changes
would not compromise the generality of our semantic tagset.
However, as the point of our approach is to derive errors
without much additional annotation, additions to the semantic
tagset should only be made when there is substantial
justification.
There were also 21 errors (15 occur, 6 detect) that were not
detected by the automatic method, but were judged as real
errors. These 21 errors may be categorized as follows:
A. 2 were due to bugs in the algorithm
B. 8 were situations where the algorithm correctly
flagged the detect point of an error, but missed the
associated occur point
C. 6 were situations that could be fixed by
modifications to the semantic tagset
D. 1 was an error that could be fixed either by a
revision to the semantic tagset or a revision to the
algorithm
E. 2 were situations where the system ignored a user
fill, and the automatic algorithm interpreted it as no
confirmation (not an error). Human judgement is
required to detect these errors
F. 2 were due to mistakes made by the semantic
annotator
6. PREVIOUS WORK
In Hirschman & Pao [5], annotation was done by manual
inspection of the exchanges in the dialogue. Each exchange
was evaluated based on the portion of information "visible to
the other party". Errors and problems were identified manually
and traced back to their point of origin. This is quite similar to
our baseline manual annotation described in section 3.
There have been other approaches to detecting and
characterizing errors in HC dialogues. Danieli [2] used
expectations to model future user ut terances, and Levow [6][7]
used utterance and pause duration, as well as pitch variability
to characterize errors and corrections. Dybkj?r, Bernsen &
Dybkj?r [4] developed a set of principles of cooperative HC
dialogue, as well as a taxonomy of errors typed according to
which of the principles are violated. Finally, Walker et. al.
[11][12] have trained an automatic classifier that identifies
and predicts problems in HC dialogues.
7. DISCUSSION
It is clear that our algorithm and semantic tagset, as they stand
now, need improvements to reduce the number of false errors
detected. However, even now the automatic method offers some
advantages over tagging errors manually, the most important
of which is that many researchers already annotate their
dialogues with semantic tags for other purposes and thus
many errors can be detected with no additional annotation.
Also, the automatic method associates errors with particular
slots, enabling researchers to pinpoint aspects of their
dialogue management strategy that need the most work.
Finally, Day et. al. [3] have shown that correcting existing
annotations is more time efficient than annotating from
scratch. In this way, the automatic method may be used to
"seed" an annotation effort, with later hand correction.
8. ACKNOWLEDGMENTS
This work was funded by the DARPA Communicator program
under contract number DAAB07-99-C201.  ? 2001 The MITRE
Corporation. All rights reserved.
9. REFERENCES
[1] Aberdeen, J. and Doran, C. Human-computer and human-
human dialogues. DARPA Communicator Principle
Investigators Meeting (Philadelphia, PA USA 2000).
http://www.dsic-web.net/ito/meetings/communicator
_sep2000/
[2] Danieli, M. On the use of expectations for detecting and
repairing human-machine miscommunication.
Proceedings of AAAI Workshop on Detecting, Repairing
and Preventing Human-Machine Miscommunication
(Portland OR, USA 1996).
[3] Day, D., Aberdeen, J., Hirschman, L., Kozierok, R.,
Robinson, P. and Vilain, M. Mixed-initiative
development of language processing systems. In
Proceedings of the Fifth Conference on Applied Natural
Language Processing (Washington DC, USA 1997).
[4] Dybkj?r, L., Bernsen, N.O. and Dybkj?r, H. Reducing
miscommunication in spoken human-machine dialogue.
Proceedings of AAAI Workshop on Detecting, Repairing
and Preventing Human-Machine Miscommunication
(Portland OR, USA 1996).
[5] Hirschman, L. and Pao, C. The cost of errors in a spoken
language system. Proceedings of the Third European
Figure 2. Portion of a Sample Dialogue Illustrating Several Errors
   U11: User said: I want to fly from <fill_depart_city leg=?1?>Seattle</fill_depart_city> to
<fill_arrive_city leg=?1?>Tokyo</fill_arrive_city>
     S12: System said: <prompt_arrive_city leg="1">to which city would you like to fly</prompt_arrive_city>
from <fill_depart_city leg="1">seattle</fill_depart_city>
U13: User said: <fill_arrive_city leg="1">seattle washington</fill_arrive_city>
    S14: System said: excuse me, i didn't hear you well. I think you want to fly from
<fill_depart_city leg="1">seattle</fill_depart_city> If this is okay, let's continue. Otherwise,
please specify the locations again.
  U15: User said: <yes>that's okay</yes>
    S16: System said: <prompt_arrive_city leg="1">what city would you like to fly to?</prompt_arrive_city>
    U17: User said: <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>
     S18: System said: flying to <fill_arrive_city leg="1">togiak</fill_arrive_city>
<prompt_date leg="1">on what date?</prompt_date>
  U19: User said: <fill_date leg="1">monday September twenty fifth</fill_date>
  S20: System said: there don't seem to be any flights from <fill_depart_city leg="1">seattle</fill_depart_city>
to <fill_arrive_city leg="1">togiak</fill_arrive_city> which depart
<fill_depart_time leg="1">about ten PM</fill_depart_time> on
<fill_date leg="1">september twenty fifth</fill_date>.
  U21: User said:  destination <fill_arrive_city leg="1">tokyo japan</fill_arrive_city>
  S22: System said: did you say you want to fly to
<prompt_arrive_city leg="1">san diego</prompt_arrive_city>?
Conference on Speech Communication and Technology
(Berlin, Germany 1993).
[6] Levow, G.A. Characterizing and recognizing spoken
corrections in human-computer dialogue. Proceedings of
COLING-ACL (Montreal, Canada 1998).
[7] Levow, G.A. Understanding recognition failures in spoken
corrections in human-computer dialogue. Proceedings of
ECSA Workshop on Dialogue and Prosody (Eindhoven,
The Netherlands 1999).
[8] Luo, X. and Papineni, K. IBM DARPA Communicator v1.0.
DARPA Communicator Principle Investigators Meeting
(Philadelphia, PA USA 2000). http://www.dsic-web.net
/ito/meetings/communicator_sep2000/
[9] Polifroni, J. and Seneff, S. Galaxy-II as an architecture for
spoken dialogue evaluation. Proceedings of the Second
International Conference on Language Resources and
Evaluation (Athens, Greece 2000).
[10] Rudnicky, A. CMU Communicator. DARPA Communicator
Principle Investigators Meeting (Philadelphia, PA USA
2000). http://www.dsic-web.net/ito/meetings
/communicator_sep2000/
[11] Walker, M., Langkilde, I., Wright, J., Gorin, A. and Litman,
D. Learning to predict problematic situations in a spoken
dialogue system: experiments with how may I help you?
Proceedings of the Seventeenth International Conference
on Machine Learning (Stanford, CA USA 2000).
[12] Walker, M., Wright, J. and Langkilde, I. Using natural
language processing and discourse features to identify
understanding errors in a spoken dialogue system.
Proceedings of the North American Meeting of the
Association of Computational Linguistics (Seattle, WA
USA 2000).
Integrated Feasibility Experiment for Bio-Security: IFE-Bio
A TIDES Demonstration
Lynette Hirschman, Kris Concepcion, Laurie Damianos, David Day, John Delmore, Lisa Ferro,
John Griffith, John Henderson, Jeff Kurtz, Inderjeet Mani, Scott Mardis, Tom McEntee, Keith
Miller, Beverly Nunan, Jay Ponte, Florence Reeder, Ben Wellner, George Wilson, Alex Yeh
The MITRE Corporation
Bedford, Massachusetts, USA and
McLean, Virginia, USA
781-271-7789
lynette@mitre.org
ABSTRACT
As part of MITRE?s work under the DARPA TIDES
(Translingual Information Detection, Extraction and
Summarization) program, we are preparing a series of
demonstrations to showcase the TIDES Integrated Feasibility
Experiment on Bio-Security (IFE-Bio).  The current
demonstration illustrates some of the resources that can be made
available to analysts tasked with monitoring infectious disease
outbreaks and other biological threats.
Keywords
Translation, information extraction, summarization, topic
detection and tracking, system integration.
1. INTRODUCTION
The long-term goal of TIDES is to provide delivery of
information on demand in real-time from live on-line sources. For
IFE-Bio, the resources made available to the analyst include e-
mail, news groups, digital library resources, and eventually (in
later versions), topic-specific segments from broadcast news.
Because of the emphasis on global monitoring, there is a need to
process incoming information in multiple languages.  The system
must deliver the appropriate information content in the
appropriate form and in the appropriate language (taken for now
to be English). This means that the IFE-Bio system will have to
deliver news stories, clusters of relevant documents, threaded
discussions, alerts on new events, tables, summaries (particularly
over document collections), answers to questions, graphs and geo-
spatial  temporal displays of information.
The demonstration system for the Human Language Technology
Conference in March 2001 represents an early stage of the full
IFE-Bio system, with an emphasis on end-to-end processing.
Future demonstrations will make use of MITRE?s Catalyst
architecture, providing an efficient, scalable architecture to
facilitate  integration of multiple stages of linguistic processing.
By June 2001, the IFE-Bio system will provide richer linguistic
processing through the integration of modules contributed by
other TIDES participants. By June 2002, the IFE-Bio system will
include additional functionality, such as real-time broadcast news
feeds, new machine translation components, support for question-
answering, cross-language information retrieval, multi-document
summarization, automatic extraction and normalization of
temporal and spatial information, and automated geospatial and
temporal displays.
2. The IFE-Bio System
The current demonstration (March 2001) highlights the basic
functionality required by an analyst, including:
? Capture of sources, including e-mail, digital library
material, news groups, and web-based resources;
? Categorizing of the sources into multiple orthogonal
hierarchies useful to the analyst, e.g., disease, region, news
source, language;
? Processing of the information through various stages,
including ?zoning? of the text to select the relevant portions
for processing; named entity detection, event detection,
extraction of temporal information, summarization, and
translation from Spanish, Portuguese, and Chinese into
English;
? Access to the information through use of any mail and news
group reader, which allows the analyst to organize, save, and
share the information in a familiar, readily accessible
environment;
? Display of the information in alternate forms, including
color-tagged documents, tables, summaries, graphs, and
geospatial, map-based displays.
Figure 1 below shows the overall functionality envisioned
for the IFE-Bio system, including capture, categorizing,
processing, access and display.
Collection capability for the current IFE-Bio system includes
email, news groups, journals, and Web resources. We have a
complete copy of the ProMED mailings (a moderated source
tracking global infectious disease outbreaks), and are routinely
collecting other information sources from the World Health
Organization and CDC.  In addition, we are collecting several
general global news feeds. Current volume is around 2000
messages per day; we estimate capacity for the current system at
around 4500 messages/day. Once we have integrated a filtering
capability, we expect the volume of messages saved in IFE-Bio
should drop significantly, since many of the global news services
report on a wide range of events and not all need to be passed on
to IFE-Bio analysts.  The categorizing of sources is done based on
the message header. The header is synthesized by extracting key
information about disease name, the country, and other relevant
information such as type of victim and source of information, as
well as date of message receipt.
The processing for the current demonstration system uses a
limited subset of the Catalyst architecture capabilities and a
number of in-house linguistic modules. The linguistic modules in
the current demonstration system include tokenization, sentence
segmentation, part-of-speech tagging, named entity detection,
temporal extraction (Mani and Wilson 2000) and source-specific
event detection.  In addition, we have incorporated the
CyberTrans embedded machine translation system which ?wraps?
available machine translation engines to make them available via
an e-mail or Web interface (Reeder 2000). Single document
summarization is performed by the MITRE WebSumm system
(Mani and Bloedorn 1999).
We carefully chose a light-weight interface mechanism for
delivery of the information to the analyst.  By treating the
incoming streams of data as feeds to a news server, the analyst can
inspect and organize the information using a familiar news and e-
mail browser. The analyst can subscribe to areas of interest, flag
important messages, watch specific threads, and create tailored
filters for monitoring outbreaks. The stories are crossed-posted to
multiple relevant news groups, based on the information in the
header, e.g., a story on Ebola in Africa would be cross posted to
the Africa regional newsgroup and to the Ebola disease
newsgroup. Search by subject and date allow the analyst to select
subsets of the messages for further processing, annotation or
sharing.  The news client provides notification of incoming
messages. In later versions, we plan to integrate topic detection
and tracking capabilities, to provide improved filtering and
routing of messages, as well as detection of new topics.  The use
of this simple delivery mechanism provides a familiar
environment with almost no learning curve, and it avoids issues of
platform and operating system dependence.
Finally, the system makes use of several different devices to
display the information appropriately. Figure 2 shows the layout
of the Netscape news browser interface.  It includes the list of
newsgroups that have been subscribed to (on the left), the list of
messages from the chosen newsgroup (on top), and a particular
message with color-coded named entities (including disease terms
displayed in red, so that they are easy to spot in the message).
What is the status of the
current Ebola outbreak?
The epidemic is contained;
as of 12/22/00, there were 
421 cases with 162 deaths
Interaction
CDC
WHO
Medical
literatureEmail:
ProMed
~ 2500
 stories/day
Internl
News
Sources  Capture
Translingual 
Information 
Detection 
Extraction 
Summarization 
U niden tif ied h emor rhagic  f
U niden tif ied h emor rhagic  f
Ebola hemorr hagic  fever  in
Re :  Ebo la hemorrhagi. ..
R e: Ebola hemo rrha gi...
ProMED
A nnotator
Ja ne Analyst
10 /17/00 1 9:37
10 /17/00 2 0:42
10 /18/00 7 :42
High
Norm al
Normal
read
rep lied
ProMED
10 /18/00 1 2:3 4 High un read
Ebola hemorr hagic  fever  in
Sour ce
D ate
Priority Status
10   99
0   105
1   57
0   10
2   34
0   50
1   1
0   25
5   200
0   45
0   0
0   0
0   0
0   0
0   6
0   32
0   3
0   1
High
Norm al
High
High
Ebola hemorrhagic feve r  -  Ugan da
U nf ilte red
O utbr eak
     C holer a
     D engue  Fe ve r
     Eb ola
I nfras tructure?
N atu ral  Di sas. ..
Spi lls
A cc id en ts
W M D Tra ckin. ..
Sus picious Il ln. ..
Sus picious De.. .
Pos sible  Biol o. ..
Pathogen threa ?
- --- --- ---------------------
W orkspa ce
      E bola
      D ra fts
      Re ports
D isease
R e: Ebola hemo rrha gi...
Location
U NK
U NK
Ebola
Ebola
Ebola
Ebola
Rabies
Rabies
U gan da
U gan da
U gan da
K eny a
U gan da
IHT
ProMED
WHO
Jo e Analyst
D ate
10 /14/00 2 3:06
10 /15/00 1 0:50
10 /16/00 2 1:45
10 /17/00 1 9:12
read
read
read
read
un read
Date: 10/16/00
Disease: Ebo la
Descripto r: hem orrh agic fever
Locatio n:          Ugan da
Disease Date:     10/14/00
Ho spital: mission ary hosp ital  in Gulu
New cases:  at least  7
Total  cases: 51
Total  dead:       31
Ebola hemorr hagic  fever  -
Ugand an M ini stry  ide ntif ies Eb ola  virus as t he c ause of  the outbreak.  KA MP ALA :
The  dreade d Eb ola  virus that struck over 300  peopl e i n Kikwit,  in  t he D emocratic
Rep ub lic  of Con go  in  1995, has ki lled 31  people in northe rn Ugan da.  A  U gandan
M ini s tr y of Heal th  sta tement  said l aboratory test s had r eveale d that  the Ebola vi rus
was  t he caus e of the  epidemi c hemorr hagic feve r whi ch has been r agi ng in the  G ulu
dis trict  since Septe mbe r.   Thr ee  of the dea d wer e s tud ent nur ses , who tre ated the first
Eb ola  patients admitt ed to a  Lac or  mis sionary hosp it al in  Gu lu  tow n.  A  task force
he ade d by G ul u dis trict adm ini str ator, Walte r O ch ora , has bee n se t up to co-or dina te
efforts to control the epi demi c.  F ie ld offic ials i n  Gul u tol d the Ka mpala-based Ne w
H ttp: //ti des2000.mi tre.org/
Pr oM ED /10162000/34n390h.ht ml
U gan da
News Repository
CATALYSTEntity Tagging
Event Extraction
Translation
Summarization
Alerting
Change detection
Threading
Cross-language IR
Topic clustering
Figure 1: Overview of the IFE-Bio Demonstration System
Local,
private
workspace
Documents
automatically
categorized
into shared,
tailorable
hierarchy
Sort by disease, location, source, date, etc.
Associated
meta-data:
header,
event,
summary,
named-entity
Figure 2: Screenshot of IFE-Bio Interface Using News Group Reader
Figure 3: Sample Summarization Automatically Generated by WebSumm
There are multiple display modalities available. The message in
Figure 2 contains a short tabular display in the beginning,
identifying disease, region and victim type. Below that is a URL
to a document summary, created by MITRE?s WebSumm system
(see Figure 3 for a sample summary).   If an incoming message is
in a language other than English, then CyberTrans is called to run
code set and language identification modules, and the language is
translated into English for further processing. Figure 4 below
shows a sample translated message; note that there are a number
of untranslated words, but it is still possible to get the gist of the
message.
In addition, we are working on a mechanism to provide
geographic and eventually, temporal display of outbreak
information. Figure 5 shows the stages of processing involved.
Stage 1 shows onamed entity and temporal tagging to identify the
items of interest. These are combined into disease events by
further linguistic processing; the result is shown in the table in
Stage 2. This spreadsheet of events serves as input for a map-
based display, shown in Stage 3. The graph plots number of new
cases and number of cumulative cases over time.  In the map, the
size of the outer dot represents total number of cases to date, and
the inner dot represents new cases.  This allows the analyst to
visualize spread of the disease, as well as the stage of the outbreak
(spreading or subsiding).
3. REFERENCES
[1] Mani, I. and Bloedorn, E. (1999). "Summarizing
Similarities and Among Related Documents".
Information Retrieval 1(1): 35-67.
[2] Mani, I. and Wilson, G. (2000). "Robust Temporal
Processing of News," Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL'2000), 69-76. New Brunswick, New
Jersey. Association for Computational Linguistics.
[3] Reeder, F.  (2000) "At Your Service:  Embedded MT
as a Service",  NAACL Workshop on Embedded MT,
March, 2000.
Figure 4: Translation from Portuguese to English Produced by CyberTrans
1. Annotate entities of interest via XML
Dise a se Source Country City_na m eDa te Ca se s Ne w _ca se s De a d
Ebola PROM ED Uganda G ula 26-O ct-2000 182 17 64
Ebola PROM ED Uganda G ula 5-Nov-2000 280 14 89
Ebola PROM ED Uganda G ulu 13-O ct-2000 42 9 30
Ebola PROM ED Uganda G ulu 15-O ct-2000 51 7 31
Ebola PROM ED Uganda G ulu 16-O ct-2000 63 12 33
Ebola PROM ED Uganda G ulu 17-O ct-2000 73 2 35
Ebola PROM ED Uganda G ulu 18-O ct-2000 94 21 39
Ebola PROM ED Uganda G ulu 19-O ct-2000 111 17 41
2. Assemble entities into events
0
50
100
150
200
250
300
350
400
10/
13/
200
0
10/
20/
200
0
10/
27/
200
0
11/
3/2
000
11/
10/
200
0
11/
17/
200
0
11/
24/
200
0
T IME
Nu
m
be
r C
as
es
Cases
New_cases
Dead
3. Display events...
   Total Cases   New Cases
Figure 5: Steps in Extraction to Support Temporal and Geospatial Displays of Disease Outbreak
MiTAP for SARS Detection 
 
 
Laurie E. Damianos, Samuel Bayer, 
Michael A. Chisholm, John Henderson,  
Lynette Hirschman, William Morgan, 
Marc Ubaldino, Guido Zarrella 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730 
{laurie, sam, chisholm, 
jhndrsn, lynette, wmorgan, 
ubaldino,jzarrella}@mitre.org 
James M. Wilson, V, MD and  
Marat G. Polyak 
Division of Integrated Biodefense 
ISIS Center, Georgetown University 
2115 Wisconsin Avenue Suite 603 
Washington, DC 20007 
{wilson, mgp5} 
@isis.imac.georgetown.edu 
 
Abstract 
The MiTAP prototype for SARS detection 
uses human language technology for detect-
ing, monitoring, and analyzing potential indi-
cators of infectious disease outbreaks and 
reasoning for issuing warnings and alerts. Mi-
TAP focuses on providing timely, multi-
lingual information access to analysts, domain 
experts, and decision-makers worldwide. Data 
sources are captured, filtered, translated, 
summarized, and categorized by content. 
Critical information is automatically extracted 
and tagged to facilitate browsing, searching, 
and scanning, and to provide key terms at a 
glance. The processed articles are made avail-
able through an easy-to-use news server and 
cross-language information retrieval system 
for access and analysis anywhere, any time. 
Specialized newsgroups and customizable fil-
ters or searches on incoming stories allow us-
ers to create their own view into the data 
while a variety of tools summarize, indicate 
trends, and provide alerts to potentially rele-
vant spikes of activity. 
1 
2 
Background 
Potentially catastrophic biological events that threaten 
US national security are steadily increasing in fre-
quency. These events pose immediate danger to ani-
mals, plants, and humans. Current disease surveillance 
systems are inadequate for detecting indicators early 
enough to ensure the rapid response needed to combat 
these biological events and corresponding public reac-
tion. Recent examples of outbreaks include both the 
HIV/AIDS and foot and mouth pandemics, the spread of 
West Nile virus to and across the US, the escape of Rift 
Valley Fever from Africa, SARS, and the translocation 
of both mad cow disease (BSE) and monkey pox to the 
United States.  
Biological surveillance systems in the United States 
rely most heavily on human medical data for signs of 
epidemic activity. These systems span multiple organi-
zations and agencies, are often not integrated, and have 
no alerting capability. As a result, responders have an 
insufficient amount of lead time to prepare for biologi-
cal events or catastrophes. 
Indications and Warnings (I&Ws) provide the poten-
tial for early alert of impending biological events, per-
haps weeks to months in advance. Sources of I&Ws 
include transportation data, telecommunication traffic, 
economic indices, Internet news, RSS feeds (RSS) in-
cluding weblogs, commerce, agricultural surveillance, 
weather, and other environmental data. Retrospective 
analyses of major infectious disease outbreaks (e.g., 
West Nile Virus and SARS) show that I&Ws were pre-
sent weeks to months in advance, but these indicators 
were missed because data sources were difficult to ob-
tain and hard to integrate. As a result, the available in-
formation was not utilized for appropriate national and 
international response. This illuminates a critical need in 
biodefense for an integrated system linking I&Ws for 
biological events from multiple and disparate sources 
with the response community. 
Introduction 
MiTAP (Damianos et al 2002) was originally devel-
oped by the MITRE Corporation under the Defense 
Advanced Research Projects Agency (DARPA) 
Translingual Information Detection Extraction and 
Summarization (TIDES) program. TIDES aims to revo-
lutionize the way that information is obtained from hu-
man language by enabling people to find and interpret 
relevant information quickly and effectively, regardless 
of language or medium. MiTAP was initially created for 
tracking and monitoring infectious disease outbreaks 
and other biological threats as part of a DARPA Inte-
grated Feasibility Experiment in biosecurity to explore 
the integration of synergistic TIDES language process-
ing technologies applied to a real world domain. The 
system has since been expanded to other domains such 
as weapons of mass destruction, satellite monitoring, 
and suspect terrorist activity. In addition, researchers 
and analysts are examining hundreds of MiTAP data 
sources for differing perspectives on conflict and hu-
manitarian relief efforts. 
Our newest MiTAP prototype explores the integra-
tion of outputs from operational data mining (anomaly 
detection), human language technology (information 
extraction, temporal tagging, machine translation, cross-
language information retrieval), and visualization tools 
to detect SARS-specific I&Ws in Asia, with relevance 
to pathogen translocation to the United States. Using 
feeds from English and Chinese language newswire, 
weblogs, and other Internet data, the system translates 
Chinese text data and tracks keyword combinations 
thought to represent I&Ws specific to SARS outbreaks 
in China. Analysts can use cross-language information 
retrieval for retrospective analysis and improving the 
I&W model, save searches to use as filters on incoming 
data, view trends, and visualize the data along a time-
line. Figure 1 shows an overview of the prototype. 
Warnings generated by this MiTAP prototype are in-
tended to complement traditional biosurveillance and 
communications already in use by the international pub-
lic health community. This system represents an expan-
sion of current US surveillance capabilities to detect 
biological agents of catastrophic potential.
 
 
Figure 1 Overview of the MiTAP prototype for SARS detection. 
3 Component Technologies 
The MiTAP prototype relies extensively on human 
language technology and expert system reasoning. 
Below, MiTAP capabilities are described briefly 
along with their contributing component 
technologies. 
3.1 
3.2 
3.3 
3.4 
3.5 
Information Processing 
After Internet news sources are captured and 
normalized, they are passed through a zoner using 
human-generated rules to identify source, date, and 
other information such as headline, or title, and 
content. The Alembic natural language analyzer (Ab-
erdeen et al 1995; Vilain and Day 1996) processes 
the zoned messages to identify paragraph, sentence, 
and word boundaries as well as part-of-speech tags. 
The messages then pass through the Alembic named 
entity recognizer for identification and tagging of 
person, organization, location, and disease names. 
Finally, the article is processed by the TempEx 
normalizing time expression tagger (Mani and Wil-
son 2000). 
For Chinese and other non-English sources, the 
CyberTrans machine translation system (Miller et al 
2001) is used to translate articles automatically into 
English. CyberTrans wraps commercial and research 
translation engines to produce a common set of 
interfaces; the current prototype makes use of the 
SYSTRAN Chinese-English system.  
RSS feeds can provide a high volume textual ge-
stalt.  Weblogs, in particular, are a good source of 
timely text, some of which is topical and all of which 
is based on personal observations and experiences. 
Aggregate measurements on these feeds can provide 
indications of public health-related phenom-
ena.  Consider the relative rates of words and phrases 
such as "stay home from" or "pneumonia.?  Geotem-
poral location of non-seasonal spikes in relative rank 
of these strings can establish suspicion for further 
investigation by I&W experts. 
Browsing 
English language data and pairs of foreign language 
documents and their translated versions are made 
available on a news server (INN 2001) for browsing. 
The system categorizes and bins articles into 
newsgroups based on their content. To do this, the 
system relies on a combination of the information 
extraction results as well as human-generated rules 
for pattern matching. Newsgroups are created to 
provide multiple perspectives on the data; analysts 
can subscribe to specific disease tracking 
newsgroups, regional newsgroups, specific data 
source newsgroups, or to customized topic tracking 
newsgroups that may be based on several related 
subjects. 
Tagged entities in each article are color-coded to 
enable rapid scanning of information and easy identi-
fication of key names. The five most frequently men-
tioned locations in each article as well as the top five 
people are presented as a list for quick reference. 
Information Retrieval 
To supplement access to the articles on the news 
server and to allow for retrospective analysis, articles 
are indexed using the Lucene information retrieval 
system (The Jakarta Project 2001) for English 
language documents and using PSE (Darwish 2002) 
for foreign language documents. Web links are 
maintained between foreign language documents and 
their translated versions to allow for more accurate 
human translations of selected documents. 
Analysts can perform full text, source-specific 
queries over the entire set of archived documents and 
view the retrieved results as a relevance-ranked list or 
as a plot across a timeline. A cross-language informa-
tion retrieval interface allows users to search in Eng-
lish across the Chinese language sources. 
Users can also save specific search constraints to 
be used as filters on incoming data. These saved 
searches provide a simple analytic capability as well 
as an alerting feature. (See below.) 
Analysis 
To assist analysts in identifying relevant and related 
articles, we have integrated multi-document summa-
rization and watch lists. Columbia University?s 
Newsblaster (McKeown et al 2002) automatically 
detects daily topics, clusters MiTAP articles around 
those topics, and generates multi-document summari-
zations which are made available on the news server. 
Multiple technologies (e.g., coreference, information 
extraction) from Alias I, Inc. (Baldwin et al 2002) 
produces comprehensive views on specific named 
entities (i.e., people or disease) across MiTAP docu-
ments. These views are summarized through ranked 
lists, highlighting important topics of the day and 
activities which might indicate disease outbreak.  
Finely-tuned searches can be saved and applied as 
filters or topic tracking mechanisms. These saved 
searches are automatically updated at specific inter-
vals and can be aggregated and displayed visually as 
bar graphs to reveal spikes of activity that otherwise 
might go undetected. 
Alerting 
The MiTAP prototype has two separate alerting ca-
pabilities: saved searches and an integrated expert 
system. The saved search functionality allows ana-
lysts to set thresholds for alerting purposes. For ex-
ample, MiTAP can send email when any new article 
arrives, when a specified maximum number of arti-
cles arrives, or when the daily number of new articles 
increases by some percentage of the total or moving 
average. 
The Human Language Indication Detector 
(HLID) performs data fusion on a number of dispa-
rate sources, compressing a large volume of informa-
tion into a smaller but more significant set of alerts. 
HLID monitors a variety of sources including MiTAP 
articles, information events in RSS feeds, and other 
dynamically updated information on the World Wide 
Web. HLID analyzes events from these sources in 
real time and generates an estimate of significance 
for each, complete with an audit trail of supporting 
and negating evidence. This allows an analyst to di-
rect a search for indicators towards interesting data 
while reducing the time spent investigating false 
alarms and insignificant events.  
HLID is composed of four major components. 
The first is an event collector, which monitors a data 
source and triggers action when an event is observed. 
These events are sent to the rule based reasoning en-
gine, an expert system shell (JESS 2004) with hand 
authored rules. The engine performs vetting and ini-
tial investigation of each event by identifying corre-
lated events, corroborating or invalidating evidence, 
and references to supporting information. The engine 
can also supplement its knowledge base by perform-
ing a directed search via the query management sys-
tem, which allows retrieval of information from a 
wide variety of sources including databases and web 
pages. Lastly, the alerting mechanism disseminates 
the conclusions reached by the system and provides 
an interface that allows an analyst to launch a deeper 
search for indicators and warnings. 
4 
5 
Acknowledgments 
This work has been funded, in part, by the Defense 
Advanced Research Projects Agency Translingual 
Information Detection Extraction and Summarization 
program under contract numbers DAAB07-01-C-
C201 and W15P7T-04-C-D001, the Office of the 
Secretary of Defense in support of the Coalition Pro-
visional Authority in Baghdad, and a MITRE Special 
Initiative for Rapid Integration of Novel Indications 
and Warnings for SARS. 
References 
Aberdeen, J., Burger, J., Day, D., Hirschman, L., 
Robinson, P., and Vilain, M. 1995. MITRE: De-
scription of the Alembic System as Used for 
MUC-6. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). 
Baldwin, B,, Moore, M., Ross, A., Shah, D.  2002. 
Trinity Information Access System. Proceedings of 
Human Lanuage Technology Conference, San 
Diego, CA. 
Damianos, L., Ponte, J., Wohlever, S., Reeder, F., 
Day, D., Wilson, G., Hirschman, L. 2002. MiTAP, 
Text and Audio Processing for Bio-Security: A 
Case Study In Proceedings of IAAI-2002: The 
Fourteenth Innovative Applications of Artificial 
Intelligence Conference, Edmonton, Alberta, Can-
ada. 
Darwish, K. PSE: A Small Search Engine written in 
Perl 2002 
http://tides.umiacs.umd.edu/software.html 
INN: InterNetNews, Internet Software Consortium 
2001, http://www.isc.org/products/INN.  
The Jakarta Project, 2001 
http://jakarta.apache.org/lucene/docs/index.html. 
JESS: the Rule Engine for the Java? Platform 2004 
http://herzberg.ca.sandia.gov/jess/  
Mani, I. and Wilson, G. 2000. Robust Temporal 
Processing of News. In Proceedings of the 38th 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL'2000), 69-76. 
McKeown, K., Barzilay, R., Evan, D., Hatzivassi-
loglou, V., Klavans, J., Sable, C., Schiffman, B., 
Sigelman, S. 2002. Tracking and Summarizing 
News on a Daily Basis with Columbia's Newsblas-
ter. In Proceedings of HLT 2002: Human Lan-
guage Technology Conference. 
Miller, K., Reeder, F., Hirschman, L., Palmer, D. 
2001. Multilingual Processing for Operational 
Users, NATO Workshop on Multilingual Process-
ing at EUROSPEECH. 
RSS RDF Site Summary http://purl.org/rss/1.0/spec 
Vilain, M. and Day, D. 1996. Finite-state phrase 
parsing by rule sequences. In Proceedings of the 
1996 International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark. 
Comparing Several Aspects of Human-Computer and
Human-Human Dialogues
Christine Doran, John Aberdeen, Laurie Damianos and Lynette Hirschman
The MITRE Corporation
202 Burlington Road
Bedford, MA 01730 USA
{cdoran,aberdeen,laurie,lynette}@mitre.org
Abstract
While researchers have many intuitions
about the differences between human-
computer and human-human interac-
tions, most of these have not previously
been subject to empirical scrutiny. This
work presents some initial experiments
in this direction, with the ultimate goal
being to use what we learn to improve
computer dialogue systems. Working
with data from the air travel domain,
we identified a number of striking dif-
ferences between the human-human and
human-computer interactions.
1 Introduction
In our initial experiments comparing human-
human (HH) and human-computer (HC) inter-
action we have annotated dialogues from the air
travel domain with several sets of tags: dialogue
act, initiative and unsolicited information. Our
aim is to begin an empirical exploration of how
these aspects of the dialogue shed light on dif-
ferences between HH and HC interactions. We
found striking differences between the human-
human and human-computer interactions. With
many of the issues we examine here, researchers
have voiced strong intuitions about the differences
between HH and HC communication, but these in-
tuitions have not previously been subject to em-
pirical scrutiny.
Why do we want to compare HH and HC in-
teractions? We believe that an examination of
the differences between HH and HC dialogues can
help those working on the HC interactions to im-
prove their systems. This will not necessarily
mean making the HC interactions ?more like? HH
interactions; rather, we believe that such analy-
sis can give us insights about the appropriateness
and success of various communicative approaches
in different settings. We are also interested in
quantifying what it means for a dialogue to be
?mixed-initiative?. There is liberal use of this
term in work on human-computer dialogues, but
there does not seem to be a clear sense of what it
really means and how to define it.
This work is being done in the context of the
Communicator Travel task. Communicator is a
DARPA-funded program involving major indus-
try and academic sites, established to provide the
next generation of intelligent conversational inter-
faces to distributed information. The goal of the
program is to move beyond the current system
initiated voice menu style of dialogue interaction
towards a more flexible strategy of shared con-
versational control between human and system.
Work up to the fall of 2000 concentrated on the
travel task, but groups are now moving into other
domains with their dialogue systems.
2 Our Data
We have tagged 20 HH dialogues and 40
HC dialogues; the HH dialogues consist of
25,208 words and 1742 turns, and the HC
dialogues consist of 8313 words and 957
turns. The HH dialogues are from the SRI
ATIS collection (available at www.ai.sri.com/?
communic/amex/amex.html) and the CMU Travel
data (www.speech.cs.cmu.edu). The HC dia-
logues are from a subset of the systems partici-
pating in the DARPA Communicator data collec-
tion which was run in June and July of 2000.1
Subjects were selected by the U.S. National In-
stitute of Standards and Technology (NIST) and
each subject was paid to call all of the 9 partic-
ipating sites with either a fixed or open travel
itinerary. The fixed itineraries were intended to
balance flight types (one-way, round-trip, multi-
leg) and destinations (large US airport, small for-
1Ideally, of course, we would want HH dialogues
collected under similar circumstances to the HC dia-
logues. However, no such data are available and col-
lecting new HH data is not currently a feasible en-
deavor for us.
eign airport, etc.).2 We made no attempt to bal-
ance the subset of data we analyzed; we merely
selected 10 random users. We then identified four
sites whose systems we expected to differ quite
widely (based on researchers? descriptions of their
systems), and extracted the 10 users? dialogues for
each site. Each dialogue was tagged with three
sets of annotations?for dialogue acts, for initia-
tive and for provision of unsolicited information
by users (HC only). Sections 2.1, 2.2 and 3.5 de-
scribe these in more detail. The passage below
is excerpted from one of the human-computer in-
teractions, using the transcribed versions of the
user?s turns.3
User said: i would like to fly from miami
florida to sydney australia on wednesday
october eleventh please
System said: okay, from Miami to Sydney on
Wednesday October 11. Can you provide the
approximate departure time or airline?
User said: [um] i prefer american airlines
and i can leave anytime
System said: There are no nonstop flights.
I have a flight connecting in Los Angeles
leaving at 7:15 a.m. a flight connecting
in Los Angeles leaving at 5:20 p.m. and a
flight connecting in Los Angeles leaving
at 7:30 p.m. Do you like any of these?
User said: yes how about the morning one
System said: American flight 912 connecting
to American flight 7362 in Los Angeles has
been added to your itinerary. you depart
Miami at 7:15 am and arrive in Sydney at
9:40 pm the following day. Where would you
like to go next?
2.1 Dialogue Act Tagging
We have done two rounds of tagging. Our start-
ing point was a 20-tag subset of the CSTAR Con-
sortium tags (Consortium, 1999). We found the
CSTAR tag set to be the most suitable for our
task of the tagging schemes we considered, based
on information provided in the survey of dialogue
act annotation schemes assembled by the MATE
project (Klein et al, 1998). We picked the CSTAR
tags because they had been designed for task-
2It is important to note that the fixed itineraries
were assigned, i.e. not real trips, and it was also evi-
dent from the transcripts that few if any of the open
itineraries represented real trips. This may well have
had some impact on the data that were collected.
3Both human-human and human-system dialogue
transcripts are reproduced with their native format-
ting, i.e. all caps, no punctuation, etc. The system
side contains punctuation resulting from the genera-
tion process.
oriented dialogues, the tag categories were suffi-
ciently clear and simple that we believed we would
be able to tag the data reliably and, finally, the
categories captured the sorts of distinctions we be-
lieved would be relevant. We rejected the DAMSL
tag set (Core and Allen, 1997; Core et al, 1999)
on the grounds that is was too sophisticated for
our purposes, covering many aspects of dialogue
structure that were not necessarily relevant for our
task such as intentionality, grounding and context
tracking. In addition, the interannotator agree-
ment levels reported for this scheme are quite low.
Some of the other tag sets we considered were
(Carletta et al, 1995; Nakatani et al, 1995; van
Vark et al, 1996; Di Eugenio et al, 1998; Jurafsky
et al, 1997).
In collaboration with AT&T, we arrived at a
set of changes to our tag set that would make
it compatible with their efforts to tag system ut-
terances automatically (Walker and Passonneau,
2001), in the hopes of being able to share re-
sults with them more easily. We added a sit-
uation/conversation/task distinction to a num-
ber of our tags (e.g. give-information split
into give-task-info, give-situation-info and
give-conversation-info). We also added a
not-understand tag and collapsed some orig-
inal tags into super-categories. Our revised tag
set had 26 tags, and two people (one who had
also done the first round of tagging) tagged the
same data set. The situation/conversation/task
distinction turned out to be extremely difficult for
the taggers to make; we believe that revisions to
the tagging guidelines could lead to some improve-
ment on this front, but without enumerating the
kinds of utterances which fall into each category,
this will remain a difficult task.
We tagged each utterance that contained some
speech, i.e. was not composed entirely of non-
speech annotation like *pause* or [click], and
we split turns4 into utterances using guidelines
that had been developed internally for another
purpose. Utterances on this definition were
roughly clause-sized units, and possibly fragmen-
tary.5 This meant that there were often multi-
ple dialogue acts (DAs) per turn, and where there
were multiple sequential DAs of the same type, we
collapsed them under a single tag on the assump-
tion that they were combining to ?perform? that
DA. We initially split some of the CSTAR tags
4Chunk of text labelled with either User said or
Expert said. It was possible for a single speaker to have
more than one sequential turn, i.e. turn 6= speaker
change.
5In hindsight, it would have been preferable to seg-
ment the dialogues in a separate step.
into implicit and explicit versions, but found
that the implicit cases were so hard to identify
that we were not using those tags, and they were
dropped from the tag set.
Tables 1 and 2 show roughly parallel sub-
dialogues from the HH and HC data.6 Each turn
is tagged with its DA, and the first expert turn
in Table 2 shows multiple DAs within a turn, a
give-information followed by an offer.
Expert:WHAT TIME DO [req-task-info]
YOU NEED TO DEPART
User:AS SOON AS [give-task-info]
POSSIBLE AFTER FIVE P.M.
Expert:THE FIRST FLIGHT [give-task-info]
AFTER FIVE P.M. ON THAT DATE IS
AT FIVE THIRTY FIVE P.M. ARRIVING
IN CHICAGO AT SIX OH SIX P.M.
ON U.S. AIR
User: IS THAT O?HARE [req-task-info]
Table 1: DA tagging in an HH Exchange
Expert: i have an American [give-task-info]
Airlines flight departing Seattle at
twelve fifty five p.m., arrives Tokyo
at three p.m. the next day.
Is that OK? [offer]
User: yes I?ll take it [accept]
Expert: Will you return to seattle[req-task-info]
from tokyo?
User: what airport [req-task-info]
Expert: Will you return to seattle[req-task-info]
from tokyo?
Table 2: DA tagging in an HC Exchange
With our first tag set, our Kappa score for
interannotator agreement on these dialogues is
0.90 (with two annotators). Not surprisingly, our
Kappa score on the second, more complex tag set
(cf. Table 10 for a list of the tags) was lower,
0.71 (0.74 on the HC data and 0.66 on the HH
data). Both scores are in line with scores re-
ported in similar tagging tasks (Klein et al, 1998):
0.56 for DAMSL (overall average), 0.83 for Map-
task (experienced coders), 0.8-0.84 for Switch-
board DAMSL and 0.83 for VerbMobil. The drop
in score between our two tag sets emphasizes an
issue which we continue to wrestle with?the trade-
off between tag set complexity and tagging accu-
racy. At what point is it more useful to have re-
6Throughout the paper, we will use expert to refer
to either the human or the computer travel agent, sys-
tem to refer exclusively to the computer travel agent,
and user to refer to the travelers.
liable results from an impoverished tag set than
results of questionable value from a sophisticated
tag set?
2.2 Initiative Tagging
There is not a clearly agreed upon definition of ini-
tiative in the literature on dialogue analysis (but
see e.g., (Chu-Carroll and Brown, 1998; Jordan
and Di Eugenio, 1997; Flammia and Zue, 1997)),
despite the fact the terms initiative and mixed-
initiative are widely used. Intuitively, it seems
that control rests with the participant who is mov-
ing a conversation ahead at a given point, or se-
lecting new topics for conversation.
After experimenting with several tagging meth-
ods, we concluded that the approach presented
in Walker and Whittaker (1990) adopted from
(Whittaker and Stenton, 1988) best captured the
aspects of the dialogue we were interested in and,
as with the DAs, could be tagged reliably on our
data.
Each turn is tagged with which participant has
control at the end of that turn, based on the utter-
ance type. Again, we did not tag turns composed
entirely of non-speech annotation, and we also ex-
cluded conventional openings and closings, follow-
ing Walker and Whittaker. Below, we list the
rules for tagging each utterance type; a prompt
is an utterance ?which did not express proposi-
tional content, such as Yeah, Okay, Uh-huh, . . . .?
(Op cit, p. 3) The classification refers to the il-
locutionary force of the item, rather than to its
particular syntactic form.
Assertion: speaker has initiative unless it is a
response to a question or command7
Question: speaker has initiative unless it is a re-
sponse to a question or command
Command: speaker has initiative
Prompt: hearer has initiative
Tables 3 and 4 show the same passages used
above, but this time tagged for initiative. To give
a sense of how the tagging rules are applied, let us
step through the HC example (Table 4). Turn (1)
is assigned expert-initiative, because it is an
assertion which is not a response to any preceding
question or command. Turn (2) is still expert-
initiative, because it is an answer to the ques-
tion Is that OK?. The third turn is a question
and expert-initiative, but turn (4) is user-
initiative because it is a question that is not a
response to the previous question. The system
7Italics show our modification to the rule.
does not address the user?s question, but rather
repeats its own question, so the final turn (5) is
expert-initiative.
Expert:WHAT TIME DO YOU [exp-init]
NEED TO DEPART
User:AS SOON AS POSSIBLE [exp-init]
AFTER FIVE P.M.
Expert:THE FIRST FLIGHT AFTER [exp-init]
FIVE P.M. ON THAT DATE IS AT
FIVE THIRTY FIVE P.M.
ARRIVING IN CHICAGO AT
SIX OH SIX P.M. ON U.S. AIR
User:IS THAT O?HARE [user-init]
Table 3: Initiative tagging in an HH Exchange
(1)Expert: i have an American [exp-init]
Airlines flight departing Seattle at
twelve fifty five p.m. , arrives Tokyo
at three p.m. the next day.
Is that OK?
(2)User: yes I?ll take it [exp-init]
(3)Expert: Will you return to seattle [exp-init]
from tokyo?
(4)User: what airport [user-init]
(5)Expert: Will you return to seattle [exp-init]
from tokyo?
Table 4: Initiative tagging in an HC Exchange
Our Kappa scores for interannotator agreement
on the initiative tagging were somewhat lower
than for DA tagging. Here, ?=0.68. In fact, our
agreement was rather high, at 87%, but because
there were so few instances of user initiative in
the HC dialogues, our agreement would have to
be exceptional to be reflected in a higher Kappa
score. While we had believed this to be the easier
task, with quite clear guidelines and only a binary
tagging choice, it in fact proved to be quite diffi-
cult. We still believe that this tag set can give
us useful insights into our data, but we would
be interested in attempting further revisions to
the tagging guidelines, particularly as regards the
definition of an ?answer?, i.e. when an answer is
responsive and when it is not.
3 Analysis
We found a number of interesting differences be-
tween the HH and HC dialogues. While we have
not yet been able to test our hypotheses about
why these differences appear, we will discuss our
ideas about them and what sorts of further work
we would like to do to subject those ideas to em-
pirical validation.
3.1 Initiative Distribution
Based on researchers? descriptions of their systems
(i.e. for the most part, ?highly mixed-initiative?),
we had expected to find some variance in the dis-
tribution of initiative across systems. As is ev-
ident from Table 5, the HC systems do not dif-
fer much from each other, but taken as whole,
the dialogues differ dramatically from the HH di-
alogues. In the HH dialogues, users and expert
share the initiative relatively equitably, while in
the HC data the experts massively dominate in
taking the initiative. Here, we are simply counting
the number of turns tagged as user-initiative or
expert-initiative.8
We also show turns to completion and overall
user satisfaction scores for each system as a refer-
ence point. User satisfaction was calculated from
five questions asked of each user after each dia-
logue. The questions use a 5-point Likert scale.
Turns to completion measures the total number
of on-task turns. We found no significant corre-
lations here, but cf. Walker et al (2001) which
provides more detailed analyses of the Communi-
cator dialogues using user satisfaction and other
metrics, within the PARADISE framework. It is
worth noting, however, that the HC D has both
the highest percentage of expert initiative and the
highest satisfaction scores, so we should not con-
clude that more initiative will necessarily lead to
happier users.
% Exp % User Turns to User
Init Init Comp Sat
HC A 86.8% 13.2% 40.5 60.0%
HC B 89.9% 10.1% 41.4 71.5%
HC C 90.6% 9.4% 36.0 68.5%
HC D 93.7% 6.3% 43.9 82.8%
HH SRI 48.3% 51.7% N/A N/A
HH CMU 54.0% 46.0% N/A N/A
Table 5: Percentages of User and Expert Initiative
in HH and HC Dialogues
In the HC dialogues, we also see a difference in
success rate for user-initiative turns. By our defi-
nition, the user ?succeeds? in taking the initiative
in the dialogue if the system responds to the initia-
tive on the first possible turn. The rate of success
8A cautionary note is warranted here. We are
not suggesting that more user-initiative is intrinsically
preferable; it may well turn out to be the case that
a completely system-directed dialogue is more pleas-
ant/efficient/etc. Rather, we are seeking to quantify
and assess what it means to be ?mixed-initiative? so
that we can better evaluate the role of initiative in
effective (task-oriented) dialogues.
is the ratio of successful user-initiatives attempts
to total user-initiatives attempts. There appears
to be a negative relationship between number of
initiative attempts and their success rate. See
Figure 1, below. HC D has a high success rate
for a relatively small number of user-initiative at-
tempts. HC A has many more occurrences of user
initiative, but does not incorporate them as well.
Figure 1: User-Initiative and Success Rate per
System
There is no determinable relationship between
user experience (i.e., the number of calls per sys-
tems) and either the amount of user-initiative or
the success rate of user-initiative.
We also looked at user-initiative with re-
spect to dialogue act type. Most user-initiatives
are request-action (26%) and request-
information (19%). Request-information
dialogue acts (e.g., What cities do you know in
Texas?, Are there any other flights?, Which air-
port is that?) are handled well by the systems
(83% success rate) while request-action dia-
logue acts (e.g., start over, scratch that, book that
flight) are not (48%). Most of the user-initiatives
that are request-action dialogue acts are the
start over command (16% of the total user-
initiatives). Corrections to flight information pre-
sented by the systems consist of 20% of the total
user-initiatives.
3.2 Overall Verbosity
In counting the number of words used, we find
that the computer experts are much more verbose
than their human users, and are relatively more
verbose than their human travel agent counter-
parts. In the HH dialogues, experts average 10.1
words/turn, while users average 7.2. In the HC di-
alogues on average, system have from 16.65-33.1
words/turn vs. the users? 2.8-4.8 words/turn. Fig-
ure 2 shows these differences for each of the four
systems and for the combined HH data.
Figure 2: Words per turn for users and experts in
the HH and HC dialogues
3.2.1 Short vs. Long Confirmations
One DA which is a basic conversational tool and
therefore an interesting candidate for analysis is
the use of confirmations. Instances of short con-
firmation, typically back-channel utterances such
as okay and uh huh were tagged as acknowl-
edge, while instances of long confirmation, as
when one participant explicitly repeats something
that the other participant has said, were tagged
as verify-X, where X=conversation-action,
task-information and task-action, This tag-
ging allows us to easily calculate the distribution
of short and long confirmations.
Overall we found in the HC dialogues a rather
different confirmation profile from the HH dia-
logues. In the HC dialogues, the systems use both
types of confirmation far more than the users do
(246 total system, 8 total user). Moreover, sys-
tems use long confirmation about five times more
often (210 vs. 36) than they use short confirma-
tion. In contrast, the experts in the HH dialogues
use somewhat more confirmations than users (247
vs. 173), but both parties use far more short than
long confirmations (340 vs. 80), just the reverse
of the HC situation. This difference partially ac-
counts for the total word count differences we saw
in the previous section. Tables 6 and 7 show the
breakdowns in these numbers for each system and
for the two sets of HH data, and begin to quantify
the striking contrasts between human and com-
puter confirmation strategies.
3.3 Number of Dialogue Acts
Another observation is that the computer experts
appear to be trying to do more. They have sig-
nificantly more DAs per turn than do their hu-
man users, whereas in the HH dialogues, the two
participants have nearly the same number of DAs
per turn (just over 1.3). In the HC dialogues, sys-
Site Expert User Total
HC A 3 (0.5%) 4 (0.7%) 7 (1.2%)
HC B 13 (1.9%) 0 (0.0%) 13 (1.9%)
HC C 20 (3.1%) 3 (0.5%) 23 (3.6%)
HC D 0 (0.0%) 0 (0.0%) 0 (0.0%)
HH SRI 95 (16.1%) 79 (13.3%) 174 (29.4%)
HH CMU 94 (12.1%) 72 (9.3%) 166 (21.4%)
Table 6: Number of short confirmations, i.e. ac-
knowledge (percentage of total dialogue acts)
Site Expert User Total
HC A 32 (5.7%) 0 (0.0%) 32 (5.7%)
HC B 74 (10.6%) 0 (0.0%) 74 (10.6%)
HC C 59 (9.2%) 1 (0.2%) 60 (9.4%)
HC D 45 (8.6%) 0 (0.0%) 45 (8.6%)
HH SRI 11 (1.9%) 11 (1.9%) 22 (3.7%)
HH CMU 47 (6.1%) 11 (1.4%) 58 (7.5%)
Table 7: Number of long confirmations i.e.
verify-X (percentage of total dialogue acts)
tems have, on average 1.6 DAs per turn where
users have just 1.0, as Figure 3 shows. If we take
a DA as representing a single dialogue ?move?,
then users in the HC dialogues are managing one
move per turn, where the systems have at least one
and often more. A common sequence for the com-
puter experts is a verify-task-information fol-
lowed by a request-task-information, such as
A flight to Atlanta. What city are you departing
from?.
Figure 3: Dialogue acts per turn for users and
experts in the HH and HC dialogues
3.4 Types of Dialogue Acts
One of our main questions going into this work
was whether there would be interestingly differ-
ent distributions of DAs in the HH and HC dia-
logues, and whether different distributions of DAs
across systems would be correlated with user sat-
isfaction. Unfortunately, we do not have user sat-
isfaction scores for the HH data, but if new data
were to be collected, this would be an essential
addition.
Tables 8 and 9 illustrate some of the main dif-
ferences between the HH and HC dialogues, and
as regards our first research question, definitely
give an interesting view of the differences between
the HH and HC conversations.
DA Overall Expert User
GiveTaskInfo 27.7% 29.7% 25.5%
Acknowledge 24.9% 26.9% 22.7%
RequestTaskInfo 11.0% 10.7% 11.4%
VerifyTaskInfo 5.4% 7.5% 3.2%
Affirm 4.8% 4.3% 5.4%
Table 8: Five most frequent DAs in Human-
Human dialogues, by percent of total DAs for col-
umn
DA Overall Expert User
GiveTaskInfo 23.7% 12.9% 46.3%
RequestTaskInfo 15.3% 22.1% 1.3%
Offer 7.7% 11.5% 0.0%
VerifyTaskInfo 7.1% 10.5% 0.1%
Apology 4.5% 6.6% 0.1%
Table 9: Five most frequent DAs in Human-
Computer dialogues, by percent of total DAs for
column
As expected in this domain, all DAs involving
exchange of task information (give-task-info,
request-task-info, and verify-task-info are
frequent in both sets of dialogues. However, in the
HH dialogues, acknowledge (e.g. the tag for
back-channel responses and general confirmations
such as right, uh huh and okay) is the second most
common DA, and does not even appear in the top
five for the HC dialogues. The DA for positive re-
sponses, affirm, is also in the top ranking for the
HH dialogues, but does not appear in the list for
the HC dialogues. Finally, offer and apology
appear frequently in the HC dialogues and not in
the top HH DAs. The appearance of these two is a
clear indication that the systems are doing things
quite differently from their human counterparts.
Turning to differences between experts and
users in these top categories, we can see that hu-
man users and experts are about equally likely
to ask for or give task-related information (give-
task-info and request-task-info). In con-
trast, in the HC dialogues nearly half of the users?
DAs are giving task information and hardly any
are requesting such information, while almost a
quarter of expert DAs are requesting information.
There is some inequity in the use of verify-task-
info in the HH dialogues, where experts perform
about twice as many verifications as users; how-
ever, in the HC dialogues, virtually all verification
is done by the expert. All of these patterns rein-
force our finding about initiative distribution; in
the HC dialogues, one disproportionately finds the
expert doing the asking and verification of task in-
formation, and the user doing the answering, while
in the HH dialogues the exchange of information
is much more balanced.
DA HC A HC B HC C HC D
accept 3.9% 3.1% 4.8% 3.4%
acknowledge 1.2% 1.9% 3.6% 0.0%
affirm 1.8% 2.4% 0.8% 9.5%
apologize 4.6% 3.7% 8.9% 0.0%
demand-conv-info 1.1% 0.0% 0.0% 0.0%
demand-sit-info 0.0% 1.6% 1.4% 1.3%
demand-task-info 3.4% 0.3% 0.0% 1.3%
give-sit-info 5.7% 6.3% 4.7% 1.9%
give-task-info 34.8% 16.0% 24.8% 20.8%
negate 2.1% 1.7% 0.8% 5.2%
not-understand 2.5% 3.7% 7.2% 0.0%
offer 3.5% 8.4% 9.4% 9.4%
open-close 2.3% 3.1% 4.8% 3.4%
please-wait 0.0% 6.2% 1.6% 3.1%
reject 1.1% 4.1% 0.3% 2.5%
req-conv-action 2.7% 4.4% 2.5% 1.0%
req-sit-action 1.1% 1.4% 0.2% 1.9%
req-sit-info 0.0% 3.3% 0.2% 3.2%
req-task-action 1.1% 1.4% 0.3% 0.2%
req-task-info 17.9% 12.6% 10.9% 21.6%
suggest-conv-action 1.6% 0.1% 2.0% 0.0%
thank 2.1% 3.4% 1.4% 1.7%
verify-conv-action 0.7% 0.7% 0.0% 0.0%
verify-task-action 2.5% 0.4% 1.9% 0.0%
verify-task-info 2.5% 9.4% 7.5% 8.6%
user satisfaction9 60.0% 71.5% 68.5% 82.8%
Table 10: Distribution of DAs by System
Table 10 gives an interesting snapshot of each
system, in terms of its overall distribution of DAs.
These numbers are reflective of the system design-
ers? decisions for their systems, and that means all
DAs are not going to be used by all systems (i.e.
0.0% may mean that that DA is not part of the
system?s repertoire).
We will concentrate here on the best and worst
9This figure combines the scores on five user satis-
faction questions. A perfect score is 100%.
received systems in terms of their overall user sat-
isfaction, HC D and HC A; the relevant numbers
are boldfaced. They also have very different di-
alogue strategies, and that is partially reflected
in the table. HC D?s dialogue strategy does not
make use of the ?social nicety? DAs employed by
other systems (acknowledge, apologize, not-
understand), and yet it still had the highest user
satisfaction of the four. This system also has the
highest proportion of affirm (more than three
times as many as the next highest system) and
req-task-info DAs, which suggests that quite a
lot of information is being solicited and the users
(because we know from Table 9 that it is primarily
the users responding) are more often than average
responding affirmatively. The fact that the per-
centage of give-task-infos is somewhere in the
middle of the range and affirms is so high may
indicate that the HC D uses more yes/no than
content questions.
Looking at the lower scoring system, HC A, we
see very different patterns. HC A has most of
the demand-task-infos, the second highest per-
centage of req-task-infos and by far the most
give-task-infos, so its dialogue strategy must
involve a large number of attempts to extract in-
formation from the user, and yet it has the fewest
offer DAs, so these don?t appear to be resulting
in suggestions of particular travel options.
Turning to correlations between DA use by
expert and user (combined across systems) and
user satisfaction, we see some expected results
but also some rather surprising correlations.
Not unexpectedly, apologies and signals of non-
understanding by the system are highly negatively
correlated with satisfaction (-0.7 and -0.9, respec-
tively). While it may seem counter-intuitive that
open-close by the user is negatively correlated
(at -0.8), those familiar with this data will un-
doubtedly have noticed that users often try to say
Goodbye repeatedly to try to end a dialogue that
is going badly. Discussion of situational informa-
tion (e.g. phone use) by the expert is highly neg-
atively correlated, but by the user, the DA req-
situation-info is perfectly positively correlated.
We cannot account for this finding.
3.5 Unsolicited Information
In the HC data we noticed that users often
provided more information than was explicitly
solicited?we call this ?unsolicited information?.
For example, when a system asks for one piece
of information, On what day would you be depart-
ing Portland?, the user might respond with ad-
ditional information such as, Thursday, October
5th before six pm from Portland back to Seattle.
78% of that unsolicited information is offered in
response to open-ended questions (e.g., How can I
help you? or What are your travel plans?). While
our initiative tagging partially captures this, there
are cases where the answer may be considered re-
sponsive (i.e. initiative does not shift away from
the participant asking the question) and yet un-
solicited information has been offered. Thus, this
category is somewhat orthogonal to our charac-
terization of initiative, although it is clearly one
way of seizing control of the conversation.10
To get at this information, we developed a third
tagging scheme for annotating unsolicited infor-
mation. We began examining just the HC doc-
uments, because the phenomenon is prevalent in
these data; we hope to perform a similar analysis
on the HH data as well. We found that the sys-
tems we examined in general handle unsolicited in-
formation well. 70% of all unsolicited information
is handled correctly by the systems, 22% is han-
dled incorrectly, and the rest could not be accu-
rately classified. Information offered in response
to open-ended questions is handled correctly more
often by the systems than unsolicited information
offered at other points in the dialogue (74% versus
56%). The former figure is not surprising, since
the systems are designed to handle ?unsolicited?
information following open-prompts. However, we
were surprised the systems did as well as they did
on unsolicited information in contexts where it
was not expected. Figure 4 shows the relationship
between frequency of various types of unsolicited
information and how well the system incorporates
that information. There appears to be some cor-
relation between the frequency of unsolicited in-
formation and the rate of success, but we do not
have enough data to make a stronger claim.
Furthermore, systems vary in response delay to
pieces of unsolicited information. We define re-
sponse delay as the number of system turns it
takes before the information is acknowledged by
the system (either correctly or incorrectly.) If a
system responds immediately to the unsolicited
information, a count of zero turns is recorded.
Figure 5 shows the difference among systems in re-
sponding to unsolicited information. We graphed
both the average total number of system turns as
well as the average number of turns minus rep-
etitions. HC B responds almost immediately to
10This issue may also be related to where in the
dialogue errors occur. We are pursuing another line
of research which looks at automatic error detection,
described in (Aberdeen et al, 2001). We believe we
may also be able to detect unsolicited information au-
tomatically, as well as to see whether it is likely to
trigger errors by the system.
Figure 4: Unsolicited Fields vs. Success Rate of
Incorporation
unsolicited information while HCs A and C take
more turns to respond. HC D has trouble under-
standing the unsolicited information, and either
keeps asking for clarification or continues to ig-
nore the human and prompts for some other piece
of information multiple times.
Figure 5: Variation of System Response to Unso-
licited Information
Figure 6 shows the different rates at which sys-
tems acknowledge unsolicited information for dif-
ferent fields. For example, departure city is recog-
nized and validated almost immediately. Return
date and flight type are incorporated fairly quickly
when the system understands what is being said.
If we look at the effects of experience on
the amount of unsolicited information offered, as
shown in Figure 7, we can see that users tend
to provide more unsolicited information over time
(i.e., as they make more calls to the systems).
This effect may be the result of increased user
confidence in the systems at handling unsolicited
information. It also may be attributed to user
boredom; as time goes on, users may be trying
to finish the task as quickly as possible. Even if
this is true, however, it demonstrates attempts by
users to take more control of the interactions as
Figure 6: System Response to Different Types of
Unsolicited Information
they become more experienced.
Figure 7: Effect of Experience on Unsolicited In-
formation
Our data also show that the success rate of in-
corporating unsolicited information improves with
user experience. The ratio of successes to failures
increases in later calls to the systems (Figure 8).
4 Discussion
This was a relatively small study, but many of
the results are sufficiently striking that we expect
them to hold over large sets of dialogues. First,
it is clear that (for our definition of the term) ini-
tiative is skewed towards the computer expert in
the human-computer dialogues, despite claims of
developers to the contrary. Whether this is de-
sirable or not is a separate issue, but we believe
it is a move forward to be able to quantify this
difference. Second, there are clear differences in
dialogue act patterns between the HH and HC di-
alogues. When the DAs correspond to basic di-
alogue moves, like questions or signals of agree-
ment, we can begin to see how the dialogue dy-
namic is different in the human computer situa-
Figure 8: Experience versus Success Rate of In-
corporating Unsolicited Information
tion. In general, the conversation was much more
balanced between traveler and expert in the HH
setting, in terms of amount of speech, types of di-
alogue acts and with respect to initiative. In the
HC conversations, the system dominated, in num-
ber of words and dialogue acts and in initiative.
We are very interested in the selection of the
?right? tag set for a given task. As we noted in
our discussion of DA tagging, we had very dif-
ferent outcomes with two closely related tag sets.
Clearly the choice of tag set is highly dependent
on the use the tagged data will be put to, how
easily the task can be characterized in the set of
tagging guidelines, and what trade-offs in accu-
racy vs. richness of representation are acceptable.
A central question we are left with is ?Why
don?t the users talk more in HC dialogues?? Is
it that they are happy to just give short, specific
answers to very directed questions? Or do they
?learn? that longer answers are likely to cause the
systems problems? Or perhaps users have pre-
conceived notions (often justified) that the com-
puter will not understand long utterances? We
may speculate that poor speech recognition per-
formance is a major factor shaping this behav-
ior, leading system designers to attempt to con-
strain what users can say, while simultneously at-
tempting to hold onto the initiative. (Walker et
al. (2001) found sentence accuracy to be one of
the significant predictors of user satisfaction in the
Summer 2000 DARPA Communicator data collec-
tion.) There are some cases where the experts in
the HC dialogues say things their human counter-
parts need not. One obvious case, which appears
in even the small example dialogues we are us-
ing here, is that the systems tend to repeat utter-
ances when there is some processing difficulty. In
the same vein, errors and misunderstandings are
more frequent in the HC data, resulting in (some
fairly verbose) efforts by the systems to identify
the problem and get the conversation back on
track.
5 Future Work
We are currently working with other Communica-
tor sites who are also looking at dialogue issues.
In addition, we are beginning to look at two new
aspects of these dialogues: task complexity and
conversational failure analysis (at the turn level,
(Aberdeen et al, 2001)). We are also interested
in examining patterns of initiative tags, i.e. con-
trol shift types and length of initiative runs, and
at relations between DAs and user satisfaction.
6 Acknowledgments
Thanks to Lori Levin and Alon Lavie at CMU for
sharing the CSTAR tagging guidelines and their
sample tagged corpus.
References
J. Aberdeen, C. Doran, L. Damianos, S. Bayer, and
L. Hirschman. 2001. Finding errors automatically
in semantically tagged dialogues. In Notebook Pro-
ceedings of the First International Conference on
Human Language Technology Research, San Diego,
CA, March.
J. C. Carletta, A. Isard, S. Isard, J. Kowtko,
G. Doherty-Sneddon, and A. Anderson. 1995.
The coding of dialogue structure in a corpus. In
J. A. Andernach, S. P. van de Burgt, and G. F.
van der Hoeven, editors, Proceedings of the Twente
Workshop on Language Technology: Corpus-based
approaches to dialogue modelling, Enschede, The
Netherlands. Universiteit Twente.
Jennifer Chu-Carroll and Michael K. Brown. 1998.
An evidential model for tracking initiative in col-
laborative dialogue interactions. User Modeling and
User-Adapted Interaction, 8(3-4):215?253.
CSTAR Consortium. 1999. Dialogue act annotation.
Unpublished Manuscript, October.
Mark Core and James Allen. 1997. Coding dialogs
with the damsl annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communica-
tive Action in Humans and Machines, Boston, MA,
November.
Mark Core, Masato Ishizaki, Johanna Moore, Chris-
tine Nakatani, Nobert Reithinger, David Traum,
and Syun Tutiya, editors. 1999. The Report of The
Third Workshop of the Discourse Resource Initia-
tive, Chiba Univeristy. Technical Report No.3 CC-
TR-99-1.
Barbara Di Eugenio, Pamela W. Jordan, and Liina
Pylkknen. 1998. The COCONUT project: dialogue
annotation manual. Technical Report ISP Techni-
cal Report 98-1, University of Pittsburgh, Decem-
ber.
Giovanni Flammia and Victor Zue. 1997. Learn-
ing the structure of mixed initiative dialogues us-
ing a corpus of annotated conversations. In Proc.
Eurospeech 97, pages 1871?1874, Rhodes, Greece,
September.
Pamela W. Jordan and Barbara Di Eugenio. 1997.
Control and initiative in collaborative problem solv-
ing dialogues. In AAAI Spring Symposium on Com-
putational Models for Mixed Initiative Interaction,
Stanford, CA.
Daniel Jurafsky, Elizabeth Shriberg, and De-
bra Biasca. 1997. Switchboard swbd-damsl
shallow-discourse-function annotation coders man-
ual. Technical Report Technical Report 97-02, Uni-
versity of Colorado Institute of Cognitive Science,
August.
Marion Klein, Niels Ole Bernsen, Sarah Davies, Laila
Dybkj?r, Juanma Garrido, Henrik Kasch, An-
dreas Mengel, Vito Pirelli, Massimo Poesio, Sil-
via Quazza, and Claudia Soria, 1998. Supported
Coding Schemes, MATE Deliverable D1.1, July.
http://mate.nis.sdu.dk/.
Christine H. Nakatani, Barbara J. Grosz, David D.
Ahn, and Julia Hirschberg. 1995. Instructions for
annotating discourse. Technical Report TR-21-95,
Harvard University.
R.J. van Vark, J.P.M. de Vreught, and L.J.M.
Rothkrantz. 1996. Analysing ovr dialogues, coding
scheme 1.0. Technical Report 96-137, Delft Univer-
sity of Technology.
Marilyn Walker and Rebecca Passonneau. 2001. Di-
alogue act tags as qualitative dialogue metrics for
spoken dialogue systems. In Notebook Proceedings
of the First International Conference on Human
Language Technology Research, San Diego, CA,
March.
Marilyn Walker and Steve Whittaker. 1990. Mixed
initiative in dialogue: An investigation into dis-
course segmentation. In Proceedings of ACL90.
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garo-
folo, L. Hirschman, A. Le, S. Lee, S. Narayan,
K. Papineni, B. Pellom, J. Polifroni, A. Potamianos,
P. Prabhu, A. Rudnicky, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2001. DARPA Com-
municator Dialog Travel Planning Systems: The
June 2000 Data Collection. Submitted., April.
Steve Whittaker and Phil Stenton. 1988. Cues and
control in expert client dialogues. In Proceedings
of the 26th Annual Meeting of the Association for
Computational Linguistics (ACL88), pages 123?
130.
