Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 1?2,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Named Entity Recognition: Different Approaches 
Sobha, L 
AU-KBC Research Centre 
MIT Campus of Anna University 
Chennai-44 
sobha@au-kbc.org          
Abstract  
The talk deals with different approaches used for Named Entity recognition and how they are used 
in developing a robust Named Entity Recognizer. The talk includes the development of tagset for 
NER and manual annotation of text.   
1
 2
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 59?66,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Domain Focused Named Entity Recognizer for Tamil Using 
Conditional Random Fields 
Vijayakrishna R 
AU-KBC Research Centre 
MIT Campus, Anna University 
Chennai, India 
vijayakrishna@au-kbc.org 
Sobha L 
AU-KBC Research Centre 
MIT Campus, Anna University 
Chennai, India 
sobha@au-kbc.org   
Abstract 
In this paper, we present a domain focused 
Tamil Named Entity Recognizer for 
tourism domain. This method takes care of 
morphological inflections of named entities 
(NE). It handles nested tagging of named 
entities with a hierarchical tagset 
containing 106 tags. The tagset is designed 
with focus to tourism domain. We have 
experimented building Conditional 
Random Field (CRF) models by training 
the noun phrases of the training data and it 
gives encouraging results. 
1 Introduction 
Named Entity Recognition (NER) is the task of 
identifying and classifying the entities such as 
person names, place names, organization names 
etc, in a given document. Named entities play a 
major role in information extraction. NER has been 
a defined subtask in Message Understanding 
Conference (MUC) since MUC 6. A well 
performing NER is important for further level of 
NLP techniques. 
In general NER is a hard problem.. Words can 
have multiple uses and there is an unbounded 
number of possible names. Many techniques have 
been applied in Indian and European languages for 
NER. Some of them are rule based system (Krupka 
and Hausman, 1998), which makes use of 
dictionary and patterns of named entities, Decision 
trees (Karkaletsis et al, 2000), Hidden Morkov 
Model (HMM) (Biker, 1997), Maximum Entropy 
Morkov Model (MEMM) (Borthwick et al, 1998), 
Conditional Random Fields (CRF) (Andrew 
McCallum and Wei Li, 2003) etc. In short, the 
approaches can be classified as rule-based 
approach, machine learning approach or hybrid 
approach. 
For Indian languages, many techniques have 
been tried by different people. MEMM system for 
Hindi NER (Kumar and Pushpak, 2006) gave an 
average F1 measure of 71.9 for a tagset of four 
named entity tags.  
NER has been done generically and also domain 
specific where a finer tagset is needed to describe 
the named entities in a domain. Domain specific 
NER is common and has been in existence for a 
long time in the Bio-domain (Settles 2004) for 
identification of protein names, gene names, DNA 
names etc.  
We have developed a domain specific 
hierarchical tagset consisting of 106 tags for 
tourism domain. We have used Conditional 
Random Fields, a machine learning approach to 
sequence labeling task, which includes NER. 
Section 2 gives a brief introduction to 
Conditional Random Fields (CRF). Section 3 
discusses the nature of named entities in Tamil, 
followed by section 4 describing the tagset used in 
tourism domain. Section 5 describes how we have 
presented the training data to build CRF models 
and how we have handled nested tagging. Sections 
6 and 7 explain the experiments and results. The 
paper is concluded in section 8. 
59
2 Conditional Random Fields (CRF) 
Conditional Random Fields (CRF) (Lafferty et al, 
2001) is a machine learning technique. CRF 
overcomes the difficulties faced in other machine 
learning techniques like Hidden Markov Model 
(HMM) (Rabiner, 1989) and Maximum Entropy 
Markov Model (MEMM) (Berger et al, 1996). 
HMM does not allow the words in the input 
sentence to show dependency among each other. 
MEMM shows a label bias problem because of its 
stochastic state transition nature. CRF overcomes 
these problems and performs better than the other 
two. HMM, MEMM and CRF are suited for 
sequence labeling task. But only MEMM and CRF 
allows linguistic rules or conditions to be 
incorporated into machine learning algorithm. 
Lafferty et al define Conditional Random Fieds 
as follows: ?Let G = (V,E) be a graph such that Y 
= (Y
v
)
v V
, so that Y is indexed by the vertices of 
G. Then (X,Y) is a conditional random field in 
case, when conditioned on X, the random variables 
Y
v 
obey the Markov property with respect to the 
graph: p(Y
v
|X,Y
w
,w?v) = p(Y
v
|X,Y
w
,w~v), where 
w~v means that w and v are neighbors in G?.  
Here X denotes a sentence and Y denotes the 
label sequence. The label sequence y which 
maximizes the likelihood probability p
?
(y|x) will 
be considered as the correct sequence, while 
testing for new sentence x with CRF model ? . The 
likelihood probability p
?
(y|x) is expressed as 
follows.  
  
where ?
k 
and ?
k 
are parameters from CRF model ? 
and f
k 
and g
k 
are the binary feature functions that 
we need to give for training the CRF model. This 
is how we integrate linguistic features into 
machine learning models like CRF.  
In NER task, the sequence of words which 
forms a sentence or a phrase can be considered as 
the sequence x and the sequence formed by named 
entity label for each word in the sequence x is the 
label sequence y. Now, the task of finding y that 
best describes x can be found by maximizing the 
likelihood probability p
?
(y|x). Thus, NER task can 
be considered as a sequence labeling task. Hence 
CRF can be used for NER task. 
3 Characteristics of Named Entities in 
Tamil 
Unlike English, there is no concept of capital 
letters in Tamil and hence no capitalization 
information is available for named entities in 
Tamil. All named entities are nouns and hence are 
Noun Phrases. But not all Noun Phrases are 
Named Entities. Since named entities are noun 
phrases, they take all morphological inflections. 
This makes a single named entity to appear as 
different words in different places. By applying 
Morphological analysis on words, the root words 
of inflected Named Entities can be obtained. These 
roots will be uninflected Named Entities which is 
what is required in most applications. Some type of 
named entities like date, money etc, occur in 
specific patterns. 
Example for inflected named entity:  
  ceVnYnYEkku (?to Chennai?).  
Example for pattern in named entity:  
2006 aktopar 25Am wewi (?25
th 
October, 
2006?) 
Pattern: <4 digits> <month> <1-2 digit> [Am   
wewi] 
4 Named Entity Tagset used 
The tagset which we use here for NER contains 
106 tags related to each other hierarchically. This 
type of tagset is motivated from ?ACE English 
Annotation Guidelines for Entities? developed by 
Linguistic Data Consortium. The tagset which we 
use is built in-house with focus to tourism domain.  
4.1 Sample Tags 
Sample tags from the entire tagset is shown below 
with their hierarchy.  
1. Enamex 
1.1. Person 
1.1.1. Individual 
1.1.1.1. Family Name 
1.1.1.2. Title 
60
1.1.2. Group 
1.2. Organization 
. . . . 
1.3. Location 
. . . . 
1.4. Facilities 
. . . . 
1.5. Locomotive 
. . . . 
1.6. Artifact 
. . . . 
1.7. Entertainment 
. . . . 
1.8. Materials 
. . . . 
1.9. Livthings 
. . . . 
1.10. Plants 
. . . . 
1.11. Disease 
. . . . 
2. Numex 
2.1. Distance 
2.2. Money 
2.3. Quantity 
2.4. Count 
3. Timex 
3.1. Time 
3.2. Year 
3.3. Month 
3.4. Date 
3.5. Day 
3.6. Period 
3.7. Sday  
Certain tags in this tagset are designed with 
focus to Tourism and Health Tourism domain, 
such as place, address, water bodies (rivers, lakes 
etc.,), religious places, museums, parks, 
monuments, airport, railway station, bus station, 
events, treatments for diseases, distance and date. 
The tags are assigned with numbers 1,2,3 for 
zero
th 
level, the tags with numbers 1.1, 1.11, 2.1 
,2.4 and 3.1 ,3.7 etc for level-1, the tags with 
numbers 1.1.1, 1.1.2, 1.2.1 etc as level-2  and the 
tags with numbers 1.1.1.1, 1.1.1.2, 1.2.4.1 etc for 
level-3  because they occur in the hierarchy in 
corresponding levels. We have 3 tags in zero
th
  
level, 22 tags in level-1, 50 tags in level-2 and 31 
tags in level-3. 
4.2 Sample Annotation 
Tamil : 
<person> <city> mawurE </city> <individual> 
manYi <familyname> Eyar </familyname> 
</individual> </person> <city> ceVnYnYEkku 
</city> vanwAr. 
English equivalent : 
<person> <city> Madhurai </city> <individual> 
Mani <familyname> Iyer </familyname> 
</individual> </person> came to <city> Chennai 
</city>. 
5 NER using CRF 
We used CRF++ (Taku Kudo, 2005), an open 
source toolkit for linear chain CRF. This tool when 
presented with the attributes extracted from the 
training data builds a CRF model with the feature 
template specified by us. When presented with the 
model thus obtained and attributes extracted from 
the test data, CRF tool outputs the test data tagged 
with the labels that has been learnt. 
5.1 Presenting training data 
Training data will contain nested tagging of named 
entities as shown in section 4.2. To handle nested 
tagging and to avoid ambiguities, we isolate the 
tagset into three subsets, each of which will 
contain tags from one level in the hierarchy. Now, 
the training data itself will be presented to CRF as 
three sets of training data. From this, we will get 
three CRF models, one for each level of hierarchy. 
Example: 
The sample sentence given in section 4.2 will be 
presented to CRF training for each level of 
hierarchy as follows: 
Level-1: 
<location> mawurE </location> <person> 
manYi Eyar </person> <location> ceVnYnYEkku 
</location> vanwAr. 
Level-2: 
<place> mawurE </place> <individual> manYi 
Eyar </individual> <place> ceVnYnYEkku 
</place> vanwAr. 
Level-3: 
<city> mawurE </city> manYi <familyname> 
Eyar </familyname> <city> ceVnYnYEkku 
</city> vanwAr. 
Notice that the tags ?location? and ?place? are 
not specified in the input sentence. In the 
61
hierarchy, the ?location? tag is the parent tag of 
?place? tag which is a parent tag of ?city? tag. Thus 
for the word ?mawurE?, level-1 tag is ?location?, 
level-2 tag is ?place? and level-3 tag is ?city?. 
5.2 Attributes and Feature Templates 
Attributes are the dependencies from which the 
system can infer a phrase to be named entity or 
not. Features are the conditions imposed on these 
attributes. Feature templates help CRF engine to 
form features from the attributes of the training 
data. From the characteristics of named entities in 
Tamil, we see that it is only the noun phrases that 
are possible candidates for Named Entities. So we 
apply Noun Phrase Chunking and consider only 
noun phrases and train on them. The attributes that 
we arrived at are explained below: 
1. Roots of words: This is to ignore 
inflections in named entities. Also to learn 
the context in which the named entity 
occurs, we consider two words prior and 
two words subsequent to the word under 
analysis and take unigram, bigram and 
trigram combinations of them as attributes. 
2. Their Parts of Speech (POS): This will 
give whether a noun is proper noun or 
common noun. POS of current word is 
considered. 
3. Words and POS combined: The present 
word combined with the POS tag of the 
previous two words and the present word 
combined with POS of the next two words 
are taken as features. 
4. Dictionary of Named Entities: A list of 
named entities is collected for each type of 
named entities. Root words are checked 
against the dictionary and if present in the 
dictionary, the dictionary feature for the 
corresponding type of named entity is 
considered positive. 
5. Patterns: Certain types of named entities 
such as date, time, money etc., show 
patterns in their occurrences. These 
patterns are listed out. The current noun 
phrase is checked against each pattern. The 
feature is taken as true for those patterns 
which are satisfied by the current noun 
phrase. 
Example Patterns:  
Date: <4 digits> <month> <1-2 digit> [Am 
wewi] 
Money: rU. <digits> [Ayiram|latcam|koti] 
(English Equivalent: 
 Rs. <digits> [thousands|lakhs|crores]) 
6. Bigram of Named Entity label 
A feature considering the bigram occurrences of 
the named entity labels in the corpus is considered. 
This is the feature that binds the consecutive 
named entity labels of a sequence and thus forming 
linear chain CRFs. Sample noun phrase with level-
1 tags:  
arulYmiku JJ  person 
cupramaNiya NNPC    person 
cuvAmi  NNPC    person 
wirukoyil   NNC location 
vayalUr NNP location  
English Equivalent:  
Gracious JJ person 
Subramaniya NNPC person 
Swami NNPC person 
Temple NNC location 
Vayalore NNP location  
Attributes are extracted for each token in the 
noun phrase. For example, the attributes for third 
token in the sample noun phrase given are as 
follows. 
1. Unigram: arulYmiku, cupramaNiya, 
cuvAmi, wirukoyil, vayalUr. 
2. Bigram: cupramaNiya/cuvAmi, cuvAmi/ 
wirukoyil 
3. Trigram: cupramaNiya/cuvAmi/wirukoyil 
4. POS of current word: NNPC 
5. Word and previous 2 POS:  JJ/NNPC/ 
cuvAmi 
6. Word and next 2 POS: cuvAmi/NNC/NNP 
7. Bigram of NE labels: person/person 
62
The CRF training process described above is 
illustrated in Figure-1.   
5.3 Presenting testing data 
Test data will also be presented in way similar to 
how we presented the training data. Test data is 
processed for Morph analysis, POS (Arulmozhi et 
al., 2004) and NP chunking (Sobha and Vijay 
Sundar Ram, 2006). Here also, the same set of 
attributes and feature templates are used. Now, the 
test data is tagged with each of the CRF models 
built for three levels of hierarchy. All the three 
outputs are merged to get a combined output. The 
CRF testing is illustrated in Figure 2. 
6 Experiments 
A 94k words corpus is collected in Tamil for 
tourism domain. Morph Analysis, POS tagging, 
NP chunking and named entity annotation are done 
manually on the corpus. This corpus contains about 
20k named entities. This corpus is split into two 
sets. One forms the training data and the other 
forms the test data. They consist of 80% and 20% 
of the total data respectively. CRF is trained with 
training data and CRF models for each of the 
levels in the hierarchy are obtained. With these 
models the test data is tagged and the output is 
evaluated manually.  
7 Results 
The results of the above experiment are as follows. 
Here, NE means Named Entity, NP means noun 
phrase.  
Number of NPs in test data = 7922 
There are totally 4059 NEs in the test data. All 
of them bear level-1 tags. Out of 4059 NEs, 3237 
NEs bear level-2 tags and 727 NEs bear level-3 
tags. The result from the system is shown in Table 
1 and Table 2. 
The system performs well for domain focused 
corpus. It identifies inflected named entities 
efficiently by considering the root form of each 
word in noun phrases. The reason for good 
Filter NPs 
CRF Testing 
Figure 2. CRF Testing for NER 
Test Data 
 
Morph Analysis, POS Tagging, NP 
chunking 
NPs from test data
Level-1 
Model
Level-2 
Model
Level-3 
Model
Merge 
All levels Merged Output 
Dictionary, 
Patterns 
Training Data (Morph Analyzed, POS 
tagged, NP chunked, NE Tagged) 
NPs from Training Data 
Root 
words, 
POS, 
Level-1 
tags 
Root 
words, 
POS, 
Level-2 
tags 
Root 
words, 
POS, 
Level-3 
tags 
CRF Training 
Level-1 
CRF 
model 
Level-2 
CRF 
model 
Level-3 
CRF 
model 
Figure 1.Training CRF for NER 
Filter NPs 
Dictionary, 
Patterns 
63
precision is that tagging is done only when the root 
word that it is seeing is already learnt from the 
training corpus or the context of the current word 
is similar to the context of the named entities that it 
has learnt from the training corpus. However, in 
some words like ?arccunYAnawi? (Arjuna River), 
the Morph Analyzer gives two root words which 
are ?arccunYa? and ?nawi?. For our case, only the 
first word is considered and the system tags it as 
?person? instead of ?waterbodies?.  
Named Entity 
Level 
Level-
1 
Level-
2 
Level-
3 
Number of NEs 
in data 
4059 3237 727 
Number of NEs 
identified by 
NER engine 
3414 2667 606 
Number of NEs 
identified 
correctly 
3056 2473 505 
Precision % 89.51 92.73 83.33 
Recall % 75.29 76.40 69.46 
F1 measure % 81.79 83.77 75.77 
 
Table 1. Evaluation of output from NER engine for 
each level   
Performance Measure Value in %
Precision 88.52 
Recall 73.71 
F1 Measure 80.44 
Table 2. Overall result from NER engine  
When there are new named entities which are 
not in training corpus, CRF tries to capture the 
context and tags accordingly. In such cases 
irrelevant context that it may learn while training 
will cause problem resulting in wrong tagging. 
This affects the precision to some extent. When the 
named entities and their context are new to CRF, 
then they are most likely not tagged. This affects 
the recall. 
From Table 1, we see that the system performs 
better for level-2 tags than for level-1 tags even 
though level-1 tags are less in number than level-2 
tags and occur more frequently than level-2 tags. 
This is so because the named entities with level-2 
tags have relatively more context and are lesser in 
length (number of words in the named entity) than 
the named entities in level-1 tags. Level-3 tags 
contain lesser number of tags than level-2 tags and 
also occur less frequently. Because of relatively 
more data sparseness, the system is unable to 
perform well for level-3 tags as it can for other 
levels. 
8 Conclusion 
We see that Conditional Random Fields is well 
suited for Named Entity recognition task in Indian 
languages also, where the inflection of named 
entities can be handled by considering their root 
forms. A good precision can be obtained by 
presenting only the noun phrases for both testing 
and training. 
References 
Arulmozhi P, Sobha L and Kumara Shanmugam B. 
2004. Parts of Speech Tagger for Tamil, Symposium 
on Indian Morphology, Phonology & Language 
Engineering, March 19-21, IIT Kharagpur. :55-57. 
Berger A, Della Pietra S and Della Pietra V. 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, 22(1). 
Bikel D M. 1997. Nymble: a high-performance learning 
name-finder. In Proceedings of the Fifth Conference 
on Applied Natural Language Processing. :194-201. 
Borthwick A, Sterling J, Agichtein E and Grishman R. 
1998. Description of the MENE named Entity System, 
In Proceedings of the Seventh Machine 
Understanding Conference (MUC-7). 
Karkaletsis V, Pailouras G and Spyropoulos C D. 2000. 
Learning decision trees for named-entity recognition 
and classification. In Proceedings of the ECAI 
Workshop on Machine Learning for Information 
Extraction. 
Krupka G R and Hausman K. 1998. Iso Quest Inc: 
Description of the NetOwl Text Extraction System as 
used for MUC-7. In Proceedings of Seventh Machine 
Understanding Conference (MUC 7). 
Kumar N, Pushpak Bhattacharyya. 2006. Named Entity 
Recognition in Hindi using MEMM. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
In Proceedings of the Eighteenth International 
64
Conference on Machine Learning (ICML-2001). 
282-289. 
Andrew McCallum and Wei Li. 2003. Early Results for 
Named Entity Recognition with Conditional Random 
Fields, Feature Induction and Web-Enhanced 
Lexicons. Seventh Conference on Natural Language 
Learning (CoNLL). 
Lawrence R. Rabiner. 1989. A Tutorial on Hidden 
Markov Models and Selected Applications in Speech 
Recognition. In Proceedings of the IEEE, 77(2):257?
286. 
Settles B. (2004). Biomedical Named Entity Recognition 
Using Conditional Random Fields and Rich Feature 
Sets. In Proceedings of the International Joint 
Workshop on Natural Language Processing in 
Biomedicine and its Applications (NLPBA), Geneva, 
Switzerland. pp:104-107. 
Sobha L, Vijay Sundar Ram R. 2006. Noun Phrase 
Chunking in Tamil. In proceedings of the MSPIL-06, 
IIT Bombay. pp:194-198. 
Taku Kudo. 2005. CRF++, an open source toolkit for 
CRF, http://crfpp.sourceforge.net . 
65
 66
Identifying Similar and Co-referring Documents Across Languages 
Pattabhi R K Rao T 
AU-KBC Research Centre, 
MIT Campus, Anna University, 
Chennai-44, India. 
pattabhi@au-kbc.org 
Sobha L 
AU-KBC Research Centre, 
MIT Campus, Anna University, 
Chennai-44, India. 
sobha@au-kbc.org 
 
 
Abstract 
This paper presents a methodology for 
finding similarity and co-reference of 
documents across languages. The similarity 
between the documents is identified ac-
cording to the content of the whole docu-
ment and co-referencing of documents is 
found by taking the named entities present 
in the document. Here we use Vector Space 
Model (VSM) for identifying both similar-
ity and co-reference. This can be applied in 
cross-lingual search engines where users 
get documents of very similar content from 
different language documents.  
1 Introduction 
In this age of information technology revolution, 
the growth of technology and easy accessibility has 
contributed to the explosion of text data on the web 
in different media forms such as online news 
magazines, portals, emails, blogs etc in different 
languages. This represents 80% of the unstructured 
text content available on the web. There is an ur-
gent need to process such huge amount of text us-
ing Natural Language Processing (NLP) tech-
niques. One of the significant challenges with the 
explosion of text data is to organize the documents 
into meaningful groups according to their content.  
The work presented in this paper has two parts 
a) finding multilingual cross-document similarity 
and b) multilingual cross-document entity co-
referencing. The present work analyzes the docu-
ments and identifies whether the documents are 
similar and co-referring. Two objects are said to be 
similar, when they have some common properties 
between them. For example, two geometrical fig-
ures are said to be similar if they have the same 
shape. Hence similarity is a measure of degree of 
resemblance between two objects. 
Two documents are said to be similar if their 
contents are same. For example a document D1 
describes about a bomb blast incident in a city and 
document D2 also describes about the same bomb 
blast incident, its cause and investigation details, 
then D1 and D2 are said to be similar. But if 
document D3 talks of terrorism in general and ex-
plains bomb blast as one of the actions in terrorism 
and not a particular incident which D1 describes, 
then documents D1 and D3 are dissimilar. The task 
of finding document similarity differs from the 
task of document clustering. Clustering is a task of 
categorization of documents based on domain/field. 
In the above example, documents D1, D2, D3 can 
be said to be in a cluster of crime domain. When 
documents are similar they share common noun 
phrases, verb phrases and named entities. While in 
document clustering, sharing of named entities and 
noun phrases is not essential but still there can be 
some noun phrases and named entities in common. 
Cross-document co-referencing of entities refers to 
the identification of same entities across the docu-
ments. When the named entities present in the 
documents which are similar and also co-
referencing, then the documents are said to be co-
referring documents. 
The paper is further organized as follows. In 
section 2, the motivation behind this paper is ex-
plained and in 3 the methodology used is described. 
Results and discussions are dealt in section 4 and 
conclusion in section 5. 
2 Motivation 
Dekang Lin (1998) defines similarity from the in-
formation theoretic perspective and is applicable if 
the domain has probabilistic model.  In the past 
decade there has been significant amount of work 
done on finding similarity of documents and orga-
nizing the documents according to their content. 
Similarity of documents are identified using differ-
ent methods such as Self-Organizing Maps (SOMs) 
(Kohonen et al 2000; Rauber, 1999), based on On-
tologies and taxanomy (Gruber, 1993; Resnik, 
1995), Vector Space Model (VSM) with similarity 
measures like Dice similarity, Jaccard?s similarity, 
cosine similarity (Salton, 1989). Bagga (Bagga et 
al., 1998) have used VSM in their work for finding 
co-references across the documents for English 
documents. Chung and Allan (2004) have worked 
on cross-document co-referencing using large scale 
corpus, where they have said ambiguous names 
from the same domain (here for example, politics) 
are harder to disambiguate when compared to 
names from different domains. In their work 
Chung and Allan compare the effectiveness of dif-
ferent statistical methods in cross-document co-
reference resolution task. Harabagiu and Maiorano 
(2000) have worked on multilingual co-reference 
resolution on English and Romanian language 
texts. In their system, ?SWIZZLE? they use a data-
driven methodology which uses aligned bilingual 
corpora, linguistic rules and heuristics of English 
and Romanian documents to find co-references. In 
the Indian context, obtaining aligned bilingual cor-
pora is difficult. Document similarity between In-
dian languages and English is tough since the sen-
tence structure differs and Indian languages are 
agglutinative in nature. In the recent years there 
has been some work done in the Indian languages, 
(Pattabhi et al 2007) have used VSM for multilin-
gual cross-document co-referencing, for English 
and Tamil, where no bilingual aligned corpora is 
used. 
One of the methods used in cross-lingual infor-
mation retrieval (CLIR) is Latent Semantic Analy-
sis (LSA) in conjunction with multilingual parallel 
aligned corpus. This approach works well for in-
formation retrieval task where it has to retrieve 
most similar document in one language to a query 
given in another language. One of the drawbacks 
of using LSA in multilingual space for the tasks of 
document clustering, document similarity is that it 
gives similar documents more based on the lan-
guage than by topic of the documents in different 
languages (Chew et al 2007). Another drawback 
of LSA is that the reduced dimension matrix is dif-
ficult to interpret semantically. The examples in 
Table 1, illustrate this. 
 Before Reduction After Reduction 
1
.
{(car),(truck),(flower)} {(1.2810*car+0.5685*tr
uck),(flower) 
2 {(car),(bottle),(flower)} {(1.2810*car+0.5685*b
ottle),(flower) 
Table 1. LSA Example 
 
In the first example the component 
(1.2810*car+0.5685*truck) can be inferred as 
?Vehicle? but in cases such as in second example, 
the component (1.2810*car+0.5685*bottle) does 
not have any interpretable meaning in natural lan-
guage. In LSA the dimension reduction factor ?k? 
has very important role to play and the value of ?k? 
can be found by doing several experiments. The 
process of doing dimension reduction in LSA is 
computationally expensive. When LSA is used, it 
reduces the dimensions statistically and when there 
is no parallel aligned corpus, this can not be inter-
preted semantically. 
Hence, in the present work, we propose VSM 
which is computationally simple, along with cosine 
similarity measure to find document similarity as 
well as entity co-referencing. We have taken Eng-
lish and three Dravidian languages viz. Tamil, Te-
lugu and Malayalam for analysis. 
3 Methodology 
In VSM, each document is represented by a vector 
which specifies how many times each term occurs 
in the document (the term frequencies). These 
counts are weighted to reflect the importance of 
each term and weighting is the inverse document 
frequency (idf). If a term t occurs in n documents 
in the collection then the ?idf? is the inverse of log 
n. This vector of weighted counts is called a "bag 
of words" representation. Words such as "stop 
words" (or function words) are not included in the 
representation.  
The documents are first pre-processed, to get 
syntactic and semantic information for each word 
in the documents. The preprocessing of documents 
involves sentence splitting, morph analysis, part-
of-speech (POS) tagging, text chunking and named 
entity tagging. The documents in English are pre-
processed using Brill?s Tagger (Brill, 1994) for 
POS tagging and fn-TBL (Ngai and Florian, 2001) 
for text chunking. The documents in Indian lan-
guages are preprocessed, using  a generic engine 
(Arulmozhi et al, 2006) for POS tagging, and text 
chunking based on TBL (Sobha and Vijay, 2006). 
For both English and Indian language documents 
the named entity tagging is done using Named En-
tity Recognizer (NER) which was developed based 
on conditional random field (CRF). The tagset 
used by the NER tagger is a hierarchical tagset, 
consists of mainly i) ENAMEX, ii) NUMEX and 
iii) TIMEX. Inside the ENAMEX there are mainly 
11 subtype?s viz. a) Person b) Organization c) Lo-
cation d) Facilities e) Locomotives f) Artifacts g) 
Entertainment h) Cuisines i) Organisms j) Plants k) 
Disease.  For the task of multilingual cross-
document entities co-referencing, the documents 
are further processed for anaphora resolution 
where the corresponding antecedents for each ana-
phor are tagged in the document. For documents in 
English and Tamil, anaphora resolution is done 
using anaphora resolution system. For documents 
in Malayalam and Telugu anaphora resolution is 
done manually. After the preprocessing of docu-
ments, the language model is built by computing 
the term frequency ? inverse document frequency 
(tf-idf) matrix. For the task of finding multilingual 
cross-document similarity, we have performed four 
different experiments. They are explained below: 
 
E1: The terms are taken from documents after 
removing the stop words. These are raw terms 
where no preprocessing of documents is done; the 
terms are unique words in the document collection. 
E2: The terms taken are the words inside the 
noun phrases, verb phrases and NER expressions 
after removing the stop words. 
E3: The whole noun phrase/verb phrase/NER 
expression is taken to be a single term. 
E4: The noun phrase/NER expression along 
with the POS tag information is taken as a single 
term. 
The first experiment is the standard VSM im-
plementation. The rest three experiments differ in 
the way the terms are taken for building the VSM. 
For building the VSM model which is common for 
all language document texts, it is essential that 
there should be translation/transliteration tool. First 
the terms are collected from individual language 
documents and a unique list is formed. After that, 
using the translation/transliteration tool the equiva-
lent terms in language L2 for language L1 are 
found. The translation is done using a bilingual 
dictionary for the terms present in the dictionary. 
For most of the NERs only transliteration is possi-
ble since those are not present in the dictionary. 
The transliteration tool is developed based on the 
phoneme match it is a rule based one. All the In-
dian language documents are represented in roman 
notation (wx-notation) for the purpose of process-
ing.  
After obtaining equivalent terms in all lan-
guages, the VSM model is built. Let S1 and S2 be 
the term vectors representing the documents D1 
and D2, then their similarity is given by equation 
(1) as shown below. 
 
Sim(S1,S2) = ? (W1j x W2j )                      -- (1) 
  tj 
 Where,  
       tj is a term present in both vectors S1and S2. 
       W1j is the weight of term tj in S1 and  
       W2j is the weight of term tj in S2. 
 
The weight of term tj in the vector S1 is calculated 
by the formula given by equation (2), below. 
 
Wij=(tf*log(N/df))/[sqrt(Si12+Si22+??+Sin2)] --(2) 
Where, 
 tf = term frequency of term tj 
 N=total number of documents in the collection 
df = number of documents in the collection that 
the term tj    occurs in. 
 sqrt represents square root 
The denominator [sqrt(Si12+Si22+??+Sin2)] is the co-
sine normalization factor. This cosine normalization 
factor is the Euclidean length of the vector Si, where ?i? 
is the document number in the collection and Sin2 is the 
square of the product of (tf*log(N/df)) for term tn in the 
vector Si. 
For the task of multilingual cross-document en-
tity co-referencing, the words with-in the anaphor 
tagged sentences are considered as terms for build-
ing the language model.  
4 Results and Discussion 
The corpus used for experiments is collected from 
online news magazines and online news portals. 
The sources in English include ?The Hindu?, 
?Times of India?, ?Yahoo News?, ?New York 
Times?, ?Bangkok Post?, ?CNN?, ?WISC?, ?The 
Independent?. The sources for Tamil include ?Di-
namani?, ?Dinathanthi?, ?Dinamalar?, ?Dina-
karan?, and ?Yahoo Tamil?. The work was primar-
ily done using English and Tamil. Later on this 
was extended for Malayalam and Telugu. The data 
sources for Malayalam are ?Malayala Manorama?, 
?Mathrubhumi?, ?Deshabhimani?, ?Deepika? and 
sources for Telugu include ?Eenadu?, ?Yahoo Te-
lugu? and ?Andhraprabha?. First we discuss about 
English and Tamil and Later Telugu and Malaya-
lam. 
The domains of the news taken include sports, 
business, politics, tourism etc. The news articles 
were collected using a crawler, and hence we find 
in the collection, a few identical news articles be-
cause they appear in different sections of the news 
magazine like in Front page section, in state sec-
tion and national section. 
The dataset totally consists of 1054 English 
news articles, 390 Tamil news articles. Here we 
discuss results in two parts; in the first part results 
pertaining to document similarity are explained. In 
second part we discuss results on multilingual 
cross-document entity co-referencing. 
4.1 Document Similarity 
The data collection was done in four instances, 
spread in a period of two months. At the first in-
stance two days news was crawled from different 
news sources in English as well as Tamil. In the 
first set 1004 English documents and 297 Tamil 
documents were collected. 
In this set when manually observed (human 
judgment) it was found that there are 90 similar 
documents forming 31 groups, rest of the docu-
ments were not similar. This is taken as gold stan-
dard for the evaluation of the system output. 
As explained in the previous section, on this set 
the four experiments were performed. In the first 
experiment (E1), no preprocessing of the docu-
ments was done except that the stop words were 
removed and the language model was built. In this 
it was observed that the number of similar docu-
ments is 175 forming 25 groups. Here it was ob-
served that along with actual similar documents, 
system also gives other not similar documents (ac-
cording to gold standard) as similar ones. This is 
due to the fact there is no linguistic information 
given to the system, hence having words alone 
does not tell the context, or in which sense it is 
used. And apart from that named entities when 
split don?t give exact meaning, for example in 
name of hotels ?Leela Palace? and ?Mysore Pal-
ace?, if split into words yields three words, 
?Leela?, ?Mysore?, and ?Palace?. In a particular 
document, an event at hotel Leela Palace is de-
scribed and the hotel is referred as Leela Palace or 
by Palace alone. Another document describes 
about Dussera festival at Mysore Palace. Now here 
the system identifies both these documents to be 
similar even though both discuss about different 
events. The precision of the system was observed 
to be 51.4%, where as the recall is 100% since all 
the documents which were similar in the gold stan-
dard is identified. Here while calculating the preci-
sion; we are considering the number of documents 
that are given by the system as similar to the num-
ber of documents similar according to the gold 
standard. 
Hence to overcome the above discussed prob-
lem, we did the second experiment (E2) where 
only words which occur inside the noun phrases, 
verb phrases and named entities are considered as 
terms for building the language model. Here it is 
observed that the number of similar documents is 
140 forming 30 groups. This gives a precision of 
64.2% and 100% recall. Even though we find a 
significant increase in the precision but still there 
are large number of false positives given by the 
system. A document consists of noun phrases and 
verb phrases, when the individual tokens inside 
these phrases are taken; it is equivalent to taking 
almost the whole document. This reduces the 
noise. The problem of ?Leela Palace? and ?Mysore 
Palace? as explained in the previous paragraph still 
persists here. 
In the third experiment (E3) the whole noun 
phrase, verb phrase and named entity is considered 
as a single term for building the language model. 
Here the phrases are not split into individual to-
kens; the whole phrase is a single term for lan-
guage model. This significantly reduces the num-
ber of false positives given by the system. The sys-
tem identifies 106 documents as similar documents 
forming 30 groups. Now the precision of the sys-
tem is 84.9%. In this experiment, the problem of 
?Leela Palace? and ?Mysore Palace? is solved. 
Though this problem was solved the precision of 
the system is low, hence we performed the fourth 
(E4) experiment. 
In the fourth experiment (E4), the part-of-speech 
(POS) information is given along with the phrase 
for building the language model. It is observed that 
the precision of the system increases. The number 
of similar documents identified is 100 forming 31 
groups. This gives a precision of 90% and a recall 
of 100%.  
Another important factor which plays a crucial 
role in implementation of language model or VSM 
is the threshold point. What is the threshold point 
that is to be taken? For obtaining an answer for this 
question, few experiments were performed by set-
ting the threshold at various points in the range 
0.75 to 0.95. When the threshold was set at 0.75 
the number of similar documents identified by the 
system was larger, not true positives but instead 
false positives. Hence the recall was high and pre-
cision was low at 50%. When the threshold was 
moved up and set at 0.81, the number of similar 
documents identified was more accurate and the 
number of false positives got reduced. The preci-
sion was found to be 66%.  When the threshold 
was moved up still further and set at 0.90, it was 
found that the system identified similar documents 
which were matching with the human judgment. 
The precision of the system was found to be 90%. 
The threshold was moved up further to 0.95, think-
ing that the precision would further improve, but 
this resulted in documents which were actually 
similar to be filtered out by the system. Hence the 
threshold chosen was 0.9, since the results ob-
tained at this threshold point had matched the hu-
man judgment. For the experiments E1, E2, E3 and 
E4 explained above, the threshold is fixed at 0.9. 
A new set of data consisting of 25 documents 
from 5 days news articles is collected. This is com-
pletely taken from single domain, terrorism. These 
news articles describe specifically the Hyderabad 
bomb blast, which occurred on August 25th 2007. 
All these 25 documents were only English docu-
ments from various news magazines. This data set 
was collected specifically to observe the perform-
ance of the system, when the documents belonging 
to single domain are given. In the new data set, 
from terrorism domain, human judgment for docu-
ment similarity was found to have 13 similar docu-
ments forming 3 groups. While using this data set 
the noun phrases, verb phrases and named entities 
along with POS information were taken as terms to 
build the language model and the threshold was set 
at 0.9, it was observed that the system finds 14 
documents to be similar forming 3 groups. Here, 
out of 14 similar documents, only 12 documents 
match with the human judgment and one document 
which ought to be identified was not identified by 
the system. The document which was not identified 
described about the current event, that is, bomb 
blast on 25th August in the first paragraph and then 
the rest of the document described about the simi-
lar events that occurred in the past. Hence the simi-
larity score obtained for this document with respect 
to other documents in the group was 0.84 which is 
lower than the threshold fixed. Hence the recall of 
the system is 92.3% and the precision of the sys-
tem is 85.7%. 
Another data set consisting of 114 documents 
was taken from tourism domain. The documents 
were both in Tamil and English, 79 documents in 
Tamil and 35 documents in English. This data set 
describes various pilgrim places and temples in 
Southern India. The human annotators have found 
21 similar documents which form a group of three. 
These similar documents describe about Lord 
Siva?s and Lord Murugan?s temples.  The system 
obtained 25 documents as similar and grouped into 
three groups. Out of 25 documents obtained as 
similar, four were dissimilar. These dissimilar 
documents described non-Siva temples in the same 
place. In these dissimilar documents the names of 
offerings, festivals performed were referred by the 
same names as in the rest of the documents of the 
group, hence these documents obtained similarity 
score of 0.96 with respect to other documents in 
the group. Here we get a precision of 84% and a 
recall of 100%. 
A new data set consisting of 46 documents was 
taken from various news magazines. This set con-
sists of 24 English documents, 11 Tamil docu-
ments, 7 Malayalam documents and 4 Telugu 
documents.  This data set describes the earthquake 
in Indonesia on 12th September 2007 and tsunami 
warning in other countries. The news articles were 
collected on two days 13th and 14th September 
2007.  
The documents collected were in different font 
encoding schemes. Hence before doing natural 
language processing such as morph-analysis, POS 
tagging etc, the documents were converted to a 
common roman notation (wx-notation) using the 
font converter for each encoding scheme. 
Here we have used multilingual dictionaries of 
place; person names etc for translation. The lan-
guage model is built by taking noun phrases and 
verb phrases along with POS information were as 
terms. In this set human annotators have found 45 
documents to be similar and have grouped them 
into one group. The document which was identi-
fied as dissimilar describes about a Tamil film 
shooting at Indonesia being done during the quake 
time. The system had identified all the 46 docu-
ments including the film shooting document in the 
collection to be similar and put into one group. The 
?film shooting? document consisted of two para-
graphs about the quake incident, other two para-
graphs consisted of statement by the film producer 
stating that the whole crew is safe and the shooting 
is temporarily suspended for next few days. Since 
this document also contained the content describ-
ing the earthquake found in other documents of the 
group, the system identified this ?film shooting? 
document to be similar. Here one interesting point 
which was found was that all the documents gave a 
very high similarity score greater than 0.95. Hence 
the precision of the system is 97.8% and recall 
100%. 
The summary of all these experiments with dif-
ferent dataset is shown in the table 2 below. 
SNo Dataset Preci-
sion % 
Recall 
% 
1 English 1004 and Tamil 
297 documents 
90.0 100.0 
2 English 25 ? terrorism 
domain documents 
85.7 92.3 
3 35 English Docs and 
Tamil 79 docs - Tour-
ism domain 
84.0 100.0 
4 46 Docs on Earth 
Quake incident ? 24 
English, 11 Tamil, 7 
Malayalam, 4 Telugu 
97.8 100.0 
Average 89.3 % 98.07% 
Table 2. Summary of Results for Document 
similarity for four different data sets 
4.2 Document Co-referencing 
The documents that were identified as similar ones 
are taken for entity co-referencing. In this work the 
identification of co-referencing documents is done 
for English and Tamil. In this section first we dis-
cuss the co-referencing task for English documents 
in terrorism domain, then for documents in English 
and Tamil in Tourism domain. In the end of this 
section we discuss about documents in English and 
Tamil, which are not domain specific. 
  In the first experiment, the document collection in 
terrorism domain is taken for co-referencing task. 
This data set of 25 documents in terrorism domain 
consists of 60 unique person names. In this work 
we consider only person names for entity co-
referencing. In this data set, 14 documents are 
identified as similar ones by the system. These 14 
documents consist of 26 unique person names. .  
The language model is built using only named 
entity terms and the noun, verb phrases occurring 
in the same sentence where the named entity oc-
curs. POS information is also provided with the 
terms. Here we find that out of 26 entities, the sys-
tem co-references correctly for 24 entities, even 
though the last names are same.  The results ob-
tained for these named entities is shown in the be-
low table Table 3. 
E
ntity 
N
am
e 
N
o. of links  
containing 
the entity
C
orrect 
R
esponses 
obtained
T
otal R
e-
sponses ob-
tained 
P
recision 
%
 
R
ecall %
 
Y S Ra-
jasekhar 
Reddy 
7 7 7 100 100 
Indrasena 
Reddy 
1 1 1 100 100 
K Jana 
Reddy 
1 1 1 100 100 
Shivaraj 
Patil 
2 2 2 100 100 
Manmohan 
Singh 
4 4 4 100 100 
Abdul Sha-
hel 
Mohammad 
1 1 2 50 100 
Mohammad 
Abdullah 
1 1 2 50 100 
Mohammad 
Amjad 
1 1 1 100 100 
Mohammad 
Yunus 
1 1 1 100 100 
Ibrahim 1 1 1 100 100 
Dawood 
Ibrahim 
1 1 1 100 100 
Madhukar 
Gupta 
3 3 3 100 100 
N Chandra-
babu Naidu 
2 2 2 100 100 
Tasnim 
Aslam 
2 2 2 100 100 
Mahender 
Agrawal 
1 1 1 100 100 
Somnath 
Chatterjee 
2 2 2 100 100 
Pervez 
Musharaff 
2 2 2 100 100 
Sonia Gan-
dhi 
2 2 2 100 100 
Taslima 1 1 1 100 100 
Nasrin 
Bandaru 
Dattatreya 
1 1 1 100 100 
L K Advani 2 2 2 100 100 
Average 95.2 100 
Table 3. Results for entity co-referencing for Eng-
lish documents in terrorism domain 
 
The system identifies the entity names ending 
with ?Reddy? correctly. These names in the docu-
ments occur along with definite descriptions which 
helps the system in disambiguating these names. 
For example ?Y S Rajasekhar Reddy? in most cases 
is referred to as ?Dr. Reddy? along with the defi-
nite description ?chief minister?. Similarly the 
other name ?K Jana Reddy? occurs with the defi-
nite description ?Home minister?. Since here we 
are taking full noun phrases as terms for building 
language model, this helps obtaining good results. 
For entities such as ?Abdul Shahel Mohammad? 
and ?Mohammad Abdullah?, it is observed that the 
both names are referred in the documents as 
?Mohammad? and surrounding phrases do not 
have any distinguishing phrases such as definite 
descriptions, which differentiate these names. Both 
these entities have been involved in masterminding 
of the Hyderabad bomb blast. Hence the system 
couldn?t disambiguate between these two named 
entities and identifies both to be same, hence it 
fails here.  
In the second experiment, the data set in Tour-
ism domain consisting of 79 Tamil Documents and 
35 English documents is taken for the task of co-
referencing. In this data set 25 documents were 
identified as similar. Now these similar documents 
of 25 are considered for entity co-referencing task. 
There are 35 unique names of Gods. Here in this 
domain, one of the interesting points is that, there 
are different names to refer to a single God. For 
example Lord Murugan, is also referred by other 
names such as ?Subramanyan?, ?Saravana?, ?Kart-
tikeyan?, ?Arumukan? etc. Simialrly for Lord Siva 
is referred by ?Parangirinathar?, ?Dharbaranes-
wara? etc. It is observed that in certain documents 
the alias names are not mentioned along with 
common names. In these instances even human 
annotators found it tough for co-referencing, hence 
the system could not identify the co-references. 
This problem of alias names can be solved by hav-
ing a thesaurus and using it for disambiguation. 
The results obtained for these named entities are 
shown in the table 4, below. 
E
ntity 
N
am
e 
N
o. of 
links  con-
taining the 
entity
C
orrect 
R
esponses 
obtained  
T
otal R
e-
sponses 
obtained 
P
recision 
%
 
R
ecall %
 
Murugan 7 7 8 87.5 100 
Shiva 10 9 9 100 90 
Parvathi 10 9 11 81.8 90 
Nala 5 5 5 100 100 
Damayan-
thi 
2 2 2 100 100 
Narada 3 3 3 100 100 
Sanees-
warar 
6 6 7 85.7 100 
Deivayani 4 4 4 100 100 
Vishnu 2 2 2 100 100 
Vinayaka 3 3 3 100 100 
Indra 2 2 2 100 100 
Thiruna-
vukkarasar 
1 1 1 100 100 
Mayan 2 2 2 100 100 
Average 96.5 98.4 
Table 4. Results for entity co-referencing for 
English and Tamil Documents in Tourism domain 
 
The co-referencing system could disambiguate a 
document which was identified as similar by the 
system and dissimilar by the human annotator. 
 Another experiment is performed where both 
English and Tamil Documents are taken for entity 
co-referencing. In this experiment we have taken 
the data set in which there are 1004 English docu-
ments and 297 Tamil documents.  The documents 
are not domain specific. Here 100 documents are 
identified as similar ones, which contains of 64 
English and 36 Tamil documents. Now we con-
sider these 100 similar documents for entity co-
referencing. In the 100 similar documents, there 
are 520 unique named entities. The table (Table 5) 
below shows results of few interesting named enti-
ties in this set of 100 similar documents. 
E
ntity 
N
am
e 
N
o. of links  
containing 
the entity
C
orrect 
R
esponses 
obtained 
T
otal R
e-
sponses ob-
tained 
P
recision 
%
 
R
ecall  %
 
Karunanidhi 7 7 7 100 100 
Manmohan Singh 15 14 16 87.5 93.3 
Sonia Gandhi 54 54 58 93.1 100 
Shivaraj Patil 8 8 10 80 100 
Prathibha Patil 24 24 26 92.3 100 
Lalu Prasad 5 5 5 100 100 
Atal Bihari Va-
jpayee 
4 4 4 100 100 
Abdul Kalam 22 22 22 100 100 
Sania Mirza 10 10 10 100 100 
Advani 8 8 8 100 100 
Average 95.3 99.3 
Table 5. Results for entity co-referencing for 
English and Tamil Documents not of any specific 
domain 
5 Conclusion 
The VSM method is a well known statistical 
method, but here it has been applied for multilin-
gual cross-document similarity, which is a first of 
its kind. Here we have tried different experiments 
and found that using phrases with its POS informa-
tion as terms for building language model is giving 
good performance. In this we have got an average 
precision of 89.3 and recall of 98.07% for docu-
ment similarity. Here we have also worked on mul-
tilingual cross-document entity co-referencing and 
obtained an average precision of 95.6 % and recall 
of 99.2 %. The documents taken for multilingual 
cross-document co-referencing are similar docu-
ments identified by the similarity system. Consid-
ering similar documents, helps indirectly in getting 
contextual information for co-referencing entities, 
because obtaining similar documents removes 
documents which are not in the same context. 
Hence this helps in getting good precision. Here 
we have worked on four languages viz. English, 
Tamil, Malayalam and Telugu. This can be applied 
for other languages too. Multilingual document 
similarity and co-referencing, helps in retrieving 
similar documents across languages. 
References 
Arulmozhi Palanisamy and Sobha Lalitha Devi. 2006. 
HMM based POS Tagger for a Relatively Free Word 
Order Language, Journal of Research on Computing 
Science, Mexico. 18:37-48. 
Bagga, Amit and Breck Baldwin. 1998. Entity-Based 
Cross-Document Coreferencing Using the Vector 
Space Model, Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics 
and the 17th International Conference on Computa-
tional Linguistics (COLING-ACL'98):79-85. 
Brill, Eric. 1994. Some Advances in transformation 
Based Part of Speech Tagging, Proceedings of the 
Twelfth International Conference on Artificial Intel-
ligence (AAAI-94), Seattle, WA 
Peter A. Chew,  Brett W. Bader, Tamara G. Kolda, Ah-
med Abdelali. 2007. Cross-Language Information 
Retrieval Using PARAFAC2, In the Proceedings 
Thirteenth International Conference on Knowledge 
Discovery and Data Mining (KDD? 07), San Jose, 
California.:143-152. 
Chung Heong Gooi and James Allan. 2004. Cross-
Document Coreference on a Large Scale Corpus, 
Proceedings of HLT-NAACL: 9-16. 
Dekang Lin.  1998. An Information-Theoretic Definition 
of Similarity, Proceedings of International Confer-
ence on Machine Learning, Madison, Wisconsin, 
July.  
T. R. Gruber. 1993. A translation approach to portable 
ontologies, Knowledge Acquisition, 5(2):199?220. 
Harabagiu M Sanda and Steven J Maiorano. 2000. Mul-
tilingual Coreference Resolution, Proceedings of 6th 
Applied Natural Language Processing Conference: 
142?149. 
Kohonen, Teuvo Kaski, Samuel Lagus, Krista Salojarvi, 
Jarkko Honkela, Jukka Paatero,Vesa Saarela, Anti.  
2000. Self organisation of a massive document col-
lection, IEEE Transactions on Neural Networks, 
11(3): 574-585. 
G. Ngai and R. Florian. 2001. Transformation-Based 
Learning in the Fast Lane, Proceedings of the 
NAACL'2001, Pittsburgh, PA: 40-47 
R K Rao Pattabhi, L Sobha, and Amit Bagga. 2007. 
Multilingual cross-document co-referencing, Pro-
ceedings of 6th Discourse Anaphora and Anaphor 
Resolution Colloquium (DAARC), March 29-30, 
2007, Portugal:115-119 
Rauber, Andreas Merkl, Dieter. 1999. The SOMLib 
digital library system,  In the Proceedings of the 3rd 
European Conference on Research and Advanced 
Technology for Digital Libraries (ECDL'99), Paris, 
France. Berlin: 323-341. 
P. Resnik. 1995. Using information content to evaluate 
semantic similarity in taxonomy, Proceedings of 
IJCAI: 448?453. 
Salton, Gerald. 1989. Automatic Text Processing: The 
Transformation, Analysis and Retrieval of Informa-
tion by Computer, Reading, MA: Addison Wesley 
Sobha L, and Vijay Sundar Ram. 2006. Noun Phrase 
Chunker for Tamil, Proceedings of the First National 
Symposium on Modeling and Shallow Parsing of In-
dian Languages (MSPIL), IIT Mumbai, India: 194-
198. 
Designing a Common POS-Tagset Framework for Indian Languages 
Sankaran Baskaran, Microsoft Research India. Bangalore. baskaran@microsoft.com 
Kalika Bali, Microsoft Research India. Bangalore. kalikab@microsoft.com 
Tanmoy Bhattacharya, Delhi University, Delhi. tanmoy1@gmail.com 
Pushpak Bhattacharyya, IIT-Bombay, Mumbai. pb@cse.iitb.ac.in 
Girish Nath Jha, Jawaharlal Nehru University, Delhi. girishj@mail.jnu.ac.in 
Rajendran S, Tamil University, Thanjavur. raj_ushush@yahoo.com 
Saravanan K, Microsoft Research India, Bangalore. v-sarak@microsoft.com 
Sobha L, AU-KBC Research Centre, Chennai. sobha@au-kbc.org 
Subbarao K V. Delhi. kvs2811@yahoo.com
 
 
Abstract 
Research in Parts-of-Speech (POS) tagset 
design for European and East Asian lan-
guages started with a mere listing of impor-
tant morphosyntactic features in one lan-
guage and has matured in later years to-
wards hierarchical tagsets, decomposable 
tags, common framework for multiple lan-
guages (EAGLES) etc. Several tagsets 
have been developed in these languages 
along with large amount of annotated data 
for furthering research. Indian Languages 
(ILs) present a contrasting picture with 
very little research in tagset design issues. 
We present our work in designing a com-
mon POS-tagset framework for ILs, which 
is the result of in-depth analysis of eight 
languages from two major families, viz. 
Indo-Aryan and Dravidian. Our framework 
follows hierarchical tagset layout similar to 
the EAGLES guidelines, but with signifi-
cant changes as needed for the ILs. 
1 Introduction 
A POS tagset design should take into consideration 
all possible morphosyntactic categories that can 
occur in a particular language or group of languag-
es (Hardie, 2004). Some effort has been made in 
the past, including the EAGLES guidelines for 
morphosyntactic annotation (Leech and Wilson, 
1996) to define guidelines for a common tagset 
across multiple languages with an aim to capture 
more detailed morphosyntactic features of these 
languages.  
However, most of the tagsets for ILs are lan-
guage specific and cannot be used for tagging data 
in other language. This disparity in tagsets hinders 
interoperability and reusability of annotated corpo-
ra. This further affects NLP research in resource 
poor ILs where non-availability of data, especially 
tagged data, remains a critical issue for researchers. 
Moreover, these tagsets capture the morphosyntac-
tic features only at a shallow level and miss out the 
richer information that is characteristic of these 
languages. 
The work presented in this paper focuses on de-
signing a common tagset framework for Indian 
languages using the EAGLES guidelines as a mod-
el. Though Indian languages belong to (mainly) 
four distinct families, the two largest being Indo-
Aryan and Dravidian, as languages that have been 
in contact for a long period of time, they share sig-
nificant similarities in morphology and syntax. 
This makes it desirable to design a common tagset 
framework that can exploit this similarity to facili-
tate the mapping of different tagsets to each other. 
This would not only allow corpora tagged with 
different tagsets for the same language to be reused 
but also achieve cross-linguistic compatibility be-
tween different language corpora. Most important-
ly, it will ensure that common categories of differ-
ent languages are annotated in the same way. 
In the next section we will discuss the impor-
tance of a common standard vis-?-vis the currently 
available tagsets for Indian languages. Section 3 
will provide the details of the design principles 
The 6th Workshop on Asian Languae Resources, 2008
89
behind the framework presented in this paper. Ex-
amples of tag categories in the common framework 
will be presented in Section 4. Section 5 will dis-
cuss the current status of the paper and future steps 
envisaged.  
2 Common Standard for POS Tagsets 
Some of the earlier POS tagsets were designed 
for English (Greene and Rubin, 1981; Garside, 
1987; Santorini, 1990) in the broader context of 
automatic parsing of English text. These tagsets 
popular even today, though designed for the same 
language differ significantly from each other mak-
ing the corpora tagged by one incompatible with 
the other. Moreover, as these are highly language 
specific tagsets they cannot be reused for any other 
language without substantial changes this requires 
standardization of POS tagsets (Hardie 2004).  
Leech and Wilson (1999) put forth a strong argu-
ment for the need to standardize POS tagset for 
reusability of annotated corpora and interopera-
bility across corpora in different languages. 
EAGLES guidelines (Leech and Wilson 1996) 
were a result of such an initiative to create stan-
dards that are common across languages that share 
morphosyntactic features. 
Several POS tagsets have been designed by a 
number of research groups working on Indian 
Languages though very few are available publicly 
(IIIT-tagset, Tamil tagset). However, as each of 
these tagsets have been motivated by specific re-
search agenda, they differ considerably in terms of 
morphosyntactic categories and features, tag defi-
nitions, level of granularity, annotation guidelines 
etc. Moreover, some of the tagsets (Tamil tagset) 
are language specific and do not scale across other 
Indian languages. This has led to a situation where 
despite strong commonalities between the lan-
guages addressed resources cannot be shared due 
to incompatibility of tasgets. This is detrimental to 
the development of language technology for Indian 
languages which already suffer from a lack of ade-
quate resources in terms of data and tools. 
In this paper, we present a common framework 
for all Indian languages where an attempt is made 
to treat equivalent morphosyntactic phenomena 
consistently across all languages. The hierarchical 
design, discussed in detail in the next section, also 
allows for a systematic method to annotate lan-
guage particular categories without disregarding 
the shared traits of the Indian languages.  
3 Design Principles 
Whilst several large projects have been concerned 
with tagset development very few have touched 
upon the design principles behind them. Leech 
(1997), Cloeren (1999) and Hardie (2004) are 
some important examples presenting universal 
principles for tagset design. 
In this section we restrict the discussion to the 
principles behind our tagset framework. Important-
ly, we diverge from some of the universal prin-
ciples but broadly follow them in a consistent way.  
Tagset structure: Flat tagsets just list down the 
categories applicable for a particular language 
without any provision for modularity or feature 
reusability. Hierarchical tagsets on the other hand 
are structured relative to one another and offer a 
well-defined mechanism for creating a common 
tagset framework for multiple languages while 
providing flexibility for customization according to 
the language and/ or application. 
Decomposability in a tagset alows different fea-
tures to be encoded in a tag by separate sub-stings. 
Decomposable tags help in better corpus analysis 
(Leech 1997) by allowing to search with an un-
derspecified search string. 
In our present framework, we have adopted the 
hierarchical layout as well as decomposable tags 
for designing the tagset. The framework will have 
three levels in the hierarchy with categories, types 
(subcategories) and features occupying the top, 
medium and the bottom layers. 
What to encode? One thumb rule for the POS 
tagging is to consider only the aspects of morpho-
syntax for annotation and not that of syntax, se-
mantics or discourse. We follow this throughout 
and focus only on the morphosyntactic aspects of 
the ILs for encoding in the framework. 
Morphology and Granularity: Indian languag-
es have complex morphology with varying degree 
of richness. Some of the languages such as those of 
the Dravidian family also display agglutination as 
an important characteristic. This entails that mor-
phological analysis is a desirable pre-process for 
the POS tagging to achieve better results in auto-
matic tagging. We encode all possible morphosyn-
tactic features in our framework assuming the exis-
The 6th Workshop on Asian Languae Resources, 2008
90
tence of morphological analysers and leave the 
choice of granularity to users. 
As pointed out by Leech (1997) some of the 
linguistically desirable distinctions may not be 
feasible computationally. Therefore, we ignore 
certain features that may not be computationally 
feasible at POS tagging level. 
Multi-words: We treat the constituents of Mul-
ti-word expressions (MWEs) like Indian Space 
Research Organization as individual words and tag 
them separately rather than giving a single tag to 
the entire word sequence. This is done because: 
Firstly, this is in accordance with the standard 
practice followed in earlier tagsets. Secondly, 
grouping MWEs into a single unit should ideally 
be handled in chunking. 
Form vs. function: We try to adopt a balance 
between form and function in a systematic and 
consistent way through deep analysis. Based on 
our analysis we propose to consider the form in 
normal circumstances and the function for words 
that are derived from other words. More details on 
this will be provided in the framework document 
(Baskaran et al2007) 
Theoretical neutrality: As Leech (1997) points 
out the annotation scheme should be theoretically 
neutral to make it clearly understandable to a larger 
group and for wider applicability. 
Diverse Language families: As mentioned ear-
lier, we consider eight languages coming from two 
major language families of India, viz. Indo-Aryan 
and Dravidian. Despite the distinct characteristics 
of these two families, it is however striking to note 
the typological parallels between them, especially 
in syntax. For example, both families follow SOV 
pattern. Also, several Indo-Aryan languages such 
as Marathi, Bangla etc. exhibit some agglutination, 
though not to the same extent of Dravidian. Given 
the strong commonalities between the two families 
we decided to use a single framework for them 
4 POS Tagset Framework for Indian lan-
guages 
The tagset framework is laid out at the following 
four levels similar to EAGLES. 
I. Obligatory attributes or values are generally 
universal for all languages and hence must be 
included in any morphosyntactic tagset. The 
major POS categories are included here. 
II. Recommended attributes or values are recog-
nised to be important sub-categories and fea-
tures common to a majority of languages.  
III. Special extensions1 
a. Generic attributes or values 
b. Language-specific attributes or values are 
the attributes that are relevant only for few lan-
guages and do not apply to most languages. 
All the tags were discussed and debated in detail 
by a group of linguists and computer scien-
tists/NLP experts for eight Indian languages viz. 
Bengali, Hindi, Kannada, Malayalam, Marathi, 
Sanskrit, Tamil and Telugu.  
Now, because of space constraints we present 
only the partial tagset framework. This is just to 
illustrate the nature of the framework and the com-
plete version as well as the rationale for different 
categories/features in the framework can be found 
in Baskaran et al (2007).2 
In the top level the following 12 categories are 
identified as universal categories for all ILs and 
hence these are obligatory for any tagset. 
 
1. [N] Nouns 7.   [PP] Postpositions  
2. [V] Verbs  8.   [DM] Demonstratives 
3. [PR] Pronouns  9.   [QT] Quantifiers 
4. [JJ] Adjectives  10. [RP] Particles  
5. [RB] Adverbs  11. [PU] Punctuations  
6. [PL] Participles  12. [RD] Residual3 
 
The partial tagset illustrated in Figure 1 high-
lights entries in recommended and optional catego-
ries for verbs and participles marked for three le-
vels.4 The features take the form of attribute-value 
pairs with values in italics and in some cases (such 
as case-markers for participles) not all the values 
are fully listed in the figure. 
5 Current Status and Future Work 
In the preceding sections we presented a common 
framework being designed for POS tagsets for In-
dian Languages. This hierarchical framework has 
                                                 
1
 We do not have many features defined under the special 
extensions and this is mainly retained for any future needs. 
2 Currently this is just the draft version and the final version 
will be made available soon 
3 For words or segments in the text occurring outside the gam-
bit of grammatical categories like foreign words, symbols,etc.   
4  These are not finalised as yet and there might be some 
changes in the final version of the framework. 
The 6th Workshop on Asian Languae Resources, 2008
91
three levels to permit flexibility and interoperabili-
ty between languages. We are currently involved in 
a thorough review of the present framework by 
using it to design the tagset for specific Indian lan-
guages. The issues that come up during this 
process will help refine and consolidate the 
framework further.  In the future, annotation guide-
lines with some recommendations for handling 
ambiguous categories will also be defined.  With 
the common framework in place, it is hoped that 
researchers working with Indian Languages would 
be able to not only reuse data annotated by each 
other but also share tools across projects and lan-
guages. 
References 
Baskaran S. et al 2007. Framework for a Common 
     Parts-of-Speech Tagset for Indic Languages. (Draft) 
    http://research.microsoft.com/~baskaran/POSTagset/ 
  
Cloeren, J. 1999. Tagsets. In Syntactic Wordclass Tagging, 
ed. Hans van Halteren, Dordrecht.: Kluwer Academic. 
Hardie, A . 2004. The Computational Analysis of Morpho-
syntactic Categories in Urdu. PhD thesis submitted to 
Lancaster University. 
Greene, B.B. and Rubin, G.M. 1981. Automatic grammati-
cal tagging of English. Providence, R.I.: Department of 
Linguistics, Brown University 
Garside, R. 1987 The CLAWS word-tagging system. In 
The Computational Analysis of English, ed. Garside, 
Leech and Sampson, London: Longman. 
Leech, G and Wilson, A. 1996. Recommendations for the 
Morphosyntactic Annotation of Corpora. EAGLES Re-
port EAG-TCWG-MAC/R. 
Leech, G. 1997. Grammatical Tagging. In Corpus Annota-
tion: Linguistic Information from Computer Text Cor-
pora, ed: Garside, Leech and McEnery, London: Long-
man  
Leech, G and Wilson, A. 1999. Standards for Tag-sets. In 
Syntactic Wordclass Tagging, ed. Hans van Halteren, 
Dordrecht: Kluwer Academic. 
Santorini, B. 1990. Part-of-speech tagging guidelines for 
the Penn Treebank Project. Technical report MS-CIS-
90-47, Department of Computer and Information 
Science, University of Pennsylvania 
IIIT-tagset. A Parts-of-Speech tagset for Indian languages. 
http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.
pdf 
Tamil tagset. AU-KBC Parts-of-Speech tagset for Tamil. 
http://nrcfosshelpline.in/smedia/images/downloads/Tam
il_Tagset-opensource.odt 
Aspect 
 Perfect 
 Imperfect 
 Progressive 
Mood 
 Declarative 
 Subjunctative/   
        Hortative 
 Conditional 
 Imperative 
 Presumptive 
Level - 3 
Nouns 
Verbs 
Pronouns 
Adjectives 
Adverbs 
Postpositions 
Demonstratives 
Quantifiers 
Particles 
Punctuations 
Residual Participles 
Level - 1 
Type 
 Finite 
 Auxiliary 
 Infinitive 
 Non-finite 
 Nominal 
Gender 
 Masculine 
 Feminine 
 Neuter 
Number 
 Singular 
 Plural/Hon. 
 Dual 
 Honourific 
Person 
 First 
 Second 
 Third 
Tense 
 Past 
 Present 
 Future 
Negative 
Type 
 General 
 Adjectival 
 Verbal 
 Nominal 
Gender 
 As in verbs 
Number 
 Singular 
 Plural 
 Dual 
Case 
 Direct 
 Oblique 
Case-markers 
 Ergative 
 Accusative 
 etc. 
Tense 
 As in verbs 
Negative 
Level - 2 
Fig-1. Tagset framework - partial representation 
The 6th Workshop on Asian Languae Resources, 2008
92
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 53?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Approach to Text Summarization 
 
 
Sankar K Sobha L 
AU-KBC Research Centre AU-KBC Research Centre 
MIT Campus, Anna University MIT Campus, Anna University 
Chennai- 44. Chennai- 44. 
sankar@au-kbc.org sobha@au-kbc.org 
 
 
 
 
 
Abstract 
We propose an efficient text summarization 
technique that involves two basic opera-
tions. The first operation involves finding 
coherent chunks in the document and the 
second operation involves ranking the text 
in the individual coherent chunks and pick-
ing the sentences that rank above a given 
threshold. The coherent chunks are formed 
by exploiting the lexical relationship be-
tween adjacent sentences in the document. 
Occurrence of words through repetition or 
relatedness by sense relation plays a major 
role in forming a cohesive tie. The pro-
posed text ranking approach is based on a 
graph theoretic ranking model applied to 
text summarization task. 
1 Introduction 
Automated summarization is an important area in 
NLP research. A variety of automated summariza-
tion schemes have been proposed recently. NeATS 
(Lin and Hovy, 2002) is a sentence position, term 
frequency, topic signature and term clustering 
based approach and MEAD (Radev et al, 2004) is 
a centroid based approach. Iterative graph based 
Ranking algorithms, such as Kleinberg?s HITS 
algorithm (Kleinberg, 1999) and Google?s Page-
Rank (Brin and Page, 1998) have been traditionally 
and successfully used in web-link analysis, social 
networks and more recently in text processing ap-
plications (Mihalcea and Tarau, 2004), (Mihalcea 
et al, 2004), (Erkan and Radev, 2004) and (Mihal-
cea, 2004). These iterative approaches have a high 
time complexity and are practically slow in dy-
namic summarization. Proposals are also made for 
coherence based automated summarization system 
(Silber and McCoy, 2000). 
We propose a novel text summarization tech-
nique that involves two basic operations, namely 
finding coherent chunks in the document and rank-
ing the text in the individual coherent chunks 
formed. 
For finding coherent chunks in the document, we 
propose a set of rules that identifies the connection 
between adjacent sentences in the document. The 
connected sentences that are picked based on the 
rules form coherent chunks in the document.  For 
text ranking, we propose an automatic and unsu-
pervised graph based ranking algorithm that gives 
improved results when compared to other ranking 
algorithms. The formation of coherent chunks 
greatly improves the amount of information of the 
text picked for subsequent ranking and hence the 
quality of text summarization.  
The proposed text ranking technique employs a 
hybrid approach involving two phases; the first 
phase employs word frequency statistics and the 
second phase involves a word position and string 
pattern based weighing algorithm to find the 
weight of the sentence. A fast running time is 
achieved by using a compression hash on each sen-
tence.  
53
This paper is organized as follows: section 2 
discusses lexical cohesion, section 3 discusses the 
text ranking algorithm and section 4 describes the 
summarization by combining lexical cohesion and 
summarization. 
2 Lexical Cohesion 
Coherence in linguistics makes the text semantical-
ly meaningful. It is achieved through semantic fea-
tures such as the use of deictic (a deictic is an 
expression which shows the direction. ex: that, 
this.), anaphoric (a referent which requires an ante-
cedent in front. ex: he, she, it.), cataphoric (a refe-
rent which requires an antecedent at the back.), 
lexical relation and proper noun repeating elements 
(Morris and Hirst, 1991). Robert De Beaugrande 
and Wolfgang U. Dressler define coherence as a 
?continuity of senses? and ?the mutual access and 
relevance within a configuration of concepts and 
relations? (Beaugrande and Dressler, 1981). Thus a 
text gives meaning as a result of union of meaning 
or senses in the text.  
The coherence cues present in a sentence are di-
rectly visible when we go through the flow of the 
document. Our approach aims to achieve this ob-
jective with linguistic and heuristic information.  
The identification of semantic neighborhood, oc-
currence of words through repetition or relatedness 
by sense relation namely synonyms, hyponyms and 
hypernym, plays a major role in forming a cohesive 
tie (Miller et al, 1990). 
2.1 Rules for finding Coherent chunks 
When parsing through a document, the relationship 
among adjacent sentences is determined by the 
continuity that exists between them.  
We define the following set of rules to find co-
herent chunks in the document. 
 
Rule 1 
 
The presence of connectives (such as accordingly, 
again, also, besides) in present sentence indicates 
the connectedness of the present sentence with the 
previous sentence. When such connectives are 
found, the adjacent sentences form coherent 
chunks. 
 
 
 
Rule 2 
 
A 3rd person pronominal in a given sentence refers 
to the antecedent in the previous sentence, in such 
a way that the given sentence gives the complete 
meaning with respect to the previous sentence. 
When such adjacent sentences are found, they form 
coherent chunks.  
 
Rule 3 
 
 The reappearance of NERs in adjacent sentences 
is an indication of connectedness. When such adja-
cent sentences are found, they form coherent 
chunks. 
 
Rule 4 
 
An ontology relationship between words across 
sentences can be used to find semantically related 
words across adjacent sentences that appear in the 
document. The appearance of related words is an 
indication of its coherence and hence forms cohe-
rent chunks. 
All the above rules are applied incrementally to 
achieve the complete set of coherent chunks. 
2.1.1 Connecting Word 
The ACE Corpus was used for studying the cohe-
rence patterns between adjacent sentences of the 
document. From our analysis, we picked out a set 
of keywords such that, the appearance of these 
keywords at the beginning of the sentence provide 
a strong lexical tie with the previous sentence. 
The appearance of the keywords ?accordingly, 
again, also, besides, hence, henceforth, however, 
incidentally, meanwhile, moreover, namely, never-
theless, otherwise, that is, then, therefore, thus, 
and, but, or, yet, so, once, so that, than, that, till, 
whenever, whereas and wherever?, at the begin-
ning of the present sentence was found to be highly 
coherent with the previous sentence.  
Linguistically a sentence cannot start with the 
above words without any related introduction in 
the previous sentence.   
Furthermore, the appearance of the keywords 
?consequently, finally, furthermore?, at the begin-
ning or middle of the present sentence was found 
to be highly cohesive with the previous sentence.  
Example 1 
54
1. a The train was late. 
1. b However I managed to reach the wedding 
on time. 
 
In Example 1, the connecting word however binds 
with the situation of the train being late. 
Example 2 
1. a The cab driver was late. 
1. b The bike tyre was punctured.  
1. c The train was late. 
1 .d Finally, I managed to arrive at the wed-
ding on time by calling a cab. 
Example 3 
1. a The cab driver was late. 
1. b The bike tyre was punctured.  
1. c The train was late. 
1. d I could not wait any more; I finally ma-
naged to reach the wedding on time by calling a 
cab. 
 
In Example 2, the connecting word finally binds 
with the situation of him being delayed. Similarly, 
in Example 3, the connecting word finally, though 
it comes in the middle of the sentence, it still binds 
with the situation of him being delayed. 
2.1.2 Pronominals 
In this approach we have a set of pronominals 
which establishes coherence in the text. From our 
analysis, it was observed that if the pronominals 
?he, she, it, they, her, his, hers, its, their, theirs?, 
appear in the present sentence; its antecedent may 
be in the same or previous sentence.  
It is also found that if the pronominal is not pos-
sessive (i.e. the antecedent appears in the previous 
sentence or previous clause), then the present sen-
tence and the previous sentences are connected. 
However, if the pronominal is possessive then it 
behaves like reflexives such as ?himself?, ?herself? 
which has subject as its antecedent. Hence the pos-
sibility of connecting it with the previous sentence 
is very unlikely. Though pronominal resolution 
cannot be done at a window size of 2 alone, still 
we are looking at window size 2 alone to pick 
guaranteed connected sentences. 
 
Example 4 
1. a Ravi is a good boy. 
1. b He always speaks the truth. 
 
In Example 4, the pronominal he in the second sen-
tence refers to the antecedent Ravi in the first sen-
tence. 
 
Example 5 
1. a He is the one who got the first prize. 
 
In example 5 the pronominal he is possessive and 
it doesn?t need an antecedent to convey the mean-
ing. 
2.1.3 NERs Reappearance 
Two adjacent sentences are said to be coherent 
when both the sentences contain one or more reap-
pearing nouns. 
 
Example 6 
1. a Ravi is a good boy. 
1. b Ravi scored good marks in exams. 
 
Example 7 
1. a The car race starts at noon. 
1. b Any car is allowed to participate. 
 
Example 6 and Example 7 demonstrates the cohe-
rence between the two sentences through reappear-
ing nouns. 
2.1.4 Thesaurus Relationships 
WordNet covers most of the sense relationships. 
To find the semantic neighborhood between adja-
cent sentences, most of the lexical relationships 
such as synonyms, hyponyms, hypernyms, mero-
nyms, holonyms and gradation can be used (Fell-
baum 1998). Hence, semantically related terms are 
captured through this process.  
 
Example 8 
1. a The bicycle has two wheels. 
1. b The wheels provide speed and stability. 
 
In Example 8, bicycle and wheels are related 
through bicycle is the holonym of wheels. 
 
2.2 Coherence Finding Algorithm 
 
The algorithm is carried out in four phases. Initial-
ly, each of the 4 cohesion rules is individually ap-
plied over the given document to give coherent 
chunks. Next, the coherent chunks obtained in each 
55
phases are merged together to give the global cohe-
rent chunks in the document. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
. 
 
 
 
 
 
 
Figure 1: Flow of Coherence chunker 
 
Figure 1, shows the flow and rule positions in the 
coherence chunk identification module. 
 
2.3 Evaluation 
 
One way to evaluate the coherence finding algo-
rithm is to compare against human judgments 
made by readers, evaluating against text pre 
marked by authors and to see the improved result 
in the computational task. In this paper we will see 
the computational method to see the improved re-
sult. 
3 Text Ranking 
The proposed graph based text ranking algorithm 
consists of three steps: (1) Word Frequency Analy-
sis; (2) A word positional and string pattern based 
weight calculation algorithm; (3) Ranking the sen-
tences by normalizing the results of step (1) and 
(2).  
The algorithm is carried out in two phases. The 
weight metric obtained at the end of each phase is 
averaged to obtain the final weight metric. Sen-
tences are sorted in non ascending order of weight. 
3.1 Graph 
Let G (V, E) be a weighted undirected complete 
graph, where V is set of vertices and E is set of 
weighted edges.  
S1
S2
S3
S6
S5
S4  
 
Figure 2: A complete undirected graph 
 
In figure 2, the vertices in graph G represent the set 
of all sentences in the given document. Each sen-
tence in G is related to every other sentence 
through the set of weighted edges in the complete 
graph. 
3.2 Phase 1 
Let the set of all sentences in document S= {si | 1 ? 
i ? n}, where n is the number of sentences in S. 
The sentence weight (SW) for each sentence is cal-
culated by average affinity weight of words in it. 
For a sentence si= {wj | 1 ? j ? mi} where mi is the 
number of words in sentence si, (1 ? i ? n) the af-
finity weight AW of a word wj is calculated as fol-
lows:  
 
( , )
( )
( )
j k
k
j
IsEqual w w
w SAW w
WC S
? ?
=
?
           (1) 
where S is the set of all sentences in the given 
document, wk is a word in S, WC (S) is the total 
number of words in S and function IsEqual(x, y) 
returns an integer count 1 if x and y are equal else 
integer count 0 is returned by the function. 
Input Text 
Connecting Word 
Possessive Pronoun 
Noun Reappearance 
Coherent Chunks 
Thesaurus Relationships 
56
Next, we find the sentence weight SW (si) of 
each sentence si (1 ? i ? n) as follows: 
 
1
( ) ( )i j
i j i
SW s AW w
m w s
=
? ?
?                       (2) 
 
At the end of phase 1, the graph vertices hold 
the sentence weight as illustrated in figure 4.  
 
 
 
Figure 2: Sample text taken for the ranking 
process. 
 
 
 
Figure 4: Sample graph of Sentence weight calcu-
lation in phase 1 
3.3 Compression hash 
A fast compression hash function over word w is 
given as follows: 
 
H (w) = (c1ak-1+c2ak-2 +c3ak-3+...+cka0) mod p    (3) 
 
where w= {c1, c2, c3 ... ck} is the ordered set of 
ASCII equivalents of alphabets in w and k the total 
number of alphabets in w. The choice of a=2 per-
mits the exponentiations and term wise multiplica-
tions in equation 3 to be binary shift operations on 
a micro processor, thereby speeding up the hash 
computation over the text. Any lexicographically 
ordered bijective map from character to integer 
may be used to generate set w. The recommenda-
tion to use ASCII equivalents is solely for imple-
mentation convenience. Set p = 26 (for English), to 
cover the sample space of the set of alphabets un-
der consideration.  
Compute H (w) for each word in sentence si to 
obtain the hashed set  
 
1 2( ) { ( ), ( )... ( )}ii mH s H w H w H w=             (4) 
 
Next, invert each element in the set H (si) back 
to its ASCII equivalent to obtain the set 
 
1 2? ? ? ? ?( ) { ( ), ( )... ( )}ii mH s H c H c H c=                (5) 
    Then, concatenate the elements in set ? iH(s )  to 
obtain the string ?is ; where ?is  is the compressed 
representation of sentence si. The hash operations 
are carried out to reduce the computational com-
plexity in phase 2, by compressing the sentences 
and at the same time retaining their structural 
properties, specifically word frequency, word posi-
tion and sentence patterns.  
3.4 Levenshtein Distance 
Levenshtein distance (LD) between two strings 
string1 and string2 is a metric that is used to find 
the number of operations required to convert 
string1 to string2 or vice versa; where the set of 
possible operations on the character is insertion, 
deletion, or substitution. 
The LD algorithm is illustrated by the following 
example 
 
LD (ROLL, ROLE) is 1 
LD (SATURDAY, SUNDAY) is 3 
[1]"The whole show is dreadful," she cried, com-
ing out of the menagerie of M. Martin. 
[2]She had just been looking at that daring specu-
lator "working with his hyena" to speak in the 
style of the program. 
[3]"By what means," she continued, "can he have 
tamed these animals to such a point as to be cer-
tain of their affection for." 
[4]"What seems to you a problem," said I, inter-
rupting, "is really quite natural." 
[5]"Oh!" she cried, letting an incredulous smile 
wander over her lips. 
[6]"You think that beasts are wholly without pas-
sions?" Quite the reverse; we can communicate to 
them all the vices arising in our own state of civi-
lization. 
57
3.5 Levenshtein Similarity Weight 
Consider two strings, string1 and string2 where ls1 
is the length of string1 and ls2 be the length of 
string2. Compute MaxLen=maximum (ls1, ls2). 
Then LSW between string1 and string2 is the dif-
ference between MaxLen and LD, divided by Max-
Len. Clearly, LSW lies in the interval 0 to 1. In case 
of a perfect match between two words, its LSW is 1 
and in case of a total mismatch, its LSW is 0. In all 
other cases, 0 < LSW <1. The LSW metric is illu-
strated by the following example. 
LSW (ABC, ABC) =1 
LSW (ABC, XYZ) =0 
LSW (ABCD, EFD) =0.25 
 
Hence, to find the Levenshtein similarity 
weight, first find the Levenshtein distance LD us-
ing which LSW is calculated by the equation 
 
? ? ? ?( , ) ( , )? ?( , )
? ?( , )
i j i j
i j
i j
MaxLen s s LD s s
LSW s s
MaxLen s s
?
=       (6) 
where, ?is and j?s are the concatenated string out-
puts of equation 5. 
3.6 Phase 2 
Let S = {si | 1 ? i ? n} be the set of all sentences in 
the given document; where n is the number of sen-
tences in S. Further, si = {wj | 1 ? j ? m}, where m 
is the number of words in sentence si.  
 
 
Figure 5: Sample graph for Sentence weight calcu-
lation in phase 2 
 
is S ? ? ,find 1 2? ? ? ? ?( ) { ( ), ( )... ( )}ii mH s H c H c H c=  
using equation 3 and 4. Then, concatenate the ele-
ments in set ? iH(s )  to obtain the string ?is ; where ?is  
is the compressed representation of sentence si. 
Each string ?is ; 1 ? i ? n is represented as the 
vertex of the complete graph as in figure 5 
and ? i?S={s |1 i n}? ? . For the graph in figure 5, 
find the Levenshtein similarity weight LSW be-
tween every vertex using equation 6. Find vertex 
weight (VW) for each string i?s ; 1 ? l ? n by  
 
1? ? ?( ) ( , )
?? l?
l l i
i
VW s LSW s s
n
s s S
=
? ? ?
?            (7) 
4 Text Ranking 
The rank of sentence si; 1 ? i ? n is computed as 
 
?( ) ( )
( ) ;1
2
i i
i
SW s VW s
Rank s i n
+
= ? ?           (8) 
where, ( )iSW s  is calculated by equation 2 of 
phase 1 and ?( )iVW s  is found using equation 7 of 
phase 2. Arrange the sentences si; 1 ? i ? n, in non 
increasing order of their ranks. 
( )iSW s  in phase 1 holds the sentence affinity in 
terms of word frequency and is used to determine 
the significance of the sentence in the overall rak-
ing scheme. ?( )iVW s  in phase 2 helps in the overall 
ranking by determining largest common subse-
quences and other smaller subsequences then as-
signing weights to it using LSW. Further, since 
named entities are represented as strings, repeated 
occurrences are weighed efficiently by LSW, the-
reby giving it a relevant ranking position.  
5 Summarization 
Summarization is done by applying text ranking 
over the global coherent chunks in the document. 
The sentences whose weight is above the threshold 
is picked and rearranged in the order in which the 
sentences appeared in the original document. 
 
 
 
58
6 Evaluation 
The ROUGE evaluation toolkit is employed to 
evaluate the proposed algorithm. ROUGE, an au-
tomated summarization evaluation package based 
on Ngram statistics, is found to be highly corre-
lated with human evaluations (Lin and Hovy, 
2003a).  
The evaluations are reported in ROUGE-1 me-
trics, which seeks unigram matches between the 
generated and the reference summaries. The 
ROUGE-1 metric is found to have high correlation 
with human judgments at a 95% confidence level 
and hence used for evaluation. (Mihalcea and Ta-
rau, 2004) a graph based ranking model with 
Rouge score 0.4904, (Mihalcea, 2004) Graph-
based Ranking Algorithms for Sentence Extrac-
tion, Applied to Text Summarization with Rouge 
score 0.5023.  
Table 1 shows the ROUGE Score of 567 news 
articles provided during the Document Under-
standing Evaluations 2002(DUC, 2002) using the 
proposed algorithm without the inclusion of cohe-
rence chunker module. 
 
 
 
Table 2 shows the ROUGE Score of 567 news 
articles provided during the Document Under-
standing Evaluations 2002(DUC, 2002) using the 
proposed algorithm after the inclusion of cohe-
rence chunker module. 
 
 
Comparatively Table 2, which is the the 
ROUGE score for summary including the cohe-
rence chunker module gives better result. 
7 Related Work 
Text extraction is considered to be the important 
and foremost process in summarization. Intuitive-
ly, a hash based approach to graph based ranking 
algorithm for text ranking works well on the task 
of extractive summarization. A notable study re-
port on usefulness and limitations of automatic 
sentence extraction is reported in (Lin and Hovy, 
2003b), which emphasizes the need for efficient 
algorithms for sentence ranking and summariza-
tion.  
8 Conclusions 
In this paper, we propose a coherence chunker 
module and a hash based approach to graph based 
ranking algorithm for text ranking. In specific, we 
propose a novel approach for graph based text 
ranking, with improved results comparative to ex-
isting ranking algorithms. The architecture of the 
algorithm helps the ranking process to be done in a 
time efficient way. This approach succeeds in 
grabbing the coherent sentences based on the lin-
guistic and heuristic rules; whereas other super-
vised ranking systems do this process by training 
the summary collection. This makes the proposed 
algorithm highly portable to other domains and 
languages. 
References 
ACE Corpus. NIST 2008 Automatic Content Extraction 
Evaluation(ACE08). 
http://www.itl.nist.gov/iad/mig/tests/ace/2008/ 
Brin and L. Page. 1998. The anatomy of a large scale 
hypertextualWeb search engine. Computer Networks 
and ISDN Systems, 30 (1 ? 7).  
Erkan and D. Radev. 2004. Lexpagerank: Prestige in 
multi document text summarization. In Proceedings 
of the Conference on Empirical Methods in Natural 
Language Processing, Barcelona, Spain, July. 
Fellbaum, C., ed. WordNet: An electronic lexical data-
base. MIT Press, Cambridge (1998). 
Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604-
632. 
ROUGE-1 
ROUGE-L 
    0.5312 
    0.4978 
Score 
ROUGE-1 
ROUGE-L 
    0.5103 
    0.4863 
Score 
Table 1: ROUGE Score for the news article 
summarization task without coherence 
chunker, calculated across 567 articles. 
Table 2: ROUGE Score for the news article 
summarization task with coherence chunker, 
calculated across 567 articles. 
59
Lin and E.H. Hovy. From Single to Multi document 
Summarization: A Prototype System and its Evalua-
tion. In Proceedings of ACL-2002. 
Lin and E.H. Hovy. 2003a. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. In 
Proceedings of Human Language Technology Confe-
rence (HLT-NAACL 2003), Edmonton, Canada, May.  
Lin and E.H. Hovy. 2003b. The potential and limitations 
of sentence extraction for summarization. In Pro-
ceedings of the HLT/NAACL Workshop on Automatic 
Summarization, Edmonton, Canada, May. 
Mihalcea. 2004. Graph-based ranking algorithms for 
sentence extraction, applied to text summarization. In 
Proceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2004) 
(companion volume), Barcelona, Spain.  
Mihalcea and P. Tarau. 2004. TextRank - bringing order 
into texts. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), Barcelona, Spain. 
Mihalcea, P. Tarau, and E. Figa. 2004. PageRank on 
semantic networks, with application to word sense 
disambiguation. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics 
(COLING 2004), Geneva, Switzerland. 
Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., 
Miller, K. J. Introduction to WordNet: An on-line 
lexical database. Journal of Lexicography (1990). 
Morris, J., Hirst, G. Lexical cohesion computed by the-
saural relations as an indicator of the structure of 
text. Computational Linguistics (1991). 
Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based 
summarization of multiple documents. Information 
Processing and Management, 40: 919-938, 2004. 
Robert de Beaugrande and Wolfgang Dressler. Intro-
duction to Text Linguistics. Longman, 1981. 
Silber, H. G., McCoy, K. F. Efficient text summariza-
tion using lexical chains. In Proceedings of Intelli-
gent User Interfaces. (2000). 
60
