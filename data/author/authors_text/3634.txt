Extracting Important Sentences with Support Vector Machines
Tsutomu HIRAO and Hideki ISOZAKI and Eisaku MAEDA
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
{hirao,isozaki,maeda}@cslab.kecl.ntt.co.jp
Yuji MATSUMOTO
Graduate School of Information and Science, Nara Institute of Science and Technology
8516-9, Takayama, Ikoma, Nara 630-0101 Japan
matsu@is.aist-nara.ac.jp
Abstract
Extracting sentences that contain important in-
formation from a document is a form of text
summarization. The technique is the key to the
automatic generation of summaries similar to
those written by humans. To achieve such ex-
traction, it is important to be able to integrate
heterogeneous pieces of information. One ap-
proach, parameter tuning by machine learning,
has been attracting a lot of attention. This pa-
per proposes a method of sentence extraction
based on Support Vector Machines (SVMs). To
confirm the method?s performance, we conduct
experiments that compare our method to three
existing methods. Results on the Text Summa-
rization Challenge (TSC) corpus show that our
method offers the highest accuracy. Moreover,
we clarify the different features effective for ex-
tracting different document genres.
1 Introduction
Extracting important sentences means extract-
ing from a document only those sentences that
have important information. Since some sen-
tences are lost, the result may lack coherence,
but important sentence extraction is one of
the basic technologies for generating summaries
that are useful for humans to browse. There-
fore, this technique plays an important role in
automatic text summarization.
Many researchers have been studied impor-
tant sentence extraction since the late 1950?s
(Luhn, 1958). Conventional methods focus on
sentence features and define significance scores.
The features include key words, sentence posi-
tion, and certain linguistic clues. Edmundson
(1969) and Nobata et al (2001) have proposed
scoring functions to integrate heterogeneous fea-
tures. However, we can not tune the parameter
values by hand when the number of features is
large.
When a large quantity of training data is
available, tuning can be effectively realized by
machine learning. In recent years, machine
learning has attracted attention in the field of
automatic text summarization. Aone et al
(1998) and Kupiec et al (1995) employed
Bayesian classifiers, Mani et al (1998), Nomoto
et al (1997), Lin (1999), and Okumura et
al. (1999) used decision tree learning. How-
ever, most machine learning methods overfit the
training data when many features are given.
Therefore, we need to select features carefully.
Support Vector Machines (SVMs) (Vapnik,
1995) is robust even when the number of
features is large. Therefore, SVMs have
shown good performance for text categoriza-
tion (Joachims, 1998), chunking (Kudo and
Matsumoto, 2001), and dependency structure
analysis (Kudo and Matsumoto, 2000).
In this paper, we present an important sen-
tence extraction technique based on SVMs. We
verified the technique against the Text Summa-
rization Challenge (TSC) (Fukushima and Oku-
mura, 2001) corpus.
2 Important Sentence Extraction
based on Support Vector Machines
2.1 Support Vector Machines (SVMs)
SVM is a supervised learning algorithm for 2-
class problems.
Training data is given by
(x
1
, y
1
), ? ? ? , (xu, yu), xj ? Rn, yj ? {+1,?1}.
Here, x
j
is a feature vector of the j-th
sample; y
j
is its class label, positive(+1) or
negative(?1). SVM separates positive and neg-
ative examples by a hyperplane defined by
w ? x + b = 0, w ? Rn, b ? R, (1)
Positive
Negative
margin
wx + b = 0
wx + b = 1
wx + b = -1
Support Vector
Figure 1: Support Vector Machines.
where ??? represents the inner product.
In general, such a hyperplane is not unique.
Figure 1 shows a linearly separable case. The
SVM determines the optimal hyperplane by
maximizing the margin. A margin is the dis-
tance between negative examples and positive
examples.
Since training data is not necessarily linearly
separable, slack variables (?
j
) are introduced for
all x
j
. These ?
j
incur misclassification error,
and should satisfy the following inequalities:
w ? xj + b ? 1? ?j
w ? xj + b ? ?1 + ?j . (2)
Under these constraints, the following objective
function is to be minimized.
1
2
||w||2 + C
u
?
j=1
?j . (3)
The first term in (3) corresponds to the size
of the margin and the second term represents
misclassification.
By solving a quadratic programming prob-
lem, the decision function f(x) = sgn(g(x)) can
be derived where
g(x) =
( 
?
i=1
?iyixi ? x+ b
)
. (4)
The decision function depends on only sup-
port vectors (x
i
). Training examples, except
for support vectors, have no influence on the
decision function.
Non-linear decision surfaces can be realized
by replacing the inner product of (4) with a ker-
nel function K(x ? x
i
) :
g(x) =
( 
?
i=1
?iyiK(xi,x) + b
)
. (5)
In this paper, we use polynomial kernel func-
tions that have been very effective when applied
to other tasks, such as natural language pro-
cessing (Joachims, 1998; Kudo and Matsumoto,
2001; Kudo and Matsumoto, 2000):
K(x,y) = (x ? y + 1)d. (6)
2.2 Sentence Ranking by using Support
Vector Machines
Important sentence extraction can be regarded
as a two-class problem: important or unimpor-
tant. However, the proportion of important sen-
tences in training data will differ from that in
the test data. The number of important sen-
tences in a document is determined by a sum-
marization rate that is given at run-time. A
simple solution for this problem is to rank sen-
tences in a document. We use g(x) the distance
from the hyperplane to x to rank the sentences.
2.3 Features
We define the boolean features discussed below
that are associated with sentence S
i
by taking
past studies into account (Zechner, 1996; No-
bata et al, 2001; Hirao et al, 2001; Nomoto
and Matsumoto, 1997).
We use 410 boolean variables for each S
i
.
Where x = (x[1], ? ? ? , x[410]). A real-valued fea-
ture normalized between 0 and 1 is represented
by 10 boolean variables. Each variable corre-
sponds to an internal [i/10,(i + 1)/10) where
i = 0 to 9. For example, Posd = 0.75 is rep-
resented by ?0000000100? because 0.75 belongs
to [7/10,8/10).
Position of sentences
We define three feature functions for the posi-
tion of S
i
. First, Lead is a boolean that corre-
sponds to the output of the lead-based method
described below1 . Second, Posd is S
i
?s position
in a document. Third, Posp is S
i
?s position in a
paragraph. The first sentence obtains the high-
est score, the last obtains the lowest score:
1 When a sentence appears in the first N of document,
we assign 1 to the sentence. An N was given for each
document by TSC committee.
Posd(Si) = 1?BD(Si)/|D(Si)|
Posp(Si) = 1?BP (Si)/|P (Si)|.
Here, |D(S
i
)| is the number of characters in
the document D(S
i
) that contains S
i
; BD(S
i
)
is the number of characters before S
i
in D(S
i
);
|P (S
i
)| is the number of characters of the para-
graph P (S
i
) that contains S
i
, and BP (S
i
) is
the number of characters before S
i
in the para-
graph.
Length of sentences
We define a feature function that addresses the
length of sentence as
Len(Si) = |Si|/ max
S
z
?D(S
i
)
|Sz|.
Here, |S
i
| is the number of characters of sen-
tence S
i
, and max
S
z
?D
|S
z
| is the maximum
number of characters in a sentence that belongs
to D(S
i
).
In addition, the length of a previous sentence
Len
?1(Si) = Len(Si?1) and the length of a next
sentence Len+1(Si) = Len(Si+1) are also fea-
tures of sentence S
i
.
Weight of sentences
We defined the feature function that weights
sentences based on frequency-based word
weighting as
Wf (Si) =
?
t
tf(t, Si) ? w(t, D(Si)).
Here, Wf(S
i
) is the summention of weighting
w(t, D(S
i
)) of words that appear in a sentence.
tf(t, S
i
) is term frequency of t in S
i
. We used
only nouns. In addition, we define word weight
w(t, D(S
i
)) based on a specific field (Hara et al,
1997):
w(t, D(Si)) = ?
(
1
T
T
?
z=1
?z
Vz
)
+?
(
tf(t, D(Si))
?
t? tf(t
?, D(Si))
)
.
Here, T is the number of sentence in a docu-
ment, and V
z
is the number of words in sentence
S
z
? D(S
i
) (repetitions are ignored). Also, ?
z
is
a boolean value: that is 1 when t appears inS
z
.
The first term of the equation above is the
weighting of a word in a specific field. The sec-
ond term is the occurrence probability of word
t.
We set parameters ? and ? as 0.8, 0.2, re-
spectively. The weight of a previous sentence
Wf
?1(Si)=Wf (Si?1), and the weight of a next
sentence Wf +1(Si)=Wf (Si+1) are also features
of sentence S
i
.
Density of key words
We define the feature function Den(S
i
) that
represents density of key words in a sentence
by using Hanning Window function (f
H
(k, m)):
Den(Si) = maxm
m+Win/2
?
k=m?Win/2
fH(k,m) ? a(k, Si),
where f
H
(k, m) is given by
f
H
(k,m) =
{
1
2
(
1 + cos2? k?m
W in
)
(|k ?m| ? Win/2)
0 (|k ?m| > Win/2).
The key words (KW ) are the top 30% of
words in a document according to w(t, D(S
i
)).
Also, m is the center position of the window,
Win = |S
i
|/2. In addition, a(k, S
i
) is defined as
follows:
a(k, S
i
) =
?
?
?
?
?
w(t,D) Where a word t (? KW ) begins
at k
0 k is not the beginning position
of a word in KW.
Named Entities
x[r]=1 (1?r?8) indicates that a certain Named
Entity class appears in S
i
. The number of
Named Entity classes is 8 (Sekine and Eriguchi,
2000), e.g., PERSON, LOCATION. We use
Isozaki?s NE recognizer (Isozaki, 2001).
Conjunctions
x[r]=1 (9?r?61) if and only if a certain con-
junction is used in the sentence. The number of
conjunctions is 53.
Functional words
x[r]=1 (62?r?234) if and only if a certain func-
tional word such as ga, ha, and ta is used in
the sentence. The number of functional words
is 173.
Part of speech
x[r]=1 (235?r?300) if and only if a certain part
of speech such as ?Noun-jiritsu? and ?Verb-
jiritsu? is used in the sentence. The number
of part of speech is 66.
Semantical depth of nouns
x[r]=1 (301?r?311) if and only if S
i
contains
a noun at a certain semantical depth according
to a Japanese lexicon, Goi-Taikei (Ikehara et al,
1997). The number of depth levels is 11. For
instance, Semdep=2 means that a noun in S
i
belongs to the second depth level.
Document genre
x[r]=1 (312?r?315) if and only if the docu-
ment belongs to a certain genre. The genre is
explicitly written in the header of each docu-
ment. The number of genres is four: General,
National, Editorial, and Commentary.
Symbols
x[r]=1 (r=316) if and only if sentence includes
a certain symbol (for example: ?,&,
).
Conversation
x[r]=1 (r=317) if and only if S
i
includes a con-
versation style expression.
Assertive expressions
x[r]=1 (r=318) if and only if S
i
includes an as-
sertive expression.
3 Experimental settings
3.1 Corpus
We used the data set of TSC (Fukushima and
Okumura, 2001) summarization collection for
our evaluation. TSC was established as a sub-
task of NTCIR-2 (NII-NACSIS Test Collection
for IR Systems). The corpus consists of 180
Japanese documents2 from the Mainichi News-
papers of 1994, 1995, and 1998. In each doc-
ument, important sentences were manually ex-
tracted at summarization rates of 10%, 30%,
and 50%. Note that the summarization rates
depend on the number of sentences in a doc-
ument not the number of characters. Table 1
shows the statistics.
3.2 Evaluated methods
We compared four methods: decision tree learn-
ing, boosting, lead, and SVM. At each summa-
rization rate, we trained classifiers and classified
test documents.
Decision tree learning method
We used C4.5 (Quinlan, 1993) for our experi-
ments with the default settings. We used the
2 Each document is presented in SGML style with sen-
tence and paragraph separators attached.
features described in section 2. Sentences were
ranked according to their certainty factors given
by C4.5.
Boosting method
We used C5.0, which applies boosting to deci-
sion tree learning. The number of rounds was
set to 10. Sentences were ranked according to
their certainty factors given by C5.0.
Lead-based method
The first N sentences of a document were se-
lected. N was determined according to the sum-
marization rates.
SVM method
This is our method as outlined in section 2. We
used the second-order polynomial kernel, and
set C (in equation (3)) as 0.0001. We used
TinySVM3 .
3.3 Measures for evaluation
In the TSC corpus, the number of sentences to
be extracted was explicitly given by the TSC
committee. When we extract sentences accord-
ing to that number, Precision, Recall, and F-
measure become the same value. We call this
value Accuracy. Accuracy is defined as follows:
Accuracy = b/a ? 100,
where a is the specified number of important
sentences, and b is the number of true impor-
tant sentences that were contained in system?s
output.
4 Results
Table 2 shows the results of five-fold cross vali-
dation by using all 180 documents.
For all summarization rates and all genres,
SVM achieved the highest accuracy, the lead-
based method the lowest. Let the null hypoth-
esis be ?There are no differences among the
scores of the four methods?. We tested this null
hypothesis at a significance level of 1% by using
Tukey?s method. Although the SVM?s perfor-
mance was best, the differences were not sta-
tistically significant at 10%. At 30% and 50%,
SVM performed better than the other methods
with a statistical significance.
3 http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/
Table 1: Details of data sets.
General National Editorial Commentary
# of documents 16 76 41 47
# of sentences 342 1721 1362 1096
# of important sentences (10%) 34 172 143 112
# of important sentences (30%) 103 523 414 330
# of important sentences (50%) 174 899 693 555
Table 2: Evaluation results of cross validation.
Summarization rate 10%
Genre SVM C4.5 C5.0 Lead
General 55.7 55.2 52.4 47.9
Editorial 34.2 33.6 27.9 31.6
National 61.4 52.0 56.3 51.8
Commentary 28.7 27.4 21.4 15.9
Average 46.2 41.4 40.4 37.4
Summarization rate 30%
Genre SVM C4.5 C5.0 Lead
General 51.0 45.7 50.4 50.5
Editorial 47.8 41.6 43.3 36.7
National 55.9 44.1 49.3 54.3
Commentary 48.7 39.4 40.1 32.4
Average 51.6 42.4 45.7 44.2
Summarization rate 50%
Genre SVM C4.5 C5.0 Lead
General 65.2 63.0 60.2 60.4
Editorial 60.6 54.1 54.6 51.0
National 63.3 58.7 58.7 61.5
Commentary 65.7 59.6 60.6 50.4
Average 63.5 58.2 58.4 56.1
5 Discussion
Table 2 shows that Editorial and Commentary
are more difficult than the other genres. We
can consider two reasons for the poor scores of
Editorial and Commentary:
? These genres have no feature useful for dis-
crimination.
? Non-standard features are useful in these
genres.
Accordingly, we conduct an experiment to
clarify genre dependency4 .
4 We did not use General because the number of doc-
uments in this genre was insufficient.
1 Extract 36 documents at random from
genre i for training.
2 Extract 4 documents at random from genre
j for test.
3 Repeat this 10 times for all combinations
of (i, j).
Table 3 shows that the result implies that
non-standard features are useful in Editorial
and Commentary documents.
Now, we examine effective features in each
genre. Since we used the second order polyno-
mial kernel, we can expand g(x) as follows:
g(x) = b+

?
i=1
wi + 2

?
i=1
wi
u
?
k=1
xi[k]x[k] +

?
i=1
wi
u
?
h=1
u
?
k=1
xi[h]xi[k]x[h]x[k ], (7)
where ) is the number of support vectors, and
w
i
equals ?
i
y
i
.
We can rewrite it as follows when all vectors
are boolean:
g(x) = W
0
+
u
?
k=1
W
1
[k]x[k] +
u?1
?
h=1
u
?
k=h+1
W
2
[k, h]x[h]x[k] (8)
where
W0 = b +
?

i=1 wi,W1[k] = 3
?

i=1 wixi[k], and
W2[h, k] = 2
?

i=1 wixi[h]xi[k].
Therefore, W1[k] indicates the significance of
an individual feature and W2[h, k] indicates the
significance of a feature pair. When |W1[k]| or
|W2[h, k]| was large, the feature or the feature
pair had a strong influence on the optimal hy-
perplane.
Table 3: Evaluation results for three genres.
Training \ Test
National Editorial Commentary
10% 30% 50% 10% 30% 50% 10% 30% 50%
National 63.4 57.6 65.5 32.8 39.4 53.6 24.0 39.5 60.8
Editorial 49.3 46.8 58.4 33.9 49.1 64.4 24.9 43.6 62.1
Commentary 37.4 43.3 61.1 18.4 41.8 57.8 30.6 49.6 67.0
Table 4: Effective features and their pairs
Summarization rate 10%
National Editorial Commentary
Lead ? ga 0.9?Posd?1.0 ? 0.7?Wf<0.8 0.9?P osd?1.0 ? Semdep=2
0.9?Posd?1.0 ? ga NE ? de 0.5?Len
+1
<0.6 ? Noun-hijiritsu
Lead ? ta 0.9?Posd?1.0 ? de 0.0?P osp<0.1 ? 0.5?Wf
+1
<0.6
0.9?Posd?1.0 ? ta Lead ? 0.7?Wf<0.8 0.8?P osd<0.9 ? Particle
Summarization rate 30%
National Editorial Commentary
Lead ? Semdep=6 0.0?Posp<0.1 ? ga Aux verb ? Semdep=2
0.9?Posd?1.0 ? Semdep=6 0.9?Posd?1.0 ? NE Verb-jiritsu ? Semdep=2
Lead ? ga Lead ? NE Semdep=2
0.9?Posd?1.0 0.0?P osd<0.1 0.0?Posp<0.1 ? 0.5?Den<0.6
Summarization rate 50%
National Editorial Commentary
Lead 0.0?Posp<0.1 ? Semdep=6 0.0?Posp<0.1 ? Particle
Lead ? ha 0.0?Posp<0.1 ? ga 0.2?P osd<0.3
Lead ? Verb-jiritsu 0.0?Posp<0.1 0.4?Len<0.5
Lead ? ta 0.0?P osd<0.1 0.0?Posp<0.1
Table 4 shows some of the effective features
that had large weights W1[k], W2[h, k] for each
genre.
Effective features common to three genres at
three rates were sentence positions. Since Na-
tional has a typical newspaper style, the begin-
ning of the document was important. More-
over, ?ga? and ?ta? were important. These
functional words are used when a new event is
introduced.
In Editorial and Commentary, the end of a
paragraph and that of a document were impor-
tant. The reason for this result is that subtopic
or main topic conclusions are common in those
positions. This implies that National has a dif-
ferent text structure from Editorial and Com-
mentary.
Moreover, in Editorial, ?de? and sentence
weight was important. In Commentary, seman-
tically shallow words, sentence weight and the
length of a next sentence were important.
In short, we confirmed that the feature(s) ef-
fective for discriminating a genre differ with the
genre.
6 Conclusion
This paper presented a SVM-based important
sentence extraction technique. Comparisons
were made using the lead-based method, deci-
sion tree learning method, and boosting method
with the summarization rates of 10%, 30%,
and 50%. The experimental results show that
the SVM-based method outperforms the other
methods at all summarization rates. Moreover,
we clarified the effective features for three gen-
res, and showed that the important features
vary with the genre.
In our future work, we would like to apply our
method to trainable Question Answering Sys-
tem SAIQA-II developed in our group.
Acknowledgement
We would like to thank all the members of the
Knowledge Processing Research Group for valu-
able comments and discussions.
References
C. Aone, M. Okurowski, and J. Gorlinsky. 1998.
Trainable Scalable Summarization Using Ro-
bust NLP and Machine Learning. Proc. of the
17th COLING and 36th ACL, pages 62?66.
H. Edmundson. 1969. New methods in
automatic abstracting. Journal of ACM,
16(2):246?285.
T. Fukushima and M. Okumura. 2001. Text
Summarization Challenge Text summariza-
tion evaluation in Japan. Proc. of the
NAACL2001 Workshop on Automatic sum-
marization, pages 51?59.
M. Hara, H. Nakajima, and T. Kitani. 1997.
Keyword Extraction Using a Text Format
and Word Importance in a Specific Filed (in
Japanese). Transactions of Information Pro-
cessing Society of Japan, 38(2):299?309.
T. Hirao, M. Hatayama, S. Yamada, and
K. Takeuchi. 2001. Text Summarization
based on Hanning Window and Dependency
structure analysis. Proc. of the 2nd NTCIR
Workshop, pages 349?354.
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,
H. Nakaiwa, K. Ogura, Y. Ooyama, and
Y. Hayashi. 1997. Goi-Taikei ? A Japanese
Lexicon (in Japanese). Iwanami Shoten.
H. Isozaki. 2001. Japanese Named Entity
Recognition based on Simple Rule Generator
and Decision Tree Learning. Proc. of the 39th
ACL, pages 306?313.
T. Joachims. 1998. Text Categorization with
Support Vector Machines: Learning with
Many Relevant Features. Proc. of ECML,
pages 137?142.
T. Kudo and Y. Matsumoto. 2000. Japane De-
pendency Structure Analysis Based on Su-
port Vector Machines. Proc. of EMNLP and
VLC, pages 18?25.
T. Kudo and Y. Matsumoto. 2001. Chunking
with Support Vector Machine. Proc. of the
2nd NAACL, pages 192?199.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A
Trainable Document Summarizer. Proc. of
the 18th ACM-SIGIR, pages 68?73.
Chin-Yew Lin. 1999. Training a Selection Func-
tion for Extraction. Proc. of the 18th ACM-
CIKM, pages 55?62.
H. Luhn. 1958. The Automatic Creation of Lit-
erature Abstracts. IBM Journal of Research
and Development, 2(2):159?165.
I. Mani and E. Bloedorn. 1998. Machine Learn-
ing of Generic and User-Focused Summariza-
tion. Proc. of the 15th AAAI, pages 821?826.
C. Nobata, S. Sekine, M. Murata, K. Uchimoto,
M. Utiyama, and H. Isahara. 2001. Sentence
Extraction System Assembling Multiple Ev-
idence. Proc. of the 2nd NTCIR Workshop,
pages 319?324.
T. Nomoto and Y. Matsumoto. 1997. The Reli-
ability of Human Coding and Effects on Auto-
matic Abstracting (in Japanese). The Special
Interest Group Notes of IPSJ (NL-120-11),
pages 71?76.
M. Okumura, Y. Haraguchi, and H. Mochizuki.
1999. Some Observations on Automatic
Text Summarization Based on Decision Tree
Learning (in Japanese). Proc. of the 59th Na-
tional Convention of IPSJ (5N-2), pages 393?
394.
J. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
S. Sekine and Y. Eriguchi. 2000. Japanese
Named Entity Extraction Evaluation - Anal-
ysis of Results -. Proc. of the 18th COLING,
pages 1106?1110.
V. Vapnik. 1995. The Nature of Statistical
Learning Theory. New York.
K. Zechner. 1996. Fast Generation of Abstracts
from General Domain Text Corpora by Ex-
tracting Relevant Sentences. Proc. of the 16th
COLING, pages 986?989.
Efficient Support Vector Classifiers
for Named Entity Recognition
Hideki Isozaki and Hideto Kazawa
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
 
isozaki,kazawa  @cslab.kecl.ntt.co.jp
Abstract
Named Entity (NE) recognition is a task in which
proper nouns and numerical information are ex-
tracted from documents and are classified into cat-
egories such as person, organization, and date. It
is a key technology of Information Extraction and
Open-Domain Question Answering. First, we show
that an NE recognizer based on Support Vector Ma-
chines (SVMs) gives better scores than conventional
systems. However, off-the-shelf SVM classifiers are
too inefficient for this task. Therefore, we present a
method that makes the system substantially faster.
This approach can also be applied to other simi-
lar tasks such as chunking and part-of-speech tag-
ging. We also present an SVM-based feature selec-
tion method and an efficient training method.
1 Introduction
Named Entity (NE) recognition is a task in which
proper nouns and numerical information in a docu-
ment are detected and classified into categories such
as person, organization, and date. It is a key technol-
ogy of Information Extraction and Open-Domain
Question Answering (Voorhees and Harman, 2000).
We are building a trainable Open-Domain Question
Answering System called SAIQA-II. In this paper,
we show that an NE recognizer based on Support
Vector Machines (SVMs) gives better scores than
conventional systems. SVMs have given high per-
formance in various classification tasks (Joachims,
1998; Kudo and Matsumoto, 2001).
However, it turned out that off-the-shelf SVM
classifiers are too inefficient for NE recognition.
The recognizer runs at a rate of only 85 bytes/sec
on an Athlon 1.3 GHz Linux PC, while rule-based
systems (e.g., Isozaki, (2001)) can process several
kilobytes in a second. The major reason is the
inefficiency of SVM classifiers. There are other
reports on the slowness of SVM classifiers. An-
other SVM-based NE recognizer (Yamada and Mat-
sumoto, 2001) is 0.8 sentences/sec on a Pentium III
933 MHz PC. An SVM-based part-of-speech (POS)
tagger (Nakagawa et al, 2001) is 20 tokens/sec on
an Alpha 21164A 500 MHz processor. It is difficult
to use such slow systems in practical applications.
In this paper, we present a method that makes the
NE system substantially faster. This method can
also be applied to other tasks in natural language
processing such as chunking and POS tagging. An-
other problem with SVMs is its incomprehensibil-
ity. It is not clear which features are important or
how they work. The above method is also useful for
finding useless features. We also mention a method
to reduce training time.
1.1 Support Vector Machines
Suppose we have a set of training data for a two-
class problem: 
	
	 , where 
A Deterministic Word Dependency Analyzer
Enhanced With Preference Learning
Hideki Isozaki and Hideto Kazawa and Tsutomu Hirao
NTT Communication Science Laboratories
NTT Corporation
2-4 Hikaridai, Seikacho, Sourakugun, Kyoto, 619-0237 Japan
{isozaki,kazawa,hirao}@cslab.kecl.ntt.co.jp
Abstract
Word dependency is important in parsing tech-
nology. Some applications such as Informa-
tion Extraction from biological documents ben-
efit from word dependency analysis even with-
out phrase labels. Therefore, we expect an ac-
curate dependency analyzer trainable without
using phrase labels is useful. Although such
an English word dependency analyzer was pro-
posed by Yamada and Matsumoto, its accu-
racy is lower than state-of-the-art phrase struc-
ture parsers because of the lack of top-down in-
formation given by phrase labels. This paper
shows that the dependency analyzer can be im-
proved by introducing a Root-Node Finder and
a Prepositional-Phrase Attachment Resolver.
Experimental results show that these modules
based on Preference Learning give better scores
than Collins? Model 3 parser for these subprob-
lems. We expect this method is also applicable
to phrase structure parsers.
1 Introduction
1.1 Dependency Analysis
Word dependency is important in parsing technol-
ogy. Figure 1 shows a word dependency tree. Eis-
ner (1996) proposed probabilistic models of depen-
dency parsing. Collins (1999) used dependency
analysis for phrase structure parsing. It is also stud-
ied by other researchers (Sleator and Temperley,
1991; Hockenmaier and Steedman, 2002). How-
ever, statistical dependency analysis of English sen-
tences without phrase labels is not studied very
much while phrase structure parsing is intensively
studied. Recent studies show that Information Ex-
traction (IE) and Question Answering (QA) benefit
from word dependency analysis without phrase la-
bels. (Suzuki et al, 2003; Sudo et al, 2003)
Recently, Yamada and Matsumoto (2003) pro-
posed a trainable English word dependency ana-
lyzer based on Support Vector Machines (SVM).
They did not use phrase labels by considering an-
notation of documents in expert domains. SVM
(Vapnik, 1995) has shown good performance in dif-
He
a
girl
a
telescope
with
saw
He saw a girl with a telescope.
Figure 1: A word dependency tree
ferent tasks of Natural Language Processing (Kudo
and Matsumoto, 2001; Isozaki and Kazawa, 2002).
Most machine learning methods do not work well
when the number of given features (dimensionality)
is large, but SVM is relatively robust. In Natural
Language Processing, we use tens of thousands of
words as features. Therefore, SVM often gives good
performance.
However, the accuracy of Yamada?s analyzer is
lower than state-of-the-art phrase structure parsers
such as Charniak?s Maximum-Entropy-Inspired
Parser (MEIP) (Charniak, 2000) and Collins? Model
3 parser. One reason is the lack of top-down infor-
mation that is available in phrase structure parsers.
In this paper, we show that the accuracy of the
word dependency parser can be improved by adding
a base-NP chunker, a Root-Node Finder, and a
Prepositional Phrase (PP) Attachment Resolver. We
introduce the base-NP chunker because base NPs
are important components of a sentence and can be
easily annotated. Since most words are contained
in a base NP or are adjacent to a base NP, we ex-
pect that the introduction of base NPs will improve
accuracy.
We introduce the Root-Node Finder because Ya-
mada?s root accuracy is not very good. Each sen-
tence has a root node (word) that does not modify
any other words and is modified by all other words
directly or indirectly. Here, the root accuracy is de-
fined as follows.
Root Accuracy (RA) =
#correct root nodes / #sentences (= 2,416)
We think that the root node is also useful for depen-
dency analysis because it gives global information
to each word in the sentence.
Root node finding can be solved by various ma-
chine learning methods. If we use classifiers, how-
ever, two or more words in a sentence can be classi-
fied as root nodes, and sometimes none of the words
in a sentence is classified as a root node. Practically,
this problem is solved by getting a kind of confi-
dence measure from the classifier. As for SVM,
f(x) defined below is used as a confidence measure.
However, f(x) is not necessarily a good confidence
measure.
Therefore, we use Preference Learning proposed
by Herbrich et al (1998) and extended by Joachims
(2002). In this framework, a learning system is
trained with samples such as ?A is preferable to
B? and ?C is preferable to D.? Then, the system
generalizes the preference relation, and determines
whether ?X is preferable to Y? for unseen X and
Y. This framework seems better than SVM to select
best things.
On the other hand, it is well known that attach-
ment ambiguity of PP is a major problem in parsing.
Therefore, we introduce a PP-Attachment Resolver.
The next sentence has two interpretations.
He saw a girl with a telescope.
1) The preposition ?with? modifies ?saw.? That is, he
has the telescope. 2) ?With? modifies ?girl.? That is,
she has the telescope.
Suppose 1) is the correct interpretation. Then,
?with modifies saw? is preferred to ?with mod-
ifies girl.? Therefore, we can use Preference
Learning again.
Theoretically, it is possible to build a new De-
pendency Analyzer by fully exploiting Preference
Learning, but we do not because its training takes
too long.
1.2 SVM and Preference Learning
Preference Learning is a simple modification of
SVM. Each training example for SVM is a pair
(yi, xi), where xi is a vector, yi = +1 means that
xi is a positive example, and yi = ?1 means that xi
is a negative example. SVM classifies a given test
vector x by using a decision function
f(x) = wf ? ?(x) + b =
?`
i
yi?iK(x, xi) + b,
where {?i} and b are constants and ` is the number
of training examples. K(xi, xj) = ?(xi) ? ?(xj) is
a predefined kernel function. ?(x) is a function that
maps a vector x into a higher dimensional space.
Training of SVM corresponds to the follow-
ing quadratic maximization (Cristianini and Shawe-
Taylor, 2000)
W (?) =
?`
i=1
?i ?
1
2
?`
i,j=1
?i?jyiyjK(xi, xj),
where 0 ? ?i ? C and
?`
i=1 ?iyi = 0. C is a soft
margin parameter that penalizes misclassification.
On the other hand, each training example
for Preference Learning is given by a triplet
(yi, xi.1, xi.2), where xi.1 and xi.2 are vectors. We
use xi.? to represent the pair (xi.1, xi.2). yi = +1
means that xi.1 is preferable to xi.2. We can regard
their difference ?(xi.1) ? ?(xi.2) as a positive ex-
ample and ?(xi.2) ? ?(xi.1) as a negative example.
Symmetrically, yi = ?1 means that xi.2 is prefer-
able to xi.1.
Preference of a vector x is given by
g(x) = wg??(x) =
?`
i
yi?i(K(xi.1, x)?K(xi.2, x)).
If g(x) > g(x?) holds, x is preferable to x?. Since
Preference Learning uses the difference ?(xi.1) ?
?(xi.2) instead of SVM?s ?(xi), it corresponds to
the following maximization.
W (?) =
?`
i=1
?i ?
1
2
?`
i,j=1
?i?jyiyjK(xi.?, xj.?)
where 0 ? ?i ? C and K(xi.?, xj.?) =
K(xi.1, xj.1) ? K(xi.1, xj.2) ? K(xi.2, xj.1) +
K(xi.2, xj.2). The above linear constraint
?`
i=1 ?iyi = 0 for SVM is not applied to
Preference Learning because SVM requires this
constraint for the optimal b, but there is no b in g(x).
Although SVMlight (Joachims, 1999) provides an
implementation of Preference Learning, we use our
own implementation because the current SVMlight
implementation does not support non-linear kernels
and our implementation is more efficient.
Herbrich?s Support Vector Ordinal Regression
(Herbrich et al, 2000) is based on Preference Learn-
ing, but it solves an ordered multiclass problem.
Preference Learning does not assume any classes.
2 Methodology
Instead of building a word dependency corpus from
scratch, we use the standard data set for comparison.
Dependency Analyzer? PP-Attachment Resolver?
Root-Node Finder?
Base NP Chunker?
(POS Tagger)
?
= SVM, ? = Preference Learning
Figure 2: Module layers in the system
That is, we use Penn Treebank?s Wall Street Journal
data (Marcus et al, 1993). Sections 02 through 21
are used as training data (about 40,000 sentences)
and section 23 is used as test data (2,416 sentences).
We converted them to word dependency data by us-
ing Collins? head rules (Collins, 1999).
The proposed method uses the following proce-
dures.
? A base NP chunker: We implemented an
SVM-based base NP chunker, which is a sim-
plified version of Kudo?s method (Kudo and
Matsumoto, 2001). We use the ?one vs. all
others? backward parsing method based on an
?IOB2? chunking scheme. By the chunking,
each word is tagged as
? B: Beginning of a base NP,
? I: Other elements of a base NP.
? O: Otherwise.
Please see Kudo?s paper for more details.
? A Root-Node Finder (RNF): We will describe
this later.
? A Dependency Analyzer: It works just like Ya-
mada?s Dependency Analyzer.
? A PP-Attatchment Resolver (PPAR): This re-
solver improves the dependency accuracy of
prepositions whose part-of-speech tags are IN
or TO.
The above procedures require a part-of-speech
tagger. Here, we extract part-of-speech tags from
the Collins parser?s output (Collins, 1997) for sec-
tion 23 instead of reinventing a tagger. According
to the document, it is the output of Ratnaparkhi?s
tagger (Ratnaparkhi, 1996). Figure 2 shows the ar-
chitecture of the system. PPAR?s output is used to
rewrite the output of the Dependency Analyzer.
2.1 Finding root nodes
When we use SVM, we regard root-node finding as
a classification task: Root nodes are positive exam-
ples and other words are negative examples.
For this classification, each word wi in a tagged
sentence T = (w1/p1, . . . , wi/pi, . . . , wN/pN ) is
characterized by a set of features. Since the given
POS tags are sometimes too specific, we introduce
a rough part-of-speech qi defined as follows.
? q = N if p = NN, NNP, NNS,
NNPS, PRP, PRP$, POS.
? q = V if p = VBD, VB, VBZ, VBP,
VBN.
? q = J if p = JJ, JJR, JJS.
Then, each word is characterized by the following
features, and is encoded by a set of boolean vari-
ables.
? The word itself wi, its POS tags pi and
qi, and its base NP tag bi = B, I,O.
We introduce boolean variables such as
current word is John and cur-
rent rough POS is J for each of these
features.
? Previous word wi?1 and its tags, pi?1, qi?1,
and bi?1.
? Next word wi+1 and its tags, pi+1, qi+1, and
bi+1.
? The set of left words {w0, . . . , wi?1}, and
their tags, {p0, . . . , pi?1}, {q0, . . . , qi?1}, and
{b0, . . . , bi?1}. We use boolean variables such
as one of the left words is Mary.
? The set of right words {wi+1, . . . , wN},
and their POS tags, {pi+1, . . . , pN} and
{qi+1, . . . , qN}.
? Whether the word is the first word or not.
We also add the following boolean features to get
more contextual information.
? Existence of verbs or auxiliary verbs (MD) in
the sentence.
? The number of words between wi
and the nearest left comma. We
use boolean variables such as near-
est left comma is two words away.
? The number of words between wi and the near-
est right comma.
Now, we can encode training data by using these
boolean features. Each sentence is converted to the
set of pairs {(yi, xi)} where yi is +1 when xi cor-
responds to the root node and yi is ?1 otherwise.
For Preference Learning, we make the set of triplets
{(yi, xi.1, xi.2)}, where yi is always +1, xi.1 corre-
sponds to the root node, and xi.2 corresponds to a
non-root word in the same sentence. Such a triplet
means that xi.1 is preferable to xi.2 as a root node.
2.2 Dependency analysis
Our Dependency Analyzer is similar to Ya-
mada?s analyzer (Yamada and Matsumoto,
2003). While scanning a tagged sentence
T = (w1/p1, . . . , wn/pn) backward from the
end of the sentence, each word wi is classified into
three categories: Left, Right, and Shift.1
? Right: Right means that wi directly modifies
the right word wi+1 and that no word in T
modifies wi. If wi is classified as Right, the
analyzer removes wi from T and wi is regis-
tered as a left child of wi+1.
? Left: Left means that wi directly modifies the
left word wi?1 and that no word in T modifies
wi. If wi is classified as Left, the analyzer re-
moves wi from T and wi is registered as a right
child of wi?1.
? Shift: Shift means that wi is not next to its
modificand or is modified by another word in
T . If wi is classified as Shift, the analyzer
does nothing for wi and moves to the left word
wi?1.
This process is repeated until T is reduced to a
single word (= root node). Since this is a three-class
problem, we use ?one vs. rest? method. First, we
train an SVM classifier for each class. Then, for
each word in T , we compare their values: fLeft(x),
fRight(x), and fShift(x). If fLeft(x) is the largest,
the word is classified as Left.
However, Yamada?s algorithm stops when all
words in T are classified as Shift, even when T has
two or more words. In such cases, the analyzer can-
not generate complete dependency trees.
Here, we resolve this problem by reclassifying a
word in T as Left or Right. This word is selected in
terms of the differences between SVM outputs:
? ?Left(x) = fShift(x) ? fLeft(x),
? ?Right(x) = fShift(x) ? fRight(x).
These values are non-negative because fShift(x)
was selected. For instance, ?Left(x) ' 0 means that
fLeft(x) is almost equal to fShift(x). If ?Left(xk)
gives the smallest value of these differences, the
word corresponding to xk is reclassified as Left. If
1Yamada used a two-word window, but we use a one-word
window for simplicity.
?Right(xk) gives the smallest value, the word cor-
responding to xk is reclassified as Right. Then, we
can resume the analysis.
We use the following basic features for each word
in a sentence.
? The word itself wi and its tags pi, qi, and bi,
? Whether wi is on the left of the root node or on
the right (or at the root node). The root node is
determined by the Root-Node Finder.
? Whether wi is inside a quotation.
? Whether wi is inside a pair of parentheses.
? wi?s left children {wi1, . . . , wik}, which
were removed by the Dependency Analyzer
beforehand because they were classified as
?Right.? We use boolean variables such as
one of the left child is Mary.
Symmetrically, wi?s right children
{wi1, . . . , wik} are also used.
However, the above features cover only near-
sighted information. If wi is next to a very long
base NP or a sequence of base NPs, wi cannot get
information beyond the NPs. Therefore, we add the
following features.
? Li, Ri: Li is available when wi immediately
follows a base NP sequence. Li is the word be-
fore the sequence. That is, the sentence looks
like:
. . . Li ? a base NP ? wi . . .
Ri is defined symmetrically.
The following features of neigbors are also used
as wi?s features.
? Left words wi?3, . . . , wi?1 and their basic fea-
tures.
? Right words wi+1, . . . , wi+3 and their basic
features.
? The analyzer?s outputs (Left/Right/Shift) for
wi+1, . . . , wi+3. (This analyzer runs backward
from the end of T .)
If we train SVM by using the whole data at once,
training will take too long. Therefore, we split
the data into six groups: nouns, verbs, adjectives,
prepositions, punctuations, and others.
2.3 PP attachment
Since we do not have phrase labels, we use all
prepositions (except root nodes) as training data.
We use the following features for resolving PP at-
tachment.
? The preposition itself: wi.
? Candidate modificand wj and its POS tag.
? Left words (wi?2, wi?1) and their POS tags.
? Right words (wi+1, wi+2) and their POS tags.
? Previous preposition.
? Ending word of the following base NP and its
POS tag (if any).
? i ? j, i.e., Number of the words between wi
and wj .
? Number of commas between wi and wj .
? Number of verbs between wi and wj .
? Number of prepositions between wi and wj .
? Number of base NPs between wi and wj .
? Number of conjunctions (CCs) between wi and
wj .
? Difference of quotation depths between wi and
wj . If wi is not inside of a quotation, its quo-
tation depth is zero. If wj is in a quotation, its
quotation depth is one. Hence, their difference
is one.
? Difference of parenthesis depths between wi
and wj .
For each preposition, we make the set of triplets
{(yi, xi,1, xi,2)}, where yi is always +1, xi,1 corre-
sponds to the correct word that is modified by the
preposition, and xi,2 corresponds to other words in
the sentence.
3 Results
3.1 Root-Node Finder
For the Root-Node Finder, we used a quadratic ker-
nel K(xi, xj) = (xi ? xj + 1)2 because it was better
than the linear kernel in preliminary experiments.
When we used the ?correct? POS tags given in the
Penn Treebank, and the ?correct? base NP tags given
by a tool provided by CoNLL 2000 shared task2,
RNF?s accuracy was 96.5% for section 23. When
we used Collins? POS tags and base NP tags based
on the POS tags, the accuracy slightly degraded to
95.7%. According to Yamada?s paper (Yamada and
2http://cnts.uia.ac.be/conll200/chunking/
Matsumoto, 2003), this root accuracy is better than
Charniak?s MEIP and Collins? Model 3 parser.
We also conducted an experiment to judge the ef-
fectiveness of the base NP chunker. Here, we used
only the first 10,000 sentences (about 1/4) of the
training data. When we used all features described
above and the POS tags given in Penn Treebank,
the root accuracy was 95.4%. When we removed
the base NP information (bi, Li, Ri), it dropped
to 94.9%. Therefore, the base NP information im-
proves RNF?s performance.
Figure 3 compares SVM and Preference Learn-
ing in terms of the root accuracy. We used the
first 10,000 sentences for training again. Accord-
ing to this graph, Preference Learning is better than
SVM, but the difference is small. (They are bet-
ter than Maximum Entropy Modeling3 that yielded
RA=91.5% for the same data.) C does not affect the
scores very much unless C is too small. In this ex-
periment, we used Penn?s ?correct? POS tags. When
we used Collins? POS tags, the scores dropped by
about one point.
3.2 Dependency Analyzer and PPAR
As for the dependency learning, we used the same
quadratic kernel again because the quadratic kernel
gives the best results according to Yamada?s experi-
ments. The soft margin parameter C is 1 following
Yamada?s experiment. We conducted an experiment
to judge the effectiveness of the Root-Node Finder.
We follow Yamada?s definition of accuracy that ex-
cludes punctuation marks.
Dependency Accuracy (DA) =
#correct parents / #words (= 49,892)
Complete Rate (CR) =
#completely parsed sentences / #sentences
According to Table 1, DA is only slightly improved,
but CR is more improved.
3http://www2.crl.go.jp/jt/a132/members/mutiyama/
software.html
SVM
Preference Learning
Accuracy (%)
C0.10.030.010.0030.0010.00030.0001
90
91
92
93
94
95
96 ?????
?
?
??????
?
Figure 3: Comparison of SVM and Preference
Learning in terms of Root Accuracy (Trained with
10,000 sentences)
DA RA CR
without RNF 89.4% 91.9% 34.7%
with RNF 89.6% 95.7% 35.7%
The
Dependency Analyzer was trained with 10,000
sentences. RNF was trained with all of the training data.
DA: Dependency Accuracy, RA: Root Acc., CR:
Complete Rate
Table 1: Effectiveness of the Root-Node Finder
Accuracy (%)
C
Preference Learning
SVM
0.1
0.03
0.01
0.003
0.001
0.0003
0.000170
72
74
76
78
80
82
?
??
?
?
?
?
?
?
?
?
?
????
?
?
?????
?
?
Figure 4: Comparison of SVM and Preference
Learning in terms of Dependency Accuracy of
prepositions (Trained with 5,000 sentences)
Figure 4 compares SVM and Preference Learning
in terms of the Dependency Accuracy of preposi-
tions. SVM?s performance is unstable for this task,
and Preference Learning outperforms SVM. (We
could not get scores of Maximum Entropy Model-
ing because of memory shortage.)
Table 2 shows the improvement given by PPAR.
Since training of PPAR takes a very long time, we
used only the first 35,000 sentences of the train-
ing data. We also calculated the Dependency Accu-
racy of Collins? Model 3 parser?s output for section
23. According to this table, PPAR is better than the
Model 3 parser.
Now, we use PPAR?s output for each preposition
instead of the dependency parser?s output unless the
modification makes the dependency tree into a non-
tree graph. Table 3 compares the proposed method
with other methods in terms of accuracy. This data
except ?Proposed? was cited from Yamada?s paper.
IN TO average
Collins Model 3 84.6% 87.3% 85.1%
Dependency Analyzer 83.4% 86.1% 83.8%
PPAR 85.3% 87.7% 85.7%
PPAR was trained with 35,000 sentences. The number
of IN words is 5,950 and that of TO is 1,240.
Table 2: PP-Attachment Resolver
DA RA CR
with MEIP 92.1% 95.2% 45.2%
phrase info. Collins Model3 91.5% 95.2% 43.3%
without Yamada 90.3% 91.6% 38.4%
phrase info. Proposed 91.2% 95.7% 40.7%
Table 3: Comparison with related work
According to this table, the proposed method is
close to the phrase structure parsers except Com-
plete Rate. Without PPAR, DA dropped to 90.9%
and CR dropped to 39.7%.
4 Discussion
We used Preference Learning to improve the SVM-
based Dependency Analyzer for root-node finding
and PP-attachment resolution. Preference Learn-
ing gave better scores than Collins? Model 3 parser
for these subproblems. Therefore, we expect that
our method is also applicable to phrase structure
parsers. It seems that root-node finding is relatively
easy and SVM worked well. However, PP attach-
ment is more difficult and SVM?s behavior was un-
stable whereas Preference Learning was more ro-
bust. We want to fully exploit Preference Learn-
ing for dependency analysis and parsing, but train-
ing takes too long. (Empirically, it takes O(`2) or
more.) Further study is needed to reduce the compu-
tational complexity. (Since we used Isozaki?s meth-
ods (Isozaki and Kazawa, 2002), the run-time com-
plexity is not a problem.)
Kudo and Matsumoto (2002) proposed an SVM-
based Dependency Analyzer for Japanese sen-
tences. Japanese word dependency is simpler be-
cause no word modifies a left word. Collins and
Duffy (2002) improved Collins? Model 2 parser
by reranking possible parse trees. Shen and Joshi
(2003) also used the preference kernel K(xi.?, xj.?)
for reranking. They compare parse trees, but our
system compares words.
5 Conclusions
Dependency analysis is useful and annotation of
word dependency seems easier than annotation of
phrase labels. However, lack of phrase labels makes
dependency analysis more difficult than phrase
structure parsing. In this paper, we improved a de-
terministic dependency analyzer by adding a Root-
Node Finder and a PP-Attachment Resolver. Pref-
erence Learning gave better scores than Collins?
Model 3 parser for these subproblems, and the per-
formance of the improved system is close to state-
of-the-art phrase structure parsers. It turned out
that SVM was unstable for PP attachment resolu-
tion whereas Preference Learning was not. We ex-
pect this method is also applicable to phrase struc-
ture parsers.
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 132?139.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures, and the voted percep-
tron. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics
(ACL), pages 263?270.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics, pages 16?23.
Michael Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, Univ. of Pennsylvania.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines. Cam-
bridge University Press.
Jason M. Eisner. 1996. Three new probabilistic
models for dependency parsing: An exploration.
In Proceedings of the International Conference
on Computational Linguistics, pages 340?345.
Ralf Herbrich, Thore Graepel, Peter Bollmann-
Sdorra, and Klaus Obermayer. 1998. Learning
preference relations for information retrieval. In
Proceedings of ICML-98 Workshop on Text Cate-
gorization and Machine Learning, pages 80?84.
Ralf Herbrich, Thore Graepel, and Klaus Ober-
mayer, 2000. Large Margin Rank Boundaries for
Ordinal Regression, chapter 7, pages 115?132.
MIT Press.
Julia Hockenmaier and Mark Steedman. 2002.
Generative models for statistical parsing with
combinatory categorial grammar. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 335?342.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recog-
nition. In Proceedings of COLING-2002, pages
390?396.
Thorsten Joachims. 1999. Making large-scale
support vector machine learning practical. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola,
editors, Advances in Kernel Methods, chapter 16,
pages 170?184. MIT Press.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the ACM Conference on Knowledge Discovery
and Data Mining.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
NAACL-2001, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking.
In Proceedings of CoNLL, pages 63?69.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a large annotated
corpus of english: the penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing.
Libin Shen and Aravind K. Joshi. 2003. An SVM
based voting algorithm with application to parse
reranking. In Proceedings of the Seventh Confer-
ence on Natural Language Learning, pages 9?16.
Daniel Sleator and Davy Temperley. 1991. Parsing
English with a Link grammar. Technical Report
CMU-CS-91-196, Carnegie Mellon University.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern represen-
tation model for automatic IE pattern acquisi-
tion. In Proceedings of the Annual Meeting of the
Association for Cimputational Linguistics, pages
224?231.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical direct acyclic
graph kernel: Methods for structured natural lan-
guage data. In Proceedings of ACL-2003, pages
32?39.
Vladimir N. Vapnik. 1995. The Nature of Statisti-
cal Learning Theory. Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical dependency analysis. In Proceedings of
the International Workshop on Parsing Technolo-
gies, pages 195?206.
Dependency-based Sentence Alignment for Multiple Document
Summarization
Tsutomu HIRAO and Jun SUZUKI and Hideki ISOZAKI and Eisaku MAEDA
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
 
hirao,jun,isozaki,maeda  @cslab.kecl.ntt.co.jp
Abstract
In this paper, we describe a method of automatic
sentence alignment for building extracts from ab-
stracts in automatic summarization research. Our
method is based on two steps. First, we introduce
the ?dependency tree path? (DTP). Next, we calcu-
late the similarity between DTPs based on the ESK
(Extended String Subsequence Kernel), which con-
siders sequential patterns. By using these proce-
dures, we can derive one-to-many or many-to-one
correspondences among sentences. Experiments us-
ing different similarity measures show that DTP
consistently improves the alignment accuracy and
that ESK gives the best performance.
1 Introduction
Many researchers who study automatic summariza-
tion want to create systems that generate abstracts
of documents rather than extracts. We can gener-
ate an abstract by utilizing various methods, such
as sentence compaction, sentence combination, and
paraphrasing. In order to implement and evalu-
ate these techniques, we need large-scale corpora
in which the original sentences are aligned with
summary sentences. These corpora are useful for
training and evaluating sentence extraction systems.
However, it is costly to create these corpora.
Figure 1 shows an example of summary sentences
and original sentences from TSC-2 (Text Summa-
rization Challenge 2) multiple document summa-
rization data (Okumura et al, 2003). From this ex-
ample, we can see many-to-many correspondences.
For instance, summary sentence (A) consists of a
part of source sentence (A). Summary sentence (B)
consists of parts of source sentences (A), (B), and
(C). It is clear that the correspondence among the
sentences is very complex. Therefore, robust and
accurate alignment is essential.
In order to achieve such alignment, we need not
only syntactic information but also semantic infor-
mation. Therefore, we combine two methods. First,
we introduce the ?dependency tree path? (DTP) for
Source(A): 
	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 764?773, Prague, June 2007. c?2007 Association for Computational Linguistics
Online Large-Margin Training for Statistical Machine Translation
Taro Watanabe Jun Suzuki Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
{taro,jun,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
We achieved a state of the art performance
in statistical machine translation by using
a large number of features with an online
large-margin training algorithm. The mil-
lions of parameters were tuned only on a
small development set consisting of less than
1K sentences. Experiments on Arabic-to-
English translation indicated that a model
trained with sparse binary features outper-
formed a conventional SMT system with a
small number of features.
1 Introduction
The recent advances in statistical machine transla-
tion have been achieved by discriminatively train-
ing a small number of real-valued features based ei-
ther on (hierarchical) phrase-based translation (Och
and Ney, 2004; Koehn et al, 2003; Chiang, 2005) or
syntax-based translation (Galley et al, 2006). How-
ever, it does not scale well with a large number of
features of the order of millions.
Tillmann and Zhang (2006), Liang et al (2006)
and Bangalore et al (2006) introduced sparse binary
features for statistical machine translation trained on
a large training corpus. In this framework, the prob-
lem of translation is regarded as a sequential labeling
problem, in the same way as part-of-speech tagging,
chunking or shallow parsing. However, the use of a
large number of features did not provide any signifi-
cant improvements over a conventional small feature
set.
Bangalore et al (2006) trained the lexical choice
model by using Conditional Random Fields (CRF)
realized on a WFST. Their modeling was reduced to
Maximum Entropy Markov Model (MEMM) to han-
dle a large number of features which, in turn, faced
the labeling bias problem (Lafferty et al, 2001).
Tillmann and Zhang (2006) trained their feature set
using an online discriminative algorithm. Since the
decoding is still expensive, their online training ap-
proach is approximated by enlarging a merged k-
best list one-by-one with a 1-best output. Liang
et al (2006) introduced an averaged perceptron al-
gorithm, but employed only 1-best translation. In
Watanabe et al (2006a), binary features were trained
only on a small development set using a variant of
voted perceptron for reranking k-best translations.
Thus, the improvement is merely relative to the
baseline translation system, namely whether or not
there is a good translation in their k-best.
We present a method to estimate a large num-
ber of parameters ? of the order of millions ?
using an online training algorithm. Although it
was intuitively considered to be prone to overfit-
ting, training on a small development set ? less
than 1K sentences ? was sufficient to achieve im-
proved performance. In this method, each train-
ing sentence is decoded and weights are updated at
every iteration (Liang et al, 2006). When updat-
ing model parameters, we employ a memorization-
variant of a local updating strategy (Liang et al,
2006) in which parameters are optimized toward
a set of good translations found in the k-best list
across iterations. The objective function is an ap-
proximated BLEU (Watanabe et al, 2006a) that
scales the loss of a sentence BLEU to a document-
wise loss. The parameters are trained using the
764
Margin Infused Relaxed Algorithm (MIRA) (Cram-
mer et al, 2006). MIRA is successfully employed
in dependency parsing (McDonald et al, 2005) or
the joint-labeling/chunking task (Shimizu and Haas,
2006). Experiments were carried out on an Arabic-
to-English translation task, and we achieved signif-
icant improvements over conventional minimum er-
ror training with a small number of features.
This paper is organized as follows: First, Sec-
tion 2 introduces the framework of statistical ma-
chine translation. As a baseline SMT system, we
use the hierarchical phrase-based translation with
an efficient left-to-right generation (Watanabe et al,
2006b) originally proposed by Chiang (2005). In
Section 3, a set of binary sparse features are defined
including numeric features for our baseline system.
Section 4 introduces an online large-margin training
algorithm using MIRA with our key components.
The experiments are presented in Section 5 followed
by discussion in Section 6.
2 Statistical Machine Translation
We use a log-linear approach (Och, 2003) in which
a foreign language sentence f is translated into an-
other language, for example English, e, by seeking a
maximum solution:
e? = argmax
e
wT ? h( f , e) (1)
where h( f , e) is a large-dimension feature vector. w
is a weight vector that scales the contribution from
each feature. Each feature can take any real value,
such as the log of the n-gram language model to
represent fluency, or a lexicon model to capture the
word or phrase-wise correspondence.
2.1 Hierarchical Phrase-based SMT
Chiang (2005) introduced the hierarchical phrase-
based translation approach, in which non-terminals
are embedded in each phrase. A translation is gener-
ated by hierarchically combining phrases using the
non-terminals. Such a quasi-syntactic structure can
naturally capture the reordering of phrases that is not
directly modeled by a conventional phrase-based ap-
proach (Koehn et al, 2003). The non-terminal em-
bedded phrases are learned from a bilingual corpus
without a linguistically motivated syntactic struc-
ture.
Based on hierarchical phrase-based modeling, we
adopted the left-to-right target generation method
(Watanabe et al, 2006b). This method is able to
generate translations efficiently, first, by simplifying
the grammar so that the target side takes a phrase-
prefixed form, namely a target normalized form.
Second, a translation is generated in a left-to-right
manner, similar to the phrase-based approach using
Earley-style top-down parsing on the source side.
Coupled with the target normalized form, n-gram
language models are efficiently integrated during the
search even with a higher order of n.
2.2 Target Normalized Form
In Chiang (2005), each production rule is restricted
to a rank-2 or binarized form in which each rule con-
tains at most two non-terminals. The target normal-
ized form (Watanabe et al, 2006b) further imposes
a constraint whereby the target side of the aligned
right-hand side is restricted to a Greibach Normal
Form like structure:
X ?
?
?, ?b?,?
?
(2)
where X is a non-terminal, ? is a source side string of
arbitrary terminals and/or non-terminals. ?b? is a cor-
responding target side where ?b is a string of termi-
nals, or a phrase, and ? is a (possibly empty) string
of non-terminals. ? defines one-to-one mapping be-
tween non-terminals in ? and ?. The use of phrase
?b as a prefix maintains the strength of the phrase-
base framework. A contiguous English side with a
(possibly) discontiguous foreign language side pre-
serves phrase-bounded local word reordering. At
the same time, the target normalized framework still
combines phrases hierarchically in a restricted man-
ner.
2.3 Left-to-Right Target Generation
Decoding is performed by parsing on the source side
and by combining the projected target side. We
applied an Earley-style top-down parsing approach
(Wu and Wong, 1998; Watanabe et al, 2006b; Zoll-
mann and Venugopal, 2006). The basic idea is
to perform top-down parsing so that the projected
target side is generated in a left-to-right manner.
The search is guided with a push-down automaton,
which keeps track of the span of uncovered source
765
word positions. Combined with the rest-cost esti-
mation aggregated in a bottom-up way, our decoder
efficiently searches for the most likely translation.
The use of a target normalized form further sim-
plifies the decoding procedure. Since the rule form
does not allow any holes for the target side, the inte-
gration with an n-gram language model is straight-
forward: the prefixed phrases are simply concate-
nated and intersected with n-gram.
3 Features
3.1 Baseline Features
The hierarchical phrase-based translation system
employs standard numeric value features:
? n-gram language model to capture the fluency
of the target side.
? Hierarchical phrase translation probabilities in
both directions, h(?|?b?) and h(?b?|?), estimated
by relative counts, count(?, ?b?).
? Word-based lexically weighted models of
hlex(?|?b?) and hlex(?b?|?) using lexical transla-
tion models.
? Word-based insertion/deletion penalties that
penalize through the low probabilities of the
lexical translation models (Bender et al, 2004).
? Word/hierarchical-phrase length penalties.
? Backtrack-based penalties inspired by the dis-
tortion penalties in phrase-based modeling
(Watanabe et al, 2006b).
3.2 Sparse Features
In addition to the baseline features, a large number
of binary features are integrated in our MT system.
We may use any binary features, such as
h( f , e) =
?
?
?
?
?
?
?
?
?
1 English word ?violate? and Arabic
word ?tnthk? appeared in e and f .
0 otherwise.
The features are designed by considering the decod-
ing efficiency and are based on the word alignment
structure preserved in hierarchical phrase transla-
tion pairs (Zens and Ney, 2006). When hierarchi-
cal phrases are extracted, the word alignment is pre-
served. If multiple word alignments are observed
ei?1 ei ei+1 ei+2 ei+3 ei+4
f j?1 f j f j+1 f j+2 f j+3
Figure 1: An example of sparse features for a phrase
translation.
with the same source and target sides, only the fre-
quently observed word alignment is kept to reduce
the grammar size.
3.2.1 Word Pair Features
Word pair features reflect the word correspon-
dence in a hierarchical phrase. Figure 1 illustrates
an example of sparse features for a phrase trans-
lation pair f j, ..., f j+2 and ei, ..., ei+3 1. From the
word alignment encoded in this phrase, we can ex-
tract word pair features of (ei, f j+1), (ei+2, f j+2) and
(ei+3, f j).
The bigrams of word pairs are also used to
capture the contextual dependency. We assume
that the word pairs follow the target side order-
ing. For instance, we define ((ei?1, f j?1), (ei, f j+1)),
((ei, f j+1), (ei+2, f j+2)) and ((ei+2, f j+2), (ei+3, f j)) in-
dicated by the arrows in Figure 1.
Extracting bigram word pair features following
the target side ordering implies that the correspond-
ing source side is reordered according to the tar-
get side. The reordering of hierarchical phrases is
represented by using contextually dependent word
pairs across their boundaries, as with the feature
((ei?1, f j?1), (ei, f j+1)) in Figure 1.
3.2.2 Insertion Features
The above features are insufficient to capture the
translation because spurious words are sometimes
inserted in the target side. Therefore, insertion fea-
tures are integrated in which no word alignment is
associated in the target. The inserted words are asso-
ciated with all the words in the source sentence, such
as (ei+1, f1), ..., (ei+1, fJ) for the non-aligned word
ei+1 with the source sentence f J1 in Figure 1. In the
1For simplicity, we show an example of phrase translation
pairs, but it is trivial to define the features over hierarchical
phrases.
766
f j?1
f j f j+1
f j+2
f j+3
X 1
X 2
X 3
Figure 2: Example hierarchical features.
same way, we will be able to include deletion fea-
tures where a non-aligned source word is associated
with the target sentence. However, this would lead to
complex decoding in which all the translated words
are memorized for each hypothesis, and thus not in-
tegrated in our feature set.
3.2.3 Target Bigram Features
Target side bigram features are also included to
directly capture the fluency as in the n-gram lan-
guage model (Roark et al, 2004). For instance, bi-
gram features of (ei?1, ei), (ei, ei+1), (ei+1, ei+2)... are
observed in Figure 1.
3.2.4 Hierarchical Features
In addition to the phrase motivated features, we
included features inspired by the hierarchical struc-
ture. Figure 2 shows an example of hierarchical
phrases in the source side, consisting of X 1 ?
?
f j?1X 2 f j+3
?
, X 2 ?
?
f j f j+1X 3
?
and X 3 ?
?
f j+2
?
.
Hierarchical features capture the dependency of
the source words in a parent phrase to the source
words in child phrases, such as ( f j?1, f j), ( f j?1, f j+1),
( f j+3, f j), ( f j+3, f j+1), ( f j, f j+2) and ( f j+1, f j+2) as in-
dicated by the arrows in Figure 2. The hierarchical
features are extracted only for those source words
that are aligned with the target side to limit the fea-
ture size.
3.3 Normalization
In order to achieve the generalization capability, the
following normalized tokens are introduced for each
surface form:
? Word class or POS.
? 4-letter prefix and suffix. For instance, the word
Algorithm 1 Online Training Algorithm
Training data: T = {( f t, et)}Tt=1
m-best oracles: O = {}Tt=1
i = 0
1: for n = 1, ..., N do
2: for t = 1, ..., T do
3: Ct ? bestk( f t; wi)
4: Ot ? oraclem(Ot ? Ct; et)
5: wi+1 = update wi using Ct w.r.t. Ot
6: i = i + 1
7: end for
8: end for
9: return
?NT
i=1 w
i
NT
?violate? is normalized to ?viol+? and ?+late?
by taking the prefix and suffix, respectively.
? Digits replaced by a sequence of ?@?. For ex-
ample, the word ?2007/6/27? is represented as
?@@@@/@/@@?.
We consider all possible combination of those to-
ken types. For example, the word pair feature (vi-
olate, tnthk) is normalized and expanded to (viol+,
tnthk), (viol+, tnth+), (violate, tnth+), etc. using the
4-letter prefix token type.
4 Online Large-Margin Training
Algorithm 1 is our generic online training algo-
rithm. The algorithm is slightly different from other
online training algorithms (Tillmann and Zhang,
2006; Liang et al, 2006) in that we keep and up-
date oracle translations, which is a set of good trans-
lations reachable by a decoder according to a met-
ric, i.e. BLEU (Papineni et al, 2002). In line 3,
a k-best list is generated by bestk(?) using the cur-
rent weight vector wi for the training instance of
( f t, et). Each training instance has multiple (or, pos-
sibly one) reference translations et for the source
sentence f t. Using the k-best list, m-best oracle
translations Ot is updated by oraclem(?) for every it-
eration (line 4). Usually, a decoder cannot generate
translations that exactly match the reference transla-
tions due to its beam search pruning and OOV. Thus,
we cannot always assign scores for each reference
translation. Therefore, possible oracle translations
are maintained according to an objective function,
767
i.e. BLEU. Tillmann and Zhang (2006) avoided the
problem by precomputing the oracle translations in
advance. Liang et al (2006) presented a similar up-
dating strategy in which parameters were updated
toward an oracle translation found in Ct, but ignored
potentially better translations discovered in the past
iterations.
New wi+1 is computed using the k-best list Ct with
respect to the oracle translations Ot (line 5). After N
iterations, the algorithm returns an averaged weight
vector to avoid overfitting (line 9). The key to this
online training algorithm is the selection of the up-
dating scheme in line 5.
4.1 Margin Infused Relaxed Algorithm
The Margin Infused Relaxed Algorithm (MIRA)
(Crammer et al, 2006) is an online version of the
large-margin training algorithm for structured clas-
sification (Taskar et al, 2004) that has been suc-
cessfully used for dependency parsing (McDonald et
al., 2005) and joint-labeling/chunking (Shimizu and
Haas, 2006). The basic idea is to keep the norm of
the updates to the weight vector as small as possible,
considering a margin at least as large as the loss of
the incorrect classification.
Line 5 of the weight vector update procedure in
Algorithm 1 is replaced by the solution of:
w?i+1 = argmin
wi+1
||wi+1 ? wi|| + C
?
e?,e?
?(e?, e?)
subject to
si+1( f t, e?) ? si+1( f t, e?) + ?(e?, e?) ? L(e?, e?; et)
?(e?, e?) ? 0
?e? ? Ot,?e? ? Ct (3)
where si( f t, e) =
{
wi
}T ? h( f t, e). ?(?) is a non-
negative slack variable and C ? 0 is a constant to
control the influence to the objective function. A
larger C implies larger updates to the weight vec-
tor. L(?) is a loss function, for instance difference of
BLEU, that measures the difference between e? and
e? according to the reference translations et. In this
update, a margin is created for each correct and in-
correct translation at least as large as the loss of the
incorrect translation. A larger error means a larger
distance between the scores of the correct and incor-
rect translations. Following McDonald et al (2005),
only k-best translations are used to form the margins
in order to reduce the number of constraints in Eq. 3.
In the translation task, multiple translations are ac-
ceptable. Thus, margins for m-oracle translation are
created, which amount to m ? k large-margin con-
straints. In this online training, only active features
constrained by Eq. 3 are kept and updated, unlike
offline training in which all possible features have to
be extracted and selected in advance.
The Lagrange dual form of Eq. 3 is:
max?(?)?0 ?
1
2
||
?
e?,e?
?(e?, e?)
(
h( f t, e?) ? h( f t, e?)
)
||2
+
?
e?,e?
?(e?, e?)L(e?, e?; et)
?
?
e?,e?
?(e?, e?)
(
si( f t, e?) ? si( f t, e?)
)
subject to
?
e?,e?
?(e?, e?) ? C (4)
with the weight vector update:
wi+1 = wi +
?
e?,e?
?(e?, e?)
(
h( f t, e?) ? h( f t, e?)
)
(5)
Equation 4 is solved using a QP-solver, such as a co-
ordinate ascent algorithm, by heuristically selecting
(e?, e?) and by updating ?(?) iteratively:
?(e?, e?) = max (0, ?(e?, e?) + ?(e?, e?)) (6)
?(e?, e?) =
L(e?, e?; et) ?
(
si( f t, e?) ? si( f t, e?)
)
||h( f t, e?) ? h( f t, e?)||2
C is used to clip the amount of updates.
A single oracle with 1-best translation is analyti-
cally solved without a QP-solver and is represented
as the following perceptron-like update (Shimizu
and Haas, 2006):
? = max
?
?
?
?
?
?
?
?
0, min
?
?
?
?
?
?
?
?
C,
L(e?, e?; et) ?
(
si( f t, e?) ? si( f t, e?)
)
||h( f t, e?) ? h( f t, e?)||2
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Intuitively, the update amount is controlled by the
margin and the loss between the correct and incor-
rect translations and by the closeness of two transla-
tions in terms of feature vectors. Indeed, Liang et al
(2006) employed an averaged perceptron algorithm
in which ? value was always set to one. Tillmann
and Zhang (2006) used a different update style based
on a convex loss function:
? = ?L(e?, e?; et) ?max
(
0, 1 ?
(
si( f t, e?) ? si( f t, e?)
))
768
Table 1: Experimental results obtained by varying normalized tokens used with surface form.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
surface form 492K 11.32 54.11 10.57 49.01 10.77 48.05
w/ prefix/suffix 4,204K 12.38 63.87 10.42 48.74 10.58 47.18
w/ word class 2,689K 10.87 49.59 10.63 49.55 10.89 48.79
w/ digits 576K 11.01 50.72 10.66 49.67 10.84 48.39
all token types 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
where ? > 0 is a learning rate for controlling the
convergence.
4.2 Approximated BLEU
We used the BLEU score (Papineni et al, 2002) as
the loss function computed by:
BLEU(E; E) = exp
?
?
?
?
?
?
?
?
1
N
N
?
n=1
log pn(E, E)
?
?
?
?
?
?
?
?
? BP(E, E)
(7)
where pn(?) is the n-gram precision of hypothesized
translations E = {et}Tt=1 given reference translations
E = {et}Tt=1 and BP(?) ? 1 is a brevity penalty. BLEU
is computed for a set of sentences, not for a sin-
gle sentence. Our algorithm requires frequent up-
dates on the weight vector, which implies higher cost
in computing the document-wise BLEU. Tillmann
and Zhang (2006) and Liang et al (2006) solved
the problem by introducing a sentence-wise BLEU.
However, the use of the sentence-wise scoring does
not translate directly into the document-wise score
because of the n-gram precision statistics and the
brevity penalty statistics aggregated for a sentence
set. Thus, we use an approximated BLEU score
that basically computes BLEU for a sentence set, but
accumulates the difference for a particular sentence
(Watanabe et al, 2006a).
The approximated BLEU is computed as follows:
Given oracle translations O for T , we maintain the
best oracle translations OT1 =
{
e?1, ..., e?T
}
. The ap-
proximated BLEU for a hypothesized translation e?
for the training instance ( f t, et) is computed over OT1
except for e?t, which is replaced by e?:
BLEU({e?1, ..., e?t?1, e?, e?t+1, ..., e?T }; E)
The loss computed by the approximated BLEU mea-
sures the document-wise loss of substituting the cor-
rect translation e?t into an incorrect translation e?.
The score can be regarded as a normalization which
scales a sentence-wise score into a document-wise
score.
5 Experiments
We employed our online large-margin training pro-
cedure for an Arabic-to-English translation task.
The training data were extracted from the Ara-
bic/English news/UN bilingual corpora supplied by
LDC. The data amount to nearly 3.8M sentences.
The Arabic part of the bilingual data is tokenized by
isolating Arabic scripts and punctuation marks. The
development set comes from the MT2003 Arabic-
English NIST evaluation test set consisting of 663
sentences in the news domain with four reference
translations. The performance is evaluated by the
news domain MT2004/MT2005 test set consisting
of 707 and 1,056 sentences, respectively.
The hierarchical phrase translation pairs are ex-
tracted in a standard way (Chiang, 2005): First,
the bilingual data are word alignment annotated by
running GIZA++ (Och and Ney, 2003) in two di-
rections. Second, the word alignment is refined
by a grow-diag-final heuristic (Koehn et al, 2003).
Third, phrase translation pairs are extracted together
with hierarchical phrases by considering holes. In
the last step, the hierarchical phrases are constrained
so that they follow the target normalized form con-
straint. A 5-gram language model is trained on the
English side of the bilingual data combined with the
English Gigaword from LDC.
First, the use of normalized token types in Sec-
tion 3.3 is evaluated in Table 1. In this setting, all
the structural features in Section 3.2 are used, but
differentiated by the normalized tokens combined
with surface forms. Our online large-margin train-
ing algorithm performed 50 iterations constrained
769
Table 2: Experimental results obtained by incrementally adding structural features.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
word pairs 11,042K 11.05 51.63 10.43 48.69 10.73 47.72
+ target bigram 11,230K 11.19 53.49 10.40 48.60 10.66 47.47
+ insertion 13,489K 11.21 52.20 10.77 50.33 10.93 48.08
+ hierarchical 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
Table 3: Experimental results for varying k-best and m-oracle translations.
# features 2003 (dev) 2004 2005
NIST BLEU [%] NIST BLEU [%] NIST BLEU [%]
baseline 10.64 46.47 10.83 49.33 10.90 47.03
1-oracle 1-best 8,735K 11.25 52.63 10.82 50.77 10.93 48.11
1-oracle 10-best 10,480K 11.24 53.45 10.55 49.10 10.82 48.49
10-oracle 1-best 8,416K 10.70 47.63 10.83 48.88 10.76 46.00
10-oracle 10-best 13,759K 11.24 52.85 10.66 49.81 10.85 48.41
sentence-BLEU 14,587K 11.10 51.17 10.82 49.97 10.86 47.04
by 10-oracle and 10-best list. When decoding, a
1000-best list is generated to achieve better oracle
translations. The training took nearly 1 day using 8
cores of Opteron. The translation quality is eval-
uated by case-sensitive NIST (Doddington, 2002)
and BLEU (Papineni et al, 2002)2. The table also
shows the number of active features in which non-
zero values were assigned as weights. The addition
of prefix/suffix tokens greatly increased the number
of active features. The setting severely overfit to the
development data, and therefore resulted in worse
results in open tests. The word class3 with surface
form avoided the overfitting problem. The digit se-
quence normalization provides a similar generaliza-
tion capability despite of the moderate increase in
the active feature size. By including all token types,
we achieved better NIST/BLEU scores for the 2004
and 2005 test sets. This set of experiments indi-
cates that a token normalization is useful especially
trained on a small data.
Second, we used all the normalized token types,
but incrementally added structural features in Ta-
ble 2. Target bigram features account for only the
fluency of the target side without considering the
source/target correspondence. Therefore, the in-
2We used the tool available at http://www.nist.gov/
speech/tests/mt/
3We induced 50 classes each for English and Arabic.
clusion of target bigram features clearly overfit to
the development data. The problem is resolved by
adding insertion features which can take into ac-
count an agreement with the source side that is not
directly captured by word pair features. Hierarchi-
cal features are somewhat effective in the 2005 test
set by considering the dependency structure of the
source side.
Finally, we compared our online training algo-
rithm with sparse features with a baseline system
in Table 3. The baseline hierarchical phrase-based
system is trained using standard max-BLEU training
(MERT) without sparse features (Och, 2003). Table
3 shows the results obtained by varying the m-oracle
and k-best size (k, m = 1, 10) using all structural
features and all token types. We also experimented
sentence-wise BLEU as an objective function con-
strained by 10-oracle and 10-best list. Even the 1-
oracle 1-best configuration achieved significant im-
provements over the baseline system. The use of
a larger k-best list further optimizes to the devel-
opment set, but at the cost of degraded translation
quality in the 2004 test set. The larger m-oracle size
seems to be harmful if coupled with the 1-best list.
As indicated by the reduced active feature size, 1-
best translation seems to be updated toward worse
translations in 10-oracles that are ?close? in terms
of features. We achieved significant improvements
770
Table 4: Two-fold cross validation experiments.
closed test open test
NIST BLEU NIST BLEU
[%] [%]
baseline 10.71 44.79 10.68 44.44
online 11.58 53.42 10.90 47.64
when the k-best list size was also increased. The
use of sentence-wise BLEU as an objective provides
almost no improvement in the 2005 test set, but is
comparable for the 2004 test set.
As observed in three experiments, the 2004/2005
test sets behaved differently, probably because of
the domain mismatch. Thus, we conducted a two-
fold cross validation using the 2003/2004/2005 test
sets to observe the effect of optimization as shown
in Table 44. The MERT baseline system performed
similarly both in closed and open tests. Our on-
line large-margin training with 10-oracle and 10-
best constraints and the approximated BLEU loss
function significantly outperformed the baseline sys-
tem in the open test. The development data is almost
doubled in this setting. The MERT approach seems
to be confused with the slightly larger data and with
the mixed domains from different epochs.
6 Discussion
In this work, the translation model consisting of mil-
lions of features are successfully integrated. In or-
der to avoid poor overfitting, features are limited to
word-based features, but are designed to reflect the
structures inside hierarchical phrases. One of the
benefit of MIRA is its flexibility. We may include
as many constraints as possible, like m-oracle con-
straints in our experiments. Although we described
experiments on the hierarchical phrase-based trans-
lation, the online training algorithm is applicable to
any translation systems, such as phrase-based trans-
lations and syntax-based translations.
Online discriminative training has already been
studied by Tillmann and Zhang (2006) and Liang
et al (2006). In their approach, training was per-
formed on a large corpus using the sparse features of
phrase translation pairs, target n-grams and/or bag-
of-word pairs inside phrases. In Tillmann and Zhang
4We split data by document, not by sentence.
(2006), k-best list generation is approximated by a
step-by-step one-best merging method that separates
the decoding and training steps. The weight vector
update scheme is very similar to MIRA but based
on a convex loss function. Our method directly em-
ploys the k-best list generated by the fast decoding
method (Watanabe et al, 2006b) at every iteration.
One of the benefits is that we avoid the rather expen-
sive cost of merging the k-best list especially when
handling millions of features.
Liang et al (2006) employed an averaged percep-
tron algorithm. They decoded each training instance
and performed a perceptron update to the weight
vector. An incorrect translation was updated toward
an oracle translation found in a k-best list, but dis-
carded potentially better translations in the past iter-
ations.
An experiment has been undertaken using a small
development set together with sparse features for the
reranking of a k-best translation (Watanabe et al,
2006a). They relied on a variant of a voted percep-
tron, and achieved significant improvements. How-
ever, their work was limited to reranking, thus the
improvement was relative to the performance of the
baseline system, whether or not there was a good
translation in a list. In our work, the sparse features
are directly integrated into the DP-based search.
The design of the sparse features was inspired
by Zens and Ney (2006). They exploited the
word alignment structure inside the phrase trans-
lation pairs for discriminatively training a reorder-
ing model in their phrase-based translation. The re-
ordering model simply classifies whether to perform
monotone decoding or not. The trained model is
treated as a single feature function integrated in Eq.
1. Our approach differs in that each sparse feature is
individually integrated in Eq. 1.
7 Conclusion
We exploited a large number of binary features
for statistical machine translation. The model was
trained on a small development set. The optimiza-
tion was carried out by MIRA, which is an online
version of the large-margin training algorithm. Mil-
lions of sparse features are intuitively considered
prone to overfitting, especially when trained on a
small development set. However, our algorithm with
771
millions of features achieved very significant im-
provements over a conventional method with a small
number of features. This result indicates that we
can easily experiment many alternative features even
with a small data set, but we believe that our ap-
proach can scale well to a larger data set for further
improved performance. Future work involves scal-
ing up to larger data and more features.
Acknowledgements
We would like to thank reviewers and our colleagues
for useful comment and discussion.
References
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2006. Sequence classification for machine trans-
lation. In Proc. of Interspeech 2006, pages 1157?
1160, Pittsburgh.
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system?. In Proc. of IWSLT 2004, pages 79?84,
Kyoto, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL
2005, pages 263?270, Ann Arbor, Michigan, June.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, March.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In In Proc. ARPA Workshop on Human Lan-
guage Technology.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006, pages 961?968, Sydney, Aus-
tralia, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289. Morgan Kaufmann, San Francisco, CA.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of COL-
ING/ACL 2006, pages 761?768, Sydney, Australia,
July.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proc. of ACL 2005, pages 91?98, Ann Ar-
bor, Michigan, June.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318, Philadelphia, Pennsylvania.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language model-
ing with conditional random fields and the percep-
tron algorithm. In Proc. of ACL 2004, pages 47?54,
Barcelona, Spain, July.
Nobuyuki Shimizu and Andrew Haas. 2006. Exact de-
coding for jointly labeling and chunking sequences.
In Proc. of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 763?770, Sydney, Australia,
July.
Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In
Proc. of EMNLP 2004, pages 1?8, Barcelona, Spain,
July.
Christoph Tillmann and Tong Zhang. 2006. A discrimi-
native global training algorithm for statistical MT. In
Proc. of COLING/ACL 2006, pages 721?728, Sydney,
Australia, July.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006a. NTT Statistical Machine Translation
for IWSLT 2006. In Proc. of IWSLT 2006, pages 95?
102, Kyoto, Japan.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006b. Left-to-right target generation for hierarchi-
cal phrase-based translation. In Proc. of COLING/ACL
2006, pages 777?784, Sydney, Australia, July.
772
Dekai Wu and Hongsing Wong. 1998. Machine transla-
tion with a stochastic grammatical channel. In Proc.
of COLING 98, pages 1408?1415, Montreal, Quebec,
Canada.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proc. of WSMT 2006, pages 55?63, New York City,
June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. of WSMT 2006, pages 138?141, New York City,
June.
773
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 791?800, Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Structured Output Learning
based on a Hybrid Generative and Discriminative Approach
Jun Suzuki, Akinori Fujino and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{jun, a.fujino, isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a framework for
semi-supervised structured output learning
(SOL), specifically for sequence labeling,
based on a hybrid generative and discrim-
inative approach. We define the objective
function of our hybrid model, which is writ-
ten in log-linear form, by discriminatively
combining discriminative structured predic-
tor(s) with generative model(s) that incor-
porate unlabeled data. Then, unlabeled
data is used in a generative manner to in-
crease the sum of the discriminant functions
for all outputs during the parameter estima-
tion. Experiments on named entity recogni-
tion (CoNLL-2003) and syntactic chunking
(CoNLL-2000) data show that our hybrid
model significantly outperforms the state-
of-the-art performance obtained with super-
vised SOL methods, such as conditional ran-
dom fields (CRFs).
1 Introduction
Structured output learning (SOL) methods, which
attempt to optimize an interdependent output space
globally, are important methodologies for certain
natural language processing (NLP) tasks such as
part-of-speech tagging, syntactic chunking (Chunk-
ing) and named entity recognition (NER), which are
also referred to as sequence labeling tasks. When we
consider the nature of these sequence labeling tasks,
a semi-supervised approach appears to be more nat-
ural and appropriate. This is because the number of
features and parameters typically become extremely
large, and labeled examples can only sparsely cover
the parameter space, even if thousands of labeled ex-
amples are available. In fact, many attempts have re-
cently been made to develop semi-supervised SOL
methods (Zhu et al, 2003; Li and McCallum, 2005;
Altun et al, 2005; Jiao et al, 2006; Brefeld and
Scheffer, 2006).
With the generative approach, we can easily in-
corporate unlabeled data into probabilistic models
with the help of expectation-maximization (EM) al-
gorithms (Dempster et al, 1977). For example, the
Baum-Welch algorithm is a well-known algorithm
for training a hidden Markov model (HMM) of se-
quence learning. Generally, with sequence learning
tasks such as NER and Chunking, we cannot expect
to obtain better performance than that obtained us-
ing discriminative approaches in supervised learning
settings.
In contrast to the generative approach, with the
discriminative approach, it is not obvious how un-
labeled training data can be naturally incorporated
into a discriminative training criterion. For ex-
ample, the effect of unlabeled data will be elimi-
nated from the objective function if the unlabeled
data is directly used in traditional i.i.d. conditional-
probability models. Nevertheless, several attempts
have recently been made to incorporate unlabeled
data in the discriminative approach. An approach
based on pairwise similarities, which encourage
nearby data points to have the same class label, has
been proposed as a way of incorporating unlabeled
data discriminatively (Zhu et al, 2003; Altun et al,
2005; Brefeld and Scheffer, 2006). However, this
approach generally requires joint inference over the
whole data set for prediction, which is not practi-
cal as regards the large data sets used for standard
sequence labeling tasks in NLP. Another discrim-
inative approach to semi-supervised SOL involves
the incorporation of an entropy regularizer (Grand-
791
valet and Bengio, 2004). Semi-supervised condi-
tional random fields (CRFs) based on a minimum
entropy regularizer (SS-CRF-MER) have been pro-
posed in (Jiao et al, 2006). With this approach, the
parameter is estimated to maximize the likelihood of
labeled data and the negative conditional entropy of
unlabeled data. Therefore, the structured predictor
is trained to separate unlabeled data well under the
entropy criterion by parameter estimation.
In contrast to these previous studies, this paper
proposes a semi-supervised SOL framework based
on a hybrid generative and discriminative approach.
A hybrid approach was first proposed in a super-
vised learning setting (Raina et al, 2003) for text
classification. (Fujino et al, 2005) have developed a
semi-supervised approach by discriminatively com-
bining a supervised classifier with generative mod-
els that incorporate unlabeled data. We extend this
framework to the structured output domain, specifi-
cally for sequence labeling tasks. Moreover, we re-
formalize the objective function to allow the incor-
poration of discriminative models (structured pre-
dictors) trained from labeled data, since the original
framework only considers the combination of gen-
erative classifiers. As a result, our hybrid model can
significantly improve on the state-of-the-art perfor-
mance obtained with supervised SOL methods, such
as CRFs, even if a large amount of labeled data is
available, as shown in our experiments on CoNLL-
2003 NER and CoNLL-2000 Chunking data. In
addition, compared with SS-CRF-MER, our hybrid
model has several good characteristics including a
low calculation cost and a robust optimization in
terms of a sensitiveness of hyper-parameters. This
is described in detail in Section 5.3.
2 Supervised SOL: CRFs
This paper focuses solely on sequence labeling
tasks, such as named entity recognition (NER) and
syntactic chunking (Chunking), as SOL problems.
Thus, let x=(x1, . . . , xS)?X be an input sequence,
and y=(y0, . . . , yS+1)?Y be a particular output se-
quence, where y0 and yS+1 are special fixed labels
that represent the beginning and end of a sequence.
As regards supervised sequence learning, CRFs
are recently introduced methods that constitute flex-
ible and powerful models for structured predictors
based on undirected graphical models that have been
globally conditioned on a set of inputs (Lafferty
et al, 2001). Let ? be a parameter vector and
f(ys?1, ys,x) be a (local) feature vector obtained
from the corresponding position s given x. CRFs
define the conditional probability, p(y|x), as being
proportional to a product of potential functions on
the cliques. That is, p(y|x) on a (linear-chain) CRF
can be defined as follows:
p(y|x;?) = 1Z(x)
S+1?
s=1
exp(? ? f (ys?1, ys,x)).
Z(x) =
?
y
?S+1
s=1 exp(? ? f(ys?1, ys,x)) is a nor-
malization factor over all output values, Y , and is
also known as the partition function.
For parameter estimation (training), given labeled
data Dl = {(xk,yk)}Kk=1, the Maximum a Posteri-
ori (MAP) parameter estimation, namely maximiz-
ing log p(?|Dl), is now the most widely used CRF
training criterion. Thus, we maximize the following
objective function to obtain optimal ?:
LCRF(?) =
?
k
[
? ?
?
s
f s ? logZ(x
k)
]
+ log p(?), (1)
where f s is an abbreviation of f(ys?1, ys,x) and
p(?) is a prior probability distribution of ?. A
gradient-based optimization algorithm such as L-
BFGS (Liu and Nocedal, 1989) is widely used for
maximizing Equation (1). The gradient of Equation
(1) can be written as follows:
?LCRF(?) =
?
k
Ep?(yk,xk;?)
[?
s
f s
]
?
?
k
Ep(Y|xk;?)
[?
s
f s
]
+? log p(?).
Calculating Ep(Y|x,?) as well as the partition func-
tion Z(x) is not always tractable. However, for
linear-chain CRFs, a dynamic programming algo-
rithm similar in nature to the forward-backward al-
gorithm in HMMs has already been developed for
an efficient calculation (Lafferty et al, 2001).
For prediction, the most probable output, that is,
y? = argmaxy?Y p(y|x;?), can be efficiently ob-
tained by using the Viterbi algorithm.
3 Hybrid Generative and Discriminative
Approach to Semi-Supervised SOL
In this section, we describe our formulation of a
hybrid approach to SOL and a parameter estima-
tion method for sequence predictors. We assume
792
that we have a set of labeled and unlabeled data,
D = {Dl,Du}, where Dl = {(xn,yn)}Nn=1 and
Du = {xm}Mm=1.
Let us assume that we have I-units of discrimina-
tive models, pDi , and J-units of generative models,
pGj . Our hybrid model for a structured predictor is
designed by the discriminative combination of sev-
eral joint probability densities of x and y, p(x,y).
That is, the posterior probability of our hybrid model
is defined by providing the log-values of p(x,y) as
the features of a log-linear model, such that:
R(y|x;?,?,?)
=
?
i p
D
i (x,y;?i)?i
?
j p
G
j (x,y; ?j)?j?
y
?
i p
D
i (x,y;?i)?i
?
j p
G
j (x,y; ?j)?j
=
?
i p
D
i (y|x;?i)?i
?
j p
G
j (x,y; ?j)?j?
y
?
i p
D
i (y|x;?i)?i
?
j p
G
j (x,y; ?j)?j
.
(2)
Here, ? = {{?i}Ii=1, {?j}
I+J
j=I+1} represents the
discriminative combination weight of each model
where ?i,?j? [0, 1]. Moreover, ?={?i}Ii=1 and ?=
{?j}Jj=1 represent model parameters of individual
models estimated from labeled and unlabeled data,
respectively. Using pD(x,y) = pD(y|x)pD(x), we
can derive the third line from the second line, where
pDi (x;?i)?i for all i are canceled out. Thus, our hy-
brid model is constructed by combining discrimina-
tive models, pDi (y|x;?i), with generative models,
pGj (x,y;?j).
Hereafter, let us assume that our hybrid model
consists of CRFs for discriminative models, pDi , and
HMMs for generative models, pGj , shown in Equa-
tion (2), since this paper focuses solely on sequence
modeling. For HMMs, we consider a first order
HMM defined in the following equation:
p(x,y|?) =
S+1?
s=1
?ys?1,ys?ys,xs ,
where ?ys?1,ys and ?ys,xs represent the transition
probability between states ys?1 and ys and the sym-
bol emission probability of the s-th position of the
corresponding input sequence, respectively, where
?yS+1,xS+1 = 1.
It can be seen that the formalization in the log-
linear combination of our hybrid model is very sim-
ilar to that of LOP-CRFs (Smith et al, 2005). In
fact, if we only use a combination of discriminative
models (CRFs), which is equivalent to ?j = 0 for
all j, we obtain essentially the same objective func-
tion as that of the LOP-CRFs. Thus, our framework
can also be seen as an extension of LOP-CRFs that
enables us to incorporate unlabeled data.
3.1 Discriminative Combination
For estimating the parameter ?, let us assume that
we already have discriminatively trained models on
labeled data, pDi (y|x;?i). We maximize the fol-
lowing objective function for estimating parameter
? under a fixed ?:
LHySOL(?|?) =
?
n
logR(yn|xn;?,?,?)+log p(?). (3)
where p(?) is a prior probability distribution of ?.
The value of ? providing a global maximum of
LHySOL(?|?) is guaranteed under an arbitrary fixed
value in the ? domain, since LHySOL(?|?) is a con-
cave function of ?. Thus, we can easily maximize
Equation (3) by using a gradient-based optimization
algorithm such as (bound constrained) L-BFGS (Liu
and Nocedal, 1989).
3.2 Incorporating Unlabeled Data
We cannot directly incorporate unlabeled data for
discriminative training such as Equation (3) since
the correct outputs y for unlabeled data are un-
known. On the other hand, generative approaches
can easily deal with unlabeled data as incomplete
data (data with missing variable y) by using a mix-
ture model. A well-known way to achieve this in-
corporation is to maximize the log likelihood of un-
labeled data with respect to the marginal distribution
of generative models as
L(?) =
?
m
log
?
y
p(xm,y; ?).
In fact, (Nigam et al, 2000) have reported that using
unlabeled data with a mixture model can improve
the text classification performance.
According to Bayes? rule, p(y|x;?) ?
p(x,y;?), the discriminant functions of gener-
ative classifiers are provided by generative models
p(x,y;?). Therefore, we can regard L(?) as the
logarithm of the sum of discriminant functions for
all missing variables y of unlabeled data. Following
this view, we can directly incorporate unlabeled
data into our hybrid model by maximizing the
793
discriminant functions g of our hybrid model in
the same way as for a mixture model as explained
above. Thus, we maximize the following objective
function for estimating the model parameters ? for
generative models of unlabeled data:
G(?|?) =
?
m
log
?
y
g(xm,y;?) + log p(?). (4)
where p(?) is a prior probability distribution of ?.
Here, the discriminant function g of output y given
input x in our hybrid model can be obtained by the
numerator on the third line of Equation (2), since the
denominator does not affect the determination of y,
that is,
g(x,y;?) =
?
i
pDi (y|x;?i)?i
?
j
pGj (x,y; ?j)?j .
Under a fixed ?, we can estimate the local max-
imum of G(?|?) around the initialized value of ?
by an iterative computation such as the EM algo-
rithm (Dempster et al, 1977). Let ??? and ?? be
estimates of ? in the next and current steps, respec-
tively. Using Jensen?s inequality, log a ? a ? 1,
we obtain a Q-function that satisfies the inequality
G(???|?)?G(??|?)?Q(???,??;?)?Q(??,??;?),
such that
Q(???,??;?)
=
?
j
?j
?
m
?
y
R(y|xm;?,??,?) log pGj (xm,y;???)
+ log p(???).
(5)
Since Q(??,??;?) is independent of ???, we can
improve the value of G(?|?) by computing ??? to
maximize Q(???,??;?). We can obtain a ? es-
timate by iteratively performing this update while
G(?|?) is hill climbing.
As shown in Equation (5), R is used for estimat-
ing the parameter ?. The intuitive effect of maxi-
mizing Equation (4) is similar to performing ?soft-
clustering?. That is, unlabeled data is clustered with
respect to the R distribution, which also includes in-
formation about labeled data, under the constraint of
generative model structures.
3.3 Parameter Estimation Procedure
According to our definition, the ? and ? estima-
tions are mutually dependent. That is, the param-
eters of the hybrid model, ?, should be estimated
1.Given training set: Du = {xm}Mm=1 and
Dl = {D?l = {(xk,yk)}Kk=1, D??l = {(xn,yn)}Nn=1}
2.Compute ?, using D?l.
3.Initialize ?(0), ?(0) and t ? 0.
4.Perform the following until |?
(t+1)??(t)|
|?(t)| < ?.
4.1. Compute ?(t+1) to maximize Equation (4)
under fixed ?(t) and ? using Du.
4.2. Compute ?(t+1) to maximize Equation (3)
under fixed ?(t+1) and ? using D??l .
4.3. t ? t + 1.
5.Output a structured predictor R(y|x,?,?(t),?(t)).
Figure 1: Algorithm of learning model parameters
used in our hybrid model.
using Equation (3) with a fixed ?, while the param-
eters of the generative models, ?, should be esti-
mated using Equation (4) with a fixed ?. As a solu-
tion to our parameter estimation, we search for the
? and ? that maximize LHySOL(?|?) and G(?|?)
simultaneously. For this search, we compute ? and
? by maximizing the objective functions shown in
Equations (4) and (3) iteratively and alternately. We
summarize the algorithm for estimating these model
parameters in Figure 1.
Note that during the ? estimation (procedure 4.2
in Figure 1), ? can be over-fitted to the labeled train-
ing data if we use the same labeled training data as
used for the? estimation. There are several possible
ways to reduce this over-fit. In this paper, we select
one of the simplest; we divide the labeled training
data Dl into two distinct sets D?l and D??l . Then, D?l
and D??l are individually used for estimating ? and
?, respectively. In our experiments, we divide the
labeled training data Dl so that 4/5 is used for D?l
and the remaining 1/5 for D??l .
3.4 Efficient Parameter Estimation Algorithm
Let NR(x) represent the denominator of Equation
(2), that is the normalization factor of R. We can
rearrange Equation (2) as follows:
R(y|x;?,?,?) =
?
s
?
i
[
V Di,s
]?i ?
j
[
V Gj,s
]?j
NR(x)
?
i[Zi(x)]?i
, (6)
where V Di,s represents the potential function of the
s-th position of the sequence in the i-th CRF and
V Gj,s represents the probability of the s-th position
in the j-th HMM, that is, V Di,s = exp(?i ? f s) and
V Gj,s = ?ys?1,ys?ys,xs , respectively. See the Ap-
pendix for the derivation of Equation (6) from Equa-
tion (2).
794
To estimate ?(t+1), namely procedure 4.2 in Fig-
ure 1, we employ the derivatives with respect to ?i
and ?j shown in Equation (6), which are the parame-
ters of the discriminative and generative models, re-
spectively. Thus, we obtain the following derivatives
with respect to ?i:
?LHySOL(?|?)
??i
=
?
n
log pDi (yn|xn) +
?
n
logZDi (xn)
?
?
n
ER(Y|xn;?,?,?)
[?
s
log V Di,s
]
.
The first and second terms are constant during it-
erative procedure 4 in our optimization algorithm
shown in Figure 1. Thus, we only need to calcu-
late these values once at the beginning of proce-
dure 4. Let ?s(y) and ?s(y) represent the forward
and backward state costs at position s with output
y for corresponding input x. Let Vs(y, y?) repre-
sent the products of the total value of the transition
cost between s?1 and s with labels y and y? in the
corresponding input sequence, that is, Vs(y, y?) =?
i[V Di,s(y, y?)]?i
?
j [V Gj,s(y, y?)]?j . The third term,
which indicates the expectation of potential func-
tions, can be rewritten in the form of a forward-
backward algorithm, that is,
ER(Y|x;?,?,?)
[?
s
log V Di,s
]
= 1ZR(x)
?
s
?
y,y?
?s?1(y)Vs(y, y?)?s(y
?) log V Di,s(y, y?),
(7)
where ZR(x) represents the partition function of our
hybrid model, that is, ZR(x)=NR(x)
?
i[Zi(x)]?i .
Hence, the calculation of derivatives with respect to
?i is tractable since we can incorporate the same
forward-backward algorithm as that used in a stan-
dard CRF.
Then, the derivatives with respect to ?j , which are
the parameters of generative models, can be written
as follows:
?LHySOL(?|?)
??j
=
?
n
log pGj (xn,yn)?
?
n
ER(Y|xn;?,?,?)
[?
s
log V Gj,s
]
.
Again, the second term, which indicates the expec-
tation of transition probabilities and symbol emis-
sion probabilities, can be rewritten in the form of a
forward-backward algorithm in the same manner as
?i, where the only difference is that V Di,s is substi-
tuted by V Gj,s in Equation (7).
To estimate?(t+1), which is procedure 4.1 in Fig-
ure 1, the same forward-backward algorithm as used
in standard HMMs is available since the form of our
Q-function shown in Equation (5) is the same as that
of standard HMMs. The only difference is that our
method uses marginal probabilities given by R in-
stead of the p(x,y;?) of standard HMMs.
Therefore, only a forward-backward algorithm is
required for the efficient calculation of our param-
eter estimation process. Note that even though our
hybrid model supports the use of a combination of
several generative and discriminative models, we
only need to calculate the forward-backward algo-
rithm once for each sample during optimization pro-
cedures 4.1 and 4.2. This means that the required
number of executions of the forward-backward al-
gorithm for our parameter estimation is independent
of the number of models used in the hybrid model.
In addition, after training, we can easily merge all
the parameter values in a single parameter vector.
This means that we can simply employ the Viterbi-
algorithm for evaluating unseen samples, as well as
that of standard CRFs, without any additional cost.
4 Experiments
We examined our hybrid model (HySOL) by ap-
plying it to two sequence labeling tasks, named
entity recognition (NER) and syntactic chunking
(Chunking). We used the same Chunking and
?English? NER data as those used for the shared
tasks of CoNLL-2000 (Tjong Kim Sang and Buch-
holz, 2000) and CoNLL-2003 (Tjong Kim Sang and
Meulder, 2003), respectively.
For the baseline method, we performed a condi-
tional random field (CRF), which is exactly the same
training procedure described in (Sha and Pereira,
2003) with L-BFGS. Moreover, LOP-CRF (Smith et
al., 2005) is also compared with our hybrid model,
since the formalism of our hybrid model can be seen
as an extension of LOP-CRFs as described in Sec-
tion 3. For CRF, we used the Gaussian prior as
the second term on the RHS in Equation (1), where
?2 represents the hyper-parameter in the Gaussian
prior. In contrast, for LOP-CRF and HySOL, we
used the Dirichlet priors as the second term on the
795
?1 f(words), f(lwords), f(poss), f(wtypes),
f(poss?1, poss), f(wtypes?1, wtypes),
f(poss, poss+1), f(wtypes, wtypes+1),
f(pref1s), f(pref2s), f(pref3s), f(pref4s),
f(suf1s), f(suf2s), f(suf3s), f(suf4s)
?2 f(words), f(lwords), f(poss), f(wtypes),
f(words?1), f(lwords?1), f(poss?1), f(wtypes?1),
f(words?2), f(lwords?2), f(poss?2), f(wtypes?2),
f(poss?2, poss?1), f(wtypes?2, wtypes?1)
?3 f(words), f(lwords), f(poss), f(wtypes),
f(words+1), f(lwords+1), f(poss+1), f(wtypes+1),
f(words+2), f(lwords+2), f(poss+2), f(wtypes+2),
f(poss+1, poss+2), f(wtypes+1, wtypes+2)
?4 all of the above
lword : lowercase of word, wtype : ?word type?
pref1-4: 1-4 character prefix of word
suf1-4 : 1-4 character suffix of word
Table 1: Features used in NER experiments
RHS in Equations (3), and (4), where ? and ? are the
hyper-parameters in each Dirichlet prior.
4.1 Named Entity Recognition Experiments
The English NER data consists of 203,621, 51,362
and 46,435 words from 14,987, 3,466 and 3,684 sen-
tences in training, development and test data, re-
spectively, with four named entity tags, PERSON,
LOCATION, ORGANIZATION and MISC, plus the
?O? tag. The unlabeled data consists of 17,003,926
words from 1,029,122 sentences. These data sets are
exactly the same as those provided for the shared
task of CoNLL-2003.
We slightly extended the feature set of the sup-
plied data by adding feature types such as ?word
type?, and word prefix and suffix. Examples of
?word type? include whether the word is capitalized,
contains digit or contains punctuation, which basi-
cally follows the baseline features of (Sutton et al,
2006) without regular expressions. Note that, unlike
several previous studies, we did not employ addi-
tional information from external resources such as
gazetteers. All our features can be automatically ex-
tracted from the supplied data.
For LOP-CRF and HySOL, we used four base dis-
criminative models trained by CRFs with different
feature sets. Table 1 shows the feature sets we used
for training these models. The design of these fea-
ture sets was derived from a suggestion in (Smith et
al., 2005), which exhibited the best performance in
the several feature division. Note that the CRF for
the comparison method was trained by using all fea-
?1 f(words), (poss),
f(words?1, words), f(poss?1, poss),
f(words, words+1), f(poss, poss+1)
?2 f(words), (poss),
f(words?1), f(poss?1), f(words?2), f(poss?2),
f(words?2, words?1), f(poss?2, poss?1)
?3 f(words), (poss),
f(words+1), f(poss+1), f(words+2), f(poss+2),
f(words+1, words+2), f(poss+1, poss+2)
?4 all of the above
Table 2: Features used in Chunking experiments
ture types, namely the same as ?4.
As we explained in Section 3.3, for training
HySOL, the parameters of four discriminative mod-
els, ?, were trained from 4/5 of the labeled training
data, and ? were trained from remaining 1/5. For
the features of the generative models, we used all of
the feature types shown in Figure 1. Note that one
feature type corresponds to one HMM. Thus, each
HMM maintains to consist of a non-overlapping fea-
ture set since each feature type only generates one
symbol per state.
4.2 Syntactic Chunking Experiments
CoNLL-2000 Chunking data was obtained from the
Wall Street Journal (WSJ) corpus: sections 15-18 as
training data (8,936 sentences and 211,727 words),
and section 20 as test data (2,012 sentences and
47,377 words), with 11 different chunk-tags, such
as NP and VP plus the ?O? tag, which represents the
region outside any target chunk.
For LOP-CRF and HySOL, we also used four
base discriminative models trained by CRFs with
different feature sets. Table 2 shows the feature set
we used in the Chunking experiments. We used the
feature set of the supplied data without any exten-
sion of additional feature types.
To train HySOL, we used the same unlabeled data
as used for our NER experiments (17,003,926 words
from the Reuters corpus). Moreover, the division of
the labeled training data and the feature set of the
generative models were derived in the same man-
ner as our NER experiments (see Section 4.1). That
is, we divided the labeled training data into 4/5 for
estimating ? and 1/5 for estimating ?; one feature
type shown in Table 2 is assigned in one generative
model.
796
methods (hyper-params) F?=1 (gain) Sent (gain)
CRF (?2=100.0) 84.70 - 78.30 -
(4/5 labeled data, ?2=100.0) 83.74 (-0.96) 77.06 (-1.24)
LOP-CRF (??=0.1) 84.90 (+0.20) 79.02 (+0.72)
HySOL (??=0.1,??=0.0001) 87.20 (+2.50) 81.19 (+2.89)
(w/o prior) 86.86 (+2.16) 80.75 (+2.45)
w/o pGj ?j ( ??=1.0) 84.56 (-0.14) 78.23 (-0.07)
Table 3: NER performance (CoNLL-2003)
methods (hyper-params) F?=1 (gain) Sent (gain)
CRF (?2=10.0) 93.87 - 59.84 -
(4/5 labeled data, ?2=10.0) 93.70 (-0.17) 58.85 (-0.99)
LOP-CRF (??=0.1) 93.91 (+0.04) 60.34 (+0.50)
HySOL (??=1.0,??=0.0001) 94.30 (+0.43) 61.73 (+1.89)
(w/o prior) 94.17 (+0.30) 61.23 (+1.39)
w/o pGj ?j (??=1.0) 93.84 (-0.03) 59.74 (-0.10)
Table 4: Chunking performance (CoNLL-2000)
5 Results and Discussion
We evaluated the performance in terms of the F?=1
score, which is the evaluation measure used in
CoNLL-2000 and 2003, and sentence accuracy,
since all the methods in our experiments optimize
sequence loss. Tables 3 and 4 show the results of
the NER and Chunking experiments, respectively.
The F?=1 and ?Sent? columns show the performance
evaluated using the F?=1 score and sentence accu-
racy, respectively. ?2, ? and ?, which are the hyper-
parameters in Gaussian or Dirichlet priors, are se-
lected from a certain value set by using a develop-
ment set1, that is, ?2 ? {0.01, 0.1, 1, 10, 100, 1000},
? ? 1 = ?? ? {0.01, 0.1, 1, 10} and ? ? 1 = ?? ?
{0.00001, 0.0001, 0.001, 0.01}. The second rows of
CRF in Tables 3 and 4 represent the performance of
base discriminative models used in HySOL with all
the features, which are trained with 4/5 of the la-
beled training data. The third rows of HySOL show
the performance obtained without using generative
models (unlabeled data). The model itself is essen-
tially the same as LOP-CRFs. However the perfor-
mance in the third HySOL rows was consistently
lower than that of LOP-CRF since the discrimina-
tive models in HySOL are trained with 4/5 labeled
data.
As shown in Tables 3 and 4, HySOL signifi-
1Chunking (CoNLL-2000) data has no common develop-
ment set. Thus, our preliminary examination employed by using
4/5 labeled training data with the remaining 1/5 as development
data to determine the hyper-parameter values.


 
 
    
	 
      
 	 
   
 	 
   
 	 
   
       
      
      
      
 
  	 
   


 
 
   
  	 
   
	 
      
 	 
   
 	 
   
 	 
   
       
      
      
      
 
(a) NER (b) Chunking
Figure 2: Changes in the performance and the con-
vergence condition value (procedure 4 in Figure 1)
of HySOL.
cantly improved the performance of supervised set-
ting, CRF and LOP-CRF, as regards both NER and
Chunking experiments.
5.1 Impact of Incorporating Unlabeled Data
The contributions provided by incorporating unla-
beled data in our hybrid model can be seen by com-
parison with the performance of the first and third
rows in HySOL, namely a 2.64 point F-score and a
2.96 point sentence accuracy gain in the NER exper-
iments and a 0.46 point F-score and a 1.99 point sen-
tence accuracy gain in the Chunking experiments.
We believe there are two key ideas that enable
the unlabeled data in our approach to exhibit this
improvement compared with the the state-of-the-art
performance provided by discriminative models in
supervised settings. First, unlabeled data is only
used for optimizing Equation (4) to obtain a similar
effect to ?soft-clustering?, which can be calculated
without information about the correct output. Sec-
ond, by using a combination of generative models,
we can enhance the flexibility of the feature design
for unlabeled data. For example, we can handle ar-
bitrary overlapping features, similar to those used in
discriminative models, for unlabeled data by assign-
ing one feature type for one generative model as in
our experiments.
5.2 Impact of Iterative Parameter Estimation
Figure 2 shows the changes in the performance and
the convergence condition value of HySOL dur-
ing parameter estimation iteration in our NER and
Chunking experiments, respectively. As shown in
the figure, HySOL was able to reach the conver-
797
gence condition in a small number of iterations in
our experiments. Moreover, the change in the per-
formance remains quite stable during the iteration.
However, theoretically, our optimization procedure
is not guaranteed to converge in the ? and ? space,
since the optimization of ? has local maxima. Even
if we were unable to meet the convergence condi-
tion, we were easily able to obtain model parame-
ters by performing a sufficient fixed number of itera-
tions, and then select the parameters when Equation
(4) obtained the maximum objective value.
5.3 Comparison with SS-CRF-MER
When we consider semi-supervised SOL methods,
SS-CRF-MER (Jiao et al, 2006) is the most compet-
itive with HySOL, since both methods are defined
based on CRFs. We planned to compare the perfor-
mance with that of SS-CRF-MER in our NER and
Chunking experiments. Unfortunately, we failed to
implement SS-CRF-MER since it requires the use of
a slightly complicated algorithm, called the ?nested?
forward-backward algorithm.
Although, we cannot compare the performance,
our hybrid approach has several good characteris-
tics compared with SS-CRF-MER. First, it requires
a higher order algorithm, namely a ?nested? forward-
backward algorithm, for the parameter estimation of
unlabeled data whose time complexity is O(L3S2)
for each unlabeled data, where L and S represent the
output label size and unlabeled sample length, re-
spectively. Thus, our hybrid approach is more scal-
able for the size of unlabeled data, since HySOL
only needs a standard forward-backward algorithm
whose time complexity is O(L2S). In fact, we
still have a question as to whether SS-CRF-MER
is really scalable in practical time for such a large
amount of unlabeled data as used in our experi-
ments, which is about 680 times larger than that of
(Jiao et al, 2006). Scalability for unlabeled data
will become really important in the future, as it will
be natural to use millions or billions of unlabeled
data for further improvement. Second, SS-CRF-
MER has a sensitive hyper-parameter in the objec-
tive function, which controls the influence of the un-
labeled data. In contrast, our objective function only
has a hyper-parameter of prior distribution, which is
widely used for standard MAP estimation. More-
over, the experimental results shown in Tables 3 and
F?=1 additional resources
ASO-semi 89.31 unlabeled data (27M words)
(Ando and Zhang, 2005)
(Florian et al, 2003) 88.76 their own large gazetteers,
2M-word labeled data
(Chieu and Ng, 2003) 88.31 their own large gazetteers,
very elaborated features
HySOL 88.14 unlabeled data (17M words)
supplied gazetters
HySOL 87.20 unlabeled data (17M words)
Table 5: Previous top systems in NER (CoNLL-
2003) experiments
F?=1 additional resources
ASO-semi 94.39 unlabeled data
(Ando and Zhang, 2005) (15M words: WSJ)
HySOL 94.30 unlabeled data
(17M words: Reuters)
(Zhang et al, 2002) 94.17 full parser output
(Kudo and Matsumoto, 2001) 93.91 ?
Table 6: Previous top systems in Chunking
(CoNLL-2000) experiments
4 indicate that HySOL is rather robust with respect
to the hyper-parameter since we can obtain fairly
good performance without a prior distribution.
5.4 Comparison with Previous Top Systems
With respect to the performance of NER and Chunk-
ing tasks, the current best performance is reported
in (Ando and Zhang, 2005), which we refer to as
?ASO-semi?, as shown in Figures 5 and 6. ASO-
semi also incorporates unlabeled data solely for
the additional information in the same way as our
method. Unfortunately, our results could not reach
their level of performance, although the size and
source of the unlabeled data are not the same for cer-
tain reasons. First, (Ando and Zhang, 2005) does not
describe the unlabeled data used in their NER ex-
periments in detail, and second, we are not licensed
to use the TREC corpus including WSJ unlabeled
data that they used for their Chunking experiments
(training and test data for Chunking is derived from
WSJ). Therefore, we simply used the supplied unla-
beled data of the CoNLL-2003 shared task for both
NER and Chunking. If we consider the advantage of
our approach, our hybrid model incorporating gener-
ative models seems rather intuitive, since it is some-
times difficult to find out a design of effective auxil-
iary problems for the target problem.
Interestingly, the additional information obtained
798
F?=1 (gain)
HySOL (??=0.1,??=0.0001) 87.20 -
+ w/ F-score opt. (Suzuki et al, 2006) 88.02 (+0.82)
+ unlabeled data (17M ? 27M words) 88.41 (+0.39)
+ supplied gazetters 88.90 (+0.49)
+ add dev. set for estimating ? 89.27 (+0.37)
Table 7: The HySOL performance with the F-
score optimization technique and some additional
resources in NER (CoNLL-2003) experiments
F?=1 (gain)
HySOL (??=0.1,??=0.0001) 94.30 -
+ w/ F-score opt. (Suzuki et al, 2006) 94.36 (+0.06)
Table 8: The HySOL performance with the F-score
optimization technique on Chunking (CoNLL-2000)
experiments
from unlabeled data appear different from each
other. ASO-semi uses unlabeled data for construct-
ing auxiliary problems to find the ?shared structures?
of auxiliary problems that are expected to improve
the performance of the main problem. Moreover,
it is possible to combine both methods, for exam-
ple, by incorporating the features obtained with their
method in our base discriminative models, and then
construct a hybrid model using our method. There-
fore, there may be a possibility of further improving
the performance by this simple combination.
In NER, most of the top systems other than
ASO-semi boost performance by employing exter-
nal hand-crafted resources such as large gazetteers.
This is why their results are superior to those ob-
tained with HySOL. In fact, if we simply add the
gazetteers included in CoNLL-2003 supplied data as
features, HySOL achieves 88.14.
5.5 Applying F-score Optimization Technique
In addition, we can simply apply the F-score opti-
mization technique for the sequence labeling tasks
proposed in (Suzuki et al, 2006) to boost the
HySOL performance since the base discriminative
models pD(y|x) and discriminative combination,
namely Equation (3), in our hybrid model basically
uses the same optimization procedure as CRFs. Ta-
bles 7 and 8 show the F-score gain when we apply
the F-score optimization technique. As shown in the
Tables, the F-score optimization technique can eas-
ily improve the (F-score) performance without any
additional resources or feature engineering.
In NER, we also examined HySOL with addi-
tional resources to observe the performance gain.
The third row represents the performance when we
add approximately 10M words of unlabeled data (to-
tal 27M words)2 that are derived from 1996/11/15-
30 articles in Reuters corpus. Then, the fourth and
fifth rows represent the performance when we add
the supplied gazetters in the CoNLL-2003 data as
features, and adding development data as training
data of ?. In this case, HySOL achieved a com-
parable performance to that of the current best sys-
tem, ASO-semi, in both NER and Chunking exper-
iments even though the NER experiment is not a
fair comparison since we added additional resources
(gazetters and dev. set) that ASO-semi does not use
in training.
6 Conclusion and Future Work
We proposed a framework for semi-supervised SOL
based on a hybrid generative and discriminative ap-
proach. Experimental results showed that incorpo-
rating unlabeled data in a generative manner has
the power to further improve on the state-of-the-art
performance provided by supervised SOL methods
such as CRFs, with the help of our hybrid approach,
which discriminatively combines with discrimina-
tive models. In future we intend to investigate more
appropriate model and feature design for unlabeled
data, which may further improve the performance
achieved in our experiments.
Appendix
Let V Di,s = exp(? ? f s) and V Gj,s = ?ys?1,ys?ys,xs .
Equation (6) can be obtained by the following rear-
rangement of Equation (2) :
R(y|x;?,?,?)
=
?
i p
D
i (y|x,?i)?i
?
j p
G
j (x,y, ?j)?j?
y
?
i p
D
i (y|x,?i)?i
?
j p
G
j (x,y, ?j)?j
= 1NR(x)
?
i
[?
s V
D
i,s
Zi(x)
]?i?
j
[?
s
V Gj,s
]?j
= 1
NR(x)
?
i[Zi(x)]?i
?
i
[?
s
V Di,s
]?i?
j
[?
s
V Gj,s
]?j
= 1
NR(x)
?
i[Zi(x)]?i
?
s
?
i
[
V Di,s
]?i ?
j
[
V Gj,s
]?j .
2In order to keep the consistency of POS tags, we re-
attached POS tags of the supplied data set and new 10M words
of unlabeled data using a POS tagger trained from WSJ corpus.
799
References
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum Margin Semi-Supervised Learning for Struc-
tured Variables. In Proc. of NIPS*2005.
R. Ando and T. Zhang. 2005. A High-Performance
Semi-Supervised Learning Method for Text Chunking.
In Proc. of ACL-2005, pages 1?9.
U. Brefeld and T. Scheffer. 2006. Semi-Supervised
Learning for Structured Output Variables. In Proc. of
ICML-2006.
H. L. Chieu and Hwee T. Ng. 2003. Named Entity
Recognition with a Maximum Entropy Approach. In
Proc. of CoNLL-2003, pages 160?163.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39:1?38.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named Entity Recognition through Classifier Combi-
nation. In Proc. of CoNLL-2003, pages 168?171.
A. Fujino, N. Ueda, and K. Saito. 2005. A Hybrid Gen-
erative/Discriminative Approach to Semi-Supervised
Classifier Design. In Proc. of AAAI-05, pages 764?
769.
Y. Grandvalet and Y. Bengio. 2004. Semi-Supervised
Learning by Entropy Minimization. In Proc. of
NIPS*2004, pages 529?536.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-Supervised Conditional Random
Fields for Improved Sequence Segmentation and La-
beling. In Proc. of COLING/ACL-2006, pages 209?
216.
T. Kudo and Y. Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. of NAACL 2001, pages
192?199.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 282?289.
W. Li and A. McCallum. 2005. Semi-Supervised Se-
quence Modeling with Syntactic Topic Models. In
Proc. of AAAI-2005, pages 813?818.
D. C. Liu and J. Nocedal. 1989. On the Limited Memory
BFGS Method for Large Scale Optimization. Math.
Programming, Ser. B, 45(3):503?528.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
2000. Text Classification from Labeled and Unlabeled
Documents using EM. Machine Learning, 39:103?
134.
R. Raina, Y. Shen, A. Y. Ng, and A. McCallum. 2003.
Classification with Hybrid Generative/Discriminative
Models. In Proc. of NIPS*2003.
F. Sha and F. Pereira. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proc. of HLT/NAACL-2003,
pages 213?220.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarith-
mic Opinion Pools for Conditional Random Fields. In
Proc. of ACL-2005, pages 10?17.
C. Sutton, M. Sindelar, and A. McCallum. 2006. Reduc-
ing Weight Undertraining in Structured Discriminative
Learning. In Proc. of HTL-NAACL 2006, pages 89?95.
J. Suzuki, E. McDermott, and H. Isozoki. 2006. Training
Conditional Random Fields with Multivariate Evalua-
tion Measure. In Proc. of COLING/ACL-2006, pages
217?224.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In
Proc. of CoNLL-2000 and LLL-2000, pages 127?132.
E. T. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proc. of
CoNLL-2003, pages 142?147.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
Chunking based on a Generalization of Winnow. Ma-
chine Learning Research, 2:615?637.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
Supervised Learning using Gaussian Fields and Har-
monic Functions. In Proc.of ICML-2003, pages 912?
919.
800
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551?560,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
An Empirical Study of Semi-supervised Structured Conditional Models
for Dependency Parsing
Jun Suzuki, Hideki Isozaki
NTT CS Lab., NTT Corp.
Kyoto, 619-0237, Japan
jun@cslab.kecl.ntt.co.jp
isozaki@cslab.kecl.ntt.co.jp
Xavier Carreras, and Michael Collins
MIT CSAIL
Cambridge, MA 02139, USA
carreras@csail.mit.edu
mcollins@csail.mit.edu
Abstract
This paper describes an empirical study
of high-performance dependency parsers
based on a semi-supervised learning ap-
proach. We describe an extension of semi-
supervised structured conditional models
(SS-SCMs) to the dependency parsing
problem, whose framework is originally
proposed in (Suzuki and Isozaki, 2008).
Moreover, we introduce two extensions re-
lated to dependency parsing: The first ex-
tension is to combine SS-SCMs with an-
other semi-supervised approach, described
in (Koo et al, 2008). The second exten-
sion is to apply the approach to second-
order parsing models, such as those de-
scribed in (Carreras, 2007), using a two-
stage semi-supervised learning approach.
We demonstrate the effectiveness of our
proposed methods on dependency parsing
experiments using two widely used test
collections: the Penn Treebank for En-
glish, and the Prague Dependency Tree-
bank for Czech. Our best results on
test data in the above datasets achieve
93.79% parent-prediction accuracy for En-
glish, and 88.05% for Czech.
1 Introduction
Recent work has successfully developed depen-
dency parsing models for many languages us-
ing supervised learning algorithms (Buchholz and
Marsi, 2006; Nivre et al, 2007). Semi-supervised
learning methods, which make use of unlabeled
data in addition to labeled examples, have the po-
tential to give improved performance over purely
supervised methods for dependency parsing. It
is often straightforward to obtain large amounts
of unlabeled data, making semi-supervised ap-
proaches appealing; previous work on semi-
supervised methods for dependency parsing in-
cludes (Smith and Eisner, 2007; Koo et al, 2008;
Wang et al, 2008).
In particular, Koo et al (2008) describe a
semi-supervised approach that makes use of clus-
ter features induced from unlabeled data, and gives
state-of-the-art results on the widely used depen-
dency parsing test collections: the Penn Tree-
bank (PTB) for English and the Prague Depen-
dency Treebank (PDT) for Czech. This is a very
simple approach, but provided significant perfor-
mance improvements comparing with the state-
of-the-art supervised dependency parsers such as
(McDonald and Pereira, 2006).
This paper introduces an alternative method for
semi-supervised learning for dependency parsing.
Our approach basically follows a framework pro-
posed in (Suzuki and Isozaki, 2008). We extend it
for dependency parsing, which we will refer to as
a Semi-supervised Structured Conditional Model
(SS-SCM). In this framework, a structured condi-
tional model is constructed by incorporating a se-
ries of generative models, whose parameters are
estimated from unlabeled data. This paper de-
scribes a basic method for learning within this ap-
proach, and in addition describes two extensions.
The first extension is to combine our method with
the cluster-based semi-supervised method of (Koo
et al, 2008). The second extension is to apply the
approach to second-order parsing models, more
specifically the model of (Carreras, 2007), using
a two-stage semi-supervised learning approach.
We conduct experiments on dependency parsing
of English (on Penn Treebank data) and Czech (on
the Prague Dependency Treebank). Our experi-
ments investigate the effectiveness of: 1) the basic
SS-SCM for dependency parsing; 2) a combina-
tion of the SS-SCM with Koo et al (2008)?s semi-
supervised approach (even in the case we used the
same unlabeled data for both methods); 3) the two-
stage semi-supervised learning approach that in-
551
corporates a second-order parsing model. In ad-
dition, we evaluate the SS-SCM for English de-
pendency parsing with large amounts (up to 3.72
billion tokens) of unlabeled data .
2 Semi-supervised Structured
Conditional Models for Dependency
Parsing
Suzuki et al (2008) describe a semi-supervised
learning method for conditional random fields
(CRFs) (Lafferty et al, 2001). In this paper we
extend this method to the dependency parsing
problem. We will refer to this extended method
as Semi-supervised Structured Conditional Mod-
els (SS-SCMs). The remainder of this section de-
scribes our approach.
2.1 The Basic Model
Throughout this paper we will use x to denote an
input sentence, and y to denote a labeled depen-
dency structure. Given a sentence x with n words,
a labeled dependency structure y is a set of n de-
pendencies of the form (h,m, l), where h is the
index of the head-word in the dependency, m is
the index of the modifier word, and l is the label
of the dependency. We use h = 0 for the root of
the sentence. We assume access to a set of labeled
training examples, {x
i
,y
i
}
N
i=1
, and in addition a
set of unlabeled examples, {x
?
i
}
M
i=1
.
In conditional log-linear models for dependency
parsing (which are closely related to conditional
random fields (Lafferty et al, 2001)), a distribu-
tion over dependency structures for a sentence x
is defined as follows:
p(y|x) =
1
Z(x)
exp{g(x,y)}, (1)
where Z(x) is the partition function, w is a pa-
rameter vector, and
g(x,y) =
?
(h,m,l)?y
w ? f(x, h,m, l)
Here f(x, h,m, l) is a feature vector represent-
ing the dependency (h,m, l) in the context of the
sentence x (see for example (McDonald et al,
2005a)).
In this paper we extend the definition of g(x,y)
to include features that are induced from unlabeled
data. Specifically, we define
g(x,y) =
?
(h,m,l)?y
w ? f(x, h,m, l)
+
?
(h,m,l)?y
k
?
j=1
v
j
q
j
(x, h,m, l). (2)
In this model v
1
, . . . , v
k
are scalar parameters that
may be positive or negative; q
1
. . . q
k
are func-
tions (in fact, generative models), that are trained
on unlabeled data. The v
j
parameters will dictate
the relative strengths of the functions q
1
. . . q
k
, and
will be trained on labeled data.
For convenience, we will use v to refer to the
vector of parameters v
1
. . . v
k
, and q to refer to the
set of generative models q
1
. . . q
k
. The full model
is specified by values for w,v, and q. We will
write p(y|x;w,v,q) to refer to the conditional
distribution under parameter values w,v,q.
We will describe a three-step parameter estima-
tion method that: 1) initializes the q functions
(generative models) to be uniform distributions,
and estimates parameter values w and v from la-
beled data; 2) induces new functions q
1
. . . q
k
from
unlabeled data, based on the distribution defined
by the w,v,q values from step (1); 3) re-estimates
w and v on the labeled examples, keeping the
q
1
. . . q
k
from step (2) fixed. The end result is a
model that combines supervised training with gen-
erative models induced from unlabeled data.
2.2 The Generative Models
We now describe how the generative models
q
1
. . . q
k
are defined, and how they are induced
from unlabeled data. These models make direct
use of the feature-vector definition f(x,y) used in
the original, fully supervised, dependency parser.
The first step is to partition the d fea-
tures in f(x,y) into k separate feature vectors,
r
1
(x,y) . . . r
k
(x,y) (with the result that f is the
concatenation of the k feature vectors r
1
. . . r
k
). In
our experiments on dependency parsing, we parti-
tioned f into up to over 140 separate feature vec-
tors corresponding to different feature types. For
example, one feature vector r
j
might include only
those features corresponding to word bigrams in-
volved in dependencies (i.e., indicator functions
tied to the word bigram (x
m
, x
h
) involved in a de-
pendency (x, h,m, l)).
We then define a generative model that assigns
a probability
q
?
j
(x, h,m, l) =
d
j
?
a=1
?
r
j,a
(x,h,m,l)
j,a
(3)
to the d
j
-dimensional feature vector r
j
(x, h,m, l).
The parameters of this model are ?
j,1
. . . ?
j,d
j
;
552
they form a multinomial distribution, with the con-
straints that ?
j,a
? 0, and
?
a
?
j,a
= 1. This
model can be viewed as a very simple (naive-
Bayes) model that defines a distribution over fea-
ture vectors r
j
? R
d
j
. The next section describes
how the parameters ?
j,a
are trained on unlabeled
data.
Given parameters ?
j,a
, we can simply define the
functions q
1
. . . q
k
to be log probabilities under the
generative model:
q
j
(x, h,m, l) = log q
?
j
(x, h,m, l)
=
d
j
?
a=1
r
j,a
(x, h,m, l) log ?
j,a
.
We modify this definition slightly, be introducing
scaling factors c
j,a
> 0, and defining
q
j
(x, h,m, l) =
d
j
?
a=1
r
j,a
(x, h,m, l) log
?
j,a
c
j,a
(4)
In our experiments, c
j,a
is simply a count of the
number of times the feature indexed by (j, a) ap-
pears in unlabeled data. Thus more frequent fea-
tures have their contribution down-weighted in the
model. We have found this modification to be ben-
eficial.
2.3 Estimating the Parameters of the
Generative Models
We now describe the method for estimating the
parameters ?
j,a
of the generative models. We
assume initial parameters w,v,q, which define
a distribution p(y|x
?
i
;w,v,q) over dependency
structures for each unlabeled example x
?
i
. We will
re-estimate the generative models q, based on un-
labeled examples. The likelihood function on un-
labeled data is defined as
M
?
i=1
?
y
p(y|x
?
i
;w,v,q)
?
(h,m,l)?y
log q
?
j
(x
?
i
, h,m, l),
(5)
where q
?
j
is as defined in Eq. 3. This function re-
sembles the Q function used in the EM algorithm,
where the hidden labels (in our case, dependency
structures), are filled in using the conditional dis-
tribution p(y|x
?
i
;w,v,q).
It is simple to show that the estimates ?
j,a
that
maximize the function in Eq. 5 can be defined as
follows. First, define a vector of expected counts
based on w,v,q as
?
r
j
=
M
?
i=1
?
y
p(y|x
?
i
;w,v,q)
?
(h,m,l)?y
r
j
(x
?
i
, h,m, l).
Note that it is straightforward to calculate these ex-
pected counts using a variant of the inside-outside
algorithm (Baker, 1979) applied to the (Eisner,
1996) dependency-parsing data structures (Paskin,
2001) for projective dependency structures, or the
matrix-tree theorem (Koo et al, 2007; Smith and
Smith, 2007; McDonald and Satta, 2007) for non-
projective dependency structures.
The estimates that maximize Eq. 5 are then
?
j,a
=
r?
j,a
?
d
j
a=1
r?
j,a
.
In a slight modification, we employ the follow-
ing estimates in our model, where ? > 1 is a pa-
rameter of the model:
?
j,a
=
(? ? 1) + r?
j,a
d
j
? (? ? 1) +
?
d
j
a=1
r?
j,a
. (6)
This corresponds to a MAP estimate under a
Dirichlet prior over the ?
j,a
parameters.
2.4 The Complete Parameter-Estimation
Method
This section describes the full parameter estima-
tion method. The input to the algorithm is a set
of labeled examples {x
i
,y
i
}
N
i=1
, a set of unla-
beled examples {x
?
i
}
M
i=1
, a feature-vector defini-
tion f(x,y), and a partition of f into k feature vec-
tors r
1
. . . r
k
which underly the generative mod-
els. The output from the algorithm is a parameter
vector w, a set of generative models q
1
. . . q
k
, and
parameters v
1
. . . v
k
, which define a probabilistic
dependency parsing model through Eqs. 1 and 2.
The learning algorithm proceeds in three steps:
Step 1: Estimation of a Fully Supervised
Model. We choose the initial value q
0
of the
generative models to be the uniform distribution,
i.e., we set ?
j,a
= 1/d
j
for all j, a. We then de-
fine the regularized log-likelihood function for the
labeled examples, with the generative model fixed
at q
0
, to be:
L(w,v;q
0
) =
n
?
i=1
log p(y
i
|x
i
;w,v,q
0
)
?
C
2
(
||w||
2
+ ||v||
2
)
553
This is a conventional regularized log-likelihood
function, as commonly used in CRF models. The
parameter C > 0 dictates the level of regular-
ization in the model. We define the initial pa-
rameters (w
0
,v
0
) = argmax
w,v
L(w,v;q
0
).
These parameters can be found using conventional
methods for estimating the parameters of regu-
larized log-likelihood functions (in our case we
use LBFGS (Liu and Nocedal, 1989)). Note that
the gradient of the log-likelihood function can be
calculated using the inside-outside algorithm ap-
plied to projective dependency parse structures, or
the matrix-tree theorem applied to non-projective
structures.
Step 2: Estimation of the Generative Mod-
els. In this step, expected count vectors
?
r
1
. . .
?
r
k
are first calculated, based on the distribution
p(y|x;w
0
,v
0
,q
0
). Generative model parameters
?
j,a
are calculated through the definition in Eq. 6;
these estimates define updated generative models
q
1
j
for j = 1 . . . k through Eq. 4. We refer to the
new values for the generative models as q
1
.
Step 3: Re-estimation of w and v. In
the final step, w
1
and v
1
are estimated as
argmax
w,v
L(w,v;q
1
) where L(w,v;q
1
) is de-
fined in an analogous way to L(w,v;q
0
). Thus w
and v are re-estimated to optimize log-likelihood
of the labeled examples, with the generative mod-
els q
1
estimated in step 2.
The final output from the algorithm is the set of
parameters (w
1
,v
1
,q
1
). Note that it is possible to
iterate the method?steps 2 and 3 can be repeated
multiple times (Suzuki and Isozaki, 2008)?but
in our experiments we only performed these steps
once.
3 Extensions
3.1 Incorporating Cluster-Based Features
Koo et al (2008) describe a semi-supervised
approach that incorporates cluster-based features,
and that gives competitive results on dependency
parsing benchmarks. The method is a two-stage
approach. First, hierarchical word clusters are de-
rived from unlabeled data using the Brown et al
clustering algorithm (Brown et al, 1992). Sec-
ond, a new feature set is constructed by represent-
ing words by bit-strings of various lengths, corre-
sponding to clusters at different levels of the hier-
archy. These features are combined with conven-
tional features based on words and part-of-speech
tags. The new feature set is then used within a
conventional discriminative, supervised approach,
such as the averaged perceptron algorithm.
The important point is that their approach uses
unlabeled data only for the construction of a new
feature set, and never affects to learning algo-
rithms. It is straightforward to incorporate cluster-
based features within the SS-SCM approach de-
scribed in this paper. We simply use the cluster-
based feature-vector representation f(x,y) intro-
duced by (Koo et al, 2008) as the basis of our ap-
proach.
3.2 Second-order Parsing Models
Previous work (McDonald and Pereira, 2006; Car-
reras, 2007) has shown that second-order parsing
models, which include information from ?sibling?
or ?grandparent? relationships between dependen-
cies, can give significant improvements in accu-
racy over first-order parsing models. In principle
it would be straightforward to extend the SS-SCM
approach that we have described to second-order
parsing models. In practice, however, a bottle-
neck for the method would be the estimation of
the generative models on unlabeled data. This
step requires calculation of marginals on unlabeled
data. Second-order parsing models generally re-
quire more costly inference methods for the cal-
culation of marginals, and this increased cost may
be prohibitive when large quantities of unlabeled
data are employed.
We instead make use of a simple ?two-stage? ap-
proach for extending the SS-SCM approach to the
second-order parsing model of (Carreras, 2007).
In the first stage, we use a first-order parsing
model to estimate generative models q
1
. . . q
k
from
unlabeled data. In the second stage, we incorpo-
rate these generative models as features within a
second-order parsing model. More precisely, in
our approach, we first train a first-order parsing
model by Step 1 and 2, exactly as described in
Section 2.4, to estimate w
0
, v
0
and q
1
. Then,
we substitute Step 3 as a supervised learning such
as MIRA with a second-order parsing model (Mc-
Donald et al, 2005a), which incorporates q
1
as a
real-values features. We refer this two-stage ap-
proach to as two-stage SS-SCM.
In our experiments we use the 1-best MIRA
algorithm (McDonald and Pereira, 2006)
1
as a
1
We used a slightly modified version of 1-best MIRA,
whose difference can be found in the third line in Eq. 7,
namely, including L(y
i
,y).
554
(a) English dependency parsing
Data set (WSJ Sec. IDs) # of sentences # of tokens
Training (02?21) 39,832 950,028
Development (22) 1,700 40,117
Test (23) 2,012 47,377
Unlabeled 1,796,379 43,380,315
(b) Czech dependency parsing
Data set # of sentences # of tokens
Training 73,088 1,255,590
Development 7,507 126,030
Test 7,319 125,713
Unlabeled 2,349,224 39,336,570
Table 1: Details of training, development, test data
(labeled data sets) and unlabeled data used in our
experiments
parameter-estimation method for the second-order
parsing model. In particular, we perform the fol-
lowing optimizations on each update t = 1, ..., T
for re-estimating w and v:
min ||w
(t+1)
?w
(t)
||+ ||v
(t+1)
? v
(t)
||
s.t. S(x
i
,y
i
)? S(x
i
,
?
y) ? L(y
i
,
?
y)
?
y = argmax
y
S(x
i
,y) + L(y
i
,y),
(7)
where L(y
i
,y) represents the loss between correct
output of i?th sample y
i
and y. Then, the scoring
function S for each y can be defined as follows:
S(x,y) =w ? (f
1
(x,y) + f
2
(x,y))
+B
k
?
j=1
v
j
q
j
(x,y),
(8)
where B represents a tunable scaling factor, and
f
1
and f
2
represent the feature vectors of first and
second-order parsing parts, respectively.
4 Experiments
We now describe experiments investigating the ef-
fectiveness of the SS-SCM approach for depen-
dency parsing. The experiments test basic, first-
order parsing models, as well as the extensions
to cluster-based features and second-order parsing
models described in the previous section.
4.1 Data Sets
We conducted experiments on both English and
Czech data. We used the Wall Street Journal
sections of the Penn Treebank (PTB) III (Mar-
cus et al, 1994) as a source of labeled data for
English, and the Prague Dependency Treebank
(PDT) 1.0 (Haji?c, 1998) for Czech. To facili-
tate comparisons with previous work, we used ex-
actly the same training, development and test sets
Corpus article name (mm/yy) # of sent. # of tokens
BLLIP wsj 00/87?00/89 1,796,379 43,380,315
Tipster wsj 04/90?03/92 1,550,026 36,583,547
North wsj 07/94?12/96 2,748,803 62,937,557
American reu 04/94?07/96 4,773,701 110,001,109
Reuters reu 09/96?08/97 12,969,056 214,708,766
English afp 05/94?12/06 21,231,470 513,139,928
Gigaword apw 11/94?12/06 46,978,725 960,733,303
ltw 04/94?12/06 10,524,545 230,370,454
nyt 07/94?12/06 60,752,363 1,266,531,274
xin 01/95?12/06 12,624,835 283,579,330
total 175,949,903 3,721,965,583
Table 2: Details of the larger unlabeled data set
used in English dependency parsing: sentences ex-
ceeding 128 tokens in length were excluded for
computational reasons.
as those described in (McDonald et al, 2005a;
McDonald et al, 2005b; McDonald and Pereira,
2006; Koo et al, 2008). The English dependency-
parsing data sets were constructed using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to dependency tree repre-
sentations. We split the data into three parts:
sections 02-21 for training, section 22 for de-
velopment and section 23 for test. The Czech
data sets were obtained from the predefined train-
ing/development/test partition in the PDT. The un-
labeled data for English was derived from the
Brown Laboratory for Linguistic Information Pro-
cessing (BLLIP) Corpus (LDC2000T43)
2
, giving
a total of 1,796,379 sentences and 43,380,315
tokens. The raw text section of the PDT was
used for Czech, giving 2,349,224 sentences and
39,336,570 tokens. These data sets are identical
to the unlabeled data used in (Koo et al, 2008),
and are disjoint from the training, development
and test sets. The datasets used in our experiments
are summarized in Table 1.
In addition, we will describe experiments that
make use of much larger amounts of unlabeled
data. Unfortunately, we have no data available
other than PDT for Czech, this is done only for
English dependency parsing. Table 2 shows the
detail of the larger unlabeled data set used in our
experiments, where we eliminated sentences that
have more than 128 tokens for computational rea-
sons. Note that the total size of the unlabeled data
reaches 3.72G (billion) tokens, which is approxi-
2
We ensured that the sentences used in the PTB were
excluded from the unlabeled data, since sentences used in
BLLIP corpus are a super-set of the PTB.
555
mately 4,000 times larger than the size of labeled
training data.
4.2 Features
4.2.1 Baseline Features
In general we will assume that the input sentences
include both words and part-of-speech (POS) tags.
Our baseline features (?baseline?) are very simi-
lar to those described in (McDonald et al, 2005a;
Koo et al, 2008): these features track word and
POS bigrams, contextual features surrounding de-
pendencies, distance features, and so on. En-
glish POS tags were assigned by MXPOST (Rat-
naparkhi, 1996), which was trained on the train-
ing data described in Section 4.1. Czech POS tags
were obtained by the following two steps: First,
we used ?feature-based tagger? included with the
PDT
3
, and then, we used the method described in
(Collins et al, 1999) to convert the assigned rich
POS tags into simplified POS tags.
4.2.2 Cluster-based Features
In a second set of experiments, we make use of the
feature set used in the semi-supervised approach
of (Koo et al, 2008). We will refer to this as the
?cluster-based feature set? (CL). The BLLIP (43M
tokens) and PDT (39M tokens) unlabeled data sets
shown in Table 1 were used to construct the hierar-
chical clusterings used within the approach. Note
that when this feature set is used within the SS-
SCM approach, the same set of unlabeled data is
used to both induce the clusters, and to estimate
the generative models within the SS-SCM model.
4.2.3 Constructing the Generative Models
As described in section 2.2, the generative mod-
els in the SS-SCM approach are defined through
a partition of the original feature vector f(x,y)
into k feature vectors r
1
(x,y) . . . r
k
(x,y). We
follow a similar approach to that of (Suzuki and
Isozaki, 2008) in partitioning f(x,y), where the
k different feature vectors correspond to different
feature types or feature templates. Note that, in
general, we are not necessary to do as above, this
is one systematic way of a feature design for this
approach.
4.3 Other Experimental Settings
All results presented in our experiments are given
in terms of parent-prediction accuracy on unla-
3
Training, development, and test data in PDT already con-
tains POS tags assigned by the ?feature-based tagger?.
beled dependency parsing. We ignore the parent-
predictions of punctuation tokens for English,
while we retain all the punctuation tokens for
Czech. These settings match the evaluation setting
in previous work such as (McDonald et al, 2005a;
Koo et al, 2008).
We used the method proposed by (Carreras,
2007) for our second-order parsing model. Since
this method only considers projective dependency
structures, we ?projectivized? the PDT training
data in the same way as (Koo et al, 2008). We
used a non-projective model, trained using an ap-
plication of the matrix-tree theorem (Koo et al,
2007; Smith and Smith, 2007; McDonald and
Satta, 2007) for the first-order Czech models, and
projective parsers for all other models.
As shown in Section 2, SS-SCMs with 1st-order
parsing models have two tunable parameters, C
and ?, corresponding to the regularization con-
stant, and the Dirichlet prior for the generative
models. We selected a fixed value ? = 2, which
was found to work well in preliminary experi-
ments.
4
The value of C was chosen to optimize
performance on development data. Note that C
for supervised SCMs were also tuned on develop-
ment data. For the two-stage SS-SCM for incor-
porating second-order parsing model, we have ad-
ditional one tunable parameter B shown in Eq. 8.
This was also chosen by the value that provided
the best performance on development data.
In addition to providing results for models
trained on the full training sets, we also performed
experiments with smaller labeled training sets.
These training sets were either created through
random sampling or by using a predefined subset
of document IDs from the labeled training data.
5 Results and Discussion
Table 3 gives results for the SS-SCM method un-
der various configurations: for first and second-
order parsing models, with and without the clus-
ter features of (Koo et al, 2008), and for varying
amounts of labeled data. The remainder of this
section discusses these results in more detail.
5.1 Effects of the Quantity of Labeled Data
We can see from the results in Table 3 that our
semi-supervised approach consistently gives gains
4
An intuitive meaning of ? = 2 is that this adds one
pseudo expected count to every feature when estimating new
parameter values.
556
(a) English dependency parsing: w/ 43M token unlabeled data (BLLIP)
WSJ sec. IDs wsj 21 random selection random selection wsj 15?18 wsj 02-21(all)
# of sentences / tokens 1,671 / 40,039 2,000 / 48,577 8,000 / 190,958 8,936 / 211,727 39,832 / 950,028
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 85.63 86.80 87.02 88.05 89.23 90.45 89.43 90.85 91.21 92.53
SS-SCM (1od) 87.16 88.40 88.07 89.55 90.06 91.45 90.23 91.63 91.72 93.01
(gain over Sup. SCM) (+1.53) (+1.60) (+1.05) (+1.50) (+0.83) (+1.00) (+0.80) (+0.78) (+0.51) (+0.48)
Supervised MIRA (2od) 87.99 89.05 89.20 90.06 91.20 91.75 91.50 92.14 93.02 93.54
2-stage SS-SCM(+MIRA) (2od) 88.88 89.94 90.03 90.90 91.73 92.51 91.95 92.73 93.45 94.13
(gain over Sup. MIRA) (+0.89) (+0.89) (+0.83) (+0.84) (+0.53) (+0.76) (+0.45) (+0.59) (+0.43) (+0.59)
(b) Czech dependency parsing: w/ 39M token unlabeled data (PDT)
PDT Doc. IDs random selection c[0-9]* random selection l[a-i]* (all)
# of sentences / tokens 2,000 / 34,722 3,526 / 53,982 8,000 / 140,423 14,891 / 261,545 73,008 /1,225,590
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 75.67 77.82 76.88 79.24 80.61 82.85 81.94 84.47 84.43 86.72
SS-SCM (1od) 76.47 78.96 77.61 80.28 81.30 83.49 82.74 84.91 85.00 87.03
(gain over Sup. SCM) (+0.80) (+1.14) (+0.73) (+1.04) (+0.69) (+0.64) (+0.80) (+0.44) (+0.57) (+0.31)
Supervised MIRA (2od) 78.19 79.60 79.58 80.77 83.15 84.39 84.27 85.75 86.82 87.76
2-stage SS-SCM(+MIRA) (2od) 78.71 80.09 80.37 81.40 83.61 84.87 84.95 86.00 87.03 88.03
(gain over Sup. MIRA) (+0.52) (+0.49) (+0.79) (+0.63) (+0.46) (+0.48) (+0.68) (+0.25) (+0.21) (+0.27)
Table 3: Dependency parsing results for the SS-SCM method with different amounts of labeled training
data. Supervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order ap-
proaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approaches
described in this paper. ?Baseline? refers to models without cluster-based features, ?CL? refers to models
which make use of cluster-based features.
in performance under various sizes of labeled data.
Note that the baseline methods that we have used
in these experiments are strong baselines. It is
clear that the gains from our method are larger for
smaller labeled data sizes, a tendency that was also
observed in (Koo et al, 2008).
5.2 Impact of Combining SS-SCM with
Cluster Features
One important observation from the results in Ta-
ble 3 is that SS-SCMs can successfully improve
the performance over a baseline method that uses
the cluster-based feature set (CL). This is in spite
of the fact that the generative models within the
SS-SCM approach were trained on the same un-
labeled data used to induce the cluster-based fea-
tures.
5.3 Impact of the Two-stage Approach
Table 3 also shows the effectiveness of the two-
stage approach (described in Section 3.2) that inte-
grates the SS-SCM method within a second-order
parser. This suggests that the SS-SCM method
can be effective in providing features (generative
models) used within a separate learning algorithm,
providing that this algorithm can make use of real-
valued features.
91.5
92.0
92.5
93.0
93.5
10 100 1,000 10,000
CL
baseline
43.4M 143M
468M 1.38G
3.72G
(Mega tokens)
Unlabeled data size: [Log-scale]
P
a
r
e
n
t
-
p
r
e
d
ic
t
io
n
 
A
c
c
u
r
a
c
y
(BLLIP)
Figure 1: Impact of unlabeled data size for the SS-
SCM on development data of English dependency
parsing.
5.4 Impact of the Amount of Unlabeled Data
Figure 1 shows the dependency parsing accuracy
on English as a function of the amount of unla-
beled data used within the SS-SCM approach. (As
described in Section 4.1, we have no unlabeled
data other than PDT for Czech, hence this section
only considers English dependency parsing.) We
can see that performance does improve as more
unlabeled data is added; this trend is seen both
with and without cluster-based features. In addi-
tion, Table 4 shows the performance of our pro-
posed method using 3.72 billion tokens of unla-
557
feature type baseline CL
SS-SCM (1st-order) 92.23 93.23
(gain over Sup. SCM) (+1.02) (+0.70)
2-stage SS-SCM(+MIRA) (2nd-order) 93.68 94.26
(gain over Sup. MIRA) (+0.66) (+0.72)
Table 4: Parent-prediction accuracies on develop-
ment data with 3.72G tokens unlabeled data for
English dependency parsing.
beled data. Note, however, that the gain in perfor-
mance as unlabeled data is added is not as sharp
as might be hoped, with a relatively modest dif-
ference in performance for 43.4 million tokens vs.
3.72 billion tokens of unlabeled data.
5.5 Computational Efficiency
The main computational challenge in our ap-
proach is the estimation of the generative mod-
els q = ?q
1
. . . q
k
? from unlabeled data, partic-
ularly when the amount of unlabeled data used
is large. In our implementation, on the 43M to-
ken BLLIP corpus, using baseline features, it takes
about 5 hours to compute the expected counts re-
quired to estimate the parameters of the generative
models on a single 2.93GHz Xeon processor. It
takes roughly 18 days of computation to estimate
the generative models from the larger (3.72 billion
word) corpus. Fortunately it is simple to paral-
lelize this step; our method takes a few hours on
the larger data set when parallelized across around
300 separate processes.
Note that once the generative models have been
estimated, decoding with the model, or train-
ing the model on labeled data, is relatively in-
expensive, essentially taking the same amount of
computation as standard dependency-parsing ap-
proaches.
5.6 Results on Test Data
Finally, Table 5 displays the final results on test
data. There results are obtained using the best
setting in terms of the development data perfor-
mance. Note that the English dependency pars-
ing results shown in the table were achieved us-
ing 3.72 billion tokens of unlabeled data. The im-
provements on test data are similar to those ob-
served on the development data. To determine
statistical significance, we tested the difference of
parent-prediction error-rates at the sentence level
using a paired Wilcoxon signed rank test. All eight
comparisons shown in Table 5 are significant with
(a) English dependency parsing: w/ 3.72G token ULD
feature set baseline CL
SS-SCM (1st-order) 91.89 92.70
(gain over Sup. SCM) (+0.92) (+0.58)
2-stage SS-SCM(+MIRA) (2nd-order) 93.41 93.79
(gain over Sup. MIRA) (+0.65) (+0.48)
(b) Czech dependency parsing: w/ 39M token ULD (PDT)
feature set baseline CL
SS-SCM (1st-order) 84.98 87.14
(gain over Sup. SCM) (+0.58) (+0.39)
2-stage SS-SCM(+MIRA) (2nd-order) 86.90 88.05
(gain over Sup. MIRA) (+0.15) (+0.36)
Table 5: Parent-prediction accuracies on test data
using the best setting in terms of development data
performances in each condition.
(a) English dependency parsers on PTB
dependency parser test description
(McDonald et al, 2005a) 90.9 1od
(McDonald and Pereira, 2006) 91.5 2od
(Koo et al, 2008) 92.23 1od, 43M ULD
SS-SCM (w/ CL) 92.70 1od, 3.72G ULD
(Koo et al, 2008) 93.16 2od, 43M ULD
2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD
(b) Czech dependency parsers on PDT
dependency parser test description
(McDonald et al, 2005b) 84.4 1od
(McDonald and Pereira, 2006) 85.2 2od
(Koo et al, 2008) 86.07 1od, 39M ULD
(Koo et al, 2008) 87.13 2od, 39M ULD
SS-SCM (w/ CL) 87.14 1od, 39M ULD
2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD
Table 6: Comparisons with the previous top sys-
tems: (1od, 2od: 1st- and 2nd-order parsing
model, ULD: unlabeled data).
p < 0.01.
6 Comparison with Previous Methods
Table 6 shows the performance of a number of
state-of-the-art approaches on the English and
Czech data sets. For both languages our ap-
proach gives the best reported figures on these
datasets. Our results yield relative error reduc-
tions of roughly 27% (English) and 20% (Czech)
over McDonald and Pereira (2006)?s second-order
supervised dependency parsers, and roughly 9%
(English) and 7% (Czech) over the previous best
results provided by Koo et. al. (2008)?s second-
order semi-supervised dependency parsers.
Note that there are some similarities between
our two-stage semi-supervised learning approach
and the semi-supervised learning method intro-
duced by (Blitzer et al, 2006), which is an exten-
sion of the method described by (Ando and Zhang,
558
2005). In particular, both methods use a two-stage
approach; They first train generative models or
auxiliary problems from unlabeled data, and then,
they incorporate these trained models into a super-
vised learning algorithm as real valued features.
Moreover, both methods make direct use of exist-
ing feature-vector definitions f(x,y) in inducing
representations from unlabeled data.
7 Conclusion
This paper has described an extension of the
semi-supervised learning approach of (Suzuki and
Isozaki, 2008) to the dependency parsing problem.
In addition, we have described extensions that in-
corporate the cluster-based features of Koo et al
(2008), and that allow the use of second-order
parsing models. We have described experiments
that show that the approach gives significant im-
provements over state-of-the-art methods for de-
pendency parsing; performance improves when
the amount of unlabeled data is increased from
43.8 million tokens to 3.72 billion tokens. The ap-
proach should be relatively easily applied to lan-
guages other than English or Czech.
We stress that the SS-SCM approach requires
relatively little hand-engineering: it makes di-
rect use of the existing feature-vector representa-
tion f(x,y) used in a discriminative model, and
does not require the design of new features. The
main choice in the approach is the partitioning
of f(x,y) into components r
1
(x,y) . . . r
k
(x,y),
which in our experience is straightforward.
References
R. Kubota Ando and T. Zhang. 2005. A Framework for
Learning Predictive Structures from Multiple Tasks
and Unlabeled Data. Journal of Machine Learning
Research, 6:1817?1853.
J. K. Baker. 1979. Trainable Grammars for Speech
Recognition. In Speech Communication Papers for
the 97th Meeting of the Acoustical Society of Amer-
ica, pages 547?550.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
Adaptation with Structural Correspondence Learn-
ing. In Proc. of EMNLP-2006, pages 120?128.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram
Models of Natural Language. Computational Lin-
guistics, 18(4):467?479.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of CoNLL-X, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of EMNLP-
CoNLL, pages 957?961.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann.
1999. A Statistical Parser for Czech. In Proc. of
ACL, pages 505?512.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
COLING-96, pages 340?345.
Jan Haji?c. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevov?a, pages 12?19. Prague Karolinum,
Charles University Press.
T. Koo, A. Globerson, X. Carreras, and M. Collins.
2007. Structured Prediction Models via the Matrix-
Tree Theorem. In Proc. of EMNLP-CoNLL, pages
141?150.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
Semi-supervised Dependency Parsing. In Proc. of
ACL-08: HLT, pages 595?603.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 282?289.
D. C. Liu and J. Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Programming, Ser. B, 45(3):503?528.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. McDonald and F. Pereira. 2006. Online Learning of
Approximate Dependency Parsing Algorithms. In
Proc. of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the Com-
plexity of Non-Projective Data-Driven Dependency
Parsing. In Proc. of IWPT, pages 121?132.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-margin Training of Dependency Parsers.
In Proc. of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Haji?c.
2005b. Non-projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proc. of HLT-
EMNLP, pages 523?530.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
EMNLP-CoNLL, pages 915?932.
Mark A. Paskin. 2001. Cubic-time Parsing and Learn-
ing Algorithms for Grammatical Bigram. Technical
report, University of California at Berkeley, Berke-
ley, CA, USA.
559
A. Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proc. of EMNLP,
pages 133?142.
D. A. Smith and J. Eisner. 2007. Bootstrapping
Feature-Rich Dependency Parsers with Entropic Pri-
ors. In Proc. of EMNLP-CoNLL, pages 667?677.
D. A. Smith and N. A. Smith. 2007. Probabilis-
tic Models of Nonprojective Dependency Trees. In
Proc. of EMNLP-CoNLL, pages 132?140.
J. Suzuki and H. Isozaki. 2008. Semi-supervised
Sequential Labeling and Segmentation Using Giga-
Word Scale Unlabeled Data. In Proc. of ACL-08:
HLT, pages 665?673.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised Convex Training for Dependency Pars-
ing. In Proc. of ACL-08: HLT, pages 532?540.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
560
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 145?152, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Kernel-based Approach for Automatic Evaluation of Natural Language
Generation Technologies: Application to Automatic Summarization
Tsutomu Hirao
NTT Communication Science Labs.
NTT Corp.
hirao@cslab.kecl.ntt.co.jp
Manabu Okumura
Precision and Intelligence Labs.
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Hideki Isozaki
NTT Communication Science Labs.
NTT Corp.
isozaki@cslab.kecl.ntt.co.jp
Abstract
In order to promote the study of auto-
matic summarization and translation, we
need an accurate automatic evaluation
method that is close to human evalua-
tion. In this paper, we present an eval-
uation method that is based on convolu-
tion kernels that measure the similarities
between texts considering their substruc-
tures. We conducted an experiment us-
ing automatic summarization evaluation
data developed for Text Summarization
Challenge 3 (TSC-3). A comparison with
conventional techniques shows that our
method correlates more closely with hu-
man evaluations and is more robust.
1 Introduction
Automatic summarization, machine translation, and
paraphrasing have attracted much attention recently.
These tasks include text-to-text language genera-
tion. Evaluation workshops are held in the U.S.
and Japan, e.g., the Document Understanding Con-
ference (DUC)1, NIST Machine Translation Evalu-
ation2 as part of the TIDES project, the Text Sum-
marization Challenge (TSC)3 of the NTCIR project,
and the International Workshop on Spoken Lan-
guage Translation (IWSLT)4.
These evaluation workshops employ human eval-
uations, which are essential in terms of achieving
1http://duc.nist.gov
2http://www.nist.gov/speech/tests/mt/
3http://www.lr.titech.ac.jp/tsc
4http://www.slt.atr.co.jp/IWSLT2004
high quality evaluations results. However, human
evaluations require a huge effort and the cost is con-
siderable. Moreover, we cannot automatically eval-
uate a new system even if we use the corpora built
for these workshops, and we cannot conduct re-
evaluation experiments.
To cope with this situation, there is a particular
need to establish a high quality automatic evalua-
tion method. Once this is done, we can expect great
progress to be made on natural language generation.
In this paper, we propose a novel automatic
evaluation method for natural language generation
technologies. Our method is based on the Ex-
tended String Subsequence Kernel (ESK) (Hirao
et al, 2004b) which is a kind of convolution ker-
nel (Collins and Duffy, 2001). ESK allows us to
calculate the similarities between a pair of texts tak-
ing account of word sequences, their word sense se-
quences and their combinations.
We conducted an experimental evaluation using
automatic summarization evaluation data developed
for TSC-3 (Hirao et al, 2004a). The results of the
comparison with ROUGE-N (Lin and Hovy, 2003;
Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b;
Lin and Och, 2004) and ROUGE-L (Lin, 2004a;
Lin, 2004b) show that our method correlates more
closely with human evaluations and is more robust.
2 Related Work
Automatic evaluation methods for automatic sum-
marization and machine translation are grouped into
two classes. One is the longest common subse-
quence (LCS) based approach (Hori et al, 2003;
Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The
other is the N-gram based approach (Papineni et al,
145
Table 1: Components of vectors corresponding to S1 and S2. Bold subsequences are common to S1 and S2.
 
subsequence S1 S2   subsequence S1 S2   subsequence S1 S2
Becoming 1 1 Becoming-is 



astronaut-DREAM 0 

DREAM 1 1 Becoming-my  astronaut-ambition 0 

SPACEMAN 1 1 SPACEMAN-DREAM 

astronaut-is 0 1
a 1 0 SPACEMAN-ambition 0 

astronaut-my 0 
ambition 0 1 SPACEMAN-dream   0 cosmonaut-DREAM   0
1
an 0 1 SPACEMAN-great 

0 cosmonaut-dream   0
astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great 

0
cosmonaut 1 0 SPACEMAN-my  cosmonaut-is 1 0
dream 1 0 a-DREAM 

0 cosmonaut-my  0
great 1 0 a-SPACEMAN 1 0 great-DREAM 1 0
is 1 1 2 a-cosmonaut 1 0 2 great-dream 1 0
my 1 1 a-dream 

0 is-DREAM 


Becoming-DREAM 

a-great   0 is-ambition 0 
Becoming-SPACEMAN  a-is  0 is-dream 

0
Becoming-a 1 0 a-my 

0 is-great  0
Becoming-ambition 0 

an-DREAM 0   is-my 1 1
2 Becoming-an 0 1 an-SPACEMAN 0 1 my-DREAM  1
Becoming-astronaut 0  an-ambition 0   my-ambition 0 1
Becoming-cosmonaut  0 an-astronaut 0 1 my-dream  0
Becoming-dream  0 an-is 0  my-great 1 0
Becoming-great 

0 an-my 0 

2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b;
Soricut and Brill, 2004).
Hori et. al (2003) proposed an automatic eval-
uation method for speech summarization based on
word recognition accuracy. They reported that their
method is superior to BLEU (Papineni et al, 2002)
in terms of the correlation between human assess-
ment and automatic evaluation. Lin (2004a; 2004b)
and Lin and Och (2004) proposed an LCS-based au-
tomatic evaluation measure called ROUGE-L. They
applied ROUGE-L to the evaluation of summariza-
tion and machine translation. The results showed
that the LCS-based measure is comparable to N-
gram-based automatic evaluation methods. How-
ever, these methods tend to be strongly influenced
by word order.
Various N-gram-based methods have been pro-
posed since BLEU, which is now widely used for the
evaluation of machine translation. Lin et al (2003)
proposed a recall-oriented measure, ROUGE-N,
whereas BLEU is precision-oriented. They reported
that ROUGE-N performed well as regards automatic
summarization. In particular, ROUGE-1, i.e., uni-
gram matching, provides the best correlation with
human evaluation. Soricut et. al (2004) proposed
a unified measure. They integrated a precision-
oriented measure with a recall-oriented measure by
using an extension of the harmonic mean formula. It
performs well in evaluations of machine translation,
automatic summarization, and question answering.
However, N-gram based methods have a critical
problem; they cannot consider co-occurrences with
gaps, although the LCS-based method can deal with
them. Therefore, Lin and Och (2004) introduced
skip-bigram statistics for the evaluation of machine
translation. However, they did not consider longer
skip-n-grams such as skip-trigrams. Moreover, their
method does not distinguish between bigrams and
skip-bigrams.
3 Kernel-based Automatic Evaluation
The above N-gram-based methods correlated
closely with human evaluations. However, we
think some skip-n-grams (n 	
 ) are useful. In this
paper, we employ the Extended String Subsequence
Kernel (ESK), which considers both n-grams and
skip-n-grams. In addition, the ESK allows us to add
word senses to each word. The use of word senses
enables flexible matching even when paraphrasing
is used.
The ESK is a kind of convolution kernel (Collins
and Duffy, 2001). Convolution kernels have recently
attracted attention as a novel similarity measure in
natural language processing.
3.1 ESK
The ESK is an extension of the String Subsequence
Kernel (SSK) (Lodhi et al, 2002) and the Word Se-
quence Kernel (WSK) (Cancedda et al, 2003).
The ESK receives two node sequences as inputs
146
and maps each of them into a high-dimensional vec-
tor space. The kernel?s value is simply the inner
product of the two vectors in the vector space. In
order to discount long-skip-n-grams, the decay pa-
rameter  is introduced.
We explain the computation of the ESK?s value
whose inputs are the sentences (S1 and S2) shown
below. In the example, word senses are shown in
braces.
S1 Becoming a cosmonaut:  SPACEMAN  is my great
dream:  DREAM 
S2 Becoming an astronaut:  SPACEMAN  is my ambi-
tion:  DREAM 
In this case, ?cosmonaut? and ?astronaut? share
the same sense  SPACEMAN  and ?ambition? and
?dream? also share the same sense  DREAM  . We
can use WordNet for English and Goitaikei (Ikehara
et al, 1997) for Japanese.
Table 1 shows the subsequences derived from S1
and S2 and its weights. Note that the subsequence
length is two or less. From the table, there are fif-
teen subsequences5 that are common to S1 and S2.
Therefore, ffCorpus-based Question Answering for why-Questions
Ryuichiro Higashinaka and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
{rh,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a corpus-based ap-
proach for answering why-questions. Con-
ventional systems use hand-crafted patterns
to extract and evaluate answer candidates.
However, such hand-crafted patterns are
likely to have low coverage of causal expres-
sions, and it is also difficult to assign suit-
able weights to the patterns by hand. In our
approach, causal expressions are automati-
cally collected from corpora tagged with se-
mantic relations. From the collected expres-
sions, features are created to train an an-
swer candidate ranker that maximizes the
QA performance with regards to the corpus
of why-questions and answers. NAZEQA, a
Japanese why-QA system based on our ap-
proach, clearly outperforms a baseline that
uses hand-crafted patterns with a Mean Re-
ciprocal Rank (top-5) of 0.305, making it
presumably the best-performing fully imple-
mented why-QA system.
1 Introduction
Following the trend of non-factoid QA, we are
seeing the emergence of work on why-QA; e.g.,
answering generic ?why X?? questions (Verberne,
2006). However, since why-QA is an inherently dif-
ficult problem, there have only been a small number
of fully implemented systems dedicated to solving
it. Recent systems at NTCIR-61 Question Answer-
ing Challenge (QAC-4) can handle why-questions
(Fukumoto et al, 2007). However, their perfor-
mance is much lower (Mori et al, 2007) than that
of factoid QA systems (Fukumoto et al, 2004;
Voorhees and Dang, 2005).
We consider that this low performance is due to
the great amount of hand-crafting involved in the
1http://research.nii.ac.jp/ntcir/ntcir-ws6/ws-en.html
systems. Currently, most of the systems rely on
hand-crafted patterns to extract and evaluate answer
candidates (Fukumoto et al, 2007). Such patterns
include typical cue phrases and POS-tag sequences
related to causality, such as ?because of? and ?by
reason of.? However, as noted in (Inui and Okumura,
2005), causes are expressed in various forms, and
it is difficult to cover all such expressions by hand.
Hand-crafting is also very costly. Some patterns
may be more indicative of causes than others. There-
fore, it may be useful to assign different weights to
the patterns for better answer candidate extraction,
but currently this must be done by hand (Mori et al,
2007). It is not clear whether theweights determined
by hand are suitable.
In this paper, we propose a corpus-based approach
for why-QA in order to reduce this hand-crafting
effort. We automatically collect causal expressions
from corpora to improve the coverage of causal ex-
pressions, and utilize a machine learning technique
to train a ranker of answer candidates on the ba-
sis of features created from the expressions together
with other possible features related to causality. The
ranker is trained to maximize the QA performance
with regards to a corpus of why-questions and an-
swers, automatically tuning the weights of the fea-
tures.
This paper is organized as follows: Section 2 de-
scribes previous work onwhy-QA, and Section 3 de-
scribes our approach. Section 4 describes the imple-
mentation of our approach, and Section 5 presents
the evaluation results. Section 6 summarizes and
mentions future work.
2 Previous Work
Although systems that can answer why-questions
are emerging, they tend to have limitations in that
they can answer questions only with causal verbs
(Girju, 2003), in specific domains (Khoo et al,
418
2000), or questions covered by a specific knowl-
edge base (Curtis et al, 2005). Recently, Verberne
(2006; 2007a) has been intensively working on why-
QA based on the Rhetorical Structure Theory (RST)
(Mann and Thompson, 1988). However, her ap-
proach requires manually annotated corpora with
RST relations.
When we look for fully implemented systems for
generic ?why X?? questions, we only find a small
number of such systems. Since why-QA would be
a challenging task when tackled straightforwardly,
requiring common-sense knowledge and semantic
interpretation of questions and answer candidates,
current systems place higher priority on achiev-
ability and therefore use hand-crafted patterns and
heuristics to extract causal expressions as answer
candidates and use conventional sentence similarity
metrics for answer candidate evaluation (Fukumoto,
2007; Mori et al, 2007). We argue, in this paper,
that this hand-crafting is the cause of the current
low performance levels. Recently, (Shima and Mi-
tamura, 2007) applied a machine learning approach
to why-QA, but they also rely on manually selected
cue words to create their features.
Semantic Role Labeling (SRL) techniques can be
used to automatically detect causal expressions. In
the CoNLL-2005 shared task (SRL for English), the
best system found causal adjuncts with a reasonable
accuracy of 65% (Ma`rquez et al, 2005). However,
when we analyzed the data, we found that more than
half of the causal adjuncts contain explicit cues such
as ?because.? Since causes are reported to be ex-
pressed by a wide variety of linguistic phenomena,
not just explicit cues (Inui and Okumura, 2005), fur-
ther verification is needed before SRL can be safely
used for why-QA.
Why-questions are a subset of non-factoid ques-
tions. Since non-factoid questions are observed
in many FAQ sites, such sites have been regarded
as valuable resources for the development of non-
factoid QA systems. Examples include Burke et al
(1997), who used FAQ corpora to analyze questions
to achieve accurate question-type matching; Soricut
and Brill (2006), who used them to train statistical
models for answer evaluation and formulation; and
Mizuno et al (2007), who used them to train clas-
sifiers of question and answer-types. However, they
do not focus on why-questions and do not use any
causal knowledge, which is considered to be useful
for explicit why-questions (Soricut and Brill, 2006).
3 Approach
In this paper, we propose a corpus-based approach
for why-QA in order to reduce the hand-crafting ef-
fort that is currently necessary. We first automat-
ically collect causal expressions from corpora and
use them to create features to represent an answer
candidate. The features are then used to train an an-
swer candidate ranker that maximizes the QA per-
formance with regards to a corpus of why-questions
and answers. We also enumerate possible features
that may be useful for why-QA to be incorporated
in the training to improve the QA performance.
Following the systems at QAC-4 (Fukumoto,
2007) and the answer analysis in (Verberne, 2007b;
Verberne et al, 2007), we consider the task of why-
QA to be a sentence/paragraph extraction task. We
also assume that a document retrieval module of a
system returns top-N documents for a question on
the basis of conventional IR-related metrics and all
sentences/paragraphs extracted from them are re-
garded as answer candidates. Hence, the task be-
comes the ranking of given sentences/paragraphs.
For an answer candidate (a sentence or a para-
graph) to be the correct answer, the candidate should
(1) have an expression indicating a cause and (2)
be similar to the question in content, and (3) some
causal relation should be observed between the can-
didate and the question. For example, an answer
candidate ?X was arrested for fraud.? is likely to
be a correct answer to the question ?Why was X
arrested?? because ?for fraud? expresses a cause,
the question and the answer are both about the same
event (X being arrested), and ?fraud? and ?arrest? in-
dicate a causal relation between the question and the
candidate. Condition (3) would be especially use-
ful when the candidates do not have obvious cues
or topically similar words/phrases to the question;
it may be worthwhile to rely on some prior causal
knowledge to select one over others. Although cur-
rent working systems (Fukumoto, 2007; Mori et al,
2007) do not explicitly state these conditions, they
can be regarded as using hand-crafted patterns for
(1) and (3).2 Lexical similarity metrics, such as co-
sine similarity and n-gram overlaps, are generally
used for (2).
We represent each answer candidate with causal
expression, content similarity, and causal relation
2(3) is dealt with in a manner similar to the treatment of
?cause of death? in (Smith et al, 2005).
419
features that encode how it complies with the three
conditions. Here, the causal expression features are
those based on the causal expressions we aim to col-
lect automatically. For the other two types of fea-
tures, we turn to the existing similarity metrics and
dictionaries to derive features that would be useful
for why-QA. To train a ranker, we create a corpus of
why-questions and answers and adopt one of thema-
chine learning algorithms for ranking. The follow-
ing sections describe the three types of features, the
corpus creation, and the ranker training. The actual
instances of the features, the corpus, and the ranker
will be presented in Section 4.
3.1 Causal Expression Features
With the increasing attention paid to SRL, we cur-
rently have a number of corpora, such as PropBank
(Palmer, 2005) and FrameNet (Baker et al, 1998),
that are tagged with semantic relations including a
causal relation. Since text spans for such relations
are annotated in the corpora, we can simply col-
lect the spans marked by a causal relation as causal
expressions. Since an answer candidate that has a
matching expression for one of the collected causal
expressions is likely to be expressing a cause as
well, we can make the existence of each expression
a feature. Although the collected causal expressions
without any modification might be used to create
features, for generality, it would be better to abstract
them into syntactic patterns. From m causal expres-
sions/patterns automatically extracted from corpora,
we can create m binary features.
In addition, some why-QA systems may already
possess some good hand-crafted patterns to detect
causal expressions. Since there is no reason not to
use them if we know they are useful for why-QA,
we can create a feature indicatingwhether an answer
candidate matches existing hand-crafted patterns.
3.2 Content Similarity Features
In general, if a question and an answer candidate
share many words, it is likely that they are about
the same content. From this assumption, we cre-
ate a feature that encodes the lexical similarity of an
answer candidate to the question. To calculate its
value, existing sentence similarity metrics, such as
cosine similarity or n-gram overlaps, can be used.
Even if a question and an answer candidate do not
share the same words, they may still be about the
same content. One such case is when they are about
the same topic. To express this case as a feature, we
can use the similarity of the question and the docu-
ment in which the answer candidate is found. Since
the documents from which we extract answer candi-
dates typically have scores output by an IR engine
that encode their relevance to the question, we can
use this score or simply the rank of the retrieved doc-
ument as a feature.
A question and an answer candidate may be se-
mantically expressing the same content with differ-
ent expressions. The simplest case is when syn-
onyms are used to describe the same content; e.g.,
when ?arrest? is used instead of ?apprehend.? For
such cases, we can exploit existing thesauri. We
can create a feature encoding whether synonyms of
words in the question are found in the answer can-
didate. We could also use the value of semantic
similarity and relatedness measures (Pedersen et al,
2004) or the existence of hypernym or hyponym re-
lations as features.
3.3 Causal Relation Features
There are semantic lexicons where a semantic re-
lation between concepts is indicated. For example,
the EDR dictionary3 shows whether a causal relation
holds between two concepts; e.g., between ?murder?
and ?arrest.? Using such dictionaries, we can create
pairs of expressions, one indicating a cause and the
other its effect. If we find an expression for a cause
in the answer candidate and that for an effect in the
question, it is likely that they hold a causal relation.
Therefore, we can create a feature encoding whether
this is the case. In cases where such semantic lex-
icons are not available, they may be automatically
constructed, although with noise, using causal min-
ing techniques such as (Marcu and Echihabi, 2002;
Girju, 2003; Chang and Choi, 2004).
3.4 Creating a QA Corpus
For ranker training, we need a corpus of why-
questions and answers. Because we regard the
task of why-QA as a ranking of given sen-
tences/paragraphs, it is best to prepare the corpus in
the same setting. Therefore, we use the following
procedure to create the corpus: (a) create a question,
(b) use an IR engine to retrieve documents for the
question, (c) select among all sentences/paragraphs
in the retrieved documents those that contain the an-
swer to the question, and (d) store the question and a
3http://www2.nict.go.jp/r/r312/EDR/index.html
420
set of selected sentences/paragraphs with their doc-
ument IDs as answers.
3.5 Training a Ranker
Having created the QA corpus, we can apply exist-
ing machine learning algorithms for ranking, such
as RankBoost (Freund et al, 2003) or Ranking
SVM (Joachims, 2002), so that the selected sen-
tences/paragraphs are preferred to non-selected ones
on the basis of their features. Good ranking would
result in goodMean Reciprocal Rank (MRR), which
is one of the most commonly used measures in QA.
4 Implementation
Using our approach, we implemented a Japanese
why-QA system, NAZEQA (?Naze? means ?why?
in Japanese). The system was built as an extension
to our factoid QA system, SAIQA (Isozaki, 2004;
Isozaki, 2005), and works as follows:
1. The question is analyzed by a rule-based ques-
tion analysis component to derive a question
type; ?REASON? for a why-question.
2. The document retrieval engine extracts n-best
documents from Mainichi newspaper articles
(1998?2001) using DIDF (Isozaki, 2005), a
variant of the IDF metric. We chose 20 as n.
All sentences/paragraphs in the n documents
are extracted as answer candidates. Whether
to use sentences or paragraphs as answer can-
didates is configurable.
3. The feature extraction component produces, for
each answer candidate, causal expression, con-
tent similarity, and causal relation features en-
coding how it satisfies conditions (1)?(3) de-
scribed in Section 3.
4. The SVM ranker trained by a QA corpus ranks
the answer candidates based on the features.
5. The top-N answer candidates are presented to
the user as answers.
In the following sections, we describe the features
(399 in all), the QA corpus, and the ranker.
4.1 Causal Expression Features
(F1?F394: AUTO-Causal Expression) We au-
tomatically extracted causal expressions from the
EDR dictionary. The EDR dictionary is a suite
of corpora and dictionaries and includes the EDR
corpus, the EDR concept dictionary (hierarchy of
word senses), and the EDR Japanese word dictio-
nary (sense to word mappings). The EDR corpus
is a collection of independent Japanese sentences
taken from various sources, such as newspaper ar-
ticles, magazines, and dictionary glosses. The cor-
pus is annotated with semantic relations including a
causal relation in a manner similar to PropBank and
FrameNet corpora. We extracted regions marked by
?cause? tags and abstracted them by leaving only
the functional words (auxiliary verbs and case, as-
pect, tense markers) and replacing others with wild-
cards ?*.? For example, a causal expression ?ar-
rested for fraud? would be abstracted to ?*-PASS
for *.? We used CaboCha4 as a morphological ana-
lyzer. From 8,747 regions annotated with ?cause,?
we obtained 394 causal expression patterns after fil-
tering out those that occurred only once. Finally, we
have 394 binary features representing the existence
of each abstracted causal expression pattern.
(F395: MAN-Causal Expression) We emulate the
manually created patterns described in (Fukumoto,
2007) and create a binary feature indicating whether
an answer candidate is matched by the patterns.
4.2 Content Similarity Features
(F396: Question-Candidate Cosine Similarity)
We use the cosine similarity between a question and
an answer candidate using the word frequency vec-
tors of the content words. We chose nouns, verbs,
and adjectives as content words.
(F397: Question-Document Relevance) We use,
as a feature, the inverse of the rank of the document
where the answer candidate is found.
(F398: Synonym Pair) This is a binary feature that
indicates whether a word and its synonym appear
in an answer candidate and a question, respectively.
We use the combination of the EDR concept dictio-
nary and the EDR Japanese word dictionary as a the-
saurus to collect synonym pairs. We have 133,486
synonym pairs.
4.3 Causal Relation Feature
(F399: Cause-Effect Pair) This is a binary fea-
ture that indicates whether a word representing a
cause and a word corresponding to its effect ap-
pear in an answer candidate and a question, respec-
tively. We used the EDR concept dictionary to find
pairs of word senses holding a causal relation and
4http://chasen.org/?taku/software/cabocha/
421
Q13: Why are pandas on the verge of extinction?
(000217262)
A:000217262,L2 Since pandas are not good at raising
their offspring, the Panda Preservation Center in
Sichuan Province is promoting artificial insemina-
tion as well as the training of mother pandas.
A:000217262,L3 A mother panda often gives birth to
two cubs, but when there are two cubs, one is dis-
carded, and young mothers sometimes crush their
babies to death.
A:000406060,L6 However, because of the recent devel-
opment in the midland, they are becoming extinct.
A:010219075,L122 The most common cause of the ex-
tinction for mammals, birds, and plants is degrada-
tion and destruction of habitat, followed by hunting
and poaching for mammals and the impact of alien
species for birds.
Figure 1: An excerpt from the WHYQA collection.
The number in parentheses is the ID of the docu-
ment used to come up with the question. The an-
swers were headed by the document ID and the line
number where the sentence is found in the docu-
ment. (N.B. The above sentences were translated by
the authors.)
expanded the senses to corresponding words using
the EDR Japanese word dictionary to create cause-
effect word pairs. We have 355,641 cause-effect
word pairs.
4.4 WHYQA Collection
Since QAC-4 does not provide official answer sets
and their questions include only a small number
of why-questions, we created a corpus of why-
questions and answers on our own.
An expert, who specializes in text analysis and
is not one of authors, created questions from arti-
cles randomly extracted from Mainichi newspaper
articles (1998?2001). Then, for each question, she
created sentence-level answers by selecting the sen-
tences that she considered to fully include the an-
swer from a list of sentences from top-20 documents
returned from the text retrieval engine with the ques-
tion as input. Paragraph-level answers were auto-
matically created from the sentence-level answers
by selecting the paragraphs containing the answer
sentences.
The analyst was instructed not to create ques-
tions by simply converting existing declarative sen-
tences into interrogatives. It took approximately five
months to create 1,000 question and answer sets
(called the WHYQA collection). All questions are
guaranteed to have answers. Figure 1 lists an exam-
ple question and answer sentences in the collection.
4.5 Training a Ranker by Ranking SVM
Using the WHYQA collection, we trained rank-
ing models using the ranking SVM (Joachims,
2002) (with a linear kernel) that minimizes the
pairwise ranking error among the answer candi-
dates. In the training data, the answers were la-
beled ?+1? and non-answers ??1.? When using sen-
tences as answers, there are 4,849 positive exam-
ples and 521,177 negative examples. In the case of
paragraphs, there are 4,371 positive examples and
261,215 negative examples.
5 Evaluation
For evaluation, we compared the proposed system
(NAZEQA) with two baselines. Baseline-1 (COS)
simply uses, for answer candidate evaluation, the co-
sine similarity between an answer candidate and a
question based on frequency vectors of their con-
tent words. The aim of having this baseline is to see
how the system performs without any use of causal
knowledge. Baseline-2 (FK) uses hand-crafted pat-
terns described in (Fukumoto, 2007) to narrow down
the answer candidates to those having explicit causal
expressions, which are then ranked by the cosine
similarity to the question. NAZEQA and the two
baselines used the same document retrieval engine
to obtain the top-20 documents and ranked the sen-
tences or paragraphs in these documents.
5.1 Results
We made each system output the top-1, 5, 10, and 20
answer sentences and paragraphs for all 1,000 ques-
tions in the WHYQA collection. We used the MRR
and coverage as the evaluation metrics. Coverage
means the rate of questions that can be answered
by the top-N answer candidates. Table 1 shows the
MRRs and coverage for the baselines and NAZEQA.
A 10-fold cross validation was used for the evalua-
tion of NAZEQA.
We can see from the table that NAZEQA is bet-
ter in all comparisons. A statistical test (a sign
test that compares the number of times one sys-
tem places the correct answer before the other)
showed that NAZEQA is significantly better than
FK for the top-5, 10, and 20 answers in the sen-
tence and paragraph-levels (p<0.01). Although the
sentence-level MRR for NAZEQA is rather low, the
paragraph-level MRR for the top-5 answers is 0.305,
which is reasonably high for a non-factoid QA sys-
tem (Mizuno et al, 2007). The coverage is also
422
MRR Coverage
top-N COS FK NZQ COS FK NZQ
Sentences as answer candidates:
top-1 0.036 0.091+ 0.113 3.6% 9.1% 11.3%
top-5 0.086 0.139+ 0.196* 19.1% 23.1% 35.4%
top-10 0.102 0.149+ 0.216* 31.3% 30.7% 50.4%
top-20 0.115 0.152 0.227* 51.4% 35.5% 66.6%
Paragraphs as answer candidates:
top-1 0.065 0.152+ 0.186 6.5% 15.2% 18.6%
top-5 0.140 0.245+ 0.305* 29.2% 41.6% 53.1%
top-10 0.166 0.257+ 0.328* 48.8% 50.5% 70.3%
top-20 0.181 0.262+ 0.339* 70.7% 56.4% 85.6%
Table 1: Mean Reciprocal Rank (MRR) and cov-
erage for the baselines (COS and FK) and the pro-
posed NAZEQA (NZQ in the table) system for the
entire WHYQA collection. The top-1, 5, 10, and
20 mean the numbers of topmost candidates used
to calculate MRR and coverage. Asterisks indicate
NAZEQA?s statistical significance (p<0.01) over
FK, and ?+? FK?s over COS.
Feature Set Sent. Para.
All features (NAZEQA) 0.181 0.287
w/o F1?F394 (AUTO-Causal Exp.) 0.138* 0.217*
w/o F395 (MAN-Causal Exp.) 0.179 0.286
w/o F396 (Q-Cand. Cosine Similarity) 0.131* 0.188*
w/o F397 (Doc.-Q Relevance ) 0.161 0.275
w/o F398 (Synonym Pair) 0.180 0.282
w/o F399 (Cause-Effect Pair) 0.184 0.287
Table 2: Performance changes in MRR (top-5) when
we exclude one of the feature sets. Asterisks indi-
cate a statistically significant drop in performance
from NAZEQA. In this experiment, we used a two-
fold cross validation to reduce computational cost.
high for NAZEQA, making it possible to find an-
swers within the top-10 sentences and top-5 para-
graphs for more than 50% of the questions. Because
there are no why-QA systems known to be better
than NAZEQA in MRR and coverage and because
NAZEQA clearly outperforms a competitive base-
line (FK), we conclude that NAZEQA has one of
the best performance levels for why-QA.
It is interesting to know how each of the feature
sets (e.g., AUTO-Causal Expression Features) con-
tributes to the QA performance. Table 2 shows how
the performance in MRR (top-5) changes when one
of the feature sets is excluded in the training. Al-
though the drop in performance by removing the
Question-Candidate Cosine Similarity feature is un-
derstandable, the performance also drops signifi-
cantly from NAZEQA when we exclude AUTO-
Causal Expression features, showing the effective-
ness of our automatically collected causal patterns.
Rank Feature Name Weight
1 Question-Candidate Cosine Similarity 4.66
2 Exp.[de (by) * wo (-ACC) * teshimai (-PERF)] 1.86
3 Exp.[no (of) * niyote wa (according to)] 1.44
4 Exp.[no (of) * na (AUX) * no (of) * de (by)] 1.42
5 Exp.[no (of) * ya (or) * niyotte (by)] 1.35
6 Exp.[no (of) * ya (or) * no (of) * de (by)] 1.30
7 Exp.[na (AUX) * niyotte (by)] 1.23
8 Exp.[koto niyotte (by the fact that)] 1.22
9 Exp.[to (and) * no (of) * niyotte (by)] 1.20
10 Document-Question Relevance 0.89
...
27 Synonym Pair 0.40
102 MAN-Causal Expression 0.16
127 Cause-Effect Pair 0.15
Table 3: Weights of features learned by the rank-
ing SVM. ?AUTO-Causal Expression? is denoted as
?Exp.? for lack of space. AUX means an auxiliary
verb. The abstracted causal expression patterns are
shown in square brackets with their English transla-
tions in parentheses.
The MAN-Causal Expression, Synonym Pair, and
Cause-Effect Pair features, do not seem to contribute
much to the performance. One of the reasons for
the small contribution of the MAN-Causal Expres-
sion feature may be that the manual patterns used to
create this feature overlap greatly with the automat-
ically collected causal expression patterns, lowering
the impact of the MAN-Causal Expression feature.
The small contribution of the Synonym Pair feature
is probably attributed to the way the answers were
created in the creation of the WHYQA Collection.
Since the answer candidates from which the expert
chose the answers were those retrieved by a text re-
trieval engine that uses lexical similarity to retrieve
relevant documents, it is possible that the answers
that contain synonyms had already been filtered out
in the beginning, making the Synonym Pair feature
less effective. Without the Cause-Effect Pair feature,
the performance does not change or even improves
a little when sentences are used as answers. The
reason for this may be that the syntactically well-
formed sentences of the newspaper articles might
have made causal cues and patterns more effective
than prior causal knowledge. We need to investigate
the difference between the manually created causal
patterns and the automatically collected ones. We
also need to investigate whether the Synonym Pair
and Cause-Effect Pair features could be useful in
other conditions; e.g., when answers are created in
different ways. We also need to examine the quality
of our synonym and cause-effect word pairs because
423
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 1  2  3  4  5  6  7  8  9  10
N
um
be
r o
f q
ue
sti
on
s
Rank of the first correct answer
Baseline-1 (COS)
Baseline-2 (FK)
NAZEQA
Figure 2: Distribution of the ranks of first correct
answers. Paragraphs were used as answers. A 10-
fold cross validationwas used to evaluate NAZEQA.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 100  200  300  400  500  600  700  800  900
M
R
R
Number of training samples
top-1
top-5
top-10
top-20
Figure 3: Learning curve: Performance changes
when answering Q1?Q100 with different sizes of
training samples. Paragraphs are used as answer
candidates.
their quality itself may be to blame.
Furthermore, analyzing the trained ranking mod-
els allows us to calculate the weights given to the
features (Hirao et al, 2002). Table 3 shows the
weights of the top-10 features. We also include in
the table the weights of the Synonym Pair, MAN-
Causal Expression and Cause Effect Pair features so
that the role of all three types of features in our ap-
proach can be shown. The analyzed model was the
one trained with all 1,000 questions in the WHYQA
collection with paragraphs as answers. Just as sug-
gested by Table 2, the Question-Candidate Cosine
Similarity feature plays a key role, followed by au-
tomatically collected causal expression features.
Figure 2 shows the distribution of the ranks of
the first correct answers for all questions in the
WHYQA collection for COS, FK, and NAZEQA.
The distribution of COS is almost uniform, indicat-
ing that lexical similarity cannot be directly trans-
lated into causality. The figure also shows that
NAZEQA consistently outperforms FK.
It may be useful to know how much training data
is needed to train a ranker. We therefore fixed the
test set to Q1?Q100 in the WHYQA collection and
trained rankers with nine different sizes of train-
ing data (100?900) created from Q101?{Q200 ? ? ?
Q1000}. Figure 3 shows the learning curve. Natu-
rally, the performance improves as we increase the
data. However, the performance gains begin to de-
crease relatively early, possibly indicating the limi-
tation of our approach. Since our approach heavily
relies on surface patterns, the use of syntactic and
semantic features may be necessary.
6 Summary and Future Work
This paper proposed corpus-based QA for why-
questions. We automatically collected causal ex-
pressions from semantically tagged corpora and
used them to create features to train an answer can-
didate ranker that maximizes the QA performance
with regards to the corpus of why-questions and an-
swers. The implemented system NAZEQA outper-
formed baselines with an MRR (top-5) of 0.305 and
the coverage was also high, making NAZEQA pre-
sumably the best-performing system as a fully im-
plemented why-QA system.
As future work, we are planning to investigate
other features that may be useful for why-QA. We
also need to examine how QA performance and the
weights of the features differ when we use other
sources for answer retrieval. In this work, we fo-
cused only on the ?cause? relation in the EDR cor-
pus to obtain causal expressions. However, there are
other relations, such as ?purpose,? that may also
be related to causality (Verberne, 2006).
Although we believe our approach is language-
independent, it would be worth verifying it by creat-
ing an English version of NAZEQA based on causal
expressions that can be derived from PropBank and
FrameNet. Finally, we are planning to make public
some of the WHYQA collection at the authors? web-
page so that various why-QA systems can be com-
pared.
Acknowledgments
We thank Jun Suzuki, Kohji Dohsaka, Masaaki Na-
gata, and all members of the Knowledge Processing
424
Research Group for helpful discussions and com-
ments. We also thank the anonymous reviewers for
their valuable suggestions.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet Project. In Proc. COLING-ACL,
pages 86?90.
Robin Burke, Kristian Hammond, Vladimir Kulyukin, Steve
Lytinen, Noriko Tomuro, and Scott Schoenberg. 1997.
Question answering from frequently asked question files:
Experiences with the FAQFinder system. AI Magazine,
18(2):57?66.
Du-Seong Chang and Key-Sun Choi. 2004. Causal relation
extraction using cue phrase and lexical pair probabilities. In
Proc. IJCNLP, pages 61?70.
Jon Curtis, Gavin Matthews, and David Baxter. 2005. On the
effective use of Cyc in a question answering system. In Proc.
IJCAI Workshop on Knowledge and Reasoning for Answer-
ing Questions, pages 61?70.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer.
2003. An efficient boosting algorithm for combining prefer-
ences. Journal of Machine Learning Research, 4:933?969.
Jun?ichi Fukumoto, Tsuneaki Kato, and Fumito Masui. 2004.
Question answering challenge for five ranked answers and
list answers ? overview of NTCIR4 QAC2 subtask 1 and 2
?. In Proc. NTCIR, pages 283?290.
Jun?ichi Fukumoto, Tsuneaki Kato, Fumito Masui, and
Tsunenori Mori. 2007. An overview of the 4th question an-
swering challenge (QAC-4) at NTCIR workshop 6. In Proc.
NTCIR, pages 483?440.
Jun?ichi Fukumoto. 2007. Question answering system for non-
factoid type questions and automatic evaluation based on BE
method. In Proc. NTCIR, pages 441?447.
Roxana Girju. 2003. Automatic detection of causal relations
for question answering. In Proc. ACL 2003 Workshop on
Multilingual Summarization and Question Answering, pages
76?83.
Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Mat-
sumoto. 2002. Extracting important sentences with support
vector machines. In Proc. 19th COLING, pages 342?348.
Takashi Inui and Manabu Okumura. 2005. Investigating the
characteristics of causal relations in Japanese text. In Proc.
ACL 2005 Workshop on Frontiers in Corpus Annotation II:
Pie in the Sky.
Hideki Isozaki. 2004. NTT?s question answering system for
NTCIR QAC2. In Proc. NTCIR, pages 326?332.
Hideki Isozaki. 2005. An analysis of a high-performance
Japanese question answering system. ACM Transactions on
Asian Language Information Processing (TALIP), 4(3):263?
279.
Thorsten Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. KDD, pages 133?142.
Christopher S. G. Khoo, Syin Chan, and Yun Niu. 2000. Ex-
tracting causal knowledge from a medical database using
graphical patterns. In Proc. 38th ACL, pages 336?343.
W. Mann and S. Thompson. 1988. Rhetorical structure theory:
Toward a functional theory of text organization. In Text, vol-
ume 8, pages 243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. In Proc. 40th
ACL, pages 368?375.
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and Neus Catala`.
2005. Semantic role labeling as sequential tagging. In Proc.
CoNLL, pages 193?196.
Junta Mizuno, Tomoyosi Akiba, Atsushi Fujii, and Katunobu
Itou. 2007. Non-factoid question answering experiments
at NTCIR-6: Towards answer type detection for realworld
questions. In Proc. NTCIR, pages 487?492.
Tatsunori Mori, Mitsuru Sato, Madoka Ishioroshi, Yugo
Nishikawa, Shigenori Nakano, and Kei Kimura. 2007. A
monolithic approach and a type-by-type approach for non-
factoid question-answering ? YokohamaNational University
at NTCIR-6 QAC ?. In Proc. NTCIR, pages 469?476.
Martha Palmer. 2005. The proposition bank: An annotated
corpus of semantic roles. Comp. Ling., 31(1):71?106.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. Wordnet::Similarity - Measuring the Relatedness of
Concepts. In Proc. HLT-NAACL (Demonstration Papers),
pages 38?41.
Hideki Shima and Teruko Mitamura. 2007. JAVELIN III: An-
swering non-factoid questions in Japanese. In Proc. NTCIR,
pages 464?468.
Troy Smith, Thomas M. Repede, and Steven L. Lytinen. 2005.
Determining the plausibility of answers to questions. In
Proc. AAAI Workshop on Inference for Textual Question An-
swering, pages 52?58.
Radu Soricut and Eric Brill. 2006. Automatic question answer-
ing using the web: Beyond the factoid. Journal of Informa-
tion Retrieval, 9:191?206.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno
Coppen. 2007. Evaluating discourse-based answer extrac-
tion for why-question answering. In Proc. SIGIR (Posters
and Demonstrations), pages 735?736.
Suzan Verberne. 2006. Developing an approach for why-
question answering. In Proc. 11th European Chapter of
ACL, pages 39?46.
Suzan Verberne. 2007a. Evaluating answer extraction for why-
QA using RST-annotated Wikipedia texts. In Proc. 12th
ESSLLI Student Session, pages 255?266.
Suzan Verberne. 2007b. Paragraph retrieval for why-question
answering. In Proc. Doctoral Consortium Workshop at
SIGIR-2007, page 922.
Ellen M. Voorhees and Hoa Trang Dang. 2005. Overview of
the TREC 2005 question answering track. In Proc. TREC.
425
Multi-label Text Categorization with Model Combination
based on F1-score Maximization
Akinori Fujino, Hideki Isozaki, and Jun Suzuki
NTT Communication Science Laboratories
NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0237
{a.fujino,isozaki,jun}@cslab.kecl.ntt.co.jp
Abstract
Text categorization is a fundamental task in
natural language processing, and is gener-
ally defined as a multi-label categorization
problem, where each text document is as-
signed to one or more categories. We fo-
cus on providing good statistical classifiers
with a generalization ability for multi-label
categorization and present a classifier de-
sign method based on model combination
and F
1
-score maximization. In our formu-
lation, we first design multiple models for
binary classification per category. Then,
we combine these models to maximize the
F
1
-score of a training dataset. Our experi-
mental results confirmed that our proposed
method was useful especially for datasets
where there were many combinations of cat-
egory labels.
1 Introduction
Text categorization is a fundamental task in such
aspects of natural language processing as informa-
tion retrieval, information extraction, and text min-
ing. Since a text document often belongs to multiple
categories in real tasks such as web pages and in-
ternational patent categorization, text categorization
is generally defined as assigning one or more pre-
defined category labels to each data sample. There-
fore, developing better classifiers with a generaliza-
tion ability for such multi-label categorization tasks
is an important issue in the field of machine learning.
A major and conventional machine learning ap-
proach to multi-label categorization is based on bi-
nary classification. With this approach, we assume
the independence of categories and design a binary
classifier for each category that determines whether
or not to assign a category label to data samples.
Statistical classifiers such as the logistic regression
model (LRM), the support vector machine (SVM),
and naive Bayes are employed as binary classi-
fiers (Joachims, 1998).
In text categorization, the F
1
-score is often used
to evaluate classifier performance. Recently, meth-
ods for training binary classifiers to maximize the
F
1
-score have been proposed for SVM (Joachims,
2005) and LRM (Jansche, 2005). It was con-
firmed experimentally that these training methods
were more effective for obtaining binary classifiers
with better F
1
-score performance than the minimum
error rate and maximum likelihood used for train-
ing conventional classifiers, especially when there
was a large imbalance between positive and nega-
tive samples. In multi-label categorization, macro-
and micro-averaged F
1
-scores are often used to eval-
uate classification performance. Therefore, we can
expect to improve multi-label classification perfor-
mance by using binary classifiers trained to maxi-
mize the F
1
-score.
On the other hand, classification frameworks
based on classifier combination have also been stud-
ied in many previous works such as (Wolpert, 1992;
Larkey and Croft, 1996; Ting and Witten, 1999;
Ghahramani and Kim, 2003; Bell et al, 2005;
Fumera and Roli, 2005), to provide better classi-
fier systems. In the classifier combination research
field, it is known that weighted linear combinations
of multiple classifiers often provide better classifica-
tion performance than individual classifiers.
823
We present a classifier design method based on
the combination of multiple binary classifiers to im-
prove multi-label classification performance. In our
framework, we first train multiple binary classifiers
for each category. Then, we combine these bi-
nary classifiers with weights estimated to maximize
micro- or macro-averaged F
1
-scores, which are of-
ten used for evaluating multi-label classifiers. To es-
timate combination weights, we extend the F
1
-score
maximization training algorithm for LRM described
in (Jansche, 2005). Using three real text datasets,
we show experimentally that our classifier design
method is more effective than the conventional bi-
nary classification approaches to multi-label catego-
rization.
Our method is based on a binary classification ap-
proach. However, Kazawa et al (2005) proposed
a method for modeling a map directly from data
samples to the combination of assigned category la-
bels, and confirmed experimentally that the method
outperformed conventional binary classification ap-
proaches. Therefore, we also compare our method
with the direct mapping method experimentally.
2 F
1
-score Maximization Training of LRM
We first review the F
1
-score maximization training
method for linear models using a logistic function
described in (Jansche, 2005). The method was pro-
posed in binary classification settings, where classi-
fiers determine a class label assignment y ? {1, 0}
for a data sample represented by a feature vector x.
Here, y(n) = 1 (= 0) indicates that the class label is
assigned (unassigned) to the nth feature vector x(n).
The discriminative function of a binary classifier
based on a linear model is often defined as
f(x;?) = ?t
1
x + ?
0
, (1)
where ? = (?
0
,?t
1
)t is a model parameter vector,
and ?t
1
x implies the inner product of ?
1
and x. A
binary classifier using f(x;?) outputs a predicted
class label assignment y? for x as y?(n) = 1 (= 0)
when f(x(n);?) ? 0 (< 0).
An LRM is a binary classifier that uses the dis-
criminative function f(x;?). In this model, the
class posterior probability distribution is defined by
using a logistic function:
g(z) = {1 + exp(?z)}?1. (2)
That is, P (y = 1|x;?) = g(f(x;?)) and P (y =
0|x;?) = 1 ? P (y = 1|x;?) = g(?f(x;?)).
The LRM determines that y(n) = 1 (= 0) when
P (y = 1|x(n);?) ? 0.5 (< 0.5), since g(0) = 0.5.
The model parameter vector ? is usually estimated
to maximize the likelihood of P (y|x;?) for training
dataset D = {x(m), y(m)}Mm=1 and the prior proba-
bility density of ?:
JR(?) =
M
?
m=1
log P (y(m)|x(m);?) + log p(?). (3)
In this paper, the classifier design approach that em-
ploys this training method is called LRM-L.
By contrast, in the training method proposed
by (Jansche, 2005), the discriminative function
f(x;w) is estimated to maximize the F
1
-score of
training dataset D. This training method employs an
approximate form of the F
1
-score obtained by using
a logistic function.
The F
1
-score is defined as F
1
= 2(1/PR +
1/RE)?1, where PR and RE represent precision
and recall defined as PR = C/A and RE = C/B,
respectively. Here, C represents the number of data
samples whose true and predicted class label assign-
ments, y(n) and y?(n), respectively, correspond to 1.
A represents the number of data samples for which
y?
(n) = 1. B represents the number of data samples
for which y(n) = 1. C , A, and B are computed
for training dataset D as C =
?M
m=1 y
(m)
y?
(m)
,
A =
?M
m=1 y?
(m)
, and B =
?M
m=1 y
(m)
.
In (Jansche, 2005), y?(m) was approximated by us-
ing the discriminative and logistic functions shown
in Eqs. (1) and (2) as
y?
(m)
? g(?f(x(m);?)), ? > 0, (4)
because lim??? g(?f(x(m);?)) = y?(m). Then, an
approximate distribution of the F
1
-score for training
dataset D was provided as
F?
1
(?) =
2
?M
m=1 g(?f(x;?))y
(m)
?M
m=1 y
(m) +
?M
m=1 g(?f(x;?))
. (5)
The ? estimate for the discriminative function
f(x;?) can be computed to maximize JF (?) =
log F?
1
(?) + log p(?) around the initial ? value by
using a gradient method. In this paper, the classi-
fier design approach that uses this training method
is called LRM-F.
824
3 Proposed Method
We propose a framework for designing a multi-label
classifier based on the combination of multiple mod-
els. In our formulation, multiple models are com-
bined with weights estimated to maximize the F
1
-
scores of the training dataset. In this section, we
show our formulation for model combination and
training methods for combination weights.
3.1 Combination of Multiple Models for
Multi-label Categorization
Multi-label categorization is the task of selecting
multiple category labels from K pre-defined cat-
egory labels for each data sample. Multi-label
classifiers provide a map from a feature vector
x to a category label assignment vector y =
(y
1
, . . . , yk, . . . , yK)
t
, where y(n)k = 1 (= 0) indi-
cates that the kth category label is assigned (unas-
signed) to x(n).
In our formulation, we first design multiple mod-
els for binary classification per category and ob-
tain J ? K discriminative functions, where J is the
number of models. The discriminative function of
the jth model for the kth category is denoted by
fjk(x;?jk), where ?jk represents the model param-
eter vector. Let ? = {?jk}j,k be a model parameter
set. We train model parameter vectors individually
with each model training algorithm and obtain the
estimate ?? = {??jk}jk. Then, we define the dis-
criminative function of our multi-label classifier by
combining multiple models such as
fk(x; ??,w) =
J
?
j=1
wjfjk(x; ??jk) + w0, ?k, (6)
where w = (w
0
, w
1
, . . . , wj , . . . , wJ)
t is a weight
parameter vector and is independent of k. wj pro-
vides the combination weight of the jth model, and
w
0
is the bias factor for adjusting the threshold of
the category label assignment.
We estimate the w value to maximize the
micro-averaged F
1
-score (F?), which is often used
for evaluating multi-label categorization perfor-
mance. The F?-score of training dataset D =
{x(m),y(m)}Mm=1 is calculated as
F? =
2
?M
m=1
?K
k=1 y
(m)
k y?
(m)
k
?M
m=1
?K
k=1 y
(m)
k +
?M
m=1
?K
k=1 y?
(m)
k
, (7)
We provide an approximate form of the F?-score of
the training dataset, F??(??,w), by using the approx-
imation:
y?
(m)
k ? g(?fk(x
(m); ??,w)), ? > 0, (8)
as shown in Eq. (4). In our proposed method, w is
estimated to maximize F??(??,w).
However, training dataset D is also used to es-
timate ?. Using the same training data samples
for both ? and w may lead to a bias estimation of
w. Thus, we used an n-fold cross-validation of the
training data samples to estimate w as in (Wolpert,
1992). Let ??(?m) be the model parameter set esti-
mated by using n ? 1 training data subsets not con-
taining {x(m),y(m)}. Then, using
F?? =
2
?
m,k y
(m)
k g(?fk(x; ??
(?m)
,w))
?
m,k y
(m)
k +
?
m,k g(?fk(x; ??
(?m)
,w))
, (9)
we provide the objective function of w such that
J?(w) = log F?? + log p(w), (10)
where p(w) is a prior probability density of w.
We use a Gaussian prior (Chen and Rosenfeld,
1999) with the form as p(w) ? ?Jj=0 exp{?(wj ?
?j)
2
/2?2j }, where ?j , and ?j are hyperparameters in
the Gaussian prior. We compute an estimate of w to
maximize J?(w) around the initial w value by using
a quasi-Newton method. In this paper, this formula-
tion is called model combination by micro-averaged
F
1
-score maximization (MC-F?).
3.2 Other Training Methods
In multi-label categorization problems, the macro-
averaged F
1
-score (FM ) is also used to evaluate
classifiers. Moreover, the average labeling F
1
-score
(FL) has been used to evaluate the average labeling
performance of classifiers for data samples (Kazawa
et al, 2005). These F
1
-scores are computed for
training dataset D as
FM =
1
K
K
?
k=1
2
?M
m=1 y
(m)
k y?
(m)
k
?M
m=1 y
(m)
k +
?M
m=1 y?
(m)
k
, (11)
FL =
1
M
M
?
m=1
2
?K
k=1 y
(m)
k y?
(m)
k
?K
k=1 y
(m)
k +
?K
k=1 y?
(m)
k
. (12)
Using Eq. (8), we can also obtain the approxi-
mate forms, F?M (??,w) and F?L(??,w), of the FM -
825
and FL-scores, and then present similar objective
functions to that for the F?-score. Therefore, in
the next section, we examine experimentally the per-
formance of classifiers obtained by estimating w to
maximize F?M (??,w) and F?L(??,w). In this paper,
these model combination methods based on FM -
and FL-scores are called MC-FM and MC-FL, re-
spectively.
4 Experiments
4.1 Test Collections
To evaluate our proposed method empirically, we
used three test collections: Reuters-21578 (Reuters),
WIPO-alpha (WIPO), and Japanese Patent (JPAT)
datasets. Reuters and WIPO are English document
datasets and have often been employed for bench-
mark tests of multi-label classifiers.
The Reuters dataset contains news articles from
the Reuters newswire and consists of 135 topic cate-
gories. Following the setup in (Yang and Liu, 1999),
we extracted 7770 and 3019 articles as training and
test samples, respectively. A subset consisting of the
training and test samples contained 90 topic cate-
gories. We removed vocabulary words included ei-
ther in the stoplist or in only one article. There were
16365 vocabulary words in the dataset.
The WIPO dataset consists of patent documents
categorized using the International Patent Classifica-
tion (IPC) taxonomy (Fall et al, 2003). The IPC tax-
onomy has four hierarchical layers: Section, Class,
Subclass, and Group. Using patent documents be-
longing to Section D (textiles; paper), we evalu-
ated classifiers in a task that consisted of selecting
assigned category labels from 160 groups for each
patent document. Following the setting provided in
the dataset, we extracted 1352 and 358 patent docu-
ments as training and test samples, respectively. We
removed vocabulary words in the same way as for
Reuters. There were 45895 vocabulary words in the
dataset.
The JPAT dataset (Iwayama et al, 2007) con-
sists of Japanese patent documents published be-
tween 1993 and 1999 by the Japanese Patent Office.
These documents are categorized using a taxonomy
consisting of Themes and F-terms. The themes are
top-label categories, and the patent documents be-
longing to each theme are categorized by using F-
Reuters WIPO JPAT
Nav 1.17 1.28 10.5
Nmax 15 6 40
K 90 160 268
Nds 10789 1710 2464
NLC 468 378 2430
Nds/NLC 23.1 4.52 1.01
Table 1: Statistical information of three datasets:
Nav and Nmax are the average and maximum num-
ber of assigned category labels per data sample, re-
spectively. K and Nds are the number of category
labels and data samples, respectively. NLC is the
number of category label combinations appearing in
each dataset.
terms. Using patent documents belonging to Theme
5J104, we evaluated classifiers in a task that con-
sisted of selecting assigned category labels from 268
F-terms for each patent document. 1920 patent doc-
uments published between 1993 and 1997 were used
as training samples, and 544 patent documents pub-
lished between 1998 and 1999 were used test sam-
ples. We extracted Japanese nouns, verbs, and adjec-
tives from patent documents by using a morpholog-
ical analyzer named MeCab 1, and removed vocab-
ulary words included in only one patent document.
There were 21135 vocabulary words in the dataset.
Table 1 shows statistical information about the
category label assignment of the data samples for the
three datasets. The average numbers of assigned cat-
egory labels per data sample, Nav , for Reuters and
WIPO were close to 1 and much smaller than that
for JPAT. The number of category label combina-
tions, NLC , included in JPAT was larger than those
for Reuters and WIPO. These statistical information
results show that JPAT is a more complex multi-label
dataset than Reuters or WIPO.
4.2 Experimental Settings
For text categorization tasks, we employed word-
frequency vectors of documents as feature vectors
input into classifiers, using the independent word-
based representation, known as the Bag-of-Words
(BOW) representation. We normalized the L1-
norms of the word-frequency vectors to 1, to miti-
gate the effect of vector size on computation. We
did not employ any word weighting methods such
as inverse document frequency (IDF).
1http://mecab.sourceforge.net/
826
We constructed three multi-label text classifiers
based on our proposed model combination methods,
MC-F?, MC-FM , and MC-FL, where LRM and
SVM (J = 2) were employed as binary classifica-
tion models combined with each method. We trained
the LRM by using LRM-L described in Section 2,
where a Gaussian prior was used as the prior proba-
bility density of the parameter vectors. We provided
the SVM by using SVMlight 2 (SVM-L), where we
employed a linear kernel function and tuned the C
(penalty cost) parameter as a hyperparameter.
To evaluate our proposed method, we examined
the micro- and macro-averaged, and average label-
ing F
1
-scores (F?, FM , and FL), of test samples ob-
tained with the three classifiers based on MC-F?,
MC-FM , and MC-FL. We compared the perfor-
mance of the three classifiers with that of two binary
classification approaches, where LRM-L or SVM-L
was used for binary classification.
We also examined two binary classification ap-
proaches using LRM-F and SVM-F. For LRM-F, we
used a Gaussian prior and provided the initial pa-
rameter vector with a parameter estimate obtained
with LRM-L. SVM-F is a binary classifier design
approach that employs SVMperf 3. For SVM-F, we
used a linear kernel function, set the L (loss parame-
ter) parameter to maximize the F
1
-score, and tuned
the C (penalty cost) parameter as a hyperparameter.
Moreover, we examined the performance of the
Maximal Margin Labeling (MML) method (Kazawa
et al, 2005), which models the map from feature
vectors to category label assignment vectors, be-
cause it was reported that MML provides better per-
formance than binary classification approaches.
We tuned the hyperparameter of SVM-F for JPAT
to provide good performance for test samples, be-
cause the computational cost for training was high.
We tuned the other hyperparameters by using a 10-
fold cross-validation of training samples.
4.3 Results and Discussion
In Table 2, we show the classification performance
obtained for three datasets with our proposed and
other methods described in Section 4.2. We ex-
amined nine evaluation scores: the micro-averaged
F
1
-score (F?), precision (P?), and recall (R?), the
2http://svmlight.joachims.org/
3http://svmlight.joachims.org/svm perf.html
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 87.0 (87.4/86.7) 51.3 (60.0/48.4) 90.0 (90.1/92.3)
MC-FM 85.0 (80.8/89.5) 53.9 (54.9/58.4) 89.7 (88.5/94.1)
MC-FL 86.3 (84.3/88.3) 53.4 (59.6/52.6) 90.0 (89.3/93.6)
LRM-L 85.2 (87.3/83.2) 46.1 (55.0/43.1) 86.9 (87.6/88.6)
LRM-F 85.2 (87.2/83.2) 47.4 (58.5/42.7) 87.0 (87.6/88.7)
SVM-L 87.1 (92.9/82.0) 48.9 (58.9/45.8) 88.1 (89.3/88.8)
SVM-F 82.4 (78.9/86.2) 51.4 (49.4/60.1) 87.4 (86.9/91.4)
MML 87.8 (92.6/83.4) 59.3 (62.6/60.0) 91.2 (91.7/93.2)
(a) Reuters
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 51.4 (57.3/46.6) 30.4 (35.8/30.3) 46.9 (48.3/51.5)
MC-FM 48.1 (46.1/50.4) 32.2 (33.8/36.0) 46.8 (46.3/56.0)
MC-FL 48.6 (45.8/51.9) 32.5 (33.4/36.5) 47.1 (46.4/56.8)
LRM-L 40.5 (68.0/28.9) 22.1 (33.7/17.9) 32.7 (36.5/32.0)
LRM-F 41.0 (68.6/29.2) 22.3 (34.0/18.1) 33.2 (37.0/32.4)
SVM-L 41.8 (61.9/31.5) 24.4 (34.2/21.0) 35.1 (38.8/35.3)
SVM-F 48.3 (53.8/43.8) 32.3 (37.4/31.8) 45.6 (47.9/49.6)
MML 48.6 (54.9/43.6) 30.8 (36.5/29.7) 49.4 (56.2/48.4)
(b) WIPO
Method F? (P?/R?) FM (PM /RM ) FL (PL/RM )
MC-F? 41.8 (42.6/41.1) 17.5 (21.4/17.4) 40.2 (43.5/44.4)
MC-FM 40.6 (35.8/46.7) 20.2 (20.4/23.1) 39.4 (37.7/50.6)
MC-FL 42.1 (42.3/41.9) 17.6 (21.1/17.8) 40.5 (43.2/45.2)
LRM-L 33.9 (44.4/27.4) 15.8 (20.9/14.0) 32.2 (46.5/29.9)
LRM-F 36.9 (44.6/31.5) 16.9 (22.9/14.7) 35.1 (47.3/34.1)
SVM-L 33.3 (39.6/28.7) 16.3 (20.9/14.6) 31.9 (42.4/31.6)
SVM-F 32.2 (28.6/36.8) 19.7 (15.0/38.4) 31.0 (30.7/40.0)
MML 32.7 (42.1/26.8) 14.7 (19.4/12.9) 32.2 (51.8/30.5)
(c) JPAT
Table 2: Micro- and macro-averaged, and average
labeling F
1
-scores (%) with our proposed and con-
ventional methods.
macro-averaged F
1
-score (FM ), precision (PM ),
and recall (RM ), and the average labeling F1-score
(FL), precision (PL), and recall (RL) of the test sam-
ples. FM and PM were calculated by regarding both
the F
1
-score and precision as zero for the categories
where there were no data samples predicted as posi-
tive samples.
LRM-F and SVM-F outperformed LRM-L and
SVM-L in terms of FM -score for the three datasets,
respectively. The training methods of LRM-F and
SVM-F were useful to improve the FM -scores of
LRM and SVM, as reported in (Jansche, 2005;
Joachims, 2005). The F?- and FL-scores of LRM-F
were similar or better than those of LRM-L. LRM-
F was effective in improving not only the FM -score
but also the F?- and FL-scores obtained with LRM.
Let us evaluate our model combination methods.
827
MC-F? provided better F?-scores than LRM-F and
SVM-F. The FM -scores of MC-FM were similar or
better than those of LRM-F and SVM-F. Moreover,
MC-FL outperformed LRM-F and SVM-F in terms
of FL-scores. The binary classifiers designed by us-
ing LRM-F and SVM-F were trained to maximize
the F
1
-score for each category. On the other hand,
MC-F?, MC-FM , and MC-FL classifiers were con-
structed by combining LRM and SVM with weights
estimated to maximize the F?-, FM -, and FL-scores,
respectively. The experimental results show that our
training methods for combination weights were use-
ful for obtaining better multi-label classifiers.
MC-F?, MC-FM , and MC-FL outperformed
MML as regards the three F
1
-scores for JPAT. How-
ever, MML performed better for Reuters than MC-
F?, MC-FM , and MC-FL, and provided a better FL-
score for WIPO. As shown in Table 1, there were
more category label combinations for JPAT than for
Reuters or WIPO. As a result, there were fewer data
samples for the same category label assignment for
JPAT. Therefore, MML, which learns the map di-
rectly from the feature vectors to the category label
assignment vectors, would have been overfitted to
the training dataset for JPAT. By contrast, our model
combination methods employ binary classifiers for
each category, which mitigates such an overfitting
problem. Our model combination methods will be
useful for complex datasets where there are many
category label combinations.
5 Conclusion
We proposed a multi-label classifier design method
based on model combination. The main idea be-
hind our proposed method is to combine multiple
models with weights estimated to maximize evalua-
tion scores such as the micro- and macro-averaged,
and average labeling F
1
-scores. Using three real
text datasets, we confirmed experimentally that our
proposed method provided similar or better perfor-
mance than conventional binary classification ap-
proaches to multi-label categorization. We also con-
firmed that our proposed method was useful for
datasets where there were many combinations of
category labels. Future work will involve training
our multi-label classifier by using labeled and un-
labeled samples, which are data samples with and
without category label assignment.
References
David A. Bell, J. W. Guan, and Yaxin Bi. 2005. On
combining classifier mass functions for text categorization.
IEEE Transactions on Knowledge and Data Engineering,
17(10):1307?1319.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior
for smoothing maximum entropy models. Technical report,
Carnegie Mellon University.
C. J. Fall, A. To?rcsva?ri, K. Benzineb, and G. Karetka. 2003.
Automated categorization in the international patent classifi-
cation. ACM SIGIR Forum, 37(1):10?25.
Giorgio Fumera and Fabio Roli. 2005. A theoretical and exper-
imental analysis of linear combiners for multiple classifier
systems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 27(6):942?956.
Zoubin Ghahramani and Hyun-Chul Kim. 2003. Bayesian
classifier combination. Technical report, Gatsby Computa-
tional Neuroscience Unit, University College London.
Makoto Iwayama, Atsushi Fujii, and Noriko Kando. 2007.
Overview of classification subtask at NTCIR-6 patent re-
trieval task. In Proceedings of the 6th NTCIR Workshop
Meeting on Evaluation of Information Access Technologies
(NTCIR-6), pages 366?372.
Martin Jansche. 2005. Maximum expected F-measure train-
ing of logistic regression models. In Proceedings of
Human Language Technology Conference and Conference
on Empirical Methods in Natural Language Processing
(HLT/EMNLP2005), pages 692?699.
Thorsten Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant features. In
Proceedings of the 10th European Conference on Machine
Learning (ECML ?98), pages 137?142.
Thorsten Joachims. 2005. A support vector method for multi-
variate performance measures. In Proceedings of the 22nd
International Conference on Machine Learning (ICML?05),
pages 377?384.
Hideto Kazawa, Tomonori Izumitani, Hirotoshi Taira, and
Eisaku Maeda. 2005. Maximal margin labeling for multi-
topic text categorization. In Advances in Neural Information
Processing Systems 17, pages 649?656. MIT Press, Cam-
bridge, MA.
Leah S. Larkey and W. Bruce Croft. 1996. Combining classi-
fiers in text categorization. In Proceedings of the 19th ACM
International Conference on Research and Development in
Information Retrieval (SIGIR-96), pages 289?297.
Kai Ming Ting and Ian H. Witten. 1999. Issues in stacked
generalization. Journal of Artificial Intelligence Research,
10:271?289.
David H. Wolpert. 1992. Stacked generalization. Newral Net-
works, 5(2):241?259.
Yiming Yang and Xin Liu. 1999. A re-examination of text
categorization methods. In Proceedings of the 22nd ACM
International Conference on Research and Development in
Information Retrieval (SIGIR-99), pages 42?49.
828
Japanese Named Entity Recognition based on
a Simple Rule Generator and Decision Tree Learning
Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Souraku-gun, Kyoto
619-0237, Japan
isozaki@cslab.kecl.ntt.co.jp
Abstract
Named entity (NE) recognition is a
task in which proper nouns and nu-
merical information in a document are
detected and classified into categories
such as person, organization, location,
and date. NE recognition plays an es-
sential role in information extraction
systems and question answering sys-
tems. It is well known that hand-crafted
systems with a large set of heuris-
tic rules are difficult to maintain, and
corpus-based statistical approaches are
expected to be more robust and require
less human intervention. Several statis-
tical approaches have been reported in
the literature. In a recent Japanese NE
workshop, a maximum entropy (ME)
system outperformed decision tree sys-
tems and most hand-crafted systems.
Here, we propose an alternative method
based on a simple rule generator and
decision tree learning. Our exper-
iments show that its performance is
comparable to the ME approach. We
also found that it can be trained more
efficiently with a large set of training
data and that it improves readability.
1 Introduction
Named entity (NE) recognition is a task in
which proper nouns and numerical informa-
tion in a document are detected and classi-
fied into categories such as person, organiza-
tion, location, and date. NE recognition plays
an essential role in information extraction sys-
tems (see MUC documents (1996)) and ques-
tion answering systems (see TREC-QA docu-
ments, http://trec.nist.gov/). When
you want to know the location of the Taj Ma-
hal, traditional IR techniques direct you to rele-
vant documents but do not directly answer your
question. NE recognition is essential for finding
possible answers from documents. Although it
is easy to build an NE recognition system with
mediocre performance, it is difficult to make it re-
liable because of the large number of ambiguous
cases. For instance, we cannot determine whether
?Washington? is a person?s name or a location?s
name without the necessary context.
There are two major approaches to building NE
recognition systems. The first approach employs
hand-crafted rules. It is well known that hand-
crafted systems are difficult to maintain because it
is not easy to predict the effect of a small change
in a rule. The second approach employs a statis-
tical method, which is expected to be more robust
and to require less human intervention. Several
statistical methods have been reported in the liter-
ature (Bikel et al, 1999; Borthwick, 1999; Sekine
et al, 1998; Sassano and Utsuro, 2000).
IREX (Information Retrieval and Extraction
Exercise, (Sekine and Eriguchi, 2000; IRE,
1999)) was held in 1999, and fifteen systems par-
ticipated in the formal run of the Japanese NE ex-
cercise. In the formal run, participants were re-
quested to tag two data sets (GENERAL and AR-
REST), and their scores were compared in terms
of F-measure, i.e., the harmonic mean of ?recall?
and ?precision? defined as follows.
  recall = x/(the number of correct NEs)
  precision = x/(the number of NEs extracted
by the system)
where x is the number of NEs correctly ex-
tracted and classified by the system.
GENERAL was the larger test set, and its
best system was a hand-crafted one that at-
tained F=83.86%. The second best system
(F=80.05%) was also hand-crafted but enhanced
with transformation-based error-driven learning.
The third best system (F=77.37%) was Borth-
wick?s ME system enhanced with hand-crafted
rules and dictionaries (1999). Thus, the best three
systems used quite different approaches.
In this paper, we propose an alternative ap-
proach based on a simple rule generator and de-
cision tree learning (RG+DT). Our experiments
show that its performance is comparable to the
ME method, and we found that it can be trained
more efficiently with a large set of training data.
By adding in-house data, the proposed system?s
performance was improved by several points,
while a standard ME toolkit crashed.
When we try to extract NEs in Japanese, we
encounter several problems that are not serious
in English. It is relatively easy to detect En-
glish NEs because of capitalization. In Japanese,
there is no such useful hint. Proper nouns and
common nouns look very similar. In English,
it is also easy to tokenize a sentence because of
inter-word spacing. In Japanese, inter-word spac-
ing is rarely used. We can use an off-the-shelf
morphological analyzer for tokenization, but its
word boundaries may differ from the correspond-
ing NE boundaries in the training data. For in-
stance, a morphological analyzer may divide a
four-character expression OO-SAKA-SHI-NAI
into two words OO-SAKA (= Osaka) and SHI-
NAI (= in the city), but the training data would be
tagged as <LOCATION>OO-SAKA-SHI</LO-
CATION>NAI (= in <LOCATION>Osaka City
</LOCATION>). Moreover, unknown words are
often divided excessively or incorrectly because
an analyzer tries to interpret a sentence as a se-
quence of known words.
Throughout this paper, the typewriter-style font
is used for Japanese, and hyphens indicate char-
acter boundaries. Different types of charac-
ters are used in Japanese: hiragana, katakana,
kanji, symbols, numbers, and letters of the Ro-
man alphabet. We use 17 character types for
words, e.g., single-kanji, all-kanji,
all-katakana, all-uppercase, float
(for floating point numbers), small-integer
(up to 4 digits).
2 Methodology
Our RG+DT system (Fig. 1) generates a recogni-
tion rule from each NE in the training data. Then,
the rule is refined by decision tree learning. By
applying the refined recognition rules to a new
document, we get NE candidates. Then, non-
overlapping candidates are selected by a kind of
longest match method.
2.1 Generation of recognition rules
In our method, each tokenized NE is converted
to a recognition rule that is essentially a sequence
of part-of-speech (POS) tags in the NE. For in-
stance, OO-SAKA-GIN-KOU (= Osaka Bank)
is tokenized into two words: OO-SAKA:all-
kanji:location-name (= Osaka) and GIN-
KOU:all-kanji:common-noun (= Bank),
where location-name and common-noun
are POS tags. In this case, we get the following
recognition rule. Here, ?*? matches anything.
*:*:location-name,
*:*:common-noun
-> ORGANIZATION
However, this rule is not very good. For in-
stance, OO-SAKA-WAN (= Osaka Bay) follows
this pattern, but it is a location?s name. GIN-
KOU and WAN strongly imply ORGANIZATION
and LOCATION, respectively. Thus, the last word
of an NE is often a head that is more useful than
other words for the classification. Therefore, we
register the last word into a suffix dictionary for
each non-numerical NE class (i.e., ORGANIZA-
TION, PERSON, LOCATION, and ARTIFACT)
in order to accept only reliable candidates. If the
last word appears in two or more different NE, we
call it a reliable NE suffix. We register only reli-
able ones.
NE candidatesdocument
recog. rule 1
recog. rule 2
recog. rule n
:
dt-rules 1
dt-rules 2
dt-rules n
:
(longest match)
arbitration NE index
Figure 1: Rough sketch of RG+DT system
In the above examples, the last words were
common nouns. However, the last word can also
be a proper noun. For instance, we will get
the following rule from <ORGANIZATION>OO-
SAKA-TO-YO-TA</ORGANIZATION> (= Os-
aka Toyota) because Japanese POS taggers know
that TO-YO-TA is an organization name (a kind
of proper noun).
*:*:location-name, *:*:org-name
-> ORGANIZATION,0,0
Since Yokohama Honda and Kyoto Sony
also follow this pattern, the second element
*:*:org-name should not be restricted to the
words in the training data. Therefore, we do not
restrict proper nouns by a suffix dictionary, and
we do not restrict numbers either.
In addition, the first or last word of an NE may
contain an NE boundary as we described before
(SHI</LOCATION>NAI). In this case, we can
get OO-SAKA-SHI by removing no character of
the first word OO-SAKA and one character of the
last word SHI-NAI. Accordingly, this modifica-
tion can be represented by two integers: 0,1.
Furthermore, one-word NEs are different from
other NEs in the following respects.
  The word is usually a proper noun, an un-
known word, or a number; otherwise, it is an
exceptional case.
  The character type of a one-word NE gives a
useful hint for its classification. For instance,
all-uppercasewords (e.g., IOC) are of-
ten classified as ORGANIZATION.
Since unknown words are often proper
nouns, we assume they are tagged as
misc-proper-noun. If the training
data contains <ORGANIZATION>I-O-
C</ORGANIZATION> and I-O-C (= IOC) is
an unknown word, we will get I-O-C:all-
uppercase:misc-proper-noun.
By considering these facts, we modify the
above rule generation. That is, we replace every
word in an NE and its character type by ?*? to get
the left-hand side of the corresponding recogni-
tion rule except the following cases.
A word that contains an NE boundary If the
first or last word of the NE contains an NE
boundary (e.g, SHI</LOCATION>NAI),
the word is not replaced by ?*?. The number
of characters to be deleted is also recorded
in the right-hand side of the recognition rule.
One-word NE The following exceptions are ap-
plied to one-word NEs. If the word is a
proper noun or a number, its character type
is not replaced by ?*?. Otherwise, the word
is not replaced by ?*?.
The last word of a longer NE The following
exceptions are applied to the last word of a
non-numerical NE that is composed of two
or more words when the word is neither a
proper noun nor a number. If the last word
is a reliable NE suffix (i.e., it appears in
two or more different NEs in the class), its
information (i.e., the last word, its character
type, and its POS tag) is registered into a
suffix dictionary for the NE class. The last
word of the recognition rule must be an ele-
ment of the suffix dictionary. Unreliable NE
suffixes are not replaced by ?*?. Suffixes of
numerical NEs (i.e., DATE, TIME, MONEY,
PERCENT) are not replaced, either.
Now, we obtain the following recognition rules
from the above examples.
*:all-uppercase:misc-proper-noun
-> ORGANIZATION,0,0.
*:*:location-name,
SHI-NAI:*:common-noun
-> LOCATION,0,1.
*:*:location-name,
*:*:common-noun
-> ORGANIZATION,0,0.
The first rule extracts CNN as an organization.
The second rule extracts YOKO-HAMA-SHI (=
Yokohama City) from YOKO-HAMA-SHI-NAI
(= in Yokohama City). The third rule extracts
YOKO-HAMA-GIN-KOU (= Yokohama Bank) as
an organization. Note that, in this rule, the second
element (*:*:common-noun) is constrained
by the suffix dictionary for ORGANIZATION be-
cause it is neither a proper noun nor a number.
Hence, the rule does not match YOKO-HAMA-
WAN (= Yokohama Bay). If the suffix dictionary
also happens to have KOU-KOU:all-kanji:
commmon-noun (= senior high school), the rule
also matches YOKO-HAMA-KOU-KOU (= Yoko-
hama Senior High School).
IREX introduced <ARTIFACT> for product
names, prizes, pacts, books, and fine arts, among
other nouns. Titles of books and fine arts are often
long and have atypical word patterns. However,
they are often delimited by a pair of symbols that
correspond to quotation marks in English. Some
atypical organization names are also delimited by
these symbols. In order to extract such a long NE,
we concatenate all words within a pair of such
symbols into one word. We employ the first and
last word of the quoted words as extra features. In
addition, we do not regard the quotation symbols
as adjacent words because they are constant and
lack semantic meaning.
When a large amount of training data is given,
thousands of recognition rules are generated. For
efficiency, we compile these recognition rules by
using a hash table that converts a hash key into
a list of relevant rules that have to be examined.
We make this hash table as follows. If the left-
hand side of a rule contains only one element, the
element is used as a hash key and its rule identi-
fier is appended to the corresponding rule list. If
the left-hand side contains two or more elements,
the first two elements are concatenated and used
as a hash key and its rule identifier is appended
to the corresponding rule list. After this compila-
tion, we can efficiently apply all of the rules to a
new document. By taking the first two elements
into consideration, we can reduce the number of
rules that need to be examined.
2.2 Refinement of recognition rules
Some recognition rules are not reliable. For in-
stance, we get the following rule when a person?s
name is incorrectly tagged as a location?s name
by a POS tagger.
*:all-kanji:location-name
-> PERSON,0,0
Therefore, we have to consider a way to refine the
recognition rules.
By applying each recognition rule to the un-
tagged training data, we can obtain NE candidates
for the rule. By comparing the candidates with the
given answer for the training data, we can classify
them into positive examples and negative exam-
ples for the recognition rule. Consequently, we
can apply decision tree learning to classify these
examples correctly. We represent each example
by a list of features: words in the NEs,

pre-
ceding words,  succeeding words, their character
types, and their POS tags. If we consider one pre-
ceding word and two succeeding words, the fea-
ture list for a two-word named entity (
	 ) will
be 
 	 ,  	 ,  	 ,   ,   ,   ,  	 ,  	 ,  	 ,  ,  ,

 ,  ,  ,  ,  , where   	 is the preceding
word and 

and  are the succeeding words.
 is  ?s character type and Spoken Interactive ODQA System: SPIQA
Chiori Hori, Takaaki Hori, Hajime Tsukada,
Hideki Isozaki, Yutaka Sasaki and Eisaku Maeda
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
We have been investigating an interactive
approach for Open-domain QA (ODQA)
and have constructed a spoken interactive
ODQA system, SPIQA. The system de-
rives disambiguating queries (DQs) that
draw out additional information. To test
the efficiency of additional information re-
quested by the DQs, the system recon-
structs the user?s initial question by com-
bining the addition information with ques-
tion. The combination is then used for an-
swer extraction. Experimental results re-
vealed the potential of the generated DQs.
1 Introduction
Open-domain QA (ODQA), which extracts answers
from large text corpora, such as newspaper texts, has
been intensively investigated in the Text REtrieval
Conference (TREC). ODQA systems return an ac-
tual answer in response to a question written in a
natural language. However, the information in the
first question input by a user is not usually sufficient
to yield the desired answer. Interactions for col-
lecting additional information to accomplish QA are
needed. To construct more precise and user-friendly
ODQA systems, a speech interface is used for the
interaction between human beings and machines.
Our goal is to construct a spoken interactive
ODQA system that includes an automatic speech
recognition (ASR) system and an ODQA system.
To clarify the problems presented in building such
a system, the QA systems constructed so far have
been classified into a number of groups, depending
on their target domains, interfaces, and interactions
to draw out additional information from users to ac-
complish set tasks, as is shown in Table 1. In this
table, text and speech denote text input and speech
input, respectively. The term ?addition? represents
additional information queried by the QA systems.
This additional information is separate to that de-
rived from the user?s initial questions.
Table 1: Domain and data structure for QA systems
target domain specific open
data structure knowledge DB unstructured text
without addition CHAT-80 SAIQAtext
with addition MYCIN (SPIQA?)
without addition Harpy VAQA
speech
with addition JUPITER (SPIQA?)
? SPIQA is our system.
To construct spoken interactive ODQA systems,
the following problems must be overcome: 1. Sys-
tem queries for additional information to extract an-
swers and effective interaction strategies using such
queries cannot be prepared before the user inputs the
question. 2. Recognition errors degrade the perfor-
mance of QA systems. Some information indispens-
able for extracting answers is deleted or substituted
with other words.
Our spoken interactive ODQA system, SPIQA,
copes with the first problem by adopting disam-
biguating users? questions using system queries. In
addition, a speech summarization technique is ap-
plied to handle recognition errors.
2 Spoken Interactive QA system: SPIQA
Figure 1 shows the components of our system, and
the data that flows through it. This system com-
prises an ASR system (SOLON), a screening filter
that uses a summarization method, and ODQA en-
gine (SAIQA) for a Japanese newspaper text corpus,
a Deriving Disambiguating Queries (DDQ) module,
and a Text-to-Speech Synthesis (TTS) engine (Fi-
nalFluet).
ASR
TTS
Screening
filter
ODQA engine
(SAIQA)
DDQ
module
Answer
derived?
Answer
sentence generator
Question
reconstructor
No
Yes
Additional
info. New question
First
question
Question/
Additional info.
User Answer/
DDQ speech
Answer
sentence
DDQ
sentence
Recognition
result
Answer
Figure 1: Components and data flow in SPIQA.
ASR system
Our ASR system is based on the Weighted Finite-
State Transducers (WFST) approach that is becom-
ing a promising alternative formulation for the tra-
ditional decoding approach. The WFST approach
offers a unified framework representing various
knowledge sources in addition to producing an op-
timized search network of HMM states. We com-
bined cross-word triphones and trigrams into a sin-
gle WFST and applied a one-pass search algorithm
to it.
Screening filter
To alleviate degradation of the QA?s perfor-
mance by recognition errors, fillers, word fragments,
and other distractors in the transcribed question, a
screening filter that removes these redundant and
irrelevant information and extracts meaningful in-
formation is required. The speech summarization
approach (C. Hori et. al., 2003) is applied to the
screening process, wherein a set of words maximiz-
ing a summarization score that indicates the appro-
priateness of summarization is extracted automati-
cally from a transcribed question, and these words
are then concatenated together. The extraction pro-
cess is performed using a Dynamic Programming
(DP) technique.
ODQA engine
The ODQA engine, SAIQA, has four compo-
nents: question analysis, text retrieval, answer hy-
pothesis extraction, and answer selection.
DDQ module
When the ODQA engine cannot extract an appro-
priate answer to a user?s question, the question is
considered to be ?ambiguous.? To disambiguate the
initial questions, the DDQ module automatically de-
rives disambiguating queries (DQs) that require in-
formation indispensable for answer extraction. The
situations in which a question is considered ambigu-
ous are those when users? questions exclude indis-
pensable information or indispensable information
is lost through ASR errors. These instances of miss-
ing information should be compensated for by the
users.
To disambiguate a question, ambiguous phrases
within it should be identified. The ambiguity of
each phrase can be measured by using the struc-
tural ambiguity and generality score for the phrase.
The structural ambiguity is based on the dependency
structure of the sentence; phrase that is not modified
by other phrases is considered to be highly ambigu-
ous. Figure 2 has an example of a dependency struc-
ture, where the question is separated into phrases.
Each arrow represents the dependency between two
phrases. In this example, ?the World Cup? has no
Which  country won the  world  cupin Southeast Asia ?
Figure 2: Example of dependency structure.
modifiers and needs more information to be identi-
fied. ?Southeast Asia? also has no modifiers. How-
ever, since ?the World Cup?appears more frequently
than ?Southeast Asia? in the retrieved corpus, ?the
World Cup? is more difficult to identify. In other
words, words that frequently occur in a corpus rarely
help to extract answers in ODQA systems. There-
fore, it is adequate for the DDQ module to generate
questions relating to ?World Cup? in this example,
such as ?What kind of World Cup?? , ?What year
was the World Cup held??.
The structural ambiguity of the n-th phrase is de-
fined as
A
D
(P
n
) = log
{
1 ?
?
N
i=1:i=n
D(P
i
, P
n
)
}
,
where the complete question is separated into N
phrases, and D(P
i
, P
n
) is the probability that phrase
P
n
will be modified by phrase P
i
, which can be cal-
culated using Stochastic Dependency Context-Free
Grammar (SDCFG) (C. Hori et. al., 2003).
Using this SDCFG, only the number of non-
terminal symbols is determined and all combina-
tions of rules are applied recursively. The non-
terminal symbol has no specific function, such as
a noun phrase. All the probabilities of rules are
stochastically estimated based on data. Probabilities
for frequently used rules become greater, and those
for rarely used rules become smaller. Even though
transcription results given by a speech recognizer are
ill-formed, the dependency structure can be robustly
estimated by our SDCFG.
The generality score is defined as
A
G
(P
n
) =
?
w?P
n
:w=cont log P (w),
where P (w) is the unigram probability of w based
on the corpus to be retrieved. Thus, ?w = cont?
means that w is a content word such as a noun, verb
or adjective.
We generate the DQs using templates of interrog-
ative sentences. These templates contain an inter-
rogative and a phrase taken from the user?s question,
i.e., ?What kind of * ??, ?What year was * held??
and ?Where is * ??.
The DDQ module selects the best DQ based on its
linguistic appropriateness and the ambiguity of the
phrase. The linguistic appropriateness of DQs can
be measured by using a language model, N-gram.
Let S
mn
be a DQ generated by inserting the n-th
phrase into the m-th template. The DDQ module
selects the DQ that maximizes the DQ score:
H(S
mn
) = ?
L
L(S
mn
)+?
D
A
D
(P
n
)+?
G
A
G
(P
n
),
where L(?) is a linguistic score such as the loga-
rithm for trigram probability, and ?
L
, ?
D
, and ?
G
are weighting factors to balance the scores.
Hence, the module can generate a sentence that
is linguistically appropriate and asks the user to dis-
ambiguate the most ambiguous phrase in his or her
question.
3 Evaluation Experiments
Questions consisting of 69 sentences read aloud by
seven male speakers were transcribed by our ASR
system. The question transcriptions were processed
with a screening filter and input into the ODQA
engine. Each question consisted of about 19 mor-
phemes on average. The sentences were grammat-
ically correct, formally structured, and had enough
information for the ODQA engine to extract the cor-
rect answers. The mean word recognition accuracy
obtained by the ASR system was 76%.
3.1 Screening filter
Screening was performed by removing recognition
errors using a confidence measure as a threshold and
then summarizing it within an 80% to 100% com-
paction ratio. In this summarization technique, the
word significance and linguistic score for summa-
rization were calculated using text from Mainichi
newspapers published from 1994 to 2001, compris-
ing 13.6M sentences with 232M words. The SD-
CFG for the word concatenation score was calcu-
lated using the manually parsed corpus of Mainichi
newspapers published from 1996 to 1998, consist-
ing of approximately 4M sentences with 68M words.
The number of non-terminal symbols was 100. The
posterior probability of each transcribed word in a
word graph obtained by ASR was used as the confi-
dence score.
3.2 DDQ module
The word generality score A
G
was computed using
the same Mainichi newspaper text described above,
while the SDCFG for the dependency ambiguity
score A
D
for each phrase was the same as that used
in (C. Hori et. al., 2003). Eighty-two types of inter-
rogative sentences were created as disambiguating
queries for each noun and noun-phrase in each ques-
tion and evaluated by the DDQ module. The linguis-
tic score L indicating the appropriateness of inter-
rogative sentences was calculated using 1000 ques-
tions and newspaper text extracted for three years.
The structural ambiguity score A
D
was calculated
based on the SDCFG, which was used for the screen-
ing filter.
3.3 Evaluation method
The DQs generated by the DDQ module were eval-
uated in comparison with manual disambiguation
queries. Although the questions read by the seven
speakers had sufficient information to extract ex-
act answers, some recognition errors resulted in a
loss of information that was indispensable for ob-
taining the correct answers. The manual DQs were
made by five subjects based on a comparison of
the original written questions and the transcription
results given by the ASR system. The automatic
DQs were categorized into two classes: APPRO-
PRIATE when they had the same meaning as at
least one of the five manual DQs, and INAPPRO-
PRIATE when there was no match. The QA per-
formance in using recognized (REC) and screened
questions (SCRN) were evaluated by MRR (Mean
Reciprocal Rank) (http://trec.nist.gov/data/qa.html).
SCRN was compared with the transcribed question
that just had recognition errors removed (DEL). In
addition, the questions reconstructed manually by
merging these questions and additional information
requested the DQs generated by using SCRN, (DQ)
were also evaluated. The additional information was
extracted from the original users? question without
recognition errors. In this study, adding information
by using the DQs was performed only once.
3.4 Evaluation results
Table 2 shows the evaluation results in terms of
the appropriateness of the DQs and the QA-system
MRRs. The results indicate that roughly 50% of the
DQs generated by the DDQ module based on the
screened results were APPROPRIATE. The MRR
for manual transcription (TRS) with no recognition
errors was 0.43. In addition, we could improve the
MRR from 0.25 (REC) to 0.28 (DQ) by using the
DQs only once. Experimental results revealed the
potential of the generated DQs in compensating for
the degradation of the QA performance due to recog-
nition errors.
4 Conclusion
The proposed spoken interactive ODQA system,
SPIQA copes with missing information by adopt-
ing disambiguation of users? questions by system
queries. In addition, a speech summarization tech-
nique was applied for handling recognition errors.
Although adding information was performed using
DQs only once, experimental results revealed the
potential of the generated DQs to acquire indispens-
able information that was lacking for extracting an-
swers. In addition, the screening filter helped to gen-
erate the appropriate DQs. Future research will in-
Table 2: Evaluation results of disambiguating
queries generated by the DDQ module.
Word MRR w/o IN-SPK
acc. REC DEL SCRN DQ errors APP APP
A 70% 0.19 0.16 0.17 0.23 4 32 33
B 76% 0.31 0.24 0.29 0.31 8 36 25
C 79% 0.26 0.18 0.26 0.30 10 34 25
D 73% 0.27 0.21 0.24 0.30 4 35 30
E 78% 0.24 0.21 0.24 0.27 7 31 31
F 80% 0.28 0.25 0.30 0.33 8 34 27
G 74% 0.22 0.19 0.19 0.22 3 35 31
AVG 76% 0.25 0.21 0.24 0.28 9% 49% 42%
An integer without a % other than MRRs indicates number of
sentences. Word acc.:word accuracy, SPK:speaker, AVG: aver-
aged values, w/o errors: transcribed sentences without recog-
nition errors, APP: appropriate DQs and InAPP: inappropriate
DQs.
clude an evaluation of the appropriateness of DQs
derived repeatedly to obtain the final answers. In
addition, the interaction strategy automatically gen-
erated by the DDQ module should be evaluated in
terms of how much the DQs improve QA?s total per-
formance.
References
F. Pereira et. al., ?Definite Clause Grammars for Language
Analysis ?a Survey of the Formalism and a Comparison with
Augmented Transition Networks,? Artificial Intelligence, 13:
231-278, 1980.
E. H. Shortliffe, ?Computer-Based Medical Consultations:
MYCIN,? Elsevier/North Holland, New York NY, 1976.
B. Lowerre et. al., ?The Harpy speech understanding system,?
W. A. Lea (Ed.), Trends in Speech recognition, pp. 340, Pren-
tice Hall.
L. D. Erman et. al., ?The Hearsay-II Speech-Understanding
System: Integrating Knowledge to Resolve Uncertainty,?
ACM computing Survays, Vol. 12, No. 2, pp. 213 ? 253,
1980.
V. Zue, et al, ?JUPITER: A Telephone-Based Conversational
Interface for Weather Information,? IEEE Transactions on
Speech and Audio Processing, Vol. 8, No. 1, 2000.
S. Harabagiu et. al., ?Open-Domain Voice-Activated Ques-
tion Answering,? COLING2002, Vol.I, pp. 321?327, Taipei,
2002.
C. Hori et. al., ?A Statistical Approach for Automatic Speech
Summarization,? EURASIP Journal on Applied Signal Pro-
cessing (EURASIP), pp128?139, 2003.
Y. Sasaki et. al., ?NTT?s QA Systems for NTCIR QAC-1,?
Working Notes of the Third NTCIR Workshop Meeting,
pp.63?70, 2002.
Convolution Kernels with Feature Selection
for Natural Language Processing Tasks
Jun Suzuki, Hideki Isozaki and Eisaku Maeda
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto,619-0237 Japan
{jun, isozaki, maeda}@cslab.kecl.ntt.co.jp
Abstract
Convolution kernels, such as sequence and tree ker-
nels, are advantageous for both the concept and ac-
curacy of many natural language processing (NLP)
tasks. Experiments have, however, shown that the
over-fitting problem often arises when these ker-
nels are used in NLP tasks. This paper discusses
this issue of convolution kernels, and then proposes
a new approach based on statistical feature selec-
tion that avoids this issue. To enable the proposed
method to be executed efficiently, it is embedded
into an original kernel calculation process by using
sub-structure mining algorithms. Experiments are
undertaken on real NLP tasks to confirm the prob-
lem with a conventional method and to compare its
performance with that of the proposed method.
1 Introduction
Over the past few years, many machine learn-
ing methods have been successfully applied to
tasks in natural language processing (NLP). Espe-
cially, state-of-the-art performance can be achieved
with kernel methods, such as Support Vector
Machine (Cortes and Vapnik, 1995). Exam-
ples include text categorization (Joachims, 1998),
chunking (Kudo and Matsumoto, 2002) and pars-
ing (Collins and Duffy, 2001).
Another feature of this kernel methodology is that
it not only provides high accuracy but also allows us
to design a kernel function suited to modeling the
task at hand. Since natural language data take the
form of sequences of words, and are generally ana-
lyzed using discrete structures, such as trees (parsed
trees) and graphs (relational graphs), discrete ker-
nels, such as sequence kernels (Lodhi et al, 2002),
tree kernels (Collins and Duffy, 2001), and graph
kernels (Suzuki et al, 2003a), have been shown to
offer excellent results.
These discrete kernels are related to convolution
kernels (Haussler, 1999), which provides the con-
cept of kernels over discrete structures. Convolution
kernels allow us to treat structural features without
explicitly representing the feature vectors from the
input object. That is, convolution kernels are well
suited to NLP tasks in terms of both accuracy and
concept.
Unfortunately, experiments have shown that in
some cases there is a critical issue with convolution
kernels, especially in NLP tasks (Collins and Duffy,
2001; Cancedda et al, 2003; Suzuki et al, 2003b).
That is, the over-fitting problem arises if large ?sub-
structures? are used in the kernel calculations. As a
result, the machine learning approach can never be
trained efficiently.
To solve this issue, we generally eliminate large
sub-structures from the set of features used. How-
ever, the main reason for using convolution kernels
is that we aim to use structural features easily and
efficiently. If use is limited to only very small struc-
tures, it negates the advantages of using convolution
kernels.
This paper discusses this issue of convolution
kernels, and proposes a new method based on statis-
tical feature selection. The proposed method deals
only with those features that are statistically signif-
icant for kernel calculation, large significant sub-
structures can be used without over-fitting. More-
over, the proposed method can be executed effi-
ciently by embedding it in an original kernel cal-
culation process by using sub-structure mining al-
gorithms.
In the next section, we provide a brief overview
of convolution kernels. Section 3 discusses one is-
sue of convolution kernels, the main topic of this
paper, and introduces some conventional methods
for solving this issue. In Section 4, we propose
a new approach based on statistical feature selec-
tion to offset the issue of convolution kernels us-
ing an example consisting of sequence kernels. In
Section 5, we briefly discuss the application of the
proposed method to other convolution kernels. In
Section 6, we compare the performance of conven-
tional methods with that of the proposed method by
using real NLP tasks: question classification and
sentence modality identification. The experimental
results described in Section 7 clarify the advantages
of the proposed method.
2 Convolution Kernels
Convolution kernels have been proposed as a con-
cept of kernels for discrete structures, such as se-
quences, trees and graphs. This framework defines
the kernel function between input objects as the con-
volution of ?sub-kernels?, i.e. the kernels for the
decompositions (parts) of the objects.
Let X and Y be discrete objects. Conceptually,
convolution kernels K(X, Y ) enumerate all sub-
structures occurring in X and Y and then calculate
their inner product, which is simply written as:
K(X,Y ) = ??(X), ?(Y )? =
?
i
?i(X) ? ?i(Y ). (1)
? represents the feature mapping from the
discrete object to the feature space; that is,
?(X) = (?1(X), . . . , ?i(X), . . .). With sequence
kernels (Lodhi et al, 2002), input objects X and Y
are sequences, and ?i(X) is a sub-sequence. With
tree kernels (Collins and Duffy, 2001), X and Y are
trees, and ?i(X) is a sub-tree.
When implemented, these kernels can be effi-
ciently calculated in quadratic time by using dy-
namic programming (DP).
Finally, since the size of the input objects is not
constant, the kernel value is normalized using the
following equation.
K?(X,Y ) = K(X,Y )?
K(X,X) ? K(Y, Y )
(2)
The value of K?(X, Y ) is from 0 to 1, K?(X, Y ) = 1
if and only if X = Y .
2.1 Sequence Kernels
To simplify the discussion, we restrict ourselves
hereafter to sequence kernels. Other convolution
kernels are briefly addressed in Section 5.
Many kinds of sequence kernels have been pro-
posed for a variety of different tasks. This paper
basically follows the framework of word sequence
kernels (Cancedda et al, 2003), and so processes
gapped word sequences to yield the kernel value.
Let ? be a set of finite symbols, and ?n be a set
of possible (symbol) sequences whose sizes are n
or less that are constructed by symbols in ?. The
meaning of ?size? in this paper is the number of
symbols in the sub-structure. Namely, in the case of
sequence, size n means length n. S and T can rep-
resent any sequence. si and tj represent the ith and
jth symbols in S and T , respectively. Therefore, a
S
T
1
2
1
1 21 ?+
?
?
1
? ?
1
1
1
1
a,   b,   c,   aa,  ab,  ac,  ba,  bc,  aba,  aac,  abc,  bac,  abac
abcS = abacT =
p r o d .
10
10
1
0 0
1
0
2 1 1 0 1 3? ?+ 0 ? 0 0 ? 0
( a, b, c, ab, ac, bc, abc)( a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac) 
u
35 3? ?+ +k e r n e l  v al u e
?
s e q u e n ce s s u b-s e q u e n ce s
1
0
0
Figure 1: Example of sequence kernel output
sequence S can be written as S = s1 . . . si . . . s|S|,
where |S| represents the length of S. If sequence
u is contained in sub-sequence S[i : j] def= si . . . sj
of S (allowing the existence of gaps), the position
of u in S is written as i = (i1 : i|u|). The length
of S[i] is l(i) = i|u| ? i1 + 1. For example, if
u = ab and S = cacbd, then i = (2 : 4) and
l(i) = 4 ? 2 + 1 = 3.
By using the above notations, sequence kernels
can be defined as:
KSK(S, T ) =
?
u??n
?
i|u=S[i]
??(i)
?
j|u=T [j]
??(j), (3)
where ? is the decay factor that handles the gap
present in a common sub-sequence u, and ?(i) =
l(i)?|u|. In this paper, | means ?such that?. Figure 1
shows a simple example of the output of this kernel.
However, in general, the number of features |?n|,
which is the dimension of the feature space, be-
comes very high, and it is computationally infeasi-
ble to calculate Equation (3) explicitly. The efficient
recursive calculation has been introduced in (Can-
cedda et al, 2003). To clarify the discussion, we
redefine the sequence kernels with our notation.
The sequence kernel can be written as follows:
KSK(S, T ) =
n?
m=1
?
1?i?|S|
?
1?j?|T |
Jm(Si, Tj). (4)
where Si and Tj represent the sub-sequences Si =
s1, s2, . . . , si and Tj = t1, t2, . . . , tj , respectively.
Let Jm(Si, Tj) be a function that returns the
value of common sub-sequences if si = tj .
Jm(Si, Tj) = J ?m?1(Si, Tj) ? I(si, tj) (5)
I(si, tj) is a function that returns a matching
value between si and tj . This paper defines I(si, tj)
as an indicator function that returns 1 if si = tj , oth-
erwise 0.
Then, J ?m(Si, Tj) and J ??m(Si, Tj) are introduced
to calculate the common gapped sub-sequences be-
tween Si and Tj .
J ?m(Si, Tj) =
?
??
??
1 if m = 0,
0 if j = 0 and m > 0,
?J ?m(Si, Tj?1) + J ??m(Si, Tj?1)
otherwise
(6)
J ??m(Si, Tj) =
?
?
?
0 if i = 0,
?J ??m(Si?1, Tj) + Jm(Si?1, Tj)
otherwise
(7)
If we calculate Equations (5) to (7) recursively,
Equation (4) provides exactly the same value as
Equation (3).
3 Problem of Applying Convolution
Kernels to NLP tasks
This section discusses an issue that arises when ap-
plying convolution kernels to NLP tasks.
According to the original definition of convolu-
tion kernels, all the sub-structures are enumerated
and calculated for the kernels. The number of sub-
structures in the input object usually becomes ex-
ponential against input object size. As a result, all
kernel values K?(X, Y ) are nearly 0 except the ker-
nel value of the object itself, K?(X, X), which is 1.
In this situation, the machine learning process be-
comes almost the same as memory-based learning.
This means that we obtain a result that is very pre-
cise but with very low recall.
To avoid this, most conventional methods use an
approach that involves smoothing the kernel values
or eliminating features based on the sub-structure
size.
For sequence kernels, (Cancedda et al, 2003) use
a feature elimination method based on the size of
sub-sequence n. This means that the kernel calcula-
tion deals only with those sub-sequences whose size
is n or less. For tree kernels, (Collins and Duffy,
2001) proposed a method that restricts the features
based on sub-trees depth. These methods seem to
work well on the surface, however, good results are
achieved only when n is very small, i.e. n = 2.
The main reason for using convolution kernels
is that they allow us to employ structural features
simply and efficiently. When only small sized sub-
structures are used (i.e. n = 2), the full benefits of
convolution kernels are missed.
Moreover, these results do not mean that larger
sized sub-structures are not useful. In some cases
we already know that larger sub-structures are sig-
nificant features as regards solving the target prob-
lem. That is, these significant larger sub-structures,
Table 1: Contingency table and notation for the chi-
squared value
c c?
?
row
u Ouc = y Ouc? Ou = x
u? Ou?c Ou?c? Ou??
column Oc = M Oc? N
which the conventional methods cannot deal with
efficiently, should have a possibility of improving
the performance furthermore.
The aim of the work described in this paper is
to be able to use any significant sub-structure effi-
ciently, regardless of its size, to solve NLP tasks.
4 Proposed Feature Selection Method
Our approach is based on statistical feature selection
in contrast to the conventional methods, which use
sub-structure size.
For a better understanding, consider the two-
class (positive and negative) supervised classifica-
tion problem. In our approach we test the statisti-
cal deviation of all the sub-structures in the training
samples between the appearance of positive samples
and negative samples. This allows us to select only
the statistically significant sub-structures when cal-
culating the kernel value.
Our approach, which uses a statistical metric to
select features, is quite natural. We note, however,
that kernels are calculated using the DP algorithm.
Therefore, it is not clear how to calculate kernels ef-
ficiently with a statistical feature selection method.
First, we briefly explain a statistical metric, the chi-
squared (?2) value, and provide an idea of how
to select significant features. We then describe a
method for embedding statistical feature selection
into kernel calculation.
4.1 Statistical Metric: Chi-squared Value
There are many kinds of statistical metrics, such as
chi-squared value, correlation coefficient and mu-
tual information. (Rogati and Yang, 2002) reported
that chi-squared feature selection is the most effec-
tive method for text classification. Following this
information, we use ?2 values as statistical feature
selection criteria. Although we selected ?2 values,
any other statistical metric can be used as long as it
is based on the contingency table shown in Table 1.
We briefly explain how to calculate the ?2 value
by referring to Table 1. In the table, c and c? rep-
resent the names of classes, c for the positive class
S
T
1
2
1
1 21 ?+
?
?
1
? ?
1
( )2 u? 0.1 0.5 1.2
1
1
1
1.5 0.9 0.8
a,   b,   c,   aa,  ab,  ac,  ba,  bc,  aba,  aac,  abc,  bac,  abac
abcS = abacT =
p r o d .
10
10
1
0 0
1
0
2 1 1 0 1 3? ?+ 0 ? 0 0 ? 0
1.0? =t h r e s h o l d
2.5
1 1 ?
( a, b, c, ab, ac, bc, abc)( a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac) 
u
35 3? ?+ +
2 ?+
0 0 0 0
2 1 1 0 1 3? ?+ 0 ? 0 0 ? 0
k e r n e l  v al u e
k e r n e l  v al u e  u n d e r  t h e  f e at u r e  s e l e ct i o n
f e at u r e  s e l e ct i o n
?
s e q u e n ce s s u b-s e q u e n ce s
1
0
0
0
Figure 2: Example of statistical feature selection
and c? for the negative class. Ouc, Ouc?, Ou?c and Ou?c?
represent the number of u that appeared in the pos-
itive sample c, the number of u that appeared in the
negative sample c?, the number of u that did not ap-
pear in c, and the number of u that did not appear
in c?, respectively. Let y be the number of samples
of positive class c that contain sub-sequence u, and
x be the number of samples that contain u. Let N
be the total number of (training) samples, and M be
the number of positive samples.
Since N and M are constant for (fixed) data, ?2
can be written as a function of x and y,
?2(x, y) = N(Ouc ? Ou?c? ? Ou?c ? Ouc?)
2
Ou ? Ou? ? Oc ? Oc?
. (8)
?2 expresses the normalized deviation of the obser-
vation from the expectation.
We simply represent ?2(x, y) as ?2(u).
4.2 Feature Selection Criterion
The basic idea of feature selection is quite natural.
First, we decide the threshold ? of the ?2 value. If
?2(u) < ? holds, that is, u is not statistically signif-
icant, then u is eliminated from the features and the
value of u is presumed to be 0 for the kernel value.
The sequence kernel with feature selection
(FSSK) can be defined as follows:
KFSSK(S, T ) =
?
???2(u)|u??n
?
i|u=S[i]
??(i)
?
j|u=T [j]
??(j). (9)
The difference between Equations (3) and (9) is
simply the condition of the first summation. FSSK
selects significant sub-sequence u by using the con-
dition of the statistical metric ? ? ?2(u).
Figure 2 shows a simple example of what FSSK
calculates for the kernel value.
4.3 Efficient ?2(u) Calculation Method
It is computationally infeasible to calculate ?2(u)
for all possible u with a naive exhaustive method.
In our approach, we use a sub-structure mining al-
gorithm to calculate ?2(u). The basic idea comes
from a sequential pattern mining technique, PrefixS-
pan (Pei et al, 2001), and a statistical metric prun-
ing (SMP) method, Apriori SMP (Morishita and
Sese, 2000). By using these techniques, all the sig-
nificant sub-sequences u that satisfy ? ? ?2(u) can
be found efficiently by depth-first search and prun-
ing. Below, we briefly explain the concept involved
in finding the significant features.
First, we denote uv, which is the concatenation of
sequences u and v. Then, u is a specific sequence
and uv is any sequence that is constructed by u with
any suffix v. The upper bound of the ?2 value of
uv can be defined by the value of u (Morishita and
Sese, 2000).
?2(uv)?max
(
?2(yu, yu), ?2(xu ? yu, 0)
)
=??2(u)
where xu and yu represent the value of x and y
of u. This inequation indicates that if ??2(u) is less
than a certain threshold ? , all sub-sequences uv can
be eliminated from the features, because no sub-
sequence uv can be a feature.
The PrefixSpan algorithm enumerates all the sig-
nificant sub-sequences by using a depth-first search
and constructing a TRIE structure to store the sig-
nificant sequences of internal results efficiently.
Specifically, PrefixSpan algorithm evaluates uw,
where uw represents a concatenation of a sequence
u and a symbol w, using the following three condi-
tions.
1. ? ? ?2(uw)
2. ? > ?2(uw), ? > ??2(uw)
3. ? > ?2(uw), ? ? ??2(uw)
With 1, sub-sequence uw is selected as a significant
feature. With 2, sub-sequence uw and arbitrary sub-
sequences uwv, are less than the threshold ? . Then
w is pruned from the TRIE, that is, all uwv where v
represents any suffix pruned from the search space.
With 3, uw is not selected as a significant feature
because the ?2 value of uw is less than ? , however,
uwv can be a significant feature because the upper-
bound ?2 value of uwv is greater than ? , thus the
search is continued to uwv.
Figure 3 shows a simple example of PrefixSpan
with SMP that searches for the significant features
a b c cd b c ab a ca cd a b d
a b c cd b cb a ca cd a b d
?
a b c d
b c
1.0? =
b:c:d:
+ 1-1+ 1-1-1
au =
w =
( )2 uw? ( )2? uw?
T R I E  r e p r e s e n t at i o n
x y
+ 1-1+ 1-1+ 1
abu =
d
c
?
w
231
121
+ 1-1+ 1-1-1
class t r ai n i n g  d at a
su f f i x
c:d:w =
x y11 10
5.00.0 5.00.8 5.00.8 2 .22 .2
1 .90.1
1 .91.9
0.80.85.02 .2
a:b:c:d:
+ 1-1+ 1-1-1
u = ?
w =
x y5442
2220
c
d
1.91 .9
0.80.8
?
a b c cd b c ab a ca cd a b d
su f f i x
su f f i xa b c cd b cb a ca cd a b d5N = 2M =
2
3
1
4
5
se ar ch  o r d e r
p r u n e d
p r u n e d
Figure 3: Efficient search for statistically significant
sub-sequences using the PrefixSpan algorithm with
SMP
by using a depth-first search with a TRIE represen-
tation of the significant sequences. The values of
each symbol represent ?2(u) and ??2(u) that can be
calculated from the number of xu and yu. The TRIE
structure in the figure represents the statistically sig-
nificant sub-sequences that can be shown in a path
from ? to the symbol.
We exploit this TRIE structure and PrefixSpan
pruning method in our kernel calculation.
4.4 Embedding Feature Selection in Kernel
Calculation
This section shows how to integrate statistical fea-
ture selection in the kernel calculation. Our pro-
posed method is defined in the following equations.
KFSSK(S, T ) =
n?
m=1
?
1?i?|S|
?
1?j?|T |
Km(Si, Tj) (10)
Let Km(Si, Tj) be a function that returns the sum
value of all statistically significant common sub-
sequences u if si = tj .
Km(Si, Tj) =
?
u??m(Si,Tj)
Ju(Si, Tj), (11)
where ?m(Si, Tj) represents a set of sub-sequences
whose size |u| is m and that satisfy the above condi-
tion 1. The ?m(Si, Tj) is defined in detail in Equa-
tion (15).
Then, let Ju(Si, Tj), J ?u(Si, Tj) and J ??u (Si, Tj)
be functions that calculate the value of the common
sub-sequences between Si and Tj recursively, as
well as equations (5) to (7) for sequence kernels. We
introduce a special symbol ? to represent an ?empty
sequence?, and define ?w = w and |?w| = 1.
Juw(Si, Tj) =
?
?
?
J ?u(Si, Tj) ? I(w)
if uw ? ??|uw|(Si, Tj),
0 otherwise
(12)
where I(w) is a function that returns a matching
value of w. In this paper, we define I(w) is 1.
??m(Si, Tj) has realized conditions 2 and 3; the
details are defined in Equation (16).
J ?u(Si, Tj) =
?
??
??
1 if u = ?,
0 if j = 0 and u 6= ?,
?J ?u(Si, Tj?1) + J ??u (Si, Tj?1)
otherwise
(13)
J ??u (Si, Tj) =
?
?
?
0 if i = 0,
?J ??u (Si?1, Tj) + Ju(Si?1, Tj)
otherwise
(14)
The following five equations are introduced to se-
lect a set of significant sub-sequences. ?m(Si, Tj)
and ??m(Si, Tj) are sets of sub-sequences (features)
that satisfy condition 1 and 3, respectively, when
calculating the value between Si and Tj in Equa-
tions (11) and (12).
?m(Si, Tj) = {u | u ? ??m(Si, Tj), ? ? ?2(u)} (15)
??m(Si, Tj) =
?
?
?
?(???m?1(Si, Tj), si)
if si = tj
? otherwise
(16)
?(F,w) = {uw | u ? F, ? ? ??2(uw)}, (17)
where F represents a set of sub-sequences. No-
tice that ?m(Si, Tj) and ??m(Si, Tj) have only sub-
sequences u that satisfy ? ? ?2(uw) or ? ?
??2(uw), respectively, if si = tj(= w); otherwise
they become empty sets.
The following two equations are introduced for
recursive set operations to calculate ?m(Si, Tj) and
??m(Si, Tj).
???m(Si, Tj) =
?
???
???
{?} if m = 0,
? if j = 0 and m > 0,
???m(Si, Tj?1) ? ????m(Si, Tj?1)
otherwise
(18)
????m(Si, Tj) =
?
?
?
? if i = 0 ,
????m(Si?1, Tj) ? ??m(Si?1, Tj)
otherwise
(19)
In the implementation, Equations (11) to (14) can
be performed in the same way as those used to cal-
culate the original sequence kernels, if the feature
selection condition of Equations (15) to (19) has
been removed. Then, Equations (15) to (19), which
select significant features, are performed by the Pre-
fixSpan algorithm described above and the TRIE
representation of statistically significant features.
The recursive calculation of Equations (12) to
(14) and Equations (16) to (19) can be executed in
the same way and at the same time in parallel. As a
result, statistical feature selection can be embedded
in oroginal sequence kernel calculation based on a
dynamic programming technique.
4.5 Properties
The proposed method has several important advan-
tages over the conventional methods.
First, the feature selection criterion is based on
a statistical measure, so statistically significant fea-
tures are automatically selected.
Second, according to Equations (10) to (18), the
proposed method can be embedded in an original
kernel calculation process, which allows us to use
the same calculation procedure as the conventional
methods. The only difference between the original
sequence kernels and the proposed method is that
the latter calculates a statistical metric ?2(u) by us-
ing a sub-structure mining algorithm in the kernel
calculation.
Third, although the kernel calculation, which uni-
fies our proposed method, requires a longer train-
ing time because of the feature selection, the se-
lected sub-sequences have a TRIE data structure.
This means a fast calculation technique proposed
in (Kudo and Matsumoto, 2003) can be simply ap-
plied to our method, which yields classification very
quickly. In the classification part, the features (sub-
sequences) selected in the learning part must be
known. Therefore, we store the TRIE of selected
sub-sequences and use them during classification.
5 Proposed Method Applied to Other
Convolution Kernels
We have insufficient space to discuss this subject in
detail in relation to other convolution kernels. How-
ever, our proposals can be easily applied to tree ker-
nels (Collins and Duffy, 2001) by using string en-
coding for trees. We enumerate nodes (labels) of
tree in postorder traversal. After that, we can em-
ploy a sequential pattern mining technique to select
statistically significant sub-trees. This is because we
can convert to the original sub-tree form from the
string encoding representation.
Table 2: Parameter values of proposed kernels and
Support Vector Machines
parameter value
soft margin for SVM (C) 1000
decay factor of gap (?) 0.5
threshold of ?2 (? ) 2.70553.8415
As a result, we can calculate tree kernels with sta-
tistical feature selection by using the original tree
kernel calculation with the sequential pattern min-
ing technique introduced in this paper. Moreover,
we can expand our proposals to hierarchically struc-
tured graph kernels (Suzuki et al, 2003a) by using
a simple extension to cover hierarchical structures.
6 Experiments
We evaluated the performance of the proposed
method in actual NLP tasks, namely English ques-
tion classification (EQC), Japanese question classi-
fication (JQC) and sentence modality identification
(MI) tasks.
We compared the proposed method (FSSK) with
a conventional method (SK), as discussed in Sec-
tion 3, and with bag-of-words (BOW) Kernel
(BOW-K)(Joachims, 1998) as baseline methods.
Support Vector Machine (SVM) was selected as
the kernel-based classifier for training and classifi-
cation. Table 2 shows some of the parameter values
that we used in the comparison. We set thresholds
of ? = 2.7055 (FSSK1) and ? = 3.8415 (FSSK2)
for the proposed methods; these values represent the
10% and 5% level of significance in the ?2 distribu-
tion with one degree of freedom, which used the ?2
significant test.
6.1 Question Classification
Question classification is defined as a task similar to
text categorization; it maps a given question into a
question type.
We evaluated the performance by using data
provided by (Li and Roth, 2002) for English
and (Suzuki et al, 2003b) for Japanese question
classification and followed the experimental setting
used in these papers; namely we use four typical
question types, LOCATION, NUMEX, ORGANI-
ZATION, and TIME TOP for JQA, and ?coarse?
and ?fine? classes for EQC. We used the one-vs-rest
classifier of SVM as the multi-class classification
method for EQC.
Figure 4 shows examples of the question classifi-
cation data used here.
question types input object : word sequences ([ ]: information of chunk and ? ?: named entity)
ABBREVIATION what,[B-NP] be,[B-VP] the,[B-NP] abbreviation,[I-NP] for,[B-PP] Texas,[B-NP],?B-GPE? ?,[O]
DESCRIPTION what,[B-NP] be,[B-VP] Aborigines,[B-NP] ?,[O]
HUMAN who,[B-NP] discover,[B-VP] America,[B-NP],?B-GPE? ?,[O]
Figure 4: Examples of English question classification data
Table 3: Results of the Japanese question classification (F-measure)
(a) TIME TOP (b) LOCATION (c) ORGANIZATION (d) NUMEX
n
FSSK1
FSSK2
SK
BOW-K
1 2 3 4 ?
- .961 .958 .957 .956
- .961 .956 .957 .956
- .946 .910 .866 .223
.902 .909 .886 .855 -
1 2 3 4 ?
- .795 .793 .798 .792
- .788 .799 .804 .800
- .791 .775 .732 .169
.744 .768 .756 .747 -
1 2 3 4 ?
- .709 .720 .720 .723
- .703 .710 .716 .720
- .705 .668 .594 .035
.641 690 .636 .572 -
1 2 3 4 ?
- .912 .915 .908 .908
- .913 .916 .911 .913
- .912 .885 .817 .036
.842 .852 .807 .726 -
6.2 Sentence Modality Identification
For example, sentence modality identification tech-
niques are used in automatic text analysis systems
that identify the modality of a sentence, such as
?opinion? or ?description?.
The data set was created from Mainichi news arti-
cles and one of three modality tags, ?opinion?, ?de-
cision? and ?description? was applied to each sen-
tence. The data size was 1135 sentences consist-
ing of 123 sentences of ?opinion?, 326 of ?decision?
and 686 of ?description?. We evaluated the results
by using 5-fold cross validation.
7 Results and Discussion
Tables 3 and 4 show the results of Japanese and En-
glish question classification, respectively. Table 5
shows the results of sentence modality identifica-
tion. n in each table indicates the threshold of the
sub-sequence size. n = ? means all possible sub-
sequences are used.
First, SK was consistently superior to BOW-K.
This indicates that the structural features were quite
efficient in performing these tasks. In general we
can say that the use of structural features can im-
prove the performance of NLP tasks that require the
details of the contents to perform the task.
Most of the results showed that SK achieves its
maximum performance when n = 2. The per-
formance deteriorates considerably once n exceeds
4. This implies that SK with larger sub-structures
degrade classification performance. These results
show the same tendency as the previous studies dis-
cussed in Section 3. Table 6 shows the precision and
recall of SK when n = ?. As shown in Table 6, the
classifier offered high precision but low recall. This
is evidence of over-fitting in learning.
As shown by the above experiments, FSSK pro-
Table 6: Precision and recall of SK: n = ?
Precision Recall F
MI:Opinion .917 .209 .339
JQA:LOCATION .896 .093 .168
vided consistently better performance than the con-
ventional methods. Moreover, the experiments con-
firmed one important fact. That is, in some cases
maximum performance was achieved with n =
?. This indicates that sub-sequences created us-
ing very large structures can be extremely effective.
Of course, a larger feature space also includes the
smaller feature spaces, ?n ? ?n+1. If the perfor-
mance is improved by using a larger n, this means
that significant features do exist. Thus, we can im-
prove the performance of some classification prob-
lems by dealing with larger substructures. Even if
optimum performance was not achieved with n =
?, difference between the performance of smaller
n are quite small compared to that of SK. This indi-
cates that our method is very robust as regards sub-
structure size; It therefore becomes unnecessary for
us to decide sub-structure size carefully. This in-
dicates our approach, using large sub-structures, is
better than the conventional approach of eliminating
sub-sequences based on size.
8 Conclusion
This paper proposed a statistical feature selection
method for convolution kernels. Our approach can
select significant features automatically based on a
statistical significance test. Our proposed method
can be embedded in the DP based kernel calcula-
tion process for convolution kernels by using sub-
structure mining algorithms.
Table 4: Results of English question classification (Accuracy)
(a) coarse (b) fine
n
FSSK1
FSSK2
SK
BOW-K
1 2 3 4 ?
- .908 .914 .916 .912
- .902 .896 .902 .906
- .912 .914 .912 .892
.728 .836 .864 .858 -
1 2 3 4 ?
- .852 .854 .852 .850
- .858 .856 .854 .854
- .850 .840 .830 .796
.754 .792 .790 .778 -
Table 5: Results of sentence modality identification (F-measure)
(a) opinion (b) decision (c) description
n
FSSK1
FSSK2
SK
BOW-K
1 2 3 4 ?
- .734 .743 .746 .751
- .740 .748 .750 .750
- .706 .672 .577 .058
.507 .531 .438 .368 -
1 2 3 4 ?
- .828 .858 .854 .857
- .824 .855 .859 .860
- .816 .834 .830 .339
.652 .708 .686 .665 -
1 2 3 4 ?
- .896 .906 .910 .910
- .894 .903 .909 .909
- .902 .913 .910 .808
.819 .839 .826 .793 -
Experiments show that our method is superior to
conventional methods. Moreover, the results indi-
cate that complex features exist and can be effective.
Our method can employ them without over-fitting
problems, which yields benefits in terms of concept
and performance.
References
N. Cancedda, E. Gaussier, C. Goutte, and J.-M.
Renders. 2003. Word-Sequence Kernels. Jour-
nal of Machine Learning Research, 3:1059?1082.
M. Collins and N. Duffy. 2001. Convolution Ker-
nels for Natural Language. In Proc. of Neural In-
formation Processing Systems (NIPS?2001).
C. Cortes and V. N. Vapnik. 1995. Support Vector
Networks. Machine Learning, 20:273?297.
D. Haussler. 1999. Convolution Kernels on Dis-
crete Structures. In Technical Report UCS-CRL-
99-10. UC Santa Cruz.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rel-
evant Features. In Proc. of European Conference
on Machine Learning (ECML ?98), pages 137?
142.
T. Kudo and Y. Matsumoto. 2002. Japanese Depen-
dency Analysis Using Cascaded Chunking. In
Proc. of the 6th Conference on Natural Language
Learning (CoNLL 2002), pages 63?69.
T. Kudo and Y. Matsumoto. 2003. Fast Methods for
Kernel-based Text Analysis. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), pages 24?31.
X. Li and D. Roth. 2002. Learning Question Clas-
sifiers. In Proc. of the 19th International Con-
ference on Computational Linguistics (COLING
2002), pages 556?562.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cris-
tianini, and C. Watkins. 2002. Text Classification
Using String Kernel. Journal of Machine Learn-
ing Research, 2:419?444.
S. Morishita and J. Sese. 2000. Traversing Item-
set Lattices with Statistical Metric Pruning. In
Proc. of ACM SIGACT-SIGMOD-SIGART Symp.
on Database Systems (PODS?00), pages 226?
236.
J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto.
2001. PrefixSpan: Mining Sequential Patterns
Efficiently by Prefix-Projected Pattern Growth.
In Proc. of the 17th International Conference on
Data Engineering (ICDE 2001), pages 215?224.
M. Rogati and Y. Yang. 2002. High-performing
Feature Selection for Text Classification. In
Proc. of the 2002 ACM CIKM International Con-
ference on Information and Knowledge Manage-
ment, pages 659?661.
J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda.
2003a. Hierarchical Directed Acyclic Graph Ker-
nel: Methods for Natural Language Data. In
Proc. of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-2003),
pages 32?39.
J. Suzuki, Y. Sasaki, and E. Maeda. 2003b. Kernels
for Structured Natural Language Data. In Proc.
of the 17th Annual Conference on Neural Infor-
mation Processing Systems (NIPS2003).
Proceedings of the 43rd Annual Meeting of the ACL, pages 189?196,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Boosting-based parse reranking with subtree features
Taku Kudo ? Jun Suzuki Hideki Isozaki
NTT Communication Science Laboratories.
2-4 Hikaridai, Seika-cho, Soraku, Kyoto, Japan
{taku,jun,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper introduces a new application of boost-
ing for parse reranking. Several parsers have been
proposed that utilize the all-subtrees representa-
tion (e.g., tree kernel and data oriented parsing).
This paper argues that such an all-subtrees repre-
sentation is extremely redundant and a compara-
ble accuracy can be achieved using just a small
set of subtrees. We show how the boosting algo-
rithm can be applied to the all-subtrees representa-
tion and how it selects a small and relevant feature
set efficiently. Two experiments on parse rerank-
ing show that our method achieves comparable or
even better performance than kernel methods and
also improves the testing efficiency.
1 Introduction
Recent work on statistical natural language pars-
ing and tagging has explored discriminative tech-
niques. One of the novel discriminative approaches
is reranking, where discriminative machine learning
algorithms are used to rerank the n-best outputs of
generative or conditional parsers. The discrimina-
tive reranking methods allow us to incorporate vari-
ous kinds of features to distinguish the correct parse
tree from all other candidates.
With such feature design flexibility, it is non-
trivial to employ an appropriate feature set that has
a good discriminative ability for parse reranking. In
early studies, feature sets were given heuristically by
simply preparing task-dependent feature templates
(Collins, 2000; Collins, 2002). These ad-hoc solu-
tions might provide us with reasonable levels of per-
?Currently, Google Japan Inc., taku@google.com
formance. However, they are highly task dependent
and require careful design to create the optimal fea-
ture set for each task. Kernel methods offer an ele-
gant solution to these problems. They can work on a
potentially huge or even infinite number of features
without a loss of generalization. The best known
kernel for modeling a tree is the tree kernel (Collins
and Duffy, 2002), which argues that a feature vec-
tor is implicitly composed of the counts of subtrees.
Although kernel methods are general and can cover
almost all useful features, the set of subtrees that is
used is extremely redundant. The main question ad-
dressed in this paper concerns whether it is possible
to achieve a comparable or even better accuracy us-
ing just a small and non-redundant set of subtrees.
In this paper, we present a new application of
boosting for parse reranking. While tree kernel
implicitly uses the all-subtrees representation, our
boosting algorithm uses it explicitly. Although this
set-up makes the feature space large, the l1-norm
regularization achived by boosting automatically se-
lects a small and relevant feature set. Such a small
feature set is useful in practice, as it is interpretable
and makes the parsing (reranking) time faster. We
also incorporate a variant of the branch-and-bound
technique to achieve efficient feature selection in
each boosting iteration.
2 General setting of parse reranking
We describe the general setting of parse reranking.
? Training data T is a set of input/output pairs, e.g.,
T = {?x1,y1?, . . . , ?xL,yL?}, where xi is an in-
put sentence, and yi is a correct parse associated
with the sentence xi.
? Let Y(x) be a function that returns a set of candi-
189
date parse trees for a particular sentence x.
? We assume that Y(xi) contains the correct parse
tree yi, i.e., yi ? Y(xi) ?
? Let ?(y) ? Rd be a feature function that maps
the given parse tree y into Rd space. w ? Rd is
a parameter vector of the model. The output parse
y? of this model on input sentence x is given as:
y? = argmaxy?Y(x)w ? ?(y).
There are two questions as regards this formula-
tion. One is how to set the parameters w, and the
other is how to design the feature function ?(y). We
briefly describe the well-known solutions to these
two problems in the next subsections.
2.1 Parameter estimation
We usually adopt a general loss function Loss(w),
and set the parameters w that minimize the loss,
i.e., w? = argminw?Rd Loss(w). Generally, the loss
function has the following form:
Loss(w) =
L?
i=1
L(w,?(yi),xi),
where L(w,?(yi),xi) is an arbitrary loss function.
We can design a variety of parameter estimation
methods by changing the loss function. The follow-
ing three loss functions, LogLoss, HingeLoss, and
BoostLoss, have been widely used in parse rerank-
ing tasks.
LogLoss = ? log
? X
y?Y(xi)
exp
?
w ? [?(yi)? ?(y)]
??
HingeLoss =
X
y?Y(xi)
max(0, 1?w ? [?(yi)? ?(y)])
BoostLos =
X
y?Y(xi)
exp
?
?w ? [?(yi)? ?(y)]
?
LogLoss is based on the standard maximum like-
lihood optimization, and is used with maximum en-
tropy models. HingeLoss captures the errors only
when w ? [?(yi)? ?(y)]) < 1. This loss is closely
related to the maximum margin strategy in SVMs
(Vapnik, 1998). BoostLoss is analogous to the
boosting algorithm and is used in (Collins, 2000;
Collins, 2002).
?In the real setting, we cannot assume this condition. In this
case, we select the parse tree y? that is the most similar to yi and
take y? as the correct parse tree yi.
2.2 Definition of feature function
It is non-trivial to define an appropriate feature func-
tion ?(y) that has a good ability to distinguish the
correct parse yi from all other candidates
In early studies, the feature functions were given
heuristically by simply preparing feature templates
(Collins, 2000; Collins, 2002). However, such
heuristic selections are task dependent and would
not cover all useful features that contribute to overall
accuracy.
When we select the special family of loss func-
tions, the problem can be reduced to a dual form that
depends only on the inner products of two instances
?(y1) ??(y2). This property is important as we can
use a kernel trick and we do not need to provide an
explicit feature function. For example, tree kernel
(Collins and Duffy, 2002), one of the convolution
kernels, implicitly maps the instance represented in
a tree into all-subtrees space. Even though the fea-
ture space is large, inner products under this feature
space can be calculated efficiently using dynamic
programming. Tree kernel is more general than fea-
ture templates since it can use the all-subtrees repre-
sentation without loss of efficiency.
3 RankBoost with subtree features
A simple question related to kernel-based parse
reranking asks whether all subtrees are really needed
to construct the final parameters w. Suppose we
have two large trees t and t?, where t? is simply gen-
erated by attaching a single node to t. In most cases,
these two trees yield an almost equivalent discrimi-
native ability, since they are very similar and highly
correlated with each other. Even when we exploit all
subtrees, most of them are extremely redundant.
The motivation of this paper is based on the above
observation. We think that only a small set of sub-
trees is needed to express the final parameters. A
compact, non-redundant, and highly relevant feature
set is useful in practice, as it is interpretable and in-
creases the parsing (reranking) speed.
To realize this goal, we propose a new boosting-
based reranking algorithm based on the all-subtrees
representation. First, we describe the architecture of
our reranking method. Second, we show a connec-
tion between boosting and SVMs, and describe how
the algorithm realizes the sparse feature representa-
190
 


 


 


 


 

 
	


Figure 1: Labeled ordered tree and subtree relation
tion described above.
3.1 Preliminaries
Let us introduce a labeled ordered tree (or simply
?tree?), its definition and notations, first.
Definition 1 Labeled ordered tree (Tree)
A labeled ordered tree is a tree where each node is
associated with a label and is ordered among its sib-
lings, that is, there is a first child, second child, third
child, etc.
Definition 2 Subtree
Let t and u be labeled ordered trees. We say that t
matches u, or t is a subtree of u (t ? u), if there is a
one-to-one function ? from nodes in t to u, satisfying
the conditions: (1) ? preserves the parent-daughter
relation, (2) ? preserves the sibling relation, (3) ?
preserves the labels.
We denote the number of nodes in t as |t|. Figure 1
shows an example of a labeled ordered tree and its
subtree and non-subtree.
3.2 Feature space given by subtrees
We first assume that a parse tree y is represented in
a labeled ordered tree. Note that the outputs of part-
of-speech tagging, shallow parsing, and dependency
analysis can be modeled as labeled ordered trees.
The feature set F consists of all subtrees seen in
the training data, i.e.,
F = ?i,y?Y(xi){t | t ? y}.
The feature mapping ?(y) is then given by letting
the existence of a tree t be a single dimension, i.e.,
?(y) = {I(t1 ? y), . . . , I(tm ? y)} ? {0, 1}m,
where I(?) is the indicator function, m = |F|, and
{t1, . . . , tm} ? F . The feature space is essentially
the same as that of tree kernel ?
?Strictly speaking, tree kernel uses the cardinality of each
subtree
3.3 RankBoost algorithm
The parameter estimation method we adopt is a vari-
ant of the RankBoost algorithm introduced in (Fre-
und et al, 2003). Collins et al used RankBoost to
parse reranking tasks (Collins, 2000; Collins, 2002).
The algorithm proceeds for K iterations and tries to
minimize the BoostLoss for given training data?.
At each iteration, a single feature (hypothesis) is
chosen, and its weight is updated.
Suppose we have current parameters:
w = {w1, w2, . . . , wm} ? Rm.
New parameters w??k,?? ? Rm are then given by
selecting a single feature k and updating the weight
through an increment ?:
w??k,?? = {w1, w2, . . . , wk + ?, . . . , wm}.
After the update, the new loss is given:
Loss(w??k,??) =
X
i, y?Y(xi)
exp
?
?w??k,?? ? [?(yi)? ?(y)]
?
. (1)
The RankBoost algorithm iteratively selects the op-
timal pair ?k?, ??? that minimizes the loss, i.e.,
?k?, ??? = argmin
?k,??
Loss(w??k,??).
By setting the differential of (1) at 0, the following
optimal solutions are obtained:
k? = argmax
k=1,...,m
????
q
W+k ?
q
W?k
????, and ? =
1
2 log
W+k?
W?k?
, (2)
where W bk =
?
i,y?Y(xi) D(yi,y) ? I[I(tk ? yi)?I(tk ? y) = b], b ? {+1,?1}, and D(yi,y) =
exp (?w ? [?(yi)? ?(y)]).
Following (Freund et al, 2003; Collins, 2000), we
introduce smoothing to prevent the case when either
W+k or W?k is 0 ?:
? = 12 log
W+k? + ?Z
W?k? + ?Z
, where Z =
X
i,y?Y(xi)
D(yi,y) and ? ? R+.
The function Y(x) is usually performed by a
probabilistic history-based parser, which can output
not only a parse tree but the log probability of the
?In our experiments, optimal settings for K were selected
by using development data.
?For simplicity, we fix ? at 0.001 in all our experiments.
191
tree. We incorporate the log probability into the
reranking by using it as a feature:
?(y) = {L(y), I(t1 ? y), . . . , I(tm ? y)}, and
w = {w0, w1, w2, . . . , wm},
where L(y) is the log probability of a tree y un-
der the base parser and w0 is the parameter of L(y).
Note that the update algorithm (2) does not allow us
to calculate the parameter w0, since (2) is restricted
to binary features. To prevent this problem, we use
the approximation technique introduced in (Freund
et al, 2003).
3.4 Sparse feature representation
Recent studies (Schapire et al, 1997; Ra?tsch, 2001)
have shown that both boosting and SVMs (Vapnik,
1998) work according to similar strategies: con-
structing optimal parameters w that maximize the
smallest margin between positive and negative ex-
amples. The critical difference is the definition of
margin or the way they regularize the vector w.
(Ra?tsch, 2001) shows that the iterative feature selec-
tion performed in boosting asymptotically realizes
an l1-norm ||w||1 regularization. In contrast, it is
well known that SVMs are reformulated as an l2-
norm ||w||2 regularized algorithm.
The relationship between two regularizations has
been studied in the machine learning community.
(Perkins et al, 2003) reported that l1-norm should
be chosen for a problem where most given features
are irrelevant. On the other hand, l2-norm should be
chosen when most given features are relevant. An
advantage of the l1-norm regularizer is that it often
leads to sparse solutions where most wk are exactly
0. The features assigned zero weight are thought to
be irrelevant features as regards classifications.
The l1-norm regularization is useful for our set-
ting, since most features (subtrees) are redundant
and irrelevant, and these redundant features are au-
tomatically eliminated.
4 Efficient Computation
In each boosting iteration, we have to solve the fol-
lowing optimization problem:
k? = argmax
k=1,...,m
gain(tk),
where gain(tk) =
???
?
W+k ?
?
W?k
???.
It is non-trivial to find the optimal tree tk? that maxi-
mizes gain(tk), since the number of subtrees is ex-
ponential to its size. In fact, the problem is known
to be NP-hard (Yang, 2004). However, in real appli-
cations, the problem is manageable, since the max-
imum number of subtrees is usually bounded by a
constant. To solve the problem efficiently, we now
adopt a variant of the branch-and-bound algorithm,
similar to that described in (Kudo and Matsumoto,
2004)
4.1 Efficient Enumeration of Trees
Abe and Zaki independently proposed an efficient
method, rightmost-extension, for enumerating all
subtrees from a given tree (Abe et al, 2002; Zaki,
2002). First, the algorithm starts with a set of trees
consisting of single nodes, and then expands a given
tree of size (n?1) by attaching a new node to it to
obtain trees of size n. However, it would be inef-
ficient to expand nodes at arbitrary positions of the
tree, as duplicated enumeration is inevitable. The
algorithm, rightmost extension, avoids such dupli-
cated enumerations by restricting the position of at-
tachment. Here we give the definition of rightmost
extension to describe this restriction in detail.
Definition 3 Rightmost Extension (Abe et al, 2002;
Zaki, 2002)
Let t and t? be labeled ordered trees. We say t? is a
rightmost extension of t, if and only if t and t? satisfy
the following three conditions:
(1) t? is created by adding a single node to t, (i.e.,
t ? t? and |t|+ 1 = |t?|).
(2) A node is added to a node existing on the unique
path from the root to the rightmost leaf (rightmost-
path) in t.
(3) A node is added as the rightmost sibling.
Consider Figure 2, which illustrates example tree t
with labels drawn from the set L = {a, b, c}. For
the sake of convenience, each node in this figure has
its original number (depth-first enumeration). The
rightmost-path of the tree t is (a(c(b))), and it oc-
curs at positions 1, 4 and 6 respectively. The set of
rightmost extended trees is then enumerated by sim-
ply adding a single node to a node on the rightmost
path. Since there are three nodes on the rightmost
path and the size of the label set is 3 (= |L|), a to-
192
b a c12 4
a b5 6c3
b a c12 4
a b5 6c3
b a c12 4
a b5 6c3
b a c12 4
a b5 6c3
rightmost- path
t
rightmost extension 
7
7 7
t?
},,{ cbaL =
},,{ cba
},,{ cba},,{ cba
Figure 2: Rightmost extension
tal of 9 trees are enumerated from the original tree
t. By repeating the rightmost-extension process re-
cursively, we can create a search space in which all
trees drawn from the set L are enumerated.
4.2 Pruning
Rightmost extension defines a canonical search
space in which we can enumerate all subtrees from
a given set of trees. Here we consider an upper
bound of the gain that allows subspace pruning in
this canonical search space. The following obser-
vation provides a convenient way of computing an
upper bound of the gain(tk) for any super-tree tk?
of tk.
Observation 1 Upper bound of the gain(tk)
For any tk? ? tk, the gain of tk? is bounded by
?(tk):
gain(tk?) =
????
q
W+k? ?
q
W?k?
????
? max(
q
W+k? ,
q
W?k? )
? max(
q
W+k ,
q
W?k ) = ?(tk),
since tk? ? tk ? W bk? ? W bk , b ? {+1,?1}.
We can efficiently prune the search space spanned
by the rightmost extension using the upper bound of
gain ?(t). During the traverse of the subtree lattice
built by the recursive process of rightmost extension,
we always maintain the temporally suboptimal gain
? of all the previously calculated gains. If ?(t) < ? ,
the gain of any super-tree t? ? t is no greater than ? ,
and therefore we can safely prune the search space
spanned from the subtree t. In contrast, if ?(t) ? ? ,
we cannot prune this space, since there might be a
super-tree t? ? t such that gain(t?) ? ? .
4.3 Ad-hoc techniques
In real applications, we also employ the following
practical methods to reduce the training costs.
? Size constraint
Larger trees are usually less effective to discrimi-
nation. Thus, we give a size threshold s, and use
subtrees whose size is no greater than s. This con-
straint is easily realized by controlling the right-
most extension according to the size of the trees.
? Frequency constraint
The frequency-based cut-off has been widely used
in feature selections. We employ a frequency
threshold f , and use subtrees seen on at least one
parse for at least f different sentences. Note that
a similar branch-and-bound technique can also be
applied to the cut-off. When we find that the fre-
quency of a tree t is no greater than f , we can safely
prune the space spanned from t as the frequencies
of any super-trees t? ? t are also no greater than f .
? Pseudo iterations
After several 5- or 10-iterations of boosting, we al-
ternately perform 100- or 300 pseudo iterations, in
which the optimal feature (subtree) is selected from
the cache that maintains the features explored in the
previous iterations. The idea is based on our ob-
servation that a feature in the cache tends to be re-
used as the number of boosting iterations increases.
Pseudo iterations converge very fast, and help the
branch-and-bound algorithm find new features that
are not in the cache.
5 Experiments
5.1 Parsing Wall Street Journal Text
In our experiments, we used the same data set that
used in (Collins, 2000). Sections 2-21 of the Penn
Treebank were used as training data, and section
23 was used as test data. The training data con-
tains about 40,000 sentences, each of which has an
average of 27 distinct parses. Of the 40,000 train-
ing sentences, the first 36,000 sentences were used
to perform the RankBoost algorithm. The remain-
ing 4,000 sentences were used as development data.
Model2 of (Collins, 1999) was used to parse both
the training and test data.
To capture the lexical information of the parse
trees, we did not use a standard CFG tree but a
lexicalized-CFG tree where each non-terminal node
has an extra lexical node labeled with the head word
of the constituent. Figure 3 shows an example of the
lexicalized-CFG tree used in our experiments. The
193
TOP
S
(saw) NP
(I) PRP
I
VP
(saw) VBD
saw
NP
(girl) DT
a
NN
girl
Figure 3: Lexicalized CFG tree for WSJ parsing
head word, e.g., (saw), is put as a leftmost constituent
size parameter s and frequency parameter f were ex-
perimentally set at 6 and 10, respectively. As the
data set is very large, it is difficult to employ the ex-
periments with more unrestricted parameters.
Table 1 lists results on test data for the Model2 of
(Collins, 1999), for several previous studies, and for
our best model. We achieve recall and precision of
89.3/%89.6% and 89.9%/90.1% for sentences with
? 100 words and ? 40 words, respectively. The
method shows a 1.2% absolute improvement in av-
erage precision and recall (from 88.2% to 89.4% for
sentences ? 100 words), a 10.1% relative reduc-
tion in error. (Collins, 2000) achieved 89.6%/89.9%
recall and precision for the same datasets (sen-
tences ? 100 words) using boosting and manu-
ally constructed features. (Charniak, 2000) extends
PCFG and achieves similar performance to (Collins,
2000). The tree kernel method of (Collins and
Duffy, 2002) uses the all-subtrees representation and
achieves 88.6%/88.9% recall and precision, which
are slightly worse than the results obtained with our
model. (Bod, 2001) also uses the all-subtrees repre-
sentation with a very different parameter estimation
method, and realizes 90.06%/90.08% recall and pre-
cision for sentences of ? 40 words.
5.2 Shallow Parsing
We used the same data set as the CoNLL 2000
shared task (Tjong Kim Sang and Buchholz, 2000).
Sections 15-18 of the Penn Treebank were used as
training data, and section 20 was used as test data.
As a baseline model, we used a shallow parser
based on Conditional Random Fields (CRFs), very
similar to that described in (Sha and Pereira, 2003).
CRFs have shown remarkable results in a number
of tagging and chunking tasks in NLP. n-best out-
puts were obtained by a combination of forward
MODEL ? 40 Words (2245 sentences)
LR LP CBs 0 CBs 2 CBs
CO99 88.5% 88.7% 0.92 66.7% 87.1%
CH00 90.1% 90.1% 0.74 70.1% 89.6%
CO00 90.1% 90.4% 0.74 70.3% 89.6%
CO02 89.1% 89.4% 0.85 69.3% 88.2%
Boosting 89.9% 90.1% 0.77 70.5% 89.4%
MODEL ? 100 Words (2416 sentences)
LR LP CBs 0 CBs 2 CBs
CO99 88.1% 88.3% 1.06 64.0% 85.1%
CH00 89.6% 89.5% 0.88 67.6% 87.7%
CO00 89.6% 89.9% 0.87 68.3% 87.7%
CO02 88.6% 88.9% 0.99 66.5% 86.3%
Boosting 89.3% 89.6% 0.90 67.9% 87.5%
Table 1: Results for section 23 of the WSJ Treebank
LR/LP = labeled recall/precision. CBs is the average number
of cross brackets per sentence. 0 CBs, and 2CBs are the per-
centage of sentences with 0 or ? 2 crossing brackets, respec-
tively. COL99 = Model 2 of (Collins, 1999). CH00 = (Char-
niak, 2000), CO00=(Collins, 2000). CO02=(Collins and Duffy,
2002).
Viterbi search and backward A* search. Note that
this search algorithm yields optimal n-best results
in terms of the CRFs score. Each sentence has at
most 20 distinct parses. The log probability from
the CRFs shallow parser was incorporated into the
reranking. Following (Collins, 2000), the training
set was split into 5 portions, and the CRFs shallow
parser was trained on 4/5 of the data, then used to
decode the remaining 1/5. The outputs of the base
parser, which consist of base phrases, were con-
verted into right-branching trees by assuming that
two adjacent base phrases are in a parent-child re-
lationship. Figure 4 shows an example of the tree
for shallow parsing task. We also put two virtual
nodes, left/right boundaries, to capture local transi-
tions. The size parameter s and frequency parameter
f were experimentally set at 6 and 5, respectively.
Table 2 lists results on test data for the baseline
CRFs parser, for several previous studies, and for
our best model. Our model achieves a 94.12 F-
measure, and outperforms the baseline CRFs parser
and the SVMs parser (Kudo and Matsumoto, 2001).
(Zhang et al, 2002) reported a higher F-measure
with a generalized winnow using additional linguis-
tic features. The accuracy of our model is very simi-
lar to that of (Zhang et al, 2002) without using such
additional features. Table 3 shows the results for our
best model per chunk type.
194
TOP
NP
PRP
(L) I (R)
VP
VBD
(L) saw (R)
NP
DT
(L) a
NN
girl (R)
EOS
Figure 4: Tree representation for shallow parsing
Represented in a right-branching tree with two virtual nodes
MODEL F?=1
CRFs (baseline) 93.76
8 SVMs-voting (Kudo and Matsumoto, 2001) 93.91
RW + linguistic features (Zhang et al, 2002) 94.17
Boosting (our model) 94.12
Table 2: Results of shallow parsing
F?=1 is the harmonic mean of precision and recall.
6 Discussion
6.1 Interpretablity and Efficiency
The numbers of active (non-zero) features selected
by boosting are around 8,000 and 3,000 in the WSJ
parsing and shallow parsing, respectively. Although
almost all the subtrees are used as feature candi-
dates, boosting selects a small and highly relevant
subset of features. When we explicitly enumerate
the subtrees used in tree kernel, the number of ac-
tive features might amount to millions or more. Note
that the accuracies under such sparse feature spaces
are still comparable to those obtained with tree ker-
nel. This result supports our first intuition that we
do not always need all the subtrees to construct the
parameters.
The sparse feature representations are useful in
practice as they allow us to analyze what kinds of
features are relevant. Table 4 shows examples of
active features along with their weights wk. In the
shallow parsing tasks, subordinate phrases (SBAR)
are difficult to analyze without seeing long depen-
dencies. Subordinate phrases usually precede a sen-
tence (NP and VP). However, Markov-based shal-
low parsers, such as MEMM or CRFs, cannot cap-
ture such a long dependency. Our model automat-
ically selects useful subtrees to obtain an improve-
ment on subordinate phrases. It is interesting that the
Precision Recall F?=1
ADJP 80.35% 73.41% 76.72
ADVP 83.88% 82.33% 83.10
CONJP 42.86% 66.67% 52.17
INTJ 50.00% 50.00% 50.00
LST 0.00% 0.00% 0.00
NP 94.45% 94.36% 94.41
PP 97.24% 98.07% 97.65
PRT 76.92% 75.47% 76.19
SBAR 90.70% 89.35% 90.02
VP 93.95% 94.72% 94.33
Overall 94.11% 94.13% 94.12
Table 3: Results of shallow parsing per chunk type
tree (SBAR(IN(for))(NP(VP(TO)))) has a large positive
weight, while the tree (SBAR((IN(for))(NP(O)))) has a
negative weight. The improvement on subordinate
phrases is considerable. We achieve 19% of the rel-
ative error reduction for subordinate phrase (from
87.68 to 90.02 in F-measure)
The testing speed of our model is much higher
than that of other models. The speeds of rerank-
ing for WSJ parsing and shallow parsing are 0.055
sec./sent. and 0.042 sec./sent. respectively, which
are fast enough for real applications ?.
6.2 Relationship to previous work
Tree kernel uses the all-subtrees representation not
explicitly but implicitly by reducing the problem to
the calculation of the inner-products of two trees.
The implicit calculation yields a practical computa-
tion in training. However, in testing, kernel meth-
ods require a number of kernel evaluations, which
are too heavy to allow us to realize real applications.
Moreover, tree kernel needs to incorporate a decay
factor to downweight the contribution of larger sub-
trees. It is non-trivial to set the optimal decay factor
as the accuracies are sensitive to its selection.
Similar to our model, data oriented parsing (DOP)
methods (Bod, 1998) deal with the all-subtrees rep-
resentation explicitly. Since the exact computa-
tion of scores for DOP is NP-complete, several ap-
proximations are employed to perform an efficient
parsing. The critical difference between our model
and DOP is that our model leads to an extremely
sparse solution and automatically eliminates redun-
dant subtrees. With the DOP methods, (Bod, 2001)
also employs constraints (e.g., depth of subtrees) to
?We ran these tests on a Linux PC with Pentium 4 3.2 Ghz.
195
WSJ parsing
w active trees that contain the word ?in?
0.3864 (VP(NP(NNS(plants)))(PP(in)))
0.3326 (VP(VP(PP)(PP(in)))(VP))
0.2196 (NP(VP(VP(PP)(PP(in)))))
0.1748 (S(NP(NNP))(PP(in)(NP)))
... ...
-1.1217 (PP(in)(NP(NP(effect))))
-1.1634 (VP(yield)(PP(PP))(PP(in)))
-1.3574 (NP(PP(in)(NP(NN(way)))))
-1.8030 (NP(PP(in)(NP(trading)(JJ))))
shallow parsing
w active trees that contain the phrase ?SBAR?
1.4500 (SBAR(IN(for))(NP(VP(TO))))
0.6177 (VP(SBAR(NP(VBD)))
0.6173 (SBAR(NP(VP(?))))
0.5644 (VP(SBAR(NP(VP(JJ)))))
.. ..
-0.9034 (SBAR(IN(for))(NP(O)))
-0.9181 (SBAR(NP(O)))
-1.0695 (ADVP(NP(SBAR(NP(VP)))))
-1.1699 (SBAR(NP(NN)(NP)))
Table 4: Examples of active features (subtrees)
All trees are represented in S-expression. In the shallow parsing
task, O is a special phrase that means ?out of chunk?.
select relevant subtrees and achieves the best results
for WSJ parsing. However, these techniques are not
based on the regularization framework focused on
this paper and do not always eliminate all the re-
dundant subtrees. Even using the methods of (Bod,
2001), millions of subtrees are still exploited, which
leads to inefficiency in real problems.
7 Conclusions
In this paper, we presented a new application of
boosting for parse reranking, in which all subtrees
are potentially used as distinct features. Although
this set-up greatly increases the feature space, the
l1-norm regularization performed by boosting se-
lects a compact and relevant feature set. Our model
achieved a comparable or even better accuracy than
kernel methods even with an extremely small num-
ber of features (subtrees).
References
Kenji Abe, Shinji Kawasoe, Tatsuya Asai, Hiroki Arimura, and
Setsuo Arikawa. 2002. Optimized substructure discovery
for semi-structured data. In Proc. of PKDD, pages 1?14.
Rens Bod. 1998. Beyond Grammar: An Experience Based The-
ory of Language. CSLI Publications/Cambridge University
Press.
Rens Bod. 2001. What is the minimal set of fragments that
achieves maximal parse accuracy? In Proc. of ACL, pages
66?73.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL, pages 132?139.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proc. of ACL.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML, pages 175?182.
Michael Collins. 2002. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In Proc. of
ACL, pages 489?496.
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for combining
preferences. Journal of Machine Learning Research, 4:933?
969.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support
vector machines. In Proc. of NAACL, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In Proc. of
EMNLP, pages 301?308.
Simon Perkins, Kevin Lacker, and James Thiler. 2003. Graft-
ing: Fast, incremental feature selection by gradient descent
in function space. Journal of Machine Learning Research,
3:1333?1356.
Gunnar. Ra?tsch. 2001. Robust Boosting via Convex Optimiza-
tion. Ph.D. thesis, Department of Computer Science, Uni-
versity of Potsdam.
Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun
Lee. 1997. Boosting the margin: a new explanation for the
effectiveness of voting methods. In Proc. of ICML, pages
322?330.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proc. of HLT-NAACL, pages
213?220.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In Proc.
of CoNLL-2000 and LLL-2000, pages 127?132.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-
Interscience.
Guizhen Yang. 2004. The complexity of mining maximal fre-
quent itemsets and maximal frequent patterns. In Proc. of
SIGKDD.
Mohammed Zaki. 2002. Efficiently mining frequent trees in a
forest. In Proc. of SIGKDD, pages 71?80.
Tong Zhang, Fred Damerau, and David Johnson. 2002. Text
chunking based on a generalization of winnow. Journal of
Machine Learning Research, 2:615?637.
196
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 217?224,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Training Conditional Random Fields with Multivariate Evaluation
Measures
Jun Suzuki, Erik McDermott and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{jun, mcd, isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a framework for train-
ing Conditional Random Fields (CRFs)
to optimize multivariate evaluation mea-
sures, including non-linear measures such
as F-score. Our proposed framework is
derived from an error minimization ap-
proach that provides a simple solution for
directly optimizing any evaluation mea-
sure. Specifically focusing on sequential
segmentation tasks, i.e. text chunking and
named entity recognition, we introduce a
loss function that closely reflects the tar-
get evaluation measure for these tasks,
namely, segmentation F-score. Our ex-
periments show that our method performs
better than standard CRF training.
1 Introduction
Conditional random fields (CRFs) are a recently
introduced formalism (Lafferty et al, 2001) for
representing a conditional model p(y|x), where
both a set of inputs, x, and a set of outputs,
y, display non-trivial interdependency. CRFs are
basically defined as a discriminative model of
Markov random fields conditioned on inputs (ob-
servations) x. Unlike generative models, CRFs
model only the output y?s distribution over x. This
allows CRFs to use flexible features such as com-
plicated functions of multiple observations. The
modeling power of CRFs has been of great ben-
efit in several applications, such as shallow pars-
ing (Sha and Pereira, 2003) and information ex-
traction (McCallum and Li, 2003).
Since the introduction of CRFs, intensive re-
search has been undertaken to boost their effec-
tiveness. The first approach to estimating CRF pa-
rameters is the maximum likelihood (ML) criterion
over conditional probability p(y|x) itself (Laf-
ferty et al, 2001). The ML criterion, however,
is prone to over-fitting the training data, espe-
cially since CRFs are often trained with a very
large number of correlated features. The maximum
a posteriori (MAP) criterion over parameters, ?,
given x and y is the natural choice for reducing
over-fitting (Sha and Pereira, 2003). Moreover,
the Bayes approach, which optimizes both MAP
and the prior distribution of the parameters, has
also been proposed (Qi et al, 2005). Furthermore,
large margin criteria have been employed to op-
timize the model parameters (Taskar et al, 2004;
Tsochantaridis et al, 2005).
These training criteria have yielded excellent re-
sults for various tasks. However, real world tasks
are evaluated by task-specific evaluation mea-
sures, including non-linear measures such as F-
score, while all of the above criteria achieve op-
timization based on the linear combination of av-
erage accuracies, or error rates, rather than a given
task-specific evaluation measure. For example, se-
quential segmentation tasks (SSTs), such as text
chunking and named entity recognition, are gener-
ally evaluated with the segmentation F-score. This
inconsistency between the objective function dur-
ing training and the task evaluation measure might
produce a suboptimal result.
In fact, to overcome this inconsistency, an
SVM-based multivariate optimization method has
recently been proposed (Joachims, 2005). More-
over, an F-score optimization method for logis-
tic regression has also been proposed (Jansche,
2005). In the same spirit as the above studies, we
first propose a generalization framework for CRF
training that allows us to optimize directly not
only the error rate, but also any evaluation mea-
sure. In other words, our framework can incor-
porate any evaluation measure of interest into the
loss function and then optimize this loss function
as the training objective function. Our proposed
framework is fundamentally derived from an ap-
proach to (smoothed) error rate minimization well
217
known in the speech and pattern recognition com-
munity, namely the Minimum Classification Er-
ror (MCE) framework (Juang and Katagiri, 1992).
The framework of MCE criterion training supports
the theoretical background of our method. The ap-
proach proposed here subsumes the conventional
ML/MAP criteria training of CRFs, as described
in the following.
After describing the new framework, as an ex-
ample of optimizing multivariate evaluation mea-
sures, we focus on SSTs and introduce a segmen-
tation F-score loss function for CRFs.
2 CRFs and Training Criteria
Given an input (observation) x?X and parameter
vector ? = {?1, . . . , ?M}, CRFs define the con-
ditional probability p(y|x) of a particular output
y ? Y as being proportional to a product of po-
tential functions on the cliques of a graph, which
represents the interdependency of y and x. That
is:
p(y|x; ?) = 1Z?(x)
?
c?C(y,x)
?c(y,x; ?)
where ?c(y,x; ?) is a non-negative real value po-
tential function on a clique c ? C(y,x). Z?(x)=
?
y??Y
?
c?C(y?,x) ?c(y?,x; ?) is a normalization
factor over all output values, Y .
Following the definitions of (Sha and Pereira,
2003), a log-linear combination of weighted fea-
tures, ?c(y,x; ?) = exp(? ? f c(y,x)), is used
as individual potential functions, where f c rep-
resents a feature vector obtained from the corre-
sponding clique c. That is, ?c?C(y,x) ?c(y,x) =
exp(??F (y,x)), where F (y,x)=?c f c(y,x) is
the CRF?s global feature vector for x and y.
The most probable output y? is given by y? =
arg maxy?Y p(y|x; ?). However Z?(x) never af-
fects the decision of y? since Z?(x) does not de-
pend on y. Thus, we can obtain the following dis-
criminant function for CRFs:
y? = arg max
y?Y
? ? F (y,x). (1)
The maximum (log-)likelihood (ML) of the
conditional probability p(y|x; ?) of training
data {(xk,y?k)}Nk=1 w.r.t. parameters ? is
the most basic CRF training criterion, that is,
arg max?
?
k log p(y?k|xk; ?), where y?k is the
correct output for the given xk. Maximizing
the conditional log-likelihood given by CRFs is
equivalent to minimizing the log-loss function,
?
k? log p(y?k|xk; ?). We minimize the follow-
ing loss function for the ML criterion training of
CRFs:
LML? =
?
k
[
?? ? F (y?k,xk) + logZ?(xk)
]
.
To reduce over-fitting, the Maximum a
Posteriori (MAP) criterion of parameters
?, that is, arg max?
?
k log p(?|y?k,xk) ?
?
k log p(y?k|xk; ?)p(?), is now the most widely
used CRF training criterion. Therefore, we
minimize the following loss function for the MAP
criterion training of CRFs:
LMAP? = LML? ? log p(?). (2)
There are several possible choices when selecting
a prior distribution p(?). This paper only con-
siders L?-norm prior, p(?)? exp(?||?||?/?C),
which becomes a Gaussian prior when ?=2. The
essential difference between ML and MAP is sim-
ply that MAP has this prior term in the objective
function. This paper sometimes refers to the ML
and MAP criterion training of CRFs as ML/MAP.
In order to estimate the parameters ?, we seek a
zero of the gradient over the parameters ?:
?LMAP? = ?? log p(?) +
?
k
[
?F (y?k,xk)
+
?
y?Yk
exp(??F (y,xk))
Z?(xk)
?F (y,xk)
]
.
(3)
The gradient of ML is Eq. 3 without the gradient
term of the prior, ?? log p(?).
The details of actual optimization procedures
for linear chain CRFs, which are typical CRF ap-
plications, have already been reported (Sha and
Pereira, 2003).
3 MCE Criterion Training for CRFs
The Minimum Classification Error (MCE) frame-
work first arose out of a broader family of ap-
proaches to pattern classifier design known as
Generalized Probabilistic Descent (GPD) (Kata-
giri et al, 1991). The MCE criterion minimizes
an empirical loss corresponding to a smooth ap-
proximation of the classification error. This MCE
loss is itself defined in terms of a misclassifica-
tion measure derived from the discriminant func-
tions of a given task. Via the smoothing parame-
ters, the MCE loss function can be made arbitrarily
close to the binary classification error. An impor-
tant property of this framework is that it makes it
218
possible in principle to achieve the optimal Bayes
error even under incorrect modeling assumptions.
It is easy to extend the MCE framework to use
evaluation measures other than the classification
error, namely the linear combination of error rates.
Thus, it is possible to optimize directly a variety of
(smoothed) evaluation measures. This is the ap-
proach proposed in this article.
We first introduce a framework for MCE crite-
rion training, focusing only on error rate optimiza-
tion. Sec. 4 then describes an example of mini-
mizing a different multivariate evaluation measure
using MCE criterion training.
3.1 Brief Overview of MCE
Let x ? X be an input, and y ? Y be an output.
The Bayes decision rule decides the most probable
output y? for x, by using the maximum a posteriori
probability, y? = arg maxy?Y p(y|x; ?). In gen-
eral, p(y|x; ?) can be replaced by a more general
discriminant function, that is,
y? = arg max
y?Y
g(y,x,?). (4)
Using the discriminant functions for the possi-
ble output of the task, the misclassification mea-
sure d() is defined as follows:
d(y?,x,?)=?g(y?,x,?) + max
y?Y\y?
g(y,x,?). (5)
where y? is the correct output for x. Here it can
be noted that, for a given x, d()?0 indicates mis-
classification. By using d(), the minimization of
the error rate can be rewritten as the minimization
of the sum of 0-1 (step) losses of the given training
data. That is, arg min? L? where
L?=
?
k
?(d(y?k,xk,?)). (6)
?(r) is a step function returning 0 if r<0 and 1 oth-
erwise. That is, ? is 0 if the value of the discrimi-
nant function of the correct output g(y?k,xk,?) is
greater than that of the maximum incorrect output
g(yk,xk,?), and ? is 1 otherwise.
Eq. 5 is not an appropriate function for op-
timization since it is a discontinuous function
w.r.t. the parameters ?. One choice of contin-
uous misclassification measure consists of sub-
stituting ?max? with ?soft-max?, maxk rk ?
log ?k exp(rk). As a result
d(y?,x,?)=?g?+log
[
A
?
y?Y\y?
exp(?g)
]
1
?
, (7)
where g? = g(y?,x,?), g= g(y,x,?), and A=
1
|Y|?1 . ? is a positive constant that represents L?-
norm. When ? approaches ?, Eq. 7 converges to
Eq. 5. Note that we can design any misclassifi-
cation measure, including non-linear measures for
d(). Some examples are shown in the Appendices.
Of even greater concern is the fact that the step
function ? is discontinuous; minimization of Eq.
6 is therefore NP-complete. In the MCE formal-
ism, ?() is replaced with an approximated 0-1 loss
function, l(), which we refer to as a smoothing
function. A typical choice for l() is the sigmoid
function, lsig(), which is differentiable and pro-
vides a good approximation of the 0-1 loss when
the hyper-parameter ? is large (see Eq. 8). An-
other choice is the (regularized) logistic function,
llog(), that gives the upper bound of the 0-1 loss.
Logistic loss is used as a conventional CRF loss
function and provides convexity while the sigmoid
function does not. These two smoothing functions
can be written as follows:
lsig = (1 + exp(?? ? d(y?,x,?) ? ?))?1
llog = ??1 ? log(1 + exp(? ? d(y?,x,?) + ?)), (8)
where ? and ? are the hyper-parameters of the
training.
We can introduce a regularization term to re-
duce over-fitting, which is derived using the same
sense as in MAP, Eq. 2. Finally, the objective func-
tion of the MCE criterion with the regularization
term can be rewritten in the following form:
LMCE? = Fl,d,g,?
[
{(xk,y?k)}Nk=1
]
+ ||?||
?
?C . (9)
Then, the objective function of the MCE criterion
that minimizes the error rate is Eq. 9 and
FMCEl,d,g,? =
1
N
N
?
k=1
l(d(y?k,xk,?)) (10)
is substituted for Fl,d,g,?. Since N is constant, we
can eliminate the term 1/N in actual use.
3.2 Formalization
We simply substitute the discriminant function of
the CRFs into that of the MCE criterion:
g(y,x,?) = log p(y|x; ?) ? ? ? F (y,x) (11)
Basically, CRF training with the MCE criterion
optimizes Eq. 9 with Eq. 11 after the selection of
an appropriate misclassification measure, d(), and
219
smoothing function, l(). Although there is no re-
striction on the choice of d() and l(), in this work
we select sigmoid or logistic functions for l() and
Eq. 7 for d().
The gradient of the loss function Eq. 9 can be
decomposed by the following chain rule:
?LMCE? =
?F()
?l() ?
?l()
?d() ?
?d()
?? +
||?||??1
C .
The derivatives of l() w.r.t. d() given in Eq.
8 are written as: ?lsig/?d = ? ? lsig ? (1? lsig) and
?llog/?d= lsig.
The derivative of d() of Eq. 7 w.r.t. parameters
? is written in this form:
?d()
?? = ?
Z?(x, ?)
Z?(x, ?)?exp(?g?)
?F (y?,x)
+
?
y?Y
[
exp(?g)
Z?(x, ?)?exp(?g?)
?F (y,x)
] (12)
where g = ? ?F (y,x), g? = ? ?F (y?,x), and
Z?(x, ?)=
?
y?Y exp(?g).
Note that we can obtain exactly the same loss
function as ML/MAP with appropriate choices of
F(), l() and d(). The details are provided in the
Appendices. Therefore, ML/MAP can be seen as
one special case of the framework proposed here.
In other words, our method provides a generalized
framework of CRF training.
3.3 Optimization Procedure
With linear chain CRFs, we can calculate the ob-
jective function, Eq. 9 combined with Eq. 10,
and the gradient, Eq. 12, by using the variant of
the forward-backward and Viterbi algorithm de-
scribed in (Sha and Pereira, 2003). Moreover, for
the parameter optimization process, we can simply
exploit gradient descent or quasi-Newton methods
such as L-BFGS (Liu and Nocedal, 1989) as well
as ML/MAP optimization.
If we select ? = ? for Eq. 7, we only need
to evaluate the correct and the maximum incor-
rect output. As we know, the maximum output
can be efficiently calculated with the Viterbi al-
gorithm, which is the same as calculating Eq. 1.
Therefore, we can find the maximum incorrect
output by using the A* algorithm (Hart et al,
1968), if the maximum output is the correct out-
put, and by using the Viterbi algorithm otherwise.
It may be feared that since the objective func-
tion is not differentiable everywhere for ?=?,
problems for optimization would occur. How-
ever, it has been shown (Le Roux and McDer-
mott, 2005) that even simple gradient-based (first-
order) optimization methods such as GPD and (ap-
proximated) second-order methods such as Quick-
Prop (Fahlman, 1988) and BFGS-based methods
have yielded good experimental optimization re-
sults.
4 Multivariate Evaluation Measures
Thus far, we have discussed the error rate ver-
sion of MCE. Unlike ML/MAP, the framework of
MCE criterion training allows the embedding of
not only a linear combination of error rates, but
also any evaluation measure, including non-linear
measures.
Several non-linear objective functions, such as
F-score for text classification (Gao et al, 2003),
and BLEU-score and some other evaluation mea-
sures for statistical machine translation (Och,
2003), have been introduced with reference to the
framework of MCE criterion training.
4.1 Sequential Segmentation Tasks (SSTs)
Hereafter, we focus solely on CRFs in sequences,
namely the linear chain CRF. We assume that x
and y have the same length: x=(x1, . . . , xn) and
y=(y1, . . . , yn). In a linear chain CRF, yi depends
only on yi?1.
Sequential segmentation tasks (SSTs), such as
text chunking (Chunking) and named entity recog-
nition (NER), which constitute the shared tasks
of the Conference of Natural Language Learn-
ing (CoNLL) 2000, 2002 and 2003, are typical
CRF applications. These tasks require the extrac-
tion of pre-defined segments, referred to as tar-
get segments, from given texts. Fig. 1 shows typ-
ical examples of SSTs. These tasks are gener-
ally treated as sequential labeling problems incor-
porating the IOB tagging scheme (Ramshaw and
Marcus, 1995). The IOB tagging scheme, where
we only consider the IOB2 scheme, is also shown
in Fig. 1. B-X, I-X and O indicate that the word
in question is the beginning of the tag ?X?, inside
the tag ?X?, and outside any target segment, re-
spectively. Therefore, a segment is defined as a
sequence of a few outputs.
4.2 Segmentation F-score Loss for SSTs
The standard evaluation measure of SSTs is the
segmentation F-score (Sang and Buchholz, 2000):
F? =
(?2 + 1) ? TP
?2 ? FN + FP + (?2 + 1) ? TP (13)
220
He   reckons   the  current  account  deficit   will   narrow   to    only   #   1.8   billion  .
NP VP NP VP PP NP
B-NP B-VP B-NP I-NP I-NP I-NP B-VP I-VP B-PP B-NP I-NP I-NP I-NP O
x:
y:
Seg.:
United  Nation   official   Ekeus Smith   heads   for   Baghdad  . 
B-ORG I-ORG O OOB-PER I-PER B-LOC O
x:
y:
Seg.: ORG PER LOC
Text Chunking Named Entity Recognition
y1 y2 y3 y4 y5 y6 y7 y8 y9 y10 y11 y12 y13 y14Dep.: y1 y2 y3 y4 y5 y6 y7 y8 y9Dep.:
Figure 1: Examples of sequential segmentation tasks (SSTs): text chunking (Chunking) and named entity
recognition (NER).
where TP , FP and FN represent true positive,
false positive and false negative counts, respec-
tively.
The individual evaluation units used to calcu-
late TP , FN and PN , are not individual outputs
yi or output sequences y, but rather segments. We
need to define a segment-wise loss, in contrast to
the standard CRF loss, which is sometimes re-
ferred to as an (entire) sequential loss (Kakade
et al, 2002; Altun et al, 2003). First, we con-
sider the point-wise decision w.r.t. Eq. 1, that is,
y?i = arg maxyi?Y1 g(y,x, i,?). The point-wise
discriminant function can be written as follows:
g(y,x, i,?) = max
y??Y|y|[yi]
? ? F (y?,x) (14)
where Yj represents a set of all y whose length
is j, and Y[yi] represents a set of all y that con-
tain yi in the i?th position. Note that the same
output y? can be obtained with Eqs. 1 and 14,
that is, y? = (y?1, . . . , y?n). This point-wise dis-
criminant function is different from that described
in (Kakade et al, 2002; Altun et al, 2003), which
is calculated based on marginals.
Let ysj be an output sequence correspond-
ing to the j-th segment of y, where sj repre-
sents a sequence of indices of y, that is, sj =
(sj,1, . . . , sj,|sj |). An example of the Chunk-
ing data shown in Fig. 1, ys4 is (B-VP, I-VP)
where s4 = (7, 8). Let Y[ysj ] be a set of all
outputs whose positions from sj,1 to sj,|sj | are
ysj = (ysj,1 , . . . , ysj,|sj |). Then, we can define a
segment-wise discriminant function w.r.t. Eq. 1.
That is,
g(y,x, sj ,?) = max
y??Y|y|[ysj ]
? ? F (y?,x). (15)
Note again that the same output y? can be obtained
using Eqs. 1 and 15, as with the piece-wise dis-
criminant function described above. This property
is needed for evaluating segments since we do not
know the correct segments of the test data; we can
maintain consistency even if we use Eq. 1 for test-
ing and Eq. 15 for training. Moreover, Eq. 15 ob-
viously reduces to Eq. 14 if the length of all seg-
ments is 1. Then, the segment-wise misclassifica-
tion measure d(y?,x, sj ,?) can be obtained sim-
ply by replacing the discriminant function of the
entire sequence g(y,x,?) with that of segment-
wise g(y,x, sj ,?) in Eq. 7.
Let s?k be a segment sequence corresponding to
the correct output y?k for a given xk, and S(xk)
be all possible segments for a given xk. Then, ap-
proximated evaluation functions of TP , FP and
FN can be defined as follows:
TPl =
?
k
?
s?j?s?k
[
1?l(d(y?k,xk, s?j ,?))
]
??(s?j )
FPl =
?
k
?
s?j?S(xk)\s?k
l(d(y?k,xk, s?j ,?))??(s?j)
FNl =
?
k
?
s?j?s?k
l(d(y?k,xk, s?j ,?))??(s?j )
where ?(sj) returns 1 if segment sj is a target seg-
ment, and returns 0 otherwise. For the NER data
shown in Fig. 1, ?ORG?, ?PER? and ?LOC? are the
target segments, while segments that are labeled
?O? in y are not. Since TPl should not have a
value of less than zero, we select sigmoid loss as
the smoothing function l().
The second summation of TPl and FNl per-
forms a summation over correct segments s?. In
contrast, the second summation in FPl takes all
possible segments into account, but excludes the
correct segments s?. Although an efficient way to
evaluate all possible segments has been proposed
in the context of semi-Markov CRFs (Sarawagi
and Cohen, 2004), we introduce a simple alter-
native method. If we select ? = ? for d() in
Eq. 7, we only need to evaluate the segments cor-
responding to the maximum incorrect output y? to
calculate FPl. That is, s?j ? S(xk)\s?k can be
reduced to s?j ? s?k, where s?k represents segments
corresponding to the maximum incorrect output y?.
In practice, this reduces the calculation cost and so
we used this method for our experiments described
in the next section.
Maximizing the segmentation F?-score, Eq. 13,
221
is equivalent to minimizing ?
2?FN+FP
(?2+1)?TP , since Eq.
13 can also be written as F? = 11+ ?2?FN+FP(?2+1)?TP
. Thus,
an objective function closely reflecting the seg-
mentation F?-score based on the MCE criterion
can be written as Eq. 9 while replacing Fl,d,g,?
with:
FMCE-Fl,d,g,? =
?2 ? FNl + FPl
(?2 + 1) ? TPl
. (16)
The derivative of Eq. 16 w.r.t. l() is given by the
following equation:
?FMCE-Fl,d,g,?
?l() =
{
?2
ZD
+ (?
2+1)?ZN
Z2D
, if ?(s?j ) = 1
1
ZD
, otherwise
whereZN andZD represent the numerator and de-
nominator of Eq. 16, respectively.
In the optimization process of the segmentation
F-score objective function, we can efficiently cal-
culate Eq. 15 by using the forward and backward
Viterbi algorithm, which is almost the same as
calculating Eq. 3 with a variant of the forward-
backward algorithm (Sha and Pereira, 2003). The
same numerical optimization methods described
in Sec. 3.3 can be employed for this optimization.
5 Experiments
We used the same Chunking and ?English? NER
task data used for the shared tasks of CoNLL-
2000 (Sang and Buchholz, 2000) and CoNLL-
2003 (Sang and De Meulder, 2003), respectively.
Chunking data was obtained from the Wall
Street Journal (WSJ) corpus: sections 15-18 as
training data (8,936 sentences and 211,727 to-
kens), and section 20 as test data (2,012 sentences
and 47,377 tokens), with 11 different chunk-tags,
such as NP and VP plus the ?O? tag, which repre-
sents the outside of any target chunk (segment).
The English NER data was taken from the
Reuters Corpus21. The data consists of 203,621,
51,362 and 46,435 tokens from 14,987, 3,466
and 3,684 sentences in training, development and
test data, respectively, with four named entity
tags, PERSON, LOCATION, ORGANIZATION
and MISC, plus the ?O? tag.
5.1 Comparison Methods and Parameters
For ML and MAP, we performed exactly the same
training procedure described in (Sha and Pereira,
2003) with L-BFGS optimization. For MCE, we
1http://trec.nist.gov/data/reuters/reuters.html
only considered d() with ? = ? as described in
Sec. 4.2, and used QuickProp optimization2.
For MAP, MCE and MCE-F, we used the L2-
norm regularization. We selected a value of C
from 1.0? 10n where n takes a value from -5 to 5
in intervals 1 by development data3. The tuning of
smoothing function hyper-parameters is not con-
sidered in this paper; that is, ?=1 and ?=0 were
used for all the experiments.
We evaluated the performance by Eq. 13 with
? = 1, which is the evaluation measure used in
CoNLL-2000 and 2003. Moreover, we evaluated
the performance by using the average sentence ac-
curacy, since the conventional ML/MAP objective
function reflects this sequential accuracy.
5.2 Features
As regards the basic feature set for Chunking, we
followed (Kudo and Matsumoto, 2001), which is
the same feature set that provided the best result
in CoNLL-2000. We expanded the basic features
by using bigram combinations of the same types
of features, such as words and part-of-speech tags,
within window size 5.
In contrast to the above, we used the original
feature set for NER. We used features derived only
from the data provided by CoNLL-2003 with the
addition of character-level regular expressions of
uppercases [A-Z], lowercases [a-z], digits [0-9] or
others, and prefixes and suffixes of one to four let-
ters. We also expanded the above basic features by
using bigram combinations within window size 5.
Note that we never used features derived from ex-
ternal information such as the Web, or a dictionary,
which have been used in many previous studies but
which are difficult to employ for validating the ex-
periments.
5.3 Results and Discussion
Our experiments were designed to investigate the
impact of eliminating the inconsistency between
objective functions and evaluation measures, that
is, to compare ML/MAP and MCE-F.
Table 1 shows the results of Chunking and NER.
The F?=1 and ?Sent? columns show the perfor-
mance evaluated using segmentation F-score and
2In order to realize faster convergence, we applied online
GPD optimization for the first ten iterations.
3Chunking has no common development set. We first
train the systems with all but the last 2000 sentences in the
training data as a development set to obtain C, and then re-
train them with all the training data.
222
Table 1: Performance of text chunking and named
entity recognition data (CoNLL-2000 and 2003)
Chunking NER
l() n F?=1 Sent n F?=1 Sent
MCE-F (sig) 5 93.96 60.44 4 84.72 78.72
MCE (log) 3 93.92 60.19 3 84.30 78.02
MCE (sig) 3 93.85 60.14 3 83.82 77.52
MAP 0 93.71 59.15 0 83.79 77.39
ML - 93.19 56.26 - 82.39 75.71
sentence accuracy, respectively. MCE-F refers to
the results obtained from optimizing Eq. 9 based
on Eq. 16. In addition, we evaluated the error
rate version of MCE. MCE(log) and MCE(sig)
indicate that logistic and sigmoid functions are
selected for l(), respectively, when optimizing
Eq. 9 based on Eq. 10. Moreover, MCE(log) and
MCE(sig) used d() based on ?=?, and were op-
timized using QuickProp; these are the same con-
ditions as used for MCE-F. We found that MCE-F
exhibited the best results for both Chunking and
NER. There is a significant difference (p<0.01)
between MCE-F and ML/MAP with the McNemar
test, in terms of the correctness of both individual
outputs, yki , and sentences, yk.
NER data has 83.3% (170524/204567) and
82.6% (38554/46666) of ?O? tags in the training
and test data, respectively while the correspond-
ing values of the Chunking data are only 13.1%
(27902/211727) and 13.0% (6180/47377). In gen-
eral, such an imbalanced data set is unsuitable for
accuracy-based evaluation. This may be one rea-
son why MCE-F improved the NER results much
more than the Chunking results.
The only difference between MCE(sig) and
MCE-F is the objective function. The correspond-
ing results reveal the effectiveness of using an ob-
jective function that is consistent as the evalua-
tion measure for the target task. These results
show that minimizing the error rate is not opti-
mal for improving the segmentation F-score eval-
uation measure. Eliminating the inconsistency be-
tween the task evaluation measure and the objec-
tive function during the training can improve the
overall performance.
5.3.1 Influence of Initial Parameters
While ML/MAP and MCE(log) is convex w.r.t.
the parameters, neither the objective function of
MCE-F, nor that of MCE(sig), is convex. There-
fore, initial parameters can affect the optimization
Table 2: Performance when initial parameters are
derived from MAP
Chunking NER
l() n F?=1 Sent n F?=1 Sent
MCE-F (sig) 5 94.03 60.74 4 85.29 79.26
MCE (sig) 3 93.97 60.59 3 84.57 77.71
results, since QuickProp as well as L-BFGS can
only find local optima.
The previous experiments were only performed
with all parameters initialized at zero. In this ex-
periment, the parameters obtained by the MAP-
trained model were used as the initial values of
MCE-F and MCE(sig). This evaluation setting ap-
pears to be similar to reranking, although we used
exactly the same model and feature set.
Table 2 shows the results of Chunking and NER
obtained with this parameter initialization setting.
When we compare Tables 1 and 2, we find that
the initialization with the MAP parameter values
further improves performance.
6 Related Work
Various loss functions have been proposed for de-
signing CRFs (Kakade et al, 2002; Altun et al,
2003). This work also takes the design of the loss
functions for CRFs into consideration. However,
we proposed a general framework for designing
these loss function that included non-linear loss
functions, which has not been considered in pre-
vious work.
With Chunking, (Kudo and Matsumoto, 2001)
reported the best F-score of 93.91 with the vot-
ing of several models trained by Support Vec-
tor Machine in the same experimental settings
and with the same feature set. MCE-F with the
MAP parameter initialization achieved an F-score
of 94.03, which surpasses the above result without
manual parameter tuning.
With NER, we cannot make a direct compari-
son with previous work in the same experimental
settings because of the different feature set, as de-
scribed in Sec. 5.2. However, MCE-F showed the
better performance of 85.29 compared with (Mc-
Callum and Li, 2003) of 84.04, which used the
MAP training of CRFs with a feature selection ar-
chitecture, yielding similar results to the MAP re-
sults described here.
223
7 Conclusions
We proposed a framework for training CRFs based
on optimization criteria directly related to target
multivariate evaluation measures. We first pro-
vided a general framework of CRF training based
on MCE criterion. Then, specifically focusing
on SSTs, we introduced an approximate segmen-
tation F-score objective function. Experimental
results showed that eliminating the inconsistency
between the task evaluation measure and the ob-
jective function used during training improves the
overall performance in the target task without any
change in feature set or model.
Appendices
Misclassification measures
Another type of misclassification measure using
soft-max is (Katagiri et al, 1991):
d(y,x,?) = ?g? +
[
A
?
y?Y\y?
g?
]
1
?
.
Another d(), for g in the range [0,?):
d(y,x,?) =
[
A
?
y?Y\y? g
?
] 1
? /g?.
Comparison of ML/MAP and MCE
If we select llog() with ?=1 and ?=0, and use Eq.
7 with ?= 1 and without the term A for d(). We
can obtain the same loss function as ML/MAP:
log (1 + exp(?g? + log(Z? ? exp(g?))))
= log
(
exp(g?) + (Z? ? exp(g?))
exp(g?)
)
= ?g? + log(Z?).
References
Y. Altun, M. Johnson, and T. Hofmann. 2003. Investigating
Loss Functions and Optimization Methods for Discrimi-
native Learning of Label Sequences. In Proc. of EMNLP-
2003, pages 145?152.
S. E. Fahlman. 1988. An Empirical Study of Learning
Speech in Backpropagation Networks. In Technical Re-
port CMU-CS-88-162, Carnegie Mellon University.
S. Gao, W. Wu, C.-H. Lee, and T.-S. Chua. 2003. A Maxi-
mal Figure-of-Merit Approach to Text Categorization. In
Proc. of SIGIR?03, pages 174?181.
P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A Formal
Basis for the Heuristic Determination of Minimum Cost
Paths. IEEE Trans. on Systems Science and Cybernetics,
SSC-4(2):100?107.
M. Jansche. 2005. Maximum Expected F-Measure Training
of Logistic Regression Models. In Proc. of HLT/EMNLP-
2005, pages 692?699.
T. Joachims. 2005. A Support Vector Method for Multivari-
ate Performance Measures. In Proc. of ICML-2005, pages
377?384.
B. H. Juang and S. Katagiri. 1992. Discriminative Learning
for Minimum Error Classification. IEEE Trans. on Signal
Processing, 40(12):3043?3053.
S. Kakade, Y. W. Teh, and S. Roweis. 2002. An Alterna-
tive Objective Function for Markovian Fields. In Proc. of
ICML-2002, pages 275?282.
S. Katagiri, C. H. Lee, and B.-H. Juang. 1991. New Dis-
criminative Training Algorithms based on the Generalized
Descent Method. In Proc. of IEEE Workshop on Neural
Networks for Signal Processing, pages 299?308.
T. Kudo and Y. Matsumoto. 2001. Chunking with Support
Vector Machines. In Proc. of NAACL-2001, pages 192?
199.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data. In Proc. of ICML-2001, pages
282?289.
D. C. Liu and J. Nocedal. 1989. On the Limited Memory
BFGS Method for Large-scale Optimization. Mathematic
Programming, (45):503?528.
A. McCallum and W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields Fea-
ture Induction and Web-Enhanced Lexicons. In Proc. of
CoNLL-2003, pages 188?191.
F. J. Och. 2003. Minimum Error Rate Training in Statistical
Machine Translation. In Proc. of ACL-2003, pages 160?
167.
Y. Qi, M. Szummer, and T. P. Minka. 2005. Bayesian Con-
ditional Random Fields. In Proc. of AI & Statistics 2005.
L. A. Ramshaw and M. P. Marcus. 1995. Text Chunking
using Transformation-based Learning. In Proc. of VLC-
1995, pages 88?94.
J. Le Roux and E. McDermott. 2005. Optimization Methods
for Discriminative Training. In Proc. of Eurospeech 2005,
pages 3341?3344.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction
to the CoNLL-2000 Shared Task: Chunking. In Proc. of
CoNLL/LLL-2000, pages 127?132.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction
to the CoNLL-2003 Shared Task: Language-Independent
Named Entity Recognition. In Proc. of CoNLL-2003,
pages 142?147.
S. Sarawagi and W. W. Cohen. 2004. Semi-Markov Condi-
tional Random Fields for Information Extraction. In Proc
of NIPS-2004.
F. Sha and F. Pereira. 2003. Shallow Parsing with Con-
ditional Random Fields. In Proc. of HLT/NAACL-2003,
pages 213?220.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-Margin
Markov Networks. In Proc. of NIPS-2004.
I. Tsochantaridis, T. Joachims and T. Hofmann, and Y. Altun.
2005. Large Margin Methods for Structured and Interde-
pendent Output Variables. JMLR, 6:1453?1484.
224
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 617?624,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Incorporating speech recognition confidence into
discriminative named entity recognition of speech data
Katsuhito Sudoh Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan
{sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes a named entity recog-
nition (NER) method for speech recogni-
tion results that uses confidence on auto-
matic speech recognition (ASR) as a fea-
ture. The ASR confidence feature indi-
cates whether each word has been cor-
rectly recognized. The NER model is
trained using ASR results with named en-
tity (NE) labels as well as the correspond-
ing transcriptions with NE labels. In ex-
periments using support vector machines
(SVMs) and speech data from Japanese
newspaper articles, the proposed method
outperformed a simple application of text-
based NER to ASR results in NER F-
measure by improving precision. These
results show that the proposed method is
effective in NER for noisy inputs.
1 Introduction
As network bandwidths and storage capacities
continue to grow, a large volume of speech data
including broadcast news and PodCasts is becom-
ing available. These data are important informa-
tion sources as well as such text data as newspaper
articles and WWW pages. Speech data as infor-
mation sources are attracting a great deal of inter-
est, such as DARPA?s global autonomous language
exploitation (GALE) program. We also aim to use
them for information extraction (IE), question an-
swering, and indexing.
Named entity recognition (NER) is a key tech-
nique for IE and other natural language process-
ing tasks. Named entities (NEs) are the proper ex-
pressions for things such as peoples? names, loca-
tions? names, and dates, and NER identifies those
expressions and their categories. Unlike text data,
speech data introduce automatic speech recogni-
tion (ASR) error problems to NER. Although im-
provements to ASR are needed, developing a ro-
bust NER for noisy word sequences is also impor-
tant. In this paper, we focus on the NER of ASR
results and discuss the suppression of ASR error
problems in NER.
Most previous studies of the NER of speech
data used generative models such as hidden
Markov models (HMMs) (Miller et al, 1999;
Palmer and Ostendorf, 2001; Horlock and King,
2003b; Be?chet et al, 2004; Favre et al, 2005).
On the other hand, in text-based NER, better re-
sults are obtained using discriminative schemes
such as maximum entropy (ME) models (Borth-
wick, 1999; Chieu and Ng, 2003), support vec-
tor machines (SVMs) (Isozaki and Kazawa, 2002),
and conditional random fields (CRFs) (McCal-
lum and Li, 2003). Zhai et al (2004) applied a
text-level ME-based NER to ASR results. These
models have an advantage in utilizing various fea-
tures, such as part-of-speech information, charac-
ter types, and surrounding words, which may be
overlapped, while overlapping features are hard to
use in HMM-based models.
To deal with ASR error problems in NER,
Palmer and Ostendorf (2001) proposed an HMM-
based NER method that explicitly models ASR er-
rors using ASR confidence and rejects erroneous
word hypotheses in the ASR results. Such rejec-
tion is especially effective when ASR accuracy is
relatively low because many misrecognized words
may be extracted as NEs, which would decrease
NER precision.
Motivated by these issues, we extended their ap-
proach to discriminative models and propose an
NER method that deals with ASR errors as fea-
617
tures. We use NE-labeled ASR results for training
to incorporate the features into the NER model as
well as the corresponding transcriptions with NE
labels. In testing, ASR errors are identified by
ASR confidence scores and are used for the NER.
In experiments using SVM-based NER and speech
data from Japanese newspaper articles, the pro-
posed method increased the NER F-measure, es-
pecially in precision, compared to simply applying
text-based NER to the ASR results.
2 SVM-based NER
NER is a kind of chunking problem that can
be solved by classifying words into NE classes
that consist of name categories and such chunk-
ing states as PERSON-BEGIN (the beginning of
a person?s name) and LOCATION-MIDDLE (the
middle of a location?s name). Many discrimi-
native methods have been applied to NER, such
as decision trees (Sekine et al, 1998), ME mod-
els (Borthwick, 1999; Chieu and Ng, 2003), and
CRFs (McCallum and Li, 2003). In this paper, we
employ an SVM-based NER method in the follow-
ing way that showed good NER performance in
Japanese (Isozaki and Kazawa, 2002).
We define three features for each word: the
word itself, its part-of-speech tag, and its charac-
ter type. We also use those features for the two
preceding and succeeding words for context de-
pendence and use 15 features when classifying a
word. Each feature is represented by a binary
value (1 or 0), for example, ?whether the previous
word is Japan,? and each word is classified based
on a long binary vector where only 15 elements
are 1.
We have two problems when solving NER
using SVMs. One, SVMs can solve only a
two-class problem. We reduce multi-class prob-
lems of NER to a group of two-class problems
using the one-against-all approach, where each
SVM is trained to distinguish members of a
class (e.g., PERSON-BEGIN) from non-members
(PERSON-MIDDLE, MONEY-BEGIN, ... ). In this
approach, two or more classes may be assigned to
a word or no class may be assigned to a word. To
avoid these situations, we choose class c that has
the largest SVM output score gc(x) among all oth-
ers.
The other is that the NE label sequence must be
consistent; for example, ARTIFACT-END
must follow ARTIFACT-BEGIN or
Speech data
NE-labeled
transcriptions
Transcriptions ASR results
ASR-based
training data
Text-based
training data
           Manual
transcription ASR
NE labeling
Setting ASR
confidence
feature to 1
Alignment
&
identifying
ASR errors
and NEs
Figure 1: Procedure for preparing training data.
ARTIFACT-MIDDLE. We use a Viterbi search to
obtain the best and consistent NE label sequence
after classifying all words in a sentence, based
on probability-like values obtained by applying
sigmoid function sn(x) = 1/(1 + exp(??nx)) to
SVM output score gc(x).
3 Proposed method
3.1 Incorporating ASR confidence into NER
In the NER of ASR results, ASR errors cause NEs
to be missed and erroneous NEs to be recognized.
If one or more words constituting an NE are mis-
recognized, we cannot recognize the correct NE.
Even if all words constituting an NE are correctly
recognized, we may not recognize the correct NE
due to ASR errors on context words. To avoid
this problem, we model ASR errors using addi-
tional features that indicate whether each word is
correctly recognized. Our NER model is trained
using ASR results with a feature, where feature
values are obtained through alignment to the cor-
responding transcriptions. In testing, we estimate
feature values using ASR confidence scores. In
this paper, this feature is called the ASR confidence
feature.
Note that we only aim to identify NEs that are
correctly recognized by ASR, and NEs containing
ASR errors are not regarded as NEs. Utilizing er-
roneous NEs is a more difficult problem that is be-
yond the scope of this paper.
3.2 Training NER model
Figure 1 illustrates the procedure for preparing
training data from speech data. First, the speech
618
data are manually transcribed and automatically
recognized by the ASR. Second, we label NEs
in the transcriptions and then set the ASR con-
fidence feature values to 1 because the words in
the transcriptions are regarded as correctly recog-
nized words. Finally, we align the ASR results to
the transcriptions to identify ASR errors for the
ASR confidence feature values and to label cor-
rectly recognized NEs in the ASR results. Note
that we label the NEs in the ASR results that exist
in the same positions as the transcriptions. If a part
of an NE is misrecognized, the NE is ignored, and
all words for the NE are labeled as non-NE words
(OTHER). Examples of text-based and ASR-based
training data are shown in Tables 1 and 2. Since
the name Murayama Tomiichi in Table 1 is mis-
recognized in ASR, the correctly recognized word
Murayama is also labeled OTHER in Table 2. An-
other approach can be considered, where misrec-
ognized words are replaced by word error symbols
such as those shown in Table 3. In this case, those
words are rejected, and those part-of-speech and
character type features are not used in NER.
3.3 ASR confidence scoring for using the
proposed NER model
ASR confidence scoring is an important technique
in many ASR applications, and many methods
have been proposed including using word poste-
rior probabilities on word graphs (Wessel et al,
2001), integrating several confidence measures us-
ing neural networks (Schaaf and Kemp, 1997),
using linear discriminant analysis (Kamppari and
Hazen, 2000), and using SVMs (Zhang and Rud-
nicky, 2001).
Word posterior probability is a commonly used
and effective ASR confidence measure. Word pos-
terior probability p([w; ?, t]|X) of word w at time
interval [?, t] for speech signal X is calculated as
follows (Wessel et al, 2001):
p([w; ?, t]|X)
=
?
W?W [w;?,t]
{
p(X|W ) (p(W ))?
}?
p(X) , (1)
where W is a sentence hypothesis, W [w; ?, t] is
the set of sentence hypotheses that include w in
[?, t], p(X|W ) is a acoustic model score, p(W )
is a language model score, ? is a scaling param-
eter (?<1), and ? is a language model weight.
? is used for scaling the large dynamic range of
Word Confidence NE label
Murayama 1 PERSON-BEGIN
Tomiichi 1 PERSON-END
shusho 1 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 1: An example of text-based training data.
Word Confidence NE label
Murayama 1 OTHER
shi 0 OTHER
ni 0 OTHER
ichi 0 OTHER
shiyo 0 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 2: An example of ASR-based training data.
Word Confidence NE label
Murayama 1 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
Table 3: An example of ASR-based training data
with word error symbols.
p(X|W )(p(W ))? to avoid a few of the top hy-
potheses dominating posterior probabilities. p(X)
is approximated by the sum over all sentence hy-
potheses and is denoted as
p(X) =
?
W
{
p(X|W ) (p(W ))?
}? . (2)
p([w; ?, t]|X) can be efficiently calculated using a
forward-backward algorithm.
In this paper, we use SVMs for ASR confidence
scoring to achieve a better performance than when
using word posterior probabilities as ASR confi-
dence scores. SVMs are trained using ASR re-
sults, whose errors are known through their align-
ment to their reference transcriptions. The follow-
ing features are used for confidence scoring: the
word itself, its part-of-speech tag, and its word
posterior probability; those of the two preceding
and succeeding words are also used. The word
itself and its part-of-speech are also represented
619
by a set of binary values, the same as with an
SVM-based NER. Since all other features are bi-
nary, we reduce real-valued word posterior prob-
ability p to ten binary features for simplicity: (if
0 < p ? 0.1, if 0.1 < p ? 0.2, ... , and if
0.9 < p ? 1.0). To normalize SVMs? output
scores for ASR confidence, we use a sigmoid func-
tion sw(x) = 1/(1 + exp(??wx)). We use these
normalized scores as ASR confidence scores. Al-
though a large variety of features have been pro-
posed in previous studies, we use only these sim-
ple features and reserve the other features for fur-
ther studies.
Using the ASR confidence scores, we estimate
whether each word is correctly recognized. If the
ASR confidence score of a word is greater than
threshold tw, the word is estimated as correct, and
we set the ASR confidence feature value to 1; oth-
erwise we set it to 0.
3.4 Rejection at the NER level
We use the ASR confidence feature to suppress
ASR error problems; however, even text-based
NERs sometimes make errors. NER performance
is a trade-off between missing correct NEs and
accepting erroneous NEs, and requirements dif-
fer by task. Although we can tune the parame-
ters in training SVMs to control the trade-off, it
seems very hard to find appropriate values for all
the SVMs. We use a simple NER-level rejection
by modifying the SVM output scores for the non-
NE class (OTHER). We add constant offset value to
to each SVM output score for OTHER. With a large
to, OTHER becomes more desirable than the other
NE classes, and many words are classified as non-
NE words and vice versa. Therefore, to works as a
parameter for NER-level rejection. This approach
can also be applied to text-based NER.
4 Experiments
We conducted the following experiments related
to the NER of speech data to investigate the per-
formance of the proposed method.
4.1 Setup
In the experiment, we simulated the procedure
shown in Figure 1 using speech data from the
NE-labeled text corpus. We used the training
data of the Information Retrieval and Extraction
Exercise (IREX) workshop (Sekine and Eriguchi,
2000) as the text corpus, which consisted of 1,174
Japanese newspaper articles (10,718 sentences)
and 18,200 NEs in eight categories (artifact, or-
ganization, location, person, date, time, money,
and percent). The sentences were read by 106
speakers (about 100 sentences per speaker), and
the recorded speech data were used for the exper-
iments. The experiments were conducted with 5-
fold cross validation, using 80% of the 1,174 ar-
ticles and the ASR results of the corresponding
speech data for training SVMs (both for ASR con-
fidence scoring and for NER) and the rest for the
test.
We tokenized the sentences into words and
tagged the part-of-speech information using the
Japanese morphological analyzer ChaSen 1 2.3.3
and then labeled the NEs. Unreadable to-
kens such as parentheses were removed in to-
kenization. After tokenization, the text cor-
pus had 264,388 words of 60 part-of-speech
types. Since three different kinds of charac-
ters are used in Japanese, the character types
used as features included: single-kanji
(words written in a single Chinese charac-
ter), all-kanji (longer words written in Chi-
nese characters), hiragana (words written
in hiragana Japanese phonograms), katakana
(words written in katakana Japanese phono-
grams), number, single-capital (words
with a single capitalized letter), all-capital,
capitalized (only the first letter is capital-
ized), roman (other roman character words), and
others (all other words). We used all the fea-
tures that appeared in each training set (no feature
selection was performed). The chunking states in-
cluded in the NE classes were: BEGIN (beginning
of a NE), MIDDLE (middle of a NE), END (ending
of a NE), and SINGLE (a single-word NE). There
were 33 NE classes (eight categories * four chunk-
ing states + OTHER), and therefore we trained 33
SVMs to distinguish words of a class from words
of other classes. For NER, we used an SVM-based
chunk annotator YamCha 2 0.33 with a quadratic
kernel (1 + ~x ? ~y)2 and a soft margin parameter
of SVMs C=0.1 for training and applied sigmoid
function sn(x) with ?n=1.0 and Viterbi search to
the SVMs? outputs. These parameters were exper-
imentally chosen using the test set.
We used an ASR engine (Hori et al, 2004) with
a speaker-independent acoustic model. The lan-
1http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)
2http://www.chasen.org/?taku/software/yamcha/
620
guage model was a word 3-gram model, trained
using other Japanese newspaper articles (about
340 M words) that were also tokenized using
ChaSen. The vocabulary size of the word 3-gram
model was 426,023. The test-set perplexity over
the text corpus was 76.928. The number of out-
of-vocabulary words was 1,551 (0.587%). 223
(1.23%) NEs in the text corpus contained such out-
of-vocabulary words, so those NEs could not be
correctly recognized by ASR. The scaling param-
eter ? was set to 0.01, which showed the best ASR
error estimation results using word posterior prob-
abilities in the test set in terms of receiver operator
characteristic (ROC) curves. The language model
weight ? was set to 15, which is a commonly used
value in our ASR system. The word accuracy ob-
tained using our ASR engine for the overall dataset
was 79.45%. In the ASR results, 82.00% of the
NEs in the text corpus remained. Figure 2 shows
the ROC curves of ASR error estimation for the
overall five cross-validation test sets, using SVM-
based ASR confidence scoring and word posterior
probabilities as ASR confidence scores, where
True positive rate
= # correctly recognized words estimated as correct
# correctly recognized words
False positive rate
= # misrecognized words estimated as correct
# misrecognized words .
In SVM-based ASR confidence scoring, we used
the quadratic kernel and C=0.01. Parameter ?w of
sigmoid function sw(x) was set to 1.0. These pa-
rameters were also experimentally chosen. SVM-
based ASR confidence scoring showed better per-
formance in ASR error estimation than simple
word posterior probabilities by integrating mul-
tiple features. Five values of ASR confidence
threshold tw were tested in the following experi-
ments: 0.2, 0.3, 0.4, 0.5, and 0.6 (shown by black
dots in Figure 2).
4.2 Evaluation metrics
Evaluation was based on an averaged NER F-
measure, which is the harmonic mean of NER pre-
cision and recall:
NER precision = # correctly recognized NEs
# recognized NEs
NER recall = # correctly recognized NEs
# NEs in original text
.
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Tr
ue
 p
os
itv
e 
ra
te
 (%
)
False positive rate (%)
=0.3
=0.4
SVM-based
confidence
scoring
Word posterior probability
tw
t
t
t
w
=0.2tw
=0.6w
=0.5w
Figure 2: SVM-based confidence scoring outper-
forms word posterior probability for ASR error es-
timation.
A recognized NE was accepted as correct if and
only if it appeared in the same position as its refer-
ence NE through alignment, in addition to having
the correct NE surface and category, because the
same NEs might appear more than once. Compar-
isons of NE surfaces did not include differences
in word segmentation because of the segmentation
ambiguity in Japanese. Note that NER recall with
ASR results could not exceed the rate of the re-
maining NEs after ASR (about 82%) because NEs
containing ASR errors were always lost.
In addition, we also evaluated the NER perfor-
mance in NER precision and recall with NER-
level rejection using the procedure in Section 3.4,
by modifying the non-NE class scores using offset
value to.
4.3 Compared methods
We compared several combinations of features
and training conditions for evaluating the effect of
incorporating the ASR confidence feature and in-
vestigating differences among training data: text-
based, ASR-based, and both.
Baseline does not use the ASR confidence fea-
ture and is trained using text-based training data
only.
NoConf-A does not use the ASR confidence
feature and is trained using ASR-based training
data only.
621
Method Confidence Training Test F-measure (%) Precision (%) Recall (%)
Baseline Text ASR 67.00 70.67 63.70
NoConf-A Not used ASR ASR 65.52 78.86 56.05
NoConf-TA Text+ASR ASR 66.95 77.55 58.91
Conf-A ASR ASR? 67.69 76.69 60.59
Proposed Used Text+ASR ASR? 69.02 78.13 61.81
Conf-Reject Used? Text+ASR ASR? 68.77 77.57 61.78
Conf-UB Used Text+ASR ASR?? 73.14 87.51 62.83
Transcription Not used Text Text 84.04 86.27 81.93
Table 4: NER results in averaged NER F-measure, precision, and recall without considering NER-level
rejection (to = 0). ASR word accuracy was 79.45%, and 82.00% of NEs remained in ASR results.
(?Unconfident words were rejected and replaced by word error symbols, ?tw = 0.4, ??ASR errors were
known.)
NoConf-TA does not use the ASR confidence
feature and is trained using both text-based and
ASR-based training data.
Conf-A uses the ASR confidence feature and is
trained using ASR-based training data only.
Proposed uses the ASR confidence feature and
is trained using both text-based and ASR-based
training data.
Conf-Reject is almost the same as Proposed,
but misrecognized words are rejected and replaced
with word error symbols, as described at the end
of Section 3.2.
The following two methods are for reference.
Conf-UB assumes perfect ASR confidence scor-
ing, so the ASR errors in the test set are known.
The NER model, which is identical to Proposed,
is regarded as the upper-boundary of Proposed.
Transcription applies the same model as Base-
line to reference transcriptions, assuming word ac-
curacy is 100%.
4.4 NER Results
In the NER experiments, Proposed achieved the
best results among the above methods. Table
4 shows the NER results obtained by the meth-
ods without considering NER-level rejection (i.e.,
to = 0), using threshold tw = 0.4 for Conf-A,
Proposed, and Conf-Reject, which resulted in the
best NER F-measures (see Table 5). Proposed
showed the best F-measure, 69.02%. It outper-
formed Baseline by 2.0%, with a 7.5% improve-
ment in precision, instead of a recall decrease of
1.9%. Conf-Reject showed slightly worse results
Method tw F (%) P (%) R (%)
0.2 66.72 71.28 62.71
0.3 67.32 73.68 61.98
Conf-A 0.4 67.69 76.69 60.59
0.5 67.04 79.64 57.89
0.6 64.48 81.90 53.14
0.2 68.08 72.54 64.14
0.3 68.70 75.11 63.31
Proposed 0.4 69.02 78.13 61.81
0.5 68.17 80.88 58.93
0.6 65.39 83.00 53.96
0.2 68.06 72.49 64.14
0.3 68.61 74.88 63.31
Conf-Reject 0.4 68.77 77.57 61.78
0.5 67.93 80.23 58.91
0.6 64.93 82.05 53.73
Table 5: NER results with varying ASR confi-
dence score threshold tw for Conf-A, Proposed,
and Conf-Reject. (F: F-measure, P: precision, R:
recall)
than Proposed. Conf-A resulted in 1.3% worse F-
measure than Proposed. NoConf-A and NoConf-
TA achieved 7-8% higher precision than Base-
line; however, their F-measure results were worse
than Baseline because of the large drop of recall.
The upper-bound results of the proposed method
(Conf-UB) in F-measure was 73.14%, which was
4% higher than Proposed.
Figure 3 shows NER precision and recall with
NER-level rejection by to for Baseline, NoConf-
TA, Proposed, Conf-UB, and Transcription. In the
figure, black dots represent results with to = 0,
as shown in Table 4. By all five methods, we
622
 0
 20
 40
 60
 80
 100
 50  60  70  80  90  100
Reca
ll (%)
Precision (%)
Baseline
NoConf-TA
ProposedConf-UB
Transcription
Figure 3: NER precision and recall with NER-
level rejection by to
obtained higher precision with to > 0. Pro-
posed achieved more than 5% higher precision
than Baseline on most recall ranges and showed
higher precision than NoConf-TA on recall ranges
higher than about 35%.
5 Discussion
The proposed method effectively improves NER
performance, as shown by the difference between
Proposed and Baseline in Tables 4 and 5. Improve-
ment comes from two factors: using both text-
based and ASR-based training data and incorpo-
rating ASR confidence feature. As shown by the
difference between Baseline and the methods us-
ing ASR-based training data (NoConf-A, NoConf-
TA, Conf-A, Proposed, Conf-Reject), ASR-based
training data increases precision and decreases
recall. In ASR-based training data, all words
constituting NEs that contain ASR errors are re-
garded as non-NE words, and those NE exam-
ples are lost in training, which emphasizes NER
precision. When text-based training data are also
available, they compensate for the loss of NE
examples and recover NER recall, as shown by
the difference between the methods without text-
based training data (NoConf-A, Conf-A) and those
with (NoConf-TA, Proposed). The ASR confi-
dence feature also increases NER recall, as shown
by the difference between the methods without
it (NoConf-A, NoConf-TA) and with it (Conf-A,
Proposed). This suggests that the ASR confidence
feature helps distinguish whether ASR error influ-
ences NER and suppresses excessive rejection of
NEs around ASR errors.
With respect to the ASR confidence feature, the
small difference between Conf-Reject and Pro-
posed suggests that ASR confidence is a more
dominant feature in misrecognized words than the
other features: the word itself, its part-of-speech
tag, and its character type. In addition, the dif-
ference between Conf-UB and Proposed indicated
that there is room to improve NER performance
with better ASR confidence scoring.
NER-level rejection also increased precision, as
shown in Figure 3. We can control the trade-
off between precision and recall with to accord-
ing to the task requirements, even in text-based
NER. In the NER of speech data, we can ob-
tain much higher precision using both ASR-based
training data and NER-level rejection than using
either one.
6 Related work
Recent studies on the NER of speech data consider
more than 1-best ASR results in the form of N-best
lists and word lattices. Using many ASR hypothe-
ses helps recover the ASR errors of NE words in
1-best ASR results and improves NER accuracy.
Our method can be extended to multiple ASR hy-
potheses.
Generative NER models were used for multi-
pass ASR and NER searches using word lattices
(Horlock and King, 2003b; Be?chet et al, 2004;
Favre et al, 2005). Horlock and King (2003a)
also proposed discriminative training of their NER
models. These studies showed the advantage of
using multiple ASR hypotheses, but they do not
use overlapping features.
Discriminative NER models were also applied
to multiple ASR hypotheses. Zhai et al (2004) ap-
plied text-based NER to N-best ASR results, and
merged the N-best NER results by weighted vot-
ing based on several sentence-level results such as
ASR and NER scores. Using the ASR confidence
feature does not depend on SVMs and can be used
with their method and other discriminative mod-
els.
7 Conclusion
We proposed a method for NER of speech data
that incorporates ASR confidence as a feature
of discriminative NER, where the NER model
623
is trained using both text-based and ASR-based
training data. In experiments using SVMs,
the proposed method showed a higher NER F-
measure, especially in terms of improving pre-
cision, than simply applying text-based NER to
ASR results. The method effectively rejected erro-
neous NEs due to ASR errors with a small drop of
recall, thanks to both the ASR confidence feature
and ASR-based training data. NER-level rejection
also effectively increased precision.
Our approach can also be used in other tasks
in spoken language processing, and we expect it
to be effective. Since confidence itself is not lim-
ited to speech, our approach can also be applied to
other noisy inputs, such as optical character recog-
nition (OCR). For further improvement, we will
consider N-best ASR results or word lattices as in-
puts and introduce more speech-specific features
such as word durations and prosodic features.
Acknowledgments We would like to thank
anonymous reviewers for their helpful comments.
References
Fre?de?ric Be?chet, Allen L. Gorin, Jeremy H. Wright,
and Dilek Hakkani-Tu?r. 2004. Detecting and ex-
tracting named entities from spontaneous speech in a
mixed-initiative spoken dialogue context: How May
I Help You? Speech Communication, 42(2):207?
225.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proc. CoNLL, pages 160?163.
Beno??t Favre, Fre?de?ric Be?chet, and Pascal Noce?ra.
2005. Robust named entity extraction from large
spoken archives. In Proc. HLT-EMNLP, pages 491?
498.
Takaaki Hori, Chiori Hori, and Yasuhiro Minami.
2004. Fast on-the-fly composition for weighted
finite-state transducers in 1.8 million-word vocab-
ulary continuous-speech recognition. In Proc. IC-
SLP, volume 1, pages 289?292.
James Horlock and Simon King. 2003a. Discrimi-
native methods for improving named entity extrac-
tion on speech data. In Proc. EUROSPEECH, pages
2765?2768.
James Horlock and Simon King. 2003b. Named en-
tity extraction from word lattices. In Proc. EU-
ROSPEECH, pages 1265?1268.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING, pages 390?396.
Simo O. Kamppari and Timothy J. Hazen. 2000. Word
and phone level acoustic confidence scoring. In
Proc. ICASSP, volume 3, pages 1799?1802.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. CoNLL, pages 188?191.
David Miller, Richard Schwartz, Ralph Weischedel,
and Rebecca Stone. 1999. Named entity extraction
from broadcast news. In Proceedings of the DARPA
Broadcast News Workshop, pages 37?40.
David D. Palmer and Mari Ostendorf. 2001. Im-
proving information extraction by modeling errors
in speech recognizer output. In Proc. HLT, pages
156?160.
Thomas Schaaf and Thomas Kemp. 1997. Confidence
measures for spontaneous speech recognition. In
Proc. ICASSP, volume II, pages 875?878.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation - analysis of re-
sults. In Proc. COLING, pages 25?30.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in Japanese texts. In Proc. the
Sixth Workshop on Very Large Corpora, pages 171?
178.
Frank Wessel, Ralf Schlu?ter, Klaus Macherey, and
Hermann Ney. 2001. Confidence measures for
large vocabulary continuous speech recognition.
IEEE Transactions on Speech and Audio Process-
ing, 9(3):288?298.
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine
Carpuat, and Dekai Wu. 2004. Using N-best lists
for named entity recognition from chinese speech.
In Proc. HLT-NAACL, pages 37?40.
Rong Zhang and Alexander I. Rudnicky. 2001. Word
level confidence annotation using combinations of
features. In Proc. EUROSPEECH, pages 2105?
2108.
624
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777?784,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Left-to-Right Target Generation for Hierarchical Phrase-based
Translation
Taro Watanabe Hajime Tsukada Hideki Isozaki
2-4, Hikaridai, Seika-cho, Soraku-gun,
Kyoto, JAPAN 619-0237
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
We present a hierarchical phrase-based
statistical machine translation in which a
target sentence is efficiently generated in
left-to-right order. The model is a class
of synchronous-CFG with a Greibach Nor-
mal Form-like structure for the projected
production rule: The paired target-side
of a production rule takes a phrase pre-
fixed form. The decoder for the target-
normalized form is based on an Early-
style top down parser on the source side.
The target-normalized form coupled with
our top down parser implies a left-to-
right generation of translations which en-
ables us a straightforward integration with
ngram language models. Our model was
experimented on a Japanese-to-English
newswire translation task, and showed sta-
tistically significant performance improve-
ments against a phrase-based translation
system.
1 Introduction
In a classical statistical machine translation, a for-
eign language sentence f J1 = f1, f2, ... fJ is trans-
lated into another language, i.e. English, eI1 =
e1, e2, ..., eI by seeking a maximum likely solution
of:
e?I1 = argmax
eI1
Pr(eI1| f J1 ) (1)
= argmax
eI1
Pr( f J1 |eI1)Pr(eI1) (2)
The source channel approach in Equation 2 inde-
pendently decomposes translation knowledge into
a translation model and a language model, respec-
tively (Brown et al, 1993). The former repre-
sents the correspondence between two languages
and the latter contributes to the fluency of English.
In the state of the art statistical machine transla-
tion, the posterior probability Pr(eI1| f J1 ) is directly
maximized using a log-linear combination of fea-
ture functions (Och and Ney, 2002):
e?I1 = argmax
eI1
exp
(
?M
m=1 ?mhm(eI1, f J1 )
)
?
e? I
?
1
exp
(
?M
m=1 ?mhm(e? I
?
1 , f J1 )
) (3)
where hm(eI1, f J1 ) is a feature function, such as
a ngram language model or a translation model.
When decoding, the denominator is dropped since
it depends only on f J1 . Feature function scaling
factors ?m are optimized based on a maximum
likely approach (Och and Ney, 2002) or on a direct
error minimization approach (Och, 2003). This
modeling allows the integration of various fea-
ture functions depending on the scenario of how
a translation is constituted.
A phrase-based translation model is one of the
modern approaches which exploits a phrase, a
contiguous sequence of words, as a unit of transla-
tion (Koehn et al, 2003; Zens and Ney, 2003; Till-
man, 2004). The idea is based on a word-based
source channel modeling of Brown et al (1993):
It assumes that eI1 is segmented into a sequence
of K phrases e?K1 . Each phrase e?k is transformed
into ?fk. The translated phrases are reordered to
form f J1 . One of the benefits of the modeling is
that the phrase translation unit preserves localized
word reordering. However, it cannot hypothesize
a long-distance reordering required for linguisti-
cally divergent language pairs. For instance, when
translating Japanese to English, a Japanese SOV
structure has to be reordered to match with an En-
777
glish SVO structure. Such a sentence-wise move-
ment cannot be realized within the phrase-based
modeling.
Chiang (2005) introduced a hierarchical phrase-
based translation model that combined the
strength of the phrase-based approach and a
synchronous-CFG formalism (Aho and Ullman,
1969): A rewrite system initiated from a start
symbol which synchronously rewrites paired non-
terminals. Their translation model is a binarized
synchronous-CFG, or a rank-2 of synchronous-
CFG, in which the right-hand side of a production
rule contains at most two non-terminals. The form
can be regarded as a phrase translation pair with
at most two holes instantiated with other phrases.
The hierarchically combined phrases provide a
sort of reordering constraints that is not directly
modeled by a phrase-based model.
Rules are induced from a bilingual corpus with-
out linguistic clues first by extracting phrase trans-
lation pairs, and then by generalizing extracted
phrases with holes (Chiang, 2005). Even in a
phrase-based model, the number of phrases ex-
tracted from a bilingual corpus is quadratic to
the length of bilingual sentences. The grammar
size for the hierarchical phrase-based model will
be further exploded, since there exists numerous
combination of inserting holes to each rule. The
spuriously increasing grammar size will be prob-
lematic for decoding without certain heuristics,
such as a length based thresholding.
The integration with a ngram language model
further increases the cost of decoding especially
when incorporating a higher order ngram, such as
5-gram. In the hierarchical phrase-based model
(Chiang, 2005), and an inversion transduction
grammar (ITG) (Wu, 1997), the problem is re-
solved by restricting to a binarized form where at
most two non-terminals are allowed in the right-
hand side. However, Huang et al (2005) reported
that the computational complexity for decoding
amounted to O(J3+3(n?1)) with n-gram even using
a hook technique. The complexity lies in mem-
orizing the ngram?s context for each constituent.
The order of ngram would be a dominant factor
for higher order ngrams.
As an alternative to a binarized form, we
present a target-normalized hierarchical phrase-
based translation model. The model is a class of a
hierarchical phrase-based model, but constrained
so that the English part of the right-hand side
is restricted to a Greibach Normal Form (GNF)-
like structure: A contiguous sequence of termi-
nals, or a phrase, is followed by a string of non-
terminals. The target-normalized form reduces the
number of rules extracted from a bilingual corpus,
but still preserves the strength of the phrase-based
approach. An integration with ngram language
model is straightforward, since the model gener-
ates a translation in left-to-right order. Our de-
coder is based on an Earley-style top down pars-
ing on the foreign language side. The projected
English-side is generated in left-to-right order syn-
chronized with the derivation of the foreign lan-
guage side. The decoder?s implementation is taken
after a decoder for an existing phrase-based model
with a simple modification to account for produc-
tion rules. Experimental results on a Japanese-to-
English newswire translation task showed signif-
icant improvement against a phrase-based model-
ing.
2 Translation Model
A weighted synchronous-CFG is a rewrite system
consisting of production rules whose right-hand
side is paired (Aho and Ullman, 1969):
X ? ??, ?,?? (4)
where X is a non-terminal, ? and ? are strings of
terminals and non-terminals. For notational sim-
plicity, we assume that ? and ? correspond to the
foreign language side and the English side, re-
spectively. ? is a one-to-one correspondence for
the non-terminals appeared in ? and ?. Starting
from an initial non-terminal, each rule rewrites
non-terminals in ? and ? that are associated with
?.
Chiang (2005) proposed a hierarchical phrase-
based translation model, a binary synchronous-
CFG, which restricted the form of production rules
as follows:
? Only two types of non-terminals allowed: S
and X.
? Both of the strings ? and ? must contain at
least one terminal item.
? Rules may have at most two non-terminals
but non-terminals cannot be adjacent for the
foreign language side ?.
The production rules are induced from a bilingual
corpus with the help of word alignments. To al-
leviate a data sparseness problem, glue rules are
778
added that prefer combining hierarchical phrases
in a serial manner:
S ?
?
S 1 X2 , S 1 X2
?
(5)
S ?
?
X 1 , X1
?
(6)
where boxed indices indicate non-terminal?s link-
ages represented in ?.
Our model is based on Chiang (2005)?s frame-
work, but further restricts the form of production
rules so that the aligned right-hand side ? follows
a GNF-like structure:
X ?
?
?, ?b?,?
?
(7)
where ?b is a string of terminals, or a phrase,
and beta is a (possibly empty) string of non-
terminals. The foreign language at right-hand side
? still takes an arbitrary string of terminals and
non-terminals. The use of a phrase ?b as a pre-
fix keeps the strength of the phrase-base frame-
work. A contiguous English side coupled with
a (possibly) discontiguous foreign language side
preserves a phrase-bounded local word reordering.
At the same time, the target-normalized frame-
work still combines phrases hierarchically in a re-
stricted manner.
The target-normalized form can be regarded as
a type of rule in which certain non-terminals are
always instantiated with phrase translation pairs.
Thus, we will be able to reduce the number of rules
induced from a bilingual corpus, which, in turn,
help reducing the decoding complexity.
The contiguous phrase-prefixed form generates
English in left-to-right order. Therefore, a decoder
can easily hypothesize a derivation tree integrated
with a ngram language model even with higher or-
der.
Note that we do not imply arbitrary
synchronous-CFGs are transformed into the
target normalized form. The form simply restricts
the grammar extracted from a bilingual corpus
explained in the next section.
2.1 Rule Extraction
We present an algorithm to extract production
rules from a bilingual corpus. The procedure is
based on those for the hierarchical phrase-based
translation model (Chiang, 2005).
First, a bilingual corpus is annotated with word
alignments using the method of Koehn et al
(2003). Many-to-many word alignments are in-
duced by running a one-to-many word alignment
model, such as GIZA++ (Och and Ney, 2003), in
both directions and by combining the results based
on a heuristic (Koehn et al, 2003).
Second, phrase translation pairs are extracted
from the word alignment corpus (Koehn et al,
2003). The method exhaustively extracts phrase
pairs ( f j+mj , ei+ni ) from a sentence pair ( f J1 , eI1) that
do not violate the word alignment constraints a:
?(i?, j?) ? a : j? ? [ j, j + m], i? ? [i, i + n]
?(i?, j?) ? a : j? ? [ j, j + m], i? < [i, i + n]
?(i?, j?) ? a : j? < [ j, j + m], i? ? [i, i + n]
Third, based on the extracted phrases, production
rules are accumulated by computing the ?holes?
for contiguous phrases (Chiang, 2005):
1. A phrase pair ( ?f , e?) constitutes a rule
X ?
?
?f , e?
?
2. A rule X ? ??, ?? and a phrase pair ( ?f , e?) s.t.
? = ?? ?f??? and ? = e??e?? constitutes a rule
X ?
?
?? X k ?
??, e?? X k ?
?
Following Chiang (2005), we applied constraints
when inducing rules with non-terminals:
? At least one foreign word must be aligned to
an English word.
? Adjacent non-terminals are not allowed for
the foreign language side.
2.2 Phrase-based Rules
The rule extraction procedure described in Section
2.1 is a corpus-based, therefore will be easily suf-
fered from a data sparseness problem. The hier-
archical phrase-based model avoided this problem
by introducing the glue rules 5 and 6 that com-
bined hierarchical phrases sequentially (Chiang,
2005).
We use a different method of generalizing pro-
duction rules. When production rules without non-
terminals are extracted in step 1 of Section 2.1,
X ?
?
?f , e?
?
(8)
then, we also add production rules as follows:
X ?
?
?f X 1 , e? X 1
?
(9)
X ?
?
X 1 ?f , e? X 1
?
(10)
X ?
?
X 1 ?f X 2 , e? X 1 X 2
?
(11)
X ?
?
X 2 ?f X 1 , e? X 1 X 2
?
(12)
779
The international terrorism also is a possible threat in Japan
Reference translation: ?International terrorism is a threat
even to Japan?
(a) Translation by a phrase-based model. (b) A derivation tree representation for Figure 1(a).Indices in
non-terminal X represent the order to perform rewriting.
Figure 1: An example of Japanese-to-English translation by a phrase-based model.
We call them phrase-based rules, since four types
of rules are generalized directly from phrase trans-
lation pairs.
The class of rules roughly corresponds to the re-
ordering constraints used in a phrase-based model
during decoding. Rules 8 and 9 are sufficient to re-
alize a monotone decoding in which phrase trans-
lation pairs are simply combined sequentially.
With rules 10 and 11, the non-terminal X 1 behaves
as a place holder where certain number of foreign
words are skipped. Therefore, those rules real-
ize a window size constraint used in many phrase-
based models (Koehn et al, 2003). The rule 12
further gives an extra freedom for the phrase pair
reordering. The rules 8 through 12 can be in-
terpreted as ITG-constraints where phrase trans-
lation pairs are hierarchically combined either in
a monotonic way or in an inverted manner (Zens
and Ney, 2003; Wu, 1997). Thus, by controlling
what types of phrase-based rules employed in a
grammar, we will be able to simulate a phrase-
based translation model with various constraints.
This reduction is rather natural in that a finite state
transducer, or a phrase-based model, is a subclass
of a synchronous-CFG.
Figure 1(a) shows an example Japanese-to-
English translation by a phrase-based model de-
scribed in Section 5. Using the phrase-based rules,
the translation results is represented as a derivation
tree in Figure 1(b).
3 Decoding
Our decoder is an Earley-style top down parser on
the foreign language side with a beam search strat-
egy. Given an input sentence f J1 , the decoder seeks
for the best English according to Equation 3 us-
ing the feature functions described in Section 4.
The English output sentence is generated in left-
to-right order in accordance with the derivation of
the foreign language side synchronized with the
cardinality of already translated foreign word po-
sitions.
The decoding process is very similar to those
described in (Koehn et al, 2003): It starts from an
initial empty hypothesis. From an existing hypoth-
esis, new hypothesis is generated by consuming
a production rule that covers untranslated foreign
word positions. The score for the newly generated
hypothesis is updated by combining the scores of
feature functions described in Section 4. The En-
glish side of the rule is simply concatenated to
form a new prefix of English sentence. Hypothe-
ses that consumed m foreign words are stored in a
priority queue Qm.
Hypotheses in Qm undergo two types of prun-
ing: A histogram pruning preserves at most M hy-
potheses inQm. A threshold pruning discards a hy-
potheses whose score is below the maximum score
of Qm multiplied with a threshold value ?. Rules
are constrained by their foreign word span of a
non-terminal. For a rule consisting of more than
two non-terminals, we constrained so that at least
one non-terminal should span at most ? words.
The decoder is characterized as a weighted
synchronous-CFG implemented with a push-down
automaton rather a weighted finite state transducer
(Aho and Ullman, 1969). Each hypothesis main-
tains following knowledge:
? A prefix of English sentence. For space ef-
ficiency, the prefix is represented as a word
graph.
? Partial contexts for each feature function.
For instance, to compute a 5-gram language
model feature, we keep the consecutive last
four words of an English prefix.
780
? A stack that keeps track of the uncovered for-
eign word spans. The stack for an initial hy-
pothesis is initialized with span [1, J].
When extending a hypothesis, the associated stack
structure is popped. The popped foreign word
span [ jl, jr] is used to locate the rules for uncov-
ered foreign word positions. We assume that the
decoder accumulates all the applicable rules from
a large database and stores the extracted rules in a
chart structure. The decoder identifies what rules
to consume when extending a hypothesis using the
chart structure. A new hypothesis is created with
an updated stack by pushing foreign non-terminal
spans: For each rule spanning [ jl, jr] at foreign-
side with non-terminal spans of [kl1, kr1], [kl2, kr2], ...,
the non-terminal spans are pushed in the reverse
order of the projected English side. For example,
A rule with foreign word non-terminal spans:
X ?
?
X 2 : [kl2, kr2] ?f X 1 : [kl1, kr1], e? X 1 X 2
?
will update a stack by pushing the foreign word
spans [kl2, kr2] and [kl1, kr1] in order. This ordering
assures that, when popped, the English-side will
be generated in left-to-right order. A hypothesis
with an empty stack implies that the hypothesis
has covered all the foreign words.
Figure 2 illustrates the decoding process for the
derivation tree in Figure 1(b). Starting from the
initial hypothesis of [1, 11], the stack is updated in
accordance with non-terminal?s spans. The span
is popped and the rule with the foreign word pan
[1, 11] is looked up from the chart structure. The
stack structure for the newly created hypothesis is
updated by pushing non-terminal spans [4, 11] and
[1, 2].
Our decoder is based on an in-house devel-
oped phrase-based decoder which uses a bit vec-
tor to represent uncovered foreign word positions
for each hypothesis. We basically replaced the
bit vector structure to the stack structure: Al-
most no modification was required for the word
graph structure and the beam search strategy im-
plemented for a phrase-based modeling. The use
of a stack structure directly models a synchronous-
CFG formalism realized as a push-down automa-
tion, while the bit vector implementation is con-
ceptualized as a finite state transducer. The cost
of decoding with the proposed model is cubic to
foreign language sentence length.
Rules Stack
[1, 11]
X : [1, 11]?
?
X 1 : [1, 2] X 2 : [4, 11], The X 1 X 2
? [1, 2]
[4, 11]
X : [1, 2]?
?
X 1 : [2, 2], international X 1
? [2, 2]
[4, 11]
X : [2, 2]? ? , terrorism? [4, 11]
X : [4, 11]?
?
X 2 : [4, 5] X 1 : [7, 11], also X 1 X 2
? [7, 11]
[4, 5]
X : [7, 11]?
?
X 1 : [7, 9] , is a X 1
? [7, 9]
[4, 5]
X : [7, 9]?
?
X 1 : [9, 9], possible X 1
? [9, 9]
[4, 5]
X : [9, 9]? ? , threat? [4, 5]
X : [4, 5]?
?
X 1 : [4, 4] , in X 1
?
[4, 4]
X : [4, 4]? ? , Japan?
Figure 2: An example decoding process of Fig-
ure 1(b) with a stack to keep track of foreign word
spans.
4 Feature Functions
The decoder for our translation model uses a log-
linear combination of feature functions, or sub-
models, to seek for the maximum likely translation
according to Equation 3. This section describes
the models experimented in Section 5, mainly
consisting of count-based models, lexicon-based
models, a language model, reordering models and
length-based models.
4.1 Count-based Models
Main feature functions h?( f J1 |eI1,D) and
h?(eI1| f J1 ,D) estimate the likelihood of two
sentences f J1 and eI1 over a derivation tree D.
We assume that the production rules in D are
independent of each other:
h?( f J1 |eI1,D) = log
?
??,???D
?(?|?) (13)
?(?|?) is estimated through the relative frequency
on a given bilingual corpus.
?(?|?) = count(?, ?)?
? count(?, ?)
(14)
where count(?) represents the cooccurrence fre-
quency of rules ? and ?.
The relative count-based probabilities for the
phrase-based rules are simply adopted from the
original probabilities of phrase translation pairs.
4.2 Lexicon-based Models
We define lexically weighted feature functions
hw( f J1 |eI1,D) and hw(eI1| f J1 ,D) applying the inde-
pendence assumption of production rules as in
781
Equation 13.
hw( f J1 |eI1,D) = log
?
??,???D
pw(?|?) (15)
The lexical weight pw(?|?) is computed from word
alignments a inside ? and ? (Koehn et al, 2003):
pw(?|?, a) =
|?|
?
i=1
1
|{ j|(i, j) ? a}|
?
?(i, j)?a
t(? j|?i)
(16)
where t(?) is a lexicon model trained from the word
alignment annotated bilingual corpus discussed in
Section 2.1. The alignment a also includes non-
terminal correspondence with t(X k |X k ) = 1. If we
observed multiple alignment instances for ? and ?,
then, we take the maximum of the weights.
pw(?|?) = max
a
pw(?|?, a) (17)
4.3 Language Model
We used mixed-cased n-gram language model. In
case of 5-gram language model, the feature func-
tion is expressed as follows:
hlm(eI1) = log
?
i
pn(ei|ei?4ei?3ei?2ei?1) (18)
4.4 Reordering Models
In order to limit the reorderings, two feature func-
tions are employed based on the backtracking of
rules during the top-down parsing on foreign lan-
guage side.
hh(eI1, f J1 ,D) =
?
Di?back(D)
height(Di) (19)
hw(eI1, f J1 ,D) =
?
Di?back(D)
width(Di) (20)
where back(D) is a set of subtrees backtracked
during the derivation of D, and height(Di) and
width(Di) refer the height and width of subtreeDi,
respectively. In Figure 1(b), for instance, a rule of
X 1 with non-terminals X 2 and X 4 , two rules X 2
and X 3 spanning two terminal symbols should be
backtracked to proceed to X 4 . The rationale is that
positive scaling factors prefer a deeper structure
whereby negative scaling factors prefer a mono-
tonized structure.
4.5 Length-based Models
Three trivial length-based feature functions were
used in our experiment.
hl(eI1) = I (21)
hr(D) = rule(D) (22)
hp(D) = phrase(D) (23)
Table 1: Japanese/English news corpus
Japanese English
train sentence 175,384
dictionary + 1,329,519
words 8,373,478 7,222,726
vocabulary 297,646 397,592
dev. sentence 1,500
words 47,081 39,117
OOV 45 149
test sentence 1,500
words 47,033 38,707
OOV 51 127
Table 2: Phrases/rules extracted from the
Japanese/English bilingual corpus. Figures do not
include phrase-based rules.
# rules/phrases
Phrase 5,433,091
Normalized-2 6,225,630
Normalized-3 6,233,294
Hierarchical 12,824,387
where rule(D) and phrase(D) are the number
of production rules extracted in Section 2.1 and
phrase-based rules generalized in Section 2.2, re-
spectively. The English length feature function
controls the length of output sentence. Two feature
functions based on rule?s counts are hypothesized
to control whether to incorporate a production rule
or a phrase-based rule into D.
5 Experiments
The bilingual corpus used for our experiments was
obtained from an automatically sentence aligned
Japanese/English Yomiuri newspaper corpus con-
sisting of 180K sentence pairs (refer to Table
1) (Utiyama and Isahara, 2003). From one-to-
one aligned sentences, 1,500 sentence pairs were
sampled for a development set and a test set1.
Since the bilingual corpus is rather small, es-
pecially for the newspaper translation domain,
Japanese/English dictionaries consisting of 1.3M
entries were added into a training set to alleviate
an OOV problem2.
Word alignments were annotated by a HMM
translation model (Och and Ney, 2003). After
1Japanese sentences were segmented by MeCab available
from http://mecab.sourceforge.jp.
2The dictionary entries were compiled from JE-
DICT/JNAMEDICT and an in-house developed dictionary.
782
the annotation via Viterbi alignments with refine-
ments, phrases translation pairs and production
rules were extracted (refer to Table 2). We per-
formed the rule extraction using the hierarchi-
cal phrase-based constraint (Hierarchical) and our
proposed target-normalized form with 2 and 3
non-terminals (Normalized-2 and Normalized-3).
Phrase translation pairs were also extracted for
comparison (Phrase). We did not threshold the
extracted phrases or rules by their length. Ta-
ble 2 shows that Normalized-2 extracted slightly
larger number of rules than those for phrase-
based model. Including three non-terminals did
not increase the grammar size. The hierarchical
phrase-based translation model extracts twice as
large as our target-normalized formalism. The
target-normalized form is restrictive in that non-
terminals should be consecutive for the English-
side. This property prohibits spuriously extracted
production rules.
Mixed-casing 3-gram/5-gram language models
were estimated from LDC English GigaWord 2 to-
gether with the 100K English articles of Yomiuri
newspaper that were used neither for development
nor test sets 3.
We run the decoder for the target-normalized
hierarchical phrase-based model consisting of at
most two non-terminals, since adding rules with
three non-terminals did not increase the grammar
size. ITG-constraint simulated phrase-based rules
were also included into our grammar. The foreign
word span size was thresholded so that at least one
non-terminal should span at most 7 words.
Our phrase-based model employed all feature
functions for the hierarchical phrase-based system
with additional feature functions:
? A distortion model that penalizes the re-
ordering of phrases by the number of words
skipped | j ? ( j? + m?) ? 1|, where j is the for-
eign word position for a phrase f j+mj trans-
lated immediately after a phrase for f j?+m?j?
(Koehn et al, 2003).
? Lexicalized reordering models constrain the
reordering of phrases whether to favor mono-
tone, swap or discontinuous positions (Till-
man, 2004).
The phrase-based decoder?s reordering was con-
strained by ITG-constraints with a window size of
3We used SRI ngram language modeling toolkit with lim-
ited vocabulary size.
Table 3: Results for the Japanese-to-English
newswire translation task.
BLEU NIST
[%]
Phrase 3-gram 7.14 3.21
5-gram 7.33 3.19
Normalized-2 3-gram 10.00 4.11
5-gram 10.26 4.20
7.
The translation results are summarized in Table
3. Two systems were contrasted by 3-gram and 5-
gram language models. Results were evaluated by
ngram precision based metrics, BLEU and NIST,
on the casing preserved single reference test set.
Feature function scaling factors for each system
were optimized on BLEU score under the devel-
opment set using a downhill simplex method. The
differences of translation qualities are statistically
significant at the 95% confidence level (Koehn,
2004). Although the figures presented in Table
3 are rather low, we found that Normalized-2 re-
sulted in statistically significant improvement over
Phrase. Figure 3 shows some translation results
from the test set.
6 Conclusion
The target-normalized hierarchical phrase-based
model is based on a more general hierarchical
phrase-based model (Chiang, 2005). The hier-
archically combined phrases can be regarded as
an instance of phrase-based model with a place
holder to constraint reordering. Such reorder-
ing was realized either by an additional constraint
for decoding, such as window constraints, IBM
constraints or ITG-constraints (Zens and Ney,
2003), or by lexicalized reordering feature func-
tions (Tillman, 2004). In the hierarchical phrase-
based model, such reordering is explicitly repre-
sented in each rule.
As experimented in Section 5, the use of the
target-normalized form reduced the grammar size,
but still outperformed a phrase-based system.
Furthermore, the target-normalized form coupled
with our top down parsing on the foreign lan-
guage side allows an easier integration with ngram
language model. A decoder can be implemented
based on a phrase-based model by employing a
stack structure to keep track of untranslated for-
eign word spans.
The target-normalized form can be interpreted
783
Reference: Japan needs to learn a lesson from history to ensure that it not repeat its mistakes .
Phrase: At the same time , it never mistakes that it is necessary to learn lessons from the history of criminal .
Normalized-2: It is necessary to learn lessons from history so as not to repeat similar mistakes in the future .
Reference: The ministries will dispatch design and construction experts to China to train local engineers and to
research technology that is appropriate to China?s economic situation .
Phrase: Japan sent specialists to train local technicians to the project , in addition to the situation in China and
its design methods by exception of study .
Normalized-2: Japan will send experts to study the situation in China , and train Chinese engineers , construction
design and construction methods of the recipient from .
Reference: The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending relief
measures to the village and the city of Niigata .
Phrase: The Health and Welfare Ministry in that the Japanese people in the village are made law .
Normalized-2: The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata .
Figure 3: Sample translations from two systems: Phrase and Normalized-2
as a set of rules that reorders the foreign lan-
guage to match with English language sequen-
tially. Collins et al (2005) presented a method
with hand-coded rules. Our method directly learns
such serialization rules from a bilingual corpus
without linguistic clues.
The translation quality presented in Section 5
are rather low due to the limited size of the bilin-
gual corpus, and also because of the linguistic dif-
ference of two languages. As our future work,
we are in the process of experimenting our model
for other languages with rich resources, such as
Chinese and Arabic, as well as similar language
pairs, such as French and English. Additional
feature functions will be also investigated that
were proved successful for phrase-based models
together with feature functions useful for a tree-
based modeling.
Acknowledgement
We would like to thank to our colleagues, espe-
cially to Hideto Kazawa and Jun Suzuki, for useful
discussions on the hierarchical phrase-based trans-
lation.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37?56.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL 2005, pages 263?270, Ann Arbor, Michigan,
June.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL 2005, pages 531?540,
Ann Arbor, Michigan, June.
Liang Huang, Hao Zhang, and Daniel Gildea. 2005.
Machine translation as lexicalized parsing with
hooks. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 65?73,
Vancouver, British Columbia, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395, Barcelona, Spain, July.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL 2002,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL
2003, pages 160?167.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
HLT-NAACL 2004: Short Papers, pages 101?104,
Boston, Massachusetts, USA, May 2 - May 7.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proc. of ACL 2003, pages
72?79.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377?403.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In Proc. of ACL 2003, pages 144?151.
784
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 117?120,
Prague, June 2007. c?2007 Association for Computational Linguistics
Learning to Rank Definitions to Generate Quizzes for Interactive
Information Presentation
Ryuichiro Higashinaka and Kohji Dohsaka and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
{rh,dohsaka,isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper proposes the idea of ranking def-
initions of a person (a set of biographi-
cal facts) to automatically generate ?Who
is this?? quizzes. The definitions are or-
dered according to how difficult they make
it to name the person. Such ranking would
enable users to interactively learn about a
person through dialogue with a system with
improved understanding and lasting motiva-
tion, which is useful for educational sys-
tems. In our approach, we train a ranker
that learns from data the appropriate ranking
of definitions based on features that encode
the importance of keywords in a definition
as well as its content. Experimental results
show that our approach is significantly better
in ranking definitions than baselines that use
conventional information retrieval measures
such as tf*idf and pointwisemutual informa-
tion (PMI).
1 Introduction
Appropriate ranking of sentences is important, as
noted in sentence ordering tasks (Lapata, 2003), in
effectively delivering content. Whether the task is
to convey news texts or definitions, the objective is
to make it easier for users to understand the content.
However, just conveying it in an encyclopedia-like
or temporal order may not be the best solution, con-
sidering that interaction between a system and a user
improves understanding (Sugiyama et al, 1999) and
that the cognitive load in receiving information is be-
lieved to correlate with memory fixation (Craik and
Lockhart, 1972).
In this paper, we discuss the idea of ranking defi-
nitions as a way to present people?s biographical in-
formation to users, and propose ranking definitions
to automatically generate a ?Who is this?? quiz.
Here, we use the term ?definitions of a person? to
mean a short series of biographical facts (See Fig. 1).
The definitions are ordered according to how diffi-
cult they make it to name the person. The ranking
also enables users to easily come up with answer
candidates. The definitions are presented to users
one by one as hints until users give the correct name
(See Fig. 2). Although the interaction would take
time, we could expect improved understanding of
people?s biographical information by users through
their deliberation and the long lasting motivation af-
forded by the entertaining nature of quizzes, which
is important in tutorial tasks (Baylor and Ryu, 2003).
Previous work on definition ranking has used
measures such as tf*idf (Xu et al, 2004) or ranking
models trained to encode the likelihood of a defini-
tion being good (Xu et al, 2005). However, such
measures/models may not be suitable for quiz-style
ranking. For example, a definition having a strong
co-occurrence with a person may not be an easy hint
when it is about a very minor detail. Certain de-
scriptions, such as a person?s birthplace, would have
to come early so that users can easily start guessing
who the person is. In our approach, we train a ranker
that learns from data the appropriate ranking of def-
initions. Note that we only focus on the ranking of
definitions and not on the interaction with users in
this paper. We also assume that the definitions to be
ranked are given.
Section 2 describes the task of ranking definitions,
and Section 3 describes our approach. Section 4 de-
scribes our collection of ranking data and the rank-
ing model training using the ranking support vector
machine (SVM), and Section 5 presents the evalu-
ation results. Section 6 summarizes and mentions
future work.
2 Ranking Definitions for Quizzes
Figure 1 shows a list of definitions of Natsume
Soseki, a famous Japanese novelist, in their original
ranking at the encyclopedic website goo (http://dic-
tionary.goo.ne.jp/) and in the quiz-style ranking we
aim to achieve. Such a ranking would realize a dia-
logue like that in Fig. 2. At the end of the dialogue,
the user would be able to associate the person and
the definitions better, and it is expected that some
new facts could be learned about that person.
117
Original Ranking:
1. Novelist and scholar of British literature.
2. Real name: Kinnosuke.
3. Born in Ushigome, Edo.
4. Graduated from the University of Tokyo.
5. Master of early-modern literature along with Mori Ogai.
6. After the success of ?I Am a Cat?, quit all teaching jobs and joined
Asahi Shimbun.
7. Published masterpieces in Asahi Shimbun.
8. Familiar with Haiku, Chinese poetry, and calligraphy.
9. Works include ?Botchan?, ?Sanshiro?, etc.
?
Quiz-style Ranking:
1. Graduated from the University of Tokyo.
2. Born in Ushigome, Edo.
3. Novelist and scholar of British literature.
4. Familiar with Haiku, Chinese poetry, and calligraphy.
5. Published masterpieces in Asahi Shimbun.
6. Real name: Kinnosuke.
7. Master of early-modern literature along with Mori Ogai.
8. After the success of ?I Am a Cat?, quit all teaching jobs and joined
Asahi Shimbun.
9. Works include ?Botchan?, ?Sanshiro?, etc.
Figure 1: List of definitions of Natsume Soseki, a
famous Japanese novelist, in their original ranking in
the encyclopedia and in the quiz-style ranking. The
definitions were translated by the authors.
Ranking definitions is closely related to defini-
tional question answering and sentence ordering
in multi-document summarization. In definitional
question answering, measures related to information
retrieval (IR), such as tf*idf or pointwise mutual in-
formation (PMI), have been used to rank sentences
or information nuggets (Xu et al, 2004; Sun et al,
2005). Such measures are used under the assump-
tion that outstanding/co-occurring keywords about a
definiendum characterize that definiendum. How-
ever, this assumptionmay not be appropriate in quiz-
style ranking; most content words in the definitions
are already important in the IR sense, and strong co-
occurrence may not guarantee high ranks for hints
to be presented later because the hint can be too spe-
cific. An approach to creating a ranking model of
definitions in a supervised manner using machine
learning techniques has been reported (Xu et al,
2005). However, the model is only used to distin-
guish definitions from non-definitions on the basis
of features related mainly to linguistic styles.
In multi-document summarization, the focus has
been mainly on creating cohesive texts. (Lapata,
2003) uses the probability of words in adjacent sen-
tences as constraints to maximize the coherence of
all sentence-pairs in texts. Although we acknowl-
edge that having cohesive definitions is important,
since we are not creating a single text and the dia-
logue that we aim to achieve would involve frequent
user/system interaction (Fig. 2), we do not deal with
the coherence of definitions in this paper.
 
S1 Who is this? First hint: Graduated from the
University of Tokyo.
U1 Yoshida Shigeru?
S2 No, not even close! Second hint: Born in
Ushigome, Edo.
U2 I don?t know.
S3 OK. Third hint: Novelist and scholar of
British literature.
U3 Murakami Haruki?
S4 Close! Fourth hint: Familiar with Haiku,
Chinese poetry, and calligraphy.
U4 Mori Ogai?
S5 Very close! Fifth hint: Published master-
pieces in Asahi Shimbun.
U5 Natsume Soseki?
S6 That?s right!
 
Figure 2: Example dialogue based on the quiz-style
ranking of definitions. S stands for a system utter-
ance and U for a user utterance.
3 Approach
Since it is difficult to know in advance what char-
acteristics are important for quiz-style ranking, we
learn the appropriate ranking of definitions from
data. The approach is the same as that of (Xu et al,
2005) in that we adopt a machine learning approach
for definition ranking, but is different in that what is
learned is a quiz-style ranking of sentences that are
already known to be good definitions.
First, we collect ranking data. For this purpose,
we turn to existing encyclopedias for concise biogra-
phies. Then, we annotate the ranking. Secondly, we
devise a set of features for a definition. Since the
existence of keywords that have high scores in IR-
related measures may suggest easy hints, we incor-
porate the scores of IR-related measures as features
(IR-related features).
Certain words tend to appear before or after oth-
ers in a biographical document to convey particular
information about people (e.g., words describing oc-
cupations at the beginning; those describing works
at the end, etc.) Therefore, we use word positions
within the biography of the person in question as
features (positional features). Biographies can be
found in online resources, such as biography.com
(http://www.biography.com/) and Wikipedia. In ad-
dition, to focus on the particular content of the def-
inition, we use bag-of-words (BOW) features, to-
gether with semantic features (e.g., semantic cate-
gories in Nihongo Goi-Taikei (Ikehara et al, 1997)
or word senses in WordNet) to complement the
sparseness of BOW features. We describe the fea-
tures we created in Section 4.2. Finally, we create
a ranking model using a preference learning algo-
118
rithm, such as the ranking SVM (Joachims, 2002),
which learns ranking by reducing the pairwise rank-
ing error.
4 Experiment
4.1 Data Collection
We collected biographies (in Japanese) from the goo
encyclopedia. We first mined Wikipedia to calcu-
late the PageRankTMof people using the hyper-link
structure. After sorting them in descending order by
the PageRank score, we extracted the top-150 peo-
ple for whom we could find an entry in the goo en-
cyclopedia. Then, 11 annotators annotated rankings
for each of the 150 people individually. The annota-
tors were instructed to rank the definitions assuming
that they were creating a ?who is this?? quiz; i.e.,
to place the definition that is the most characteris-
tic of the person in question at the end. The mean
of the Kendall?s coefficients of concordance for the
150 people was sufficiently high at 0.76 with a stan-
dard deviation of 0.13. Finally, taking the means of
ranks given to each definition, we merged the indi-
vidual rankings to create the reference rankings. An
example of a reference ranking is the bottom one in
Fig. 1. There are 958 definition sentences in all, with
each person having approximately 6?7 definitions.
4.2 Deriving Features
We derived our IR-related features based on
Mainichi newspaper articles (1991?2004) and
Wikipedia articles. We used these two different
sources to take into account the difference in the
importance of terms depending on the text. We
also used sentences, sections (for Wikipedia arti-
cles only) and documents as units to calculate doc-
ument frequency, which resulted in the creation of
five frequency tables: (i) Mainichi-Document, (ii)
Mainichi-Sentence, (iii) Wikipedia-Document, (iv)
Wikipedia-Section, and (v) Wikipedia-Sentence.
Using the five frequency tables, we calculated, for
each content word (nouns, verbs, adjectives, and un-
known words) in the definition, (1) frequency (the
number of documents where the word is found), (2)
relative frequency (frequency divided by the maxi-
mum number of documents), (3) co-occurrence fre-
quency (the number of documents where both the
word and the person?s name are found), (4) rela-
tive co-occurrence frequency, and (5) PMI. Then, we
took the minimum, maximum, and mean values of
(1)?(5) for all content words in the definition as fea-
tures, deriving 75 (5 ? 5 ? 3) features. Then, using
the Wikipedia article (called an entry) for the person
in question, we calculated (1)?(4) within the entry,
and calculated tf*idf scores of words in the defini-
tion using the term frequency in the entry. Again, by
taking the minimum, maximum, and mean values of
(1)?(4) and tf*idf, we yielded 15 (5 ? 3) features,
for a total of 90 (75 + 15) IR-related features.
Positional features were derived also using the
Wikipedia entry. For each word in the definition, we
calculated (a) the number of times the word appears
in the entry, (b) the minimum position of the word in
the entry, (c) its maximum position, (d) its mean po-
sition, and (e) the standard deviation of the positions.
Note that positions are either ordinal or relative; i.e.,
the relative position is calculated by dividing the or-
dinal position by the total number of words in the
entry. Then, we took the minimum, maximum, and
mean values of (a)?(e) for all content words in the
definition as features, deriving 30 (5 ? 2 (ordinal or
relative positions)? 3) features.
For the BOW features, we first parsed all our
definitions with CaboCha (a Japanese morphologi-
cal/dependency parser, http://chasen.org/?taku/soft-
ware/cabocha/) and extracted all content words to
make binary features representing the existence of
each content word. There are 2,156 BOW features
in our data.
As for the semantic features, we used the seman-
tic categories in NihongoGoi-Taikei. Since there are
2,715 semantic categories, we created 2,715 features
representing the existence of each semantic category
in the definition. Semantic categories were assigned
to words in the definition by a morphological ana-
lyzer that comes with ALT/J-E, a Japanese-English
machine translation system (Ikehara et al, 1991).
In total, we have 4,991 features to represent each
definition. We calculated all feature values for all
definitions in our data to be used for the learning.
4.3 Training Ranking Models
Using the reference ranking data, we trained a rank-
ing model using the ranking SVM (Joachims, 2002)
(with a linear kernel) that minimizes the pairwise
ranking error among the definitions of each person.
5 Evaluation
To evaluate the performance of the ranking model,
following (Xu et al, 2004; Sun et al, 2005), we
compared it with baselines that use only the scores
of IR-related and positional features for ranking, i.e.,
sorting. Table 1 shows the performance of the rank-
ing model (by the leave-one-out method, predicting
the ranking of definitions of a person by other peo-
119
Rank Description Ranking Error
1 Proposed ranking model 0.185
2 Wikipedia-Sentence-PMI-max 0.299
3 Wikipedia-Section-PMI-max 0.309
4 Wikipedia-Document-PMI-max 0.312
5 Mainichi-Sentence-PMI-max 0.318
6 Mainichi-Document-PMI-max 0.325
7 Mainichi-Sentence-relative-co-occurrence-max 0.338
8 Wikipedia-Entry-ordinal-Min-max 0.338
9 Wikipedia-Sentence-relative-co-occurrence-max 0.339
10 Wikipedia-Entry-relative-Min-max 0.340
11 Wikipedia-Entry-ordinal-Mean-mean 0.342
Table 1: Performance of the proposed rankingmodel
and that of 10 best-performing baselines.
ple?s rankings) and that of the 10 best-performing
baselines. The ranking error is pairwise ranking er-
ror; i.e., the rate of misordered pairs. A descrip-
tive name is given for each baseline. For example,
Wikipedia-Sentence-PMI-max means that we used
the maximum PMI values of content words in the
definition calculated from Wikipedia, with sentence
as the unit for obtaining frequencies.
Our ranking model outperforms all of the base-
lines. McNemar?s test showed that the difference be-
tween the proposed model and the best-performing
baseline is significant (p<0.00001). The results also
show that PMI is more effective in quiz-style rank-
ing than any other measure. The fact that max is im-
portant probably means that the mere existence of a
word that has a high PMI score is enough to raise the
ranking of a hint. It is also interesting thatWikipedia
gives better ranking, which is probably because peo-
ple?s names and related keywords are close to each
other in such descriptive texts.
Analyzing the ranking model trained by the rank-
ing SVM allows us to calculate the weights given to
the features (Hirao et al, 2002). Table 2 shows the
top-10 features in weights in absolute figures when
all samples were used for training. It can be seen
that high PMI values and words/semantic categories
related to government or creation lead to easy hints,
whereas semantic categories, such as birth and oth-
ers (corresponding to the person in ?a person from
Tokyo?), lead to early hints. This supports our in-
tuitive notion that birthplaces should be presented
early for users to start thinking about a person.
6 Summary and Future Work
This paper proposed ranking definitions of a person
to automatically generate a ?Who is this?? quiz.
Using reference ranking data that we created man-
ually, we trained a ranking model using a ranking
SVM based on features that encode the importance
of keywords in a definition as well as its content.
Rank Feature Name Weight
1 Wikipedia-Sentence-PMI-max 0.723
2 SemCat:33 (others/someone) -0.559
3 SemCat:186 (creator) 0.485
4 BOW:bakufu (feudal government) 0.451
5 SemCat:163 (sovereign/ruler/monarch) 0.422
6 Wikipedia-Document-PMI-max 0.409
7 SemCat:2391 (birth) -0.404
8 Wikipedia-Section-PMI-max 0.402
9 SemCat:2595 (unit; e.g., numeral classifier) 0.374
10 SemCat:2606 (plural; e.g., plural form) -0.368
Table 2: Weights of features learned for ranking def-
initions by the ranking SVM. SemCat denotes it is
a semantic-category feature with its semantic cate-
gory ID followed by the description of the category
in parentheses. BOW denotes a BOW feature.
Experimental results show that our ranking model
significantly outperforms baselines that use single
IR-related and positional measures for ranking. We
are currently in the process of building a dialogue
system that uses the quiz-style ranking for definition
presentation. We are planning to examine how the
different rankings affect the understanding and mo-
tivation of users.
References
Amy Baylor and Jeeheon Ryu. 2003. Does the presence of
image and animation enhance pedagogical agent persona?
Journal of Educational Computing Research, 28(4):373?
395.
Fergus I. M. Craik and Robert S. Lockhart. 1972. Levels of
processing: A framework for memory research. Journal of
Verbal Learning and Verbal Behavior, 11:671?684.
Tsutomu Hirao, Hideki Isozaki, Eisaku Maeda, and Yuji Mat-
sumoto. 2002. Extracting important sentences with support
vector machines. In Proc. 19th COLING, pages 342?348.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-editing
?Effects of new methods in ALT-J/E?. In Proc. Third Ma-
chine Translation Summit: MT Summit III, pages 101?106.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ? A
Japanese Lexicon. Iwanami Shoten.
Thorsten Joachims. 2002. Optimizing search engines using
clickthrough data. In Proc. KDD, pages 133?142.
Mirella Lapata. 2003. Probabilistic text structuring: Exper-
iments with sentence ordering. In Proc. 41st ACL, pages
545?552.
Akira Sugiyama, Kohji Dohsaka, and Takeshi Kawabata. 1999.
A method for conveying the contents of written texts by spo-
ken dialogue. In Proc. PACLING, pages 54?66.
Renxu Sun, Jing Jiang, Yee Fan Tan, Hang Cui, Tat-Seng Chua,
and Min-Yen Kan. 2005. Using syntactic and semantic rela-
tion analysis in question answering. In Proc. TREC.
Jinxi Xu, Ralph Weischedel, and Ana Licuanan. 2004. Eval-
uation of an extraction-based approach to answering defini-
tional questions. In Proc. SIGIR, pages 418?424.
Jun Xu, Yunbo Cao, Hang Li, and Min Zhao. 2005. Rank-
ing definitions with supervised learning methods. In Proc.
WWW, pages 811?819.
120
Proceedings of ACL-08: HLT, pages 665?673,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semi-Supervised Sequential Labeling and Segmentation
using Giga-word Scale Unlabeled Data
Jun Suzuki and Hideki Isozaki
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{jun, isozaki}@cslab.kecl.ntt.co.jp
Abstract
This paper provides evidence that the use of
more unlabeled data in semi-supervised learn-
ing can improve the performance of Natu-
ral Language Processing (NLP) tasks, such
as part-of-speech tagging, syntactic chunking,
and named entity recognition. We first pro-
pose a simple yet powerful semi-supervised
discriminative model appropriate for handling
large scale unlabeled data. Then, we describe
experiments performed on widely used test
collections, namely, PTB III data, CoNLL?00
and ?03 shared task data for the above three
NLP tasks, respectively. We incorporate up
to 1G-words (one billion tokens) of unlabeled
data, which is the largest amount of unlabeled
data ever used for these tasks, to investigate
the performance improvement. In addition,
our results are superior to the best reported re-
sults for all of the above test collections.
1 Introduction
Today, we can easily find a large amount of un-
labeled data for many supervised learning applica-
tions in Natural Language Processing (NLP). There-
fore, to improve performance, the development of
an effective framework for semi-supervised learning
(SSL) that uses both labeled and unlabeled data is at-
tractive for both the machine learning and NLP com-
munities. We expect that such SSL will replace most
supervised learning in real world applications.
In this paper, we focus on traditional and impor-
tant NLP tasks, namely part-of-speech (POS) tag-
ging, syntactic chunking, and named entity recog-
nition (NER). These are also typical supervised
learning applications in NLP, and are referred to
as sequential labeling and segmentation problems.
In some cases, these tasks have relatively large
amounts of labeled training data. In this situation,
supervised learning can provide competitive results,
and it is difficult to improve them any further by
using SSL. In fact, few papers have succeeded in
showing significantly better results than state-of-the-
art supervised learning. Ando and Zhang (2005) re-
ported a substantial performance improvement com-
pared with state-of-the-art supervised learning re-
sults for syntactic chunking with the CoNLL?00
shared task data (Tjong Kim Sang and Buchholz,
2000) and NER with the CoNLL?03 shared task
data (Tjong Kim Sang and Meulder, 2003).
One remaining question is the behavior of SSL
when using as much labeled and unlabeled data
as possible. This paper investigates this question,
namely, the use of a large amount of unlabeled data
in the presence of (fixed) large labeled data.
To achieve this, it is paramount to make the SSL
method scalable with regard to the size of unlabeled
data. We first propose a scalable model for SSL.
Then, we apply our model to widely used test collec-
tions, namely Penn Treebank (PTB) III data (Mar-
cus et al, 1994) for POS tagging, CoNLL?00 shared
task data for syntactic chunking, and CoNLL?03
shared task data for NER. We used up to 1G-words
(one billion tokens) of unlabeled data to explore the
performance improvement with respect to the unla-
beled data size. In addition, we investigate the per-
formance improvement for ?unseen data? from the
viewpoint of unlabeled data coverage. Finally, we
compare our results with those provided by the best
current systems.
The contributions of this paper are threefold.
First, we present a simple, scalable, but power-
ful task-independent model for semi-supervised se-
quential labeling and segmentation. Second, we re-
port the best current results for the widely used test
665
collections described above. Third, we confirm that
the use of more unlabeled data in SSL can really lead
to further improvements.
2 Conditional Model for SSL
We design our model for SSL as a natural semi-
supervised extension of conventional supervised
conditional random fields (CRFs) (Lafferty et al,
2001). As our approach for incorporating unla-
beled data, we basically follow the idea proposed in
(Suzuki et al, 2007).
2.1 Conventional Supervised CRFs
Let x?X and y?Y be an input and output, where
X and Y represent the set of possible inputs and out-
puts, respectively. C stands for the set of cliques in
an undirected graphical model G(x,y), which indi-
cates the interdependency of a given x and y. yc
denotes the output from the corresponding clique c.
Each clique c?C has a potential function ?c. Then,
the CRFs define the conditional probability p(y|x)
as a product of ?cs. In addition, let f =(f1, . . ., fI)
be a feature vector, and ? = (?1, . . ., ?I) be a pa-
rameter vector, whose lengths are I . p(y|x;?) on a
CRF is defined as follows:
p(y|x;?) = 1
Z(x)
?
c
?c(yc,x;?), (1)
where Z(x) =
?
y?Y
?
c?C ?c(yc,x;?) is the par-
tition function. We generally assume that the po-
tential function is a non-negative real value func-
tion. Therefore, the exponentiated weighted sum
over the features of a clique is widely used, so that,
?c(yc,x;?)=exp(? ? f c(yc,x)) where f c(yc,x)
is a feature vector obtained from the corresponding
clique c in G(x,y).
2.2 Semi-supervised Extension for CRFs
Suppose we have J kinds of probability mod-
els (PMs). The j-th joint PM is represented by
pj(xj ,y;?j) where ?j is a model parameter. xj =
Tj(x) is simply an input x transformed by a pre-
defined function Tj . We assume xj has the same
graph structure as x. This means pj(xj ,y) can
be factorized by the cliques c in G(x,y). That is,
pj(xj ,y;?j)=
?
c pj(xjc,yc;?j). Thus, we can in-
corporate generative models such as Bayesian net-
works including (1D and 2D) hidden Markov mod-
els (HMMs) as these joint PMs. Actually, there is
a difference in that generative models are directed
graphical models while our conditional PM is an
undirected. However, this difference causes no vi-
olations when we construct our approach.
Let us introduce ??=(?1, . . ., ?I, ?I+1, . . ., ?I+J),
and h = (f1, . . ., fI, log p1, . . ., log pJ), which is
the concatenation of feature vector f and the log-
likelihood of J-joint PMs. Then, we can define a
new potential function by embedding the joint PMs;
??c(yc,x;?
?,?)
= exp(? ? f c(yc,x)) ?
?
j
pj(xjc,yc;?j)?I+j
= exp(?? ? hc(yc,x)).
where ?= {?j}Jj=1, and hc(yc,x) is h obtained
from the corresponding clique c in G(x,y). Since
each pj(xjc,yc) has range [0, 1], which is non-
negative, ??c can also be used as a potential func-
tion. Thus, the conditional model for our SSL can
be written as:
P (y|x;??,?) = 1
Z ?(x)
?
c
??c(yc,x;?
?,?), (2)
where Z ?(x)=
?
y?Y
?
c?C ??c(yc,x;??,?). Here-
after in this paper, we refer to this conditional model
as a ?Joint probability model Embedding style Semi-
Supervised Conditional Model?, or JESS-CM for
short.
Given labeled data, Dl={(xn,yn)}Nn=1, the MAP
estimation of ?? under a fixed ? can be written as:
L1(??|?) =
?
n
logP (yn|xn;??,?) + log p(??),
where p(??) is a prior probability distribution of ??.
Clearly, JESS-CM shown in Equation 2 has exactly
the same form as Equation 1. With a fixed ?, the
log-likelihood, log pj , can be seen simply as the fea-
ture functions of JESS-CM as with fi. Therefore,
embedded joint PMs do not violate the global con-
vergence conditions. As a result, as with super-
vised CRFs, it is guaranteed that ?? has a value that
achieves the global maximum of L1(??|?). More-
over, we can obtain the same form of gradient as that
of supervised CRFs (Sha and Pereira, 2003), that is,
?L1(??|?) =EP? (Y,X ;??,?)
[
h(Y,X )
]
?
?
n
EP (Y|xn;??,?)
[
h(Y,xn)
]
+? log p(??).
Thus, we can easily optimize L1 by using the
forward-backward algorithm since this paper solely
666
focuses on a sequence model and a gradient-based
optimization algorithm in the same manner as those
used in supervised CRF parameter estimation.
We cannot naturally incorporate unlabeled data
into standard discriminative learning methods since
the correct outputs y for unlabeled data are un-
known. On the other hand with a generative ap-
proach, a well-known way to achieve this incorpora-
tion is to use maximum marginal likelihood (MML)
parameter estimation, i.e., (Nigam et al, 2000).
Given unlabeled data Du = {xm}Mm=1, MML esti-
mation in our setting maximizes the marginal distri-
bution of a joint PM over a missing (hidden) variable
y, namely, it maximizes
?
m log
?
y?Y p(xm,y; ?).
Following this idea, there have been introduced
a parameter estimation approach for non-generative
approaches that can effectively incorporate unla-
beled data (Suzuki et al, 2007). Here, we refer to it
as ?Maximum Discriminant Functions sum? (MDF)
parameter estimation. MDF estimation substitutes
p(x,y) with discriminant functions g(x,y). There-
fore, to estimate the parameter ? of JESS-CM by
using MDF estimation, the following objective func-
tion is maximized with a fixed ??:
L2(?|??) =
?
m
log
?
y?Y
g(xm,y;??,?) + log p(?),
where p(?) is a prior probability distribution of
?. Since the normalization factor does not af-
fect the determination of y, the discriminant func-
tion of JESS-CM shown in Equation 2 is defined
as g(x,y;??,?) =
?
c?C ??c(yc,x;??,?). With
a fixed ??, the local maximum of L2(?|??) around
the initialized value of? can be estimated by an iter-
ative computation such as the EM algorithm (Demp-
ster et al, 1977).
2.3 Scalability: Efficient Training Algorithm
A parameter estimation algorithm of ?? and ? can
be obtained by maximizing the objective functions
L1(??|?) and L2(?|??) iteratively and alternately.
Figure 1 summarizes an algorithm for estimating ??
and ? for JESS-CM.
This paper considers a situation where there are
many more unlabeled data M than labeled data N ,
that is, N << M . This means that the calculation
cost for unlabeled data is dominant. Thus, in order
to make the overall parameter estimation procedure
Input: training data D = {Dl,Du}
where labeled data Dl = {(xn,yn)}Nn=1,
and unlabeled data Du = {xm}Mm=1
Initialize: ?(0) ? uniform distribution, t ? 0
do
1. t ? t + 1
2. (Re)estimate ??:
maximize L1(??|?) with fixed ???(t?1) using Dl.
3. Estimate ?(t): (Initial values = ?(t?1))
update one step toward maximizing L2(?|??)
with fixed ?? using Du.
do until |?
(t)??(t?1)|
|?(t?1)| < ?.
Reestimate ??: perform the same procedure as 1.
Output: a JESS-CM, P (y|x,??,?(t)).
Figure 1: Parameter estimation algorithm for JESS-CM.
scalable for handling large scale unlabeled data, we
only perform one step of MDF estimation for each t
as explained on 3. in Figure 1. In addition, the cal-
culation cost for estimating parameters of embedded
joint PMs (HMMs) is independent of the number of
HMMs, J , that we used (Suzuki et al, 2007). As a
result, the cost for calculating the JESS-CM param-
eters, ?? and ?, is essentially the same as execut-
ing T iterations of the MML estimation for a single
HMM using the EM algorithm plus T +1 time opti-
mizations of the MAP estimation for a conventional
supervised CRF if it converged when t = T . In
addition, our parameter estimation algorithm can be
easily performed in parallel computation.
2.4 Comparison with Hybrid Model
SSL based on a hybrid generative/discriminative ap-
proach proposed in (Suzuki et al, 2007) has been
defined as a log-linear model that discriminatively
combines several discriminative models, pDi , and
generative models, pGj , such that:
R(y|x;?,?,?)
=
?
i pDi (y|x;?i)?i
?
j pGj (xj ,y;?j)?j?
y
?
i pDi (y|x;?i)?i
?
j pGj (xj ,y;?j)?j
,
where ?={?i}Ii=1, and ?={{?i}Ii=1, {?j}I+Jj=I+1}.
With the hybrid model, if we use the same labeled
training data to estimate both ? and ?, ?js will be-
come negligible (zero or nearly zero) since pDi is al-
ready fitted to the labeled training data while pGj are
trained by using unlabeled data. As a solution, a
given amount of labeled training data is divided into
two distinct sets, i.e., 4/5 for estimating ?, and the
667
remaining 1/5 for estimating ? (Suzuki et al, 2007).
Moreover, it is necessary to split features into sev-
eral sets, and then train several corresponding dis-
criminative models separately and preliminarily. In
contrast, JESS-CM is free from this kind of addi-
tional process, and the entire parameter estimation
procedure can be performed in a single pass. Sur-
prisingly, although JESS-CM is a simpler version of
the hybrid model in terms of model structure and
parameter estimation procedure, JESS-CM provides
F -scores of 94.45 and 88.03 for CoNLL?00 and ?03
data, respectively, which are 0.15 and 0.83 points
higher than those reported in (Suzuki et al, 2007)
for the same configurations. This performance im-
provement is basically derived from the full bene-
fit of using labeled training data for estimating the
parameter of the conditional model while the com-
bination weights, ?, of the hybrid model are esti-
mated solely by using 1/5 of the labeled training
data. These facts indicate that JESS-CM has sev-
eral advantageous characteristics compared with the
hybrid model.
3 Experiments
In our experiments, we report POS tagging, syntac-
tic chunking and NER performance incorporating up
to 1G-words of unlabeled data.
3.1 Data Set
To compare the performance with that of previ-
ous studies, we selected widely used test collec-
tions. For our POS tagging experiments, we used
the Wall Street Journal in PTB III (Marcus et al,
1994) with the same data split as used in (Shen et
al., 2007). For our syntactic chunking and NER ex-
periments, we used exactly the same training, devel-
opment and test data as those provided for the shared
tasks of CoNLL?00 (Tjong Kim Sang and Buchholz,
2000) and CoNLL?03 (Tjong Kim Sang and Meul-
der, 2003), respectively. The training, development
and test data are detailed in Table 11 .
The unlabeled data for our experiments was
taken from the Reuters corpus, TIPSTER corpus
(LDC93T3C) and the English Gigaword corpus,
third edition (LDC2007T07). As regards the TIP-
1The second-order encoding used in our NER experiments
is the same as that described in (Sha and Pereira, 2003) except
removing IOB-tag of previous position label.
(a) POS-tagging: (WSJ in PTB III)
# of labels 45
Data set (WSJ sec. IDs) # of sent. # of words
Training 0?18 38,219 912,344
Development 19?21 5,527 131,768
Test 22?24 5,462 129,654
(b) Chunking: (WSJ in PTB III: CoNLL?00 shared task data)
# of labels 23 (w/ IOB-tagging)
Data set (WSJ sec. IDs) # of sent. # of words
Training 15?18 8,936 211,727
Development N/A N/A N/A
Test 20 2,012 47,377
(c) NER: (Reuters Corpus: CoNLL?03 shared task data)
# of labels 29 (w/ IOB-tagging+2nd-order encoding)
Data set (time period) # of sent. # of words
Training 22?30/08/96 14,987 203,621
Development 30?31/08/96 3,466 51,362
Test 06?07/12/96 3,684 46,435
Table 1: Details of training, development, and test data
(labeled data set) used in our experiments
data abbr. (time period) # of sent. # of words
Tipster wsj 04/90?03/92 1,624,744 36,725,301
Reuters reu 09/96?08/97* 13,747,227 215,510,564
Corpus *(excluding 06?07/12/96)
English afp 05/94?12/96 5,510,730 135,041,450
Gigaword apw 11/94?12/96 7,207,790 154,024,679
ltw 04/94?12/96 3,094,290 72,928,537
nyt 07/94?12/96 15,977,991 357,952,297
xin 01/95?12/96 1,740,832 40,078,312
total all 48,903,604 1,012,261,140
Table 2: Unlabeled data used in our experiments
STER corpus, we extracted all the Wall Street Jour-
nal articles published between 1990 and 1992. With
the English Gigaword corpus, we extracted articles
from five news sources published between 1994 and
1996. The unlabeled data used in this paper is de-
tailed in Table 2. Note that the total size of the unla-
beled data reaches 1G-words (one billion tokens).
3.2 Design of JESS-CM
We used the same graph structure as the linear chain
CRF for JESS-CM. As regards the design of the fea-
ture functions fi, Table 3 shows the feature tem-
plates used in our experiments. In the table, s indi-
cates a focused token position. Xs?1:s represents the
bi-gram of feature X obtained from s? 1 and s po-
sitions. {Xu}Bu=A indicates that u ranges from A to
B. For example, {Xu}s+2u=s?2 is equal to five feature
templates, {Xs?2, Xs?1, Xs, Xs+1, Xs+2}. ?word
type? or wtp represents features of a word such as
capitalization, the existence of digits, and punctua-
tion as shown in (Sutton et al, 2006) without regular
expressions. Although it is common to use external
668
(a) POS tagging:(total 47 templates)
[ys], [ys?1:s], {[ys, pf-Ns], [ys, sf-Ns]}9N=1,
{[ys,wdu], [ys,wtpu], [ys?1:s,wtpu]}
s+2
u=s?2,
{[ys,wdu?1:u], [ys,wtpu?1:u], [ys?1:s,wtpu?1:u]}
s+2
u=s?1
(b) Syntactic chunking: (total 39 templates)
[ys], [ys?1:s], {[ys,wdu], [ys, posu], [ys,wdu, posu],
[ys?1:s,wdu], [ys?1:s, posu]}
s+2
u=s?2, {[ys,wdu?1:u],
[ys, posu?1:u], {[ys?1:s, posu?1:u]}
s+2
u=s?1,
(c) NER: (total 79 templates)
[ys], [ys?1:s], {[ys,wdu], [ys, lwdu], [ys, posu], [ys,wtpu],
[ys?1:s, lwdu], [ys?1:s, posu], [ys?1:s,wtpu]}
s+2
u=s?2,
{[ys, lwdu?1:u], [ys, posu?1:u], [ys,wtpu?1:u],
[ys?1:s, posu?1:u], [ys?1:s,wtpu?1:u]}
s+2
u=s?1,
[ys, poss?1:s:s+1], [ys,wtps?1:s:s+1], [ys?1:s, poss?1:s:s+1],
[ys?1:s,wtps?1:s:s+1], [ys,wd4ls], [ys,wd4rs],
{[ys, pf-Ns], [ys, sf-Ns], [ys?1:s, pf-Ns], [ys?1:s, sf-Ns]}4N=1
wd: word, pos: part-of-speech lwd : lowercase of word,
wtp: ?word type?, wd4{l,r}: words within the left or right 4 tokens
{pf,sf}-N: N character prefix or suffix of word
Table 3: Feature templates used in our experiments




    

	
		

	

	
      
     
    
   
   Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 826?833,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Syntax-Free Approach to Japanese Sentence Compression
Tsutomu HIRAO, Jun SUZUKI and Hideki ISOZAKI
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan
{hirao,jun,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Conventional sentence compression meth-
ods employ a syntactic parser to compress
a sentence without changing its mean-
ing. However, the reference compres-
sions made by humans do not always re-
tain the syntactic structures of the original
sentences. Moreover, for the goal of on-
demand sentence compression, the time
spent in the parsing stage is not negligi-
ble. As an alternative to syntactic pars-
ing, we propose a novel term weighting
technique based on the positional infor-
mation within the original sentence and
a novel language model that combines
statistics from the original sentence and a
general corpus. Experiments that involve
both human subjective evaluations and au-
tomatic evaluations show that our method
outperforms Hori?s method, a state-of-the-
art conventional technique. Because our
method does not use a syntactic parser, it
is 4.3 times faster than Hori?s method.
1 Introduction
In order to compress a sentence while retaining
its original meaning, the subject-predicate rela-
tionship of the original sentence should be pre-
served after compression. In accordance with this
idea, conventional sentence compression methods
employ syntactic parsers. English sentences are
usually analyzed by a full parser to make parse
trees, and the trees are then trimmed (Knight and
Marcu, 2002; Turner and Charniak, 2005; Unno
et al, 2006). For Japanese, dependency trees are
trimmed instead of full parse trees (Takeuchi and
Matsumoto, 2001; Oguro et al, 2002; Nomoto,
2008)1 This parsing approach is reasonable be-
cause the compressed output is grammatical if the
1Hereafter, we refer these compression processes as ?tree
trimming.?
input is grammatical, but it offers only moderate
compression rates.
An alternative to the tree trimming approach
is the sequence-oriented approach (McDonald,
2006; Nomoto, 2007; Clarke and Lapata, 2006;
Hori and Furui, 2003). It treats a sentence as a se-
quence of words and structural information, such
as a syntactic or dependency tree, is encoded in
the sequence as features. Their methods have the
potential to drop arbitrary words from the original
sentence without considering the boundary deter-
mined by the tree structures. However, they still
rely on syntactic information derived from fully
parsed syntactic or dependency trees.
We found that humans usually ignored the syn-
tactic structures when compressing sentences. For
example, in many cases, they compressed the sen-
tence by dropping intermediate nodes of the syn-
tactic tree derived from the source sentence. We
believe that making compression strongly depen-
dent on syntax is not appropriate for reproducing
reference compressions. Moreover, on-demand
sentence compression is made problematic by the
time spent in the parsing stage.
This paper proposes a syntax-free sequence-
oriented sentence compression method. To main-
tain the subject-predicate relationship in the com-
pressed sentence and retain fluency without us-
ing syntactic parsers, we propose two novel fea-
tures: intra-sentence positional term weighting
(IPTW) and the patched language model (PLM).
IPTW is defined by the term?s positional informa-
tion in the original sentence. PLM is a form of
summarization-oriented fluency statistics derived
from the original sentence and the general lan-
guage model. The weight parameters for these
features are optimized within the Minimum Clas-
sification Error (MCE) (Juang and Katagiri, 1992)
learning framework.
Experiments that utilize both human subjective
and automatic evaluations show that our method is
826
????? ?
?? ? ? ? ??
?? ?? ?
Source Sentence
??? ?
????? ????
??????? ?
Chunk 1
Chunk 2 Chunk 3
Chunk 4
Chunk 5
Chunk 6
Chunk 7
Compressed Sentence
Chunk7 = a part of Chunk6 + parts of Chunk4
????? ?? ?
suitei shi ta
haiten nitsuite fukutake ga
edamonbubun no
kouhyou shi te nai
center shiken de
??? ?
????? ????
Chunk 1
Chunk 2 Chunk 3
suitei shi ta
haiten nitsuite fukutake ga
center shiken
edamon no
edamon no
center shiken
Compression
compound noun
i
Figure 1: An example of the dependency relation between an original sentence and its compressed
variant.
superior to conventional sequence-oriented meth-
ods that employ syntactic parsers while being
about 4.3 times faster.
2 Analysis of reference compressions
Syntactic information does not always yield im-
proved compression performance because humans
usually ignore the syntactic structures when they
compress sentences. Figure 1 shows an exam-
ple. English translation of the source sentence is
?Fukutake Publishing Co., Ltd. presumed prefer-
ential treatment with regard to its assessed scores
for a part of the questions for a series of Center
Examinations.? and its compression is ?Fukutake
presumed preferential scores for questions for a
series of Center Examinations.?
In the figure, each box indicates a syntactic
chunk, bunsetsu. The solid arrows indicate de-
pendency relations between words2. We observe
that the dependency relations are changed by com-
pression; humans create compound nouns using
the components derived from different portions of
the original sentence without regard to syntactic
constraints. ?Chunk 7? in the compressed sen-
tence was constructed by dropping both content
and functional words and joining other content
words contained in ?Chunk 4? and ?Chunk 6? of
2Generally, a dependency relation is defined between bun-
setsu. Therefore, in order to identify word dependencies, we
followed Kudo?s rule (Kudo and Matsumoto, 2004)
the original sentence. ?Chunk 5? is dropped com-
pletely. This compression cannot be achieved by
tree trimming.
According to an investigation in our corpus of
manually compressed Japanese sentences, which
we used in the experimental evaluation, 98.7% of
them contain at least one segment that does not
retain the original tree structure. Human usually
compress sentences by dropping the intermediate
nodes in the dependency tree. However, the re-
sulting compressions retain both adequacy and flu-
ency. This statistic supports the view that sentence
compression that strongly depends on syntax is
not useful in reproducing reference compressions.
We need a sentence compression method that can
drop intermediate nodes in the syntactic tree ag-
gressively beyond the tree-scoped boundary.
In addition, sentence compression methods that
strongly depend on syntactic parsers have two
problems: ?parse error? and ?decoding speed.?
44% of sentences output by a state-of-the-art
Japanese dependency parser contain at least one
error (Kudo and Matsumoto, 2005). Even more, it
is well known that if we parse a sentence whose
source is different from the training data of the
parser, the performance could be much worse.
This critically degrades the overall performance
of sentence compression. Moreover, summariza-
tion systems often have to process megabytes of
documents. Parsers are still slow and users of on-
827
demand summarization systems are not prepared
to wait for parsing to finish.
3 A Syntax Free Sequence-oriented
Sentence Compression Method
As an alternative to syntactic parsing, we pro-
pose two novel features, intra-sentence positional
term weighting (IPTW) and the patched language
model (PLM) for our syntax-free sentence com-
pressor.
3.1 Sentence Compression as a
Combinatorial Optimization Problem
Suppose that a compression system reads sen-
tence x= x1 , x2, . . . , xj , . . . , xN , where xj
is the j-th word in the input sentence. The
system then outputs the compressed sentence y
=y1, y2, . . . , yi, . . . , yM , where yi is the i-
th word in the output sentence. Here, yi ?
{x1, . . . , xN}. We assume y0=x0=<s> (BOS)
and yM+1=xN+1=</s> (EOS). We define func-
tion I(?), which maps word yi to the index of
the word in the original sentence. For example,
if source sentence is x = x1, x2, . . . , x5 and its
compressed variant is y = x1, x3, x4, I(y1) = 1,
I(y2) = 3, I(y3) = 4.
We define a significance score f(x, y,?) for
compressed sentence y based on Hori?s method
(Hori and Furui, 2003). ? = {?g, ?h} is a pa-
rameter vector.
f(x, y;?) =
M+1
?
i=1
{g(x, I(yi);?g) +
h(x, I(yi), I(yi?1);?h)} (1)
The first term of equation (1) (g(?)) is the impor-
tance of each word in the output sentence, and the
second term (h(?)) is the the linguistic likelihood
between adjacent words in the output sentence.
The best subsequence y?=argmax
y
f(x, y;?) is
identified by dynamic programming (DP) (Hori
and Furui, 2003).
3.2 Features
We use IPTW to define the significance score
g(x, I(yi);?g). Moreover, we use PLM to define
the linguistic likelihood h(x, I(yi+1), I(yi);?h).
3.2.1 Intra-sentence Positional Term
Weighting (IPTW)
IDF is a global term weighting scheme in that it
measures the significance score of a word in a
text corpus, which could be extremely large. By
contrast, this paper proposes another type of term
weighting; it measures the positional significance
score of a word within its sentence. Here, we as-
sume the following hypothesis:
? The ?significance? of a word depends on its
position within its sentence.
In Japanese, the main subject of a sentence
usually appears at the beginning of the sentence
(BOS) and the main verb phrase almost always
appears at the end of the sentence (EOS). These
words or phrases are usually more important than
the other words in the sentence. In order to
add this knowledge to the scoring function, term
weight is modeled by the following Gaussian mix-
ture.
N(psn(x, I(yi));?g) =
m1
1
?
2??1
exp
(
?
1
2
(
psn(x, I(yi)) ? ?1
?1
)2
)
+
m2
1
?
2??2
exp
(
?
1
2
(
psn(x, I(yi)) ? ?2
?2
)2
)
(2)
Here, ?g = {?k, ?k, mk}k=1,2. psn(x, I(yi))
returns the relative position of yi in the original
sentence x which is defined as follows:
psn(x, I(yi)) =
start(x, I(yi))
length(x)
(3)
?length(x)? denotes the number of characters in
the source sentence and ?start(x, I(yi))? denotes
the accumulated run of characters from BOS to
(x, I(yi)). In equation (2), ?k,?k indicates the
mean and the standard deviation for the normal
distribution, respectively. mk is a mixture param-
eter.
We use the distribution (2) in defining
g(x, I(yi);?g) as follows:
g(x, I(yi);?g) =
?
?
?
?
?
?
?
IDF(x, I(yi)) ? N(psn(x, I(yi);?g)
if pos(x,I(yi)) = noun, verb, adjective
Constant ? N(psn(x, I(yi);?g)
otherwise
(4)
828
Here, pos(x, I(yi)) denotes the part-of-speech tag
for yi. ?g is optimized by using the MCE learning
framework.
3.2.2 Patched Language Model
Many studies on sentence compression employ the
n-gram language model to evaluate the linguistic
likelihood of a compressed sentence. However,
this model is usually computed by using a huge
volume of text data that contains both short and
long sentences. N-gram distribution of short sen-
tences may different from that of long sentences.
Therefore, the n-gram probability sometimes dis-
agrees with our intuition in terms of sentence com-
pression. Moreover, we cannot obtain a huge
corpus consisting solely of compressed sentences.
Even if we collect headlines as a kind of com-
pressed sentence from newspaper articles, corpus
size is still too small. Therefore, we propose
the following novel linguistic likelihood based on
statistics derived from the original sentences and a
huge corpus:
PLM(x, I(yj), I(yj?1)) =
?
?
?
1 if I(yj) = I(yj?1) + 1
?PLM Bigram(x, I(yj), I(yj?1))
otherwise
(5)
PLM stands for Patched Language Model.
Here, 0 ? ?PLM ? 1, Bigram(?) indicates word
bigram probability. The first line of equation (5)
agrees with Jing?s observation on sentence align-
ment tasks (Jing and McKeown, 1999); that is,
most (or almost all) bigrams in a compressed sen-
tence appear in the original sentence as they are.
3.2.3 POS bigram
Since POS bigrams are useful for rejecting un-
grammatical sentences, we adopt them as follows:
Ppos(x, I(yi+1)|I(yi)) =
P (pos(x, I(yi+1))|pos(x, I(yi))). (6)
Finally, the linguistic likelihood between adja-
cent words within y is defined as follows:
h(x, I(yi+1), I(yi);?h) =
PLM(x, I(yi+1), I(yi)) +
?(pos(x,I(y
i+1
))|pos(x,I(y
i
)))Ppos(x, I(yi+1)|I(yi))
3.3 Parameter Optimization
We can regard sentence compression as a two class
problem: we give a word in the original sentence
class label +1 (the word is used in the compressed
output) or ?1 (the word is not used). In order to
consider the interdependence of words, we employ
the Minimum Classification Error (MCE) learning
framework (Juang and Katagiri, 1992), which was
proposed for learning the goodness of a sequence.
xt denotes the t-th original sentence in the training
data set T . y?t denotes the reference compression
that is made by humans and y?t is a compressed
sentence output by a system.
When using the MCE framework, the misclas-
sification measure is defined as the difference be-
tween the score of the reference sentence and that
of the best non-reference output and we optimize
the parameters by minimizing the measure.
d(y, x;?) = {
|T |
?
t=1
f(xt, y?t ;?)
? max
y?
t
6=y?
t
f(xt, y?t;?)} (7)
It is impossible to minimize equation (7) because
we cannot derive the gradient of the function.
Therefore, we employ the following sigmoid func-
tion to smooth this measure.
L(d(x, y;?)) =
|T |
?
t=1
1
1 + exp(?c ? d(xt, yt;?))
(8)
Here, c is a constant parameter. To minimize equa-
tion (8), we use the following equation.
?L=
?L
?d
(
?d
??1
,
?d
??2
, . . .
)
=0 (9)
Here, ?L?d is given by:
?L
?d
=
c
1 + exp (?c ? d)
(
1 ?
1
1 + exp (?c ? d)
)
(10)
Finally, the parameters are optimized by using
the iterative form. For example, ?w is optimized
as follows:
?w(new) = ?w(old) ? 
?L
??w(old)
(11)
829
Our parameter optimization procedure can be
replaced by another one such as MIRA (McDon-
ald et al, 2005) or CRFs (Lafferty et al, 2001).
The reason why we employed MCE is that it is
very easy to implement.
4 Experimental Evaluation
4.1 Corpus and Evaluation Measures
We randomly selected 1,000 lead sentences (a lead
sentence is the first sentence of an article exclud-
ing the headline.) whose length (number of words)
was greater than 30 words from the Mainichi
Newspaper from 1994 to 2002. There were five
different ideal compressions (reference compres-
sions produced by human) for each sentence; all
had a 0.6 compression rate. The average length of
the input sentences was about 42 words and that of
the reference compressions was about 24 words.
For MCE learning, we selected the reference
compression that maximize the BLEU score (Pap-
ineni et al, 2002) (= argmaxr?RBLEU(r, R\r))
from the set of reference compressions and used it
as correct data for training. Note that r is a ref-
erence compression and R is the set of reference
compressions.
We employed both automatic evaluation and hu-
man subjective evaluation. For automatic evalua-
tion, we employed BLEU (Papineni et al, 2002)
by following (Unno et al, 2006). We utilized 5-
fold cross validation, i.e., we broke the whole data
set into five blocks and used four of them for train-
ing and the remainder for testing and repeated the
evaluation on the test data five times changing the
test block each time.
We also employed human subjective evaluation,
i.e., we presented the compressed sentences to six
human subjects and asked them to evaluate the
sentence for fluency and importance on a scale 1
(worst) to 5 (best). For each source sentence, the
order in which the compressed sentences were pre-
sented was random.
4.2 Comparison of Sentence Compression
Methods
In order to investigate the effectiveness of the pro-
posed features, we compared our method against
Hori?s model (Hori and Furui, 2003), which is
a state-of-the-art Japanese sentence compressor
based on the sequence-oriented approach.
Table 1 shows the feature set used in our exper-
iment. Note that ?Hori?? indicates the earlier ver-
Table 1: Configuration setup
Label g() h()
Proposed IPTW PLM + POS
w/o PLM IPTW Bigram+POS
w/o IPTW IDF PLM+POS
Hori? IDF Trigram
Proposed+Dep IPTW PLM + POS +Dep
w/o PLM+Dep IPTW Bigram+POS+Dep
w/o IPTW+Dep IDF PLM+POS+Dep
Hori IDF Trigram+Dep
Table 2: Results: automatic evaluation
Label BLEU
Proposed .679
w/o PLM .617
w/o IPTW .635
Hori? .493
Proposed+Dep .632
w/o PLM+Dep .669
w/o IPTW+Dep .656
Hori .600
sion of Hori?s method which does not require the
dependency parser. For example, label ?w/o IPTW
+ Dep? employs IDF term weighting as function
g(?) and word bigram, part-of-speech bigram and
dependency probability between words as func-
tion h(?) in equation (1).
To obtain the word dependency probability, we
use Kudo?s relative-CaboCha (Kudo and Mat-
sumoto, 2005). We developed the n-gram lan-
guage model from a 9 year set of Mainichi News-
paper articles. We optimized the parameters by
using the MCE learning framework.
5 Results and Discussion
5.1 Results: automatic evaluation
Table 2 shows the evaluation results yielded by
BLUE at the compression rate of 0.60.
Without introducing dependency probability,
both IPTW and PLM worked well. Our method
achieved the highest BLEU score. Compared to
?Proposed?, ?w/o IPTW? offers significantly worse
performance. The results support the view that our
hypothesis, namely that the significance score of
a word depends on its position within a sentence,
is effective for sentence compression. Figure 2
shows an example of Gaussian mixture with pre-
830
00.05
0.1
0.15
0.2
0 N/4 N/2 3N/4 N
x1, x2, ,xj, ,xN<S> </S>
x
Figure 2: An example of Gaussian mixture with
predicted parameters
dicted parameters. From the figure, we can see
that the positional weights for words have peaks
at BOS and EOS. This is because, in many cases,
the subject appears at the beginning of Japanese
sentences and the predicate at the end.
Replacing PLM with the bigram language
model (w/o PLM) degrades the performance sig-
nificantly. This result shows that the n-gram lan-
guage model is improper for sentence compres-
sion because the n-gram probability is computed
by using a corpus that includes both short and long
sentences. Most bigrams in a compressed sentence
followed those in the source sentence.
The dependency probability is very helpful pro-
vided either IPTW or PLM is employed. For ex-
ample, ?w/o PLM + Dep? achieved the second
highest BLEU score. The difference of the score
between ?Proposed? and ?w/o PLM + Dep? is only
0.01 but there were significant differences as de-
termined by Wilcoxon signed rank test. Compared
to ?Hori??, ?Hori? achieved a significantly higher
BLEU score.
The introduction of both IPTW and PLM makes
the use of dependency probability unnecessary. In
fact, the score of ?Proposed + Dep? is not good.
We believe that this is due to overfitting. PLM
is similar to dependency probability in that both
features emphasize word pairs that occurred as
bigrams in the source sentence. Therefore, by
introducing dependency probability, the informa-
tion within the feature vector is not increased even
though the number of features is increased.
Table 3: Results: human subjective evaluations
Label Fluency Importance
Proposed 4.05 (?0.846) 3.33 (?0.854)
w/o PLM + Dep 3.91 (?0.759) 3.24 (?0.753)
Hori? 3.09 (?0.899) 2.34 (?0.696)
Hori 3.28 (?0.924) 2.64 (?0.819)
Human 4.86 (?0.268) 4.66 (?0.317)
5.2 Results: human subjective evaluation
We used human subjective evaluations to compare
our method to human compression, ?w/o PLM +
Dep? which achieved the second highest perfor-
mance in the automatic evaluation, ?Hori?? and
?Hori?. We randomly selected 100 sentences from
the test corpus and evaluated their compressed
variants in terms of ?fluency? and ?importance.?
Table 3 shows the results, mean score of all
judgements as well as the standard deviation.
The results indicate that human compression
achieved the best score in both fluency and impor-
tance. Human compression significantly outper-
formed other compression methods. This results
supports the idea that humans can easily compress
sentences with the compression rate of 0.6. Of
the automatic methods, our method achieved the
best score in both fluency and importance while
?Hori?? was the worst performer. Our method sig-
nificantly outperformed both ?Hori? and ?Hori??
on both metrics. Moreover, our method outper-
formed ?w/o PLM + Dep? again. However, the
differences in the scores are not significant. We
believe that this is due to a lack of data. If we use
more data for the significant test, significant dif-
ferences will be found. Although our method does
not employ any explicit syntactic information, its
fluency and importance are extremely good. This
confirms the effectiveness of the new features of
IPTW and PLM.
5.3 Comparison of decoding speed
We compare the decoding speed of our method
against that of Hori?s method.
We measured the decoding time for all 1,000
test sentences on a standard Linux Box (CPU:
Intel c? CoreTM 2 Extreme QX9650 (3.00GHz),
Memory: 8G Bytes). The results were as follows:
Proposed: 22.14 seconds
(45.2 sentences / sec),
831
Hori: 95.34 seconds
(10.5 sentences / sec).
Our method was about 4.3 times faster than
Hori?s method due to the latter?s use of depen-
dency parser. This speed advantage is significant
when on-demand sentence compression is needed.
6 Related work
Conventional sentence compression methods em-
ploy the tree trimming approach to compress a
sentence without changing its meaning. For in-
stance, most English sentence compression meth-
ods make full parse trees and trim them by ap-
plying the generative model (Knight and Marcu,
2002; Turner and Charniak, 2005), discrimina-
tive model (Knight and Marcu, 2002; Unno et
al., 2006). For Japanese sentences, instead of us-
ing full parse trees, existing sentence compression
methods trim dependency trees by the discrim-
inative model (Takeuchi and Matsumoto, 2001;
Nomoto, 2008) through the use of simple lin-
ear combined features (Oguro et al, 2002). The
tree trimming approach guarantees that the com-
pressed sentence is grammatical if the source sen-
tence does not trigger parsing error. However, as
we mentioned in Section 2, the tree trimming ap-
proach is not suitable for Japanese sentence com-
pression because in many cases it cannot repro-
duce human-produced compressions.
As an alternative to these tree trimming
approaches, sequence-oriented approaches have
been proposed (McDonald, 2006; Nomoto, 2007;
Hori and Furui, 2003; Clarke and Lapata, 2006).
Nomoto (2007) and McDonald (2006) employed
the random field based approach. Hori et al
(2003) and Clarke et al (2006) employed the lin-
ear model with simple combined features. They
simply regard a sentence as a word sequence and
structural information, such as full parse tree or
dependency trees, are encoded in the sequence as
features. The advantage of these methods over the
tree trimming approach is that they have the poten-
tial to drop arbitrary words from the original sen-
tence without the need to consider the boundaries
determined by the tree structures. This approach is
more suitable for Japanese compression than tree
trimming. However, they still rely on syntactic
information derived from full parsed trees or de-
pendency trees. Moreover, their use of syntactic
parsers seriously degrades the decoding speed.
7 Conclusions
We proposed a syntax free sequence-oriented
Japanese sentence compression method with two
novel features: IPTW and PLM. Our method
needs only a POS tagger. It is significantly supe-
rior to the methods that employ syntactic parsers.
An experiment on a Japanese news corpus re-
vealed the effectiveness of the new features. Al-
though the proposed method does not employ any
explicit syntactic information, it outperformed,
with statistical significance, Hori?s method a state-
of-the-art Japanese sentence compression method
based on the sequence-oriented approach.
The contributions of this paper are as follows:
? We revealed that in compressing Japanese
sentences, humans usually ignore syntactic
structures; they drop intermediate nodes of
the dependency tree and drop words within
bunsetsu,
? As an alternative to the syntactic parser, we
proposed two novel features, Intra-sentence
positional term weighting (IPTW) and the
Patched language model (PLM), and showed
their effectiveness by conducting automatic
and human evaluations,
? We showed that our method is about 4.3 times
faster than Hori?s method which employs a
dependency parser.
References
J. Clarke and M. Lapata. 2006. Models for sentence
compression: A comparison across domains, train-
ing requirements and evaluation measures. In Proc.
of the 21st COLING and 44th ACL, pages 377?384.
C. Hori and S. Furui. 2003. A new approach to auto-
matic speech summarization. IEEE trans. on Multi-
media, 5(3):368?378.
H. Jing and K. McKeown. 1999. The Decomposition
of Human-Written Summary Sentences. In Proc. of
the 22nd SIGIR, pages 129?136.
B. H. Juang and S. Katagiri. 1992. Discriminative
Learning for Minimum Error Classification. IEEE
Trans. on Signal Processing, 40(12):3043?3053.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction. Artificial Intelligence,
139(1):91?107.
832
T. Kudo and Y. Matsumoto. 2004. A Boosting Algo-
rithm for Classification of Semi-Structured Text. In
Proc. of the EMNLP, pages 301?308.
T. Kudo and Y. Matsumoto. 2005. Japanese De-
pendency Parsing Using Relative Preference of De-
pendency (in japanese). IPSJ Journal, 46(4):1082?
1092.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
the 18th ICML, pages 282?289.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line Large Margrin Training of Dependency Parser.
In Proc. of the 43rd ACL, pages 91?98.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proc. of
the 11th EACL, pages 297?304.
T. Nomoto. 2007. Discriminative sentence compres-
sion with conditional random fields. Information
Processing and Management, 43(6):1571?1587.
T. Nomoto. 2008. A generic sentence trimmer with
crfs. In Proc. of the ACL-08: HLT, pages 299?307.
R. Oguro, H. Sekiya, Y. Morooka, K. Takagi, and
K. Ozeki. 2002. Evaluation of a japanese sentence
compression method based on phrase significance
and inter-phrase dependency. In Proc. of the TSD
2002, pages 27?32.
K. Papineni, S. Roukos, T. Ward, and W-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistic (ACL),
pages 311?318.
K. Takeuchi and Y. Matsumoto. 2001. Acquisition
of sentence reduction rules for improving quality of
text summaries. In Proc. of the 6th NLPRS, pages
447?452.
J. Turner and E. Charniak. 2005. Supervised and un-
supervised learning for sentence compression. In
Proc. of the 43rd ACL, pages 290?297.
Y. Unno, T. Ninomiya, Y. Miyao, and J. Tsujii. 2006.
Trimming cfg parse trees for sentence compression
using machine learning approach. In Proc. of the
21st COLING and 44th ACL, pages 850?857.
833
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 341?344,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Succinct N-gram Language Model
Taro Watanabe Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan
{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Efficient processing of tera-scale text data
is an important research topic. This pa-
per proposes lossless compression of N -
gram language models based on LOUDS,
a succinct data structure. LOUDS suc-
cinctly represents a trie with M nodes as a
2M + 1 bit string. We compress it further
for the N -gram language model structure.
We also use ?variable length coding? and
?block-wise compression? to compress val-
ues associated with nodes. Experimental
results for three large-scale N -gram com-
pression tasks achieved a significant com-
pression rate without any loss.
1 Introduction
There has been an increase in available N -gram
data and a large amount of web-scaled N -gram
data has been successfully deployed in statistical
machine translation. However, we need either a
machine with hundreds of gigabytes of memory
or a large computer cluster to handle them.
Either pruning (Stolcke, 1998; Church et al,
2007) or lossy randomizing approaches (Talbot
and Brants, 2008) may result in a compact repre-
sentation for the application run-time. However,
the lossy approaches may reduce accuracy, and
tuning is necessary. A lossless approach is obvi-
ously better than a lossy one if other conditions
are the same. In addtion, a lossless approach can
easly combined with pruning. Therefore, lossless
representation of N -gram is a key issue even for
lossy approaches.
Raj and Whittaker (2003) showed a general N -
gram language model structure and introduced a
lossless algorithm that compressed a sorted integer
vector by recursively shifting a certain number of
bits and by emitting index-value inverted vectors.
However, we need more compact representation.
In this work, we propose a succinct way to
represent the N -gram language model structure
based on LOUDS (Jacobson, 1989; Delpratt et
al., 2006). It was first introduced by Jacobson
(1989) and requires only a small space close to
the information-theoretic lower bound. For an M
node ordinal trie, its information-theoretical lower
bound is 2M ? O(lg M) bits (lg(x) = log
2
(x))
1-gram 2-gram 3-gram
probability
back-off
pointer
word idprobabilityback-offpointer
word id
probability
back-off
pointer
Figure 1: Data structure for language model
and LOUDS succinctly represents it by a 2M + 1
bit string. The space is further reduced by consid-
ering the N -gram structure. We also use variable
length coding and block-wise compression to com-
press the values associated with each node, such as
word ids, probabilities or counts.
We experimented with English Web 1T 5-gram
from LDC consisting of 25 GB of gzipped raw
text N -gram counts. By using 8-bit floating point
quantization 1, N -gram language models are com-
pressed into 10 GB, which is comparable to a lossy
representation (Talbot and Brants, 2008).
2 N -gram Language Model
We assume a back-off N -gram language model in
which the conditional probability Pr(w
n
|w
n?1
1
)
for an arbitrary N -gram wn
1
= (w
1
, ..., w
n
) is re-
cursively computed as follows.
?(w
n
1
) if wn
1
exists.
?(w
n?1
1
)Pr(w
n
|w
n?1
2
) if wn?1
1
exists.
Pr(w
n
|w
n?1
2
) otherwise.
?(w
n
1
) and ?(wn
1
) are smoothed probabilities and
back-off coefficients, respectively.
The N -grams are stored in a trie structure as
shown in Figure 1. N -grams of different orders
are stored in different tables and each row corre-
sponds to a particular wn
1
, consisting of a word id
for w
n
, ?(w
n
1
), ?(w
n
1
) and a pointer to the first po-
sition of the succeeding (n + 1)-grams that share
the same prefix wn
1
. The succeeding (n+1)-grams
are stored in a contiguous region and sorted by the
word id of w
n+1
. The boundary of the region is de-
termined by the pointer of the next N -gram in the
1The compact representation of the floating point is out of
the scope of this paper. Therefore, we use the term lossless
even when using floating point quantization.
341
0
1 2 3 4
5 6 7 8 9 10
11 12 13 14 15
(a) Trie structure
node id 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32
LOUDS bit 1 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0
(b) Corresponding LOUDS bit string
0 1 2 3
4 5 6 7 8 9
10 11 12 13 14
(c) Trie structure for N -gram
node id 0 1 2 3 4 5 6 7 8 9
bit position 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
LOUDS bit 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0
(d) Corresponding N -gram optimized LOUDS bit string
Figure 2: Optimization of LOUDS bit string for N -gram data
row. When an N -gram is traversed, binary search
is performed N times. If each word id corresponds
to its node position in the unigram table, we can
remove the word ids for the first order.
Our implementation merges across different or-
ders of N -grams, then separates into multiple ta-
bles such as word ids, smoothed probabilities,
back-off coefficients, and pointers. The starting
positions of different orders are memorized to al-
low access to arbitrary orders. To store N -gram
counts, we use three tables for word ids, counts
and pointers. We share the same tables for word
ids and pointers with additional probability and
back-off coefficient tables.
To support distributed computation (Brants et
al., 2007), we further split the N -gram data into
?shards? by hash values of the first bigram. Uni-
gram data are shared across shards for efficiency.
3 Succinct N -gram Structure
The table of pointers described in the previous
section represents a trie. We use a succinct data
structure LOUDS (Jacobson, 1989; Delpratt et al,
2006) for compact representation of the trie.
For an M node ordinal trie, there exist
1
2M+1
(
2M+1
M
)
different tries. Therefore,
its information-theoretical lower bound is
lg
?
1
2M+1
(
2M+1
M
)
?
? 2M ? O(lg M) bits.
LOUDS represents a trie with M nodes as a
2M + O(M) bit string.
The LOUDS bit string is constructed as follows.
Starting from the root node, we traverse a trie in
level order. For each node with d ? 0 children, the
bit string 1d0 is emitted. In addition, 10 is prefixed
to the bit string emitted by an imaginary super-root
node pointing to the root node. Figure 2(a) shows
an example trie structure. The nodes are numbered
in level order, and from left to right. The cor-
responding LOUDS bit string is shown in Figure
2(b). Since the root node 0 has four child nodes,
it emits four 1s followed by 0, which marks the
end of the node. Before the root node, we assume
an imaginary super root node emits 10 for its only
child, i.e., the root node. After the root node, its
first child or node 1 follows. Since (M + 1)0s and
M1s are emitted for a trie with M nodes, LOUDS
occupies 2M + 1 bits.
We define a basic operation on the bit string.
sel
1
(i) returns the position of the i-th 1. We can
also define similar operations over zero bit strings,
sel
0
(i). Given sel
b
, we define two operations for
a node x. parent(x) gives x?s parent node and
firstch(x) gives x?s first child node:
parent(x) = sel
1
(x + 1) ? x ? 1, (1)
firstch(x) = sel
0
(x + 1) ? x. (2)
To test whether a child node exists, we sim-
ply check firstch(x) 6= firstch(x + 1). Sim-
ilarly, the child node range is determined by
[firstch(x),firstch(x + 1)).
3.1 Optimizing N -gram Structure for Space
We propose removing redundant bits from the
baseline LOUDS representation assuming N -
gram structures. Since we do not store any infor-
mation in the root node, we can safely remove the
root so that the imaginary super-root node directly
points to unigram nodes. The node ids are renum-
bered and the first unigram is 0. In this way, 2 bits
are saved.
The N -gram data structure has a fixed depth N
and takes a flat structure. Since the highest or-
der N -grams have no child nodes, they emit 0NN
in the tail of the bit stream, where N
n
stands for
the number of n-grams. By memorizing the start-
ing position of the highest order N -grams, we can
completely remove N
N
bits.
The imaginary super-root emits 1N10 at the be-
ginning of the bit stream. By memorizing the bi-
gram starting position, we can remove the N
1
+ 1
bits.
Finally, parent(x) and firstch(x) are rewritten as
342
integer seq. 52 156 260 364
coding 0x34 0x9c 0x01 0x04 0x01 0x6c
boundary 1 1 0 1 0 1
Figure 3: Example of variable length coding
follows:
parent(x) = sel
1
(x + 1 ?N
1
) + N
1
? x, (3)
firstch(x) = sel
0
(x) + N
1
+ 1 ? x. (4)
Figure 2(c) shows the N -gram optimized trie
structure (N = 3) from Figure 2 with N
1
= 4
and N
3
= 5. The parent of node 8 is found by
sel
1
(8+1?4) = 5 and 5+4?8 = 1. The first child
is located by sel
0
(8) = 16 and 16+4+1?8 = 13.
When accessing the N -gram data structure,
sel
b
(i) operations are used extensively. We use an
auxiliary dictionary structure proposed by Kim et
al. (2005) and Jacobson (1989) that supports an
efficient sel
1
(i) (sel
0
(i)) with the dictionary. We
omit the details due to lack of space.
3.2 Variable Length Coding
The above method compactly represents pointers,
but not associated values, such as word ids or
counts. Raj and Whittaker (2003) proposed in-
teger compression on each range of the word id
sequence that shared the same N -gram prefix.
Here, we introduce a simple but more effec-
tive variable length coding for integer sequences
of word ids and counts. The basic idea comes from
encoding each integer by the smallest number of
required bytes. Specifically, an integer within the
range of 0 to 255 is coded as a 1-byte integer,
the integers within the range of 256 to 65,535 are
stored as 2-byte integers, and so on. We use an ad-
ditional bit vector to indicate the boundary of the
byte sequences. Figure 3 presents an example in-
teger sequence, 52, 156, 260 and 364 with coded
integers in hex decimals with boundary bits.
In spite of the length variability, the system
can directly access a value at index i as bytes
in [sel
1
(i) + 1, sel
1
(i + 1) + 1) by the efficient
sel
1
operation assuming that sel
1
(0) yields ?1.
For example, the value 260 at index 2 in Figure
3 is mapped onto the byte range of [sel
1
(2) +
1, sel
1
(3) + 1) = [2, 4).
3.3 Block-wise Compression
We further compress every 8K-byte data block of
all tables in N -grams by using a generic com-
pression library, zlib, employed in UNIX gzip.
We treat a sequence of 4-byte floats in the prob-
ability table as a byte stream, and compress ev-
ery 8K-byte block. To facilitate random access to
the compressed block, we keep track of the com-
pressed block?s starting offsets. Since the offsets
are in sorted order, we can apply sorted integer
compression (Raj and Whittaker, 2003). Since N -
gram language model access preserves some local-
ity, N -gram with block compression is still practi-
cal enough to be usable in our system.
4 Experiments
We applied the proposed representation to 5-gram
trained by ?English Gigaword 3rd Edition,? ?En-
glish Web 1T 5-gram? from LDC, and ?Japanese
Web 1T 7-gram? from GSK. Since their tendencies
are the same, we only report in this paper the re-
sults on English Web 1T 5-gram, where the size
of the count data in gzipped raw text format is
25GB, the number of N-grams is 3.8G, the vocab-
ulary size is 13.6M words, and the number of the
highest order N-grams is 1.2G.
We implemented an N -gram indexer/estimator
using MPI inspired by the MapReduce imple-
mentation of N -gram language model index-
ing/estimation pipeline (Brants et al, 2007).
Table 1 summarizes the overall results. We
show the initial indexed counts and the final lan-
guage model size by differentiating compression
strategies for the pointers, namely the 4-byte raw
value (Trie), the sorted integer compression (In-
teger) and our succinct representation (Succinct).
The ?block? indicates block compression. For the
sake of implementation simplicity, the sorted in-
teger compression used a fixed 8-bit shift amount,
although the original paper proposed recursively
determined optimum shift amounts (Raj and Whit-
taker, 2003). 8-bit quantization was performed
for probabilities and back-off coefficients using a
simple binning approach (Federico and Cettolo,
2007).
N -gram counts were reduced from 23.59GB
to 10.57GB by our succinct representation with
block compression. N -gram language models of
42.65GB were compressed to 18.37GB. Finally,
the 8-bit quantized N -gram language models are
represented by 9.83GB of space.
Table 2 shows the compression ratio for the
pointer table alone. Block compression employed
on raw 4-byte pointers attained a large reduc-
tion that was almost comparable to sorted inte-
ger compression. Since large pointer value tables
are sorted, even a generic compression algorithm
could achieve better compression. Using our suc-
cinct representation, 2.4 bits are required for each
N -gram. By using the ?flat? trie structure, we
approach closer to its information-theoretic lower
bound beyond the LOUDS baseline. With block
compression, we achieved 1.8 bits per N -gram.
Table 3 shows the effect of variable length
coding and block compression for the word ids,
counts, probabilities and back-off coefficients. Af-
ter variable-length coding, the word id is almost
half its original size. We assign a word id for each
343
w/o block w/ block
Counts Trie 23.59 GB 12.21 GB
Integer 14.59 GB 11.18 GB
Succinct 12.62 GB 10.57 GB
Language Trie 42.65 GB 20.01 GB
model Integer 33.65 GB 18.98 GB
Succinct 31.67 GB 18.37 GB
Quantized Trie 24.73 GB 11.47 GB
language Integer 15.73 GB 10.44 GB
model Succinct 13.75 GB 9.83 GB
Table 1: Summary of N -gram compression
total per N -gram
4-byte Pointer 12.04 GB 27.24 bits
+block compression 2.42 GB 5.48 bits
Sorted Integer 3.04 GB 6.87 bits
+block compression 1.39 GB 3.15 bits
Succinct 1.06 GB 2.40 bits
+block compression 0.78 GB 1.76 bits
Table 2: Compression ratio for pointers
word according to its reverse sorted order of fre-
quency. Therefore, highly frequent words are as-
signed smaller values, which in turn occupies less
space in our variable length coding. With block
compression, we achieved further 1 GB reduction
in space. Since the word id sequence preserves
local ordering for a certain range, even a generic
compression algorithm is effective.
The most frequently observed count in N -gram
data is one. Therefore, we can reduce the space
by the variable length coding. Large compression
rates are achieved for both probabilities and back-
off coefficients.
5 Conclusion
We provided a succinct representation of the N -
gram language model without any loss. Our
method approaches closer to the information-
theoretic lower bound beyond the LOUDS base-
line. Experimental results showed our succinct
representation drastically reduces the space for
the pointers compared to the sorted integer com-
pression approach. Furthermore, the space of
N -grams was significantly reduced by variable
total per N -gram
word id size (4 bytes) 14.09 GB 31.89 bits
+variable length 6.72 GB 15.20 bits
+block compression 5.57 GB 12.60 bits
count size (8 bytes) 28.28 GB 64.00 bits
+variable length 4.85 GB 10.96 bits
+block compression 4.22 GB 9.56 bits
probability size (4 bytes) 14.14 GB 32.00 bits
+block compression 9.55 GB 21.61 bits
8-bit quantization 3.54 GB 8.00 bits
+block compression 2.64 GB 5.97 bits
backoff size (4 bytes) 9.76 GB 22.08 bits
+block compression 2.48 GB 5.61 bits
8-bit quantization 2.44 GB 5.52 bits
+block compression 0.85 GB 1.92 bits
Table 3: Effects of block compression
length coding and block compression. A large
amount of N -gram data is reduced from unin-
dexed gzipped 25 GB text counts to 10 GB of
indexed language models. Our representation is
practical enough though we did not experimen-
tally investigate the runtime efficiency in this pa-
per. The proposed representation enables us to
utilize a web-scaled N -gram in our MT compe-
tition system (Watanabe et al, 2008). Our suc-
cinct representation will encourage new research
on web-scaled N -gram data without requiring a
larger computer cluster or hundreds of gigabytes
of memory.
Acknowledgments
We would like to thank Daisuke Okanohara for his
open source implementation and extensive docu-
mentation of LOUDS, which helped our original
coding.
References
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine transla-
tion. In Proc. of EMNLP-CoNLL 2007.
K. Church, T. Hart, and J. Gao. 2007. Compressing
trigram language models with Golomb coding. In
Proc. of EMNLP-CoNLL 2007.
O. Delpratt, N. Rahman, and R. Raman. 2006. Engi-
neering the LOUDS succinct tree representation. In
Proc. of the 5th International Workshop on Experi-
mental Algorithms.
M. Federico and M. Cettolo. 2007. Efficient handling
of n-gram language models for statistical machine
translation. In Proc. of the 2nd Workshop on Statis-
tical Machine Translation.
G. Jacobson. 1989. Space-efficient static trees and
graphs. In 30th Annual Symposium on Foundations
of Computer Science, Nov.
D. K. Kim, J. C. Na, J. E. Kim, and K. Park. 2005. Ef-
ficient implementation of rank and select functions
for succinct representation. In Proc. of the 5th Inter-
national Workshop on Experimental Algorithms.
B. Raj and E. W. D. Whittaker. 2003. Lossless com-
pression of language model structure and word iden-
tifiers. In Proc. of ICASSP 2003, volume 1.
A. Stolcke. 1998. Entropy-based pruning of backoff
language models. In Proc. of the ARPA Workshop
on Human Language Technology.
D. Talbot and T. Brants. 2008. Randomized language
models via perfect hash functions. In Proc. of ACL-
08: HLT.
T. Watanabe, H. Tsukada, and H. Isozaki. 2008. NTT
SMT system 2008 at NTCIR-7. In Proc. of the 7th
NTCIR Workshop, pages 420?422.
344
Japanese Zero Pronoun Resolution based on
Ranking Rules and Machine Learning
Hideki Isozaki and Tsutomu Hirao
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
2-4 Hikaridai, Seika-cho, Souraku-gun, Kyoto, Japan, 619-0237
(isozaki,hirao)@cslab.kecl.ntt.co.jp
Abstract
Anaphora resolution is one of the most
important research topics in Natural Lan-
guage Processing. In English, overt pro-
nouns such as she and definite noun
phrases such as the company are anaphors
that refer to preceding entities (an-
tecedents). In Japanese, anaphors are of-
ten omitted, and these omissions are called
zero pronouns. There are two major ap-
proaches to zero pronoun resolution: the
heuristic approach and the machine learn-
ing approach. Since we have to take var-
ious factors into consideration, it is diffi-
cult to find a good combination of heuris-
tic rules. Therefore, the machine learn-
ing approach is attractive, but it requires
a large amount of training data. In this
paper, we propose a method that com-
bines ranking rules and machine learning.
The ranking rules are simple and effective,
while machine learning can take more fac-
tors into account. From the results of our
experiments, this combination gives better
performance than either of the two previ-
ous approaches.
1 Introduction
Anaphora resolution is an important research topic
in Natural Language Processing. For instance,
machine translation systems should identify an-
tecedents of anaphors (such as he or she) in the
source language to achieve better translation quality
in the target language.
We are now studying open-domain question an-
swering systems1, and we expect QA systems to
benefit from anaphora resolution. Typical QA sys-
tems try to answer a user?s question by finding rel-
evant phrases from large corpora. When a correct
answer phrase is far from the keywords given in
the question, the systems will not succeed in find-
ing the answer. If the system can correctly resolve
anaphors, it will find keywords or answers repre-
sented by anaphors, and the chances of finding the
answer will increase. From this motivation, we are
developing our system toward the ability to resolve
anaphors in full-text newspaper articles.
In Japanese, anaphors are often omitted and these
omissions are called zero pronouns. Since they do
not give any hints (e.g., number or gender) about an-
tecedents, automatic zero pronoun resolution is dif-
ficult. In this paper, we focus on resolving the zero
pronoun, which is shortened for simplicity to ?zero.?
Most studies on Japanese zero pronoun resolution
have not tried to resolve zeros in full-text newspa-
per articles. They have discussed simple sentenses
(Kameyama, 1986; Walker et al, 1994; Yamura-
Takei et al, 2002), dialogues (Yamamoto et al,
1997), stereotypical lead sentences of newspaper ar-
ticles (Nakaiwa and Ikehara, 1993), intrasentential
resolution (Nakaiwa and Ikehara, 1996; Ehara and
Kim, 1996) or organization names in newspaper ar-
ticles (Aone and Bennett, 1995).
There are two approaches to the problem: the
heuristic approach and the machine learning ap-
1http://trec.nist.gov/data/qa.html
proach. The Centering Theory (Grosz et al, 1995)
is important in the heuristic approach. Walker
et al (1994) proposed forward center ranking for
Japanese. Kameyama (1986) emphasized the im-
portance of a property-sharing constraint. Okumura
and Tamura (1996) experimented on the roles of
conjunctive postpositions in complex sentences.
However, these improvements are not sufficient
for resolving zeros accurately. Murata and Na-
gao (1997) proposed complicated heuristic rules that
take various features of antecedents and anaphors
into account. We have to take even more factors into
account, but it is difficult to maintain such heuris-
tic rules. Therefore, recent studies employ machine
learning approaches. However, it is also difficult to
prepare a sufficient number of annotated corpora.
In this paper, we propose a method that com-
bines these two approaches. Heuristic ranking rules
give a general preference, while a machine learn-
ing method excludes inappropriate antecedent can-
didates. From the results of our experiments, the
proposed method shows better performance than ei-
ther of the two approaches alone.
Before giving a description of our methodology,
we briefly introduce the grammar of the Japanese
language here. A Japanese sentence is a sequence
of bunsetsus:
  
	
. A bunsetsu is a se-
quence of content words (e.g., nouns, adjectives,
and verbs) followed by zero or more functional
words (e.g., particles and auxiliary verbs):  







. A bunsetsu modifies one of
the following bunsetsus. A particle (joshi) marks the
grammatical case of the noun phrase immediately
before it. For example, ga is nominative (subject),
wo is accusative (object), ni is dative (object2), and
wa marks a topic.
Tomu ga
Tom=subj
/ Bobu ni
Bob=object2
/ hon wo
book=object
/ okutta.
sent
(Tom sent a book to Bob.)
Bunsetsu dependency is represented by a list of
bunsetsu pairs (modifier, modified). For instance,

ffEvaluation Measures Considering Sentence Concatenation for
Automatic Summarization by Sentence or Word Extraction
Chiori Hori, Tsutomu Hirao and Hideki Isozaki
NTT Communication Science Laboratories
{chiori, hirao, isozaki}@cslab.kecl.ntt.co.jp
Abstract
Automatic summaries of text generated through
sentence or word extraction has been evaluated by
comparing them with manual summaries generated
by humans by using numerical evaluation measures
based on precision or accuracy. Although sentence
extraction has previously been evaluated based only
on precision of a single sentence, sentence concate-
nations in the summaries should be evaluated as
well. We have evaluated the appropriateness of sen-
tence concatenations in summaries by using eval-
uation measures used for evaluating word concate-
nations in summaries through word extraction. We
determined that measures considering sentence con-
catenation much better reflect the human judgment
rather than those based only on the precision of a
single sentence.
1 Introduction
Summarization Target and Approach
The amount of text is explosively increasing day by
day, and it is becoming very difficult to manage in-
formation by reading all the text. To manage infor-
mation easily and find target information quickly,
we need technologies for summarizing text. Al-
though research into text summarization started in
the 1950?s, it is still largely in the research phase
(Mani and Maybury, 1999). Several projects on
text summarization have been carried out. 1 In
these project, text summarization has so far focused
on summarizing single documents through sentence
extraction. Recently, summarizing multiple docu-
ments with the same topic has been made a tar-
get. The major approach to extracting sentences that
have significant information is statistical, i.e., su-
pervised learning from parallel corpora consisting
of original texts and their summarization (Kupiec et
1SUMMAC in the Tipster project by DARPA (http://www-
nlpir.nist.gov/related projects/tipster summac) and DUC in
the TIDES project (http://duc.nist.gov/) in the U.S. TSC
(http://research.nii.ac.jp/ntcir/) in the NTCIR by NII (The Na-
tional Institute of Informatica) in Japan.
al., 1995) (Aone et al, 1998) (Mani and Bloedorn,
1998).
Several summarization techniques for multime-
dia including image, speech, and text have been re-
searched. Manually transcribed newswire speech
(TDT data) and meeting speech (Zechner, 2003)
have been set as summarization targets. The need
to automatically generate summaries from speech
has led to research on summarizing transcription re-
sults obtained by automatic speech recognition in-
stead of manually transcribed speech (Hori and Fu-
rui, 2000a). This summarization approach is word
extraction (sentence compaction) that attempts to
extract significant information, exclude acoustically
and linguistically unreliable words, and maintain
the meanings of the original speech.
The summarization approaches that have been
mainly researched so far are extracting sentences
or words from original text or transcribed speech.
There has also been research on generating an ?ab-
stract? like the much higher level summarization
composed freely by human experts (Jing, 2002).
This approach includes not only extracting sen-
tences but also combining sentences to generate new
sentences, replacing words, reconstructing syntactic
structure, and so on.
Evaluation Measures for Summarization
Metrics that can be used to accurately evaluate
the various appropriateness to summarization are
needed.The simplest and probably the ideal way of
evaluating automatic summarization is to have hu-
man subjects read the summaries and evaluate them
in terms of the appropriateness of summarization.
However, this type of evaluation is too expensive
for comparing the efficiencies of many different ap-
proaches precisely and repeatedly. We thus need au-
tomatic evaluation metrics to numerically validate
the efficiency of various approaches repeatedly and
consistently.
Automatic summaries can be evaluated by com-
paring them with manual summaries generated by
humans. The similarities between the targets and
the automatically processed results provide metrics
indicating the extent to which the task was accom-
plished. The similarity that can better reflect sub-
jective judgments is a better metric.
To create correct answers for automatic sum-
marization, humans generate manual summaries
through sentence or word extraction. However,
references consisting of manual summaries vary
among humans. The problems in validating auto-
matic summaries by comparing them with various
references are as follows:
? correct answers for automatic results cannot be
unified because of subjective variation,
? the coverage of correct answers in the collected
manual summaries is unknown, and
? the reliability of references in the collected
manual summaries is not always guaranteed.
When the similarity between automatic results
and references is used for the evaluation metrics,
the similarity determination function counts over-
lapping of each component or sequence of com-
ponents in the automatic results. If concatenations
between components in a summary had no mean-
ing, the overlap of a single component between the
automatic results and the references can represent
the extent of summarization. However, concatena-
tions between sentences or words have meanings,
so some concatenations of sentences or words in the
automatic summaries sometimes generate meanings
different from the original. The evaluation metrics
for summarization should thus consider each con-
catenation between components in the automatic re-
sults.
To evaluate sentence automatically generated
with taking consideration word concatenation into
by using references varied among humans, vari-
ous metrics using n-gram precision and word ac-
curacy have been proposed: word string preci-
sion (Hori and Furui, 2000b) for summarization
through word extraction, ROUGE (Lin and Hovy,
2003) for abstracts, and BLEU (Papineni et al,
2002) for machine translation. Evaluation metrics
based on word accuracy, summarization accuracy
(SumACCY), using a word network made by merg-
ing manual summaries has been proposed (Hori and
Furui, 2001). In addition, to solve the problems for
the coverage of correct answers and the reliability
of manual summaries as correct answers, weighted
summarization accuracy (WSumACCY) in which
SumACCY is weighted by the majority of the hu-
mans? selections, has been proposed (Hori and Fu-
rui, 2003a).
In contrast, summarization through sentence ex-
traction has been evaluated using only single sen-
tence precision. Sentence extraction should also be
evaluated using measures that take into account sen-
tence concatenations, the coverage of correct an-
swers, and the reliability of manual summaries.
This paper presents evaluation results of auto-
matic summarization through sentence or word ex-
traction using the above mentioned metrics based on
n-gram precision and sentence/word accuracy and
examines how well these measures reflect the judg-
ments of humans as well.
2 Evaluation Metrics for Extraction
In summarization through sentence or word extrac-
tion under a specific summarization ratio, the order
of the sentences or words and the length of the sum-
maries are restricted by the original documents or
sentences. Metrics based on the accuracy of the
components in the summary is a straight-forward
approach to measuring similarities between the tar-
get and automatic summaries.
2.1 Accuracy
In the field of speech recognition, automatic recog-
nition results are compared with manual transcrip-
tion results. The conventional metric for speech
recognition is recognition accuracy calculated based
on word accuracy:
ACCY
= Len ? (Sub + Ins + Del)Len ? 100[%], (1)
where Sub, Ins, Del, and Len are the numbers
of substitutions, insertions, deletions, and words in
the manual transcription, respectively. Although
word accuracy cannot be used to directly evaluate
the meanings of sentences, higher accuracy indi-
cates that more of the original information has been
preserved. Since the meaning of the original doc-
uments is generated by combining sentences, this
metric can be applied to the evaluation for sentence
extraction. Sentence accuracy defined by eq. (1)
with words replaced by sentences represents how
much the automatic result is similar to the answer
and how well it preserves the original meaning.
Accuracy is the simplest and most efficient metric
when the target for the automatic summaries can be
set as only one answer. However, there are usually
multiple targets for each automatic summary due to
the variation in manual summarization among hu-
mans. Therefore, it is not easy to use accuracy to
evaluate automatic summaries. Subjective variation
results into two problems:
? how to consider all possible correct answers in
the manual summaries, and
? how to measure the similarity between the
evaluation sentence and multiple manual sum-
maries.
If we could collect all possible manual sum-
maries, the one most similar to the automatic re-
sult could be chosen as the correct answer and used
for the evaluation. The sentence or word accuracy
compared with the most similar manual summary is
denoted as NrstACCY. However, in real situations,
the number of manual summaries that could be col-
lected is limited. The coverage of correct answers in
the collected manual summaries is unknown. When
the coverage is low, the summaries are compared
with inappropriate targets, and the NrstACCY ob-
tained by such comparison does not provide an effi-
cient measure.
2.2 N-gram Precision
One way to cope with the coverage problem is to
use local matching of components or component
strings with all the manual summaries instead of
using a measure comparing a word sequence as a
whole sentence, such as NrstACCY. The similar-
ity can be measured by counting the precision, i.e.,
the number of sentence or word n-gram overlapping
between the automatic result and all the references.
Even if there are multiple targets for an automatic
summary, the precision of components in each orig-
inal can be used to evaluate the similarity between
the automatic result and the multiple references.
Precision is an efficient way of evaluating the sim-
ilarity of component occurrence between automatic
results and targets with a different order of compo-
nents and different lengths.
In the evaluation of summarization through ex-
traction, a component occurring in a different loca-
tion in the original is considered to be a different
component even if it is the same component as one
in the result. When an answer for the automatic re-
sult can be unified and the lengths of the automatic
result and its answer are the same, accuracy counts
insertion errors and deletion errors and thus has both
the precision and recall characteristics.
Since meanings are basically conveyed by word
strings rather than single words, word string preci-
sion (Hori and Furui, 2000b) can be used to evalu-
ate linguistic precision and the maintenance of the
original meanings of an utterance. In this method,
word strings of various lengths, that is n-grams, are
used as components for measuring precision. The
extraction ratio, pn, of each word string consist-
ing of n words in a summarized sentence, V =
v1, v2, . . . , vM , is given by
pn =
M
?
m=n
?(vm?n+1, . . . , vm?1, vm)
M ? n + 1 , (2)
where
?(un) =
{ 1 if un ? Un
0 if un /? Un , (3)
un: each word string consisting of n words
Un: a set of word strings consisting of n words
in all manual summarizations.
When n is 1, pn corresponds to the precision of
each word, and when n is the same length as a
summarized sentence (n = M ), pn indicates the
precision of the summarized sentence itself.
2.3 Summarization Accuracy: SumACCY
Summarization accuracy (SumACCY) was pro-
posed to cope with the problem of correct answer
coverage and various references among humans
(Hori and Furui, 2001). To cover all possible correct
answers for summarization using a limited number
of manual summaries, all the manual summaries
are merged into a word network. In this evaluation
method, the word sequence in the network closest to
the evaluation word sequence is considered to be the
target answer. The word accuracy of the automatic
result is calculated in comparison with the target an-
swer extracted from the network.
Since summarization is processed by extracting
words from an original; the words cannot be re-
placed by other words, and the order of words can-
not be changed. Multiple manual summaries can
be combined into a network that represents the vari-
ations. Each set of words that could be extracted
from the network consists of words and word strings
occurring at least once in all the manual summaries.
The network made by the manual summaries can
be considered to represent all possible variations of
correct summaries.
SUB The beautiful cherry blossoms in Japan bloom in spring
A The cherry blossoms in Japan
B cherry blossoms in Japan bloom
C beautiful cherry bloom in spring
D beautiful cherry blossoms in spring
E The beautiful cherry blossoms bloom
Table 1: Example of manual summarization by sen-
tence compaction
<s> </s> The beautiful cherry blossoms inJapan bloomin spring
Figure 1: Word network made by merging manual
summaries
The sentence ?The beautiful cherry blossoms in
Japan bloom in spring.? is assumed to be manually
summarized as shown in Table 1. In this example,
five words are extracted from the nine words. There-
fore, the summarization ratio is 56%. The variations
of manual summaries are merged into a word net-
work, as shown in Fig. 1. We use <s> and </s>
as the beginning and ending symbols of a sentence.
Although ?Cherry blossoms bloom in spring? is not
among the manual answers in Table 1, this sentence,
which could be extracted from the network, is con-
sidered a correct answer.
When references consisting of manual sum-
maries cannot cover all possible answers and lack
the appropriate answer for an automatic summary,
SumACCY calculated using such a network is bet-
ter than NrstACCY for evaluating the automatic re-
sult. This evaluation method gives a penalty for
each word concatenation in the automatic results
that is excluded in the network, so it can be used
to evaluate the sentence-level appropriateness more
precisely than matching each word in all the refer-
ences.
2.4 Weighted SumACCY: WSumACCY
In SumACCY, all possible sets of words extracted
from the network of manually summarized sen-
tences are equally used as target answers. How-
ever, the set of words containing word strings se-
lected by many humans would presumably be better
and give more reliable answers. To obtain reliability
that reflects the majority of selections by humans,
the summarization accuracy is weighted by a pos-
terior probability based on the manual summariza-
tion network. The reliability of a sentence extracted
from the network is defined as the product of the
ratios of the number of subjects who selected each
word to the total number of subjects. The weighted
summarization accuracy is given by
WSumACCY
= P? (v1 . . . vM |R) ? SumACCY
P? (v?1 . . . v?M? |R)
, (4)
where P? (v1 . . . vM |R) is the reliability score of a
set of words v1 . . . vM in the manual summariza-
tion network, R, and M represents the total num-
ber of words in the target answer. The set of words
v?1 . . . v?M? represents the word sequence that maxi-
mizes the reliability score, P? (?|R), given by
P? (v1 . . . vM |R)
=
( M
?
m=2
C(vm?1, vm|R)
HR
)
1
M?1
, (5)
where vm is the m-th word in the sentence ex-
tracted from the network as the target answer, and
C(x, y|R) indicates the number of subjects who se-
lected the word connection of x and y. Here, ?word
connection? means an arc in the manual summariza-
tion network. HR is the number of subjects.
2.5 Evaluation Experiments
Newspaper articles and broadcast news speech were
automatically summarized through sentence extrac-
tion and word extraction respectively under the
given summarization ratio, which is the ratio of the
numbers of sentences or words in the summary to
that in the original.
The automatic summarization results were sub-
jectively evaluated by ten human subjects. The sub-
jects read these summaries and rated each one from
1 (incorrect) to 5 (perfect). The automatic sum-
maries were also evaluated by using the numerical
metrics SumACCY, WSumACCY, NrstACCY,
and n-gram precision (1 ? n ? 5) in compari-
son with reference summaries generated by humans.
The precisions of 1-gram, . . ., 5-gram are denoted
PREC1, . . ., PREC5. The numerical evaluation re-
sults were averaged over the number of automatic
summaries.
Note that the subjects who judged the automatic
summaries did not include anyone who generated
the references. To examine the similarity of the hu-
man judgments and that of the manual summaries,
the kappa statistics, ?, was calculated using eq. (A-
1) in the Appendix.
Finally, to examine how much the evaluation
measures reflected the human judgment, the correla-
tion coefficients between the human judgments and
the numerical evaluation results were calculated.
Sentence extraction
Sixty articles in Japanese newspaper published in
94, 95, and 98 were automatically summarized with
a 30% summarization ratio. Half the articles were
general news report (NEWS), and other half were
columns (EDIT).
The automatic summarization was performed us-
ing a Support Vector Machine (SVM) (Hirao et al,
2003), random extraction (RDM), the lead method
(LEAD) extracting sentences from the head of ar-
ticles. In comparison with these automatic sum-
maries, manual summaries (TSC) was also evalu-
ated.
These 4 types of summaries, SVM, RDM, LEAD,
and TSC were read and rated 1 to 5 by 10 humans.
The summaries were evaluated in terms of extrac-
tion of significance information (SIG), coherence
of sentences (COH), maintenance of original mean-
ings (SEM), and appropriateness of summary as a
whole (WHOLE).
To numerically evaluate the results using the ob-
jective metrics, 20 other human subjects gener-
ated manual summaries through sentence extrac-
tion. These manual summaries were set as the target
set for the automatic summaries.
Word extraction
Japanese TV news broadcasts aired in 1996 were
automatically recognized and summarized sentence
by sentence (Hori and Furui, 2003b). They con-
sisted of 50 utterances by a female announcer. The
out-of-vocabulary (OOV) rate for the 20k word vo-
cabulary was 2.5%, and the test-set perplexity was
54.5. Fifty utterances with word recognition accu-
racy above 90%, which was the average rate over the
50 utterances, were selected and used for the evalu-
ation. The summarization ratio was set to 40%.
Nine automatic summaries with various summa-
rization accuracies from 40% to 70% and a manual
summary (SUB) were selected as a test set. These
ten summaries for each utterance were judged in
terms of the appropriateness of the summary as a
whole (WHOLE).
To numerically evaluate the results using the ob-
jective metrics, 25 humans generated manual sum-
maries through word extraction. These manual
summaries were set as a target set for the automatic
summaries, and merged into a network. Note that a
set of 24 manual summaries made by other subjects
was used as the target for SUB.
2.6 Evaluation Results
Figures 2 and 3 show the correlation coefficients
between the judgments of the subjects and the nu-
merical evaluation results for EDIT and NEWS.
They show that the measures based on accuracy
much better reflected human judgments than those
of the n-gram precisions for evaluating SIG and
WHOLE for both EDIT and NEWS. On the other
hand, PREC2 better reflected the human judgments
for evaluating COH and SEM. These results show
that measures taking into account sentence concate-
nations better reflected human judgments than sin-
gle component precision. The precisions of longer
0
0.2
0.4
0.6
0.8
1.0
SIG COH SEM WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
Evaluation point
SumACCY
WSumACCY
  NrstACCY
PREC1
PREC2
PREC3
PREC4
PREC5
Figure 2: Correlation coefficients between human
judgment and numerical evaluation results for EDIT
0
0.2
0.4
0.6
0.8
1.0
SIG COH SEM WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
Evaluation point
SumACCY
WSumACCY
  NrstACCY
PREC1
PREC2
PREC3
PREC4
PREC5
Figure 3: Correlation coefficients between hu-
man judgment and numerical evaluation results for
NEWS
sentence strings (PREC3 to PREC5) didn?t reflect
the human judgments for all the conditions. These
results show that meanings of the original article can
maintain by the concatenations of only a few sen-
tences in summarization through sentence extrac-
tion.
Table 2 lists the kappa statistics for the manual
summaries and the human judgments for EDIT and
NEWS. The manual results varied among humans
DATA SUMMARIES ?
EDIT manual summaries 0.35
NEWS manual summaries 0.39
Table 2: Kappa statistics for manual summaries and
human judgments for sentence extraction.
and the similarity among humans was low. The
kappa statistics for NEWS is slightly higher than
that for EDIT. The difference of similarities among
manual summaries is due to the difference in struc-
tures of information in each article. Although the
articles in EDIT had a discourse structure, NEWS
had isolated and stereotyped information scattered
throughout the articles.
While the human judgments for NEWS were sim-
ilar, those for EDIT varied.The difficulty in evaluat-
ing COH and SEM in EDIT is due to the variation
in both manual summaries and human judgment.
Figure 4 shows the correlation coefficients be-
tween the judgments of the subjects and the numer-
ical evaluation results for summaries of broadcast
news speech through word extraction. Table 3 lists
0
0.2
0.4
0.6
0.8
1
WHOLE
C
or
re
la
tio
n 
co
ef
fic
ie
nt
s
Evaluation point
PREC1
PREC2
PREC3
PREC4
PREC5
SumACCY
WSumACC
NrstACCY  
Figure 4: Correlation coefficients between human
judgment and numerical evaluation results for sum-
maries through word extraction
the kappa statistics for the manual summaries and
the human judgments for summaries through word
extraction. In word extraction, the human judg-
DATA SUMMARIES ?
Broadcast news manual summaries 0.47
Table 3: Kappa statistics for manual summaries and
human judgments for word extraction
ments and the manual summaries were very similar
among the subjects.
As shown in figure 4, WSumACCY yielded the
best correlation to the human judgments. This
means that the correctness as a sentence and the
weight (that is how many subjects support the ex-
tracted phrases in summarized sentences) are im-
portant in summarization through word extraction.
In comparison with the results of sentence extrac-
tion in Figures 2 and 3, PREC1 effectively reflected
the human judgments for word extraction. Since in
the manual summarized sentences through word ex-
traction under the low summarization ratio, the sen-
tences were summarized based on significance word
extraction rather than syntactic structure mainte-
nance to generate grammatically correct sentences.
3 Conclusion
We have presented the results of evaluating
the appropriateness of the sentence concatena-
tions in summaries generated using SumACCY,
WSumACCY, NrstACCY and n-gram precision.
We found that the measures taking into account sen-
tence concatenation much better reflected the judg-
ments of humans than did the single sentence pre-
cision, so the concatenation of sentences in sum-
maries should be evaluated.
Although the human judgments and the man-
ual summaries for word extraction did not vary
much among the subjects, those for sentence extrac-
tion for single article summarization greatly varied
among the subjects. As a result, it is very difficult to
set correct answers for single article summarization
through sentence extraction.
Future works involves experiments to examine
the efficiency of each numerical measures in re-
sponse to the coverage of correct answers.
4 Acknowledgments
We thank NHK (Japan Broadcasting Corporation)
for providing the broadcast news database. We
also thank Prof. Sadaoki Furui at Tokyo Institute
of Technology for providing the summaries of the
broadcast news speech.
References
C. Aone, M. Okurowski, and J. Gorlinsky. 1998.
Trainable scalable summarization using robust
NLP and machine learning. In Proceedings ACL,
pages 62?66.
J. Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
T. Hirao, K. Takeuchi, H. Isozaki, Y. Sasaki, and
E. Maeda. 2003. SVM-based multi-document
summarization integrating sentence extraction
with bunsetsu elimination. IEICE Trans. Inf. &
Syst., E86-D(9):1702?1709.
C. Hori and S. Furui. 2000a. Automatic speech
summarization based on word significance and
linguistic likelihood. In Proceedings ICASSP,
volume 3, pages 1579?1582.
C. Hori and S. Furui. 2000b. Improvements in
automatic speech summarization and evaluation
methods. In Proceedings ICSLP, volume 4,
pages 326?329.
C. Hori and S. Furui. 2001. Advances in auto-
matic speech summarization. In Proceedings Eu-
rospeech, volume 3, pages 1771?1774.
C. Hori and S. Furui. 2003a. Evaluation methods
for automatic speech summarization. In Proceed-
ings Eurospeech, pages 2825?2828.
C. Hori and S. Furui. 2003b. A new approach to
automatic speech summarization. IEEE Transac-
tions on Multimedia, 3:368?378.
H. Jing. 2002. Using hidden markov modeling to
decompose human-written summaries. Compu-
tational Linguistics, 28(4):527?543.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings of
the 18th ACM-SIGIR, pages 68?73.
Chin-Yew Lin and E. H. Hovy. 2003. Auto-
matic evaluation of summaries using n-gram
co-occurrence statistics. In Proceedings HLT-
NAACL.
I. Mani and E. Bloedorn. 1998. Machine learning
of general and user-focused summarization. In
Proceedings of the 15th National Conference on
Artificial Intelligence, pages 821?826.
I. Mani and M. Maybury. 1999. Advances in Auto-
matic Text Summarization. The MIT Press.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings ACL.
K. Takeuchi and Y. Matsumoto. 2001. Relation be-
tween text structure and linguistic clues: An in-
vestigation on text structure of newspaper arti-
cles. Mathematical Linguistics, 22(8).
K. Zechner. 2003. Automatic summarization of
open-domain multiparty dialogues in diverse
genres. Computational Linguistics, 28(4):447?
485.
Appendix
? is given by
? = P (A) ? P (E)1 ? P (E) , (A-1)
where P (A) and P (E) are the probabilities of hu-
man agreement and chance agreement, respectively,
so ? is adjusted by the possibility of chance agree-
ment. This measure was used to assess agreement of
human selections for discourse segmentation (Car-
letta, 1996).
In this study, kappa was calculated using a table
of objects and categories (Takeuchi and Matsumoto,
2001). P (A) was calculated using
P (A) = 1N
N
?
i=1
Si, (A-2)
where N is the number of trials to select one class
among all classes, and Si is the probability that two
humans at least agree at the i-th selection:
Si =
m
?
j=1
nij C2
kC2
, (A-3)
where k and m are the number of subjects and
classes, respectively. When the task is sentence or
word extraction, the number of classes is two, i.e.,
extract/not extract. The numerator of eq. (A-3)
shows the sum of the combinations that two humans
at least agree for each class; nij is the number of hu-
mans who select the j-th class at the i-th selection.
P (E) is the probability of chance agreement by
at least two humans:
P (E) =
m
?
j=1
pj2, (A-4)
where pj is the probability of selecting the j-th class
given by
Pj =
N
?
i=1
nij
Nk , (A-5)
where the total number of humans who select the j-
th class for each trial is divided by the total number
of trials performed by all humans.
Proceedings of the Workshop on Statistical Machine Translation, pages 122?125,
New York City, June 2006. c?2006 Association for Computational Linguistics
NTT System Description for the WMT2006 Shared Task
Taro Watanabe Hajime Tsukada Hideki Isozaki
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun,
Kyoto, Japan 619-0237
{taro,tsukada,isozaki}@kecl.ntt.co.jp
Abstract
We present two translation systems ex-
perimented for the shared-task of ?Work-
shop on Statistical Machine Translation,?
a phrase-based model and a hierarchical
phrase-based model. The former uses a
phrasal unit for translation, whereas the
latter is conceptualized as a synchronous-
CFG in which phrases are hierarchically
combined using non-terminals. Experi-
ments showed that the hierarchical phrase-
based model performed very comparable
to the phrase-based model. We also report
a phrase/rule extraction technique differ-
entiating tokenization of corpora.
1 Introduction
We contrasted two translation methods for the
Workshop on Statistical Machine Translation
(WMT2006) shared-task. One is a phrase-based
translation in which a phrasal unit is employed
for translation (Koehn et al, 2003). The other is
a hierarchical phrase-based translation in which
translation is realized as a set of paired production
rules (Chiang, 2005). Section 2 discusses those two
models and details extraction algorithms, decoding
algorithms and feature functions.
We also explored three types of corpus pre-
processing in Section 3. As expected, different
tokenization would lead to different word align-
ments which, in turn, resulted in the divergence
of the extracted phrase/rule size. In our method,
phrase/rule translation pairs extracted from three
distinctly word-aligned corpora are aggregated into
one large phrase/rule translation table. The experi-
ments and the final translation results are presented
in Section 4.
2 Translation Models
We used a log-linear approach (Och and Ney,
2002) in which a foreign language sentence f J1 =f1, f2, ... fJ is translated into another language, i.e.
English, eI1 = e1, e2, ..., eI by seeking a maximum
likelihood solution of
e?I1 = argmax
eI1
Pr(eI1| f J1 ) (1)
= argmax
eI1
exp
(
?M
m=1 ?mhm(eI1, f J1 )
)
?
e? I
?
1
exp
(
?M
m=1 ?mhm(e? I
?
1 , f J1 )
)(2)
In this framework, the posterior probability
Pr(eI1| f J1 ) is directly maximized using a log-linear
combination of feature functions hm(eI1, f J1 ), such
as a ngram language model or a translation model.
When decoding, the denominator is dropped since it
depends only on f J1 . Feature function scaling factors
?m are optimized based on a maximum likelihood
approach (Och and Ney, 2002) or on a direct error
minimization approach (Och, 2003). This modeling
allows the integration of various feature functions
depending on the scenario of how a translation is
constituted.
In a phrase-based statistical translation (Koehn
et al, 2003), a bilingual text is decomposed as K
phrase translation pairs (e?1, ?fa?1), (e?2, ?fa?2 ), ...: The in-
put foreign sentence is segmented into phrases ?f K1 ,
122
mapped into corresponding English e?K1 , then, re-
ordered to form the output English sentence accord-
ing to a phrase alignment index mapping a?.
In a hierarchical phrase-based translation (Chi-
ang, 2005), translation is modeled after a weighted
synchronous-CFG consisting of production rules
whose right-hand side is paired (Aho and Ullman,
1969):
X ? ??, ?,??
where X is a non-terminal, ? and ? are strings of ter-
minals and non-terminals. ? is a one-to-one corre-
spondence for the non-terminals appeared in ? and
?. Starting from an initial non-terminal, each rule
rewrites non-terminals in ? and ? that are associated
with ?.
2.1 Phrase/Rule Extraction
The phrase extraction algorithm is based on those
presented by Koehn et al (2003). First, many-
to-many word alignments are induced by running
a one-to-many word alignment model, such as
GIZA++ (Och and Ney, 2003), in both directions
and by combining the results based on a heuristic
(Och and Ney, 2004). Second, phrase translation
pairs are extracted from the word aligned corpus
(Koehn et al, 2003). The method exhaustively ex-
tracts phrase pairs ( f j+mj , ei+ni ) from a sentence pair
( f J1 , eI1) that do not violate the word alignment con-
straints a.
In the hierarchical phrase-based model, produc-
tion rules are accumulated by computing ?holes? for
extracted contiguous phrases (Chiang, 2005):
1. A phrase pair ( ?f , e?) constitutes a rule:
X ?
?
?f , e?
?
2. A rule X ? ??, ?? and a phrase pair ( ?f , e?) s.t.
? = ?? ?f??? and ? = ??e???? constitutes a rule:
X ?
?
?? X k ?
??, ?? X k ?
??
?
2.2 Decoding
The decoder for the phrase-based model is a left-to-
right generation decoder with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words. The decoding process is very
similar to those described in (Koehn et al, 2003):
It starts from an initial empty hypothesis. From an
existing hypothesis, new hypothesis is generated by
consuming a phrase translation pair that covers un-
translated foreign word positions. The score for the
newly generated hypothesis is updated by combin-
ing the scores of feature functions described in Sec-
tion 2.3. The English side of the phrase is simply
concatenated to form a new prefix of English sen-
tence.
In the hierarchical phrase-based model, decoding
is realized as an Earley-style top-down parser on the
foreign language side with a beam search strategy
synchronized with the cardinality of already trans-
lated foreign words (Watanabe et al, 2006). The ma-
jor difference to the phrase-based model?s decoder is
the handling of non-terminals, or holes, in each rule.
2.3 Feature Functions
Our phrase-based model uses a standard pharaoh
feature functions listed as follows (Koehn et al,
2003):
? Relative-count based phrase translation proba-
bilities in both directions.
? Lexically weighted feature functions in both di-
rections.
? The supplied trigram language model.
? Distortion model that counts the number of
words skipped.
? The number of words in English-side and the
number of phrases that constitute translation.
For details, please refer to Koehn et al (2003).
In addition, we added three feature functions to
restrict reorderings and to represent globalized in-
sertion/deletion of words:
? Lexicalized reordering feature function scores
whether a phrase translation pair is monotoni-
cally translated or not (Och et al, 2004):
hlex(a?K1 | ?f K1 , e?K1 ) = log
K
?
k=1
pr(?k | ?fa?k , e?k) (3)
where ?k = 1 iff a?k ? a?k?1 = 1 otherwise ?k = 0.
? Deletion feature function penalizes words that
do not constitute a translation according to a
123
Table 1: Number of word alignment by different preprocessings.
de-en es-en fr-en en-de en-es en-fr
lower 17,660,187 17,221,890 16,176,075 17,596,764 17,237,723 16,220,520
stem 17,110,890 16,601,306 15,635,900 17,052,808 16,597,274 15,658,940
prefix4 16,975,398 16,540,767 15,610,319 16,936,710 16,530,810 15,613,755
intersection 12,203,979 12,677,192 11,645,404 12,218,997 12,688,773 11,653,242
union 23,186,379 21,709,212 20,760,539 23,066,052 21,698,267 20,789,570
Table 2: Number of phrases extracted from differently preprocessed corpora.
de-en es-en fr-en en-de en-es en-fr
lower 37,711,217 61,161,868 56,025,918 38,142,663 60,619,435 55,198,497
stem 46,550,101 75,610,696 68,210,968 46,749,195 75,473,313 67,733,045
prefix4 53,429,522 78,193,818 70,514,377 53,647,033 78,223,236 70,378,947
merged 80,260,191 111,153,303 103,523,206 80,666,414 110,787,982 102,940,840
lexicon model t( f |e) (Bender et al, 2004):
hdel(eI1, f J1 ) =
J
?
j=1
[
max
0?i?I
t( f j|ei) < ?del
]
(4)
The deletion model simply counts the number
of words whose lexicon model probability is
lower than a threshold ?del. Likewise, we also
added an insertion model hins(eI1, f J1 ) that pe-
nalizes the spuriously inserted English words
using a lexicon model t(e| f ).
For the hierarchical phrase-based model, we em-
ployed the same feature set except for the distortion
model and the lexicalized reordering model.
3 Phrase Extraction from Different Word
Alignment
We prepared three kinds of corpora differentiated
by tokenization methods. First, the simplest pre-
processing is lower-casing (lower). Second, corpora
were transformed by a Porter?s algorithm based mul-
tilingual stemmer (stem) 1. Third, mixed-cased cor-
pora were truncated to the prefix of four letters of
each word (prefix4). For each differently tokenized
corpus, we computed word alignments by a HMM
translation model (Och and Ney, 2003) and by a
word alignment refinement heuristic of ?grow-diag-
final? (Koehn et al, 2003). Different preprocessing
yields quite divergent alignment points as illustrated
in Table 1. The table also shows the numbers for
the intersection and union of three alignment anno-
tations.
The (hierarchical) phrase translation pairs are ex-
tracted from three distinctly word aligned corpora.
1We used the Snowball stemmer from http://snowball.
tartarus.org
In this process, each word is recovered into its lower-
cased form. The associated counts are aggregated
to constitute relative count-based feature functions.
Table 2 summarizes the size of phrase tables in-
duced from the corpora. The number of rules ex-
tracted for the hierarchical phrase-based model was
roughly twice as large as those for the phrase-based
model. Fewer word alignments resulted in larger
phrase translation table size as observed in the ?pre-
fix4? corpus. The size is further increased by our
aggregation step (merged).
Different induction/refinement algorithms or pre-
processings of a corpus bias word alignment. We
found that some word alignments were consistent
even with different preprocessings, though we could
not justify whether such alignments would match
against human intuition. If we could trust such
consistently aligned words, reliable (hierarchical)
phrase translation pairs would be extracted, which,
in turn, would result in better estimates for relative
count-based feature functions. At the same time, dif-
ferently biased word alignment annotations suggest
alternative phrase translation pairs that is useful for
increasing the coverage of translations.
4 Results
Table 3 shows the open test translation results on
2005 and 2006 test set (the development-test set and
the final test set) 2. We used the merged (hierar-
chical) phrase tables for decoding. Feature function
scaling factors were optimized on BLEU score us-
ing the supplied development set that is identical to
the 2005?s development set. We observed that our
2We did not differetiated in-domain or out-of-domain for
2006 test set.
124
Table 3: Open test on the 2005/2006 test sets (BLEU [%]).
de-en es-en fr-en en-de en-es en-fr
test2005 Phrase 25.72 30.97 30.97 18.08 30.48 32.14
Rule 25.14 30.11 30.31 17.96 27.96 31.04
2005?s best 24.77 30.95 30.27
test2006 Phrase 23.16 29.90 27.89 15.79 29.54 29.19
Rule 22.74 28.80 27.28 15.99 26.56 27.86
results are very comparable to the last year?s best re-
sults in test2005. Also found that our hierarchical
phrase-based translation (Rule) performed slightly
inferior to the phrase-based translation (Phrase) in
both test sets. The hierarchically combined phrases
seem to be too flexible to represent the relationship
of similar language pairs. Note that our hierarchical
phrase-based model performed better in the English-
to-German translation task. Those language pair re-
quires rather distorted reordering, which could be
represented by hierarchically combined phrases.
We also conducted additional studies on how
differently aligned corpora might affect the trans-
lation quality on Spanish-to-English task for the
2005 test set. Using our phrase-based model,
the BLEU scores for lower/stem/prefix4 were
30.90/30.89/30.76, respectively. The differences of
translation qualities were statistically significant at
the 95% confidence level. Our phrase translation
pairs aggregated from all the differently prepro-
cessed corpora improved the translation quality.
5 Conclusion
We presented two translation models, a phrase-
based model and a hierarchical phrase-based model.
The former performed as well as the last year?s best
system, whereas the latter performed comparable to
our phrase-based model. We are going to experi-
ment new feature functions to restrict the too flexible
reordering represented by our hierarchical phrase-
based model.
We also investigated different word alignment an-
notations, first using lower-cased corpus, second
performing stemming, and third retaining only 4-
letter prefix. Differently preprocessed corpora re-
sulted in quite divergent word alignment. Large
phrase/rule translation tables were accumulated
from three distinctly aligned corpora, which in turn,
increased the translation quality.
References
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax
directed translations and the pushdown assembler. J.
Comput. Syst. Sci., 3(1):37?56.
Oliver Bender, Richard Zens, Evgeny Matusov, and Her-
mann Ney. 2004. Alignment templates: the RWTH
SMT system?. In Proc. of IWSLT 2004, pages 79?84,
Kyoto, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL
2005, pages 263?270, Ann Arbor, Michigan, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL 2003, pages 48?54, Edmonton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL 2002, pages
295?302.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Shankar Fraser, Alex a
nd Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings, pages
161?168, Boston, Massachusetts, USA, May 2 - May
7.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING-ACL
2006 (to appear), Sydney, Australia, July.
125
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 124?127,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Analysis of Listening-oriented Dialogue for Building Listening Agents
Toyomi Meguro, Ryuichiro Higashinaka, Kohji Dohsaka, Yasuhiro Minami, Hideki Isozaki
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237, Japan
{meguro,rh,dohsaka,minami,isozaki}@cslab.kecl.ntt.co.jp
Abstract
Our aim is to build listening agents that
can attentively listen to the user and sat-
isfy his/her desire to speak and have him-
self/herself heard. This paper investigates
the characteristics of such listening-oriented
dialogues so that such a listening process
can be achieved by automated dialogue sys-
tems. We collected both listening-oriented
dialogues and casual conversation, and ana-
lyzed them by comparing the frequency of
dialogue acts, as well as the dialogue flows
using HiddenMarkovModels (HMMs). The
analysis revealed that listening-oriented dia-
logues and casual conversation have charac-
teristically different dialogue flows and that
it is important for listening agents to self-
disclose before asking questions and to utter
more questions and acknowledgment than in
casual conversation to be good listeners.
1 Introduction
Although task-oriented dialogue systems have been
actively researched over the years (Walker et al,
2001), systems that perform more flexible (less task-
oriented) dialogues such as chats are beginning to be
actively investigated from their social and entertain-
ment aspects (Bickmore and Cassell, 2001; Higuchi
et al, 2008).
This paper deals with dialogues in which one con-
versational participant attentively listens to the other
(hereafter, listening-oriented dialogue). Our aim is
to build listening agents that can implement such a
listening process so that a user can satisfy his/her
desire to speak and have him/herself heard. Such
agents would lead the user?s state of mind for the
better as in a therapy session, although we want our
listening agents to help users mentally in everyday
conversation. It should also be noted that the pur-
pose of the listening-oriented dialogue is to simply
listen to users, not to elicit information as in inter-
views.
L: The topic is ?travel?, so did you
travel during summer vacation?
(QUESTION)
S: I like traveling. (SELF-DISCLOSURE)
L: Oh! I see! (SYMPATHY)
Why do you like to travel? (QUESTION)
S: This summer, I just went back
to my hometown.
(SELF-DISCLOSURE)
I was busy at work, but I?m
planning to go to Kawaguchi
Lake this weekend.
(SELF-DISCLOSURE)
I like traveling because it is
stimulating.
(SELF-DISCLOSURE)
L: Going to unusual places
changes one?s perspective,
doesn?t it?
(SYMPATHY)
You said you?re going to go to
Kawaguchi Lake this weekend.
Is this travel?
(QUESTION)
Will you go by car or train? (QUESTION)
Figure 1: Excerpt of a typical listening-oriented di-
alogue. Dialogue acts corresponding to utterances
are shown in parentheses (See Section 3.1 for their
meanings). The dialogue was originally in Japanese
and was translated by the authors.
There has been little research on listening agents.
One exception is (Maatman et al, 2005), which
showed that systems can make the user have the
sense of being heard by using gestures, such as nod-
ding and shaking of the head. Although our work is
similar to theirs, the difference is that we focus more
on verbal communication instead of non-verbal one.
For the purpose of gaining insight into how to
build our listening agents, we collected listening-
oriented dialogues as well as casual conversation,
and compared them in order to reveal the charac-
teristics of the listening-oriented dialogue. Figure 1
shows an example of a typical listening-oriented di-
alogue. In the figure, the conversational participants
talk about travel with the listener (L), repeatedly ask-
ing the speaker (S) to make self-disclosure.
2 Approach
We analyze the characteristics of listening-oriented
dialogues by comparing them with casual conversa-
tion. Here, casual conversation means a dialogue
where conversational participants have no prede-
fined roles (i.e., listeners and speakers). In this
124
study, we collect dialogues in texts because we want
to avoid the particular problems of voice, such as
filled pauses and interruptions, although we plan to
deal with speech input in the future.
As a procedure, we first collect listening-oriented
dialogues and casual conversation using human sub-
jects. Then, we label the collected dialogues with
dialogue act tags (see Section 3.1 for details of the
tags) to facilitate the analysis of the data. In the anal-
ysis, we examine the frequency of the tags in each
type of dialogue. We also look into the difference of
dialogue flows by modeling each type of dialogue by
Hidden Markov Models (HMMs) and comparing the
obtained models. We employ HMMs because they
are useful for modeling sequential data especially
when the number of states is unknown. We check
whether the HMMs for the listening-oriented dia-
logue and casual conversation can be successfully
distinguished from each other to see if the listen-
ing process can be successfully modeled. We also
analyze the transitions between states in the created
HMMs to examine the dialogue flows. We note that
HMMs have been used to model task-oriented dia-
logues (Shirai, 1996) and casual conversation (Iso-
mura et al, 2006). In this study, we use HMMs to
model and analyze listening-oriented dialogues.
3 Data collection
We recruited 16 participants. Eight participated as
listeners and the other eight as speakers. The male-
to-female ratio was even. The participants were 21
to 29 years old. Each participant engaged in four di-
alogues: two casual conversations followed by two
listening-oriented dialogues with a fixed role of lis-
tener/speaker. In listening-oriented dialogue, the lis-
teners were instructed to make it easy for the speak-
ers to say what they wanted to say. When col-
lecting the casual conversation, listeners were not
aware that they would be listeners afterwards. Lis-
teners had never met nor talked to the speakers prior
to the data collection. The listeners and speakers
talked over Microsoft Live MessengerTMin different
rooms; therefore, they could not see each other.
In each conversation, participants chatted for 30
minutes about their favorite topic that they selected
from the topic list we prepared. The topics were
food, travel, movies, music, entertainers, sports,
health, housework and childcare, personal comput-
ers and the Internet, animals, fashion and games. Ta-
ble 1 shows the number of collected dialogues, utter-
ances and words in each utterance of listeners and
Listening Casual
# dialogues 16 16
# utterances 850 720
# words Listener 20.60 17.92
per utt. Speaker 26.46 21.44
Table 1: Statistics of collected dialogues.
speakers. Generally, utterances in listening-oriented
dialogue were longer than those in casual conversa-
tion, probably because the subjects explained them-
selves in detail to make themselves better under-
stood.
At the end of each dialogue, the participants
filled out questionnaires that asked for their sat-
isfaction levels of dialogue, as well as how well
they could talk about themselves to their conver-
sational partners on the 10-point Likert scale. The
analysis of the questionnaire results showed that, in
listening-oriented dialogue, speakers were having a
better sense of making themselves heard than in ca-
sual conversation (Welch?s pairwise t-test; p=0.016)
without any degradation in the satisfaction level of
dialogue. This indicates that the subjects were suc-
cessfully performing attentive listening and that it is
meaningful to investigate the characteristics of the
collected listening-oriented dialogues.
3.1 Dialogue act
We labeled the collected dialogues using the dia-
logue act tag set: (1) SELF-DISCLOSURE (disclo-
sure of one?s preferences and feelings), (2) INFOR-
MATION (delivery of objective information), (3) AC-
KNOWLEDGMENT (encourages the conversational
partner to speak), (4) QUESTION (utterances that ex-
pect answers), (5) SYMPATHY (sympathetic utter-
ances and praises) and, (6) GREETING (social cues
to begin/end a dialogue).
We selected these tags from the DAMSL tag set
(Jurafsky et al, 1997) that deals with general con-
versation and also from those used to label therapy
conversation (Ivey and Ivey, 2002). Since our work
is still preliminary, we selected only a small num-
ber of labels that we thought were important for
modeling utterances in our collected dialogues, al-
though we plan to incorporate other tags in the fu-
ture. We expected that self-disclosure would occur
quite often in our data because the subjects were to
talk about their favorite topics and the participants
would be willing to communicate about their expe-
riences and feelings. We also expected that the lis-
teners would sympathize often to make others talk
with ease. Note that sympathy has been found useful
to increase closeness between conversational partic-
125
Listener Speaker
Casual Listening Casual Listening
DISC 66.6% 44.5% 53.3% 57.3%
INFO 6.5% 1.4% 5.6% 5.2%
ACK 8.0% 12.3% 6.6% 6.9%
QUES 4.1% 25.8% 21.3% 14.0%
SYM 2.6% 3.7% 3.2% 3.3%
GR 10.9% 9.8% 7.2% 9.6%
OTHER 1.3% 2.5% 2.9% 3.7%
Table 2: Rates of dialogue act tags.
DISC INFO ACK QUES SYM GR
Increase 0 0 8 8 5 4
Decrease 8 8 0 0 3 4
Table 3: Number of listeners whose tags in-
creased/decreased in listening-oriented dialogue.
ipants (Reis and Shaver, 1998).
A single annotator, who is not one of the authors,
labeled each utterance using the seven tags (six di-
alogue act tags plus OTHER). As a result, 1,177
tags were labeled to the utterances in the listening-
oriented dialogues and 1,312 tags to those in casual
conversation. The numbers of tags and utterances do
not match because, in text dialogue, an utterance can
be long and may be annotated with several tags.
4 Analysis
4.1 Comparing the frequency of dialogue acts
We compared the frequency of the dialogue act tags
in listening-oriented dialogues and casual conversa-
tion. Table 2 shows the rates of the tags in each type
of dialogue. In the table, OTHER means the expres-
sions that did not fall into any of our six dialogue
acts, such as facial expressions and mistypes. Table
3 shows the number of listeners whose rates of tags
increased or decreased from casual conversation to
listening-oriented dialogue.
Compared to casual conversation, the rates of
SELF-DISCLOSURE and INFORMATION decreased
in the listening-oriented dialogue. On the other
hand, the rates of ACKNOWLEDGMENT and QUES-
TION increased. This means that the listeners tended
to hold the transmission of information and focused
on letting speakers self-disclose or deliver informa-
tion. It can also be seen that the speakers decreased
QUESTION to increase self-disclosure.
4.2 Modeling dialogue act sequences by HMM
We analyzed the flow of listening-oriented dialogue
and casual conversation by modeling their dialogue
act sequences using HMMs. We defined 14 obser-
vation symbols, corresponding to the seven tags for
a listener and the same number of tags for a speaker.
L:Greeting:0.483S:Greeting:0.39
L:Self-disclosure:0.107L:Question:0.456S:Ack:0.224 S:Self-disclosure:0.828
L:Self-disclosure:0.579L:Ack:0.132
0.1
0.58
0.13
0.38
0.83
0.41
0.55
0.51
?
? ?
?
Figure 2: Ergodic HMM for listening-oriented dia-
logue. Circled numbers represent state IDs.
We trained the following two types of HMMs for
each type of dialogue.
Ergodic HMM: Each state emits all 14 observation
symbols. All states are connected to each other.
Speaker HMM: Half the states in this HMM only
emit one speaker?s dialogue acts and the other
half emit other speaker?s dialogue acts. All
states are connected to each other.
The EM algorithm was used to train the HMMs.
To find the best fitting HMM with minimal states,
we trained 1,000 HMMs for each type of HMM by
increasing the number of states from one to ten and
training 100 HMMs for each number of states. This
was necessary because the HMMs severely depend
on the initial probabilities. From the 1,000 HMMs,
we chose the most fitting model using the MDL
(Minimum Description Length) criterion.
4.2.1 Distinguishing Dialogue Types
We performed an experiment to examine whether
the trained HMMs can distinguish listening-oriented
dialogues and casual conversation. For this exper-
iment, we used eight listening-oriented dialogues
and eight casual conversations to train HMMs and
made them classify the remaining 16 dialogues. We
found that Ergodic HMM can distinguish the dia-
logues with an accuracy of 87.5%, and the Speaker
HMM achieved 100% accuracy. This indicates that
we can successfully train HMMs for each type of
dialogue and that investigating the trained HMMs
would show the characteristics of each type of di-
alogue. In the following sections, we analyze the
HMMs trained using all 16 dialogues of each type.
4.2.2 Analysis of Ergodic HMM
Figure 2 shows the Ergodic HMM for listening-
oriented dialogue. It can be seen that the major flow
126
L:Greeting:0.888
L:Self-disclosure:0.445L:Question:0.492 S:Self-disclosure:0.835
L:Self-disclosure:0.556L:Ack:0.27
S:Greeting:0.98
S:Self-disclosure:0.125S:Ack:0.661
0.42 0.370.43
0.38
0.11
0.56 0.51
0.18 0.92
0.47
0.63
0.25
? ?
? ?
? ?
Figure 3: Speaker HMM for listening-oriented dia-
logue.
S2:Greeting:0.775
S2:Self-disclosure:0.523S2:Question:0.414S1:Self-disclosure:0.644S1:Question:0.26
S2:Self-disclosure:0.629S2:Ack:0.12
S1:Greeting:0.848
S1:Self-disclosure:0.662S1:Ack:0.135
0.45 0.350.45
0.45 0.110.16
0.32 0.42
0.43
0.1
0.740.51
0.12
0.76 0.15
? ?
? ?
? ?
Figure 4: Speaker HMM for casual conversation.
of dialogue acts are: 2? L?s question ? 3? S?s self-
disclosure ? 4? L?s self-disclosure ? 2? L?s ques-
tion. This flow indicates that listeners tend to self-
disclose before the next question, showing the cycle
of reciprocal self-disclosure. This indicates that lis-
tening agents would need to have the capability of
self-disclosure in order to become human-like lis-
teners.
4.2.3 Analysis of Speaker HMM
Figures 3 and 4 show the Speaker HMMs for
listening-oriented dialogue and casual conversation,
respectively. Here, L and S correspond to S1 and
S2. It can be clearly seen that the two HMMs
have very similar structures. From the probabili-
ties, states with the same IDs seem to correspond to
each other. When we compare state IDs 3 and 5, it
can be seen that, when speakers take the role of lis-
teners, they reduce self-disclosure while increasing
questions and acknowledgment. Questions seem to
have more importance in listening-oriented dialogue
than in casual conversation, indicating that listening
agents need to have a good capability of generating
questions. The agents would also need to explicitly
increase acknowledgment in their utterances. Note
that, compared to spoken dialogue, acknowledgment
has to be performed consciously in text-based dia-
logue. When we compare state ID 4, we see that
the speaker starts questioning in casual conversation,
whereas the speaker only self-discloses in listening-
oriented dialogue. This shows that, in our data, the
speakers are successfully concentrating on making
self-disclosure in listening-oriented dialogue.
5 Conclusion and Future work
We collected listening-oriented dialogue and ca-
sual conversation, and compared them to find the
characteristics of listening-oriented dialogues that
are useful for building automated listening agents.
Our analysis found that it is important for listen-
ing agents to self-disclose before asking questions
and that it is necessary to utter more questions and
acknowledgment than in casual conversation to be
good listeners. As future work, we plan to use a
more elaborate tag set to further analyze the dia-
logue flows. We also plan to extend the HMMs
to Partially Observable Markov Decision Processes
(POMDPs) (Williams and Young, 2007) to achieve
dialogue management of listening agents from data.
References
Timothy Bickmore and Justine Cassell. 2001. Relational
agents: A model and implementation of building user trust.
In Proc. ACM CHI, pages 396?403.
Shinsuke Higuchi, Rafal Rzepka, and Kenji Araki. 2008. A
casual conversation system using modality and word associ-
ations retrieved from the web?. In EMNLP, pages 382?390.
Naoki Isomura, Fujio Toriumi, and Kenichiro Ishii. 2006.
Evaluation method of non-task-oriented dialogue system by
HMM. In Proc. the 4th Symposium on Intelligent Media In-
tegration for Social Information Infrastructure, pages 149?
152.
Allen E. Ivey and Mary Bradford Ivey. 2002. Intentional Inter-
viewing and Counseling: Facilitating Client Development in
a Multicultural Society. Brooks/Cole Publishing Company.
Dan Jurafsky, Liz Shriberg, and Debra Biasca, 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function Annota-
tion Coders Manual.
Martijn Maatman, Jonathan Gratch, and Stacy Marsella. 2005.
Natural behavior of a listening agent. Lecture Notes in Com-
puter Science, 3661:25?36.
Harry T. Reis and Phillip Shaver. 1998. Intimacy as an inter-
personal process. In S. Duck, editor, Handbook of personal
relationships, pages 367?398. John Wiley & Sons Ltd.
Katsuhiko Shirai. 1996. Modeling of spoken dialogue with and
without visual information. In Proc. ICSLP, volume 1, pages
188?191.
Marilyn A. Walker, Rebecca Passonneau, and Julie E. Boland.
2001. Quantitative and qualitative evaluation of darpa com-
municator spoken dialogue systems. In Proc. ACL, pages
515?522.
Jason D. Williams and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393?422.
127
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944?952,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Evaluation of Translation Quality for Distant Language Pairs
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, Hajime Tsukada
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,hirao,kevinduh,sudoh,tsukada}@cslab.kecl.ntt.co.jp
Abstract
Automatic evaluation of Machine Translation
(MT) quality is essential to developing high-
quality MT systems. Various evaluation met-
rics have been proposed, and BLEU is now
used as the de facto standard metric. How-
ever, when we consider translation between
distant language pairs such as Japanese and
English, most popular metrics (e.g., BLEU,
NIST, PER, and TER) do not work well. It
is well known that Japanese and English have
completely different word orders, and special
care must be paid to word order in transla-
tion. Otherwise, translations with wrong word
order often lead to misunderstanding and in-
comprehensibility. For instance, SMT-based
Japanese-to-English translators tend to trans-
late ?A because B? as ?B because A.? Thus,
word order is the most important problem
for distant language translation. However,
conventional evaluation metrics do not sig-
nificantly penalize such word order mistakes.
Therefore, locally optimizing these metrics
leads to inadequate translations. In this pa-
per, we propose an automatic evaluation met-
ric based on rank correlation coefficients mod-
ified with precision. Our meta-evaluation of
the NTCIR-7 PATMT JE task data shows that
this metric outperforms conventional metrics.
1 Introduction
Automatic evaluation of machine translation (MT)
quality is essential to developing high-quality ma-
chine translation systems because human evaluation
is time consuming, expensive, and irreproducible. If
we have a perfect automatic evaluation metric, we
can tune our translation system for the metric.
BLEU (Papineni et al, 2002b; Papineni et al,
2002a) showed high correlation with human judg-
ments and is still used as the de facto standard au-
tomatic evaluation metric. However, Callison-Burch
et al (2006) argued that the MT community is overly
reliant on BLEU by showing examples of poor per-
formance. For Japanese-to-English (JE) translation,
Echizen-ya et al (2009) showed that the popular
BLEU and NIST do not work well by using the sys-
tem outputs of the NTCIR-7 PATMT (patent transla-
tion) JE task (Fujii et al, 2008). On the other hand,
ROUGE-L (Lin and Hovy, 2003), Word Error Rate
(WER), and IMPACT (Echizen-ya and Araki, 2007)
worked better.
In these studies, Pearson?s correlation coefficient
and Spearman?s rank correlation ? with human eval-
uation scores are used to measure how closely an
automatic evaluation method correlates with human
evaluation. This evaluation of automatic evaluation
methods is called meta-evaluation. In human eval-
uation, people judge the adequacy and the fluency of
each translation.
Denoual and Lepage (2005) pointed out that
BLEU assumes word boundaries, which is ambigu-
ous in Japanese and Chinese. Here, we assume
the word boundaries given by ChaSen, one of the
standard morphological analyzers (http://chasen-
legacy.sourceforge.jp/) following Fujii et al
(2008)
In JE translation, most Statistical Machine Trans-
lation (SMT) systems translate the Japanese sen-
tence
(J0) kare wa sono hon wo yonda node
sekaishi ni kyoumi ga atta
which means
944
(R0) he was interested in world
history because he read the book
into an English sentence such as
(H0) he read the book because he was
interested in world history
in which the cause and the effect are swapped. Why
does this happen? The former half of (J0) means ?He
read the book,? and the latter half means ?(he) was
interested in world history.? The middle word
?node? between them corresponds to ?because.?
Therefore, SMT systems output sentences like (H0).
On the other hand, Rule-based Machine Translation
(RBMT) systems correctly give (R0).
In order to find (R0), SMT systems have to search
a very large space because we cannot restrict its
search space with a small distortion limit. Most
SMT systems thus fail to find (R0).
Consequently, the global word order is essential
for translation between distant language pairs, and
wrong word order can easily lead to misunderstand-
ing or incomprehensibility. Perhaps, some readers
do not understand why we emphasize word order
from this example alone. A few more examples
will clarify what happens when SMT is applied to
Japanese-to-English translation. Even the most fa-
mous SMT service available on the web failed to
translate the following very simple sentence at the
time of writing this paper.
Japanese: meari wa jon wo koroshita.
Reference: Mary killed John.
SMT output: John killed Mary.
Since it cannot translate such a simple sentence, it
obviously cannot translate more complex sentences
correctly.
Japanese: bobu ga katta hon wo jon wa yonda.
Reference: John read a book that Bob bought.
SMT output: Bob read the book John bought.
Another example is:
Japanese: bobu wa meari ni yubiwa wo kau
tameni, jon no mise ni itta.
Reference: Bob went to John?s store to buy a
ring for Mary.
SMT output: Bob Mary to buy the ring, John
went to the store.
In this way, this SMT service usually gives incom-
prehensible or misleading translations, and thus peo-
ple prefer RBMT services. Other SMT systems also
tend to make similar word order mistakes, and spe-
cial care should be paid to the translation between
distant language pairs such as Japanese and English.
Even Japanese people cannot solve this word or-
der problem easily: It is well known that Japanese
people are not good at speaking English.
From this point of view, conventional automatic
evaluation metrics of translation quality disregard
word order mistakes too much. Single-reference
BLEU is defined by a geometrical mean of n-gram
precisions pn and is modified by Brevity Penalty
(BP) min(1, exp(1? r/h)), where r is the length of
the reference and h is the length of the hypothesis.
BLEU = BP? (p1p2p3p4)
1/4.
Its range is [0, 1]. The BLEU score of (H0) with ref-
erence (R0) is 1.0?(11/11?9/10?6/9?4/8)1/4 =
0.740. Therefore, BLEU gives a very good score to
this inadequate translation because it checks only n-
grams and does not regard global word order.
Since (R0) and (H0) look similar in terms of flu-
ency, adequacy is more important than fluency in
the translation between distant language pairs.
Similarly, other popular scores such as NIST,
PER, and TER (Snover et al, 2006) also give
relatively good scores to this translation. NIST
also considers only local word orders (n-grams).
PER (Position-Independent Word Error Rate) was
designed to disregard word order completely.
TER (Snover et al, 2006) was designed to allow
phrase movements without large penalties. There-
fore, these standard metrics are not optimal for eval-
uating translation between distant language pairs.
In this paper, we propose an alternative automatic
evaluation metric appropriate for distant language
pairs. Our method is based on rank correlation co-
efficients. We use them to compare the word ranks
in the reference with those in the hypothesis.
There are two popular rank correlation coeffi-
cients: Spearman?s ? and Kendall?s ? (Kendall,
1975). In Isozaki et al (2010), we used Kendall?s ?
to measure the effectiveness of our Head Finaliza-
tion rule as a preprocessor for English-to-Japanese
translation, but we measured the quality of transla-
tion by using conventional metrics.
945
It is not clear how well ? works as an automatic
evaluation metric of translation quality. Moreover,
Spearman?s ? might work better than Kendall?s ? .
As we discuss later, ? considers only the direction
of the rank change, whereas ? considers the distance
of the change.
The first objective of this paper is to examine
which is the better metric for distant language pairs.
The second objective is to find improvements of
these rank correlation-metrics.
Spearman?s ? is based on Pearson?s correlation
coefficients. Suppose we have two lists of numbers
x = [0.1, 0.4, 0.2, 0.6],
y = [0.9, 0.6, 0.2, 0.7].
To obtain Pearson?s coefficients between x and y,
we use the raw values in these lists. If we substitute
their ranks for their raw values, we get
x? = [1, 3, 2, 4] and y? = [4, 2, 1, 3].
Then, Spearman?s ? between x and y is given by
Pearson?s coefficients between x? and y?. This ?
can be rewritten as follows when there is no tie:
? = 1?
?
i d
2
i
n+1C3
.
Here, di indicates the difference in the ranks of the
i-th element. Rank distances are squared in this
formula. Because of this square, we expect that ?
decreases drastically when there is an element that
significantly changes in rank. But we are also afraid
that ? may be too severe for alternative good trans-
lations.
Since Pearson?s correlation metric assumes lin-
earity, nonlinear monotonic functions can change
its score. On the other hand, Spearman?s ? and
Kendall?s ? uses ranks instead of raw evaluation
scores, and simple application of monotonic func-
tions cannot change them (use of other operations
such as averaging sentence scores can change them).
2 Methodology
2.1 Word alignment for rank correlations
We have to determine word ranks to obtain rank cor-
relation coefficients. Suppose we have:
(R1) John hit Bob yesterday
(H1) Bob hit John yesterday
The 1st word ?John? in R1 becomes the 3rd word
in H1. The 2nd word ?hit? in R1 becomes the 2nd
word in H1. The 3rd word ?Bob? in R1 becomes the
1st word in H1. The 4th word ?yesterday? in R1 be-
comes the 4th word in H1. Thus, we get H1?s word
order list [3, 2, 1, 4]. The number of all pairs of in-
tegers in this list is 4C2 = 6. It has three increasing
pairs: (3,4), (2,4), and (1,4). Since Kendall?s ? is
given by:
? = 2?
the number of increasing pairs
the number of all pairs
? 1,
H1?s ? is 2? 3/6? 1 = 0.0.
In this case, we can obtain Spearman?s ? as fol-
lows: ?John? moved by d1 = 2 words, ?hit? moved
by d2 = 0 words, ?Bob? moved by d3 = 2 words,
and ?yesterday? moved by d4 = 0 words. Therefore,
H1?s ? is 1? (22 + 02 + 22 + 02)/5C3 = 0.2.
Thus, ? considers only the direction of the move-
ment, whereas ? considers the distance of the move-
ment. Both ? and ? have the same range [?1, 1]. The
main objective of this paper is to clarify which rank
correlation is closer to human evaluation scores.
We have to consider the limitation of the rank cor-
relation metrics. They are defined only when there
is one-to-one correspondence. However, a refer-
ence sentence and a hypothesis sentence may have
different numbers of words. They may have two or
more occurrences of the same word in one sentence.
Sometimes, a word in the reference does not appear
in the hypothesis, or a word in the hypothesis does
not appear in the reference. Therefore, we cannot
calculate ? and ? following the above definitions in
general.
Here, we determine the correspondence of words
between hypotheses and references as follows. First,
we find one-to-one corresponding words. That is,
we find words that appear in both sentences and only
once in each sentence. Suppose we have:
(R2) the boy read the book
(H2) the book was read by the boy
By removing non-aligned words by one-to-one cor-
respondence, we get:
946
(R3) boy read book
(H3) book read boy
Thus, we lost ?the.? We relax this one-to-one cor-
respondence constraint by using one-to-one corre-
sponding bigrams. (R2) and (H2) share ?the boy?
and ?the book,? and we can align these instances of
?the? correctly.
(R4) the1 boy2 read3 the4 book5
(H4) the4 book5 read3 the1 boy2
Now, we have five aligned words, and H4?s word
order is represented by [4, 5, 3, 1, 2].
In returning to H0 and R0, we find that each of
these sentences has eleven words. Almost all words
are aligned by one-to-one correspondence but ?he?
is not aligned because it appears twice in each sen-
tence. By considering one-to-one corresponding bi-
grams (?he was? and ?he read?), ?he? is aligned as
follows.
(R5) he1 was2 interested3 in4 world5
history6 because7 he8 read9 the10
book11
(H5) he8 read9 the10 book11 because7
he1 was2 interested3 in4 world5
history6
H5?s word order is [8, 9, 10, 11, 7, 1, 2, 3, 4, 5, 6].
The number of increasing pairs is: 4C2 = 6 pairs in
[8, 9, 10, 11] and 6C2 = 15 pairs in [1, 2, 3, 4, 5,
6]. Then we obtain ? = 2 ? (6 + 15)/11C2 ? 1 =
?0.236. On the other hand,
?
i d
2
i = 5
2 ? 6 + 22 +
72 ? 4 = 350, and we obtain ? = 1 ? 350/12C3 =
?0.591.
Therefore, both Spearman?s ? and Kendall?s ?
give very bad scores to the misleading translation
H0. This fact implies they are much better metrics
than BLEU, which gave a good score to it. ? is much
lower than ? as we expected.
In general, we can use higher-order n-grams for
this alignment, but here we use only unigrams and
bigrams for simplicity. This algnment algorithm is
given in Figure 1. Since some hypothesis words do
not have corresponding reference words, the output
integer list worder is sometimes shorter than the
evaluated sentence. Therefore, we should not use
worder[i] ? i as di directly. We have to renumber
the list by rank as we did in Section 1.
Read a hypothesis sentence h = h1h2 . . . hm
and its reference sentence r = r1r2 . . . rn.
Initialize worder with an empty list.
For each word hi in h:
? If hi appears only once each in h and r, append j
s.t. rj = hi to worder.
? Otherwise, if the bigram hihi+1 appears only once
each in h and r, append j s.t. rjrj+1 = hihi+1 to
worder.
? Otherwise, if the bigram hi?1hi appears only once
each in h and r, append j s.t. rj?1rj = hi?1hi to
worder.
Return worder.
Figure 1: Word alignment algorithm for rank correlation
2.2 Word order metrics and meta-evaluation
metrics
These rank correlation metrics sometimes have neg-
ative values. In order to make them just like other
automatic evaluation metrics, we normalize them as
follows.
? Normalized Kendall?s ? : NKT = (? + 1)/2.
? Normalized Spearman?s ?: NSR = (?+ 1)/2.
Accordingly, NKT is 0.382 and NSR is 0.205.
These metrics are defined only when the number
of aligned words is two or more. We define both
NKT and NSR as zero when the number is one or
less. Consequently, these normalized metrics have
the same range [0, 1].
In order to avoid confusion, we use these abbre-
viations (NKT and NSR) when we use rank corre-
lations as word order metrics, because these cor-
relation metrics are also used in the machine trans-
lation community for meta-evaluation. For meta-
evaluation, we use Spearman?s ? and Pearson?s cor-
relation coefficient and call them ?Spearman? and
?Pearson,? respectively.
2.3 Overestimation problem
Since we measure the rank correlation of only cor-
responding words, these metrics will overestimate
the correlation. For instance, a hypothesis sentence
might have only two corresponding words among
947
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
BP (brevity penalty)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
? ?
?
?
? ?
?
?
?
?
?
?
?
??
?
?
?
?? ??
?
?
?
??
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
??
?
?
?
??
?
? ?
?
??
?
?
?
?
? ? ?
? ? ? ??
?
?
?
? ?
?
?
?
?? ??
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
??
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
? ?
?
?
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
? ?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
? ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0 0.2 0.4 0.6 0.8 1.0
0
0.2
0.4
0.6
0.8
1.0
P (precision)
no
rm
al
iz
ed
av
er
ag
e
ad
eq
ua
cy
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?? ?
?
?
?
?
?
?
?
??
?
? ?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
??
?
???
?
??? ?
?
?
? ?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
? ?
?
?
?
?
?
?
?
?
?
?
?
?
? ?
??
?
?
?
?
??
?
?
?
?
?
?
?
? ?
? ?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
??
?
?
?
?? ?
?
???
?
?
??
?
??
?
?
?
?
? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?? ?
?
?
?
?
?
??
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
?
?
?
?
? ??
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
? ?
??
?
? ?
??
?
?
?
?
?
Figure 2: Scatter plots of normalized average adequacy with brevity penalty (left) and precision (right).
(Each ? corresponds to one sentence generated by one MT system)
dozens of words. In this case, these two words
determine the score of the whole sentence. If the
two words appear in their order in the reference,
the whole sentence obtains the best score, NSR =
NKT = 1.0, in spite of the fact that only two words
matched.
Solving this overestimation problem is the second
objective of this paper. BLEU uses ?Brevity Penalty
(BP)? (Section 1) to reduce the scores of too-short
sentences. We can combine the above word order
metrics with BP, e.g., NKT? BP and NSR? BP.
However, we cannot very much expect from this
solution because BP scores do not correlate with
human judgments well. The left graph of Figure
2 shows a scatter plot of BP and ?normalized av-
erage adequacy.? This graph has 15 (systems) ?
100 (sentences) dots. Each dot (?) corresponds to
one sentence from one translation system.
In the NTCIR-7 data, three human judges gave
five-point scores (1, 2, 3, 4, 5) for ?adequacy? and
?fluency? of each translated sentence. Although
each system translated 1,381 sentences, only 100
sentences were evaluated by the judges.
For each translated sentence, we averaged three
judges? adequacy scores and normalized this aver-
age x by (x?1)/4. This is our ?normalized average
adequacy,? and the dots appears only at multiples of
1/3? 1/4.
This graph shows that BP has very little correla-
tion with adequacy, and we cannot expect BP to im-
prove the meta-evaluation performance very much.
Perhaps, BP?s poor performance was caused by the
fact that most MT systems output almost the same
number of words, and if the number exceeds the
length of the reference, BP=1.0 holds.
Therefore, we have to consider other modifiers
for this overestimation problem. We can use other
common metrics such as precision, recall, and F-
measure to reduce the overestimation of NSR and
NKT.
? Precision: P = c/|h|, where c is the number of
corresponding words and |h| is the number of
words in the hypothesis sentence h.
? Recall: R = c/r, where |r| is the number of
words in the reference sentence r.
? F-measure: F? = (1 + ?2)PR/(?2P + R),
where ? is a parameter.
In (R2)&(H2)?s case, precision is 5/7 = 0.714 and
recall is 5/5 = 1.000.
Which metric should we use? Our preliminary
experiments with NTCIR-7 data showed that preci-
sion correlated best with adequacy among these
three metrics (P , R, and F?=1). In addition, BLEU
is essentially made for precision. Therefore, preci-
sion seems the most promising modifier.
The right graph of Figure 2 shows a scatter plot
of precision and normalized average adequacy. The
graph shows that precision has more correlation with
adequacy than BP. We can observe that sentences
with very small P values usually obtain very low
adequacy scores but those with mediocre P values
often obtain good adequacy scores.
948
If we multiply P directly by NSR or NKT, those
sentences with mediocre P values will lose too
much of their scores. The use of
?
x will miti-
gate this problem. Since
?
P is closer to 1.0 than
P itself, multiplication of
?
P instead of P itself
will save these sentences. If we apply
?
x twice
(
??
P = 4
?
P ), it will further save them. There-
fore, we expect?
?
P and? 4
?
P to work better than
?P . Now, we propose two new metrics:
NSRP? and NKTP?,
where ? is a parameter (0 ? ? ? 1).
3 Experiments
3.1 Meta-evaluation with NTCIR-7 data
In order to compare automatic translation evalua-
tion methods, we use submissions to the NTCIR-7
Patent Translation (PATMT) task (Fujii et al, 2008).
Fourteen MT systems participated in the Japanese-
English intrinsic evaluation. There were two Rule-
Based MT (RMBT) systems and one Example-
based MT (EBMT) system. All other systems were
Statistical MT (SMT) systems. The task organiz-
ers provided a baseline SMT system. These 15 sys-
tems translated 1,381 Japanese sentences into En-
glish. The organizers evaluated these translations by
using BLEU and human judgments. In the human
judgements, three experts independently evaluated
100 selected sentences in terms of ?adequacy? and
?fluency.?
For automatic evaluation, we used a single refer-
ence sentence for each of these 100 manually evalu-
ated sentences. Echizen-ya et al (2009) used multi-
reference data, but their data is not publicly available
yet.
For this meta-evaluation, we measured the
corpus-level correlation between the human evalua-
tion scores and the automatic evaluation scores. We
simply averaged scores of 100 sentences for the pro-
posed metrics. For existing metrics such as BLEU,
we followed their definitions for corpus-level eval-
uation instead of simple averages of sentence-level
scores. We used default settings for conventional
metrics, but we tuned GTM (Melamed et al, 2007)
with -e option. This option controls preferences
on longer word runs. We also used the para-
phrase database TERp (http://www.umiacs.umd.
edu/?snover/terp) for METEOR (Banerjee and
Lavie, 2005).
3.2 Meta-evaluation with WMT-07 data
We developed our metric mainly for automatic eval-
uation of translation quality for distant language
pairs such as Japanese-English, but we also want
to know how well the metric works for similar lan-
guage pairs. Therefore, we also use the WMT-
07 data (Callison-Burch et al, 2007) that covers
only European language pairs. Callison-Burch et al
(2007) tried different human evaluation methods and
showed detailed evaluation scores. The Europarl test
set has 2,000 sentences, and The News Commentary
test set has 2,007 sentences.
This data has different language pairs: Spanish,
French, German ? English. We exclude Czech-
English because there were so few systems (See the
footnote of p. 146 in their paper).
4 Results
4.1 Meta-evaluation with NTCIR-7 data
Table 1 shows the main results of this paper. The
left part has corpus-level meta-evaluation with ade-
quacy. Error metrics, WER, PER, and TER, have
negative correlation coefficients, but we did not
show their minus signs here.
Both NSR-based metrics and NKT-based metrics
perform better than conventional metrics for this NT-
CIR PATMT JE translation data. As we expected,
?BP and ?P (1/1) performed badly. Spearman of
BP itself is zero.
NKT performed slightly better than NSR. Per-
haps, NSR penalized alternative good translations
too much. However, one of the NSR-based metrics,
NSRP 1/4, gave the best Spearman score of 0.947,
and the difference between NSRP? and NKTP?
was small. Modification with P led to this improve-
ment.
NKT gave the best Pearson score of 0.922. How-
ever, Pearson measures linearity and we can change
its score through a nonlinear monotonic function
without changing Spearman very much. For in-
stance, (NSRP 1/4)1.5 also has Spearman of 0.947
but its Pearson is 0.931, which is better than NKT?s
0.922. Thus, we think Spearman is a better meta-
evaluation metric than Pearson.
949
Table 1: NTCIR-7 Meta-evaluation: correlation with hu-
man judgments (Spm = Spearman, Prs = Pearson)
human judge Adequacy Fluency
eval\ meta-eval Spm Prs Spm Prs
P 0.615 0.704 0.672 0.876
R 0.436 0.669 0.461 0.854
F?=1 0.525 0.692 0.543 0.871
BP 0.000 0.515 -0.007 0.742
NSR 0.904 0.906 0.869 0.910
NSRP 1/8 0.937 0.905 0.890 0.934
NSRP 1/4 0.947 0.900 0.901 0.944
NSRP 1/2 0.937 0.890 0.926 0.949
NSRP 1/1 0.883 0.872 0.883 0.939
NSR ? BP 0.851 0.874 0.769 0.910
NKT 0.940 0.922 0.887 0.931
NKTP 1/8 0.940 0.913 0.908 0.944
NKTP 1/4 0.940 0.904 0.908 0.949
NKTP 1/2 0.929 0.890 0.897 0.949
NKTP 1/1 0.897 0.869 0.879 0.936
NKT ? BP 0.829 0.878 0.726 0.918
ROUGE-L 0.903 0.874 0.889 0.932
ROUGE-S(4) 0.593 0.757 0.640 0.869
IMPACT 0.797 0.813 0.751 0.932
WER 0.894 0.822 0.836 0.926
TER 0.854 0.806 0.372 0.856
PER 0.375 0.642 0.393 0.842
METEOR(TERp) 0.490 0.708 0.508 0.878
GTM(-e 12) 0.618 0.723 0.601 0.850
NIST 0.343 0.661 0.372 0.856
BLEU 0.515 0.653 0.500 0.795
The right part of Table 1 shows correlation with
fluency, but adequacy is more important, because
our motivation is to provide a metric that is useful to
reduce incomprehensible or misunderstanding out-
puts of MT systems. Again, the correlation-based
metrics gave better scores than conventional metrics,
and BP performed badly. NSR-based metrics proved
to be as good as NKT-based metrics.
Meta-evaluation scores of the de facto standard
BLEU is much lower than those of other metrics.
Echizen-ya et al (2009) reported that IMPACT per-
formed very well for sentence-level evaluation of
NTCIR-7 PATMT JE data. This corpus-level result
also shows that IMPACT works better than BLEU,
but ROUGE-L, WER, and our methods give better
scores than IMPACT.
Table 2: WMT-07 meta-evaluation: Each source lan-
guage has two columns: the left one is News Corpus and
the right one is Europarl.
Spearman?s ? with human ?rank?
source French Spanish German
NSR 0.775 0.837 0.523 0.766 0.700 0.593
NSRP 1/8 0.821 0.857 0.786 0.595 0.400 0.685
NSRP 1/4 0.821 0.857 0.786 0.455 0.400 0.714
NSRP 1/2 0.821 0.857 0.786 0.347 0.400 0.714
NKT 0.845 0.857 0.607 0.838 0.700 0.630
NKTP 1/8 0.793 0.857 0.786 0.595 0.400 0.714
NKTP 1/4 0.793 0.857 0.786 0.524 0.400 0.714
NKTP 1/2 0.793 0.857 0.786 0.347 0.400 0.714
BLEU 0.786 0.679 0.750 0.595 0.400 0.821
WER 0.607 0.857 0.750 0.429 0.000 0.500
ROUGEL 0.893 0.739 0.786 0.707 0.700 0.857
ROUGES 0.883 0.679 0.786 0.690 0.400 0.929
4.2 Meta-evaluation with WMT-07 data
Callison-Burch et al (2007) have performed differ-
ent human evaluation methods for different language
pairs and different corpora. Their Table 5 shows
inter-annotator agreements for the human evaluation
methods. According to their table, the ?sentence
ranking? (or ?rank?) method obtained better agree-
ment than ?adequacy.? Therefore, we show Spear-
man?s ? for ?rank.? We used the scores given in
their Tables 9, 10, and 11. (The ?constituent? meth-
ods obtained the best inter-annotator agreement, but
these methods focus on local translation quality and
have nothing to do with global word order, which we
are discussing here.)
Table 2 shows that our metrics designed for
distant language pairs are comparable to conven-
tional methods even for similar language pairs, but
ROUGE-L and ROUGE-S performed better than
ours for French News Corpus and German Europarl.
BLEU scores in this table agree with those in Table
17 of Callison-Burch et al (2007) within rounding
errors.
After some experiments, we noticed that the use
ofR instead of P often gives better scores for WMT-
07, but it degrades NTCIR-7 scores. We can extend
our metric by F? , weighted harmonic mean of P and
R, or any other interpolation, but the introduction
of new parameters into our metric makes it difficult
950
to control. Improvement without new parameters is
beyond the scope of this paper.
5 Discussion
It has come to our attention that Birch et al (2010)
has independently proposed an automatic evaluation
method based on Kendall?s ? . First, they started
with Kendall?s ? distance, which can be written as
?1?NKT? in our terminology, and then subtracted
it from one. Thus, their metric is nothing but NKT.
Then, they proposed application of the square root
to get better Pearson by improving ?the sensitivity
to small reorderings.? Since they used ?Kendall?s ??
and ?Kendall?s ? distance? interchangeably, it is not
clear what they mean by ?
?
Kendall?s ? ,? but per-
haps they mean 1 ?
?
1?NKT because
?
NKT is
more insensitive to small reorderings. Table 3 shows
the performance of these metrics for NTCIR-7 data.
Pearson?s correlation coefficient with adequacy was
improved by 1 ?
?
1? NKT, but other scores were
degraded in this experiment.
The difference between our method and Birch et
al. (2010)?s method comes from the fact that we
used Japanese-English translation data and Spear-
man?s correlation for meta-evaluation, whereas they
used Chinese-English translation data and only Pear-
son?s correlation for meta-evaluation. Chinese word
order is different from English, but Chinese is a
Subject-Verb-Object (SVO) language and thus is
much closer to English word order than Japanese,
a typical SOV language.
We preferred NSR because it penalizes global
word order mistakes much more than does NKT, and
as discussed above, global word order mistakes of-
ten lead to incomprehensibility and misunderstand-
ing.
On the other hand, they also tried Hamming dis-
tance, and summarized their experiments as follows:
However, the Hamming distance seems to
be more informative than Kendall?s tau for
small amounts of reordering.
This sentence and the introduction of the square root
to NKT imply that Chinese word order is close to
that of English, and they have to measure subtle
word order mistakes.
Table 3: NTCIR-7 meta-evaluation: Effects of square
root (b(x) = 1?
?
1? x)
NKT
?
NKT b(NKT)
Spearman w/ adequacy 0.940 0.940 0.922
Pearson w/ adequacy 0.922 0.817 0.941
Spearman w/ fluency 0.887 0.865 0.858
Pearson w/ fluency 0.931 0.917 0.833
In spite of these differences, the two groups inde-
pendently recognized the usefulness of rank correla-
tions for automatic evaluation of translation quality
for distant language pairs.
In their WMT-2010 paper (Birch and Osborne,
2010), they multiplied NKT with the brevity penalty
and interpolated it with BLEU for the WMT-2010
shared task. This fact implies that incomprehensible
or misleading word order mistakes are rare in trans-
lation among European languages.
6 Conclusions
When Statistical Machine Translation is applied to
distant language pairs such as Japanese and English,
word order becomes an important problem. SMT
systems often fail to find an appropriate translation
because of a large search space. Therefore, they
often output misleading or incomprehensible sen-
tences such as ?A because B? vs. ?B because A.? To
penalize such inadequate translations, we presented
an automatic evaluation method based on rank corre-
lation. There were two questions for this approach.
First, which correlation coefficient should we use:
Spearman?s ? or Kendall?s ?? Second, how should
we solve the overestimation problem caused by the
nature of one-to-one correspondence?
We answered these questions through our exper-
iments using the NTCIR-7 PATMT JE translation
data. For the first question, ? was slightly better
than ?, but ? was improved by precision. For the
second question, it turned out that BLEU?s Brevity
Penalty was counter-productive. A precision-based
penalty gave a better solution. With this precision-
based penalty, both ? and ? worked well and they
outperformed conventional methods for NTCIR-7
data. For similar language pairs, our method was
comparable to conventional evaluation methods. Fu-
951
ture work includes extension of the method so that it
can outperform conventional methods even for sim-
ilar language pairs.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgements. In Proc. of ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and Summarization, pages 65?72.
Alexandra Birch and Miles Osborne. 2010. LRscore for
evaluating lexical and reordering quality in MT. In
Proceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 327?
332.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating reorder-
ing. Machine Translation, 24(1):15?26.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluatiing the role of Bleu in ma-
chine translation research. In Proc. of the Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Chrstof Monz, and Josh Schroeder. 2007.
(Meta-)Evaluation of machine translation. In Proc. of
the Workshop on Machine Translation (WMT), pages
136?158.
Etienne Denoual and Yves Lepage. 2005. BLEU in char-
acters: towards automatic MT evaluation in languages
without word delimiters. In Companion Volume to the
Proceedings of the Second International Joint Confer-
ence on Natural Language Processing, pages 81?86.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In Proceedings of MT Summit XII Workshop on Patent
Translation, pages 151?158.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimohata,
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, and Noriko Kando. 2009. Meta-
evaluation of automatic evaluation methods for ma-
chine translation using patent translation data in ntcir-
7. In Proceedings of the 3rd Workshop on Patent
Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A simple re-
ordering rule for SOV languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 250?257.
Maurice G. Kendall. 1975. Rank Correlation Methods.
Charles Griffin.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 71?78.
Dan Melamed, Ryan Green, and Joseph P. Turian. 2007.
Precision and recall of machine translation. In Proc.
of NAACL-HLT, pages 61?63.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002a. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish Results. In
Proc. of the International Conference on Human Lan-
guage Technology Research (HLT), pages 132?136.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002b. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
952
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 636?641,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Condensed Feature Representations from Large Unsupervised
Data Sets for Supervised Learning
Jun Suzuki, Hideki Isozaki, and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{suzuki.jun, isozaki.hideki, nagata.masaaki}@lab.ntt.co.jp
Abstract
This paper proposes a novel approach for ef-
fectively utilizing unsupervised data in addi-
tion to supervised data for supervised learn-
ing. We use unsupervised data to gener-
ate informative ?condensed feature represen-
tations? from the original feature set used in
supervised NLP systems. The main con-
tribution of our method is that it can of-
fer dense and low-dimensional feature spaces
for NLP tasks while maintaining the state-of-
the-art performance provided by the recently
developed high-performance semi-supervised
learning technique. Our method matches the
results of current state-of-the-art systems with
very few features, i.e., F-score 90.72 with
344 features for CoNLL-2003 NER data, and
UAS 93.55 with 12.5K features for depen-
dency parsing data derived from PTB-III.
1 Introduction
In the last decade, supervised learning has become
a standard way to train the models of many natural
language processing (NLP) systems. One simple but
powerful approach for further enhancing the perfor-
mance is to utilize a large amount of unsupervised
data to supplement supervised data. Specifically,
an approach that involves incorporating ?clustering-
based word representations (CWR)? induced from
unsupervised data as additional features of super-
vised learning has demonstrated substantial perfor-
mance gains over state-of-the-art supervised learn-
ing systems in typical NLP tasks, such as named en-
tity recognition (Lin and Wu, 2009; Turian et al,
2010) and dependency parsing (Koo et al, 2008).
We refer to this approach as the iCWR approach,
The iCWR approach has become popular for en-
hancement because of its simplicity and generality.
The goal of this paper is to provide yet another
simple and general framework, like the iCWR ap-
proach, to enhance existing state-of-the-art super-
vised NLP systems. The differences between the
iCWR approach and our method are as follows; sup-
pose F is the original feature set used in supervised
learning, C is the CWR feature set, andH is the new
feature set generated by our method. Then, with the
iCWR approach, C is induced independently from
F , and used in addition to F in supervised learning,
i.e., F ? C. In contrast, in our method H is directly
induced from F with the help of an existing model
already trained by supervised learning with F , and
used in place of F in supervised learning.
The largest contribution of our method is that
it offers an architecture that can drastically reduce
the number of features, i.e., from 10M features
in F to less than 1K features in H by construct-
ing ?condensed feature representations (COFER)?,
which is a new and very unique property that can-
not be matched by previous semi-supervised learn-
ing methods including the iCWR approach. One
noteworthy feature of our method is that there is no
need to handle sparse and high-dimensional feature
spaces often used in many supervised NLP systems,
which is one of the main causes of the data sparse-
ness problem often encountered when we learn the
model with a supervised leaning algorithm. As a
result, NLP systems that are both compact and high-
performance can be built by retraining the model
with the obtained condensed feature set H.
2 Condensed Feature Representations
Let us first define the condensed feature set H. In
this paper, we call the feature set generally used in
supervised learning, F , the original feature set. Let
N andM represent the numbers of features inF and
H, respectively. We assume M?N , and generally
M N . A condensed feature hm ?H is charac-
636
Potencies are multiplied by a positive constant ?
0 1 2-1-2 3 4
Section 3.3: Feature potency quantization
Feature potency Section 3.4: Condensed feature construction
F N  (e.g., N=100M)Original feature set Section 3.1: Feature potency estimation
Features mapped into this area will be zeroed by the effect of C
C
0
-C Section 3.2: Feature potency discountingFeature potency?  0
Feature potency 
? ( Integer Space N )Condensed feature setH Each condensed feature is represented as a set of features in the original feature set F.-1/?1/?3/?? -2/?M  (e.g., M=1K) The potencies are also utilized as an (M+1)-th condensed feature
H
(Quantized feature potency)
Features mapped into zero are discarded and never mapped into any condensed features
Figure 1: Outline of our method to construct a condensed
feature set.
r?(x) =
X
y?Y(x)
r(x,y)/|Y(x)|.
V +D (fn) =
X
x?D
fn(x, y?)(r(x, y?) ? r?(x))
V ?D (fn) = ?
X
x?D
X
y?Y(x)\y?
fn(x,y)(r(x,y) ? r?(x))
Rn=
X
x?D
X
y?Y(x)
r(x,y)fn(x,y), An=
X
x?D
r?(x)
X
y?Y(x)
fn(x,y)
Figure 2: Notations used in this paper.
terized as a set of features in F , that is, hm =Sm
where Sm ? F . We assume that each original fea-
ture fn?F maps, at most, to one condensed feature
hm. This assumption prevents two condensed fea-
tures from containing the same original feature, and
some original features from not being mapped to any
condensed feature. Namely, Sm ? Sm? =? for all m
and m?, where m 6=m?, and
?M
m=1 Sm?F hold.
The value of each condensed feature is calcu-
lated by summing the values of the original fea-
tures assigned to it. Formally, let X and Y repre-
sent the sets of all possible inputs and outputs of
a target task, respectively. Let x ? X be an in-
put, and y ? Y(x) be an output, where Y(x) ? Y
represents the set of possible outputs given x. We
write the n-th feature function of the original fea-
tures, whose value is determined by x and y, as
fn(x,y), where n ? {1, . . . , N}. Similarly, we
write them-th feature function of the condensed fea-
tures as hm(x,y), where m?{1, . . . ,M}. We state
that the value of hm(x,y) is calculated as follows:
hm(x,y)=
?
fn?Sm fn(x,y).
3 Learning COFERs
The remaining part of our method consists of the
way to map the original features into the condensed
features. For this purpose, we define the feature po-
tency, which is evaluated by employing an existing
supervised model with unsupervised data sets. Fig-
ure 1 shows a brief sketch of the process to construct
the condensed features described in this section.
3.1 Self-taught-style feature potency estimation
We assume that we have a model trained by super-
vised learning, which we call the ?base supervised
model?, and the original feature set F that is used
in the base supervised model. We consider a case
where the base supervised model is a (log-)linear
model, and use the following equation to select the
best output y? given x:
y?=argmax
y?Y(x)
?N
n=1 wnfn(x,y), (1)
where wn is a model parameter (or weight) of fn.
Linear models are currently the most widely-used
models and are employed in many NLP systems.
To simplify the explanation, we define function
r(x,y), where r(x,y) returns 1 if y = y? is obtained
from the base supervised model given x, and 0 oth-
erwise. Let r?(x) represent the average of r(x,y) in
x (see Figure 2 for details). We also define V +D (fn)
and V ?D (fn) as shown in Figure 2 where D repre-
sents the unsupervised data set. V +D (fn) measures
the positive correlation with the best output y? given
by the base supervised model since this is the sum-
mation of all the (weighted) feature values used in
the estimation of the one best output y? over all x in
the unsupervised data D. Similarly, V ?D (fn) mea-
sures the negative correlation with y?. Next, we de-
fine VD(fn) as the feature potency of fn: VD(fn) =
V +D (fn)? V
?
D (fn).
An intuitive explanation of VD(fn) is as follows;
if |VD(fn)| is large, the distribution of fn has either
a large positive or negative correlation with the best
output y? given by the base supervised model. This
implies that fn is an informative and potent feature
in the model. Then, the distribution of fn has very
small (or no) correlation to determine y? if |VD(fn)|
is zero or near zero. In this case, fn can be evaluated
as an uninformative feature in the model. From this
perspective, we treat VD(fn) as a measure of feature
potency in terms of the base supervised model.
The essence of this idea, evaluating features
against each other on a certain model, is widely
used in the context of semi-supervised learning,
i.e., (Ando and Zhang, 2005; Suzuki and Isozaki,
637
2008; Druck and McCallum, 2010). Our method
is rough and a much simpler framework for imple-
menting this fundamental idea of semi-supervised
learning developed for NLP tasks. We create a
simple framework to achieve improved flexibility,
extendability, and applicability. In fact, we apply
the framework by incorporating a feature merging
and elimination architecture to obtain effective con-
densed feature sets for supervised learning.
3.2 Feature potency discounting
To discount low potency values, we redefine feature
potency as V ?D(fn) instead of VD(fn) as follows:
V ?D(fn) =
?
?
?
log [Rn+C]?log[An] if Rn?An<?C
0 if ? C?Rn?An?C
log [Rn?C]?log[An] if C<Rn?An
where Rn and An are defined in Figure 2. Note
that VD(fn) = V +D (fn) ? V
?
D (fn) = Rn ? An.
The difference from VD(fn) is that we cast it in the
log-domain and introduce a non-negative constant
C. The introduction of C is inspired by the L1-
regularization technique used in supervised learning
algorithms such as (Duchi and Singer, 2009; Tsu-
ruoka et al, 2009). C controls how much we dis-
count VD(fn) toward zero, and is given by the user.
3.3 Feature potency quantization
We define V ?D(fn) as V ?D(fn) = d?V ?D(fn)e if
V ?D(fn) > 0 and V ?D(fn) = b?V ?D(fn)c otherwise,
where ? is a positive user-specified constant. Note
that V ?D(fn) always becomes an integer, that is,
V ?D(fn) ?N where N = {. . . ,?2,?1, 0, 1, 2, . . .}.
This calculation can be seen as mapping each fea-
ture into a discrete (integer) space with respect to
V ?D(fn). ? controls the range of V ?D(fn) mapping
into the same integer.
3.4 Condensed feature construction
Suppose we have M different quantized feature po-
tency values in V ?D(fn) for all n, which we rewrite
as {um}Mm=1. Then, we define Sm as a set of fn
whose quantized feature potency value is um. As
described in Section 2, we define the m-th con-
densed feature hm(x,y) as the summation of all
the original features fn assigned to Sm. That is,
hm(x,y) =
?
fn?Sm fn(x,y). This feature fusion
process is intuitive since it is acceptable if features
with the same (similar) feature potency are given the
same weight by supervised learning since they have
the same potency with regard to determining y?. ?
determines the number of condensed features to be
made; the number of condensed features becomes
large if ? is large. Obviously, the upper bound of
the number of condensed features is the number of
original features.
To exclude possibly unnecessary original features
from the condensed features, we discard feature fn
for all n if un = 0. This is reasonable since, as de-
scribed in Section 3.1, a feature has small (or no)
effect in achieving the best output decision in the
base supervised model if its potency is near 0. C in-
troduced in Section 3.2 mainly influences how many
original features are discarded.
Additionally, we also utilize the ?quantized? fea-
ture potency values themselves as a new feature.
The reason behind is that they are also very infor-
mative for supervised learning. Their use is impor-
tant to further boost the performance gain offered
by our method. For this purpose, we define ?(x,y)
as ?(x,y) =
?M
m=1(um/?)hm(x,y). We then
use ?(x,y) as the (M + 1)-th feature of our con-
densed feature set. As a result, the condensed fea-
ture set obtained with our method is represented as
H = {h1(x,y), . . . , hM (x,y), ?(x,y)}.
Note that the calculation cost of ?(x,y) is negli-
gible. We can calculate the linear discriminant func-
tion g(x,y) as: g(x,y) =
?M
m=1 wmhm(x,y) +
wM+1?(x,y) =
?M
m=1 w?mhm(x,y), where w?m =
(wm + wM+1um/?). We emphasize that once
{wm}M+1m=1 are determined by supervised learning,
we can calculate w?m in a preliminary step before
the test phase. Thus, our method also takes the form
of a linear model. The number of features for our
method is essentially M even if we add ?.
3.5 Application to Structured Prediction Tasks
We modify our method to better suit structured pre-
diction problems in terms of calculation cost. For a
structured prediction problem, it is usual to decom-
pose or factorize output structure y into a set of lo-
cal sub-structures z to reduce the calculation cost
and to cope with the sparsity of the output space
Y . This factorization can be accomplished by re-
stricting features that are extracted only from the in-
formation within decomposed local sub-structure z
638
and given input x. We write z ? y when the lo-
cal sub-structure z is a part of output y, assuming
that output y is constructed by a set of local sub-
structures. Then formally, the n-th feature is written
as fn(x, z), and fn(x,y) =
?
z?y fn(x, z) holds.
Similarly, we introduce r(x, z), where r(x, z) = 1
if z ? y?, and r(x, z) = 0 otherwise, namely z /? y?.
We define Z(x) as the set of all local sub-
structures possibly generated for all y in Y(x).
Z(x) can be enumerated easily, unless we use typi-
cal first- or second-order factorization models by the
restriction of efficient decoding algorithms, which is
the typical case for many NLP tasks such as named
entity recognition and dependency parsing.
Finally, we replace all Y(x) with Z(x), and use
fn(x, z) and r(x, z) instead of fn(x,y) and r(x,y),
respectively, in Rn and An. When we use these sub-
stitutions, there is no need to incorporate an efficient
algorithm such as dynamic programming into our
method. This means that our feature potency esti-
mation can be applied to the structured prediction
problem at low cost.
3.6 Efficient feature potency computation
Our feature potency estimation described in Section
3.1 to 3.3 is highly suitable for implementation in
the MapReduce framework (Dean and Ghemawat,
2008), which is a modern distributed parallel com-
puting framework. This is because Rn and An can
be calculated by the summation of a data-wise cal-
culation (map phase), and V ?D(fn) can be calculated
independently by each feature (reduce phase). We
emphasize that our feature potency estimation can
be performed in a ?single? map-reduce process.
4 Experiments
We conducted experiments on two different NLP
tasks, namely NER and dependency parsing. To fa-
cilitate comparisons with the performance of previ-
ous methods, we adopted the experimental settings
used to examine high-performance semi-supervised
NLP systems; i.e., NER (Ando and Zhang, 2005;
Suzuki and Isozaki, 2008) and dependency pars-
ing (Koo et al, 2008; Chen et al, 2009; Suzuki
et al, 2009). For the supervised datasets, we used
CoNLL?03 (Tjong Kim Sang and DeMeulder, 2003)
shared task data for NER, and the Penn Treebank III
(PTB) corpus (Marcus et al, 1994) for dependency
parsing. We prepared a total of 3.72 billion token
text data as unsupervised data following the instruc-
tions given in (Suzuki et al, 2009).
4.1 Comparative Methods
We mainly compare the effectiveness of COFER
with that of CWR derived by the Brown algorithm.
The iCWR approach yields the state-of-the-art re-
sults with both dependency parsing data derived
from PTB-III (Koo et al, 2008), and the CoNLL?03
shared task data (Turian et al, 2010). By compar-
ing COFER with iCWR we can clarify its effective-
ness in terms of providing better features for super-
vised learning. We use the term active features to
refer to features whose corresponding model param-
eter is non-zero after supervised learning. It is well-
known that we can discard non-active features from
the trained model without any loss after finishing su-
pervised learning. Finally, we compared the perfor-
mance in terms of the number of active features in
the model given by supervised learning. We note
here that the number of active features for COFER
is the number of features hm if w?m = 0, which is
not wm = 0 for a fair comparison.
Unlike COFER, iCWR does not have any archi-
tecture to winnow the original feature set used in
supervised learning. For a fair comparison, we
prepared L1-regularized supervised learning algo-
rithms, which try to reduce the non-zero parameters
in a model. Specifically, we utilized L1-regularized
CRF (L1CRF) optimized by OWL-QN (Andrew
and Gao, 2007) for NER, and the online struc-
tured output learning version of FOBOS (Duchi
and Singer, 2009; Tsuruoka et al, 2009) with L1-
regularization (ostL1FOBOS) for dependency pars-
ing. In addition, we also examined L2 regular-
ized CRF (Lafferty et al, 2001) optimized by L-
BFGS (Liu and Nocedal, 1989) (L2CRF) for NER,
and the online structured output learning version of
the Passive-Aggressive algorithm (ostPA) (Cram-
mer et al, 2006) for dependency parsing to illus-
trate the baseline performance regardless of the ac-
tive feature number.
4.2 Settings for COFER
We utilized baseline supervised learning mod-
els as the base supervised models of COFER.
639
86.0
88.0
90.0
92.0
94.0
96.0
1.0E+01 1.0E+03 1.0E+05 1.0E+07 1.0E+09
iCWR+COFER: L2CRF iCWR+COFER: L1CRFCOFER: L2CRF COFER: L1CRFiCWR: L2CRF iCWR: L1CRFSup.L2CRF Sup.L1CRF
F-scor
e
# of active features [log-scale]
?=1e+01?=1e+02 ?=1e+04
?=1e+00
proposed
90.0
91.0
92.0
93.0
94.0
95.0
1.E+02 1.E+04 1.E+06 1.E+08
iCWR+COFER: ostPA iCWR+COFER: ostL1FOBOSCOFER: ostPA COFER: ostL1FOBOSiCWR: ostPA iCWR: ostL1FOBOSSup.ostPA Sup.ostL1FOBOS
# of active features [log-scale]
Unlabe
led At
tachme
nt Sco
re ?=1e+00
?=1e+05?=1e+01 ?=1e+03proposed
(a) NER (F-score) (b) dep. parsing (UAS)
Figure 3: Performance vs. size of active features in the
trained model on the development sets
In addition, we also report the results when we
treat iCWR as COFER?s base supervised mod-
els (iCWR+COFER). This is a very natural and
straightforward approach to combining these two.
We generally handle several different types of fea-
tures such as words, part-of-speech tags, word sur-
face forms, and their combinations. Suppose we
have K different feature types, which are often de-
fined by feature templates, i.e., (Suzuki and Isozaki,
2008; Lin andWu, 2009). In our experiments, we re-
strict the merging of features during the condensed
feature construction process if and only if the fea-
tures are the same feature type. As a result, COFER
essentially consists ofK different condensed feature
sets. The numbers of feature typesK were 79 and 30
for our NER and dependency parsing experiments,
respectively. We note that this kind of feature par-
tition by their types is widely used in the context of
semi-supervised learning (Ando and Zhang, 2005;
Suzuki and Isozaki, 2008).
4.3 Results and Discussion
Figure 3 displays the performance on the develop-
ment set with respect to the number of active fea-
tures in the trained models given by each supervised
learning algorithm. In both NER and dependency
parsing experiments, COFER significantly outper-
formed iCWR. Moreover, COFER was surprisingly
robust in relation to the number of active features
in the model. These results reveal that COFER pro-
vides effective feature sets for certain NLP tasks.
We summarize the noteworthy results in Figure 3,
and also the performance of recent top-line systems
for NER and dependency parsing in Table 1. Over-
all, COFER matches the results of top-line semi-
NER system dev. test #.USD #.AF
Sup.L1CRF 90.40 85.08 0 0.57M
iCWR: L1CRF 93.33 89.99 3,720M 0.62M
COFER: L1CRF (? = 1e+ 00) 93.42 88.81 3,720M 359
(? = 1e+ 04) 93.60 89.22 3,720M 2.46M
iCWR+COFER: (? = 1e+ 00) 94.39 90.72 3,720M 344
L1CRF (? = 1e+ 04) 94.91 91.02 3,720M 5.94M
(Ando and Zhang, 2005) 93.15 89.31 27M N/A
(Suzuki and Isozaki, 2008) 94.48 89.92 1,000M N/A
(Ratinov and Roth, 2009) 93.50 90.57 N/A N/A
(Turian et al, 2010) 93.95 90.36 37M N/A
(Lin and Wu, 2009) N/A 90.90 700,000M N/A
Dependency parser dev. test #.USD #.AF
ostL1FOBOS 93.15 92.82 0 6.80M
iCWR: ostL1FOBOS 93.69 93.49 3,720M 9.67M
COFER:ostL1FOBOS (? = 1e+ 03) 93.53 93.23 3,720M 20.7K
(? = 1e+ 05) 93.91 93.71 3,720M 3.23M
iCWR+COFER: (? = 1e+ 03) 93.93 93.55 3,720M 12.5K
ostL1FOBOS (? = 1e+ 05) 94.33 94.22 3,720M 5.77M
(Koo and Collins, 2010) 93.49 93.04 0 N/A
(Martins et al, 2010) N/A 93.26 0 55.25M
(Koo et al, 2008) 93.30 93.16 43M N/A
(Chen et al, 2009) N/A 93.16 43M N/A
(Suzuki et al, 2009) 94.13 93.79 3,720M N/A
Table 1: Comparison with previous top-line systems on
test data. (#.USD: unsupervised data size. #.AF: the size
of active features in the trained model.)
supervised learning systems even though it uses far
fewer active features.
In addition, the combination of iCWR+COFER
significantly outperformed the current best results
by achieving a 0.12 point gain from 90.90 to 91.02
for NER, and a 0.43 point gain from 93.79 to 94.22
for dependency parsing, with only 5.94M and 5.77M
features, respectively.
5 Conclusion
This paper introduced the idea of condensed feature
representations (COFER) as a simple and general
framework that can enhance the performance of ex-
isting supervised NLP systems. We also proposed
a method that efficiently constructs condensed fea-
ture sets through discrete feature potency estima-
tion over unsupervised data. We demonstrated that
COFER based on our feature potency estimation can
offer informative dense and low-dimensional feature
spaces for supervised learning, which is theoreti-
cally preferable to the sparse and high-dimensional
feature spaces often used in many NLP tasks. Exist-
ing NLP systems can be made more compact with
higher performance by retraining their models with
our condensed features.
640
References
Rie Kubota Ando and Tong Zhang. 2005. A High-
Performance Semi-Supervised Learning Method for
Text Chunking. In Proceedings of 43rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 1?9.
Galen Andrew and Jianfeng Gao. 2007. Scalable
Training of L1-regularized Log-linear Models. In
Zoubin Ghahramani, editor, Proceedings of the 24th
Annual International Conference on Machine Learn-
ing (ICML 2007), pages 33?40. Omnipress.
Wenliang Chen, Jun?ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving Dependency
Parsing with Subtrees from Auto-Parsed Data. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 570?579.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585.
Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce:
Simplified Data Processing on Large Clusters. Com-
mun. ACM, 51(1):107?113.
Gregory Druck and Andrew McCallum. 2010. High-
Performance Semi-Supervised Learning using Dis-
criminatively Constrained Generative Models. In Pro-
ceedings of the International Conference on Machine
Learning (ICML 2010), pages 319?326.
John Duchi and Yoram Singer. 2009. Efficient On-
line and Batch Learning Using Forward Backward
Splitting. Journal of Machine Learning Research,
10:2899?2934.
Terry Koo and Michael Collins. 2010. Efficient Third-
Order Dependency Parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1?11.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In Pro-
ceedings of ACL-08: HLT, pages 595?603.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the International Conference on Ma-
chine Learning (ICML 2001), pages 282?289.
Dekang Lin and Xiaoyun Wu. 2009. Phrase Cluster-
ing for Discriminative Learning. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
1030?1038.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimization.
Math. Programming, Ser. B, 45(3):503?528.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo Parsers: Depen-
dency Parsing by Approximate Variational Inference.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?
44.
Lev Ratinov and Dan Roth. 2009. Design Challenges
and Misconceptions in Named Entity Recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL-2009),
pages 147?155.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
Sequential Labeling and Segmentation Using Giga-
Word Scale Unlabeled Data. In Proceedings of ACL-
08: HLT, pages 665?673.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichael
Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for Depen-
dency Parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 551?560.
Erik Tjong Kim Sang and Fien De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proceed-
ings of CoNLL-2003, pages 142?147.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic Gradient Descent Training
for L1-regularized Log-linear Models with Cumula-
tive Penalty. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 477?485.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 384?394.
641
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 244?251,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Head Finalization: A Simple Reordering Rule for SOV Languages
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, Kevin Duh
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan
{isozaki,sudoh,tsukada,kevinduh}@cslab.kecl.ntt.co.jp
Abstract
English is a typical SVO (Subject-Verb-
Object) language, while Japanese is a typ-
ical SOV language. Conventional Statis-
tical Machine Translation (SMT) systems
work well within each of these language
families. However, SMT-based translation
from an SVO language to an SOV lan-
guage does not work well because their
word orders are completely different. Re-
cently, a few groups have proposed rule-
based preprocessing methods to mitigate
this problem (Xu et al, 2009; Hong et al,
2009). These methods rewrite SVO sen-
tences to derive more SOV-like sentences
by using a set of handcrafted rules. In this
paper, we propose an alternative single re-
ordering rule: Head Finalization. This
is a syntax-based preprocessing approach
that offers the advantage of simplicity. We
do not have to be concerned about part-
of-speech tags or rule weights because the
powerful Enju parser allows us to imple-
ment the rule at a general level. Our ex-
periments show that its result, Head Final
English (HFE), follows almost the same
order as Japanese. We also show that this
rule improves automatic evaluation scores.
1 Introduction
Statistical Machine Translation (SMT) is useful
for building a machine translator between a pair of
languages that follow similar word orders. How-
ever, SMT does not work well for distant language
pairs such as English and Japanese, since English
is an SVO language and Japanese is an SOV lan-
guage.
Some existing methods try to solve this word-
order problem in language-independent ways.
They usually parse input sentences and learn a re-
ordering decision at each node of the parse trees.
For example, Yamada and Knight (2001), Quirk et
al. (2005), Xia and McCord (2004), and Li et al
(2007) proposed such methods.
Other methods tackle this problem in language-
dependent ways (Katz-Brown and Collins, 2008;
Collins et al, 2005; Nguyen and Shimazu, 2006).
Recently, Xu et al (2009) and Hong et al (2009)
proposed rule-based preprocessing methods for
SOV languages. These methods parse input sen-
tences and reorder the words using a set of hand-
crafted rules to get SOV-like sentences.
If we could completely reorder the words in in-
put sentences by preprocessing to match the word
order of the target language, we would be able to
greatly reduce the computational cost of SMT sys-
tems.
In this paper, we introduce a single reordering
rule: Head Finalization. We simply move syntac-
tic heads to the end of the corresponding syntactic
constituents (e.g., phrases and clauses). We use
only this reordering rule, and we do not have to
consider part-of-speech tags or rule weights be-
cause the powerful Enju parser allows us to im-
plement the rule at a general level.
Why do we think this works? The reason is
simple: Japanese is a typical head-final language.
That is, a syntactic head word comes after non-
head (dependent) words. SOV is just one as-
pect of head-final languages. In order to imple-
ment this idea, we need a parser that outputs syn-
tactic heads. Enju is such a parser from the
University of Tokyo (http://www-tsujii.is.s.
u-tokyo.ac.jp/enju). We discuss other parsers
in section 5.
There is another kind of head: semantic heads.
Hong et al (2009) used Stanford parser (de Marn-
effe et al, 2006), which outputs semantic head-
based dependencies; Xu et al (2009) also used the
same representation.
The use of syntactic heads and the number
of dependents are essential for the simplicity of
244
Head Finalization (See Discussion). Our method
simply checks whether a tree node is a syntactic
head. We do not have to consider what we are
moving and how to move it. On the other hand, Xu
et al had to introduce dozens of weighted rules,
probably because they used the semantic head-
based dependency representation without restric-
tion on the number of dependents.
The major difference between our method and
the above conventional methods, other than its
simplicity, is that our method moves not only verbs
and adjectives but also functional words such as
prepositions.
2 Head Finalization
Figure 1 shows Enju?s XML output for the simple
sentence: ?John hit a ball.? The tag <cons>
indicates a nonterminal node and <tok> indicates
a terminal node or a word (token). Each node has
a unique id. Head information is given by the
node?s head attribute. For instance, node c0?s head
is node c3, and c3 is a VP, or verb phrase. Thus,
Enju treats not only words but also non-terminal
nodes as heads.
Enju outputs at most two child nodes for each
node. One child is a head and the other is a depen-
dent. c3?s head is c4, which is VX, or a fragment of
a verb phrase. c4?s head is t1 or hit, which is VBD
or a past-tense verb. The upper picture of Figure 2
shows the parse tree graphically. Here, ? indicates
an edge that is linked from a ?head.?
Our Head Finalization rule simply swaps two
children when the head child appears before the
dependent child. In the upper picture of Fig. 2, c3
has two children c4 and c5. Here, c3?s head c4
appears before c5, so c4 and c5 are swapped.
The lower picture shows the swapped result.
Then we get John a ball hit, which has the
same word order as its Japanese translation jon wa
bohru wo utta except for the functional words a,
wa, and wo.
We have to add Japanese particles wa (topic
marker) or ga (nominative case marker) for John
and wo (objective case marker) for ball to get an
acceptable Japanese sentence.
It is well known that SMT is not good at gen-
erating appropriate particles from English, whitch
does not have particles. Particle generation was
tackled by a few research groups (Toutanova and
Suzuki, 2007; Hong et al, 2009).
Here, we use Enju?s output to generate seeds
?sentence id=?s0? parse status=?success??
?cons id=?c0? cat=?S? xcat=?? head=?c3??
?cons id=?c1? cat=?NP? xcat=?? head=?c2??
?cons id=?c2? cat=?NX? xcat=?? head=?t0??
?tok id=?t0? cat=?N? pos=?NNP?
base=?john??John?/tok?
?/cons?
?/cons?
?cons id=?c3? cat=?VP? xcat=?? head=?c4??
?cons id=?c4? cat=?VX? xcat=?? head=?t1??
?tok id=?t1? cat=?V? pos=?VBD? base=?hit?
arg1=?c1? arg2=?c5??hit?/tok?
?/cons?
?cons id=?c5? cat=?NP? xcat=?? head=?c7??
?cons id=?c6? cat=?DP? xcat=?? head=?t2?
?tok id=?t2? cat=?D? pos=?DT? base=?a?
arg1=?c7??a?/tok?
?/cons?
?cons id=?c7? cat=?NX? xcat=?? head=?t3??
?tok id=?t3? cat=?N? pos=?NN?
base=?ball??ball?/tok?
?/cons?
?/cons?
?/cons?
?/cons?
.?/sentence?
Figure 1: Enju?s XML output (some attributes are
removed for readability).
t0
John
t1
hit
t2
a
t3
ball
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Original English?
t0
John
jon (wa)
t1
hit
utta
t2
a
?
t3
ball
bohru (wo)
c7?c6?
c5?
c4?
c3?
c2?
c1?
c0 Head Final English?
Figure 2: Head Finalization of a simple sentence
(? indicates a head).
245
2
John
5
went
7
to
9
the
10
police
12
because
15
Mary
17
lost
19
his
20
wallet
1? 14?8? 18?
6?
4? 16?
13?
11?
3?
0 Original English?
2
John
jon (wa)
5
Mary
meari (ga)
19
his
kare no
20
wallet
saifu (wo)
17
lost
nakushita
12
because
node
9
the
?
10
police
keisatsu
7
to
ni
5
went
itta
1? 14? 8?18?
6?
4?16?
13?
11?
3 ?
0 Head Final English?
Figure 3: Head-Finalizing a complex sentence.
for particles. As Fig. 1 shows, the verb hit has
arg1="c1" and arg2="c5". This indicates that c1
(John) is the subject of hit and c5 (a ball) is
the object of hit. We add seed words va1 after
arg1 and va2 after arg2. Then, we obtain John
va1 a ball va2 hit. We do not have to add
arg2 for be because be?s arg2 is not an object but
a complement. We introduced the idea of particle
seed words independently but found that it is very
similar to Hong et al (2009)?s method for Korean.
Figure 3 shows Enju?s parse tree for a
more complicated sentence ?John went to the
police because Mary lost his wallet.? For
brevity, we hide the terminal nodes, and we re-
moved the nonterminal nodes? prefix c.
Conventional Rule-Based Machine Translation
(RBMT) systems swap X and Y of ?X because Y?
and move verbs to the end of each clause. Then we
get ?Mary his wallet lost because John the police
to went.? Its word-to-word translation is a fluent
Japanese sentence: meari (ga) kare no saifu (wo)
nakushita node jon (wa) keisatsu ni itta.
On the other hand, our Head Finalization with
particle seed words yields a slightly different word
order ?John va1 Mary va1 his wallet va2 lost
because the police to went.? Its word-to-word
translation is jon wa meari ga kare no saifu wo
nakushita node keisatsu ni itta. This is also an ac-
ceptable Japanese sentence.
This difference comes from the syntactic role
of ?because.? In our method, Enju states that
because is a dependent of went, whereas RBMT
systems treat because as a clause conjunction.
When we use Xu et al?s preprocessing method,
?because? moves to the beginning of the sentence.
We do not know a good monotonic translation of
the result.
Preliminary experiments show that HFE looks
good as a first approximiation of Japanese word
order. However, we can make it better by intro-
ducing some heuristic rules. (We did not see the
test set to develop these heuristic rules.)
From a preliminary experiment, we found that
coordination expressions such as A and B and A
or B are reordered as B and A and B or A. Al-
though A and B have syntactically equal positions,
the order of these elements sometimes matters.
Therefore, we decided to stop swapping them at
coordination nodes, which are indicated cat and
xcat attributes of the Enju output. We call this
the coordination exception rule. In addition,
we avoid Enju?s splitting of numerical expressions
such as ?12,345? and ?(1)? because this splitting
leads to inappropriate word orders.
246
3 Experiments
In order to show how closely our Head Finaliza-
tion makes English follow Japanese word order,
we measured Kendall?s ? , a rank correlation co-
efficient. We also measured BLEU (Papineni et
al., 2002) and other automatic evaluation scores to
show that Head Finalization can actually improve
the translation quality.
We used NTCIR7 PAT-MT?s Patent corpus (Fu-
jii et al, 2008). Its training corpus has 1.8 mil-
lion sentence pairs. We used MeCab (http://
mecab.sourceforge.net/) to segment Japanese
sentences.
3.1 Rough evaluation of reordering
First, we examined rank correlation between Head
Final English sentences produced by the Head Fi-
nalization rule and Japanese reference sentences.
Since we do not have handcrafted word alignment
data for an English-to-Japanese bilingual corpus,
we used GIZA++ (Och and Ney, 2003) to get au-
tomatic word alignment.
Based on this automatic word alignment, we
measured Kendall?s ? for the word order between
HFE sentences and Japanese sentences. Kendall?s
? is a kind of rank correlation measure defined as
follows. Suppose a list of integers such as L = [2,
1, 3, 4]. The number of all integer pairs in this list
is 4C2 = 4 ? 3/(2 ? 1) = 6. The number of in-
creasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4),
and (3, 4). Kendall?s ? is defined by
? = #increasing pairs
#all pairs
? 2? 1.
In this case, we get ? = 5/6? 2? 1 = 0.667.
For each sentence in the training data,
we calculate ? based on a GIZA++ align-
ment file, en-ja.A3.final. (We also tried
ja-en.A3.final, but we got similar results.) It
looks something like this:
John hit a ball .
NULL ({3}) jon ({1}) wa ({}) bohru ({4})
wo ({}) utta ({2}) . ({5})
Numbers in ({ }) indicate corresponding En-
glish words. The article ?a? has no correspond-
ing word in Japanese, and such words are listed
in NULL ({ }). From this alignment information,
we get an integer list [1, 4, 2, 5]. Then, we get
? = 5/4C2 ? 2? 1 = 0.667.
For HFE in Figure 2, we will get the following
alignment.
John va1 a ball va2 hit .
NULL ({3}) jon ({1}) wa ({2}) bohru ({4})
wo ({5}) utta ({6}) . ({7})
Then, we get [1, 2, 4, 5, 6, 7] and ? = 1.0. We
use ? or the average of ? over all training sentences
to observe the tendency.
Sometimes, one Japanese word corresponds to
an English phrase:
John went to Costa Rica .
NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5})
ni ({3}) itta ({2}) . ({6})
We get [1, 4, 5, 3, 2, 6] from this alignment.
When the same word (or derivative words) ap-
pears twice or more in a single English sentence,
two or more non-consecutive words in the English
sentence are aligned to a single Japanese word:
rate of change of speed
NULL ({}) sokudo ({5}) henka ({3})
no ({2 4}) wariai ({1})
We excluded the ambiguously aligned words (2
4) from the calculation of ? . We use only [5, 3,
1] and get ? = ?1.0. The exclusion of these
words will be criticized by statisticians, but even
this rough calculation of ? sheds light on the weak
points of Head Finalization.
Because of this exclusion, the best value ? =
1.0 does not mean that we obtained the perfect
word ordering, but low ? values imply failures. In
section 4, we use ? to analyze failures.
By examining low ? sentences, we found that
patent documents have a lot of expressions such
as ?motor 2.? These are reordered (2 motor) and
slightly degrade ? . We did not notice this problem
until we handled the patent corpus because these
expressions are rare in other documents such as
news articles. Here, we added a rule to keep these
expressions.
We did not use any dictionary in our experi-
ment, but if we add dictionary entries to the train-
ing data, it raises ? because most entries are short.
One-word entries do not affect ? because we can-
not calculate ? . Most multi-word entries are short
noun phrases that are not reordered (? = 1.0).
Therefore, we should exclude dictionary entries
from the calculation of ? .
3.2 Quality of translation
It must be noted that the rank correlation does not
directly measure the quality of translation. There-
fore, we also measured BLEU and other automatic
evaluation scores of the translated sentences. We
used Moses (Koehn, 2010) for Minimum Error
Rate Training and decoding.
247
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of English sentences
0%
5%
10%
15%
20%
-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0
? of Head Finalized English sentences
Figure 4: Distribution of ?
We used the development set (915 sentences) in
the NTCIR7 PAT-MT PSD data as well as the for-
mal run test set (1,381 sentences).
In the NTCIR7 PAT-MT workshop held in 2008,
its participants used different methods such as hi-
erarchical phrase-based SMT, RBMT, and EBMT
(Example-Based Machine Translation). However,
the organizers? Moses-based baseline system ob-
tained the best BLEU score.
4 Results
First, we show ? values to evaluate word order,
and then we show BLEU and other automatic eval-
uation scores.
4.1 Rank correlation
The original English sentences have ? = 0.451.
Head Finalization improved it to 0.722. Figure
4 shows the distribution of ? for all training sen-
tences. HFE reduces the percentage of low ? sen-
tences: 49.6% of the 1.8 million HFE sentences
have ? ? 0.8 and 15.1% have ? = 1.0.
We also implemented Xu et al?s method with
the Stanford parser 1.6.2. Its ? was 0.624. The
rate of the sentences with ? ? 0.8 was 30.6% and
the rate of ? = 1.0 was 4.3%.
We examined low ? sentences of our method
and found the following reasons for low ? values.
? The sentence pair is not an exact one-to-one
translation. A Japanese reference sentence
for ?I bought the cake.? can be some-
thing like ?The cake I bought.? or ?The
person who bought the cake is me.?
? Mistakes in Enju?s tagging or parsing. We
encountered certain POS tag mistakes:
? VBZ/NNS mistake: ?advances? of ?. . .
device advances along . . .? is VBZ,
main cause count
tagging/parsing mistakes 12
VBN/VBD mistake (4)
VBZ/NNS mistake (2)
comma or and (2)
inexact translation 7
wrong alignment 1
Table 1: Main causes of 20 worst sentences
but NNS is assigned.
? VBN/VBD mistake: ?encoded? of
?. . . the error correction encoded
data is supplied . . .? is VBN, but
VBD is assigned.
These tagging mistakes lead to global parsing
mistakes. In addition, just like other parsers,
Enju tends to make mistakes when a sentence
has a comma or ?and.?
? Mistakes/Ambiguity of GIZA++ automatic
word alignment. Ambiguity happens when
a single sentence has two or more occur-
rences of a word or derivatives of a word
(e.g., difference/different/differential). As we
described above, ambiguously aligned words
are removed from calculation of ? , and small
reordering mistakes in other words are em-
phasized.
We analyzed the 20 worst sentences with ? <
?0.5 when we used only 400,000 sentences for
GIZA++. Their causes are summarized in Table
1. In general, low ? sentences have two or more
causes, but here we show only the most influen-
tial cause for each sentence. This table shows that
mistakes in tagging and parsing are major causes
of low ? values. When we used all of 1.8 million
248
Method BLEU WER TER
proposed (0) 30.79 0.663 0.554
proposed (3) 30.97 0.665 0.554
proposed (6) 31.21 0.660 0.549
proposed (9) 31.11 0.661 0.549
proposed (12) 30.98 0.662 0.551
proposed (15) 31.00 0.662 0.552
no va (6) 30.99 0.669 0.559
Organizer 30.58 0.755 0.592
Table 2: Automatic Evaluation of Translation
Quality (Numbers in parentheses indicate distor-
tion limits).
sentence pairs, only 11 sentences had ? < ?0.5
among the 1.8 million sentences.
4.2 Automatic Evaluation of Translation
Quality
In general, it is believed that translation between
English and Japanese requires a large distortion
limit (dl), which restricts how far a phrase can
move. SMT reasearchers working on E-J or J-
E translation often use dl=?1 (unlimited) as a
default value, and this takes a long translation
time.
For PATMT J-E translation, Katz-Brown and
Collins (2008) showed that dl=unlimited is the
best and it requires a very long translation time.
For PATMT E-J translation, Kumai et al (2008)
claimed that they achieved the best result ?when
the distortion limit was 20 instead of ?1.?
Table 2 compares the single-reference BLEU
score of the proposed method and that of the
Moses-based system by the NTCIR-7 PATMT
organizers. This organizers? system was better
than all participants (Fujii et al, 2008) in terms
of BLEU. Here, we used Bleu Kit (http://
www.mibel.cs.tsukuba.ac.jp/norimatsu/
bleu kit/) following the PATMT?s overview
paper (Fujii et al, 2008). The table shows that
dl=6 gives the best result, and even dl=0 (no
reordering in Moses) gives better scores than the
organizers? Moses.
Table 2 also shows Word Error Rates (WER)
and Translation Error Rates (TER) (Snover et al,
2006). Since they are error rates, smaller is better.
Although the improvement of BLEU is not very
impressive, the score of WER is greatly reduced.
This difference comes from the fact that BLEU
measures only local word order, while WER mea-
Method ROUGE-L IMPACT PER
proposed (6) 0.480 0.369 0.390
no va (6) 0.475 0.368 0.398
Organizer 0.403 0.339 0.384
Table 3: Improvement in word order
sures global word order. Another line ?no va?
stands for our method without vas or particle
seeds. Without particle seeds, all scores slightly
drop.
Echizen-ya et al (2009) showed that IMPACT
and ROUGE-L are highly correlated to human
evaluation in evaluating J-E patent translation.
Therefore, we also used these evaluation methods
here for E-J translation. Table 3 shows that the
proposed method is also much better than the or-
ganizers? Moses in terms of these measures. With-
out particle seeds, these scores also drop slightly.
On the other hand, Position-independent Word
Error Rate (PER), which completely disregards
word order, does not change very much. These
facts indicate that our method improves word or-
der, which is the most important problem in E-J
translation.
The organizers? Moses uses dl=unlimited, and
it has been reported that its MERT training took
two weeks. On the other hand, our MERT training
with dl=6 took only eight hours on a PC: Xeon
X5570 2.93 GHz. Our method takes extra time to
parse sentences by Enju, but it is easy to run the
parser in parallel.
5 Discussion
Our method used an HPSG parser, which gives
rich information, but it is not easy to build such a
parser. It is much easier to build word dependency
parsers and Penn Treebank-style parsers. In order
use these parsers, we have to add some heuristic
rules.
5.1 Word Dependency Parsers
At first, we thought that we could substitute a word
dependency parser for Enju by simply rephrasing
a head with a modified word. Xu et al (2009)
used a semantic head-based dependency parser for
a similar purpose. Even when we use a syntac-
tic head-based dependency parser instead, we en-
countered their ?excessive movement? problem.
A straightforward application of their rules
changes
249
3
John
5
hit
7
the
8
ball
10
but
13
Sam
15
threw
17
the
18
ball
16?
14?
12?
11?
9?
6?
4?
2?
1?
0?
xcat="COOD"
cat="COOD"
Figure 5: Head Finilization does not mix up
clauses
(0) John hit the ball but Sam threw the ball.
to
(1) John the ball but Sam the ball threw hit.
Here, the two clauses are mixed up. To prevent
this, they disallow any movement across punctua-
tion and conjunctions. Then they get a better re-
sult:
(2) John the ball hit but Sam the ball threw.
When we used Enju, these clauses were not
mixed up. Enju-based Head Finalization gave the
same word order as (2):
(3) John va1 ball va2 hit but Sam va1 ball va2
throw.
Figure 5 shows Enju?s parse tree. When Head Fi-
nalization swaps the children of a mother node,
the children do not move beyond the range of
the mother node. Therefore, Head Finalization
based on Enju does not mix up the first clause
John hit the ball covered by Node 1 with the
second clause Sam threw the ball covered by
Node 11. Moreover, our coordination exception
rule keeps the order of these clauses. Thus, non-
terminal nodes in Enju?s output are useful to pro-
tect clauses.
When we use a word-dependency parser, we as-
sume that the modified words are heads. Further-
more, the Head Finalization rule is rephrased as
?move modified words after modifiers.? There-
fore, hit is moved after threw just like (2), and
the two clauses become mixed up. Consequently,
we need a heuristic rule like Xu?s.
5.2 Penn Treebank-style parsers
We also tried Charniak-Johnson?s parser (Char-
niak and Johnson, 2005). PyInputTree
(http://www.cs.brown.edu/?dmcc/software/
PyInputTree/) gives heads. Enju outputs at
most two children for a mother node, but Penn
Treebank-style parsers do not have such a limita-
tion on the number of children. This fact causes a
problem.
When we use Enju, ?This toy is popular in
Japan? is reordered as ?This toy va1 Japan in
popular is.? Its monotonic translation is fluent:
kono omocha wa nihon de ninki ga aru.
On the other hand, Charniak-Johnson?s parser
outputs the following S-expression for this sen-
tence (we added asterisks (*) to indicate heads).
(S (NP (DT This) (NN* toy))
(VP* (AUX* is)
(ADJP (JJ* popular))
(PP (IN* in) (NP (NNP* Japan)))))
Simply moving heads to the end introduces
?Japan in? between ?is? and ?popular?: this toy
va1 popular Japan in is. It is difficult to translate
this monotonically because of this interruption.
Reversing the children order (Xu et al, 2009)
reconnects is and popular. We get ?This toy
(va1) Japan in popular is? from the follow-
ing reversed S-expression.
(S (NP (DT This) (NN* toy))
(VP* (PP (IN* in) (NP (NNP* Japan)))
(ADJP (JJ* popular))
(AUX* is)))
5.3 Limitation of Head Finalization
Head Finalization gives a good first approximation
of Japanese word order in spite of its simplicity.
However, it is not perfect. In fact, a small distor-
tion limit improved the performance.
Sometimes, the Japanese language does not
have an appropriate word for monotonic transla-
tion. For instance, ?I have no time? becomes
?I va1 no time va2 have.? Its monotonic trans-
lation is ?watashi wa nai jikan wo motteiru,?
but this sentence is not acceptable. An acceptable
literal translation is ?watashi wa jikan ga nai.?
Here, ?no? corresponds to ?nai? at the end of the
sentence.
6 Conclusion
To solve the word-order problem between SVO
languages and SOV langugages, we introduced
a new reordering rule called Head Finalization.
This rule is simple, and we do not have to consider
POS tags or rule weights. We also showed that this
reordering improved automatic evaluation scores
of English-to-Japanese translation. Improvement
of the BLEU score is not very impressive, but
other evaluation scores (WER, TER, LOUGE-L,
and IMPACT) are greatly improved.
250
However, Head Finalization requires a sophis-
ticated HPSG tagger such as Enju. We showed
that severe failures are caused by Enju?s POS tag-
ging mistakes. We discussed the problems of other
parsers and how to solve them.
Our future work is to build our own parser that
makes fewer errors and to apply Head Finalization
to other SOV languages such as Korean.
Acknowledgements
We would like to thank Dr. Yusuke Miyao for
his useful advice on the usage of Enju. We also
thank anonymous reviewers for their valuable sug-
gestions.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
173?180.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of the Language Resources and Evaluation
Conference (LREC), pages 449?454.
Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimo-
hata, Atsushi Fujii, Masao Utiyama, Mikio Ya-
mamoto, Takehito Utsuro, and Noriko Kando. 2009.
Meta-evaluation of automatic evaluation methods
for machine translation using patent translation data
in NTCIR-7. In Proceedings of the 3rd Workshop on
Patent Translation, pages 9?16.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2008. Overview of the patent
translation task at the NTCIR-7 workshop. In Work-
ing Notes of the NTCIR Workshop Meeting (NTCIR),
pages 389?400.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging morpho-syntactic gap be-
tween source and target sentences for English-
Korean statistical machine translation. In Proc. of
ACL-IJCNLP, pages 233?236.
Jason Katz-Brown and Michael Collins. 2008. Syn-
tactic reordering in preprocessing for Japanese ?
English translation: MIT system description for
NTCIR-7 patent translation task. In Working Notes
of the NTCIR Workshop Meeting (NTCIR).
Philipp Koehn, 2010. MOSES, Statistical Machine
Translation System, User Manual and Code Guide.
Hiroyuki Kumai, Hirohiko Segawa, and Yasutsugu
Morimoto. 2008. NTCIR-7 patent translation ex-
periments at Hitachi. In Working Notes of the NT-
CIR Workshop Meeting (NTCIR), pages 441?444.
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,
Minghui Li, and Yi Guan. 2007. A probabilistic
approach to syntax-based reordering for statistical
machine translation. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 720?727.
Thai Phuong Nguyen and Akira Shimazu. 2006.
Improving phrase-based statistical machine transla-
tion with morphosyntactic transformation. Machine
Translation, 20(3):147?166.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL), pages 271?279.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Kristina Toutanova and Hisami Suzuki. 2007. Gener-
ating case markers in machine translation. In Proc.
of NAACL-HLT, pages 49?56.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the International Con-
ference on Computational Linguistics (COLING),
pages 508?514.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for Subject-Object-Verb languages. In Proc.
of NAACL-HLT, pages 245?253.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 523?530.
251
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 375?383,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
N-best Reranking by Multitask Learning
Kevin Duh Katsuhito Sudoh Hajime Tsukada Hideki Isozaki Masaaki Nagata
NTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{kevinduh,sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
nagata.masaaki@lab.ntt.co.jp
Abstract
We propose a new framework for N-best
reranking on sparse feature sets. The idea
is to reformulate the reranking problem as
a Multitask Learning problem, where each
N-best list corresponds to a distinct task.
This is motivated by the observation that
N-best lists often show significant differ-
ences in feature distributions. Training a
single reranker directly on this heteroge-
nous data can be difficult.
Our proposed meta-algorithm solves this
challenge by using multitask learning
(such as ?1/?2 regularization) to discover
common feature representations across N-
best lists. This meta-algorithm is simple to
implement, and its modular approach al-
lows one to plug-in different learning algo-
rithms from existing literature. As a proof
of concept, we show statistically signifi-
cant improvements on a machine transla-
tion system involving millions of features.
1 Introduction
Many natural language processing applications,
such as machine translation (MT), parsing, and
language modeling, benefit from the N-best
reranking framework (Shen et al, 2004; Collins
and Koo, 2005; Roark et al, 2007). The advan-
tage of N-best reranking is that it abstracts away
the complexities of first-pass decoding, allowing
the researcher to try new features and learning al-
gorithms with fast experimental turnover.
In the N-best reranking scenario, the training
data consists of sets of hypotheses (i.e. N-best
lists) generated by a first-pass system, along with
their labels. Given a new N-best list, the goal is
to rerank it such that the best hypothesis appears
near the top of the list. Existing research have fo-
cused on training a single reranker directly on the
entire data. This approach is reasonable if the data
is homogenous, but it fails when features vary sig-
nificantly across different N-best lists. In partic-
ular, when one employs sparse feature sets, one
seldom finds features that are simultaneously ac-
tive on multiple N-best lists.
In this case, we believe it is more advantageous
to view the N-best reranking problem as a multi-
task learning problem, where each N-best list cor-
responds to a distinct task. Multitask learning, a
subfield of machine learning, focuses on how to
effectively train on a set of different but related
datasets (tasks). Our heterogenous N-best list data
fits nicely with this assumption.
The contribution of this work is three-fold:
1. We introduce the idea of viewing N-best
reranking as a multitask learning problem.
This view is particularly apt to any general
reranking problem with sparse feature sets.
2. We propose a simple meta-algorithm that
first discovers common feature representa-
tions across N-bests (via multitask learning)
before training a conventional reranker. Thus
it is easily applicable to existing systems.
3. We demonstrate that our proposed method
outperforms the conventional reranking ap-
proach on a English-Japanese biomedical
machine translation task involving millions
of features.
The paper is organized as follows: Section 2 de-
scribes the feature sparsity problem and Section 3
presents our multitask solution. The effectiveness
of our proposed approach is validated by experi-
ments demonstrated in Section 4. Finally, Sections
5 and 6 discuss related work and conclusions.
2 The Problem of Sparse Feature Sets
For concreteness, we will describe N-best rerank-
ing in terms of machine translation (MT), though
375
our approach is agnostic to the application. In MT
reranking, the goal is to translate a foreign lan-
guage sentence f into an English sentence e by
picking from a set of likely translations. A stan-
dard approach is to use a linear model:
e? = argmax
e?N(f)
wT ? h(e, f) (1)
where h(e, f) is a D-dimensional feature vector,
w is the weight vector to be trained, and N(f) is
the set of likely translations of f , i.e. the N-best
list. The feature h(e, f) can be any quantity de-
fined in terms of the sentence pair, such as transla-
tion model and language model probabilities.
Here we are interested in situations where the
feature definitions can be quite sparse. A com-
mon methodology in reranking is to first design
feature templates based on linguistic intuition and
domain knowledge. Then, numerous features are
instantiated based on the training data seen. For
example, the work of (Watanabe et al, 2007) de-
fines feature templates based on bilingual word
alignments, which lead to extraction of heavily-
lexicalized features of the form:
h(e, f) =
?
?
?
?
?
?
?
1 if foreign word ?Monsieur?
and English word ?Mr.?
co-occur in e,f
0 otherwise
(2)
One can imagine that such features are sparse
because it may only fire for input sentences that
contain the word ?Monsieur?. For all other input
sentences, it is an useless, inactive feature.
Another common feature involves word ngram
templates, for example:
h(e, f) =
?
?
?
1 if English trigram
?Mr. Smith said? occurs in e
0 otherwise
(3)
In this case, all possible trigrams seen in the N-
best list are extracted as features. One can see
that this kind of feature can be very sensitive to
the first-pass decoder: if the decoder has loose re-
ordering constraints, then we may extract expo-
nentially many nonsense ngram features such as
?Smith said Mr.? and ?said Smith Mr.?. Granted,
the reranker training algorithm may learn that
these nonsense ngrams are indicative of poor hy-
potheses, but it is unlikely that the exact same non-
sense ngrams will appear given a different test sen-
tence.
In summary, the following issues compound to
create extremely sparse feature sets:
1. Feature templates are heavily-lexicalized,
which causes the number of features to grow
unbounded as the the amount of data in-
creases.
2. The input (f ) has high variability (e.g. large
vocabulary size), so that features for different
inputs are rarely shared.
3. The N-best list output also exhibits high vari-
ability (e.g. many different word reorder-
ings). Larger N may improve reranking per-
formance, but may also increase feature spar-
sity.
When the number of features is too large, even
popular reranking algorithms such as SVM (Shen
et al, 2004) and MIRA (Watanabe et al, 2007;
Chiang et al, 2009) may fail. Our goal here is to
address this situation.
3 Proposed Reranking Framework
In the following, we first give an intuitive com-
parison between single vs. multiple task learning
(Section 3.1), before presenting the general meta-
algorithm (Section 3.2) and particular instantia-
tions (Section 3.3).
3.1 Single vs. Multiple Tasks
Given a set of I input sentences {f i}, the training
data for reranking consists of a set of I N-best lists
{(Hi,yi)}i=1,...,I , where Hi are features and yi
are labels.
To clarify the notation:1 for an input sentence
f i, there is a N-best list N(f i). For a N-best list
N(f i), there are N feature vectors corresponding
to the N hypotheses, each with dimension D. The
collection of feature vectors for N(f i) is repre-
sented by Hi, which can be seen as a D ? N
matrix. Finally, the N -dimensional vector of la-
bels yi indicates the translation quality of each hy-
pothesis in N(f i). The purpose of the reranker
training algorithm is to find good parameters from
{(Hi,yi)}.
1Generally we use bold font h to represent a vector, bold-
capital font H to represent a matrix. Script h and h(?) may
be scalar, function, or sentence (depends on context).
376
The conventional method of training a single
reranker (single task formulation) involves opti-
mizing a generic objective such as:
argmin
w
I
?
i=1
L(w,Hi,yi) + ??(w) (4)
where w ? RD is the reranker trained on all lists,
and L(?) is some loss function. ?(w) is an op-
tional regularizer, whose effect is traded-off by the
constant ?. For example, the SVM reranker for
MT (Shen et al, 2004) defines L(?) to be some
function of sentence-level BLEU score, and ?(w)
to be the large margin regularizer.2
On the other hand, multitask learning involves
solving for multiple weights, w1,w2, . . . ,wI ,
one for each N-best list. One class of multitask
learning algorithms, Joint Regularization, solves
the following objective:
arg min
w1,..,wI
I
?
i=1
L(wi,Hi,yi) + ??(w1, ..,wI )
(5)
The loss decomposes by task but the joint regu-
larizer ?(w1, ..,wI) couples together the different
weight parameters. The key is to note that multi-
ple weights allow the algorithm to fit the heteroge-
nous data better, compared to a single weight vec-
tor. Yet these weights are still tied together so that
some information can be shared across N-best lists
(tasks).
One instantiation of Eq. 5 is ?1/?2 regular-
ization: ?(w1, ..,wI) , ||W||1,2, where W =
[w1|w2| . . . |wI ]T is a I-by-D matrix of stacked
weight vectors. The norm is computed by first tak-
ing the 2-norm on columns of W, then taking a
1-norm on the resulting D-length vector. This en-
courages the optimizer to choose a small subset of
features that are useful across all tasks.
For example, suppose two different sets of
weight vectors Wa and Wb for a 2 lists, 4 fea-
tures reranking problem. The ?1/?2 norm for Wa
is 14; the ?1/?2 norm for Wb is 12. If both have
the same loss L(?) in Eq. 5, the multitask opti-
mizer would prefer Wb since more features are
shared:
Wa :
?
4 0 0 3
0 4 3 0
?
Wb :
?
4 3 0 0
0 4 3 0
?
4 4 3 3 ? 14 4 5 3 0 ? 12
2In MT, evaluation metrics like BLEU do not exactly de-
compose across sentences, so for some training algorithms
this loss is an approximation.
3.2 Proposed Meta-algorithm
We are now ready to present our general reranking
meta-algorithm (see Algorithm 1), termed Rerank-
ing by Multitask Learning (RML).
Algorithm 1 Reranking by Multitask Learning
Input: N-best data {(Hi,yi)}i=1,...,I
Output: Common feature representation hc(e, f)
and weight vector wc
1: [optional] RandomHashing({Hi})
2: W = MultitaskLearn({(Hi ,yi)})
3: hc = ExtractCommonFeature(W)
4: {Hic} = RemapFeature({Hi}, hc)
5: wc = ConventionalReranker({(Hic ,yi)})
The first step, random hashing, is optional. Ran-
dom hashing is an effective trick for reducing the
dimension of sparse feature sets without suffer-
ing losses in fidelity (Weinberger et al, 2009;
Ganchev and Dredze, 2008). It works by collaps-
ing random subsets of features. This step can be
performed to speed-up multitask learning later. In
some cases, the original feature dimension may be
so large that hashed representations may be neces-
sary.
The next two steps are key. A multitask learn-
ing algorithm is run on the N-best lists, and a com-
mon feature space shared by all lists is extracted.
For example, if one uses the multitask objective
of Eq. 5, the result of step 2 is a set of weights
W. ExtractCommonFeature(W) then returns the
feature id?s (either from original or hashed repre-
sentation) that receive nonzero weight in any of
W.3 The new features hc(e, f) are expected to
have lower dimension than the original features
h(e, f). Section 3.3 describes in detail different
multitask methods that can be plugged-in to this
step.
The final two steps involve a conventional
reranker. In step 4, we remap the N-best list
data according to the new feature representations
hc(e, f). In step 5, we train a conventional
reranker on this common representation, which by
now should have overcome sparsity issues. Us-
ing a conventional reranker at the end allows us
to exploit existing rerankers designed for specific
NLP applications. In a sense, our meta-algorithm
simply involves a change of representation for
the conventional reranking scenario, where the
3For example in Wb, features 1-3 have nonzero weights
and are extracted. Feature 4 is discarded.
377
new representation is found by multitask methods
which are well-suited to heterogenous data.
3.3 Multitask Objective Functions
Here, we describe various multitask methods that
can be plugged in Step 2 of Algorithm 1. Our
goal is to demonstrate that a wide range of existing
methods from the multitask learning literature can
be brought to our problem. We categorize multi-
task methods into two major approaches:
1. Joint Regularization: Eq. 5 is an exam-
ple of joint regularization, with ?1/?2 norm being
a particular regularizer. The idea is to use the reg-
ularizer to ensure that the learned functions of re-
lated tasks are close to each other. The popular
?1/?2 objective can be optimized by various meth-
ods, such as boosting (Obozinski et al, 2009) and
convex programming (Argyriou et al, 2008). Yet
another regularizer is the ?1/?? norm (Quattoni et
al., 2009), which replaces the 2-norm with a max.
One could also define a regularizer to ensure
that each task-specific wi is close to some average
parameter, e.g.
?
i ||wi ? wavg||2. If we inter-
pret wavg as a prior, we begin to see links to Hier-
archical Bayesian methods for multitask learning
(Finkel and Manning, 2009; Daume, 2009).
2. Shared Subspace: This approach assumes
that there is an underlying feature subspace that
is common to all tasks. Early works on multi-
task learning implement this by neural networks,
where different tasks have different output layers
but share the same hidden layer (Caruana, 1997).
Another method is to write the weight vector
as two parts w = [u;v] and let the task-specific
function be uT ? h(e, f) + vT ? ? ? h(e, f) (Ando
and Zhang, 2005). ? is a D??D matrix that maps
the original features to a subspace common to all
tasks. The new feature representation is computed
by the projection hc(e, f) , ? ? h(e, f).
Multitask learning is a vast field and relates to
areas like collaborative filtering (Yu and Tresp,
2005) and domain adaptation. Most methods as-
sume some common representation and is thus ap-
plicable to our framework. The reader is urged to
refer to citations in, e.g. (Argyriou et al, 2008) for
a survey.
4 Experiments and Results
As a proof of concept, we perform experiments
on a MT system with millions of features. We
use a hierarchical phrase-based system (Chiang,
100 101 102 103 104
10?7
10?6
10?5
10?4
10?3
10?2
10?1
100
P(
fea
tur
e o
cc
urs
 in
 x 
lis
ts)
x
Figure 1: This log-log plot shows that there are
many rare features and few common features. The
probability that a feature occurs in x number of N-
best lists behaves according to the power-law x??,
where ? = 2.28.
2007) to generate N-best lists (N=100). Sparse
features used in reranking are extracted according
to (Watanabe et al, 2007). Specifically, the major-
ity are lexical features involving joint occurrences
of words within the N-best lists and source sen-
tences.
It is worth noting that the fact that the first pass
system is a hierarchical system is not essential to
the feature extraction step; similar features can be
extracted with other systems as first-pass, e.g. a
phrase-based system. That said, the extent of the
feature sparsity problem may depend on the per-
formance of the first-pass system.
We experiment with medical domain MT, where
large numbers of technical vocabulary cause spar-
sity challenges. Our corpora consists of English
abstracts from PubMed4 with their Japanese trans-
lations. The first-pass system is built on hierarchi-
cal phrases extracted from 17k sentence pairs and
target (Japanese) language models trained on 800k
medical-domain sentences. For our reranking ex-
periments, we used 500 lists as the training set5,
500 lists as held-out, and another 500 for test.
4.1 Data Characteristics
We present some statistics to illustrate the feature
sparsity problem: From 500 N-best lists, we ex-
tracted a total of 2.4 million distinct features. By
type, 75% of these features occur in only one N-
best list in the dataset. Less than 3% of features
4A database of the U.S. National Library of Medicine.
5In MT, training data for reranking is sometimes referred
to as ?dev set? to distinguish from the data used in first-pass.
Also, while the 17k bitext may seem small compared to other
MT work, we note that 1st pass translation quality (around 28
BLEU) is high enough to evaluate reranking methods.
378
occur in ten or more lists. The distribution of fea-
ture occurrence is clearly Zipfian, as seen in the
power-law plot in Figure 1.
We can also observe the feature growth rate (Ta-
ble 1). This is the number of new features intro-
duced when an additional N-best list is seen. It is
important to note that on average, 2599 new fea-
tures are added everytime a new N-best list is seen.
This is as much as 2599/4188 = 62% of the ac-
tive features. Imagine an online training algorithm
(e.g. MIRA or perceptron) on this kind of data:
whenever a loss occurs and we update the weight
vector, less than half of the weight vector update
applies to data we have seen thus far. Herein lies
the potential for overfitting.
From observing the feature grow rate, one may
hypothesize that adding large numbers of N-best
lists to the training set (500 in the experiments
here) may not necessarily improve results. While
adding data potentially improves the estimation
process, it also increases the feature space dramat-
ically. Thus we see the need for a feature extrac-
tion procedure.
(Watanabe et al, 2007) also reports the possibil-
ity of overfitting in their dataset (Arabic-English
newswire translation), especially when domain
differences are present. Here we observe this ten-
dency already on the same domain, which is likely
due to the highly-specialized vocabulary and the
complex sentence structures common in research
paper abstracts.
4.2 MT Results
Our goal is to compare different feature represen-
tations in reranking: The baseline reranker uses
the original sparse feature representation. This is
compared to feature representations discovered by
three different multitask learning methods:
? Joint Regularization (Obozinski et al, 2009)
? Shared Subspace (Ando and Zhang, 2005)
? Unsupervised Multitask Feature Selection
(Abernethy et al, 2007).6
We use existing implementations of the above
methods.7 The conventional reranker (Step 5, Al-
6This is not a standard multitask algorithm since most
multitask algorithms are supervised. We include it to see
if unsupervised or semi-supervised multitask algorithms is
promising. Intuitively, the method tries to select subsets of
features that are correlated across multiple tasks using ran-
dom sampling (MCMC). Features that co-occur in different
tasks form a high probability path.
7Available at http://multitask.cs.berkeley.edu
Nbest id #NewFt #SoFar #Active
1 3900 3900 3900
2 7535 11435 7913
3 6078 17513 7087
4 3868 21381 4747
5 1896 23277 2645
6 3542 26819 4747
....
100 2440 289118 4299
101 1639 290757 2390
102 3468 294225 4755
103 2350 296575 3824
Average 2599 ? 4188
Table 1: Feature growth rate: For N-best list i in
the table, we have (#NewFt = number of new fea-
tures introduced since N-best i ? 1) ; (#SoFar =
Total number of features defined so far); and (#Ac-
tive = number of active features for N-best i). E.g.,
we extracted 7535 new features from N-best 2;
combined with the 3900 from N-best 1, the total
features so far is 11435.
gorithm 1) used in all cases is SVMrank.8 Our
initial experiments show that the SVM baseline
performance is comparable to MIRA training, so
we use SVM throughout. The labels for the SVM
are derived as in (Shen et al, 2004), where top
10% of hypotheses by smoothed sentence-BLEU
is ranked before the bottom 90%. All multitask
learning methods work on hashed features of di-
mension 4000 (Step 1, Algorithm 1). This speeds
up the training process.
All hyperparameters of the multitask method
are tuned on the held-out set. In particular, the
most important is the number of common features
to extract, which we pick from {250, 500, 1000}.
Table 2 shows the results by BLEU (Papineni
et al, 2002) and PER. The Oracle results are ob-
tained by choosing the best hypothesis per N-best
list by sentence-level BLEU, which achieved 36.9
BLEU in both Train and Test. A summary of our
observations is:
1. The baseline (All sparse features) overfits. It
achieves the oracle BLEU score on the train
set (36.9) but performs poorly on the test
(28.6).
2. Similar overfitting occurs when traditional ?1
regularization is used to select features on
8Available at http://svmlight.joachims.org
379
the sparse feature representation9 . ?1 reg-
ularization is a good method of handling
sparse features for classification problems,
but in reranking the lack of tying between
lists makes this regularizer inappropriate. A
small set of around 1200 features are chosen:
they perform well independently on each task
in the training data, but there is little sharing
with the test data.
3. All three multitask methods obtained features
that outperformed the baseline. The BLEU
scores are 28.8, 28.9, 29.1 for Unsupervised
Feature Selection, Joint Regularization, and
Shared Subspace, respectively, which all out-
perform the 28.6 baseline. All improvements
are statistically significant by bootstrap sam-
pling test (1000 samples, p < 0.05) (Zhang
et al, 2004).
4. Shared Subspace performed the best. We
conjecture this is because its feature projec-
tion can create new feature combinations that
is more expressive than the feature selection
used by the two other methods.
5. PER results are qualitatively similar to BLEU
results.
6. As a further analysis, we are interested in see-
ing whether multitask learning extracts novel
features, especially those that have low fre-
quency. Thus, we tried an additional feature
representation (feature threshold) which only
keeps features that occur in more than x N-
bests, and concatenate these high-frequency
features to the multitask features. The fea-
ture threshold alone achieves nice BLEU re-
sults (29.0 for x > 10), but the combination
outperforms it by statistically significant mar-
gins (29.3-29.6). This implies that multitask
learning is extracting features that comple-
ment well with high frequency features.
For the multitask features, improvements of 0.2
to 1.0 BLEU are modest but consistent. Figure
2 shows the BLEU of bootstrap samples obtained
as part of the statistical significance test. We see
that multitask almost never underperform base-
line in any random sampling of the data. This im-
plies that the proposed meta-algorithm is very sta-
9Optimized by the Vowpal Wabbit toolkit:
http://hunch.net/vw/
ble, i.e. it is not a method that sometimes improves
and sometimes degrades.
Finally, a potential question to ask is: what
kinds of features are being selected by the
multitask learning algorithms? We found that
that two kinds of features are usually selected:
one is general features that are not lexicalized,
such as ?count of phrases?, ?count of dele-
tions/insertions?, ?number of punctuation marks?.
The other kind is lexicalized features, such as
those in Equations 2 and 3, but involving functions
words (like the Japanese characters ?wa?, ?ga?,
?ni?, ?de?) or special characters (such as numeral
symbol and punctuation). These are features that
can be expected to be widely applicable, and it is
promising that multitask learning is able to recover
these from the millions of potential features. 10
?0.2 0 0.2 0.4 0.6 0.8 1 1.2
0
50
100
150
200
250
300
BLEU(shared subspace)?BLEU(baseline sparse feature)
Bo
ot
st
ra
p 
sa
m
pl
es
Figure 2: BLEU difference of 1000 bootstrap sam-
ples. 95% confidence interval is [.15, .90] The
proposed approach therefore seems to be a stable
method.
5 Related Work in NLP
Previous reranking work in NLP can be classified
into two different research focuses:
1. Engineering better features: In MT, (Och
and others, 2004) investigates features extracted
from a wide variety of syntactic representations,
such as parse tree probability on the outputs. Al-
though their results show that the proposed syntac-
tic features gave little improvements, they point to
some potential reasons, such as domain mismatch
for the parser and overfitting by the reranking
10Note: In order to do this analysis, we needed to run Joint
Regularization on the original feature representation, since
the hashed representations are less interpretable. This turns
out to be computationally prohibitive in the time being so we
only ran on a smaller data set of 50 lists. Recently new op-
timization methods that are orders of magnitude faster have
been developed (Liu et al, 2009), which makes larger-scale
experiments possible.
380
Train Test Test
Feature Representation #Feature BLEU BLEU PER
(baselines)
First pass 20 29.5 28.5 38.3
All sparse features (Main baseline) 2.4M 36.9 28.6 38.2
All sparse features w/ ?1 regularization 1200 36.5 28.5 38.6
Random hash representation 4000 33.0 28.5 38.2
(multitask learning)
Unsupervised FeatureSelect 500 32.0 28.8 37.7
Joint Regularization 250 31.8 28.9 37.5
Shared Subspace 1000 32.9 29.1 37.3
(combination w/ high-frequency features)
(a) Feature threshold x > 100 3k 31.7 27.9 38.2
(b) Feature threshold x > 10 60k 35.8 29.0 37.9
Unsupervised FeatureSelect + (b) 60.5k 36.2 29.3 37.6
Joint Regularization + (b) 60.25k 36.1 29.4 37.5
Shared Subspace + (b) 61k 36.2 29.6 37.3
Oracle (best possible) ? 36.9 36.9 33.1
Table 2: Results for different feature sets, with corresponding feature size and train/test BLEU/PER. All
multitask features give statistically significant improvements over the baselines (boldfaced), e.g. Shared
Subspace: 29.1 BLEU vs Baseline: 28.6 BLEU. Combinations of multitask features with high frequency
features also give significant improvements over the high frequency features alone.
method. Recent work by (Chiang et al, 2009) de-
scribes new features for hierarchical phrase-based
MT, while (Collins and Koo, 2005) describes
features for parsing. Evaluation campaigns like
WMT (Callison-Burch et al, 2009) and IWSLT
(Paul, 2009) also contains a wealth of information
for feature engineering in various MT tasks.
2. Designing better training algorithms: N-
best reranking can be seen as a subproblem of
structured prediction, so many general structured
prediction algorithms (c.f. (Bakir et al, 2007))
can be applied. In fact, some structured predic-
tion algorithms, such as the MIRA algorithm used
in dependency parsing (McDonald et al, 2005)
and MT (Watanabe et al, 2007) uses iterative
sets of N-best lists in its training process. Other
training algorithms include perceptron-style algo-
rithms (Liang et al, 2006), MaxEnt (Charniak and
Johnson, 2005), and boosting variants (Kudo et al,
2005).
The division into two research focuses is conve-
nient, but may be suboptimal if the training algo-
rithm and features do not match well together. Our
work can be seen as re-connecting the two focuses,
where the training algorithm is explicitly used to
help discover better features.
Multitask learning is currently an active subfield
within machine learning. There has already been
some applications in NLP: For example, (Col-
lobert and Weston, 2008) uses a deep neural net-
work architecture for multitask learning on part-
of-speech tagging, chunking, semantic role label-
ing, etc. They showed that jointly learning these
related tasks lead to overall improvements. (De-
selaers et al, 2009) applies similar methods for
machine transliteration. In information extraction,
learning different relation types can be naturally
cast as a multitask problem (Jiang, 2009; Carlson
et al, 2009). Our work can be seen as following
the same philosophy, but applied to N-best lists.
In other areas, (Reichart et al, 2008) introduced
an active learning strategy for annotating multitask
linguistic data. (Blitzer et al, 2006) applies the
multitask algorithm of (Ando and Zhang, 2005)
to domain adaptation problems in NLP. We expect
that more novel applications of multitask learning
will appear in NLP as the techniques become scal-
able and standard.
6 Discussion and Conclusion
N-best reranking is a beneficial framework for ex-
perimenting with large feature sets, but unfortu-
nately feature sparsity leads to overfitting. We ad-
dressed this by re-casting N-best lists as multitask
381
learning data. Our MT experiments show consis-
tent statistically significant improvements.
From the Bayesian view, multitask formulation
of N-best lists is actually very natural: Each N-
best is generated by a different data-generating
distribution since the input sentences are different,
i.e. p(e|f1) 6= p(e|f2). Yet these N-bests are re-
lated since the general p(e|f) distribution depends
on the same first-pass models.
The multitask learning perspective opens up
interesting new possibilities for future work, e.g.:
? Different ways to partition data into tasks,
e.g. clustering lists by document structure, or
hierarchical clustering of data
? Multitask learning on lattices or N-best lists
with larger N. It is possible that a larger hy-
pothesis space may improve the estimation of
task-specific weights.
? Comparing multitask learning to sparse on-
line learning of batch data, e.g. (Tsuruoka et
al., 2009).
? Modifying the multitask objective to incorpo-
rate application-specific loss/decoding, such
as Minimum Bayes Risk (Kumar and Byrne,
2004)
? Using multitask learning to aid large-scale
feature engineering and visualization.
Acknowledgments
We have received numerous helpful comments
throughout the course of this work. In partic-
ular, we would like to thank Albert Au Yeung,
Jun Suzuki, Shinji Watanabe, and the three anony-
mous reviewers for their valuable suggestions.
References
Jacob Abernethy, Peter Bartlett, and Alexander
Rakhlin. 2007. Multitask learning with expert ad-
vice. In COLT.
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks
and unlabeled data. JMLR.
Andreas Argyriou, Theodoros Evgeniou, and Massim-
iliano Pontil. 2008. Convex multitask feature learn-
ing. Machine Learning, 73(3).
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola,
B. Taskar, and S. V. N. Vishwanathan, editors. 2007.
Predicting structured data. MIT Press.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In EMNLP.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In
WMT.
Andrew Carlson, Justin Betteridge, Estevam Hruschka,
and Tom Mitchell. 2009. Coupling semi-supervised
learning of categories and relations. In NAACL
Workshop on Semi-supervised learning for NLP
(SSLNLP).
Rich Caruana. 1997. Multitask learning. Machine
Learning, 28.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
David Chiang, Wei Wang, and Kevin Knight. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural langauge parsing. Computa-
tional Linguistics, 31(1).
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In ICML.
Hal Daume. 2009. Bayesian multitask learning with
latent hierarchies. In UAI.
Thomas Deselaers, Sasa Hasan, Oliver Bender, and
Hermann Ney. 2009. A deep learning approach to
machine transliteration. In WMT.
Jenny Rose Finkel and Chris Manning. 2009. Hier-
archical Bayesian domain adaptation. In NAACL-
HLT.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In ACL-
2008 Workshop on Mobile Language Processing.
Jing Jiang. 2009. Multitask transfer learning for
weakly-supervised relation extraction. In ACL.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree fea-
tures. In ACL.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In ACL.
382
J. Liu, S. Ji, and J. Ye. 2009. Multi-task feature learn-
ing via efficient l2,1-norm minimization. In UAI.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large margin training of de-
pendency parsers. In ACL.
Guillaume Obozinski, Ben Taskar, and Michael Jor-
dan. 2009. Joint covariate selection and joint sub-
space selection for multiple classification problems.
Statistics and Computing.
F.J. Och et al 2004. A smorgasbord of features for
statistical machine translation. In HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In ACL.
Michael Paul. 2009. Overview of the iwslt 2009 eval-
uation campaign. In IWSLT.
Ariadna Quattoni, Xavier Carreras, Michael Collins,
and Trevor Darrell. 2009. An efficient projection
for L1-Linfinity regularization. In ICML.
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari
Rappoport. 2008. Multi-task active learning for lin-
guistic annotations. In ACL.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Computer Speech and Language, 21(2).
Libin Shen, Anoop Sarkar, and Franz Och. 2004. Dis-
criminative reranking for machine translation. In
HLT-NAACL.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumulative
penalty. In ACL-IJCNLP.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In EMNLP-
CoNLL.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In ICML.
Kai Yu and Volker Tresp. 2005. Learning to learn and
collaborative filtering. In NIPS-2005 Workshop on
Inductive Transfer.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
LREC.
383
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 287?292,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Dependency-based Automatic Enumeration of Semantically Equivalent
Word Orders for Evaluating Japanese Translations
Hideki Isozaki, Natsume Kouchi
Okayama Prefectural University
111 Kuboki, Soja-shi, Okayama, 719-1197, Japan
isozaki@cse.oka-pu.ac.jp
Tsutomu Hirao
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Sorakugun, Kyoto, 619-0237, Japan
hirao.tsutomu@lab.ntt.co.jp
Abstract
Scrambling is acceptable reordering of
verb arguments in languages such as
Japanese and German. In automatic eval-
uation of translation quality, BLEU is
the de facto standard method, but BLEU
has only very weak correlation with hu-
man judgements in case of Japanese-to-
English/English-to-Japanese translations.
Therefore, alternative methods, IMPACT
and RIBES, were proposed and they have
shown much stronger correlation than
BLEU. Now, RIBES is widely used in
recent papers on Japanese-related transla-
tions. RIBES compares word order of MT
output with manually translated reference
sentences but it does not regard scram-
bling at all. In this paper, we present a
method to enumerate scrambled sentences
from dependency trees of reference sen-
tences. Our experiments based on NTCIR
Patent MT data show that the method im-
proves sentence-level correlation between
RIBES and human-judged adequacy.
1 Introduction
Statistical Machine Translation has grown with an
automatic evaluation method BLEU (Papineni et
al., 2002). BLEU measures local word order by n-
grams and does not care about global word order.
In JE/EJ translations, this insensitivity degrades
BLEU?s correlation with human judgements.
Therefore, alternative automatic evaluation
methods are proposed. Echizen-ya and Araki
(2007) proposed IMPACT. Isozaki et al. (2010)
presented the idea of RIBES. Hirao et al. (2011)
named this method ?RIBES? (Rank-based Intu-
itive Bilingual Evaluation Score). This version of
RIBES was defined as follows:
RIBES = NKT? P
?
Table 1: Meta-evaluation of NTCIR-7 JE task data
(Spearman?s ?, System-level correlation)
BLEU METEOR ROUGE-L IMPACT RIBES
0.515 0.490 0.903 0.826 0.947
where NKT (Normalized Kendall?s ? ) is defined
by (? + 1)/2. This NKT is used for measur-
ing word order similarity between a reference sen-
tence and an MT output sentence. Thus, RIBES
penalizes difference of global word order. P is
precision of unigrams. RIBES is defined for each
test sentence and averaged RIBES is used for eval-
uating the entire test corpus.
Table 1 is a table in an IWSLT-2012 invited
talk (http://hltc.cs.ust.hk/iwslt/slides/
Isozaki2012 slides.pdf). METEOR was pro-
posed by Banerjee and Lavie (2005). ROUGE-L
was proposed by Lin and Och (2004). According
to this table, RIBES with ? = 0.2 has a very
strong correlation (Spearman?s ? = 0.947) with
human-judged adequacy. For each sentence,
we use the average of adequacy scores of three
judges. Here, we call this average ?Adequacy?.
We focus on Adequacy because current SMT
systems tend to output inadequate sentences.
Note that only single reference translations are
available for this task although use of multiple
references is common for BLEU.
RIBES is publicly available from http://
www.kecl.ntt.co.jp/icl/lirg/ribes/ and
was used as a standard quality measure in recent
NTCIR PatentMT tasks (Goto et al., 2011; Goto
et al., 2013). Table 2 shows the result of meta-
evaluation at NICTR-9/10 PatentMT. The table
shows that RIBES is more reliable than BLEU
and NIST.
Current RIBES has the following improve-
ments.
? BLEU?s Brevity Penalty (BP) was introduced
287
Table 2: Meta-evaluation at NTCIR-9/10
PatentMT (Spearman?s ?, Goto et al. 2011, 2013)
BLEU NIST RIBES
NTCIR-9 JE ?0.042 ?0.114 0.632
NTCIR-9 EJ ?0.029 ?0.074 0.716
NTCIR-10 JE 0.31 0.36 0.88
NTCIR-10 EJ 0.36 0.22 0.79
in order to penalize too short sentences.
RIBES = NKT? P
?
? BP
?
where ? = 0.25 and ? = 0.10. BLEU uses
BP for the entire test corpus, but RIBES uses
it for each sentence.
? The word alignment algorithm in the original
RIBES used only bigrams for disambiguation
when the same word appears twice or more
in one sentence. This restriction is now re-
moved, and longer n-grams are used to get a
better alignment.
RIBES is widely used in recent Annual Mee-
ings of the (Japanese) Association for NLP. In-
ternational conference papers on Japanese-related
translations also use RIBES. (Wu et al., 2012;
Neubig et al., 2012; Goto et al., 2012; Hayashi
et al., 2013). Dan et al. (2012) uses RIBES for
Chinese-to-Japanese translation.
However, we have to take ?scrambling? into
account when we think of Japanese word order.
Scrambling is also observed in other languages
such as German. Current RIBES does not regard
this fact.
2 Methodology
For instance, a Japanese sentence S1
jon ga sushi-ya de o-sushi wo tabe-ta .
(John ate sushi at a sushi restaurant.)
has the following acceptable word orders.
1. jon ga sushi-ya de o-sushi wo tabe-ta .
2. jon ga o-sushi wo sushi-ya de tabe-ta .
3. sushi-ya de jon ga o-sushi wo tabe-ta .
4. sushi-ya de o-sushi wo jon ga tabe-ta .
5. o-sushi wo jon ga sushi-ya de tabe-ta .
6. o-sushi wo sushi-ya de jon ga tabe-ta .
The boldface short words ?ga?, ?de?, and
?wo?, are case markers (?Kaku joshi? in
Japanese).
tabe-ta
sushi-ya dejon ga
o-sushi wo
Figure 1: Dependency Tree of S1
? ?ga? is a nominative case marker that means
the noun phrase before it is the subject of a
following verb/adjective.
? ?de? is a locative case marker that means the
noun phrase before it is the location of a fol-
lowing verb/adjective.
? ?wo? is an accusative case marker that means
the noun phrase before it is the direct object
of a following verb.
The term ?scrambling? stands for these accept-
able permutations. These case markers explicitly
show grammatical cases and reordering of them
does not hurt interpretation of these sentences. Al-
most all other permutations of words are not ac-
ceptable (?).
? jon ga de sushi-ya o-sushi tabe-ta wo .
? jon de sushi-ya ga o-sushi wo tabe-ta .
? jon tabe-ta ga o-sushi wo sushi-ya de .
? sushi-ya ga jon tabe-ta de o-sushi wo .
Most readers unfamiliar with Japanese will not
understand which word order is acceptable.
2.1 Scrambling as Post-Order Traversal of
Depenedncy Trees
Here, we describe this ?scrambling? from the
viewpoint of Computer Science. Figure 1 shows
S1?s dependency tree. Each box indicates a ?bun-
setsu? or a grammatical chunk of words. Each ar-
row starts from a modifier (dependent) to its head.
The root of S1 is ?tabe-ta? (ate). This verb
has three modifiers:
? ?jon ga? (John is its subject)
? ?sushi-ya de? (A sushi restaurant is its location)
? ?o-sushi wo? (Sushi is its object)
It is well known that Japanese is a typical head-
final language. In order to generate a head-final
word order from this dependency tree, we should
output tree nodes in post-order. That is, we have
to output all children of a node N before the node
N itself.
288
mi-ta
ato ni kabuki wo
tabe-ta
sushi-ya dejon ga
o-sushi wo
Figure 2: Dependency Tree of S2
All of the above acceptable word orders follows
this post-order. Even in post-order traverse, prece-
dence among children is not determined and this
fact leads to different permutations of children. In
the above example, the root ?tabe-ta? has three
children, and its permutation is 3! = 6.
2.2 Simple Case Marker Constraint
Figure 2 shows the dependency tree of a more
complicated sentence S2:
jon ga sushi-ya de o-sushi wo tabe-ta
ato ni kabuki wo mi-ta .
(John watched kabuki after eating sushi at a shushi
restaurant)
Kabuki is a traditional Japanese drama performed
in a theatre. In this case, the root ?mi-ta?
(watched) has two children: ?ato ni? (after it)
and ?kabuki wo? (kabuki is its object).
? ?ni? is a dative/locative case marker that
means the noun phrase before it is an indi-
rect object or a location/time of a following
verb/adjective.
In this case, we obtain 3!?2! = 12 permutations:
1. *S1P* ato ni kabuki wo mi-ta .
2. kabuki wo *S1P* ato ni mi-ta .
Here, *S1P* is any of the above 3! permutations
of S1. If we use S1?s 3 as *S1P* in S2?s 1, we get
sushi-ya de jon ga o-sushi wo tabe-ta
ato ni kabuki wo mi-ta .
However, we cannot accept all of these permu-
tations equally. For instance,
kabuki wo o-sushi wo sushi-ya de
jon ga tabe-ta ato ni mi-ta .
is comprehensible but strange. This strangness
comes from the two objective markers ?wo? be-
fore the first verb ?tabe-ta.? Which did John
eat, kabuki or sushi? Semantically, we cannot
eat kabuki (drama), and we can understand this
sentence. But syntactic ambiguity causes this
strangeness. Without semantic knowledge about
kabuki and sushi, we cannot disambiguate this
case.
For readers/listeners, we should avoid such
syntactically ambiguous sentences. Modifiers
(here, ?kabuki wo?) of a verb (here, ?mi-ta?,
watched) should not be placed before another verb
(here, ?tabe-ta?, ate).
In Japanese, verbs and adjectives are used sim-
ilarly. In general, adjectives are not modified by
?wo? case markers. Therefore, we can place ?wo?
case markers before adjectives. In the following
sentences, ?atarashii? (new) is an adjective
and placing ?inu wo? (A dog is the direct object)
before ?atarashii? does not make the sentence
ambiguous.
? atarashii ie ni inu wo ture te itta .
((Someone) took the dog to the new house.)
? inu wo atarashii ie ni ture te itta .
This idea leads to the following Simple Case
Marker Constraint:
Definition 1 (Simple Case Marker Constraint)
If a reordered sentence has a case marker phrase
of a verb that precedes another verb before the
verb, the sentence is rejected. ?wo? case markers
can precede adjectives before the verb.
This is a primitive heuristic constraint and there
must be better ways to make it more flexible.
If we use Nihongo Goi Taikei (Ikehara et al.,
1997), we will be able to implement such a flex-
isble constraint. For example, some verbs such
as ?sai-ta? (bloomed) are never modified by
?wo? case marker phrases. Therefore, the follow-
ing sentence is not ambiguous at all although the
wo phrase precedes ?sai-ta?.
? hana ga sai-ta ato ni sono ki wo mi-ta.
((Someone) saw the tree after it bloomed.)
? sono ki wo hana ga sai-ta ato ni mi-ta.
2.3 Evaluation with scrambled sentences
As we mentioned before, RIBES measures global
word order similarity between machine-translated
sentences and reference sentences. It does not re-
gard scrambling at all. When the target language
allows scrambling just like Japanese, RIBES
should consider scrambling.
Once we have a correct dependency tree of the
reference sentence, we enumerate scrambled sen-
tences by reordering children of each node. The
289
number of the reordered sentences depend on the
structure of the dependency tree.
Current RIBES code (RIBES-1.02.4) assumes
that every sentence has a fixed number of refer-
ences, but here the number of automatically gen-
erated reference sentences depends on the depen-
dency structure of the original reference sentence.
Therefore, we modified the code for variable num-
bers of reference sentences. RIBES-1.02.4 simply
uses the maximum value of the scores for different
reference sentences, and we followed it.
Here, we compare the following four methods.
? single: We use only single reference transla-
tions provided by the NTCIR organizers.
? postOrder: We generate all permutations of
the given reference sentence generated by
post-order traversals of its dependency tree.
This can be achieved by the following two
steps. First, we enumerate all permutations
of child nodes at each node. Then, we com-
bine these permutations. This is implemented
by cartesian products of the permutation sets.
? caseMarkers: We reorder only ?case marker
(kaku joshi) phrases?. Here, a ?case marker
phrase? is post-order traversal of a subtree
rooted at a case marker bunsetsu. For in-
stance, the root of the following sentence S3
has a non-case marker child ?kaburi ,?
(wear) between case marker children, ?jon
ga? and ?zubon wo? (Trousers are the ob-
ject). Figure 3 shows its dependency tree.
jon ga shiroi boushi wo kaburi ,
kuroi zubon wo hai te iru.
(John wears a white hat and wears black trousers.)
This is implemented by removing non-case
marker nodes from the set of child nodes
to be reordered in the above ?postOrder?
method. For simplicity, we do not reorder
other markers such as the topic marker ?wa?
here. This is future work.
? proposed: We reorder only contiguous case
marker children of a node, and we accept sen-
tences that satisfy the aforementioned Sim-
ple Case Marker Constraint. S3?s root node
has two case marker children, but they are
not contiguous. Therefore, we do not reorder
them. We expect that the constraint inhibit
generation of incomprehensible or mislead-
ing sentences.
hai te iru.
kaburi ,
jon ga
zubon wo
boushi wo
shiroi
kuroi
Figure 3: Dependency Tree of S3
Table 3: Distribution of the number of generated
permutations
#permutations 1 2 4 6 8 12 16 24 >24
single 100 0 0 0 0 0 0 0 0
proposed 70 20 7 3 0 0 0 0 0
caseMarkers 64 23 4 6 2 2 0 2 0
postOrder 1 17 9 11 4 12 1 12 33
3 Results
We applied the above four methods to the ref-
erence sentences of human-judged 100 sentences
of NTCIR-7 Patent MT EJ task. (Fujii et al.,
2008) We applied CaboCha (Kudo and Mat-
sumoto, 2002) to the reference sentences, and
manually corrected the dependency trees because
Japanese dependency parsers are not satisfactory
in terms of sentence accuracy (Tamura et al.,
2007).
To support this manual correction, CaboCha?s
XML output was automatically converted
to dependency tree pictures by using
cabochatrees package for L
A
T
E
X. http://
softcream.oka-pu.ac.jp/wp/wp-content/
uploads/cabochatrees.pdf. Then, it is easy
to find mistakes of the dependency trees. In
addition, CaboCha?s dependency accuracy is very
high (89?90%) (Kudo and Matsumoto, 2002).
Therefore, it took only one day to fix dependency
trees of one hundred reference sentences.
Table 3 shows distribution of the number of
word orders generated by the above methods. Pos-
tOrder sometimes generates tens of thousands of
permutations.
Figure 4 shows a sentence-level scatter plot
between Adequacy and RIBES for the baseline
Moses system. Each ? indicates a sentence.
Arrows indicate significant improvements of
RIBES scores by the proposed method. For in-
stance, the?mark at (5.0, 0.53) corresponds to an
MT output:
290
Adequacy0 1 2 3 4 5
RIBES
0
0.2
0.4
0.6
0.8
1
Average of RIBES: 0.706? 0.719
Pearson?s r: 0.607? 0.663
Spearman?s ?: 0.607? 0.670
Figure 4: Scatter plot between Adequacy and
RIBES for 100 human-judged sentences in the
output of NTCIR-7?s baseline Moses system and
the effects of the proposed method
indekkusu kohna wo zu 25 ni shimesu .
which is a Japanese translation of ?FIG.25 shows
the index corner.? The reference sentence for this
sentence is
zu 25 ni indekkusu kohna wo shimeshi
te iru .
In this case, RIBES is 0.53, but all of the three
judges evaluated this as 5 of 5-point scale. That
is, RIBES disagrees with human judges. The pro-
posed method reorders this reference sentence as
follows:
indekkusu kohna wo zu 25 ni shimeshi
te iru .
This is very close to the above MT output and
RIBES is 0.884 for this automatically reordered
reference sentence. This shows that automatic re-
ordering reduces the gap between single-reference
RIBES and Adequacy.
Although RIBES strongly correlates with ade-
quacy at the system level (Table 1), it has only
mediocre correlation with adequacy at the sen-
tence level: Spearman?s ? is 0.607 for the baseline
Moses system. The ?proposed? method improves
it to 0.670.
We can draw similar scatter plots for each sys-
tem. Table 4 summarises such improvement of
correlations. And this is the main result of this
Table 4: Improvement of sentence-level correla-
tion between Adequacy and RIBES for human-
judged NTCIR-7 EJ systems (MAIN RESULT)
Pearson?s r Spearman?s ?
single? proposed single? proposed
tsbmt 0.466 ? 0.472 0.439 ? 0.452
Moses 0.607 ? 0.663 0.607 ? 0.670
NTT 0.709 ? 0.735 0.692 ? 0.727
NICT-ATR 0.620 ? 0.631 0.582 ? 0.608
kuro 0.555 ? 0.608 0.515 ? 0.550
Table 5: Increase of averaged RIBES scores
Adeq. RIBES
system single proposed caseMarkers postOrder
tsbmt 3.527 0.715 0.7188 0.719 0.7569
moses 2.897 0.706 0.7192 0.722 0.781
NTT 2.740 0.671 0.683 0.686 0.7565
NICT-ATR 2.587 0.655 0.664 0.670 0.749
kuro 2.420 0.629 0.638 0.647 0.752
paper. The ?proposed? method consistently im-
proves sentence-level correlation between Ade-
quacy and RIBES.
Table 5 shows increase of averaged RIBES, but
this increase is not always an improvement. We
expected that ?PostOrder? generates not only ac-
ceptable sentences but also incomprehensible or
misleading sentences. This must be harmful to the
automatic evaluation by RIBES. Accoding to this
table, PostOrder gave higher RIBES scores to all
systems and correlation between RIBES and Ade-
quacy is lost as expected.
The ranking by RIBES-1.02.4 with ?single?
reference sentences completely agrees with Ad-
equacy, but the weakest constraint, ?postOrder?,
disagrees. Spearman?s ? of the two ranks is 0.800
but Pearson?s r is as low as 0.256. It generates too
many incomprehensible/misleading word orders,
and they also raise RIBES scores of bad transla-
tions. On the other hand, ?proposed? and ?case-
Markers? agree with Adequacy except the ranks
of tsbmt and the baseline Moses.
4 Concluding Remarks
RIBES is now widely used in Japanese-related
translation evaluation. But RIBES sometimes pe-
nalizes good sentences because it does not re-
gard scrambling. Once we have correct depen-
dency trees of reference sentences, we can auto-
matically enumerate semantically equivalent word
291
orders. Less constrained reordering tend to gener-
ate syntactically ambiguous sentences. They be-
come incomprehensible or misleading sentences.
In order to avoid them, we introduced Simple
Case Marker Constraint and restricted permuta-
tions to contiguous case marker children of verbs/
adjectives. Then, sentence-level correlation coef-
ficients were improved.
The proposed enumeration method is also ap-
plicable to other automatic evaluation methods
such as BLEU, IMPACT, and ROUGE-L, but we
have to modify their codes for variable numbers of
multi-reference sentences. We will examine them
in the full paper.
We hope our method is also useful for other lan-
guages that have scrambling.
Acknowledgement
This research was supported by NTT Communi-
cation Science Laboratories.
References
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with im-
proved correlation with human judgements. In Proc.
of ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for MT and Summarization, pages
65?72.
Han Dan, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proceedings of SSST-6, Sixth
Workshop on Syntax, Semantics and Structure in
Statistical Translation, pages 57?66.
Hiroshi Echizen-ya and Kenji Araki. 2007. Automatic
evaluation of machine translation based on recursive
acquisition of an intuitive common parts continuum.
In MT Summit XI, pages 151?158.
Atsushi Fujii, Masao Uchimura, Mikio Yamamoto, and
Takehito Usturo. 2008. Overview of the patent
machine translation task at the NTCIR-7 workshop.
In Working Notes of the NTCIR Workshop Meeting
(NTCIR).
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Working Notes of the NTCIR Workshop Meeting
(NTCIR).
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for japanese-english statis-
tical machine translation. In Proc. of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 311?316.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K. Tsou. 2013. Overview of the patent
machine translation task at the NTCIR-10 work-
shop. In Working Notes of the NTCIR Workshop
Meeting (NTCIR).
Katsuhiko Hayashi, Katsuhito Sudoh, Hajime Tsukada,
Jun Suzuki, and Masaaki Nagata. 2013. Shift-
reduce word reordering for machine translation.
In Proc. of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1382?1386.
Tsutomu Hirao, Hideki Isozaki, Kevin Duh, Katsuhito
Sudoh, Hajime Tsukada, and Masaaki Nagao. 2011.
RIBES: An automatic evaluation method of trans-
lation based on rank correlation (in Japanese). In
Proc. of the Annual Meeting of the Association for
Natural Language Processing, pages 1115?1118.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei ? A Japanese Lexicon (in Japanese).
Iwanami Shoten.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Kat-
suhito Sudoh, Hajime Tsukada, and Masaaki Na-
gata. 2010. Automatic evaluation of translation
quality for distant language pairs. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 944?952.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of the Conference on Computational Natural
Language Learning (CoNLL).
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of translation quality using longest com-
mon subsequences and skip-bigram statistics. In
Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), pages 605?612.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proc. of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 843?853.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 311?318.
Akihiro Tamura, Hiroya Takamura, and Manabu Oku-
mura. 2007. Japanese dependency analysis using
the ancestor-descendant relation. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 600?609.
Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsu-
jii. 2012. Akamon: An open source toolkit for
tree/forest-based statistical machine translation. In
Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), pages 127?132.
292
