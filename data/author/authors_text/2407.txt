Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 571?578, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Cost-Benefit Analysis of Hybrid Phone-Manner Representations for ASR
Eric Fosler-Lussier
Department of Computer Science and Engineering
Department of Linguistics
Ohio State University
Columbus, OH 43210
fosler@cse.ohio-state.edu
C. Anton Rytting
Department of Linguistics
Ohio State University
Columbus, OH 43210
rytting@ling.ohio-state.edu
Abstract
In the past decade, several researchers
have started reinvestigating the use of
sub-phonetic models for lexical represen-
tations within automatic speech recogni-
tion systems. Lest history repeat itself,
it may be instructive to mine the further
past for models of lexical representations
in the lexical access literature. In this
work, we re-evaluate the model of Briscoe
(1989), in which a hybrid strategy of lex-
ical representation between phones and
manner classes is promoted. While many
of Briscoe?s assumptions do not match up
with current ASR processing models, we
show that his conclusions are essentially
correct, and that reconsidering this struc-
ture for ASR lexica is an appropriate av-
enue for future ASR research.
1 Introduction
Almost every state-of-the-art large vocabulary au-
tomatic speech recognition (ASR) system requires
the sharing of sub-word units in order to achieve
the desired vocabulary coverage. Traditionally,
these sub-word units are determined by the phones
or phonemes of a language (depending on desired
detail of representation). However, phonetic (or
phonemic) representation has its pitfalls (cf. (Os-
tendorf, 1999)). Among the problems cited in
the literature are that (1) segments are often dif-
ficult for machines to recognize from the acoustic
cues alone, because the acoustic cues to a particu-
lar phoneme are multi-faceted, and (2) the intended
words and phrases are not always recoverable even
from correctly recognized segments, because speak-
ers themselves will also fail to articulate words with
the dictionary-listed phonemes. The first of these
problems refers to the discriminability of phonemes
within an inventory; the second to the reliability of
(actual) phone sequences mapping to the canonical
phonemic representations of words. This is partic-
ularly true in conversational speech (such as that
found in the Switchboard corpus), where pragmatic
context and conversational conventions assist human
comprehension (but not current ASR systems).
A common approach for handling pronunciation
variation is to introduce alternative entries into the
lexicon. However, phones that are perceived as non-
canonical (for example, when an /eh/ is heard as
an /ih/ by linguistic transcribers) often are closer
in acoustic space to the Gaussian means of the
canonical phones, rather than the perceived phones
(Sarac?lar et al, 2000). This insight suggests that
acoustic models need to be cognizant of potential
pronunciation changes. Thus the lexical and acous-
tic models should work hand in hand.
Another way to model this type of pronunciation
variation is to find the commonalities that the canon-
ical and perceived phone share in terms of a sub-
phonetic representation. In the past decade, a signif-
icant community in acoustic-phonetic ASR research
has been turning to distinctive features (Jakobson et
al., 1952) for building ASR lexica. While an ex-
haustive description of these approaches is beyond
the scope of this paper, estimates of phonological
feature probabilities have been combined to obtain
phone probabilities (Kirchhoff, 1998), or incorpo-
rated into ?feature bundles? that allow representa-
571
tion of phonological processes (Finke et al, 1999).
More recent work has integrated phonological
features into graphical models (Livescu et al, 2003)
and landmark based systems (Juneja and Espy-
Wilson, 2004). The common thread among this
research is the notion that acoustic models should
be sensitive to sub-phonetic information. With this
trend in phonological representation research, it is
time to re-examine some older hypotheses about lex-
ical access and speech processing in order to gain
some insight in this current featural renaissance.
Sub-phonetic ASR research is also driven by the
fact that deviations from canonical pronunciation
and from correct perception of phones is far from
random; indeed, there have been a number of stud-
ies demonstrating that both of these variations have
defined, modelable trends. Deviations from canon-
ical pronunciation can be described by phonologi-
cal rules, and errors in perception also tend to con-
form to phonological patterns. By and large, con-
fusions occur (at least in humans) between phones
with phonological features in common (e.g., (Miller
and Nicely, 1955)). In particular, three features
(voicing, manner, and place) have been postulated as
relatively invariant (see e.g., (Stevens, 1981), quoted
in (Church, 1987)). It follows from this phonetic de-
tection based on the most reliable features may han-
dle highly variable speech more robustly than sys-
tems which demand full identity over all the features
for a given phone or phone sequence.
Consequently, a number of researchers have pre-
viously suggested using certain broad classes of seg-
ments, rather than full phonemic identification, for a
first pass on recognition. For instance, Shipman and
Zue (1982), working on large-vocabulary isolated
word recognition, used both two-way consonant-
vowel distinctions and a six-way distinction based
on manner in order to divide their 20,000-word dic-
tionary into ?cohorts? or groups of words. They
found that this partial specification of segments re-
duced the search space of word candidates signifi-
cantly. Carlson et al (1985) found similar results
for English and four other languages.
2 A suggested compromise: a hybrid
phone-manner representation
Briscoe (1989) extended this broad-class approach
to address the problem of lexical access on con-
nected speech. However, Briscoe argues against the
use of broad, manner-based classes at all times. He
argues that manner cues provide no particular ad-
vantage for stressed syllables, but that all cues are
sufficiently reliable in stressed syllables to justify
a full segmental analysis. Working with a 30,000-
word lexicon, Briscoe shows that the manner-based
broad classes for weak (reduced) syllables, together
with full identification of strong (unreduced) sylla-
bles constrained the set of possible candidates satis-
factorily. Unfortunately, he only provides results for
one sentence from his corpus.
This approach proposes to adjust the granularity
of recognition dynamically, depending on the stress
level of the current syllable. The details of how this
would be managed are left somewhat vague. As it
stands, it would seem to depend crucially on first de-
tecting the stress of each frame, so as to determine
which alphabet of symbols to apply to incoming in-
put. Alternatively, it could recognize the broad class
as a first pass, and then refine this into a full phone-
mic analysis for stressed syllables in a second pass,
at the cost of multiplying passes through the speech
data. It is not possible in this system to recover from
the miscategorization of stress.
One possible remedy is to bypass a hard decision
on stress and run both a manner-based broad-class
detector and a traditional phonemic system in paral-
lel. These then may be combined according to the
probability of lexical stress, such that those frames
judged less likely to be stressed weight the broad-
class analysis more heavily, and those judged more
likely to be stressed weight the narrow phonemic
analysis more heavily. Its advantage is that a full
phonemic analysis is recoverable for each frame and
phone, but those in weak syllables (and hence less
likely to be accurate) weigh in less heavily.
Briscoe?s analysis is in terms of lexical access ac-
tivations: taking a cue from the lexical access com-
munity, he assumes that any ?partially activated?
word (e.g., ?boat? and ?both? being active after pro-
cessing ?bo?) will contribute linearly to the process-
ing time in ASR. However, most large-vocabulary
ASR systems today use a tree-based lexicon where
common phonetic prefixes of words are processed
only once, thus invalidating this conjecture. Briscoe
experimented with several triggers for starting a new
word ? at every phone, at the beginnings of sylla-
bles, at the beginnings of syllables with unreduced
vowels, and at the beginnings of word boundaries.
572
Category Example (Vietnamese) Cohorts
# of Words 6247
Stress pattern only
(lower bound) 0001 84
Identify phones
in stressed syllables 0001:m iy z 4709
+CV pattern of
unstressed syllables 0001:CV:VC:CV:m iy z 5609
+manner pattern of
unstressed syllables 0001:FV:CS:NV:m iy z 6076
Phonetic prons.
(upper bound) v iy . eh t . n aa . m iy z 6152
Table 1: Cohorts for varied lexical representations
However, the latter three require oracle information
as to where word or syllable boundaries can occur. A
more appropriate measure commensurate with cur-
rent ASR practice would be to only allow words to
start where a previous word hypothesis ends.
In the remainder of the paper, we seek to validate
(or invalidate) Briscoe?s claim that a hybrid phonetic
and feature model is appropriate for ASR process-
ing. In the 15 years since Briscoe?s paper, the ASR
community has developed large phonetically tran-
scribed corpora and more advanced computational
tools (such as the AT&T Finite-State Toolkit (Mohri
et al, 2001)) that we can apply to this problem.
3 Experiment 1: Effective Partitioning by
Manner-based Broad Classes
Our first experiment explores various types of broad
classes to determine the effects of these encodings
on cohort size within a sample 6,000 word dictio-
nary.1 Here we use the lexical stress-marked dictio-
nary provided with the TIMIT database (Garofolo
et al, 1993), which was syllabified using the NIST
Tsylb2 syllabifier (Fisher, 1996).
Rather than calculate cohort size directly, we cal-
culate the number of cohorts into which our dictio-
nary is partitioned, a measure which Carlson et al
(1985) showed to correlate well with expected co-
hort size. (Note that this is an inverse correlation.)
This describes the static discriminability of the lexi-
con: systems that have words with the same lexical
representation will not be able to discriminate be-
tween these two words acoustically and must rely on
the language model to discriminate between them.
1
?Cohort size? is used here (as with Shipman and Zue
(1982)) to mean the number of distinct vocabulary items that
match a particular broad-class encoding. It is not intended to
imply a particular theory of lexical access.
Before proceeding, it may useful to set upper and
lower bounds for this exercise (Table 1). An obvi-
ous upper bound is the full phonemic disambigua-
tion of every word. Of the 6247 words in the dictio-
nary, 6152 unique pronunciations are found (a few
cohorts consisting of sets of homophones). A con-
venient lower bound is the lexical stress pattern of
the word, devoid of any segmental information: e.g.,
?unidirectional? has its stress on the 4th of 6 sylla-
bles; hence, 000100 is its lexical-stress profile. 84
unique lexical-stress profiles exist in the dictionary.
Between these two bounds, three variant broad-
class partitions were explored for isolated word
recognition. All three use the lower-bound stress
profile as a starting place, combined with full phone-
mic information for the syllable with primary stress.
The first, with no additional segmental information,
produces 4709 distinct cohorts. The second adds a
consonant-vowel (CV) profile for the unstressed syl-
lables, which boosts the number of distinct cohorts
to 5609. The final partition replaces the CV pro-
file with a six-class manner-based broad-class par-
tition (Nasals, Stops, Fricatives, Glides, Liquids,
and Vowels). Including a manner-class representa-
tion for unstressed vowels increases the number of
cohorts to 6076, which is very close to the upper
bound. Thus, there is not much loss of lexical dis-
criminability when using this type of representation.
3.1 Caveats
Now, for this scheme to be maximally useful for
recognition, several conditions must obtain. First,
we have assumed that we can reliably detect lex-
ically stressed syllables within the speech signal.
Waibel (Waibel, 1988) has shown that stress cor-
relates with various acoustic cues such as spectral
change. As a side experiment, we have shown
that very basic methods provide encouraging re-
sults (only sketched here due to space constraints).
We re-annotated TIMIT with lexical stress mark-
ings, where all frames of each stressed syllable (in-
cluding onset and coda consonants, not just the nu-
cleus) were marked as stressed. A multi-layer per-
ceptron with 100 hidden units was trained to pre-
dict P (Stress|Acoustics) with a nine-frame context
window. No additional phonetic information be-
sides the binary label stressed/unstressed was used
in training. Frame-by-frame results on the TIMIT
test set were 75% accurate (chance: 52%), and when
573
MLP output was greater than 0.9, a precision of 89%
was obtained (recall: 20%). While far from perfect,
this result strongly suggests that even very simple
methods can predict lexical stress fairly reasonably.
A second assumption in the above analysis was
that words occur in isolation. It is clear that in con-
nected speech, there are a larger number of poten-
tial lexical confusions. A third assumption is that
those features we are relying upon in our partitions
(namely, all features within stressed syllables, and
manner of articulation for unstressed syllables) are
perfectly reliable and discriminable. In the next two
sections, we relax these assumptions by applying ex-
tensions of this method to connected speech.
4 Experiment 2: What does a hybrid
representation buy you?
As Experiment 1 shows, the hybrid phone/feature
representation does not drastically decrease the dis-
criminability of the (albeit small) lexicon. It is also
possible that such a representation reduces pronun-
ciation variation, by allowing the canonical repre-
sentation to more closely match actual pronuncia-
tions. For example, we have demonstrated that for
common ASR corpora (Switchboard and TIMIT),
segments in unstressed syllables were much more
likely to deviate from their canonical lexical rep-
resentation (Fosler-Lussier et al, 1999). If phones
that deviate from canonical still keep the same man-
ner class, then a dictionary built with Briscoe-esque
representations should more closely match the ac-
tual pronunciations of words in running speech (as
transcribed by a phonetician).
4.1 Method
In order to test this theory, we used phonetic data
from (Fosler-Lussier et al, 1999) in which the
ICSI phonetic transcripts of the Switchboard corpus
(Greenberg et al, 1996; NIST, 1992) were aligned to
a syllabified version of the Pronlex dictionary (Lin-
guistic Data Consortium (LDC), 1996), which has
71014 entries for 66293 words. In this alignment,
for every canonical phone given by the lexicon, there
were zero or more corresponding realized phones.
From these data we extracted the canonical and real-
ized pronunciation of each word token, for a total of
38,527 tokens. Generally, high-frequency function
words show the most variation, so they may benefit
most from a manner-based representation.
Lexicon type Strict Matching
matching w/ deletion
1) Phonetic units 37.0% 50.1%
2) Manner-based function words 50.2% 69.6%
3) + Manner for unstressed syls 53.4% 74.6%
4) + Manner for secondary stress 55.7% 77.9%
5) Manner for all syls 60.7% 85.2%
Table 2: Percent of words pronounced canonically
for phonetic and hybrid lexical representations
Given these word pronunciation data, we can ex-
amine how many word tokens have transcriptions
that match their dictionary-listed pronunciations,
given the broad-class mappings for various sets of
syllables. We built lexica and mapped phonetic tran-
scriptions according to five different criteria:
1. Every segment is phone based (no classes).
2. Function words use manner-based classes.
3. Unstressed syllables and function words use
manner only.
4. Secondary stressed syllables also use manner.
(Primary stressed syllables are phone based.)
5. Every segment uses manner-based classes.
We noted in the data (as others have done) that a
large proportion of the pronunciation variation was
due to phone deletion (29% of words) ? which
would not be handled by the manner-based lexicon.
However, it is likely that not every phone deletion
leads to an ASR error (as attested by the fact that
state-of-the-art Switchboard ASR error rates are typ-
ically less than 29%). Often there is enough residual
phonetic evidence of the deleted phone, or enough
phonetic evidence in other parts of the word, to rec-
ognize a word correctly despite the deletion. Thus,
we decided to use a two-part strategy in calculating
canonical pronunciation (Table 2). The first column,
?strict matching?, allows no insertions or deletions
when comparing the canonical and realized pronun-
ciation. ?Matching with deletion? reports the ideal
situation where phone deletions were perfectly re-
coverable in their canonical form. Including and ig-
noring deletions provides upper and lower bounds
on the true lexical access results. (Insertions are rel-
atively rare and not anticipated to affect the results
significantly, and hence are not examined.)
4.2 Results and Discussion
In Table 2, we see that a standard ASR lexicon ap-
proach (strict matching 1), does not match the tran-
574
scribed data very well, with only 37% of words
pronounced according to the dictionary. The strict
matching hybrid scenario on line 3 most closely re-
sembles Briscoe?s experiment, and shows a marked
improvement in matching the dictionary and real-
ized pronunciations; comparing the two, we see that
using manner-based broad classes reduces mismatch
by 25% of the total error (from 63% error to 47%),
most of which comes from improved modeling of
function words (line 2). Whether this gain in repre-
sentation is worthwhile will depend of course on the
cost in terms of the increased hypothesis space.
By allowing for perfect deletion recovery (which
will of necessity entail another large expansion of
the hypothesis space), a somewhat more optimistic
is obtained. Comparing the ?matching with dele-
tion? columns of lines 1 and 3, we see that a little
over half of the non-deletion pronunciation variation
is due to manner changes in unstressed syllables.
Again, a good chunk of this is in function words.
By moving to manner class for stressed syllables as
well would bring the hypothetical error from 25% to
15%, but at the cost of a huge explosion in the hy-
pothesis space (as Briscoe rightly points out and as
discussed in the next section).
One interesting implication of this data is that
over all types of segments (stressed and unstressed),
roughly three-quarters of word pronunciation vari-
ants differ from the canonical only in terms of
within-manner variation and phonetic deletion.
The moral of this story is that manner-based broad
classes may be a useful type of back off from truly
reduced and variable syllables (particularly func-
tion words), but the full benefit of such a maneuver
would only be realized after a reasonable solution
for recovering large-scale deletions is found. This
may come from predicting with increased specificity
where deletions are likely to occur (e.g., complex
codas), and what reduced realizations (e.g., of func-
tion words) are most common.
5 Experiment 3: What is the cost of a
hybrid representation?
Briscoe measured the cost of hybrid representation
in terms of the number of lexical activations that
a partially-completed word creates (see Section 2).
Yet Briscoe?s methodology has several shortcom-
ings when applied to today?s ASR technology; a
summary of the arguments presented above are: (1)
Tree-based lexica now share processing for words
with identical prefixes. (2) New words are acti-
vated only when other word hypotheses end. (3) We
now have a large amount of phonetically transcribed,
spontaneous speech. (4) Perfect stress detection is
not really achievable.
Given criticism 1, a better measure of potential
processing requirements is to generate a lattice of
hypothesized words and count the number of arcs in
the lattice. This lattice can be constructed in such
a way that criticism 2 is satisfied. In the next sec-
tion, we present a finite state machine formalism for
generating such a lattice.
We apply this technique to the phonetic transcrip-
tion of the Switchboard corpus (thus alleviating crit-
icism 3). However, this introduces several problems.
As Experiment 2 shows, many words have pronun-
ciations that do not appear in the dictionary. Thus,
we must find a way to alleviate the mismatch be-
tween the phonetic transcription and the dictionary
in a way that is plausible for ASR processing.
We can address criticism 4 by creating phone-
based and manner-based transcriptions that will run
in parallel; thus, the lattice generator would be
free to choose whichever representation allows the
matching to a dictionary word.
5.1 Method
In this experiment we consider a finite-state trans-
ducer model of the strategy described above. This
corresponds not to the ASR system as a whole, but
rather to the pronunciation model of a traditional
system. We assume that the pronunciation as given
by the transcriber is correct, but we model the trans-
formation of realized phones into canonical dictio-
nary pronunciations. Since we are only investigating
the combined acoustic-phonetic-lexical representa-
tion, we have left out the re-weighting and prun-
ing of hypotheses due to integration of a language
model, discourse model, or any other constraints.
Specifically, this model consists of three finite
state transducers composed. The first FSM, R,
encodes the representation of the realized phonetic
transcription of the spoken corpus. In order to match
this to dictionary pronunciations, we train a confu-
sion matrix on all realized/canonical phone pairs, to
obtain P (dictionary phone|transcribed phone);
these confusion probabilities are encoded as a finite
state transducer C. Thus, C is derived by computing
575
the strength of all correspondences between the
phonetic transcription of what was actually said
at the phone level and the canonical pronuncia-
tion of the corresponding words. This confusion
matrix consists of three parts, corresponding to
substitutions, insertions, and deletions.
1. Pairwise substitutions are counted to yield a
standard confusion matrix.
2. Where two or more realized phones correspond
to a single canonical phone (a rare occurrence,
as in e.g., really /r iy l iy/ ? [r ih ax l iy]), each
realized phone is allowed (independently) to be
either deleted or substituted with its pairwise
confusions from (1).
3. Deleted phones are assumed to be potentially
recoverable (as in Experiment 2), so both an
epsilon transition and the canonical pronunci-
ation are preserved in the confusion matrix.
In each of these confusion matrices, we have al-
ways preserved the pathway from each realized ut-
terance to its canonical representation for the whole
corpus. So for this seen corpus, it is always possi-
ble in theory to recover the canonical representation,
such that the right answer is always one of the pos-
sible hypotheses. While this may seem a bit strange,
here we can only overestimate the potential hypoth-
esis space (by adding the correct string and by as-
suming that deletions are recoverable); the point of
this exercise is to see the number of total hypotheses
(the search space) generated under such a system.
The third transducer, D, is the ASR dictionary that
we wish to test. Thus, composingR?C?Dwill give
the graph of all potential complete hypotheses in this
space. Figure 1 shows a pruned hypothesis graph for
the phrase ?it?s really sad? (the full hypothesis graph
has 12216 arcs).
5.2 Results and Discussion
By choosing different sub-word representations, we
can test Briscoe?s contention that backing off to
manner-based broad classes for certain (e.g., un-
stressed) syllables will reduce the search space
and/or facilitate recovery of the intended word
string. When a phone is substituted with a manner
class, we construct C so that the generated confu-
sions are over manner classes rather than phones.
0
1
at
it
out
2
it?s
sh
3
sri
re
rea
rhee
4
rio
5
real
reel
rial
6
really
riely
<PHDEL>
a
are
i
oh
uh
are
or
our
we
were
yeah
you
7
was
with
a
are
i
oh
uh
sh
8
sad
ad
add
Figure 1: Pruned hypothesis graph for It?s really sad
Figure 2 shows how the number of hypotheses per
word changes as a function of the number of words
in the hypothesis. Note that if the relationship were
linear, we would expect to see a flat line. The figure
demonstrates that that Briscoe?s conclusions were
correct, given the assumption that one can accu-
rately detect lexical stress (as illustrated by the line
with circles on 2). Across all utterances, the average
number of hypotheses per word for the hybrid dictio-
nary was 510 (roughly 1/3 of the phone-based aver-
age of 1429). However, when one allows for the fact
that stress detection is not perfect, one sees an in-
crease in the amount of necessary computation: the
non-ideal hybrid dictionary has an average of 3322
0 2 4 6 8 10 12 14 16 18 20
0
0.5
1
1.5
2
2.5
x 104 Average number of hypotheses per word by number of words in utterance
Number of words in transcript
Av
er
ag
e 
nu
m
be
r o
f h
yp
ot
he
se
s 
pe
r w
or
d
Phone?based lexicon
Phone + manner lexicon, idealized syllable stress
Phone + manner lexicon, non?idealized syllable stress
Manner?based lexicon
Figure 2: Average number of hypotheses per word
as a function of number of words in utterance
576
hypotheses per word (2.3 times the phone-based av-
erage). Yet this is much lower than the potential
growth of the hypothesis space given with manner-
only dictionaries. This dictionary generated a hy-
pothesis space 12 times as large as a phone based
dictionary (17186 hypotheses/word average); more-
over, the curve grows significantly as a function of
the number of words, so longer utterances will take
disproportionately more space. Thus, Briscoe?s hy-
pothesis that purely manner-based decoding is too
expensive seems to be confirmed.
6 Integration into ASR
This paper has investigated hybrid representations
along computational phonology lines, but we have
also trained an ASR system with a hybrid lexicon for
the Wall Street Journal (WSJ0) corpus. Space does
not permit a full explanation of the experiment here
(for more details, see (Fosler-Lussier et al, 2005)),
but we include the results from this experiment as
evidence of the validity of the approach.
In this experiment, we trained phonetic and
manner-based acoustic models for all segments us-
ing the flat-start recipe of the HTK recognizer
(Young et al, 2002). After a number of itera-
tions of EM-training, we constructed a hybrid set of
acoustic models and lexicon in which phones in un-
stressed syllables were replaced with manner classes
(Hybrid-all). We also derived a lexicon in which the
recognizer could choose whether a manner or pho-
netic representation was appropriate for unstressed
segments (Hybrid-choice). During evaluation, we
found that the Hybrid-choice lexicon degraded only
slightly over a phone-based lexicon (9.9% word er-
ror vs. 9.1%), and in fact improved recognition
in mild (10dB SNR) additive car noise (13.0% vs.
15.4%). The Hybrid-all was worse on clean speech
(13.1% WER) but statistically the same as phone-
based on noisy speech (15.8%). While not conclu-
sive, this suggests that hybrid models may provide
an interesting avenue for robustness research.
7 Conclusion
Our studies verify to some degree Briscoe?s claim
that a hybrid representation for lexical modeling,
with stressed syllables receiving full phonetic rep-
resentation and unstressed syllables represented by
manner classes, can improve ASR processing. How-
ever, our analysis shows that the argument for this
hypothesis plays out along very different lines than
in Briscoe?s study. A hybrid phone-manner lexi-
con can theoretically benefit ASR because (a) the
discriminative power of the lexicon is not reduced
greatly, (b) such a representation is a much better
model of the types of pronunciation variation seen
in spontaneous speech corpora such as Switchboard,
and (c) the theoretical average hypothesis space in-
creases only by a little over a factor of 2. This
last fact is contrary to Briscoe?s finding that the
search space would be reduced because it incorpo-
rates more realistic assumptions about the detection
of stressed versus unstressed syllables.
These experiments were designed primarily to in-
vestigate the validity of Briscoe?s claims, and thus
we attempted to remain true to his model. However,
it is clear that our analysis can be extended in sev-
eral ways. We have begun experimenting with prun-
ing the hypothesis graph to remove unlikely arcs ?
this would give a more accurate model of the ASR
processing that would occur. However, this only
makes sense if language model constraints are in-
tegrated into the processing, since some word se-
quences in the graph would be discarded as unlikely.
This analysis could also benefit from a more accu-
rate model of the ASR system?s transformation be-
tween realized phones and lexical representations.
This could be achieved by comparing the Gaussian
acoustic model distributions in an HMM system or
sampling the acoustic model?s space (McAllaster et
al., 1998). Both of these extensions will be consid-
ered in future work.
The results clearly indicate that further investiga-
tion and development of a hybrid lexical strategy in
an ASR system is worthwhile, particularly for spon-
taneous speech corpora where the problem of pro-
nunciation variation is most rampant.
Acknowledgments
The authors would like to thank Keith Johnson,
Monica Rajamanohar, and Yu Wang for discussion
of this work. This work was funded in part by NSF
grant ITR-0427413; the opinions and conclusions
expressed in this work are those of the authors and
not of any funding agency.
577
References
E. J. Briscoe. 1989. Lexical access in connected speech
recognition. In Proc. 27th Annual Meeting of the As-
sociation for Computational Linguistics, pages 84?90.
R. Carlson, K. Elenius, B. Granstro?m, and H. Hunni-
cutt. 1985. Phonetic and orthographic properties of
the basic vocabulary of five european languages. In
STL-QPSR 1/1985, pages 63?94, Stockholm. Speech
Transmission Laboratory, Dept. of Speech Communi-
cation, Royal Institute of Technology.
K. W. Church. 1987. Phonological Parsing in Speech
Recognition. Kluwer, Dordrecht.
M. Finke, J. Fritsch, and D. Koll. 1999. Modeling and
efficient decoding of large vocabulary conversational
speech. In Conversational Speech Recognition Work-
shop: DARPA Hub-5E Evaluation.
W. Fisher, 1996. The tsylb2 Program: Algorithm De-
scription. NIST. Part of the tsylb2-1.1 package.
E. Fosler-Lussier, S. Greenberg, and N. Morgan. 1999.
Incorporating contextual phonetics into automatic
speech recognition. In Int?l Congress of Phonetic Sci-
ences, San Francisco, California.
E. Fosler-Lussier, C. A. Rytting, and S. Srinivasan. 2005.
Phonetic ignorance is bliss: Investigating the effects of
phonetic information reduction on asr performance. In
Proc. Interspeech, Lisbon, Portugal.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, and
N. Dahlgren. 1993. DARPA TIMIT acoustic-phonetic
continuous speech corpus. Technical Report NISTIR
4930, NIST, Gaithersburg, MD.
S. Greenberg, J. Hollenbach, and D. Ellis. 1996. Insights
into spoken language gleaned from phonetic transcrip-
tion of the switchboard corpus. In Proc. 4th Int?l Con-
ference on Spoken Language Processing. Philadel-
phia, PA.
R. Jakobson, G. Fant, and M. Halle. 1952. Preliminar-
ies to speech analysis. Technical Report 13, Acoustics
Laboratory, Massachusetts Instutite of Technology.
A. Juneja and C. Espy-Wilson. 2004. Significance of in-
variant acoustic cues in a probabilistic framework for
landmark-based speech recognition. In From Sound to
Sense: Fifty+ Years of Discoveries in Speech Commu-
nication, Cambridge, MA. MIT.
K. Kirchhoff. 1998. Combining articulatory and acoustic
information for speech recognition in noisy and rever-
berant environments. In Proc. 5th Int?l Conference on
Spoken Language Processing, Sydney.
Linguistic Data Consortium (LDC). 1996. The PRON-
LEX pronunciation dictionary. Available from the
LDC, ldc@unagi.cis.upenn.edu. Part of the COMLEX
distribution.
K. Livescu, J. Glass, and J. Bilmes. 2003. Hidden
feature models for speech recognition using dynamic
bayesian networks. In Proc. 8th European Conference
on Speech Communication and Technology, Geneva,
Switzerland.
D. McAllaster, L. Gillick, F. Scattone, and M. New-
man. 1998. Fabricating conversational speech data
with acoustic models: A program to examine model-
data mismatch. In Proc. 5th Int?l Conference on Spo-
ken Language Processing, pages 1847?1850, Sydney,
Australia.
G. Miller and P. Nicely. 1955. Analysis of some per-
ceptual confusions among some english consonants.
Journal of Acoustical Society of America, 27:338?52.
M. Mohri, F. Pereira, and M. Riley, 2001.
AT&T FSM LibraryTM ? General-Purpose
Finite-State Machine Software Tools. AT&T,
Florham Park, New Jersey. Available at
http://www.research.att.com/sw/tools/fsm.
NIST. 1992. Switchboard Corpus: Recorded telephone
conversations. National Institute of Standards and
Technology Speech Disc 9-1 to 9-25.
M. Ostendorf. 1999. Moving beyound the ?beads-on-
a-string? model of speech. In 1999 IEEE Workshop
on Automatic Speech Recognition and Understanding,
Keystone, Colorado.
M. Sarac?lar, H. Nock, and S. Khudanpur. 2000. Pronun-
ciation modeling by sharing Gaussian densities across
phonetic models. Computer Speech and Language,
14:137?160.
D. W. Shipman and V. W. Zue. 1982. Properties of large
lexicons: Implications for advanced isolated word
recognition systems. In Proc. Int?l Conference on
Acoustics, Speech, and Signal Processing, volume 82,
pages 546?549, Paris, France.
K. Stevens. 1981. Invariant acoustic correlates of pho-
netic features. Journal of Acoustical Society of Amer-
ica, 69 suppl. 1:S31.
A. Waibel. 1988. Prosody and Speech Recognition.
Morgan Kaufmann, San Mateo, California.
S. Young, G. Evermann, T. Hain, D. Kershaw,
G. Moore, J. Odell, D. Ollason, D. Povey,
V. Valtchev, and P. Woodland, 2002. The HTK
Book. Cambridge Unveristy Engineering Department.
http://htk.eng.cam.ac.uk.
578
Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 157?160,
New York, June 2006. c?2006 Association for Computational Linguistics
Sentence Planning for Realtime Navigational Instructions
Laura Stoia and Donna K. Byron and
Darla Magdalene Shockley and Eric Fosler-Lussier
The Ohio State University
Computer Science and Engineering
2015 Neil Ave., Columbus, Ohio 43210
stoia|dbyron|shockley|fosler@cse.ohio-state.edu
Abstract
In the current work, we focus on systems that
provide incremental directions and monitor
the progress of mobile users following those
directions. Such directions are based on dy-
namic quantities like the visibility of reference
points and their distance from the user. An
intelligent navigation assistant might take ad-
vantage of the user?s mobility within the set-
ting to achieve communicative goals, for ex-
ample, by repositioning him to a point from
which a description of the target is easier to
produce. Calculating spatial variables over a
corpus of human-human data developed for
this study, we trained a classifier to detect con-
texts in which a target object can be felici-
tously described. Our algorithm matched the
human subjects with 86% precision.
1 Introduction and Related Work
Dialog agents have been developed for a variety of
navigation domains such as in-car driving directions
(Dale et al, 2003), tourist information portals (John-
ston et al, 2002) and pedestrian navigation (Muller,
2002). In all these applications, the human partner
receives navigation instructions from a system. For
these domains, contextual features of the physical
setting must be taken into account for the agent to
communicate successfully.
In dialog systems, one misunderstanding can of-
ten lead to additional errors (Moratz and Tenbrink,
2003), so the system must strategically choose in-
structions and referring expressions that can be
clearly understood by the user. Human cognition
studies have found that the in front of/behind axis
is easier to perceive than other relations (Bryant et
al., 1992). In navigation tasks, this suggests that de-
scribing an object when it is in front of the follower
is preferable to using other spatial relations. Studies
on direction-giving language have found that speak-
ers interleave repositioning commands (e.g. ?Turn
right 90 degrees?) designating objects of interest
(e.g. ?See that chair??) and action commands (e.g.
?Keep going?)(Tversky and Lee, 1999). The con-
tent planner of a spoken dialog system must decide
which of these dialog moves to produce at each turn.
A route plan is a linked list of arcs between nodes
representing locations and decision-points in the
world. A direction-giving agent must perform sev-
eral content-planning and surface realization steps,
one of which is to decide how much of the route
to describe to the user at once (Dale et al, 2003).
Thus, the system selects the next target destination
and must describe it to the user. In an interactive
system, the generation agent must not only decide
what to say to the user but also when to say it.
2 Dialog Collection Procedure
Our task setup employs a virtual-reality (VR) world
in which one partner, the direction-follower (DF),
moves about in the world to perform a series of
tasks, such as pushing buttons to re-arrange ob-
jects in the room, picking up items, etc. The part-
ners communicated through headset microphones.
The simulated world was presented from first-person
perspective on a desk-top computer monitor. The
DF has no knowledge of the world map or tasks.
His partner, the direction-giver (DG), has a paper
2D map of the world and a list of tasks to complete.
During the task, the DG has instant feedback about
157
video frame: 00:13:16
00:13:16 ?keep going forward?
video frame: 00:15:12
00:14:05 ?ok, stop?
00:15:20 ?turn right?
video frame: 00:17:07
00:17:19: ?and go through that door
[D6]?Figure 1: An example sequence with repositioning
DG: ok, yeah, go through that door [D9, locate]
turn to your right
?mkay, and there?s a door [D11, vague]
in there um, go through the one
straight in front of you [D11, locate]
ok, stop... and then turn around and look at
the buttons [B18,B20,B21]
ok, you wanna push the button that?s there
on the left by the door [B18]
ok, and then go through the door [D10]
look to your left
there, in that cabinet there [C6, locate]
Figure 2: Sample dialog fragment
the DF?s location in the VR world, via mirroring of
his partner?s screen on his own computer monitor.
The DF can change his position or orientation within
the virtual world independently of the DG?s direc-
tions, but since the DG knows the task, their collab-
oration is necessary. In this study, we are most inter-
ested in the behavior of the DG, since the algorithm
we develop emulates this role. Our paid participants
were recruited in pairs, and were self-identified na-
tive speakers of North American English.
The video output of DF?s computer was captured
to a camera, along with the audio stream from both
microphones. A logfile created by the VR engine
recorded the DF?s coordinates, gaze angle, and the
position of objects in the world. All 3 data sources
were synchronized using calibration markers. A
technical report is available (Byron, 2005) that de-
scribes the recording equipment and software used.
Figure 2 is a dialog fragment in which the DG
steers his partner to a cabinet, using both a sequence
of target objects and three additional repositioning
commands (in bold) to adjust his partner?s spatial
relationship with the target.
2.1 Developing the Training Corpus
We recorded fifteen dialogs containing a total of
221 minutes of speech. The corpus was transcribed
and word-aligned. The dialogs were further anno-
tated using the Anvil tool (Kipp, 2004) to create a
set of target referring expressions. Because we are
interested in the spatial properties of the referents
of these target referring expressions, the items in-
cluded in this experiment were restricted to objects
with a defined spatial position (buttons, doors and
cabinets). We excluded plural referring expressions,
since their spatial properties are more complex, and
also expressions annotated as vague or abandoned.
Overall, the corpus contains 1736 markable items,
of which 87 were annotated as vague, 84 abandoned
and 228 sets.
We annotated each referring expression with a
boolean feature called Locate that indicates whether
the expression is the first one that allowed the fol-
lower to identify the object in the world, in other
words, the point at which joint spatial reference was
achieved. The kappa (Carletta, 1996) obtained on
this feature was 0.93. There were 466 referring ex-
pressions in the 15-dialog corpus that were anno-
tated TRUE for this feature.
The dataset used in the experiments is a consensus
version on which both annotators agreed on the set
of markables. Due to the constraints introduced by
the task, referent annotation achieved almost perfect
agreement. Annotators were allowed to look ahead
in the dialog to assign the referent. The data used in
the current study is only the DG?s language.
3 Algorithm Development
The generation module receives as input a route plan
produced by a planning module, composed of a list
of graph nodes that represent the route. As each sub-
sequent target on the list is selected, content plan-
ning considers the tuple of variables   ID, LOC 
where ID is an identifier for the target and LOC is
the DF?s location (his Cartesian coordinates and ori-
entation angle). Target ID?s are always object id?s
to be visited in performing the task, such as a door
158
 = Visible area(  )
 = Angle to target
	 = distance to target
In this scene:
Distractors = 5

 B1, B2, B3, C1, D1 
VisDistracts = 3 
 B2, B3, C1 
VisSemDistracts = 2 
 B2, B3 
Figure 3: An example configuration with spatial context fea-
tures. The target obje ct is B4 and [B1, B2, B3, B4, C1, D1] are
perceptually accessible.
that the DF must pass through. The VR world up-
dates the value of LOC at a rate of 10 frames/sec.
Using these variables, the content planner must de-
cide whether the DF?s current location is appropriate
for producing a referring expression to describe the
object.
The following features are calculated from this in-
formation: absolute Angle between target and fol-
lower?s view direction, which implicitly gives the in
front relation, Distance from target, visible distrac-
tors (VisDistracts), visible distractors of the same
semantic category (VisSemDistracts), whether the
target is visible (boolean Visible), and the target?s
semantic category (Cat: button/door/cabinet). Fig-
ure 3 is an example spatial configuration with these
features identified.
3.1 Decision Tree Training
Training examples from the annotation data are tu-
ples containing the ID of the annotated description,
the LOC of the DF at that moment (from the VR en-
gine log), and a class label: either Positive or Nega-
tive. Because we expect some latency between when
the DG judges that a felicity condition is met and
when he begins to speak, rather than using spatial
context features that co-occur with the onset of each
description, we averaged the values over a 0.3 sec-
ond window centered at the onset of the expression.
Negative contexts are difficult to identify since
they often do not manifest linguistically: the DG
may say nothing and allow the user to continue mov-
ing along his current vector, or he may issue a move-
ment command. A minimal criterion for producing
an expression that can achieve joint spatial reference
is that the addressee must have perceptual accessi-
bility to the item. Therefore, negative training exam-
ples for this experiment were selected from the time-
periods that elapsed between the follower achiev-
ing perceptual access to the object (coming into the
same room with it but not necessarily looking at it),
but before the Locating description was spoken. In
these negative examples, we consider the basic felic-
ity conditions for producing a descriptive reference
to the object to be met, yet the DG did not produce
a description. The dataset of 932 training examples
was balanced to contain 50% positive and 50% neg-
ative examples.
3.2 Decision Tree Performance
This evaluation is based on our algorithm?s ability
to reproduce the linguistic behavior of our human
subjects, which may not be ideal behavior.
The Weka1 toolkit was used to build a decision
tree classifier (Witten and Frank, 2005). Figure 4
shows the resulting tree. 20% of the examples were
held out as test items, and 80% were used for train-
ing with 10 fold cross validation. Based on training
results, the tree was pruned to a minimum of 30 in-
stances per leaf. The final tree correctly classified
 of the test data.
The number of positive and negative examples
was balanced, so the first baseline is 50%. To incor-
porate a more elaborate baseline, we consider that a
description will be made only if the referent is visi-
ble to the DF. Marking all cases where the referent
was visible as describe-id and all the other examples
as delay gives a higher baseline of 70%, still 16%
lower than the result of our tree.2
Previous findings in spatial cognition consider an-
gle, distance and shape as the key factors establish-
ing spatial relationships (Gapp, 1995), the angle de-
viation being the most important feature for projec-
tive spatial relationship. Our algorithm also selects
Angle and Distance as informative features. Vis-
Distracts is selected as the most important feature
by the tree, suggesting that having a large number
of objects to contrast makes the description harder,
which is in sync with human intuition. We note that
Visible is not selected, but that might be due to the
fact that it reduces to Angle  . In terms of the
referring expression generation algorithm described
by (Reiter and Dale, 1992), in which the description
which eliminates the most distractors is selected, our
1http://www.cs.waikato.ac.nz/ml/weka/
2not all positive examples were visible
159
results suggest that the human subjects chose to re-
duce the size of the distractor set before producing a
description, presumably in order to reduce the com-
putational load required to calculate the optimal de-
scription.
VisDistracts <= 3
| Angle <= 33
| | Distance <=154: describe-id (308/27)
| | Distance > 154: delay (60/20)
| Angle > 33
| | Distance <= 90
| | | Angle <=83:describe-id(79/20)
| | | Angle > 83: delay (53/9)
| | Distance >90: delay(158/16)
VisDistracts > 3: delay (114/1)
Figure 4: The decision tree obtained.
Class Precision Recall F-measure
describe-id 0.822 0.925 0.871
delay 0.914 0.8 0.853
Table 1: Detailed Performance
The exact values of features shown in our deci-
sion tree are specific to our environment. However,
the features themselves are domain-independent and
are relevant for any spatial direction-giving task, and
their relative influence over the final decision may
transfer to a new domain. To incorporate our find-
ings in a system, we will monitor the user?s context
and plan a description only when our tree predicts it.
4 Conclusions and Future Work
We describe an experiment in content planning for
spoken dialog agents that provide navigation in-
structions. Navigation requires the system and the
user to achieve joint reference to objects in the envi-
ronment. To accomplish this goal human direction-
givers judge whether their partner is in an appropri-
ate spatial configuration to comprehend a reference
spoken to an object in the scene. If not, one strategy
for accomplishing the communicative goal is to steer
their partner into a position from which the object is
easier to describe.
The algorithm we developed in this study, which
takes into account spatial context features replicates
our human subject?s decision to produce a descrip-
tion with 86%, compared to a 70% baseline based
on the visibility of the object. Although the spatial
details will vary for other spoken dialog domains,
the process developed in this study for producing de-
scription dialog moves only at the appropriate times
should be relevant for spoken dialog agents operat-
ing in other navigation domains.
Building dialog agents for situated tasks provides
a wealth of opportunity to study the interaction be-
tween context and linguistic behavior. In the future,
the generation procedure for our interactive agent
will be further developed in areas such as spatial de-
scriptions and surface realization. We also plan to
investigate whether different object types in the do-
main require differential processing, as prior work
on spatial semantics would suggest.
5 Acknowledgements
We would like to thank the OSU CSE department for funding
this work, our participants in the study and to M. White and
our reviewers for useful comments on the paper. We also thank
Brad Mellen for building the virtual world.
References
D. J. Bryant, B. Tversky, and N. Franklin. 1992. Internal and
external spatial frameworks representing described scenes.
Journal of Memory and Language, 31:74?98.
D. K. Byron. 2005. The OSU Quake 2004 corpus of two-
party situated problem-solving dialogs. Technical Report
OSU-CISRC-805-TR57, The Ohio State University Com-
puter Science and Engineering Department, Sept., 2005.
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22(2):249?
254.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using natural
language generation for navigational assistance. In M. Oud-
shoorn, editor, Proceedings of the 26th Australasian Com-
puter Science Conference, Adelaide, Australia.
K. Gapp. 1995. Angle, distance, shape, and their relationship
to projective relations. Technical Report 115, Universitat des
Saarlandes.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialogue systems. In Pro-
ceedings of the 40  Annual Meeting of the Association for
Computational Linguistics (ACL ?02), pages 376?383.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
R. Moratz and T. Tenbrink. 2003. Instruction modes for
joint spatial reference between naive users and a mobile
robot. In Proc. RISSP 2003 IEEE International Conference
on Robotics, Intelligent Systems and Signal Processing, Spe-
cial Session on New Methods in Human Robot Interaction.
C. Muller. 2002. Multimodal dialog in a pedestrian navi-
gation system. In Proceedings of ISCA Tutorial and Re-
search Workshop on Multi-Modal Dialogue in Mobile En-
vironments.
E. Reiter and R. Dale. 1992. A fast algorithm for the generation
of referring expressions. COLING.
B. Tversky and P. U. Lee. 1999. Pictorial and verbal tools for
conveying routes. Stade, Germany.
I. Witten and E. Frank. 2005. Data Mining: Practical machine
learning tools and techniques, 2nd Edition. Morgan Kauf-
mann, San Francisco.
160
Proceedings of NAACL HLT 2007, Companion Volume, pages 13?16,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
JOINT VERSUS INDEPENDENT PHONOLOGICAL FEATURE
MODELS WITHIN CRF PHONE RECOGNITION
Ilana Bromberg?, Jeremy Morris?, and Eric Fosler-Lussier??
?Department of Linguistics
?Department of Computer Science and Engineering
The Ohio State University, Columbus, OH
bromberg@ling.ohio-state.edu, {morrijer, fosler}@cse.ohio-state.edu
Abstract
We compare the effect of joint modeling
of phonological features to independent
feature detectors in a Conditional Random
Fields framework. Joint modeling of fea-
tures is achieved by deriving phonological
feature posteriors from the posterior prob-
abilities of the phonemes. We find that
joint modeling provides superior perfor-
mance to the independent models on the
TIMIT phone recognition task. We ex-
plore the effects of varying relationships
between phonological features, and sug-
gest that in an ASR system, phonological
features should be handled as correlated,
rather than independent.
1 Introduction
Phonological features have received attention as a
linguistically-based representation for sub-word in-
formation in automatic speech recognition. These
sub-phonetic features allow for a more refined repre-
sentation of speech by allowing for temporal desyn-
chronization between articulators, and help account
for some phonological changes common in sponta-
neous speech, such as devoicing (Kirchhoff, 1999;
Livescu, 2005). A number of methods have been de-
veloped for detecting acoustic phonological features
and related acoustic landmarks directly from data
using Multi-Layer Perceptrons (Kirchhoff, 1999),
Support Vector Machines (Hasegawa-Johnson et al,
2005; Sharenborg et al, 2006), or Hidden Markov
Models (Li and Lee, 2005). These techniques
typically assume that acoustic phonological feature
events are independent for ease of modeling.
In one study that broke the independence assump-
tion (Chang et al, 2001), the investigators devel-
oped conditional detectors: MLP detectors of acous-
tic phonological features that are hierarchically de-
pendent on a different phonological class. In (Ra-
jamanohar and Fosler-Lussier, 2005) it was shown
that such a conditional training of detectors tended
to have correlated frame errors, and that improve-
ments in detection could be obtained by training
joint detectors. For many features, the best detector
can be obtained by collapsing MLP phone posteriors
into feature classes by marginalizing across phones
within a class. This was shown only for frame-level
classification rather than phone recognition.
Posterior estimates of phonological feature
classes, as in Table 1, particularly those derived
from MLPs, have been used as input to HMMs
(Launay et al, 2002), Dynamic Bayesian Networks
(DBNs) (Frankel et al, 2004; Livescu, 2005),
and Conditional Random Fields (CRFs) (Morris
and Fosler-Lussier, 2006). Here we evaluate
phonological feature detectors created from MLP
phone posterior estimators (joint feature models)
rather than the independently trained MLP feature
detectors used in previous work.
2 Conditional Random Fields
CRFs (Lafferty et al, 2001) are a joint model of
a label sequence conditioned on a set of inputs.
No independence is assumed among the input; the
CRF model discriminates between hypothesized la-
bel sequences according to an exponential function
of weighted feature functions:
P (y|x) ? exp
?
i
(S(x,y, i) + T(x,y, i)) (1)
13
Class Feature Values
SONORITY Vowel, Obstruent, Sonorant, Syllabic, Silence
VOICE Voiced, Unvoiced, N/A
MANNER Fricative, Stop, Stop-Closure, Flap, Nasal, Approximant, Nasalflap, N/A
PLACE Labial, Dental, Alveolar, Palatal, Velar, Glottal, Lateral, Rhotic, N/A
HEIGHT High, Mid, Low, Lowhigh, Midhigh, N/A
FRONT Front, Back, Central, Backfront, N/A
ROUND Round, Nonround, Roundnonround, Nonroundround, N/A
TENSE Tense, Lax N/A
Table 1: Phonetic feature classes and associated values
where P (y|x) is the probability of label sequence
y given an input frame sequence x, i is the frame
index, and S and T are a set of state feature functions
and a set of transition feature functions, defined as:
S(x, y, i) =
?
j
?jsj(y, x, i), and (2)
T (x, y, i) =
?
k
?ktk(yi?1, yi, x, i) (3)
where ? and ? are weights determined by the learn-
ing algorithm. In NLP applications, the component
feature functions sj and tk are typically realized as
binary indicator functions indicating the presence or
absence of a feature, but in ASR applications it is
more typical to utilize real-valued functions, such as
those derived from the sufficient statistics of Gaus-
sians (e.g., (Gunawardana et al, 2005)).
We can use posterior estimates of phone classes or
phonological feature classes from MLPs as feature
functions (inputs) within the CRF model. A more
detailed description of this CRF paradigm can be
found in (Morris and Fosler-Lussier, 2006), which
shows that the results of phone recognition using
CRFs is comparable to that of HMMs or Tandem
systems, with fewer constraints being imposed on
the model. State feature functions in our system are
defined such that
s?,f (yi,x, i) =
{
NNf (xi), ifyi = ?
0, otherwise
(4)
where the MLP output for feature f at time i is
NNf (xi). This allows for an association between
a phone ? and a feature f (even if f is traditionally
not associated with ?).
In this study, we experiment with different meth-
ods of generating these feature functions. In various
experiments, they are generated by training MLP
phone detectors, by evaluating the feature informa-
tion inherent in the MLP phone posteriors, and by
training independent MLPs to detect the various fea-
tures within the classes described. The use of CRFs
allows us to explore the dependencies among feature
classes, as well as the usefulness of phone posteriors
versus feature classes as inputs.
3 Experimental Setup
We use the TIMIT speech corpus for all training and
testing (Garofolo et al, 1993). The acoustic data
is manually labeled at the phonetic level, and we
propagate this phonetic label information to every
frame of data. For the feature analyses, we employ
a lookup table that defines each phone in terms of
8 feature classes, as shown in Table 1. We extract
acoustic features in the form of 12th order PLP fea-
tures plus delta coefficients. We then use these as
inputs to several sets of neural networks using the
ICSI QuickNet MLP neural network software (John-
son, 2004), with the 39 acoustic features as input, a
varying number of phone or feature class posteriors
as output, and 1000 hidden nodes.
4 Joint Phone Posteriors vs. Independent
Feature Posteriors
The first experiment contrasts joint versus indepen-
dent feature modeling within the CRF system. We
compare a set of phonological feature probabilities
derived from the phone posteriors (a joint model)
with MLP phone posteriors and with independently
trained MLP phonological feature posteriors.
The inputs to the first CRF are sets of 61 state fea-
ture functions from the phonemic MLP posteriors,
each function is an estimate of the posterior proba-
14
Input Type. Phn. Accuracy Phn. Correct
Phones 67.27 68.77
Features 65.25 66.65
Phn. ? Feat. 66.45 67.94
Table 2: Results for Exp. 1: Phone and feature pos-
teriors as input to the CRF phone recognition
bility of one phone. The inputs to the second CRF
model are sets of 44 functions corresponding to the
phonological features listed in Table 1. The CRF
models are trained to associate these feature func-
tions with phoneme labels, incorporating the pat-
terns of variation seen in the MLPs.
The results show that phone-based posteri-
ors produce better phone recognition results than
independently-trained phonological features. This
could be due in part to the larger number of param-
eters in the system, but it could also be due to the
joint modeling that occurs in the phone classifier.
In order to equalize the feature spaces, we use the
output of the phoneme classifier to derive phonolog-
ical feature posteriors. In each frame we sum the
MLP phone posteriors of all phones that contain a
given feature. For instance, in the first frame, for
the feature LOW, we sum the posterior estimates at-
tributed to the phones aa, ae and ao. This is repeated
for each feature in each frame. The CRF model is
trained on these data and tested accordingly. The re-
sults are significantly better (p?.001) than the previ-
ous features model, but are significantly worse than
the phone posteriors (p?.005).
The results of Experiment 1 confirm the hypoth-
esis of (Rajamanohar and Fosler-Lussier, 2005) that
joint modeling using several types of feature infor-
mation is superior to individual modeling in phone
recognition, where only phoneme information is
used. The difference between the phone posteriors
and individual feature posteriors seems to be related
both to the larger CRF parameter space with larger
input, and the joint modeling provided by phone
posteriors.
5 Phonological Feature Class Analysis
In the second experiment, we examine the influence
of each feature class on the accuracy of the recog-
nizer. We iteratively remove the set of state fea-
ture functions corresponding to each feature class
Class Removed Feats. Phn. Acc. Phn. Corr.
None 44 65.25 66.65
Sonority 39 65.15 66.58
Voice 41 63.60* 65.03*
Manner 36 58.92* 60.60*
Place 35 53.22* 55.13*
Height 38 62.58* 64.07*
Front 39 64.51* 65.95*
Round 39 65.19 66.64
Tense 41 64.20* 65.65*
* p?.05, different from no features removed
Table 3: Results of Exp. 2: Removing feature
classes from the input
from the input to the CRF. The original functions
are the output of the independently-trained feature
class MLPs. The phone recognition accuracy for the
CRF having removed each class is shown in Table 3.
In Table 4 we show how removing each feature class
affects the labeling of vowels and consonants.
Manner provides an example of the influence of a
single feature class. Both the Accuracy and Correct-
ness scores decrease significantly when features as-
sociated with Manner are removed. Manner features
distinguish consonants but not vowels, so the effect
is concentrated on the recognition of consonants.
The results of Experiment 2 show that certain fea-
ture classes are redundant from the point of view of
phone recognition. In English, Round is correlated
with Front. When we remove Round, the phonemes
remain uniquely identified by the other classes. The
same is true for the Sonority class. The results show
that the inclusion of these redundant features is not
detrimental to the recognition accuracy. Accuracy
and Correctness improve non-significantly when the
redundant features are included.
Clearly, the ?independent? phonological feature
streams are not truly independent. Otherwise, per-
formance would decrease overall as we removed
each feature class, assuming predictiveness.
Removal of Place causes a slight worsening of
recognition of vowels. This is surprising, because
Place does not characterize vowels. An analysis of
the MLP activations showed that the detector for
Place=N/A is a stronger indicator for vowels than
is the Sonority=Vowel detector. This is especially
true for the vowel ax, which is frequent in the data,
15
Class Removed Percent Correct:
Vowels Consonants
None 62.68 68.91
Sonority 62.18 69.08
Voice 62.39 66.53*
Manner 61.84 59.89*
Place 60.77* 51.94*
Height 55.92* 68.69
Frontness 60.80* 68.87
Roundness 62.25 69.13
Tenseness 60.15* 68.76
* p?.05, different from no features removed
Table 4: Effect of removing each feature class on
recognition accuracy of vowels and consonants
thus greatly influences the vowel recognition statis-
tic. Removing the Place detectors leads to a loss in
vowel vs. consonant information. This results in an
increased number of consonant for vowel substitu-
tions (from 560 to 976), thus a decrease in vowel
recognition accuracy.
Besides extending the findings in (Rajamanohar
and Fosler-Lussier, 2005), this provides a cautionary
tale for incorporating redundant phonological fea-
ture estimators into ASR: these systems need to be
able to handle correlated input, either by design (as
in a CRF), through full or semi-tied covariance ma-
trices in HMMs, or by including the appropriate sta-
tistical dependencies in DBNs.
6 Summary
We have shown the effect of using joint model-
ing of phonetic feature information in conjunction
with the use of CRFs as a discriminative classifier.
Phonetic posteriors, as joint models of phonological
features, provide superior phone recognition perfor-
mance over independently-trained phonological fea-
ture models. We also find that redundant features are
often modeled well within the CRF framework.
7 Acknowledgments
The authors thank the International Computer Sci-
ence Institute for providing the neural network soft-
ware. The authors also thank four anonymous re-
viewers. This work was supported by NSF ITR
grant IIS-0427413; the opinions and conclusions ex-
pressed in this work are those of the authors and not
of any funding agency.
References
S. Chang, S. Greenberg, and M. Wester. 2001. An eli-
tist approach to articulatory-acoustic feature classifica-
tion. In Interspeech.
J. Frankel, M. Wester, and S. King. 2004. Articulatory
feature recognition using dynamic bayesian networks.
In ICSLP.
J. Garofolo, L. Lamel, W. Fisher, J. Fiscus, D. Pallett, and
N. Dahlgren. 1993. DARPA TIMIT acoustic-phonetic
continuous speech corpus. Technical Report NISTIR
4930, National Institute of Standards and Technology,
Gaithersburg, MD, February. Speech Data published
on CD-ROM: NIST Speech Disc 1-1.1, October 1990.
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. In Interspeech.
M. Hasegawa-Johnson et al 2005. Landmark-based
speech recognition: Report of the 2004 Johns Hopkins
Summer Workshop. In ICASSP.
D. Johnson. 2004. ICSI Quicknet software package.
http://www.icsi.berkeley.edu/Speech/qn.html.
K. Kirchhoff. 1999. Robust Speech Recognition Using
Articulatory Information. Ph.D. thesis, University of
Bielefeld.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the 18th International Conference on Machine Learn-
ing.
B. Launay, O. Siohan, A. C. Surendran, and C.-H. Lee.
2002. Towards knowledge based features for large vo-
cabulary automatic speech recognition. In ICASSP.
J. Li and C.-H. Lee. 2005. On designing and evaluating
speech event detectors. In Interspeech.
K. Livescu. 2005. Feature-Based Pronunciation Model-
ing for Automatic Speech Recognition. Ph.D. thesis,
MIT.
J. Morris and E. Fosler-Lussier. 2006. Combining pho-
netic attributes using conditional random fields. In In-
terspeech.
M. Rajamanohar and E. Fosler-Lussier. 2005. An evalu-
ation of hierarchical articulatory feature detectors. In
IEEE Automatic Speech Recogntion and Understand-
ing Workshop.
O. Sharenborg, V. Wan, and R.K. Moore. 2006. Cap-
turing fine-phonetic variation in speech through auto-
matic classification of articulatory features. In ITRW
on Speech Recognition and Intrinsic Variation.
16
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 204?205,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Robust Extraction of Subcategorization Data from Spoken Language 
 
Jianguo Li & Chris Brew Eric Fosler-Lussier 
Department of Linguistics Department of Computer Science & Engineering 
The Ohio State University, USA The Ohio State University, USA 
{jianguo|cbrew}@ling.ohio-state.edu fosler@cse.ohio-state.edu 
 
 
 
 
1 Introduction 
Subcategorization data has been crucial for various 
NLP tasks. Current method for automatic SCF ac-
quisition usually proceeds in two steps: first, gen-
erate all SCF cues from a corpus using a parser, 
and then filter out spurious SCF cues with statisti-
cal tests. Previous studies on SCF acquisition have 
worked mainly with written texts; spoken corpora 
have received little attention. Transcripts of spoken 
language pose two challenges absent in written 
texts: uncertainty about utterance segmentation and 
disfluency. 
     Roland & Jurafsky (1998) suggest that there are 
substantial subcategorization differences between 
spoken and written corpora. For example, spoken 
corpora tend to have fewer passive sentences but 
many more zero-anaphora structures than written 
corpora. In light of such subcategorization differ-
ences, we believe that an SCF set built from spo-
ken language may, if of acceptable quality, be of 
particular value to NLP tasks involving syntactic 
analysis of spoken language.  
2 SCF Acquisition System  
Following the design proposed by Briscoe and 
Carroll (1997), we built an SCF acquisition system 
consisting of the following four components: 
Charniak?s parser (Charniak, 2000); an SCF ex-
tractor; a lemmatizer; and an SCF evaluator. The 
first three components are responsible for generat-
ing SCF cues from the training corpora and the last 
component, consisting of the Binomial Hypothesis 
Test (Brent, 1993) and a back-off algorithm 
(Sarkar & Zeman, 2000), is used to filter SCF cues 
on the basis of their reliability and likelihood.  
We evaluated our system on a million word 
written corpus and a comparable spoken corpus 
from BNC.  For type precision and recall, we used 
14 verbs selected by Briscoe & Carroll (1997) and 
evaluated our results against SCF entries in 
COMLEX (Grishman et al, 1994). We also calcu-
lated token recall and the results are summarized in 
the following table. 
Corpus Written Spoken 
type precision 93.1% 91.2% 
type recall 48.2% 46.4% 
token recall 82.3% 80% 
Table 1: Type precision, recall and token recall 
3 Detecting Incorrect SCF Cues 
We examined the way segmentation errors and 
disfluency affects our acquisition system ? the sta-
tistical parser and the extractor in particular ? in 
proposing SCF cues and explored ways to detect 
incorrect SCF cues. We extracted 500 SCF cues 
from the ViC corpus (Pitt, et al 2005) and identi-
fied four major reasons that seem to have caused 
the extractor to propose incorrect SCF cues: multi-
ple utterances; missing punctuation; disfluency; 
parsing errors.  
Error analysis reveals that segmentation errors 
and disfluencies cause the parser and the extractor 
to tend to make systematic errors in proposing SCF 
cues ? incorrect SCF cues are likely to have an 
extra complement. We therefore proposed the fol-
lowing two sets of linguistic heuristics for auto-
matically detecting incorrect SCF cues: 
Linguistic Heuristic Set 1: The following SCF 
cues are extremely unlikely whatever the verb. Re-
ject an SCF cue as incorrect if it contains the fol-
lowing patterns: 
? [(NP) PP NP]: We reach out [to your friends] [your 
neighbor]. 
? [NP PP-to S]: Would I want them to say [that][to 
me] [would I want them to do that to me]. 
? [NP NP S]: They just beat [Indiana in basketball] 
[the- Saturday] [I think it was um-hum]. 
204
? [PP-p PP-p]: He starts living [with the] [with the 
guys]. 
Linguistic Heuristic Set 2: The following SCF 
cues are all possibly valid SCFs: for SCF cues of 
the following type, check if the given verb takes it 
in COMLEX. If not, reject it: 
? [(NP) S]: When he was dying [what did he say]. 
? [PP-to S]: The same thing happened [to him] [uh 
he had a scholarship]. 
? [(NP) NP]: OU had a heck of time beating [them] 
[uh-hum]. 
? [(NP) INF]: You take [the plate] from the table 
[rinse them off] and put them by the sink. 
Given the utilization of a gold standard in the 
heuristics, it would be improper to build an end-to-
end system and evaluate against COMLEX. In-
stead, we evaluate by seeing how often our heuris-
tics succeed producing results agreeable to a 
human judge. 
To evaluate the robustness of our linguistic heu-
ristics, we conducted a cross-corpora and cross-
parser comparison. We used 1,169 verb tokens 
from the ViC corpus and another 1,169 from the 
Switchboard corpus. 
Cross-corpus Comparison: The purpose of the 
cross-corpus comparison is to show that our lin-
guistic heuristics based on the data from one spo-
ken corpus can be applied to other spoken corpora. 
Therefore, we applied our heuristics to the ViC and 
the Switchboard corpus parsed by Charniak?s 
parser. We calculated the percentage of incorrect 
SCF cues before and after applying our linguistic 
heuristics. The results are shown in Table 2.  
Charniak?s parser ViC Switchboard 
before heuristics 18.8% 9.5% 
after heuristics 6.4% 4.6% 
Table 2: Incorrect SCF cue rate before and after heuristics 
 
Table 2 shows that the incorrect SCF cue rate 
has been reduced to roughly the same level for the 
two spoken corpora after applying our linguistic 
heuristics. 
Cross-parser Comparison: The purpose of the 
cross-parser comparison is to show that our lin-
guistic heuristics based on the data parsed by one 
parser can be applied to other parsers as well. To 
this end, we applied our heuristics to the 
Switchboard corpus parsed by both Charniak?s 
parser and Bikel?s parsing engine (Bikel, 2004). 
Again, we calculated the percentage of incorrect 
SCF cues before and after applying our heuristics. 
The results are displayed in Table 3. 
Although our linguistic heuristics works slightly 
better for data parsed by Charniak? parser, the in-
correct SCF cue rate after applying heuristics re-
mains at about the same level for the two different 
parsers we used. 
Switchboard Charniak Bikel 
before heuristics 9.5% 9.2% 
after heuristics 4.6% 5.4% 
Table 3: Incorrect SCF cue rate before and after heuristics 
4 Conclusion 
We showed that it should not be assumed that stan-
dard statistical parsers will fail on language that is 
very different from what they are trained on. Spe-
cifically, the results of Experiment 1 showed that it 
is feasible to apply current SCF extraction 
technology to spoken language. Experiment 2 
showed that incorrect SCF cues due to segmenta-
tion errors and disfluency can be recognized by our 
linguistic heuristics. We have shown that our SCF 
acquisition system as a whole will work for the 
different demands of spoken language. 
5 Acknowledgements 
This work was supported by NSF grant 0347799 to 
the second author, and by a summer fellowship 
from the Ohio State Center for Cognitive Science 
to the first author. 
References  
Biekl, D. 2004. Intricacies of Collins? Parsing Model. Computational 
Linguistics, 30(4): 470-511 
Brent, M. 1993. From Grammar to Lexicon: Unsupervised Learning 
of Lexical Syntax. Computational Lingusitics: 19(3): 243-262 
Briscoe, E. & Carroll, G. 1997. Automatic Extraction of Subcategori-
zation from Corpora. In Proceedings of the 5th ACL Conference on 
Applied Natural Language Processing, Washington, DC. 356-363 
Chaniak, E. 2000. A Maximum-Entropy-Inspired Parser. In Proceed-
ings of the 2000 Conference of the North American Chapter of 
ACL. 132-139 
Grishman, R., Macleod, C. & Meyers, A. 1994. COMLEX Syntax: 
Building a Computational Lexicon. In Proceedings of the Interna-
tional Conference on Computational Lingusitics, COLING-94, 
Kyoto, Japan. 268-272 
Pitt, M., Johnson, K., Hume, E., Kiesling, S., Raymond, W. 2005. 
They Buckeye Corpus of Conversational Speech: Labeling Con-
ventions and a Test of Transcriber Reliability. Speech Communica-
tion, 45: 89-95 
Roland, D. & Jurafsky, D. 1998. How Verb Subcategorization Fre-
quency Affected by the Corpus Choice. In Proceedings of 17th In-
ternational Conference on Computational Lingusitics, 2: 1122-
1128 
Sarkar, A. & Zeman, D. 2000. Automatic Extraction of Subcategoriza-
tion Frames for Czech. In Proceedings of the 19th International 
Conference on Computational Lingusitics. 691-697 
205
Proceedings of the Fourth International Natural Language Generation Conference, pages 81?88,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Noun Phrase Generation for Situated Dialogs
Laura Stoia and Darla Magdalene Shockley and Donna K. Byron and Eric Fosler-Lussier
The Ohio State University
Computer Science and Engineering
2015 Neil Ave., Columbus, Ohio 43210
stoia|shockley|dbyron|fosler@cse.ohio-state.edu
Abstract
We report on a study examining the gener-
ation of noun phrases within a spoken di-
alog agent for a navigation domain. The
task is to provide real-time instructions
that direct the user to move between a se-
ries of destinations within a large interior
space. A subtask within sentence plan-
ning is determining what form to choose
for noun phrases. This choice is driven by
both the discourse history and spatial con-
text features derived from the direction-
follower?s position, e.g. his view angle,
distance from the target referent and the
number of similar items in view. The al-
gorithm was developed as a decision tree
and its output was evaluated by a group of
human judges who rated 62.6% of the ex-
pressions generated by the system to be as
good as or better than the language origi-
nally produced by human dialog partners.
1 Introduction
In today?s world of mobile, context-aware com-
puting, intelligent software agents are being de-
ployed in a wide variety of domains to aid hu-
mans in performing navigation tasks. Exam-
ples include hand-held tourist information por-
tals (Johnston et al, 2002) campus tour guides
(Yang et al, 1999; Long et al, 1996; Striegnitz
et al, 2005), direction-giving avatars for visitors
to a building (Cassell et al, 2002; Chou et al,
2005), in-car driving direction systems (Dale et al,
2003; Wahlster et al, 2001), and pedestrian navi-
gation systems (Muller, 2002). These applications
present an exciting and challenging new frontier
for dialog agents, since attributes of the real-world
setting must be combined with other contextual
factors for the agent to communicate successfully.
In the current work, we focus on a scenario
in which the system provides incremental direc-
tions to a mobile user who is following the instruc-
tions as they are produced. Unlike the rigid di-
rections produced by applications like Mapquest,1
which describes the entire route from start to fin-
ish, this task requires realtime instructions issued
while monitoring the user?s progress. Instructions
are based on dynamic local context variables such
as the visibility of and distance to reference points.
In referring to items in the setting, human speak-
ers produce a wide variety of noun phrase forms,
including descriptions that are headed by a com-
mon noun and that employ a definite, indefinite, or
demonstrative determiner, one anaphors, and pro-
nouns such as it, this and that. Our goal in the
current work is to model that entire space of varia-
tion, which makes the task more difficult than the
noun phrase generation task defined in many pre-
vious studies that simplify the alternatives down to
description or pronoun.
In order to study this process, we developed a
task domain in which a human partner is directed
through an interior space (a graphically-presented
3D virtual world) to perform a sequence of ma-
nipulation tasks. In the first stages of the work, we
collected and annotated a corpus of human-human
dialogs from this domain. Then, using this data,
we trained a decision-tree classifier to utilize con-
text variables such as distance, target object visi-
bility, discourse history, etc., to determine lexical
properties of referring expressions to be produced
by the generation component of our dialog system.
2 Generation for Situated Tasks
Many previous projects, such as (Lauria et al,
2001; Moratz and Tenbrink, 2003; Skubic et al,
2002), inter alia, study interpretation of situated
language, e.g. for giving directions to a robot. The
focus of our work is rather on generating naviga-
tion instructions for a human partner to follow.
Linguistic studies have shown that speakers se-
lect noun phrase forms to refer to entities based on
a variety of factors. Some of the factors are intrin-
sic to the object being described, while others are
features of the context in which the expression is
spoken. The entity?s status within the discourse,
1www.mapquest.com
81
spatial position, and the presence of similar items
from which the target referent must be distin-
guished, have all been found to cause changes to
the lexical properties chosen for a particular refer-
ring expression (i.e. (Gundel et al, 1993; Prince,
1981; Grosz et al, 1995)). This variation is ex-
pressed in terms of the determiner chosen (e.g.
that/a), the head noun (e.g. that/door/one), and
the presence of additional modifiers such as pre-
nominal adjectives or prepositional phrases.
In natural language generation, the process of
generating referring expressions occurs in stages
(Reiter and Dale, 1992). The process we explore
in this paper is the sentence planning stage, which
determines whether the context supports generat-
ing a particular referring expression as a pronoun,
description, one-anaphor, etc.
There has been extensive research in both au-
tomatic route description and on general noun
phrase (NP) generation, but few projects consider
extra-linguistic information as part of the context
that influences dialog behavior. (Poesio et al,
1999) applies statistical techniques for the prob-
lem of NP generation. However, even though the
corpus used in that study contains descriptions of
museum items visually accessible to the user, the
features used in generation were mostly linguis-
tic, and included little information about the vi-
sual or spatial properties of the referent. Another
related study in statistical NP generation (Cheng et
al., 2001) focuses on choosing the modifiers to be
included. Again, no features derived from the sit-
uated world were used in that study. (Maass et al,
1995) use features from the world, including ob-
jects? color, height, width, and visibility, as well as
the user?s direction of travel and distance from ob-
jects, for generating instructions in a situated task.
However, their focus is on selecting landmarks and
descriptions under time pressure, rather than se-
lecting the linguistic form to be produced.
3 Data Collection
Our task setup is designed to elicit natural, spon-
taneous situated language examples from human
partners. The experimental platform employs a
virtual-reality (VR) world in which one partner,
the direction-follower (DF), moves about to per-
form a series of tasks, such as pushing buttons to
re-arrange objects in the room, finding and picking
up treasures, etc. The simulated world was pre-
sented from first-person perspective on a desk-top
computer monitor. The DF had no knowledge of
the world map or tasks.
DG: you can currently see three buttons... there?s
actually a fourth button that?s kind of hidden
DF: yeah
DG: by this cabinet on the right
DF: I know, yeah
DG: ok, um, so what you wanna do is you want to
go in and you?re gonna press one of the buttons
that?s on the right hand wall, so you wanna go
all the way straight into the room and then face
the wall
DF: mhm
DG: there with the two buttons
DF: yep
DG: um and you wanna push the one that?s on the left
Figure 1: Sample dialog fragment and accompanying video
frame
His partner, the direction-giver (DG), had a pa-
per 2D map of the world and a list of tasks to
complete. As they performed the task, the DG
had instant feedback about the DF?s location in
the VR world, via mirroring of the DF?s computer
screen on the DG?s computer monitor. The part-
ners communicated through headset microphones.
Our paid participants were self-identified native
speakers of North American English. Figure 1
shows an example view of the world and the ac-
companying dialog fragment.
The video output of DF?s computer was cap-
tured to a camera, along with the audio from both
microphones. A logfile created by the VR soft-
ware recorded the DF?s coordinates, gaze angle,
and the position of objects in the world 10 times
per second. These data sources were synchronized
using calibration markers. A technical report is
available that describes the recording equipment
and software used (Byron, 2005).
3.1 Corpus and Annotation Scheme
Using the above-described setup, we created a cor-
pus consisting of 15 dialogs containing a total of
221 minutes of speech. It was transcribed and
word-aligned using Praat 2 and SONIC.3 The di-
alogs were further annotated using the Anvil soft-
ware (Kipp, 2004) to identify a set of target refer-
ring expressions in the corpus. Because we are in-
2http://www.praat.org
3http://cslr.colorado.edu/beginweb/speech recognition/sonic.html/
82
terested in the spatial properties of the referents of
these target referring expressions, the items of in-
terest in this experiment were restricted to objects
with a defined spatial position.
Each object in the virtual world was assigned a
symbolic id, and the id of each target referring ex-
pression was added to the annotation. Referring
expressions with plural referents were marked as
Set, and were labeled with a list of the members
in the set. Expressions were also annotated as ei-
ther vague when the referent was not clear at the
time of utterance or abandoned in case the utter-
ance was cut short. Items that did not contain a
surface realization of the head of the NP (e.g., on
the left), were marked with the tag empty.
The corpus contains 1736 target expressions, of
which 221 were Vague, 45 were Empty, and 228
were Sets. The remaining 1242 form the set of test
items in the experiment described below. Vague
items were excluded since we do not wish for the
algorithm we develop to reproduce this behavior.
Set items were excluded in order to avoid the more
complex calculation of spatial properties associ-
ated with plural entities.
The data used in the experiments is a consensus
version on which both annotators, two of the au-
thors, agreed on the set of target expressions and
their properties. Due to the constraints introduced
by the task, referent annotation achieved almost
perfect agreement. The data used in this study is
only the DG?s language.
4 Algorithm Development
Our ultimate goal is to provide input to a surface
realization component for NP generation, given
the ID of a target referent and a vector of context
features. It is desirable for these context features
to be automatically derived, to limit the reliance
on human annotation, so we restricted out study to
features that either were derived automatically, or
required minimal human annotation.
One impact of this decision is that even though
the linguistic literature predicts that syntactic fea-
tures such as grammatical role are important in
selecting NP forms, these features were difficult
to obtain. Our corpus contains spontaneous spo-
ken discourse, which has no sentence boundaries
and relaxed structural constraints. Thus, automatic
parsing was problematic. With improved parsing
techniques, we may include syntactic information
in the decision process for NP generation in future,
but this was not included in the current study.
Following (Poesio et al, 1999), we consider the
det a, the, that, none
head it, that, one, noun, none
mod +, -
The possible values of each NP frame slot
[
det : none
head : it
mod : ?
] [
det : that
head : noun
mod : +
]
it that button on the right
NP frames for it and that button on the right
Figure 2: NP frame slot values and examples
information conveyed by an NP to be divided into
four slots which must be filled to be able to gen-
erate the NP form: a determiner/quantifier, a pre
or post-modifier and a head noun slot. There were
very few examples of premodifiers in the corpus,
so we collapsed the modifier feature. Therefore,
the output from our algorithm is an NP frame spec-
ifying values for the three slots for each target ex-
pression. Figure 2 shows the possible values in
each slot and example slot values for two NPs. The
number of occurrences in the entire corpus for the
NP frame slot values are shown in Table 2.4
In the experimental VR world developed for this
study, all items from the same category were de-
signed to look identical. This was intended to en-
courage the subjects to use referring expressions
that rely on spatial attributes or deictic reference
such as that one. The spatial properties of target
referents and distractors are used as inputs to the
content planning algorithm. Their values in this
study were calculated automatically based on ge-
ometric properties of the virtual world.
To form the training dataset, we processed each
target expression with a syntactic chunker.5 The
partial parse it produced was further processed
with a regular-expression matcher to isolate the
values corresponding to the three slots. Parser er-
rors caused some low-count NP frame values, so
we retained only items that occurred at least 10
times in the entire corpus. Any parser errors that
remained in the data were not hand corrected, in
order to minimize human intervention.
4.1 Context Features
Given the restrictions that we impose over what
is accessible to the learning algorithm, we devel-
oped a set of features for each referring expression
that characterize both the referent and the context
in which the expression was spoken. The context
4The two possible tags for Mod occurred in almost equal
proportion (49%/51%)
5http://www.ltg.ed.ac.uk/software/chunk/index.html
83
Dialog history features
1. Count and chainCount the mention counts for the referent over the dialog and inside a reference chaina
2. DeltaTime and DeltaTimeChain the time elapsed since it was last mentioned in the dialog overall or in a chain
3. PrevSpeaker the previous speaker that mentioned the ID (either DG or DF)
4. Mod
i?1
, Det
i?1
, Head
i?1
the values of the slots of the NP frame of the prior mention of the same referent
5. Mod
i?2
, Det
i?2
, Head
i?2
the previous-1 values of the slots
6. WordDistance and the number of words spoken by both speakers since the last mention of the ID
chainWordDistance overall or in the chain
7. Type
i?1
indicates if the previous mention was in a Set, was Vague, or was a test item
Spatial/Visual featuresb
8. Distance the distance between the referent and the DF?s VR coordinates
9. Angle the angle between the center of the DF?s view angle and the center of the referent
10. Visible a boolean value which indicates if the object is visible
Relation to other objects in the world
11. Visible Distractors the number of other objects besides the target referent in the field of view
12. SameCatVisDistractors the number of visible distractors of the same type as the referent
Object category and its information status
13. Cat the semantic category of the referent: door/cabinet/button
14. First Locate indicates if this is the first expression that allowed the DF to identify the object
in the world. The point where joint spatial reference is accomplished.
Table 1: The Context Features Used by the Algorithm
amention counts are not considered over vague or ambiguous tags, or over sets.
bnote that an Angle value smaller than 500 ensures the object is visible
Det Head
Value Count Percent Value Count Percent
the 364 39% noun 558 60%
that/this 264 29% one 166 18%
none 253 27% it 116 13%
a 46 5% that 57 6%
none 30 3%
Table 2: Distribution of Det and Head values in the corpus
v = Visible area(100o)
? = Angle to target
d = distance to target
In this scene:
VisDistr =3 {B2, B3, C1}
VisSemDistr =2 {B2, B3}
Figure 3: An example configuration with spatial context
features. The target object is B4.
features are not only linguistic but also derived
from the extralinguistic situation, including spatial
relations between the referent and the DF?s posi-
tion and orientation at that instant. The context
feature for each target expression includes these
automatically-calculated attributes as well as fea-
tures from the annotation described above. Table 1
describes the full set of context features, and Fig-
ure 3 shows a schematic of the spatial context fea-
tures.
The mention history of any target referent is im-
portant for determining the form to use in a subse-
quent referring expression. Ideally, the discourse
history feature should indicate whether a refer-
ent has already been discussed, and the distance
between a new mention and its antecedent. But
determining the discourse status of items in this
world was complicated by two factors. All ob-
jects in the world of the same semantic category
had identical visual features, and the VR world
in which the task is conducted is a maze, which
required the subjects to perform tasks, move to a
different portion of the maze, and possibly return
to a previously visited room. Due to the visual
and spatial confusion possible in this setting, there
is no guarantee that our subjects could accurately
calculate whether they were discussing the same
object they had encountered before, or remember
whether that object had been discussed. While
the subjects were focused on a task in a particular
room, however, it is reasonable to expect that they
could remember which items had been discussed.
Therefore, the discourse histories of target objects
were calculated using a re-initialization process.
Each time the subjects left a VR room to pursue a
different task, if more than 25s elapsed before the
next mention of objects in that room, those sub-
sequent expressions were considered to be in new
coreference chains. This time constant was estab-
lished by examining pronominal referring expres-
sions in the training dialogs.
These features were used as input to develop a
classifier to determine NP frames for unseen tar-
get referents in context. We chose decision trees
due to their ease of interpretation, but we plan to
test other machine learning techniques in the fu-
ture. 5 dialogs were held out as unseen data and
the remaining 10 were used to train and adjust the
parameters of the decision process. The first pro-
cedure was to test whether the three slot values
are interdependent. In contrast to previous work,
84
which focused on predicting the values for one of
the slots at a time, we hold that due to their inter-
dependence, these decisions should not be made
separately. For example, a noun form that has the
pronoun it as the head will never have a modifier
or a determiner. If the three slots are independent,
training three separate classifiers and then com-
bining their decisions will yield better results. On
the other hand, if they are dependent, better results
will be obtained through training a single classifier
on the combined label. Unfortunately, combining
the labels is problematic due to data sparsity. To
test these dependencies, we trained several deci-
sion trees, varying the independence assumptions:
Independent - a decision tree was trained for each
slot and their outputs combined at the end.
Joint - a decision tree was trained for the com-
bined label for all three slots
Conditional - three decision trees were trained
in sequence, each having access to the output of
the previous tree. For example, Mod-Det-Head
means that first the Mod tree was trained, then a
tree to classify Det, using the output from Mod,
and finally a tree for Head , using both the Det
and Mod values.
All possible orderings between Mod, Head and
Det were tested. The best result obtained was from
the ordering Mod-Det-Head, but the differences
between the orderings were not significant. The
10 fold cross-validation results are shown in Ta-
ble 3. There were 632 items in the data set. The
Conditional trees outperformed the Independent
trees by 9%, which is significant at the level of
(p < .0002).
As our training data suggests, we test the Mod-
Det-Head trees against our held out data. We
decided to use a leave one out method of train-
ing/testing due to the sparsity of data.
Independent Joint Mod-Det-Head
22.0 % 28.8 % 31.0 %
Table 3: Testing independence of the slot values
Decision tree classifiers offer the opportunity to
examine the relevance of particular features in the
final decision. Algorithm 1 and 2 show example
trees derived for the Mod and Det features (the
Head tree is not shown due to space limitations).
We found that there are significant dependencies
between the slots in the NP form. Each time one of
the slots? values was available to the decision pro-
cess, it was selected as most informative feature in
the next tree. The spatial features were selected as
informative in all the trees, most prevelantly in the
Algorithm 1 An example decision tree for Mod
if FirstLocate = True then
if V isibleDistractors = 0 then
if Distance ? 116 then
return Mod: -
else
return Mod:+
else
if SameCatV isDistractors = 0 then
if V isibleDistractors ? 2 then
if Angle ? 38 then
return Mod: -
else
return Mod: +
else
return Mod: +
else
return Mod: +
else
if chainWordDistance = 0 then
if prevMention 6= Set/AllV ague then
if firstMention = True then
return Mod: +
else
if Angle ? 27 then
return Mod: -
else
return Mod: +
else
if noprevMention then
return Mod: +
else {prevMention = Set/AllV ague}
return Mod: -
else
return Mod: -
Algorithm 2 An example decision tree for Det
if Mod : ? then
if FirstLocate = True then
return Det:that
else
if prevMention 6= Set/AllV ague then
if notV isible then
if Cat = Button/Cabinet then
return Det:none
else {Cat = Door}
return Det:that
else {isV isible}
if Head
i?1
= it then
return Det:none
else if Head
i?1
= noun then
if DeltaT ime ? 6.3 then
if Cat = Button/Cabinet then
return Det:none
else {Cat = Door}
return Det:that
else
return Det:the
else if Head
i?1
= one/none/low then
return Det:that
else {Head
i?1
= that}
return Det:none
else if noprevMention then
return Det:that
else {prevMention = Set/AllV ague}
return Det:none
else {target has modifier}
return Det:the
85
decision tree for Mod, suggesting that the decision
of including extra information is driven largely by
the spatial configuration. The information status
features and discourse history, such as First Lo-
cate, Type, and attributes of the prior mention,
were selected as good predictors for the Det slot.
5 Evaluation
We report several methods of evaluating the NP
frames produced using the process given by the
decision trees. First, we report the results of a
strict evaluation in which the system?s output must
exactly match expressions produced by the hu-
man subjects. We also compare this result with
a hand-crafted Centering-style generation algo-
rithm. Requiring the algorithm to exactly match
human performance is an overly-strict criterion,
since in many contexts several possible referring
expression forms could be equally felicitous in a
given context, so we also conducted a human judg-
ment study. The 5 test dialogs contain 295 target
expressions.
5.1 Exact Match Evaluation
The output of the decision tree classifier was com-
pared to the expressions observed in the test dia-
log. Table 4 reports the results of this evaluation.
The accuracy obtained was 31.2%. The most fre-
quent tag gives a 20.0% baseline performance us-
ing this strict match criterion.
Exact match results
Predicted All three features det mod head
Correct 31% 48% 72% 56%
Exact match: head feature per value
Predicted noun it none one that
Correct 65% 64% 0% 30% 38%
Exact match: det feature per value
Predicted a none that the
Correct 0% 49% 36% 66%
Table 4: Classifier results using Exact-match criterion
5.2 Comparison to Centering
For purposes of comparing the performance of our
generation algorithm to existing work on genera-
tion of NPs, we performed a manual evaluation of
the centering-style generation algorithm described
in (Kibble and Power, 2000) against our dialog
corpus. Algorithms developed according to the
centering framework use discourse coherence to
make decisions about pronominalization (Grosz et
al., 1995), where coherence is measured in terms
of topical continuity from one sentence to the next.
Centering designates the backward-looking cen-
ter (Cb) as the item in the current sentence that
was most topical in the previous sentence. There-
fore, to perform a centering-style evaluation, the
dialogs must be broken into sentence-like units,
and a ranking procedure must be devised for the
items mentioned in each unit.
The current evaluation corpus, being a spo-
ken dialog, has not been parsed to automatically
determine the syntactic or dependency structure,
but rather was manually segmented into utterance
units, where each unit contained a main predicate
and its satellites. The items mentioned in each unit
were ranked according to thematic roles, using the
ranking {AGENT > PATIENT > COMP > AD-
JUNCT}, and excluding references to the speakers
themselves, which often appear in AGENT posi-
tion (Byron and Stent, 1998). The Cb in each unit,
if there is one, is the highest-ranked item from the
prior unit?s list that is repeated in the current unit?s
list. Following a procedure similar to that reported
by Kibble and Power, our decision procedure rec-
ommends pronominalizing an item if it is the Cb
of its unit and if it is in Subject position, otherwise
a description is generated. Based on this rule, all
items that are being mentioned for the first time
in the discourse are predicted to require a descrip-
tion.
Although most prior studies take the recom-
mendation to pronominalize to mean that a per-
sonal pronoun (e.g. it) should be generated, due
to the demonstrative nature of our domain, the de-
cision to produce a pronoun can result in either a
demonstrative or a personal pronoun. Therefore,
we considered the algorithm?s output to match hu-
man production when the target expression in the
human corpus was either a personal or demonstra-
tive pronoun, and the algorithm generated either
category of pronoun. Table 5 shows the compari-
son of our system?s output and the output from the
centering algorithm on anaphoric mentions. The 5
dialogs used for testing in this study contained 145
such items. Both algorithms obtained a similar ac-
curacy (64.8% our system vs. 64.1% centering)
and over-generated pronoouns. Although our al-
gorithm does not outperform centering, it assumes
less structural analysis of the input text.
5.3 Human Judgment Evaluation
Evaluating generation studies by calculating their
similarity to human spontaneous speech may not
be the ideal performance metric, since several dif-
ferent realizations may be equally felicitous in a
86
Pron Desc Total
Human Production 28 117 145
Predicted by Our Algorithm 55 90
Predicted by Centering 64 81
Table 5: Comparison to Coherence-based Generation
Figure 4: The Anvil software tool used for judging
given context. Therefore, we also performed a
human judgement evaluation. In this evaluation,
judges compared the NPs generated by our algo-
rithm to the NPs produced by human subjects, and
to NPs with randomly generated feature assign-
ments. Judges viewed test NPs in the context of
the original test corpus.
To re-create the context in which the original
expression was produced, the video, audio, and
dialog transcript were played for the judges us-
ing the Anvil annotation tool (Kipp, 2004). The
judges could play or pause the video as they
wished. Using the word-alignments established
during the data annotation phase, the audio of the
test NPs was replaced by silence, and the words
were removed in the transcript shown in the time-
line viewer. For each test item, the judges were
presented with a selection box showing two pos-
sible referring expressions that they were asked to
compare using a qualitative ranking (option 1 is
better, option 2 is better, or they are equal), given
a particular target ID and the context. Figure 4
shows a screen-shot of the judges? annotation tool.
The judges did not know the source of the expres-
sions they evaluated (system, human production,
or random). The 10 judges were volunteers from
the university community who were self-identified
native speakers of English. They were not com-
pensated for their time.
The decision tree selected NP-frame slot val-
ues which were converted into realized NPs. The
Det and Head choices were directly translated into
surface forms (for Head=noun we chose a consis-
tent common noun for each semantic class: but-
All Items
System compared to Human Trials: 577
equal 45.9%
system preferred 16.6%
(system equal or preferred to human) (62.6%)
human preferred 37.4%
System compared to Random Trials: 289
equal 24.2%
system preferred 53.3%
(system equal or preferred to random) (77.5%)
random preferred 22.5%
Random compared to Human Trials: 292
equal 23.3%
random preferred 13.0%
(random equal or preferred to human) (36.3%)
human preferred 65.7%
Items with two judges & judges agreed
System against Human Trials: 197
equal 37.3%
system preferred 19.8%
(system equal or preferred to human) (57.1%)
human preferred 36.6%
Table 6: Results of Human Judging
ton, door or cabinet. If the system?s selection of
Mod feature matched the value from the corpus,
we used the expression produced by the original
speaker. If the original expression did not include
a modifier, but the system selected Mod:+, we lex-
icalized this feature to a simple but correct spatial
description like on the right, on the left or in front.
Table 6 shows the results of human judging.
The system?s output was either equal or preferred
to the original spontaneous language in 62.6%
of cases where these two choices were compared
directly. Interestingly, the randomly-generated
choice was preferred over the original spontaneous
language in 13.0% of trials, and was preferred over
the system?s output in 22.5% of trials. Aggregat-
ing over all judges, the system?s performance was
judged to be much better than random, but not as
good as the original human language.
Trials were balanced among judges so that each
target item was seen by four judges: with two
comparing the system?s response to the original
human language, one comparing the system to
random, and one comparing the human to random.
There were 282 trials for which 2 judges saw the
identical pair of choices. Out of these, the two
judges? responses agreed in 197 cases, producing
an inter-annotator reliability (kappa score) of 0.51,
with raw agreement of 69% and expected agree-
ment of 37%. Although this is a relatively low
kappa value, we believe that the aggregate judg-
ments of all of the judges over all of the test items
are still informative, since the scores of items for
which we have two judgements follow a very sim-
87
ilar pattern to the overal distribution of responses.
The low inter-annotator agreement may be due to
the substitutability of the expressions.
6 Conclusions and Future Work
In this paper we describe a generation study for
situated dialog and a novel evaluation setup of the
system?s output. The algorithm decides upon the
determiner, head and modifier values to be pro-
duced in a noun phrase describing an object in
a particular moment in the dialog. The decision
is influenced by dialog history, spatial and visual
relations and information status of the ID to be
described. Our algorithm achieved 31.2% exact
match with human language, but human evalua-
tors judged the output as good as or better than the
original human language 62.6% of the time.
For our future work, we intend to develop the
generation module of a dialog system that per-
forms the direction giver?s role. We plan to incor-
porate the results of this study in an extension of
(Reiter and Dale, 1992) algorithm that would take
into account other types of properties of the ob-
jects like visual salience, temporal attributes (for
example time elapsed between mentions), if it par-
ticipated in an action (like the case of a door open-
ing, or a button being pushed) or its importance to
the overall task completion.
Acknowledgments
The authors would like to thank our undergradu-
ate RA, Bradley Mellen, for building the virtual
world, the 11 judges who rated the system output,
and the anonymous reviewers.
References
D. Byron and A. Stent. 1998. A preliminary model of center-
ing in dialog. In Proceedings of ACL ?98, pp. 1475?1477.
D. Byron. 2005. The OSU Quake 2004 corpus of two-party
situated problem-solving dialogs. Technical Report OSU-
CISRC-805-TR57, The Ohio State University Computer
Science and Engineering Department, September.
J. Cassell, T. Stocky, T. Bickmore, Y. Gao, Y. Nakano,
K. Ryokai, D. Tversky, C. Vaucelle, and H. Vilhjalmsson.
2002. MACK: Media lab Autonomous Conversational
Kiosk. In Proceedings of IMAGINA?02, Monte Carlo, Jan-
uary.
H. Cheng, M. Poesio, R. Henschel, and C. Mellish. 2001.
Corpus-based NP modifier generation. In NAACL ?01,
pp. 1?8, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
S. Chou, W. Hsieh, F. Gandon, and N. Sadeh. 2005. Se-
mantic web technologies for context-aware museum tour
guide applications. In Proceedings of the 2005 Interna-
tional Workshop on Web and Mobile Information Systems.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using
natural language generation for navigational assistance.
In M. Oudshoorn, editor, Proceedings of the 26th Aus-
tralasian Computer Science Conference, Adelaide, Aus-
tralia.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering: A
framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?226.
J. Gundel, N. Hedberg, and R. Zacharski. 1993. Cognitive
status and the form of referring expressions in discourse.
Language, 69(2):274?307.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialogue systems. In Pro-
ceedings of ACL ?02, pp. 376?383.
R. Kibble and R. Power. 2000. An integrated framework for
text planning and pronominalisation. In Proceedings of
INLG?2000, pp. 77?84.
M. Kipp. 2004. Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation. Dis-
sertation.com.
S. Lauria, G. Bugmann, T. Kyriacou, J. Bos, and E. Klein.
2001. Training personal robots using natural langauge in-
struction. IEEE Intelligent Systems, 16(5):2?9.
S. Long, R. Kooper, G. Abowd, and C. Atkesonet. 1996.
Rapid prototyping of mobile context-aware applications:
The cyberguide case study. In 2nd ACM International
Conference on Mobile Computing and Networking (Mo-
biCom?96), November 10-12.
W. Maass, J. Baus, and J. Paul. 1995. Visual grounding of
route descriptions in dynamic environments.
R. Moratz and T. Tenbrink. 2003. Instruction modes for joint
spatial reference between naive users and a mobile robot.
In Proc. RISSP 2003 IEEE International Conference on
Robotics, Intelligent Systems and Signal Processing, Spe-
cial Session on NewMethods in Human Robot Interaction.
C. Muller. 2002. Multimodal dialog in a pedestrian navi-
gation system. In Proceedings of ISCA Tutorial and Re-
search Workshop on Multi-Modal Dialogue in Mobile En-
vironments.
M. Poesio, R. Henschel, J. Hitzeman, and R. Kibble. 1999.
Statistical NP generation: A first report. Utrecht, August.
E. Prince. 1981. On the inferencing of indefinite this NPs.
In Aravind K. Joshi, Bonnie Lynn Webber, and Ivan Sag,
editors, Elements of Discourse Understanding, pp. 231?
250. Cambridge University Press.
E. Reiter and R. Dale. 1992. A fast algorithm for the gen-
erations referring expressions. In Proceedings of COL-
ING ?92, pp. 232?238.
M. Skubic, D. Perzanowski, A. Schultz, and W. Adams.
2002. Using spatial language in a human-robot dialog.
In 2002 IEEE International Conference on Robotics and
Automation, Washington, D.C.
K. Striegnitz, P. Tepper, A. Lovett, and J. Cassell. 2005.
Knowledge representation for generating locating gestures
in route directions. In Proceedings of Workshop on Spa-
tial Language and Dialogue (5th Workshop on Language
and Space), Delmenhorst, Germany, October.
W. Wahlster, N. Reithinger, and A. Blocher. 2001.
Smartkom: Towards multimodal dialogues with anthropo-
morphic interface agents. In International Status Confer-
ence: Lead Projects HumanComputer -Interaction, Saar-
bruecken, Germany.
J. Yang, W. Yang, M. Denecke, and A. Waibel. 1999. Smart
sight: a tourist assistant system. In Proceedings of the
3rd International Symposium on Wearable Computers, pp.
73?78, San Francisco, California, 18-19 October.
88
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 36?44,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Strategies for Teaching ?Mixed? Computational Linguistics classes
Eric Fosler-Lussier
Dept. of Computer Science and Engineering
Dept. of Linguistics
The Ohio State University
Columbus, OH 43210, USA
fosler@cse.ohio-state.edu
Abstract
Many of the computational linguistics classes
at Ohio State draw a diverse crowd of students,
who bring different levels of preparation to
the classroom. In the same classroom, we of-
ten get graduate and undergraduate students
from Linguistics, Computer Science, Electri-
cal Engineering and other departments; teach-
ing the same material to all of these students
presents an interesting challenge to the in-
structor. In this paper, I discuss some of the
teaching strategies that I have employed to
help integrate students in two classes on auto-
matic speech recognition topics; strategies for
a graduate seminar class and a standard ?lec-
ture? class are presented. Both courses make
use of communal, online activities to facilitate
interaction between students.
1 Introduction
As one of the themes of the Teach-CL08 workshop
suggests, teaching students of many kinds and many
levels of preparation within a single course can be
an interesting challenge; this situation is much more
prevalent in a cross-disciplinary area such as compu-
tational linguistics (as well as medical bioinformat-
ics, etc.). At Ohio State, we also define the compu-
tational linguistics field relatively broadly, including
automatic speech recognition and (more recently)
information retrieval as part of the curriculum. Thus,
we see three major variations in the preparation of
students at OSU:
1. Home department: most of the students tak-
ing CL courses are either in the Linguistics
or Computer Science and Engineering depart-
ments, although there have been students from
foreign language departments, Electrical En-
gineering, Psychology, and Philosophy. Al-
though there are exceptions, typically the en-
gineers have stronger mathematical and com-
putational implementation skills and the non-
engineers have a stronger background in the
theoretical linguistics literature. Bringing these
groups together requires a balancing between
the strengths of each group.
2. Specialization (or lack thereof): Many of the
students, particularly in seminar settings, have
particular research agendas that are not tradi-
tionally aligned with the topic of the class (e.g.,
students interested in parsing or computer vi-
sion taking an ASR-learning course). Further-
more, there are often students who are not se-
nior enough to have a particular research track,
but are interested in exploring the area of the
course. Our courses need to be designed to
reach across areas and draw on other parts of
the curriculum in order to both provide con-
nections with the student?s current knowledge
base, and allow the student to take away use-
ful lessons even they do not plan to pursue the
topic of the course further.
3. Graduate vs. undergraduate students: in
both the CSE and Linguistics departments at
Ohio State, CL (and many other) courses are
open to both undergraduates and graduate stu-
dents. These courses fall far enough down the
prerequisite chain that the undergraduates who
36
enroll are usually very motivated (and conse-
quently do well), but one must keep in mind
the differences in abilities and time constraints
of each type of student. If the graduate stu-
dents outnumber the undergraduates, introduc-
ing mentoring opportunities can provide a re-
warding experience for all concerned.
From a practical perspective, this diversity
presents a significant challenge ? especially in uni-
versities where enrollment concerns drive curricular
matters to some degree. Inclusiveness is also a rea-
sonable goal from a financial, not just a pedagog-
ical, perspective. CSE enrollments have declined
significantly since the dot-com bust (Vegso, 2008),
and while the declines are not as sharp as they once
were, the current environment makes it more diffi-
cult to justify teaching narrow, advanced courses to
only a few students (even if this were the practice in
the past).
In this paper, I describe a number of strategies
that have been successful in bringing all of these
diverse populations into two different classes of-
fered at OSU: a graduate seminar and a under-
grad/graduate lecture class. The topic of both classes
was statistical language processing, with a signif-
icant emphasis on ASR. Sample activities are dis-
cussed from each class.
While there are significant differences in the way
that each class runs, there are several common ele-
ments that I try to provide in all of my classes.:
I first establish the golden rule: primary
among my self-imposed rules is to make clear to
all participants that all points of view are to be re-
spected (although not necessarily agreed with), and
that students are coming to this class with different
strengths. If possible, an activity that integrates both
linguistic and computer science knowledge should
be brought in within the first week of the class; in
teaching CSE courses, I tend to emphasize the lin-
guistics a bit more in the first week.
I try to help students to engage with each
other: a good way to foster inter- and intra-
disciplinary respect is to have the students work col-
laboratively towards some goal. This can be chal-
lenging in a diverse student population setting; mon-
itoring progress of students and gently suggesting
turn-taking/mentoring strategies as well as design-
ing activities that speak to multiple backgrounds
can help ease the disparity between student back-
grounds. Preparing the students to engage with each
other on the same level by introducing online pre-
class activities can also help bring students together.
I try to allow students to build on previous
knowledge via processes other than lecturing: a
lecture, presented by either a student or a profes-
sor, is a ?one-size-fits-all? solution that in a diverse
population can sometimes either confuse unprepared
students, bore prepared students, or both. Interac-
tive in-class and out-of-class activities have the ad-
vantage of real-time evaluation of the understanding
of students. This is not to say that I never lecture;
but as a goal, lecturing should be short in duration
and focused on coordinating understanding among
the students. Over the years, I am gradually reduc-
ing the amount of lecturing I do, replacing it with
other activities.
By putting some simple techniques into place,
both students and I have noticed a significant im-
provement in the quality of classes. In Section 2,
I describe improvements to a graduate seminar that
facilitated interaction among a diverse group of par-
ticipants. The most recent offering of the 10-week
seminar class had 22 participants: 14 from CSE,
7 from Linguistics, and one from another depart-
ment. In my informal evaluation of background, 13
of the 22 participants were relatively new to the field
of computational linguistics (< 2 years experience).
Student-directed searching for background materi-
als, pre-posing of questions via a class website, and
blind reviewing of extended project abstracts by fel-
low students were effective strategies for providing
common ground.
Section 3 describes improvements in a lecture-
style class (Foundations of Spoken Language Pro-
cessing) which has a similarly diverse participant
base: the most recently completed offering had 7
CSE and 3 Linguistics Students, with the under-
grad/graduate student ratio 3:7. Devoting one of the
two weekly sessions to in-class group practical ex-
ercises also bolstered performance of all students.
2 Seminar structure
In developing a graduate seminar on machine learn-
ing for language processing, I was faced with a seri-
37
ous challenge: the previous seminar offering (on se-
quential machine learning) two years prior was not
as inspiring as one would hope, with several students
not actively participating in the class. This happened
in part because students were permitted to suggest
papers to read that week, which usually came from
their own research area and often had esoteric termi-
nology and mathematics. There was nothing wrong
with the papers per se, but many of the students were
not able to bridge the gap from their own experience
to get into the depths of the current paper. While I
thought having students partially control the seminar
agenda might provide ownership of the material, in
practice it gave a few students control of the session
each time. In the more recent offering, this problem
was likely to be exacerbated: the increased diversity
of backgrounds of the students in the class suggested
that it would be difficult to find common ground for
discussing advanced topics in machine learning.
In previous seminars, students had given
computer-projected presentations of papers, which
led to rather perfunctory, non-engaged discussions.
In the offering two years prior, I had banned
computerized presentations, but was faced with
the fact that many students still came unprepared
for discussions, so the sessions were somewhat
hit-and-miss.
In sum, a reorganization of the class seemed de-
sirable that would encourage more student partici-
pation, provide students the opportunity to improve
their background understanding, and still cover ad-
vanced topics.
2.1 A revised seminar structure
The previous instantiation of the seminar met twice
weekly for 1 1/2 hours; in the most recent offering
the seminar was moved to a single 2 1/2 hour block
on Fridays. Each week was assigned a pair of stu-
dent facilitators who were to lead the discussion for
the week. The instructor chose roughly four papers
on the topic of the week: one or two were more ba-
sic, overview papers (e.g., the Rabiner HMM tuto-
rial (Rabiner, 1989) or Lafferty et al?s Conditional
Random Fields paper (Lafferty et al, 2001)), and
the remaining were more advanced papers. Students
then had varying assigned responsibilities relating
to these papers and the topic throughout the week.
Out-of-class assignments were completed using dis-
cussion boards as part of Ohio State?s online course
management system.
The first assignment (due Tuesday evening) was
to find relevant review articles or resources (such as
class or tutorial slides) on the internet relating to the
topic of the week. Each student was to write and
post a short, one-paragraph summary of the tuto-
rial and its strengths and weaknesses. Asking the
students to find their own ?catch-up? resources pro-
vided a wealth of information for the class to look
at, as well as boosting the confidence of many stu-
dents by letting them find the information that best
suited them. I usually picked one (or possibly two)
of the tutorials for the class to examine as a whole
that would provide additional grounding for class
discussions.
The second assignment (due Thursday evening
at 8 pm) was for each student to post a series of
questions on the readings of the week. At a min-
imum, each student was required to ask one ques-
tion per week, but all of the students far exceeded
this. Comments such as ?I totally don?t understand
this section? were welcome (and encouraged) by the
instructor. Often (but not exclusively) these ques-
tions would arise from students whose background
knowledge was sparser. In the forum, there was a
general air of collegiality in getting everyone up to
speed: students often read each others? questions
and commented on them inline. Figure 1 shows a
sample conversation from the course; many of the
small clarifications that students needed were han-
dled in this manner, whereas the bigger discussion
topics were typically dealt with in class. Students
often pointed out the types of background informa-
tion that, if discussed in class, could help them better
understand the papers.
The facilitators of the week then worked Thurs-
day evening to collate the questions, find the ones
that were most common across the attendees or that
would lead to good discussion points, and develop
an order for the discussion on Friday. Facilitators
started each Friday session with a summary of the
main points of the papers (10-15 minutes maximum)
and then started the discussion by putting the ques-
tions up to the group. It was important that the facil-
itators did not need to know the answers to the ques-
tions, but rather how to pose the questions so that a
group discussion ensued. Facilitators almost always
38
Student 1: After reading all of these papers on [topic], astoundingly, a few of the concepts have started to sink in. The
formulas are mostly gibberish, but at least they?re familiar. Anyhow, I have only mostly dumb questions....
? [Paper 1]:
? Anyone want to talk about Kullback-Leibler divergence?
? We?ve see this before, but I forget. What is an l2 norm?
? What?s the meaning of an equal symbol with a delta over it?
? When it talked about the ?SI mode?, does that mean ?speaker independent??
? [Paper 2]:
? In multiple places, we see the where we have a vector and a matrix, and they compute the product of the
transpose of the vector with the matrix with the vector. Why are they doing that?
? [Paper 3]:
? I came away from this paper feeling like they gave a vague description of what they did, followed by results.
I mean, nice explanation of [topic] in general, but their whole innovation, as far as I can tell, fits into section
[section number]. I feel like I?m missing something huge here.
? [Paper 4]:
? So, they want to maximize 2/|w|, so they decide instead to minimize |w|2/2. Why? I mean, I get that it?s a
reciprocal, so you change from max to min, and that squaring it still makes it a minimization problem. But
why square it? Is this another instance of making the calculus easier?
? What are the ?sx? and ?si? training sentences?
Student 2: But why square it? Is this another instance of making the calculus easier? I think so. I think it has to do
with the fact that we will take its derivative, hence the 2 and 1/2 cancel each other. And since they?re just getting an
argmax, the 2 exponent doesn?t matter, since the maximum x2 can be found by finding the maximum x.
Student 3: ?sx? are the phonetically-compact sentences in the TIMIT database and ?si? are the phonetically-diverse
sentences.
Student 4: Ah thanks for that; I?ve wondered the same thing when seeing the phrase ?TIMIT si/sx?
Student 5: Oh, so ?si? and ?sx? do not represent the phones they are trying to learn and discern?
Figure 1: Conversation during question posting period in online discussion forum. Participants and papers have been
anonymized to protect the students.
had something to contribute to the conversation; re-
leasing them from absolutely needing to be sure of
the answer made them (and other participants) able
to go out on a limb more in the discussion.
I found that since the instructor usually has more
background knowledge with respect to many of the
questions asked, it was critical for me to have a
sense of timing for when the discussion was falter-
ing or getting off track and needed for me to jump
in. I spent roughly a half hour total of each session
(in 5-10 minute increments) up at the blackboard
quickly sketching some material (such as algorithms
unknown to about half of the class) to make a con-
nection. However, it was also important for me to
realize when to let the control of the class revert to
the facilitators.
The blackboard was a communal workspace: in
some of the later classes students also started to get
up and use the board to make points, or make points
on top of other students drawings. In the future, I
will encourage students to use this space from the
first session. I suspect that the lack of electronic pre-
sentation media contributed to this dynamism.
2.2 Class projects
The seminar required individual or team projects
that were developed through the term; presentations
took place in the last session and in finals week.
Three weeks prior to the end of term, each team sub-
mitted a two-page extended abstract describing their
39
What single aspect of the course did you find most helpful? Why?
Discussions.
Very good papers used.
Style of teaching atmosphere.
Just the discussion style.
Informal, discussion based.
The project.
[Instructor] really explained the intuitions behind the dense math. The pictorial method to explain algorithms.
The breadth of NLP problems addressed.
Instructor?s encouragement to get students involved in the classroom discussion.
Interaction between students, sharing questions.
Reading many papers on these topics was good training on how to pull out the important parts of the papers.
What single change in the course would you most like to see? Why?
There are a lot of papers ? keeps us busy and focused on the course, but it may be too much to comprehend in a single
term.
More background info.
None.
I think it is highly improved from 2 years ago. Good job.
Less emphasis on ASR.
Have some basic optional exercises on some of the math techniques discussed.
Less reading, covered at greater depth.
Make the material slightly less broad in scope.
More quick overviews of the algorithms for those of us who haven?t studied them before.
Figure 2: Comments from student evaluation forms for the seminar class
work, as if for a conference submission.
Each abstract was reviewed by three members
of the class using a standard conference reviewing
form; part of the challenge of the abstract submis-
sion is that it needed to be broad enough to be re-
viewed by non-experts in their area, but also needed
to be detailed enough to show that it was a reason-
able project. The reviews were collated and pro-
vided back to authors; along with the final project
writeup the team was required to submit a letter ex-
plaining how they handled the criticisms of the re-
viewers. This proved to be an excellent exercise
in perspective-taking (both in reviewing and writing
the abstract) and provided experience in tasks that
are critical to academic success.
I believe that injecting the tutorial-finding and
question-posting activities also positively affected
the presentations; many of the students used ter-
minology that was developed/discussed during the
course of the term. The project presentations
were generally stronger than presentations for other
classes that I have run in the past.
2.3 Feedback on new course structure
The student evaluations of the course were quite
positive (in terms of numeric scores), but perhaps
more telling were the free-form comments on the
course itself. Figure 2 shows some of the comments,
which basically show that students enjoyed the dy-
namic, interactive atmosphere; the primary negative
comment was about how much material was pre-
sented in the course.
After this initial experiment, some of my col-
leagues adopted the technique of preparing the stu-
dents for class via electronic discussion boards for
their own seminars. This has been used already for
two CL seminars (one at Ohio State and another at
University of Tu?bengen), and plans for a third semi-
nar at OSU (in a non-CL setting) are underway. The
professors leading those courses have also reported
positive experiences in increased interaction in the
class.
40
All in all, while the course was clearly not perfect,
it seems that many of the simple strategies that were
put into place helped bridge the gap between the
backgrounds of students; almost all of the students
found the class a rewarding experience. It is not
clear how this technique will scale to large classes:
there were roughly 20 participants in the seminar
(including auditors who came occasionally); dou-
bling the number of postings would probably make
facilitation much more difficult, so modifications
might be necessary to accommodate larger classes.
3 Group work within a lecture class
I have seen similar issues in diversity of prepara-
tion in an undergraduate/graduate lecture class enti-
tled ?Foundations of Spoken Language Processing.?
This class draws students from CSE, ECE, and Lin-
guistics departments, and from both undergraduate
and graduate populations.
3.1 Course structure
In early offerings of this class, I had primarily pre-
sented the material in lecture format; however, when
I taught it most recently, I divided the material
into weekly topics. I presented lectures on Tues-
day only, whereas on most Thursdays students com-
pleted group lab assignments; the remaining Thurs-
days were for group discussions. For the practi-
cal labs, students would bring their laptops to class,
connect wirelessly to the departmental servers, and
work together to solve some introductory problems.
The course utilizes several technologies for build-
ing system components: MATLAB for signal pro-
cessing, the AT&T Finite State Toolkit (Mohri et
al., 2001) for building ASR models and doing text
analysis1, and the SRI Language Modeling Toolkit
(Stolcke, 2002) for training n-gram language mod-
els. One of the key ideas behind the class is that
students learn to build an end-to-end ASR system
from the component parts, which helps them iden-
tify major research areas (acoustic features, acous-
tic models, search, pronunciation models, and lan-
guage models). We also re-use the same FST tools
to build the first pieces of a speech synthesis mod-
ule. Componentized technologies allow the students
1Subsequent offerings of the course will likely use the Open-
FST toolkit (Riley et al, 2008).
to take the first step beyond using a black-box sys-
tem and prepare them to understand the individual
components more deeply. The FST formalism helps
the Linguistics students, who often come to the class
with knowledge of formal language theory.
The group activities that get students of varying
backgrounds to interact constitute the heart of the
course, and provide a basis for the homework assign-
ments. Figure 3 outlines the practical lab sessions
and group discussions that were part of the most re-
cent offering of the course.
Weeks 1 and 3 offer complementary activities that
tend to bring the class together early on in the term.
In the first week, students are given some speech
examples from the TIMIT database; the first ex-
ample they see is phonetically labeled. Using the
Wavesurfer program (Sjo?lander and Beskow, 2000),
students look for characteristics in spectrograms that
are indicative of particular phonemes. Students are
then presented with a second, unlabeled utterance
that they need to phonetically label according to a
pronunciation chart. The linguists, who generally
have been exposed to this concept previously, tend
to lead groups; most students are surprised at how
difficult the task is, and this task provokes good
discussion about the difference between canonical
phonemes versus realized phones.
In the third week, students are asked to recreate
the spectrograms by implementing the mel filter-
bank equations in MATLAB. Engineering students
who have seen MATLAB before tend to take the
lead in this session, but there has been enough rap-
port among the students at this point, and there is
enough intuition behind the math in the tutorial in-
structions, that nobody in the previous session had
trouble grasping what was going on with the math:
almost all of the students completed the follow-on
homework, which was to fully compute Mel Fre-
quency Cepstral Coefficients (MFCCs) based on the
spectrogram code they developed in class. Because
both linguists and engineers have opportunities to
take the lead in these activities they help to build
groups that trust and rely on each other.
The second week?s activity is a tutorial that I had
developed for the Johns Hopkins University Sum-
mer School on Human Language Technology (sup-
ported by NSF and NAACL) based around the Finite
State Toolkit; the tutorial acquaints students with
41
Week Lecture topic Group activity
1 Speech production &
perception
Group discussion about spectrograms and phonemes; groups use
Wavesurfer (Sjo?lander and Beskow, 2000) to transcribe speech data.
2 Finite state representations Use FST tools for a basic language generation task where parts of speech
are substituted with words; use FST tools to break a simple letter-
substitution cipher probabilistically.
3 Frequency analysis &
acoustic features
Use MATLAB to implement Mel filterbanks and draw spectrograms (re-
ferring back to Week 1); use spectral representations to develop a ?Radio
Rex? simulation.
4 Dynamic Time Warping,
Acoustic Modeling
Quiz; Group discussion: having read various ASR toolkit manuals, if you
were a technical manager who needed to direct someone to implement
a system, which would you choose? What features does each toolkit
provide?
5 HMMs, EM, and Search The class acts out the token passing algorithm (Young et al, 1989), with
each group acting as a single HMM for a digit word (one, two, three...),
and post-it notes being exchanged as tokens.
6 Language models Build language models using the SRILM toolkit (Stolcke, 2002), and
compute the perplexity of Wall Street Journal text.
7 Text Analysis &
Speech Synthesis
Use FST tools to turn digit strings like ?345? into the corresponding word
string (?three hundred and forty five?). This tutorial grants more indepen-
dence than previous ones; students are expected to figure out that ?0? can
be problematic, for example.
8 Speech Synthesis
Speaker Recognition
Group discussion on a speaker recognition and verification tutorial paper
(Campbell, 1997)
9 Spoken Dialogue Systems Quiz; General discussion of any topic in the class.
10 Project presentations over the course of both sessions
Week Homework Task
2 Rank poker hands and develop end-to-end ASR system, both using finite state toolkit.
3 Finish Radio Rex implementation, compute MFCCs.
4 Replace week 2 homework?s acoustic model with different classifier/probabilistic model.
5 Implement Viterbi algorithm for isolated words.
6 Lattice rescoring with language models trained by the student.
7 Text normalization of times, dates, money, addresses, phone numbers, course numbers.
Figure 3: Syllabus for Foundations of Spoken Language Processing class with group activities and homeworks.
various finite state operations; the two tasks are a
simplified language generation task (convert the se-
quence ?DET N V DET N? into a sentence like ?the
man bit the dog?) and a cryptogram solver (solve
a simple substitution cipher by comparing frequen-
cies of crypttext letters versus frequencies of plain-
text letters). The students get experience, in par-
ticular, with transducer composition (which is novel
for almost all of the students); these techniques are
used in the first homework, which involves build-
ing a transducer-based pronunciation model for dig-
its (converting ?w ah n? into ?ONE?) and imple-
menting a FST composition chain for an ASR sys-
tem, akin to that of (Mohri et al, 2002). A sub-
sequent homework reuses this chain, but asks stu-
dents to implement a new acoustic model and re-
place the acoustic model outputs that are given in
the first homework. Similarly, practical tutorials on
language models (Week 6) and text analysis (Week
7) feed into homework assignments on rescoring
lattices with language models and turning different
kinds of numeric strings (addresses, time, course
numbers) into word strings.
Using group activities raises the question of how
to evaluate individual understanding. Homework
assignments in this class are designed to extend
the work done in-class, but must be done individ-
ually. Because many people will be starting from a
42
group code base, assignments will often look simi-
lar. Since the potential for plagiarism is a concern,
it is important that the assignments extend the group
activities enough that one can distinguish between
group and individual effort.
Another group activity that supports a homework
assignment is the Token Passing tutorial (Week 5).
The Token Passing algorithm (Young et al, 1989)
describes how to extend the Viterbi algorithm to
continuous speech: each word in the vocabulary is
represented by a single HMM, and as the Viterbi al-
gorithm reaches the end of an HMM at a particu-
lar timeframe, a token is ?emitted? from the HMM
recording the ending time, word identity, acous-
tic score, and pointer to the previous word-token.
The students are divided into small groups and each
group is assigned a digit word (one, two, ...) with
a particular pronunciation. The HMM topology as-
sumes only one, self-looping state per phone for
simplicity. The instructor then displays on the pro-
jector a likelihood for every phone for the first time
frame. The groups work to assign the forward prob-
abilities for the first frame. Once every group is syn-
chronized, the second frame of data likelihoods is
displayed, and students then again calculate forward
probabilities, and so forth. After the second frame,
some groups (?two?) start to emit tokens, which are
posted on the board; groups then have to also con-
sider starting a new word at the third time step. The
activity continues for roughly ten frames, at which
point the global best path is found. Including this ac-
tivity has had a beneficial effect on homework per-
formance: a significantly higher proportion of stu-
dents across all backgrounds correctly completed an
assignment to build an isolated word decoder in this
offering of the class compared to the previous offer-
ing.
Some of the activities were more conceptual in
nature, involving reading papers or manuals and
discussing the high-level concepts in small groups
(Weeks 4 and 8), with each group reporting back to
the class. One of the skills I hope to foster in stu-
dents is the ability to pick out the main points of pa-
pers during the reports back to the main group; I am
still thinking about ways to tie these activities into
strengthening the project presentations (Week 10).
For the next offering of the class in the upcoming
quarter, I would like to reuse the ideas developed in
the seminar to reduce the amount of lecturing. The
strategy I am considering is to give the students the
old lecture slides as well as readings, and have them
post questions the evening before class; we can then
focus discussion on the points they did not under-
stand. This will likely require the instructor to seed
the online pre-discussion with some of the impor-
tant points from the slides. These changes can be
discussed at the workshop.
3.2 Feedback
Student evaluations of the course were very positive;
in response to ?what single aspect of the course did
you find most helpful?,? half of the students chose to
respond, and all of the responses focused on the util-
ity of the hands-on practicals or homeworks. Anec-
dotally, I also felt that students were better able to re-
tain the concepts presented in the course in the most
recent offering than in previous offerings.
4 Summary
In trying to serve multiple populations of students
with different aims and goals, I have found that
activities can be designed that foster students? de-
velopment through team problem-solving and small
group work. Online resources such as discussion
boards and tutorials using software toolkits can be
effectively deployed to minimize the discrepancy in
preparations of the students.
Moving away from lecture formats (either in lec-
ture class or seminar presentations) has been helpful
in fostering cross-disciplinary interaction for both
seminar and lecture classes. I have found that ac-
tive learning techniques, such as the ones described
here, provide more immediate feedback to the in-
structor as to what material is understood and what
material needs extra emphasis.
Acknowledgments
The author would like to thank the anonymous students
who agreed to have their conversations published and
whose comments appear throughout the paper, as well as
Mike White for providing input on the use of the seminar
strategies in other contexts. This work was supported in
part by NSF CAREER grant IIS-0643901. The opinions
and findings expressed here are of the author and not of
any funding agency.
43
References
J.P. Campbell. 1997. Speaker recognition: A tutorial.
Proceedings of IEEE, 85:1437?1462.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. 18th In-
ternational Conference on Machine Learning.
M. Mohri, F. Pereira, and M. Riley, 2001. AT&T FSM
LibraryTM ? General-Purpose Finite-State Machine
Software Tools. AT&T, Florham Park, New Jersey.
Available at http://research.att.com/?fsmtools/fsm.
M. Mohri, F. Pereira, and M. Riley. 2002. Weighted
finite-state transducers in speech recognition. Com-
puter Speech and Language, 16(1):69?88.
L. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2).
M. Riley, J. Schalkwyk, W. Skut, C. Allauzen, and
M. Mohri. 2008. OpenFst library. www.openfst.org.
K. Sjo?lander and J. Beskow. 2000. Wavesurfer ? an open
source speech tool. In Proceedings of ICSLP, Beijing.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. Int?l Conf. on Spoken Lan-
guage Processing (ICSLP 2002), Denver, Colorado.
J. Vegso. 2008. Enrollments and degree produc-
tion at us cs departments drop further in 2006/2007.
http://www.cra.org/wp/index.php?p=139.
S. Young, N. Russell, and J. Thornton. 1989. To-
ken passing: a simple conceptual model for connected
speech recognition systems. Technical Report TR-38,
Cambridge University Engineering Department, Cam-
bridge, England.
44
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 28?31,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Using the Wiktionary Graph Structure for Synonym Detection
Timothy Weale, Chris Brew, Eric Fosler-Lussier
Department of Computer Science and Engineering
The Ohio State University
{weale,cbrew,fosler}@cse.ohio-state.edu
Abstract
This paper presents our work on using
the graph structure of Wiktionary for syn-
onym detection. We implement seman-
tic relatedness metrics using both a direct
measure of information flow on the graph
and a comparison of the list of vertices
found to be ?close? to a given vertex. Our
algorithms, evaluated on ESL 50, TOEFL
80 and RDWP 300 data sets, perform bet-
ter than or comparable to existing seman-
tic relatedness measures.
1 Introduction
The recent creation of large-scale, collabora-
tively constructed semantic resources provides re-
searchers with cheap, easily accessible informa-
tion. Previous metrics used for synonym detec-
tion had to be built using co-occurrence statistics
of collected corpora (Higgins, 2004) or expensive,
expert-created resources such as WordNet or Ro-
get?s Thesaurus (Jarmasz and Szpakowicz, 2003).
Here, we evaluate the effectiveness of Wiktionary,
a collaboratively constructed resource, as a source
of semantic relatedness information for the syn-
onym detection problem.
Researching these metrics is important because
they have been empirically shown to improve per-
formance in a variety of NLP applications, includ-
ing word sense disambiguation (Turdakov and Ve-
likhov, 2008), real-world spelling errors (Budan-
itsky and Hirst, 2006) and coreference resolution
(Strube and Ponzetto, 2006).
Synonym detection is a recognized testbed
for comparing semantic relatedness metrics (e.g
(Zesch et al, 2008)). In this task, a target word
or phrase is presented to the system, which is then
presented with four alternative words or phrases.
The goal of the system is to pick the alternative
most related to the target. Example questions can
be found in Figure 1.
Through the Wikimedia Foundation,
1
volun-
teers have created two large-scale, collaborative
resources that have been used in previous related-
ness research ? Wikipedia (an encyclopedia) and
Wiktionary (a dictionary). These sources have
been used for synonym detection and replicating
human relatedness evaluations using the category
structure (Strube and Ponzetto, 2006), local link
structure (Milne and Witten, 2008) and (Turdakov
and Velikhov, 2008) and global features (Zesch et
al., 2008). They contain related information but
focus on different information needs; which infor-
mation source provides better results depends on
the needs of the task. We use Wiktionary which,
due to its role as a dictionary, focuses on common
words and definitions ? the type of information
found in our synonym detection problems.
Both Wikipedia and Wiktionary are organized
around a basic ?page? unit, containing informa-
tion about an individual word, phrase or entity
in the world ? definitions, thesaurus entries, pro-
nunciation guides and translations in Wiktionary
and general biographical, organizational or philo-
sophical information in Wikipedia. In both data
sets, pages are linked to each each other and to
a user-created category structure ? a graph struc-
ture where pages are vertices of the graph and page
links are the graph edges. We will leverage this
graph for determining relatedness.
1
http://www.wikimedia.org/
Source Word Alternative Words
make earn, print, trade, borrow
flawed imperfect, tiny, lustrous, crude
solitary alone, alert, restless, fearless
Figure 1: Example TOEFL Questions
28
2 Extracting Relatedness Measures
We define relatedness based on information flow
through the entire Wiktionary graph, rather than
by any local in-bound or out-bound link structure.
This provides a global measurement of vertex im-
portance, as we do not limit the approach to com-
paring immediate neighbors.
To do this, we first run the PageRank algorithm
(Brin and Page, 1998) iteratively over the graph
until convergence to measure the a priori impor-
tance of each vertex in graph:
~
PR
t+1
= ??
(
~
PR
t
? E
)
+ (1 ? ?) ?
~
J (1)
In this, E contains the edge transition probabilities,
set to a uniform out-bound probability.
~
PR holds
the PageRank value for each vertex and
~
J is uni-
form vector used to randomly transition between
vertices. Traditionally, ? = 0.85 and is used to
tradeoff between a strict transition model and the
random-walk model.
We then adopt the extensions proposed in (Ol-
livier and Senellart, 2007) (OS) to determine re-
latedness given a source vertex:
~
R
t+1
= ??
(
~
R
t
? E + (
~
S ?
~
PR)
)
+(1??)?
~
J
(2)
~
S is a vector that contains zeros except for a one
at our source vertex, and
~
PR removes an overall
value of 1 based on the a priori PageRank value of
the vertex. In this way, vertices close to the source
are rewarded with weight and vertices that have a
high a priori importance are penalized. When
~
R
converges, it contains measures of importance for
vertices based on the source vertex.
Final relatedness values are then calculated
from the vector generated by Equation 2 and the
a priori importance of the vector based on the
PageRank from Equation 1:
rel
OS
(w, a) =
~
R
w
[a] ? log
(
1
PR[a]
)
(3)
w is the vertex for the source word and a is the
alternative word vertex. ThePR[a] penalty is used
to further ensure that our alternative vertex is not
highly valued simply because it is well-connected.
Applying Equation 3 provides comparable se-
mantic relatedness performance (see Tables 1 and
2). However, cases exist where a single data value
is insufficient to make an adequate determination
of word relatedness because of small differences
for candidate words. We can incorporate addi-
tional relatedness information about our vertices
by leveraging information about the set of vertices
deemed ?most related? to our current vertex.
2.1 Integrating N-Best Neighbors
We add information by looking at the similarity
between the n-best related words for each vertex.
Intuitively, given a source word w and candidate
alternatives a
1
and a
2
,
2
we look at the set of words
that are semantically related to each of the can-
didates (represented as vectors W , A
1
and A
2
).
If the overlap between elements of W and A
1
is
greater thanW andA
2
, A
1
is more likely to be the
synonym of W .
Highly-ranked shared elements are good indi-
cators of relatedness and should contribute more
than low-ranked related words. Lists with many
low-ranked words could be an artifact of the data
set and should not be ranked higher than ones con-
taining a few high-ranked words.
Our ranked-list comparison metric (NB) is a se-
lective mean reciprocal ranking function:
rel
NB
(
~
W,
~
A, n) =
n
?
r=1
1
r
? ?(W
r
?
~
A) (4)
~
W is the n-best list based on the source vertex
and
~
A is the n-best list based on the alternative
vertex. Values are added to our relatedness metric
based on the position of a vertex in the target list
and the traditional Dirac ?-function, which has a
value of one if the target vertex appears anywhere
in our candidate list and a zero in all other cases.
Each metric (OS and NB) will have different
ranges. We therefore normalize the reported value
by scaling each based on the maximum value for
that portion in order to achieve a uniform scale.
Our final metric (OS+NB) is created by aver-
aging the two normalized scores. In this work,
both scores are given equal weighting. Deriving
weightings for combining the two scores will be
part of our future work.
rel
OS+NB
(w
i
,
j
) =
OS(c
i
, c
j
) + NB(c
i
, c
j
, n)
2
(5)
In this, OS() returns the normalized rel
OS
()
value and NB() returns the normalized rel
NB
value. The maximum rel
P+N
() value of 1.0 is
achieved if c
j
has the highest PageRank-based
value and the highest N-Best value.
2
See Figure 1
29
Source
ESL TOEFL
Acc. (%) Acc. (%)
JPL 82 78.8
LC-IR 78 81.3
OS 86 88.8
NB 80 88.8
OS+NB 88 93.8
Table 1: ESL and TOEFL Performance
3 Evaluation
We present performance results on three data sets.
The first, ESL, uses 50 questions from the English
as a Second Language test (Turney, 2001). Next,
an 80 question data set from the Test of English
as a Foreign Language (TOEFL) is used (Lan-
dauer and Dumais, 1997). Finally, we evaluate
on the Reader?s Digest WordPower (RDWP) data
set (Jarmasz and Szpakowicz, 2003). This is a set
of 300 synonym detection problems gathered from
the Word Power game of the Canadian edition of
Reader?s Digest Word from 2000 ? 2001.
We use the Feb. 03, 2009 version of the English
Wiktionary data set
3
for extracting graph structure
and relatedness information.
Table 1 presents the performance of our algo-
rithm on the ESL and TOEFL test sets. Our results
are compared to Jarmasz and Szpakowicz (2003),
which uses a path-based cost on the structure
of Roget?s Thesaurus (JPL) and a cooccurence-
based metric, LC-IR (Higgins, 2004), which con-
strained context to only consider adjacent words in
structured web queries.
Information about our algorithm?s performance
on the RDWP test set is found in Table 2. Our re-
sults are compared to the previously mentioned al-
gorithms and also the work of Zesch et al (2008).
Their first metric (ZPL) uses the path length be-
tween two graph vertices for relatedness determi-
nation. The second, (ZCV), creates concept vec-
tors based on a distribution of pages that contain a
particular word.
RDWP is not only larger then the previous two,
but also more complictated. TOEFL and ESL
average 1.0 and 1.008 number of words in each
source and alternative, respectively. For RDWP
each entry averages 1.4 words.
We map words and phrases to graph vertices by
first matching against the page title. If there is no
3
http://download.wikimedia.org
match, we follow the approach outlined in (Zesch
et al, 2008). Common words are removed from
the phrase
4
and for every remaining word in the
phrase, we determine the page mapping for that
individual word. The relatedness of the phrase
is then set to be the maximum relatedness value
attributed to any of the individual words in the
phrase.
Random guessing by an algorithm could in-
crease algorithm performance through random
chance. Therefore, we present both a overall
percentage and also a precision-based percentage.
The first (Raw) is defined as the correct number of
guesses over all questions. The second (Prec) is
defined as the correct number of guesses divided
by only those questions that were attempted.
3.1 Discussion
For NB and OS+NB, we set n = 3000 based on
TOEFL data set training.
5
Testing was then per-
formed on the ESL and RDWP data set.
As shown in Table 1, the OS algorithm per-
forms better on the task than the comparison sys-
tems. On its own, NB relatedness performs well ?
at or slightly worse than OS. Combining the two
measures increases performance on both data sets.
While our TOEFL results are below the reported
performance of (Turney et al, 2003) (97.5%), we
do not use any task-dependent learning for our re-
sults and our algorithms have better performance
than any individual module in their system.
Combining OS with NB mitigates the influence
of OS when it is not confident. OS correctly picks
?pinnacle? as a synonym of ?zenith? with a relat-
edness value 126,000 times larger than its next
competitor. For ?consumed?, OS is wrong, giving
?bred? a higher score than ?eaten? ? but only by
a value 1.2 times that of ?eaten?. The latter case
is overcome by the addition of n-best information
while the former is unaffected.
Table 2 demonstrates that we have results com-
parable to existing state-of-the-art measures. Our
choice of n resulted in reduced scores on this task
when compared to using the OS metric by itself.
But, our algorithm still outperforms both the ZPL
and ZCV metrics for our data set in raw scores and
in three out of the four precision measures. Fur-
ther refinement of the RDWP data set mapping or
changing our metric score to a weighted sum of
4
Defined here as: {and, or, to, be, the, a, an, of, on, in, for,
with, by, into, is, no}
5
Out of 1.1 million vertices
30
Metric Source Attempted Score # Ties Raw Prec
JPL Roget?s 300 223 0 .74 .74
LC-IR Web 300 224.33 - .75 .75
ZPL
Wikipedia
226 88.33 96 .29 .39
ZCV 288 165.83 2 .55 .58
ZPL
Wiktionary
201 103.7 55 .35 .52
ZCV 174 147.3 3 .49 .85
OS
Wiktionary
300 234 0 .78 .78
NB 300 212 0 .71 .71
OS+NB 300 227 0 .76 .76
Table 2: Reader?s Digest WordPower 300 Overall Performance
sorts (rather than a raw maximum) could result in
increased performance.
Wiktionary?s coverage enables all words in the
first two tasks to be found (with the exception of
?bipartisanly?). Enough of the words in the RDWP
task are found to enable the algorithm to attempt
all synonym detection questions.
4 Conclusion and Future Work
In this paper, we have demonstrated the effective-
ness of Wiktionary as a source of relatedness in-
formation when coupled with metrics based on
information flow using synonym detection as our
evaluation testbed.
Our immediate work will be in learning weights
for the combination measure, using (Turney et al,
2003) as our guideline. Additional work will be in
automatically determining an effective value for n
across all data sets.
Long-term work will be in modifying the page
transition values to achieve non-uniform transition
values. Links are of differing quality, and the tran-
sition probabilities should reflect that.
References
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-Scale Hypertextual Web Search En-
gine. Computer Networks and ISDN Systems, 30(1?
7):107?117.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based Measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13?47.
Derrick Higgins. 2004. Which Statistics Reflect Se-
mantics? Rethinking Synonymy and Word Similar-
ity. In Proceedings of the International Conference
on Linguistic Evidence.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
Thesaurus and Semantic Similarity. In Proceedings
of Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2003).
Thomas K. Landauer and Susan T. Dumais. 1997. A
Solution to Plato?s Problem: The Latent Semantic
Analysis Theory of Acquisition, Induction, and Rep-
resentation of Knowledge. Psychological Review.
David Milne and Ian H. Witten. 2008. An Effec-
tive, Low-Cost Measure of Semantic Relatedness
Obtained from Wikipedia Links. In Proceedings of
AAAI 2008.
Yann Ollivier and Pierre Senellart. 2007. Finding Re-
lated Pages Using Green Measures: An Illustration
with Wikipedia. In Proceedings of AAAI 2007.
Michael Strube and Simone Paolo Ponzetto. 2006.
WikiRelate! Computing Semantic Relatedness Us-
ing Wikipedia. In AAAI.
Denis Turdakov and Pavel Velikhov. 2008. Semantic
relatedness metric for Wikipedia concepts based on
link analysis and its application to word sense dis-
ambiguation. In Proceedings of CEUR.
Peter D. Turney, Michael L. Littman, Jeffrey Bigham,
and Victor Shnayder. 2003. Combining Indepen-
dent Modules in Lexical Multiple-Choice Problems.
In Recent Advances in Natural Language Processing
III: Selected Papers from RANLP 2003.
Peter D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning (ECML-2001), pages 491?502, Freidburg,
Germany.
Torsten Zesch, Christof Muller, and Iryna Gurevych.
2008. Using Wiktionary for Computing Semantic
Relatedness. In Proceedings of AAAI 2008.
31
Discourse Segmentation of Multi-Party Conversation
Michel Galley Kathleen McKeown
Columbia University
Computer Science Department
1214 Amsterdam Avenue
New York, NY 10027, USA
{galley,kathy}@cs.columbia.edu
Eric Fosler-Lussier
Columbia University
Electrical Engineering Department
500 West 120th Street
New York, NY 10027, USA
fosler@ieee.org
Hongyan Jing
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, USA
hjing@us.ibm.com
Abstract
We present a domain-independent topic
segmentation algorithm for multi-party
speech. Our feature-based algorithm com-
bines knowledge about content using a
text-based algorithm as a feature and
about form using linguistic and acous-
tic cues about topic shifts extracted from
speech. This segmentation algorithm uses
automatically induced decision rules to
combine the different features. The em-
bedded text-based algorithm builds on lex-
ical cohesion and has performance compa-
rable to state-of-the-art algorithms based
on lexical information. A significant er-
ror reduction is obtained by combining the
two knowledge sources.
1 Introduction
Topic segmentation aims to automatically divide text
documents, audio recordings, or video segments,
into topically related units. While extensive research
has targeted the problem of topic segmentation of
written texts and spoken monologues, few have stud-
ied the problem of segmenting conversations with
many participants (e.g., meetings). In this paper, we
present an algorithm for segmenting meeting tran-
scripts. This study uses recorded meetings of typi-
cally six to eight participants, in which the informal
style includes ungrammatical sentences and overlap-
ping speakers. These meetings generally do not have
pre-set agendas, and the topics discussed in the same
meeting may or may not related.
The meeting segmenter comprises two compo-
nents: one that capitalizes on word distribution to
identify homogeneous units that are topically cohe-
sive, and a second component that analyzes conver-
sational features of meeting transcripts that are in-
dicative of topic shifts, like silences, overlaps, and
speaker changes. We show that integrating features
from both components with a probabilistic classifier
(induced with c4.5rules) is very effective in improv-
ing performance.
In Section 2, we review previous approaches to
the segmentation problem applied to spoken and
written documents. In Section 3, we describe the
corpus of recorded meetings intended to be seg-
mented, and the annotation of its discourse structure.
In Section 4, we present our text-based segmenta-
tion component. This component mainly relies on
lexical cohesion, particularly term repetition, to de-
tect topic boundaries. We evaluated this segmenta-
tion against other lexical cohesion segmentation pro-
grams and show that the performance is state-of-the-
art. In the subsequent section, we describe conver-
sational features, such as silences, speaker change,
and other features like cue phrases. We present a
machine learning approach for integrating these con-
versational features with the text-based segmenta-
tion module. Experimental results show a marked
improvement in meeting segmentation with the in-
corporation of both sets of features. We close with
discussions and conclusions.
2 Related Work
Existing approaches to textual segmentation can be
broadly divided into two categories. On the one
hand, many algorithms exploit the fact that topic
segments tend to be lexically cohesive. Embodi-
ments of this idea include semantic similarity (Mor-
ris and Hirst, 1991; Kozima, 1993), cosine similarity
in word vector space (Hearst, 1994), inter-sentence
similarity matrix (Reynar, 1994; Choi, 2000), en-
tity repetition (Kan et al, 1998), word frequency
models (Reynar, 1999), or adaptive language models
(Beeferman et al, 1999). Other algorithms exploit
a variety of linguistic features that may mark topic
boundaries, such as referential noun phrases (Pas-
sonneau and Litman, 1997).
In work on segmentation of spoken docu-
ments, intonational, prosodic, and acoustic indica-
tors are used to detect topic boundaries (Grosz and
Hirschberg, 1992; Nakatani et al, 1995; Hirschberg
and Nakatani, 1996; Passonneau and Litman, 1997;
Hirschberg and Nakatani, 1998; Beeferman et al,
1999; Tu?r et al, 2001). Such indicators include
long pauses, shifts in speaking rate, great range in
F0 and intensity, and higher maximum accent peak.
These approaches use different learning mecha-
nisms to combine features, including decision trees
(Grosz and Hirschberg, 1992; Passonneau and Lit-
man, 1997; Tu?r et al, 2001) exponential models
(Beeferman et al, 1999) or other probabilistic mod-
els (Hajime et al, 1998; Reynar, 1999).
3 The ICSI Meeting Corpus
We have evaluated our segmenter on the ICSI Meet-
ing corpus (Janin et al, 2003). This corpus is one of
a growing number of corpora with human-to-human
multi-party conversations. In this corpus, record-
ings of meetings ranged primarily over three differ-
ent recurring meeting types, all of which concerned
speech or language research.1 The average duration
is 60 minutes, with an average of 6.5 participants.
They were transcribed, and each conversation turn
was marked with the speaker, start time, end time,
and word content.
From the corpus, we selected 25 meetings to be
segmented, each by at least three subjects. We
opted for a linear representation of discourse, since
finer-grained discourse structures (e.g. (Grosz and
Sidner, 1986)) are generally considered to be diffi-
cult to mark reliably. Subjects were asked to mark
each speaker change (potential boundary) as either
boundary or non-boundary. In the resulting anno-
tation, the agreed segmentation based on majority
1While it would be desirable to have a broader variety of
meetings, we hope that experiments on this corpus will still
carry some generality.
opinion contained 7.5 segments per meeting on av-
erage, while the average number of potential bound-
aries is 770. We used Cochran?s Q (1950) to eval-
uate the agreement among annotators. Cochran?s
test evaluates the null hypothesis that the number
of subjects assigning a boundary at any position is
randomly distributed. The test shows that the inter-
judge reliability is significant to the 0.05 level for 19
of the meetings, which seems to indicate that seg-
ment identification is a feasible task.2
4 Segmentation based on Lexical Cohesion
Previous work on discourse segmentation of written
texts indicates that lexical cohesion is a strong in-
dicator of discourse structure. Lexical cohesion is
a linguistic property that pertains to speech as well,
and is a linguistic phenomenon that can also be ex-
ploited in our case: while our data does not have
the same kind of syntactic and rhetorical structure
as written text, we nonetheless expect that informa-
tion from the written transcription alone should pro-
vide indications about topic boundaries. In this sec-
tion, we describe our work on LCseg, a topic seg-
menter based on lexical cohesion that can handle
both speech and text, but that is especially designed
to generate the lexical cohesion feature used in the
feature-based segmentation described in Section 5.
4.1 Algorithm Description
LCseg computes lexical chains, which are thought
to mirror the discourse structure of the underly-
ing text (Morris and Hirst, 1991). We ignore syn-
onymy and other semantic relations, building a re-
stricted model of lexical chains consisting of sim-
ple term repetitions, hypothesizing that major topic
shifts are likely to occur where strong term repeti-
tions start and end. While other relations between
lexical items also work as cohesive factors (e.g. be-
tween a term and its super-ordinate), the work on
linear topic segmentation reporting the most promis-
ing results account for term repetitions alone (Choi,
2000; Utiyama and Isahara, 2001).
The preprocessing steps of LCseg are common to
many segmentation algorithms. The input document
is first tokenized, non-content words are removed,
2Four other meetings failed short the significance test, while
there was little agreement on the two last ones (p > 0.1).
and remaining words are stemmed using an exten-
sion of Porter?s stemming algorithm (Xu and Croft,
1998) that conflates stems using corpus statistics.
Stemming will allow our algorithm to more accu-
rately relate terms that are semantically close.
The core algorithm of LCseg has two main parts:
a method to identify and weight strong term repeti-
tions using lexical chains, and a method to hypothe-
size topic boundaries given the knowledge of multi-
ple, simultaneous chains of term repetitions.
A term is any stemmed content word within the
text. A lexical chain is constructed to consist of all
repetitions ranging from the first to the last appear-
ance of the term in the text. The chain is divided into
subchains when there is a long hiatus of h consecu-
tive sentences with no occurrence of the term, where
h is determined experimentally. For each hiatus, a
new division is made and thus, we avoid creating
weakly linked chains.
For all chains that have been identified, we use a
weighting scheme that we believe is appropriate to
the task of inducing the topical or sub-topical struc-
ture of text. The weighting scheme depends on two
factors:
Frequency: chains containing more repeated
terms receive a higher score.
Compactness: shorter chains receive a higher
weight than longer ones. If two chains of different
lengths contain the same number of terms, we assign
a higher score to the shortest one. Our assumption
is that the shorter one, being more compact, seems
to be a better indicator of lexical cohesion.3
We apply a variant of a metric commonly used
in information retrieval, TF.IDF (Salton and Buck-
ley, 1988), to score term repetitions. If R1 . . . Rn is
the set of all term repetitions collected in the text,
t1 . . . tn the corresponding terms, L1 . . . Ln their re-
spective lengths,4 and L the length of the text, the
adapted metric is expressed as follows, combining
frequency (freq(ti)) of a term ti and the compact-
ness of its underlying chain:
score(Ri) = freq(ti) ? log( LLi )
3The latter parameter might seem controversial at first, and
one might assume that longer chains should receive a higher
score. However we point out that in a linear model of dis-
course, chains that almost span the entire text are barely indica-
tive of any structure (assuming boundaries are only hypothe-
sized where chains start and end).
4All lengths are expressed in number of sentences.
In the second part of the algorithm, we combine
information from all term repetitions to compute a
lexical cohesion score at each sentence break (or,
in the case of spoken conversations, speaker turn
break). This step of our algorithm is very similar
in spirit to TextTiling (Hearst, 1994). The idea is to
work with two adjacent analysis windows, each of
fixed size k. For each sentence break, we determine
a lexical cohesion function by computing the cosine
similarity at the transition between the two windows.
Instead of using word counts to compute similarity,
we analyze lexical chains that overlap with the two
windows. The similarity between windows (A and
B) is computed with:5
cosine(A,B) =
?
i
wi,A?wi,B??
i
w2i,A
?
i
w2i,B
where
wi,? =
{
score(Ri) if Ri overlaps ? ? {A,B}
0 otherwise
The similarity computed at each sentence break
produces a plot that shows how lexical cohesion
changes over time; an example is shown in Figure 1.
The lexical cohesion function is then smoothed us-
ing a moving average filter, and minima become po-
tential segment boundaries. Then, in a manner quite
similar to (Hearst, 1994), the algorithm determines
for every local minimum mi how sharp of a change
there is in the lexical cohesion function. The algo-
rithm looks on each side of mi for maxima of cohe-
sion, and once it eventually finds one on each side (l
and r), it computes the hypothesized segmentation
probability:
p(mi) = 12 [LCF(l) + LCF(r) ? 2 ? LCF(m)]
where LCF(x) is the value of the lexical cohesion
function at x.
This score is supposed to capture the sharpness of
the change in lexical cohesion, and give probabilities
close to 1 for breaks like sentence 179 in Figure 1.
Finally, the algorithm selects the hypothesized
boundaries with the highest computed probabilities.
If the number of reference boundaries is unknown,
the algorithm has to make a guess. It computes the
5Normalizing anything in these windows has little ef-
fect, since the cosine similarity is scale invariant, that is
cosine(?xa, xb) = cosine(xa, xb) for ? > 0.
20 40 60 80 100 120 140 160 180 200 220 240 260
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 1: Application of the LCseg algorithm on the concatenation of 16 WSJ stories. Numbers on the
x-axis represent sentence indices, and y-axis represents the lexical cohesion function. The representative
example presented here is segmented by LCseg with an error of Pk = 15.79, while the average performance
of the algorithm is Pk = 15.31 on the WSJ test corpus (unknown number of segments).
mean and the variance of the hypothesized probabil-
ities of all potential boundaries (local minima). As
we can see in Figure 1, there are many local minima
that do not correspond to actual boundaries. Thus,
we ignore all potential boundaries with a probability
lower than plimit. For the remaining points, we com-
pute the threshold using the average (?) and standard
deviation (?) of the p(mi) values, and each potential
boundary mi above the threshold ??? ?? is hypoth-
esized as a real boundary.
4.2 Evaluation
We evaluate LCseg against two state-of-the-art seg-
mentation algorithms based on lexical cohesion
(Choi, 2000; Utiyama and Isahara, 2001). We use
the error metric Pk proposed by Beeferman et al
(1999) to evaluate segmentation accuracy. It com-
putes the probability that sentences k units (e.g. sen-
tences) apart are incorrectly determined as being ei-
ther in different segments or in the same one. Since
it has been argued in (Pevzner and Hearst, 2002) that
Pk has some weaknesses, we also include results ac-
cording to the WindowDiff (WD) metric (which is
described in the same work).
A test corpus of concatenated6 texts extracted
from the Brown corpus was built by Choi (2000)
to evaluate several domain-independent segmenta-
tion algorithms. We reuse the same test corpus for
our evaluation, in addition to two other test corpora
we constructed to test how segmenters scale across
genres and how they perform with texts with various
6Concatenated documents correspond to reference seg-
ments.
number of segments.7 We designed two test corpora,
each of 500 documents, using concatenated texts
extracted from the TDT and WSJ corpora, ranging
from 4 to 22 in number of segments.
LCseg depends on several parameters. Parameter
tuning was performed on three tuning corpora of one
thousand texts each.8 We performed searches for the
optimal settings of the four tunable parameters in-
troduced above; the best performance was achieved
with h = 11 (hiatus length for dividing a chain into
parts), k = 2 (analysis window size), plimit = 0.1
and ? = 12 (thresholding limits for the hypothesized
boundaries).
As shown in Table 1, our algorithm is signifi-
cantly better than (Choi, 2000) (labeled C99) on
all three test corpora, according to a one-sided t-
test of the null hypothesis of equal mean at the 0.01
level. It is not clear whether our algorithm is better
than (Utiyama and Isahara, 2001) (U00). When the
number of segments is provided to the algorithms,
our algorithm is significantly better than Utiyama?s
on WSJ, better on Brown (but not significant), and
significantly worse on TDT. When the number of
boundaries is unknown, our algorithm is insignifi-
cantly worse on Brown, but significantly better on
WSJ and TDT ? the two corpora designed to have
a varying number of segments per document. In the
case of the Meeting corpus, none of the algorithms
are significantly different than the others, due to the
7All texts in Choi?s test corpus have exactly 10 segments.
We are concerned that the adjustments of any algorithm param-
eters might overfit this predefined number of segments.
8These texts are different from the ones used for evaluation.
Brown corpus
known unknown
Pk WD Pk WD
C99 11.19% 13.86% 12.07% 14.57%
U00 8.77% 9.44% 9.76% 10.32%
LCseg 8.69% 9.42% 10.49% 11.37%
p-val. 0.42 0.48 0.027 0.0037
TDT corpus
C99 9.37% 11.91% 10.18% 12.72%
U00 4.70% 6.29% 8.70% 11.12%
LCseg 6.15% 8.41% 6.95% 9.09%
p-val. 1.1e-05 2.8e-07 4.5e-05 2.8e-05
WSJ corpus
C99 19.61% 26.42% 22.32% 29.81%
U00 15.18% 21.54% 17.71% 24.06%
LCseg 12.21% 18.25% 15.31% 22.14%
p-val. 1.4e-08 1.7e-08 2.6e-04 0.0063
Meeting corpus
C99 33.79% 37.25% 47.42% 58.08%
U00 31.99% 34.49% 37.39% 40.43%
LCseg 26.37% 29.40% 31.91% 35.88%
p-val. 0.026 0.14 0.14 0.23
Table 1: Comparison C99 and U00. The p-values in
the table are the results of significance tests between
U00 and LCseg. Bold-faced values are scores that
are statistically significant.
small test set size.
In conclusion, LCseg has a performance compara-
ble to state-of-the-art text segmentation algorithms,
with the added advantage of computing a segmen-
tation probability at each potential boundary. This
information can be effectively used in the feature-
based segmenter to account for lexical cohesion, as
described in the next section.
5 Feature-based Segmentation
In the previous section, we have concentrated exclu-
sively on the consideration of content (through lexi-
cal cohesion) to determine the structure of texts, ne-
glecting any influence of form. In this section, we
explore formal devices that are indicative of topic
shifts, and explain how we use these cues to build a
segmenter targeting conversational speech.
5.1 Probabilistic Classifiers
Topic segmentation is reduced here to a classifica-
tion problem, where each utterance break Bi is ei-
ther considered a topic boundary or not. We use
statistical modeling techniques to build a classifier
that uses local features (e.g. cue phrases, pauses)
to determine if an utterance break corresponds to
a topic boundary. We chose C4.5 and C4.5rules
(Quinlan, 1993), two programs to induce classifi-
cation rules in the form of decision trees and pro-
duction rules (respectively). C4.5 generates an un-
pruned decision tree, which is then analyzed by
C4.5rules to generate a set of pruned production
rules (it tries to find the most useful subset of them).
The advantage of pruned rules over decision trees is
that they are easier to analyze, and allow combina-
tion of features in the same rule (feature interactions
are explicit).
The greedy nature of decision rule learning algo-
rithms implies that a large set of features can lead
to bad performance and generalization capability. It
is desirable to remove redundant and irrelevant fea-
tures, especially in our case since we have little data
labeled with topic shifts; with a large set of fea-
tures, we would risk overfitting the data. We tried
to restrict ourselves to features whose inclusion is
motivated by previous work (pauses, speech rate)
and added features that are specific to multi-speaker
speech (overlap, changes in speaker activity).
5.2 Features
Cue phrases: previous work on segmentation has
found that discourse particles like now, well pro-
vide valuable information about the structure of texts
(Grosz and Sidner, 1986; Hirschberg and Litman,
1994; Passonneau and Litman, 1997). We analyzed
the correlation between words in the meeting cor-
pus and labeled topic boundaries, and automatically
extracted utterance-initial cue phrases9 that are sta-
tistically correlated with boundaries. For every word
in the meeting corpus, we counted the number of its
occurrences near any topic boundary, and its num-
ber of appearances overall. Then, we performed ?2
significance tests (e.g. figure 2 for okay) under the
null hypothesis that no correlation exists. We se-
lected terms whose ?2 value rejected the hypothesis
under a 0.01-level confidence (the rejection criterion
is ?2 ? 6.635). Finally, induced cue phrases whose
usage has never been described in other work were
removed (marked with ? in Table 3). Indeed, there
is a risk that the automatically derived list of cue
phrases could be too specific to the word usage in
9As in (Litman and Passonneau, 1995), we restrict ourselves
to the first lexical item of any utterance, plus the second one if
the first item is also a cue word.
Near boundary Distant
okay 64 740
Other 657 25896
Table 2: okay (?2 = 89.11, df = 1, p < 0.01).
okay 93.05 but 13.57
shall ? 27.34 so 11.65
anyway 23.95 and 10.99
we?re ? 17.67 should ? 10.21
alright 16.09 good ? 7.70
let?s ? 14.54
Table 3: Automatically selected cue phrases.
these meetings.
Silences: previous work has found that ma-
jor shifts in topic typically show longer silences
(Passonneau and Litman, 1993; Hirschberg and
Nakatani, 1996). We investigated the presence of
silences in meetings and their correlation with topic
boundaries, and found it necessary to make a distinc-
tion between pauses and gaps (Levinson, 1983). A
pause is a silence that is attributable to a given party,
for example in the middle of an adjacency pair, or
when a speaker pauses in the middle of her speech.
Gaps are silences not attributable to any party, and
last until a speaker takes the initiative of continuing
the discussion. As an approximation of this distinc-
tion, we classified a silence that follows a question or
in the middle of somebody?s speech as a pause, and
any other silences as a gap. While the correlation be-
tween long silences and discourse boundaries seem
to be less pervasive in meetings than in other speech
corpora, we have noticed that some topic boundaries
are preceded (within some window) by numerous
gaps. However, we found little correlation between
pauses and topic boundaries.
Overlaps: we also analyzed the distribution of
overlapping speech by counting the average overlap
rate within some window. We noticed that, many
times, the beginning of segments are characterized
by having little overlapping speech.
Speaker change: we sometimes noticed a corre-
lation between topic boundaries and sudden changes
in speaker activity. For example, in Figure 2, it
is clear that the contribution of individual speakers
to the discussion can greatly change from one dis-
course unit to the next. We try to capture significant
changes in speakership by measuring the dissimilar-
ity between two analysis windows. For each poten-
tial boundary, we count for each speaker i the num-
ber of words that are uttered before (Li) and after
(Ri) the potential boundary (we limit our analysis
to a window of fixed size). The two distributions
are normalized to form two probability distributions
l and r, and significant changes of speakership are
detected by computing their Jensen-Shannon diver-
gence:
JS(l, r) = 12 [D(l||avgl,r) + D(r||avgl,r)]
where D(l||r) is the KL-divergence between the
two distributions.
Lexical cohesion: we also incorporated the lexi-
cal cohesion function computed by LCseg as a fea-
ture of the multi-source segmenter in a manner simi-
lar to the knowledge source combination performed
by (Beeferman et al, 1999) and (Tu?r et al, 2001).
Note that we use both the posterior estimate com-
puted by LCseg and the raw lexical cohesion func-
tion as features of the system.
5.3 Features: Selection and Combination
For every potential boundary Bi, the classifier ana-
lyzes features in a window surrounding Bi to decide
whether it is a topic boundary or not. It is generally
unclear what is the optimal window size and how
features should be analyzed. Windows of various
sizes can lead to different levels of prediction, and
in some cases, it might be more appropriate to only
extract features preceding or following Bi.
We avoided making arbitrary choices of parame-
ters; instead, for any feature F and a set F1, . . . , Fn
of possible ways to measure the feature (different
window sizes, different directions), we picked the Fi
that is in isolation the best predictor of topic bound-
aries (among F1, . . . , Fn). Table 4 presents for each
feature the analysis mode that is the most useful on
the training data.
5.4 Evaluation
We performed 25-fold cross-validation for evaluat-
ing the induced probabilistic classifier, computing
the average of Pk and WD on the held-out meet-
ings. Feature selection and decision rule learning
0 10 20 30
Figure 2: speaker activity in a meeting. Each row represent the speech activity of one speaker, utterance of
words being represented as black. Vertical lines represent topic shifts. The x-axis represents time.
Feature Tag Size (sec.) Side
Cue phrases CUE 5 both
Silence (gaps) SIL 30 left
Overlap? OVR 30 right
Speaker activity ACT 5 both
Lexical cohesion LC 30 both
?: the size of the window that was used to compute the
JS-divergence was also determined automatically.
Table 4: Parameters for feature analysis.
is always performed on sets of 24 meetings, while
the held-out data is used for testing. Table 5 gives
some examples of the type of rules that are learned.
The first rule states that if the value for the lexical
cohesion (LC) function is low at the current sen-
tence break, there is at least one CUE phrase, there
is less than three seconds of silence to the left of the
break,10 and a single speaker holds the floor for a
longer period of time than usual to the right of the
break, then we have a topic break. In general, we
found that the derived rules show that lexical cohe-
sion plays a stronger role than most other features
in determining topic breaks. Nonetheless, the quan-
titative results summarized in table 6, which corre-
spond to the average performance on the held-out
sets, show that the integration of conversational fea-
tures with the text-based segmenter outperforms ei-
ther alone.
6 Conclusions
We presented a domain-independent segmentation
algorithm for multi-party conversation that inte-
grates features based on content with features based
on form. The learned combination of features results
in a significant increase in accuracy over previous
10Note that rules are not always meaningful in isolation and
it is likely that a subordinate rule in the tree to this one would do
further tests on silence to determine if a topic boundary exists.
Condition Decision Conf.
LC ? 0.67,CUE ? 1,
OVR ? 1.20,SIL ? 3.42 yes 94.1
LC ? 0.35,SIL > 3.42,
OVR ? 4.55 yes 92.2
CUE ? 1,ACT > 0.1768,
OVR ? 1.20,LC ? 0.67 yes 91.6
. . .
default no
Table 5: A selection of the most useful rules learned
by C4.5rules along with their confidence levels.
Times for OVR and SIL are expressed in seconds.
Pk WD
feature-based 23.00% 25.47%
LCseg 31.91% 35.88%
U00 37.39% 40.43%
p-value 2.14e-04 3.30e-04
Table 6: Performance of the feature-based seg-
menter on the test data.
approaches to segmentation when applied to meet-
ings. Features based on form that are likely to in-
dicate topic shifts are automatically extracted from
speech. Content based features are computed by a
segmentation algorithm that utilizes a metric of lex-
ical cohesion and that performs as well as state-of-
the-art text-based segmentation techniques. It works
both with written and spoken texts. The text-based
segmentation approach alone, when applied to meet-
ings, outperforms all other segmenters, although the
difference is not statistically significant.
In future work, we would like to investigate the
effects of adding prosodic features, such as pitch
ranges, to our segmenter, as well as the effect of
using errorful speech recognition transcripts as op-
posed to manually transcribed utterances.
An implementation of our lexical cohesion seg-
menter is freely available for educational or research
purposes.11
Acknowledgments
We are grateful to Julia Hirschberg, Dan Ellis, Eliz-
abeth Shriberg, and Mari Ostendorf for their helpful
advice. We thank our ICSI project partners for grant-
ing us access to the meeting corpus and for useful
discussions. This work was funded under the NSF
project Mapping Meetings (IIS-012196).
References
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning,
34(1?3):177?210.
F. Choi. 2000. Advances in domain independent linear
text segmentation. In Proc. of NAACL?00.
W. Cochran. 1950. The comparison of percentages in
matched samples. Biometrika, 37:256?266.
B. Grosz and J. Hirschberg. 1992. Some intonational
characteristics of discourse structure. In Proc. of
ICSLP-92, pages 429?432.
B. Grosz and C. Sidner. 1986. Attention, intentions and
the structure of discourse. Computational Linguistics,
12(3).
M. Hajime, H. Takeo, and O. Manabu. 1998. Text seg-
mentation with multiple surface linguistic cues. In
COLING-ACL, pages 881?885.
M. Hearst. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of the ACL.
J. Hirschberg and D. Litman. 1994. Empirical studies
on the disambiguation of cue phrases. Computational
Linguistics, 19(3):501?530.
J. Hirschberg and C. Nakatani. 1996. A prosodic anal-
ysis of discourse segments in direction-giving mono-
logues. In Proc. of the ACL.
J. Hirschberg and C. Nakatani. 1998. Acoustic indicators
of topic segmentation. In Proc. of ICSLP.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proc. of ICASSP-03, Hong Kong (to appear).
11http://www.cs.columbia.edu/?galley/research.html
M.-Y. Kan, J. Klavans, and K. McKeown. 1998. Linear
segmentation and segment significance. In Proc. 6th
Workshop on Very Large Corpora (WVLC-98).
H. Kozima. 1993. Text segmentation based on similarity
between words. In Proc. of the ACL.
S. Levinson. 1983. Pragmatics. Cambridge University
Press.
D. Litman and R. Passonneau. 1995. Combining multi-
ple knowledge sources for discourse segmentation. In
Proc. of the ACL.
J. Morris and G. Hirst. 1991. Lexcial cohesion computed
by thesaural relations as an indicator of the structure of
text. Computational Linguistics, 17:21?48.
C. Nakatani, J. Hirschberg, and B. Grosz. 1995. Dis-
course structure in spoken language: Studies on
speech corpora. In AAAI-95 Symposium on Empirical
Methods in Discourse Interpretation.
R. Passonneau and D. Litman. 1993. Intention-based
segmentation: Human reliability and correlation with
linguistic cues. In Proc. of the ACL.
R. Passonneau and D. Litman. 1997. Discourse seg-
mentation by human and automated means. Compu-
tational Linguistics, 23(1):103?139.
L. Pevzner and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmenta-
tion. Computational Linguistics, 28 (1):19?36.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Machine Learning. Morgan Kaufmann.
J. Reynar. 1994. An automatic method of finding topic
boundaries. In Proc. of the ACL.
J. Reynar. 1999. Statistical models for topic segmenta-
tion. In Proc. of the ACL.
G. Salton and C. Buckley. 1988. Term weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing and Management, 24(5):513?523.
G. Tu?r, D. Hakkani-Tu?r, A. Stolcke, and E. Shriberg.
2001. Integrating prosodic and lexical cues for auto-
matic topic segmentation. Computational Linguistics,
27(1):31?57.
M. Utiyama and H. Isahara. 2001. A statistical model
for domain-independent text segmentation. In Proc. of
the ACL.
J. Xu and B. Croft. 1998. Corpus-based stemming using
cooccurrence of word variants. ACM Transactions on
Information Systems, 16(1):61?81.
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 725?728,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Investigations into the Crandem Approach to Word Recognition
Rohit Prabhavalkar, Preethi Jyothi, William Hartmann, Jeremy Morris, and Eric Fosler-Lussier
Department of Computer Science and Engineering
The Ohio State University, Columbus, OH
{prabhava,jyothi,hartmanw,morrijer,fosler}@cse.ohio-state.edu
Abstract
We suggest improvements to a previously pro-
posed framework for integrating Conditional
Random Fields and Hidden Markov Models,
dubbed a Crandem system (2009). The pre-
vious authors? work suggested that local la-
bel posteriors derived from the CRF were too
low-entropy for use in word-level automatic
speech recognition. As an alternative to the
log posterior representation used in their sys-
tem, we explore frame-level representations
derived from the CRF feature functions. We
also describe a weight normalization transfor-
mation that leads to increased entropy of the
CRF posteriors. We report significant gains
over the previous Crandem system on the Wall
Street Journal word recognition task.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) have recently emerged as a promising
new paradigm in the domain of Automatic Speech
Recognition (ASR). Unlike Hidden Markov Mod-
els (HMMs), CRFs are direct discriminative models:
they predict the probability of a label sequence con-
ditioned on the input. As a result, CRFs can capture
long-range dependencies in the data and avoid the
need for restrictive independence assumptions. Vari-
ants of CRFs have been successfully used in phone
recognition tasks (Gunawardana et al, 2005; Morris
and Fosler-Lussier, 2008; Hifny and Renals, 2009).
While the improvements in the phone recognition
task are encouraging, recent efforts have been di-
rected towards extending the CRF paradigm to the
word recognition level (Zweig and Nguyen, 2009;
Morris and Fosler-Lussier, 2009). The Crandem
system (Morris and Fosler-Lussier, 2009) is one of
the promising approaches in this regard. The Cran-
dem system is directly inspired by the techniques
of the Tandem system (Hermansky et al, 2000),
where phone-label posterior estimates produced by
a Multi-Layer Perceptron (MLP) are transformed
into a suitable acoustic representation for a standard
HMM. In both systems, the frame-based log poste-
rior vector of P (phone|acoustics) over all phones is
decorrelated using the Karhunen-Loeve (KL) trans-
form; unlike MLPs, CRFs take into account the en-
tire label sequence when computing local posteriors.
However, posterior estimates from the CRF tend to
be overconfident compared to MLP posteriors (Mor-
ris and Fosler-Lussier, 2009).
In this paper, we analyze the interplay between
the various steps involved in the Crandem process.
Is the local posterior representation from the CRF
the best representation? Given that the CRF poste-
rior estimates can be overconfident, what transfor-
mations to the posteriors are appropriate?
In Section 2 we briefly describe CRFs and the
Crandem framework. We suggest techniques for im-
proving Crandem word recognition performance in
Section 3. Details of experiments and our results are
discussed in Sections 4 and 5 respectively. We con-
clude with a discussion of future work in Section 6.
2 CRFs and the Crandem System
Conditional random fields (Lafferty et al, 2001) ex-
press the probability of a label sequence Q condi-
tioned on the input data X as a log-linear sum of
725
weighted feature functions,
p(Q|X) =
exp
P
t
P
j ?jsj(qt, X) +
P
j ?jfj(qt?1, qt, X)
Z(X)
(1)
where sj(?) and fj(?) are known as state feature
functions and transition feature functions respec-
tively, and ?j and ?j are the associated weights.
Z(X) is a normalization term that ensures a valid
probability distribution. Given a set of labeled ex-
amples, the CRF is trained to maximize the con-
ditional log-likelihood of the training set. The
log-likelihood is concave over the entire parameter
space, and can be maximized using standard convex
optimization techniques (Lafferty et al, 2001; Sha
and Pereira, 2003). The local posterior probability
of a particular label can be computed via a forward-
backward style algorithm. Mathematically,
p(qt = q|X) =
?t(q|X)?t(q|X)
Z(X)
(2)
where ?t(q|X) and ?t(q|X) accumulate contribu-
tions associated with possible assignments of la-
bels before and after the current time-step t. The
Crandem system utilizes these local posterior val-
ues from the CRF analogously to the way in which
MLP-posteriors are treated in the Tandem frame-
work (Hermansky et al, 2000), by applying a log
transformation to the posteriors. These transformed
outputs are then decorrelated using a KL-transform
and then dimensionality-reduced to be used as a re-
placement for MFCCs in a HMM system. While
the MLP is usually reduced to 39 dimensions, the
standard CRF benefits from a higher dimensionality
reduction (to 19 dimensions). The decorrelated out-
puts are then used as an input representation for a
conventional HMM system.
3 Improving Crandem Recognition
Results
Morris and Fosler-Lussier (2009) indicate that the
local posterior outputs from the CRF model pro-
duces features that are more heavily skewed to the
dominant phone class than the MLP system, leading
to an increase in word recognition errors. In order
to correct for this, we perform a non-linear trans-
formation on the local CRF posterior representa-
tion before applying a KL-transform and subsequent
stages. Specifically, we normalize all of the weights
?j and ?j in Equation 1 by a fixed positive constant
n to obtain normalized weights ??j and ?
?
j . We note
that the probability of a label sequence computed us-
ing the transformed weights, p?(Q|X), is equivalent
to taking the nth-root of the CRF probability com-
puted using the unnormalized weights, with a new
normalization term Z ?(X)
p?(Q|X) =
p(Q|X)1/n
Z ?(X)
(3)
where, p(Q|X) is as defined in Equation 1. Also
observe that the monotonicity of the nth-root func-
tion ensures that if p(Q1|X) > p(Q2|X) then
p?(Q1|X) > p?(Q2|X). In other words, the rank
order of the n-best phone recognition results are not
impacted by this change. The transformation does,
however, increase the entropy between the domi-
nant class from the CRF and its competitors, since
p?(Q|X) < p(Q|X). As we shall discuss in Section
5, this transformation helps improve word recogni-
tion performance in the Crandem framework.
Our second set of experiments are based on the
following observation regarding the CRF posteriors.
As can be seen from Equation 2, the CRF posteri-
ors involve a global normalization over the entire ut-
terance as opposed to the local normalization of the
MLP posteriors in the output softmax layer. This
motivates the use of representations derived from
the CRF that are ?local? in some sense. We there-
fore propose two alternative representations that are
modeled along the lines of the linear outputs from an
MLP. The first uses the sum of the state feature func-
tions, to obtain a vector f state(X, t) for each time
step t and input utterance X of length |Q| dimen-
sions, where Q is the set of possible phone labels
f state(X, t) =
?
?
?
j
?jsj(q,X)
?
?
T
?q ? Q
(4)
where q is a particular phone label. Note that the
lack of an exponential term in this representation en-
sures that the representation is less ?spiky? than the
CRF posteriors. Additionally, the decoupling of the
representation from the transition feature functions
could potentially allow the system to represent rel-
726
ative ambiguity between multiple phones hypothe-
sized for a given frame.
The second ?local? representation that we experi-
mented with incorporates the CRF transition feature
functions as follows. For each utterance X we per-
form a Viterbi decoding of the most likely state se-
quence Qbest = argmaxQ{p(Q|X)} hypothesized
for the utterance X . We then augmented the state
feature representation with the sum of the transition
features corresponding to the phone label hypothe-
sized for the previous frame (qbestt?1) to obtain a vector
f trans(X, t) of length |Q|,
f trans(X, t) =
"
X
j
?jsj(q,X) +
X
j
?jfj(q
best
t?1 , q,X)
#T
(5)
As a final note, following (Morris and Fosler-
Lussier, 2009), our CRF systems are trained using
the linear outputs of MLPs as its state feature func-
tions and transition biases as the transition feature
functions. Hence, f state is a linear transformation of
the MLP linear outputs down to |Q| dimensions.1
Both f state and f trans can thus be viewed as an im-
plicit mapping performed by the CRF of the in-
put feature function dimensions down to |Q| dimen-
sions. Note that the CRF implicitly uses informa-
tion concerning the underlying phone labels unlike
dimensionality reduction using KL-transform.
4 Experimental Setup
To evaluate our proposed techniques, we carried
out word recognition experiments on the speaker-
independent portion of the Wall Street Journal 5K
closed vocabulary task (WSJ0). Since the corpus is
not phonetically transcribed, we first trained a stan-
dard HMM recognition system using PLP features
and produced phonetic transcriptions by force align-
ing the training data. These were used to train an
MLP phone classifier with a softmax output layer,
using a 9-frame window of PLPs with 4000 hidden
layer units to predict one of the 41 phone labels (in-
cluding silence and short pause). The linear outputs
of the MLP were used to train a baseline Tandem
system. We then trained a CRF using the MLP lin-
ear outputs as its state feature functions. We extract
1We note that our system uses an additional state bias feature
that has a fixed value of 1. However, since this is a constant
term, it has no role to play in the derived representation.
System Accuracy (%)
Crandem-baseline 89.4%
Tandem-baseline 91.8%
Crandem-NormMax 91.4%
Crandem-Norm5 92.1%
Crandem-state 91.7%
Crandem-trans 91.0%
Table 1: Word recognition results on the WSJ0 task
local posteriors as well as the two ?local? representa-
tions described in Section 3. These input represen-
tations were then normalized at the utterance level,
before applying a KL-transformation to decorrelate
them and reduce dimensionality to 39 dimensions.
Finally, each of these representations was used to
train a HMM system with intra-word triphones and
16 Gaussians per mixture using the Hidden Markov
Model Toolkit (Young et al, 2002).
5 Results
Results for each of the experiments described in
Section 4 are reported in Table 1 on the 330-
sentence standard 5K non-verbalized test set. The
Crandem-baseline represents the system of (Mor-
ris and Fosler-Lussier, 2009). Normalizing the
CRF weights of the system by either the weight
with largest absolute value (CRF-NormMax) or by
5 (tuned on the development set) leads to signif-
icant improvements (p ? 0.005) over the Cran-
dem baseline. Similarly, using either the state fea-
ture sum (Crandem-state) or the representation aug-
mented with the transition features (Crandem-trans)
leads to significant improvements (p ? 0.005) over
the Crandem baseline. Note that the performance of
these systems is comparable to the Tandem baseline.
To further analyze the results obtained using the
state feature sum representations and the Tandem
baseline, we compute the mean distance for each
phone HMM from every other phone HMM ob-
tained at the end of the GMM-HMM training phase.
The distance between two HMMs is computed as a
uniformly weighted sum of the average distances be-
tween the GMMs of a one-to-one alignment of states
corresponding to the two HMMs. GMM distances
are computed using a 0.5-weighted sum of inter-
dispersions normalized by self-dispersions (Wang et
727
Figure 1: Normalized mean distances for each of the phone models from every other phone model trained using the
Tandem MLP baseline and the state feature sum representation.
al., 2004). Distances between monomodal Gaus-
sian distributions were computed using the Bhat-
tacharyya distance measure. The phone HMM dis-
tances are normalized using the maximum phone
distance for each system. As can be seen in Figure
1, the mean distances obtained from the state feature
sum representation are consistently greater than the
corresponding distances in the Tandem-MLP sys-
tem, indicating larger separability of the phones in
the feature space. Similar trends were seen with the
transition feature sum representation.
6 Conclusions and Future Work
In this paper, we report significant improvements
over the Crandem baseline. The weight normaliza-
tion experiments confirmed the hypothesis that in-
creasing the entropy of the CRF posteriors leads to
better word-level recognition. Our experiments with
directly extracting frame-level representations from
the CRF reinforce this conclusion. Although our ex-
periments with the systems using the state feature
sum and transition feature augmented representation
did not lead to improvements over the Tandem base-
line, the increased separability of the phone models
trained using these representations is encouraging.
In the future, we intend to examine techniques by
which these representations could be used to further
improve word recognition results.
Acknowledgement: The authors gratefully ac-
knowledge support by NSF grants IIS-0643901 and
IIS-0905420 for this work.
References
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden conditional random fields for phone
classification. Interspeech.
H. Hermansky, D. Ellis, and S. Sharma. 2000. Tan-
dem connectionist feature stream extraction for con-
ventional hmm systems. ICASSP.
Y. Hifny and S. Renals. 2009. Speech recognition using
augmented conditional random fields. IEEE Trans-
actions on Audio, Speech, and Language Processing,
17(2):354?365.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. ICML.
J. Morris and E. Fosler-Lussier. 2008. Conditional ran-
dom fields for integrating local discriminative classi-
fiers. IEEE Transactions on Acoustics, Speech, and
Language Processing, 16(3):617?628.
J. Morris and E. Fosler-Lussier. 2009. Crandem: Con-
ditional random fields for word recognition. Inter-
speech.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. NAACL.
Xu Wang, Peng Xuan, and Wang Bingxi. 2004. A gmm-
based telephone channel classification for mandarin
speech recognition. ICSP.
S. Young, G. Evermann, T. Hain, D. Kershaw, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and
P. Woodland. 2002. The HTK Book. Cambridge Uni-
versity Press.
G. Zweig and P. Nguyen. 2009. A segmental crf ap-
proach to large vocabulary continuous speech recogni-
tion. ASRU.
728
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 548?552,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Ranking-based readability assessment for early primary children?s literature
Yi Ma, Eric Fosler-Lussier
Dept. of Computer Science & Engineering
The Ohio State University
Columbus, OH 43210, USA
may,fosler@cse.ohio-state.edu
Robert Lofthus
Xerox Corporation
Rochester, NY 14604, USA
Robert.Lofthus@xerox.com
Abstract
Determining the reading level of children?s lit-
erature is an important task for providing edu-
cators and parents with an appropriate reading
trajectory through a curriculum. Automating
this process has been a challenge addressed
before in the computational linguistics litera-
ture, with most studies attempting to predict
the particular grade level of a text. However,
guided reading levels developed by educators
operate at a more fine-grained level, with mul-
tiple levels corresponding to each grade. We
find that ranking performs much better than
classification at the fine-grained leveling task,
and that features derived from the visual lay-
out of a book are just as predictive as standard
text features of level; including both sets of
features, we find that we can predict the read-
ing level up to 83% of the time on a small cor-
pus of children?s books.
1 Introduction
Determining the reading level of a text has received
significant attention in the literature, dating back to
simple arithmetic metrics to assess the reading level
based on syllable counts (Flesch, 1948). In the com-
putational linguistics community, several projects
have attempted to determine the grade level of a text
(2nd/3rd/4th/etc). However, the education commu-
nity typically makes finer distinctions in reading lev-
els, with each grade being covered by multiple lev-
els. Moreover, there are multiple scales within the
educational community; for example 1st grade is ap-
proximately covered by levels 3?14 on the Reading
Recovery scale,1 or levels C to H in the Fountas and
Pinnell leveling system.2
For grade-level assessment, classification and
regression approaches have been very promising.
However, it is not clear that an increased number of
classes will allow classification techniques to suc-
ceed with a more fine-grained leveling system. Sim-
ilarly, regression techniques may have problems if
the reading levels are not linearly distributed. In this
work, we investigate a ranking approach to book lev-
eling, and apply this to a fine-grained leveling prob-
lem for Kindergarten through 2nd grade books. The
ranking approach also allows us to be more agnostic
to the particular leveling system: for the vast ma-
jority of pairs of books, different systems will rank
the levels of the books the same way, even if the
exact differences in levels are not the same. Since
most previous work uses classification techniques,
we compare against an SVM multi-class classifier
as well as an SVM regression approach.
What has not received much attention in recent
research is the visual layout of the page. Yet, if one
walks into a bookstore and rummages through the
children?s section, it is very easy to tell the reading
level of a book just by thumbing through the pages.
Visual clues such as the number of text lines per
page, or the area of text boxes relative to the illustra-
tions, or the font size, give instant information to the
reader about the reading level of the book. What is
not clear is if this information is sensitive enough to
deliver a fine-grained assessment of the book. While
1http://www.readingrecovery.org
2http://www.fountasandpinnellleveledbooks.com
548
publishers may have standard guidelines for content
providers on visual layout, these guidelines likely
differ from publisher to publisher and are not avail-
able for the general public. Moreover, in the digi-
tal age teachers are also content providers who do
not have access to these guidelines, so our proposed
ranking system would be very helpful as they cre-
ate reading materials such as worksheets, web pages,
etc.
2 Related Work
Due to the limitations of traditional approaches,
more advanced methods which use statistical lan-
guage processing techniques have been introduced
by recent work in this area (Collins-Thompson and
Callan, 2004; Schwarm and Ostendorf, 2005; Feng
et al, 2010). Collins-Thompson and Callan (2004)
used a smoothed unigram language model to pre-
dict the grade reading levels of web page documents
and short passages. Heilman et al (2007) com-
bined a language modeling approach with grammar-
based features to improve readability assessment for
first and second language texts. Schwarm/Petersen
and Ostendorf (2005; 2009) used a support vector
machine to combine surface features with language
models and parsed features. The datasets used in
these previous related works mostly consist of web
page documents and short passages, or articles from
educational newspapers. Since the datasets used are
text-intensive, many efforts have been made to in-
vestigate text properties at a higher linguistic level,
such as discourse analysis, language modeling, part-
of-speech and parsed-based features. However, to
the best of our knowledge, no prior work attempts to
rank scanned children?s books (in fine-grained read-
ing levels) directly by analyzing the visual layout of
the page.
3 Ranking Book Leveling Algorithm
Our proposed method can be regarded as a modi-
fied version of a standard ranking algorithm, where
we develop a leveling classification by first rank-
ing books, and then assigning the level based on
the ranking output. Given a set of leveled books,
the process to generate a prediction for a new target
book involves the following two steps.
In the first step, we extract features from each
book, and train an off-the-shelf ranking model to
minimize the pairwise error of books. During the
test phase (second step), we rank all of the leveled
training books as well as the new target (test) book
using the trained ranking model. The predicted read-
ing level of the target book then can be inferred from
the reading levels of neighboring leveled books in
the rank-ordered list of books (in our experiment, we
take into account a window of three books above and
below the target book with reading levels weighted
by distance). Intuitively, we can imagine a book-
shelf in which books are sorted by their reading lev-
els. The ranker?s prediction of the reading level of a
target book corresponds to inserting the target book
into the sorted bookshelf.
4 Data Preparation
4.1 Book Selection, Scanning and Markup
We have processed 36 children?s books which range
from reading level A to L (3 books each level). The
golden standard key reading levels of those books
are obtained from Fountas and Pinnell leveled book
list (Fountas and Pinnell, 1996) in which letter A in-
dicates the easiest books to read and letter L iden-
tifies more challenging books; this range covers
roughly Kindergarten through Second Grade. The
set of children?s books covers a large variety of gen-
res, series and publishers.
After seeking permission from the publishers,3
all of the books are scanned and OCRed (Optical
Character Recognized) to create PDF versions of
the book. In order to facilitate the feature extrac-
tion process, we manually annotate each book using
Adobe Acrobat markup drawing tools before con-
verting them into corresponding XML files. The
annotation process consists of two straightforward
steps: first, draw surrounding rectangles around the
location of text content; second, find where the pri-
mary illustration images are and mark them using
rectangle markups. Then the corresponding XML
can be generated directly from Adobe Acrobat with
one click on a customized menu item, which is im-
plemented by using Adobe Acrobat JavaScript API.
3This is perhaps the most time-consuming part of the pro-
cess.
549
# of partitions 1 2 3 4
?1 Accuracy %
SVM Ranker 72.2 69.4 80.6 83.3
SVM Classifier 47.2 61.1 55.6 63.9
SVM Regression 72.2 61.1 58.3 58.3
Flesch-Kincaid 30.6 30.6 30.6 19.4
Spache 27.8 13.9 13.9 11.1
Average leveling error ? standard deviation
SVM Ranker 1.00 ? 0.99 1.03 ? 0.91 0.94 ? 0.83 0.92 ? 0.73
SVM Classifier 2.00 ? 1.60 1.86 ? 1.69 1.78 ? 1.57 1.44 ? 1.23
SVM Regression 1.14 ? 1.13 1.25 ? 1.11 1.33 ? 1.22 1.36 ? 1.22
Flesch-Kincaid 3.03 ? 2.21 3.03 ? 2.29 3.08 ? 2.31 3.31 ? 2.28
Spache 4.06 ? 3.33 4.72 ? 3.27 4.83 ? 3.34 5.19 ? 3.21
Table 1: Per-book (averaged) results for ranking versus classification, reporting accuracy within one level and average
error for different numbers of partitions
4.2 Feature Design
4.2.1 Surface-level Features
We extract a number of purely text-based features
that have typically been used in the education litera-
ture (e.g., (Flesch, 1948)), including:
1. Number of words; 2. Number of letters per
word; 3. Number of sentences; 4. Average sentence
length; 5. Type-token ratio of the text content.
4.2.2 Visually-oriented Features
In this feature set, we include a number of features
that would not be available without looking at the
physical layout of the page; with the annotated PDF
versions of the book we are able to extract:
1. Page count; 2. Number of words per page; 3.
Number of sentences per page; 4. Number of text
lines per page; 5. Number of words per text line;
6. Number of words per annotated text rectangle;
7. Number of text lines per annotated text rectan-
gle; 8. Average ratio of annotated text rectangle area
to page area; 9. Average ratio of annotated image
rectangle area to page area; 10. Average ratio of an-
notated text rectangle area to annotated image rect-
angle area; 11. Average font size.
The OCR process provides some of this informa-
tion automatically; while we have manually anno-
tated rectangles for this study one could theoreti-
cally use the OCR information and vision process-
ing techniques to extract rectangles automatically.
5 Experiments
5.1 Ranking vs. Classification/Regression
In this experiment, we look at whether treating book
leveling as a ranking problem is promising com-
pared to using classification/regression techniques.
Besides taking a whole book as input, we also exper-
iment with partitioning each book uniformly into 2,
3, or 4 parts, treating each sub-book as an indepen-
dent entity. We use a leave-n-out paradigm ? dur-
ing each iteration of the training (iterated through all
books), the system leaves out all n partitions corre-
sponding to one book and then tests on all partitions
corresponding to the held-out book. By averaging
the results for the partitions of the held-out book, we
can obtain its predicted reading level.
For ranking, we use the SVMrank ranker
(Joachims, 2006), which learns a (sparse) weight
vector that minimizes the number of swapped pairs
in the training set. The test book is inserted into the
ordering of the training books by the ranking algo-
rithm, and the level is assigned by averaging the lev-
els of the books above and below the order. To com-
pare the performance of our method with classifiers,
we use both SVMmulticlass classifier (Tsochantaridis
et al, 2004) and SVMlight (with regression learning
option) (Joachims, 1999) to determine the level of
the book directly. All systems are given the same
set of surface text-based and visual-based features
(Sections 4.2.1 and 4.2.2) as input.
550
# of partitions 1 2 3 4
?1 Accuracy %
All Features 72.2 69.4 80.6 83.3
Surface Features 61.1 63.9 58.3 61.1
Visual Features 72.2 72.2 72.2 83.3
Average leveling error ? standard deviation
All Features 1.00 ? 0.99 1.03 ? 0.91 0.94 ? 0.83 0.92 ? 0.73
Surface Features 1.42 ? 1.18 1.28 ? 1.00 1.44 ? 0.91 1.28 ? 1.11
Visual Features 1.03 ? 0.88 0.94 ? 0.86 1.03 ? 0.81 0.89 ? 0.82
Table 2: Per-book (averaged) results for all, surface-only, and visual-only features, reporting accuracy within one level
and average error for different numbers of partitions
We score the systems in two ways: first, we com-
pute the accuracy of the system by claiming it is cor-
rect if the book level is within ?1 of the true level.4
The second scoring method is the absolute error of
number of levels away from the true value, averaged
over all of the books.
As we can observe from Table 1, our ranking
system constantly beats the other two approaches
(the ranker is statistically significantly better than
the classifier at p < 0.05 level ? figures in bold).
One bit of interesting discovery is that SVM regres-
sion needs more data in order to have reliable results,
as the performance is downgraded when the number
of partitions goes up; the ranking approach benefits
from averaging the increasing number of partitions.5
All three methods have the same style of learner
(support vector learning), which suggests that the
performance gain is due to using a ranking crite-
rion in our method. Therefore we believe ranking
is likely a more effective and accurate method than
classification for this task.
One might also wonder how a traditional measure
of reading level (in this case, the Flesch-Kincaid
(Flesch, 1948) and Spache (Spache, 1953) Grade
Level) would hold up for this data. Flesch-Kincaid
and Spache predictions are linearly converted from
calculated grade levels to Fountas-Pinnell levels; all
of the systems utilizing our full feature set outper-
form these two baselines by a significant amount on
both ?1 accuracy and average leveling error.
4Note that this is still rather fine-grained as there are multi-
ple book levels per grade level.
5We only partition the books up to 4 sub-books because the
shortest book we have only contains 4 PDF pages (8 ?book?
pages) and further partitioning the book will lead to sparse data.
5.2 Visual vs. Surface Features
In order to evaluate the benefits of using visual cues
to assess reading levels, we repeat the experiments
using SVMrank based on our proposed ranking book
leveling algorithm with only the visual features or
only surface features.
Table 2 shows that the visual features surprisingly
outperform the surface features (statistically signif-
icant at p < 0.05 level ? figures in bold) and on
some partition levels, visual cues even beat the com-
bination of all features. We note, however, that for
early children?s books, pictures and textual layout
dominate the book content over text. Visual features
can be as useful as traditional surface text-based fea-
tures, but as one moves out of primary literature, we
suspect text features will likely be more effective for
leveling as content becomes more complex.
6 Conclusions
In this paper, we proposed a ranking-based book lev-
eling algorithm to assess reading level for children?s
literature. Our experimental results showed that the
ranking-based approach performs significantly bet-
ter than classification approaches as used in current
literature. The increased number of classes deterio-
rates the performance of classifiers in a fine-grained
leveling system. We also introduced visual features
into readability assessment and have seen consider-
able benefits of using visual cues. Since our target
data are children?s books that contain many illustra-
tions and pictures, it is quite reasonable to utilize vi-
sual content to help predict a more accurate reading
level. Future studies in early childhood readability
need to take visual content into account.
551
References
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT / NAACL 2004, volume 4, pages
193?200, Boston, USA.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A comparison of features for automatic read-
ability assessment. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 276?284, Beijing, China. As-
sociation for Computational Linguistics.
R. Flesch. 1948. A new readability yardstick. Journal of
applied psychology, 32(3):221?233.
I. Fountas and G. Pinnell. 1996. Guided Reading:
Good First Teaching for All Children. Heinemann,
Portsmouth, NH.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of NAACL
HLT, pages 460?467.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vec-
tor Learning, chapter 11, pages 169?184. MIT Press,
Cambridge, MA.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217?226. ACM.
S. Petersen and M. Ostendorf. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech & Language, 23(1):89?106.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 523?530. Association for Computational
Linguistics.
G. Spache. 1953. A new readability formula for primary-
grade reading materials. The Elementary School Jour-
nal, 53(7):410?413.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the twenty-first international conference on Machine
learning, page 104. ACM.
552
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 731?741,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploring Semi-Supervised Coreference Resolution of Medical Concepts
using Semantic and Temporal Features
Preethi Raghavan?, Eric Fosler-Lussier?, and Albert M. Lai?
?Department of Computer Science and Engineering
?Department of Biomedical Informatics
The Ohio State University, Columbus, Ohio, USA
{raghavap, fosler}@cse.ohio-state.edu, albert.lai@osumc.edu
Abstract
We investigate the task of medical concept
coreference resolution in clinical text using
two semi-supervised methods, co-training and
multi-view learning with posterior regulariza-
tion. By extracting semantic and temporal
features of medical concepts found in clinical
text, we create conditionally independent data
views; co-training MaxEnt classifiers on this
data works almost as well as supervised learn-
ing for the task of pairwise coreference resolu-
tion of medical concepts. We also train Max-
Ent models with expectation constraints, using
posterior regularization, and find that poste-
rior regularization performs comparably to or
slightly better than co-training. We describe
the process of semantic and temporal feature
extraction and demonstrate our methods on a
corpus of case reports from the New England
Journal of Medicine and a corpus of patient
narratives obtained from The Ohio State Uni-
versity Wexner Medical Center.
1 Introduction
The clinical community creates and uses a variety
of semi-structured and unstructured electronic tex-
tual documents that include medical reports such
as admission notes, progress notes, pathology re-
ports, radiology reports and hospital discharge sum-
maries. The documents, collectively termed clini-
cal narratives, account for various medical condi-
tions, procedures, diagnoses and assessments in a
patient?s medical history. Researchers have inves-
tigated ways in which clinical text can be automati-
cally processed for enabling access to relevant infor-
mation for physicians and health researchers (Embi
and Payne, 2009). One application is to support pa-
tient recruitment into clinical trials (research studies
that try to answer scientific questions to find bet-
ter ways to prevent, diagnose, or treat a disease)
by matching patient characteristics against eligibil-
ity criteria (Raghavan and Lai, 2010). While there
has been significant efforts to move to structured
data collection, clinical narratives remain a critical
data source for these tasks.
Extracting structured information from unstruc-
tured clinical text using natural language processing
(NLP) is complicated by the distinct clinical report-
ing sub-language characterized by incomplete sen-
tences and domain specific abbreviations (Friedman
et al, 2002). The large number of clinical narra-
tives generated per patient, over the years, along
with redundant information within and across narra-
tives, further adds to the complexity of using infor-
mation structured using NLP. There is a tendency to
copy and edit parts of an old clinical narrative when-
ever a new one is created, thus leading to redundant
information in clinical narratives of a patient. Fur-
thermore, since different types of clinical narratives
are created for different purposes, certain narratives
may summarize information from various other, at
times older, clinical narratives. All of this makes the
task of automatically processing unstructured clin-
ical narratives significantly difficult. However, the
ability to resolve medical concept coreferences helps
deal with redundant information within and across
clinical narratives and thus produce a unique list of
medical concepts in the patient?s clinical history.
We investigate the task of resolving references to
731
the same medical concept in the clinical narratives
of a patient using supervised and semi-supervised
methods. Our main contributions are as follows:
1. Since manual coreference annotation of patient
narratives is a slow and expensive process and pub-
licly available datasets are difficult to acquire, we
study the application of semi-supervised methods,
co-training and using expectation constraints with
posterior regularization, to medical concept coref-
erence resolution (MCCR).
2. We work with the hypothesis that if two medical
concepts have the same meaning and have occurred
at the same time, there is a very high probability that
they corefer. Based on this hypothesis, we explain
extraction of semantic and temporal feature sets that
are effectively used for MCCR.
3. We propose a method to associate medical con-
cepts with time durations centered around admission
and discharge dates of the patient using CRFs.
4. With the help of corpora created from the New
England Journal of Medicine (NEJM) and actual pa-
tient narratives obtained from the medical center, we
demonstrate that the semi-supervised methods per-
form comparably with supervised learning for pair-
wise MCCR using a MaxEnt classifier.
2 Related Work
Free-text reports form a significant portion of the
information content in a patient?s medical record.
There is great need for tools that can structure the
information in clinical text for use in various stud-
ies studies such as clinical trials, quality assess-
ment of healthcare delivery in institutions, and pub-
lic health research. Researchers have been investi-
gating ways in which clinical free-text can be struc-
tured to transform the information content in a clin-
ical narrative into a representation suitable for com-
putational analysis (Ananiadou et al, 2004). Medi-
cal NLP systems like Mayo?s cTakes (Savova et al,
2010), IBM?s MedKAT,1 and MedLEE (Chiang et
al., 2010), have components specifically trained or
designed for the clinical domain, to support tasks
such as named entity recognition. Previous at-
tempts at learning temporal relations between med-
ical events in clinical text include work by Jung et
1https://cabig-kc.nci.nih.gov/Vocab/KC/
index.php/OHNLP
al. (2011) and Zhou et al (2006). Gaizauskas et
al. (2006) learn the temporal relations before, after,
is included between events from a corpus of clinical
text much like the event-event relation tlink learn-
ing in Timebank (Pustejovsky et al, 2003). A com-
prehensive survey of temporal reasoning in medi-
cal data is provided by Zhou and Hripcsak (2007).
Chapman et al (2011) discuss barriers to NLP de-
velopment in the clinical domain.
Coreference resolution is a well-studied prob-
lem in computational linguistics (Ng, 2010; Raghu-
nathan et al, 2010). Supervised machine learn-
ing algorithms have been previously used for noun
phrase coreference resolution with fairly good re-
sults (Soon et al, 2001; Raghunathan et al, 2010).
Recently, the i2b2 challenge2 on coreference reso-
lution examined coreference resolution in clinical
data. The problem addressed in our paper is simi-
lar to the task described in the i2b2 challenge.3 Be-
sides the i2b2 challenge, there has not been signifi-
cant work in MCCR. This may be due to various pri-
vacy concerns and the efforts required to anonymize
and annotate massive amounts of patient narratives.
Zheng et al (2011) review heuristic-based, super-
vised and unsupervised methods for coreference res-
olution in the context of the clinical domain. He
(2007) studied coreference resolution in discharge
summaries, treating coreference resolution as a bi-
nary classification problem and investigated critical
features for coreference resolution for entities that
fall into five medical semantic categories commonly
appearing in discharge summaries. However, we fo-
cus on feature extraction to determine the similarity
between medical concepts, both in terms of meaning
and time of occurrence, for resolving coreferences
within and across all types of clinical narratives.
A disadvantage of supervised machine learning
approaches is the need for an unknown amount of
annotated training data for optimal performance.
Researchers then began to experiment with weakly
supervised machine learning algorithms such as co-
training (Blum and Mitchell, 1998). Muller et al
(2002) investigate the practical applicability of co-
training for the task of building a classifier for coref-
erence resolution and observed that the results were
2https://www.i2b2.org/NLP/Coreference/
3https://www.i2b2.org/NLP/Coreference/assets/
CoreferenceGuidelines.pdf
732
mostly negative for their dataset.
Ganchev et al (2010) propose a posterior regular-
ization framework for weakly supervised learning to
derive a multi-view learning algorithm. Multi-view
methods typically begin by assuming that each view
alone can yield a good predictor. Under this as-
sumption, we can regularize the models from each
view by constraining the amount by which we per-
mit them to disagree on unlabeled instances. In the
proposed approach, they train a model for each view,
and use constraints that the models should agree on
the label distribution.
We investigate the applicability of these two weakly
supervised methods to the task of MCCR using se-
mantic and temporal views. Savova et al (2011) dis-
cuss the creation of a corpus for coreference resolu-
tion in the clinical narrative. We annotate a corpus of
clinical narratives to tag medical concepts, temporal
relations, and coreference information. We use this
corpus as a gold standard to evaluate the proposed
approach to resolving coreferences between medical
concepts in clinical text.
To summarize, we study the problem of intra and
cross-narrative coreference resolution on longitudi-
nal patient data using relatedness between medical
concepts in terms of semantics and time. Further,
we importantly demonstrate that this task gives us
reasonable results even when modeled as a semi-
supervised problem. Creating annotated clinical cor-
pora is tedious, time consuming, and costly, as it
requires experts with medical domain knowledge.
Thus, the ability to train semi-supervised models
with limited labeled data for MCCR would be of
tremendous value.
3 Problem Description
Coreference resolution in clinical text refers to the
problem of identifying all medical concepts that re-
fer to the same medical concept. Medical con-
cepts are medical entities, events or states associ-
ated with the patient?s medical condition and health-
care. These include medical conditions, drugs ad-
ministered, diseases, procedures and lab tests as well
as normal health situations like pregnancy affecting
the patient?s health. The task of MCCR is similar to
noun phrase coreference resolution. However, med-
ical concepts are not restricted to noun phrases. For
instance, the actions cauterize and cauterization are
both considered medical concepts.
To make the task of identifying medical concepts
from clinical text more deterministic, any contigu-
ous group of words that have a direct or close match
in the Unified Medical Language System (UMLS)
Metathesaurus4 is considered a medical concept.
The UMLS includes a large Metathesaurus of con-
cepts and terms from many biomedical vocabular-
ies and a lexicon which contains syntactic, morpho-
logical, and orthographic information for biomedi-
cal and common words in the English language.
Problem Formulation. Consider a corpus of clini-
cal narratives, where multiple clinical narratives are
associated with each patient. If Pi, i ? {1, 2, ..., n}
where n is the number of patients in corpus, then
for each Pi, we have a set of associated clinical nar-
ratives. Each clinical narrative in turn has a set of
medical concepts. Thus, each Pi has a set of associ-
ated medical concepts, M = {M1,M2,M3, ..} that
occur within each clinical narrative as well as across
clinical narratives for that Pi. We study the problem
of MCCR of all medical concepts in M for each Pi.
4 Semantic and Temporal Features
We extract features based on semantic and tempo-
ral relatedness for each pair of medical concepts.
Semantic relatedness measures closeness between
medical concepts in terms of their meaning. This is
quantified by measuring distance between medical
events in the UMLS Metathesaurus graph structure
(Xiang et al, 2011). Temporal relatedness measures
the closeness between medical concepts in terms of
when they occurred. This is achieved by first, learn-
ing to assign every medical concept to a time-bin,
and then using the time-bin as a feature for learn-
ing to resolve coreferences. Extracting semantic and
temporal features helps identify conditionally inde-
pendent views of the data for co-training classifiers.
As previously noted by Nigam and Ghani (2000), it
is hard to identify conditionally independent views
for real-data problems. However, we believe there
are no natural dependencies between the semantic
and temporal feature sets. While semantic features
help identify synonymous medical concepts, that
alone may not guarantee coreference. Medical con-
4https://uts.nlm.nih.gov/home.html
733
Clinical 
Text 
Semantic Feature 
Extraction 
Temporal Feature 
Extraction using 
CRFs 
Co-train 
Posterior 
Regularization 
Coreference 
decisions 
Section 4 Section 5 
Medical Concept 
Coreference Resolution 
(MCCR) 
OR 
Figure 1: MCCR pipeline: Extract semantic and tempo-
ral features from clinical text to train MaxEnt classifiers
for medical concept coreference resolution using 1) Co-
training or 2) Posterior Regularization
cepts that are similar in meaning, but dissimilar in
terms of their time of occurrence, most probably do
not corefer. Similarly, medical concepts that occur
during the same time duration but are dissimilar in
terms of meaning, most probably do not corefer.
Semantic Relatedness. We leverage the UMLS
to derive a semantic relatedness score between med-
ical concepts. The UMLS codifies concepts found
in various medical vocabularies (e.g., ICD5 and
SNOMED-CT6) and includes relationships between
various concepts. The medical concepts and their
relationships are modeled in a graph structure. We
use the k-Neighborhood decentralization method
(kDLS) (Xiang et al, 2011) to index and transi-
tively traverse associated relations between concept
unique identifiers (CUIs) in the UMLS graph. The
UMLS uses semantic relations to mark the avail-
able links between two concepts. Around 2,404,937
CUIs and 15,333,246 links between them are seen in
the full UMLS graph structure. The kDLS method
is shown to outperform both breadth-first and depth-
first search in terms of speed and various other
measures in finding important information, such as
reachability, distance, and a summary of paths, be-
tween two concepts in the UMLS graph structure.
The relation between two concepts Mj (denoted by
x) and Mk (denoted by y) is measured as follows.
R(x, y) =
?
p?D(x,y)
1
?length(p)?1
+
?
q?D(y,x)
1
?length(q)?1
where D(x, y) is the set of paths from x to y and
D(y, x) is the set of paths from y to x obtained us-
5http://www.cdc.gov/nchs/icd.htm
6http://www.ihtsdo.org/snomed-ct/
ing the kDLS method, excluding paths with length
equal to 1. In order to make the measurement be-
tween a medical concepts unbiased against the avail-
able links in the UMLS that directly connect them,
the paths with length being 1 between them are not
counted. Each path?s contribution to the relation
score R(x, y) is determined by its length and ?. ? is
varied between 1 to 50; if ? is set to 1, then all paths
contribute equally to R irrespective of their lengths.
When ? increases, more weight will be placed on
the short paths as opposed to the long paths. Xiang
et al (2011) observe several fold enrichment values
when ? is varied between 5 and 15.
Besides traversing the UMLS graph structure us-
ing the kDLS method to obtain a similarity score
between medical concepts, we also measure similar-
ity between medical concepts by taking into account
the surrounding context. We do so by measuring
the KL-divergence between the sentences to which
the medical concepts belong. In order to avoid the
possibility of an empty set when calculating the in-
tersection of the probability distributions, we use a
smoothing method that makes the probability distri-
butions sum to 1 (Brigitte, 2003).
Another important semantic feature is the type of
relation between the medical concepts. This feature
is calculated by first computing the stemmed word
overlap between the medical concepts and deriving
features based on exact and partial matches between
the word stems of the medical concepts. If there is
no exact or partial match between the concepts, we
query the UMLS to check if the stem of one of the
medical concepts occurs in the UMLS definition or
atoms of the other medical event. An atom is the
smallest unit of naming within the UMLS. A med-
ical concept in UMLS represents a single meaning
and contains all atoms in the UMLS that express that
meaning in any way, whether formal or casual, ver-
bose or abbreviated. All of the atoms within a con-
cept are synonymous.
Besides the described features, we also include
the UMLS semantic category of each medical con-
cept and the WordNet7 similarity score between sen-
tences containing the medical concept.
Temporal Relatedness. Clinical text is fre-
quently characterized by temporal expressions co-
7http://wordnet.princeton.edu/
734
occurring with medical concepts (Zhou and Hripc-
sak, 2007). For instance, two days ago, fever started
4 days before rash, July 10th, 2010 etc. The abil-
ity to associate medical concepts with temporal ex-
pressions helps order medical concepts and deter-
mine potential temporal overlap between them. This
in turn could be a powerful discriminatory feature
in MCCR. Consider the medical concept chest pain
that occurs multiple times in a clinical narrative. If
these mentions of chest pain have occurred at the
same time, there is a possibility that they all refer to
the same instance of the medical concept chest pain.
Instead of relying on implicit temporal references
that may or may be evident from the clinical nar-
rative, we focus on temporal expressions that are
found in most clinical narratives. We do so by lever-
aging structural properties of clinical narratives such
as section information and explicit temporal infor-
mation such as admission and discharge dates, to
learn to assign medical concepts to time periods we
refer to as time-bins.
We now proceed to explain the process of assign-
ing medical concepts to time-bins using CRFs. Clin-
ical narratives are usually formatted with a struc-
tured header with information that includes the pa-
tient admission and discharge date. Clinical narra-
tives are also typically divided into sections. Sec-
tions represent a logical, and at times, temporal
grouping of information in the narrative. Sections
such as ?history of present illness,? ?physical ex-
amination,? ?review of systems,? ?impression,? and
?assessment plan? tend to occur in a certain order
within each clinical narrative. Thus, section tran-
sitions may indicate a temporal pattern for medical
concepts across those sections. For example, ?past
medical history? (before admission), followed by
?findings on admission? (on admission), followed
by ?physical examination? (after admission). Sec-
tions of certain types may also exhibit certain tem-
poral patterns. A ?history of present illness? sec-
tion may start with diseases and diagnoses 30 years
ago and then proceed to talk about them in the con-
text of a medical condition that happened few years
ago and finally describe the patient?s condition on
admission. Given the temporal patterns within sec-
tions and at section transitions, it works well to treat
the list of medical concepts from each clinical nar-
rative as a sequence (considering them in narrative
order) and learning to label them with a correspond-
ing time-bin. We define the following sequence of
time-bins centered around admission and discharge,
{way before admission, before admission, on admis-
sion, after admission, after discharge}.
We model the problem of assigning medical con-
cepts to time-bins as a sequence labeling task using
a CRF where we predict labels from the set {way be-
fore admission, before admission, on admission, af-
ter admission, after discharge} as a sequence Y pre-
dicted from the detected medical concepts X . CRFs
use two types of features in classification, state fea-
tures and transition features. State features con-
sider relating the label y (time-bin) of a single ver-
tex (medical concept) to features corresponding to a
medical concept x, and are given by,
S(x, y, i) =
?
j ?jsj(y, x, i)
Transition features consider the mutual depen-
dence of labels yi?1 and yi (dependence between the
time-bins of the current and previous medical event
in the sequence) and are given by,
T (x, y, i) =
?
k ?ktk(yi?1, yi, x, i)
Above, sj is a state feature function, and ?j is its
associated weight and tk is a transition function, and
?k is its associated weight. In contrast to the state
function, the transition function takes as input the
current label as well as the previous label, in addition
to the data.
Example state features include indicator features
based on verbs patterns in the same sentence as that
of the medical concept, last verb before the medical
concept, and type of clinical narrative. We also in-
clude position of medical event in the narrative as
well as within each section, the temporal expres-
sions and dates co-occurring with the medical con-
cept as features and the difference between these
dates and the admission date on each clinical nar-
rative. Example transition features include section
transitions based on the sections under which the
medical concept occurs, UMLS relatedness score
between the previous and current medical concept,
difference in verb patterns between the previous and
current medical concept, difference in dates (if any)
between the dates co-occurring with the previous
and current medical concept.
In order to enable feature extraction for this learn-
ing task, we use the following heuristic-based al-
735
gorithm to automatically identify sections and asso-
ciate medical concepts with them.
1. Extract lines that are all upper-case, and longer
than a word, from all narratives in corpus. They
mostly correspond to section titles.
2. Derive the stem of each word in the title using a
Porter stemming algorithm8 and sort stemmed
titles by frequency. If two or more words in
the title overlap, they are considered the same.
This gives us a candidate set of section titles.
3. When parsing a clinical narrative, and encoun-
tering a stemmed ngram matching a section ti-
tle from the frequent list, all subsequent sen-
tences are associated with that section until a
new section title is encountered. If an exact
match is not found, we allow partially match-
ing ngrams to be considered as section titles.
Along with the time-bin that are learned using the
process described above, dates and temporal expres-
sions extracted from the annotations in our corpus
are also used as temporal features. The list of fea-
tures extracted for the task of MCCR include the
following:
1. Verb pattern in the sentence in which the med-
ical concept occurs.
2. Last verb before the medical concept in the
same sentence.
3. Type of clinical narrative.
4. Section under which the medical concept is
mentioned.
5. Position of the medical concept.
6. Dates that fall in the same sentence as the med-
ical concept.
7. Difference between admission date and the date
in the same sentence as the clinical narrative.
8. The learned time-bin of each medical concept.
We also derive features based on the overlap-
ping in time-bins for the medical concept pair
and the nature of time-bin (past, present, fu-
ture).
9. Difference in verb patterns in the sentences of
the medical concept pair.
10. Difference in dates between the medical con-
cept pair.
8http://tartarus.org/martin/PorterStemmer/
11. UMLS relatedness score between the medical
concept pair and all the UMLS related and
other features described previously in the se-
mantic relatedness section.
When applying CRFs to the problem of assigning
medical concepts to time-bins, an observation se-
quence is medical concepts in the order in which
they appear in a clinical narrative, and the state se-
quence is the corresponding label sequence of time
bins. Thus, given a sequence of concepts in narrative
order {M1,M2,M3, ..}, we learn a corresponding
label sequence of time-bins {way before admission,
before admission, on admission, after admission, af-
ter discharge}. The learned label sequence is now
used as part of the temporal feature set in co-training
and posterior regularization for MCCR.
5 Weakly Supervised Learning
5.1 Co-training
We co-train two MaxEnt classifiers, one each on the
semantic features fs and temporal features ft of the
data, to classify pairs of medical concepts as core-
fer or no-corefer in a semi-supervised fashion. We
use the co-training algorithm proposed by Blum and
Mitchell (1998).
The assumption here is that each feature set contains
sufficient information to train a model for classifica-
tion of medical concepts. Consider the concept pair,
{renal inflammation, posterior uveitis} that core-
fer. The semantic view for this concept pair may
not strongly indicate coreference. The ?UMLS rela-
tion type? feature indicates that the two concepts are
not similar in meaning. However, both concepts are
mapped to the same time-bin after admission. Thus,
the time-bin along with features extracted based on
explicit temporal expressions co-occurring with the
medical concepts indicate a coreference between the
pair of medical concepts. Similarly, the semantic
view is confident about confident about the corefer-
ence of certain medical concept pairs which do not
occur in the same time-bin. The classifiers trained
on each view complement each other in the learn-
ing process. Thus, we can leverage the predictions
made by each classifier on the unlabeled dataset to
augment the training data of both classifiers.
The co-training algorithm is shown in Table 1. We
set a threshold for an unlabeled sample to be added
736
Function coTrain
Repeat till all unlabeled data is labeled.
1. Train classifier c1 on tf s to obtain model m1
2. Train classifier c2 on tf t to obtain model m2
3. Use m1 to classify a subset of unlabeled data
and update the training data as,
tf s.subset = {usubset1, predicted label}
iff classifier confidence > 1/number of labels
4. Use m2 to classify a subset of unlabeled data
and update the training data as,
tf t.subset = {usubset2, predicted label}
iff classifier confidence > 1/number of labels
5. tf s = tf s + tf t.subset +
{usubset1, predicted label}
6. tf t = tf t + tf s.subset +
{usubset2, predicted label}
Table 1: Co-training algorithm for the binary pairwise
classification task of MCCR (Blum and Mitchell, 1998).
c = classifier, u = unlabeled data.
usubset1, usubset2 = subsets of unlabeled data.
usubset1 and usubset2 are mutually exclusive.
F = {fs, ft} is the features space divided into condition-
ally independent semantic and temporal feature sets.
tf s = {fs,l} training data consisting of semantic features
of a medical concept pair along with class label.
tf t = {ft,l} training data consisting of temporal features
of a medical concept pair along with class label.
into the labeled pool. An unlabeled sample is la-
beled in a particular iteration, if classifier confidence
> 1/number of labels. In the next iteration, ran-
domly pick a subset of unlabeled samples and label
all samples in this subset. This could include sam-
ples that have already been labeled in previous iter-
ations. A label is assigned in a subsequent iteration
if: the sample was previously labeled OR if classi-
fier confidence > threshold. The parameters in this
algorithm are the number of iterations, the pool size
of examples selected from the unlabeled set in each
iteration and the number of labeled examples added
at each iteration to the labeled data pool. Similar to
Blum and Mitchell (1998), we update the pool size
by 2p+ 2n in each iteration, where p is the number
of medical pairs that corefer and n is the number of
medical concept pairs that do not corefer.
5.2 MaxEnt with Posterior Regularization
The next semi-supervised learning method applied
to MCCR is MaxEnt with posterior regularization
using expectation constraints (Ganchev et al, 2010).
This method incorporates prior knowledge directly
on the output variables during learning. The prior
knowledge is expressed as inequalities on the ex-
pected value under the posterior distribution of user-
defined constraint features. Thus, posterior regular-
ization incorporates side-information into unsuper-
vised estimation in the form of constraints on the
model?s posteriors. It is similar to the EM algorithm
during learning, but it solves a problem similar to
Maximum Entropy inside the E-Step to enforce the
constraints.
Posterior regularization is used to derive a multi-
view learning algorithm while specifying constraints
that the models should agree on the label distri-
bution. We train MaxEnt models based on two
views of the data, semantic and temporal. This
method starts by considering the setting of complete
agreement where there is a common desired out-
put for the two models and each of the two views
is sufficiently rich to predict labels accurately. The
search is restricted to model pairs p1, p2 that sat-
isfy p1(y|x) ? p2(y|x), where p1 and p2 each de-
fine a distribution over labels. The product dis-
tribution p1(y1)p2(y2) is considered and constraint
features are defined such that the proposal distri-
bution q(y1, y2) will have the same marginal for
y1 and y2. There is one constraint feature defined
for each label y given by, ?y(y1, y2) = ?(y1 =
y)?(y2 = y), where ?(.) is the 0-1 indicator func-
tion. The constraint set Q = q : Eq[?] = 0 re-
quires that the marginals over the two output vari-
ables are identical q(y1) = q(y2). An agreement
between two models is defined as agree(p1, p2) =
argmin KL(q(y1, y2)||p1(y1)p2(y2)) | Eq [?] = 0.
In the semantic feature set, we convert the follow-
ing feature (described in Section 4) into expectation
constraints. The type of relation between the pair
of medical concepts, is derived from matching the
word stems and querying the UMLS definition and
atoms of the medical concepts. Based on the relation
between the medical concepts (i.e., partial match,
complete match, UMLS definition match, UMLS
atom match, and no match), we indicate the prob-
ability of label distribution coref and no-coref. If
the relation turns out to be no match, there is a high
probability that the medical concepts do not corefer.
In the temporal feature set, we convert the features
based on time-bins of the medical concepts in the
pair into expectation constraints.
737
Class(time-bin) Precision Recall
after discharge 96.05 62.53
before admission 94.02 92.44
on admission 33.25 75.16
way before admission 50.42 66.72
after admission 93.62 99.14
Table 2: Sequence tagging of medical concepts with
time-bins using CRFs.
6 Experimental Setup
6.1 Corpus Annotation
Annotation of clinical text is a time consuming and
costly process. Many annotation efforts have used
physicians to annotate the data. Instead, we use an-
notators that are students or recently graduated stu-
dents from diverse clinical backgrounds with vary-
ing levels of clinical experience. In spite of this di-
versity, the annotation agreement across our team of
annotators is high; all annotators agreed on 89.5% of
the events and our overall inter-annotator Cohen?s
kappa statistic (Conger, 1980) for medical events
was 0.865. The annotators mark medical concepts,
coereference chains and temporal expressions in the
clinical narratives and the NEJM case reports. They
also map each medical concept to a UMLS CUI.
6.2 Feature Extraction
The first step involves extraction of semantic and
temporal features for the annotated medical con-
cepts, as described in Section 4 from both corpora.
The semantic relatedness scores are computed us-
ing the kDLS (Xiang et al, 2011) method to calcu-
late the relationship between concepts in the UMLS
with value of ? set to 7. The type of relation be-
tween medical concepts is derived by matching word
stems in each medical concept using the Lucene9
implementation of the Porter stemming algorithm.
We query the latest release (UMLS 2011AB) of the
UMLS Metathesaurus for finding a match between
medical concept and the UMLS definition or UMLS
atoms. The WordNet similarity score is computed
using Java API for WordNet Searching (JAWS).10
Explicit temporal expressions annotated in the
corpora are included in our temporal feature set.
Medical concepts in the NEJM are mostly de-
scribed temporally relative to the patient?s admis-
9http://lucene.apache.org/
10http://lyle.smu.edu/?tspell/jaws/
Class NEJM Clinical Narratives
Precision Recall Precision Recall
coref 79.24 94.53 74.81 88.33
no-coref 86.71 90.62 83.92 94.86
Table 3: Supervised learning for MCCR.
sion. Temporal expressions like ?2 years before ad-
mission? and ?3 weeks before admission? are com-
mon. Hence, we use a heuristic-based algorithm
to associate medical concepts with explicit tempo-
ral expressions in the NEJM corpus. The algo-
rithm parses case reports and identifies the tempo-
ral expressions anchored to admission. All medi-
cal concepts following such a temporal expression
are anchored to it until a new temporal expression
is encountered. Over 88% of the medical concept-
temporal expression associations done with the al-
gorithm above is accurate when compared against
the NEJM gold standard.
As described in Section 4, we apply sequence tag-
ging using a CRF to assign medical concepts in clin-
ical narratives to time-bins. We use the implementa-
tion of CRF in Mallet,11 trained by Limited-Memory
BFGS for our experiments. We use the Stanford
POS tagger12 to identify verbs and derive verb pat-
terns. The dataset for the task of assigning medi-
cal concepts to time-bins consisted of 1613 medical
concepts. We used a 60-40 train-test split to train a
CRF using a sequence of medical concepts and ob-
served an overall accuracy of 92%. The precision
and recall values for each time-bin class is indicated
in Table 2. The percentage of medical concepts that
fall under ?way before admission? and ?on admis-
sion? are less than 5%, affecting the learning accu-
racy of those classes. When modeled as a multi-
class classification task using MaxEnt, we achieve
around 86% accuracy.
7 MCCR Results and Discussion
We perform the following experiments for pairwise
MCCR: 1) Supervised learning with a MaxEnt clas-
sifier, using the combined semantic and temporal
feature set, 2) Co-training two MaxEnt models, 3)
Training MaxEnt models with using posterior regu-
larization.
11http://mallet.cs.umass.edu/
12http://nlp.stanford.edu/software/tagger.
shtml
738
Class NEJM Clinical Narratives
Co-train Precision Recall Precision Recall
coref 70.32 82.54 69.26 87.31
no-coref 82.54 84.85 71.15 89.44
PR Precision Recall Precision Recall
coref 76.63 90.41 74.81 84.25
no-coref 80.35 89.21 78.93 87.46
Table 4: Co-training and posterior regularization (PR) for
MCCR using semantic and temporal feature sets.
We use the MaxEnt classifier available in Mallet
for 1) and 2) and the the Mallet implementation of
MaxEnt models with posterior regularization for 3).
The NEJM corpus has 722 medical concepts,
12576 candidate pairs of medical concepts includ-
ing 137 pairs that corefer. We include all 12576
pairs in our experiments. The clinical narrative cor-
pus has 1613 medical concepts. The candidate pairs
and coreference chains for each patient is as follows.
Patient 1 has 241001 candidate pairs, 29 corefer-
ence chains. Patient 2 has 149604 candidate pairs,
9 coreference chains. Patient 3 has 6,446,521 can-
didate pairs, 20 coreference chains. From all the
candidate pairs in the clinical narrative corpus, 1025
pairs corefer. We randomly sample the no-coref in-
stances to restrict the corpus size to 1 million candi-
date pairs of medical concepts.
The results for all 3 experiments for both corpora
is shown in Tables 3, 4. We also train-test a super-
vised MaxEnt classifier on a 60-40 split of the en-
tire corpus. This gives us a precision of 74.81% and
88.33% recall (coref) for the binary classification
task of pairwise MCCR in the clinical narratives cor-
pus. In the both the semi-supervised experiments,
we use an initial labeled pool size of 30 where 12
medical concept pairs that corefer (p) and 18 that do
not corefer (n). The growth size is each iteration of
co-training is 2p+2n. At each iteration, confidently
labeled examples are added to the training set from
the previous iteration. The co-training algorithm
is run until all unlabeled instances become labeled.
The parameters in the posterior regularization im-
plementation include the regularization penalty for
each step and the number of iterations. We use the
default values (maxIterations=100, pGaussianPrior-
Variance=0.1, qGaussianPriorVariance=1000) sug-
gested on the Mallet toolkit page (Bellare et al,
2009). Co-training two MaxEnt models based on
independent semantic and temporal views of the
data results in 69.26% precision and 87.31% recall
(coref), whereas training MaxEnt models with ex-
pectation constraints gives us 74.81% precision and
84.25% recall (coref), on the corpus of clinical nar-
ratives.
Posterior regularization does better than co-training
and the performance of both the semi-supervised
methods is comparable to if not as good as the super-
vised classifier trained on a 60-40 split of the corpus.
Thus, our results indicate that the use of semantic
and temporal features is effective for MCCR in clin-
ical text. It is clear from the co-training and poste-
rior regularization results that treating MCCR as a
semi-supervised problem works.
8 Conclusions
We investigated the task of MCCR in clinical text us-
ing supervised and semi-supervised learning meth-
ods. We create annotated corpora of clinical text
with case reports from the NEJM and narratives ob-
tained from The Ohio State University Wexner Med-
ical Center. We work with the hypothesis that de-
termining semantic and temporal similarity between
medical concepts helps resolve coreferences. In
order to test this hypothesis, we describe the pro-
cess of semantic and temporal feature extraction
from clinical text. We demonstrate the effective-
ness of the extracted features in a supervised binary
classification task for MCCR with MaxEnt classi-
fiers (using the combined feature set) as well as us-
ing semi-supervised methods of co-training MaxEnt
classifiers and training MaxEnt models using pos-
terior regularization (using two independent views
of the data - semantic view and temporal view).
Thus, we show that MCRR can be performed using
semi-supervised learning with semantic and tempo-
ral views of the data.
Acknowledgments
The project described was supported by the
National Center for Research Resources,
Grant UL1RR025755, KL2RR025754, and
TL1RR025753, and is now at the National
Center for Advancing Translational Sciences,
Grant 8KL2TR000112-05, 8UL1TR000090-05,
8TL1TR000091-05. The content is solely the re-
sponsibility of the authors and does not necessarily
represent the official views of the NIH.
739
References
Sophia Ananiadou, Carol Freidman, and Jun??chi Tsu-
jii. 2004. Introduction: named entity recognition in
biomedicine. J. of Biomedical Informatics, pages 393?
395.
Kedar Bellare, Gregory Druck, and Andrew McCallum.
2009. Alternating projections for learning with expec-
tation constraints. In Proceedings of the Twenty-Fifth
Conference on Uncertainty in Artificial Intelligence,
UAI ?09, pages 43?50.
Avrim Blum and Tom M. Mitchell. 1998. Combin-
ing labeled and unlabeled data with co-training. In
COLT?98, pages 92?100.
Bigi Brigitte. 2003. Using Kullback-Leibler distance for
text categorization. In Proceedings of the 25th Euro-
pean conference on IR research, ECIR?03, pages 305?
319.
Wendy W Chapman, Prakash M Nadkarni, Lynette
Hirschman, Guergana K Savova Leonard W D?Avolio,
and Ozlem Uzuner. 2011. Overcoming barriers to
NLP for clinical text: the role of shared tasks and the
need for additional creative solutions. In JAMIA.
Jung-Hsien Chiang, Jou-Wei Lin, and Chen-Wei Yang.
2010. Automated evaluation of electronic discharge
notes to assess quality of care for cardiovascular dis-
eases using Medical Language Extraction and Encod-
ing System (MedLEE). JAMIA, pages 245?252.
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. In Psychological Bulletin
Vol 88(2), pages 322?328.
Peter J Embi and Philip Payne. 2009. Clinical research
informatics: challenges, opportunities and definition
for an emerging domain. Journal of the American
Medical Informatics Association, 16(3):316?327.
Carol Friedman, Pauline Kra, and Andrey Rzhetsky.
2002. Two biomedical sublanguages: a description
based on the theories of Zellig Harris. Journal of
Biomedical Informatics, 35(4):222?235.
Rob Gaizauskas, Henk Harkema, Mark Hepple, and An-
drea Setzer. 2006. Task-oriented extraction of tem-
poral information: The case of clinical narratives.
In Proceedings of the Thirteenth International Sym-
posium on Temporal Representation and Reasoning,
TIME ?06, pages 188?195.
Kuzman Ganchev, Joo Graa, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, pages 2001?2049.
Tian Ye He. 2007. Coreference Resolution on Entities
and Events for Hospital Discharge Summaries. EECS,
Cambridge, MA, MIT. M.Eng.
Hyuckchul Jung, James Allen, Nate Blaylock, Will
de Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records: ini-
tial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011 Workshop,
BioNLP ?11, pages 146?154.
Christoph Muller, Stefan Rapp, and Michael Strube.
2002. Applying co-training to reference resolution. In
ACL, pages 352?359.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
ACL, pages 1396?1411.
Kamal Nigam and Rayid Ghani. 2000. Analyzing
the effectiveness and applicability of co-training. In
CIKM?00, pages 86?93.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. Timeml: Robust
specification of event and temporal expressions in text.
In New Directions in Question Answering?03, pages
28?34.
Preethi Raghavan and Albert M. Lai. 2010. Leveraging
natural language processing of clinical narratives for
phenotype modeling. In PIKM?10, pages 57?66.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, pages 492?
501, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Guergana K. Savova, James J. Masanz, Philip V.
Ogren, Jiaping Zheng, Sunghwan Sohn, Karin Kip-
per Schuler, and Christopher G. Chute. 2010. Mayo
clinical text analysis and knowledge extraction sys-
tem (cTAKES): architecture, component evaluation
and applications. JAMIA, pages 507?513.
Guergana K. Savova, Wendy Webber Chapman, Jiaping
Zheng, and Rebecca S. Crowley. 2011. Anaphoric
relations in the clinical narrative: corpus creation.
JAMIA, 18(4):459?465.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, pages 521?544.
Yang Xiang, Kewei Lu, Stephen L James, Tara B Bor-
lawsky, Kun Huang, and Philip R O Payne. 2011. k-
neighborhood decentralization: A comprehensive so-
lution to index the UMLS for scale knowledge discov-
ery. In Journal of Biomedical Informatics.
Jiaping Zheng, Wendy Webber Chapman, Rebecca S.
Crowley, and Guergana K. Savova. 2011. Coreference
resolution: A review of general methodologies and ap-
plications in the clinical domain. Journal of Biomedi-
cal Informatics, 44(6):1113?1122.
740
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data - a review with emphasis
on medical natural language processing. Journal of
Biomedical Informatics, pages 183?202.
Li Zhou, Genevieve B. Melton, Simon Parsons, and
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clinical
narrative. Journal of Biomedical Informatics, pages
424?439.
741
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 70?74,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning to Temporally Order Medical Events in Clinical Text
Preethi Raghavan?, Eric Fosler-Lussier?, and Albert M. Lai?
?Department of Computer Science and Engineering
?Department of Biomedical Informatics
The Ohio State University, Columbus, Ohio, USA
{raghavap, fosler}@cse.ohio-state.edu, albert.lai@osumc.edu
Abstract
We investigate the problem of ordering med-
ical events in unstructured clinical narratives
by learning to rank them based on their time
of occurrence. We represent each medical
event as a time duration, with a correspond-
ing start and stop, and learn to rank the
starts/stops based on their proximity to the ad-
mission date. Such a representation allows us
to learn all of Allen?s temporal relations be-
tween medical events. Interestingly, we ob-
serve that this methodology performs better
than a classification-based approach for this
domain, but worse on the relationships found
in the Timebank corpus. This finding has im-
portant implications for styles of data repre-
sentation and resources used for temporal re-
lation learning: clinical narratives may have
different language attributes corresponding to
temporal ordering relative to Timebank, im-
plying that the field may need to look at a
wider range of domains to fully understand the
nature of temporal ordering.
1 Introduction
There has been considerable research on learning
temporal relations between events in natural lan-
guage. Most learning problems try to classify event
pairs as related by one of Allen?s temporal rela-
tions (Allen, 1981) i.e., before, simultaneous, in-
cludes/during, overlaps, begins/starts, ends/finishes
and their inverses (Mani et al, 2006). The Timebank
corpus, widely used for temporal relation learning,
consists of newswire text annotated for events, tem-
poral expressions, and temporal relations between
events using TimeML (Pustejovsky et al, 2003). In
Timebank, the notion of an ?event? primarily con-
sists of verbs or phrases that denote change in state.
However, there may be a need to rethink how we
learn temporal relations between events in different
domains. Timebank, its features, and established
learning techniques like classification, may not work
optimally in many real-world problems where tem-
poral relation learning is of great importance.
We study the problem of learning temporal rela-
tions between medical events in clinical text. The
idea of a medical ?event? in clinical text is very dif-
ferent from events in Timebank. Medical events
are temporally-associated concepts in clinical text
that describe a medical condition affecting the pa-
tient?s health, or procedures performed on a patient.
Learning to temporally order events in clinical text
is fundamental to understanding patient narratives
and key to applications such as longitudinal studies,
question answering, document summarization and
information retrieval with temporal constraints. We
propose learning temporal relations between medi-
cal events found in clinical narratives by learning to
rank them. This is achieved by representing medical
events as time durations with starts and stops and
ranking them based on their proximity to the admis-
sion date.1 This implicitly allows us to learn all of
Allen?s temporal relations between medical events.
In this paper, we establish the need to rethink
the methods and resources used in temporal re-
lation learning, as we demonstrate that the re-
sources widely used for learning temporal relations
in newswire text do not work on clinical text. When
we model the temporal ordering problem in clinical
text as a ranking problem, we empirically show that
it outperforms classification; we perform similar ex-
periments with Timebank and observe the opposite
conclusion (classification outperforms ranking).
1The admission date is the only explicit date always present
in each clinical narrative.
70
e1 before e2 e1 equals e2
e1.start e1.start; e2.start
e1.stop e1.stop; e2.stop
e2.start
e2.stop
e1 overlaps with e2 e1 starts e2
e1.start e1.start; e2.start
e2.start e1.stop
e1.stop e2.stop
e2.stop
e2 during e1 e2 finishes e1
e1.start e1.start
e2.start e2.start
e2.stop e1.stop; e2.stop
e1.stop
Table 1: Allen?s temporal relations between medical
events can be realized by ordering the starts and stops
2 Related Work
The Timebank corpus provides hand-tagged fea-
tures, including tense, aspect, modality, polarity and
event class. There have been significant efforts
in machine learning of temporal relations between
events using these features and a wide range of other
features extracted from the Timebank corpus (Mani
et al, 2006; Chambers et al, 2007; Lapata and Las-
carides, 2011). The SemEval/TempEval (Verhagen
et al, 2009) challenges have often focused on tem-
poral relation learning between different types of
events from Timebank. Zhou and Hripcsak (2007)
provide a comprehensive survey of temporal reason-
ing with clinical data. There has also been some
work in generating annotated corpora of clinical text
for temporal relation learning (Roberts et al, 2008;
Savova et al, 2009). However, none of these cor-
pora are freely available. Zhou et al (2006) propose
a Temporal Constraint Structure (TCS) for medical
events in discharge summaries. They use rule-based
methods to induce this structure.
We demonstrate the need to rethink resources,
features and methods of learning temporal relations
between events in different domains with the help of
experiments in learning temporal relations in clini-
cal text. Specifically, we observe that we get better
results in learning to rank chains of medical events
to derive temporal relations (and their inverses) than
learning a classifier for the same task.
The problem of learning to rank from examples
has gained significant interest in the machine learn-
ing community, with important similarities and dif-
ferences with the problems of regression and clas-
sification (Joachims et al, 2007). The joint cumu-
lative distribution of many variables arises in prob-
HISTORY PHYSICAL                                                                DATE: 09/01/2007  NAME: Smith Daniel T                                                          MR#: XXX-XX-XXXX  ATTENDING PHYSICIAN: John Payne MD                           DOB: 03/10/1940  HISTORY OF PRESENT ILLNESS  The patient is a 67-year-old Caucasian male with a history of paresis secondary to back  injury who is bedridden status post colostomy and PEG tube who was brought by EMS with  a history of fever. The patient gives a history of fever on and off associated with chills for the last 1 month. He does give a history of decubitus ulcer on the back but his main  complaint is fever associated with epigastric discomfort.  PAST MEDICAL HISTORY  Significant for polymicrobial infection in the blood as well as in the urine in July 2007 history  of back injury with paraparesis. He is status post PEG tube and colostomy tube.  REVIEW OF SYSTEMS  Positive for decubitus ulcer. No cough. There is fever. No shortness of breath.  PHYSICAL EXAMINATION  On physical exam the patient is a debilitated malnourished gentleman in mild distress.  Abdomen showed PEG tube with discharging pus and there are multiple scars one in the  midline. It had a healing wound. Bowel sounds were present. Extremities revealed pain and  atrophied muscles in the lower extremities with decubitus ulcer which had a transparent  bandage in the decubitus area which was stage 2-3. CNS - The patient is alert and awake x3.  There was good power in both upper extremities. Cranial nerves II-XII grossly intact.  
Figure 1: Excerpt from a sanitized clinical narrative (history &
physical report) with medical events underlined.
lems of learning to rank objects in information re-
trieval and various other domains. To the best of our
understanding, there have been no previous attempts
to learn temporal relations between events using a
ranking approach.
3 Representation of Medical Events (MEs)
Clinical narratives contain unstructured text describ-
ing various MEs including conditions, diagnoses
and tests in the history of a patient, along with
some information on when they occurred. Much of
the temporal information in clinical text is implicit
and embedded in relative temporal relations between
MEs. A sample excerpt from a note is shown in
Figure 1. MEs are temporally related both qualita-
tively (e.g., paresis before colostomy) and quantita-
tively (e.g. chills 1 month before admission). Rela-
tive time may be more prevalent than absolute time
(e.g., last 1 month, post colostomy rather than on
July 2007). Temporal expressions may also be fuzzy
where history may refer to an event 1 year ago or 3
months ago. The relationship between MEs and time
is complicated. MEs could be recurring or continu-
ous vs. discrete date or time, such as fever vs. blood
in urine. Some are long lasting vs. short-lived, such
as cancer, leukemia vs. palpitations.
We represent MEs of any type of in terms of their
time duration. The idea of time duration based rep-
resentation for MEs is in the same spirit as TCS
(Zhou et al, 2006). We break every ME me into
me.start and me.stop. Given the ranking of all starts
and stops, we can now compose every one of Allen?s
temporal relations (Allen, 1981). If it is clear from
context that only the start or stop of a ME can be de-
termined, then only that is considered. For instance,
?history of paresis secondary to back injury who is
bedridden status post colostomy? indicates the start
of paresis is in the past history of the patient prior
71
to colostomy. We only know about paresis.start rel-
ative to other MEs and may not be able determine
paresis.stop. For recurring and continuous events
like chills and fever, if the time period of recurrence
is continuous (last 1 month), we consider it to be
the time duration of the event. If not continuous, we
consider separate instances of the ME. For MEs that
are associated with a fixed date or time, the start and
stop are assumed to be the same (e.g., polymicrobial
infection in the blood as well as in the urine in July
2007). In case of negated events like no cough, we
consider cough as the ME with a negative polarity.
Its start and stop time are assumed to be the same.
Polarity allows us to identify events that actually oc-
curred in the patient?s history.
4 Ranking Model and Experiments
Given a patient with multiple clinical narratives, our
objective is to induce a partial temporal ordering of
all medical events in each clinical narrative based on
their proximity to a reference date (admission).
The training data consists of medical event (ME)
chains, where each chain consists of an instance of
the start or stop of a ME belonging to the same clin-
ical narrative along with a rank. The assumption is
that the MEs in the same narrative are more or less
semantically related by virtue of narrative discourse
structure and are hence considered part of the same
ME chain. The rank assigned to an instance indi-
cates the temporal order of the event instance in the
chain. Multiple MEs could occupy the same rank.
Based on the rank of the starts and stops of event
instances relative to other event instances, the tem-
poral relations between them can be derived as indi-
cated in Table 1. Our corpus for ranking consisted
of 47 clinical narratives obtained from the medical
center and annotated with MEs, temporal expres-
sions, relations and event chains. The annotation
agreement across our team of annotators is high; all
annotators agreed on 89.5% of the events and our
overall inter-annotator Cohen?s kappa statistic (Con-
ger, 1980) for MEs was 0.865. Thus, we extracted
47 ME chains across 4 patients. The distribution of
MEs across event chains and chains across patients
(p) is as as follows. p1 had 5 chains with 68 MEs,
p2 had 9 chains with 90 MEs, p3 had 20 chains with
119 MEs and p4 had 13 chains with 82 MEs. The
distribution of chains across different types of clin-
ical narratives is shown in Figure 2. We construct
a vector of features, from the manually annotated
corpus, for each medical event instance. Although
0 
2 
4 
6 
8  
10 
12 
14 
Radiology Discharge Summaries Pathology History & Physical  
p1 p2 p3 p4 
Figure 2: Distribution of the 47 medical event chains derived
from discharge summaries, history and physical reports, pathol-
ogy and radiology notes across the 4 patients.
there is no real query in our set up, the admission
date for each chain can be thought of as the query
?date? and the MEs are ordered based on how close
or far they are from each other and the admission
date. The features extracted for each ME include
the the type of clinical narrative, section informa-
tion, ME polarity, position of the medical concept
in the narrative and verb pattern. We extract tempo-
ral expressions linked to the ME like history, before
admission, past, during examination, on discharge,
after discharge, on admission. Temporal references
to specific times like next day, previously are re-
solved and included in the feature set. We also ex-
tract features from each temporal expression indicat-
ing its closeness to the admission date. Differences
between each explicit date in the narrative is also
extracted. The UMLS(Bodenreider, 2004) semantic
category of each medical concept is also included
based on the intuition that MEs of a certain semantic
group may occur closer to admission. We tried using
features like the tense of ME or the verb preceding
the ME (if any), POS tag in ranking. We found no
improvement in accuracy upon their inclusion.
In addition to the above features, we also anchor
each ME to a coarse time-bin and use that as a fea-
ture in ranking. We define the following sequence
of time-bins centered around admission, {way be-
fore admission, before admission, on admission, af-
ter admission, after discharge}. The time-bins are
learned using a linear-chain CRF,2 where the obser-
vation sequence is MEs in the order in which they
appear in a clinical narrative, and the state sequence
is the corresponding label sequence of time-bins.
We ran ranking experiments using SVM-rank
(Joachims, 2006), and based on the ranking score
assigned to each start/stop instance, we derive the
relative temporal order of MEs in a chain.3 This in
turn allows us to infer temporal relations between
2http://mallet.cs.umass.edu/sequences.php
3In evaluating simultaneous, ?0.05 difference in ranking
score of starts/stops of MEs is counted as a match.
72
Relation Clinical Text Timebank
Ranking Classifier Ranking Classifier
begins 81.21 73.34 52.63 58.82
ends 76.33 69.85 61.32 82.87
simulatenous 85.45 71.31 50.23 56.58
includes 83.67 74.20 59.56 60.65
before 88.3 77.14 61.34 70.38
Table 2: Per-class accuracy (%) for ranking, classification on
clinical text and Timebank. We merge class ibefore into before.
all MEs in a chain. The ranking error on the test set
is 28.2%. On introducing the time-bin feature, the
ranking error drops to 16.8%. The overall accuracy
of ranking MEs on including the time-bin feature
is 82.16%. Each learned relation is now compared
with the pairwise classification of temporal relations
between MEs. We train a SVM classifier (Joachims,
1999) with an RBF kernel for pairwise classification
of temporal relations. The average classification ac-
curacy for clinical text using the same feature set is
71.33%. We used Timebank (v1.1) for evaluation,
186 newswire documents with 3345 event pairs. We
traverse transitive relations between events in Time-
bank, increasing the number of event-event links
to 6750 and create chains of related events to be
ranked. Classification works better on Timebank, re-
sulting in an overall accuracy of 63.88%, but rank-
ing gives only 55.41% accuracy. All classification
and ranking results from 10-fold cross validation are
presented in Table 2.
5 Discussion
In ranking, the objective of learning is formalized
as minimizing the fraction of swapped pairs over all
rankings. This model is well suited to the features
that are available in clinical text. The assumption
that all MEs in a clinical narrative are temporally re-
lated allows us to totally order events within each
narrative. This works because a clinical narrative
usually has a single protagonist, the patient. This as-
sumption, along with the availability of a fixed refer-
ence date in each narrative, allows us to effectively
extract features that work in ranking MEs. How-
ever, this assumption does not hold in newswire text:
there tend to be multiple protagonists, and it may be
possible to totally order only events that are linked to
the same protagonist. Ranking implicitly allows us
to learn the transitive relations between MEs in the
chain. Ranking ME starts/ stops captures relations
like includes and begins much better than classifi-
cation, primarily because of the date difference and
time-bin difference features. However, the hand-
tagged features available in Timebank are not suited
for this kind of model. The features work well with
classification but are not sufficiently informative to
learn time durations using our proposed event repre-
sentation in a ranking model. Features like ?tense?
that are used for temporal relation learning in Time-
bank are not very useful in ME ordering. Tense
is a temporal linguistic quality expressing the time
at, or during which a state or action denoted by a
verb occurs. In most cases, MEs are not verbs (e.g.,
colostomy). Even if we consider verbs co-occurring
with MEs, they are not always accurately reflective
of the MEs? temporal nature. Moreover, in discharge
summaries, almost all MEs or co-occurring verbs
are in the past tense (before the discharge date). This
is complicated by the fact that the reference time/
ME with respect to which the tense of the verb is
expressed is not always clear. Based on the type of
clinical narrative, when it was generated, the refer-
ence date for the tense of the verb could be in the
patient?s history, admission, discharge, or an inter-
mediate date between admission and discharge. For
similar reasons, features like POS and aspect are not
very informative in ordering MEs. Moreover, fea-
tures like aspect require annotators with not only a
clinical background but also some expert knowledge
in linguistics, which is not feasible.
6 Conclusions
Representing and reasoning with temporal informa-
tion in unstructured text is crucial to the field of natu-
ral language processing and biomedical informatics.
We presented a study on learning to rank medical
events. Temporally ordering medical events allows
us to induce a partial order of medical events over
the patient?s history. We noted many differences be-
tween learning temporal relations in clinical text and
Timebank. The ranking experiments on clinical text
yield better performance than classification, whereas
the performance is the exact opposite in Timebank.
Based on experiments in two very different domains,
we demonstrate the need to rethink the resources and
methods for temporal relation learning.
Acknowledgments
The project was supported by the NCRR,
Grant UL1RR025755, KL2RR025754, and
TL1RR025753, is now at the NCATS, Grant
8KL2TR000112-05, 8UL1TR000090-05,
8TL1TR000091-05. The content is solely the
responsibility of the authors and does not necessar-
ily represent the official views of the NIH.
73
References
James F. Allen. 1981. An interval-based representation
of temporal knowledge. In IJCAI, pages 221?226.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic Acids Research, 32(suppl 1):D267?
D270.
Nathanael Chambers, Shan Wang, and Daniel Jurafsky.
2007. Classifying temporal relations between events.
In ACL.
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. In Psychological Bulletin
Vol 88(2), pages 322?328.
Thorsten Joachims, Hang Li, Tie-Yan Liu, and ChengX-
iang Zhai. 2007. Learning to rank for information
retrieval (lr4ir 2007). SIGIR Forum, 41(2):58?62.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher John C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods - Support Vector Learn-
ing, pages 169?184. MIT Press.
Thorsten Joachims. 2006. Training linear SVMs in linear
time. In KDD, pages 217?226.
Mirella Lapata and Alex Lascarides. 2011. Learn-
ing sentence-internal temporal relations. CoRR,
abs/1110.1394.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In ACL.
James Pustejovsky, Jos M. Castao, Robert Ingria, Roser
Sauri, Robert J. Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir R. Radev. 2003. TimeML: Ro-
bust specification of event and temporal expressions
in text. In New Directions in Question Answering?03,
pages 28?34.
A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,
Y. Guo, and A. Setzer. 2008. Semantic Annotation of
Clinical Text: The CLEF Corpus. In Proceedings of
the LREC 2008 Workshop on Building and Evaluating
Resources for Biomedical Text Mining, pages 19?26.
Guergana K. Savova, Steven Bethard, Will Styler, James
Martin, Martha Palmer, James Masanz, and Wayne
Ward. 2009. Towards temporal relation discovery
from the clinical narrative. AMIA.
Marc Verhagen, Robert J. Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The tempeval challenge: identifying
temporal relations in text. Language Resources and
Evaluation, 43(2):161?179.
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data - a review with emphasis
on medical natural language processing. Journal of
Biomedical Informatics, pages 183?202.
Li Zhou, Genevieve B. Melton, Simon Parsons, and
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clinical
narrative. Journal of Biomedical Informatics, pages
424?439.
74
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 998?1008,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Cross-narrative temporal ordering of medical events
Preethi Raghavan
?
, Eric Fosler-Lussier
?
, No
?
emie Elhadad
?
and Albert M. Lai
?
?
The Ohio State University, Columbus, Ohio
?
Columbia University, New York, NY
{raghavap, fosler}@cse.ohio-state.edu
noemie.elhadad@columbia.edu, albert.lai@osumc.edu
Abstract
Cross-narrative temporal ordering of med-
ical events is essential to the task of gen-
erating a comprehensive timeline over a
patient?s history. We address the prob-
lem of aligning multiple medical event se-
quences, corresponding to different clin-
ical narratives, comparing the following
approaches: (1) A novel weighted finite
state transducer representation of medi-
cal event sequences that enables compo-
sition and search for decoding, and (2)
Dynamic programming with iterative pair-
wise alignment of multiple sequences us-
ing global and local alignment algorithms.
The cross-narrative coreference and tem-
poral relation weights used in both these
approaches are learned from a corpus of
clinical narratives. We present results us-
ing both approaches and observe that the
finite state transducer approach performs
performs significantly better than the dy-
namic programming one by 6.8% for the
problem of multiple-sequence alignment.
1 Introduction
Discourse structure, logical flow of sentences, and
context play a large part in ordering medical events
based on temporal relations within a clinical nar-
rative. However, cross-narrative temporal rela-
tion ordering is a challenging task as it is dif-
ficult to learn temporal relations among medical
events which are not part of the logically coherent
discourse of a single narrative. Resolving cross-
narrative temporal relationships between medical
events is essential to the task of generating an
event timeline from across unstructured clinical
narratives such as admission notes, radiology re-
ports, history and physical reports and discharge
summaries. Such a timeline has multiple applica-
tions in clinical trial recruitment (Luo et al, 2011),
medical document summarization (Bramsen et al,
2006, Reichert et al, 2010) and clinical decision
making (Demner-Fushman et al, 2009).
Given multiple temporally ordered medical
event sequences generated from each clinical nar-
rative in a patient record, how can we combine
the events to create a timeline across all the nar-
ratives? The tendency to copy-paste text and
summarize past information in newly generated
clinical narratives leads to multiple mentions of
the same medical event across narratives (Cohen
et al, 2013). These cross-narrative coreferences
act as important anchors for reasoning with in-
formation across narratives. We leverage cross-
narrative coreference information along with con-
fident cross-narrative temporal relation predictions
and learn to align and temporally order medical
event sequences across longitudinal clinical nar-
ratives. We model the problem as a sequence
alignment task and propose solving this using two
approaches. First, we use weighted finite state
machines to represent medical events sequences,
thus enabling composition and search to obtain
the most probable combined sequence of medical
events. As a contrast, we adapt dynamic program-
ming algorithms (Needleman et al, 1970, Smith
and Waterman, 1981) used to produce global and
local alignments for aligning sequences of med-
ical events across narratives. We also compare
the proposed methods with an Integer Linear Pro-
gramming (ILP) based method for timeline con-
struction (Do et al, 2012). The cross-narrative
coreference and temporal relation scores used in
both these approaches are learned from a corpus
of patient narratives from The Ohio State Univer-
sity Wexner Medical Center.
The main contribution of this paper is a general
framework that allows aligning multiple event se-
quences using cascaded weighted finite state trans-
ducers (WFSTs) with the help of efficient compo-
sition and decoding. Moreover, we demonstrate
that this method can be used for more accurate
multiple sequence alignment when compared to
998
dynamic programming or other ILP-based meth-
ods proposed in literature.
2 Related Work
In the areas of summarization and text-to-text gen-
eration, there has been prior work on several order-
ing strategies to order pieces of information ex-
tracted from different input documents (Barzilay
et al, 2002, Lapata, 2003, Bollegala et al, 2010).
In this paper, we focus on temporal ordering of in-
formation, as discussed next.
Recent state-of-the art research has focused on
the problem of temporal relation learning within
the same document, and in many cases within the
same sentence (Mani et al, 2006, Verhagen et al,
2009, Lapata and Lascarides, 2011). Chambers
and Jurafsky (2009) describe a process to induce
a partially ordered set of events related by a com-
mon protagonist by using an unsupervised distri-
butional method to learn relations between events
sharing coreferring arguments, followed by tem-
poral classification to induce partial order. The
task was carried out on the Timebank newswire
corpus, but was limited to an intra-document set-
ting. More recently, (Do et al, 2012) proposed
an ILP-based method to combine the outputs of
an event-interval and an event-event classifier for
timeline construction on the ACE 2005 corpus.
However, this approach is also restricted to events
within documents and requires annotations for
event intervals. We empirically compare our meth-
ods for timeline creation from longitudinal clinical
narratives to such an ILP-based approach in Sec-
tion 7. While a lot of this work has been done in
the news domain, there is also some recent work
in rule-based algorithms (Zhou et al, 2006) and
machine learning (Roberts et al, 2008) applied
to temporal relations between medical events in
clinical text. Clinical narratives are written in a
distinct sub-language with domain specific termi-
nology and temporal characteristics, making them
markedly different from newswire text.
There is limited prior work in learning re-
lations across documents. Ji and Grishman
(2008) extended the one sense per discourse idea
(Yarowsky, 1995) to multiple topically related
documents and propagate consistent event argu-
ments across sentences and documents. Barzi-
lay and McKeown (2005) propose a text-to-text
generation technique for synthesizing common in-
formation across documents using sentence fu-
sion. This involves multisequence dependency
tree alignment to identify phrases conveying sim-
ilar information and statistical generation to com-
bine common phrases into a sentence. Along with
syntactic features, they combine knowledge from
resources like WordNet to find similar sentences.
In case of clinical narratives and medical event
alignment, the objective is to identify a unique se-
quence of temporally ordered medical events from
across longitudinal clinical data.
To the best of our knowledge, there is no
prior work on cross-document alignment of event
sequences. Multiple sequence alignment is a
problem that arises in a variety of domains in-
cluding gene/protein alignments in bioinformat-
ics (Notredame, 2002), word alignments in ma-
chine translation (Kumar and Byrne, 2003), and
sentence alignments for summarization (Lacatusu
et al, 2004). Dynamic programming algorithms
have been popularly leveraged to produce pair-
wise and global genetic alignments, where edit
distance based metrics are used to compute the
cost of insertions, deletions and substitutions.
We use dynamic programming to compute the
best alignment, given the temporal and corefer-
ence information between medical events across
these sequences. More importantly, we propose
a cascaded WFST-based framework for cross-
document temporal ordering of medical event se-
quences. Composition and search operations can
be used to build a single transducer that inte-
grates these components, directly mapping from
input states to desired outputs, and obtain the best
alignment (Mohri et al, 2000). In natural lan-
guage processing, WFSTs have seen varied appli-
cations in machine translation (Kumar and Byrne,
2003), morphology (Sproat, 2006), named en-
tity recognition (Krstev et al, 2011) and biolog-
ical sequence alignment / generation (Whelan et
al., 2010) among others. We demonstrate that
the WFST-based approach outperforms popularly
used dynamic programming algorithms for multi-
ple sequence alignment.
3 Problem Description
Medical events are temporally-associated con-
cepts in clinical text that describe a medical con-
dition affecting the patient?s health, or procedures
performed on a patient. We represent medical
events by splitting each event into a start and a
stop. When there is insufficient information to dis-
cern the start or stop of an event, it is represented
as a single concept. If only the start is known then
the stop is set to +?, whereas when only the stop
is known , the start is set to the date of birth of the
999
e1 before e2 
e1 overlaps e2 
e1 during e2 
e1 starts with e2 
e1 finishes with e2 
e1 equals e2 
< e1.start e1.stop e2.start e2.stop 
e1.start e1.stop e2.start e2.stop 
e1.start e1.stop e2.start e2.stop 
e1.start e2.start e2.stop e1.stop 
e1.start e1.stop e2.stop e2.start 
e1.start e2.start e2.stop e1.stop 
< < 
< < < 
< < < 
< < ~ 
~ < < 
~ ~ < 
< denotes before   > denotes after ~ denotes simultaneous 
Temporal Relation Event Ordering 
Figure 1: Medical event start / stop representa-
tion mapped to Allen?s temporal relations (Allen,
1981). Temporal ordering of event starts and stops
using {before, after, simultaenous} (shown on the
right) allows us learn temporal relations between
the medical events (shown on the left). e1
start
=
e2
start
and e1
stop
= e2
stop
, when e1 and e2 core-
fer.
patient.
1
Often, for chronic ailments like hyper-
tension, we would only associate a start with the
medical event and set the stop to +?. The start of
hypertension may be associated with the temporal
expression history of in the narrative. This, when
considered along with the admission date, allows
us to relatively order hypertension with respect to
other medical events. A medical event occurrence
like chest pain may be associated with a start and
a stop, where the start may be determined by the
mention of ?patient was complaining of chest pain
yesterday? in the narrative text. Further, the nar-
rative may state that ?he continued to have chest
pain on admission, but currently he is chest pain
free?; this may be used to infer the relative stop of
chest pain. Medical events may also be instan-
taneous, for e.g., injected with antibiotic. Such
events are represented with the start and stop as
being the same. Temporal relations exist between
the start and stop of events as shown in Figure 1.
Learning temporal relations before, after and si-
multaneous between the medical event starts and
stops corresponds to learning all of Allen?s tem-
poral relations (Allen, 1981) between the medical
events. Following our previous work (Raghavan
et al, 2012c), such a representation allows us to
temporally order the event starts and stops within
each clinical narrative by learning to rank them in
relative order of time. The problem definition is as
follows:
1
Patient date of birth, admission/ discharge date are usu-
ally available in the metadata associated with a clinical nar-
rative.
dob +? 
dob 
+? 
hypertensionstart admission1 chest painstart chest painstop 
palpitationsstart myocardial infarctionstart  
MRSAstart admission2 
hypertensionstart 
heart attackstart 
dob +? cocaine usestart infectionstart  woundsstart admission3 
N1 
N2 
N3 
episodestart 
Figure 2: Given temporally ordered medical event
sequences, N
1
, N
2
, N
3
, we address the task of
combining events across these sequences by merg-
ing or ordering them to create a single comprehen-
sive timeline.
Input: Sequences of temporally ordered med-
ical event starts and stops. This corresponds to
N
1
, N
2
, and N
3
in Figure 2. Each sequence cor-
responds to a clinical narrative. The total number
of sequences correspond to the number of clinical
narratives for a patient.
Problem: Combine medical events across these
sequences to generate a timeline i.e., a single com-
prehensive sequence of medical events over all
clinical narratives of the patient.
Expected Output: In the example shown
in Figure 2, the output would be as follows:
Timeline (N
1
, N
2
, N
3
)= {cocaine use
start
<
hypertension
start
= hypertension
start
< admis-
sion1 < chest pain
start
? palpitations
start
<
chest pain
stop
< heart attack
start
= myocardial
infarction
start
< admission2 < infection
start
<
MRSA
start
< admission3 < wounds
start
}.
The goal of multiple sequence alignment is to
find an alignment that maximizes some overall
alignment score. Thus, in order to align event se-
quences, we need to compute scores correspond-
ing to cross-narrative medical event coreference
resolution and cross-narrative temporal relations.
4 Cross-Narrative Coreference
Resolution and Temporal Relation
Learning
The first approach to learning a temporal order-
ing of medical events across all clinical narratives
is to consider all pairs of events across all narra-
tives and learn to classify them as sharing one of
Allen?s temporal relations (Allen, 1981) using a
single learning model. Alternatively, a ranking ap-
1000
proach, similar to the one used to generate intra-
narrative temporal ordering, can also be extended
to the cross-narrative case. However, the features
related to narrative structure and relative and im-
plicit temporal expressions used for temporal or-
dering within a clinical narrative may not be ap-
plicable across narratives. For instance, a history
and physical report may have sections like ?past
medical history?, ?history of present illness?, ?as-
sessment and plan?, and a certain logical pattern
to the flow of text within and across these sec-
tions. Further, temporal cues like ?thereafter?,
?subsequently?, follow from the context around an
event mention. The absence of such features in the
cross-narrative case does not allow such a model
to generate accurate temporal relation predictions.
Thus, for use in our sequence alignment models,
we learn two independent classifiers for medical
event coreference and temporal relation learning
across narratives. We train a classifier to resolve
cross-narrative coreferences by extracting seman-
tic and temporal relatedness feature sets for each
pair of medical concepts. Extracting these fea-
ture sets helps us train a classifier to predict med-
ical event coreferences (Raghavan et al, 2012a).
Another classifier is then trained to classify pairs
of medical event starts and stops across narratives
as sharing temporal relations {before, after, over-
laps}. The learned cross-narrative coreference
predictions can then be used along with confi-
dent temporal relation predictions to derive a joint
probability to enable cross-narrative temporal or-
dering.
5 Narrative Sequence Alignment for
Cross-narrative Temporal Ordering
Sequence alignment algorithms have been de-
veloped and popularly used in bioinformatics.
However, multiple sequence alignment (MSA)
has been shown to be NP complete (Wang and
Jiang, 1994) and various heuristic algorithms have
been proposed to solve this problem (Notredame,
2002). We propose a novel WFST-based repre-
sentation that enables accurate decoding for MSA
when compared to popularly used dynamic pro-
gramming algorithms (Needleman et al, 1970,
Smith and Waterman, 1981) or other state of the
art methods (Do et al, 2012).
In the problem of aligning events across mul-
tiple narrative sequences, we want to align tem-
porally ordered medical events corresponding to
clinical narratives of a patient. Unlike problems
in biological sequence alignment where the sym-
chest painstart 
episodestart 
chest painstop 
episodestop 
< 
< 
a = x                             
a b 
x  y < b = y                             Score = P(a simult x | a coref x) P(a coref x )  
Figure 3: Score computation for aligning events
across temporally ordered event sequences chest
pain
start
= episode
start
< chest pain
stop
=
episode
stop
, where events across the sequences oc-
cur simultaneously and corefer.
chest painstart 
palpitationsstart 
chest painstop 
palpitationsstop 
< 
< 
a b 
x  y a ~  x                             < b   <     y                             
Score = P( a simult x | a no - coref x ) ?                     P( x before b | a no - coref x ) ?                                  P( b before  y | a no - coref x ) P( a no- coref x )   
Figure 4: Score computation for aligning events
across temporally ordered event sequences chest
pain
start
? palpitations
stop
< chest pain
stop
<
palpitations
stop
, where some events across the se-
quences occur simultaneously but do not corefer.
bols to be aligned across sequences are restricted
to a fixed set, our symbol set is not fixed or cer-
tain because the symbols correspond to medical
events in clinical narratives. Moreover, we can-
not have fixed scores for symbol transformations
since our transformations correspond to corefer-
ence and temporal relations between the medical
events across sequences. The computation of these
scores is described next.
5.1 Scoring Scheme
Let us assume a, b are medical events in the first
clinical narrative and have been temporally or-
dered so a < b. Similarly, x, y are medical events
in the second clinical narrative such that x < y.
There exists a match or an alignment between a
pair of medical events, across the sequences, in the
following cases:
1. If the medical events are simultaneous and
coreferring, denoted as a = x.
2. If the medical events are simultaneous and
non-coreferring, denoted as a ? x.
1001
hypertensionstart palpitationsstart < a b 
x  y a    <     x                             < b   <        y                              Score = P(a before  x |a no - coref x) P(a no - coref x) ?  P(x before  b | x no - coref b) P(x no - coref b) ?  P(b before  y | b no - coref y) P(b no - coref y)  
infectionstart MRSAtart < < 
Figure 5: Score computation for aligning
events across temporally ordered event se-
quences hypertension
start
< palpitations
start
<
infection
start
< MRSA
start
, where events across
the sequences do not occur simultaneously and do
not corefer.
3. If the a medical event from one sequence
is before a medical event from another se-
quence, denoted as a < x.
4. If the a medical event from one sequence is
after a medical event from another sequence,
denoted as a > x.
We now illustrate how the scores for candidate
aligned sequences are computed using the learned
cross-narrative coreference and temporal probabil-
ities for the following three scenarios:
? The medical events across sequences are si-
multaneous and corefer as illustrated in Fig-
ure 3. The joint score considers the probabil-
ity of event temporal relations simultaneous
conditioned on coreference.
? Some medical events across sequences are si-
multaneous but do not corefer as illustrated in
Figure 4. Here, the joint score considers the
joint probability of temporal relations simul-
taneous or before and no-coreference.
? The medical events across sequences are not
simultaneous and do not corefer as illustrated
in Figure 5. In this case, the joint score con-
siders the probability of the temporal relation
before and no coreference.
Thus, the coreference and temporal relation scores
can be leveraged for aligning sequences of medical
events. These scores are used in both the WFST-
based representation and decoding, as well as for
dynamic programming.
5.2 Alignment using a Weighted Finite State
Representation
A weighted finite-state transducer (WFST) is an
automaton in which each transition between states
is associated with an input symbol, an output sym-
bol, and a weight (Mohri et al, 2005). WFSTs can
be used to efficiently represent and combine se-
quences of medical events based coreference and
temporal relation information. The WFST rep-
resentation gives us the ability to talk about the
global joint probability derived from coreference
and temporal relation scores described in Section
5.1. It allows us to build a weighted lattice of se-
quences that can be searched for the most probable
sequence of medical events from across all clin-
ical narratives of a patient. We use unweighted
FSAs to represent the input described in Section
3, i.e. temporally ordered sequences of medical
events corresponding to clinical narratives. This
corresponds to N
1
and N
2
in Figure 6.
Based on whether we want to align the se-
quences purely based on coreference scores or
both coreference and temporal relation scores, the
arc weights for the WFST can be determined. M
c
12
is a WFST that maps input symbols from N
1
to
output symbols inN
2
and is weighted by the prob-
ability of coreference or no-coreference between
medical events across N
1
and N
2
. The represen-
tation in WFST M
c+t
12
shown in Figure 7 allows
us to align N
1
and N
2
based on both coreference
as well as temporal relation probabilities. The
WFST has  transitions to accommodate insertion
and deletion of medical events when combining
the sequences. Deletions correspond to the case
when an event in the first sequence does not map
to any event in the second sequence; similarly in-
sertions correspond to the case where an event in
the second sequence does not map to any event in
the first sequence. The WFST composition opera-
tion allows the outputs of one WFST to be fed to
the inputs of a second WFST or FSA. Thus, we
build our final machine by composing the three
sub-machines as,
D = N
1
?M
i
12
?N
2
. (1)
where i = c or i = c + t. This gives us a com-
bined weighted graph by mapping the output sym-
bols of the first medical event sequence to the in-
put symbols of the second medical event sequence.
The scores on the decoding graph are derived from
only the coreference probabilities if i = c and both
coreference and temporal relation probabilities if
i = c+ t.
In the medical event sequence alignment prob-
lem, we want to align multiple sequences of medi-
cal events that correspond to multiple clinical nar-
ratives of a patient. Since we want to now combine
1002
N1 
N2 
M12 
Figure 6: N
1
and N
2
are medical event sequences represented using FSAs. M
c
12
maps medical events
across N
1
and N
2
and is weighted only by the probability of coreference between events across N
1
and
N
2
.
0
c
o
c
a
in
eu
se
:-
/0
.1
3
hy
pe
rt
en
si
on
:-
/0
.2
7
c
he
st
pa
in
:-
/0
.2
3
1
-
:c
o
c
a
in
ea
bu
se
/0
.1
c
o
c
a
in
eu
se
:c
oc
ai
ne
ab
us
e/
0.
9
hy
pe
rt
en
si
on
:c
oc
ai
ne
ab
us
e/
0.
4
c
he
st
pa
in
:c
oc
ai
ne
ab
us
e/
0.
3
c
o
c
a
in
eu
se
:-
/0
.1
3
hy
pe
rt
en
si
on
:-
/0
.2
4
c
he
st
pa
in
:-
/0
.2
3
2
-
:a
dm
is
si
on
/0
.1
c
o
c
a
in
eu
se
:a
dm
is
si
on
/0
.1
hy
pe
rt
en
si
on
:a
dm
is
si
on
/0
.1
c
he
st
pa
in
:a
dm
is
si
on
/0
.1
c
o
c
a
in
eu
se
:-
/0
.1
3
hy
pe
rt
en
si
on
:-
/0
.2
4
c
he
st
pa
in
:-
/0
.2
3
3
-
:c
he
st
pa
in
/0
.1
c
o
c
a
in
eu
se
:c
he
st
pa
in
/0
.1
7
hy
pe
rt
en
si
on
:c
he
st
pa
in
/0
.2
3
c
he
st
pa
in
:c
he
st
pa
in
/0
.8
6
c
o
c
a
in
eu
se
:-
/0
.1
3
hy
pe
rt
en
si
on
:-
/0
.2
4
c
he
st
pa
in
:-
/0
.2
3
4
/4
-
:m
y
o
ca
rd
ia
li
nf
ar
ct
io
n/
0.
1
c
o
c
a
in
eu
se
:m
yo
ca
rd
ia
li
nf
ar
ct
io
n/
0.
2
hy
pe
rt
en
si
on
:m
yo
ca
rd
ia
li
nf
ar
ct
io
n/
0.
1
c
he
st
pa
in
:m
yo
ca
rd
ia
li
nf
ar
ct
io
n/
0.
1
c
o
c
a
in
eu
se
:-
/0
.1
3
hy
pe
rt
en
si
on
:-
/0
.2
4
c
he
st
pa
in
:-
/0
.2
3
Figure 7: M
c+t
12
is a WFST representation used for mapping medical events between N
1
and N
2
(from
Figure 2) and is weighted by both the coreference and temporal relation probabilities
all narrative chains belonging to the same patient,
the composition cascade to build the final com-
bined sequence will be as,
D
f
= N
1
?M
i
12
?N
2
?M
i
23
?N
3
?M
i
34
...?N
n
(2)
where i = c or i = c + t and n is the number
of medical event sequences corresponding to clin-
ical narratives for a patient. During composition
we retain intermediate paths like M
i
23
utilizing the
ability to do lazy composition (Mohri and Pereira,
1998) in order to facilitate beam search through
the multi-alignment. The best hypothesis corre-
sponds to the highest scoring path which can be
obtained using shortest path algorithms like Djik-
stra?s algorithm. The best path corresponds to the
best alignment across all medical event sequences
based on the joint probability of cross-narrative
medical event coreferences and temporal relations
across the narrative sequences.
The complexity of decoding increases exponen-
tially with the number of narrative sequences in
the composition, and exact decoding becomes in-
feasible. One solution to this problem is to do
the alignment greedily pairwise, starting from the
most recent medical event sequences, finding the
best path, and iteratively moving on to the next
sequence, and proceeding until the oldest medi-
cal event sequence. The disadvantage of such a
method is that it does not take into account con-
straints between medical events across multiple
event sequences and may lead to a less accurate
solution.
An alternative method is to use lazy compo-
sition to perform more efficient composition as
it allows practical memory usage. We also use
beam search to make for an efficient approxima-
tion to the best-path computation (Mohri et al,
2005). This allows accommodating constraints
from across multiple sequences and generates a
more accurate best path. Thus, this method gener-
ates more accurate alignments when we have more
than two sequences to be aligned.
1003
For instance, instance say a, b ? N
1
, x, y ? N
2
,
and m,n ? N
3
are temporally medical event se-
quences corresponding to narratives N
1
, N
2
and
N
3
. Based on the learned pairwise temporal rela-
tions, if we have the following constraints a < x,
m > x, m < a. Aligning N
1
and N
2
greedily
pairwise may give us the best combined sequence
as a, x, b, y ? N
12
. Now in aligning N
12
with
N
3
, we won?t be able to accommodate m > x and
m < a. However, performing a beam search over
the composed WFST in equation 2 allows us to
accommodate such constraints across multiple se-
quences. The complexity of composing two trans-
ducers is O(V
1
V
2
D
1
(logD
2
+ M
2
)) where each
edge from the first sequence matches every edge in
the second sequence and V
i
is the number of states,
D
i
is the maximum out-degree and M
i
maximum
multiplicity for the i
th
FST (Mohri et al, 2005).
We also use popular dynamic programming al-
gorithms (Needleman et al, 1970, Smith and Wa-
terman, 1981) for sequence alignment of medi-
cal events across narratives and compare it to the
WFST-based representation and decoding.
5.3 Pairwise Alignment using Dynamic
Programming
As a contrast, we adapt two dynamic program-
ming algorithms for sequence alignment: global
alignment using the Needleman Wunsch algo-
rithm (NW) (Needleman et al, 1970) and local
alignment using the Smith-Waterman algorithm
(SW) (Smith and Waterman, 1981). NW allows
us to align all events in one sequence with all
events in another sequence. A drawback of NW
is that short and highly similar sequences maybe
missed because they get overweighted by the rest
of the sequence. NW is suitable when the two se-
quences are of similar length with significant de-
gree of similarity throughout. On the other hand,
SW gives the longest sub-sequence pair that yields
maximum degree of similarity between the two
original sequences. It does not force all events
in a sequence to align with another sequence.
SW is useful in aligning sequences that differ in
length and have short patches of similarity. The
time complexity of these methods for sequences
of length m and n are O(mn).
The scoring scheme described earlier is used to
update the scoring matrix for dynamic program-
ming. In order to accommodate the temporal re-
lations before and after, we insert a null symbol
after every medical event in each sequence in the
scoring matrix. A vertical or horizontal gap arises
when cases 1, 2, 3 and 4 in Section 5.1 mentioned
above are not true. If the medical events are not
simultaneous, not before or not after, the medical
events will not align. Thus, the value of each cell
in the scoring matrix is determined by computing
the maximum score at each position C(i, j) as,
max{(C(i?1, j?1)+S
ij
), (C(i, j?1)+w),
(C(i? 1, j) + w)} (3)
where, S
ij
= max{P (i = j), P (i < j), P (i >
j)}, and w = max{(1 ? P (i = j)), (1 ? P (i <
j)), (1 ? P (i > j))}. Here, C(i ? 1, j ? 1)
corresponds to a match, whereas C(i, j ? 1) and
C(i ? 1, j) correspond to a gaps in sequence one
and two.
In case of the SW algorithm, the negative scor-
ing matrix cells are set to zero, thus making the
positively scoring local alignments visible. Back-
tracking starts at the highest scoring matrix cell
and proceeds until a cell with score zero is encoun-
tered, yielding the highest scoring local alignment.
The time and space complexity grows exponen-
tially with the number of sequences to be aligned
and finding the global optimum has been shown to
be a NP-complete problem. The time complexity
of aligning N sequences of length L is O(2
N
L
N
)
(Wang and Jiang, 1994). Thus, for MSA using
dynamic programming, we use a heuristic method
where we combine pairwise alignments iteratively
starting with the latest narrative and progressing
towards the oldest narrative.
6 Experiments and Evaluation
Corpus Description. The corpus consists of a
dataset of clinical narratives obtained from the
[redacted] medical center. The corpus has a total
of 2060 patients, and 100704 clinical narratives.
We gathered a gold standard set of seven patients
(80 clinical narratives overall) with manual anno-
tation of all medical events mentioned in the nar-
ratives, coreferences, and medical event sequence
information. The annotation agreement across
annotators is high, with 89.5% agreement corre-
sponding to inter-annotator Cohen?s kappa statis-
tic of 0.86 (Raghavan et al, 2012b). The types
of clinical narratives included 27 discharge sum-
maries, 30 history and physical reports, 15 radiol-
ogy reports and 8 pathology reports. The distribu-
tion of the number of medical event sequences and
unique medical events across patients is shown in
Table 1. The annotated dataset is used to cross-
validate and train our coreference and temporal re-
lation learning models and to evaluate our cross-
narrative medical event timeline.
1004
p1 p2 p3 p4 p5 p6 p7
No. of Narrative Sequences 5 9 20 13 8 10 15
No. of Medical events 68 90 119 82 79 72 95
% Accuracy % Avg.
WFST-framework (lazy composition and beam search)[c+t] 76.1 73.2 81.2 83.5 76.4 82.5 79.7 78.9
WFST-framework (Iterative pairwise)[c+t] 70.4 67.1 73.5 74.1 61.8 75.5 62.9 69.3
Smith Waterman (Iterative pairwise)[c+t] 71.2 69.7 75.5 75.6 66.3 77.4 68.3 72.1
Needleman-Wunsch (Iterative pairwise)[c+t] 68.1 66.3 72.1 74.4 61.1 75.5 63.6 68.7
WFST-framework (lazy composition and beam search)[c] 68.5 65.3 72.3 74.4 67.2 71.3 69.1 69.7
WFST-framework (Iterative pairwise)[c] 61.2 63.3 61.9 60.4 59.8 64.8 60.5 61.7
Smith Waterman (Iterative pairwise)[c] 60.3 63.7 68.2 62.3 58.6 66.7 60.2 62.8
Needleman-Wunsch (Iterative pairwise)[c] 56.6 60.1 59.3 65.6 54.7 63.1 58.2 59.6
Table 1: The distribution of medical events across narrative sequences and sequences across patients and
multiple sequence alignment results for the WFST-based framework, and dynamic programming using
just coreference scores [c] and using coreference as well as temporal relation scores [c+t].
Evaluation Metric. For each patient and each
method (WFST or dynamic programming), the
output timeline to evaluate is the highest scoring
candidate hypothesis derived as described above.
Accuracy of the timeline is calculated as the num-
ber of transformations required to obtain the refer-
ence sequence in the annotated gold-standard from
the one generated by our system. Transformations
are measured in terms of the minimum edit dis-
tance, insertions, deletions, and substitutions of
medical events.
Experiments and Results. We first temporally
order medical events within each clinical narrative
by learning to rank them in relative order of oc-
curence as described in our previous work (Ragha-
van et al, 2012c). The overall accuracy of rank-
ing medical events using leave-one-out cross val-
idation is 82.1%. The resulting medical event se-
quences serve as the input to the problem of cross-
narrative sequence alignment.
The cross-narrative coreference and temporal
relation pairwise classification models described
in Section 4 are trained using a Maximum en-
tropy classifier. The coreference resolution per-
forms with 71.5% precision and 82.3% recall. The
temporal relation classifier performs with 60.2%
precision and 76.3% recall. The learned pairwise
coreference and temporal relation probabilities are
now used to derive the score for the WFST and dy-
namic programming approaches.
WFST representation and decoding. We
build finite-state machines using the open source
OpenFST library.
2
We use a tropical semi-ring
weighted using the negative log-likelihood of the
computed scores. OpenFST provides tools that
can search for the highest scoring sequences ac-
cepted by the machine, and can sample from high-
scoring sequences probabilistically, by treating the
2
www.openfst.org
scores of each transition within the machine as a
negative log probability. The decoding process to
compute the most likely combined medical event
sequence can be defined as searching for the best
path in the combined graph representation (Equa-
tion 2). The best path is the one that minimizes
the total weight on a path (since the arcs are neg-
ative log probabilities). In searching for the best
path, the beam size is set to 5. The accuracy of
the WFST-based representation and beam search
across all sequences using the coreference and
temporal relation scores to obtain the combined
aligned sequence is 78.9%.
Dynamic Programming. We use the NW and
SW algorithms described in Section 5.3 to pro-
duce local and global alignments respectively. We
use the scoring scheme described in Section 5.1 to
update the cost matrix for dynamic programming
and implement the algorithms as described in Sec-
tion 5.3. The overall accuracy of sequence align-
ment with both coreference and temporal relation
scores using NW is 68.7% whereas SW gives an
accuracy of 72.1%. In case of aligning just two
sequences, both methods yield the same results.
The accuracy of cross-narrative MSA for each pa-
tient, for each method, using cross validation, is
shown in Table 1. Results indicate that the WFST-
based method outperforms the dynamic program-
ming approach for multi-sequence alignment (sta-
tistical significance p<0.05). Morever, the re-
sults using both coreference and temporal realtion
scores for alignment outperform using only coref-
erence scores for alignment using all approaches.
This indicates that cross-narrative temporal rela-
tions are important for accurately aligning medical
event sequences across narratives.
7 Discussion
We propose and evaluate different approaches to
multiple sequence alignment of medical events.
1005
Approaches to multi-alignment. We address
the problem of aligning medical event sequences
using a novel WFST-based framework and empiri-
cally demonstrate that it outperforms pairwise pro-
gressive alignment using dynamic programming.
This is mainly because the WFST-based allows us
to consider temporal constraints from across mul-
tiple sequences when performing the alignment.
Moreover, it also outperforms the integer lin-
ear programming (ILP) method for timeline con-
struction proposed in (Do et al, 2012). We im-
plemented the proposed method that also allows
combining the output of classifiers subject to some
constraints. We derive intervals from event starts
and stops and learn two perceptron classifiers for
classifying the temporal relations between events
and assigning events to intervals. The classifier
probabilities are then used to solve the optimiza-
tion problem using the lpsolve solver.
3
We also
use intra-document coreference information to re-
solve coreference before performing the global op-
timization. We observe that in case of MSA, the
optimal solution using ILP is still intractable as
the number of constraints increases exponentially
with the number of sequences. Aligning pair-
wise iteratively gives us an overall average accu-
racy of 68.2% similar to dynamic programming.
While this is comparable to the dynamic pro-
gramming performance, the WFST-based method
significantly outperforms this in case of multi-
alignments for cross-narrative temporal ordering.
Performance and error analysis. We perform
multi-alignments over medical event sequences
for a patient, where each sequence corresponds
to temporally ordered medical events in a clinical
narrative generated using the ranking model de-
scribed in (Raghavan et al, 2012c). The accuracy
of intra-narrative temporal ordering is 82.1%. The
errors in performing this intra-narrative ordering
may propagate to the cross-narrative model result-
ing in reduced accuracy. This may be addressed
by considering n-best temporally ordered medi-
cal event sequences, generated by the ranking pro-
cess, and aligning the n-best sequences using the
WFST-based framework. This could be feasible
as, practically, the WFST-based method for multi-
alignment takes only a few secs to align a pair of
medical event sequences with average length 40.
The accuracy of alignments across multiple
medical event sequences is also affected by the er-
ror induced by the coreference and temporal rela-
tion scores. Often, insufficient temporal cues leads
3
http://lpsolve.sourceforge.net/5.5/
to misclassification of events incorrectly as shar-
ing the ?simultaneous? temporal relation and often
as coreferring. This induces errors in the score cal-
culation and hence the alignments. Better meth-
ods to address the challenging problem of cross-
document temporal relation learning, perhaps with
the help of structured data from the patient record,
could improve the accuracy of alignments.
There is no clear trend with respect to the num-
ber of medical events and narratives for a patient
(Table 1.), and the alignment accuracy. In fu-
ture work, it would be interesting to examine any
such correlation and also study the scalability of
the WFST-based method for sequence alignment
on longer medical event sequences and a larger
dataset of patients. Further, the WFST-based
method may be used to model multi-alignment
tasks in other speech and language problems as
well.
8 Conclusion
We propose a novel framework for aligning med-
ical event sequences across clinical narratives
based on coreference and temporal relation infor-
mation using cascaded WFSTs. FSTs provide a
convenient and flexible framework to model se-
quences of temporally ordered medical events and
compose them into a combined graph represen-
tation. Decoding this graph allows us to jointly
maximize coreference as well as temporal relation
probabilities to derive a timeline of the most likely
temporal ordering of medical events. This ap-
proach to aligning multiple sequences of medical
events significantly outperforms other approaches
such as dynamic programming. Moreover, we
demonstrate the importance of learning tempo-
ral relations for the task timeline generation from
across multiple clinical narratives by empirically
proving that decoding using both coreference and
temporal relation scores is far more accurate than
decoding with only coreference scores.
Acknowledgments
The project was supported by Award Number
Grant R01LM011116 from the National Library
of Medicine. The content is solely the responsibil-
ity of the authors and does not necessarily repre-
sent the official views of the National Library of
Medicine or the National Institutes of Health. The
authors would like to thank Yanzhang He for his
input on the WFST-based model.
1006
References
James F. Allen. 1981. An interval-based representa-
tion of temporal knowledge. In IJCAI, pages 221?
226.
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news sum-
marization. Comput. Linguist., 31(3):297?328,
September.
Regina Barzilay, Noemie Elhadad, and Kathleen McK-
eown. 2002. Inferring strategies for sentence or-
dering in multidocument summarization. Journal of
Artificial Intelligence Research (JAIR), 17:35?55.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. Infor-
mation processing & management, 46(1):89?109.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal
graphs. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?06, pages 189?198.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In ACL/AFNLP, pages 602?610.
Raphael Cohen, Michael Elhadad, and No?emie El-
hadad. 2013. Redundancy in electronic health
record corpora: analysis, impact on text mining per-
formance and mitigation strategies. BMC bioinfor-
matics, 14(1):10.
Dina Demner-Fushman, Wendy Webber Chapman, and
Clement J. McDonald. 2009. What can natural lan-
guage processing do for clinical decision support?
Journal of Biomedical Informatics, 42(5):760?772.
Quang Xuan Do, Wei Lu, and Dan Roth. 2012. Joint
inference for event timeline construction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ?12, pages 677?687. Association for Com-
putational Linguistics.
Heng Ji and Ralph Grishman. 2008. Refining event
extraction through cross-document inference. In As-
sociation for Computational Linguistics.
Cvetana Krstev, Du?sko Vitas, Ivan Obradovi?c, and
Milo?s Utvi?c. 2011. E-dictionaries and Finite-state
automata for the recognition of named entities. In
Proceedings of the 9th International Workshop on
Finite State Methods and Natural Language Pro-
cessing, pages 48?56.
Shankar Kumar and William Byrne. 2003. A weighted
finite state transducer implementation of the align-
ment template model for statistical machine trans-
lation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, pages 63?70.
V Finley Lacatusu, Steven J Maiorano, and Sanda M
Harabagiu. 2004. Multi-document summarization
using multiple-sequence alignment. In LREC.
Mirella Lapata and Alex Lascarides. 2011. Learn-
ing sentence-internal temporal relations. CoRR,
abs/1110.1394.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics-Volume 1, pages 545?
552. Association for Computational Linguistics.
Zhihui Luo, Stephen B. Johnson, Albert M. Lai, and
Chunhua Weng. 2011. Extracting temporal con-
straints from clinical research eligibility criteria us-
ing conditional random fields. In Proc of AMIA
Symposium.
Inderjeet Mani, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006.
Machine learning of temporal relations. In ACL.
Mehryar Mohri and Fernando CN Pereira. 1998. Dy-
namic compilation of weighted context-free gram-
mars. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics-Volume 2, pages 891?897. As-
sociation for Computational Linguistics.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The design principles of a weighted
finite-state transducer library. Theoretical Computer
Science, 231(1):17?32.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2005. Weighted automata in text and speech pro-
cessing. CoRR, abs/cs/0503077.
S.B. Needleman, C.D. Wunsch, et al 1970. A general
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443?453.
C?edric Notredame. 2002. Recent progress in multiple
sequence alignment: a survey. Pharmacogenomics,
3(1):131?144.
Preethi Raghavan, Eric Fosler-Lussier, and Albert M.
Lai. 2012a. Exploring semi-supervised coreference
resolution of medical concepts using semantic and
temporal features. In North American Association
for Computational Linguistics Annual Meeting - Hu-
man Language Technologies Conference. Associa-
tion for Computational Linguistics.
Preethi Raghavan, Eric Fosler-Lussier, and Albert M.
Lai. 2012b. Inter-annotator reliability of medi-
cal events, coreferences and temporal relations in
clinical narratives by annotators with varying levels
of clinical expertise. In To appear in Proceedings
1007
of the American Medical Informatics Association.
American Medical Informatics Association.
Preethi Raghavan, Eric Fosler-Lussier, and Albert M.
Lai. 2012c. Learning to temporally order medical
events in clinical text. In ACL short paper. Associa-
tion for Computational Linguistics.
Daniel Reichert, David Kaufman, Benjamin Bloxham,
Herbert Chase, and No?emie Elhadad. 2010. Cog-
nitive analysis of the summarization of longitudinal
patient records. In AMIA Annual Symposium Pro-
ceedings, volume 2010, page 667. American Medi-
cal Informatics Association.
A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,
Y. Guo, and A. Setzer. 2008. Semantic Annotation
of Clinical Text: The CLEF Corpus. In Proceedings
of the LREC 2008 Workshop on Building and Eval-
uating Resources for Biomedical Text Mining, pages
19?26.
T.F. Smith and M.S. Waterman. 1981. Identifica-
tion of common molecular subsequences. Journal
of molecular biology, 147(1).
Richard Sproat. 2006. A Computational Theory of
Writing Systems (Studies in Natural Language Pro-
cessing). Cambridge University Press.
Marc Verhagen, Robert J. Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James
Pustejovsky. 2009. The tempeval challenge: iden-
tifying temporal relations in text. Language Re-
sources and Evaluation, 43(2):161?179.
Lusheng Wang and Tao Jiang. 1994. On the complex-
ity of multiple sequence alignment. Journal of com-
putational biology, 1(4):337?348.
Christopher Whelan, Brian Roark, and Kemal Son-
mez. 2010. Designing antimicrobial peptides with
weighted finite-state transducers. In Proceedings of
IEEE Engineering in Medical Biology Society, page
764.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Asso-
ciation for Computational Linguistics, pages 189?
196.
Li Zhou, Genevieve B. Melton, Simon Parsons, and
George Hripcsak. 2006. A temporal constraint
structure for extracting temporal information from
clinical narrative. Journal of Biomedical Informat-
ics, pages 424?439.
1008
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 58?64,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Comparing human versus automatic feature extraction for fine-grained
elementary readability assessment
Yi Ma, Ritu Singh, Eric Fosler-Lussier
Dept. of Computer Science & Engineering
The Ohio State University
Columbus, OH 43210, USA
may,singhri,fosler@cse.ohio-state.edu
Robert Lofthus
Xerox Corporation
Rochester, NY 14604, USA
Robert.Lofthus@xerox.com
Abstract
Early primary children?s literature poses some
interesting challenges for automated readabil-
ity assessment: for example, teachers often
use fine-grained reading leveling systems for
determining appropriate books for children to
read (many current systems approach read-
ability assessment at a coarser whole grade
level). In previous work (Ma et al, 2012),
we suggested that the fine-grained assess-
ment task can be approached using a ranking
methodology, and incorporating features that
correspond to the visual layout of the page
improves performance. However, the previ-
ous methodology for using ?found? text (e.g.,
scanning in a book from the library) requires
human annotation of the text regions and cor-
rection of the OCR text. In this work, we ask
whether the annotation process can be auto-
mated, and also experiment with richer syntac-
tic features found in the literature that can be
automatically derived from either the human-
corrected or raw OCR text. We find that auto-
mated visual and text feature extraction work
reasonably well and can allow for scaling to
larger datasets, but that in our particular exper-
iments the use of syntactic features adds little
to the performance of the system, contrary to
previous findings.
1 Introduction
Knowing the reading level of a children?s book
is an important task in the educational setting.
Teachers want to have leveling for books in the
school library; parents are trying to select appro-
priate books for their children; writers need guid-
ance while writing for different literacy needs (e.g.
text simplification)?reading level assessment is re-
quired in a variety of contexts. The history of as-
sessing readability using simple arithmetic metrics
dates back to the 1920s when Thorndike (1921) has
measured difficulty of texts by tabulating words ac-
cording to the frequency of their use in general lit-
erature. Most of the traditional readability formulas
were also based on countable features of text, such
as syllable counts (Flesch, 1948).
More advanced machine learning techniques such
as classification and regression have been applied
to the task of reading level prediction (Collins-
Thompson and Callan, 2004; Schwarm and Osten-
dorf, 2005; Petersen and Ostendorf, 2009; Feng et
al., 2010); such works are described in further de-
tail in the next Section 2. In recent work (Ma et al,
2012), we approached the problem of fine-grained
leveling of books, demonstrating that a ranking ap-
proach to predicting reading level outperforms both
classification and regression approaches in that do-
main. A further finding was that visually-oriented
features that consider the visual layout of the page
(e.g. number of text lines per annotated text region,
text region area compared to the whole page area
and font size etc.) play an important role in predict-
ing the reading levels of children?s books in which
pictures and textual layout dominate the book con-
tent over text.
However, the data preparation process in our pre-
vious study involves human intervention?we ask
human annotators to draw rectangle markups around
text region over pages. Moreover, we only use a
very shallow surface level text-based feature set to
58
compare with the visually-oriented features. Hence
in this paper, we assess the effect of using com-
pletely automated annotation processing within the
same framework. We are interested in exploring
how much performance will change by completely
eliminating manual intervention. At the same time,
we have also extended our previous feature set by in-
troducing a richer set of automatically derived text-
based features, proposed by Feng et al (2010),
which capture deeper syntactic complexities of the
text. Unlike our previous work, the major goal of
this paper is not trying to compare different machine
learning techniques used in readability assessment
task, but rather to compare the performance differ-
ences between with and without human labor in-
volved within our previous proposed system frame-
work.
We begin the paper with the description of re-
lated work in Section 2, followed by detailed ex-
planation regarding data preparation and automatic
annotations in Section 3. The extended features will
be covered in Section 4, followed by experimental
analysis in Section 5, in which we will compare the
results between human annotations and automatic
annotations. We will also report the system per-
formance after incorporating the rich text features
(structural features). Conclusions follow in Section
6.
2 Related Work
Since 1920, approximately 200 readability formulas
have been reported in the literature (DuBay, 2004);
statistical language processing techniques have re-
cently entered into the fray for readability assess-
ment. Si and Callan (2001) and Collins-Thompson
and Callan (2004) have demonstrated the use of lan-
guage models is more robust for web documents
and passages. Heilman et al (2007) studied the
impact of grammar-based features combined with
language modeling approach for readability assess-
ment of first and second language texts. They ar-
gued that grammar-based features are more perti-
nent for second language learners than for the first
language readers. Schwarm and Ostendorf (2005)
and Petersen and Ostendorf (2009) both used a sup-
port vector machine to classify texts based on the
reading level. They combined traditional methods
of readability assessment and the features from lan-
guage models and parsers. Aluisio et al (2010)
have developed a tool for text simplification for the
authoring process which addresses lexical and syn-
tactic phenomena to make text readable but their as-
sessment takes place at more coarse levels of liter-
acy instead of finer-grained levels used for children?s
books.
A detailed analysis of various features for auto-
matic readability assessment has been done by Feng
et al (2010). Most of the previous work has used
web page documents, short passages or articles from
educational newspapers as their datasets; typically
the task is to assess reading level at a whole-grade
level. In contrast, early primary children?s literature
is typically leveled in a more fine-grained manner,
and the research question we pursued in our previ-
ous study was to investigate appropriate methods of
predicting what we suspected was a non-linear read-
ing level scale.
Automating the process of readability assessment
is crucial for eventual widespread acceptance. Pre-
vious studies have looked at documents that were
already found in electronic form, such as web texts.
While e-books are certainly on the rise (and would
help automated processing) it is unlikely that paper
books will be completely eliminated from the pri-
mary school classroom soon. Our previous study re-
quired both manual scanning of the books and man-
ual annotation of the books to extract the location
and content of text within the book ? the necessity
of which we evaluate in this study by examining the
effects of errors from the digitization process.
3 Data Preparation and Book Annotation
Our previous study was based on a corpus of 36
scanned children?s books; in this study we have ex-
panded the set to 97 books which range from lev-
els A to N in Fountas and Pinnell Benchmark As-
sessment System 1 (Fountas and Pinnell, 2010); the
Fountas and Pinnell level serves as our gold stan-
dard. The distribution of number of books per read-
ing level is shown in Table 1. Levels A to N,
in increasing difficulty, corresponds to the primary
grade books from roughly kindergarten through
third grade. The collection of children?s books cov-
ers a large diversity of genres, series and publishers.
59
Reading # of Reading # of
Level Books Level Books
A 6 H 7
B 9 I 6
C 5 J 11
D 8 K 6
E 11 L 3
F 10 M 6
G 7 N 2
Table 1: Distribution of books over Fountas and Pinnell
reading levels
Our agreement with the books? publishers only
allows access to physical copies of books rather
than electronic versions; we scan each book into
a PDF version. This situation would be similar to
that of a contemporary classroom teacher who is se-
lecting books from the classroom or school library
for evaluating a child?s literacy progress.1 We then
use Adobe Acrobat to run OCR (Optical Character
Recognition) on the PDF books. Following our pre-
vious work, we first begin our process of annotat-
ing each book using Adobe Acrobat before convert-
ing them into corresponding XML files. Features
for each book are extracted from their correspond-
ing XMLs which contain all the text information and
book layout contents necessary to calculate the fea-
tures. Each book is manually scanned, and then an-
notated in two different ways: we use human anno-
tators (Section 3.1) and a completely automated pro-
cess (Section 3.2). The job of human annotators is
primarily to eliminate the errors made by OCR soft-
ware, as well as correctly identifying text regions on
each page. We encountered three types of typical
OCR errors for the children?s books in our set:
1. False alarms: some small illustration picture
segments (e.g. flower patterns on a little girl?s
pajama or grass growing in bunches on the
ground) are recognized as text.
2. False negatives: this is more likely to occur for
text on irregular background such as white text
1While it is clear that publishers will be moving toward elec-
tronic books which would avoid the process of scanning (and
likely corresponding OCR problems), it is also clear that phys-
ical books and documents will be present in the classroom for
years to come.
OCR Correct Example
output word
1 I 1 ? I
! I ! ? I
[ f [or ? for
O 0 1OO ? 100
nn rm wann ? warm
rn m horne ? home
IT! m aIT! ? am
1n m tilne ? time
n1. m n1.y ? my
1V W 1Ve ? We
vv w vvhen ? when
Table 2: Some common OCR errors
on black background or text overlapped with
illustrations.
3. OCR could misread the text. These are most
common errors. Some examples of this type of
error are shown in Table 2.
The two different annotation processes are explained
in the following Subsections 3.1 and 3.2.
3.1 Human Annotation
Annotators manually draw a rectangular box over
the text region on each page using Adobe Acrobat
markup drawing tools. The annotators also correct
the type 2 and 3 of OCR errors which are mentioned
above. In human annotation process, the false alarm
(type 1) errors are implicitly prevented since the an-
notators will only annotate the regions where text
truly exists on the page (no matter whether the OCR
recognized or not).
3.2 Automatic Annotation
For automatic annotation, we make use of JavaScript
API provided by Adobe Acrobat. The automatic an-
notation tool is implemented as a JavaScript plugin
menu item within Adobe Acrobat. The JavaScript
API can return the position of every single recog-
nized word on the page. Based on the position cues
of each word, we design a simple algorithm to auto-
matically cluster the words into separate groups ac-
cording to certain spatial distance thresholds.2 In-
2A distance threshold of 22 pixels was used in practice.
60
tuitively, one could imagine the words as small
floating soap bubbles on the page?where smaller
bubbles (individual words) which are close enough
will merge together to form bigger bubbles (text re-
gions) automatically. For each detected text region,
a bounding rectangle box annotation is drawn on
the page automatically. Beyond this point, the rest
of the data preparation process is identical to hu-
man annotation, in which the corresponding XMLs
will be generated from the annotated versions of
the PDF books. However, unlike human annota-
tion, automating the annotation process can intro-
duce noise into the data due to uncorrected OCR er-
rors. In correspondence to the three types of OCR
errors, automatic annotation could also draw extra
bounding rectangle boxes on non-text region (where
OCR thinks there is text there but there is not), fails
to draw bounding rectangle boxes on text region
(where OCR should have recognized text there but
it does not) and accepts many mis-recognized non-
word symbols as text content (where OCR misreads
words).
3.3 Generating XMLs From Annotated PDF
Books
This process is also implemented as another
JavaScript plugin menu item within Adobe Acrobat.
The plugin is run on the annotated PDFs and is de-
signed to be agnostic to the annotation types?it will
work on both human-annotated and auto-annotated
versions of PDFs. Once the XMLs for each chil-
dren?s book are generated, we could proceed to the
feature extraction step. The set of features we use in
the experiments are described in the following Sec-
tion 4.
4 Features
For surface-level features and visual features, we
utilize similar features proposed in our previous
study.3 For completeness? sake, we list these two
sets of features as follows in Section 4.1:
3We discard two visual features in both the human and au-
tomatic annotation that require the annotation of the location
of images on the page, as these were features that the Adobe
Acrobat JavaScript API could not directly access.
4.1 Surface-level Features and
Visually-oriented Features
? Surface-level Features
1. Number of words
2. Number of letters per word
3. Number of sentences
4. Average sentence length
5. Type-token ratio of the text content.
? Visually-oriented Features
1. Page count
2. Number of words per page
3. Number of sentences per page
4. Number of text lines per page
5. Number of words per text line
6. Number of words per annotated text rect-
angle
7. Number of text lines per annotated text
rectangle
8. Average ratio of annotated text rectangle
area to page area
9. Average font size
4.2 Structural Features
Since our previous work only uses surface level of
text features, we are interested in investigating the
contribution of high-level structural features to the
current system. Feng et al (2010) found several
parsing-based features and part-of-speech based fea-
tures to be useful. We utilize the Stanford Parser
(Klein and Manning, 2003) to extract the following
features from the XML files based on those used in
(Feng et al, 2010):
? Parsed Syntactic Features for NPs and VPs
1. Number of the NPs/VPs
2. Number of NPs/VPs per sentence
3. Average NP/VP length measured by num-
ber of words
4. Number of non-terminal nodes per parse
tree
5. Number of non-terminal ancestors per
word in NPs/VPs
? POS-based Features
61
1. Fraction of tokens labeled as
noun/preposition
2. Fraction of types labeled as
noun/preposition
3. Number of noun/preposition tokens per
sentence
4. Number of noun/preposition types per
sentence
5 Experiments
In the experiments, we look at how much the perfor-
mance dropped by switching to zero human inputs.
We also investigate the impact of using a richer set
of text-based features. We apply the ranking-based
book leveling algorithm proposed by our previous
study (Ma et al, 2012) and use the SVMrank ranker
(Joachims, 2006) for our experiments. In this sys-
tem, the ranker learns to sort the training books into
leveled order. The unknown test book is inserted
into the ordering of the training books by the trained
ranking model, and the predicted reading level is
calculated by averaging over the levels of the known
books above and below the test book. Following the
previous study, each book is uniformly partitioned
into 4 parts, treating each sub-book as an individ-
ual entity. A leave-n-out procedure is utilized for
evaluation: during each iteration of the training, the
system leaves out all n partitions (sub-books) cor-
responding to one book. In the testing phase, the
trained ranking model tests on all partitions corre-
sponding to the held-out book. We obtain a single
predicted reading level for the held-out book by av-
eraging the results for all its partitions; averaging
produces a more robust result. Two separate experi-
ments are carried out on human-annotated and auto-
annotated PDF books respectively.
We use two metrics to determine quality: first, the
accuracy of the system is computed by claiming it
is correct if the predicted book level is within ?1 of
the true reading level.4 The second scoring metric is
the absolute error of number of levels away from the
key reading level, averaged over all of the books.
4We follow our previous study to use ?1 accuracy evalu-
ation metric in order to generate consistent results and allow
easy comparison. Another thing to notice is that this is still
rather fine-grained since multiple reading levels correspond to
one single grade level.
We report the experiment results on different
combinations of feature sets: surface level features
plus visually-oriented features, surface level features
only, visually-oriented features only, structural fea-
tures only and finally combining all the features to-
gether.
5.1 Human Annotation vs. Automatic
Annotation
As we can observe from Table 3,5 overall the human
annotation gives higher accuracy than automatic an-
notation across different feature sets. The perfor-
mance difference between human annotation and au-
tomatic annotation could be attributed to the OCR
errors (described in Section 3.2) which are intro-
duced in the automatic annotation process. How-
ever, to our surprise, the best performance of human
annotation is not significantly better than automatic
annotation even at p < 0.1 level (figures in bold).6
Only for the experiment using all features does hu-
man annotation outperform the automatic annota-
tion at p < 0.1 level (still not significantly better
at p < 0.05 level, figures with asterisks). There-
fore, we believe that the extra labor involved in the
annotation step could be replaced by the automatic
process without leading to a significant performance
drop. While the process does still require manual
scanning of each book (which can be time consum-
ing depending on the kind of scanner), the automatic
processing can reduce the labor per book from ap-
proximately twenty minutes per book to just a few
seconds.
5.2 Incorporating Structural Features
Our previous study demonstrated that combin-
ing surface features with visual features produces
promising results. As mentioned above, the sec-
ond aim of this study is to see how much benefit
we can get from incorporating high-level structural
features, such as those used in (Feng et al, 2010)
(described in Section 4.2), with the features in our
previous study.
Table 3 shows that for both human and automatic
5In three of the books, the OCR completely failed; thus only
94 books are available for evaluation of the automatic annota-
tion.
6One-tailed Z-test was used with each book taken as an in-
dependent sample.
62
Annotation type Human Automatic
?1 Accuracy %
Surface+Visual features 76.3 70.2
Surface level features 69.1 64.9
Visual features 63.9 58.5
Structural features 63.9 58.5
All features 76.3? 66.0?
Average leveling error ? standard deviation
Surface+Visual features 0.99 ? 0.87 1.16 ? 0.83
Surface level features 1.24 ? 1.05 1.16 ? 0.97
Visual features 1.24 ? 1.00 1.37 ? 0.89
Structural features 1.30 ? 0.89 1.33 ? 0.91
All features 1.05 ? 0.78 1.15 ? 0.90
Table 3: Results on 97 books using human annotations vs. automatic annotations, reporting accuracy within one level
and average error for 4 partitions per book.
annotation under the ?1 accuracy metric, the vi-
sual features and the structural features have the
same performance, whose accuracy are both slightly
lower than that of surface level features. By combin-
ing the surface level features with the visual features,
the system obtains the best performance. How-
ever, by combining all three feature sets, the sys-
tem performance does not change for human annota-
tion whereas it hurts the performance for automatic
annotation?it is likely that the OCR errors existing
in the automatic annotations give rise to erroneous
structural features (e.g. the parser would produce
less robust parses for sentences which have out of
vocabulary words). Overall, we did not observe bet-
ter performance by incorporating structural features.
Using structural features on their own also did not
produce noteworthy results. Although among the
three kinds of features (surface, visual and struc-
tural), structural features have the highest computa-
tional cost, it exhibits no significant improvement to
system results. In the average leveling error metric,
the best performance is again obtained at the com-
bination of surface level features and visual features
for human annotation, whereas the performance re-
mains almost the same after incorporating structural
features for automatic annotation.
6 Conclusion
In this paper, we explore the possibility of reducing
human involvement in the specific task of predicting
reading levels of scanned children?s books by elimi-
nating the need for human annotation. Clearly there
is a trade off between the amount of human labor
involved and the accuracy of the reading level pre-
dicted. Based on the experimental results, we did
not observe significant performance drop by switch-
ing from human annotation to automatic annotation
in the task of predicting reading levels for scanned
children?s books.
We also study the effect of incorporating struc-
tural features into the proposed ranking system. The
experimental results showed that structural features
exhibit no significant effect to the system perfor-
mance. We conclude for the simply structured, short
text that appears in most children?s books, a deep
level analysis of the text properties may be overkill
for the task and produced unsatisfactory results at a
high computational cost for our task.
In the future, we are interested in investigating the
importance of each individual feature as well as ap-
plying various feature selection methods to further
improve the overall performance of the system?in
the hope that making the ranking system more ro-
bust to OCR errors introduced by automatic annota-
tion processing. Another interesting open question
is that how many scanned book pages are needed to
make a good prediction.7 Such analysis would be
very helpful for practical purposes, since a teacher
7We thank an anonymous reviewer of the paper for this sug-
gestion.
63
could just scan few sample pages instead of a full
book for a reliable prediction.
References
S. Aluisio, L. Specia, C. Gasperin, and C. Scarton. 2010.
Readability assessment for text simplification. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 1?9. Association for Computational
Linguistics.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT / NAACL 2004, volume 4, pages
193?200, Boston, USA.
W.H. DuBay. 2004. The principles of readability. Im-
pact Information, pages 1?76.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A comparison of features for automatic read-
ability assessment. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 276?284, Beijing, China. As-
sociation for Computational Linguistics.
R. Flesch. 1948. A new readability yardstick. Journal of
applied psychology, 32(3):221?233.
I. Fountas and G. Pinnell. 2010. Fountas
and pinnell benchmark assessment system 1.
http://www.heinemann.com/products/E02776.aspx.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of NAACL
HLT, pages 460?467.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217?226. ACM.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
423?430.
Y. Ma, E. Fosler-Lussier, and R. Lofthus. 2012.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of NAACL
HLT.
S. Petersen and M. Ostendorf. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech & Language, 23(1):89?106.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 523?530. Association for Computational
Linguistics.
L. Si and J. Callan. 2001. A statistical model for scien-
tific readability. In Proceedings of the tenth interna-
tional conference on Information and knowledge man-
agement, pages 574?576. ACM.
E.L. Thorndike. 1921. The teacher?s word book, volume
134. Teachers College, Columbia University New
York.
64
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 29?37,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Temporal Classification of Medical Events
Preethi Raghavan?, Eric Fosler-Lussier?, and Albert M. Lai?
?Department of Computer Science and Engineering
?Department of Biomedical Informatics
The Ohio State University, Columbus, Ohio, USA
{raghavap, fosler}@cse.ohio-state.edu, albert.lai@osumc.edu
Abstract
We investigate the task of assigning medi-
cal events in clinical narratives to discrete
time-bins. The time-bins are defined to cap-
ture when a medical event occurs relative to
the hospital admission date in each clinical
narrative. We model the problem as a se-
quence tagging task using Conditional Ran-
dom Fields. We extract a combination of lexi-
cal, section-based and temporal features from
medical events in each clinical narrative. The
sequence tagging system outperforms a sys-
tem that does not utilize any sequence infor-
mation modeled using a Maximum Entropy
classifier. We present results with both hand-
tagged as well as automatically extracted fea-
tures. We observe over 8% improvement in
overall tagging accuracy with the inclusion of
sequence information.
1 Introduction
There has been a lot of interest in building timelines
of medical events from unstructured patient narra-
tives (Jung et al, 2011; Zhou and Hripcsak, 2007).
Creating a timeline from longitudinal clinical text
requires learning temporal relations such as before,
simultaneous, includes, overlaps, begins, ends and
their inverses between medical events found within
and across patient narratives (Allen, 1981). How-
ever, learning temporal relations for fine-grained
temporal ordering of medical events in clinical text
is challenging: the temporal cues typically found in
clinical text may not always be sufficient for this
task.
An important characteristic of a clinical narrative
is that the medical events in the same narrative are
more or less semantically related by narrative dis-
course structure. However, medical events in the
narrative are not ordered chronologically. Thus, the
clinical narrative structure is not always temporally
coherent.
Moreover, extracting precise temporal features
for highly accurate temporal ordering of medical
events is difficult as the temporal relationship be-
tween medical events is varied and complicated.
Zhou and Hripcsak (2007) identify six major cate-
gories of temporal expressions from a corpus of dis-
charge summaries: ?date and time,? ?relative date
and time,? ?duration,? ?event-dependent temporal
expression,? ?fuzzy time,? and ?recurring times.?
Their study of temporal expressions in clinical text
indicates that relative time (e.g., ever since the
episode 2 days ago) may be more prevalent than ab-
solute time (e.g., 06/03/2007). Further, temporal ex-
pressions may be fuzzy where ?history of cocaine
use? may imply that cocaine use started 2 years ago
or 10 years ago.
In this paper, we address a relatively simpler task
of assigning medical events to coarsely defined time-
bins. The time-bins, way before admission, before
admission, on admission, after admission, after dis-
charge, are defined based on the relative temporal
distance of the medical event from the admission
date, which is the only explicit date almost always
found in each clinical narrative. We extract fea-
tures based on narrative structure as well as tempo-
ral expressions to label a sequence of medical events
from each clinical narrative with a highly probable
29
HISTORY   PHYSICAL                                 DATE:  06/03/2007 
NAME:  Smith Jack                           MR#:  XXX-XX-XXXX 
ATTENDING PHYSICIAN:  Bill Payne  MD             DOB:  02/28/1960 
CHIEF COMPLAINT 
Chest pain and arm infection. 
HISTORY OF PRESENT ILLNESS 
Patient is a 48-year-old male with history of cocaine use hypertension who presents with chest pain  
which started 2 days ago . He did not having  chest pain yesterday but ever since the episode 2 days ago  
he has felt a little weaker.  He did have chest pain today and this is what prompted him to come to the  
ER.  He also  notices that he has had some infections under his arms.  He states that he had to have an  
abscess I and D 3 or 4 months ago under his arm and 2 to 3 weeks ago he noticed some more spots and  
these spots have now grown and now are under both arms. Currently he is chest pain free. His blood  
pressure upon presentation was 189/106. 
REVIEW OF SYSTEMS 
On exam initial blood pressure was 189/106 current blood pressure 148/83 with heart rate of 74  
respirations  16.  Heart regular rhythm.  No murmurs.   Arms:  He does have tender areas right greater  
than left under the arm. Difficult to tell if there is any erythema but  obvious cellulitis sludge abscess  
under the right arm which is tender. 
ASSESSMENT/PLAN 
1. Chest pain history of cocaine with T-wave inversions in the inferior leads.  Currently he is chest pain free.  We will check a 2-D echocardiogram.  Consult Cardiology for a stress test.   
2. Axillary abscesses.  Consult Surgery for I and D.  We will place on IV vancomycin pain control. 
3. Cocaine abuse.  Encouraged to quit. 
 
1  
 
 
2  
 
 
3  
 
 
4  
 
 
5  
 
 
6  
 
 
7  
 
 
8  
  
9  
 
Figure 1: Excerpt from a de-identified clinical narrative
(cn1) written for a patient in 2007. Medical events are
underlined. Enumerated events (in circles) are used as an
example later in Table 1.
sequence of time-bins using Conditional Random
Fields (CRFs). The learned time-bins can be used
as an informative temporal feature for tasks such
as fine-grained temporal ordering of medical events
and medical event coreference resolution.
2 Motivation
Clinical narratives are medical reports that contain
unstructured text documenting the medical history
of the patient. Medical events are temporally-related
concepts in clinical narratives that describe medical
conditions affecting the patient?s health, or tests and
procedures performed on a patient. Sample excerpts
from two different clinical notes (cn1 and cn2) of
the same patient, generated over time, are shown in
Figures 1 and 2. We can see from the examples that
narrative structure moves back and forth in time and
is not temporally coherent. We use cn1 and cn2 as
running examples throughout the paper.
The medical events assigned to time-bins in each
clinical narrative allow us to derive a coarse tempo-
ral order between medical events within and across
the longitudinal medical history of the patient. Since
we learn time-bins centered around admission in
each narrative and we also know the admission date
and perhaps the discharge dates in cn1 and cn2, we
can derive a coarse partial order across the medi-
HISTORY   PHYSICAL                                 DATE:  06/17/2007 
NAME:  Black Jack                           MR#:  XXX-XX-XXXX 
ATTENDING PHYSICIAN:  Jack Payne MD             DOB:  02/28/1960 
He is a 48-year-old African American gentleman with a history of cocaine use and hypertension. He  
has hidradenitis of both axilla resected. The patient is MRSA positive on IV antibiotics at the present  
time.  The patient's physical condition is excellent but he had MRSA in the axilla for hidradenitis that  
was devastating.  The wounds now are very large but he is wound vac and being changed to alginate.  
Both axilla show major wounds of 20-25 cm in diameter and 4 -5 cm deep in overall size and he has  
excoriations on his chest from the tape.  The plan is to change him from vac to alginate and see him  
in a week. 
Figure 2: Excerpt from another de-identified clinical nar-
rative (cn2) for the same patient written in later in 2007.
Medical events are underlined.
cal events in cn1 and cn2. This is shown in Fig-
ure 3. Even if the discharge dates are not known,
we still know that the admission date (A1) of cn1
is 6/03/2007 and A2 of cn2 is 06/17/2007. Thus,
A2 > A1, and all the time-bins in cn2 that are on or
after admission would have happened after A2. The
partially ordered time-bins can now be used for tasks
such as medical concept coreference resolution.
In cross narrative coreference resolution tasks,
we can prune the space of candidate pairs of med-
ical events by ruling out portions of clinical nar-
ratives that will not have any coreferring medical
events. For example, in the timeline shown in Fig-
ure 3, the medical events in time-bins admission, af-
ter admission and discharge of cn2 will not corefer
with any medical event in cn1. Further, when men-
tions of the same medical events occur in different
time-bins, it could mean that they are the same in-
stance of the medical event and they corefer. For
instance, cocaine abuse and cocaine use. Similarly,
MRSA positive is assigned to time-bin on admission
whereas MRSA is assigned to before admission and
both mentions of MRSA corefer.
3 Related Work
The Timebank (Pustejovsky et al, 2003) corpus of
annotated newswire text is widely used for tempo-
ral relation learning. The TempEval challenges have
often focused on extracting different types of tempo-
ral relations from Timebank (Verhagen et al, 2009).
In Timebank, events are typically verbs that denote
change in state. Since the notion of an event in Time-
bank is different from medical events in clinical text,
it is not possible to directly train models on Time-
bank and apply them to clinical text. The THYME
work (Savova et al, 2009) extends TimeML to the
30
    A1 D1 
   A2 D2 
cocaine use  hypertension 
 chest pain   abscess 
chest pain         arm  infection 
heart regular  rhythm 
cellulitis 
2-D echocardiogram 
stress test 
MRSA positive 
hidradenitis of axilla  resected   MRSA in the axilla for hidradenitis 
wounds 
wound vac 
IV antibiotics 
alginate cocaine use  hypertension 
way before before admission after discharge 
before admission after discharge way before 
p1-cn1 
p1-cn2 
Figure 3: Medical events in clinical narratives cn1 and cn2 for patient p1 assigned to time-bins. A1 is the admission
date in cn1 and D1 is the discharge date. Similarly A2 is the admission date in cn2 and D2 is the discharge date. Thus,
we have, A1 < D1, D1 < A2, A2 < D2
medical domain to create layered annotation to be
used for event linking. Boland et al (2012) identify
the temporal knowledge representation requirements
of clinical eligibility criteria and develop a frame-
based representation designed to support semantic
annotation for temporal expressions in eligibility cri-
teria. However, the nature of data found in eligibility
criteria is different from clinical narratives.
Previous attempts at learning temporal relations
between medical events in clinical text include Jung
et al (2011) and Zhou et al (2006). Gaizauskas et
al. (2006) learn the temporal relations before, after,
is included between events from a corpus of clinical
text much like the event-event relation TLINK learn-
ing in Timebank (Pustejovsky et al, 2003). How-
ever, the corpora used in these studies are not freely
available. A comprehensive survey of temporal rea-
soning in medical data is provided by Zhou and
Hripcsak (2007).
The task addressed in this paper is at a higher
level than the temporal relation learning or tempo-
ral ordering task. Without getting into fine-grained
temporal ordering, we define coarse time-bins and
classify medical events into one of the time-bins.
We work with a similar motivation of being able
to answer clinical trial eligibility criteria with tem-
poral constraints. However, while they model the
temporal information in eligibility criteria, we pro-
cess the temporal information and medical events
in the clinical narrative to assign events to time-
bins. The learned time-bins are a step towards fine-
grained temporal ordering of medical events in clin-
ical text. More importantly, we also demonstrate
how automatic feature extraction for this task gives
us promising results, though not as good as using
hand-tagged features.
4 Problem Description
A patient could have multiple clinical narratives,
generated over a period of time, representing the pa-
tient?s longitudinal medical history. Returning to the
examples in Figures 1 and 2, in this section we de-
scribe how such clinical narratives are translated into
a temporal-bin assignment problem.
4.1 Medical event representation
Medical events in clinical narratives often have a
time duration with a corresponding start and stop
time, for example, history of hypertension (Zhou et
al., 2006). In this example, hypertension started at
some point before admission and is present to the
current date. Time duration based representation is
essential to learning the exact fine-grained tempo-
ral order of medical events within and across clin-
ical narratives. In order to keep the task of classi-
fying medical events into coarse time-bins relatively
simple and easy to learn, we use a time-point nota-
tion for representing medical events. Each mention
of a medical event is assigned to a time-bin with-
out taking into consideration whether it denotes the
beginning or end of that event. We also do not dif-
ferentiate between coreferences of the same medical
event. Thus, if chest pain is mentioned in the past
medical history and the same chest pain continues
to persist in the after admission time-bin, the two
different mentions of chest pain get anchored to dif-
31
ferent time-bins. Similarly, cocaine use started in
the history of the patient and cocaine abuse still per-
sists. We assign the two different mentions of this
medical event into different time-bins.
4.2 Time-bins
As mentioned earlier, we learn to classify medical
events into one of the following time-bins: way be-
fore admission, before admission, on admission, af-
ter admission, after discharge. The intuition behind
each time-bin label is as follows. The time-bin way
before admission is intended to capture all medical
events that happened in the past medical history of
the patient but are not mentioned as being directly
related to the present illness. Before admission cap-
tures events that occurred before admission and are
related to the present illness. On admission captures
medical events that occur on the day of admission.
After admission captures medical events that occur
between admission and discharge (during the hospi-
tal stay or clinic visit). Finally, medical events that
are supposed to occur in the future after the patient
is discharged belong to the class after discharge.
Further, the time duration of each time-bin varies
based on the patient. For instance, the hospital stay
of a patient could be 4 days or 1 month or a year.
This makes it very difficult to define exact time-bins
based on the intuitions described above. In order
to make the problem more precise and consistent
across different patients, we restrict way before ad-
mission to events that happened more than a year
ago and before admission to events that occurred in
the same year before admission. If it is unclear as
to when in the past the medical event occurred, we
assume it happened way before admission.
5 Learning time-bin assignments
We model the problem of classifying medical events
to time-bins as a sequence tagging task using CRFs
(Lafferty et al, 2001). CRFs are a joint model of
label sequence conditioned on the observation.
For the task proposed in this paper, an observation
sequence is composed of medical events in the order
in which they appear in a clinical narrative, and the
state sequence is the corresponding label sequence
of time-bins. Each label in the label sequence could
be any one of the time-bins way before admission
(wa), before admission (ba), on admission (a), after
admission (aa), after discharge (ad). Thus, given
a sequence of medical events in narrative order we
learn a corresponding label sequence of time-bins
{wb, b, a, aa, ad}.
The probability of time-bin (label) sequence y,
given a medical event (input) sequence x, is given
by,
P (Y |X) = exp
?
i
(S(x, y, i) + T (x, y, i)) (1)
where i is the medical event index and S and T are
the state and transition features respectively. State
features S consider the label of a single medical
event and are defined as,
S(x, y, i) =
?
j
?jsj(y, x, i) (2)
Transition features consider the mutual dependence
of labels yi?1 and yi (dependence between the time-
bins of the current and previous medical event in the
sequence) and are given by,
T (x, y, i) =
?
k
?ktk(yi?1, yi, x, i) (3)
where sj and tk are the state and transition feature
functions. Above, sj is a state feature function, and
?j is its associated weight and tj is a transition func-
tion, and ?j is its associated weight. In contrast to
the state function, the transition function takes as in-
put the current label as well as the previous label,
in addition to the data. The mutual dependence be-
tween the time-bins of the current and previous med-
ical events is observed frequently in sections of the
text describing the history of the patient. Around
40% of the medical events in gold standard corpus
demonstrate such dependencies.
The Maximum Entropy (MaxEnt) model (Berger
et al, 1996) estimates the probability of a time-bin
given the observed medical event. In this case, we
are interested in finding the time-bin with the maxi-
mum estimated probability.
6 Feature Space
We extract features from medical event sequences
found in each clinical narrative. The extracted
feature-set captures narrative structure in terms of
the narrative type, sections, section transitions, and
32
position in document. The medical event and the
context in which it is mentioned is captured with
the help of lexical features. The temporal features
resolve temporal references and associate medical
events with temporal expressions wherever possible.
6.1 Section-based features
Determining the document-level structure of a clin-
ical narrative is useful in mapping medical events
to time-bins. This can be achieved by identifying
different sections in different types of clinical narra-
tives and relating them to different time-bins. The
section in which the medical event is mentioned
tells us something about when it occurred. Li et al
(2010) train a hidden Markov model (HMM) to map
a sequence of sections to 15 possible known section
types in free-text narratives with high accuracy.
Commonly found sections in discharge sum-
maries and history and physical reports include:
?past medical history,? ?history of present illness,?
?findings on admission,? ?physical examination,?
?review of systems,? ?impression,? and ?assess-
ment/plan.? On the other hand, radiology notes tend
to have sections describing ?indication,? ??com-
parison,? ?findings? and ?impression?. Similarly,
pathology notes may have sections including ?clini-
cal history,? ?specimen received,? ?laboratory data?
and ?interpretation.? While some sections talk about
patient history, some other sections describe the pa-
tient?s condition after admission, or plans after dis-
charge. However, some clinical notes like cn2 in
Figure 2 may not have any section information.
The combined feature representing the type of
clinical narrative along with the section can be infor-
mative. Section transitions may also indicate a tem-
poral pattern for medical events mentioned across
those sections. For instance, ?past medical history?
(way before admission), followed by ?history of
present illness? (way before admission), followed by
?findings on admission? (on admission), followed
by ?physical examination? (after admission), fol-
lowed by ?assessment/plan? (discharge). Medical
events in different types of sections may also exhibit
different temporal patterns. A ?history of present ill-
ness? section may start with diseases and diagnoses
30 years ago and then proceed to talk about them in
the context of a medical condition that happened few
years ago and finally describe the patient?s condition
on admission.
In addition to the section information, we also use
other features extracted from the clinical narrative
structure such as the position of the medical concept
in the section and in the narrative.
6.2 Lexical features
Bigrams are pairs of words that occur in close prox-
imity to each other, and in a particular order. The
bigrams preceding the medical event in the narra-
tive can be useful in determining when it occurred.
For instance, ?history of cocaine use and hyper-
tension,? ?presents with chest pain,? ?have chest
pain,? ?since the episode,? etc. If the preceding bi-
gram contains a verb, we also extract the tense of the
verb as a feature. However, tense is not always help-
ful in learning the time of occurrence of a medical
event. Consider the following line from cn2 in Fig-
ure 2, ?He has hidradenitis of both axilla resected.?
Though ?has? is in present tense, the medical event
has actually occurred in the history and is only being
observed and noted now. Additionally, we also ex-
plicitly include the preceding bigrams and the tense
of verb for the previous and next medical event as a
feature for the current medical event.
Every medical event that occurs above a certain
frequency threshold in all the clinical narratives of
a particular patient is also represented as a binary
feature. More frequent medical events tend to occur
in the history of the patient, for example, cocaine
use. We use a threshold of 3 in our experiments.
The medical event frequency in also calculated in
combination with other features such as the type of
clinical narrative and section type.
6.3 Dictionary features
The UMLS1 includes a large Metathesaurus of con-
cepts and terms from many biomedical vocabular-
ies and a lexicon that contains syntactic, morpho-
logical, and orthographic information for biomed-
ical and common words in the English language.
We map each medical event to the closest concept
in the UMLS Metathesaurus and extract its seman-
tic category. The semantic categories in UMLS in-
clude Finding, Disease or Syndrome, Therapeutic
or Preventative procedure, Congenital abnormality,
1https://uts.nlm.nih.gov/home.html
33
and Pathologic Function. The intuition behind this is
that medical events associated with certain semantic
categories may be more likely to occur within cer-
tain time-bins. For instance, a medical event classi-
fied as ?Congenital abnormality? may be more likely
to occur way before admission.
6.4 Temporal features
Temporal features are derived from any explicit
dates that fall in the same sentence as the medical
concept. The gold-standard corpus contains anno-
tations for temporal anchors for events. Although
there are no explicit dates in cn1 and cn2, there may
be narratives where there are mentions of dates such
as fever on June 7th, 2007. In some cases, there
may also be indirect references to dates, which tell
us when the medical event occurred. The reference
date with respect to which the indirect temporal ref-
erence is made depends on the type of note. In case
of history and physical notes, the reference date is
usually the admission date. For instance, chest pain
which started 2 days ago, this would mean chest
pain which started 2 days before admission. Since
the admission date is 06/03/2007, chest pain would
have started on 06/01/2007. Similarly, 3 to 4 months
ago resolves to February 2007 or March 2007 and 2
to 3 weeks ago resolves to first or second week of
May 2007. Whenever, the exact date is fuzzy, we as-
sume the date that is farthest from the reference date
as accurate. So in case of these examples, February
2007 and first week of May 2007 are assumed to be
correct. We also calculate the difference between ad-
mission date and these dates associated with medical
events. Another fuzzy temporal expression is ?his-
tory of,? where history could mean any time frame
before admission. We assume that any medical event
mentioned along with ?history of? has occurred way
before admission.
Other implicit temporal expressions can be found
in phrases such as upon presentation yesterday, to-
day, at the present time, and now. Upon presen-
tation, at the present time, today, and now resolve
to the admission date 06/03/2007 and yesterday
resolves to the day before admission 06/02/2007.
There are some other implicit temporal expressions
expressed relative to medical events, for example,
ever since the episode 2 days ago he has felt a little
weaker. Here, episode refers to chest pain and since
chest pain happened 2 days ago, ever since then up
to the present time would resolve to the time period
between 06/01/2007 and 06/03/2007. This time pe-
riod is associated with weaker.
7 Corpus
We use annotators that are students or recently grad-
uated students from diverse clinical backgrounds
with varying levels of clinical experience to anno-
tate a corpus of clinical narratives from the medical
center. The corpus consists of narratives specifically
from MRSA cases and consists of admission notes,
radiology and pathology reports, history and physi-
cal reports and discharge summaries. The features
marked by the annotators include medical events;
corresponding time-bin; corresponding UMLS con-
cept identifier; the UMLS semantic category; tem-
poral expressions; the link between temporal expres-
sions and medical events, if any; and the section un-
der which the medical event is mentioned, if any.
The annotators marked 1854 medical events across
5 patients and 51 clinical narratives. The annotation
agreement across our team of annotators is high; all
annotators agreed on 89.5% of the events and our
overall inter-annotator Cohen?s kappa statistic (Con-
ger, 1980) for medical events was 0.865.
While we found the inter-annotator agreement
for medical event UMLS concept identifiers to be
lower than for medical events and temporal expres-
sions, agreement was still very high. We discov-
ered that in many cases there was either a dis-
crepancy in the granularity to which the medical
events were coded or whether or not clinical judg-
ment was used in selecting the concept identifier.
For example, all of our annotators marked ?B-Cell
CLL? as an event. Three of them coded this term
as ?C0023434: Chronic Lymphocytic Leukemia.?
Two others coded this event as ?C0475774: B-cell
chronic lymphocytic leukemia variant.? While both
could be considered correct annotations for ?B-Cell
CLL,? C0475774 is the more specific term. In
another example, all of the annotators marked the
phrase ?white blood cell count of 10,000.? For this
situation, one of them selected ?C0750426: white
blood cell count increased,? while another selected
?C0023508: White Blood Cell count procedure.? In
contrast, the other three selected different concept
34
identifiers, applying clinical judgment to the medi-
cal events. One other annotator selected ?C0860797:
differential white blood cell count normal.?
We use this gold-standard corpus for our exper-
iments. We conduct two sets of experiments with
the clinical narratives in this corpus: 1) Medical
event, Time-bin experiments using hand-tagged fea-
tures from the corpus and 2) Medical event, Time-
bin experiments using automatically extracted fea-
tures from the corpus.
8 Experiments
We first conducted experiments using the hand-
tagged features in our corpus. Based on these
features, we generated the section-based, lexical,
dictionary and temporal features described in the
previous sections. We used 10-fold cross vali-
dation in all our experiments. We use the Mal-
let2 implementation of CRFs and MaxEnt. CRFs
are trained by Limited-Memory Broyden-Fletcher-
Goldfarb-Shanno (BFGS) for our experiments. The
per-class accuracy values of both sequence tagging
using CRFs and using a MaxEnt model are indicated
in Table 3.
When modeled as a multi-class classification task
using MaxEnt, we get an average precision of 81.2%
and average recall of 71.4% whereas using CRFs we
obtain an average precision of 89.4% and average
recall of 79.2%. In order to determine the utility
of temporal features, we do a feature ablation study
with the temporal features removed. In this case
the average precision of the CRF is 79.5% and av-
erage recall is 67.2%. Similarly, when we remove
the section-based features, the average precision of
the CRF is 82.7% and average recall is 72.3%. The
section-based features seems to impact the precision
of the on admission and after admission time-bins
the most.
We compare our approach for classifying medi-
cal events to time-bins with the following baseline
model. We assign medical events to time-bins based
on the type of narrative, any explicit dates and sec-
tion in which they occur. Each section is associated
with a pre-defined time-bin. In the case of the sec-
tions in cn1, any medical event under ?history of
present illness? is before admission, ?review of sys-
2http://mallet.cs.umass.edu/
Medical Event Baseline MaxEnt CRF Gold
1?cocaine use ba wa wa wa
2?hypertension ba wa wa wa
3?chest pain ba ba ba ba
4?episode ba ba ba ba
5?chest pain ba ba a a
6?infections ba wa ba ba
7?abscess ba ba ba ba
8?spots ba ba ba ba
9?chest pain free ba wa a a
Table 1: Time-bin predictions by the section baseline
method, MaxEnt model and CRF for a subset of medi-
cal events marked in cn1 in Figure 1.
Class(time-bin) Section baseline
P R
way before admission (wa) 56.3 61.4
before admission (ba) 60.2 57.5
on admission (a) 63.8 59.1
after admission (aa) 57.5 68.2
after discharge (ad) 52.3 55.1
Table 2: Per-class precision (P) and recall (R) for medical
events, time-bins using hand-tagged extracted features.
tems? is after admission and ?assessment/plan? is
discharge. If the narrative has a ?past medical his-
tory? or a similar section, the events mentioned un-
der it would be assigned to way before admission.
Partial results of (medical event, time-bin) assign-
ment in cn2 as per this baseline can be seen in Table
1. However, this baseline does not work for clinical
narratives like cn2 that do not have any section in-
formation. This model gives us an average precision
of 58.02% and average recall of 60.26% across the 5
time-bins. Per-class predictions for the baseline are
shown in Table 2.
The most common false positives for the before
admission class are medical events belonging to on
admission. This may be due to lack of temporal fea-
tures to indicate that the event happened on the same
day as admission. Frequently, medical events that
belong to the aa, ba and wa time-bin get classified
as after discharge. One of the reasons for this could
be misleading section information in case of histori-
cal medical events mentioned in the assessment/plan
section.
Next, we conduct experiments using automati-
cally extracted features. This is done as follows. The
medical events are extracted using MetaMap, which
recognizes medical concepts and codes them using
35
Class(time-bin) MaxEnt CRF
P R P R
way before admission (wa) 72.4 63.5 79.8 66.7
before admission (ba) 83.4 80.8 92.0 92.4
on admission (a) 76.6 72.1 87.5 75.2
after admission (aa) 88.6 82.1 93.6 99.1
after discharge (ad) 85.2 58.7 94.3 62.5
Table 3: Per-class precision (P) and recall (R) for medical
events, time-bins using hand-tagged extracted features.
UMLS (Aronson, 2001). Based on this UMLS code,
we can extract the semantic category associated with
the code. Compared to the 1854 medical events
marked by the annotators, MetaMap identifies 1257
medical events, which are a subset of the 1854. The
UMLS coding by the annotators is more contextu-
ally relevant and precise. We use a rule-based al-
gorithm to identify and extract document structure
based features such as sections from clinical narra-
tives. The rules are formulated based on commonly
occurring sections in our corpus. We extract lines
that are all upper-case, and longer than a word and
use their stemmed representation to sort them by fre-
quency of occurrence in the corpus. While parsing
the text in each clinical narrative, on encountering
a line that matches a section title from the frequent
list, all subsequent lines are associated with that title
until a new section title is encountered. In case of the
lexical features, we extract bigrams and calculate the
tense of the verb preceding the medical event using
the Stanford NLP software.3 The temporal features
are extracted with the help of TimeText developed
by Zhou and Hripcsak (2007) that automatically an-
notates temporal expressions in clinical text. How-
ever, it is not able to capture many of the implicit
temporal references. Following this, a temporal ex-
pression is linked to a medical event if it occurs in
the same sentence as the medical event.
The average precision and recall of the Max-
Ent model using automatically extracted features is
74.3% and 66.5% respectively. Sequence tagging
using CRFs gives us an average precision and recall
of 79.6% and 69.7% respectively. Although the re-
sults are not as good as using hand-tagged features,
they are certainly promising. One reason for the loss
in accuracy could be because the automatically cal-
culated temporal features are not as precise as the
3http://nlp.stanford.edu/software/
Gold-standard Features
P R
ME 81.2 71.4
CRF 89.4 79.2
CRF(no temp. feats) 79.5 67.2
CRF(no section feats) 82.7 72.3
Automatic Features
P R
ME 74.3 66.5
CRF 79.6 69.7
Baseline (P;R) 58.02 60.26
Table 4: Overall Result Summary: Average precision
(P) and recall (R) with manually annotated gold-standard
features, automatically extracted features and the base-
line.
hand-tagged ones. These results are summarized in
Table 4.
9 Conclusion
We investigate the task of classifying medical events
in clinical narratives to coarse time-bins. We de-
scribe document structure based, lexical and tempo-
ral features in clinical text and explain how these
feature are useful in time-binning medical events.
The extracted feature-set when used in a sequence
tagging framework with CRFs gives us high accu-
racy when compared with a section-based baseline
or a MaxEnt model. The learned time-bins can
be used as an informative feature for tasks such as
fine-grained ordering of medical events and medical
event coreference resolution. We also experiment
with hand-tagged vs. automatically extracted fea-
tures for this task and observe that while automati-
cally extracted features show promising results, they
are not as good as using hand-tagged features for this
task.
Acknowledgments
The project described was supported by the
National Center for Research Resources,
Grant UL1RR025755, KL2RR025754, and
TL1RR025753, and is now at the National
Center for Advancing Translational Sciences,
Grant 8KL2TR000112-05, 8UL1TR000090-05,
8TL1TR000091-05. The content is solely the re-
sponsibility of the authors and does not necessarily
represent the official views of the NIH.
36
References
James F. Allen. 1981. An interval-based representation
of temporal knowledge. In IJCAI, pages 221?226.
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. Proc of AMIA Symposium, pages 17?21.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22:39?71.
Mary Regina Boland, Samson W. Tu, Simona Carini, Ida
Sim, and Chunhua Weng. 2012. EliXR: An Approach
to Eligibility Criteria Extraction and Representation.
Proc of AMIA Clinical Research Informatics Summit.
Anthony J. Conger. 1980. Integration and generalization
of kappas for multiple raters. In Psychological Bul-
letin Vol 88(2), pages 322?328.
Rob Gaizauskas, Henk Harkema, Mark Hepple, and An-
drea Setzer. 2006. Task-oriented extraction of tem-
poral information: The case of clinical narratives.
In Proceedings of the Thirteenth International Sym-
posium on Temporal Representation and Reasoning,
TIME ?06, pages 188?195.
Hyuckchul Jung, James Allen, Nate Blaylock, Will
de Beaumont, Lucian Galescu, and Mary Swift. 2011.
Building timelines from narrative clinical records: ini-
tial results based-on deep natural language under-
standing. In Proceedings of BioNLP 2011 Workshop,
BioNLP ?11, pages 146?154.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning, ICML ?01, pages 282?
289.
Ying Li, Sharon Lipsky Gorman, and Noemie Elhadad.
2010. Section classification in clinical notes using su-
pervised hidden markov model. In IHI, pages 744?
750.
James Pustejovsky, Jose? M. Castan?o, Robert Ingria,
Roser Sauri, Robert J. Gaizauskas, Andrea Setzer,
Graham Katz, and Dragomir R. Radev. 2003.
TimeML: Robust specification of event and temporal
expressions in text. In New Directions in Question An-
swering ?03, pages 28?34.
Guergana Savova, Steven Bethard, Will Styler, James
Martin, Martha Palmer, James Masanz, and Wayne
Ward. 2009. Towards temporal relation discovery
from the clinical narrative. Proc of AMIA Symposium,
pages 568?572.
Marc Verhagen, Robert J. Gaizauskas, Frank Schilder,
Mark Hepple, Jessica Moszkowicz, and James Puste-
jovsky. 2009. The TempEval challenge: identifying
temporal relations in text. Language Resources and
Evaluation, 43(2):161?179.
Li Zhou and George Hripcsak. 2007. Temporal rea-
soning with medical data - a review with emphasis
on medical natural language processing. Journal of
Biomedical Informatics, pages 183?202.
Li Zhou, Genevieve B. Melton, Simon Parsons, and
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clinical
narrative. Journal of Biomedical Informatics, pages
424?439.
37
