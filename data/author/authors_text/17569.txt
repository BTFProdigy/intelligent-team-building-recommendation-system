Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 592?600,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Error Mining on Dependency Trees
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Shashi Narayan
Universite? de Lorraine, LORIA, UMR 7503
Villers-le`s-Nancy, F-54600, France
shashi.narayan@loria.fr
Abstract
In recent years, error mining approaches were
developed to help identify the most likely
sources of parsing failures in parsing sys-
tems using handcrafted grammars and lexi-
cons. However the techniques they use to enu-
merate and count n-grams builds on the se-
quential nature of a text corpus and do not eas-
ily extend to structured data. In this paper, we
propose an algorithm for mining trees and ap-
ply it to detect the most likely sources of gen-
eration failure. We show that this tree mining
algorithm permits identifying not only errors
in the generation system (grammar, lexicon)
but also mismatches between the structures
contained in the input and the input structures
expected by our generator as well as a few id-
iosyncrasies/error in the input data.
1 Introduction
In recent years, error mining techniques have been
developed to help identify the most likely sources
of parsing failure (van Noord, 2004; Sagot and de la
Clergerie, 2006; de Kok et al, 2009). First, the input
data (text) is separated into two subcorpora, a corpus
of sentences that could be parsed (PASS) and a cor-
pus of sentences that failed to be parsed (FAIL). For
each n-gram of words (and/or part of speech tag) oc-
curring in the corpus to be parsed, a suspicion rate is
then computed which, in essence, captures the like-
lihood that this n-gram causes parsing to fail.
These error mining techniques have been applied
with good results on parsing output and shown to
help improve the large scale symbolic grammars and
lexicons used by the parser. However the techniques
they use (e.g., suffix arrays) to enumerate and count
n-grams builds on the sequential nature of a text cor-
pus and cannot easily extend to structured data.
There are some NLP applications though where
the processed data is structured data such as trees
or graphs and which would benefit from error min-
ing. For instance, when generating sentences from
dependency trees, as was proposed recently in the
Generation Challenge Surface Realisation Task (SR
Task, (Belz et al, 2011)), it would be useful to be
able to apply error mining on the input trees to find
the most likely causes of generation failure.
In this paper, we address this issue and propose
an approach that supports error mining on trees. We
adapt an existing algorithm for tree mining which we
then use to mine the Generation Challenge depen-
dency trees and identify the most likely causes of
generation failure. We show in particular, that this
tree mining algorithm permits identifying not only
errors in the grammar and the lexicon used by gener-
ation but also a few idiosyncrasies/error in the input
data as well as mismatches between the structures
contained in the SR input and the input structures
expected by our generator. The latter is an impor-
tant point since, for symbolic approaches, a major
hurdle to participation in the SR challenge is known
to be precisely these mismatches i.e., the fact that
the input provided by the SR task fails to match the
input expected by the symbolic generation systems
(Belz et al, 2011).
The paper is structured as follows. Section 2
presents the HybridTreeMiner algorithm, a complete
and computationally efficient algorithm developed
592
AB
CD
B
C
A
B
DC
B
C
A
B
C
B
CD
A
B
C
B
DC
Figure 1: Four unordered labelled trees. The right-
most is in Breadth-First Canonical Form
by (Chi et al, 2004) for discovering frequently oc-
curring subtrees in a database of labelled unordered
trees. Section 3 shows how to adapt this algorithm
to mine the SR dependency trees for subtrees with
high suspicion rate. Section 4 presents an experi-
ment we made using the resulting tree mining algo-
rithm on SR dependency trees and summarises the
results. Section 5 discusses related work. Section 6
concludes.
2 Mining Trees
Mining for frequent subtrees is an important prob-
lem that has many applications such as XML data
mining, web usage analysis and RNA classification.
The HybridTreeMiner (HTM) algorithm presented
in (Chi et al, 2004) provides a complete and com-
putationally efficient method for discovering fre-
quently occurring subtrees in a database of labelled
unordered trees and counting them. We now sketch
the intuition underlying this algorithm1. In the next
section, we will show how to modify this algorithm
to mine for errors in dependency trees.
Given a set of trees T , the HybridTreeMiner al-
gorithm proceeds in two steps. First, the unordered
labelled trees contained in T are converted to a
canonical form called BFCF (Breadth-First Canoni-
cal Form). In that way, distinct instantiations of the
same unordered trees have a unique representation.
Second, the subtrees of the BFCF trees are enumer-
ated in increasing size order using two tree opera-
tions called join and extension and their support (the
number of trees in the database that contains each
subtree) is recorded. In effect, the algorithm builds
an enumeration tree whose nodes are the possible
subtrees of T and such that, at depth d of this enu-
meration tree, all possible frequent subtrees consist-
ing of d nodes are listed.
1For a more complete definition see (Chi et al, 2004).
The BFCF canonical form of an unordered tree
is an ordered tree t such that t has the smallest
breath-first canonical string (BFCS) encoding ac-
cording to lexicographic order. The BFCS encod-
ing of a tree is obtained by breadth-first traver-
sal of the tree, recording the string labelling each
node, ?$? to separate siblings with distinct parents
and ?#? to represent the end of the tree2. For in-
stance, the BFCS encodings of the four trees shown
in Figure 1 are ?A$BB$C$DC#?, ?A$BB$C$CD#?,
?A$BB$DC$C#? and ?A$BB$CD$C#? respectively.
Hence, the rightmost tree is the BFCF of all four
trees.
The join and extension operations used to itera-
tively enumerate subtrees are depicted in Figure 2
and can be defined as follows.
? A leg is a leaf of maximal depth.
? Extension: Given a tree t of height ht and a
node n, extending t with n yields a tree t? (a
child of t in the enumeration tree) with height
ht? such that n is a child of one of t?s legs and
ht? is ht + 1.
? Join: Given two trees t1 and t2 of same height
h differing only in their rightmost leg and such
that t1 sorts lower than t2, joining t1 and t2
yields a tree t? (a child of t1 in the enumeration
tree) of same height h by adding the rightmost
leg of t2 to t1 at level h? 1.
A
CB
D + E ?Extension
A
CB
D
E
A
CB
D +
A
C
E
B
?Join
A
C
E
B
D
Figure 2: Join and Extension Operations
To support counting, the algorithm additionally
records for each subtree a list (called occurrence list)
2Assuming ?#? sorts greater than ?$? and both sort greater
than any other alphabets in node labels.
593
of all trees in which this subtree occurs and of its po-
sition in the tree (represented by the list of tree nodes
mapped onto by the subtree). Thus for a given sub-
tree t, the support of t is the number of elements
in that list. Occurrence lists are also used to check
that trees that are combined occur in the data. For
the join operation, the subtrees being combined must
occur in the same tree at the same position (the inter-
section of their occurrence lists must be non empty
and the tree nodes must match except the last node).
For the extension operation, the extension of a tree
t is licensed for any given occurrence in the occur-
rence list only if the planned extension maps onto
the tree identified by the occurrence.
3 Mining Dependency Trees
We develop an algorithm (called ErrorTreeMiner,
ETM) which adapts the HybridTreeMiner algorithm
to mine sources of generation errors in the Gener-
ation Challenge SR shallow input data. The main
modification is that instead of simply counting trees,
we want to compute their suspicion rate. Following
(de Kok et al, 2009), we take the suspicion rate of a
given subtree t to be the proportion of cases where t
occurs in an input tree for which generation fails:
Sus(t) =
count(t|FAIL)
count(t)
where count(t) is the number of occurrences of
t in all input trees and count(t|FAIL) is the number
of occurrences of t in input trees for which no output
was produced.
Since we work with subtrees of arbitrary length,
we also need to check whether constructing a longer
subtree is useful that is, whether its suspicion rate
is equal or higher than the suspicion rate of any of
the subtrees it contains. In that way, we avoid com-
puting all subtrees (thus saving time and space). As
noted in (de Kok et al, 2009), this also permits by-
passing suspicion sharing that is the fact that, if n2
is the cause of a generation failure, and if n2 is con-
tained in larger trees n3 and n4, then all three trees
will have high suspicion rate making it difficult to
identify the actual source of failure namely n2. Be-
cause we use a milder condition however (we accept
bigger trees whose suspicion rate is equal to the sus-
picion rate of any of their subtrees), some amount of
Algorithm 1 ErrorTreeMiner(D,minsup)
Note: D consists of Dfail and Dpass
F1 ? {Frequent 1-trees}
F2 ? ?
for i? 1, ..., |F1| do
for j ? 1, ..., |F1| do
q ? fi plus legfj
if Noord-Validation(q,minsup) then
F2 ? F2 ? q
end if
end for
end for
F ? F1 ? F2
PUSH: sort(F2)? LQueue
Enum-Grow(LQueue, F,minsup)
return F
Algorithm 2 Enum-Grow(LQueue, F,minsup)
while LQueue 6= empty do
POP: pop(LQueue)? C
for i? 1, ..., |C| do
The join operation
J ? ?
for j ? i, ..., |C| do
p? join(ci, cj)
if Noord-Validation(p,minsup) then
J ? J ? p
end if
end for
F ? F ? J
PUSH: sort(J)? LQueue
The extension operation
E ? ?
for possible leg lm of ci do
for possible new leg ln(? F1) do
q ? extend ci with ln at position lm
if Noord-Validation(q,minsup) then
E ? E ? q
end if
end for
end for
F ? F ? E
PUSH: sort(E)? LQueue
end for
end while
594
Algorithm 3 Noord-Validation(tn,minsup)
Note: tn, tree with n nodes
if Sup(tn) ? minsup then
if Sus(tn) ? Sus(tn?1),?tn?1 in tn then
return true
end if
end if
return false
suspicion sharing remains. As we shall see in Sec-
tion 4.3.2, relaxing this check though allows us to
extract frequent larger tree patterns and thereby get
a more precise picture of the context in which highly
suspicious items occur.
Finally, we only keep subtrees whose support is
above a given threshold where the support Sup(t)
of a tree t is defined as the ratio between the number
of times it occurs in an input for which generation
fails and the total number of generation failures:
Sup(t) =
count(t|FAIL)
count(FAIL)
The modified algorithm we use for error mining is
given in Algorithm 1, 2 and 3. It can be summarised
as follows.
First, dependency trees are converted to Breadth-
First Canonical Form whereby lexicographic order
can apply to the word forms labelling tree nodes, to
their part of speech, to their dependency relation or
to any combination thereof3.
Next, the algorithm iteratively enumerates the
subtrees occurring in the input data in increasing
size order and associating each subtree t with two
occurrence lists namely, the list of input trees in
which t occurs and for which generation was suc-
cessful (PASS(t)); and the list of input trees in which
t occurs and for which generation failed (FAIL(t)).
This process is initiated by building trees of size
one (i.e., one-node tree) and extending them to trees
of size two. It is then continued by extending the
trees using the join and extension operations. As
explained in Section 2 above, join and extension
only apply provided the resulting trees occur in the
data (this is checked by looking up occurrence lists).
3For convenience, the dependency relation labelling the
edges of dependency trees is brought down to the daughter node
of the edge.
Each time an n-node tree tn, is built, it is checked
that (i) its support is above the set threshold and (ii)
its suspicion rate is higher than or equal to the sus-
picion rate of all (n? 1)-node subtrees of tn.
In sum, the ETM algorithm differs from the HTM
algorithm in two main ways. First, while HTM ex-
plores the enumeration tree depth-first, ETM pro-
ceeds breadth-first to ensure that the suspicion rate
of (n-1)-node trees is always available when check-
ing whether an n-node tree should be introduced.
Second, while the HTM algorithm uses support to
prune the search space (only trees with a minimum
support bigger than the set threshold are stored), the
ETM algorithm drastically prunes the search space
by additionally checking that the suspicion rate of
all subtrees contained in a new tree t is smaller or
equal to the suspicion rate of t . As a result, while
ETM looses the space advantage of HTM by a small
margin4, it benefits from a much stronger pruning of
the search space than HTM through suspicion rate
checking. In practice, the ETM algorithm allows us
to process e.g., all NP chunks of size 4 and 6 present
in the SR data (roughly 60 000 trees) in roughly 20
minutes on a PC.
4 Experiment and Results
Using the input data provided by the Generation
Challenge SR Task, we applied the error mining al-
gorithm described in the preceding Section to debug
and extend a symbolic surface realiser developed for
this task.
4.1 Input Data and Surface Realisation System
The shallow input data provided by the SR Task
was obtained from the Penn Treebank using the
LTH Constituent-to-Dependency Conversion Tool
for Penn-style Treebanks (Pennconverter, (Johans-
son and Nugues, 2007)). It consists of a set
of unordered labelled syntactic dependency trees
whose nodes are labelled with word forms, part of
speech categories, partial morphosyntactic informa-
tion such as tense and number and, in some cases, a
sense tag identifier. The edges are labelled with the
syntactic labels provided by the Pennconverter. All
words (including punctuation) of the original sen-
4ETM needs to store all (n-1)-node trees in queues before
producing n-node trees.
595
tence are represented by a node in the tree and the
alignment between nodes and word forms was pro-
vided by the organisers.
The surface realiser used is a system based on
a Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG) for English extended with a unifica-
tion based compositional semantics. Both the gram-
mars and the lexicon were developed in view of the
Generation Challenge and the data provided by this
challenge was used as a means to debug and extend
the system. Unknown words are assigned a default
TAG family/tree based on the part of speech they
are associated with in the SR data. The surface real-
isation algorithm extends the algorithm proposed in
(Gardent and Perez-Beltrachini, 2010) and adapts it
to work on the SR dependency input rather than on
flat semantic representations.
4.2 Experimental Setup
To facilitate interpretation, we first chunked the in-
put data in NPs, PPs and Clauses and performed er-
ror mining on the resulting sets of data. The chunk-
ing was performed by retrieving from the Penn Tree-
bank (PTB), for each phrase type, the yields of the
constituents of that type and by using the alignment
between words and dependency tree nodes provided
by the organisers of the SR Task. For instance, given
the sentence ?The most troublesome report may be
the August merchandise trade deficit due out tomor-
row?, the NPs ?The most troublesome report? and
?the August merchandise trade deficit due out to-
morrow? will be extracted from the PTB and the
corresponding dependency structures from the SR
Task data.
Using this chunked data, we then ran the genera-
tor on the corresponding SR Task dependency trees
and stored separately, the input dependency trees for
which generation succeeded and the input depen-
dency trees for which generation failed. Using infor-
mation provided by the generator, we then removed
from the failed data, those cases where generation
failed either because a word was missing in the lex-
icon or because a TAG tree/family was missing in
the grammar but required by the lexicon and the in-
put data. These cases can easily be detected using
the generation system and thus do not need to be
handled by error mining.
Finally, we performed error mining on the data
using different minimal support thresholds, differ-
ent display modes (sorted first by size and second by
suspicion rate vs sorted by suspicion rate) and differ-
ent labels (part of speech, words and part of speech,
dependency, dependency and part of speech).
4.3 Results
One feature of our approach is that it permits min-
ing the data for tree patterns of arbitrary size us-
ing different types of labelling information (POS
tags, dependencies, word forms and any combina-
tion thereof). In what follows, we focus on the NP
chunk data and illustrate by means of examples how
these features can be exploited to extract comple-
mentary debugging information from the data.
4.3.1 Mining on single labels (word form, POS
tag or dependency)
Mining on a single label permits (i) assessing the
relative impact of each category in a given label cat-
egory and (ii) identifying different sources of errors
depending on the type of label considered (POS tag,
dependency or word form).
Mining on POS tags Table 1 illustrates how min-
ing on a single label (in this case, POS tags) gives
a good overview of how the different categories in
that label type impact generation: two POS tags
(POS and CC) have a suspicion rate of 0.99 indicat-
ing that these categories always lead generation to
fail. Other POS tag with much lower suspicion rate
indicate that there are unresolved issues with, in de-
creasing order of suspicion rate, cardinal numbers
(CD), proper names (NNP), nouns (NN), prepositions
(IN) and determiners (DT).
The highest ranking category (POS5) points to
a mismatch between the representation of geni-
tive NPs (e.g., John?s father) in the SR Task data
and in the grammar. While our generator ex-
pects the representation of ?John?s father? to be FA-
THER(?S?(JOHN)), the structure provided by the SR
Task is FATHER(JOHN(?S?)). Hence whenever a
possessive appears in the input data, generation fails.
This is in line with (Rajkumar et al, 2011)?s finding
that the logical forms expected by their system for
possessives differed from the shared task inputs.
5In the Penn Treebank, the POS tag is the category assigned
to possessive ?s.
596
POS Sus Sup Fail Pass
POS 0.99 0.38 3237 1
CC 0.99 0.21 1774 9
CD 0.39 0.16 1419 2148
NNP 0.35 0.32 2749 5014
NN 0.30 0.81 6798 15663
IN 0.30 0.16 1355 3128
DT 0.09 0.12 1079 10254
Table 1: Error Mining on POS tags with frequency
cutoff 0.1 and displaying only trees of size 1 sorted
by decreasing suspicion rate (Sus)
The second highest ranked category is CC for co-
ordinations. In this case, error mining unveils a
bug in the grammar trees associated with conjunc-
tion which made all sentences containing a conjunc-
tion fail. Because the grammar is compiled out of
a strongly factorised description, errors in this de-
scription can propagate to a large number of trees
in the grammar. It turned out that an error occurred
in a class inherited by all conjunction trees thereby
blocking the generation of any sentence requiring
the use of a conjunction.
Next but with a much lower suspicion rate come
cardinal numbers (CD), proper names (NNP), nouns
(NN), prepositions (IN) and determiners (DT). We
will see below how the richer information provided
by mining for larger tree patterns with mixed la-
belling information permits identifying the contexts
in which these POS tags lead to generation failure.
Mining on Word Forms Because we remove
from the failure set al cases of errors due to a miss-
ing word form in the lexicon, a high suspicion rate
for a word form usually indicates a missing or incor-
rect lexical entry: the word is present in the lexicon
but associated with either the wrong POS tag and/or
the wrong TAG tree/family. To capture such cases,
we therefore mine not on word forms alone but on
pairs of word forms and POS tag. In this way, we
found for instance, that cardinal numbers induced
many generation failures whenever they were cate-
gorised as determiners but not as nouns in our lexi-
con. As we will see below, larger tree patterns help
identify the specific contexts inducing such failures.
One interesting case stood out which pointed to
idiosyncrasies in the input data: The word form $
(Sus=1) was assigned the POS tag $ in the input
data, a POS tag which is unknown to our system and
not documented in the SR Task guidelines. The SR
guidelines specify that the Penn Treebank tagset is
used modulo the modifications which are explicitly
listed. However for the $ symbol, the Penn treebank
used SYM as a POS tag and the SR Task $, but the
modification is not listed. Similarly, while in the
Penn treebank, punctuations are assigned the SYM
POS tag, in the SR data ?,? is used for the comma,
?(? for an opening bracket and so on.
Mining on Dependencies When mining on de-
pendencies, suspects can point to syntactic construc-
tions (rather than words or word categories) that are
not easily spotted when mining on words or parts
of speech. Thus, while problems with coordination
could easily be spotted through a high suspicion rate
for the CC POS tag, some constructions are linked
neither to a specific POS tag nor to a specific word.
This is the case, for instance, for apposition which
a suspicion rate of 0.19 (286F/1148P) identified as
problematic. Similarly, a high suspicion rate (0.54,
183F/155P) on the TMP dependency indicates that
temporal modifiers are not correctly handled either
because of missing or erroneous information in the
grammar or because of a mismatch between the in-
put data and the fomat expected by the surface re-
aliser.
Interestingly, the underspecified dependency rela-
tion DEP which is typically used in cases for which
no obvious syntactic dependency comes to mind
shows a suspicion rate of 0.61 (595F/371P).
4.3.2 Mining on trees of arbitrary size and
complex labelling patterns
While error mining with tree patterns of size one
permits ranking and qualifying the various sources
of errors, larger patterns often provide more detailed
contextual information about these errors. For in-
stance, Table 1 shows that the CD POS tag has a
suspicion rate of 0.39 (1419F/2148P). The larger
tree patterns identified below permits a more specific
characterization of the context in which this POS tag
co-occurs with generation failure:
TP1 CD(IN,RBR) more than 10
TP2 IN(CD) of 1991
TP3 NNP(CD) November 1
TP4 CD(NNP(CD)) Nov. 1, 1997
597
Two patterns clearly emerge: a pattern where car-
dinal numbers are parts of a date (tree patterns TP2-
TP4) and a more specific pattern (TP1) involving
the comparative construction (e.g., more than 10).
All these patterns in fact point to a missing category
for cardinals in the lexicon: they are only associated
with determiner TAG trees, not nouns, and therefore
fail to combine with prepositions (e.g., of 1991, than
10) and with proper names (e.g., November 1).
For proper names (NNP), dates also show up be-
cause months are tagged as proper names (TP3,TP4)
as well as addresses TP5:
TP5 NNP(?,?,?,?) Brooklyn, n.y.,
For prepositions (IN), we find, in addition to the
TP1-TP2, the following two main patterns:
TP6 DT(IN) those with, some of
TP7 RB(IN) just under, little more
Pattern TP6 points to a missing entry for words
such as those and some which are categorised in the
lexicon as determiners but not as nouns. TP7 points
to a mismatch between the SR data and the format
expected by the generator: while the latter expects
the structure IN(RB), the input format provided by
the SR Task is RB(IN).
4.4 Improving Generation Using the Results of
Error Mining
Table 2 shows how implementing some of the cor-
rections suggested by error mining impacts the num-
ber of NP chunks (size 4) that can be generated. In
this experiment, the total number of input (NP) de-
pendency trees is 24995. Before error mining, gen-
eration failed on 33% of these input. Correcting
the erroneous class inherited by all conjunction trees
mentioned in Section 4.3.1 brings generation failure
down to 26%. Converting the input data to the cor-
rect input format to resolve the mismatch induced
by possessive ?s (cf. Section 4.3.1) reduce gener-
ation failure to 21%6 and combining both correc-
tions results in a failure rate of 13%. In other words,
error mining permits quickly identifying two issues
which, once corrected, reduces generation failure by
20 points.
When mining on clause size chunks, other mis-
matches were identified such as in particular, mis-
matches introduced by subjects and auxiliaries:
6For NP of size 4, 3264 structures with possessive ?s were
rewritten.
NP 4 Before After
SR Data 8361 6511
Rewritten SR Data 5255 3401
Table 2: Diminishing the number of errors using in-
formation from error mining. The table compares
the number of failures on NP chunks of size 4 be-
fore (first row) and after (second row) rewriting the
SR data to the format expected by our generator and
before (second column) and after (third column) cor-
recting the grammar and lexicon errors discussed in
Section 4.3.1
while our generator expects both the subject and the
auxiliary to be children of the verb, the SR data rep-
resent the subject and the verb as children of the aux-
iliary.
5 Related Work
We now relate our proposal (i) to previous proposals
on error mining and (ii) to the use of error mining in
natural language generation.
Previous work on error mining. (van Noord,
2004) initiated error mining on parsing results with
a very simple approach computing the parsability
rate of each n-gram in a very large corpus. The
parsability rate of an n-gram wi . . . wn is the ratio
R(wi . . . wn) =
C(wi...wn|OK)
C(wi...wn)
with C(wi . . . wn)
the number of sentences in which the n-gram
wi . . . wn occurs and C(wi . . . wn | OK) the num-
ber of sentences containing wi . . . wn which could
be parsed. The corpus is stored in a suffix array
and the sorted suffixes are used to compute the fre-
quency of each n-grams in the total corpus and in the
corpus of parsed sentences. The approach was later
extended and refined in (Sagot and de la Clergerie,
2006) and (de Kok et al, 2009) whereby (Sagot and
de la Clergerie, 2006) defines a suspicion rate for n-
grams which takes into account the number of occur-
rences of a given word form and iteratively defines
the suspicion rate of each word form in a sentence
based on the suspicion rate of this word form in the
corpus; (de Kok et al, 2009) combined the iterative
error mining proposed by (Sagot and de la Clergerie,
2006) with expansion of forms to n-grams of words
and POS tags of arbitrary length.
Our approach differs from these previous ap-
598
proaches in several ways. First, error mining is per-
formed on trees. Second, it can be parameterised to
use any combination of POS tag, dependency and/or
word form information. Third, it is applied to gener-
ation input rather than parsing output. Typically, the
input to surface realisation is a structured represen-
tation (i.e., a flat semantic representation, a first or-
der logic formula or a dependency tree) rather than a
string. Mining these structured representations thus
permits identifying causes of undergeneration in sur-
face realisation systems.
Error Mining for Generation Not much work
has been done on mining the results of surface re-
alisers. Nonetheless, (Gardent and Kow, 2007) de-
scribes an error mining approach which works on
the output of surface realisation (the generated sen-
tences), manually separates correct from incorrect
output and looks for derivation items which system-
atically occur in incorrect output but not in correct
ones. In contrast, our approach works on the input
to surface realisation, automatically separates cor-
rect from incorrect items using surface realisation
and targets the most likely sources of errors rather
than the absolute ones.
More generally, our approach is the first to our
knowledge, which mines a surface realiser for un-
dergeneration. Indeed, apart from (Gardent and
Kow, 2007), most previous work on surface reali-
sation evaluation has focused on evaluating the per-
formance and the coverage of surface realisers. Ap-
proaches based on reversible grammars (Carroll et
al., 1999) have used the semantic formulae output
by parsing to evaluate the coverage and performance
of their realiser; similarly, (Gardent et al, 2010) de-
veloped a tool called GenSem which traverses the
grammar to produce flat semantic representations
and thereby provide a benchmark for performance
and coverage evaluation. In both cases however, be-
cause it is produced using the grammar exploited by
the surface realiser, the input produced can only be
used to test for overgeneration (and performance) .
(Callaway, 2003) avoids this shortcoming by con-
verting the Penn Treebank to the format expected by
his realiser. However, this involves manually iden-
tifying the mismatches between two formats much
like symbolic systems did in the Generation Chal-
lenge SR Task. The error mining approach we pro-
pose helps identifying such mismatches automati-
cally.
6 Conclusion
Previous work on error mining has focused on appli-
cations (parsing) where the input data is sequential
working mainly on words and part of speech tags.
In this paper, we proposed a novel approach to error
mining which permits mining trees. We applied it
to the input data provided by the Generation Chal-
lenge SR Task. And we showed that this supports
the identification of gaps and errors in the grammar
and in the lexicon; and of mismatches between the
input data format and the format expected by our re-
aliser.
We applied our error mining approach to the in-
put of a surface realiser to identify the most likely
sources of undergeneration. We plan to also ex-
plore how it can be used to detect the most likely
sources of overgeneration based on the output of
this surface realiser on the SR Task data. Using the
Penn Treebank sentences associated with each SR
Task dependency tree, we will create the two tree
sets necessary to support error mining by dividing
the set of trees output by the surface realiser into a
set of trees (FAIL) associated with overgeneration
(the generated sentences do not match the original
sentences) and a set of trees (SUCCESS) associated
with success (the generated sentence matches the
original sentences). Exactly which tree should popu-
late the SUCCESS and FAIL set is an open question.
The various evaluation metrics used by the SR Task
(BLEU, NIST, METEOR and TER) could be used
to determine a threshold under which an output is
considered incorrect (and thus classificed as FAIL).
Alternatively, a strict matching might be required.
Similarly, since the surface realiser is non determin-
istic, the number of output trees to be kept will need
to be experimented with.
Acknowledgments
We would like to thank Cle?ment Jacq for useful dis-
cussions on the hybrid tree miner algorithm. The
research presented in this paper was partially sup-
ported by the European Fund for Regional Develop-
ment within the framework of the INTERREG IV A
Allegro Project.
599
References
Anja Belz, Michael White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and evalu-
ation results. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In Proceedings of the
18th International Joint Conference on Artificial Intel-
ligence, pages 811?817, Acapulco, Mexico.
John Carroll, Ann Copestake, Dan Flickinger, and Vik-
tor Paznan?ski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of the
7th European Workshop on Natural Language Gener-
ation, pages 86?95, Toulouse, France.
Yun Chi, Yirong Yang, and Richard R. Muntz. 2004.
Hybridtreeminer: An efficient algorithm for mining
frequent rooted trees and free trees using canonical
form. In Proceedings of the 16th International Con-
ference on and Statistical Database Management (SS-
DBM), pages 11?20, Santorini Island, Greece. IEEE
Computer Society.
Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error mining
in parsing results. In Proceedings of the 2009 Work-
shop on Grammar Engineering Across Frameworks
(GEAF 2009), pages 71?79, Suntec, Singapore. As-
sociation for Computational Linguistics.
Claire Gardent and Eric Kow. 2007. Spotting overgen-
eration suspect. In Proceedings of the 11th European
Workshop on Natural Language Generation (ENLG),
pages 41?48, Schloss Dagstuhl, Germany.
Claire Gardent and Laura Perez-Beltrachini. 2010. Rtg
based surface realisation for tag. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (COLING), pages 367?375, Beijing, China.
Claire Gardent, Benjamin Gottesman, and Laura Perez-
Beltrachini. 2010. Comparing the performance of
two TAG-based Surface Realisers using controlled
Grammar Traversal. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING - Poster session), pages 338?346, Beijing,
China.
Richert Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of the 16th Nordic Conference of Com-
putational Linguistics (NODALIDA), pages 105?112,
Tartu, Estonia.
Rajakrishnan Rajkumar, Dominic Espinosa, and Michael
White. 2011. The osu system for surface realization
at generation challenges 2011. In Proceedings of the
13th European Workshop on Natural Language Gen-
eration (ENLG), pages 236?238, Nancy, France.
Beno??t Sagot and E?ric de la Clergerie. 2006. Error min-
ing in parsing results. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 329?336, Sydney,
Australia.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL), pages 446?453, Barcelona, Spain.
600
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 435?445,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Hybrid Simplification using Deep Semantics and Machine Translation
Shashi Narayan
Universite? de Lorraine, LORIA
Villers-le`s-Nancy, F-54600, France
shashi.narayan@loria.fr
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Abstract
We present a hybrid approach to sentence
simplification which combines deep se-
mantics and monolingual machine transla-
tion to derive simple sentences from com-
plex ones. The approach differs from pre-
vious work in two main ways. First, it
is semantic based in that it takes as in-
put a deep semantic representation rather
than e.g., a sentence or a parse tree. Sec-
ond, it combines a simplification model
for splitting and deletion with a monolin-
gual translation model for phrase substi-
tution and reordering. When compared
against current state of the art methods,
our model yields significantly simpler out-
put that is both grammatical and meaning
preserving.
1 Introduction
Sentence simplification maps a sentence to a sim-
pler, more readable one approximating its con-
tent. Typically, a simplified sentence differs from
a complex one in that it involves simpler, more
usual and often shorter, words (e.g., use instead
of exploit); simpler syntactic constructions (e.g.,
no relative clauses or apposition); and fewer mod-
ifiers (e.g., He slept vs. He also slept). In prac-
tice, simplification is thus often modeled using
four main operations: splitting a complex sen-
tence into several simpler sentences; dropping and
reordering phrases or constituents; substituting
words/phrases with simpler ones.
As has been argued in previous work, sentence
simplification has many potential applications. It
is useful as a preprocessing step for a variety of
NLP systems such as parsers and machine trans-
lation systems (Chandrasekar et al, 1996), sum-
marisation (Knight and Marcu, 2000), sentence
fusion (Filippova and Strube, 2008) and semantic
role labelling (Vickrey and Koller, 2008). It also
has wide ranging potential societal application as a
reading aid for people with aphasis (Carroll et al,
1999), for low literacy readers (Watanabe et al,
2009) and for non native speakers (Siddharthan,
2002).
There has been much work recently on de-
veloping computational frameworks for sentence
simplification. Synchronous grammars have been
used in combination with linear integer program-
ming to generate and rank all possible rewrites of
an input sentence (Dras, 1999; Woodsend and La-
pata, 2011). Machine Translation systems have
been adapted to translate complex sentences into
simple ones (Zhu et al, 2010; Wubben et al, 2012;
Coster and Kauchak, 2011). And handcrafted
rules have been proposed to model the syntactic
transformations involved in simplifications (Sid-
dharthan et al, 2004; Siddharthan, 2011; Chan-
drasekar et al, 1996).
In this paper, we present a hybrid approach to
sentence simplification which departs from this
previous work in two main ways.
First, it combines a model encoding probabil-
ities for splitting and deletion with a monolin-
gual machine translation module which handles
reordering and substitution. In this way, we ex-
ploit the ability of statistical machine translation
(SMT) systems to capture phrasal/lexical substi-
tution and reordering while relying on a dedi-
cated probabilistic module to capture the splitting
and deletion operations which are less well (dele-
tion) or not at all (splitting) captured by SMT ap-
proaches.
Second, our approach is semantic based. While
previous simplification approaches starts from ei-
ther the input sentence or its parse tree, our model
takes as input a deep semantic representation
namely, the Discourse Representation Structure
(DRS, (Kamp, 1981)) assigned by Boxer (Curran
et al, 2007) to the input complex sentence. As we
435
shall see in Section 4, this permits a linguistically
principled account of the splitting operation in that
semantically shared elements are taken to be the
basis for splitting a complex sentence into sev-
eral simpler ones; this facilitates completion (the
re-creation of the shared element in the split sen-
tences); and this provide a natural means to avoid
deleting obligatory arguments.
When compared against current state of the art
methods (Zhu et al, 2010; Woodsend and Lapata,
2011; Wubben et al, 2012), our model yields sig-
nificantly simpler output that is both grammatical
and meaning preserving.
2 Related Work
Earlier work on sentence simplification relied
on handcrafted rules to capture syntactic sim-
plification e.g., to split coordinated and subor-
dinated sentences into several, simpler clauses
or to model active/passive transformations (Sid-
dharthan, 2002; Chandrasekar and Srinivas, 1997;
Bott et al, 2012; Canning, 2002; Siddharthan,
2011; Siddharthan, 2010). While these hand-
crafted approaches can encode precise and linguis-
tically well-informed syntactic transformation (us-
ing e.g., detailed morphological and syntactic in-
formation), they are limited in scope to purely syn-
tactic rules and do not account for lexical simpli-
fications and their interaction with the sentential
context.
Using the parallel dataset formed by Simple En-
glish Wikipedia (SWKP)1 and traditional English
Wikipedia (EWKP)2, more recent work has fo-
cused on developing machine learning approaches
to sentence simplification.
Zhu et al (2010) constructed a parallel cor-
pus (PWKP) of 108,016/114,924 complex/simple
sentences by aligning sentences from EWKP and
SWKP and used the resulting bitext to train a sim-
plification model inspired by syntax-based ma-
chine translation (Yamada and Knight, 2001).
Their simplification model encodes the probabil-
ities for four rewriting operations on the parse
tree of an input sentences namely, substitution, re-
ordering, splitting and deletion. It is combined
with a language model to improve grammatical-
ity and the decoder translates sentences into sim-
1SWKP (http://simple.wikipedia.org) is a
corpus of simple texts targeting ?children and adults who are
learning English Language? and whose authors are requested
to ?use easy words and short sentences?.
2http://en.wikipedia.org
pler ones by greedily selecting the output sentence
with highest probability.
Using both the PWKP corpus developed by
Zhu et al (2010) and the edit history of Simple
Wikipedia, Woodsend and Lapata (2011) learn a
quasi synchronous grammar (Smith and Eisner,
2006) describing a loose alignment between parse
trees of complex and of simple sentences. Fol-
lowing Dras (1999), they then generate all possi-
ble rewrites for a source tree and use integer lin-
ear programming to select the most appropriate
simplification. They evaluate their model on the
same dataset used by Zhu et al (2010) namely,
an aligned corpus of 100/131 EWKP/SWKP sen-
tences and show that they achieve better BLEU
score. They also conducted a human evaluation
on 64 of the 100 test sentences and showed again
a better performance in terms of simplicity, gram-
maticality and meaning preservation.
In (Wubben et al, 2012; Coster and Kauchak,
2011), simplification is viewed as a monolingual
translation task where the complex sentence is the
source and the simpler one is the target. To ac-
count for deletions, reordering and substitution,
Coster and Kauchak (2011) trained a phrase based
machine translation system on the PWKP corpus
while modifying the word alignment output by
GIZA++ in Moses to allow for null phrasal align-
ments. In this way, they allow for phrases to be
deleted during translation. No human evaluation
is provided but the approach is shown to result in
statistically significant improvements over a tradi-
tional phrase based approach. Similarly, Wubben
et al (2012) use Moses and the PWKP data to train
a phrase based machine translation system aug-
mented with a post-hoc reranking procedure de-
signed to rank the output based on their dissim-
ilarity from the source. A human evaluation on
20 sentences randomly selected from the test data
indicates that, in terms of fluency and adequacy,
their system is judged to outperform both Zhu et
al. (2010) and Woodsend and Lapata (2011) sys-
tems.
3 Simplification Framework
We start by motivating our approach and explain-
ing how it relates to previous proposals w.r.t.,
the four main operations involved in simplifica-
tion namely, splitting, deletion, substitution and
reordering. We then introduce our framework.
436
Sentence Splitting. Sentence splitting is ar-
guably semantic based in that in many cases, split-
ting occurs when the same semantic entity partici-
pates in two distinct eventualities. For instance, in
example (1) below, the split is on the noun bricks
which is involved in two eventualities namely,
?being resistant to cold? and ?enabling the con-
struction of permanent buildings?.
(1) C. Being more resistant to cold, bricks enabled the con-
struction of permanent buildings.
S. Bricks were more resistant to cold. Bricks enabled
the construction of permanent buildings.
While splitting opportunities have a clear coun-
terpart in syntax (i.e., splitting often occurs when-
ever a relative, a subordinate or an appositive
clause occurs in the complex sentence), comple-
tion i.e., the reconstruction of the shared element
in the second simpler clause, is arguably seman-
tically governed in that the reconstructed element
corefers with its matching phrase in the first sim-
pler clause. While our semantic based approach
naturally accounts for this by copying the phrase
corresponding to the shared entity in both phrases,
syntax based approach such as Zhu et al (2010)
and Woodsend and Lapata (2011) will often fail to
appropriately reconstruct the shared phrase and in-
troduce agreement mismatches because the align-
ment or rules they learn are based on syntax alone.
For instance, in example (2), Zhu et al (2010)
fails to copy the shared argument ?The judge? to
the second clause whereas Woodsend and Lapata
(2011) learns a synchronous rule matching (VP
and VP) to (VP. NP(It) VP) thereby failing to pro-
duce the correct subject pronoun (?he? or ?she?)
for the antecedent ?The judge?.
(2) C. The judge ordered that Chapman should receive
psychiatric treatment in prison and sentenced him to
twenty years to life.
S
1
. The judge ordered that Chapman should get psychi-
atric treatment. In prison and sentenced him to twenty
years to life. (Zhu et al, 2010)
S
2
. The judge ordered that Chapman should receive
psychiatric treatment in prison. It sentenced him to
twenty years to life. (Woodsend and Lapata, 2011)
Deletion. By handling deletion using a proba-
bilistic model trained on semantic representations,
we can avoid deleting obligatory arguments. Thus
in our approach, semantic subformulae which are
related to a predicate by a core thematic roles (e.g.,
agent and patient) are never considered for dele-
tion. By contrast, syntax based approaches (Zhu
et al, 2010; Woodsend and Lapata, 2011) do not
distinguish between optional and obligatory argu-
ments. For instance Zhu et al (2010) simplifies
(3C) to (3S) thereby incorrectly deleting the oblig-
atory theme (gifts) of the complex sentence and
modifying its meaning to giving knights and war-
riors (instead of giving gifts to knights and war-
riors).
(3) C. Women would also often give knights and warriors
gifts that included thyme leaves as it was believed to
bring courage to the bearer.
S. Women also often give knights and warriors. Gifts
included thyme leaves as it was thought to bring
courage to the saint. (Zhu et al, 2010)
We also depart from Coster and Kauchak (2011)
who rely on null phrasal alignments for deletion
during phrase based machine translation. In their
approach, deletion is constrained by the training
data and the possible alignments, independent of
any linguistic knowledge.
Substitution and Reordering SMT based ap-
proaches to paraphrasing (Barzilay and Elhadad,
2003; Bannard and Callison-Burch, 2005) and to
sentence simplification (Wubben et al, 2012) have
shown that by utilising knowledge about align-
ment and translation probabilities, SMT systems
can account for the substitutions and the reorder-
ings occurring in sentence simplification. Fol-
lowing on these approaches, we therefore rely on
phrase based SMT to learn substitutions and re-
ordering. In addition, the language model we in-
tegrate in the SMT module helps ensuring better
fluency and grammaticality.
3.1 An Example
Figure 1 shows how our approach simplifies (4C)
into (4S).
(4) C. In 1964 Peter Higgs published his second paper in
Physical Review Letters describing Higgs mechanism
which predicted a new massive spin-zero boson for the
first time.
S. Peter Higgs wrote his paper explaining Higgs mech-
anism in 1964. Higgs mechanism predicted a new ele-
mentary particle.
The DRS for (4C) produced using Boxer (Cur-
ran et al, 2007) is shown at the top of the Figure
and a graph representation3 of the dependencies
between its variables is shown immediately below.
Each DRS variable labels a node in the graph and
each edge is labelled with the relation holding be-
tween the variables labelling its end vertices. The
3The DRS to graph conversion goes through several pre-
processing steps: the relation nn is inverted making modi-
fier noun (higgs) dependent of modified noun (mechanism),
named and timex are converted to unary predicates, e.g.,
named(x, peter) is mapped to peter(x) and timex(x) =
1964 is mapped to 1964(x); and nodes are introduced for
orphan words (e.g., which).
437
((
X
0
named(X
0
, higgs, per)
named(X
0
, peter, per)
?
(
X
1
male(X
1
)
?
(
X
2
second(X
2
)
paper(X
2
)
of(X
2
, X
1
)
?
(
X
3
publish(X
3
)
agent(X
3
, X
0
)
patient(X
3
, X
2
)
;
(
X
4
named(X
4
, physical, org)
named(X
4
, review, org)
named(X
4
, letters, org)
?
X
5
thing(X
5
)
event(X
3
)
in(X
3
, X
4
)
in(X
3
, X
5
)
timex(X
5
) = 1964
)))))
;
(
X
6
;
(
X
7
, X
8
mechanism(X
8
)
nn(X
7
, X
8
)
named(X
7
, higgs, org)
?
X
9
, X
10
, X
11
, X
12
new(X
9
)
massive(X
9
)
spin-zero(X
9
)
boson(X
9
)
predict(X
10
)
event(X
10
)
describe(X
11
)
event(X
11
)
first(X
12
)
time(X
12
)
agent(X
10
, X
8
)
patient(X
10
, X
9
)
agent(X
11
, X
6
)
patient(X
11
, X
8
)
for(X
10
, X
12
)
[Discourse Representation Structure produced by BOXER]
ROOT
O
1
X
10
X
12
X
9
R
10
R
11
X
11
X
8
X
7
R
8
X
6
R
6
R
7
X
3
X
5
X
4
X
2
X
1
R
3
X
0
R
1
R
2
R
4
R
5
R
9
[DRS Graph Representation]
O
1
16 which/WDT
X
12
24, 25, 26 first/a, time/n
X
11
13
describe/v, event
X
10
17
predict/v, event
X
9
18, 19, 20
21, 22
new/a, spin-zero/a
massive/a, boson/n
X
8
14, 15 mechanism/n
X
7
14
higgs/org
X
6
6, 7, 8
??
X
5
2
thing/n, 1964
X
4
10, 11, 12
physical/org
review/org, letters/org
X
3
5
publish/v, event
X
2
6, 7, 8 second/a, paper/a
X
1
6 male/a
X
0
3, 4 higgs/per, peter/per
node pos. in S predicate/type
R
11
23
for,X
10
? X
12
R
10
17
patient,X
10
? X
9
R
9
17
agent,X
10
? X
8
R
8
?? nn,X
8
? X
7
R
7
13
patient,X
11
? X
8
R
6
13
agent,X
11
? X
6
R
5
1
in,X
3
? X
5
R
4
9
in,X
3
? X
4
R
3
6
of,X
2
? X
1
R
2
5
patient,X
3
? X
2
R
1
5
agent,X
3
? X
0
rel pos. in S predicate
ROOT
X
11
X
8
X
7
R
8
X
6
R
6
R
7
X
3
X
5
X
4
X
2
X
1
R
3
X
0
R
1
R
2
R
4
R
5
ROOT
O
1
X
10
X
12
X
9
X
8
X
7
R
8
R
9
R
10
R
11
( )
w
w
w
w

SPLIT
ROOT
X
11
X
8
X
7
R
8
X
6
R
6
R
7
X
3
X
5
X
?
2
X
1
R
3
X
0
R
1
R
2
R
5
In 1964 Peter Higgs published his
paper describing Higgs mechanism
ROOT
X
10
X
?
9
X
8
X
7
R
8
R
9
R
10
Higgs mechanism predicted
a new boson
( )
w
w
w
w

DELETION
Peter Higgs wrote his paper explaining
Higgs mechanism in 1964 .
Higgs mechanism predicted
a new elementary particle .
( )
w
w
w
w

PBMT+LM
Figure 1: Simplification of ?In 1964 Peter Higgs published his second paper in Physical Review Letters
describing Higgs mechanism which predicted a new massive spin-zero boson for the first time .?
438
two tables to the right of the picture show the pred-
icates (top table) associated with each variable and
the relation label (bottom table) associated with
each edge. Boxer also outputs the associated po-
sitions in the complex sentence for each predicate
(not shown in the DRS but in the graph tables). Or-
phan words (OW) i.e., words which have no cor-
responding material in the DRS (e.g., which at po-
sition 16), are added to the graph (node O
1
) thus
ensuring that the position set associated with the
graph exactly matches the positions in the input
sentence and thus deriving the input sentence.
Split Candidate isSplit prob.
(agent, for, patient) - (agent, in, in,
patient)
true 0.63
false 0.37
Table 1: Simplification: SPLIT
Given the input DRS shown in Figure 1, simpli-
fication proceeds as follows.
Splitting. The splitting candidates of a DRS are
event pairs contained in that DRS. More precisely,
the splitting candidates are pairs4 of event vari-
ables associated with at least one of the core the-
matic roles (e.g., agent and patient). The features
conditioning a split are the set of thematic roles as-
sociated with each event variable. The DRS shown
in Figure 1 contains three such event variables
X
3
,X
11
and X
10
with associated thematic role
sets {agent, in, in, patient}, {agent, patient} and
{agent, for, patient} respectively. Hence, there are
3 splitting candidates (X
3
-X
11
, X
3
-X
10
and X
10
-
X
11
) and 4 split options: no split or split at one of
the splitting candidates. Here the split with highest
probability (cf. Table 1) is chosen and the DRS is
split into two sub-DRS, one containing X
3
, and
the other containing X
10
. After splitting, dan-
gling subgraphs are attached to the root of the new
subgraph maximizing either proximity or position
overlap. Here the graph rooted in X
11
is attached
to the root dominating X
3
and the orphan word O
1
to the root dominating X
10
.
Deletion. The deletion model (cf. Table 2) reg-
ulates the deletion of relations and their associated
subgraph; of adjectives and adverbs; and of orphan
words. Here, the relations in between X
3
and X
4
and for between X
10
and X
12
are deleted resulting
in the deletion of the phrases ?in Physical Review
Letters? and ?for the first time? as well as the ad-
4The splitting candidates could be sets of event variables
depending on the number of splits required. Here, we con-
sider pairs for 2 splits.
jectives second, massive, spin-zero and the orphan
word which.
Substitution and Reordering. Finally the trans-
lation and language model ensures that published,
describing and boson are simplified to wrote, ex-
plaining and elementary particle respectively; and
that the phrase ?In 1964? is moved from the be-
ginning of the sentence to its end.
3.2 The Simplification Model
Our simplification framework consists of a prob-
abilistic model for splitting and dropping which
we call DRS simplification model (DRS-SM); a
phrase based translation model for substitution
and reordering (PBMT); and a language model
learned on Simple English Wikipedia (LM) for
fluency and grammaticality. Given a complex sen-
tence c, we split the simplification process into
two steps. First, DRS-SM is applied to D
c
(the
DRS representation of the complex sentence c)
to produce one or more (in case of splitting) in-
termediate simplified sentence(s) s?. Second, the
simplified sentence(s) s? is further simplified to s
using a phrase based machine translation system
(PBMT+LM). Hence, our model can be formally
defined as:
s? = argmax
s
p(s|c)
= argmax
s
p(s
?
|c)p(s|s
?
)
= argmax
s
p(s
?
|D
c
)p(s
?
|s)p(s)
where the probabilities p(s?|D
c
), p(s
?
|s) and
p(s) are given by the DRS simplification model,
the phrase based machine translation model and
the language model respectively.
To get the DRS simplification model, we com-
bine the probability of splitting with the probabil-
ity of deletion:
p(s
?
|D
c
) =
?
?:str(?(D
c
))=s
?
p(D
split
|D
c
)p(D
del
|D
split
)
where ? is a sequence of simplification opera-
tions and str(?(D
c
)) is the sequence of words as-
sociated with a DRS resulting from simplifying D
c
using ?.
The probability of a splitting operation for a
given DRS D
c
is:
p(D
split
|D
c
) =
?
?
?
SPLIT(sptruecand), split at spcand
?
spcand
SPLIT(spfalsecand), otherwise
439
relation candidate isDrop prob.
relation
word
length
range
in 0-2 true 0.22false 0.72
in 2-5 true 0.833false 0.167
mod. cand. isDrop prob.
mod word
new
true 0.22
false 0.72
massive true 0.833false 0.167
OW candidate isDrop prob.orphan
word isBoundary
and true true 0.82false 0.18
which false true 0.833false 0.167
Table 2: Simplification: DELETION (Relations, modifiers and OW respectively)
That is, if the DRS is split on the splitting candi-
date spcand, the probability of the split is then given
by the SPLIT table (Table 1) for the isSplit value
?true? and the split candidate spcand; else it is the
product of the probability given by the SPLIT table
for the isSplit value ?false? for all split candidate
considered for D
c
. As mentioned above, the fea-
tures used for determining the split operation are
the role sets associated with pairs of event vari-
ables (cf. Table 1).
The deletion probability is given by three mod-
els: a model for relations determining the deletion
of prepositional phrases; a model for modifiers
(adjectives and adverbs) and a model for orphan
words (Table 2). All three deletion models use the
associated word itself as a feature. In addition, the
model for relations uses the PP length-range as a
feature while the model for orphan words relies on
boundary information i.e., whether or not, the OW
occurs at the associated sentence boundary.
p(D
del
|D
split
) =
?
relcand
DELrel(relcand)
?
modcand
DELmod(modcand)
?
owcand
DELow(owcand)
3.3 Estimating the parameters
We use the EM algorithm (Dempster et al, 1977)
to estimate our split and deletion model parame-
ters. For an efficient implementation of EM algo-
rithm, we follow the work of Yamada and Knight
(2001) and Zhu et al (2010); and build training
graphs (Figure 2) from the pair of complex and
simple sentence pairs in the training data.
Each training graph represents a complex-
simple sentence pair and consists of two types
of nodes: major nodes (M-nodes) and operation
nodes (O-nodes). An M-node contains the DRS
representation D
c
of a complex sentence c and the
associated simple sentence(s) s
i
while O-nodes
determine split and deletion operations on their
parent M-node. Only the root M-node is consid-
ered for the split operations. For example, given
fin
del-rel?; del-mod?; del-ow?
split
root
Figure 2: An example training graph
the root M-node (D
c
, (s
1
, s
2
)), multiple success-
ful split O-nodes will be created, each one further
creating two M-nodes (D
c1
, s
1
) and (D
c2
, s
2
). For
the training pair (c, s), the root M-node (D
c
, s) is
followed by a single split O-node producing an M-
node (D
c
, s) and counting all split candidates in D
c
for failed split. The M-nodes created after split op-
erations are then tried for multiple deletion opera-
tions of relations, modifiers and OW respectively.
Each deletion candidate creates a deletion O-node
marking successful or failed deletion of the can-
didate and a result M-node. The deletion process
continues on the result M-node until there is no
deletion candidate left to process. The governing
criteria for the construction of the training graph
is that, at each step, it tries to minimize the Leven-
shtein edit distance between the complex and the
simple sentences. Moreover, for the splitting op-
eration, we introduce a split only if the reference
sentence consists of several sentences (i.e., there
is a split in the training data); and only consider
splits which maximises the overlap between split
and simple reference sentences.
We initialize our probability tables Table 1 and
Table 2 with the uniform distribution, i.e., 0.5 be-
cause all our features are binary. The EM algo-
rithm iterates over training graphs counting model
features from O-nodes and updating our probabil-
ity tables. Because of the space constraints, we
do not describe our algorithm in details. We refer
the reader to (Yamada and Knight, 2001) for more
details.
440
Our phrase based translation model is trained
using the Moses toolkit5 with its default command
line options on the PWKP corpus (except the sen-
tences from the test set) considering the complex
sentence as the source and the simpler one as the
target. Our trigram language model is trained us-
ing the SRILM toolkit6 on the SWKP corpus7.
Decoding. We explore the decoding graph sim-
ilar to the training graph but in a greedy approach
always picking the choice with maximal probabil-
ity. Given a complex input sentence c, a split O-
node will be selected corresponding to the deci-
sion of whether to split and where to split. Next,
deletion O-nodes are selected indicating whether
or not to drop each of the deletion candidate. The
DRS associated with the final M-node D
fin
is then
mapped to a simplified sentence s?
fin
which is
further simplified using the phrase-based machine
translation system to produce the final simplified
sentence s
simple
.
4 Experiments
We trained our simplification and translation mod-
els on the PWKP corpus. To evaluate perfor-
mance, we compare our approach with three other
state of the art systems using the test set provided
by Zhu et al (2010) and relying both on automatic
metrics and on human judgments.
4.1 Training and Test Data
The DRS-Based simplification model is trained
on PWKP, a bi-text of complex and simple sen-
tences provided by Zhu et al (2010). To construct
this bi-text, Zhu et al (2010) extracted complex
and simple sentences from EWKP and SWKP re-
spectively and automatically aligned them using
TF*IDF as a similarity measure. PWKP contains
108016/114924 complex/simple sentence pairs.
We tokenize PWKP using Stanford CoreNLP
toolkit8. We then parse all complex sentences
in PWKP using Boxer9 to produce their DRSs.
Finally, our DRS-Based simplification model is
trained on 97.75% of PWKP; we drop out 2.25%
of the complex sentences in PWKP which are re-
peated in the test set or for which Boxer fails to
produce DRSs.
5http://www.statmt.org/moses/
6http://www.speech.sri.com/projects/srilm/
7We downloaded the snapshots of Simple Wikipedia
dated 2013-10-30 available at http://dumps.wikimedia.org/.
8http://nlp.stanford.edu/software/corenlp.shtml
9http://svn.ask.it.usyd.edu.au/trac/candc, Version 1.00
We evaluate our model on the test set used by
Zhu et al (2010) namely, an aligned corpus of
100/131 EWKP/SWKP sentences. Boxer pro-
duces a DRS for 96 of the 100 input sentences.
These input are simplified using our simplifica-
tion system namely, the DRS-SM model and the
phrase-based machine translation system (Section
3.2). For the remaining four complex sentences,
Boxer fails to produce DRSs. These four sen-
tences are directly sent to the phrase-based ma-
chine translation system to produce simplified sen-
tences.
4.2 Automatic Evaluation Metrics
To assess and compare simplification systems, two
main automatic metrics have been used in previ-
ous work namely, BLEU and the Flesch-Kincaid
Grade Level Index (FKG).
The FKG index is a readability metric taking
into account the average sentence length in words
and the average word length in syllables. In its
original context (language learning), it was ap-
plied to well formed text and thus measured the
simplicity of a well formed sentence. In the con-
text of the simplification task however, the auto-
matically generated sentences are not necessarily
well formed so that the FKG index reduces to a
measure of the sentence length (in terms of words
and syllables) approximating the simplicity level
of an output sentence irrespective of the length
of the corresponding input. To assess simplifica-
tion, we instead use metrics that are directly re-
lated to the simplification task namely, the number
of splits in the overall (test and training) data and
in average per sentences; the number of generated
sentences with no edits i.e., which are identical to
the original, complex one; and the average Leven-
shtein distance between the system?s output and
both the complex and the simple reference sen-
tences.
BLEU gives a measure of how close a system?s
output is to the gold standard simple sentence. Be-
cause there are many possible ways of simplifying
a sentence, BLEU alone fails to correctly assess
the appropriateness of a simplification. Moreover
BLEU does not capture the degree to which the
system?s output differs from the complex sentence
input. We therefore use BLEU as a means to eval-
uate how close the systems output are to the refer-
ence corpus but complement it with further man-
ual metrics capturing other important factors when
441
evaluating simplifications such as the fluency and
the adequacy of the output sentences and the de-
gree to which the output sentence simplifies the
input.
4.3 Results and Discussion
Number of Splits Table 3 shows the proportion
of input whose simplification involved a splitting
operation. While our system splits in proportion
similar to that observed in the training data, the
other systems either split very often (80% of the
time for Zhu and 63% of the time for Woodsend)
or not at all (Wubben). In other words, when com-
pared to the other systems, our system performs
splits in proportion closest to the reference both
in terms of total number of splits and of average
number of splits per sentence.
Data Total number
of sentences % split
average split /
sentence
PWKP 108,016 6.1 1.06
GOLD 100 28 1.30
Zhu 100 80 1.80
Woodsend 100 63 2.05
Wubben 100 1 1.01
Hybrid 100 10 1.10
Table 3: Proportion of Split Sentences (% split)
in the training/test data and in average per sen-
tence (average split / sentence). GOLD is the
test data with the gold standard SWKP sentences;
Zhu, Woodsend, Wubben are the best output of the
models of Zhu et al (2010), Woodsend and Lap-
ata (2011) and Wubben et al (2012) respectively;
Hybrid is our model.
Number of Edits Table 4 indicates the edit dis-
tance of the output sentences w.r.t. both the com-
plex and the simple reference sentences as well as
the number of input for which no simplification
occur. The right part of the table shows that our
system generate simplifications which are closest
to the reference sentence (in terms of edits) com-
pared to those output by the other systems. It
also produces the highest number of simplifica-
tions which are identical to the reference. Con-
versely our system only ranks third in terms of dis-
similarity with the input complex sentences (6.32
edits away from the input sentence) behind the
Woodsend (8.63 edits) and the Zhu (7.87 edits)
system. This is in part due to the difference in
splitting strategies noted above : the many splits
applied by these latter two systems correlate with
a high number of edits.
System BLEU
Edits (Complex
to System)
Edits (System
to Simple)
LD No edit LD No edit
GOLD 100 12.24 3 0 100
Zhu 37.4 7.87 2 14.64 0
Woodsend 42 8.63 24 16.03 2
Wubben 41.4 3.33 6 13.57 2
Hybrid 53.6 6.32 4 11.53 3
Table 4: Automated Metrics for Simplification:
average Levenshtein distance (LD) to complex and
simple reference sentences per system ; number of
input sentences for which no simplification occur
(No edit).
BLEU score We used Moses support tools:
multi-bleu10 to calculate BLEU scores. The
BLEU scores shown in Table 4 show that our sys-
tem produces simplifications that are closest to the
reference.
In sum, the automatic metrics indicate that our
system produces simplification that are consis-
tently closest to the reference in terms of edit dis-
tance, number of splits and BLEU score.
4.4 Human Evaluation
The human evaluation was done online using the
LG-Eval toolkit (Kow and Belz, 2012)11. The
evaluators were allocated a trial set using a Latin
Square Experimental Design (LSED) such that
each evaluator sees the same number of output
from each system and for each test set item. Dur-
ing the experiment, the evaluators were presented
with a pair of a complex and a simple sentence(s)
and asked to rate this pair w.r.t. to adequacy (Does
the simplified sentence(s) preserve the meaning
of the input?) and simplification (Does the gen-
erated sentence(s) simplify the complex input?).
They were also asked to rate the second (sim-
plified) sentence(s) of the pair w.r.t. to fluency
(Is the simplified output fluent and grammatical?).
Similar to the Wubben?s human evaluation setup,
we randomly selected 20 complex sentences from
Zhu?s test corpus and included in the evaluation
corpus: the corresponding simple (Gold) sentence
from Zhu?s test corpus, the output of our system
(Hybrid) and the output of the other three sys-
tems (Zhu, Woodsend and Wubben) which were
provided to us by the system authors. The eval-
uation data thus consisted of 100 complex/simple
pairs. We collected ratings from 27 participants.
10http://www.statmt.org/moses/?n=Moses.SupportTools
11http://www.nltg.brighton.ac.uk/research/lg-eval/
442
All were either native speakers or proficient in En-
glish, having taken part in a Master taught in En-
glish or lived in an English speaking country for
an extended period of time.
Systems Simplification Fluency Adequacy
GOLD 3.57 3.93 3.66
Zhu 2.84 2.34 2.34
Woodsend 1.73 2.94 3.04
Wubben 1.81 3.65 3.84
Hybrid 3.37 3.55 3.50
Table 5: Average Human Ratings for simplicity,
fluency and adequacy
Table 5 shows the average ratings of the human
evaluation on a slider scale from 0 to 5. Pairwise
comparisons between all models and their statisti-
cal significance were carried out using a one-way
ANOVA with post-hoc Tukey HSD tests and are
shown in Table 6.
Systems GOLD Zhu Woodsend Wubben
Zhu ??
Woodsend ?? ??
Wubben ?N ?? ?
Hybrid N ? ?N ?N
Table 6: ?/ is/not significantly different (sig.
diff.) wrt simplicity. / is/not sig. diff. wrt
fluency. ?/N is/not sig. diff. wrt adequacy. (sig-
nificance level: p < 0.05)
With regard to simplification, our system ranks
first and is very close to the manually simpli-
fied input (the difference is not statistically signif-
icant). The low rating for Woodsend reflects the
high number of unsimplified sentences (24/100 in
the test data used for the automatic evaluation and
6/20 in the evaluation data used for human judg-
ments). Our system data is not significantly differ-
ent from the manually simplified data for simplic-
ity whereas all other systems are.
For fluency, our system rates second behind
Wubben and before Woodsend and Zhu. The
difference between our system and both Zhu
and Woodsend system is significant. In partic-
ular, Zhu?s output is judged less fluent proba-
bly because of the many incorrect splits it li-
censes. Manual examination of the data shows
that Woodsend?s system also produces incorrect
splits. For this system however, the high propor-
tion of non simplified sentences probably counter-
balances these incorrect splits, allowing for a good
fluency score overall.
Regarding adequacy, our system is against clos-
est to the reference (3.50 for our system vs.
3.66 for manual simplification). Our system, the
Wubben system and the manual simplifications
are in the same group (the differences between
these systems are not significant). The Wood-
send system comes second and the Zhu system
third (the difference between the two is signifi-
cant). Wubben?s high fluency, high adequacy but
low simplicity could be explained with their min-
imal number of edit (3.33 edits) from the source
sentence.
In sum, if we group together systems for which
there is no significant difference, our system ranks
first (together with GOLD) for simplicity; first
for fluency (together with GOLD and Wubben);
and first for adequacy (together with GOLD and
Wubben).
5 Conclusion
A key feature of our approach is that it is seman-
tically based. Typically, discourse level simplifi-
cation operations such as sentence splitting, sen-
tence reordering, cue word selection, referring ex-
pression generation and determiner choice are se-
mantically constrained. As argued by Siddharthan
(2006), correctly capturing the interactions be-
tween these phenomena is essential to ensuring
text cohesion. In the future, we would like to
investigate how our framework deals with such
discourse level simplifications i.e., simplifications
which involves manipulation of the coreference
and of the discourse structure. In the PWKP data,
the proportion of split sentences is rather low (6.1
%) and many of the split sentences are simple sen-
tence coordination splits. A more adequate but
small corpus is that used in (Siddharthan, 2006)
which consists of 95 cases of discourse simplifica-
tion. Using data from the language learning or the
children reading community, it would be interest-
ing to first construct a similar, larger scale corpus;
and to then train and test our approach on more
complex cases of sentence splitting.
Acknowledgments
We are grateful to Zhemin Zhu, Kristian Wood-
send and Sander Wubben for sharing their data.
We would like to thank our annotators for partic-
ipating in our human evaluation experiments and
to anonymous reviewers for their insightful com-
ments.
443
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics (ACL), pages 597?
604. Association for Computational Linguistics.
Regina Barzilay and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the 2003 conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 25?32. Association for Computa-
tional Linguistics.
Stefan Bott, Horacio Saggion, and Simon Mille. 2012.
Text simplification tools for spanish. In Proceedings
of the 8th International Conference on Language
Resources and Evaluation (LREC), pages 1665?
1671.
Yvonne Margaret Canning. 2002. Syntactic simplifica-
tion of Text. Ph.D. thesis, University of Sunderland.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999.
Simplifying text for language-impaired readers. In
Proceedings of 9th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), volume 99, pages 269?270. Cite-
seer.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
Knowledge-Based Systems, 10(3):183?190.
Raman Chandrasekar, Christine Doran, and Banga-
lore Srinivas. 1996. Motivations and methods for
text simplification. In Proceedings of the 16th In-
ternational conference on Computational linguistics
(COLING), pages 1041?1044. Association for Com-
putational Linguistics.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9. Association for Computational
Linguistics.
James R Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale NLP with C&C
and Boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL) on Interactive Poster and Demonstration Ses-
sions, pages 33?36. Association for Computational
Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39(1):1?38.
Mark Dras. 1999. Tree adjoining grammar and the
reluctant paraphrasing of text. Ph.D. thesis, Mac-
quarie University NSW 2109 Australia.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference (INLG), pages 25?32. Asso-
ciation for Computational Linguistics.
Hans Kamp. 1981. A theory of truth and semantic rep-
resentation. In J.A.G. Groenendijk, T.M.V. Janssen,
B.J. Stokhof, and M.J.B. Stokhof, editors, Formal
methods in the study of language, number pt. 1 in
Mathematical Centre tracts. Mathematisch Centrum.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence (AAAI) and
Twelfth Conference on Innovative Applications of
Artificial Intelligence (IAAI), pages 703?710. AAAI
Press.
Eric Kow and Anja Belz. 2012. LG-Eval: A Toolkit
for Creating Online Language Evaluation Experi-
ments. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC), pages 4033?4037.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2004. Syntactic simplification for im-
proving content selection in multi-document sum-
marization. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING), page 896. Association for Computational Lin-
guistics.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In Proceedings of the Lan-
guage Engineering Conference (LEC), pages 64?71.
IEEE Computer Society.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language and Com-
putation, 4(1):77?109.
Advaith Siddharthan. 2010. Complex lexico-syntactic
reformulation of sentences using typed dependency
representations. In Proceedings of the 6th Inter-
national Natural Language Generation Conference
(INLG), pages 125?133. Association for Computa-
tional Linguistics.
Advaith Siddharthan. 2011. Text simplification us-
ing typed dependencies: a comparison of the robust-
ness of different generation strategies. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation (ENLG), pages 2?11. Association
for Computational Linguistics.
David A Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proceedings of
the HLT-NAACL Workshop on Statistical Machine
Translation, pages 23?30. Association for Compu-
tational Linguistics.
444
David Vickrey and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL) and the
Human Language Technology Conference (HLT),
pages 344?352.
Willian Massami Watanabe, Arnaldo Candido Junior,
Vin??cius Rodriguez Uze?da, Renata Pontin de Mat-
tos Fortes, Thiago Alexandre Salgueiro Pardo, and
Sandra Maria Alu??sio. 2009. Facilita: reading as-
sistance for low-literacy readers. In Proceedings of
the 27th ACM international conference on Design of
communication, pages 29?36. ACM.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 409?420.
Association for Computational Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL): Long Papers-Volume
1, pages 1015?1024. Association for Computational
Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics (ACL), pages 523?530. Asso-
ciation for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (COLING), pages 1353?1361, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
445
Proceedings of the 14th European Workshop on Natural Language Generation, pages 40?50,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generating Elliptic Coordination
Claire Gardent
CNRS, LORIA, UMR 7503
Vandoeuvre-le`s-Nancy, F-54500, France
claire.gardent@loria.fr
Shashi Narayan
Universite? de Lorraine, LORIA, UMR 7503
Villers-le`s-Nancy, F-54600, France
shashi.narayan@loria.fr
Abstract
In this paper, we focus on the task of gen-
erating elliptic sentences. We extract from
the data provided by the Surface Realisa-
tion (SR) Task (Belz et al, 2011) 2398 in-
put whose corresponding output sentence
contain an ellipsis. We show that 9% of the
data contains an ellipsis and that both cov-
erage and BLEU score markedly decrease
for elliptic input (from 82.3% coverage for
non-elliptic sentences to 65.3% for ellip-
tic sentences and from 0.60 BLEU score
to 0.47). We argue that elided material
should be represented using phonetically
empty nodes and we introduce a set of
rewrite rules which permits adding these
empty categories to the SR data. Finally,
we evaluate an existing surface realiser on
the resulting dataset. We show that, after
rewriting, the generator achieves a cover-
age of 76% and a BLEU score of 0.74 on
the elliptical data.
1 Introduction
To a large extent, previous work on generating el-
lipsis has assumed a semantically fully specified
input (Shaw, 1998; Harbusch and Kempen, 2009;
Theune et al, 2006). Given such input, elliptic
sentences are then generated by first producing
full sentences and second, deleting from these sen-
tences substrings that were identified to obey dele-
tion constraints.
In contrast, recent work on generation often as-
sumes input where repeated material has already
been elided. This includes work on sentence com-
pression which regenerates sentences from surface
dependency trees derived from parsing the initial
text (Filippova and Strube, 2008); Surface realisa-
tion approaches which have produced results for
regenerating from the Penn Treebank (Langkilde-
Geary, 2002; Callaway, 2003; Zhong and Stent,
2005; Cahill and Van Genabith, 2006; White and
Rajkumar, 2009); and more recently, the Surface
Realisation (SR) Task (Belz et al, 2011) which
has proposed dependency trees and graphs de-
rived from the Penn Treebank (PTB) as a com-
mon ground input representation for testing and
comparing existing surface realisers. In all these
approaches, repeated material is omitted from the
representation that is input to surface realisation.
As shown in the literature, modelling the inter-
face between the empty phonology and the syn-
tactic structure of ellipses is a difficult task. For
parsing, Sarkar and Joshi (1996), Banik (2004)
and Seddah (2008) propose either to modify the
derivation process of Tree Adjoining Grammar
or to introduce elementary trees anchored with
empty category in a synchronous TAG to accom-
modate elliptic coordinations. In HPSG (Head-
Driven Phrase Structure Grammar), Levy and Pol-
lard (2001) introduce a neutralisation mechanism
to account for unlike constituent coordination ;
in LFG (Lexical Functional Grammar), Dalrym-
ple and Kaplan (2000) employ set values to model
coordination; in CCG (Combinatory Categorial
Grammar, (Steedman, 1996)), it is the non stan-
dard notion of constituency assumed by the ap-
proach which permits accounting for coordinated
structures; finally, in TLCG (Type-Logical Cat-
egorial Grammar), gapping is treated as like-
category constituent coordinations (Kubota and
Levine, 2012).
In this paper, we focus on how surface reali-
sation handles elliptical sentences given an input
where repeated material is omitted. We extract
from the SR data 2398 input whose correspond-
ing output sentence contain an ellipsis. Based on
previous work on how to annotate and to represent
ellipsis, we argue that elided material should be
represented using phonetically empty nodes (Sec-
tion 3) and we introduce a set of rewrite rules
which permits adding these empty categories to
40
the SR data (Section 4). We then evaluate our sur-
face realiser (Narayan and Gardent, 2012b) on the
resulting dataset (Section 5) and we show that, on
this data, the generator achieves a coverage of 76%
and a BLEU score, for the generated sentences, of
0.74. Section 6 discusses related work on generat-
ing elliptic coordination. Section 7 concludes.
2 Elliptic Sentences
Elliptic coordination involves a wide range of phe-
nomena including in particular non-constituent
coordination (1, NCC) i.e., cases where sequences
of constituents are coordinated; gapping (2, G)
i.e., cases where the verb and possibly some
additional material is elided; shared subjects (3,
SS) and right node raising (4, RNR) i.e., cases
where a right most constituent is shared by two or
more clauses1.
(1) [It rose]i 4.8 % in June 1998 and i 4.7% in
June 1999. NCC
(2) Sumitomo bank [donated]i $500, 000, Tokyo
prefecture i $15, 000 and the city of Osaka i
$10, 000 . Gapping
(3) [the state agency ?s figures]i i confirm pre-
vious estimates and i leave the index at 178.9 .
Shared Subject
(4) He commissions i and splendidly interprets
i [fearsome contemporary scores]i . RNR
We refer to the non elliptic clause as the source
and to the elliptic clause as the target. In the
source, the brackets indicate the element shared
with the target while in the target, the i sign in-
dicate the elided material with co-indexing indi-
cating the antecedent/ellipsis relation. In gapping
clauses, we refer to the constituents in the gapped
clause, as remnants.
3 Representing and Annotating Elided
Material
We now briefly review how elided material is rep-
resented in the literature.
Linguistic Approaches. While Sag (1976),
Williams (1977), Kehler (2002), Merchant (2001)
1Other types of elliptic coordination include sluicing and
Verb-Phrase ellipsis. These will not be discussed here be-
cause they can be handled by the generator by having the
appropriate categories in the grammar and the lexicon e.g.,
in a Tree Adjoining Grammar, an auxiliary anchoring a verb
phrase for VP ellipsis and question words anchoring a sen-
tence for sluicing.
and van Craenenbroeck (2010) have argued for
a structural approach i.e., one which posits syn-
tactic structure for the elided material, Keenan
(1971), Hardt (1993), Dalrymple et al (1991),
Ginzburg and Sag (2000) and Culicover and Jack-
endoff (2005) all defend a non structural approach.
Although no consensus has yet been reached on
these questions, many of these approaches do pos-
tulate an abstract syntax for ellipsis. That is they
posit that elided material licenses the introduction
of phonetically empty categories in the syntax or
at some more abstract level (e.g., the logical form
of generative linguistics).
Treebanks. Similarly, in computational linguis-
tics, the treebanks used to train and evaluate
parsers propose different means of representing el-
lipsis.
For phrase structure syntax, the Penn Treebank
Bracketing Guidelines extensively describe how to
annotate coordination and missing material in En-
glish (Bies et al, 1995). For shared complements
(e.g., shared subject and right node raising con-
structions), these guidelines state that the elided
material licenses the introduction of an empty
*RNR* category co-indexed with the shared com-
plement (cf. Figure 1) while gapping construc-
tions are handled by labelling the gapping rem-
nants (i.e., the constituents present in the gapping
clause) with the index of their parallel element in
the source (cf. Figure 2).
(S
(VP (VB Do)(VP (VB avoid)
(S (VP (VPG puncturing(NP *RNR*-5))
(CC or)
(VP (VBG cutting)(PP (IN into)
(NP *RNR*-5)))
(NP-5 meats)))))
Figure 1: Penn Treebank annotation for Right
Node Raising ?Do avoid puncturing i or cutting into i
[meats]i.?
(S
(S (NP-SBJ-10 Mary)
(VP (VBZ likes) (NP-11 potatoes)))
(CC and)
(S (NP-SBJ=10 Bill)
(, ,) (NP=11 ostriches)))
Figure 2: Penn Treebank annotation for gapping
?Mary [likes]i potatoes and Bill i ostriches.?
In dependency treebanks, headless elliptic con-
structs such as gapping additionally raise the is-
41
sue of how to represent the daughters of an empty
head. Three main types of approaches have been
proposed. In dependency treebanks for German
(Daum et al, 2004; Hajic? et al, 2009) and in
the Czech treebank ( ?Cmejrek et al, 2004; Hajic?
et al, 2009), one of the dependents of the head-
less phrase is declared to be the head. This is
a rather undesirable solution because it hides the
fact that there the clause lacks a head. In contrast,
the Hungarian dependency treebank (Vincze et al,
2010) explicitly represents the elided elements in
the trees by introducing phonetically empty ele-
ments that serve as attachment points to other to-
kens. This is the cleanest solution from a linguistic
point of view. Similarly, Seeker and Kuhn (2012)
present a conversion of the German Tiger treebank
which introduces empty nodes for verb ellipses if
a phrase normally headed by a verb is lacking a
head. They compare the performance of two sta-
tistical dependency parsers on the canonical ver-
sion and the CoNLL 2009 Shared Task data and
show that the converted dependency treebank they
propose yields better parsing results than the tree-
bank not containing empty heads.
In sum, while some linguists have argued for an
approach where ellipsis has no syntactic represen-
tation, many have provided strong empirical evi-
dence for positing empty nodes as place-holders
for elliptic material. Similarly, in devising tree-
banks, computational linguists have oscillated be-
tween representations with and without empty cat-
egories. In the following section, we present the
way in which elided material is represented in the
SR data; we show that it underspecifies the sen-
tences to be generated; and we propose to mod-
ify the SR representations by making the relation-
ship between ellipsis and antecedent explicit using
phonetically empty categories and co-indexing.
4 Rewriting the SR Data
The SR Task 2011 made available two types of
data for surface realisers to be tested on: shallow
dependency trees and deep dependency graphs.
Here we focus on the shallow dependency trees
i.e., on syntactic structures.
The input data provided by the SR Task were
obtained from the Penn Treebank. They were
derived indirectly from the LTH Constituent-to-
Dependency Conversion Tool for Penn-style Tree-
banks (Pennconverter, (Johansson and Nugues,
2007)) by post-processing the CoNLL data to re-
d o n a t e
Sumitomo bank
SBJ
a n d
COORD
$ 5 0 0 , 0 0 0
OBJ
Tokyo prefecture
GAP-SBJ
$ 1 5 , 0 0 0
GAP-OBJ
Figure 3: Gapping in the SR data. ?Sumitomo bank
[donated]i $500, 000 and Tokyo prefecture i $15, 000.?
move word order, inflections etc. It consists of
a set of unordered labelled syntactic dependency
trees whose nodes are labelled with word forms,
part of speech categories, partial morphosyntac-
tic information such as tense and number and, in
some cases, a sense tag identifier. The edges are
labelled with the syntactic labels provided by the
Pennconverter. All words (including punctuation)
of the original sentence are represented by a node
in the tree. Figures 3, 4, 5 and 6 show (simplified)
input trees from the SR data.
In the SR data, the representation of ellipsis
adopted in the Penn Treebank is preserved modulo
some important differences regarding co-indexing.
Gapping is represented as in the PTB by la-
belling the remnants with a marker indicating the
source element parallel to each remnant. However
while in the PTB, this parallelism is made explicit
by co-indexing (the source element is marked with
an index i and its parallel target element with the
marker = i), in the SR data this parallelism is ap-
proximated using functions. For instance, if the
remnant is parallel to the source subject, it will be
labelled GAP-SBJ (cf. Figure 3).
commission
He
SBJ
a n d
COORD
fearsome contemporary  score
OBJ
splendidly interpret
CONJ
Figure 4: Subject Sharing and RNR in the SR
data. ?[He]j j commissions i and j splendidly interprets
i [fearsome contemporary scores]i .?
For right-node raising and shared subjects, the
coindexation present in the PTB is dropped in the
SR data. As a result, the SR representation under-
42
b e
They
SBJ
s h o w
VC
n o t
ADV
t a k e
OPRD
James Madison
OBJ
or
COORD
a puff
OBJ
light up
CONJ
Figure 5: Non shared Object ?They aren?t showing
James Madison taking a puff or lighting up?
rise
It
SBJ
a n d
COORD
4.8  %
EXT
in June 1998
TMP
4.7  %
GAP-EXT
in June 1999
GAP-TMP
Figure 6: NCC in the SR data. ?It rose 4.8 % in June
1998 and 4.7% in June 1999.?
V V
FUNC COORD FUNC COORD
X CONJ W X CONJ W
GAP-FUNC ? CONJ
Y V
FUNC
Y W
Figure 7: Gapping and Non Constituent Coordina-
tion structures rewriting (V: a verb, CONJ: a con-
junctive coordination, X, Y and W three sets of de-
pendents). The antecedent verb (V) and the source
material without counterpart in the gapping clause
(W) are copied over to the gapping clause and
marked as phonetically empty.
specifies the relation between the object and the
coordinated verbs in RNR constructions: the ob-
ject could be shared as in He commissions i and splen-
didly interprets i [fearsome contemporary scores]i . (Fig-
ure 4) or not as in They aren?t showing James Madison
taking a puff or lighting up (Figure 5). In both cases,
the representation is the same i.e., the shared ob-
ject (fearsome contemporary scores) and the unshared
object (a puff ) are both attached to the first verb.
Finally, NCC structures are handled in the same
way as gapping by having the gapping remnants
labelled with a GAP prefixed function (e.g., GAP-
SBJ) indicating which element in the source the
gapping remnant is parallel to (cf. Figure 6).
Summing up, the SR representation schema un-
derspecifies ellipsis in two ways. For gapping and
non-constituent coordination, it describes paral-
lelism between source and target elements rather
than specifying the syntax of the elided material.
For subject sharing and right node raising, it fails
to explicitly specify argument sharing.
V1 V1
SUBJ COORD SUBJ COORD
X CONJ Y1 X CONJ Y1
CONJ ? CONJ
V2 V2
SUBJ
Y2 X Y2
Figure 8: Subject sharing: the subject dependent
is copied over to the target clause and marked as
phonetically empty.
To resolve this underspecification, we rewrite
the SR data using tree rewrite rules as follows.
In Gapping and NCC structures, we copy the
source material that has no (GAP- marked) coun-
terpart in the target clause to the target clause
marking it to indicate a phonetically empty cate-
gory (cf. Figure 7).
For Subject sharing, we copy the shared subject
of the source clause in the target clause and mark it
to be a phonetically empty category (cf. Figure 8).
For Right-Node-Raising, we unfold the am-
biguity producing structures where arguments
present in the source but not in the target are op-
tionally copied over to the target (cf. Figure 9).
These rewrite rules are implemented efficiently
43
V1 V1 V1
COORD OBJ COORD OBJ COORD OBJ
X1 CONJ Y X1 CONJ Y X1 CONJ Y
CONJ ?
{
CONJ , CONJ
}
V2 V2 V2
OBJ
X2 X2 X2 Y
Figure 9: Right-Node-Raising: the object dependent is optionally copied over to the target clause and
marked as phonetically empty in the source clause.
using GrGen, an efficient graph rewriting sys-
tem (Gei?et al, 2006).
5 Generating Elliptic Coordination
5.1 The Surface Realiser
To generate sentences from the SR data, we
use our surface realiser (Narayan and Gardent,
2012b), a grammar-based generator based on a
Feature-Based Lexicalised Tree Adjoining Gram-
mar (FB-LTAG) for English. This generator first
selects the elementary FB-LTAG trees associated
in the lexicon with the lemmas and part of speech
tags associated with each node in the input de-
pendency tree. It then attempts to combine the
selected trees bottom-up taking into account the
structure of the input tree (only trees that are se-
lected by nodes belonging to the same local input
tree are tried for combination). A language model
is used to implement a beam search letting through
only the n most likely phrases at each bottom up
combination step. In this experiment, we set n to
5. The generator thus outputs at most 5 sentences
for each input.
Figure 10: Gapping after rewriting ?Sumitomo bank
[donated]i $500, 000 and Tokyo prefecture i $15, 000.?
As mentioned in the introduction, most compu-
tational grammars have difficulty accounting for
ellipses and FB-LTAG is no exception.
The difficulty stems from the fact that in ellip-
tical sentences, there is meaning without sound.
As a result, the usual form/meaning mappings that
in non-elliptic sentences allow us to map sounds
onto their corresponding meanings, break down.
For instance, in the sentence John eats apples
and Mary pear, the Subject-Verb-Object structure
which can be used in English to express a binary
relation is present in the source clause but not in
the elided one. In practice, the syntax of ellipti-
cal sentences often leads to a duplication of the
grammatical system, one system allowing for non
elliptical sentences and the other for their elided
counterpart.
For parsing with TAG, two main methods have
been proposed for processing elliptical sentences.
(Sarkar and Joshi, 1996) introduces an additional
operation for combining TAG trees which yields
derivation graphs rather than trees. (Seddah, 2008)
uses Multi-Component TAG and proposes to asso-
ciate each elementary verb tree with an elliptic tree
with different pairs representing different types of
ellipses.
We could use either of these approaches for
generation. The first approach however has the
drawback that it leads to a non standard notion of
derivation (the derivation trees become derivation
graphs). The second on the other hand, induces a
proliferation of trees in the grammar and impacts
efficiency.
Instead, we show that, given an input enriched
with empty categories as proposed in the previous
section, neither the grammar nor the tree combi-
nation operation need changing. Indeed, our FB-
LTAG surface realiser directly supports the gen-
eration of elliptic sentences. It suffices to as-
sume that an FB-LTAG elementary tree may be an-
chored by the empty string. Given an input node
marked as phonetically empty, the generator will
44
then select all FB-LTAG rules that are compatible
with the lexical and the morpho-syntactic features
labelling that node. Generation will then proceed
as usual by composing the trees selected on the ba-
sis of the input using substitution and adjunction;
and by retrieving from the generation forest those
sentences whose phrase structure tree covers the
input.
For instance, given the rewritten input shown in
Figure 10, the TAG trees associated in the lexi-
con with donate will be selected; anchored with
the empty string and combined with the TAG trees
built for Tokyo Prefecture and $15,000 thus yield-
ing the derivation shown in Figure 11.
NP
Tokyo prefecture
S
NP? VP
V NP?

NP
$15,000
Figure 11: Derivation for ?Tokyo prefecture  $15,000?
5.2 The Data
We use both the SR test data (2398 sentences) and
the SR training data (26604 sentences) to evaluate
the performance of the surface realiser on elliptic
coordination. Since the realiser we are using is
not trained on this data (the grammar was written
manually), this does not bias evaluation. Using
the training data allows us to gather a larger set of
elliptic sentences for evaluation while evaluating
also on the test data allows comparison with other
realisers.
To focus on ellipses, we retrieve those sentences
which were identified by our rewrite rules as po-
tentially containing an elliptic coordination. In
essence, these rewrite rules will identify all cases
of non-constituent coordination and gapping (be-
cause these involve GAP-X dependencies with ?X?
a dependency relation and are therefore easily de-
tected) and of shared-subjects (because the tree
patterns used to detect are unambiguous i.e., only
apply if there is indeed a shared subject). For
RNR, as discussed in the previous section, the SR
format is ambiguous and consequently, the rewrite
rules might identify as object sharing cases where
in fact the object is not shared. As noted by one
of our reviewers, the false interpretation could be
Elliptic Coordination Data
Elliptic Coordination Pass BLEU ScoresCOV ALL
RNR (384)
Before 66% 0.68 0.45
After 81% 0.70 0.57
Delta +15 +0.02 +0.12
SS (1462)
Before 70% 0.74 0.52
After 75% 0.75 0.56
Delta +5 +0.01 +0.04
SS + RNR
(456)
Before 61% 0.71 0.43
After 74% 0.73 0.54
Delta +13 +0.02 +0.11
Gapping (36)
Before 3% 0.53 0.01
After 67% 0.74 0.49
Delta +64 +0.21 +0.48
NCC (60)
Before 5% 0.68 0.03
After 73% 0.74 0.54
Delta +68 +0.06 +0.51
Total (2398)
Before 65% 0.72 0.47
After 76% 0.74 0.56
Delta +11 +0.02 +0.09
Table 1: Generation results on elliptical data be-
fore and after input rewriting (SS: Shared Subject,
NCC: Non Constituent Coordination, RNR: Right
Node Raising). The number in brackets in the first
column is the number of cases. Pass stands for
the coverage of the generator. COV and ALL in
BLEU scores column stand for BLEU scores for
the covered and the total input data.
dropped out by consulting the Penn Treebank2.
The approach would not generalise to other data
however.
In total, we retrieve 2398 sentences3 potentially
containing an elliptic coordination from the SR
training data. The number and distribution of these
sentences in terms of ellipsis types are given in Ta-
ble 1. From the test data, we retrieve an additional
182 elliptic sentences.
5.3 Evaluation
We ran the surface realiser on the SR input data
both before and after rewriting elliptic coordina-
tions; on the sentences estimated to contain ellip-
sis; on sentences devoid of ellipsis; and on all sen-
tences. The results are shown in Table 2. They
indicate coverage and BLEU score before and af-
ter rewriting. BLEU score is given both with re-
spect to covered sentences (COV) i.e., the set of
input for which generation succeeds; and for all
sentences (ALL). We evaluate both with respect to
the SR test data and with respect to the SR training
2The Penn Treebank makes the RNR interpretations ex-
plicit (refer to Figure 1).
3It is just a coincidence that the size of the SR test data
and the number of extracted elliptic sentences are the same.
45
SR Data Pass BLEU ScoresCOV ALL
Test
+E (182)
Before 58% 0.59 0.34
After 67% 0.59 0.40
Delta +9 +0.00 +0.06
-E (2216)
Before 80% 0.59 0.47
After 80% 0.59 0.48
Delta +0 +0.00 +0.01
T (2398)
Before 78% 0.58 0.46
After 79% 0.59 0.47
Delta +1 +0.01 +0.01
Training
+E (2398)
(Table 1)
Before 65% 0.72 0.47
After 76% 0.74 0.56
Delta +11 +0.02 +0.09
-E (24206)
Before 82% 0.73 0.60
After 82% 0.73 0.60
Delta +0 +0.00 +0.00
T (26604)
Before 81% 0.72 0.58
After 82% 0.73 0.60
Delta +1 +0.01 +0.02
Table 2: Generation results on SR test and SR
training data before and after input rewriting (+E
stands for elliptical data, -E for non elliptical data
and T for total.)
data. We use the SR Task scripts for the computa-
tion of the BLEU score.
The impact of ellipsis on coverage and preci-
sion. Previous work on parsing showed that co-
ordination was a main source of parsing failure
(Collins, 1999). Similarly, ellipses is an important
source of failure for the TAG generator. Ellipses
are relatively frequent with 9% of the sentences
in the training data containing an elliptic struc-
ture and performance markedly decreases in the
presence of ellipsis. Thus, before rewriting, cov-
erage decreases from 82.3% for non-elliptic sen-
tences to 80.75% on all sentences (elliptic and non
elliptic sentences) and to 65.3% on the set of el-
liptic sentences. Similarly, BLEU score decreases
from 0.60 for non elliptical sentences to 0.58 for
all sentences and to 0.47 for elliptic sentences. In
sum, both coverage and BLEU score decrease as
the number of elliptic input increases.
The impact of the input representation on cov-
erage and precision. Recent work on treebank
annotation has shown that the annotation schema
adopted for coordination impacts parsing. In par-
ticular, Maier et al (2012) propose revised annota-
tion guidelines for coordinations in the Penn Tree-
bank whose aim is to facilitate the detection of co-
ordinations. And Dukes and Habash (2011) show
that treebank annotations which include phoneti-
cally empty material for representing elided mate-
rial allows for better parsing results.
Similarly, Table 2 shows that the way in which
ellipsis is represented in the input data has a strong
impact on generation. Thus rewriting the input
data markedly extends coverage with an overall
improvement of 11 points (from 65% to 76%) for
elliptic sentences and of almost 1 point for all sen-
tences.
As detailed in Table 1 though, there are impor-
tant differences between the different types of el-
liptic constructs: coverage increases by 68 points
for NCC and 64 points for gapping against only
15, 13 and 5 points for RNR, mixed RNR-Shared
Subject and Shared Subject respectively. The rea-
son for this is that sentences are generated for
many input containing the latter types of con-
structions (RNR and Shared Subject) even with-
out rewriting. In fact, generation succeeds on the
non rewritten input for a majority of RNR (66%
PASS), Shared Subject (70% PASS) and mixed
RNR-Shared Subject (61% PASS) constructions
whereas it fails for almost all cases of gapping
(3% PASS) and of NCC (5% PASS). The reason
for this difference is that, while the grammar can-
not cope with headless constructions such as gap-
ping and NCC constructions, it can often provide
a derivation for shared subject sentences by using
the finite verb form in the source sentence and the
corresponding infinitival form in the target. Since
the infinitival does not require a subject, the tar-
get sentence is generated. Similarly, RNR con-
structions can be generated when the verb in the
source clause has both a transitive and an intran-
sitive form: the transitive form is used to gener-
ate the source clause and the intransitive for the
target clause. In short, many sentences contain-
ing a RNR or a shared subject construction can be
generated without rewriting because the grammar
overgenerates i.e., it produces sentences which are
valid sentences of English but whose phrase struc-
ture tree is incorrect.
Nevertheless, as the results show, rewriting
consistently helps increasing coverage even for
RNR (+15 points), Shared Subject (+5 points) and
mixed RNR-Shared Subject (+13 points) construc-
tions because (i) not all verbs have both a transi-
tive and an intransitive verb form and (ii) the input
for the elliptic clause may require a finite form for
the target verb (e.g., in sentences such as ?[they]i
weren?t fired but instead i were neglected? where the tar-
get clause includes an auxiliary requiring a past
46
participial which in this context requires a sub-
ject).
Precision is measured using the BLEU score.
For each input, we take the best score obtained
within the 5 derivations4 produced by the gener-
ator. Since the BLEU score reflects the degree to
which a sentence generated by the system matches
the corresponding Penn Treebank sentence, it is
impacted not just by elliptic coordination but also
by all linguistic constructions present in the sen-
tence. Nonetheless, the results show that rewrit-
ing consistently improves the BLEU score with an
overall increase of 0.09 points on the set of ellip-
tic sentences. Moreover, the consistent improve-
ment in terms of BLEU score for generated sen-
tences (COV column) shows that rewriting simul-
taneously improves both coverage and precision
that is, that for those sentences that are generated,
rewriting consistently improves precision.
Analysing the remaining failure cases. To bet-
ter assess the extent to which rewriting and the FB-
LTAG generation system succeed in generating el-
liptic coordinations, we performed error mining
on the elliptic data using our error miner described
in (Narayan and Gardent, 2012a). This method
permits highlighting the most likely sources of er-
ror given two datasets: a set of successful cases
and a set of failure cases. In this case, the suc-
cessful cases is the subset of rewritten input data
for elliptic coordination cases for which genera-
tion succeeds . The failure cases is the subset for
which generation fails. If elliptic coordination was
still a major source of errors, input nodes or edges
labelled with labels related to elliptic coordination
(e.g., the COORD and the GAP-X dependency rela-
tions or the CONJ part of speech tag) would sur-
face as most suspicious forms. In practice how-
ever, we found that the 5 top sources of errors
highlighted by error mining all include the DEP re-
lation, an unknown dependency relation used by
the Pennconverter when it fails to assign a label
to a dependency edge. In other words, most of the
remaining elliptic cases for which generation fails,
fails for reasons unrelated to ellipsis.
Comparison with other surface realisers
There is no data available on the performance
of surface realisers on elliptic input. However,
the performance of the surface realiser can be
4The language model used in the generator allows only 5
likely derivations (refer to section 5.1).
compared with those participating in the shallow
track of the SR challenge. On the SR training
data, the TAG surface realiser has an average
run time of 2.78 seconds per sentence (with an
average of 20 words per sentence), a coverage
of 82% and BLEU scores of 0.73 for covered
and 0.60 for all. On the SR test data, the realiser
achieves a coverage of 79% and BLEU scores of
0.59 for covered and 0.47 for all. In comparison,
the statistical systems in the SR Tasks achieved
0.88, 0.85 and 0.67 BLEU score on the SR test
set and the best symbolic system 0.25 (Belz et al,
2011).
6 Related work
Previous work on generating elliptic sentences has
mostly focused on identifying material that could
be elided and on defining procedures capable of
producing input structures for surface realisation
that support the generation of elliptic sentences.
Shaw (1998) developed a sentence planner
which generates elliptic sentences in 3 steps. First,
input data are grouped according to their simi-
larities. Second, repeated elements are marked.
Third, constraints are used to determine which oc-
currences of a marked element should be deleted.
The approach is integrated in the PLANDoc sys-
tem (McKeown et al, 1994) and shown to gen-
erate a wide range of elliptic constructs includ-
ing RNR, VPE and NCC using FUF/SURGE (El-
hadad, 1993), a realisation component based on
Functional Unification Grammar.
Theune et al (2006) describe how elliptic sen-
tences are generated in a story generation system.
The approach covers conjunction reduction, right
node raising, gapping and stripping and uses de-
pendency trees connected by rhetorical relations
as input. Before these trees are mapped to sen-
tences, repeated elements are deleted and their an-
tecedent (thesource element) is related by a SUB-
ORROWED relation to their governor in the ellip-
tic clause and a SUIDENTICAL relation to their
governor in the antecedent clause. This is then in-
terpreted by the surface realiser to mean that the
repeated element should be realised in the source
clause, elided in the target clause and that it li-
censes the same syntactic structure in both clauses.
Harbusch and Kempen (2009) have proposed a
module called Elleipo which takes as input unre-
duced, non-elliptic, syntactic structures annotated
with lexical identity and coreference relationships
47
between words and word groups in the conjuncts;
and returns as output structures annotated with
elision marks indicating which elements can be
elided and how (i.e., using which type of ellip-
sis). The focus is on developing a language in-
dependent module which can mediate between the
unreduced input syntactic structures produced by a
generator and syntactic structures that are enriched
with elision marks rich enough to determine the
range of possible elliptic and non elliptic output
sentences.
In CCG, grammar rules (type-raising and com-
position) permit combining non constituents into a
functor category which takes the shared element as
argument; and gapping remnants into a clause tak-
ing as argument its left-hand coordinated source
clause. White (2006) describes a chart based algo-
rithm for generating with CCG and shows that it
can efficiently realise NCC and gapping construc-
tions.
Our proposal differs from these approaches in
that it focuses on the surface realisation stage (as-
suming that the repeated elements have already
been identified) and is tested on a large corpus
of newspaper sentences rather than on hand-made
document plans and relatively short sentences.
7 Conclusion
In this paper, we showed that elliptic structures
are frequent and can impact the performance of
a surface realiser. In line with linguistic theory
and with some recent results on treebank annota-
tion, we argued that the representation of ellipsis
should involve empty categories and we provided
a set of tree rewrite rules to modify the SR data ac-
cordingly. We then evaluated the performance of a
TAG based surface realiser on 2398 elliptic input
derived by the SR task from the Penn Treebank
and showed that it achieved a coverage of 76% and
a BLEU score of 0.74 on generated sentences. Our
approach relies both on the fact that the grammar
is lexicalised (each rule is associated with a word
from the input) and on TAG extended domain of
locality (which permits using a rule anchored with
the empty string to reconstruct the missing syntax
in the elided clause thereby making it grammati-
cal).
We will release the 2398 input representations
we gathered for evaluating the generation of el-
liptic coordination so as to make it possible for
other surface realisers to be evaluated on their abil-
ity to generate ellipsis. In particular, its would
be interesting to examine how other grammar
based generators perform on this dataset such
as White?s CCG based generator (2006) (which
eschews empty categories by adopting a more
flexible notion of constituency) and Carroll and
Oepen?s HPSG based generator (2005) (whose do-
main of locality differs from that of TAG).
Acknowledgments
We would like to thank Anja Belz and Mike White
for providing us with the evaluation data and the
evaluation scripts. The research presented in this
paper was partially supported by the European
Fund for Regional Development within the frame-
work of the INTERREG IV A Allegro Project.
References
Eva Banik. 2004. Semantics of VP coordination
in LTAG. In Proceedings of the 7th International
Workshop on Tree Adjoining Grammars and Re-
lated Formalisms (TAG+), volume 7, pages 118?
125, Vancouver, Canada.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Ann Bies, Mark Ferguson, Katz Katz, Robert MacIn-
tyre, Victoria Tredinnick, Grace Kim, Marry Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for treebank II style penn tree-
bank project. University of Pennsylvania.
Aoife Cahill and Josef Van Genabith. 2006. Robust
pcfg-based generation using automatically acquired
lfg approximations. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
(COLING) and the 44th annual meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1033?1040, Sydney, Australia.
Charles B Callaway. 2003. Evaluating coverage for
large symbolic nlg grammars. In Proceedings of the
18th International joint conference on Artificial In-
telligence (IJCAI), volume 18, pages 811?816, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
John Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of the 2nd International
Joint Conference on Natural Language Process-
ing (IJCNLP), pages 165?176, Jeju Island, Korea.
Springer.
48
M. ?Cmejrek, J. Hajic?, and V. Kubon?. 2004. Prague
czech-english dependency treebank: Syntactically
annotated resources for machine translation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC), Lis-
bon, Portugal.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Peter W. Culicover and Ray Jackendoff. 2005. Simpler
Syntax. Oxford University Press.
Mary Dalrymple and Ronald M. Kaplan. 2000. Fea-
ture indeterminacy and feature resolution. Lan-
guage, pages 759?798.
Mary Dalrymple, Stuart M. Sheiber, and Fernando
C. N. Pereira. 1991. Ellipsis and higher-order unifi-
cation. Linguistics and Philosophy.
Michael Daum, Kilian Foth, and Wolfgang Menzel.
2004. Automatic transformation of phrase treebanks
to dependency trees. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC), Lisbon, Portugal.
Kais Dukes and Nizar Habash. 2011. One-step sta-
tistical parsing of hybrid dependency-constituency
syntactic representations. In Proceedings of the
12th International Conference on Parsing Technolo-
gies, pages 92?103, Dublin, Ireland. Association for
Computational Linguistics.
Michael Elhadad. 1993. Using argumentation to con-
trol lexical choice: a functional unification imple-
mentation. Ph.D. thesis, Columbia University.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference (INLG), pages 25?32, Salt
Fork, Ohio, USA. Association for Computational
Linguistics.
Rubino Gei?, Gernot Veit Batz, Daniel Grund, Sebas-
tian Hack, and Adam M. Szalkowski. 2006. Grgen:
A fast spo-based graph rewriting tool. In Proceed-
ings of the 3rd International Conference on Graph
Transformation, pages 383?397. Springer. Natal,
Brasil.
Jonathan Ginzburg and Ivan Sag. 2000. Interrogative
investigations. CSLI Publications.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M.A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, et al 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 1?18.
Karin Harbusch and Gerard Kempen. 2009. Gener-
ating clausal coordinate ellipsis multilingually: A
uniform approach based on postediting. In Proceed-
ings of the 12th European Workshop on Natural Lan-
guage Generation, pages 138?145, Athens, Greece.
Association for Computational Linguistics.
Daniel Hardt. 1993. Verb phrase ellipsis: Form,
meaning and processing. Ph.D. thesis, University
of Pennsylvania.
Richert Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA), pages
105?112, Tartu, Estonia.
Edward Keenan. 1971. Names, quantifiers, and the
sloppy identity problem. Papers in Linguistics,
4:211?232.
Andrew Kehler. 2002. Coherence in discourse. CSLI
Publications.
Yusuke Kubota and Robert Levine. 2012. Gapping
as like-category coordination. In Proceedings of the
7th international conference on Logical Aspects of
Computational Linguistics (LACL), pages 135?150,
Nantes, France. Springer-Verlag.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of the
12th International Natural Language Generation
Workshop, pages 17?24.
Roger Levy and Carl Pollard. 2001. Coordination and
neutralization in HPSG. Technology, 3:5.
Wolfgang Maier, Erhard Hinrichs, Julia Krivanek, and
Sandra Ku?bler. 2012. Annotating coordination
in the Penn Treebank. In Proceedings of the 6th
Linguistic Annotation Workshop (LAW), pages 166?
174, Jeju, Republic of Korea. Association for Com-
putational Linguistics.
Kathleen McKeown, Karen Kukich, and James Shaw.
1994. Practical issues in automatic documentation
generation. In Proceedings of the fourth conference
on Applied natural language processing (ANLC),
pages 7?14, Stuttgart, Germany. Association for
Computational Linguistics.
Jason Merchant. 2001. The syntax of silence: Sluicing,
islands, and the theory of ellipsis. Oxford Univer-
sity Press.
Shashi Narayan and Claire Gardent. 2012a. Error min-
ing with suspicion trees: Seeing the forest for the
trees. In Proceedings of the 24th International Con-
ference on Computational Linguistics (COLING),
Mumbai, India.
Shashi Narayan and Claire Gardent. 2012b. Structure-
driven lexicalist generation. In Proceedings of the
24th International Conference on Computational
Linguistics (COLING), Mumbai, India.
49
Ivan Sag. 1976. Deletion and logical form. Ph.D.
thesis, Massachusetts Institute of Technology, Cam-
bridge, Massachusetts.
Anoop Sarkar and Arvind Joshi. 1996. Coordination
in tree adjoining grammars: Formalization and im-
plementation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 610?
615, Copenhagen, Denmark. Association for Com-
putational Linguistics.
Djame? Seddah. 2008. The use of mctag to process
elliptic coordination. In Proceedings of The Ninth
International Workshop on Tree Adjoining Gram-
mars and Related Formalisms (TAG+ 9), volume 1,
page 2, Tu?bingen, Germany.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a ger-
man treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey.
James Shaw. 1998. Segregatory coordination and
ellipsis in text generation. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Con-
ference on Computational Linguistics, pages 1220?
1226, Montreal, Quebec, Canada.
Mark Steedman. 1996. Surface Structure and Inter-
pretation, volume 30. MIT press Cambridge, MA.
Marie?t Theune, Feikje Hielkema, and Petra Hendriks.
2006. Performing aggregation and ellipsis using dis-
course structures. Research on Language & Compu-
tation, 4(4):353?375.
Jeoren van Craenenbroeck. 2010. The syntax of ellip-
sis: Evidence from Dutch dialects. Oxford Univer-
sity Press.
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian dependency tree-
bank. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 410?419, Singapore. Association for Compu-
tational Linguistics.
Michael White. 2006. Efficient realization of coordi-
nate structures in combinatory categorial grammar.
Research on Language & Computation, 4(1):39?75.
Edwin Williams. 1977. Discourse and logical form.
Linguistic Inquiry.
Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora. In
Proceedings of the Workshop on Using Corpora for
Natural Language Generation (UCNLG), volume 5,
pages 49?54.
50
