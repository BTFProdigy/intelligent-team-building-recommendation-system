Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 777?786,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Behind the Article: Recognizing Dialog Acts in Wikipedia Talk Pages
Oliver Ferschke?, Iryna Gurevych?? and Yevgen Chebotar?
? Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
? Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science
Technische Universita?t Darmstadt
http://www.ukp.tu-darmstadt.de
Abstract
In this paper, we propose an annota-
tion schema for the discourse analysis of
Wikipedia Talk pages aimed at the coor-
dination efforts for article improvement.
We apply the annotation schema to a cor-
pus of 100 Talk pages from the Simple
English Wikipedia and make the resulting
dataset freely available for download1. Fur-
thermore, we perform automatic dialog act
classification on Wikipedia discussions and
achieve an average F1-score of 0.82 with
our classification pipeline.
1 Introduction
Over the past decade, the paradigm of information
sharing in the web has shifted towards participa-
tory and collaborative content production. Texts
are no longer exclusively prepared by individuals
and then shared with the community. They are in-
creasingly created collaboratively by multiple au-
thors and iteratively revised by the community.
When researchers first conducted surveys on
professional writers in the 1980s, they found that
the collaborative writing process differs consider-
ably from the way individual writing is done (Pos-
ner and Baecker, 1992). In joint writing, the writ-
ers have to externalize processes that are other-
wise not made explicit, like the planning and the
organization of the text. The authors have to com-
municate how the text should be written and what
exactly it should contain.
Today, many tools are available that support
collaborative writing. A tool that has particu-
larly taken hold is the Wiki, a web-based, asyn-
1http://www.ukp.tu-darmstadt.de/data/
wikidiscourse
chronous co-authoring tool. A unique character-
istic of Wikis is the documentation of the edit
history which keeps track of every change that
is made to a Wiki page. With this information,
it is possible to reconstruct the writing process
from the beginning to the end. Additionally, many
Wikis offer their users a communication platform,
the Talk pages, where they can discuss the ongo-
ing writing process with other users.
The most prominent example for a successful,
large-scale Wiki is Wikipedia, a collaboratively
created online encyclopedia, which has grown
considerably since its launch in 2001, and con-
tains a total of almost 20 million articles in 282
languages and dialects, as of Sept. 2011. As there
is no editorial body that manages Wikipedia top-
down, it is an open question how the huge on-
line community around Wikipedia regulates and
enforces standards of behavior and article qual-
ity. The user discussions on the article Talk pages
might shed light on this issue and give an insight
into the otherwise hidden processes of collabora-
tion that, until now, could only be analyzed via
interviews or group observations in experimental
settings.
The main goal of the present paper is to analyze
the content of the discussion pages of the Simple
English Wikipedia with respect to the dialog acts
aimed at the coordination efforts for article im-
provement. Dialog acts, according to the classic
speech act theory (Austin, 1962; Searle, 1969),
represent the meaning of an utterance at the level
of illocutionary force, i.e. a dialog act label con-
cisely characterizes the intention and the role of a
contribution in a dialog. We chose the Simple En-
glish Wikipedia for our initial analysis, because
we are able to obtain more representative results
777
by covering almost 15% of all relevant Talk pages,
as opposed to the much smaller fraction we could
achieve for the English Wikipedia. The long-term
goal of this work is to identify relations between
contributions on the Talk pages and particular arti-
cle edits. We plan to analyze the relation between
article discussions and article content and identify
the edits in the article revision history that react to
the problems discussed on the Talk page. In com-
bination with article quality assessment (Yaari et
al., 2011), this opens up the possibility to iden-
tify successful patterns of collaboration which in-
crease the article quality. Furthermore, our work
will enable practical applications. By augment-
ing Wikipedia articles with the information de-
rived from automatically labeled discussions, arti-
cle readers can be made aware of particular prob-
lems that are being discussed on the Talk page
?behind the article?.
Our primary contributions in this paper are: (1)
an annotation schema for dialog acts reflecting
the efforts for coordinating the article improve-
ment; (2) the Simple English Wikipedia Dis-
cussion (SEWD) corpus, consisting of 100 seg-
mented and annotated Talk pages which we make
freely available for download; and (3) a dialog
act classification pipeline that incorporates sev-
eral state of the art machine learning algorithms
and feature selection techniques and achieves an
average F1-score of .82 on our corpus.
2 Related Work
The analysis of speech and dialog acts has its
roots in the linguistic field of pragmatics. In
1962, John Austin shifted the focus from the mere
declarative use of language as a means for making
factual statements towards its non-declarative use
as a tool for performing actions. The speech act
theory was further systematized by Searle (1969),
whose classification of illocutionary acts (Searle,
1976) is still used as a starting point for creating
dialog act classification schemata for natural lan-
guage processing.
A well known, domain- and task-independent
annotation schema is DAMSL (Core and Allen,
1997). It was created as the standard annotation
schema for dialog tagging on the utterance level
by the Discourse Resource Initiative. It uses a
four-dimensional tagset that allows arbitrary label
combinations for each utterance. Jurafsky et al
(1997) augmented the DAMSL schema to fit the
peculiarities of the Switchboard corpus. The re-
sulting SWDB-DAMSL schema contained more
than 220 distinct labels which have been clustered
to 42 coarse grained labels. Both schemata have
often been adapted for special purpose annotation
tasks.
With the rise of the social web, the amount of
research analyzing user generated discourse sub-
stantially increased. In addition to analyzing web
forums (Kim et al 2010a), chats (Carpenter and
Fujioka, 2011) and emails (Cohen et al 2004),
Wikipedia Talk pages have recently moved into
the center of attention of the research community.
Vie?gas et al(2007) manually annotate 25
Wikipedia article discussion pages with a set of
11 labels in order to analyze how Talk pages are
used for planning the work on articles and resolv-
ing disputes among the editors. Schneider et al
(2011) extend this schema and manually annotate
100 Talk pages with 15 labels. They confirm the
findings of Vie?gas et althat coordination requests
occur most frequently in the discussions.
Bender et al(2011) describe a corpus of 47
Talk pages which have been annotated for author-
ity claims and alignment moves. With this cor-
pus, the authors analyze how the participants in
Wikipedia discussions establish their credibility
and how they express agreement and disagree-
ment towards other participants or topics.
From a different perspective, Stvilia et al
(2008) analyze 60 discussion pages in regard to
how information quality (IQ) in Wikipedia arti-
cles is assessed on the Talk pages and which types
of IQ problems are identified by the community.
They describe a Wikipedia IQ assessment model
and map it to established frameworks. Further-
more, they provide a list of IQ problems along
with related causal factors and necessary actions
which has also inspired the design of our annota-
tion schema.
Finally, Laniado et al(2011) examine
Wikipedia discussion networks in order to
capture structural patterns of interaction. They
extract the thread structure from all Talk pages in
the English Wikipedia and create tree structures
of the discussion. The analysis of the graphs
reveals patterns that are unique to Wikipedia
discussions and might be used as a means to
characterize different types of Talk pages.
To the best of our knowledge, there is no
work yet that uses machine learning to automati-
778
Figure 1: Structure of a Talk page: a) Talk page title,
b) untitled discussion topic, c) titled discussion topic,
d) unsigned turns, e) signed turns, f) topic title
cally classify user contributions in Wikipedia Talk
pages. Furthermore, there is no corpus available
that reflects the efforts of article improvement in
Wikipedia discussions. This is the subject of our
work.
3 Annotation Schema
The main purpose of Wikipedia Talk pages is the
coordination of the editing process with the goal
of improving and sustaining the quality of the re-
spective article. The criteria for article quality in
Wikipedia are loosely defined in the guidelines for
?good articles?2 and ?very good articles?3. Ac-
cording to these guidelines, distinguished articles
must be well-written in simple English, compre-
hensive, neutral, stable, accurate, verifiable and
follow the Wikipedia style guidelines4. These cri-
teria are the main points of reference in the dis-
cussions on the Talk pages.
Discourse analysis, as it is performed in this pa-
per, can be carried out on various levels, depend-
ing on what is regarded as the smallest unit of the
discourse. In this work, we focus on turns, not
on individual utterances, as we are interested in a
coarse-grained analysis of the discourse-structure
as a first step towards a finer-grained discourse
analysis. We define a turn (or contribution) as the
body of text that is added by an individual contrib-
utor in one or more revisions to a single discus-
sion topic until another contributor edits the page.
Furthermore, a topic (or discussion) is the body
of turns that revolve around a single matter. They
2http://simple.wikipedia.org/wiki/WP:RGA
3http://simple.wikipedia.org/wiki/WP:RVGA
4http://simple.wikipedia.org/wiki/WP:STYLE
are usually headed by a topic title. Finally, the
thread structure designates the sequence of turns
and their indentation levels on the Talk page. A
structural overview of a Talk page and its con-
stituents can be seen in Figure 1.
We composed an annotation schema that re-
flects the coordination efforts for article improve-
ment. Therefore, we manually analyzed a set
of thirty Talk pages from the Simple English
Wikipedia to identify the types of article defi-
ciencies that are discussed and the way article
improvement is coordinated. We furthermore
incorporated the findings from an information-
scientific analysis of information quality in
Wikipedia (Stvilia et al 2008), which identifies
twelve types of quality problems, like e.g. Accu-
racy, Completeness or Relevance. Our resulting
tagset consists of 17 labels (cf. Table 1) which can
be subdivided into four higher level categories:
Article Criticism Denote comments that iden-
tify deficiencies in the article. The criticism
can refer to the article as a whole or to indi-
vidual parts of the article.
Explicit Performative Announce, report or sug-
gest editing activities.
Information Content Describe the direction of
the communication. A contribution can be
used to communicate new information to
others (IP), to request information (IS), or
to suggest changes to established facts (IC).
The IP label applies to most of the contri-
butions as most comments provide a certain
amount of new information.
Interpersonal Describe the attitude that is ex-
pressed towards other participants in the dis-
cussion and/or their comments.
Since a single turn may consist of several utter-
ances, it can consequently comprise multiple di-
alog acts. Therefore, we designed the annotation
study as a multi-label classification task, i.e. the
annotators can assign one or more labels to each
annotation unit. Each label is chosen indepen-
dently. Table 1 shows the labels, their respective
definitions and an example from our corpus.
4 Corpus Creation and Analysis
The SEWD corpus consists of 100 annotated Talk
pages extracted from a snapshot of the Simple En-
779
Label Description Example
Article Criticism
CM Content incomplete or lacking detail
It should be added (1) that voters may skip prefer-
ences, but (2) that skipping preferences has no impact
on the result of the elections.
CW Lack of accuracy or correctness
Kris Kringle is NOT a Germanic god, but an English
mispronunciation of Christkind, a German word that
means ?the baby Jesus?.
CU Unsuitable or unnecessary content
The references should be removed. The reason: The
references are too complicated for the typical reader
of simple Wikipedia.
CS Structural problems Also use sectioning, and interlinking
CL Deficiencies in language or style
This section needs to be simplified further; there are a
lot of words that are too complex for this wiki.
COBJ Objectivity issues
This article seems to take a clear pro-Christian, anti-
commercial view.
CO Other kind of criticism
I have started an article on Google. It needs improve-
ment though.
Explicit Performative
PSR Explicit suggestion, recommendation or request This section needs to be simplified further
PREF Explicit reference or pointer
Got it. The URL is http://www.dmbeatles.com/
history.php?year=1968
PFC Commitment to an action in the future Okay, I forgot to add that, I?ll do so later tonight.
PPC Report of a performed action
I took and hopefully simplified the ?[[en:Prehistoric
music?Prehistoric music]]? article from EnWP
Information Content
IP Information providing ?Depression? is the most basic term there is.
IS Information seeking
So what kind of theory would you use for your music
composing?
IC Information correcting
In linguistics and generally speaking, when Talking
about the lexicon in a language, words are usually cat-
egorized as ?nouns?, ?verbs?, ?adjectives? and so on.
The term ?doing word? does not exist.
Interpersonal
ATT+
Positive attitude towards other contributor or
acceptance
Thank you.
ATTP Partial acceptance or partial rejection
Okay, I can understand that, but some citations are
going to have to be included for [[WP:V]].
ATT-
Negative attitude towards other contributor or
rejection
Now what? You think you know so much about every-
thing, and you are not even helping?!
Table 1: Annotation schema for the dialog act classification in Wikipedia discussion pages with examples from
the SEWD Corpus. Some examples have been shortened to fit the table.
glish Wikipedia from Apr 4th 2011.5 Technically
speaking, a Talk page is a normal Wiki page lo-
cated in one of the Talk namespaces. In this work,
we focus on article Talk pages and do not re-
gard User Talk pages. We selected the discussion
pages according to the number of turns they con-
tain. First, we discarded all discussion pages with
less than four contributions. We then analyzed
the distribution of turn counts per discussion page
in the remaining set of pages and defined three
classes: (i) discussion pages with 4-10 turns, (ii)
5The snapshot contains 69900 articles and 5783 Talk
pages of which 683 contained more than 3 contributions.
pages with 11-20 turns, and (iii) pages with more
than 20 turns. We then randomly extracted 50 dis-
cussion pages from class (i), 40 pages from class
(ii) and 10 pages from class (iii). This decision is
grounded in the restricted resources for the human
annotation task.
Data Preprocessing Due to a lack of discussion
structure, extracting the discussion threads from
the Talk pages requires a substantial amount of
preprocessing. Laniado et al(2011) tackle the
thread extraction by using text indentation and in-
serted user signatures as clues. We found these
780
attributes to be insufficient for a reliable recon-
struction of the thread structure.6
Our preprocessing approach consists of three
steps: data retrieval, topic segmentation and turn
segmentation. For retrieving the discussion pages,
we use the Java Wikipedia Library (JWPL) (Zesch
et al 2008), which offers efficient, database-
driven access to the contents of Wikipedia. We
segment the individual Talk pages into discus-
sions topics using the MediaWiki parser that
comes with JWPL. In our corpus, the parser man-
aged to identify all topic boundaries without any
errors. The most complex preprocessing step is
the turn segmentation.
First, we use the revision history of the Talk
page to identify the author and the creation time
of each paragraph. We use the Wikipedia Revi-
sion Toolkit (Ferschke et al 2011) to examine the
changes between adjacent revisions of the Talk
page in order to identify the exact time a piece of
text was added as well as the author of the con-
tribution. We have to filter out malicious edits
from the history, as they would negatively affect
the segmentation process. We therefore disregard
all edits that are reverted in later later revisions.
In contrast to vandalism on article pages, this ap-
proach has proven to be sufficient to detect van-
dalism in the Talk page history.
Within each discussion topic, we aggregate all
adjacent paragraphs with the same author and the
same time stamp to one turn. In order to account
for turns that were written in multiple revisions,
we regard all time stamps within a window of 10
minutes7 as belonging to the same turn, unless the
page was edited by another user in the meantime.
Finally, the turn is marked with the indentation
level of its least indented paragraph. This infor-
mation is used to identify the relationship between
the turns, since indentation is used to indicate a
reply to an existing comment in the discussion.
A co-author of this paper evaluated the ac-
ceptability of the boundaries of each turn in the
SEWD corpus and found that 94% of the 1450
turns were correctly segmented. Turns with seg-
mentation errors were not included in the gold
standard.
6Vie?gas et al(2007) reported that only 67% of the con-
tributions on Wikipedia Talk pages are signed, which makes
signatures an unreliable predictor for turn boundaries.
7We experimentally tested values between 1 and 60 min-
utes.
Annotation Process For our annotation study,
we used the freely available MMAX2 annotation
tool8. Two annotators were introduced to the an-
notation schema by an instructor and trained on
an extra set of ten discussion pages. During the
annotation of the corpus, the annotators were al-
lowed to discuss difficult cases and could consult
the instructor if in doubt. They had access to the
segmented discussion pages within the MMAX2
tool as well as to the original Wikipedia articles
and discussion pages on the web.
The reconciliation of the annotations was car-
ried out by an expert annotator. In order to obtain
a consolidated gold standard, the expert decided
all cases in which the annotations of the two an-
notators did not match. Descriptive statistics for
the label assignments of each annotator and for
the gold standard can be seen in Table 2 and will
be further discussed in Section 4.2.
Corpus Format We publish our SEWD cor-
pus in two formats9, the original MMAX format,
and as XMI files for further processing with the
Apache Unstructured Information Management
Architecture10. For the latter format, we also pro-
vide the type system which defines all necessary
corpus specific types needed for using the data in
an NLP pipeline.
4.1 Inter-Annotator Agreement
To evaluate the reliability of our dataset, we per-
form a detailed inter-rater agreement study. For
measuring the agreement of the individual labels,
we report the observed agreement, Kappa statis-
tics (Carletta, 1996), and F1-scores. The latter are
computed by treating one annotator as the gold
standard and the other one as predictions (Hripc-
sak and Rothschild, 2005). The scores can be seen
in Table 2.
The average observed agreement across all la-
bels is P?O = .94. The individual Kappa scores
largely fall into the range that Landis and Koch
(1977) regard as substantial agreement, while
three labels are above the more strict .8 thresh-
old for reliable annotations (Artstein and Poesio,
2008). Furthermore, we obtain an overall pooled
Kappa (De Vries et al 2008) of ?pool = .67,
8http://www.mmax2.net
9http://www.ukp.tu-darmstadt.de/data/
wikidiscourse
10http://uima.apache.org
781
Annotator 1 Annotator 2 Inter-Annotator Agreement Gold Standard
Label N Percent N Percent NA1?A2 PO ? F1 N Percent
Article Criticism
CM 183 13.4% 105 7.7% 193 .93 .63 .66 116 8.5%
CW 106 7.8% 57 4.2% 120 .95 .52 .55 70 5.1%
CU 69 5.0% 35 2.6% 83 .95 .38 .40 42 3.1%
CS 164 12.0% 101 7.4% 174 .94 .66 .69 136 9.9%
CL 195 14.3% 199 14.6% 244 .93 .73 .77 219 16.0%
COBJ 27 2.0% 23 1.7% 29 .99 .84 .84 27 2.0%
CO 20 1.5% 59 4.3% 71 .95 .18 .20 48 3.5%
Explicit Performative
PSR 458 33.5% 351 25.7% 503 .86 .66 .76 406 29.7%
PREF 43 3.1% 31 2.3% 51 .98 .61 .62 45 3.3%
PFC 73 5.3% 65 4.8% 86 .98 .76 .77 77 5.6%
PPC 357 26.1% 340 24.9% 371 .97 .92 .94 358 26.2%
Information Content
IP 1084 79.3% 1027 75.1% 1135 .89 .69 .93 1070 78.3%
IS 228 16.7% 208 15.2% 256 .95 .80 .83 220 16.1%
IC 187 13.7% 109 8.0% 221 .89 .46 .51 130 9.5%
Interpersonal
ATT+ 71 5.2% 140 10.2% 151 .94 .55 .58 144 10.5%
ATTP 71 5.2% 30 2.2% 79 .96 .42 .44 33 2.4%
ATT- 67 4.9% 74 5.4% 100 .96 .56 .58 87 6.4%
Table 2: Label frequencies and inter-annotator agreement. NA1?A2 denotes the number of turns that have been
labeled with the given label by at least one annotator. PO denotes the observed agreement.
which is defined as
?pool =
P?O ? P?E
1? P?E
(1)
with
P?O =
1
L
L?
l=1
POl , P?E =
1
L
L?
l=1
PEl (2)
where L denotes the number of labels, PEl the
expected agreement and POl the observed agree-
ment of the lth label. ?pool is regarded to be more
accurate than an averaged Kappa.
For assessing the overall inter-rater reliabil-
ity of the label set assignments per turn, we
chose Krippendorff?s Alpha (Krippendorff, 1980)
using MASI, a measure of agreement on set-
valued items, as the distance function (Passon-
neau, 2006). MASI accounts for partial agree-
ment if the label sets of both annotators overlap
in at least one label. We achieved an Alpha score
of ? = .75. According to Krippendorff, datasets
with this score are considered reliable and allow
tentative conclusions to be drawn.
The CO label showed the lowest agreement of
only ? = .18. The label was supposed to cover
any criticism that is not covered by a dedicated
label. However, the annotators reported that they
chose this label when they were unsure whether a
particular criticism label would fit a certain turn
or not.
Labels in the interpersonal category all show
agreement scores below 0.6. It turned out that the
annotators had a different understanding of these
labels. While one annotator assigned the labels
for any kind of positive or negative sentiment, the
other used the labels to express agreement and
disagreement between the participants of a dis-
cussion.
A common problem for all labels were contri-
butions with a high degree of indirectness and im-
plicitness. Indirect contributions have to be in-
terpreted in the light of conversational implica-
ture theory (Grice, 1975), which requires contex-
tual knowledge for decoding the intentions of a
speaker. For example, the message
Is population density allowed to be n/a?
has the surface form of a question. However, the
context of the discussion revealed that the author
tried to draw attention to the missing figure in the
article and requested it to be filled or removed.
The annotators rarely made use of the context,
which was a major source for disagreement in the
study.
782
Another difficulty for the annotators were long
discussion turns. While the average turn consists
of 42 tokens, the largest contribution in the cor-
pus is 658 tokens long. Turns of this size can
cover multiple aspects and potentially comprise
many different dialog acts, which increases the
probability of disagreement. This issue can be ad-
dressed by going from the turn level to the utter-
ance level in future work.
A comparison of our results with the agreement
reported for other datasets shows that the reliabil-
ity of our annotations lies well within the field of
the related work. Bender et al(2011) carried out
an annotation study of social acts in 365 discus-
sions from 47 Wikipedia Talk pages. They report
Kappa scores for thirteen labels in two categories
ranging from .13 to .66 per label. The overall
agreement for each category was .50 and .59, re-
spectively, which is considerably lower than our
?pool = .67. Kim et al(2010b) annotate pairs of
posts taken from an online forum. They use a di-
alog act tagset with twelve labels customized for
modeling troubleshooting-oriented forum discus-
sions. For their corpus of 1334 posts, they report
an overall Kappa of .59. Kim et al(2010a) iden-
tify unresolved discussions in student online fo-
rums by annotating 1135 posts with five different
speech acts. They report Kappa scores per speech
act between .72 and .94. Their better results might
be due to a more coarse grained label set.
4.2 Corpus Analysis
The SEWD corpus contains 313 discussions con-
sisting of 1367 turns by 337 users. The average
length of a turn is 42 words. 208 of the 337
contributors are registered Wikipedia users, 129
wrote anonymously. On average, each contributor
wrote 168 words in 4 turns. However, there was a
cluster of 16 people with ? 20 contributions.
Table 2 shows the frequencies of all labels in
the SEWD corpus. The most frequent labels are
information providing (IP), requests (PSR) and
reports of performed edits (PPC). The IP-label
was assigned to more than 78% of all 1367 turns,
because almost every contribution provides a cer-
tain amount of information. The label was only
omitted if a turn merely consisted of a discussion
template but did not contain any text or if it exclu-
sively contained questions.
More than a quarter of the turns are labeled
with PSR and PPC, respectively. This indicates
that edit requests and reports of performed edits
are the main subject of discussion. Generally, it is
more common that edits are reported after they
have been made than to announce them before
they are carried out, as can be seen in the ratio
of PPC to PFC labels. The number of turns la-
beled with PSR is almost the same as the number
of contributions labeled with either PPC or PFC.
This allows the tentative conclusion that nearly all
requests potentially lead to an edit action. As a
matter of fact, the most common label adjacency
pair11 in the corpus is PSR?PPC, which substan-
tiates this assumption.
Article criticism labels have been assigned to
39.4% of all turns. Almost half (241) of the labels
from this class are assigned to the first turn of a
discussion. This shows that it is common to open
a discussion in reference to a particular deficiency
of the article. The large number of CL labels com-
pared to other labels from the same category is
due to the fact that the Simple English Wikipedia
requires authors to write articles in a way that they
are understandable for non-native speakers of En-
glish. Therefore, the use of adequate language is
one of the major concerns of the Simple English
Wikipedia community.
5 Automatic Dialog Act Classification
For the automatic classification of dialog acts in
Wikipedia Talk pages, we transform the multi-
label classification problem into a binary classi-
fication task (Tsoumakas et al 2010). We train a
binary classifier for each label using the WEKA
data-mining software (Hall et al 2009). We use
three learners for the classification task, a Naive
Bayes classifier, J48, an implementation of the
C4.5 decision tree algorithm (Quinlan, 1992) and
SMO, an optimization algorithm for training sup-
port vector machines (Platt, 1998). Finally, we
combine the best performing learners for each la-
bel in a UIMA-based classification pipeline (Fer-
rucci and Lally, 2004).
Features for Dialog Act Classification As fea-
tures, we use all uni-, bi- and trigrams that oc-
curred in at least three different turns. Further-
more, we include the time distance to the previ-
ous and the next turn (in seconds), the length of
the current, previous and next turn (in tokens), the
11A label transition A ? B is recorded if two adjacent
turns are labeled with A and B, respectively.
783
position of the turn within the discussion, the in-
dentation level of the turn and two binary features
indicating whether a turn references or is refer-
enced by another turn.12 In order to capture the
sequential nature of the discussions, we use the
n-grams of the previous and the next turn as addi-
tional features.
Balancing Positive and Negative Instances
Since the number of positive instances for each
label is small compared to the number of nega-
tive instances, we create a balanced dataset which
contains an equal amount of positive and nega-
tive instances. Therefore, we randomly select the
appropriate number of negative instances and dis-
card the rest. This improves the classification per-
formance on every label for all three learners.
Feature Selection Using the full set of features,
we achieve the following macro/micro averaged
F1-scores: 0.29 / 0.57 for Naive Bayes, 0.42 /
0.66 for J48 and 0.43 / 0.72 for SMO. To fur-
ther improve the classification performance, we
reduce the feature space using two feature selec-
tion techniques, the ?2 metric (Yang and Ped-
ersen, 1997) and the Information Gain approach
(Mitchell, 1997). For each label, we train separate
classifiers using the top 100, 200 and 300 features
obtained by each feature selection technique and
choose the best performing set for our final clas-
sification pipeline.
Indentation and temporal distance to the pre-
ceding turn proved to be the best ranked non-
lexical features overall. Additionally, the turn po-
sition within the topic was a crucial feature for
most labels in the criticism class and for PSR and
IS labels. This is not surprising, because article
criticism, suggestions and questions tend to oc-
cur in the beginning of a discussion. The two
reference features have not proven to be useful.
The relational information was better covered by
the indentation feature. The subjective quality of
the lexical features seems to be correlated with
the inter-annotator agreement of the respective la-
bels. Features for labels with low agreement con-
tain many n-grams without any recognizable se-
mantic connection to the label. For labels with
good agreement, the feature lists almost exclu-
sively contain meaningful lexical cues.
12A turn Y references a preceding turn X if the indenta-
tion level of Y is one level deeper than of X .
Label Human Base
Naive
Bayes
J48 SMO Best
CM .66 .07 .68 .48 .66 .68
CW .55 .01 .70 .20 .56 .70
CU .40 .07 .66 .35 .59 .66
CS .69 .09 .67 .67 .75 .75
CL .77 .11 .70 .66 .73 .73
COBJ .84 .04 .78 .51 .63 .78
CO .20 .02 .61 .06 .39 .61
PSR .76 .30 .72 .70 .76 .76
PREF .62 .00 .76 .41 .64 .76
PFC .77 .04 .70 .62 .73 .73
PPC .94 .25 .74 .82 .85 .85
IP .93 .74 .83 .93 .93 .93
IS .83 .16 .79 .86 .85 .86
IC .51 .06 .67 .32 .59 .67
ATT+ .58 .10 .61 .65 .72 .72
ATTP .44 .03 .72 .25 .62 .72
ATT- .58 .07 .52 .30 .52 .52
Macro .65 .13 .70 .52 .68 .73
Micro .79 .35 .74 .75 .80 .82
Table 3: F1-Scores for the balanced set with feature
selection on 10-fold cross-validation. Base refers to
the baseline performance, Best to our classification
pipeline.
Classification Results Table 3 shows the per-
formance of all classifiers and our final classi-
fication pipeline evaluated on 10-fold cross val-
idation. Naive Bayes performed surprisingly
well and showed the best macro averaged scores
among the three learners while SMO showed the
best micro averaged performance. We compare
our results to a random baseline and to the per-
formance of the human annotators (cf. Table 3
and Figure 2). The baseline assigns the dialog act
labels at random according to their frequency dis-
tribution in the gold standard. Our classifier out-
performed the baseline significantly on all labels.
The comparison with the human performance
shows that our system is able to reach the human
performance. In most cases, the annotation agree-
ment is reliable, and so are the results of the auto-
matic classification. For the labels CU and CO,
the inter-annotator agreement is not high. The
comparably good performance of the classifiers
on these labels shows that the instances do have
shared characteristics. Human raters, however,
have difficulties recognizing these labels consis-
tently. Thus, their definitions need to be refined in
future work.
To our knowledge, none of the related work on
discourse analysis of Wikipedia Talk pages per-
784
CM CW C
U CS CL
CO
BJ CO PS
R
PR
EF PF
C
PP
C IP IS IC
AT
T+
AT
TP
AT
T-
0
0.2
0.4
0.6
0.8
1
F
1
-s
co
re
Best Human Baseline
Figure 2: F1-Scores for our classification pipeline (Best), the human performance and baseline performance.
formed automatic dialog act classification. How-
ever, there has been previous work on classify-
ing speech acts in other discourse types. Kim et
al. (2010a) use Support Vector Machines (SVM)
and Transformation Based Learning (TBL) for
the automatic assignment of five speech acts to
posts taken from student online forums. They re-
port individual F1-scores per label which result
in a macro average of 0.59 for SVM and 0.66
for TBL. Cohen et al(2004) classify speech acts
in emails. They train five binary classifiers us-
ing several learners on 1375 emails and report F1
scores per speech act between .44 and .85. De-
spite the larger tagset, our classification approach
achieves an average F1-score of .82 and therefore
lies in the top ranks of the related work.
6 Conclusions
In this paper, we proposed an annotation schema
for the discourse analysis of Wikipedia discus-
sions aimed at the coordination efforts for article
improvement. We applied the annotation schema
to a corpus of 100 Wikipedia Talk pages, which
we make freely available for download. A thor-
ough analysis of the inter-annotator agreement
showed that the dataset is reliable. Finally, we
performed automatic dialog act classification on
Wikipedia Talk pages. Therefore, we combined
three machine learning algorithms and two feature
selection techniques to a classification pipeline,
which we trained on our SEWD corpus. We
achieve an average F1-score of .82, which is com-
parable to the human performance of .79. The
ability to automatically classify discussion pages
will help to investigate the relations between arti-
cle discussions and article edits, which is an im-
portant step towards understanding the processes
of collaboration in large-scale Wikis. Further-
more, it will be the basis for practical applications
that bring the hidden content of Talk pages to the
attention of article readers.
Acknowledgments
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the Hessian research excellence program
?Landes-Offensive zur Entwicklung Wissen-
schaftlich-o?konomischer Exzellenz? (LOEWE)
as part of the research center ?Digital Humani-
ties?.
References
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555?596, December.
John L. Austin. 1962. How to Do Things with Words.
Clarendon Press, Cambridge, UK.
Emily M. Bender, Jonathan T. Morgan, Meghan Ox-
ley, Mark Zachry, Brian Hutchinson, Alex Marin,
Bin Zhang, and Mari Ostendorf. 2011. Annotat-
ing Social Acts: Authority Claims and Alignment
Moves in Wikipedia Talk Pages. In Proceedings of
the Workshop on Language in Social Media, pages
48?57, Portland, Oregon, USA.
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics, 22(2):249?254.
Tamitha Carpenter and Emi Fujioka. 2011. The Role
and Identification of Dialog Acts in Online Chat. In
Proceesings of the Workshop on Analyzing Micro-
text at the 25th AAAI Conference on Artificial Intel-
ligence, San Francisco, CA, USA.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to Classify Email into
?Speech Acts?. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 309?316, Barcelona, ES.
785
Mark G. Core and James F. Allen. 1997. Cod-
ing dialogs with the DAMSL annotation scheme.
In Proceedings of the Working Notes of the AAAI
Fall Symposium on Communicative Action in Hu-
mans and Machines, pages 28?35, Cambridge, MA,
USA.
Han De Vries, Marc N. Elliott, David E. Kanouse, and
Stephanie S. Teleki. 2008. Using Pooled Kappa
to Summarize Interrater Agreement across Many
Items. Field Methods, 20(3):272?282.
David Ferrucci and Adam Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10:327?348.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently
Accessing Wikipedia?s Edit History. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. System Demonstrations, pages
97?102, Portland, OR, USA.
Paul Grice. 1975. Logic and Conversation. In Pe-
ter Cole and Jerry L. Morgan, editors, Syntax and
Semantics, volume 3. New York: Academic Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11:10?18.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the f-measure, and reliability in infor-
mation retrieval. Journal of the American Medical
Informatics Association, 12(3):296?298.
Dan Jurafsky, Liz Shriberg, and Debbra Biasca. 1997.
Switchboard SWBD-DAMSL Shallow-Discourse-
Function Annotation Coders Manual. Technical
Report Draft 13, University of Colorado, Institute
of Cognitive Science.
Jihie Kim, Jia Li, and Taehwan Kim. 2010a. To-
wards Identifying Unresolved Discussions in Stu-
dent Online Forums. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 84?
91, Los Angeles, CA, USA.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. In Pro-
ceedings of the Fourteenth Conference on Compu-
tational Natural Language Learning, CoNLL ?10,
pages 192?202, Stroudsburg, PA, USA.
Klaus Krippendorff. 1980. Content Analysis: An
Introduction to Its Methodology. Thousand Oaks,
CA: Sage Publications.
J. Richard Landis and Gary G. Koch. 1977. An Appli-
cation of Hierarchical Kappa-type Statistics in the
Assessment of Majority Agreement among Multi-
ple Observers. Biometrics, 33(2):363?374, June.
David Laniado, Riccardo Tasso, Yana Volkovich, and
Andreas Kaltenbrunner. 2011. When the Wikipedi-
ans Talk: Network and Tree Structure of Wikipedia
Discussion Pages. In Proceedings of the 5th Inter-
national AAAI Conference on Weblogs and Social
Media, Dublin, IE.
Tom Mitchell. 1997. Machine Learning. McGraw-
Hill Education (ISE Editions), 1st edition.
Rebecca Passonneau. 2006. Measuring Agreement on
Set-valued Items (MASI) for Semantic and Prag-
matic Annotation. In Proceedings of the Fifth In-
ternational Conference on Language Resources and
Evaluation, Genoa, IT.
John C. Platt. 1998. Fast training of support vector
machines using sequential minimal optimization.
In Advances in Kernel Methods: Support Vector
Learning, pages 185?208, Cambridge, MA, USA.
Ilona R. Posner and Ronald M. Baecker. 1992. How
People Write Together. In Proceedings of the 25th
Hawaii International Conference on System Sci-
ences, pages 127?138, Wailea, Maui, HI, USA.
Ross Quinlan. 1992. C4.5: Programs for Machine
Learning. Morgan Kaufmann, 1st edition.
Jodi Schneider, Alexandre Passant, and John G. Bres-
lin. 2011. Understanding and Improving Wikipedia
Article Discussion Spaces. In Proceedings of the
26th Symposium on Applied Computing, Taichung,
TW.
John R. Searle. 1969. Speech Acts. Cambridge Uni-
versity Press, Cambridge, UK.
John R. Searle. 1976. A classification of illocutionary
acts. Language in Society, 5:1?23.
Besiki Stvilia, Michael B. Twidale, Linda C. Smith,
and Les Gasser. 2008. Information Quality Work
Organization in Wikipedia. Journal of the Ameri-
can Society for Information Science, 59:983?1001.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis P.
Vlahavas. 2010. Mining multi-label data. In Data
Mining and Knowledge Discovery Handbook, pages
667?685. Springer.
Fernanda Vie?gas, Martin Wattenberg, Jesse Kriss, and
Frank Ham. 2007. Talk Before You Type: Coor-
dination in Wikipedia. In Proceedings of the 40th
Annual Hawaii International Conference on System
Sciences, Waikoloa, Big Island, HI, USA.
Eti Yaari, Shifra Baruchson-Arbib, and Judit Bar-Ilan.
2011. Information quality assessment of commu-
nity generated content: A user study of Wikipedia.
Journal of Information Science, 37:487?498.
Yiming Yang and Jan O. Pedersen. 1997. A Compara-
tive Study on Feature Selection in Text Categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, pages 412?420,
San Francisco, CA, USA.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, MA.
786
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97?102,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Wikipedia Revision Toolkit: Efficiently Accessing Wikipedia?s Edit History
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych
Ubiquitous Knowledge Processing Lab
Computer Science Department, Technische Universita?t Darmstadt
Hochschulstrasse 10, D-64289 Darmstadt, Germany
http://www.ukp.tu-darmstadt.de
Abstract
We present an open-source toolkit which
allows (i) to reconstruct past states of
Wikipedia, and (ii) to efficiently access the
edit history of Wikipedia articles. Recon-
structing past states of Wikipedia is a pre-
requisite for reproducing previous experimen-
tal work based on Wikipedia. Beyond that,
the edit history of Wikipedia articles has been
shown to be a valuable knowledge source for
NLP, but access is severely impeded by the
lack of efficient tools for managing the huge
amount of provided data. By using a dedi-
cated storage format, our toolkit massively de-
creases the data volume to less than 2% of
the original size, and at the same time pro-
vides an easy-to-use interface to access the re-
vision data. The language-independent design
allows to process any language represented in
Wikipedia. We expect this work to consolidate
NLP research using Wikipedia in general, and
to foster research making use of the knowl-
edge encoded in Wikipedia?s edit history.
1 Introduction
In the last decade, the free encyclopedia Wikipedia
has become one of the most valuable and com-
prehensive knowledge sources in Natural Language
Processing. It has been used for numerous NLP
tasks, e.g. word sense disambiguation, semantic re-
latedness measures, or text categorization. A de-
tailed survey on usages of Wikipedia in NLP can be
found in (Medelyan et al, 2009).
The majority of Wikipedia-based NLP algorithms
works on single snapshots of Wikipedia, which are
published by the Wikimedia Foundation as XML
dumps at irregular intervals.1 Such a snapshot only
represents the state of Wikipedia at a certain fixed
point in time, while Wikipedia actually is a dynamic
resource that is constantly changed by its millions of
editors. This rapid change is bound to have an influ-
ence on the performance of NLP algorithms using
Wikipedia data. However, the exact consequences
are largely unknown, as only very few papers have
systematically analyzed this influence (Zesch and
Gurevych, 2010). This is mainly due to older snap-
shots becoming unavailable, as there is no official
backup server. As a consequence, older experimen-
tal results cannot be reproduced anymore.
In this paper, we present a toolkit that solves
both issues by reconstructing a certain past state of
Wikipedia from its edit history, which is offered by
the Wikimedia Foundation in form of a database
dump. Section 3 gives a more detailed overview of
the reconstruction process.
Besides reconstructing past states of Wikipedia,
the revision history data also constitutes a novel
knowledge source for NLP algorithms. The se-
quence of article edits can be used as training data
for data-driven NLP algorithms, such as vandalism
detection (Chin et al, 2010), text summarization
(Nelken and Yamangil, 2008), sentence compres-
sion (Yamangil and Nelken, 2008), unsupervised
extraction of lexical simplifications (Yatskar et al,
2010), the expansion of textual entailment corpora
(Zanzotto and Pennacchiotti, 2010), or assesing the
trustworthiness of Wikipedia articles (Zeng et al,
2006).
1http://download.wikimedia.org/
97
However, efficient access to this new resource
has been limited by the immense size of the data.
The revisions for all articles in the current English
Wikipedia sum up to over 5 terabytes of text. Con-
sequently, most of the above mentioned previous
work only regarded small samples of the available
data. However, using more data usually leads to bet-
ter results, or how Church and Mercer (1993) put
it ?more data are better data?. Thus, in Section 4,
we present a tool to efficiently access Wikipedia?s
edit history. It provides an easy-to-use API for pro-
grammatically accessing the revision data and re-
duces the required storage space to less than 2% of
its original size. Both tools are publicly available
on Google Code (http://jwpl.googlecode.
com) as open source software under the LGPL v3.
2 Related Work
To our knowledge, there are currently only two alter-
natives to programmatically access Wikipedia?s re-
vision history.
One possibility is to manually parse the original
XML revision dump. However, due to the huge size
of these dumps, efficient, random access is infeasi-
ble with this approach.
Another possibility is using the MediaWiki API2,
a web service which directly accesses live data from
the Wikipedia website. However, using a web ser-
vice entails that the desired revision for every single
article has to be requested from the service, trans-
ferred over the Internet and then stored locally in
an appropriate format. Access to all revisions of
all Wikipedia articles for a large-scale analysis is
infeasible with this method because it is strongly
constricted by the data transfer speed over the In-
ternet. Even though it is possible to bypass this bot-
tleneck by setting up a local Wikipedia mirror, the
MediaWiki API can only provide full text revisions,
which results in very large amounts of data to be
transferred.
Better suited for tasks of this kind are APIs
that utilize databases for storing and accessing the
Wikipedia data. However, current database-driven
Wikipedia APIs do not support access to article re-
visions. That is why we decided to extend an es-
tablished API with the ability to efficiently access
2http://www.mediawiki.org/wiki/API
Wikipedia?s edit history. Two established Wikipedia
APIs have been considered for this purpose.
Wikipedia Miner3 (Milne and Witten, 2009) is
an open source toolkit which provides access to
Wikipedia with the help of a preprocessed database.
It represents articles, categories and redirects as Java
classes and provides access to the article content ei-
ther as MediaWiki markup or as plain text. The
toolkit mainly focuses on Wikipedia?s structure, the
contained concepts, and semantic relations, but it
makes little use of the textual content within the ar-
ticles. Even though it was developed to work lan-
guage independently, it focuses mainly on the En-
glish Wikipedia.
Another open source API for accessing Wikipedia
data from a preprocessed database is JWPL4 (Zesch
et al, 2008). Like Wikipedia Miner, it also rep-
resents the content and structure of Wikipedia as
Java objects. In addition to that, JWPL contains a
MediaWiki markup parser to further analyze the ar-
ticle contents to make available fine-grained infor-
mation like e.g. article sections, info-boxes, or first
paragraphs. Furthermore, it was explicitly designed
to work with all language versions of Wikipedia.
We have chosen to extend JWPL with our revi-
sion toolkit, as it has better support for accessing ar-
ticle contents, natively supports multiple languages,
and seems to have a larger and more active developer
community. In the following section, we present the
parts of the toolkit which reconstruct past states of
Wikipedia, while in section 4, we describe tools al-
lowing to efficiently access Wikipedia?s edit history.
3 Reconstructing Past States of Wikipedia
Access to arbitrary past states of Wikipedia is re-
quired to (i) evaluate the performance of Wikipedia-
based NLP algorithms over time, and (ii) to repro-
duce Wikipedia-based research results. For this rea-
son, we have developed a tool called TimeMachine,
which addresses both of these issues by making use
of the revision dump provided by the Wikimedia
Foundation. By iterating over all articles in the re-
vision dump and extracting the desired revision of
each article, it is possible to recover the state of
Wikipedia at an earlier point in time.
3http://wikipedia-miner.sourceforge.net
4http://jwpl.googlecode.com
98
Property Description Example Value
language The Wikipedia language version english
mainCategory Title of the main category of the
Wikipedia language version used
Categories
disambiguationCategory Title of the disambiguation category of
the Wikipedia language version used
Disambiguation
fromTimestamp Timestamp of the first snapshot to be
extracted
20090101130000
toTimestamp Timestamp of the last snapshot to be ex-
tracted
20091231130000
each Interval between snapshots in days 30
removeInputFilesAfterProcessing Remove source files [true/false] false
metaHistoryFile Path to the revision dump PATH/pages-meta-history.xml.bz2
pageLinksFile Path to the page-to-page link records PATH/pagelinks.sql.gz
categoryLinksFile Path to the category membership
records
PATH/categorylinks.sql.gz
outputDirectory Output directory PATH/outdir/
Table 1: Configuration of the TimeMachine
The TimeMachine is controlled by a single con-
figuration file, which allows (i) to restore individual
Wikipedia snapshots or (ii) to generate whole snap-
shot series. Table 1 gives an overview of the con-
figuration parameters. The first three properties set
the environment for the specific language version of
Wikipedia. The two timestamps define the start and
end time of the snapshot series, while the interval
between the snapshots in the series is set by the pa-
rameter each. In the example, the TimeMachine re-
covers 13 snapshots between Jan 01, 2009 at 01.00
p.m and and Dec 31, 2009 at 01.00 p.m at an inter-
val of 30 days. In order to recover a single snap-
shot, the two timestamps have simply to be set to
the same value, while the parameter ?each? has no
effect. The option removeInputFilesAfterProcessing
specifies whether to delete the source files after pro-
cessing has finished. The final four properties define
the paths to the source files and the output directory.
The output of the TimeMachine is a set of eleven
text files for each snapshot, which can directly be
imported into an empty JWPL database. It can be
accessed with the JWPL API in the same way as
snapshots created using JWPL itself.
Issue of Deleted Articles The past snapshot of
Wikipedia created by our toolkit is identical to the
state of Wikipedia at that time with the exception of
articles that have been deleted meanwhile. Articles
might be deleted only by Wikipedia administrators
if they are subject to copyright violations, vandal-
ism, spam or other conditions that violate Wikipedia
policies. As a consequence, they are removed from
the public view along with all their revision infor-
mation, which makes it impossible to recover them
from any future publicly available dump.5 Even
though about five thousand pages are deleted every
day, only a small percentage of those pages actually
corresponds to meaningful articles. Most of the af-
fected pages are newly created duplicates of already
existing articles or spam articles.
4 Efficient Access to Revisions
Even though article revisions are available from the
official Wikipedia revision dumps, accessing this in-
formation on a large scale is still a difficult task.
This is due to two main problems. First, the revi-
sion dump contains all revisions as full text. This
results in a massive amount of data and makes struc-
tured access very hard. Second, there is no efficient
API available so far for accessing article revisions
on a large scale.
Thus, we have developed a tool called
RevisionMachine, which solves these issues.
First, we describe our solution to the storage prob-
lem. Second, we present several use cases of the
RevisionMachine, and show how the API simplifies
experimental setups.
5http://en.wikipedia.org/wiki/Wikipedia:
DEL
99
4.1 Revision Storage
As each revision of a Wikipedia article stores the
full article text, the revision history obviously con-
tains a lot of redundant data. The RevisionMachine
makes use of this fact and utilizes a dedicated stor-
age format which stores a revision only by means
of the changes that have been made to the previous
revision. For this purpose, we have tested existing
diff libraries, like Javaxdelta6 or java-diff7, which
calculate the differences between two texts. How-
ever, both their runtime and the size of the result-
ing output was not feasible for the given size of the
data. Therefore, we have developed our own diff
algorithm, which is based on a longest common sub-
string search and constitutes the foundation for our
revision storage format.
The processing of two subsequent revisions can
be divided into four steps:
? First, the RevisionMachine searches for all
common substrings with a user-defined mini-
mal length.
? Then, the revisions are divided into blocks of
equal length. Corresponding blocks of both
revisions are then compared. If a block is
contained in one of the common substrings,
it can be marked as unchanged. Otherwise,
we have to categorize the kind of change
that occurred in this block. We differenti-
ate between five possible actions: Insert,
Delete, Replace, Cut and Paste8. This
information is stored in each block and is later
on used to encode the revision.
? In the next step, the current revision is repre-
sented by means of a sequence of actions per-
formed on the previous revision.
For example, in the adjacent revision pair
r1 : This is the very first sentence!
r2 : This is the second sentence
r2 can be encoded as
REPLACE 12 10 ?second?
DELETE 31 1
6http://javaxdelta.sourceforge.net/
7http://www.incava.org/projects/java/
java-diff
8Cut and Paste operations always occur pairwise. In ad-
dition to the other operations, they can make use of an additional
temporary storage register to save the text that is being moved.
? Finally, the string representation of this ac-
tion sequence is compressed and stored in the
database.
With this approach, we achieve to reduce the de-
mand for disk space for a recent English Wikipedia
dump containing all article revisions from 5470 GB
to only 96 GB, i.e. by 98%. The compressed data is
stored in a MySQL database, which provides sophis-
ticated indexing mechanisms for high-performance
access to the data.
Obviously, storing only the changes instead of
the full text of each revision trades in speed for
space. Accessing a certain revision now requires re-
constructing the text of the revision from a list of
changes. As articles often have several thousand re-
visions, this might take too long. Thus, in order to
speed up the recovery of the revision text, every n-th
revision is stored as a full revision. A low value of
n decreases the time needed to access a certain re-
vision, but increases the demand for storage space.
We have found n = 1000 to yield a good trade-off9.
This parameter, among a few other possibilities to
fine-tune the process, can be set in a graphical user
interface provided with the RevisionMachine.
4.2 Revision Access
After the converted revisions have been stored in
the revision database, it can either be used stand-
alone or combined with the JWPL data and ac-
cessed via the standard JWPL API. The latter op-
tion makes it possible to combine the possibilities
of the RevisionMachine with other components like
the JWPL parser for the MediaWiki syntax.
In order to set up the RevisionMachine, it is only
necessary to provide the configuration details for the
database connection (see Listing 1). Upon first ac-
cess, the database user has to have write permission
on the database, as indexes have to be created. For
later use, read permission is sufficient. Access to the
RevisionMachine is achieved via two API objects.
The RevisionIterator allows to iterate over all revi-
sions in Wikipedia. The RevisionAPI grants access
to the revisions of individual articles. In addition to
9If hard disk space is no limiting factor, the parameter can be
set to 1 to avoid the compression of the revisions and maximize
the performance.
100
/ / S e t up d a t a b a s e c o n n e c t i o n
DatabaseConfiguration db = new DatabaseConfiguration ( ) ;
db .setDatabase ( ? dbname ? ) ;
db .setHost ( ? hos tname ? ) ;
db .setUser ( ? username ? ) ;
db .setPassword ( ?pwd? ) ;
db .setLanguage (Language .english ) ;
/ / C r e a t e API o b j e c t s
Wikipedia wiki = WikiConnectionUtils .getWikipediaConnection (db ) ;
RevisionIterator revIt = new RevisionIterator (db ) ;
RevisionApi revApi = new RevisionApi (db ) ;
Listing 1: Setting up the RevisionMachine
that, the Wikipedia object provides access to JWPL
functionalities.
In the following, we describe three use cases of
the RevisionMachine API, which demonstrate how
it is easily integrated into experimental setups.
Processing all article revisions in Wikipedia
The first use case focuses on the utilization of the
complete set of article revisions in a Wikipedia snap-
shot. Listing 2 shows how to iterate over all revi-
sions. Thereby, the iterator ensures that successive
revisions always correspond to adjacent revisions of
a single article in chronological order. The start of
a new article can easily be detected by checking the
timestamp and the article id. This approach is es-
pecially useful for applications in statistical natural
language processing, where large amounts of train-
ing data are a vital asset.
Processing revisions of individual articles The
second use case shows how the RevisionMachine
can be used to access the edit history of a specific
article. The example in Listing 3 illustrates how all
revisions for the article Automobile can be retrieved
by first performing a page query with the JWPL API
and then retrieving all revision timestamps for this
page, which can finally be used to access the revi-
sion objects.
Accessing the meta data of a revision The third
use case illustrates the access to the meta data of in-
dividual revisions. The meta data includes the name
or IP of the contributor, the additional user comment
for the revision and a flag that identifies a revision as
minor or major. Listing 4 shows how the number of
edits and unique contributors can be used to indicate
the level of edit activity for an article.
5 Conclusions
In this paper, we presented an open-source toolkit
which extends JWPL, an API for accessing
Wikipedia, with the ability to reconstruct past states
of Wikipedia, and to efficiently access the edit his-
tory of Wikipedia articles.
Reconstructing past states of Wikipedia is a
prerequisite for reproducing previous experimen-
tal work based on Wikipedia, and is also a re-
quirement for the creation of time-based series of
Wikipedia snapshots and for assessing the influence
of Wikipedia growth on NLP algorithms. Further-
more, Wikipedia?s edit history has been shown to be
a valuable knowledge source for NLP, which is hard
to access because of the lack of efficient tools for
managing the huge amount of revision data. By uti-
lizing a dedicated storage format for the revisions,
our toolkit massively decreases the amount of data
to be stored. At the same time, it provides an easy-
to-use interface to access the revision data.
We expect this work to consolidate NLP re-
search using Wikipedia in general, and to foster
research making use of the knowledge encoded in
Wikipedia?s edit history. The toolkit will be made
available as part of JWPL, and can be obtained from
the project?s website at Google Code. (http://
jwpl.googlecode.com)
Acknowledgments
This work has been supported by the Volkswagen Foun-
dation as part of the Lichtenberg-Professorship Program
under grant No. I/82806, and by the Hessian research
excellence program ?Landes-Offensive zur Entwicklung
Wissenschaftlich-o?konomischer Exzellenz? (LOEWE) as
part of the research center ?Digital Humanities?. We
would also like to thank Simon Kulessa for designing and
implementing the foundations of the RevisionMachine.
101
/ / I t e r a t e ove r a l l r e v i s i o n s o f a l l a r t i c l e s
w h i l e (revIt .hasNext ( ) ) {
Revision rev = revIt .next ( )
rev .getTimestamp ( ) ;
rev .getArticleID ( ) ;
/ / p r o c e s s r e v i s i o n . . .
}
Listing 2: Iteration over all revisions of all articles
/ / Get a r t i c l e wi th t i t l e ? Automobi le ?
Page article = wiki .getPage ( ? Automobi le ? ) ;
i n t id = article .getPageId ( ) ;
/ / Get a l l r e v i s i o n s f o r t h e a r t i c l e
Collection<Timestamp> revisionTimeStamps = revApi .getRevisionTimestamps (id ) ;
f o r (Timestamp t :revisionTimeStamps ) {
Revision rev = revApi .getRevision (id , t ) ;
/ / p r o c e s s r e v i s i o n . . .
}
Listing 3: Accessing the revisions of a specific article
/ / Meta d a t a p r o v i d e d by t h e Rev i s ionAPI
StringBuffer s = new StringBuffer ( ) ;
s .append ( ? The a r t i c l e has ?+revApi .getNumberOfRevisions (pageId ) +? r e v i s i o n s .\ n ? ) ;
s .append ( ? I t has ?+revApi .getNumberOfUniqueContributors (pageId ) +? un iq ue c o n t r i b u t o r s .\ n ? ) ;
s .append (revApi .getNumberOfUniqueContributors (pageId , t r u e ) + ? a r e r e g i s t e r e d u s e r s .\ n ? ) ;
/ / Meta d a t a p r o v i d e d by t h e R e v i s i o n o b j e c t
s .append ( (rev .isMinor ( ) ? ? Minor ? : ? Major ? ) +? r e v i s i o n by : ?+rev .getContributorID ( ) ) ;
s .append ( ?\nComment : ?+rev .getComment ( ) ) ;
Listing 4: Accessing the meta data of a revision
References
Si-Chi Chin, W. Nick Street, Padmini Srinivasan, and
David Eichmann. 2010. Detecting wikipedia vandal-
ism with active learning and statistical language mod-
els. In Proceedings of the 4th workshop on Informa-
tion credibility, WICOW ?10, pages 3?10.
Kenneth W. Church and Robert L. Mercer. 1993. Intro-
duction to the special issue on computational linguis-
tics using large corpora. Computational Linguistics,
19(1):1?24.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from wikipedia.
Int. J. Hum.-Comput. Stud., 67:716?754, September.
D. Milne and I. H. Witten. 2009. An open-source toolkit
for mining Wikipedia. In Proc. New Zealand Com-
puter Science Research Student Conf., volume 9.
Rani Nelken and Elif Yamangil. 2008. Mining
wikipedia?s article revision history for training com-
putational linguistics algorithms. In Proceedings of
the AAAI Workshop on Wikipedia and Artificial Intel-
ligence: An Evolving Synergy (WikiAI), WikiAI08.
Elif Yamangil and Rani Nelken. 2008. Mining wikipedia
revision histories for improving sentence compres-
sion. In Proceedings of ACL-08: HLT, Short Papers,
pages 137?140, Columbus, Ohio, June. Association
for Computational Linguistics.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: unsupervised extraction of lexical simplifications
from wikipedia. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, HLT ?10, pages 365?368.
Fabio Massimo Zanzotto and Marco Pennacchiotti.
2010. Expanding textual entailment corpora from
wikipedia using co-training. In Proceedings of the
COLING-Workshop on The People?s Web Meets NLP:
Collaboratively Constructed Semantic Resources.
Honglei Zeng, Maher Alhossaini, Li Ding, Richard Fikes,
and Deborah L. McGuinness. 2006. Computing trust
from revision history. In Proceedings of the 2006 In-
ternational Conference on Privacy, Security and Trust.
Torsten Zesch and Iryna Gurevych. 2010. The more the
better? Assessing the influence of wikipedia?s growth
on semantic relatedness measures. In Proceedings of
the Conference on Language Resources and Evalua-
tion (LREC), Valletta, Malta.
Torsten Zesch, Christof Mueller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary. In Proceedings of the
Conference on Language Resources and Evaluation
(LREC).
102
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 721?730,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia
Oliver Ferschke?, Iryna Gurevych?? and Marc Rittberger?
? Ubiquitous Knowledge Processing Lab
Department of Computer Science, Technische Universita?t Darmstadt
? Information Center for Education
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
With the increasing amount of user gener-
ated reference texts in the web, automatic
quality assessment has become a key chal-
lenge. However, only a small amount
of annotated data is available for training
quality assessment systems. Wikipedia
contains a large amount of texts anno-
tated with cleanup templates which iden-
tify quality flaws. We show that the dis-
tribution of these labels is topically bi-
ased, since they cannot be applied freely
to any arbitrary article. We argue that it
is necessary to consider the topical restric-
tions of each label in order to avoid a sam-
pling bias that results in a skewed classifier
and overly optimistic evaluation results.
We factor out the topic bias by extracting
reliable training instances from the revi-
sion history which have a topic distribu-
tion similar to the labeled articles. This ap-
proach better reflects the situation a classi-
fier would face in a real-life application.
1 Introduction
User generated content is the main driving force
of the increasingly social web. Blogs, wikis and
forums make up a large amount of the daily infor-
mation consumed by web users. The main proper-
ties of user generated content are a low publication
threshold and little or no editorial control, which
leads to a high variance in quality. In order to nav-
igate through large repositories of information effi-
ciently and safely, users need a way to quickly as-
sess the quality of the content. Automatic quality
assessment has therefore become a key application
in today?s information society. However, there is
a lack of training data annotated with fine-grained
quality information.
Wikipedia, the largest encyclopedia on the web,
contains so-called cleanup templates, which con-
stitute a sophisticated system of user generated la-
bels that mark quality problems in articles. Re-
cently, these cleanup templates have been used for
automatically identifying articles with particular
quality flaws in order to support Wikipedia?s qual-
ity assurance process in Wikipedia. In a shared
task (Anderka and Stein, 2012b), several systems
have shown that it is possible to identify the ten
most frequent quality flaws with high recall and
fair precision.
However, quality flaw detection based on
cleanup template recognition suffers from a topic
bias that is well known from other text classifica-
tion applications such as authorship attribution or
genre identification. We discovered that cleanup
templates have implicit topical restrictions, i.e.
they cannot be applied to any arbitrary article. As
a consequence, corpora of flawed articles based
on these templates are biased towards particular
topics. We argue that it is therefore not sufficient
for evaluating a quality flaw prediction systems to
measure how well they can separate (topically re-
stricted) flawed articles from a set of random out-
liers. It is rather necessary to determine reliable
negative instances with a similar topic distribution
as the set of positive instances in order to factor
out the sampling bias. Related studies (Brooke and
Hirst, 2011) have proven that topic bias is a con-
founding factor that results in misleading cross-
validated performance while allowing only near
chance performance in practical applications.
We present an approach for factoring out the
bias from quality flaw corpora by mining reliable
negative instances for each flaw from the article
revision history. Furthermore, we employ the ar-
ticle revision history to extract reliable positive
training instances by using the version of each
article at the time it has first been identified as
flawed. This way, we avoid including articles
with outdated cleanup templates, a frequent phe-
721
nomenon that can occur when a template is not
removed after fixing a problem in an article. In
our experiments, we focus on neutrality and style
flaws, since they are of particular high importance
within the Wikipedia community (Stvilia et al,
2008; Ferschke et al, 2012a) and are recognized
beyond Wikipedia in applications such as uncer-
tainty recognition (Szarvas et al, 2012) and hedge
detection (Farkas et al, 2010).
2 Related Work
Topic bias is a known problem in text classifi-
cation. Mikros and Argiri (2007) investigate the
topic influence in authorship attribution. They
found that even simple stylometric features, such
as sentence and token length, readability mea-
sures or word length distributions show consider-
able correlations with topic. They argue that many
features that were largely considered to be topic
neutral are in fact topic-dependent variables. Con-
sequently, results obtained on multitopic corpora
are prone to be biased by the correlation of authors
with specific topics. Therefore, several authors in-
troduce topic-controlled corpora for applications
such as author identification (Koppel and Schler,
2003; Luyckx and Daelemans, 2005) or genre de-
tection (Finn and Kushmerick, 2006).
Brooke and Hirst (2011) measure the topic bias
in the International Corpus of Learner English
and found that it causes a substantial skew in clas-
sifiers for native language detection. In accor-
dance with Mikros et al, the authors found that
even non-lexicalized meta features, such as vo-
cabulary size or length statistics, depend on top-
ics and cause cross-validated performance evalua-
tions to be unrealistically high. In a practical set-
ting, these biased classifiers hardly exceed chance
performance.
As already noted above, a similar kind of topic
bias negatively influences quality flaw detection in
Wikipedia. Anderka et al (2012) automatically
identify quality flaws by predicting the cleanup
templates in unseen articles with a one-class clas-
sification approach. Based on this work, a com-
petition on quality flaw prediction has been es-
tablished (Anderka and Stein, 2012b). The win-
ning team of the inaugural edition of the task
was able to detect the ten most common qual-
ity flaws with an average F1-Score of 0.81 us-
ing a PU learning approach (Ferretti et al, 2012).
With a binary classification approach, Ferschke et
al. (2012b) achieved an average F1-Score of 0.80,
while reaching a higher precision than the winning
team.
A closer examination of the aforementioned
quality flaw detection systems reveals a systematic
sampling bias in the training data, which leads to
an overly optimistic performance evaluation and
classifiers that are biased towards particular arti-
cle topics. Our approach factors out the topic bias
from the training data by mining topically con-
trolled training instances from the Wikipedia revi-
sion history. The results show that flaw detection
is a much harder problem in a real-life scenario.
3 Quality Flaws and
Flaw Recognition in Wikipedia
Quality standards in Wikipedia are mainly defined
by the featured article criteria1 and the Wikipedia
Manual of Style2. These policies define the char-
acteristics excellent articles have to exhibit. Other
sets of quality criteria are adaptations or relax-
ations of these standards, such as the good article
criteria or the quality grading schemes of individ-
ual interest groups in Wikipedia.
In this work, we focus on quality flaws regard-
ing neutrality and style problems. We chose these
categories due to their high importance within the
Wikipedia community (Stvilia et al, 2008; Fer-
schke et al, 2012a) and due to their relevance to
content outside of Wikipedia, such as blogs or on-
line news articles. According to the Wikipedia
policies3, an article has to be written from a neu-
tral point of view. Thus, authors must avoid stat-
ing opinions and seriously contested assertions as
facts, avoid presenting uncontested factual asser-
tions as mere opinions, prefer nonjudgmental lan-
guage and indicate the relative prominence of op-
posing views. Furthermore, authors have to adhere
to the stylistic guidelines defined in the Manual of
Style. While this subsumes a broad range of is-
sues such as formatting and article structure, we
focus on the style of writing and disregard mere
structural properties.
Any articles that violate these criteria can be
marked with cleanup templates4 to indicate their
need for improvement. These templates can
thus be regarded as proxies for quality flaws in
Wikipedia.
1http://en.wikipedia.org/wiki/WP:FACR
2http://en.wikipedia.org/wiki/WP:STYLE
3http://en.wikipedia.org/wiki/WP:NPOV
4http://en.wikipedia.org/wiki/WP:TM#Cleanup
722
Flaw Description Articles Templates
Advert The article appears to be written like an advertisement and is thus not neutral 7,332 2
POV The neutrality of this article is disputed 5,086 10
Globalize The article may not represent a worldwide view of the subject 1,609 1
Peacock The article may contain wording that merely promotes the subject without
imparting verifiable information
1,195 1
Ne
utr
ali
ty
Weasel The article contains vague phrasing that often accompanies biased or unver-
ifiable information
704 4
Tone The tone of the article is not encyclopedic according to the Wikipedia Manual
of Style
4,563 6
In-universe The article describes a work or element of fiction in a primarily in-universe
stylea
2,227 1
Copy-edit The article requires copy editing for grammar, style, cohesion, tone, or
spelling
1,954 6
Trivia Contains lists of miscellaneous information 1,282 2
Essay-like The article is written like a personal reflection or essay 1,244 1
Confusing The article may be confusing or unclear to readers 1,084 1
Sty
le
Technical The article may be too technical for most readers to understand 690 2
a According to the Wikipedia Manual of Style, an in-universe perspective describes the article subject matter from the
perspective of characters within a fictional universe as if it were real.
Table 1: Neutrality and style flaw corpora used in this work
Template Clusters Since several cleanup tem-
plates might represent different manifestations of
the same quality flaw, there is a 1 to n relation-
ship between quality flaws and cleanup templates.
For instance, the templates pov-check5, pov6 and
npov language7 can all be mapped to the same
flaw concerning the neutral point of view of an ar-
ticle. This aggregation of cleanup templates into
flaw-clusters is a subjective task. It is not al-
ways clear whether a particular template refers to
an existing flaw or should be regarded as a sep-
arate class. Too many clusters will cause defini-
tion overlaps (i.e. similar cleanup templates are
assigned to different clusters), while too few clus-
ters will result in unclear flaw definitions, since
each flaw receives a wide range of possible mani-
festations.
Template Scope Another important aspect to be
considered is the difference in the scope which
cleanup templates can have. Inline-templates are
placed directly in the text and refer to the sentence
or paragraph they are placed in. Templates with
a section parameter, refer to the section they are
placed in. The majority of templates, however, re-
fer to a whole page. The consideration of the tem-
plate scope is of particular importance for qual-
ity flaw recognition problems. For example, the
presence of a cleanup template which marks a sin-
gle section as not notable does not entail that the
whole article is not notable.
5The article has been nominated for a neutrality check
6The neutrality of the article is disputed
7The article contains a non-neutral style of writing
Topical Restriction A final aspect that has not
been taken into account by related work is that
many cleanup templates have restrictions concern-
ing the pages they may be applied to. A hard re-
striction is the page type (or namespace) a tem-
plate might be used in. For example, some tem-
plates can only be used in articles while others can
only be applied to discussion pages. This is usu-
ally enforced by maintenance scripts running on
the Wikimedia servers. A soft restriction, on the
other hand, are the topics of the articles a tem-
plate can be used in. Many cleanup templates can
only be applied to articles from certain subject ar-
eas. An example with a particularly obvious re-
striction is the template in-universe (see Table 1),
which should only be applied to articles about fic-
tion. This topical restriction is neither explicitly
defined nor automatically enforced, but it plays an
important role in the quality flaw recognition task,
as the remainder of this paper will show. While
flaws merely concerning the structural or linguis-
tic properties of an article are less restricted to
individual topics, they are still affected by a cer-
tain degree of topical preference. Many subject
areas in Wikipedia are organized in WikiProjects8,
which have their own ways of reviewing and en-
suring quality within their topical scope. Depend-
ing on the quality assurance processes established
in a WikiProject, different importance is given to
individual types of flaws. Thus, the distribution
of cleanup templates regarding structural or gram-
matical flaws is also biased towards certain topics.
8http://en.wikipedia.org/wiki/WP:PROJ
723
We will henceforth subsume the concept of topical
preference under the term topical restriction.
Quality Flaw Recognition Based on the above
definition of quality flaws, we define the qual-
ity flaw recognition task similar to Anderka et
al. (2012) as follows: Given a sample of articles
in which each article has been tagged with any
cleanup template ?i from a specific template clus-
ter T f thus marking all articles in the sample with
a quality flaw f , it has to be decided whether or
not an unseen article suffers from f .
4 Data Selection and Corpus Creation
For creating our corpora, we start with selecting all
cleanup templates listed under the categories neu-
trality and style in the typology of cleanup tem-
plates provided by Anderka and Stein (2012a).
Each of the selected templates serves as the nu-
cleus of a template cluster that potentially repre-
sents a quality flaw. To each cluster, we add all
templates that are synonymous to the nucleus. The
synonyms are listed in the template description
under redirects or shortcuts. Then we iteratively
add all synonyms of the newly added template un-
til no more redirects can be found. Furthermore,
we manually inspect the lists of similar templates
in the see also sections of the template descrip-
tions and include all templates that refer to the
same concept as the other templates in the cluster.
As mentioned earlier, this is a subjective task and
largely depends on the desired granularity of the
flaw definitions. We finally merge semantically
similar template clusters to avoid too fine grained
flaw distinctions.
As a result, we obtain a total number of 94
template clusters representing 60 style flaws and
34 neutrality flaws. From each of these clusters,
we remove templates with inline or section scope
due to the reasons outlined in Section 3. We also
remove all templates that are restricted to pages
other than articles (e.g. discussion or user pages).
We use the Java Wikipedia Library (Zesch et
al., 2008) to extract all articles marked with the
selected templates. We only regard flaws with
at least 500 affected articles in the snapshot of
the English Wikipedia from January 4, 2012.
Table 1 lists the final sets of flaws used in this
work. For each flaw, the nucleus of the template
cluster is provided along with a description, the
number of affected articles, and the cluster size.
We make the corpora freely available for down-
Flaw ? F1
Advert .60 .80
Confusing .60 .80
Copy-edit .00 .50
Essay-like .60 .80
Globalize: .60 .80
In-universe .80 .90
Peacock .70 .84
POV .60 .80
Technical .90 .95
Tone .40 .70
Trivia .20 .60
Weasel .50 .74
Table 2: Agreement of human annotator with gold
standard
load under http://www.ukp.tu-darmstadt.
de/data/wiki-flaws/.
Agreement with Human Rater
Quality flaw detection in Wikipedia is based on the
assuption that cleanup templates are valid mark-
ers of quality flaws. In order to test the reliabil-
ity of these user assigned templates as quality flaw
markers, we carried out an annotation study in
which a human annotator was asked to perform the
binary flaw detection task manually. Even though
the human performance does not necessarily pro-
vide an upper boundary for the automatic classifi-
cation task, it gives insights into potentially prob-
lematic cases and ill-defined annotations. The an-
notator was provided with the template definitions
from the respective template information page as
instructions. For each of the 12 article scope flaws,
we extracted the plain text of 10 random flawed
articles and 10 random untagged articles. The an-
notator had to decide for each flaw individually
whether a given text belonged to a flawed article
or not. She was not informed about the ratio of
flawed to untagged articles.
Table 2 lists the chance corrected agreement
(Cohen?s ?) along with the F1 performance of the
human annotations against the gold standard cor-
pus. The templates copy-edit and trivia yielded
the lowest performance in the study. Even though
copy-edit templates are assigned to whole articles,
they refer to grammatical and stylistic problems of
relatively small portions of the text. This increases
the risk of overlooking a problematic span of text,
especially in longer articles. The trivia template,
on the other hand, designates sections that contain
miscellaneous information that are not well inte-
grated in the article. Upon manual inspection, we
found a wide range of possible manifestations of
724
this flaw ranging from an agglomeration of inco-
herent factoids to well-structured sections that did
not exactly match the focus of the article, which is
the main reason for the low agreement.
5 Selection of Reliable Training
Instances
Independent from the classification approach used
to identify flawed articles, reliable training data is
the most important prerequisite for good predic-
tions. On the one hand, we need a set of examples
that reliably represent a particular flaw, while on
the other hand, we need counterexamples which
reliably represent articles that do not suffer from
the same flaw. The latter aspect is most impor-
tant for discriminative classification approaches,
since they rely on negative instances for training
the classifier. However, reliable negative instances
are also important for one-class classification ap-
proaches, since it is only for the counterexam-
ples (or outliers) that the performance of one-class
classifiers can be sufficiently evaluated. It is fur-
thermore important that the positive and the neg-
ative instances do not differ systematically in any
respect other than the presence or absence of the
respective flaws, since any systematic difference
will bias the classifier. In this context, the topical
restrictions of cleanup templates have to be taken
into account. In the following, we describe our
approach to extracting reliable training instances
from the quality flaw corpora.
5.1 Reliable Positives
In previous work, the latest available versions of
flawed articles have been used as positive training
instances. However, we found upon manual in-
spection of the data that a substantial number of
articles has been significantly edited between the
time t?, at which the template was first assigned,
and the time te, at which the articles have been ex-
tracted. Using the latest version at time te can thus
include articles in which the respective flaw has
already been fixed without removing the cleanup
template. Therefore, we use the revision of the ar-
ticle at time t? to assure that the flaw is still present
in the training instance.
We use the Wikipedia Revision Toolkit (Fer-
schke et al, 2011), an enhancement of the Java
Wikipedia Library, to gain access to the revision
history of each article. For every article in the cor-
pus of positive examples for flaw f that is marked
with template ? ? T f , we backtrack the revision
history chronologically, until we find the first revi-
sion rt??1 that is not tagged with ? . We then add
the succeeding revision rt? to the corpus of reliable
positives for flaw f . In Section 6, we show that
the classification performance improves for most
flaws when using reliable positives instead of the
latest available article versions.
5.2 Reliable Negatives and Topical
Restriction
A central problem of the quality flaw recognition
approach is the fact that there are no articles avail-
able that are tagged to not contain a particular
quality problem. So far, two solutions to this issue
have been proposed in related work. Anderka et al
(2012) tackle the problem with a one-class classi-
fier that is trained on the positive instances alone
thus eradicating the need for negative instances in
the training phase. However, in order to evalu-
ate the classifier, a set of outliers is needed. The
authors circumvent this issue by evaluating their
classifiers on a set of random untagged instances
and a set of featured articles and argue that the
actual performance of predicting the quality flaws
lies between the two.
Ferretti et al (2012) follow a two step classifica-
tion approach (PU learning) that first uses a Naive
Bayes classifier trained on positive instances and
random untagged articles to pre-classify the data.
In a second phase, they use the negatives identi-
fied by the Naive Bayes classifier to train a Sup-
port Vector Machine that produces the final predic-
tions. Even though the Naive Bayes classifier was
supposed to identify reliable negatives, the authors
found no significant improvement over a random
selection of negative instances, which effectively
renders the PU learning approach redundant.
None of the above approaches consider the
issue of topical restriction mentioned in Sec-
tion 3, which introduces a systematic bias to the
data. Both approaches sample random negative in-
stances Arnd for any given set of flawed articles A f
from a set of untagged articles Au (see Fig. 1a).
In order to factor out the article topics as a ma-
jor characteristic for distinguishing flawed articles
from the set of outliers, reliable negative instances
Arel have to be sampled from the restricted topic
set Atopic that contains articles with a topic dis-
tribution similar to the flawed articles in A f (see
Fig. 1b). This will avoid the systematic bias and
725
(a) Random negatives (b) Reliable negatives
Figure 1: Sampling of negative instances for a given set of flawed articles (A f ). Random negatives (Arnd)
are sampled from articles without any cleanup templates (Au). Reliable negatives (Arel) are sampled from
the set of articles (Atopic) with the same topic distribution as A f
result in a more realistic performance evaluation.
In the following, we present our approach
to extracting reliable negative training instances
that conform with the topical restrictions of the
cleanup templates. Without loss of generality, we
assume that an article, from which a cleanup tem-
plate ? ? T f is deleted at a point in time d?, the
article no longer suffers from flaw f at that point
in time. Thus, the revision rd? is a reliable negative
instance for the flaw f . Additionally, since the ar-
ticle was once tagged with ? ? T f , it belongs to the
the same restricted topic set Atopic as the positive
instances for flaw f .
We use the Apache Hadoop9 framework and
WikiHadoop10, an input format for Wikipedia
XML dumps, for crawling the whole revision his-
tory of the English Wikipedia on a compute clus-
ter. WikiHadoop allows each Hadoop mapper to
receive adjacent revision pairs, which makes it
possible to compare the changes made from one
revision to the next. For every template ? ? T f ,
we extract all adjacent revision pairs (rd??1, rd?), in
which the first revision contains ? and the second
does not contain ?. Since there are occasions in
which a template is replaced by another template
from the same cluster, we ensure that rd? does also
not contain any other template from cluster T f be-
fore we finally add the revision to the set of reli-
able negatives for flaw f .
In the remainder of this section, we evaluate the
topical similarity between the positive and the neg-
ative set of articles for each flaw using both our
method and the original approach. In Wikipedia,
9http://hadoop.apache.org
10https://github.com/whym/wikihadoop
the topic of an article is captured by the categories
assigned to it. In order to compare two sets of arti-
cles with respect to their topical similarity, we rep-
resent each article set as a category frequency vec-
tor. Formally, we calculate for each set the vector
~C = (wc1 ,wc2 , . . . ,wcn) with wci being the weight
of category ci, i.e. the number of times it occurs in
the set, and n being the total number of categories
in Wikipedia. We can then estimate the topical
similarity of two article sets by calculating the co-
sine similarity of their category frequency vectors
~C1 B A and ~C2 B B as
sim(A, B) = A ? B?A? ?B? =
n?
i=1
Ai ? Bi
? n?
i=1
(Ai)2 ?
? n?
i=1
(Bi)2
Table 3 gives an overview of the similarity
scores between each positive training set and the
corresponding reliable negative set as well as be-
tween each positive set and a random set of un-
tagged articles. We can see that the topics of arti-
cles in the positive training sets are highly similar
to the topics of the corresponding reliable negative
articles while they show little similarity to the ar-
ticles in the random set. This implies that the sys-
tematic bias introduced by the topical restriction
has largely been eradicated by our approach.
Individual flaws have differently strong topical
restrictions. The strength of this restriction de-
pends on the size of Atopic. That is, a flaw such as
in-universe is restricted to a very narrow selection
of articles, while a flaw such as copy edit can be
applied to most articles and rather shows a topical
preference due to reasons outlined in Section 3. It
726
Cosine Similarity
Flaw (A f , Arel) (A f , Arnd)
Advert .996 .118
Confusing .996 .084
Copy-edit .993 .197
Essay-like .996 .132
Globalize .992 .023
In-universe .996 .014
Peacock .995 .310
POV .994 .252
Technical .995 .018
Tone .996 .228
Trivia .980 .184
Weasel .976 .252
Table 3: Cosine similarity scores between the cat-
egory frequency vectors of the flawed article sets
and the respective random or reliable negatives
is therefore to be expected that that flaws with a
small Atopic are more prone to the topic bias.
6 Experiments
In the following, we describe our system architec-
ture and the setup of our experiments. Our system
for quality flaw detection follows the approach by
Ferschke et al (2012b), since it has been particu-
larly designed as a modular system based on the
Unstructured Information Management Architec-
ture11, which makes it easy to extend. Instead
of using Mallet (McCallum, 2002) as a machine
learning toolkit, we employ the Weka Data Min-
ing Software (Hall et al, 2009) for classification,
since it offers a wider range of state-of-the-art ma-
chine learning algorithms. For each of the 12 qual-
ity flaws, we employ three different dataset config-
urations. The BASE configuration uses the newest
version of each flawed article as positive instances
and a random set of untagged articles as negative
instances. The RELP configuration uses reliable
positives, as described in Section 5.1, in combi-
nation with random outliers. Finally, the RELALL
configuration employs reliable positives in com-
bination with the respective reliable negatives as
described in Section 5.2.
Features
An extensive survey of features for quality flaw
recognition has been provided by Anderka et al
(2012). We selected a subset of these features for
our experiments and grouped them into four fea-
ture sets in order to determine how well differ-
ent combinations of features perform in the task.
11http://uima.apache.org
Category Feature type NO
NG
RA
M
NG
RA
M
NO
WI
KI
AL
L
Lexical Article ngrams ? ? ?
Info to noise ratio ? ? ?
Network # External links ? ?
# Outlinks ? ?
# Outlinks per sentence ? ?
# Language links ? ?
References Has reference list ? ?
# References ? ?
# References per sentence ? ?
Revision # Revisions ? ?
# Unique contributors ? ?
Structure # Empty sections ? ?
Mean section size ? ?
# Sections ? ?
# Lists ? ?
Question rate ? ? ?
Readability ARI ? ? ?
Coleman-Liau ? ? ?
Flesch ? ? ?
Flesch-Kincaid ? ? ?
Gunning Fog ? ? ?
Lix ? ? ?
SMOG-Grading ? ? ?
Named
Entity
# Person entities? ? ? ?
# Organization entities? ? ? ?
# Location entities? ? ? ?
Misc # Characters ? ? ?
# Sentences ? ? ?
# Tokens ? ? ?
Average sentence length ? ? ?
Article lead length ? ?
Lead to article ratio ? ?
# Discussions ? ?
? newly introduced feature
# number of instances
Table 4: Feature sets used in the experiments
Table 4 lists all feature types used in our experi-
ments.
Since the feature space becomes large due to the
ngram features, we prune it in two steps. First,
we filter the ngrams according to their document
frequency in the training corpus. We discard all
ngrams that occur in less than x% and more than
y% of all documents. Several values for x and
y have been evaluated in parameter tuning ex-
periments. The best results have been achieved
with x=2 and y=90. In a second step, we apply
the Information Gain feature selection approach
(Mitchell, 1997) to the remaining set to determine
the most useful features.
Learning Algorithms
We evaluated several learning algorithms from the
Weka toolkit with respect to their performance on
727
Algorithm Average F1
SVM RBF Kernel 0.82
AdaBoost (decision stumps) 0.80
SVM Poly Kernel 0.79
RBF Network 0.78
SVM Linear Kernel 0.77
SVM PUK Kernel 0.76
J48 0.75
Naive Bayes 0.72
MultiBoostAB (decision stumps) 0.71
Logistic Regression 0.60
LibSVM One Class 0.67
Table 5: Average F1-scores over all flaws on RELP
using all features
the quality flaw recognition task. Table 5 shows
the average F1-score of each algorithm on the
RELP dataset using all features. The performance
has been evaluated with 10-fold cross validation
on 2,000 documents split equally into positive
and negative instances. One class classifiers are
trained on the positive instances alone. We deter-
mined the best parameters for each algorithms in
a parameter optimization run and list the results of
the best configuration.
Overall, Support Vector Machines with RBF
kernels yielded the best average results and out-
performed the other algorithms on every flaw. We
used a sequential minimal optimization (SMO) al-
gorithm (Platt, 1998) to train the SVMs and used
different ?-values for the RBF kernel function. In
contrast to Ferretti et al (2012), we did not see sig-
nificant improvements when optimizing ? for each
individual flaw, so we determined one best setting
for each dataset. Since SVMs with RBF kernels
are a special case of RBF networks that fit a sin-
gle basis function to the data, we also used gen-
eral RBF networks that can employ multiple ba-
sis functions, but we did not achieve better results
with that approach.
One-class classification, as proposed by An-
derka et al (2012), did not perform well within
our setup. Even though we used an out-of-the-
box one class classifier, we achieve similar re-
sults as Anderka et al in their pessimistic setting,
which best resembles our configuration. However,
the performance still lacks behind the other ap-
proaches in our experiments. The best perform-
ing algorithm reported by Ferschke et al (2012b),
AdaBoost with decision stumps as a weak learner,
showed the second best results in our experiments.
7 Evaluation and Discussion
The SVMs achieve a similar cross-validated per-
formance on all feature sets containing ngrams,
showing only minor improvements for individ-
ual flaws when adding non-lexical features. This
suggests that the classifiers largely depend on
the ngrams and that other features do not con-
tribute significantly to the classification perfor-
mance. While structural quality flaws can be
well captured by special purpose features or in-
tensional modeling, as related work has shown,
more subtle content flaws such as the neutrality
and style flaws are mainly captured by the word-
ing itself. Textual features beyond the ngram level,
such as syntactic and semantic qualities of the
text, could further improve the classification per-
formance of these flaws and should be addressed
in future work. Table 6 shows the performance of
the SVMs with RBF kernel12 on each dataset us-
ing the NGRAM feature set. The average perfor-
mance based on NOWIKI is slightly lower while
using ALL features results in slightly higher aver-
age F1-scores. However, the differences are not
statistically significant and thus omitted. Classi-
fiers using the NONGRAM feature set achieved av-
erage F1-scores below 0.50 on all datasets. The
results have been obtained by 10-fold cross vali-
dation on 2,000 documents per flaw.
The classifiers trained on reliable positives and
random untagged articles (RELP) outperform the
respective classifiers based on the BASE dataset
for most flaws. This confirms our original hy-
pothesis that using the appropriate revision of each
tagged article is superior to using the latest avail-
able version from the dump. The performance on
the RELALL dataset, in which the topic bias has
been factored out, yields lower F1-scores than the
two other approaches. Flaws that are restricted to
a very narrow set of topics (i.e. Atopic in Fig. 1b
is small), such as the in-universe flaw, show the
biggest drop in performance. Since the topic
bias plays a major role in the quality flaw de-
tection task, as we have shown earlier, the topic-
controlled classifier cannot take advantage of the
topic information, while the classifiers trained on
the other corpora can make use of these charac-
teristic as the most discriminative features. In the
RELALL setting, however, the differences between
the positive and negative instances are largely de-
termined by the flaws alone. Classifiers trained on
12?=0.01 for BASE,RELP and ?=0.001 for RELALL
728
such a dataset therefore come closer to recogniz-
ing the actual quality flaws, which makes them
more useful in a practical setting despite lower
cross-validated scores.
In addition to cross-validation, we performed a
cross-corpus evaluation of the classifiers for each
flaw. Therefore, we evaluated the performance of
the unbiased classifiers (trained on RELALL) on
the biased data (RELP) and vice versa. Hereby,
the positive training and test instances remain the
same in both settings, while the unbiased data con-
tains negative instances sampled from Arel and the
unbiased data from Arnd (see Figure 1). With the
NGRAM feature set, the reliable classifiers outper-
formed the unreliable classifiers on all flaws that
can be well identified with lexical cues, such as
Advert or Technical. In the biased case, we found
both topic related and flaw specific ngrams among
the most highly ranked ngram features. In the un-
biased case, most of the informative ngrams were
flaw specific expressions. Consequently, biased
classifiers fail on the unbiased dataset in which
the positive and negative class are sampled from
the same topics, which renders the highly ranked
topic ngrams unusable. Flaws that do not largely
rely on lexical cues, however, cannot be predicted
more reliably with the unbiased classifier. This
means that additional features are needed to de-
scribe these flaw. We tested this hypothesis by us-
ing the full feature set ALL and saw a substantial
improvement on the side of the unbiased classifier,
while the performance of the biased classifier re-
mained unchanged.
A direct comparison of our results to related
work is difficult, since neutrality and style flaws
have not been targeted before in a similar manner.
However, the Advert flaw was also part of the ten
flaw types in the PAN Quality Flaw Recognition
Task (Anderka and Stein, 2012b). The best system
achieved an F1 score of 0.839, which is just be-
low the results of our system on the BASE dataset,
which is similar to the PAN setup.
8 Conclusions
We showed that text classification based on
Wikipedia cleanup templates is prone to a topic
bias which causes skewed classifiers and overly
optimistic cross-validated evaluation results. This
bias is known from other text classification appli-
cations, such as authorship attribution, genre de-
tection and native language detection. We demon-
Flaw BASE RELP RELALL
Advert .86 .88 .75
Confusing .76 .80 .70
Copy edit .81 .73 .72
Essay-like .79 .83 .64
Globalize .85 .87 .69
In-universe .96 .96 .69
Peacock .77 .82 .69
POV .75 .80 .71
Technical .87 .88 .67
Tone .70 .79 .69
Trivia .72 .77 .70
Weasel .69 .77 .72
 .79 .83 .70
Table 6: F1 scores for the 10-fold cross validation
of the SVMs with RBF kernel on all datasets using
NGRAM features
strated how to avoid the topic bias when creat-
ing quality flaw corpora. Unbiased corpora are
not only necessary for training unbiased classi-
fiers, they are also invaluable resources for gaining
a deeper understanding of the linguistic properties
of the flaws. Unbiased classifiers reflect much bet-
ter the performance of quality flaw recognition ?in
the wild?, because they detect actual flawed ar-
ticles rather than identifying the articles that are
prone to certain quality due to their topic or subject
matter. In our experiments, we presented a system
for identifying Wikipedia articles with style and
neutrality flaws, a novel category of quality prob-
lems that is of particular importance within and
outside of Wikipedia. We showed that selecting
a reliable set of positive training instances mined
from the revision history improves the classifica-
tion performance. In future work, we aim to ex-
tend our quality flaw detection system to not only
find articles that contain a particular flaw, but also
to identify the flaws within the articles, which can
be achieved by leveraging the positional informa-
tion of in-line cleanup templates.
Acknowledgments
This work has been supported by the Volks-
wagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the Hessian research excellence pro-
gram ?Landes-Offensive zur Entwicklung
Wissenschaftlich-O?konomischer Exzellenz?
(LOEWE) as part of the research center ?Digital
Humanities?.
729
References
Maik Anderka and Benno Stein. 2012a. A Break-
down of Quality Flaws in Wikipedia. In 2nd Joint
WICOW/AIRWeb Workshop on Web Quality, pages
11?18, Lyon, France.
Maik Anderka and Benno Stein. 2012b. Overview of
the 1st International Competition on Quality Flaw
Prediction in Wikipedia. In CLEF 2012 Evaluation
Labs and Workshop ? Working Notes Papers.
Maik Anderka, Benno Stein, and Nedim Lipka. 2012.
Predicting Quality Flaws in User-generated Content:
The Case of Wikipedia. In 35th International ACM
Conference on Research and Development in Infor-
mation Retrieval, Portland, OR, USA.
Julian Brooke and Graeme Hirst. 2011. Native lan-
guage detection with ?cheap? learner corpora. In
Learner Corpus Research 2011 (LCR 2011).
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 shared task: learning to detect hedges and their
scope in natural language text. In Proceedings of
the Fourteenth Conference on Computational Natu-
ral Language Learning, CoNLL ?10: Shared Task,
pages 1?12, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Edgardo Ferretti, Donato Herna?ndez Fusilier, Rafael
Guzma?n-Cabrera, Manuel Montes y Go?mez,
Marcelo Errecalde, and Paolo Rosso. 2012. On the
Use of PU Learning for Quality Flaw Prediction
in Wikipedia. In CLEF 2012 Evaluation Labs and
Workshop ? Working Notes Papers.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently Ac-
cessing Wikipedia?s Edit History. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies. System Demonstrations, pages 97?102, Port-
land, OR, USA.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebo-
tar. 2012a. Behind the Article: Recognizing Dialog
Acts in Wikipedia Talk Pages. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 777?
786, Avignon, France.
Oliver Ferschke, Iryna Gurevych, and Marc Rittberger.
2012b. FlawFinder: A Modular System for Pre-
dicting Quality Flaws in Wikipedia. In CLEF 2012
Evaluation Labs and Workshop ? Working Notes Pa-
pers, Rome, Italy.
Aidan Finn and Nicholas Kushmerick. 2006. Learning
to classify documents according to genre. Journal
of the American Society for Information Science and
Technology, 57(11):1506?1518.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Up-
date. SIGKDD Explorations, 11(1):10?18.
Moshe Koppel and Jonathan Schler. 2003. Exploit-
ing stylistic idiosyncrasies for authorship attribution.
In Workshop on Computational Approaches to Style
Analysis and Synthesis, pages 69?72.
K. Luyckx and W. Daelemans. 2005. Shallow text
analysis and machine learning for authorship attri-
bution. In Proceedings of the Fifteenth Meeting of
Computational Linguistics in the Netherlands (CLIN
2004), pages 149?160.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
George K. Mikros and Eleni K. Argiri. 2007. Inves-
tigating topic influence in authorship attribution. In
Proceedings of the SIGIR 2007 International Work-
shop on Plagiarism Analysis, Authorship Identifica-
tion, and Near-Duplicate Detection, PAN 2007, Am-
sterdam, Netherlands.
Thomas Mitchell. 1997. Machine Learning. McGraw-
Hill Education, New York, NY, USA, 1st edition.
John C Platt. 1998. Fast training of support vector
machines using sequential minimal optimization. In
Advances in Kernel Methods: Support Vector Learn-
ing, pages 185?208, Cambridge, MA, USA.
Besiki Stvilia, Michael B. Twidale, Linda C. Smith,
and Les Gasser. 2008. Information Quality Work
Organization in Wikipedia. Journal of the Ameri-
can Society for Information Science and Technology,
59(6):983?1001.
Gyo?rgy Szarvas, Veronika Vincze, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic un-
certainty. Comput. Linguist., 38(2):335?367.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco.
730
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 61?66,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
DKPro TC: A Java-based Framework for Supervised Learning
Experiments on Textual Data
Johannes Daxenberger
?
, Oliver Ferschke
??
, Iryna Gurevych
??
and Torsten Zesch
??
? UKP Lab, Technische Universit?t Darmstadt
? Information Center for Education, DIPF, Frankfurt
? Language Technology Lab, University of Duisburg-Essen
http://www.ukp.tu-darmstadt.de
Abstract
We present DKPro TC, a framework for
supervised learning experiments on tex-
tual data. The main goal of DKPro TC is
to enable researchers to focus on the actual
research task behind the learning problem
and let the framework handle the rest. It
enables rapid prototyping of experiments
by relying on an easy-to-use workflow en-
gine and standardized document prepro-
cessing based on the Apache Unstruc-
tured Information Management Architec-
ture (Ferrucci and Lally, 2004). It ships
with standard feature extraction modules,
while at the same time allowing the user
to add customized extractors. The exten-
sive reporting and logging facilities make
DKPro TC experiments fully replicable.
1 Introduction
Supervised learning on textual data is a ubiquitous
challenge in Natural Language Processing (NLP).
Applying a machine learning classifier has be-
come the standard procedure, as soon as there is
annotated data available. Before a classifier can
be applied, relevant information (referred to as
features) needs to be extracted from the data. A
wide range of tasks have been tackled in this way
including language identification, part-of-speech
(POS) tagging, word sense disambiguation, sen-
timent detection, and semantic similarity.
In order to solve a supervised learning task,
each researcher needs to perform the same set of
steps in a predefined order: reading input data,
preprocessing, feature extraction, machine learn-
ing, and evaluation. Standardizing this process
is quite challenging, as each of these steps might
vary a lot depending on the task at hand. To com-
plicate matters further, the experimental process
is usually embedded in a series of configuration
changes. For example, introducing a new fea-
ture often requires additional preprocessing. Re-
searchers should not need to think too much about
such details, but focus on the actual research task.
DKPro TC is our take on the standardization of
an inherently complex problem, namely the imple-
mentation of supervised learning experiments for
new datasets or new learning tasks.
We will make some simplifying assumptions
wherever they do not harm our goal that the frame-
work should be applicable to the widest possible
range of supervised learning tasks. For example,
DKPro TC only supports a limited set of machine
learning frameworks, as we argue that differences
between frameworks will mainly influence run-
time, but will have little influence on the final con-
clusions to be drawn from the experiment. The
main goal of DKPro TC is to enable the researcher
to quickly find an optimal experimental configura-
tion. One of the major contributions of DKPro TC
is the modular architecture for preprocessing and
feature extraction, as we believe that the focus of
research should be on a meaningful and expressive
feature set. DKPro TC has already been applied to
a wide range of different supervised learning tasks,
which makes us confident that it will be of use to
the research community.
DKPro TC is mostly written in Java and freely
available under an open source license.
1
2 Requirements
In the following, we give a more detailed overview
of the requirements and goals we have identified
for a general-purpose text classification system.
These requirements have guided the development
of the DKPro TC system architecture.
1
http://dkpro-tc.googlecode.com
61
Single-label Multi-label Regression
Document Mode
? Spam Detection
? Sentiment Detection
? Text Categorization
? Keyphrase Assignment
? Text Readability
Unit/Sequence Mode
? Named Entity Recognition
? Part-of-Speech Tagging
? Dialogue Act Tagging ? Word Difficulty
Pair Mode
? Paraphrase Identification
? Textual Entailment
? Relation Extraction ? Text Similarity
Table 1: Supervised learning scenarios and feature modes supported in DKPro TC, with example NLP
applications.
Flexibility Users of a system for supervised
learning on textual data should be able to choose
between different machine learning approaches
depending on the task at hand. In supervised ma-
chine learning, we have to distinguish between ap-
proaches based on classification and approaches
based on regression. In classification, given a
document d ? D and a set of labels C =
{c
1
, c
2
, ..., c
n
}, we want to label each document
d with L ? C, where L is the set of relevant
or true labels. In single-label classification, each
document d is labeled with exactly one label, i.e.
|L| = 1, whereas in multi-label classification, a
set of labels is assigned, i.e. |L| ? 1. Single-
label classification can further be divided into bi-
nary classification (|C| = 2) and multi-class clas-
sification (|C| > 2). In regression, real numbers
instead of labels are assigned.
Feature extraction should follow a modular de-
sign in order to facilitate reuse and to allow seam-
less integration of new features. However, the way
in which features need to be extracted from the in-
put documents depends on the the task at hand.
We have identified several typical scenarios in su-
pervised learning on textual data and propose the
following feature modes:
? In document mode, each input document will
be used as its own entity to be classified, e.g.
an email classified as wanted or unwanted
(spam).
? In unit/sequence mode, each input document
contains several units to be classified. The
units in the input document cannot be divided
into separate documents, either because the
context of each unit needs to be preserved
(e.g. to disambiguate named entities) or be-
cause they form a sequence which needs to
be kept (in sequence tagging).
? The pair mode is intended for problems
which require a pair of texts as input, e.g.
a pair of sentences to be classified as para-
phrase or non-paraphrase. It represents a
special case of multi-instance learning (Sur-
deanu et al., 2012), in which a document con-
tains exactly two instances.
Considering the outlined learning approaches and
feature modes, we have summarized typical sce-
narios in supervised learning on textual data in Ta-
ble 1 and added example applications in NLP.
Replicability and Reusability As it has been
recently noted by Fokkens et al. (2013), NLP ex-
periments are not replicable in most cases. The
problem already starts with undocumented pre-
processing steps such as tokenization or sentence
boundary detection that might have heavy impact
on experimental results. In a supervised learning
setting, this situation is even worse, as e.g. fea-
ture extraction is usually only partially described
in the limited space of a research paper. For ex-
ample, a paper might state that ?n-gram features?
were used, which encompasses a very broad range
of possible implementations.
In order to make NLP experiments replicable, a
text classification framework should (i) encourage
the user to reuse existing components which they
can refer to in research papers rather than writ-
ing their own components, (ii) document all per-
formed steps, and (iii) make it possible to re-run
experiments with minimal effort.
Apart from helping the replicability of experi-
ments, reusing components allows the user to con-
centrate on the new functionality that is specific
to the planned experiment instead of having to
reinvent the wheel. The parts of a text classifi-
cation system which can typically be reused are
62
preprocessing components, generic feature extrac-
tors, machine learning algorithms, and evaluation.
3 Architecture
We now give an overview of the DKPro TC archi-
tecture that was designed to take into account the
requirements outlined above. A core design deci-
sion is to model each of the typical steps in text
classification (reading input data and preprocess-
ing, feature extraction, machine learning and eval-
uation) as separate tasks. This modular architec-
ture helps the user to focus on the main problem,
i.e. developing and selecting good features.
In the following, we describe each module in
more detail, starting with the workflow engine that
is used to assemble the tasks into an experiment.
3.1 Configuration and Workflow Engine
We rely on the DKPro Lab (Eckart de Castilho
and Gurevych, 2011) workflow engine, which al-
lows fine-grained control over the dependencies
between single tasks, e.g. the pre-processing of a
document obviously needs to happen before the
feature extraction. In order to shield the user
from the complex ?wiring? of tasks, DKPro TC
currently provides three pre-defined workflows:
Train/Test, Cross-Validation, and Prediction (on
unseen data). Each workflow supports the feature
modes described above: document, unit/sequence,
and pair.
The user is still able to control the behavior of
the workflow by setting parameters, most impor-
tantly the sources of input data, the set of feature
extractors, and the classifier to be used. Internally,
each parameter is treated as a single dimension
in the global parameter space. Users may pro-
vide more than one value for a certain parame-
ter, e.g. specific feature sets or several classifiers.
The workflow engine will automatically run all
possible parameter value combinations (a process
called parameter sweeping).
3.2 Reading Input Data
Input data for supervised learning tasks comes in
myriad different formats which implies that read-
ing data cannot be standardized, but needs to be
handled individually for each data set. However,
the internal processing should not be dependent on
the input format. We therefore use the Common
Analysis Structure (CAS), provided by the Apache
Unstructured Information Management Architec-
ture (UIMA), to represent input documents and
annotations in a standardized way.
Under the UIMA model, reading input data
means to transform arbitrary input data into a
CAS representation. DKPro TC already provides
a wide range of readers from UIMA component
repositories such as DKPro Core.
2
The reader
also needs to assign to each classification unit an
outcome attribute that represents the relevant label
(single-label), labels (multi-label), or a real value
(regression). In unit/sequence mode, the reader
additionally needs to mark the units in the CAS.
In pair mode, a pair of texts (instead of a single
document) is stored within one CAS.
3.3 Preprocessing
In this step, additional information about the docu-
ment is added to the CAS, which efficiently stores
large numbers of stand-off annotations. In pair
mode, the preprocessing is automatically applied
to both documents.
DKPro TC allows the user to run arbitrary
UIMA-based preprocessing components as long
as they are compatible with the DKPro type sys-
tem that is currently used by DKPro Core and
EOP.
3
Thus, a large set of ready-to-use prepro-
cessing components for more than ten languages
is available, containing e.g. sentence boundary de-
tection, lemmatization, POS-tagging, or parsing.
3.4 Feature Extraction
DKPro TC ships a constantly growing number of
feature extractors. Feature extractors have access
to the document text as well as all the additional
information that has been added in the form of
UIMA stand-off annotations during the prepro-
cessing step. Users of DKPro TC can add cus-
tomized feature extractors for particular use cases
on demand.
Among the ready-to-use feature extractors con-
tained in DKPro TC, there are several ones ex-
tracting grammatical information, e.g. the plural-
singular ratio or the ratio of modal to all verbs.
Other features collect information about stylistic
cues of a document, e.g. the number of exclama-
tions or the type-token-ratio. DKPro TC is able to
extract n-grams or skip n-grams of tokens, charac-
ters, and POS tags.
Some feature extractors need access to informa-
tion about the entire document collection, e.g. in
2
http://dkpro-core-asl.googlecode.com
3
http://hltfbk.github.io/Excitement-Open-Platform/
63
order to weigh lexical features with tf.idf scores.
Such extractors have to declare that they depend
on collection level information and DKPro TC
will automatically include a special task that is
executed before the actual features are extracted.
Depending on the feature mode which has been
configured, DKPro TC will extract information
on document level, unit- and/or sequence-level, or
document pair level.
DKPro TC stores extracted features in its inter-
nal feature store. When the extraction process is
finished, a configurable data writer converts the
content from the feature store into a format which
can be handled by the utilized machine learning
tool. DKPro TC currently ships data writers for
the Weka (Hall et al., 2009), Meka
4
, and Mallet
(McCallum, 2002) frameworks. Users can also
add dedicated data writers that output features in
the format used by the machine learning frame-
work of their choice.
3.5 Supervised Learning
For the actual machine learning, DKPro TC cur-
rently relies on Weka (single-label and regres-
sion), Meka (multi-label), and Mallet (sequence
labeling). It contains a task which trains a freely
configurable classifier on the training data and
evaluates the learned model on the test data.
Before training and evaluation, the user may ap-
ply dimensionality reduction to the feature set, i.e.
select a limited number of (expectedly meaning-
ful) features to be included for training and eval-
uating the classifier. DKPro TC uses the feature
selection capabilities of Weka (single-label and re-
gression) and Mulan (multi-label) (Tsoumakas et
al., 2010).
DKPro TC can also predict labels on unseen
(i.e. unlabeled) data, using a trained classifier. In
that case, no evaluation will be carried out, but the
classifier?s prediction for each document will be
written to a file.
3.6 Evaluation and Reporting
DKPro TC calculates common evaluation scores
including accuracy, precision, recall, and F
1
-
score. Whenever sensible, scores are reported for
each individual label as well as aggregated over
all labels. To support users in further analyz-
ing the performance of a classification workflow,
DKPro TC outputs the confusion matrix, the ac-
4
http://meka.sourceforge.net
tual predictions assigned to each document, and a
ranking of the most useful features based on the
configured feature selection algorithm. Additional
task-specific reporting can be added by the user.
As mentioned before, a major goal of
DKPro TC is to increase the replicability of NLP
experiments. Thus, for each experiment, all con-
figuration parameters are stored and will be re-
ported together with the classification results.
4 Tweet Classification: A Use Case
We now give a brief summary of what a supervised
learning task might look like in DKPro TC using
a simple Twitter sentiment classification example.
Assuming that we want to classify a set of tweets
either as ?emotional? or ?neutral?, we can use the
setup shown in Listing 1. The example uses the
Groovy programming language which yields bet-
ter readable code, but pure Java is also supported.
Likewise, a DKPro TC experiment can also be set
up with the help of a configuration file, e.g. in
JSON or via Groovy scripts.
First, we create a workflow as a BatchTask-
CrossValidation which can be used to run
a cross-validation experiment on the data (using
10 folds as configured by the corresponding pa-
rameter). The workflow uses LabeledTweet-
Reader in order to import the experiment data
from source text files into the internal document
representation (one document per tweet). This
reader adds a UIMA annotation that specifies the
gold standard classification outcome, i.e. the rel-
evant label for the tweet. In this use case, pre-
processing consists of a single step: running the
ArkTweetTagger (Gimpel et al., 2011), a spe-
cialized Twitter tokenizer and POS-tagger that is
integrated in DKPro Core. The feature mode is set
to document (one tweet per CAS), and the learning
mode to single-label (each tweet is labeled with
exactly one label), cf. Table 1.
Two feature extractors are configured: One for
returning the number of hashtags and another one
returning the ratio of emoticons to tokens in the
tweet. Listing 2 shows the Java code for the sec-
ond extractor. Two things are noteworthy: (i) doc-
ument text and UIMA annotations are readily
available through the JCas object, and (ii) this is
really all that the user needs to write in order to
add a new feature extractor.
The next item to be configured is the Weka-
DataWriter which converts the internal fea-
64
BatchTaskCrossValidation batchTask = [
experimentName: "Twitter-Sentiment",
preprocessingPipeline: createEngineDescription(ArkTweetTagger), // Preprocessing
parameterSpace: [ // multi-valued parameters in the parameter space will be swept
Dimension.createBundle("reader", [
readerTrain: LabeledTweetReader,
readerTrainParams: [LabeledTweetReader.PARAM_CORPUS_PATH, "src/main/resources/tweets.txt"]]),
Dimension.create("featureMode", "document"),
Dimension.create("learningMode", "singleLabel"),
Dimension.create("featureSet", [EmoticonRatioExtractor.name, NumberOfHashTagsExtractor.name]),
Dimension.create("dataWriter", WekaDataWriter.name),
Dimension.create("classificationArguments", [NaiveBayes.name, RandomForest.name])],
reports: [BatchCrossValidationReport], // collects results from folds
numFolds: 10];
Listing 1: Groovy code to configure a DKPro TC cross-validation BatchTask on Twitter data.
public class EmoticonRatioFeatureExtractor
extends FeatureExtractorResource_ImplBase implements DocumentFeatureExtractor
{
@Override
public List<Feature> extract(JCas annoDb) throws TextClassificationException {
int nrOfEmoticons = JCasUtil.select(annoDb, EMO.class).size();
int nrOfTokens = JCasUtil.select(annoDb, Token.class).size();
double ratio = (double) nrOfEmoticons / nrOfTokens;
return new Feature("EmoticonRatio", ratio).asList();
}
}
Listing 2: A DKPro TC document mode feature extractor measuring the ratio of emoticons to tokens.
ture representation into the Weka ARFF format.
For the classification, two machine learning algo-
rithms will be iteratively tested: a Naive Bayes
classifier and a Random Forest classifier. Pass-
ing a list of parameters into the parameter space
will automatically make DKPro TC test all pos-
sible parameter combinations. The classification
task automatically trains a model on the training
data and stores the results of the evaluation on
the test data for each fold on the disk. Finally,
the evaluation scores for each fold are collected
by the BatchCrossValidationReport and
written to a single file using a tabulated format.
5 Related Work
This section will give a brief overview about tools
with a scope similar to DKPro TC. We only list
freely available software, most of which is open-
source. Unless otherwise indicated, all of the tools
are written in Java.
ClearTK (Ogren et al., 2008) is conceptually
closest to DKPro TC and shares many of its dis-
tinguishing features like the modular feature ex-
tractors. It provides interfaces to machine learn-
ing libraries such as Mallet or libsvm, offers wrap-
pers for basic NLP components, and comes with
a feature extraction library that facilitates the de-
velopment of custom feature extractors within the
UIMA framework. In contrast to DKPro TC, it is
rather designed as a programming library than a
customizable research environment for quick ex-
periments and does not provide predefined text
classification setups. Furthermore, it does not sup-
port parameter sweeping and has no explicit sup-
port for creating experiment reports.
Argo (Rak et al., 2013) is a web-based work-
bench with support for manual annotation and au-
tomatic analysis of mainly bio-medical data. Like
DKPro TC, Argo is based on UIMA, but focuses
on sequence tagging, and it lacks DKPro TC?s pa-
rameter sweeping capabilities.
NLTK (Bird et al., 2009) is a general-purpose
NLP toolkit written in Python. It offers com-
ponents for a wide range of preprocessing tasks
and also supports feature extraction and machine
learning for supervised text classification. Like
DKPro TC, it can be used to quickly setup baseline
experiments. As opposed to DKPro TC, NLTK
lacks a modular structure with respect to prepro-
cessing and feature extraction and does not sup-
port parameter sweeping.
Weka (Hall et al., 2009) is a machine learning
framework that covers only the last two steps of
DKPro TC?s experimental process, i.e. machine
learning and evaluation. However, it offers no ded-
icated support for preprocessing and feature gener-
ation. Weka is one of the machine learning frame-
works that can be used within DKPro TC for ac-
tual machine learning.
Mallet (McCallum, 2002) is another machine
65
learning framework implementing several super-
vised and unsupervised learning algorithms. As
opposed to Weka, is also supports sequence tag-
ging, including Conditional Random Fields, as
well as topic modeling. Mallet can be used as ma-
chine learning framework within DKPro TC.
Scikit-learn (Pedregosa et al., 2011) is a ma-
chine learning framework written in Python. It
offers basic functionality for preprocessing, fea-
ture selection, and parameter tuning. It provides
some methods for preprocessing such as convert-
ing documents to tf.idf vectors, but does not offer
sophisticated and customizable feature extractors
for textual data like DKPro TC.
6 Summary and Future Work
We have presented DKPro TC, a comprehensive
and flexible framework for supervised learning on
textual data. DKPro TC makes setting up exper-
iments and creating new features fast and simple,
and can therefore be applied for rapid prototyp-
ing. Its extensive logging capabilities emphasize
the replicability of results. In our own research
lab, DKPro TC has successfully been applied to a
wide range of tasks including author identification,
text quality assessment, and sentiment detection.
There are some limitations to DKPro TC which
we plan to address in future work. To reduce the
runtime of experiments with very large document
collections, we want to add support for parallel
processing of documents. While the current main
goal of DKPro TC is to bootstrap experiments on
new data sets or new applications, we also plan to
make DKPro TC workflows available as resources
to other applications, so that a model trained with
DKPro TC can be used to automatically label tex-
tual data in different environments.
Acknowledgments
This work has been supported by the Volks-
wagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806,
and by the Hessian research excellence pro-
gram ?Landes-Offensive zur Entwicklung
Wissenschaftlich-?konomischer Exzellenz?
(LOEWE) as part of the research center ?Digital
Humanities?. The authors would like give special
thanks to Richard Eckhart de Castilho, Nicolai
Erbs, Lucie Flekova, Emily Jamison, Krish
Perumal, and Artem Vovk for their contributions
to the DKPro TC framework.
References
S. Bird, E. Loper, and E. Klein. 2009. Natural Lan-
guage Processing with Python. O?Reilly Media Inc.
R. Eckart de Castilho and I. Gurevych. 2011. A
Lightweight Framework for Reproducible Parame-
ter Sweeping in Information Retrieval. In Proc. of
the Workshop on Data Infrastructures for Support-
ing Information Retrieval Evaluation, pages 7?10.
D. Ferrucci and A. Lally. 2004. UIMA: An Ar-
chitectural Approach to Unstructured Information
Processing in the Corporate Research Environment.
Natural Language Engineering, 10(3-4):327?348.
A. Fokkens, M. van Erp, M. Postma, T. Pedersen,
P. Vossen, and N. Freire. 2013. Offspring from
Reproduction Problems: What Replication Failure
Teaches Us. In Proc. ACL, pages 1691?1701.
K. Gimpel, N. Schneider, B. O?Connor, D. Das,
D. Mills, J. Eisenstein, M. Heilman, D. Yogatama,
J. Flanigan, and N. Smith. 2011. Part-of-speech
tagging for Twitter: annotation, features, and exper-
iments. In Proc. ACL, pages 42?47.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. Witten. 2009. The WEKA Data Min-
ing Software: An Update. SIGKDD Explorations,
11(1):10?18.
A. McCallum. 2002. MALLET: A Machine Learning
for Language Toolkit.
P. Ogren, P. Wetzler, and S. Bethard. 2008. ClearTK:
A UIMA toolkit for statistical natural language pro-
cessing. In Towards Enhanced Interoperability for
Large HLT Systems: UIMA for NLP workshop at
LREC, pages 32?38.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine Learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
R. Rak, A. Rowley, J. Carter, and S. Ananiadou.
2013. Development and Analysis of NLP Pipelines
in Argo. In Proc. ACL, pages 115?120.
M. Surdeanu, J. Tibshirani, R. Nallapati, and C. Man-
ning. 2012. Multi-instance multi-label learning for
relation extraction. In Proc. EMNLP-CoNLL, pages
455?465.
G. Tsoumakas, I. Katakis, and I. Vlahavas. 2010. Min-
ing Multi-label Data. Transformation, 135(2):1?20.
66
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 704?710,
Dublin, Ireland, August 23-24, 2014.
UKPDIPF: A Lexical Semantic Approach to Sentiment Polarity
Prediction in Twitter Data
Lucie Flekova
??
, Oliver Ferschke
??
and Iryna Gurevych
??
?
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Computer Science Department, Technische Universit?at Darmstadt
?
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research
http://www.ukp.tu-darmstadt.de
Abstract
We present a sentiment classification sys-
tem that participated in the SemEval 2014
shared task on sentiment analysis in Twit-
ter. Our system expands tokens in a tweet
with semantically similar expressions us-
ing a large novel distributional thesaurus
and calculates the semantic relatedness of
the expanded tweets to word lists repre-
senting positive and negative sentiment.
This approach helps to assess the polarity
of tweets that do not directly contain po-
larity cues. Moreover, we incorporate syn-
tactic, lexical and surface sentiment fea-
tures. On the message level, our system
achieved the 8th place in terms of macro-
averaged F-score among 50 systems, with
particularly good performance on the Life-
Journal corpus (F
1
=71.92) and the Twitter
sarcasm (F
1
=54.59) dataset. On the ex-
pression level, our system ranked 14 out
of 27 systems, based on macro-averaged
F-score.
1 Introduction
Microblogging sites, such as Twitter, have become
an important source of information about current
events. The fact that users write about their ex-
periences, often directly during or shortly after
an event, contributes to the high level of emo-
tions in many such messages. Being able to auto-
matically and reliably evaluate these emotions in
context of a specific event or a product would be
highly beneficial not only in marketing (Jansen et
al., 2009) or public relations, but also in political
sciences (O?Connor et al., 2010), disaster manage-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
ment, stock market analysis (Bollen et al., 2011)
or the health sector (Culotta, 2010).
Due to its large number of applications, senti-
ment analysis on Twitter is a very popular task.
Challenges arise both from the character of the
task and from the language specifics of Twit-
ter messages. Messages are normally very short
and informal, frequently using slang, alternative
spelling, neologism and links, and mostly ignor-
ing the punctuation.
Our experiments have been carried out as part
of the SemEval 2014 Task 9 - Sentiment Anal-
ysis on Twitter (Rosenthal et al., 2014), a rerun
of a SemEval-2013 Task 2 (Nakov et al., 2013).
The datasets are thus described in detail in the
overview papers. The rerun uses the same train-
ing and development data, but new test data from
Twitter and a ?surprise domain?. The task con-
sists of two subtasks: an expression-level subtask
(Subtask A) and a message-level subtask (Subtask
B). In subtask A, each tweet in a corpus contained
a marked instance of a word or phrase. The goal
is to determine whether that instance is positive,
negative or neutral in that context. In subtask B,
the goal is to classify whether the entire message
is of positive, negative, or neutral sentiment. For
messages conveying both a positive and negative
sentiment, the stronger one should be chosen.
The key components of our system are the sen-
timent polarity lexicons. In contrast to previous
approaches, we do not only count exact lexicon
hits, but also calculate explicit semantic related-
ness (Gabrilovich and Markovitch, 2007) between
the tweet and the sentiment list, benefiting from
resources such as Wiktionary and WordNet. On
top of that, we expand content words (adjectives,
adverbs, nouns and verbs) in the tweet with sim-
ilar words, which we derive from a novel corpus
of more than 80 million English Tweets gathered
by the Language Technology group
1
at TU Darm-
1
http://www.lt.informatik.tu-darmstadt.de
704
stadt.
2 Experimental setup
Our experimental setup is based on an open-source
text classification framework DKPro TC
2
(Daxen-
berger et al., 2014), which allows to combine NLP
pipelines into a configurable and modular system
for preprocessing, feature extraction and classifi-
cation. We use the unit classification mode of
DKPro TC for Subtask A and the document clas-
sification mode for Subtask B.
2.1 Preprocessing
We customized the message reader for Subtask B
to ignore the first part of the tweet when the word
but is found. This approach helps to reduce the
misleading positive hits when a negative message
is introduced positively (It?d be good, but).
For preprocessing the data, we use components
from DKPro Core
3
. Preprocessing is the same
for subtasks A and B, with the only difference
that in the subtask A the target expression is addi-
tionally annotated as text classification unit, while
the rest of the tweet is considered to be a doc-
ument context. We first segment the data with
the Stanford Segmenter
4
, apply the Stanford POS
Tagger with a Twitter-trained model (Derczynski
et al., 2013), and subsequently apply the Stan-
ford Lemmatizer
4
, TreeTagger Chunker (Schmid,
1994), Stanford Named Entity Recognizer (Finkel
et al., 2005) and Stanford Parser (Klein and Man-
ning, 2003) to each tweet. After this linguistic pre-
processing, the token segmentation of the Stanford
tools is removed and overwritten by the ArkTweet
Tagger (Gimpel et al., 2011), which is more suit-
able for recognizing hashtags and smileys as one
particular token. Finally, we expand the tweet and
proceed to feature extraction as described in detail
in Section 3.
2.2 Classification
We trained our system on the provided training
data only, excluding the dev data. We use clas-
sifiers from the WEKA (Hall et al., 2009) toolkit,
which are integrated in the DKPro TC framework.
Our final configuration consists of a SVM-SMO
classifier with a gaussian kernel. The optimal hy-
perparameters have been experimentally derived
2
http://code.google.com/p/dkpro-tc
3
http://code.google.com/p/dkpro-core-asl
4
http://nlp.stanford.edu/software/corenlp.shtml
and finally set to C=1 and G=0.01. The resulting
model was wrapped in a cost sensitive meta classi-
fier from the WEKA toolkit with the error costs set
to reflect the class imbalance in the training set.
3 Features used
We now describe the features used in our exper-
iments. For Subtask A (contextual polarity), we
extracted each feature twice - once on the tweet
level and once on the focus expression level. Only
n-gram features were extracted solely from the ex-
pressions. For Subtask B (tweet polarity), we ex-
tracted features on tweet level only. In both cases,
we use the Information Gain feature selection ap-
proach in WEKA to rank the features and prune
the feature space with a threshold of T=0.005.
3.1 Lexical features
As a basis for our similarity and expansion ex-
periments (sections 3.4 and 3.5), we use the bi-
nary sentiment polarity lexicon by Liu (2012) aug-
mented with the smiley polarity lexicon by Becker
et al. (2013) and an additional swear word list
5
[further as Liu
augmented
]. We selected this aug-
mented lexicon for two reasons: firstly, it was the
highest ranked lexical feature on the development-
test and crossvalidation experiments, secondly it
consists of two plain word lists and therefore does
not introduce another complexity dimension for
advanced feature calculations.
We further measure lexicon hits normalized per
number of tweet tokens for the following lexicons:
Pennebaker?s Linguistic Inquiry and Word Count
(LIWC) (Pennebaker et al., 2001), the NRC Emo-
tion Lexicon (Mohammad and Turney, 2013), the
NRC Hashtag Emotion Lexicon (Mohammad et
al., 2013) and the Sentiment140 lexicon (Moham-
mad et al., 2013). We use an additional lexicon
of positive, negative, very positive and very nega-
tive words, diminishers, intensifiers and negations
composed by Steinberger et al. (2012), where we
calculate the polarity score as described in their
paper.
In a complementary set of features we combine
each of the lexicons above with a list of weighted
intensifying expressions as published by Brooke
(2009). The intensity of any polar word found in
any of the emotion lexicons used is intensified or
diminished by a given weight if an intensifier (a
5
based on http://www.youswear.com
705
bit, very, slightly...) is found within the preceding
three tokens.
Additionally, we record the overall counts of
lexicon hits for positive words, negative words and
the difference of the two. In one set of features
we consider only lexicons clearly meant for binary
polarity, while a second set of features also in-
cludes other emotions, such as fear or anger, from
the NRC and the LIWC corpora.
3.2 Negation
We handle negation in two ways. On the expres-
sion level (Subtask A) we rely on the negation
dependency tag provided by the Stanford Depen-
dency Parser. This one captures verb negations
rather precisely and thus helps to handle emotional
verb expressions such as like vs don?t like. On the
tweet level (all features of Subtask B and entire-
tweet-level features of Subtask A) we adopt the
approach of Pang et al. (2002), considering as a
negation context any sequence of tokens between
a negation expression and the end of a sentence
segment as annotated by the Stanford Segmenter.
The negation expressions (don?t, can?t...) are rep-
resented by the list of invertors from Steinberger?s
lexicon (Steinberger et al., 2012). We first assign
polarity score to each word in the tweet based on
the lexicon hits and then revert it for the words ly-
ing in the negation context. This approach is more
robust than the one of the dependency governor
but is error-prone in the area of overlapping (cas-
caded) negation contexts.
3.3 N-gram features
We extract the 5,000 most frequent word uni-
grams, bigrams and trigrams cleaned with the
Snowball stopword list
6
as well as the same
amount of skip-n-grams and character trigrams.
These are extracted separately on the target ex-
pression level for subtask A and on document
level for subtask B. On the syntactic level, we
monitor the most frequent 5,000 part-of-speech
ngrams with the size up to part-of-speech quadru-
ples. Additionally, as an approximation for ex-
ploiting the key message of the sentence, we ex-
tract from the tweets a verb chunk and its left and
right neighboring noun chunks, obtaining combi-
nations such as we-go-cinema. The 1,000 most
frequent chunk triples are then used as features
similarly to ngrams.
6
http://snowball.tartarus.org/algorithms/english/stop.txt
Word Score Word (continued) Score
awesome 1,000 fun 60
amazing 194 sexy 59
great 148 cold 59
cool 104 crazy 57
good 96 fantastic 56
best 93 bored 55
beautiful 93 excited 54
nice 87 true 53
funny 84 stupid 53
cute 81 gr8 52
perfect 70 entertaining 52
wonderful 67 favorite 52
lovely 66 talented 49
tired 65 other 49
annoying 63 depressing 48
Great 63 flawless 48
new 62 inspiring 47
hilarious 62 incredible 46
bad 61 complicated 46
hot 61 gorgeous 45
Table 1: Unsupervised expansion of ?awesome?
3.4 Tweet expansion
We expanded the content words in a tweet, i.e.
nouns, verbs, adjectives and adverbs, with sim-
ilar words from a word similarity thesaurus that
was computed on 80 million English tweets from
2012 using the JoBim contextual semantics frame-
work (Biemann and Riedl, 2013). Table 1 shows
an example for a lexical expansion of the word
awesome. The score was computed using left and
right neighbor bigram features for the holing oper-
ation. The value hence shows how often the word
appeared in the same left and right context as the
original word. The upper limit of the score is set
to 1,000.
We then match the expanded tweet against the
Liu
augmented
positive and negative lexicons. We
assign to the lexicon hits of the expanded words
their (contextual similarity) expansion score, us-
ing a score of 1,000 as an anchor-value for the
original tweet, setting an expansion cut at 100.
The overall tweet score is then normalized by the
sum of word expansion scores.
3.5 Semantic similarity
Tweet messages are short and each emotional
word is very valuable for the task, even when it
may not be present in a specific lexicon. There-
fore, we calculate a semantic relatedness score
between the tweet and the positive or negative
word list. We use the ESA similarity measure
(Gabrilovich and Markovitch, 2007) as imple-
mented in the DKPro similarity software pack-
706
age (B?ar et al., 2013), calculated on English Wik-
tionary and WordNet as two separate concept
spaces. The ESA vectors are freely available
7
.
This way we obtain in total six features: sim(orig-
inal tweet word list, positive word list), sim(orig-
inal tweet word list, negative word list), differ-
ence between the two, sim(expanded tweet word
list, positive word list), sim(expanded tweet word
list, negative word list) and difference between the
two. Our SemEval run was submitted using Word-
Net vectors mainly for the shorter computation
time and lower memory requirements. However,
in our later experiments Wiktionary performed
better. We presume this can be due to a better
coverage for the Twitter corpus, although detailed
analysis of this aspect is yet to be performed.
3.6 Other features
Pak and Paroubek (2010) pointed out a relation
between the presence of different part-of-speech
types and sentiment polarity. We measure the
ratio of each part-of-speech type to each chunk.
We furthermore count the occurrences of the
dependency tag for negation. We use the Stanford
Named Entity Recognizer to count occurrence
of persons, organizations and locations in the
tweet. Additionaly, beside basic surface metrics,
such as the number of tokens, characters and
sentences, we measure the number of elon-
gated words (such as coool) in a tweet, ratio
of sentences ending with exclamation, ratio of
questions and number of positive and negative
smileys and their proportion. We capture the
smileys with the following two regular expres-
sions for positive, respectively negative ones:
[<>]?[:;=8][-o
*
?]?[)]dDpPxXoO0
*
}],
[<>]?[:;=8][-o
*
?]?[([/:{|]. We also
separately measure the sentiment of smileys at
the end of the tweet body, i.e. followed only by a
hashtag, hyperlink or nothing.
4 Results
In Subtask A, our system achieved an averaged
F-score of 81.42 on the LiveJournal corpus and
79.67 on the Twitter 2014 corpus. The highest
scores achieved in related work were 85.61 and
86.63 respectively. For subtask B, we scored 71.92
on LifeJournal and 63.77 on Twitter 2014, while
the highest F-scores reported by related work were
74.84 and 70.96.
7
https://code.google.com/p/dkpro-similarity-asl/downloads/list
Features with the highest Information Gain
were the ones based on Liu
augmented
. Adding the
weighted intensifiers of Brooke to the sentiment
lexicons did not outperform the simple lexicon
lookup. They were followed by features derived
from the lexicons of Steinberger, which includes
invertors, intensifiers and four polarity levels of
words. On the other hand, adding the weighted
intensifiers of Brooke to lexicons did not outper-
form the simple lexicon lookup. Overall, lexicon-
based features contributed to the highest perfor-
mance gain, as shown in Table 3. The negation
approach based on the Stanford dependency parser
was the most helpful, although it tripled the run-
time. Using the simpler negation context as sug-
gested in Pang et al. (2002) performed still on av-
erage better than using none.
When using WordNet, semantic similarity to
lexicons did not outperform direct lexicon hits.
Usage of Wiktionary instead lead to major im-
provement (Table 3), unfortunately after the Se-
mEval challenge.
Tweet expansion appears to improve the clas-
sification performance, however the threshold of
100 that we used in our setup was chosed too
conservatively, expanding mainly stopwords with
other stopwords or words with their spelling al-
ternatives, resulting in a noisy, little valuable fea-
ture (expansion full in Table 3). Setting
up the threshold to 50 and cleaning up both the
tweet and the expansion with Snowball stopword
list (expansion clean in Table 3), the perfor-
mance increased remarkably.
Amongst other prominent features were parts of
lexicons such as LIWC Positive emotions, LIWC
Affect, LIWC Negative emotions, NRC Joy, NRC
Anger and NRC Disgust. Informative were also
the proportions of nouns, verbs and adverbs, the
exclamation ratio or number of positive and nega-
tive smileys at the end of the tweet.
Feature(s) ?F
1
Twitter2014 ?F
1
LifeJournal
Similarity Wikt. 0.56 3.65
Similarity WN 0.0 2.61
Expansion full 0.0 0.0
Expansion clean 0.59 3.82
Lexical negation 0.24 0.13
N-gram features 0.30 0.32
Lexicon-based f. 7.85 4.74
Table 3: Performance increase where feature
added to the full setup
707
# Gold label Prediction Message
1 negative positive Your plans of attending the Great Yorkshire Show may have been washed out because
of the weather, so how about...
2 neutral positive sitting here with my belt in jean shorts watching Cena win his first title.
I think we tie for 1st my friend xD
3 neutral positive saw your LJ post ... yay for Aussies ;)
4 positive negative haha , that sucks , because the drumline will be just fine
5 positive negative ...woah, Deezer. Babel only came out on Monday, can you leave it up for longer than a day
to give slow people like me a chance?
6 positive negative Yeah so much has changed for the 6th. Lots of combat fighting. And inventory is different.
7 positive negative just finish doing it and tomorrow I?m going to the celtics game and don?t fucking say
?thanks for the invite? it?s annoying
8 positive negative Haha... Yup hopefully we will lose a few kg by mon. after hip hop can go orchard and weigh
9 positive negative U r just like my friends? I made them feel warm, happy, then make them angry and they cry?
Finally they left me? Will u leave 2? I hope not. Really hope so.
Table 2: Examples of misclassified messages
5 Error analysis
Table 2 lists a sample of misclassified messages.
The majority of errors resulted from misclassify-
ing neutral tweets as emotionally charged. This
was partly caused by the usage of emoticons and
expressions such as haha in a neutral context, such
as in examples 2 and 3. Other errors were cause by
lexicon hits of proper nouns (example 1), or by us-
ing negative words and swearwords in overall pos-
itive tweet (examples 4, 7, 9). Some tweets con-
tained domain specific vocabulary that would hit
the negative lexicon, e.g., discussing fighting and
violence in computer games would, in contrast to
other topic domains, usually have positive polar-
ity (example 6). Similar domain-specific polarity
distinction could be applied to certain verbs, e.g.,
lose weight vs. lose a game (example 8).
Another challenge for the system was the non-
standard language in twitter with a large number of
spelling variants, which was only partly captured
by the emotion lexicons tailored for this domain.
A twitter-specific lemmatizer, which would group
all variations of a misspelled word into one, could
help to improve the performance.
The length of the negation context window does
not suit all purposes. Also double negations such
as I don?t think he couldn?t... can easily misdirect
the polarity score.
6 Conclusion
We presented a sentiment classification system
that can be used on both message level and ex-
pression level with only small changes in the
framework configuration. We employed a con-
textual similarity thesaurus for the lexical expan-
sion of the messages. The expansion was not
efficient without an extensive stopword cleaning,
overweighting more common words and introduc-
ing noise. Utilizing the semantic similarity of
tweets to lexicons instead of a direct match im-
proves the score only with certain lexicons, possi-
bly dependent on the coverage. Negation by de-
pendency parsing was more beneficial to the clas-
sifier than the negation by keyword span anno-
tation. Naive combination of sentiment lexicons
was not more helpful than using individual ones
separately. Among the common source of errors
were laughing signs used in neutral messages and
swearing used in positive messages. Even within
Twitter, same words can have different polarity in
different domains (lose weight, lose game, game
with nice violent fights...). Deeper semantic in-
sights are necessary to distinguish between polar
words in context.
7 Acknowledgement
This work has been supported by the Volkswagen
Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We warmly thank Chris Biemann, Martin Riedl
and Eugen Ruppert of the Language Technology
group at TU Darmstadt for providing us with the
Twitter-based distributional thesaurus.
References
Daniel B?ar, Torsten Zesch, and Iryna Gurevych. 2013.
Dkpro similarity: An open source framework for
text similarity. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations, pages 121?126,
Sofia, Bulgaria.
Lee Becker, George Erhart, David Skiba, and Valentine
708
Matula. 2013. Avaya: Sentiment analysis on twit-
ter with self-training and polarity lexicon expansion.
Atlanta, Georgia, USA, page 333.
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2d! a framework for lexical expansion with con-
textual similarity. Journal of Language Modelling,
1(1):55?95.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Julian Brooke. 2009. A semantic approach to auto-
mated text sentiment analysis.
Aron Culotta. 2010. Towards detecting influenza epi-
demics by analyzing twitter messages. In Proceed-
ings of the First Workshop on Social Media Analyt-
ics, pages 115?122, New York, NY, USA.
Johannes Daxenberger, Oliver Ferschke, Iryna
Gurevych, and Torsten Zesch. 2014. Dkpro tc:
A java-based framework for supervised learning
experiments on textual data. In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics. System Demonstrations,
page (to appear), Baltimore, MD, USA.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing, Hissar,
Bulgaria.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), pages 363?370.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings
of the 20th International Joint Conference on Arti-
ficial Intelligence, volume 7, pages 1606?1611, Hy-
derabad, India.
Kevin Gimpel, Nathan Schneider, Brendan O?Con-
nor, Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flani-
gan, and Noah A Smith. 2011. Part-of-speech tag-
ging for twitter: Annotation, features, and experi-
ments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies: short papers-
Volume 2, pages 42?47.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an up-
date. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. Journal of the Ameri-
can Society for Information Science and Technology,
60(11):2169?2188.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Saif M Mohammad and Peter D Turney. 2013. Crowd-
sourcing a word?emotion association lexicon. Com-
putational Intelligence, 29(3):436?465.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Seman-
tic Evaluation Exercises (SemEval-2013), Atlanta,
Georgia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wil-
son. 2013. Semeval-2013 task 2: Sentiment anal-
ysis in twitter. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
312?320, Atlanta, Georgia, USA.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R Routledge, and Noah A Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Fourth International
AAAI Conference on Weblogs and Social Media,
pages 122?129.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opin-
ion mining. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), Valletta, Malta.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79?86.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Mahway: Lawrence Erlbaum Asso-
ciates, 71:2001.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
709
Torsten Zesch, editors, Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation, Dublin,
Ireland.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of in-
ternational conference on new methods in language
processing, volume 12, pages 44?49.
Josef Steinberger, Mohamed Ebrahim, Maud Ehrmann,
Ali Hurriyetoglu, Mijail Kabadjov, Polina Lenkova,
Ralf Steinberger, Hristo Tanev, Silvia V?azquez, and
Vanni Zavarella. 2012. Creating sentiment dictio-
naries via triangulation. Decision Support Systems,
53(4):689?694.
710
