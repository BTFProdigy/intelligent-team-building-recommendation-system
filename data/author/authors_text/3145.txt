







Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 3?11,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Speech Retrieval in Unknown Languages: a Pilot Study?
Xiaodan Zhuang# Jui Ting Huang# Mark Hasegawa-Johnson
Beckman Institute, Department of Electrical and Computer Engineering
University of Illinois at Urbana-Champaign, U.S.A.
{xzhuang2,jhuang29,jhasegaw}@uiuc.edu
Abstract
Most cross-lingual speech retrieval assumes
intensive knowledge about all involved lan-
guages. However, such resource may not ex-
ist for some less popular languages. Some
applications call for speech retrieval in un-
known languages. In this work, we lever-
age on a quasi-language-independent subword
recognizer trained on multiple languages, to
obtain an abstracted representation of speech
data in an unknown language. Language-
independent query expansion is achieved ei-
ther by allowing a wide lattice output for an
audio query, or by taking advantage of dis-
tinctive features in speech articulation to pro-
pose subwords most similar to the given sub-
words in a query. We propose using a re-
trieval model based on finite state machines
for fuzzy matching of speech sound patterns,
and further for speech retrieval. A pilot study
of speech retrieval in unknown languages is
presented, using English, Spanish and Russian
as training languages, and Croatian as the un-
known target language.
1 Introduction
Dramatic increase in recorded speech media calls for
efficient retrieval of audio files. Accessing speech
media of a foreign language is a particularly impor-
tant and challenging task, often referred to as cross-
lingual speech retrieval or cross-lingual spoken doc-
ument retrieval.
?This research is funded by NSF grants 0534106 and
0703624. The authors would like to thank Su-Youn Yoon for
inspiring discussion. #The student authors contribute equally.
Previous work on cross-lingual speech retrieval
mostly leverages on intensive knowledge about all
the languages involved. Most reported work inves-
tigates retrieval in a target language, in response to
audio or text queries given in a different source lan-
guage (Meng et al, 2000; Virga and Khudanpur,
2003). Usually, the speech media in the target lan-
guage, and the audio queries in the source language,
are converted to speech recognition transcripts us-
ing large-vocabulary automatic speech recognizers
(LVASR) trained for the target language and the
source language respectively. The text queries, or
transcribed audio queries, are translated to the tar-
get language. Text retrieval techniques are applied
to retrieve speech, by retrieving the correspond-
ing LVASR transcription in the target language. In
such systems, a large-vocabulary speech recognizer
trained on the target language is essential, which
requires the existence of a dictionary and labeled
acoustic training data in that language.
LVASR currently do not exist for most of the 6000
languages on Earth. In some situations, knowledge
about the target language is limited, and definitely
not sufficient to enable training LVASR. Imagine
an audio database in a target language unknown to
a user, who needs to retrieve spoken content rel-
evant to some audible query in this unknown lan-
guage. For example, the user knows how the name
?Obama? is pronounced in the target language, and
wants to retrieve all spoken documents that contain
the query word, from a database in this unknown
language. A linguist might find himself/herself in
this scenario when he or she tries to collect a large
number of utterances containing some particular
3
phrases in an unknown language. Similarly, an in-
formation analyst might wish to leverage on speech
retrieval in unknown languages to organize critical
information before engaging linguistic experts for
finer analysis. We refer to such retrieval tasks as
speech retrieval in unknown languages, in which lit-
tle knowledge about the target language is assumed.
A human linguist attempting to manually per-
form speech retrieval in an unknown language
would necessarily map the perceived speech (both
database and query) into some cognitive abstraction
or schema, representing, perhaps, the phonetic dis-
tinctions that he or she has been trained to hear.
Matching and retrieval of speech would then be per-
formed based on such an abstraction. Two cog-
nitive processes, assimilation and accommodation,
take place when human brains are to process new
information (Bernstein et al, 2007), such as speech
in an unknown language. In accommodation, the in-
ternal stored knowledge adapts to new information
with which it is confronted. In assimilation, the new
information, e.g., speech in an unknown language, is
mapped to previously stored information, e.g., sub-
words (phones) as defined by knowledge about the
languages known to the listener.
This paper models speech retrieval in unknown
languages using a machine learning model of pho-
netic assimilation. A quasi-language-independent
subword recognizer is trained to capture salient sub-
words and their acoustic distribution in multiple
languages. This recognizer is applied on an un-
known language, therefore mapping segments of the
unknown speech to subwords in the known lan-
guages. Through this machine cognitive process,
the database and queries in the unknown language
are represented as sequences of quasi-language-
independent subwords. Speech retrieval is per-
formed based on such representation. Figure 1 illus-
trates that speech retrieval in an unknown language
can be modeled as a special case of assimilation.
This task differs from the more widely studied
known-language speech retrieval task, in that no lin-
guistic knowledge of the target language is assumed.
We can only leverage on knowledge that can be
applied by assimilation to the multiple known lan-
guages. Therefore, this task is more like a cross-
lingual sound pattern retrieval task, leveraged on
quasi-language-independent subwords, rather than
Figure 1: Automatic speech retrieval in an unknown lan-
guage (below) is modeled as a special case of the cogni-
tive process called assimilation (above).
a translated spoken word/phrase retrieval task us-
ing target language LVASR transcripts, as in most
cross-lingual speech retrieval systems. The quasi-
language-independent subword recognizer is trained
on speech data other than the target language, and
therefore generates much noisier recognition results,
owing to potential mismatch between acoustic distri-
butions, lack of dictionary and lack of a word-level
language model.
To manage the extra difficulty, we adopt a sub-
word lattice representation to encode a wide hypoth-
esis space of recognized speech in the target lan-
guage. Language-independent query expansion is
achieved either by allowing a wide lattice output
for an audio query, or by taking advantage of dis-
tinctive features in speech articulation to propose
quasi-language-independent subwords most similar
to the given subwords in a query. Finite state ma-
chines (FSM) constructed from the speech lattices
are used to allow for fuzzy matching of speech
sound patterns, and further for retrieval in unknown
languages.
We carry out a pilot study of speech retrieval
in unknown languages, using English, Spanish and
Russian as training languages, and Croatian as the
unknown target language. To explain the effect of
additional knowledge about the target language, we
demonstrate the improvements in retrieval perfor-
mance that result by incrementally making available
subword sequence models and acoustic models for
the target language.
2 Quasi-Language-Independent subword
Models
2.1 Deriving a subword set
Based on the assumption that an audible phrase in an
unknown language can be represented as a sequence
4
of subwords, the question is to find an appropriate
set of subword symbols. Schultz and Waibel (2001)
reported that a global unit set for the source
languages based on International Phonetic Alpha-
bet (IPA) symbols outperforms language-dependent
phonetic units in cross-lingual word recognition
tasks, whereas language-dependent phonetic units
are better models for multilingual word recognition
(in which the target language is also one of the
source languages). A multilingual task might ben-
efit from partitioning the feature space according to
language identity, i.e., to have different subsets of
models aiming at different languages. By contrast,
a cross-lingual task calls for one consistent set of
models with language-independent properties in or-
der to maximize portability into the new language.
To capture the necessary distinctions between dif-
ferent phones across languages, we first pool to-
gether individual phone inventories for source lan-
guages, each of which has its phones tagged with
a language identity, and then performed bottom-up
clustering on the phone pool based on pairwise sim-
ilarity between their acoustic models. Each cluster
represents one distinct language-independent sub-
word symbol. Since this set is still derived from
multiple languages, we refer to these subword units
as quasi-language-independent subwords. A quasi-
language-independent subword set is derived by the
following steps:
First, we encode all speech in the known lan-
guages using a language-dependent phone set. Each
symbol in this set is defined by the phone iden-
tity and the language identity. One single-Gaussian
three-state left-to-right HMM is trained for each of
these subword units.
Second, similarity between the language-
dependent phones is estimated by the approximated
KL divergence between corresponding acoustic
models. As shown in (Vihola et al, 2002), KL
divergence between single-Gaussian left-to-right
HMMs can be approximated in closed form by
Equation 1,
KLD(U, V ) =
S?
i=1
ri
S?
j=1
aUij log
(
aUij/aVij
)(1)
+
S?
i=1
riI
(
bUi : bVi
)
, (2)
where aij is the transition probability to hidden state
j, and bi and ri are the observation distribution
and steady-state probability for hidden state i. For
single-Gaussian distribution, I
(
bUi : bVi
)
can be ap-
proximated by,
I(bUi : bVi ) = 12
[
log
???Vi
??
???Ui
??
+ tr
(
?Ui
((?Vi
)?1 ?
(?Ui
)?1))
+ tr
((?Vi
)?1 (?Ui ? ?Vi
) (
?Ui ? ?Vi
)T) ].
Third, we use the Affinity Propagation algorithm
(Frey and Dueck, 2007) to conduct pairwise cluster-
ing of phones based on the approximated KL diver-
gence between acoustic models. The tendency for a
data point (a phone) to be an exemplar of a cluster
is controlled by the preference value assigned to that
phone. The preference of a phone i is set as follows
to favor frequent phones to be cluster centers:
p(i) = k log(Ci), (3)
where Ci is the count of the phone i, and k is a nor-
malization term to control the total number of clus-
ters. To discourage subwords from the same lan-
guage to join a same cluster, pairwise distance be-
tween them are offset by an additional amount, com-
parable to the maximum pairwise distance between
the models.
The resultant subword set is supposed to cap-
ture quasi-language-independent phonetic informa-
tion, and each subword unit has relatively distinctive
acoustic distribution. These subwords are encoded
using the corresponding cluster exemplars as surro-
gates.
2.2 Recognizing subwords
An automatic speech recognition (ASR) system
(Jelinek, 1998) serves to recognize both queries
and speech database, with acoustic models for the
language-independent subwords derived from the
known languages as described in section 2.1. The
front-end features extracted from the speech data
are 39-dimensional features including 12 Perceptual
Linear Prediction (PLP) coefficients and their en-
ergy, as well as the first-order and second order re-
gression coefficients.
5
We create context-dependent models for each
subword, using the same strategy for build-
ing context-dependent triphone models in LVASR
(Woodland et al, 1994). A ?triphone? is a subword
with its context defined as its immediate preceding
and following subwords. Each triphone is repre-
sented by a continuous three-state left-to-right Hid-
den Markov Model (HMM). Additionally, there is a
one-state HMM for silence, two three-state HMMs
for noise and unknown sound respectively. The
number of Gaussian mixtures (9 to 21 Gaussians) is
optimized according to a development set consisting
of speech in the known languages. A standard tree-
based state tying technique is adopted for parameter
sharing between subwords with similar contexts.
The ?language model? (LM), or more precisely
subword sequence model, should generalize from
the known languages to the unknown language. Our
trial experiments showed that unigram statistics of
subwords and their triphones is more transferable
across languages than N-gram statistics. We also as-
sume that infrequent triphones are less likely to be
salient units that would carry the properties of the
unknown language. Thus, we select the top frequent
triphones and map the rest of the triphones to their
center phones, forming a mixed vocabulary of fre-
quent triphones and context-independent subwords.
The frequencies of these vocabulary entries are used
to estimate an unigram LM in the ASR system. Tri-
phones in the ASR output are mapped back to its
center subwords before the retrieval stage.
3 Speech Retrieval through Subword
Indexing
In many cross-lingual speech retrieval systems, the
speech media are processed by a large-vocabulary
automatic speech recognizer (LVASR), which has
access to vocabulary, dictionary, word language
model and acoustic models for the target lan-
guage. With all these resources, state-of-the-art
speech recognition could give reasonable hypoth-
esized word transcript, enabling direct application
of text retrieval techniques. However, this is not
the case in speech retrieval in unknown languages.
Moreover, without the higher level linguistic knowl-
edge, such as a word dictionary, this task aims to
find speech patterns that sound similar, as approxi-
mated by sequences of quasi-language-independent
subwords. Therefore, the sequential information in
the hypothesized subwords is critical.
To deal with the significant noise in the subword
recognition output, and to emphasize the sequential
information, we use the recognizer to obtain sub-
word lattices instead of one-best hypotheses. These
lattices can be represented as weighted automata,
which are compact representations of a large num-
ber of alternative subword sequences, each asso-
ciated with a weight indicating the uncertainty of
the data. Therefore, indexing speech in unknown
language can be achieved by indexing the corre-
sponding weighted automata with quasi-language-
independent subwords associated with the state tran-
sitions.
We adopt the weighted automata indexation algo-
rithm reported in (Allauzen et al, 2004), which is
optimal for searching subword sequences, as it takes
time linear in the sum of the query size and the num-
ber of speech media entries where it appears. The
automata indexation algorithm also preserves the se-
quential information, which is crutial for this task.
We leverage on two kinds of knowledge for query
expansion, namely empirical phone confusion and
knowledge-based phone confusion. An illustration
of our speech retrieval system is presented in Fig-
ure 2. We detail the indexing approaching as well as
query expansion and retrieval in this section.
Figure 2: Framework of speech retrieval through subword
indexing
6
3.1 Subword Finite State Machines as Speech
Indices
We construct a full index that can be used to search
for a query within all the speech utterances ui, i ?
1, ..., n. In particular, this is achieved by construct-
ing a weighted finite-state transducer T , mapping
each query x to the set of speech utterances where
it appears. Each returned speech utterance u is as-
signed a score, which is the negative log of the ex-
pected count of the query x in utterance u.
The subword lattice for speech utterance ui can be
represented as a weighted finite state automata Ai,
whose path weights correspond to the joint proba-
bility of the observed speech and the hypothesized
subword sequence. To get an automata whose path
weights correspond to desired negative log of poste-
rior probabilities, we simply need to apply a general
weight-pushing algorithm to Ai in the log semiring,
resulting in an automata Bi. In this automata Bi,
the probability of a given string x is the sum of the
probability of all paths that contains x.
The key point of constructing the index transducer
Ti for uttereance ui is to introduce new paths that
enable matching between a query and any portions
of the original paths, while properly normalizing the
path weights. This is achieved by factor selection in
(Allauzen et al, 2004). First, null output is intro-
duced to each transition in the automata, converting
the automata into a transducer. Second, a new tran-
sition is introduced from a new unique initial state to
each existing state, with null input and output. The
weight associated with this transition is the negative
log of the forward probability. Similarly, a new tran-
sition is created from each state to a new unique final
state, with null input and output as the label i of the
current utterance ui. The assicated weight is the neg-
ative log of the backward probability. General finite
state machine optimization operations (Allauzen et
al., 2007) of weighted ?-removal, determinization
and minimization over the log semiring can be ap-
plied to the resulting transducer. As shown in (Al-
lauzen et al, 2004), the path with input of string x
and output of label i has a weight corresponding to
the negative log of the expected count of x in utter-
ance ui.
To optimize the retrieval time, we divide all ut-
terances into a few groups. Within each group, the
utterance index transducers are unioned and deter-
minized to get one single index transducer for the
group. It is then feasible to expedite retrieval by
processing each group index transducer in a paral-
lel fashion.
3.2 Query Expansion
While sequential information is important, ex-
act string match is very unplausible in this chal-
lenging task, even when subword lattices encode
many alternative recognition hypotheses. Language-
independent query expansion is therefore critical for
success in retrieval. We carry out query expansion
either by allowing a wide lattice output for an audio
query, or by taking advantage of distinctive features
in speech articulation to propose quasi-language-
independent subwords most similar to the given sub-
words in a query.
In particular, for a spoken query, ASR will gen-
erate a subword lattice instead of a one-best sub-
word sequence hypothesis. With the lattice, the au-
dio query is encoded by the best hypothesis from
ASR and its empirical phone confusion. The lattice
can then be represented as a finite-state automata.
However, when the query is given as a target
language subword sequence, we can no longer use
the recognizer to obtain an expanded query. Fur-
thermore, some target language subwords may not
even exist in the quasi-language-independent sub-
word set in the recognizer. In this case, knowledge-
based phone confusion is engaged via the use of a
set of distinctive features Fj , j ? 1, ...,M for hu-
man speech (Chomsky and Halle, 1968), including
labial, alveolar, post-alveolar, retroflex, voiced, as-
pirated, front, back, etc.
We estimate similarity from phone a to phone b,
or more precisely, substitution tendency as in Equa-
tion 4,
DFsim(a, b) = log NabNa (4)
where
Nab =
M?
j=1
(F aj ? F bj = 1),
Na =
M?
j=1
(F aj 6= 0).
7
The target subword sequence is first mapped to
the derived subword set, by locating the identical
or nearest member phone in the clustering and then
adopting the surrogate for that cluster. This con-
verted sequence of derived subwords is further ex-
panded by adding the most likely alternative quasi-
language-independent subwords, parallel to each
original subword. Transitions to these alternative
subwords are associated with the corresponding sub-
stitution tendency based on distinctive features.
3.3 Search
An expanded query, either obtained from an audio
query or a subword sequence query, is represented
as a weighted finite state automata. Searching this
query in the utterances is achieved by composing the
query automata with the index transducer. This re-
sults in another finite state transducer, which is fur-
ther processed by projection on output, removal of
? arcs and determinization. The output is a list of
retrieved speech utterances, each with the expected
count of the query.
Apparently, the precision and recall of the re-
trieval results vary with the width of the subword
lattices used for indexing as well as how much the
query is expanded. We control the width of the sub-
word lattices via the number of tokens and the max-
imum probability decrease allowed for each step in
the Viterbi decoding. The extend to which a sub-
word sequence query is expanded is determined by
the lowest allowed similarity between the original
phone and an alternative phone. These parameters
are set empirically.
4 Experiments
4.1 Dataset
The known language pool should cover as many lan-
guage families as possible so that the derived sub-
words could better approximate language indepen-
dence. However, as a pilot study, this paper reports
experiments using only languages within the Indo-
European family. Table 1 summarizes the size of
speech data from each language. Croatian is used
as the unknown target language, and the other three
languages are the known languages used for de-
riving and training the quasi-language-independent
subword models. We extracted 80% of all speakers
per language for training, and 10% as a development
set.
Language ID Hours Spks Style
Croatian hrv 21.3 201 Read+answers
English hub 13.6 406 Broadcast
Spanish spa 14.6 120 Read+answers
Russian rus 2.5 63 Read+answers
Table 1: Summary for data: language ID, total length,
number of speakers and speaking style for each language.
4.2 Settings
The speech retrieval task aims to find speech utter-
ances that contain a particular query. We use two
kinds of queries: 1) subword sequence queries, tran-
scribed as a sequence of phonetic symbols in the tar-
get language; 2) audio queries, each being an audio
segment of the speech query in the target language.
Since we aim to match speech patterns that sound
like each other, the queries used in this experiment
are relatively short, about 3 to 5 syllables. This adds
to the challenge in that very limited redundant in-
formation is available for query-utterance matching.
There are totally 40 subword sequences and 40 audio
queries, each occurs in between 18 and 38 utterances
out of a set of 576 utterances.
In addition to a cross-lingual retrieval system built
using only the known languages, we incrementally
augment resource on the target language to build
more knowledgeable systems.
AM0LM0: Both the acoustic model (AM)
and the language model (LM) are quasi-language-
independent, trained using data in multiple known
languages. This happens when no transcribed
speech data or a defined phone set exist for the tar-
get language. Essentially the system has no direct
knowledge about the target language.
AM0LMt: This setting examines the perfor-
mance gap due to the acoustic model mismatch
by using a quasi-language-independent AM, but a
target language LM. Suppose that a word dictio-
nary with phonetic transcription and possibly some
text data from the target language are available,
for training a target language subword LM. To find
the mapping between target triphones and language-
independent source AMs, linguistic knowledge and
phonetic symbol notation are the only information
8
we can use. First, we map each of target mono-
phones to source phone symbols: Any source cluster
that contains a phonetic symbol with the same nota-
tion as the target phonetic symbol becomes a surro-
gate symbol for that target phone. If a target phone
is unseen to the known languages, the most similar
phone will be chosen first. The similarity is based on
the distinctive features, as discussed in Section 3.2.
Second, the target triphones are converted to possi-
ble source triphones for which acoustic models ex-
ist. Each target triphone not modeled in the source
language AM is replaced with the corresponding di-
phone (subword pair) if it exists, otherwise the cen-
ter phone.
AMtLM0: This setting examines the perfor-
mance gap due to the language model mismatch by
using a quasi-language-independent source LM, but
a target language AM. For the source triphones and
monophones that do not exist in the target AM, they
are mapped to target AMs in a way similar as de-
scribed above.
AMtLMt: Both AM and LM are trained for the
target language. This setting provides an upper
bound of the performance for different settings.
4.3 Metrics
We evaluate the performance for both subword
recognition and speech retrieval, measured as fol-
lows.
Recognition Accuracy: The ground truth is en-
coded using subwords in the target language while
the recognition output is encoded using quasi-
language-independent subwords in Section 2. To
measure the recognition accuracy, we label each
quasi-language-independent subword cluster using
the most frequent target language subword that ap-
pears in that cluster. The hypothesis subword se-
quence is then compared against the groundtruth us-
ing a dynamic-programming-based string alignment
procedure. The recognition accuracy is defined as
REC ? ACC = H?IN ? 100%, where H , I , and
N are the numbers of correct labels, insertion errors
and groundtruth labels respectively.
Retrieval Precision: The retrieval performance
is measured using Mean Average Precision (IR ?
MAP ), defined as the mean of the Average Preci-
sion (AP ) for a set of different queries x. Mean
Average Precision (IR ? MAP ) can be defined in
Equation 5. n is the number of ordered retrieved
utterances and R is the total number of relevant ut-
terances. fi is an indicator function whether the ith
retrieved utterance does contain the query. Precision
pm for top m retrieved utterances can be calculated
as pm = 1m
?m
k=1 f(k).
IR?MAP = 1Q
Q?
x=1
AP (x),
AP (x) = 1R(x)
n(x)?
i=1
fi(x)pi(x). (5)
We use IR ? MAPA and IR ? MAPS to denote
the retrieval MAP for audio queries and subword se-
quence queries respectively.
4.4 Results
Table 2 presents a few examples of the derived
quasi-language-independent subwords. As dis-
cussed in Section 2, these subwords are obtained by
bottom-up clustering of all the language-dependent
IPA phones in the multiple known languages. The
same IPA symbol across languages may lie in the
same cluster, e.g., /z/ in Cluster 1, or different clus-
ters, e.g., /j/ in Cluster 3 and 4. Although symbols
within the same language are discouraged to be in
one cluster, it still desirably happens for highly sim-
ilar pairs, e.g., /1/rus and /j/rus in Cluster 4.
Cluster ID Surrogate Other phone members
1 /z/hub /z/spa, /z/rus, /zj/rus
2 /tSj/rus /tS/hub, /tS/spa
3 /j/hub /j/spa
4 /i:/hub /1/rus, /j/rus
Table 2: Examples of quasi-language-independent sub-
words, as clusters of source language IPAs.
Table 3 compares the subword recognition
and retrieval performance for the quasi-language-
independent subwords and IPA phones. We can
Setting REC ? ACC IR?MAPA IR?MAPS
IPA 37.18% 17.90% 31.40%
AM0LM0 42.52% 23.24% 32.62%
Table 3: Performance of quasi-languange-independent
subword and IPA.
9
Setting AMtLMt AMtLM0 AM0LMt AM0LM0
REC ? ACC 73.45% 67.29% 49.88% 42.52%
IR?MAPA 58.82% 52.38% 28.32% 23.24%
IR?MAPS 76.96% 51.86% 34.95% 32.62%
Table 4: Performance of subword recognition and speech
retrieval.
see that on the unknown language Croatian, the de-
rived quasi-language-independent subwords outper-
form the IPA symbol set in both phone recognition
and retrieval using two kinds of queries.
narrow wide
10
20
30
40
50
60
70
80
Query Expansion
IR
?M
AP
(%
)
 
 
AMtLMt
AMtLM0
AM0LMt
AM0LM0
Figure 3: Speech retrieval performance for subword se-
quence queries
narrow wide
15
20
25
30
35
40
45
50
55
60
Query Expansion
IR
?M
AP
(%
)
 
 
AMtLMt
AMtLM0
AM0LMt
AM0LM0
Figure 4: Speech retrieval performance for audio queries
Table 4 presents the subword recognition accu-
racy and retrieval performance with optimal query
width. Figure 3 and Figure 4 presents speech
retrieval performance at varying query widths for
subword sequence queries and audio queries re-
spectively. It is shown that speech retrieval in
completely unknown language achieves MAP of
23.24% and 32.62% while the system trained using
the most available knowledge about the target lan-
guage reaches MAP of 58.82% and 76.96%, for au-
dio queries and subword sequence queries respec-
tively. We also demonstrate access to phone fre-
quency (AM0LMt) and acoustic data (AMtLM0)
both boosts retrieval performance, and the effect is
roughly additive (AMtLMt).
5 Conclusion and Discussion
In this work, we present a speech retrieval approach
in unknown languages. This approach leverages
on speech recognition based on quasi-language-
independent subword models derived from multi-
ple known languages, and finite state machine based
fuzzy speech pattern matching and retrieval. Our
experiments use Croatian as the unknown language
and English, Russian and Spanish as the known lan-
guages. Results show that the derived subwords out-
perform the IPA symbols, and access to the subword
language model and acoustic models in the unknown
language explains the gap between this challenging
task and retrieval with knowledge about the target
language.
The proposed retrieval approach on unknown lan-
guages can be viewed as a machine learning model
of phonetic assimiliation, in which the segments
in an unknown language are mapped to language-
independent subwords learned from the multiple
known languages. However, another important cog-
nitive process, i.e., accomodation, is not yet mod-
eled. We believe the capability to create new sub-
words unseen in the known languages would lead
to improved performance. In particular, speech seg-
ments that are hypothesized by the quasi-language-
independent subword recognizer with very low con-
fidence scores can be clustered to form these new
subwords, accomodating to the unknown language.
The approach in this work can be readily scaled
up to much larger speech corpora. In particular,
larger corpora would make it more practical to im-
plement the accomodation process discussed above.
Besides, that would also enable online adaptation
of the model parameters of the quasi-language-
independent subword recognizer. Both are believed
to promise reduced gap between retrieval perfor-
mance in a known language and an unknown lan-
guage, and are potential future work beyond this pa-
per.
10
References
C. Allauzen, M. Mohri, and M. Saraclar. 2004. Gen-
eral indexation of weighted automata ? application to
spoken utterance retrieval. In Proc. HLT-NAACL.
C. Allauzen, M. Riley, J. Schalkwyk, W. Skut, and
M.Mohri. 2007. Openfst: A general and effi-
cient weighted finite-state transducer library. In Proc.
CIAA.
Bernstein, Penner, Clarke-Stewart, and Roy. 2007. Psy-
chology. Houghton Mifflin Company.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. New York: Harper and Row.
Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:972?976.
Frederick Jelinek. 1998. Statistical Methods for Speech
Recognition. The MIT Press.
Helen Meng, Berlin Chen, Erika Grams, Sanjeev Khu-
danpur, Wai-Kit Lo, Gina-Anne Levow, Douglas Oard,
Patrick Schone, Karen Tang, Hsin-Min Wang, and
Jian Qiang Wang. 2000. Mandarin-english informa-
tion (MEI): Investigating translingual speech retrieval.
http://www.clsp.jhu.edu/ws2000/final reports/mei/ws00mei.pdf.
Tanja Schultz and Alex Waibel. 2001. Language inde-
pendent and language adaptive acoustic modeling for
speech recognition. Speech Communication, 35:31?
51.
M. Vihola, M. Harju, P. Salmela, J. Suontausta, and
J. Savela. 2002. Two dissimilarity measures for hmms
and their application in phoneme model clustering. In
Proc. ICASSP, volume 1, pages I?933 ? I?936.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of proper names in crosslingual information re-
trieval. In Proc. ACL 2003 workshop MLNER.
P.C. Woodland, J.J. Odell, V. Valtchev, and S.J. Young.
1994. Large vocabulary continuous speech recogni-
tion using HTK. In Proc. ICASSP, volume 2, pages
II/125?II/128.
11
Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 75?83,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
On Semi-Supervised Learning of Gaussian Mixture Models
for Phonetic Classification?
Jui-Ting Huang and Mark Hasegawa-Johnson
Department of Electrical and Computer Engineering
University of Illinois at Urbana-Champaign
Illinois, IL 61801, USA
{jhuang29,jhasegaw}@illinois.edu
Abstract
This paper investigates semi-supervised learn-
ing of Gaussian mixture models using an uni-
fied objective function taking both labeled and
unlabeled data into account. Two methods
are compared in this work ? the hybrid dis-
criminative/generative method and the purely
generative method. They differ in the crite-
rion type on labeled data; the hybrid method
uses the class posterior probabilities and the
purely generative method uses the data like-
lihood. We conducted experiments on the
TIMIT database and a standard synthetic data
set from UCI Machine Learning repository.
The results show that the two methods be-
have similarly in various conditions. For both
methods, unlabeled data improve training on
models of higher complexity in which the su-
pervised method performs poorly. In addition,
there is a trend that more unlabeled data re-
sults in more improvement in classification ac-
curacy over the supervised model. We also
provided experimental observations on the rel-
ative weights of labeled and unlabeled parts
of the training objective and suggested a criti-
cal value which could be useful for selecting a
good weighing factor.
1 Introduction
Speech recognition acoustic models can be trained
using untranscribed speech data (Wessel and Ney,
2005; Lamel et al, 2002; L. Wang and Woodland,
2007). Most such experiments begin by boostraping
?This research is funded by NSF grants 0534106 and
0703624.
an initial acoustic model using a limited amount of
manually transcribed data (normally in a scale from
30 minutes to several hours), and then the initial
model is used to transcribe a relatively large amount
of untranscribed data. Only the transcriptions with
high confidence measures (Wessel and Ney, 2005;
L. Wang and Woodland, 2007) or high agreement
with closed captions (Lamel et al, 2002) are se-
lected to augment the manually transcribed data, and
new acoustic models are trained on the augmented
data set.
The general procedure described above exactly
lies in the context of semi-supervised learning prob-
lems and can be categorized as a self-training algo-
rithm. Self-training is probably the simplest semi-
supervised learning method, but it is also flexible
to be applied to complex classifiers such as speech
recognition systems. This may be the reason why
little work has been done on exploiting other semi-
supervised learning methods in speech recognition.
Though not incorporated to speech recognizers yet,
there has been some work on semi-supervised learn-
ing of Hidden Markov Models (HMM) for sequen-
tial classification. Inoue and Ueda (2003) treated the
unknown class labels of the unlabeled data as hidden
variables and used the expectation-maximization
(EM) algorithm to optimize the joint likelihood of
labeled and unlabeled data. Recently Ji et al (2009)
applied a homotopy method to select the optimal
weight to balance between the log likelihood of la-
beled and unlabeled data when training HMMs.
Besides generative training of acoustic models,
discriminative training is another popular paradigm
in the area of speech recognition, but only when
75
the transcriptions are available. Wang and Wood-
land (2007) used the self-training method to aug-
ment the training set for discriminative training.
Huang and Hasegawa-Johnson (2008) investigated
another use of discriminative information from la-
beled data by replacing the likelihood of labeled data
with the class posterior probability of labeled data in
the semi-supervised training objective for Gaussian
Mixture Models (GMM), resulting in a hybrid dis-
criminative/generative objective function. Their ex-
perimental results in binary phonetic classification
showed significant improvement in classification ac-
curacy when labeled data are scarce. A similar strat-
egy called ??multi-conditional learning?? was pre-
sented in (Druck et al, 2007) applied to Markov
Random Field models for text classification tasks,
with the difference that the likelihood of labeled data
is also included in the objective. The hybrid dis-
criminative/generative objective function can be in-
terpreted as having an extra regularization term, the
likelihood of unlabeled data, in the discriminative
training criterion for labeled data. However, both
methods in (Huang and Hasegawa-Johnson, 2008)
and (Druck et al, 2007) encountered the same issue
about determining the weights for labeled and un-
labeled part in the objective function and chose to
use a development set to select the optimal weight.
This paper provides an experimental analysis on the
effect of the weight.
With the ultimate goal of applying semi-
supervised learning in speech recognition, this pa-
per investigates the learning capability of algorithms
within Gaussian Mixture Models because GMM is
the basic model inside a HMM, therefore 1) the up-
date equations derived for the parameters of GMM
can be conveniently extended to HMM for speech
recognition. 2) GMM can serve as an initial point
to help us understand more details about the semi-
supervised learning process of spectral features.
This paper makes the following contribution:
? it provides an experimental comparison of hy-
brid and purely generative training objectives.
? it studies the impact of model complexity on
learning capability of algorithms.
? it studies the impact of the amount of unlabeled
data on learning capability of algorithms.
? it analyzes the role of the relative weights of
labeled and unlabeled parts of the training ob-
jective.
2 Algorithm
Suppose a labeled set XL = (x1, . . . , xn, . . . , xNL)
has NL data points and xn ? Rd. YL =
(y1, . . . , yn, . . . , yNL) are the corresponding class
labels, where yn ? {1, 2, . . . , Y } and Y is the num-
ber of classes. In addition, we also have an unla-
beled set XU = (x1, . . . , xn, . . . , xNU ) without cor-
responding class labels. Each class is assigned a
Gaussian Mixture model, and all models are trained
given XL and XU . This section first presents the
hybrid discriminative/generative objective function
for training and then the purely generative objective
function. The parameter update equations are also
derived here.
2.1 Hybrid Objective Function
The hybrid discriminative/generative objective func-
tion combines the discriminative criterion for la-
beled data and the generative criterion for unlabeled
data:
F (?) = logP (YL|XL;?) + ? logP (XU ;?), (1)
and we chose the parameters so that (1) is maxi-
mized:
?? = argmax
?
F (?) . (2)
The first component considers the log posterior
class probability of the labeled set whereas the sec-
ond component considers the log likelihood of the
unlabeled set weighted by ?. In ASR community,
model training based the first component is usually
referred to as Maximum Mutual Information Esti-
mation (MMIE) and the second component Maxi-
mum Likelihood Estimation (MLE), therefore in this
paper we use a brief notation for (1) just for conve-
nience:
F (?) = F (DL)MMI (?) + ?F (DU )ML (?) . (3)
The two components are different in scale. First,
the size of the labeled set is usually smaller than
the size of the unlabeled set in the scenario of semi-
supervised learning, so the sums over the data sets
involve different numbers of terms; Second, the
76
scales of the posterior probability and the likeli-
hood are essentially different, so are their gradients.
While the weight ? balances the impacts of two
components on the training process, it may also im-
plicitly normalize the scales of the two components.
In section (3.2) we will discuss and provide a further
experimental analysis.
In this paper, the models to be trained are Gaus-
sian mixture models of continuous spectral feature
vectors for phonetic classes, which can be further
extended to Hidden Markov Models with extra pa-
rameters such as transition probabilities.
The maximization of (1) follows the techniques
in (Povey, 2003), which uses auxiliary functions for
objective maximization; In each iteration, a strong
or weak sense auxiliary function is maximized, such
that if the auxiliary function converges after itera-
tions, the objective function will be at a local maxi-
mum as well.
The objective function (1) can be rewritten as
F (?) = logP (XL|YL;?)? logP (XL;?)
+ ? logP (XU ;?), (4)
where the term logP (YL;?) is removed because it
is independent of acoustic model parameters.
The auxiliary function at the current parameter
?old for (4) is
G(?, ?(old)) =Gnum(?, ?(old))? Gden(?, ?(old))
+?Gden(?, ?(old);DU ) + Gsm(?, ?(old)),
(5)
where the first three terms are strong-sense auxiliary
functions for the conditional likelihood (referred to
as the numerator(num) model because it appears in
the numerator when computing the class posterior
probability) logP (XL|YL;?) and the marginal like-
lihoods (referred to as the denominator(den) model
likewise) logP (XL;?) and ? log P (XU ;?) respec-
tively. The last term is a smoothing function that
doesn?t affect the local differential but ensures that
the sum of the first three term is at least a convex
weak-sense auxiliary function for good convergence
in optimization.
Maximization of (5) leads to the update equations
for the class j and mixture m given as follows:
??jm = 1?jm
(
xnumjm, ? xdenjm + ?xdenjm(DU ) + Djm?jm
)
(6)
??2jm =
1
?jm
(
snumjm ? sdenjm + ?sdenjm(DU )
+Djm
(
?2jm + ?2jm
))
? ??2jm,
(7)
where for clarity the following substitution is used:
?jm = ?numjm ? ?denjm + ??denjm(DU ) + Djm (8)
and ?jm is the sum of the posterior probabilities of
occupation of mixture component m of class j over
the dataset:
?numjm (X) =
?
xi?X,yi=j
p (m|xi, yi = j)
?denjm(X) =
?
xi?X
p (m|xi)
(9)
and xjm and sjm are respectively the weighted
sum of xi and x2i over the whole dataset with the
weight p (m|xi, yi = j) or p (m|xi), depending on
whether the superscript is the numerator or denomi-
nator model. Djm is a constant set to be the greater
of twice the smallest value that guarantees positive
variances or ?denjm (Povey, 2003). The re-estimation
formula for mixture weights is also derived from the
Extended Baum-Welch algorithm:
c?jm =
cjm
{
?F
?cjm + C
}
?
m? cjm?
{
?F
?cjm + C
} , (10)
where the derivative was approximated (Merialdo,
1988) in the following form for practical robustness
for small-valued parameters :
?FMMI
?cjm
?
?numjm?
m? ?numjm?
?
?denjm?
m? ?denjm?
. (11)
Under our hybrid framework, there is an extra term
?denjm(DU )/
?
m? ?denjm?(DU ) that should exist in (11),
but in practice we found that adding this term to the
approximation is not better than the original form.
Therefore, we keep using MMI-only update for mix-
ture weights. The constant C is chosen such that all
parameter derivatives are positive.
77
2.2 Purely Generative Objective
In this paper we compare the hybrid objective with
the purely generative one:
F (?) = logP (XL|YL;?) + ? logP (XU ;?),
(12)
where the two components are total log likelihood of
labeled and unlabeled data respectively. (12) doesn?t
suffer from the problem of combining two heteroge-
neous probabilistic items, and the weight ? being
equal to one means that the objective is a joint data
likelihood of labeled and unlabeled set with the as-
sumption that the two sets are independent. How-
ever, DL or DU might just be a sampled set of the
population and might not reflect the true proportion,
so we keep ? to allow a flexible combination of two
criteria. On top of that, we need to adjust the relative
weights of the two components in practical experi-
ments.
The parameter update equation is a reduced form
of the equations in Section (2.1):
??jm =
xnumjm, + ?xdenjm(DU )
?numjm + ??denjm(DU )
(13)
??2jm =
snumjm + ?sdenjm(DU )
?numjm + ??denjm(DU )
? ??2jm (14)
3 Results and Discussion
The purpose of designing the learning algorithms
is for classification/recognition of speech sounds,
so we conducted phonetic classification experiments
using the TIMIT database (Garofolo et al, 1993).
We would like to investigate the relation of learning
capability of semi-supervised algorithms to other
factors and generalize our observations to other data
sets. Therefore, we used another synthetic dataset
Waveform for the evaluation of semi-supervised
learning algorithms for Gaussian Mixture model.
TIMIT: We used the same 48 phone classes and
further grouped into 39 classes according to (Lee
and Hon, 1989) as our final set of phone classes to
model. We extracted 50 speakers out of the NIST
complete test set to form the development set. All
of our experimental analyses were on the develop-
ment set. We used segmental features (Halberstadt,
1998) in the phonetic classification task. For each
phone occurrence, a fixed-length vector was calcu-
lated from the frame-based spectral features (12 PLP
coefficients plus energy) with a 5 ms frame rate and
a 25 ms Hamming window. More specifically, we
divided the frames for each phone into three regions
with 3-4-3 proportion and calculated the PLP av-
erage over each region. Three averages plus the
log duration of that phone gave a 40-dimensional
(13? 3 + 1) measurement vector.
Waveform: We used the second versions of
the Waveform dataset available at the UCI reposi-
tory (Asuncion and Newman, 2007). There are three
classes of data. Each token is described by 40 real
attributes, and the class distribution is even.
Forwaveform, because the class labels are equally
distributed, we simply assigned equal number of
mixtures for each class. For TIMIT, the phone
classes are unevenly distributed, so we assigned
variable number of Gaussian mixtures for each class
by controlling the averaged data counts per mixture.
For all experiments, the initial model is an MLE
model trained with labeled data only.
To construct a mixed labeled/unlabeled data set,
the original training set were randomly divided into
the labeled and unlabeled sets with desired ratio, and
the class labels in the unlabeled set are assumed to be
unknown. To avoid that the classifier performance
may vary with particular portions of data, we ran five
folds for every experiment, each fold corresponding
to different division of training data into labeled and
unlabeled set, and took the averaged performance.
3.1 Model Complexity
This section analyzes the learning capability of
semi-supervised learning algorithms for different
model complexities, that is, the number of mix-
tures for Gaussian mixture model. In this experi-
ment, the sizes of labeled and unlabeled set are fixed
(|DL| : |DU | = 1 : 10 and the averaged token
counts per class is around 140 for both data sets),
as we varied the total number of mixtures and eval-
uated the updated model by its classification accu-
racy. For waveform, number of mixtures was set
from 2 to 7; for TIMIT, because the number of mix-
tures per class is determined by the averaged data
counts per mixture c, we set c to 25, 20 and 15 as
the higher c gives less number of mixtures in total.
Figure 3.1 plots the averaged classification accura-
78
Figure 1: Mean classification accuracies vs. ? for different model complexity. The accuracies for the initial MLE
models are indicated in the parentheses. (a) waveform: training with the hybrid objective. (b) waveform: purely
generative objective. (c) TIMIT: training with the hybrid objective. (d) TIMIT: purely generative objective.
 !  !" #! #!" $! 
%&
' 
'#
'$
'(
')
'"
 !"
 
!
"
"
#
$
%
"
&
 
'
(
)
#$#%#&# '()*&+"
#$#%#(# '&)*'+"
#$#%#,# '-)./+"
#$#%#.# '*)-'+"
#$#%#/# 01)/-+"
 2"
 !   ! "  !#  !#"  !$ 
"(
")
""
"*
"%  3"
#
 
!
"
"
#
$
%
"
&
 
'
(
)
#3#%#&.# ..)(,+"
#3#%#&*# ..)(/+"
#3##%#-.# .,)0&+"
 !  !" #! #!" $! 
%&
' 
'#
'$
'(
')
'"
 4"
#
!
"
"
#
$
%
"
&
 
'
(
)
 
#
#$#%#&# '()*&+"
#$#%#(# '&)*'+"
#$#%#,# '-)./+"
#$#%#.# '*)-'+"
#$#%#/# 01)/-+"
 !   ! "  !#  !#"  !$ 
"(
")
""
"*
"%
#
 
#
!
"
"
#
$
%
"
&
 
'
(
)
#3#%#&.# ..)(,+"
#3#%#&*# ..)(/+"
#3#%#-.# .,)0&+"
cies of the updated model versus the value of ? with
different model complexities. The ranges of ? are
different for waveform and TIMIT because the value
of ? for each dataset has different scales.
First of all, the hybrid method and purely gen-
erative method have very similar behaviors in both
waveform and TIMIT; the differences between the
two methods are insignificant regardless of ?. The
hybrid method with ? = 0 means supervised MMI-
training with labeled data only, and the purely gener-
ative method with ? = 0means extra several rounds
of supervised MLE-training if the convergence cri-
terion is not achieved. With the small amount of la-
beled data, most of hybrid curves start slightly lower
than the purely generative ones at ? = 0, but in-
crease to as high as the purely generative ones as ?
increases.
For waveform, the accuracies increase with ? in-
creases for all cases except for the 2-mixture model.
Table 1 summarizes the numbers from Figure 3.1.
Except for the 2-mixture case, the improvement over
the supervised model (? = 0) is positively corre-
lated to the model complexity, as the largest im-
provements occur at the 5-mixture and 6-mixture
model for the hybrid and purely generative method
respectively. However, the highest complexity does
not necessarily gives the best classification accu-
racy; the 3-mixture model achieves the best accu-
racy among all models after semi-supervised learn-
ing whereas the 2-mixture model is the best model
for supervised learning using labeled data only.
Experiments on TIMIT show a similar behavior1 ;
as shown in both Figure 3.1 and Table 2, the im-
provement over the supervised model (? = 0) is
also positively correlated to the model complexity,
1Note that our baseline performance (the initial MLEmodel)
is much worse than benchmark because only 10% of the train-
ing data were used. We justified our baseline model by using
the whole training data and a similar accuracy ( 74%) to other
work (e.g. (Sha and Saul, 2007)) was obtained.
79
Table 1: The accuracies(%) of the initial MLEmodel, the supervised model (? = 0), the best accuracies with unlabeled
data and the absolute improvements (?) over ? = 0 for different model complexities for waveform. The bolded
number is the highest value along the same column.
Hybrid Purely generative
#. mix init. acc. ? = 0 best acc. ? ? = 0 best acc. ?
2 83.02 81.73 83.74 2.01 82.96 83.14 0.18
3 82.08 81.66 84.69 3.03 82.18 84.58 2.40
4 81.56 80.53 83.93 3.40 81.34 84.13 2.79
5 80.18 80.14 83.82 3.68 80.16 83.84 3.68
6 79.61 79.40 83.19 3.79 79.71 83.31 3.60
Table 2: The accuracies(%) of the initial MLEmodel, the supervised model (? = 0), the best accuracies with unlabeled
data and the absolute improvements (?) over ? = 0 for different model complexities for TIMIT. The bolded number
is the highest value along the same column.
Hybrid Purely generative
c init. acc. ? = 0 best acc. ? ? = 0 best acc. ?
25 55.34 55.47 56.58 1.11 55.32 56.7 1.38
20 55.36 55.67 56.72 1.05 55.2 56.25 1.05
15 54.72 53.71 55.39 1.68 53.7 56.09 2.39
as the most improvements occur at c = 25 for both
hybrid and purely generative methods. The semi-
supervised model consistently improves over the su-
pervised model. To summarize, unlabeled data im-
prove training on models of higher complexity, and
sometimes it helps achieve the best performance
with a more complex model.
3.2 Size of Unlabeled Data
In Figure 2, we fixed the size of the labeled set (4%
of the training set) and plotted the averaged classi-
fication accuracies for learning with different sizes
of unlabeled data. First of all, the hybrid method
and purely generative method still behave similarly
in both waveform and TIMIT. For both datasets, the
figures clearly illustrate that more unlabeled data
contributes more improvement over the supervised
model regardless of the value of ?. Generally, a data
distribution can be expected more precisely with a
larger sample size from the data pool, therefore we
expect the more unlabeled data the more precise in-
formation about the population, which improves the
learning capability.
3.3 Discussion of ?
During training, the weighted sum ofFMMI andFML
in equation (15) increases with iterations, however
FMMI and FML are not guaranteed to increase indi-
vidually. Figure 3 illustrates how ? affects the re-
spective change of the two components for a partic-
ular setting for waveform. When ? = 0, the ob-
jective function does not take unlabeled data into
account, so FMMI increases while FML decreases.
FML starts to increase for nonzero ?; ? = 0.01
corresponds to the case where both objectives in-
creases. As ? keeps growing, FMMI starts to de-
crease whereas FML keeps rising. In this partic-
ular example, ? = 0.05 is the critical value at
which FMMI changes from increasing to decreas-
ing. According to our observation, the value of ?
depends on the dataset and the relative size of la-
beled/unlabeled data. Table 3 shows the critical val-
ues for waveform and TIMIT for different sizes of
labeled data (5, 10, 15, 20% of the training set) with
a fixed set of unlabeled data (80%.) The numbers are
very different across the datasets, but there is a con-
sistent pattern within the dataset?the critical value
increases as the size of labeled set increases. One
possible explanation is that ? contains an normal-
80
Figure 2: Mean classification accuracies vs. ? for different amounts of unlabeled data (the percentage in the training
set). The averaged accuracy for the initial MLE model is 81.66% for waveform and 59.41% for TIMIT. (a) waveform:
training with the hybrid objective. (b) waveform: purely generative objective. (c) TIMIT: training with the hybrid
objective. (d) TIMIT: purely generative objective.
 !  !" #! #!" $! 
%#
%$
%&
%'
 
 
!
"
"
#
$
%
"
&
 
'
(
)
 ! " #$%
 ! " &$%
 ! " '$%
 ! " ($%
 !   ! "  !# 
"(!'
"(!)
"(!%
) ! 
) !$
 
 
!
"
"
#
$
%
"
&
 
'
(
)
 ! " )$%
 ! " *$%
 ! " ($%
 !  !" #! #!" $! 
%$
%&
%'
!
"
"
#
$
%
"
&
 
'
(
)
 
 
 
 ! " #$%
 ! " &$%
 ! " '$%
 ! " ($%
 !   ! "  !# 
"(!'
"(!)
"(!%
) ! 
 
 
 
!
"
"
#
$
%
"
&
 
'
(
)
 ! " )$%
 ! " *$%
 ! " ($%
+,-
+.-
+/-
+0-
ization factor with respect to the relative size of la-
beled/unlabeled set. The objective function in (15)
can be rewritten in terms of the normalized objective
with respect to the data size:
F (?) = |DL|F (DL)MMI (?)+?|DU |F (DU )ML (?) . (15)
where F (X) means the averaged value over the data
set X. When the labeled set size increases, ? may
have to scale up accordingly such that the relative
change of the two averaged component remains in
the same scale.
Although ? controls the dominance of the crite-
rion on labeled data or on unlabeled data, the fact
that which dominates the objective or the critical
value does not necessary indicate the best ?. How-
ever, we observed that the best ? is usually close to
or larger than the critical value, but the exact value
varies with different data. At this point, it might still
be easier to find the best weight using a small de-
velopment set. But this observation also provides a
guide about the reasonable range to search the best
? ? searching starting from the critical value and it
should reach the optimal value soon according to the
plots in Figure 3.1.
Table 3: The critical values for waveform and TIMIT
for different sizes of labeled data (percentage of training
data) with a fixed set of unlabeled data (80 %.)
Size of labeled data waveform TIMIT
5% 0.09-0.11 0.03-0.04
10% 0.12-0.14 0.07-0.08
15% 0.5-0.6 0.08-0.09
20% 1-1.5 0.11-0.12
81
Figure 3: Accuracy (left), FMMI (center), and FML (right) at different values of alpha.
 ! " # $ % & '
 (''
 (')
 ('*
 () 
 ()!
 ()"
 ()#
 ()$
 
!
"
"
#
$
%
"
&
 
'
(
)
 !"#$!%&'()*+,"#
( (-(.
( (-(./.0
( (-(./.1
( (-(./1
 ! " # $ % & '
+# 
+" 
+! 
 
 
*
+
,
,
-
 !"#$!%&'()*+,"#
( (-(.
( (-(./.0
( (-(./.1
( (-(./1
 ! " # $ % & '
+!*)
+!*&
+!*$
+!*"
+!* 
+!))
 
*
+
,
.
 
/
 
0
1
1
1
 !"#$!%&'()*+,"#
( (-(.
( (-(./.0
( (-(./.1
( (-(./1
3.4 Hybrid Criterion vs. Purely Generative
Criterion
From the previous experiments, we found that the
hybrid criterion and purely generative criterion al-
most match each other in performance and are able
to learn models of the same complexity. This implies
that the criterion on labeled data has less impact on
the overall training direction than unlabeled data. In
Section 3.2, we mentioned that the best ? is usually
larger than or close to the critical value around which
the unlabeled data likelihood tends to dominate the
training objective. This again suggests that labeled
data contribute less to the training objective function
compared to unlabeled data, and the criterion on la-
beled data doesn?t matter as much as the criterion on
unlabeled data. It is possible that most of the con-
tributions from labeled data have already been used
for training an initial MLE model, therefore little in-
formation could be extracted in the further training
process.
4 Conclusion
Regardless of the dataset and the training objective
type on labeled data, there are some general prop-
erties about the semi-supervised learning algorithms
studied in this work. First, while limited amount of
labeled data can at most train models of lower com-
plexity well, the addition of unlabeled data makes
the updated models of higher complexity much im-
proved and sometimes perform better than less com-
plex models. Second, the amount of unlabeled data
in our semi-supervised framework generally follows
?the-more-the-better? principle; there is a trend that
more unlabeled data results in more improvement in
classification accuracy over the supervised model.
We also found that the objective type on labeled
data has little impact on the updated model, in the
sense that hybrid and purely generative objectives
behave similarly in learning capability. The obser-
vation that the best ? occurs after the MMI criterion
begins to decrease supports the fact that the criterion
on labeled data contributes less than the criterion on
unlabeled data. This observation is also helpful in
determining the search range for the best ? on the
development set by locating the critical value of the
objective as a start point to perform search.
The unified training objective method has a nice
convergence property which self-training methods
can not guarantee. The next step is to extend the
similar framework to speech recognition task where
HMMs are trained and phone boundaries are seg-
mented. It would be interesting to compare it with
self-training methods in different aspects (e.g. per-
formance, reliability, stability and computational ef-
ficiency).
82
References
A. Asuncion and D.J. Newman. 2007. UCI machine
learning repository.
Gregory Druck, Chris Pal, AndrewMcCallum, and Xiao-
jin Zhu. 2007. Semi-supervised classification with hy-
brid generative/discriminative methods. In KDD ?07:
Proceedings of the 13th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 280?289, New York, NY, USA. ACM.
J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus,
D. S. Pallett, and N. L. Dahlgren. 1993. Darpa timit
acoustic phonetic continuous speech corpus.
Andrew K. Halberstadt. 1998. Heterogeneous Acous-
tic Measurements and Multiple Classifiers for Speech
Recognition. Ph.D. thesis, Massachusetts Institute of
Technology.
J.-T. Huang and Mark Hasegawa-Johnson. 2008. Max-
imum mutual information estimation with unlabeled
data for phonetic classification. In Interspeech.
Masashi Inoue and Naonori Ueda. 2003. Exploitation of
unlabeled sequences in hidden markov models. IEEE
Trans. On Pattern Analysis and Machine Intelligence,
25:1570?1581.
Shihao Ji, Layne T. Watson, and Lawrence Carin. 2009.
Semisupervised learning of hidden markov models via
a homotopymethod. IEEE Trans. Pattern Anal. Mach.
Intell., 31(2):275?287.
M.J.F. Gales L. Wang and P.C. Woodland. 2007. Un-
supervised training for mandarin broadcast news and
conversation transcription. In Proc. IEEE Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), volume 4, pages 353?356.
Lori Lamel, Jean-Luc Gauvain, and Gilles Adda. 2002.
Lightly supervised and unsupervised acoustic model
training. 16:115?129.
K.-F. Lee and H.-W. Hon. 1989. Speaker-independent
phone recognition using hidden markov models.
IEEE Transactions on Speech and Audio Processing,
37(11):1641?1648.
B. Merialdo. 1988. Phonetic recognition using hid-
den markov models and maximum mutualinformation
training. In Proc. IEEE Conference on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1,
pages 111?114.
Daniel Povey. 2003. Discriminative Training for Large
Vocabulary Speech Recognition. Ph.D. thesis, Cam-
bridge University.
Fei Sha and Lawrence K. Saul. 2007. Large margin hid-
den markov models for automatic speech recognition.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 1249?1256. MIT Press, Cambridge, MA.
Frank Wessel and Hermann Ney. 2005. Unsupervised
training of acoustic models for large vocabulary con-
tinuous speech recognition. IEEE Transactions on
Speech and Audio Processing, 13(1):23?31, January.
83
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 130?140, Dublin, Ireland, August 23-29 2014.
A PAC-Bayesian Approach to Minimum Perplexity Language Modeling
Sujeeth Bharadwaj
University of Illinois
405 N. Mathews Ave.
Urbana, IL 61801, USA
sbhara3@illinois.edu
Mark Hasegawa-Johnson
University of Illinois
405 N. Mathews Ave.
Urbana, IL 61801, USA
jhasegaw@illinois.edu
Abstract
Despite the overwhelming use of statistical language models in speech recognition, machine
translation, and several other domains, few high probability guarantees exist on their generaliza-
tion error. In this paper, we bound the test set perplexity of two popular language models ? the
n-gram model and class-based n-grams ? using PAC-Bayesian theorems for unsupervised learn-
ing. We extend the bound to sequence clustering, wherein classes represent longer context such
as phrases. The new bound is dominated by the maximum number of sequences represented by
each cluster, which is polynomial in the vocabulary size. We show that we can still encourage
small sample generalization by sparsifying the cluster assignment probabilities. We incorporate
our bound into an efficient HMM-based sequence clustering algorithm and validate the theory
with empirical results on the resource management corpus.
1 Introduction
The ability to predict unseen events from a few training examples is the holy grail of statistical language
modeling (SLM). Although the final test for any language model is its contribution to the performance of
a real system, task-independent metrics such as perplexity are popular for evaluating the general quality
of a model. Standard algorithms therefore attempt to minimize perplexity on some previously unobserved
test set, assumed to be drawn from the same distribution as the training set. This begets the question of
how the test set perplexity is related to training set perplexity ? every paper on SLM has an answer, with
varying levels of theoretical and empirical justification.
The problem of data sparsity and generalization can be traced back to at least as early as Good (1953),
and possibly Laplace, who recognizes that the maximum likelihood (ML) estimate of event frequencies
(n-grams) cannot handle unseen events. Smoothing techniques such as the add-one estimator (Lidstone,
1920) and the Good-Turing estimator (Good, 1953) assign a non-zero probability to events that have
never been observed in the training set. Recently, Ohannessian and Dahleh (2012) strengthened the
theory by showing that Good-Turing estimation is consistent when the data generating process is heavy-
tailed. In the context of this paper, smoothing was perhaps the first attempt to bound generalization error,
in that it successfully guarantees a finite test set perplexity.
It is evident that smoothing of the n-gram estimate alone is not sufficient. Techniques that incorporate
lower and higher order n-grams, such as Katz (1987) smoothing, Jelinek-Mercer (1980) interpolation,
and Kneser-Ney (1995) smoothing, have become standard (Rosenfeld, 2000). Chen and Goodman (1999)
provide a thorough empirical comparison of smoothing methods and uncover useful relationships be-
tween the test set cross-entropy (log perplexity) and the size of the training set, model order, etc. A
Bayesian interpretation further explains why some of the techniques (don?t) work. Teh (2006) discusses
fundamental limitations of the Dirichlet process (Mackay and Peto, 1995) and proposes the hierarchi-
cal Pitman-Yor language model as a better way of generating the heavy-tailed (power law) distributions
exhibited in natural language.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
130
Instead of directly modeling a heavy-tailed distribution over words, class-based models address data
sparsity by estimating n-grams over clusters of words. Intuitively, clustering is a transformation of the
event space from the space of word n-grams, in which most events are rare, to the space of class n-grams,
which is more densely measured and therefore requires fewer training examples. Brown et al. (1992)
show that the clustering function that maximizes the training data likelihood must also maximize mu-
tual information between adjacent clusters; although several useful clustering algorithms are based on
this principle, no provable guarantees currently exist. Moreover, word transitions are never completely
captured by the underlying class transitions, and some tradeoff between accurate estimation of frequent
events (word n-grams) and generalization to unseen events (class n-grams) is desired ? class-based mod-
els are therefore often interpolated with word n-grams using some of the previously described Bayesian
methods (Rosenfeld, 2000).
Our survey of SLM techniques and their treatment of generalization error has been rather brief and
certainly not comprehensive. We focus primarily on n-grams and related models since they have domi-
nated SLM over the last several decades (Rosenfeld, 2000), and therefore serve as a good starting point
for further analysis. The existing literature suggests that apart from empirical validation and intuition,
no provable guarantees exist on the generalization error of language models. Bayesian techniques work
well only to the extent the prior assumptions are valid; in this paper, we present theoretical guarantees
that hold irrespective of the correctness of the prior.
Model selection approaches such as the Akaike Information Criterion (AIC) (Akaike, 1973) and its
variants (Burnham and Anderson, 2002) quantify the tradeoff between complexity and goodness of fit. In
the context of a language model, it can be shown that test set cross entropy is approximately the training
set cross entropy plus the number of model parameters. Unfortunately, such bounds are loose and do
not provide significant algorithmic insight ? at best, they recommend the smallest model that works well
on the training set. Chen (2009) obtained a very accurate relationship for exponential language models
by estimating the test set performance with linear regression. Although empirical, his approximation
leads to better models based on l
1
+ l
2
2
regularization. Exponential models are often motivated with
the minimum discrimination information (MDI) principle, which roughly states that of all distributions
satisfying a particular set of features, the exponential family is the centroid (minimizes distortion relative
to the farthest possible true distribution) (Rosenfeld, 1996). This does not bound the generalization error
in the manner we wish to, but it is nevertheless a useful property that complements Chen?s observations.
In this paper, we strive for the best of both worlds ? we present PAC-Bayesian theory as a powerful tool
for deriving high probability guarantees as well as efficient and well-motivated algorithms. In the next
section, we state some useful PAC-Bayesian theorems. In Section 3, we present our main results. We
apply the PAC-Bayesian bounds to n-grams, class-based n-grams, and also sequence clustering, where
classes represent longer context such as phrases. We show that for sequence clustering, the bound is
dominated by the maximum number of sequences represented by each cluster, and consequently requires
many more training examples than a class-based model over words. We address this issue by sparsifying
the cluster assignment probabilities using the l
?
norm, 0 < ? < 1, an effective proxy for the intractable
l
0
norm. In Section 4, we show how our bound can be incorporated into an HMM-based clustering
algorithm. In Section 5, we validate the theory presented in this paper with some empirical results on the
resource management corpus.
2 PAC-Bayesian Bounds
PAC-Bayesian theory is a useful framework for combining frequentist bounds with the notion of a prior.
Probably approximately correct (PAC) learning bounds the worst case generalization error of the best hy-
pothesis selected from a hypothesis space ? and therefore treats all hypotheses uniformly (Valiant, 1984).
PAC-Bayesian bounds, however, place a prior over the hypothesis space while making no assumptions on
the data generating distribution (McAllester, 1998). Thus, PAC-Bayesian bounds can both 1) incorporate
prior information, and 2) provide frequentist guarantees on the expected performance. They have been
successfully applied to classification settings such as the support vector machine (SVM) (McAllester,
2003; Langford, 2005), yielding significantly tighter bounds. Seldin and Tishby (2010) extend the frame-
131
work to include unsupervised learning tasks such as density estimation and clustering. Since statistical
language modeling at its core is a discrete density estimation problem, we focus on the bounds developed
by Seldin and Tishby (2010) and summarize key results in the following subsection.
2.1 Unsupervised Learning
Given a d-dimensional product space X
(1)
? ... ? X
(d)
and a collection of N samples, S, independent
and identically distributed (i.i.d.) according to some unknown distribution p(x
1
, ..., x
d
) over the product
space, we want to estimate p(x
1
, ..., x
d
) with some model q(x
1
, ..., x
d
). In the case of clustering (e.g.
class-based models), we make the following assumption on q(x
1
, ..., x
d
) [Note: we make no assumptions
on the true distribution p(x
1
, ..., x
d
)]:
q(x
1
, ..., x
d
) =
?
c
1
,...,c
d
q(c
1
, ..., c
d
)
d
?
i=1
q(x
i
|c
i
) (1)
where c
i
= h
i
(x
i
) for some clustering function h
i
: X
(i)
7? C
(i)
. We refer to them collectively as a
clustering function h, h = {h
i
}
d
i=1
; hence h : X
(1)
? ...?X
(d)
7? C
(1)
? ...?C
(d)
. We assume that the
original space X
(1)
? ... ? X
(d)
has finite cardinality, with n
i
= |X
(i)
|, and likewise for the clustered
space C
(1)
? ...? C
(d)
, where m
i
= |C
(i)
| is the number of clusters. We define a hypothesis space,H, to
be the space of all possible clustering functions h H.
For h  H, we define the distributions p
h
(c
1
, ..., c
d
) =
?
x
1
,...,x
d
p(x
1
, ..., x
d
)
?
d
i=1
?(h
i
(x
i
) = c
i
)
and p?
h
(c
1
, ..., c
d
) =
?
x
1
,...,x
d
p?(x
1
, ..., x
d
)
?
d
i=1
?(h
i
(x
i
) = c
i
), where p(x
1
, ..., x
d
) is the unknown
true distribution, and p?(x
1
, ..., x
d
) is the empirical (maximum likelihood) estimate. The delta func-
tion, ?(arg), takes a value of 1 only when arg is true, and 0 otherwise. We can extend to
the original space with the model assumption in Equation (1). For example, p
h
(x
1
, ..., x
d
) =
?
c
1
,...,c
d
p
h
(c
1
, ..., c
d
)
?
d
i=1
q(x
i
|c
i
).
The key difference between PAC learning and the PAC-Bayesian framework is the following notion
of a random predictor, which is a distributionQ(h), learnt over the hypothesis spaceH. Inference works
as follows: for a new sample (x
1
, ..., x
d
), we first draw a hypothesis h from H at random according to
the distribution Q(h). We then return q(x
1
, ..., x
d
) according to the model described by Equation (1)
and the clustering function h. The PAC-Bayesian framework therefore allows for a second level of aver-
aging over Q, and we can define the induced distributions: p
Q
(c
1
, ..., c
d
) =
?
h
Q(h)p
h
(c
1
, ..., c
d
) and
p?
Q
(c
1
, ..., c
d
) =
?
h
Q(h)p?
h
(c
1
, ..., c
d
). Again, we can extend to the original space with p
Q
(x
1
, ..., x
d
)
and p?
Q
(x
1
, ..., x
d
) using the model assumption in Equation (1). Note that p
Q
(x
1
, ..., x
d
) is unknown
since p(x
1
, ..., x
d
) is unknown; but the goal is to bound some notion of generalization error, such as the
KL-divergence KL(p?
Q
(x
1
, ..., x
d
)||p
Q
(x
1
, ..., x
d
)).
The Change of Measure Inequality (CMI) (Seldin and Tishby, 2010) is central to almost every PAC-
Bayesian bound, so we briefly state it here. For any measurable function ?(h) on H and for any distri-
butions Q(h) and P(h):
E
Q(h)
[?(h)] ? KL(Q||P) + lnE
P(h)
[
e
?(h)
]
(2)
where KL(Q||P) = E
Q(h)
[
ln
Q(h)
P(h)
]
is the KL-divergence betweenQ andP . The proof is fairly straight-
forward and is a direct consequence of rewriting ?(h) as ln
(
e
?(h)
Q(h)
P(h)
P(h)
Q(h)
)
.
Seldin and Tishby (2010) apply the CMI with ?(h) = N ? KL(p?
h
(x
1
, ..., x
d
)||p
h
(x
1
, ..., x
d
)) and
simplify the KL-divergence term by recognizing that 1) {q(c
i
|x
i
)}
d
i=1
defines a distribution over all
possible clusterings, and hence Q = {q(c
i
|x
i
)}
d
i=1
; and 2) a specific P , which they call the prior, can be
defined without making any assumptions on the true distribution p(x
1
, ..., x
d
). Note that P is not a prior
in the Bayesian sense: 1) it indicates preference on the structure of the hypothesis, not an assumption
on the data generating distribution, although the latter could be a consequence of the former; 2) the
bound holds regardless of P; and 3) the bound holds regardless ofQ, which is not necessarily the Bayes
posterior.
132
The following prior on H makes no assumptions on p(x
1
, ..., x
d
). We present a simplified version of
the prior developed by Seldin and Tishby (2010):
P(h) ?
1
exp
[
?
d
i=1
m
i
lnn
i
+ n
i
lnm
i
]
(3)
The prior is based on a combinatorial argument. In order to select a clustering function h
i
for some
i, we first need to pick a cardinality profile (number of elements per cluster) for the m
i
clusters; there
are n
m
i
i
such profiles, hence the first term in the sum. Next, given a cardinality profile, we need to
bound the number of ways in which each of the n
i
elements can be assigned to the clusters given their
sizes; there are at most m
n
i
i
possibilities, hence the second term in the sum. The CMI with ?(h) =
N ? KL(p?
h
(x
1
, ..., x
d
)||p
h
(x
1
, ..., x
d
)), our modified prior, and a few information theoretic results lead
to the following bound.
PAC-Bayesian Clustering: For any distribution p over X
(1)
? ...?X
(d)
and an i.i.d. sample S of sizeN
according to p, with probability at least 1??, for all distributions of cluster functionsQ = {q(c
i
|x
i
)}
d
i=1
,
the following holds:
KL(p?
Q
(x
1
, ..., x
d
)||p
Q
(x
1
, ..., x
d
)) ?
?
d
i=1
n
i
lnm
i
+K
1
N
(4)
where K
1
=
?
d
i=1
m
i
lnn
i
+ (M ? 1) ln(N + 1) + ln
d+1
?
, and M =
?
d
i=1
m
i
. Although this
shows convergence, in applications such as language modeling, we are interested in directly bound-
ing the test set perplexity or cross-entropy. Seldin and Tishby (2010) smooth p?
Q
(x
1
, ..., x
d
) to bound
E
p(x
1
,...,x
d
)
[? ln p?
Q(x
1
,...,x
d
)
] and provide the following useful result based on Equation (4).
Bound on Cross-Entropy: For any probability measure p overX
(1)
?...?X
(d)
and an i.i.d. sample S of
size N according to p, with probability 1? ? for all distributions of cluster functionsQ = {q(c
i
|x
i
)}
d
i=1
:
E
p(x
1
,...,x
d
)
[? ln p?
Q
(x
1
, ..., x
d
)] ? ?I(p?
Q
(c
1
, ..., c
d
)) + ln(M)
?
?
d
i=1
n
i
lnm
i
+K
1
2N
+K
2
(5)
where p?
Q
(x
1
, ..., x
d
) is now the smoothed empirical estimate induced by Q, I(p?
Q
(c
1
, ..., c
d
)) =
?
d
i=1
H(p?
Q
(c
i
))?H(p?
Q
(c
1
, ..., c
d
)) is the multi-information of the clustering,M andK
1
are as defined
in Equation (4), and K
2
is an additional term, K
2
? I(p?
Q
(c
1
, ..., c
d
)), and the bound is non-negative.
3 Language Models
Since language modeling is yet another density estimation problem in which we want to minimize the test
set perplexity, the bound in Equation (5) readily applies to both word n-grams and class-based n-grams.
Note that the bounds are on cross-entropy, which is log perplexity, but we use the two terms almost
interchangeably. We are now interested in estimating the unknown true distribution p(v
1
, ..., v
n
) over
the space V
n
, where V is some vocabulary consisting of V = |V| words. The degenerate case, d = 1,
X
(1)
= V
n
, is the case of word n-grams and results in a bound that is dominated by n
1
= |X
(1)
| = V
n
.
This suggests that the number of training samples, N , must be on the same order as V
n
for the bound
(and hence the estimate) to be meaningful.
It is also clear why class-based models are favored whenever they work. In this case, d = n, X
(i)
= V
for all 1 ? i ? d, and the bound in Equation (5) reduces to something linear in V (since ?i, n
i
=
|X
(i)
| = V ). Moreover, the clustering function is the same for all i ? that is, word clusters do not depend
on the position in the n-gram. Assuming K word clusters, the number of training examples, N , only
needs to be on the order of K
n
+ nV , achieving effective small sample generalization especially when
K << V . In the following subsections, we extend the bound to sequences and present a unique approach
to regularize the bound.
133
3.1 Sequence Clustering
We have discussed two extreme cases, namely d = 1 and d = n, that correspond to word n-grams and
class-based n-grams, respectively. In practice, they are often interpolated to retain the advantages of
both, as shown in the following model:
q(v
1
, ..., v
n
) = ?q(v
1
, ..., v
n
) + (1? ?)
?
c
1
,...,c
n
q(c
1
, ..., c
n
)
n
?
i=1
q(v
i
|c
i
) (6)
for some 0 < ? < 1. A Bayesian interpretation of the above model is to select between the n-gram
and the class-based model with probabilities ? and 1 ? ?, respectively. In other words, for each n-
gram (v
1
, ..., v
n
), we simply flip an ?-biased coin to decide on one of the two models. In this paper,
we interpolate across the entire spectrum, 1 ? d ? n, instead of just the extreme cases ? that is, we
capture clusters over not just words, but also sequences of words (phrases). Previous results by Deligne
and Bimbot (1995), Ries et al. (1996), and Justo and Torres (2007) indicate that clustering over phrases
is practically useful and leads to significant improvements.
Suppose our goal is to estimate the probability of a trigram, for example, ?the cat sat.?
In the case of d = 1, we directly estimate the joint probability p(the, cat, sat). In the
standard class-based model, where d = 3, we estimate with the model p(the, cat, sat) =
?
c
1
,c
2
,c
3
p(c
1
, c
2
, c
3
)p(the|c
1
)p(cat|c
2
)p(sat|c
3
). The intermediate cases, such as d = 2 in this ex-
ample, are often neglected. The theory we subsequently develop interpolates over all four segmenta-
tions, including the missing ones: p(the, cat, sat) =
?
c
1
,c
2
p(c
1
, c
2
)p(the cat|c
1
)p(sat|c
2
) as well as
p(the, cat, sat) =
?
c
1
,c
2
p(c
1
, c
2
)p(the|c
1
)p(cat sat|c
2
).
In general, an n-gram has 2
n?1
possible segmentations, as illustrated in the previous example. Sup-
pose f  F is a particular segmentation from the space of all possible segmentations, and we explicitly
define it as the following mapping:
f : V
n
7? X
(1)
? ...?X
(d)
(7)
where 1 ? d ? n and f is simply a segmentation that does not modify the joint distribution; that is,
p(v
1
, ..., v
n
) = p(x
1
, ..., x
d
). If f is fixed a priori, we can immediately apply the bounds derived in
Equation (5) over the segmented space X
(1)
? ...? X
(d)
. This is the case where we decide on a model,
such as the standard class-based model (d = n), and simply use it.
An extension to the case of interpolated models is straightforward. We modify the hypothesis space
H to not only include all possible clusterings, but also all possible segmentations. The new random pre-
diction Q over H works as follows: given an n-gram (v
1
, ..., v
n
), draw a segmentation f  F according
to the distribution pi = (pi
1
, ..., pi
2
n?1), where the segmentations are indexed by j = 1, ..., 2
n?1
(the
ordering does not matter), and pi
j
is the probability of drawing segmentation j; pick a clustering as in
the random classifier described in Equation (5) for the new segmented space; and estimate q(v
1
, ..., v
n
)
according to the model described by the previous steps. The bound, in terms of pi, is given below.
PAC-Bayes Sequence Clustering: For any probability measure p over V
n
, and an i.i.d. sample S of
size N drawn according to p, with probability 1 ? ? for all distributions of segmentations pi and for all
distributions of cluster functions Q:
E
p(v
1
,...,v
n
)
[? ln p?
Q
(v
1
, ..., v
n
)] ?
2
n?1
?
j=1
?
?
K
3
(j) + ln(M(j))
?
?
d(j)
i=1
V
a
i
(j)
lnm
i
(j) +K
1
(j)
2N
?
?
pi
j
(8)
K
3
(j) = ?I(p?
Q
(c
1
, ..., c
d(j)
)) +K
2
(j)
where ?j ?i, 1 ? a
i
(j) ? n, and ?j,
?
d(j)
i=1
a
i
(j) = n, and V
a
i
(j)
simply replaces n
i
in Equation (5)
for a given j. The term K
2
(j) is from Equation (5). Note that all terms such as m
i
(j), the number of
clusters corresponding to the space, their product M(j), and additional terms K
1
(j), K
2
(j) now depend
on the segmentation j since X
(i)
and d(j) depend on j.
134
We can favor certain segmentations (e.g. those that require few training examples), but note that the
bound above is true regardless of the distribution over possible segmentations, pi. Also, the bound is
dominated by the exponent a
i
(j) and the constraint
?
d(j)
i=1
a
i
(j) = n. Hence, the bound is polyno-
mial in V for all segmentations except the standard class-based setting where d(j) = n, in which case
?i, a
i
(j) = 1. For example, if d(j) = n ? 1 for some segmentation j, there exists some i such that
a
i
(j) = 2 and hence represents clusters of bigrams. If d(j) = n ? 2, there exists some segmentation j,
and a space i such that a
i
(j) = 3, and so on until d(j) = 1, and this is the case of word n-grams where
a
1
(j) = n.
3.2 Bound Minimization
Imposing the restriction ?j ?i, a
i
(j) = 1 is simple, and although it can guarantee the small-sample
benefits of a standard class-based model, it is not a useful strategy for incorporating the constraint. Since
a
i
(j) corresponds to the original space X
(i)
for a given j, restricting a
i
(j) would restrict X
(i)
to an
a priori, fixed set of V elements. To learn the best possible set of V elements, however, we need to
minimize the effective size of X
(i)
. For example, suppose we are estimating trigrams over V
3
using the
following segmentation: X
(1)
= V and X
(2)
= V
2
? i.e. a bigram over clusters of words and clusters of
word bigrams. The unconstrained bound is dominated by X
(2)
. We can restrict the effective size of X
(2)
by assigning zero probability to the vast majority of its elements, by constraining the hypothesis space
to consider only cluster assignment functions q(x
i
|c
i
) in which n
2
<< V
2
of the elements have nonzero
probability. Thus, every word sequence in V
d
can be generated by the d = n segmentation, but every
other segmentation is constrained to generate at most a subset of V
d
with nonzero probability.
We achieve this by imposing the restriction on the random predictor Q. By Bayes rule, q(c
i
|x
i
) =
q(x
i
|c
i
)q(c
i
)
q(x
i
)
and we can alternatively define Q as Q = {q(c
i
), q(x
i
), q(x
i
|c
i
)}
d
i=1
. Our goal is to learn
a Q that minimizes the RHS of Equation (5), which includes maximizing the multi-information term,
as well as constraining n
i
. As expected, q(x
i
) controls the absolute size of X
(i)
and q(x
i
|c
i
) controls
the effective size based on the clustering. The dominant term in all of our bounds is n
i
(or a
i
, with
n
i
= V
a
i
), which results from the second term in the prior defined in Equation (3), since it bounds the
number of ways in which the n
i
items can be assigned to the m
i
clusters. Alternatively, we can represent
this quantity with an upper bound,
(
?
c
i
?q(x
i
|c
i
)?
0
)
lnm
i
. We can write q(x
i
) =
?
c
i
q(x
i
|c
i
)q(c
i
),
and n
i
= ?q(x
i
)?
0
= ?
?
c
i
q(x
i
|c
i
)q(c
i
)?
0
; by the triangle inequality and scale invariance of the l
0
norm, this is less than or equal to
?
c
i
?q(x
i
|c
i
)?
0
. We therefore limit the upper bound,
?
c
i
?q(x
i
|c
i
)?
0
,
by sparsifying q(x
i
|c
i
) for every cluster c
i
.
The Optimization Problem: Given some segmentation, we want to find a random predictorQ ? a class-
based model over the fixed segmentation ? such that the bound in Equation (5) is minimized, which is
given by the following optimization problem:
maximize
Q
I(p?
Q
(c
1
, ..., c
d
))
subject to ?q(x
i
|c
i
)?
0
? V, ? c
i
 C
(i)
, i = 1, . . . , d
(9)
Since such optimization problems are known to be NP-complete, we use a computationally tractable
proxy. The standard practice is to use the l
1
norm instead of the l
0
norm; although non-convex, we resort
to the l
?
norm, 0 < ? < 1, since q(x
i
|c
i
) is a probability vector with a fixed l
1
norm. We therefore solve
the following problem:
maximize
Q
I(p?
Q
(c
1
, ..., c
d
))
subject to ?q(x
i
|c
i
)?
?
? V, ? c
i
 C
(i)
, i = 1, . . . , d
(10)
We have shown that one way to regularize the bound for a non-trivial sequence clustering problem,
regardless of whether the segmentation is fixed or if we are interpolating across all segmentations, is
to sparsify the cluster assignment probabilities for every cluster. There are many ways to sparsify a
probability vector (Pilanci et al., 2012; Kyrillidis et al., 2013), and we select the l
?
norm, 0 < ? <
135
1, for its simplicity and success in other applications (Chartrand and Staneva, 2008). Our approach
guarantees manageable bounds on the test set cross-entropy for a general class of SLMs, without making
any assumptions on the true distribution p(v
1
, ..., v
n
).
The Bayesian Connection A Bayesian interpretation of our regularization provides additional insight
into other successful models, such as the hierarchical Pitman-Yor language model (HPYLM). In our
approach, we impose the restriction ?q(x
i
|c
i
)?
?
? V , 0 < ? < 1, for every cluster c
i
. It can be
shown that this is equivalent to a sub-exponential prior on q(x
i
|c
i
) (Hastie et al., 2009). Since q(x
i
) =
?
c
i
q(x
i
|c
i
)q(c
i
) and we make the assumption that q(x
i
|c
i
) is sub-exponential for every c
i
, we are
consequently assuming that q(x
i
) is also sub-exponential. Although the PAC-Bayesian bounds hold
regardless of the true distribution, our regularization technique implicitly assumes that it is heavy-tailed.
The key to HPYLM?s success within the Bayesian setting is a better prior that matches the heavy-
tailed distribution of natural language (Teh, 2006) ? the regularization approach developed in this paper
reassuringly corresponds to the assumption that the true distribution is heavy-tailed (sub-exponential).
On the other hand, it may be possible to derive provable guarantees for HPYLM within the context of
our clustering model. The main difference between HPYLM and the less successful Dirichlet process
(DP) is the Chinese restaurant process, which assigns new tables (clusters) to customers (samples) much
more aggressively in the former model than in the latter (Teh, 2006). HPYLM therefore has far fewer
customers (samples) per table (cluster) than DP, resulting in significantly sparser q(x
i
|c
i
).
4 An Efficient HMM Algorithm
The hidden Markov model (HMM) is a popular tool for modeling sequences and has been used in several
speech and language clustering tasks (Rabiner, 1989; Smyth, 1997; Li and Biswas, 1999). Over its rich
history, several techniques, including regularization and sparsification of the HMM parameters, have
been developed (Bicego et al., 2007; Bharadwaj et al., 2013). The goal of this section is to show how our
bound easily fits into a well-established model such as the HMM.
We can rewrite the standard class-based model by making a Markov assumption on q(c
1
, ..., c
n
):
q(x
1
, ..., x
d
, c
1
, ..., c
d
) =
d
?
i=1
q(x
i
|c
i
)q(c
i
|c
i?1
) (11)
where {x
i
}
d
i=1
is some segmentation of (v
1
, ..., v
n
)  V
n
. The HMM literature refers to c
i
as the hidden
state, q(x
i
|c
i
) as the observation probability, and q(c
i
|c
i?1
) as the state transition probability (Rabiner,
1989). If we consider each state of the HMM to be a cluster, then as before, q(c
i
|x
i
) = q(x
i
|c
i
)
q(c
i
)
q(x
i
)
is a distribution over all possible clustering functions. To solve the optimization problem described in
Equation (10), we need to maximize the multi-information I(q(c
1
, ..., c
n
)) while satisfying the constraint
?q(x
i
|c
i
)?
?
? V . We can rewrite the constrained optimization problem as an unconstrained problem
using a Lagrangian, and solve for q(x
i
|c
i
) with an l
?
regularized version of the expectation maximization
(EM) algorithm, similar to Bharadwaj et al. (2013).
To maximize the multi-information term I(q(c
1
, ..., c
d
)) in Equation (10), we sparsify the state tran-
sition probabilities q(c
i
|c
i?1
). This provably works when we use l
?
regularization, 0 < ? < 1 for
sparsifying q(c
i
|c
i?1
). The Renyi ?-entropy of a random variable with some probability distribution
q is defined to be H
?
(q) =
?
1??
log ?q?
?
and there are two useful results we use (Principe, 2010): 1)
lim
??1
H
?
(q) = H(q), whereH(q) is the Shannon entropy; and 2)H
?
(q) is non-increasing in ?. Thus,
for ? < 1,H
?
(q) is an upper bound on the Shannon entropy. Since l
?
regularization minimizes the Renyi
?-entropy, which for 0 < ? < 1 is an upper bound on the Shannon entropy, it effectively maximizes the
mutual information between c
i
and c
i?1
, given that I(q?
Q
(c
i
, c
i?1
)) = H(q?
Q
(c
i
))?H(q?
Q
(c
i
|c
i?1
)).
Thus, we have shown that at least in the context of clustering, sparsifying both the observation prob-
abilities and the state transition probabilities of an HMM using the l
?
prior directly minimizes general-
ization error.
136
500 700 900 1100 1300 1500 1700 19007
89
1011
1213
14
Training set size (# sentences)
Test s
et cros
s?entr
opy
 
 HMMSparse HMM
Figure 1: Test set cross-entropy of HMM vs l
?
-regularized (sparse) HMM as a function of the number
of training sentences
5 Experiments
We test our approach on a subset of the resource management (RM) corpus (Price et al., 1993), which
consists of naval commands that span approximately V = 1000 words. First, we show that l
?
regular-
ization works. Figure 1 shows the estimated test set cross-entropy of an unregularized HMM and of an
l
?
-regularized HMM as a function of the number of training sentences. We vary the training set size from
10 to 2000 sentences and test the models on 800 sentences; Figure 1 reports the average cross-entropy
on brackets of training sizes ? 10-100, 110-200, and so on. The l
?
-regularized HMM requires additional
tunable parameters such as the value of ?. To simplify the search on a separate 300 sentence development
set, we make a (rather restrictive) assumption that ? for both the transition and observation probabilities
is the same, and that ? is independent of the size of the training set. Our solutions are therefore not opti-
mal, but adequate to demonstrate our claims. To ensure that the cross-entropy is bounded, we smooth all
estimates with add-one smoothing. For small training datasets, the unregularized HMM learns models
that assign near-zero likelihood to some of the test sentences; hence, we only present results for training
set sizes greater than 500 sentences.
Like many other model selection results, Figure 1 suggests that model sparsity is essential when train-
ing datasets are small. In this example, about 900 sentences are required for the unregularized HMM
to outperform the sparse HMM. In the context of the theory developed in earlier sections, it was shown
that test set cross-entropy is proportional to
n
i
N
, where N is the number of training examples. In practical
settings, N is fixed; hence, the only strategy for minimizing cross-entropy is to minimize n
i
. Figure 1
confirms that l
?
regularization successfully sparsifies q(x
i
|c
i
), the observation probabilities of the HMM,
thereby minimizing n
i
.
We also compare how the test set cross-entropy improves as a function of the training set size for four
different models: 1) a baseline bigram model estimated over words; 2) a baseline class-based model
using Brown?s algorithm (Brown et al., 1992) with K = 20 clusters, learnt over the entire dataset so that
it is also representative of knowledge-based approaches in which the true clusters are known a priori;
3) l
?
-regularized HMM with 20 ergodic states; and 4) a special case of 3) in which the state transitions
are constrained to artificially form m
1
= 10 word clusters (10 states) and m
2
= 5 clusters that represent
word bigrams (10 states, where the 5 clusters are modeled with 2 left-to-right states each); therefore, the
model represents an interpolation between the standard class-based model and word bigrams, but is of
the exact same complexity as 2) and 3).
Figure 2 shows the estimated test set cross-entropy for each of the four models. The values of ?
used in our experiments are ? = 0.7 for the words only case and ? = 0.9 for sequences. It is clear
137
200 400 600 800 1000 1200 1400 1600 1800 20006
7
8
9
10
11
12
Training set size (# sentences)
Test
 set 
cros
s?en
tropy
 
 1) word bigrams2) class?based (Brown)3) HMM (words only)4) HMM (sequences)
Figure 2: Test set cross-entropy as a function of the number of training sentences for the four settings
from Figure 2 that l
?
regularization helps even in the case of a standard class-based model, the bound
for which is already linear in V . With fewer than 100 sentences, l
?
regularization can both learn the
clusters and estimate their transitions reasonably well, and surpasses Brown for training set sizes of
N ? 800 sentences. Brown?s algorithm in 2) finds clusters such that pairwise mutual information
terms are maximized; in 3), we not only maximize the mutual information, but we also reduce the
effective V by ensuring that each cluster (or state) specializes and represents as few words as possible.
As the number of training examples increases, estimates of class transitions indeed improve, but the
class-based assumption itself becomes too restrictive. In 4), which represents an interpolated model,
we see the tradeoff achieved by incorporating sequences: for small training sets, the model achieves
better generalization than word bigrams, but is worse than the class-based model; and for larger training
sets, the interpolated model learns better representations of high frequency events and outperforms the
class-based models represented by 2) and 3).
The value of ? in 3) is 0.7, whereas ? in 4) is 0.9; this seems counter-intuitive at first, but note that
a smaller ? does not necessarily imply sparser observation probabilities; however, it implies a heavier
distribution in a Bayesian setting. A Bayesian interpretation therefore suggests that in 4), the model itself
is better equipped to cope with heavy tails, whereas a more aggressive ? is required in 3).
6 Conclusion
By defining a random clustering model (a model in which there is a distribution over possible cluster
assignments, e.g. an HMM), it is possible to specialize published PAC-Bayesian cross-entropy bounds
to the cases of n-gram and class-based n-gram estimation. A distribution over segmentations allows
derivation of a cross-entropy bound on sequence clustering algorithms, which can be made useful by
sparsifying the sequence cluster observation probabilities. An efficient l
?
regularization technique can
be used to maximize sparsity, thereby minimizing the test set cross-entropy.
Acknowledgements
We are grateful to the SST Group at Illinois and the anonymous reviewers for valuable feedback. Thanks
also to Jitendra Ajmera, Om Deshmukh, and Ashish Verma for their contributions to the clustering
algorithm. This work was supported by the NSF CDI Program Grant Number BCS 0941268 and ARO
W9111NF-09-1-0383; the opinions expressed in this work are those of the authors and do not necessarily
reflect the views of the funding agencies.
138
References
Hirotugu Akaike. 1973. Information theory and an extension of the maximum likelihood principle. In Proceedings
of the Second International Symposium on Information Theory, pages 267?281.
Sujeeth Bharadwaj, Mark Hasegawa-Johnson, , Jitendra Ajmera, Om Deshmukh, and Ashish Verma. 2013. Sparse
hidden Markov models for purer clusters. In Proceedings of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 3098?3102.
Manuele Bicego, Marco Cristani, and Vittorio Murino. 2007. Sparseness achievement in hidden Markov models.
In Proceedings of of the International Conference on Inage Analysis and Processing, pages 67?72.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
Kenneth P. Burnham and David R. Anderson. 2002. Model selection and multi-model inference: a practical
information-theoretic approach. Springer.
Rick Chartrand and Valentina Staneva. 2008. Restricted isometry properties and nonconvex compressive sensing.
Inverse Problems, 24:1?14.
Stanley F. Chen and Josha Goodman. 1999. An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 13:359?394.
Stanley F. Chen. 2009. Performance prediction for exponential language models. In Proceedings of NAACL HTL.
Sabine Deligne and Frederic Bimbot. 1995. Language modeling by variable length sequences: theoretical for-
mulation and evaluation of multigrams. In Proceedings of the IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 169?172.
I.J. Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrica,
40(3):237?264.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction. Springer.
Frederick Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse
data. In Proceedings of Workshop on Pattern Recognition in Practice, pages 381?397.
Raquel Justo and M. Ines Torres. 2007. Different approaches to class-based language models using word segments.
Computer Recognition Systems 2, Advances in Soft Computing, 45:421?428.
Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400?401.
Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings
of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 181?184.
Anastasios Kyrillidis, Stephen Becker, Volkan Cevher, and Christoph Koch. 2013. Sparse projections onto the
simplex. JMLR: Workshop and Conference Proceedings, Proceedings of the 30th International Conference on
Machine Learning, 28(2):235?243.
John Langford. 2005. Tutorial on practical prediction theory for classification. The Journal of Machine Learning
Research, 6:273?306.
Cen Li and Gautam Biswas. 1999. Clustering sequence data using hidden Markov model representation. In
Proceedings of the SPIE ?99 Conference on Data Mining and Knowledge Discovery, pages 14?21.
G.J. Lidstone. 1920. Note on the general case of the Bayes-Laplace formula for inductive or a posteriori proba-
bilities. Transactions of the Faculty of Actuaries, 8:182?192.
David J.C. Mackay and Linda C. Bauman Peto. 1995. A hierarchical Dirichlet language model. Natural Language
Engineering, 1(03):289?308.
David McAllester. 1998. Some PAC-Bayesian theorems. In COLT? 98 Proceedings of the eleventh annual
conference on Computational Learning Theory, pages 230?234.
David McAllester. 2003. Simplified PAC-Bayesian margin bounds. In COLT? 03 Proceedings of the sixteenth
annual conference on Computational Learning Theory, pages 202?215.
139
Mesrob I. Ohannessian and Munther A. Dahleh. 2012. Rare probability estimation under regularly varying heavy
tails. JMLR: Workshop and Conference Proceedings, 25th Annual Conference on Learning Theory, 23(21):1?
24.
Mert Pilanci, Laurent El Ghaoui, and Venkat Chandrasekaran. 2012. Recovery of sparse probability measures
via convex programming. In Advances in Neural Information Processing Systems (NIPS), volume 25, pages
2429?2437.
P. Price, W.M. Fisher, J. Bernstein, and D.S. Pallett. 1993. Resource Management RM 2.0. In Linguistic Data
Consortium, Philadelphia.
Jose C. Principe. 2010. Information Theoretic Learning. Springer.
Lawrence R. Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286.
Klaus Ries, Finn Dag Buo, and Alex Waibel. 1996. Class phrase models for language modeling. In ICSLP ?96
Proceedings of the Fourth International Conference on Spoken Language, pages 398?401.
Ronald Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer,
Speech, and Language, 10:187?228.
Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proceedings
of the IEEE, 88(8).
Yevgeny Seldin and Naftali Tishby. 2010. PAC-Bayesian analysis of co-clustering and beyond. The Journal of
Machine Learning Research, 11:3595?3646.
Padhraic Smyth. 1997. Clustering sequences with hidden Markov models. In Advances in Neural Information
Processing (NIPS), volume 9, pages 648?654.
Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 985?992.
L.G. Valiant. 1984. A theory of the learnable. Communications of the ACM, 27(11):1134?1142.
140
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 72?79,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
State-Transition Interpolation and MAP Adaptation for HMM-based
Dysarthric Speech Recognition
Harsh Vardhan Sharma
Beckman Institute
405 North Mathews Avenue
Urbana, IL 61801, USA
hsharma@illinois.edu
Mark Hasegawa-Johnson
Beckman Institute
405 North Mathews Avenue
Urbana, IL 61801, USA
jhasegaw@illinois.edu
Abstract
This paper describes the results of our experi-
ments in building speaker-adaptive recogniz-
ers for talkers with spastic dysarthria. We
study two modifications ? (a) MAP adapta-
tion of speaker-independent systems trained
on normal speech and, (b) using a transition
probability matrix that is a linear interpolation
between fully ergodic and (exclusively) left-
to-right structures, for both speaker-dependent
and speaker-adapted systems. The experi-
ments indicate that (1) for speaker-dependent
systems, left-to-right HMMs have lower word
error rate than transition-interpolated HMMs,
(2) adapting all parameters other than transi-
tion probabilities results in the highest recog-
nition accuracy compared to adapting any
subset of these parameters or adapting all
parameters including transition probabilities,
(3) performing both transition-interpolation
and adaptation gives higher word error rate
than performing adaptation alone and, (4)
dysarthria severity is not a sufficient indica-
tor of the relative performance of speaker-
dependent and speaker-adapted systems.
1 Introduction
After more than two decades of research, speech
recognition is a well-established and reliable
human-computer interaction technology. The accu-
racy of the newest generation of large vocabulary
speech recognizers, after adaptation to a user with-
out speech pathology, is high enough to provide a
useful human-computer interface especially for peo-
ple who find it difficult to type with a keyboard.
Automatic speech recognition (ASR) systems
generally assume that the speech signal is a realisa-
tion of some message encoded as a sequence of one
or more symbols. To effect the reverse operation of
recognising the underlying symbol sequence given a
spoken utterance, the continuous speech waveform
is first converted to a sequence of equally spaced
discrete parameter vectors. The role of the recog-
niser is to effect a mapping between sequences of
speech vectors and the wanted underlying symbol
sequences. Most speech recognizers today are based
on the hidden Markov model (HMM) paradigm: it is
assumed that the sequence of observed speech vec-
tors is generated by a Markov model as shown in
Fig. 1. A Markov model is a finite state machine
which changes state once every time unit and each
time t that a state j is entered, a speech vector ot is
generated from the probability density bj(ot) which
a22 a33 a44
a23 a34 a45a12
a35
a24a13
HiddenMarkovModel
ObservationSequence
1 2 3 4 5 6 7
b2( 1) b2( 2) b3( 3) b3( 4)b4( 5) b4( 6) b4( 7)
Figure 1: The Markov generation model.
72
is a mixture-Gaussian density for most standard sys-
tems. The transition from state i to state j is also
probabilistic and is governed by the discrete prob-
ability aij . Fig. 1 shows an example of this pro-
cess where the five state model moves through the
state sequence X = 1, 2, 2, 3, 3, 4, 4, 4, 5 in order to
generate the sequence o1 to o7. The entry and exit
states (1, 5) are non-emitting. This is to facilitate
the construction of composite models: most systems
use HMMs to perform modeling at the phone-level
rather than word-level; as such, word-level mod-
els are constructed by stringing together phone-level
HMMs for the constituent phones.
Fig. 2 shows how HMMs can be used for isolated
word recognition. Firstly, an HMM is trained for
each vocabulary word using a number of examples
of that word ? given a set of training examples cor-
responding to a particular model, the parameters of
that model ({aij} and {bj(ot)}) are determined by a
robust and efficient re-estimation procedure. In this
example, the vocabulary consists of just three words:
?one?, ?two? and ?three?. Secondly, to recognise
some unknown word, the likelihood (probability) of
each model generating that word is calculated and
the most likely model identifies the word.
EstimateModels
P( | 2)
(a) TRAINING
Unknown = 
Training Examples
?one?
1
2
3
?two? ?three?
1 2 3
(b) RECOGNITION
P( | 1) P( | 3)
Choose Max
Figure 2: Using HMMs for isolated word recognition.
For creating a speech recognizer for a particular
speaker, there are two approaches: one is to create
a speaker-dependent (SD) system by utlizing speech
of that speaker alone to train the HMMs; the other
is to create a speaker-adapted (SA) system by first
training the HMMs in a speaker-independent fashion
by utlizing speech of several speakers, and then cus-
tomising the HMMs to the characteristics of the par-
ticular speaker by using training examples of their
speech to modify the HMM parameters. The param-
eter values do not get overwritten; they are adjusted
using a regularized or constrained machine learning
algorithm. Regularization (e.g., using Maximum A
Posteriori learning) or constraints (e.g., using lin-
ear transformations) allow the SA model to use far
more trainable parameters per minute of training
data without over-training the system.
Despite the advances in speech technology, their
benefits have not been available to people with gross
motor impairments mainly because these impair-
ments include a component of dysarthria ? a group
of motor speech disorders resulting from disturbed
muscular control of the speech mechanism due to
damage of the peripheral or central nervous system.
Dysarthria is often a symptom of a gross motor dis-
order, whose other symptoms usually make it hard
to use a keyboard and mouse. Published case stud-
ies have shown that some dysarthric users may find
it easier to use an ASR system instead of a key-
board (Carlson and Bernstein, 1987; Coleman and
Meyers, 1991; Deller et al, 1988; Deller et al,
1991; Fried-Oken, 1985). Polur and Miller stud-
ied the development of HMM-based small vocabu-
lary (eight repetitions each of ten digits and fifteen
?command? words in English) SD systems for three
male subjects subjectively classified by a trained
clinician as moderately dysarthric (Polur and Miller,
2005a; Polur and Miller, 2005b). They found that
an ergodic HMM with a slight left-to-right character
(called a transition-interpolated HMM from hereon)
provides higher word recognition accuracy (WRA)
than a standard left-to-right HMM, apparently be-
cause the transition-interpolated HMM is able to
capture outlier events as a backward or nonlinear
progress through the intended word. The benefit
of using ergodic modeling over left-to-right mod-
eling in distorted speech applications with disrup-
tion events, pause events, and limited training data
has also been noted earlier by Deller, Hsu and Fer-
rier (Deller et al, 1991). Section 2.1.2 explains
73
in more detail the difference between these HMM
topologies.
Speaking for long periods of time is tiring, espe-
cially for a person with dysarthria, therefore it is dif-
ficult for a person with dysarthria to train a speaker-
dependent ASR. Speaker adaptation then seems a
useful method to overcome this obstacle in devel-
oping dysarthric speech recognizers. Raghavendra
et al (Raghavendra et al, 2001) have compared
recognition accuracies of an SA system and an SD
system. They found that the SA system adapted
well to the speech of talkers with mild or moder-
ate dysarthria, but the recognition scores were lower
than for an unimpaired speaker. The subject with
severe dysarthria was able to achieve better perfor-
mance with the SD system than with the SA sys-
tem. These findings were also supported by Rudz-
icz (Rudzicz, 2007) who compared the performance
of SD and ?SA? systems on the Nemours database
(Menendez-Pidal et al, 1996) by varying indepen-
dently the amount of data for training and the num-
ber of Gaussian components used for modeling the
output probability distributions. The ?SA? technique
implemented is not speaker-adaptation in the con-
ventional sense: it uses the parameter values for the
speaker-independent system as the starting point to
train HMMs for a particular dysarthric speaker. In
a training algorithm without regularization or con-
straint terms, it is possible for a system of this type
to over-train, resulting in loss of accuracy on test
data from the same speaker, and Rudzicz?s results
suggest that such over-training may have occurred
in some cases. He further concluded that there was
not enough data in the database to represent intra-
speaker variation.
The study described in this paper investigated the
development of medium vocabulary HMM recog-
nizers for dysarthric speech of various degrees of
severity with the following aims: (1) to test the per-
formance of SA systems relative to SD systems, for
various degrees of dysarthria severity, (2) to test the
performance of an SD system employing transition-
interpolated HMMs relative to an SD system using
strictly left-to-right HMMs, (3) to test the perfor-
mance of an SA system with transition-interpolated
HMMs relative to an SD system having strictly left-
to-right HMMs and, (4) to see if the results in the
above three cases are essentially a function of the
talker?s dysarthria severity.
2 Experimental Setup
2.1 Modifications investigated
The following modifications to the HMM structure
were studied in our experiments:
2.1.1 Adaptation
All SA systems were developed by adapting a
speaker-independent system in a Maximum A Pos-
teriori (MAP) manner, as outlined by Gauvain and
Lee (Gauvain and Lee, 1991; Gauvain and Lee,
1992). MAP adaptation involves the use of prior
knowledge about the model parameter distribution.
Hence, if we know what the parameters of the model
are likely to be (before observing any adaptation
data) using the prior knowledge, we might well be
able to make good use of the limited adaptation data,
to obtain a decent MAP estimate. For MAP adapta-
tion purposes, the informative priors that are gener-
ally used are the speaker independent model param-
eters (empirical Bayes approach). In (Gauvain and
Lee, 1991), they derive expressions of MAP esti-
mates for all HMM parameters except the transition
probabilities (Gaussian mixture-component means,
diagonal Gaussian mixture-component covariance
matrices and, mixture-component weights) and also
provide an initialization scheme for the prior den-
sity of these parameters. In (Gauvain and Lee,
1992), they derive expressions for MAP estimates of
transition probabilities in addition to those for full-
covariance Gaussian mixture-component parame-
ters, and provide a MAP variant of the Expectation-
Maximization (EM) re-estimation algorithm. All
systems developed in our study modeled the obser-
vations as mixture of Gaussians with diagonal co-
variance matrices.
2.1.2 Transition-Interpolation
Fig. 3 illustrates the topologies of strictly left-to-
right (LR) and transition-interpolated (TI) HMMs
with 3 emitting states. If A = {aij} be the N ?
N transition probability matrix for an N-state HMM,
then we have for an LR HMM: for each state i,
0 < aii , ai,i+1 < 1; aii + ai,i+1 = 1 and aij =
0 for j 6= i, i + 1. In other words, each emitting
state has only two possible state-transitions: given
the current state, the HMM either remains in the
74
same state or moves into the succeeding state; it will
not jump over states or go to a preceding state.
Strictly
Left-to-Right
HMM
a22 a33 a44
a23 a34 a45a12
Transition-
Interpolated
HMM
a22 a33 a44
a23 a34 a45a12
?32 ?43
?24
?42
Figure 3: Difference between strictly left-to-right and
transition-interpolated HMM topologies.
The TI model is an LR model which has non-zero
transition probabilties for jumps and transitions to
preceding states from a particular state (for emit-
ting states). These probabilties are however small
compared to self-transition and next-state?transition
probabilties. A TI HMM is initialized as follows: for
each emitting state i, aij =  for j 6= i, i+1 where
0 <  << 1; aii , ai,i+1 >>  and
?N
j=1 aij =
1. After this initialization, the transition probabil-
ity matrix is re-estimated for speaker-dependent sys-
tems using the standard Maximum Likelihood EM
algorithm, and for speaker-adapted systems using
the MAP variant of the EM algorithm.
2.2 Data used
The experiments described in this paper utilized
speech of 7 speakers from the UA-Speech database
(Kim et al, 2008). This corpus was constructed with
the aim of developing large-vocabulary dysarthric
ASR systems which would allow users to enter un-
limited text into a computer. All speakers exhib-
ited symptoms of spastic dysarthria, according to an
informal evaluation by a certified speech-language
pathologist. Each speaker recorded 765 isolated
words in 3 blocks of 255 words each; (a) common
to all blocks: 10 digits (D), 19 computer commands
(C), 26 radio alphabet letters (L), and 100 common
words (CW) selected from the Brown corpus of writ-
ten English; and (b) unique to each block: 100 un-
common words (UW) selected from children?s nov-
els digitized by Project Gutenberg. Vocabularies D
and CW were primarily composed of monosylla-
bles, C and L of bisyllables, and UW of polysyl-
labic words. The speakers? speech was affected by
dysarthria associated with cerebral palsy. Data ac-
quisition and intelligibility assessment is described
in more detail in (Kim et al, 2008). Two hundred
distinct words were selected from the recording of
the second block: 10 digits, 25 radio alphabet letters,
19 computer commands and, 73 words randomly se-
lected from each of the CW and UW categories. Five
naive listeners were recruited for each speaker and
were instructed to provide orthographic transcrip-
tions of each word that they thought the speaker
said. The percentage of correct responses was then
averaged across five listeners to obtain each speak-
ers intelligibility. Table 1 lists the speakers whose
speech materials from the UA-Speech database were
used, along with their human listener intelligibility
ratings. The first letter of the speaker code (?M? or
?F?) indicates their gender.
Speaker Age Speech Intelligbility (%)
M09 18 high (86%)
M05 21 mid (58%)
M06 18 low (39%)
F02 30 low (29%)
M07 58 low (28%)
F03 51 very low (6%)
M04 >18 very low (2%)
Table 1: Summary of Speaker Information (in decreasing
order of human listener intelligibility rating).
For building the ?MAP prior? speaker-
independent system, the unadapted HMMs were
trained on speech from the TIMIT corpus (Garofolo
et al, 1993).
2.3 System Configurations
Table 2 lists the characteristics of the various sys-
tem configurations that were studied: SD stands
for speaker-dependent, SA for speaker-adapted;
LR implies use of strictly left-to-right HMMs, TI
for transition-interpolated HMMs; ?m?,?v?,?w?,?t?
respectively denote means, variances, mixture-
75
component weights and transition probabilities.
These systems were developed for each of the seven
System (Type) HMM Parameters adapted
C00 (SD) LR ?
C01 (SD) TI ?
C11 (SA) LR m
C12 (SA) LR m,v
C13 (SA) LR m,v,w
C14 (SA) LR m,v,w,t
C15 (SA) TI m,v,w,t
Table 2: Summary of ASR System Configurations
speakers listed in Table 1, and employed word-
internal, context-dependent triphone HMMs, with
three hidden states and observations modeled as
mixture-of-Gaussians. Configuration C00 was de-
veloped by Sharma and Hasegawa-Johnson (2009)
and is the baseline configuration for the present ex-
periments. For configurations C11 through C15,
the speaker-independent systems trained on TIMIT
employed left-to-right HMMs. For systems C15,
the transition-interpolation was performed after ob-
taining the speaker-independent TIMIT-trained left-
to-right HMMs and before adaptation to the UA-
Speech speaker?s data: the original non-zero entries
in the transition probability matrices were scaled
down so that the sum of each row was unity after
changing the zero-entries to . For each speaker, all
of blocks 1 and 3 were used as training data (sys-
tems C00, C01) or adaptation data (systems C11-
C15) and all of block 2 was used for testing. The
speaker-independent system was trained on all of
TIMIT?s training data and was tested on speech of
32 randomly chosen speakers from its test data.
The features extracted from the speech waveform
comprised of 12 Perceptual Linear Prediction co-
efficients (Hermansky, 1990) for 25 ms Hamming-
windowed segments obtained every 10 ms, plus the
energy of the windowed segment. ?Velocity? and
?Acceleration? components were also calculated for
this 13-dimensional feature, which finally resulted
in a 39-dimensional acoustic feature vector.
The measure used for assessing the performance
of the developed recognizers is the fraction of task?
vocabulary words correctly recognized (in percent),
defined in Equation 1.
PWC =
# words correctly recognized
# words attempted
? 100
(1)
For each configuration, the number of Gaussian
components in the state-specific observation proba-
bility densities was increased (in an iterative man-
ner) in powers of 2, from 1 to 32 components (for
C00 and C01) or 64 components (for C11-C15):
standard methods for choosing this number (using
development test data) could not be employed on
account of insufficient data. The results reported
in the next section should therefore be interpreted
as development test results. In order to avoid over-
tuning, the number of Gaussian components was
constrained to be the same across all speakers. For
the speaker-dependent systems (C00 and C01), re-
sults are for HMMs with 2 Gaussian components per
probability density. For the speaker-adapted systems
(C11-C15), results are for HMMs with 32 Gaussian
components per probability density: while train-
ing the speaker-independent TIMIT system, it was
found that the phone recognition accuracy increased
monotonically when going from 1 to 32 Gaussian
components but decreased when going from 32 to
64 components.
3 Results
Tables 3, 4 list the PWC scores for the various sys-
tem configurations developed. The speakers are
listed in decreasing order of intelligibility rating.
The scores for systems C00 are restated here from
Sharma and Hasegawa-Johnson (2009) (Table 6, un-
der the column ?T10?).
We see that speaker-dependent systems with left-
to-right HMMs (C00) have higher recognition ac-
curacy than the speaker-dependent systems with
transition-interpolated HMMs (C01), for all speak-
ers except M06. System C11 for a particular
speaker, with adaptation of Gaussian means alone
performs either better or worse than both sys-
tems C00 and C01 for that speaker. System C12
with adaptation of Gaussian means and variances,
has better recognition accuracy than both speaker-
dependent systems, for all speakers except F02 and
M07 (worse than both speaker-dependent systems).
System C13 with adaptation of all parameters ex-
76
System Configuration
Speaker C00 C01 C11 C12
M09 52.04 47.3 57.1 62.1
M05 35.52 33.7 31 39.4
M06 34.01 36.1 38.6 38.5
F02 35.06 32.8 20.8 26.9
M07 43.87 40.7 32 35.9
F03 12.61 11.3 17.4 22.2
M04 2.82 1.7 3.7 4.2
Table 3: PWC scores for each speaker?s configurations
C00-C12.
System Configuration
Speaker C00 C13 C14 C15
M09 52.04 66.4 65.8 64.2
M05 35.52 45.2 44 38.1
M06 34.01 40.7 40.1 39.2
F02 35.06 30.4 29.7 26.6
M07 43.87 43 41.8 35.9
F03 12.61 27.7 26.2 25.7
M04 2.82 4.2 3.8 3.1
Table 4: PWC scores for each speaker?s configurations
C00,C13-C15.
cept transition-probabilities has the highest recogni-
tion accuracy for all subjects except F02 and M07
(highest among speaker-adapted systems only). Sys-
tem C14 which adapts all parameters including tran-
sition probabilities, always performs worse than the
corresponding system C13, for all speakers. How-
ever, like system C13, it has better recognition ac-
curacy than both speaker-dependent systems for all
speakers except F02 and M07. Finally, perform-
ing transition-interpolation and adaptation of all pa-
rameters (system C15) worsens the performance to
below that of the corresponding system C14; addi-
tionally, C15 has better recognition accuracy than
both speaker-dependent systems whenever the cor-
responding C13 (and C14) system also performs bet-
ter than them.
These results are plotted in Fig. 4 along with
the human listeners? intelligibility ratings of these
speakers (the black circles). For speakers M09 and
M05, system C13 with the best overall PWC score
is still far from doing as well as human listeners. For
M09 M05 M06 F02 M07 F03 M040
10
20
30
40
50
60
70
80
90
speaker (from left: decreasing order of intelligibility)
# Wo
rds C
orrec
t (%)
C00C01C11C12C13C14C15
Figure 4: PWC scores for various system configurations
(the black circles indicate speakers? human listener intel-
ligibility ratings).
the remaining subjects, it has however been able to
do as well or better than human listeners even when
it performed worse than the corresponding speaker-
dependent systems (C00,C01): in fact, for speaker
M06, it does better than human listeners when the
speaker-dependent systems don?t.
Fig. 5 plots, for all speakers, the percentage dif-
ference PWC(x)/PWC(C00)-1 between the PWC of
system x (x ? {C01? C15}) and the PWC of sys-
tem C00.
M09 M05 M06 F02 M07 F03 M04?40
?20
0
20
40
60
80
100
120
speaker (from left: decreasing order of intelligibility)
Cha
nge 
in Re
cogn
ition
 Acc
urac
y (%
) C01C11C12C13C14C15
Figure 5: Percentage change in PWC scores for vari-
ous system configurations relative to configuration C00?s
PWC score.
For speakers who have an intelligibility rating
above 35% or below 25%, the speaker-adapted
systems generally do better than their speaker-
dependent counterparts. System C01, with tran-
sition interpolation, performs worse than system
77
C00 for all speakers except M06. The surpris-
ing result though is that for speakers with highly
severe dysarthria (F03 and M04), speaker-adapted
systems have substantially better recognition ac-
curacies than their speaker-dependent counterparts,
when previous studies have indicated that for such
subjects, speaker-dependent systems perform better
than speaker-adapted systems.
4 Conclusions
This study investigated adaptation and state-
transition interpolation techniques for medium vo-
cabulary HMM-based speech recognition of talkers
with spastic dysarthria. It was found that performing
transition-interpolation generally worsens recogni-
tion performance when compared to left-to-right
HMMs. Performing both adaptation and transition-
interpolation results in higher recognition accuracy
compared to the speaker-dependent systemwith left-
to-right HMMs but adaptation-only systems have
still better performance. This implies that state-
transitions not accounted for in left-to-right HMMs
do not capture (or capture rather poorly) the out-
lier events that differentiate dysarthric speech from
unimpaired speech at the sub-phone level.
The most interesting outcome of our experiments
is that for subjects that have very severe dysarthria,
speaker-adaptation was able to achieve substantial
improvement in recognition accuracy, compared to
the speaker-dependent systems. This finding is sig-
nificant in that it is contrary to the conclusions of
previously published studies. The results reported
in this paper therefore suggest that the severity of
dysarthria as quantified by the subject?s intelligibil-
ity rating is not a sufficient indicator of the rela-
tive performance of speaker-dependent and speaker-
adapted systems.
References
Gloria S. Carlson and Jared Bernstein. 1987. Speech
Recognition of Impaired Speech. Proceedings of
RESNA 10th Annual Conference on Rehabilitation
Technology, 165?167.
Colette L. Coleman and Lawrence S. Meyers. 1991.
Computer Recognition of the Speech of Adults with
Cerebral Palsy and Dysarthria. AAC: Augmentative
and Alternative Communication, 7(1):34?42.
John R. Deller, D. Frank Hsu and Linda J. Ferrier. 1988.
Encouraging Results in the Automated Recognition
of Cerebral Palsy Speech. IEEE Transactions on
Biomedical Engineering, 35(3):218?220.
John R. Deller, D. Frank Hsu and Linda J. Ferrier. 1991.
On the use of Hidden Markov modelling for Recog-
nition of Dysarthric Speech. Computer Methods and
Programs in Biomedicine, 35(2):125?139.
Melanie Fried-Oken. 1985. Voice Recognition Device as
a Computer Interface for Motor and Speech Impaired
People. Archives of Physical Medicine and Rehabili-
tation, 66:678?681.
Jean-luc Gauvain and Chin-hui Lee. 1991. Bayesian
Learning of Gaussian Mixture Densities for Hidden
Markov Models. Proceedings of DARPA Speech and
Natural Language Workshop, 272?277.
Jean-luc Gauvain and Chin-hui Lee. 1992. MAP Estima-
tion of Continuous Density HMM: Theory and Appli-
cations. Proceedings of DARPA Speech and Natural
Language Workshop, 185?190.
John S. Garofolo, Lori F. Lamel, William M.
Fisher, Jonathan G. Fiscus, David S. Pallett,
Nancy L. Dahlgren and Victor Zue. 1993.
TIMIT Acoustic-Phonetic Continuous Speech Corpus.
http://www.ldc.upenn.edu/Catalog/LDC93S1.html.
Hynek Hermansky. 1990. Perceptual Linear Predictive
(PLP) Analysis of Speech. Journal of the Acoustical
Society of America, 87(4):1738?1752.
Heejin Kim, Mark Hasegawa-Johnson, Adrienne Perl-
man, Jon Gunderson, Thomas Huang, Kenneth Watkin
and Simone Frame. 2008. Dysarthric Speech
Database for Universal Access Research. Proceedings
of Interspeech, Brisbane, Australia, 22?26.
Xavier Menendez-Pidal, James B. Polikoff, Shirley M.
Peters, Jennie E. Leonzio, H. T. Bunnell. 1996. The
Nemours Database of Dysarthric Speech. Proceedings
of the Fourth International Conference on Spoken Lan-
guage Processing, Philadelphia, PA, USA.
Prasad D. Polur and Gerald E. Miller. 2005a. Effect
of High-Frequency Spectral Components in Computer
Recognition of Dysarthric Speech based on a Mel-
Cepstral Stochastic Model. Journal of Rehabilitation
Research & Development, 42(3):363?372.
Prasad D. Polur and Gerald E. Miller. 2005b. Experi-
ments with Fast Fourier Transform, Linear Predictive
and Cepstral Coefficients in Dysarthric Speech Recog-
nition Algorithms using Hidden Markov Model. IEEE
Transactions on Neural Systems and Rehabilitation
Engineering, 13(4):558?561.
Parimala Raghavendra, Elisabet Rosengren and Sheri
Hunnicutt. 2001. An Investigation of Different
Degrees of Dysarthric Speech as Input to Speaker-
Adaptive and Speaker-Dependent Recognition Sys-
78
tems. AAC: Augmentative and Alternative Communi-
cation, 17(4):265?275.
Frank Rudzicz. 2007. Comparing Speaker-Dependent
and Speaker-Adaptive Acoustic Models for Recogniz-
ing Dysarthric Speech. Proceedings of ASSETS?07,
Tempe, AZ, USA.
Harsh Vardhan Sharma and Mark Hasegawa-Johnson.
2009. Universal Access: Speech Recognition for Talk-
ers with Spastic Dysarthria. Proceedings of Inter-
speech, Brighton, UK, 1451?1454.
79
