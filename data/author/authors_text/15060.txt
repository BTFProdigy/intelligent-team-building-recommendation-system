Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846?1851,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Constructing Information Networks Using One Single Model
Qi Li
?
Heng Ji
?
Yu Hong
??
Sujian Li
?
?
Computer Science Department, Rensselaer Polytechnic Institute, USA
?
School of Computer Science and Technology, Soochow University, China
?
Key Laboratory of Computational Linguistics, Peking University, MOE, China
?
{liq7,hongy2,jih}@rpi.edu,
?
lisujian@pku.edu.cn
Abstract
In this paper, we propose a new frame-
work that unifies the output of three infor-
mation extraction (IE) tasks - entity men-
tions, relations and events as an informa-
tion network representation, and extracts
all of them using one single joint model
based on structured prediction. This novel
formulation allows different parts of the
information network fully interact with
each other. For example, many rela-
tions can now be considered as the re-
sultant states of events. Our approach
achieves substantial improvements over
traditional pipelined approaches, and sig-
nificantly advances state-of-the-art end-to-
end event argument extraction.
1 Introduction
Information extraction (IE) aims to discover entity
mentions, relations and events from unstructured
texts, and these three subtasks are closely inter-
dependent: entity mentions are core components
of relations and events, and the extraction of rela-
tions and events can help to accurately recognize
entity mentions. In addition, the theory of eventu-
alities (D?olling, 2011) suggested that relations can
be viewed as states that events start from and result
in. Therefore, it is intuitive but challenging to ex-
tract all of them simultaneously in a single model.
Some recent research attempted to jointly model
multiple IE subtasks (e.g., (Roth and Yih, 2007;
Riedel and McCallum, 2011; Yang and Cardie,
2013; Riedel et al., 2009; Singh et al., 2013; Li et
al., 2013; Li and Ji, 2014)). For example, Roth and
Yih (2007) conducted joint inference over entity
mentions and relations; Our previous work jointly
extracted event triggers and arguments (Li et al.,
2013), and entity mentions and relations (Li and
Ji, 2014). However, a single model that can ex-
tract all of them has never been studied so far.
Asif Mohammed Hanif detonated explosives in Tel Aviv
AttackPerson Weapon Geopolitical Entity
Place
InstrumentAttacker
Agent-Artifact
Physical
x1 x2 x3 x4 x5 x6 x7 x8x:
y:
Figure 1: Information Network Representation.
Information nodes are denoted by rectangles. Ar-
rows represent information arcs.
For the first time, we uniformly represent the IE
output from each sentence as an information net-
work, where entity mentions and event triggers are
nodes, relations and event-argument links are arcs.
We apply a structured perceptron framework with
a segment-based beam-search algorithm to con-
struct the information networks (Collins, 2002; Li
et al., 2013; Li and Ji, 2014). In addition to the per-
ceptron update, we also apply k-best MIRA (Mc-
Donald et al., 2005), which refines the perceptron
update in three aspects: it is flexible in using var-
ious loss functions, it is a large-margin approach,
and it can use mulitple candidate structures to tune
feature weights.
In an information network, we can capture the
interactions among multiple nodes by learning
joint features during training. In addition to the
cross-component dependencies studied in (Li et
al., 2013; Li and Ji, 2014), we are able to cap-
ture interactions between relations and events. For
example, in Figure 1, if we know that the Person
mention ?Asif Mohammed Hanif ? is an Attacker
of the Attack event triggered by ?detonated?, and
the Weapon mention ?explosives? is an Instrument,
we can infer that there exists an Agent-Artifact
relation between them. Similarly we can infer
the Physical relation between ?Asif Mohammed
Hanif ? and ?Tel Aviv?.
However, in practice many useful interactions
are missing during testing because of the data spar-
1846
sity problem of event triggers. We observe that
21.5% of event triggers appear fewer than twice in
the ACE?05
1
training data. By using only lexical
and syntactic features we are not able to discover
the corresponding nodes and their connections. To
tackle this problem, we use FrameNet (Baker and
Sato, 2003) to generalize event triggers so that
semantically similar triggers are clustered in the
same frame.
The following sections will elaborate the de-
tailed implementation of our new framework.
2 Approach
We uniformly represent the IE output from each
sentence as an information network y = (V,E).
Each node v
i
? V is represented as a triple
?u
i
, v
i
, t
i
? of start index u
i
, end index v
i
, and node
type t
i
. A node can be an entity mention or an
event trigger. A particular type of node is ? (nei-
ther entity mention nor event trigger), whose max-
imal length is always 1. Similarly, each infor-
mation arc e
j
? E is represented as ?u
j
, v
j
, r
j
?,
where u
j
and v
j
are the end offsets of the nodes,
and r
j
is the arc type. For instance, in Fig-
ure 1, the event trigger ?detonated? is represented
as ?4, 4, Attack?, the entity mention ?Asif Mo-
hammed Hanif ? is represented as ?1, 3, Person?,
and their argument arc is ?4, 3, Attacker?. Our
goal is to extract the whole information network y
for a given sentence x.
2.1 Decoding Algorithm
Our joint decoding algorithm is based on ex-
tending the segment-based algorithm described in
our previous work (Li and Ji, 2014). Let x =
(x
1
, ..., x
m
) be the input sentence. The decoder
performs two types of actions at each token x
i
from left to right:
? NODEACTION(i, j): appends a new node
?j, i, t? ending at the i-th token, where i? d
t
<
j ? i, and d
t
is the maximal length of type-t
nodes in training data.
? ARCACTION(i, j): for each j < i, incremen-
tally creates a new arc between the nodes ending
at the j-th and i-th tokens respectively: ?i, j, r?.
After each action, the top-k hypotheses are se-
lected according to their features f(x, y
?
) and
1
http://www.itl.nist.gov/iad/mig//tests/ace
weights w:
best
k
y
?
?buffer
f(x, y
?
) ?w
Since a relation can only occur between a pair of
entity mentions, an argument arc can only occur
between an entity mention and an event trigger,
and each edge must obey certain entity type con-
straints, during the search we prune invalid AR-
CACTIONs by checking the types of the nodes
ending at the j-th and the i-th tokens. Finally, the
top hypothesis in the beam is returned as the final
prediction. The upper-bound time complexity of
the decoding algorithm is O(d ? b ? m
2
), where d
is the maximum size of nodes, b is the beam size,
and m is the sentence length. The actual execution
time is much shorter, especially when entity type
constraints are applied.
2.2 Parameter Estimation
For each training instance (x, y), the structured
perceptron algorithm seeks the assignment with
the highest model score:
z = argmax
y
?
?Y(x)
f(x, y
?
) ?w
and then updates the feature weights by using:
w
new
= w + f(x, y)? f(x, z)
We relax the exact inference problem by the afore-
mentioned beam-search procedure. The stan-
dard perceptron will cause invalid updates be-
cause of inexact search. Therefore we apply early-
update (Collins and Roark, 2004), an instance of
violation-fixing methods (Huang et al., 2012). In
the rest of this paper, we override y and z to denote
prefixes of structures.
In addition to the simple perceptron update, we
also apply k-best MIRA (McDonald et al., 2005),
an online large-margin learning algorithm. During
each update, it keeps the norm of the change to
feature weights w as small as possible, and forces
the margin between y and the k-best candidate z
greater or equal to their loss L(y, z). It is formu-
lated as a quadratic programming problem:
min ?w
new
?w?
s.t. w
new
f(x, y)?w
new
f(x, z) ? L(y, z)
?z ? best
k
(x,w)
We employ the following three loss functions
for comparison:
1847
Freq. Relation Type Event Type Arg-1 Arg-2 Example
159 Physical Transport Artifact Destination He
(arg-1)
was escorted
(trigger)
into Iraq
(arg-2)
.
46 Physical Attack Target Place Many people
(arg-1)
were in the cafe
(arg-2)
during the blast
(trigger)
.
42 Agent-Artifact Attack Attacker Instrument Terrorists
(arg-1)
might use
(trigger)
the devices
(arg-2)
as weapons.
41 Physical Transport Artifact Origin The truck
(arg-1)
was carrying
(trigger)
Syrians fleeing the war in Iraq
(arg-2)
.
33 Physical Meet Entity Place They
(arg-1)
have reunited
(trigger)
with their friends in Norfolk
(arg-2)
.
32 Physical Die Victim Place Two Marines
(arg-1)
were killed
(trigger)
in the fighting in Kut
(arg-2)
.
28 Physical Attack Attacker Place Protesters
(arg-1)
have been clashing
(trigger)
with police in Tehran
(arg-2)
.
26 ORG-Affiliation End-Position Person Entity NBC
(arg-2)
is terminating
(trigger)
freelance reporter Peter Arnett
(arg-1)
.
Table 1: Frequent overlapping relation and event types in the training set.
? The first one is F
1
loss:
L
1
(y, z) = 1?
2 ? |y ? z|
|y|+ |z|
When counting the numbers, we treat each node
and arc as a single unit. For example, in Fig-
ure 1, |y| = 6.
? The second one is 0-1 loss:
L
2
(y, z) =
{
1 y 6= z
0 y = z
It does not discriminate the extent to which z
deviates from y.
? The third loss function counts the difference be-
tween y and z:
L
3
(y, z) = |y|+ |z| ? 2 ? |y ? z|
Similar to F
1
loss function, it penalizes both
missing and false-positive units. The difference
is that it is sensitive to the size of y and z.
2.3 Joint Relation-Event Features
By extracting three core IE components in a joint
search space, we can utilize joint features over
multiple components in addition to factorized fea-
tures in pipelined approaches. In addition to the
features as described in (Li et al., 2013; Li and
Ji, 2014), we can make use of joint features be-
tween relations and events, given the fact that
relations are often ending or starting states of
events (D?olling, 2011). Table 1 shows the most
frequent overlapping relation and event types in
our training data. In each partial structure y
?
dur-
ing the search, if both arguments of a relation par-
ticipate in an event, we compose the correspond-
ing argument roles and relation type as a joint fea-
ture for y
?
. For example, for the structure in Fig-
ure 1, we obtain the following joint relation-event
features:
Attacker Instrument
Agent-Artifact
Attacker Place
Physical
Split Sentences Mentions Relations Triggers Arguments
Train 7.2k 25.7k 4.8k 2.8k 4.5k
Dev 1.7k 6.3k 1.2k 0.7k 1.1k
Test 1.5k 5.3k 1.1k 0.6k 1.0k
Table 2: Data set
0 20 40 60 80 100Number of instances0
2
4
6
8
10
12
14
Freq
uenc
y
Trigger WordsFrame IDs
Figure 2: Distribution of triggers and their frames.
2.4 Semantic Frame Features
One major challenge of constructing information
networks is the data sparsity problem in extract-
ing event triggers. For instance, in the sen-
tence: ?Others were mutilated beyond recogni-
tion.? The Injure trigger ?mutilated? does not oc-
cur in our training data. But there are some sim-
ilar words such as ?stab? and ?smash?. We uti-
lize FrameNet (Baker and Sato, 2003) to solve
this problem. FrameNet is a lexical resource for
semantic frames. Each frame characterizes a ba-
sic type of semantic concept, and contains a num-
ber of words (lexical units) that evoke the frame.
Many frames are highly related with ACE events.
For example, the frame ?Cause harm? is closely
related with Injure event and contains 68 lexical
units such as ?stab?, ?smash? and ?mutilate?.
Figure 2 compares the distributions of trigger
words and their frame IDs in the training data. We
can clearly see that the trigger word distribution
suffers from the long-tail problem, while Frames
reduce the number of triggers which occur only
1848
Methods
Entity Mention (%)
Relation (%)
Event Trigger (%)
Event Argument (%)
P R F
1
P R F
1
P R F
1
P R F
1
Pipelined Baseline
83.6 75.7 79.5
68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7
Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1
Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A
Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6
Joint w/ MIRA w/ F
1
Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4
Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9
Joint w/ MIRA w/ L
3
Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8
Table 3: Overall performance on test set.
once in the training data from 100 to 60 and al-
leviate the sparsity problem. For each token, we
exploit the frames that contain the combination of
its lemma and POS tag as features. For the above
example, ?Cause harm? will be a feature for ?mu-
tilated?. We only consider tokens that appear in
at most 2 frames, and omit the frames that occur
fewer than 20 times in our training data.
3 Experiments
3.1 Data and Evaluation
We use ACE?05 corpus to evaluate our method
with the same data split as in (Li and Ji, 2014). Ta-
ble 2 summarizes the statistics of the data set. We
report the performance of extracting entity men-
tions, relations, event triggers and arguments sep-
arately using the standard F
1
measures as defined
in (Ji and Grishman, 2008; Chan and Roth, 2011):
? An entity mention is correct if its entity type (7
in total) and head offsets are correct.
? A relation is correct if its type (6 in total) and the
head offsets of its two arguments are correct.
? An event trigger is correct if its event subtype
(33 in total) and offsets are correct.
? An argument link is correct if its event subtype,
offsets and role match those of any of the refer-
ence argument mentions.
In this paper we focus on entity arguments while
disregard values and time expressions because
they can be most effectively extracted by hand-
crafted patterns (Chang and Manning, 2012).
3.2 Results
Based on the results of our development set, we
trained all models with 21 iterations and chose the
beam size to be 8. For the k-best MIRA updates,
we set k as 3. Table 3 compares the overall perfor-
mance of our approaches and baseline methods.
Our joint model with perceptron update out-
performs the state-of-the-art pipelined approach
in (Li et al., 2013; Li and Ji, 2014), and further
improves the joint event extraction system in (Li
et al., 2013) (p < 0.05 for entity mention extrac-
tion, and p < 0.01 for other subtasks, accord-
ing to Wilcoxon Signed RankTest). For the k-
best MIRA update, the L
3
loss function achieved
better performance than F
1
loss and 0-1 loss on
all sub-tasks except event argument extraction. It
also significantly outperforms perceptron update
on relation extraction and event argument extrac-
tion (p < 0.01). It is particularly encouraging to
see the end output of an IE system (event argu-
ments) has made significant progress (12.2% ab-
solute gain over traditional pipelined approach).
3.3 Discussions
3.3.1 Feature Study
Rank Feature Weight
1 Frame=Killing Die 0.80
2 Frame=Travel Transport 0.61
3 Physical(Artifact, Destination) 0.60
4 w
1
=?home? Transport 0.59
5 Frame=Arriving Transport 0.54
6 ORG-AFF(Person, Entity) 0.48
7 Lemma=charge Charge-Indict 0.45
8 Lemma=birth Be-Born 0.44
9 Physical(Artifact,Origin) 0.44
10 Frame=Cause harm Injure 0.43
Table 4: Top Features about Event Triggers.
Table 4 lists the weights of the most significant
features about event triggers. The 3
rd
, 6
th
, and
9
th
rows are joint relation-event features. For in-
stance, Physical(Artifact, Destination) means the
arguments of a Physical relation participate in a
Transport event as Artifact and Destination. We
can see that both the joint relation-event features
1849
and FrameNet based features are of vital impor-
tance to event trigger labeling. We tested the im-
pact of each type of features by excluding them in
the experiments of ?MIRA w/ L
3
loss?. We found
that FrameNet based features provided 0.8% and
2.2% F
1
gains for event trigger and argument la-
beling respectively. Joint relation-event features
also provided 0.6% F
1
gain for relation extraction.
3.3.2 Remaining Challenges
Event trigger labeling remains a major bottleneck.
In addition to the sparsity problem, the remain-
ing errors suggest to incorporate external world
knowledge. For example, some words act as trig-
gers for some certain types of events only when
they appear together with some particular argu-
ments:
? ?Williams picked up the child again and this
time, threw
Attack
her out the window.?
The word ?threw? is used as an Attack event
trigger because the Victim argument is a ?child?.
? ?Ellison to spend $10.3 billion to get
Merge Org
his company.? The common word ?get? is
tagged as a trigger of Merge Org, because its
object is ?company?.
? ?We believe that the likelihood of them
using
Attack
those weapons goes up.?
The word ?using? is used as an Attack event
trigger because the Instrument argument is
?weapons?.
Another challenge is to distinguish physical and
non-physical events. For example, in the sentence:
? ?we are paying great attention to their ability to
defend
Attack
on the ground.?,
our system fails to extract ?defend? as an Attack
trigger. In the training data, ?defend? appears mul-
tiple times, but none of them is tagged as Attack.
For instance, in the sentence:
? ?North Korea could do everything to defend it-
self. ?
?defend? is not an Attack trigger since it does not
relate to physical actions in a war. This challenge
calls for deeper understanding of the contexts.
Finally, some pronouns are used to refer to ac-
tual events. Event coreference is necessary to rec-
ognize them correctly. For example, in the follow-
ing two sentences from the same document:
? ?It?s important that people all over the world
know that we don?t believe in the war
Attack
.?,
? ?Nobody questions whether this
Attack
is right
or not.?
?this? refers to ?war? in its preceding contexts.
Without event coreference resolution, it is difficult
to tag it as an Attack event trigger.
4 Conclusions
We presented the first joint model that effectively
extracts entity mentions, relations and events
based on a unified representation: information
networks. Experiment results on ACE?05 cor-
pus demonstrate that our approach outperforms
pipelined method, and improves event-argument
performance significantly over the state-of-the-art.
In addition to the joint relation-event features, we
demonstrated positive impact of using FrameNet
to handle the sparsity problem in event trigger la-
beling.
Although our primary focus in this paper is in-
formation extraction in the ACE paradigm, we be-
lieve that our framework is general to improve
other tightly coupled extraction tasks by capturing
the inter-dependencies in the joint search space.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award, Disney Research Award
and RPI faculty start-up grant. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.
References
Collin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proc. ACL, pages 161?164.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551?560.
1850
Angel X. Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proc. LREC, pages 3735?3740.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
Johannes D?olling. 2011. Aspectual coercion and even-
tuality structure. pages 189?226.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Proc.
ACL.
Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proc. ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73?82.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. ACL, pages 91?98.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proc. EMNLP.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic ap-
proach to bio-molecular event extraction. In Proc.
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing: Shared Task.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640?1649.
1851
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73?82,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Event Extraction via Structured Prediction with Global Features
Qi Li Heng Ji Liang Huang
Departments of Computer Science and Linguistics
The Graduate Center and Queens College
City University of New York
New York, NY 10016, USA
{liqiearth, hengjicuny, liang.huang.sh}@gmail.com
Abstract
Traditional approaches to the task of ACE
event extraction usually rely on sequential
pipelines with multiple stages, which suf-
fer from error propagation since event trig-
gers and arguments are predicted in isola-
tion by independent local classifiers. By
contrast, we propose a joint framework
based on structured prediction which ex-
tracts triggers and arguments together so
that the local predictions can be mutu-
ally improved. In addition, we propose
to incorporate global features which ex-
plicitly capture the dependencies of multi-
ple triggers and arguments. Experimental
results show that our joint approach with
local features outperforms the pipelined
baseline, and adding global features fur-
ther improves the performance signifi-
cantly. Our approach advances state-of-
the-art sentence-level event extraction, and
even outperforms previous argument la-
beling methods which use external knowl-
edge from other sentences and documents.
1 Introduction
Event extraction is an important and challeng-
ing task in Information Extraction (IE), which
aims to discover event triggers with specific types
and their arguments. Most state-of-the-art ap-
proaches (Ji and Grishman, 2008; Liao and Gr-
ishman, 2010; Hong et al, 2011) use sequential
pipelines as building blocks, which break down
the whole task into separate subtasks, such as
trigger identification/classification and argument
identification/classification. As a common draw-
back of the staged architecture, errors in upstream
component are often compounded and propagated
to the downstream classifiers. The downstream
components, however, cannot impact earlier deci-
sions. For example, consider the following sen-
tences with an ambiguous word ?fired?:
(1) In Baghdad, a cameraman died when an
American tank fired on the Palestine Hotel.
(2) He has fired his air defense chief .
In sentence (1), ?fired? is a trigger of type Attack.
Because of the ambiguity, a local classifier may
miss it or mislabel it as a trigger of End-Position.
However, knowing that ?tank? is very likely to be
an Instrument argument of Attack events, the cor-
rect event subtype assignment of ?fired? is obvi-
ously Attack. Likewise, in sentence (2), ?air de-
fense chief? is a job title, hence the argument clas-
sifier is likely to label it as an Entity argument for
End-Position trigger.
In addition, the local classifiers are incapable
of capturing inter-dependencies among multiple
event triggers and arguments. Consider sentence
(1) again. Figure 1 depicts the corresponding
event triggers and arguments. The dependency be-
tween ?fired? and ?died? cannot be captured by the
local classifiers, which may fail to attach ?camera-
man? to ?fired? as a Target argument. By using
global features, we can propagate the Victim ar-
gument of the Die event to the Target argument
of the Attack event. As another example, know-
ing that an Attack event usually only has one At-
tacker argument, we could penalize assignments
in which one trigger has more than one Attacker.
Such global features cannot be easily exploited by
a local classifier.
Therefore, we take a fresh look at this prob-
lem and formulate it, for the first time, as a struc-
tured learning problem. We propose a novel joint
event extraction algorithm to predict the triggers
and arguments simultaneously, and use the struc-
tured perceptron (Collins, 2002) to train the joint
model. This way we can capture the dependencies
between triggers and argument as well as explore
73
In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.
AttackDie
Instrument
Place
Victim
Target
Instrument
Target
Place
Figure 1: Event mentions of example (1). There are two event mentions that share three arguments,
namely the Die event mention triggered by ?died?, and the Attack event mention triggered by ?fired?.
arbitrary global features over multiple local pre-
dictions. However, different from easier tasks such
as part-of-speech tagging or noun phrase chunking
where efficient dynamic programming decoding is
feasible, here exact joint inference is intractable.
Therefore we employ beam search in decoding,
and train the model using the early-update percep-
tron variant tailored for beam search (Collins and
Roark, 2004; Huang et al, 2012).
We make the following contributions:
1. Different from traditional pipeline approach,
we present a novel framework for sentence-
level event extraction, which predicts triggers
and their arguments jointly (Section 3).
2. We develop a rich set of features for event
extraction which yield promising perfor-
mance even with the traditional pipeline
(Section 3.4.1). In this paper we refer to them
as local features.
3. We introduce various global features to ex-
ploit dependencies among multiple triggers
and arguments (Section 3.4.2). Experi-
ments show that our approach outperforms
the pipelined approach with the same set of
local features, and significantly advances the
state-of-the-art with the addition of global
features which brings a notable further im-
provement (Section 4).
2 Event Extraction Task
In this paper we focus on the event extraction task
defined in Automatic Content Extraction (ACE)
evaluation.1 The task defines 8 event types and
33 subtypes such as Attack, End-Position etc. We
introduce the terminology of the ACE event ex-
traction that we used in this paper:
1http://projects.ldc.upenn.edu/ace/
? Event mention: an occurrence of an event
with a particular type and subtype.
? Event trigger: the word most clearly ex-
presses the event mention.
? Event argument: an entity mention, tempo-
ral expression or value (e.g. Job-Title) that
serves as a participant or attribute with a spe-
cific role in an event mention.
? Event mention: an instance that includes one
event trigger and some arguments that appear
within the same sentence.
Given an English text document, an event ex-
traction system should predict event triggers with
specific subtypes and their arguments from each
sentence. Figure 1 depicts the event triggers and
their arguments of sentence (1) in Section 1. The
outcome of the entire sentence can be considered a
graph in which each argument role is represented
as a typed edge from a trigger to its argument.
In this work, we assume that argument candi-
dates such as entities are part of the input to the
event extraction, and can be from either gold stan-
dard or IE system output.
3 Joint Framework for Event Extraction
Based on the hypothesis that facts are inter-
dependent, we propose to use structured percep-
tron with inexact search to jointly extract triggers
and arguments that co-occur in the same sentence.
In this section, we will describe the training and
decoding algorithms for this model.
3.1 Structured perceptron with beam search
Structured perceptron is an extension to the stan-
dard linear perceptron for structured prediction,
which was proposed in (Collins, 2002). Given a
sentence instance x ? X , which in our case is a
sentence with argument candidates, the structured
perceptron involves the following decoding prob-
74
lem which finds the best configuration z ? Y ac-
cording to the current model w:
z = argmax
y??Y(x)
w ? f(x, y?) (1)
where f(x, y?) represents the feature vector for in-
stance x along with configuration y?.
The perceptron learns the model w in an on-
line fashion. Let D = {(x(j), y(j))}nj=1 be the set
of training instances (with j indexing the current
training instance). In each iteration, the algorithm
finds the best configuration z for x under the cur-
rent model (Eq. 1). If z is incorrect, the weights
are updated as follows:
w = w + f(x, y)? f(x, z) (2)
The key step of the training and test is the de-
coding procedure, which aims to search for the
best configuration under the current parameters. In
simpler tasks such as part-of-speech tagging and
noun phrase chunking, efficient dynamic program-
ming algorithms can be employed to perform ex-
act inference. Unfortunately, it is intractable to
perform the exact search in our framework be-
cause: (1) by jointly modeling the trigger labeling
and argument labeling, the search space becomes
much more complex. (2) we propose to make use
of arbitrary global features, which makes it infea-
sible to perform exact inference efficiently.
To address this problem, we apply beam-search
along with early-update strategy to perform inex-
act decoding. Collins and Roark (2004) proposed
the early-update idea, and Huang et al (2012) later
proved its convergence and formalized a general
framework which includes it as a special case. Fig-
ure 2 describes the skeleton of perceptron train-
ing algorithm with beam search. In each step of
the beam search, if the prefix of oracle assign-
ment y falls out from the beam, then the top re-
sult in the beam is returned for early update. One
could also use the standard-update for inference,
however, with highly inexact search the standard-
update generally does not work very well because
of ?invalid updates?, i.e., updates that do not fix a
violation (Huang et al, 2012). In Section 4.5 we
will show that the standard perceptron introduces
many invalid updates especially with smaller beam
sizes, also observed by Huang et al (2012).
To reduce overfitting, we used averaged param-
eters after training to decode test instances in our
experiments. The resulting model is called aver-
aged perceptron (Collins, 2002).
Input: Training set D = {(x(j), y(j))}ni=1,
maximum iteration number T
Output: Model parameters w
1 Initialization: Set w = 0;
2 for t? 1...T do
3 foreach (x, y) ? D do
4 z ? beamSearch (x, y,w)
5 if z 6= y then
6 w? w + f(x, y[1:|z|])? f(x, z)
Figure 2: Perceptron training with beam-
search (Huang et al, 2012). Here y[1:i] de-
notes the prefix of y that has length i, e.g.,
y[1:3] = (y1, y2, y3).
3.2 Label sets
Here we introduce the label sets for trigger and ar-
gument in the model. We use L ? {?} to denote
the trigger label alphabet, where L represents the
33 event subtypes, and ? indicates that the token
is not a trigger. Similarly, R ? {?} denotes the
argument label sets, whereR is the set of possible
argument roles, and ? means that the argument
candidate is not an argument for the current trig-
ger. It is worth to note that the set R of each par-
ticular event subtype is subject to the entity type
constraints defined in the official ACE annotation
guideline2. For example, the Attacker argument
for an Attack event can only be one of PER, ORG
and GPE (Geo-political Entity).
3.3 Decoding
Let x = ?(x1, x2, ..., xs), E? denote the sentence
instance, where xi represents the i-th token in the
sentence and E = {ek}mk=1 is the set of argument
candidates. We use
y = (t1, a1,1, . . . , a1,m, . . . , ts, as,1, . . . , as,m)
to denote the corresponding gold standard struc-
ture, where ti represents the trigger assignment for
the token xi, and ai,k represents the argument role
label for the edge between xi and argument candi-
date ek.
2http://projects.ldc.upenn.edu/ace/docs/English-Events-
Guidelines v5.4.3.pdf
75
y = (t1, a1,1, a1,2, t2, a2,1, a2,2,| {z }
arguments for x2
t3, a3,1, a3,2)
g(1) g(2) h(2, 1) h(3, 2)
Figure 3: Example notation with s = 3,m = 2.
For simplicity, throughout this paper we use
yg(i) and yh(i,k) to represent ti and ai,k, respec-
tively. Figure 3 demonstrates the notation with
s = 3 and m = 2. The variables for the toy sen-
tence ?Jobs founded Apple? are as follows:
x = ?(Jobs,
x2? ?? ?
founded, Apple),
E? ?? ?
{JobsPER,AppleORG}?
y = (?,?,?, Start Org? ?? ?
t2
, Agent, Org? ?? ?
args for founded
,?,?,?)
Figure 4 describes the beam-search procedure
with early-update for event extraction. During
each step with token i, there are two sub-steps:
? Trigger labeling We enumerate all possible
trigger labels for the current token. The linear
model defined in Eq. (1) is used to score each
partial configuration. Then the K-best par-
tial configurations are selected to the beam,
assuming the beam size is K.
? Argument labeling After the trigger label-
ing step, we traverse all configurations in the
beam. Once a trigger label for xi is found in
the beam, the decoder searches through the
argument candidates E to label the edges be-
tween each argument candidate and the trig-
ger. After labeling each argument candidate,
we again score each partial assignment and
select the K-best results to the beam.
After the second step, the rank of different trigger
assignments can be changed because of the argu-
ment edges. Likewise, the decision on later argu-
ment candidates may be affected by earlier argu-
ment assignments.
The overall time complexity for decoding is
O(K ? s ?m).
3.4 Features
In this framework, we define two types of fea-
tures, namely local features and global features.
We first introduce the definition of local and global
features in this paper, and then describe the im-
plementation details later. Recall that in the lin-
ear model defined in Eq. (1), f(x, y) denotes the
features extracted from the input instance x along
Input: Instance x = ?(x1, x2, ..., xs), E? and
the oracle output y if for training.
K: Beam size.
L ? {?}: trigger label alphabet.
R? {?}: argument label alphabet.
Output: 1-best prediction z for x
1 Set beam B ? [] /*empty configuration*/
2 for i? 1...s do
3 buf ? {z? ? l | z? ? B, l ? L ? {?}}
B ?K-best(buf )
4 if y[1:g(i)] 6? B then
5 return B[0] /*for early-update*/
6 for ek ? E do /*search for arguments*/
7 buf ? ?
8 for z? ? B do
9 buf ? buf ? {z? ? ?}
10 if z?g(i) 6= ? then /*xi is a trigger*/
11 buf ? buf ? {z? ? r | r ? R}
12 B ?K-best(buf )
13 if y[1:h(i,k)] 6? B then
14 return B[0] /*for early-update*/
15 return B[0]
Figure 4: Decoding algorithm for event extrac-
tion. z?l means appending label l to the end of
z. During test, lines 4-5 & 13-14 are omitted.
with configuration y. In general, each feature in-
stance f in f is a function f : X ? Y ? R, which
maps x and y to a feature value. Local features are
only related to predictions on individual trigger or
argument. In the case of unigram tagging for trig-
ger labeling, each local feature takes the form of
f(x, i, yg(i)), where i denotes the index of the cur-
rent token, and yg(i) is its trigger label. In practice,
it is convenient to define the local feature function
as an indicator function, for example:
f1(x, i, yg(i)) =
{
1 if yg(i) = Attack and xi = ?fire?
0 otherwise
The global features, by contrast, involve longer
range of the output structure. Formally,
each global feature function takes the form of
f(x, i, k, y), where i and k denote the indices
of the current token and argument candidate in
decoding, respectively. The following indicator
function is a simple example of global features:
f101(x, i, k, y) =
?
??
??
1 if yg(i) = Attack and
y has only one ?Attacker?
0 otherwise
76
Category Type Feature Description
Trigger
Lexical
1. unigrams/bigrams of the current and context words within the window of size 2
2. unigrams/bigrams of part-of-speech tags of the current and context words within the
window of size 2
3. lemma and synonyms of the current token
4. base form of the current token extracted from Nomlex (Macleod et al, 1998)
5. Brown clusters that are learned from ACE English corpus (Brown et al, 1992; Miller et
al., 2004; Sun et al, 2011). We used the clusters with prefixes of length 13, 16 and 20 for
each token.
Syntactic
6. dependent and governor words of the current token
7. dependency types associated the current token
8. whether the current token is a modifier of job title
9. whether the current token is a non-referential pronoun
Entity
Information
10. unigrams/bigrams normalized by entity types
11. dependency features normalized by entity types
12. nearest entity type and string in the sentence/clause
Argument
Basic
1. context words of the entity mention
2. trigger word and subtype
3. entity type, subtype and entity role if it is a geo-political entity mention
4. entity mention head, and head of any other name mention from co-reference chain
5. lexical distance between the argument candidate and the trigger
6. the relative position between the argument candidate and the trigger: {before, after,
overlap, or separated by punctuation}
7. whether it is the nearest argument candidate with the same type
8. whether it is the only mention of the same entity type in the sentence
Syntactic
9. dependency path between the argument candidate and the trigger
10. path from the argument candidate and the trigger in constituent parse tree
11. length of the path between the argument candidate and the trigger in dependency graph
12. common root node and its depth of the argument candidate and parse tree
13. whether the argument candidate and the trigger appear in the same clause
Table 1: Local features.
3.4.1 Local features
In general there are two kinds of local features:
Trigger features The local feature func-
tion for trigger labeling can be factorized as
f(x, i, yg(i)) = p(x, i) ? q(yg(i)), where p(x, i) is
a predicate about the input, which we call text fea-
ture, and q(yg(i)) is a predicate on the trigger label.
In practice, we define two versions of q(yg(i)):
q0(yg(i)) = yg(i) (event subtype)
q1(yg(i)) = event type of yg(i)
q1(yg(i)) is a backoff version of the standard un-
igram feature. Some text features for the same
event type may share a certain distributional sim-
ilarity regardless of the subtypes. For example,
if the nearest entity mention is ?Company?, the
current token is likely to be Personnel no matter
whether it is End-Postion or Start-Position.
Argument features Similarly, the local fea-
ture function for argument labeling can be rep-
resented as f(x, i, k, yg(i), yh(i,k)) = p(x, i, k) ?
q(yg(i), yh(i,k)), where yh(i,k) denotes the argu-
ment assignment for the edge between trigger
word i and argument candidate ek. We define two
versions of q(yg(i), yh(i,k)):
q0(yg(i), yh(i,k)) =
?
??
??
yh(i,k) if yh(i,k) is Place,
Time or None
yg(i) ? yh(i,k) otherwise
q1(yg(i), yh(i,k)) =
{
1 if yh(i,k) 6=None
0 otherwise
It is notable that Place and Time arguments are
applicable and behave similarly to all event sub-
types. Therefore features for these arguments are
not conjuncted with trigger labels. q1(yh(i,k)) can
be considered as a backoff version of q0(yh(i,k)),
which does not discriminate different argument
roles but only focuses on argument identification.
Table 1 summarizes the text features about the in-
put for trigger and argument labeling. In our ex-
periments, we used the Stanford parser (De Marn-
effe et al, 2006) to create dependency parses.
3.4.2 Global features
Table 2 summarizes the 8 types of global features
we developed in this work. They can be roughly
divided into the following two categories:
77
Category Feature Description
Trigger
1. bigram of trigger types occur in the same sentence or the same clause
2. binary feature indicating whether synonyms in the same sentence have the same trigger label
3. context and dependency paths between two triggers conjuncted with their types
Argument
4. context and dependency features about two argument candidates which share the same role within the
same event mention
5. features about one argument candidate which plays as arguments in two event mentions in the same
sentence
6. features about two arguments of an event mention which are overlapping
7. the number of arguments with each role type of an event mention conjuncted with the event subtype
8. the pairs of time arguments within an event mention conjuncted with the event subtype
Table 2: Global features.
Transport
(transport)
Entity
(women)
Entity
(children)
Art
ifac
t Artifact
conj and
(a)
Entity
(cameramen)
Die
(died)
Attack
(fired)
Vic
tim
Target
advcl
(b)
End-Position
(resigned)
Entity Entity
[co-chief executive of [Vivendi Universal Entertainment]]
Pos
itio
n Entity
Overlapping
(c)
Figure 5: Illustration of global features (4-6) in Table 2.
Event Probability
Attack 0.34
Die 0.14
Transport 0.08
Injure 0.04
Meet 0.02
Table 3: Top 5 event subtypes that co-occur with
Attack event in the same sentence.
Trigger global feature This type of feature
captures the dependencies between two triggers
within the same sentence. For instance: feature (1)
captures the co-occurrence of trigger types. This
kind of feature is motivated by the fact that two
event mentions in the same sentence tend to be se-
mantically coherent. As an example, from Table 3
we can see that Attack event often co-occur with
Die event in the same sentence, but rarely co-occur
with Start-Position event. Feature (2) encourages
synonyms or identical tokens to have the same la-
bel. Feature (3) exploits the lexical and syntactic
relation between two triggers. A simple example
is whether an Attack trigger and a Die trigger are
linked by the dependency relation conj and.
Argument global feature This type of feature
is defined over multiple arguments for the same
or different triggers. Consider the following sen-
tence:
(3) Trains running to southern Sudan were used
to transport abducted women and children.
The Transport event mention ?transport? has
two Artifact arguments, ?women? and ?chil-
dren?. The dependency edge conj and be-
tween ?women? and ?children? indicates that
they should play the same role in the event men-
tion. The triangle structure in Figure 5(a) is an ex-
ample of feature (4) for the above example. This
feature encourages entities that are linked by de-
pendency relation conj and to play the same role
Artifact in any Transport event.
Similarly, Figure 5(b) depicts an example of
feature (5) for sentence (1) in Section 1. In this ex-
ample, an entity mention is Victim argument to Die
event and Target argument to Attack event, and the
two event triggers are connected by the typed de-
pendency advcl. Here advcl means that the word
?fired? is an adverbial clause modier of ?died?.
Figure 5(c) shows an example of feature (6) for
the following sentence:
(4) Barry Diller resigned as co-chief executive of
Vivendi Universal Entertainment.
The job title ?co-chief executive of Vivendi Uni-
versal Entertainment? overlaps with the Orga-
nization mention ?Vivendi Universal Entertain-
ment?. The feature in the triangle shape can be
considered as a soft constraint such that if a Job-
Title mention is a Position argument to an End-
Position trigger, then the Organization mention
78
which appears at the end of it should be labeled
as Entity argument for the same trigger.
Feature (7-8) are based on the statistics about
different arguments for the same trigger. For in-
stance, in many cases, a trigger can only have one
Place argument. If a partial configuration mis-
takenly classifies more than one entity mention as
Place arguments for the same trigger, then it will
be penalized.
4 Experiments
4.1 Data set and evaluation metric
We utilized the ACE 2005 corpus as our testbed.
For comparison, we used the same test set with 40
newswire articles (672 sentences) as in (Ji and Gr-
ishman, 2008; Liao and Grishman, 2010) for the
experiments, and randomly selected 30 other doc-
uments (863 sentences) from different genres as
the development set. The rest 529 documents (14,
840 sentences) are used for training.
Following previous work (Ji and Grishman,
2008; Liao and Grishman, 2010; Hong et al,
2011), we use the following criteria to determine
the correctness of an predicted event mention:
? A trigger is correct if its event subtype and
offsets match those of a reference trigger.
? An argument is correctly identified if its event
subtype and offsets match those of any of the
reference argument mentions.
? An argument is correctly identified and clas-
sified if its event subtype, offsets and argu-
ment role match those of any of the reference
argument mentions.
Finally we use Precision (P), Recall (R) and F-
measure (F1) to evaluate the overall performance.
4.2 Baseline system
Chen and Ng (2012) have proven that perform-
ing identification and classification in one step is
better than two steps. To compare our proposed
method with the previous pipelined approaches,
we implemented two Maximum Entropy (Max-
Ent) classifiers for trigger labeling and argument
labeling respectively. To make a fair comparison,
the feature sets in the baseline are identical to the
local text features we developed in our framework
(see Figure 1).
4.3 Training curves
We use the harmonic mean of the trigger?s F1
measure and argument?s F1 measure to measure
the performance on the development set.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21# of training iteration0.44
0.46
0.48
0.50
0.52
0.54
0.56
0.58
0.60
Harm
onic
 me
an
local+globallocal
Figure 6: Training curves on dev set.
Figure 6 shows the training curves of the aver-
aged perceptron with respect to the performance
on the development set when the beam size is 4.
As we can see both curves converge around itera-
tion 20 and the global features improve the over-
all performance, compared to its counterpart with
only local features. Therefore we set the number
of iterations as 20 in the remaining experiments.
4.4 Impact of beam size
The beam size is an important hyper parameter in
both training and test. Larger beam size will in-
crease the computational cost while smaller beam
size may reduce the performance. Table 4 shows
the performance on the development set with sev-
eral different beam sizes. When beam size = 4, the
algorithm achieved the highest performance on the
development set with trigger F1 = 67.9, argument
F1 = 51.5, and harmonic mean = 58.6. When
the size is increased to 32, the accuracy was not
improved. Based on this observation, we chose
beam size as 4 for the remaining experiments.
4.5 Early-update vs. standard-update
Huang et al (2012) define ?invalid update? to be
an update that does not fix a violation (and instead
reinforces the error), and show that it strongly
(anti-)correlates with search quality and learning
quality. Figure 7 depicts the percentage of in-
valid updates in standard-update with and with-
out global features, respectively. With global fea-
tures, there are numerous invalid updates when the
79
Beam size 1 2 4 8 16 32
Training time (sec) 993 2,034 3,982 8,036 15,878 33,026
Harmonic mean 57.6 57.7 58.6 58.0 57.8 57.8
Table 4: Comparison of training time and accuracy on the dev set.
1 2 4 8 16 32beam size0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
% of
 inva
lid u
pdat
es
local+globallocal
Figure 7: Percentage of the so-called ?invalid up-
dates? (Huang et al, 2012) in standard perceptron.
Strategy F1 on Dev F1 on TestTrigger Arg Trigger Arg
Standard (b = 1) 68.3 47.4 64.4 49.8
Early (b = 1) 68.9 49.5 65.2 52.1
Standard (b = 4) 68.4 50.5 67.1 51.4
Early (b = 4) 67.9 51.5 67.5 52.7
Table 5: Comparison between the performance
(%) of standard-update and early-update with
global features. Here b stands for beam size.
beam size is small. The ratio decreases mono-
tonically as beam size increases. The model with
only local features made much smaller numbers
of invalid updates, which suggests that the use of
global features makes the search problem much
harder. This observation justify the application of
early-update in this work. To further investigate
the difference between early-update and standard-
update, we tested the performance of both strate-
gies, which is summarized in Table 5. As we can
see the performance of standard-update is gener-
ally worse than early-update. When the beam size
is increased (b = 4), the gap becomes smaller as
the ratio of invalid updates is reduced.
4.6 Overall performance
Table 6 shows the overall performance on the blind
test set. In addition to our baseline, we compare
against the sentence-level system reported in Hong
et al (2011), which, to the best of our knowledge,
is the best-reported system in the literature based
on gold standard argument candidates. The pro-
posed joint framework with local features achieves
comparable performance for triggers and outper-
forms the staged baseline especially on arguments.
By adding global features, the overall performance
is further improved significantly. Compared to
the staged baseline, it gains 1.6% improvement
on trigger?s F-measure and 8.8% improvement on
argument?s F-measure. Remarkably, compared to
the cross-entity approach reported in (Hong et al,
2011), which attained 68.3% F1 for triggers and
48.3% for arguments, our approach with global
features achieves even better performance on ar-
gument labeling although we only used sentence-
level information.
We also tested the performance with argument
candidates automatically extracted by a high-
performing name tagger (Li et al, 2012b) and an
IE system (Grishman et al, 2005). The results
are summarized in Table 7. The joint approach
with global features significantly outperforms the
baseline and the model with only local features.
We also show that it outperforms the sentence-
level baseline reported in (Ji and Grishman, 2008;
Liao and Grishman, 2010), both of which at-
tained 59.7% F1 for triggers and 36.6% for argu-
ments. Our approach aims to tackle the problem of
sentence-level event extraction, thereby only used
intra-sentential evidence. Nevertheless, the perfor-
mance of our approach is still comparable with the
best-reported methods based on cross-document
and cross-event inference (Ji and Grishman, 2008;
Liao and Grishman, 2010).
5 Related Work
Most recent studies about ACE event extraction
rely on staged pipeline which consists of separate
local classifiers for trigger labeling and argument
labeling (Grishman et al, 2005; Ahn, 2006; Ji and
Grishman, 2008; Chen and Ji, 2009; Liao and Gr-
ishman, 2010; Hong et al, 2011; Li et al, 2012a;
Chen and Ng, 2012). To the best of our knowl-
edge, our work is the first attempt to jointly model
these two ACE event subtasks.
80
Methods
Trigger
Identification (%)
Trigger Identification
+ classification (%)
Argument
Identification (%) Argument Role (%)P R F1 P R F1 P R F1 P R F1
Sentence-level in Hong et al (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5
Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9
Joint w/ local features 77.4 62.3 69.0 73.7 59.3 65.7 69.7 39.6 50.5 64.1 36.5 46.5
Joint w/ local + global features 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
Cross-entity in Hong et al (2011)? N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3
Table 6: Overall performance with gold-standard entities, timex, and values. ?beyond sentence level.
Methods Trigger F1 Arg F1
Ji and Grishman (2008)
cross-doc Inference
67.3 42.6
Ji and Grishman (2008)
sentence-level
59.7 36.6
MaxEnt classifiers 64.7 (?1.2) 33.7 (?10.2)
Joint w/ local 63.7 (?2.0) 35.8 (?10.7)
Joint w/ local + global 65.6 (?1.9) 41.8 (?10.9)
Table 7: Overall performance (%) with predicted
entities, timex, and values. ? indicates the perfor-
mance drop from experiments with gold-standard
argument candidates (see Table 6).
For the Message Understanding Conference
(MUC) and FAS Program for Monitoring Emerg-
ing Diseases (ProMED) event extraction tasks,
Patwardhan and Riloff (2009) proposed a proba-
bilistic framework to extract event role fillers con-
ditioned on the sentential event occurrence. Be-
sides having different task definitions, the key
difference from our approach is that their role
filler recognizer and sentential event recognizer
are trained independently but combined in the test
stage. Our experiments, however, have demon-
strated that it is more advantageous to do both
training and testing with joint inference.
There has been some previous work on joint
modeling for biomedical events (Riedel and Mc-
Callum, 2011a; Riedel et al, 2009; McClosky et
al., 2011; Riedel and McCallum, 2011b). (Mc-
Closky et al, 2011) is most closely related to our
approach. They casted the problem of biomedi-
cal event extraction as a dependency parsing prob-
lem. The key assumption that event structure can
be considered as trees is incompatible with ACE
event extraction. In addition, they used a separate
classifier to predict the event triggers before ap-
plying the parser, while we extract the triggers and
argument jointly. Finally, the features in the parser
are edge-factorized. To exploit global features,
they applied a MaxEnt-based global re-ranker. In
comparison, our approach is a unified framework
based on beam search, which allows us to exploit
arbitrary global features efficiently.
6 Conclusions and Future Work
We presented a joint framework for ACE event ex-
traction based on structured perceptron with inex-
act search. As opposed to traditional pipelined
approaches, we re-defined the task as a struc-
tured prediction problem. The experiments proved
that the perceptron with local features outperforms
the staged baseline and the global features further
improve the performance significantly, surpassing
the current state-of-the-art by a large margin.
As shown in Table 7, the overall performance
drops substantially when using predicted argu-
ment candidates. To improve the accuracy of end-
to-end IE system, we plan to develop a complete
joint framework to recognize entities together with
event mentions for future work. Also we are inter-
ested in applying this framework to other IE tasks
such as relation extraction.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149,
U.S. NSF EAGER Award under Grant No. IIS-
1144111, U.S. DARPA Award No. FA8750-13-2-
0041 in the ?Deep Exploration and Filtering of
Text? (DEFT) Program, a CUNY Junior Faculty
Award, and Queens College equipment funds. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
81
References
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, pages 1?8.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Zheng Chen and Heng Ji. 2009. Language specific
issue and feature exploration in chinese event ex-
traction. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, pages 209?212.
Chen Chen and Vincent Ng. 2012. Joint modeling for
chinese event extraction with rich linguistic features.
In COLING, pages 529?544.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description.
In Proceedings of ACE 2005 Evaluation Workshop.
Washington.
Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of ACL, pages 1127?1136.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254?262.
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-
bin Hou. 2012a. Employing compositional seman-
tics and discourse consistency in chinese event ex-
traction. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1006?1016.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and
Fei Huang. 2012b. Joint bilingual name tagging for
parallel corpora. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 1727?1731.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of ACL, pages 789?797.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX, volume 98, pages 187?193.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL, pages 1626?1635.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL,
volume 4, pages 337?342.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1, pages 151?
160.
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1?
12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, pages 46?50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings
of the Workshop on Current Trends in Biomedical
Natural Language Processing: Shared Task, pages
41?49.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
521?529.
82
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?614,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Name-aware Machine Translation
Haibo Li? Jing Zheng? Heng Ji? Qi Li? Wen Wang?
? Computer Science Department and Linguistics Department
Queens College and Graduate Center, City University of New York
New York, NY, USA 10016
{lihaibo.c, hengjicuny, liqiearth}@gmail.com
? Speech Technology & Research Laboratory
SRI International
Menlo Park, CA, USA 94025
{zj, wwang}@speech.sri.com
Abstract
We propose a Name-aware Machine
Translation (MT) approach which can
tightly integrate name processing into MT
model, by jointly annotating parallel cor-
pora, extracting name-aware translation
grammar and rules, adding name phrase
table and name translation driven decod-
ing. Additionally, we also propose a new
MT metric to appropriately evaluate the
translation quality of informative words,
by assigning different weights to differ-
ent words according to their importance
values in a document. Experiments on
Chinese-English translation demonstrated
the effectiveness of our approach on en-
hancing the quality of overall translation,
name translation and word alignment over
a high-quality MT baseline1.
1 Introduction
A shrinking fraction of the world?s Web pages are
written in English, therefore the ability to access
pages across a range of languages is becoming in-
creasingly important. This need can be addressed
in part by cross-lingual information access tasks
such as entity linking (McNamee et al, 2011; Cas-
sidy et al, 2012), event extraction (Hakkani-Tur
et al, 2007), slot filling (Snover et al, 2011) and
question answering (Parton et al, 2009; Parton
and McKeown, 2010). A key bottleneck of high-
quality cross-lingual information access lies in the
performance of Machine Translation (MT). Tradi-
tional MT approaches focus on the fluency and
accuracy of the overall translation but fall short
in their ability to translate certain content word-
s including critical information, especially names.
1Some of the resources and open source programs devel-
oped in this work are made freely available for research pur-
pose at http://nlp.cs.qc.cuny.edu/NAMT.tgz
A typical statistical MT system can only trans-
late 60% person names correctly (Ji et al, 2009).
Incorrect segmentation and translation of names
which often carry central meanings of a sentence
can also yield incorrect translation of long con-
texts. Names have been largely neglected in the
prior MT research due to the following reasons:
? The current dominant automatic MT scoring
metrics (such as Bilingual Evaluation Under-
study (BLEU) (Papineni et al, 2002)) treat
all words equally, but names have relative low
frequency in text (about 6% in newswire and
only 3% in web documents) and thus are vast-
ly outnumbered by function words and com-
mon nouns, etc..
? Name translations pose a greater complexity
because the set of names is open and highly
dynamic. It is also important to acknowledge
that there are many fundamental differences
between the translation of names and other
tokens, depending on whether a name is ren-
dered phonetically, semantically, or a mixture
of both (Ji et al, 2009).
? The artificial settings of assigning low
weights to information translation (compared
to overall word translation) in some large-
scale government evaluations have discour-
aged MT developers to spend time and ex-
plore resources to tackle this problem.
We propose a novel Name-aware MT (NAMT)
approach which can tightly integrate name pro-
cessing into the training and decoding processes of
an end-to-end MT pipeline, and a new name-aware
metric to evaluate MT which can assign different
weights to different tokens according to their im-
portance values in a document. Compared to pre-
vious methods, the novel contributions of our ap-
proach are:
1. Tightly integrate joint bilingual name tag-
ging into MT training by coordinating tagged
604
names in parallel corpora, updating word seg-
mentation, word alignment and grammar ex-
traction (Section 3.1).
2. Tightly integrate name tagging and transla-
tion into MT decoding via name-aware gram-
mar (Section 3.2).
3. Optimize name translation and context trans-
lation simultaneously and conduct name
translation driven decoding with language
model (LM) based selection (Section 3.2).
4. Propose a new MT evaluation metric which
can discriminate names and non-informative
words (Section 4).
2 Baseline MT
As our baseline, we apply a high-performing
Chinese-English MT system (Zheng, 2008; Zheng
et al, 2009) based on hierarchical phrase-based
translation framework (Chiang, 2005). It is based
on a weighted synchronous context-free grammar
(SCFG). All SCFG rules are associated with a set
of features that are used to compute derivation
probabilities. The features include:
? Relative frequency in two directions P (?|?)
andP (?|?), estimating the likelihoods of one
side of the rule r: X ?< ?, ? > translating
into the other side, where ? and ? are strings
of terminals and non-terminals in the source
side and target side. Non-terminals in ? and
? are in one-to-one correspondence.
? Lexical weights in two directions: Pw(?|?)
andPw(?|?), estimating likelihoods of word-
s in one side of the rule r: X ?< ?, ? >
translating into the other side (Koehn et al,
2003).
? Phrase penalty: a penalty exp(1) for a rule
with no non-terminal being used in deriva-
tion.
? Rule penalty: a penalty exp(1) for a rule
with at least one non-terminal being used in
derivation.
? Glue rule penalty: a penalty exp(1) if a glue
rule used in derivation.
? Translation length: number of words in trans-
lation output.
Our previous work showed that combining mul-
tiple LMs trained from different sources can lead
to significant improvement. The LM used for de-
coding is a log-linear combination of four word
n-gram LMs which are built on different English
corpora (details described in section 5.1), with
the LM weights optimized on a development set
and determined by minimum error rate training
(MERT), to estimate the probability of a word giv-
en the preceding words. All four LMs were trained
using modified Kneser-Ney smoothing algorithm
(Chen and Goodman, 1996) and converted into
Bloom filter LMs (Talbot and Brants, 2008) sup-
porting memory map.
The scaling factors for all features are optimized
by minimum error rate training algorithm to max-
imize BLEU score (Och, 2003). Given an input
sentence in the source language, translation into
the target language is cast as a search problem,
where the goal is to find the highest-probability
derivation that generates the source-side sentence,
using the rules in our SCFG. The source-side
derivation corresponds to a synchronous target-
side derivation and the terminal yield of this target-
side derivation is the output of the system. We em-
ploy our CKY-style chart decoder, named SRInter-
p, to solve the search problem.
3 Name-aware MT
We tightly integrate name processing into the
above baseline to construct a NAMT model. Fig-
ure 1 depicts the general procedure.
3.1 Training
This basic training process of NAMT requires us
to apply a bilingual name tagger to annotate par-
allel training corpora. Traditional name tagging
approaches for single languages cannot address
this requirement because they were all built on da-
ta and resources which are specific to each lan-
guage without using any cross-lingual features.
In addition, due to separate decoding processes
the results on parallel data may not be consistent
across languages. We developed a bilingual joint
name tagger (Li et al, 2012) based on condition-
al random fields that incorporates both monolin-
gual and cross-lingual features and conducts join-
t inference, so that name tagging from two lan-
guages can mutually enhance each other and there-
fore inconsistent results can be corrected simulta-
neously. This joint name tagger achieved 86.3%
bilingual pair F-measure with manual alignment
and 84.4% bilingual pair F-measure with automat-
ic alignment as reported in (Li et al, 2012). Given
a parallel sentence pair we first apply Giza++ (Och
and Ney, 2003) to align words, and apply this join-
605
Dec
odin
g
Hier
arch
ical 
Phra
sed-
base
d M
T
Tran
slate
d Te
xt
Tran
slate
Bi-te
xt 
Data Sou
rce Text
Join
t
Nam
e Ta
gger
Sou
rce L
angu
age 
Nam
e Ta
gger
Nam
e Tr
ansl
ator
Trai
ning
Nam
e Pa
ir M
iner
Extr
act s
ourc
e lan
guag
e na
mes
 
and
 add
 the
m to
 dict
iona
ries 
for 
sour
ce la
ngu
age 
nam
e ta
gger
 E
xtra
ct n
ame
 pair
s an
d 
add
 the
m to
tran
slati
on 
dict
iona
ry
Extr
act a
nd a
dd 
nam
e pa
irs t
o
phra
se ta
ble
GIZA
++
Rule
 Extr
acto
r
Extr
act S
CFG
 rule
s wi
th 
com
bina
tion
 of n
ame
-rep
lace
d 
data
 and
 orig
inal 
bi-te
xt d
ata
Rep
lace
 nam
es w
ith 
non
-term
inals
 and
 
com
bine
 with
 the
 
orig
inal 
para
llel d
ata
Figure 1: Architecture of Name-aware Machine Translation System.
t bilingual name tagger to extract three types of
names: (Person (PER), Organization (ORG) and
Geo-political entities (GPE)) from both the source
side and the target side. We pair two entities from
two languages, if they have the same entity type
and are mapped together by word alignment. We
ignore two kinds of names: multi-word names
with conflicting boundaries in two languages and
names only identified in one side of a parallel sen-
tence.
We built a NAMT system from such name-
tagged parallel corpora. First, we replace tagged
name pairs with their entity types, and then
use Giza++ and symmetrization heuristics to re-
generate word alignment. Since the name tags ap-
pear very frequently, the existence of such tags
yields improvement in word alignment quality.
The re-aligned parallel corpora are used to train
our NAMT system based on SCFG. Since the joint
name tagger ensures that each tagged source name
has a corresponding translation on the target side
(and vice versa), we can extract SCFG rules by
treating the tagged names as non-terminals.
However, the original parallel corpora contain
many high-frequency names, which can already be
handled well by the baseline MT. Some of these
names carry special meanings that may influence
translations of the neighboring words, and thus re-
placing them with non-terminals can lead to infor-
mation loss and weaken the translation model. To
address this issue, we merged the name-replaced
parallel data with the original parallel data and ex-
tract grammars from the combined corpus. For ex-
ample, given the following sentence pair:
? -???e???e????? .
? China appeals to world for non involvement
in Angola conflict .
after name tagging it becomes
? GPE??e???e GPE?? .
? GPE appeals to world for non involvement in
GPE conflict .
Both sentence pairs are kept in the combined data
to build the translation model.
3.2 Decoding
During decoding phase, we extract names with
the baseline monolingual name tagger described
in (Li et al, 2012) from a source document. It-
s performance is comparable to the best report-
ed results on Chinese name tagging on Automat-
ic Content Extraction (ACE) data (Ji and Grish-
man, 2006; Florian et al, 2006; Zitouni and Flo-
rian, 2008; Nguyen et al, 2010). Then we ap-
ply a state-of-the-art name translation system (Ji
et al, 2009) to translate names into the target lan-
guage. The name translation system is composed
of the following steps: (1) Dictionary matching
based on 150,041 name translation pairs; (2) Sta-
tistical name transliteration based on a structured
perceptron model and a character based MT mod-
el (Dayne and Shahram, 2007); (3) Context infor-
mation extraction based re-ranking.
In our NAMT framework, we add the following
extensions to name translation.
We developed a name origin classifier based on
Chinese last name list (446 name characters) and
name structure parsing features to distinguish Chi-
nese person names and foreign person names (Ji,
2009), so that pinyin conversion is applied for Chi-
nese names while name transliteration is applied
only for foreign names. This classifier works rea-
sonably well in most cases (about 92% classifica-
tion accuracy), except when a common Chinese
last name appears as the first character of a foreign
606
name, such as ?1?? which can be translated ei-
ther as ?Jolie? or ?Zhu Li?.
For those names with fewer than five instances
in the training data, we use the name translation
system to provide translations; for the rest of the
names, we leave them to the baseline MT mod-
el to handle. The joint bilingual name tagger was
also exploited to mine bilingual name translation
pairs from parallel training corpora. The mapping
score between a Chinese name and an English
name was computed by the number of aligned to-
kens. A name pair is extracted if the mapping
score is the highest among all combinations and
the name types on both sides are identical. It is
necessary to incorporate word alignment as addi-
tional constraints because the order of names is of-
ten changed after translation. Finally, the extract-
ed 9,963 unique name translation pairs were also
used to create an additional name phrase table for
NAMT. Manual evaluation on 2,000 name pairs
showed the accuracy is 86%.
The non-terminals in SCFG rules are rewritten
to the extracted names during decoding, therefore
allow unseen names in the test data to be trans-
lated. Finally, based on LMs, our decoder ex-
ploits the dynamically created phrase table from
name translation, competing with originally ex-
tracted rules, to find the best translation for the
input sentence.
4 Name-aware MT Evaluation
Traditional MT evaluation metrics such as
BLEU (Papineni et al, 2002) and Translation Ed-
it Rate (TER) (Snover et al, 2006) assign the
same weights to all tokens equally. For exam-
ple, incorrect translations of ?the? and ?Bush? will
receive the same penalty. However, for cross-
lingual information processing applications, we
should acknowledge that certain informationally
critical words are more important than other com-
mon words. In order to properly evaluate the trans-
lation quality of NAMT methods, we propose to
modify the BLEU metric so that they can dynam-
ically assign more weights to names during evalu-
ation.
BLEU considers the correspondence between a
system translation and a human translation:
BLEU = BP ? exp
( N?
n=1
wn log pn
)
(1)
where BP is brevity penalty defined as follows:
BP =
{
1 if c > r,
e(1?r/c) if c ? r. (2)
where wn is a set of positive weights summing to
one and usually uniformly set as wn = 1/N , c is
the length of the system translation and r is the
length of reference translation, and pn is modified
n-gram precision defined as:
pn =
?
C?Candidates
?
n-gram?C
Countclip(n-gram)
?
C??Candidates
?
n-gram??C?
Countclip(n-gram?)
(3)
where C and C ? are translation candidates in the
candidate sentence set, if a source sentence is
translated to many candidate sentences.
As in BLEU metric, we first count the maxi-
mum number of times an n-gram occurs in any s-
ingle reference translation. The total count of each
candidate n-gram is clipped at sentence level by it-
s maximum reference count. Then we add up the
weights of clipped n-grams and divide them by the
total weight of all n-grams.
Based on BLEU score, we design a name-aware
BLEU metric as follows. Depending on whether a
token t is contained in a name in reference trans-
lation, we assign a weight weightt to t as follows:
weightt ={
1? e?tf(t,d)?idf(t,D), if t never appears in names
1 + PEZ , if t occurs in name(s)
(4)
where PE is the sum of penalties of non-name
tokens and Z is the number of tokens within all
names:
PE =
?
t never appears in names
e?tf(t,d)?idf(t,D) (5)
In this paper, the tf ? idf score is computed at sen-
tence level, therefore, D is the sentence set and
each d ? D is a sentence.
The weight of an n-gram in reference translation
is the sum of weights of all tokens it contains.
weightngram =
?
t?ngram
weightt (6)
Next, we compute the weighted modified n-
gram precision Countweight?clip(n-gram) as fol-
lows:
Countweight?clip(n-gram) =?
if the ngrami is correctly translated
weightngrami (7)
607
The Countclip(n-gram) in the equation 3 is
substituted with aboveCountweight?clip(n-gram).
When we sum up the total weight of all n-grams of
a candidate translation, some n-grams may contain
tokens which do not exist in reference translation.
We assign the lowest weight of tokens in reference
translation to these rare tokens.
We also add an item, name penalty NP , to
penalize the output sentences which contain too
many or too few names:
NP = e?(uv?1)2/2? (8)
where u is the number of name tokens in system
translation and v is the number of name tokens in
reference translation.
Finally the name-aware BLEU score is defined
as:
BLEUNA = BP ?NP ? exp
( N?
n=1
wn logwpn
)
(9)
This new metric can also be applied to evalu-
ate MT approaches which emphasize other types
of facts such as events, by simply replacing name
tokens by other fact tokens.
5 Experiments
In this section we present the experimental results
of NAMT compared to the baseline MT.
5.1 Data Set
We used a large Chinese-English MT training cor-
pus from various sources and genres (including
newswire, web text, broadcast news and broadcast
conversations) for our experiments. We also used
some translation lexicon data and Wikipedia trans-
lations. The majority of the data sets were col-
lected or made available by LDC for U.S. DARPA
Translingual Information Detection, Extraction
and Summarization (TIDES) program, Global Au-
tonomous Language Exploitation (GALE) pro-
gram, Broad Operational Language Translation
(BOLT) program and National Institute of Stan-
dards and Technology (NIST) MT evaluations.
The training corpus includes 1,686,458 sentence
pairs. The joint name tagger extracted 1,890,335
name pairs (295,087 Persons, 1,269,056 Geo-
political entities and 326,192 Organizations).
Four LMs, denoted LM1, LM2, LM3, and
LM4, were trained from different English cor-
pora. LM1 is a 7-gram LM trained on the tar-
get side of Chinese-English and Egyptian Arabic-
English parallel text, English monolingual discus-
sion forums data R1-R4 released in BOLT Phase
1 (LDC2012E04, LDC2012E16, LDC2012E21,
LDC2012E54), and English Gigaword Fifth Edi-
tion (LDC2011T07). LM2 is a 7-gram LM trained
only on the English monolingual discussion fo-
rums data listed above. LM3 is a 4-gram LM
trained on the web genre among the target side
of all parallel text (i.e., web text from pre-BOLT
parallel text and BOLT released discussion fo-
rum parallel text). LM4 is a 4-gram LM trained
on the English broadcast news and conversation
transcripts released under the DARPA GALE pro-
gram. Note that for LM4 training data, some tran-
scripts were quick transcripts and quick rich tran-
scripts released by LDC, and some were generated
by running flexible alignment of closed captions or
speech recognition output from LDC on the audio
data (Venkataraman et al, 2004).
In order to demonstrate the effectiveness and
generality of our approach, we evaluated our ap-
proach on seven test sets from multiple genres and
domains. We asked four annotators to annotate
names in four reference translations of each sen-
tence and an expert annotator to adjudicate result-
s. The detailed statistics and name distribution of
each test data set is shown in Table 1. The per-
centage of names occurred fewer than 5 times in
training data are listed in the brackets in the last
column of the table.
5.2 Overall Performance
Besides the new name-aware MT metric, we also
adopt two traditional metrics, TER to evaluate the
overall translation performance and Named Entity
Weak Accuracy (NEWA) (Hermjakob et al, 2008)
to evaluate the name translation performance.
TER measures the amount of edits required to
change a system output into one of the reference
translations. Specifically:
TER = # of editsaverage # of reference words (10)
Possible edits include insertion, substitution dele-
tion and shifts of words.
The NEWA metric is defined as follows. Us-
ing a manually assembled name variant table, we
also support the matching of name variants (e.g.,
?World Health Organization? and ?WHO?).
NEWA = Count # of correctly translated namesCount # of names in references (11)
608
Corpus Genre Sentence # Word # Token # GPE(%) PER(%) ORG(%) All namesin source in reference (% occurred < 5)
BOLT 1 forum 1,200 20,968 24,193 875(82.9) 90(8.5) 91(8.6) 1,056 (51.4)
BOLT 2 forum 1,283 23,707 25,759 815(73.7) 141(12.8) 149(13.5) 1,105 (65.9)
BOLT 3 forum 2,000 38,595 42,519 1,664(80.4) 204(9.8) 204(9.8) 2,072 (47.4)
BOLT 4 forum 1,918 41,759 47,755 1,852(80.0) 348(25.0) 113(5.0) 2,313 (53.3)
BOLT 5 blog 950 23,930 26,875 352(42.5) 235(28.3) 242(29.2) 829 (55.3)
NIST2006 news&blog 1,664 38,442 45,914 1,660(58.2) 568(19.9) 625(21.9) 2,853 (73.1)
NIST2008 news&blog 1,357 32,646 37,315 700(47.9) 367(25.1) 395(27.0) 1,462 (72.0)
Table 1: Statistics and Name Distribution of Test Data Sets.
Metric System BOLT 1 BOLT 2 BOLT 3 BOLT 4 BOLT 5 NIST2006 NIST2008
BLEU
Baseline 14.2 14.0 17.3 15.6 15.3 35.5 29.3
NPhrase 14.1 14.4 17.1 15.4 15.3 35.4 29.3
NAMT 14.2 14.6 16.9 15.7 15.5 36.3 30.0
Name-aware BLEU
Baseline 18.2 17.9 18.6 17.6 18.3 36.1 31.7
NPhrase 18.1 18.8 18.5 18.1 18.0 35.8 31.8
NAMT 18.4 19.5 19.7 18.2 18.9 39.4 33.1
TER
Baseline 70.6 71.0 69.4 70.3 67.1 58.7 61.0
NPhrase 70.6 70.4 69.4 70.4 67.1 58.7 60.9
NAMT 70.3 70.2 69.2 70.1 66.6 57.7 60.5
NEWA
All
Baseline 69.7 70.1 73.9 72.3 60.6 66.5 60.4
NPhrase 69.8 71.1 73.8 72.5 60.6 68.3 61.9
NAMT 71.4 72.0 77.7 75.1 62.7 72.9 63.2
GPE
Baseline 72.8 78.4 80.0 78.7 81.3 79.2 76.0
NPhrase 73.6 79.3 79.2 78.9 82.3 82.6 79.5
NAMT 74.2 80.2 82.8 80.4 79.3 85.5 79.3
PER
Baseline 53.3 44.7 45.1 49.4 48.9 54.2 51.2
NPhrase 52.2 45.4 48.9 48.5 47.6 55.1 50.9
NAMT 55.6 45.4 58.8 55.2 56.2 60.0 52.3
ORG
Baseline 56.0 49.0 52.9 38.1 41.7 44.0 41.3
NPhrase 50.5 50.3 54.4 40.7 41.3 42.2 40.7
NAMT 60.4 52.3 55.4 41.6 45.0 51.0 44.8
Table 2: Translation Performance (%).
For better comparison with NAMT, besides the
original baseline, we develop the other baseline
system by adding name translation table into the
phrase table (NPhrase).
Table 2 presents the performance of overal-
l translation and name translation. We can see
that except for the BOLT3 data set with BLEU
metric, our NAMT approach consistently outper-
formed the baseline system for all data sets with
all metrics, and provided up to 23.6% relative er-
ror reduction on name translation. According to
Wilcoxon Matched-Pairs Signed-Ranks Test, the
improvement is not significant with BLEU metric,
but is significant at 98% confidence level with all
of the other metrics. The gains are more signifi-
cant for formal genres than informal genres main-
ly because most of the training data for name tag-
ging and name translation were from newswire.
Furthermore, using external name translation table
only did not improve translation quality in most
test sets except for BOLT2. Therefore, it is im-
portant to use name-replaced corpora for rule ex-
traction to fully take advantage of improved word
alignment.
Many errors from the baseline MT approach oc-
curred because some parts of out-of-vocabulary
names were mistakenly segmented into common
words. For example, the baseline MT system mis-
takenly translated a person name ?Y?? (Sun
Honglei)? into ?Sun red thunder?. In informal
genres such as discussion forums and web blogs,
even common names often appear in rare form-
s due to misspelling or morphing. For example,
?e8l (Obama)? was mistakenly translated into
?Ma Olympic?. Such errors can be compounded
when word re-ordering was applied. For example,
the following sentence: ????????/:
'J/iy (Guo Meimei?s strength real-
ly is formidable, I really admire her)? was mis-
takenly translated into ?Guo the strength of the
America and the America also really strong , ah
, really admire her? by the baseline MT system
because the person name ???? (Guomeimei)?
was mistakenly segmented into three words ??
(Guo)?, ?? (the America)? and ?? (the Ameri-
ca)?. But our NAMT approach successfully iden-
tified and translated this name and also generated
better overall translation: ?Guo Meimei ?s power
is also really strong , ah , really admire her?.
609
B L E U N a m e - a w a r eB L E U024681 01 2
1 41 61 82 0Score A u t o m a t i c  M e t r i c s H u m .  1  H u m .  2 H u m .  30 . 00 . 51 . 01 . 52 . 02 . 5
3 . 03 . 54 . 0 b a s e l i n e N A M T Score H u m a n  E v a l u a t i o n
Figure 2: Scores based on Automatic Metrics and Human
Evaluation.
5.3 Name-aware BLEU vs The Human
Evaluation
In order to investigate the correlation between
name-aware BLEU scores and human judgment
results, we asked three bi-lingual speakers to judge
our translation output from the baseline system
and the NAMT system, on a Chinese subset of 250
sentences (each sentence has two corresponding
translations from baseline and NAMT) extracted
randomly from 7 test corpora. The annotators rat-
ed each translation from 1 (very bad) to 5 (very
good) and made their judgments based on whether
the translation is understandable and conveys the
same meaning.
We computed the name-aware BLEU scores on
the subset and also the aggregated average scores
from human judgments. Figure 2 shows that
NAMT consistently achieved higher scores with
both name-aware BLEU metric and human judge-
ment. Furthermore, we calculated three Pearson
product-moment correlation coefficients between
human judgment scores and name-aware BLEU s-
cores of these two MT systems. Give the sample
size and the correlation coefficient value, the high
significance value of 0.99 indicates that name-
aware BLEU tracks human judgment well.
5.4 Word Alignment
It is also important to investigate the impact of our
NAMT approach on improving word alignmen-
t. We conducted the experiment on the Chinese-
English Parallel Treebank (Li et al, 2010) with
ground-truth word alignment. The detailed pro-
cedure following NAMT framework is as follows:
(1) Ran the joint bilingual name tagger; (2) Re-
placed each name string with its name type (PER,
ORG or GPE), and ran Giza++ on the replaced
sentences; (3) Ran Giza++ on the words within
Words Method P R F 
Baseline Giza++ 69.8 47.8 56.7 
Joint Name 
Tagging 
70.4 48.1 57.1 
 
Overall 
Words 
Ground-truth 
Name Tagging 
(Upper-bound) 
71.3 48.9 58.0 
Baseline Giza++ 86.0 31.4 46.0 Words 
Within 
Names 
Joint Name 
Tagging 
77.6 37.2 50.3 
 
 
 
 
 
 
 
 
 
 
Table 3: Impact of Joint Bilingual Name Tagging on Word
Alignment (%).
each name pair. (4) Merged (2) and (3) to pro-
duce the final word alignment results. In order to
compare with the upper-bound gains, we also mea-
sured the performance of applying ground-truth
name tagging with the above procedures.
The experiment results are shown in Table 3.
For the words within names, our approach provid-
ed significant gains by enhancing F-measure from
46.0% to 50.3%. Only 10.6% words are within
names, therefore the upper-bound gains on over-
all word alignment is only 1.3%. Our joint name
tagging approach achieved 0.4% (statistically sig-
nificant) improvement over the baseline. In Fig-
ure 3 we categorized the sentences according to
the percentage of name words in each sentence and
measured the improvement for each category. We
can clearly see that as the sentences include more
names, the gains achieved by our approach tend to
be greater.
5.5 Remaining Error Analysis
Although the proposed model has significantly en-
hanced translation quality, some challenges re-
main. We analyze some major sources of the re-
maining errors as follows.
1. Name Structure Parsing.
We found that the gains of our NAMT approach
were mainly achieved for names with one or two
components. When the name structure becomes
too complicated to parse, name tagging and name
translation are likely to produce errors, especially
for long nested organizations. For example, ??0
???b?@? (Anti-malfeasance Bureau of
Gutian County Procuratorate) consists of a nested
organization name with a GPE as modifier: ??
0???b? (Gutian County Procuratorate) and
an ORG name: ??@? (Anti-malfeasance Bu-
reau).
2. Name abbreviation tagging and translation.
Some organization abbreviations are also dif-
ficult to extract because our name taggers have
610
0~10 10~20 20~30 30~40 >40
-0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
F-Measure Gains in Overall Word Alignment (%)
#name tokens/#all tokens(%)
 Baseline Giza++
 Joint Name Tagging
 Ground-truth Name Tagging (Upper-bound)
Figure 3: Word alignment gains according to the percentage
of name words in each sentence.
not incorporated any coreference resolution tech-
niques. For example, without knowing that ?FAW?
refers to ?First Automotive Works? in ?FAW has
also utilized the capital market to directly fi-
nance, and now owns three domestic listed compa-
nies?, our system mistakenly labeled it as a GPE.
The same challenge exists in name alignment and
translation (for example, ? i (Min Ge)? refer-
s to ? -??Zi}?XProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 402?412,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Incremental Joint Extraction of Entity Mentions and Relations
Qi Li Heng Ji
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180, USA
{liq7,jih}@rpi.edu
Abstract
We present an incremental joint frame-
work to simultaneously extract entity men-
tions and relations using structured per-
ceptron with efficient beam-search. A
segment-based decoder based on the idea
of semi-Markov chain is adopted to the
new framework as opposed to traditional
token-based tagging. In addition, by virtue
of the inexact search, we developed a num-
ber of new and effective global features
as soft constraints to capture the inter-
dependency among entity mentions and
relations. Experiments on Automatic Con-
tent Extraction (ACE)
1
corpora demon-
strate that our joint model significantly
outperforms a strong pipelined baseline,
which attains better performance than the
best-reported end-to-end system.
1 Introduction
The goal of end-to-end entity mention and re-
lation extraction is to discover relational struc-
tures of entity mentions from unstructured texts.
This problem has been artificially broken down
into several components such as entity mention
boundary identification, entity type classification
and relation extraction. Although adopting such
a pipelined approach would make a system com-
paratively easy to assemble, it has some limita-
tions: First, it prohibits the interactions between
components. Errors in the upstream components
are propagated to the downstream components
without any feedback. Second, it over-simplifies
the problem as multiple local classification steps
without modeling long-distance and cross-task de-
pendencies. By contrast, we re-formulate this
task as a structured prediction problem to reveal
the linguistic and logical properties of the hidden
1
http://www.itl.nist.gov/iad/mig//tests/ace
structures. For example, in Figure 1, the output
structure of each sentence can be interpreted as a
graph in which entity mentions are nodes and re-
lations are directed arcs with relation types. By
jointly predicting the structures, we aim to address
the aforementioned limitations by capturing: (i)
The interactions between two tasks. For exam-
ple, in Figure 1a, although it may be difficult for
a mention extractor to predict ?1,400? as a Per-
son (PER) mention, the context word ?employs?
between ?tire maker? and ?1,400? strongly in-
dicates an Employment-Organization (EMP-ORG)
relation which must involve a PER mention. (ii)
The global features of the hidden structure. Var-
ious entity mentions and relations share linguis-
tic and logical constraints. For example, we
can use the triangle feature in Figure 1b to en-
sure that the relations between ?forces?, and each
of the entity mentions ?Somalia
/GPE
?, ?Haiti
/GPE
?
and ?Kosovo
/GPE
?, are of the same type (Physical
(PHYS), in this case).
Following the above intuitions, we introduce
a joint framework based on structured percep-
tron (Collins, 2002; Collins and Roark, 2004) with
beam-search to extract entity mentions and rela-
tions simultaneously. With the benefit of inexact
search, we are also able to use arbitrary global
features with low cost. The underlying learning
algorithm has been successfully applied to some
other Natural Language Processing (NLP) tasks.
Our task differs from dependency parsing (such as
(Huang and Sagae, 2010)) in that relation struc-
tures are more flexible, where each node can have
arbitrary relation arcs. Our previous work (Li et
al., 2013) used perceptron model with token-based
tagging to jointly extract event triggers and argu-
ments. By contrast, we aim to address a more chal-
lenging task: identifying mention boundaries and
types together with relations, which raises the is-
sue that assignments for the same sentence with
different mention boundaries are difficult to syn-
402
The tire maker| {z }
ORG
still employs 1,400| {z }
PER
.
EMP-ORG
(a) Interactions between Two Tasks
... US|{z}
GPE
forces| {z }
PER
in Somalia| {z }
GPE
, Haiti|{z}
GPE
and Kosovo| {z }
GPE
.
EMP-ORG
PHYS
conj and
GPE
PER
GPE
PHYSPHY
S
conj and
(b) Example of Global Feature
Figure 1: End-to-End Entity Mention and Relation Extraction.
chronize during search. To tackle this problem,
we adopt a segment-based decoding algorithm de-
rived from (Sarawagi and Cohen, 2004; Zhang and
Clark, 2008) based on the idea of semi-Markov
chain (a.k.a, multiple-beam search algorithm).
Most previous attempts on joint inference of en-
tity mentions and relations (such as (Roth and Yih,
2004; Roth and Yih, 2007)) assumed that entity
mention boundaries were given, and the classifiers
of mentions and relations are separately learned.
As a key difference, we incrementally extract en-
tity mentions together with relations using a single
model. The main contributions of this paper are as
follows:
1. This is the first work to incrementally predict
entity mentions and relations using a single
joint model (Section 3).
2. Predicting mention boundaries in the joint
framework raises the challenge of synchroniz-
ing different assignments in the same beam. We
solve this problem by detecting entity mentions
on segment-level instead of traditional token-
based approaches (Section 3.1.1).
3. We design a set of novel global features based
on soft constraints over the entire output graph
structure with low cost (Section 4).
Experimental results show that the proposed
framework achieves better performance than
pipelined approaches, and global features provide
further significant gains.
2 Background
2.1 Task Definition
The entity mention extraction and relation
extraction tasks we are addressing are those
of the Automatic Content Extraction (ACE)
program
2
. ACE defined 7 main entity types
including Person (PER), Organization (ORG),
Geographical Entities (GPE), Location (LOC),
2
http://www.nist.gov/speech/tests/ace
Facility (FAC), Weapon (WEA) and Vehicle
(VEH). The goal of relation extraction
3
is to
extract semantic relations of the targeted types
between a pair of entity mentions which ap-
pear in the same sentence. ACE?04 defined 7
main relation types: Physical (PHYS), Person-
Social (PER-SOC), Employment-Organization
(EMP-ORG), Agent-Artifact (ART), PER/ORG
Affiliation (Other-AFF), GPE-Affiliation
(GPE-AFF) and Discourse (DISC). ACE?05 kept
PER-SOC, ART and GPE-AFF, split PHYS into
PHYS and a new relation type Part-Whole,
removed DISC, and merged EMP-ORG and
Other-AFF into EMP-ORG.
Throughout this paper, we use? to denote non-
entity or non-relation classes. We consider rela-
tion asymmetric. The same relation type with op-
posite directions is considered to be two classes,
which we refer to as directed relation types.
Most previous research on relation extraction
assumed that entity mentions were given In this
work we aim to address the problem of end-to-end
entity mention and relation extraction from raw
texts.
2.2 Baseline System
In order to develop a baseline system repre-
senting state-of-the-art pipelined approaches, we
trained a linear-chain Conditional Random Fields
model (Lafferty et al, 2001) for entity mention ex-
traction and a Maximum Entropy model for rela-
tion extraction.
Entity Mention Extraction Model We re-cast
the problem of entity mention extraction as a se-
quential token tagging task as in the state-of-the-
art system (Florian et al, 2006). We applied the
BILOU scheme, where each tag means a token is
the Beginning, Inside, Last, Outside, and Unit of
an entity mention, respectively. Most of our fea-
tures are similar to the work of (Florian et al,
3
Throughout this paper we refer to relation mention as re-
lation since we do not consider relation mention coreference.
403
2004; Florian et al, 2006) except that we do not
have their gazetteers and outputs from other men-
tion detection systems as features. Our additional
features are as follows:
? Governor word of the current token based on de-
pendency parsing (Marneffe et al, 2006).
? Prefix of each word in Brown clusters learned
from TDT5 corpus (Sun et al, 2011).
Relation Extraction Model Given a sentence
with entity mention annotations, the goal of base-
line relation extraction is to classify each mention
pair into one of the pre-defined relation types with
direction or ? (non-relation). Most of our relation
extraction features are based on the previous work
of (Zhou et al, 2005) and (Kambhatla, 2004). We
designed the following additional features:
? The label sequence of phrases covering the two
mentions. For example, for the sentence in Fig-
ure 1a, the sequence is ?NP VP NP?. We also
augment it by head words of each phrase.
? Four syntactico - semantic patterns described in
(Chan and Roth, 2010).
? We replicated each lexical feature by replacing
each word with its Brown cluster.
3 Algorithm
3.1 The Model
Our goal is to predict the hidden structure of
each sentence based on arbitrary features and con-
straints. Let x ? X be an input sentence, y
?
? Y
be a candidate structure, and f(x, y
?
) be the fea-
ture vector that characterizes the entire structure.
We use the following linear model to predict the
most probable structure y? for x:
y? = argmax
y
?
?Y(x)
f(x, y
?
) ?w (1)
where the score of each candidate assignment is
defined as the inner product of the feature vector
f(x, y
?
) and feature weights w.
Since the structures contain both entity men-
tions relations, and we also aim to exploit global
features. There does not exist a polynomial-time
algorithm to find the best structure. In practice
we apply beam-search to expand partial configu-
rations for the input sentence incrementally to find
the structure with the highest score.
3.1.1 Joint Decoding Algorithm
One main challenge to search for entity mentions
and relations incrementally is the alignment of dif-
ferent assignments. Assignments for the same sen-
tence can have different numbers of entity men-
tions and relation arcs. The entity mention ex-
traction task is often re-cast as a token-level se-
quential labeling problem with BIO or BILOU
scheme (Ratinov and Roth, 2009; Florian et al,
2006). A naive solution to our task is to adopt this
strategy by treating each token as a state. How-
ever, different assignments for the same sentence
can have various mention boundaries. It is un-
fair to compare the model scores of a partial men-
tion and a complete mention. It is also difficult to
synchronize the search process of relations. For
example, consider the two hypotheses ending at
?York? for the same sentence:
AllanU-PER from? NewB-ORG YorkI-ORG Stock Exchange
AllanU-PER from? NewB-GPE YorkL-GPE Stock Exchange
PHYS
PHYS
The model would bias towards the incorrect as-
signment ?New
/B-GPE
York
/L-GPE
? since it can
have more informative features as a complete
mention (e.g., a binary feature indicating if the
entire mention appears in a GPE gazetter). Fur-
thermore, the predictions of the two PHYS rela-
tions cannot be synchronized since ?New
/B-FAC
York
/I-FAC
? is not yet a complete mention.
To tackle these problems, we employ the idea of
semi-Markov chain (Sarawagi and Cohen, 2004),
in which each state corresponds to a segment
of the input sequence. They presented a vari-
ant of Viterbi algorithm for exact inference in
semi-Markov chain. We relax the max operation
by beam-search, resulting in a segment-based de-
coder similar to the multiple-beam algorithm in
(Zhang and Clark, 2008). Let
?
d be the upper bound
of entity mention length. The k-best partial assign-
ments ending at the i-th token can be calculated as:
B[i] = k-BEST
y
?
?{y
[1..i]
|y
[1:i?d]
?B[i?d], d=1...
?
d}
f(x, y
?
) ?w
where y
[1:i?d]
stands for a partial configuration
ending at the (i-d)-th token, and y
[i?d+1,i]
corre-
sponds to the structure of a new segment (i.e., sub-
sequence of x) x
[i?d+1,i]
. Our joint decoding algo-
rithm is shown in Figure 2. For each token index
i, it maintains a beam for the partial assignments
whose last segments end at the i-th token. There
are two types of actions during the search:
404
Input: input sentence x = (x
1
, x
2
, ..., x
m
).
k: beam size.
T ? {?}: entity mention type alphabet.
R? {?}: directed relation type alphabet.
4
d
t
: max length of type-t segment, t ? T ? {?}.
Output: best configuration y? for x
1 initialize m empty beams B[1..m]
2 for i? 1...m do
3 for t ? T ? {?} do
4 for d? 1...d
t
, y
?
? B[i? d] do
5 k ? i? d+ 1
6 B[i]? B[i] ? APPEND(y
?
, t, k, i)
7 B[i]? k-BEST(B[i])
8 for j ? (i? 1)...1 do
9 buf? ?
10 for y
?
? B[i] do
11 if HASPAIR(y
?
, i, j) then
12 for r ? R ? {?} do
13 buf? buf ? LINK(y
?
, r, i, j)
14 else
15 buf? buf ? {y
?
}
16 B[i]? k-BEST(buf)
17 return B[m][0]
Figure 2: Joint Decoding for Entity Men-
tions and Relations. HASPAIR(y
?
, i, j) checks
if there are two entity mentions in y
?
that
end at token x
i
and token x
j
, respectively.
APPEND(y
?
, t, k, i) appends y
?
with a type-t
segment spanning from x
k
to x
i
. Similarly
LINK(y
?
, r, i, j) augments y
?
by assigning a di-
rected relation r to the pair of entity mentions
ending at x
i
and x
j
respectively.
1. APPEND (Lines 3-7). First, the algorithm
enumerates all possible segments (i.e., subse-
quences) of x ending at the current token with
various entity types. A special type of seg-
ment is a single token with non-entity label (?).
Each segment is then appended to existing par-
tial assignments in one of the previous beams to
form new assignments. Finally the top k results
are recorded in the current beam.
2. LINK (Lines 8-16). After each step of APPEND,
the algorithm looks backward to link the newly
identified entity mentions and previous ones (if
any) with relation arcs. At the j-th sub-step,
it only considers the previous mention ending
at the j-th previous token. Therefore different
4
The same relation type with opposite directions is con-
sidered to be two classes in R.
configurations are guaranteed to have the same
number of sub-steps. Finally, all assignments
are re-ranked with new relation information.
There are m APPEND actions, each is followed by
at most (i?1) LINK actions (line 8). Therefore the
worst-case time complexity is O(
?
d ?k ?m
2
), where
?
d is the upper bound of segment length.
3.1.2 Example Demonstration
the
tire
maker still
employs
1,400
.
?
PER
ORG
...
x
y EMP-ORG
Figure 3: Example of decoding steps. x-axis
and y-axis represent the input sentence and en-
tity types, respectively. The rectangles denote seg-
ments with entity types, among which the shaded
ones are three competing hypotheses ending at
?1,400?. The solid lines and arrows indicate cor-
rect APPEND and LINK actions respectively, while
the dashed indicate incorrect actions.
Here we demonstrate a simple but concrete ex-
ample by considering again the sentence described
in Figure 1a. Suppose we are at the token ?1,400?.
At this point we can propose multiple entity men-
tions with various lengths. Assuming ?1,400
/PER
?,
?1,400
/?
? and ?(employs 1,400)
/PER
? are possi-
ble assignments, the algorithm appends these new
segments to the partial assignments in the beams
of the tokens ?employs? and ?still?, respectively.
Figure 3 illustrates this process. For simplicity,
only a small part of the search space is presented.
The algorithm then links the newly identified men-
tions to the previous ones in the same configu-
ration. In this example, the only previous men-
tion is ?(tire maker)
/ORG
?. Finally, ?1,400
/PER
? will
be preferred by the model since there are more
indicative context features for EMP-ORG relation
between ?(tire maker)
/PER
? and ?1,400
/PER
?.
405
3.2 Structured-Perceptron Learning
To estimate the feature weights, we use struc-
tured perceptron (Collins, 2002), an extension
of the standard perceptron for structured pre-
diction, as the learning framework. Huang et
al. (2012) proved the convergency of structured
perceptron when inexact search is applied with
violation-fixing update methods such as early-
update (Collins and Roark, 2004). Since we use
beam-search in this work, we apply early-update.
In addition, we use averaged parameters to reduce
overfitting as in (Collins, 2002).
Figure 4 shows the pseudocode for struc-
tured perceptron training with early-update. Here
BEAMSEARCH is identical to the decoding algo-
rithm described in Figure 2 except that if y
?
, the
prefix of the gold standard y, falls out of the beam
after each execution of the k-BEST function (line 7
and 16), then the top assignment z and y
?
are re-
turned for parameter update. It is worth noting that
this can only happen if the gold-standard has a seg-
ment ending at the current token. For instance, in
the example of Figure 1a, B[2] cannot trigger any
early-update since the gold standard does not con-
tain any segment ending at the second token.
Input: training set D = {(x
(j)
, y
(j)
)}
N
i=1
,
maximum iteration number T
Output: model parameters w
1 initialize w? 0
2 for t? 1...T do
3 foreach (x, y) ? D do
4 (x, y
?
, z)? BEAMSEARCH (x, y,w)
5 if z 6= y then
6 w? w + f(x, y
?
)? f(x, z)
7 return w
Figure 4: Perceptron algorithm with beam-
search and early-update. y
?
is the prefix of the
gold-standard and z is the top assignment.
3.3 Entity Type Constraints
Entity type constraints have been shown effective
in predicting relations (Roth and Yih, 2007; Chan
and Roth, 2010). We automatically collect a map-
ping table of permissible entity types for each rela-
tion type from our training data. Instead of apply-
ing the constraints in post-processing inference,
we prune the branches that violate the type con-
straints during search. This type of pruning can
reduce search space as well as make the input for
parameter update less noisy. In our experiments,
only 7 relation mentions (0.5%) in the dev set and
5 relation mentions (0.3%) in the test set violate
the constraints collected from the training data.
4 Features
An advantage of our framework is that we can
easily exploit arbitrary features across the two
tasks. This section describes the local features
(Section 4.1) and global features (Section 4.2) we
developed in this work.
4.1 Local Features
We design segment-based features to directly eval-
uate the properties of an entity mention instead of
the individual tokens it contains. Let y? be a pre-
dicted structure of a sentence x. The entity seg-
ments of y? can be expressed as a list of triples
(e
1
, ..., e
m
), where each segment e
i
= ?u
i
, v
i
, t
i
?
is a triple of start index u
i
, end index v
i
, and entity
type t
i
. The following is an example of segment-
based feature:
f
001
(x, y?, i) =
?
?
?
?
?
1 if x
[y?.u
i
,y?.v
i
]
= tire maker
y?.t
(i?1)
, y?.t
i
= ?,ORG
0 otherwise
This feature is triggered if the labels of the (i?1)-
th and the i-th segments are ??,ORG?, and the text
of the i-th segment is ?tire maker?. Our segment-
based features are described as follows:
Gazetteer features Entity type of each segment
based on matching a number of gazetteers includ-
ing persons, countries, cities and organizations.
Case features Whether a segment?s words are
initial-capitalized, all lower cased, or mixture.
Contextual features Unigrams and bigrams of
the text and part-of-speech tags in a segment?s
contextual window of size 2.
Parsing-based features Features derived from
constituent parsing trees, including (a) the phrase
type of the lowest common ancestor of the tokens
contained in the segment, (b) the depth of the low-
est common ancestor, (c) a binary feature indicat-
ing if the segment is a base phrase or a suffix of a
base phrase, and (d) the head words of the segment
and its neighbor phrases.
In addition, we convert each triple ?u
i
, v
i
, t
i
? to
BILOU tags for the tokens it contains to imple-
ment token-based features. The token-based men-
406
tion features and local relation features are identi-
cal to those of our pipelined system (Section 2.2).
4.2 Global Entity Mention Features
By virtue of the efficient inexact search, we are
able to use arbitrary features from the entire
structure of y? to capture long-distance dependen-
cies. The following features between related entity
mentions are extracted once a new segment is ap-
pended during decoding.
Coreference consistency Coreferential entity
mentions should be assigned the same entity type.
We determine high-recall coreference links be-
tween two segments in the same sentence using
some simple heuristic rules:
? Two segments exactly or partially string match.
? A pronoun (e.g., ?their?,?it?) refers to previous
entity mentions. For example, in ?they have
no insurance on their cars?, ?they? and ?their?
should have the same entity type.
? A relative pronoun (e.g., ?which?,?that?, and
?who?) refers to the noun phrase it modifies in
the parsing tree. For example, in ?the starting
kicker is nikita kargalskiy, who may be 5,000
miles from his hometown?, ?nikita kargalskiy?
and ?who? should both be labeled as persons.
Then we encode a global feature to check
whether two coreferential segments share the same
entity type. This feature is particularly effective
for pronouns because their contexts alone are of-
ten not informative.
Neighbor coherence Neighboring entity men-
tions tend to have coherent entity types. For ex-
ample, in ?Barbara Starr was reporting from the
Pentagon?, ?Barbara Starr? and ?Pentagon? are
connected by a dependency link prep from and
thus they are unlikely to be a pair of PER men-
tions. Two types of neighbor are considered: (i)
the first entity mention before the current segment,
and (ii) the segment which is connected by a sin-
gle word or a dependency link with the current
segment. We take the entity types of the two seg-
ments and the linkage together as a global feature.
For instance, ?PER prep from PER? is a feature
for the above example when ?Barbara Starr? and
?Pentagon? are both labeled as PER mentions.
Part-of-whole consistency If an entity men-
tion is semantically part of another mention (con-
nected by a prep of dependency link), they should
be assigned the same entity type. For example,
in ?some of Iraq?s exiles?, ?some? and ?exiles?
are both PER mentions; in ?one of the town?s two
meat-packing plants?, ?one? and ?plants? are both
FAC mentions; in ?the rest ofAmerica?, ?rest? and
?America? are both GPE mentions.
4.3 Global Relation Features
Relation arcs can also share inter-dependencies or
obey soft constraints. We extract the following
relation-centric global features when a new rela-
tion hypothesis is made during decoding.
Role coherence If an entity mention is involved
in multiple relations with the same type, then its
roles should be coherent. For example, a PER
mention is unlikely to have more than one em-
ployer. However, a GPE mention can be a physical
location for multiple entity mentions. We combine
the relation type and the entity mention?s argument
roles as a global feature, as shown in Figure 5a.
Triangle constraint Multiple entity mentions
are unlikely to be fully connected with the same
relation type. We use a negative feature to penalize
any configuration that contains this type of struc-
ture. An example is shown in Figure 5b.
Inter-dependent compatibility If two entity
mentions are connected by a dependency link, they
tend to have compatible relations with other enti-
ties. For example, in Figure 5c, the conj and de-
pendency link between ?Somalia? and ?Kosovo?
indicates they may share the same relation type
with the third entity mention ?forces?.
Neighbor coherence Similar to the entity men-
tion neighbor coherence feature, we also combine
the types of two neighbor relations in the same
sentence as a bigram feature.
5 Experiments
5.1 Data and Scoring Metric
Most previous work on ACE relation extraction
has reported results on ACE?04 data set. As
we will show later in our experiments, ACE?05
made significant improvement on both relation
type definition and annotation quality. Therefore
we present the overall performance on ACE?05
data. We removed two small subsets in informal
genres - cts and un, and then randomly split the re-
maining 511 documents into 3 parts: 351 for train-
ing, 80 for development, and the rest 80 for blind
test. In order to compare with state-of-the-art we
also performed the same 5-fold cross-validation on
bnews and nwire subsets of ACE?04 corpus as in
previous work. The statistics of these data sets
407
(GPE Somalia)
(PER forces)
(GPE US)
EMP-ORGEMP
-OR
G
?
(a)
(GPE Somalia)
(PER forces)
(GPE Haiti)
PHYSPHY
S
PHYS
?
(b)
(GPE Somalia)
(PER forces)
(GPE Kosovo)
PHYSPHY
S
conj and
(c)
Figure 5: Examples of Global Relation Features.
0 5 10 15 20 25# of training iterations0.70
0.72
0.74
0.76
0.78
0.80
F_1 s
core
mention local+globalmention local
(a) Entity Mention Performance
0 5 10 15 20 25# of training iterations0.30
0.35
0.40
0.45
0.50
0.55
F_1 s
core
relation local+globalrelation local
(b) Relation Performance
Figure 6: Learning Curves on Development Set.
are summarized in Table 1. We ran the Stanford
CoreNLP toolkit
5
to automatically recover the true
cases for lowercased documents.
Data Set # sentences # mentions # relations
ACE?05
Train 7,273 26,470 4,779
Dev 1,765 6,421 1,179
Test 1,535 5,476 1,147
ACE?04 6,789 22,740 4,368
Table 1: Data Sets.
We use the standard F
1
measure to evaluate the
performance of entity mention extraction and re-
lation extraction. An entity mention is considered
correct if its entity type is correct and the offsets
of its mention head are correct. A relation men-
tion is considered correct if its relation type is
correct, and the head offsets of two entity men-
tion arguments are both correct. As in Chan and
5
http://nlp.stanford.edu/software/corenlp.shtml
Roth (2011), we excluded the DISC relation type,
and removed relations in the system output which
are implicitly correct via coreference links for fair
comparison. Furthermore, we combine these two
criteria to evaluate the performance of end-to-end
entity mention and relation extraction.
5.2 Development Results
In general a larger beam size can yield better per-
formance but increase training and decoding time.
As a tradeoff, we set the beam size as 8 through-
out the experiments. Figure 6 shows the learn-
ing curves on the development set, and compares
the performance with and without global features.
From these figures we can clearly see that global
features consistently improve the extraction per-
formance of both tasks. We set the number of
training iterations as 22 based on these curves.
5.3 Overall Performance
Table 2 shows the overall performance of various
methods on the ACE?05 test data. We compare
our proposed method (Joint w/ Global) with the
pipelined system (Pipeline), the joint model with
only local features (Joint w/ Local), and two hu-
man annotators who annotated 73 documents in
ACE?05 corpus.
We can see that our approach significantly out-
performs the pipelined approach for both tasks. As
a real example, for the partial sentence ?a marcher
from Florida? from the test data, the pipelined ap-
proach failed to identify ?marcher? as a PER men-
tion, and thus missed the GEN-AFF relation be-
tween ?marcher? and ?Florida?. Our joint model
correctly identified the entity mentions and their
relation. Figure 7 shows the details when the
joint model is applied to this sentence. At the
token ?marcher?, the top hypothesis in the beam
is ???,???, while the correct one is ranked sec-
ond best. After the decoder processes the token
?Florida?, the correct hypothesis is promoted to
the top in the beam by the Neighbor Coherence
features for PER-GPE pair. Furthermore, after
408
Model
Entity Mention (%)
Relation (%) Entity Mention + Relation (%)
Score P R F
1
P R F
1
P R F
1
Pipeline 83.2 73.6 78.1 67.5 39.4 49.8 65.1 38.1 48.0
Joint w/ Local 84.5 76.0 80.0 68.4 40.1 50.6 65.3 38.3 48.3
Joint w/ Global 85.2 76.9 80.8 68.9 41.9 52.1 65.4 39.8 49.5
Annotator 1 91.8 89.9 90.9 71.9 69.0 70.4 69.5 66.7 68.1
Annotator 2 88.7 88.3 88.5 65.2 63.6 64.4 61.8 60.2 61.0
Inter-Agreement 85.8 87.3 86.5 55.4 54.7 55.0 52.3 51.6 51.9
Table 2: Overall performance on ACE?05 corpus.
steps hypotheses rank
(a)
ha? marcher?i
1
ha? marcherPERi
2
(b)
ha? marcher? from?i
1
ha? marcherPER from?i
4
(c)
ha? marcherPER from? FloridaGPEi
1
ha? marcher? from? FloridaGPEi
2
(d)
ha? marcherPER from? FloridaGPEi
GEN-AFF
1
ha? marcher? from? FloridaGPEi
4
Figure 7: Two competing hypotheses for ?a
marcher from Florida? during decoding.
linking the two mentions by GEN-AFF relation,
the ranking of the incorrect hypothesis ???,???
is dropped to the 4-th place in the beam, resulting
in a large margin from the correct hypothesis.
The human F
1
score on end-to-end relation ex-
traction is only about 70%, which indicates it is a
very challenging task. Furthermore, the F
1
score
of the inter-annotator agreement is 51.9%, which
is only 2.4% above that of our proposed method.
Compared to human annotators, the bottleneck
of automatic approaches is the low recall of rela-
tion extraction. Among the 631 remaining miss-
ing relations, 318 (50.3%) of them were caused
by missing entity mention arguments. A lot of
nominal mention heads rarely appear in the train-
ing data, such as persons (?supremo?, ?shep-
herd?, ?oligarchs?, ?rich?), geo-political entity
mentions (?stateside?), facilities (?roadblocks?,
?cells?), weapons (?sim lant?, ?nukes?) and ve-
hicles (?prams?). In addition, relations are often
implicitly expressed in a variety of forms. Some
examples are as follows:
? ?Rice has been chosen by President Bush to
become the new Secretary of State? indicates
?Rice? has a PER-SOC relation with ?Bush?.
? ?U.S. troops are now knocking on the door of
Baghdad? indicates ?troops? has a PHYS rela-
tion with ?Baghdad?.
? ?Russia and France sent planes to Baghdad? in-
dicates ?Russia? and ?France? are involved in
an ART relation with ?planes? as owners.
In addition to contextual features, deeper se-
mantic knowledge is required to capture such im-
plicit semantic relations.
5.4 Comparison with State-of-the-art
Table 3 compares the performance on ACE?04
corpus. For entity mention extraction, our joint
model achieved 79.7% on 5-fold cross-validation,
which is comparable with the best F
1
score 79.2%
reported by (Florian et al, 2006) on single-
fold. However, Florian et al (2006) used some
gazetteers and the output of other Information Ex-
traction (IE) models as additional features, which
provided significant gains ((Florian et al, 2004)).
Since these gazetteers, additional data sets and ex-
ternal IE models are all not publicly available, it is
not fair to directly compare our joint model with
their results.
For end-to-end entity mention and relation ex-
traction, both the joint approach and the pipelined
baseline outperform the best results reported
by (Chan and Roth, 2011) under the same setting.
6 Related Work
Entity mention extraction (e.g., (Florian et al,
2004; Florian et al, 2006; Florian et al, 2010; Zi-
touni and Florian, 2008; Ohta et al, 2012)) and
relation extraction (e.g., (Reichartz et al, 2009;
Sun et al, 2011; Jiang and Zhai, 2007; Bunescu
and Mooney, 2005; Zhao and Grishman, 2005;
Culotta and Sorensen, 2004; Zhou et al, 2007;
Qian and Zhou, 2010; Qian et al, 2008; Chan
and Roth, 2011; Plank and Moschitti, 2013)) have
drawn much attention in recent years but were
409
Model
Entity Mention (%)
Relation (%) Entity Mention + Relation (%)
Score P R F
1
P R F
1
P R F
1
Chan and Roth (2011) - 42.9 38.9 40.8 -
Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9
Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1
Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3
Table 3: 5-fold cross-validation on ACE?04 corpus. Bolded scores indicate highly statistical significant
improvement as measured by paired t-test (p < 0.01)
usually studied separately. Most relation extrac-
tion work assumed that entity mention boundaries
and/or types were given. Chan and Roth (2011) re-
ported the best results using predicted entity men-
tions.
Some previous work used relations and en-
tity mentions to enhance each other in joint
inference frameworks, including re-ranking (Ji
and Grishman, 2005), Integer Linear Program-
ming (ILP) (Roth and Yih, 2004; Roth and Yih,
2007; Yang and Cardie, 2013), and Card-pyramid
Parsing (Kate and Mooney, 2010). All these
work noted the advantage of exploiting cross-
component interactions and richer knowledge.
However, they relied on models separately learned
for each subtask. As a key difference, our ap-
proach jointly extracts entity mentions and rela-
tions using a single model, in which arbitrary soft
constraints can be easily incorporated. Some other
work applied probabilistic graphical models for
joint extraction (e.g., (Singh et al, 2013; Yu and
Lam, 2010)). By contrast, our work employs an
efficient joint search algorithm without modeling
joint distribution over numerous variables, there-
fore it is more flexible and computationally sim-
pler. In addition, (Singh et al, 2013) used gold-
standard mention boundaries.
Our previous work (Li et al, 2013) used struc-
tured perceptron with token-based decoder to
jointly predict event triggers and arguments based
on the assumption that entity mentions and other
argument candidates are given as part of the in-
put. In this paper, we solve a more challeng-
ing problem: take raw texts as input and identify
the boundaries, types of entity mentions and rela-
tions all together in a single model. Sarawagi and
Cohen (2004) proposed a segment-based CRFs
model for name tagging. Zhang and Clark (2008)
used a segment-based decoder for word segmenta-
tion and pos tagging. We extended the similar idea
to our end-to-end task by incrementally predicting
relations along with entity mention segments.
7 Conclusions and Future Work
In this paper we introduced a new architecture
for more powerful end-to-end entity mention and
relation extraction. For the first time, we ad-
dressed this challenging task by an incremental
beam-search algorithm in conjunction with struc-
tured perceptron. While detecting mention bound-
aries jointly with other components raises the chal-
lenge of synchronizing multiple assignments in
the same beam, a simple yet effective segment-
based decoder is adopted to solve this problem.
More importantly, we exploited a set of global fea-
tures based on linguistic and logical properties of
the two tasks to predict more coherent structures.
Experiments demonstrated our approach signifi-
cantly outperformed pipelined approaches for both
tasks and dramatically advanced state-of-the-art.
In future work, we plan to explore more soft and
hard constraints to reduce search space as well as
improve accuracy. In addition, we aim to incorpo-
rate other IE components such as event extraction
into the joint model.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award and RPI faculty start-up
grant. The views and conclusions contained in
this document are those of the authors and should
not be interpreted as representing the official poli-
cies, either expressed or implied, of the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on.
410
References
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proc. HLT/EMNLP, pages 724?731.
Yee Seng Chan and Dan Roth. 2010. Exploiting back-
ground knowledge for relation extraction. In Proc.
COLING, pages 152?160.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551?560.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. ACL,
pages 423?429.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In Proc. HLT-NAACL, pages 1?8.
Radu Florian, Hongyan Jing, Nanda Kambhatla, and
Imed Zitouni. 2006. Factorizing complex models:
A case study in mention detection. In Proc. ACL.
Radu Florian, John F. Pitrelli, Salim Roukos, and Imed
Zitouni. 2010. Improving mention detection robust-
ness to noisy input. In Proc. EMNLP, pages 335?
345.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
ACL, pages 1077?1086.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142?151.
Heng Ji and Ralph Grishman. 2005. Improving name
tagging by reference resolution and relation detec-
tion. In Proc. ACL, pages 411?418.
Jing Jiang and ChengXiang Zhai. 2007. A systematic
exploration of the feature space for relation extrac-
tion. In Proc. HLT-NAACL.
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for information extraction. In Proc. ACL,
pages 178?181.
Rohit J. Kate and Raymond Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proc. ACL, pages 203?212.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML, pages 282?289.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73?82.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 449,454.
Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and
Sophia Ananiadou. 2012. Open-domain anatomi-
cal entity mention detection. In Proc. ACL Work-
shop on Detecting Structure in Scholarly Discourse,
pages 27?36.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proc.
ACL, pages 1498?1507.
Longhua Qian and Guodong Zhou. 2010. Clustering-
based stratified seed sampling for semi-supervised
relation classification. In Proc. EMNLP, pages 346?
355.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proc. COLING, pages 697?704.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. CONLL, pages 147?155.
Frank Reichartz, Hannes Korte, and Gerhard Paass.
2009. Composite kernels for relation extraction. In
Proc. ACL-IJCNLP (Short Papers), pages 365?368.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proc. CoNLL.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Proc. NIPS.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proc. ACL, pages 521?529.
411
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640?1649.
Xiaofeng Yu and Wai Lam. 2010. Jointly identifying
entities and extracting relations in encyclopedia text
via a graphical model approach. In Proc. COLING
(Posters), pages 1399?1407.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proc. ACL, pages 1147?1157.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proc. ACL, pages 419?426.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proc. ACL, pages 427?434.
Guodong Zhou, Min Zhang, Dong-Hong Ji, and
Qiaoming Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proc. EMNLP-CoNLL, pages
728?736.
Imed Zitouni and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proc. EMNLP,
pages 600?609.
412
Cross-lingual Slot Filling from Comparable Corpora
Matthew Snover, Xiang Li, Wen-Pin Lin, Zheng Chen, Suzanne Tamang,
Mingmin Ge, Adam Lee, Qi Li, Hao Li, Sam Anzaroot, Heng Ji
Computer Science Department
Queens College and Graduate Center
City University of New York
New York, NY 11367, USA
msnover@qc.cuny.edu, hengji@cs.qc.cuny.edu
Abstract
This paper introduces a new task of
crosslingual slot filling which aims to dis-
cover attributes for entity queries from
crosslingual comparable corpora and then
present answers in a desired language. It is
a very challenging task which suffers from
both information extraction and machine
translation errors. In this paper we ana-
lyze the types of errors produced by five
different baseline approaches, and present
a novel supervised rescoring based valida-
tion approach to incorporate global evi-
dence from very large bilingual compara-
ble corpora. Without using any additional
labeled data this new approach obtained
38.5% relative improvement in Precision
and 86.7% relative improvement in Recall
over several state-of-the-art approaches.
The ultimate system outperformed mono-
lingual slot filling pipelines built on much
larger monolingual corpora.
1 Introduction
The slot filling task at NIST TAC Knowledge
Base Population (KBP) track (Ji et al, 2010)
is a relatively new and popular task with the
goal of automatically building profiles of enti-
ties from large amounts of unstructured data,
and using these profiles to populate an existing
knowledge base. These profiles consist of nu-
merous slots such as ?title?, ?parents? for per-
sons and ?top-employees? for organizations. A
variety of approaches have been proposed to ad-
dress both tasks with considerable success; nev-
ertheless, all of the KBP tasks so far have been
limited to monolingual processing. However, as
the shrinking fraction of the world?s Web pages
are written in English, many slot fills can only
be discovered from comparable documents in
foreign languages. By comparable corpora we
mean texts that are about similar topics, but
are not in general translations of each other.
These corpora are naturally available, for ex-
ample, many news agencies release multi-lingual
news articles on the same day. In this paper we
propose a new and more challenging crosslin-
gual slot filling task, to find information for any
English query from crosslingual comparable cor-
pora, and then present its profile in English.
We developed complementary baseline ap-
proaches which combine two difficult problems:
information extraction (IE) and machine trans-
lation (MT). In this paper we conduct detailed
error analysis to understand how we can exploit
comparable corpora to construct more complete
and accurate profiles.
Many correct answers extracted from our
baselines will be reported multiple times in any
external large collection of comparable docu-
ments. We can thus take advantage of such in-
formation redundancy to rescore candidate an-
swers. To choose the best answers we consult
large comparable corpora and corresponding IE
results. We prefer those answers which fre-
quently appear together with the query in cer-
tain IE contexts, including co-occurring names,
coreference links, relations and events. For ex-
ample, we prefer ?South Korea? instead of ?New
York Stock Exchange? as the ?per:employee of ?
answer for ?Roh Moo-hyun? using global ev-
idence from employment relation extraction.
Such global knowledge from comparable corpora
110
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 110?119,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
provides substantial improvement over each in-
dividual baseline system and even state-of-the-
art monolingual slot filling systems. Compared
to previous methods of exploiting comparable
corpora, our approach is novel in multiple as-
pects because it exploits knowledge from: (1)
both local and global statistics; (2) both lan-
guages; and (3) both shallow and deep analysis.
2 Related Work
Sudo et al (2004) found that for a crosslin-
gual single-document IE task, source language
extraction and fact translation performed no-
tably better than machine translation and tar-
get language extraction. We observed the same
results. In addition we also demonstrate that
these two approaches are complementary and
can be used to boost each other?s results in a
statistical rescoring model with global evidence
from large comparable corpora.
Hakkani-Tur et al (2007) described a filtering
mechanism using two crosslingual IE systems
for improving crosslingual document retrieval.
Many previous validation methods for crosslin-
gual QA, such as those organized by Cross Lan-
guage Evaluation Forum (Vallin et al, 2005), fo-
cused on local information which involves only
the query and answer (e.g. (Kwork and Deng,
2006)), keyword translation (e.g. (Mitamura et
al., 2006)) and surface patterns (e.g. (Soubbotin
and Soubbotin, 2001)). Some global valida-
tion approaches considered information redun-
dancy based on shallow statistics including co-
occurrence, density score and mutual informa-
tion (Clarke et al, 2001; Magnini et al, 2001;
Lee et al, 2008), deeper knowledge from depen-
dency parsing (e.g. (Shen et al, 2006)) or logic
reasoning (e.g. (Harabagiu et al, 2005)). How-
ever, all of these approaches made limited efforts
at disambiguating entities in queries and limited
use of fact extraction in answer search and vali-
dation.
Several recent IE studies have stressed the
benefits of using information redundancy on
estimating the correctness of the IE out-
put (Downey et al, 2005; Yangarber, 2006;
Patwardhan and Riloff, 2009; Ji and Grish-
man, 2008). Some recent research used com-
parable corpora to re-score name translitera-
tions (Sproat et al, 2006; Klementiev and Roth,
2006) or mine new word translations (Fung and
Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao
and Zhai, 2005; Hassan et al, 2007; Udupa et
al., 2009; Ji, 2009). To the best of our knowl-
edge, this is the first work on mining facts from
comparable corpora for answer validation in a
new crosslingual entity profiling task.
3 Experimental Setup
3.1 Task Definition
The goal of the KBP slot filling task is to extract
facts from a large source corpus regarding cer-
tain attributes (?slots?) of an entity, which may
be a person or organization, and use these facts
to augment an existing knowledge base (KB).
Along with each slot answer, the system must
provide the ID of a document which supports
the correctness of this answer. KBP 2010 (Ji et
al., 2010) defines 26 types of attributes for per-
sons (such as the age, birthplace, spouse, chil-
dren, job title, and employing organization) and
16 types of attributes for organizations (such
as the top employees, the founder, the year
founded, the headquarters location, and the sub-
sidiaries).
The new problem we define in this paper is an
extension of this task to a crosslingual paradigm.
Given a query in a target language t and a col-
lection of documents in a source language s,
a system must extract slot answers about the
query and present the answers in t. In this pa-
per we examine a specific setting of s=Chinese
and t=English.
To score crosslingual slot filling, we pool all
the system responses and group equivalent an-
swers into equivalence classes. Each system re-
sponse is rated as correct, wrong, inexact or re-
dundant. Given these judgments, we calculate
the precision, recall and F-measure of each sys-
tem, crediting only correct answers.
3.2 Data and Query Selection
We use the comparable corpora of English
TDT5 (278,358 documents) and Chinese TDT5
111
(56,424 documents) as our source collection.
For query selection, we collected all the en-
tities from the entire source collection and
counted their frequencies. We then selected 50
informative entities (25 persons and 25 organiza-
tions) which were located in the middle range of
frequency counts. Among the 25 person queries,
half are Chinese-specific names, and half are
non-Chinese names. The 25 organizations fol-
low a representative distribution according to
the entity subtypes defined in NIST Automatic
Content Extraction (ACE) program1.
3.3 Baseline Pipelines
3.3.1 Overview
We employ the following two types of base-
line crosslingual slot filling pipelines to process
Chinese documents. Figure 1 and Table 1 shows
the five system pipelines we have used to con-
duct our experiments.
Type A Translate Chinese texts into English,
and apply English slot filling systems to the
translations.
Type B Translate English queries into Chinese,
apply Chinese slot filling systems to Chinese
texts, and translate answers back to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Machine 
Translation 
English 
Texts 
Chinese 
Texts 
English Candidate Answers 
English
Query
English Slot Filling 
 Answer 
Translation  Pattern Matching 
 Supervised
Classification
Chinese Slot Filling 
 Supervised 
Classification 
Chinese 
Query 
 Query 
Translation
Figure 1: Overview of Baseline Crosslingual Slot Fill-
ing Pipelines
1http://www.itl.nist.gov/iad/mig/tests/ace/
 
Pipeline Label Components Data 
(1) English Supervised Classification Mono-
lingual (2) English Pattern Matching 
English 
TDT5 
 
(3) 
MT+English 
Supervised 
Classification Type A  
(4) 
MT+English 
Pattern Matching Cross-lingual 
Type 
B 
 
 
(5) 
Query Translation 
+Chinese Supervised 
Classification 
+Answer Translation 
Chinese 
TDT5 
 
 
 
Table 1: Monolingual and Crosslingual Baseline Slot
Filling Pipelines
3.3.2 Monolingual Slot Filling
We applied a state-of-the-art bilingual slot
filling system (Chen et al, 2010) to process
bilingual comparable corpora. This baseline
system includes a supervised ACE IE pipeline
and a bottom-up pattern matching pipeline.
The IE pipeline includes relation extraction and
event extraction based on maximum entropy
models that incorporate diverse lexical, syntac-
tic, semantic and ontological knowledge. The
extracted ACE relations and events are then
mapped to KBP slot fills. In pattern matching,
we extract and rank patterns based on a dis-
tant supervision approach (Mintz et al, 2009)
that uses entity-attribute pairs from Wikipedia
Infoboxes and Freebase (Bollacker et al, 2008).
We set a low threshold to include more answer
candidates, and then a series of filtering steps
to refine and improve the overall pipeline re-
sults. The filtering steps include removing an-
swers which have inappropriate entity types or
have inappropriate dependency paths to the en-
tities.
3.3.3 Document and Name Translation
We use a statistical, phrase-based MT sys-
tem (Zens and Ney, 2004) to translate Chinese
documents into English for Type A Approaches.
The best translation is computed by using a
weighted log-linear combination of various sta-
tistical models: an n-gram language model, a
phrase translation model and a word-based lex-
112
icon model. The latter two models are used in
source-to-target and target-to-source directions.
The model scaling factors are optimized with re-
spect to the BLEU score similar to (Och, 2003).
The training data includes 200 million running
words in each language. The total language
model training data consists of about 600 mil-
lion running words.
We applied various name mining approaches
from comparable corpora and parallel corpora,
as described in (Ji et al, 2009) to extract and
translate names in queries and answers in Type
B approaches. The accuracy of name translation
is about 88%. For those names not covered by
these pairs, we relied on Google Translate 2 to
obtain results.
4 Analysis of Baseline Pipelines
In this section we analyze the coverage (Sec-
tion 4.1) and precision (Section 4.2) results of
the baseline pipelines. We then illustrate the
potential for global validation from comparable
corpora through a series of examples.
4.1 Coverage Analysis: Toward
Information Fusion
Table 2 summarizes the Precision (P), Recall
(R) and F-measure (F) of baseline pipelines and
the union of their individual results.
Table 2: Baseline Pipeline Results 
System P R F 
(1) 0.08 0.54 0.15 
(2) 0.02 0.35 0.03 Mono- 
lingual Union of 
(1)+(2) 
0.03 0.69 0.05 
(3) 0.04 0.04 0.04 
(4) 0.03 0.25 0.05 
Union of 
(3)+(4) 0.03 0.26 0.05 
(5) 0.04 0.46 0.08 
Cross- 
lingual 
Union of 
(3)+(4)+(5) 0.03 0.56 0.05 
Compara
ble 
Corpora 
Union of 
(1)+(2)+(3)+
(4)+(5) 
0.02 1 0.04 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
2http://translate.google.com/
Although crosslingual pipelines used a much
smaller corpus than monolingual pipelines, they
extracted comparable number of correct answers
(66 vs. 81) with a slightly better precision.
In fact, the crosslingual pipeline (5) performs
even better than monolingual pipeline (2), es-
pecially on the employment slots. In particu-
lar, 96.35% of the correct answers for Chinese-
specific person queries (e.g. ?Tang Jiaxuan?)
were extracted from Chinese data. Even for
those facts discovered from English data, they
are about quite general slots such as ?title? and
?employee of ?. In contrast, Chinese data covers
more diverse biographical slots such as ?family
members? and ?schools attended?.
Compared to the union of Type A approaches
(pipelines (3)+(4)), Pipeline (5) returned many
more correct answers with higher precision. The
main reason is that Type A approaches suffer
from MT errors. For example, MT mistakenly
translated the query name ?Celine Dion? into
?Clinton? and thus English slot filling compo-
nents failed to identify any answers. One can
hypothesize that slot filling on MT output can
be improved by re-training extraction compo-
nents directly from MT output. However, our
experiments of learning patterns from MT out-
put showed negative impact, mainly because
MT errors were too diverse to generalize. In
other cases even though slot filling produced cor-
rect results, MT still failed to translate the an-
swer names correctly. For example, English slot
filling successfully found a potential answer for
?org:founded by? of the query ?Microsoft? from
the following MT output: ?The third largest of
the Microsoft common founder Alan Doss , aged
50, and net assets of US 22 billion.?; however,
the answer string ?Paul Allen? was mistakenly
translated into ?Alan Doss?. MT is not so cru-
cial for ?per:title? slot because it does not require
translation of contexts.
To summarize, 59% of the missing errors were
due to text, query or answer translation errors
and 20% were due to slot filling errors. Never-
theless, the union of (3)+(4)+(5) still contain
more correct answers. These baseline pipelines
were developed from a diverse set of algorithms,
and typically showed strengths in specific slots.
113
In general we can conclude that monolin-
gual and crosslingual pipelines are complemen-
tary. Combining the responses from all baseline
pipelines, we can get similar number of correct
answers compared to one single human annota-
tor.
4.2 Precision Analysis: Toward Global
Validation
The spurious errors from baseline crosslingual
slot filling pipelines reveal both the shortcom-
ings of the MT system and extraction across
languages. Table 3 shows the distribution of
spurious errors.
Pipeline Spurious Errors Distribution
Content Translation 
+ Extraction 
85% 
Query Translation 13% 
Type A 
Answer Translation 2% 
Word Segmentation 34% 
Relation Extraction 33% 
Coreference 17% 
Semantic Type 13% 
Type B 
Slot Type 3% 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Table 3: Distribution of Spurious Errors
Table 3 indicates a majority (85%) of spurious
errors from Type A pipelines were due to ap-
plying monolingual slot filling methods to MT
output which preserves Chinese structure.
As demonstrated in previous work (e.g. (Par-
ton and McKeown, 2010; Ji et al, 2009)),
we also found that many (14.6%) errors were
caused by the low quality of name translation
for queries and answers.
For example, ?????/McGinty? was mis-
takenly translated into the query name ?Kim
Jong-il?, which led to many incorrect answers
such as ?The British Royal joint military re-
search institute? for ?per:employee of ?.
In contrast, the spurious errors from Type B
pipelines were more diverse. Chinese IE com-
ponents severely suffered from word segmen-
tation errors (34%), which were then directly
propagated into Chinese document retrieval and
slot filling. Many segmentation errors occurred
with out-of-vocabulary names, especially per-
son names and nested organization names. For
example, the name ????/Yao Mingbao? was
mistakenly segmented into two words ???/Yao
Ming? and ??/bao?, and thus the document was
mistakenly retrieved for the query ?Yao Ming?.
In many cases (33%) Chinese relation and
event extraction components failed to cap-
ture Chinese-specific structures due to the lim-
ited size of training corpora. For example,
from the context ???????????????
?/Xiao Wan-chang, who were invited to be-
come the economics consultant for Chen Shui-
bian?, Chinese slot filling system mistakenly ex-
tracted ?consultant? as a ?per:title? answer for
the query ?Chen Shui-bian? using a common
pattern ?<query><title>?.
13% of errors were caused due to invalid se-
mantic types for certain slots. For example,
many metaphoric titles such as ?tough guy?
don?t match the definition of ?per:title? in the
annotation guideline ?employment or member-
ship position?.
5 Global Validation
Based on the above motivations we propose to
incorporate global evidence from a very large
collection of comparable documents to refine
local decisions. The central idea is to over-
generate candidate answers from multiple weak
baselines to ensure high upper-bound of recall,
and then conduct effective global validation to
filter spurious errors while keeping good answers
in order to enhance precision.
5.1 Supervised Rescoring
Ideally, we want to choose a validation model
which can pick out important features in a con-
text wider than that used by baseline pipelines.
Merging individual systems to form the union of
answers can be effective, but Table 2 shows that
simple union of all pipelines produced worse F-
measure than the best pipeline.
In this paper we exploit the reranking
paradigm, commonly used in information re-
trieval, to conduct global validation. By model-
ing the empirical distribution of labeled training
data, statistical models are used to identify the
114
strengths and weaknesses (e.g. high and low pre-
cision slots) of individual systems, and rescore
answers accordingly. Specially, we develop a
supervised Maximum Entropy (MaxEnt) based
model to rescore the answers from the pipelines,
selecting only the highest-scoring answers.
The rescorer was trained (using cross-
validation) on varying subsets of the features.
The threshold at which an answer is deemed to
be true is chosen to maximize the F-Measure on
the training set.
5.2 Validation Features
Table 4 describes the validation features used for
rescoring, where q is the query, q? the Chinese
translation of q, t the slot type, a the candidate
answer, a? the Chinese form of a, s the context
sentence and d is the context document support-
ing a.
The feature set benefits from multiple dimen-
sions of crosslingual slot filling. These features
were applied to both languages wherever anno-
tation resources were available.
In the KBP slot filling task, slots are of-
ten dependent on each other, so we can im-
prove the results by improving the ?coherence?
of the story (i.e. consistency among all gener-
ated answers - query profiles). We use feature
f2 to check whether the same answer was gen-
erated for conflicting slots, such as per:parents
and per:children.
Compared to traditional QA tasks, slot fill-
ing is a more fine-grained task in which differ-
ent slots are expected to obtain semantically
different answers. Therefore, we explored se-
mantic constraints in both local and global con-
texts. For example, we utilized bilingual name
gazetteers from ACE training corpora, Google
n-grams (Ji and Lin, 2009) and the geonames
website 3 to encode features f6, f8 and f9; The
org:top members/employees slot requires a sys-
tem to distinguish whether a person member/
employee is in the top position, thus we encoded
f10 for this purpose.
The knowledge used in our baseline pipelines
is relatively static ? it is not updated during the
3http://www.geonames.org/statistics/
extraction process. Achieving high performance
for cross-lingual slot filling requires that we take
a broader view, one that looks outside a sin-
gle document or a single language in order to
exploit global knowledge. Fortunately, as more
and more large crosslingual comparable corpora
are available, we can take advantage of informa-
tion redundancy to validate answers. The basic
intuition is that if a candidate answer a is cor-
rect, it should appear together with the query
q repeatedly, in different documents, or even in
certain coreference links, relations and events.
For example, ?David Kelly - scientist?, and
??????/Shintaro Ishihara - ??/governor?
pairs appear frequently in ?title? coreference
links in both English and Chinese corpora;
?Elizabeth II? is very often involved in an ?em-
ployment? relation with ?United Kingdom? in
English corpora. On the other hand, some in-
correct answers with high global statistics can be
filtered out using these constraints. For exam-
ple, although the query ????/Tang Jiaxuan?
appears frequently together with the candidate
per:title answer ???/personnel?, it is linked by
few coreference links; in contrast, it?s coreferen-
tial with the correct title answer ?????/State
Council member? much more frequently.
We processed cross-lingual comparable cor-
pora to extract coreference links, relations and
events among mentions (names, nominals and
time expressions etc.) and stored them in an
external knowledge base. Any pair of <q, a>
is then compared to the entries in this knowl-
edge base. We used 157,708 documents from
Chinese TDT5 and Gigaword to count Chinese
global statistics, and 7,148,446 documents from
DARPA GALE MT training corpora to count
English global statistics, as shown in features
f12 and f13. Fact based global features f14, f15,
f16 and f17, were calculated from 49,359 Chi-
nese and 280,513 English documents (annotated
by the bilingual IE system in Section 3.3.2.
6 Experiments
In this section, we examine the overall perfor-
mance of this method. We then discuss the
usefulness of the individual sets of features. In
115
Characteristics 
Scope Depth Language 
Description 
f1: frequency of <q, a, t> that appears in all baseline outputs Global 
(Cross-
system) 
Shallow 
 English f2: number of conflicting slot types in which answer a appears in all baseline 
outputs 
f3: conjunction of t and whether a is a year answer Shallow English 
f4: conjunction of t and whether a includes numbers or letters 
f5: conjunction of place t and whether a is a country name 
f6: conjunction of per:origin t and whether a is a nationality 
f7: if t=per:title, whether a is an acceptable title 
f8: if t requires a name answer, whether a is a name 
Local 
Deep 
 
English 
 
f9: whether a has appropriate semantic type 
f10: conjunction of org:top_members/employees and whether there is a high-level 
title in s 
Global 
(Within-
Document) 
Deep English 
f11: conjunction of alternative name and whether a is an acronym of q 
Chinese f12: conditional probability of q/q' and a/a' appear in the same document Shallow 
(Statistics) English f13: conditional probability  of q/q' and a/a' appear in the same sentence 
Both f14:  co-occurrence of q/q' and a/a'  appear in coreference links 
English f15: co-occurrence of q/q' and a/a'  appear in relation/event links 
English f16: conditional probability of q/q' and a/a' appear in relation/event links 
Global 
(Cross-
document 
in 
comparable 
corpora) 
Deep 
(Fact-
based) 
English f17: mutual information of q/q' and a/a' appear in relation/event links 
 
Table 4: Validation Features for Crosslingual Slot Filling
the following results, the baseline features are
always used in addition to any other features.
6.1 Overall Performance
Because of the data scarcity, ten-fold cross-
validation, across queries, was used to train
and test the system. Quantitative results after
combining answers from multiple pipelines are
shown in Table 5. We used two basic features,
one is the slot type and the other is the entity
type of the query (i.e. person or organization).
This basic feature set is already successful in im-
proving the precision of the pipelines, although
this results in a number of correct answers be-
ing discarded as well. By adding the additional
validation features described previously, both
the f-score and precision of the models are im-
proved. In the case of the cross-lingual pipelines
(3+4+5) the number of correct answers chosen
is almost doubled while increasing the precision
of the output.
6.2 Impact of Global Validation
A comparison of the benefits of global versus lo-
cal features are shown in Table 6, both of which
dramatically improve scores over the baseline
features. The global features are universally
Pipelines F P R
Basic Features
1+2 0.31 0.31 0.30
3+4+5 0.26 0.39 0.20
1+2+3+4+5 0.27 0.29 0.25
Full Features
1+2 0.37 0.30 0.46
3+4+5 0.36 0.35 0.37
1+2+3+4+5 0.31 0.28 0.35
Table 5: Using Basic Features to Filter Answers
more beneficial than the local features, although
the local features generate results with higher
precision at the expense of the number of correct
answers returned. The global features are espe-
cially useful for pipelines 3+4+5, where the per-
formance using just these features reaches those
of using all other features ? this does not hold
true for the monolingual pipelines however.
6.3 Impact of Fact-driven Deep
Knowledge
The varying benefit of fact-driven cross-
document features and statistical cross-
document features are shown in Table 7.
116
Pipelines F P R
Local Features
1+2 0.34 0.35 0.33
3+4+5 0.29 0.40 0.22
1+2+3+4+5 0.27 0.32 0.24
Global Features
1+2 0.35 0.30 0.42
3+4+5 0.37 0.36 0.38
1+2+3+4+5 0.33 0.29 0.38
Table 6: The Benefit of Global versus Local Features
While both feature sets are beneficial, the
monolingual pipelines (1+2) benefit more
from statistical features while the cross-lingual
pipelines (3+4+7) benefit slightly more from
the fact-based features. Despite this bias, the
overall results when the features are used in
all pipelines are very close with the fact-based
features being slightly more useful overall.
Pipelines F P R
Fact-Based Features
1+2 0.33 0.27 0.42
3+4+5 0.35 0.43 0.29
1+2+3+4+5 0.30 0.27 0.34
Statistical Features
1+2 0.37 0.34 0.40
3+4+5 0.34 0.35 0.33
1+2+3+4+5 0.29 0.25 0.34
Table 7: Fact vs. Statistical Cross-Doc Features
Translation features were only beneficial to
pipelines 3, 4, and 5, and provided a slight in-
crease in precision from 0.39 to 0.42, but pro-
vided no noticeable benefit when used in con-
junction with results from pipelines 1 and 2.
This is because the answers where translation
features would be most useful were already be-
ing selected by pipelines 1 and 2 using the base-
line features.
6.4 Discussion
The use of any re-scoring, even with baseline
features, provides large gains over the union of
the baseline pipelines, removing large number
of incorrect answers. The use of more sophis-
ticated features provided substantial gains over
the baseline features. In particular, global fea-
tures proved very effective. Further feature en-
gineering to address the remaining errors and
the dropped correct answer would likely provide
increasing gains in performance.
In addition, two human annotators, indepen-
dently, conducted the same task on the same
data, with a second pass of adjudication. The F-
scores of inter-annotator agreement were 52.0%
for the first pass and 73.2% for the second pass.
This indicates that slot filling remains a chal-
lenging task for both systems and human anno-
tators?only one monolingual system exceeded
30% F-score in the KBP2010 evaluation.
7 Conclusion and Future Work
Crosslingual slot filling is a challenging task
due to limited performance in two separate ar-
eas: information extraction and machine trans-
lation. Various methods of combining tech-
niques from these two areas provided weak yet
complementary baseline pipelines. We proposed
an effective approach to integrate these base-
lines and enhance their performance using wider
and deeper knowledge from comparable cor-
pora. The final system based on cross-lingual
comparable corpora outperformed monolingual
pipelines on much larger monolingual corpora.
The intuition behind our approach is that
over-generation of candidate answers from weak
baselines provides a potentially strong recall
upper-bound. The remaining enhancement be-
comes simpler: filtering errors. Our experiments
also suggest that our rescoring models tend to
over-fit due to small amount of training data.
Manual annotation and assessment are quite
costly, motivating future work in active learning
and semi-supervised learning methods. In addi-
tion, we plan to apply our results as feedback to
improve MT performance on facts using query
and answer-driven language model adaptation.
We have demonstrated our approach on English-
Chinese pair, but the framework is language-
independent; ultimately we would like to extend
the task to extracting information from more
languages.
117
Acknowledgments
This work was supported by the U.S. NSF CAREER
Award under Grant IIS-0953149 and PSC-CUNY
Research Program. Any opinions, findings, and con-
clusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily
reflect the views of the National Science Foundation.
References
K. Bollacker, R. Cook, and P. Tufts. 2008. Free-
base: A shared database of structured general hu-
man knowledge. In Proc. National Conference on
Artificial Intelligence.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Marissa Passantino, and Heng Ji. 2010. Top-
down and bottom-up: A combined approach to
slot filling. Lecture Notes in Computer Science,
6458:300?309, December.
C. L. A. Clarke, G. V. Cormack, and T.R. Lynam.
2001. Exploiting redundancy in question answer-
ing. In Proc. SIGIR2001.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A Probabilistic Model of Redundancy in
Information Extraction. In Proc. IJCAI 2005.
Pascale Fung and Lo Yuen Yee. 1998. An ir ap-
proach for translating new words from nonparallel
and comparable texts. In COLING-ACL.
Dilek Hakkani-Tur, Heng Ji, and Ralph Grishman.
2007. Using information extraction to improve
cross-lingual document retrieval. In Proc. RANLP
workshop on Multi-source, Multilingual Informa-
tion Extraction and Summarization.
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden,
A. Hickl, and P. Wang. 2005. Employing two
question answering systems in trec 2005. In Proc.
TREC2005.
Ahmed Hassan, Haytham Fahmy, and Hany Has-
san. 2007. Improving named entity translation
by exploiting comparable and parallel corpora. In
RANLP.
Heng Ji and Ralph Grishman. 2008. Refining Event
Extraction through Cross-Document Inference. In
Proc. of ACL-08: HLT, pages 254?262.
Heng Ji and Dekang Lin. 2009. Gender and animacy
knowledge discovery from web-scale n-grams for
unsupervised person mention detection. In Proc.
PACLIC2009.
Heng Ji, Ralph Grishman, Dayne Freitag, Matthias
Blume, John Wang, Shahram Khadivi, Richard
Zens, and Hermann Ney. 2009. Name translation
for distillation. Handbook of Natural Language
Processing and Machine Translation: DARPA
Global Autonomous Language Exploitation.
Heng Ji, Ralph Grishman, Hoa Trang Dang, and
Kira Griffitt. 2010. An overview of the tac2010
knowledge base population track. In Proc.
TAC2010.
Heng Ji. 2009. Mining name translations from com-
parable corpora by creating bilingual information
networks. In ACL-IJCNLP 2009 workshop on
Building and Using Comparable Corpora (BUCC
2009): from Parallel to Non-parallel Corpora.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL 2006.
K.-L. Kwork and P. P. Deng. 2006. Chinese
question-answering: Comparing monolingual with
english-chinese cross-lingual results. In Asia In-
formation Retrieval Symposium.
Cheng-Wei Lee, Yi-Hsun Lee, and Wen-Lian Hsu.
2008. Exploring shallow answer ranking features
in cross-lingual and monolingual factoid question
answering. Computational Linguistics and Chi-
nese Language Processing, 13:1?26, March.
B. Magnini, M. Negri, R. Prevete, and H. Tanev.
2001. Is it the right answer?: Exploiting web
redundancy for answer validation. In Proc.
ACL2001.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In ACL-IJCNLP
2009.
Teruko Mitamura, Mengqiu Wang, Hideki Shima,
and Frank Lin. 2006. Keyword translation accu-
racy and cross-lingual question answering in chi-
nese and japanese. In EACL 2006 Workshop on
MLQA.
F. J. Och. 2003. Minimum error rate training in
statistical machine translaton. In Proc.ACL2003.
Kristen Parton and Kathleen McKeown. 2010. Mt
error detection for cross-lingual question answer-
ing. Proc. COLING2010.
Siddharth Patwardhan and Ellen Riloff. 2009. A
Unified Model of Phrasal and Sentential Evidence
for Information Extraction. In Proc. EMNLP
2009.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and ger-
man corpora. In ACL 1999.
Li Shao and Hwee Tou Ng. 2004. Mining new word
translations from comparable corpora. In COL-
ING2004.
D. Shen, G. Saarbruechen, and D. Klakow. 2006.
Exploring correlation of dependency relation
paths for answer extraction. In Proc. ACL2006.
118
M. M. Soubbotin and S. M. Soubbotin. 2001. Pat-
terns of potential answer expressions as clues to
the right answers. In Proc. TREC2001.
Richard Sproat, Tao Tao, and ChengXiang Zhai.
2006. Named entity transliteration with compa-
rable corpora. In ACL 2006.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2004. Cross-lingual information extraction evalu-
ation. In Proc. COLING2004.
Tao Tao and Chengxiang Zhai. 2005. Mining com-
parable bilingual text corpora for cross-language
information integration. In Proc. KDD2005.
Raghavendra Udupa, K. Saravanan, A. Kumaran,
and Jagadeesh Jagarlamudi. 2009. Mint: A
method for effective and scalable mining of named
entity transliterations from large comparable cor-
pora. In EACL2009.
Alessandro Vallin, Bernardo Magnini, Danilo Gi-
ampiccolo, Lili Aunimo, Christelle Ayache, Petya
Osenova, Anselmo Peas, Maaren de Rijke, Bogdan
Sacaleanu, Diana Santos, and Richard Sutcliffe.
2005. Overview of the clef 2005 multilingual ques-
tion answer track. In Proc. CLEF2005.
Roman Yangarber. 2006. Verification of Facts across
Document Boundaries. In Proc. International
Workshop on Intelligent Information Access.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In Proc. HLT/NAACL 2004.
119
