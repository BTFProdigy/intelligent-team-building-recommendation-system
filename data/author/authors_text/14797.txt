Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747?756,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Midge: Generating Image Descriptions From Computer Vision
Detections
Margaret Mitchell?
Xufeng Han?
Jesse Dodge??
Alyssa Mensch??
Amit Goyal??
Alex Berg?
Kota Yamaguchi?
Tamara Berg?
Karl Stratos?
Hal Daume? III??
?U. of Aberdeen and Oregon Health and Science University, m.mitchell@abdn.ac.uk
? Stony Brook University, {aberg,tlberg,xufhan,kyamagu}@cs.stonybrook.edu
??U. of Maryland, {hal,amit}@umiacs.umd.edu
?Columbia University, stratos@cs.columbia.edu
??U. of Washington, dodgejesse@gmail.com, ??MIT, acmensch@mit.edu
Abstract
This paper introduces a novel generation
system that composes humanlike descrip-
tions of images from computer vision de-
tections. By leveraging syntactically in-
formed word co-occurrence statistics, the
generator filters and constrains the noisy
detections output from a vision system to
generate syntactic trees that detail what
the computer vision system sees. Results
show that the generation system outper-
forms state-of-the-art systems, automati-
cally generating some of the most natural
image descriptions to date.
1 Introduction
It is becoming a real possibility for intelligent sys-
tems to talk about the visual world. New ways of
mapping computer vision to generated language
have emerged in the past few years, with a fo-
cus on pairing detections in an image to words
(Farhadi et al 2010; Li et al 2011; Kulkarni et
al., 2011; Yang et al 2011). The goal in connect-
ing vision to language has varied: systems have
started producing language that is descriptive and
poetic (Li et al 2011), summaries that add con-
tent where the computer vision system does not
(Yang et al 2011), and captions copied directly
from other images that are globally (Farhadi et al
2010) and locally similar (Ordonez et al 2011).
A commonality between all of these ap-
proaches is that they aim to produce natural-
sounding descriptions from computer vision de-
tections. This commonality is our starting point:
We aim to design a system capable of producing
natural-sounding descriptions from computer vi-
sion detections that are flexible enough to become
more descriptive and poetic, or include likely in-
The bus by the road with a clear blue sky
Figure 1: Example image with generated description.
formation from a language model, or to be short
and simple, but as true to the image as possible.
Rather than using a fixed template capable of
generating one kind of utterance, our approach
therefore lies in generating syntactic trees. We
use a tree-generating process (Section 4.3) simi-
lar to a Tree Substitution Grammar, but preserv-
ing some of the idiosyncrasies of the Penn Tree-
bank syntax (Marcus et al 1995) on which most
statistical parsers are developed. This allows us
to automatically parse and train on an unlimited
amount of text, creating data-driven models that
flesh out descriptions around detected objects in a
principled way, based on what is both likely and
syntactically well-formed.
An example generated description is given in
Figure 1, and example vision output/natural lan-
guage generation (NLG) input is given in Fig-
ure 2. The system (?Midge?) generates descrip-
tions in present-tense, declarative phrases, as a
na??ve viewer without prior knowledge of the pho-
tograph?s content.1
Midge is built using the following approach:
An image processed by computer vision algo-
rithms can be characterized as a triple <Ai, Bi,
Ci>, where:
1Midge is available to try online at:
http://recognition.cs.stonybrook.edu:8080/?mitchema/midge/.
747
stuff: sky .999
id: 1
atts: clear:0.432, blue:0.945
grey:0.853, white:0.501 ...
b. box: (1,1 440,141)
stuff: road .908
id: 2
atts: wooden:0.722 clear:0.020 ...
b. box: (1,236 188,94)
object: bus .307
id: 3
atts: black:0.872, red:0.244 ...
b. box: (38,38 366,293)
preps: id 1, id 2: by id 1, id 3: by id 2, id 3: below
Figure 2: Example computer vision output and natu-
ral language generation input. Values correspond to
scores from the vision detections.
? Ai is the set of object/stuff detections with
bounding boxes and associated ?attribute?
detections within those bounding boxes.
? Bi is the set of action or pose detections as-
sociated to each ai ? Ai.
? Ci is the set of spatial relationships that hold
between the bounding boxes of each pair
ai, aj ? Ai.
Similarly, a description of an image can be char-
acterized as a triple <Ad, Bd, Cd> where:
? Ad is the set of nouns in the description with
associated modifiers.
? Bd is the set of verbs associated to each ad ?
Ad.
? Cd is the set of prepositions that hold be-
tween each pair of ad, ae ? Ad.
With this representation, mapping <Ai, Bi, Ci>
to <Ad, Bd, Cd> is trivial. The problem then
becomes: (1) How to filter out detections that
are wrong; (2) how to order the objects so that
they are mentioned in a natural way; (3) how to
connect these ordered objects within a syntacti-
cally/semantically well-formed tree; and (4) how
to add further descriptive information from lan-
guage modeling alone, if required.
Our solution lies in usingAi andAd as descrip-
tion anchors. In computer vision, object detec-
tions form the basis of action/pose, attribute, and
spatial relationship detections; therefore, in our
approach to language generation, nouns for the
object detections are used as the basis for the de-
scription. Likelihood estimates of syntactic struc-
ture and word co-occurrence are conditioned on
object nouns, and this enables each noun head in
a description to select for the kinds of structures it
tends to appear in (syntactic constraints) and the
other words it tends to occur with (semantic con-
straints). This is a data-driven way to generate
likely adjectives, prepositions, determiners, etc.,
taking the intersection of what the vision system
predicts and how the object noun tends to be de-
scribed.
2 Background
Our approach to describing images starts with
a system from Kulkarni et al(2011) that com-
poses novel captions for images in the PASCAL
sentence data set,2 introduced in Rashtchian et
al. (2010). This provides multiple object detec-
tions based on Felzenszwalb?s mixtures of multi-
scale deformable parts models (Felzenszwalb et
al., 2008), and stuff detections (roughly, mass
nouns, things like sky and grass) based on linear
SVMs for low level region features.
Appearance characteristics are predicted using
trained detectors for colors, shapes, textures, and
materials, an idea originally introduced in Farhadi
et al(2009). Local texture, Histograms of Ori-
ented Gradients (HOG) (Dalal and Triggs, 2005),
edge, and color descriptors inside the bounding
box of a recognized object are binned into his-
tograms for a vision system to learn to recognize
when an object is rectangular, wooden, metal,
etc. Finally, simple preposition functions are used
to compute the spatial relations between objects
based on their bounding boxes.
The original Kulkarni et al(2011) system gen-
erates descriptions with a template, filling in slots
by combining computer vision outputs with text
based statistics in a conditional random field to
predict the most likely image labeling. Template-
based generation is also used in the recent Yang et
al. (2011) system, which fills in likely verbs and
prepositions by dependency parsing the human-
written UIUC Pascal-VOC dataset (Farhadi et al
2010) and selecting the dependent/head relation
with the highest log likelihood ratio.
Template-based generation is useful for auto-
matically generating consistent sentences, how-
ever, if the goal is to vary or add to the text pro-
duced, it may be suboptimal (cf. Reiter and Dale
(1997)). Work that does not use template-based
generation includes Yao et al(2010), who gener-
ate syntactic trees, similar to the approach in this
2http://vision.cs.uiuc.edu/pascal-sentences/
748
Kulkarni et al This is a pic-
ture of three persons, one bot-
tle and one diningtable. The
first rusty person is beside the
second person. The rusty bot-
tle is near the first rusty per-
son, and within the colorful
diningtable. The second per-
son is by the third rusty per-
son. The colorful diningtable
is near the first rusty person,
and near the second person,
and near the third rusty person.
Kulkarni et al This is
a picture of two potted-
plants, one dog and one
person. The black dog is
by the black person, and
near the second feathered
pottedplant.
Yang et al Three people
are showing the bottle on the
street
Yang et al The person is
sitting in the chair in the
room
Midge: people with a bottle at
the table
Midge: a person in black
with a black dog by potted
plants
Figure 3: Descriptions generated by Midge, Kulkarni
et al(2011) and Yang et al(2011) on the same images.
Midge uses the Kulkarni et al(2011) front-end, and so
outputs are directly comparable.
paper. However, their system is not automatic, re-
quiring extensive hand-coded semantic and syn-
tactic details. Another approach is provided in
Li et al(2011), who use image detections to se-
lect and combine web-scale n-grams (Brants and
Franz, 2006). This automatically generates de-
scriptions that are either poetic or strange (e.g.,
?tree snowing black train?).
A different line of work transfers captions of
similar images directly to a query image. Farhadi
et al(2010) use <object,action,scene> triples
predicted from the visual characteristics of the
image to find potential captions. Ordonez et al
(2011) use global image matching with local re-
ordering from a much larger set of captioned pho-
tographs. These transfer-based approaches result
in natural captions (they are written by humans)
that may not actually be true of the image.
This work learns and builds from these ap-
proaches. Following Kulkarni et aland Li et al
the system uses large-scale text corpora to esti-
mate likely words around object detections. Fol-
lowing Yang et al the system can hallucinate
likely words using word co-occurrence statistics
alone. And following Yao et al the system aims
black, blue, brown, colorful, golden, gray,
green, orange, pink, red, silver, white, yel-
low, bare, clear, cute, dirty, feathered, flying,
furry, pine, plastic, rectangular, rusty, shiny,
spotted, striped, wooden
Table 1: Modifiers used to extract training corpus.
for naturally varied but well-formed text, generat-
ing syntactic trees rather than filling in a template.
In addition to these tasks, Midge automatically
decides what the subject and objects of the de-
scription will be, leverages the collected word co-
occurrence statistics to filter possible incorrect de-
tections, and offers the flexibility to be as de-
scriptive or as terse as possible, specified by the
user at run-time. The end result is a fully au-
tomatic vision-to-language system that is begin-
ning to generate syntactically and semantically
well-formed descriptions with naturalistic varia-
tion. Example descriptions are given in Figures 4
and 5, and descriptions from other recent systems
are given in Figure 3.
The results are promising, but it is important to
note that Midge is a first-pass system through the
steps necessary to connect vision to language at
a deep syntactic/semantic level. As such, it uses
basic solutions at each stage of the process, which
may be improved: Midge serves as an illustration
of the types of issues that should be handled to
automatically generate syntactic trees from vision
detections, and offers some possible solutions. It
is evaluated against the Kulkarni et alsystem, the
Yang et alsystem, and human-written descrip-
tions on the same set of images in Section 5, and
is found to significantly outperform the automatic
systems.
3 Learning from Descriptive Text
To train our system on how people describe im-
ages, we use 700,000 (Flickr, 2011) images with
associated descriptions from the dataset in Or-
donez et al(2011). This is separate from our
evaluation image set, consisting of 840 PASCAL
images. The Flickr data is messier than datasets
created specifically for vision training, but pro-
vides the largest corpus of natural descriptions of
images to date.
We normalize the text by removing emoticons
and mark-up language, and parse each caption
using the Berkeley parser (Petrov, 2010). Once
parsed, we can extract syntactic information for
individual (word, tag) pairs.
749
a cow with sheep with a gray sky people with boats a brown cow people at
green grass by the road a wooden table
Figure 4: Example generated outputs.
Awkward Prepositions Incorrect Detections
a person boats under a black bicycle at the sky a yellow bus cows by black sheep
on the dog the sky a green potted plant with people by the road
Figure 5: Example generated outputs: Not quite right
We compute the probabilities for different
prenominal modifiers (shiny, clear, glowing, ...)
and determiners (a/an, the, None, ...) given a
head noun in a noun phrase (NP), as well as the
probabilities for each head noun in larger con-
structions, listed in Section 4.3. Probabilities are
conditioned only on open-class words, specifi-
cally, nouns and verbs. This means that a closed-
class word (such as a preposition) is never used to
generate an open-class word.
In addition to co-occurrence statistics, the
parsed Flickr data adds to our understanding of
the basic characteristics of visually descriptive
text. Using WordNet (Miller, 1995) to automati-
cally determine whether a head noun is a physical
object or not, we find that 92% of the sentences
have no more than 3 physical objects. This in-
forms generation by placing a cap on how many
objects are mentioned in each descriptive sen-
tence: When more than 3 objects are detected,
the system splits the description over several sen-
tences. We also find that many of the descriptions
are not sentences as well (tagged as S, 58% of the
data), but quite commonly noun phrases (tagged
as NP, 28% of the data), and expect that the num-
ber of noun phrases that form descriptions will be
much higher with domain adaptation. This also
informs generation, and the system is capable of
generating both sentences (contains a main verb)
and noun phrases (no main verb) in the final im-
age description. We use the term ?sentence? in the
rest of this paper to refer to both kinds of complex
phrases.
4 Generation
Following Penn Treebank parsing guidelines
(Marcus et al 1995), the relationship between
two head nouns in a sentence can usually be char-
acterized among the following:
1. prepositional (a boy on the table)
2. verbal (a boy cleans the table)
3. verb with preposition (a boy sits on the table)
4. verb with particle (a boy cleans up the table)
5. verb with S or SBAR complement (a boy
sees that the table is clean)
The generation system focuses on the first three
kinds of relationships, which capture a wide range
of utterances. The process of generation is ap-
proached as a problem of generating a semanti-
cally and syntactically well-formed tree based on
object nouns. These serve as head noun anchors
in a lexicalized syntactic derivation process that
we call tree growth.
Vision detections are associated to a {tag
word} pair, and the model fleshes out the tree de-
tails around head noun anchors by utilizing syn-
tactic dependencies between words learned from
the Flickr data discussed in Section 3. The anal-
ogy of growing a tree is quite appropriate here,
where nouns are bundles of constraints akin to
seeds, giving rise to the rest of the tree based on
the lexicalized subtrees in which the nouns are
likely to occur. An example generated tree struc-
ture is shown in Figure 6, with noun anchors in
bold.
750
NP
PP
NP
NN
table
DT
the
IN
at
NP
PP
NP
NN
bottle
DT
a
IN
with
NP
NN
people
DT
-
Figure 6: Tree generated from tree growth process.
Midge was developed using detections run on
Flickr images, incorporating action/pose detec-
tions for verbs as well as object detections for
nouns. In testing, we generate descriptions for
the PASCAL images, which have been used in
earlier work on the vision-to-language connection
(Kulkarni et al 2011; Yang et al 2011), and al-
lows us to compare systems directly. Action and
pose detection for this data set still does not work
well, and so the system does not receive these de-
tections from the vision front-end. However, the
system can still generate verbs when action and
pose detectors have been run, and this framework
allows the system to ?hallucinate? likely verbal
constructions between objects if specified at run-
time. A similar approach was taken in Yang et al
(2011). Some examples are given in Figure 7.
We follow a three-tiered generation process
(Reiter and Dale, 2000), utilizing content determi-
nation to first cluster and order the object nouns,
create their local subtrees, and filter incorrect de-
tections; microplanning to construct full syntactic
trees around the noun clusters, and surface real-
ization to order selected modifiers, realize them as
postnominal or prenominal, and select final out-
puts. The system follows an overgenerate-and-
select approach (Langkilde and Knight, 1998),
which allows different final trees to be selected
with different settings.
4.1 Knowledge Base
Midge uses a knowledge base that stores models
for different tasks during generation. These mod-
els are primarily data-driven, but we also include
a hand-built component to handle a small set of
rules. The data-driven component provides the
syntactically informed word co-occurrence statis-
tics learned from the Flickr data, a model for or-
dering the selected nouns in a sentence, and a
model to change computer vision attributes to at-
tribute:value pairs. Below, we discuss the three
main data-driven models within the generation
Unordered Ordered
bottle, table, person ? person, bottle, table
road, sky, cow ? cow, road, sky
Figure 8: Example nominal orderings.
pipeline. The hand-built component contains plu-
ral forms of singular nouns, the list of possible
spatial relations shown in Table 3, and a map-
ping between attribute values and modifier sur-
face forms (e.g., a green detection for person is to
be realized as the postnominal modifier in green).
4.2 Content Determination
4.2.1 Step 1: Group the Nouns
An initial set of object detections must first be
split into clusters that give rise to different sen-
tences. If more than 3 objects are detected in the
image, the system begins splitting these into dif-
ferent noun groups. In future work, we aim to
compare principled approaches to this task, e.g.,
using mutual information to cluster similar nouns
together. The current system randomizes which
nouns appear in the same group.
4.2.2 Step 2: Order the Nouns
Each group of nouns are then ordered to deter-
mine when they are mentioned in a sentence. Be-
cause the system generates declarative sentences,
this automatically determines the subject and ob-
jects. This is a novel contribution for a general
problem in NLG, and initial evaluation (Section
5) suggests it works reasonably well.
To build the nominal ordering model, we use
WordNet to associate all head nouns in the Flickr
data to all of their hypernyms. A description is
represented as an ordered set [a1...an] where each
ap is a noun with position p in the set of head
nouns in the sentence. For the position pi of each
hypernym ha in each sentence with n head nouns,
we estimate p(pi|n, ha).
During generation, the system greedily maxi-
mizes p(pi|n, ha) until all nouns have been or-
dered. Example orderings are shown in Figure 8.
This model automatically places animate objects
near the beginning of a sentence, which follows
psycholinguistic work in object naming (Branigan
et al 2007).
4.2.3 Step 3: Filter Incorrect Attributes
For the system to be able to extend coverage as
new computer vision attribute detections become
available, we develop a method to automatically
751
A person sitting on a sofa Cows grazing Airplanes flying A person walking a dog
Figure 7: Hallucinating: Creating likely actions. Straightforward to do, but can often be wrong.
COLOR purple blue green red white ...
MATERIAL plastic wooden silver ...
SURFACE furry fluffy hard soft ...
QUALITY shiny rust dirty broken ...
Table 2: Example attribute classes and values.
group adjectives into broader attribute classes,3
and the generation system uses these classes when
deciding how to describe objects. To group adjec-
tives, we use a bootstrapping technique (Kozareva
et al 2008) that learns which adjectives tend to
co-occur, and groups these together to form an at-
tribute class. Co-occurrence is computed using
cosine (distributional) similarity between adjec-
tives, considering adjacent nouns as context (i.e.,
JJ NN constructions). Contexts (nouns) for adjec-
tives are weighted using Pointwise Mutual Infor-
mation and only the top 1000 nouns are selected
for every adjective. Some of the learned attribute
classes are given in Table 2.
In the Flickr corpus, we find that each attribute
(COLOR, SIZE, etc.), rarely has more than a single
value in the final description, with the most com-
mon (COLOR) co-occurring less than 2% of the
time. Midge enforces this idea to select the most
likely word v for each attribute from the detec-
tions. In a noun phrase headed by an object noun,
NP{NN noun}, the prenominal adjective (JJ v) for
each attribute is selected using maximum likeli-
hood.
4.2.4 Step 4: Group Plurals
How to generate natural-sounding spatial rela-
tions and modifiers for a set of objects, as opposed
to a single object, is still an open problem (Fu-
nakoshi et al 2004; Gatt, 2006). In this work, we
use a simple method to group all same-type ob-
jects together, associate them to the plural form
listed in the KB, discard the modifiers, and re-
turn spatial relations based on the first recognized
3What in computer vision are called attributes are called
values in NLG. A value like red belongs to a COLOR at-
tribute, and we use this distinction in the system.
member of the group.
4.2.5 Step 5: Gather Local Subtrees Around
Object Nouns
1 2
NP
NN
n
JJ* ?DT{0,1} ? S
VP{VBZ} ?NP{NN n}
3 4
NP
VP{VB(G|N)} ?NP{NN n}
NP
PP{IN} ?NP{NN n}
5 6
PP
NP{NN n}IN ?
VP
PP{IN} ?VB(G|N|Z) ?
7
VP
NP{NN n}VB(G|N|Z) ?
Figure 9: Initial subtree frames for generation, present-
tense declarative phrases. ? marks a substitution site,
* marks ? 0 sister nodes of this type permitted, {0,1}
marks that this node can be included of excluded.
Input: set of ordered nouns, Output: trees preserving
nominal ordering.
Possible actions/poses and spatial relationships
between objects nouns, represented by verbs and
prepositions, are selected using the subtree frames
listed in Figure 9. Each head noun selects for its
likely local subtrees, some of which are not fully
formed until the Microplanning stage. As an ex-
ample of how this process works, see Figure 10,
which illustrates the combination of Trees 4 and
5. For simplicity, we do not include the selection
of further subtrees. The subject noun duck se-
lects for prepositional phrases headed by different
prepositions, and the object noun grass selects
for prepositions that head the prepositional phrase
in which it is embedded. Full PP subtrees are cre-
ated during Microplanning by taking the intersec-
tion of both.
The leftmost noun in the sequence is given a
rightward directionality constraint, placing it as
the subject of the sentence, and so it will only se-
752
a over b a above b b below a b beneath a a by b b by a a on b b under a
b underneath a a upon b a over b
a by b a against b b against a b around a a around b a at b b at a a beside b
b beside a a by b b by a a near b b near a b with a a with b
a in b a in b b outside a a within b a by b b by a
Table 3: Possible prepositions from bounding boxes.
Subtree frames:
NP
PP{IN} ?NP{NN n1}
PP
NP{NN n2}IN ?
Generated subtrees:
NP
PP
IN
above, on, by
NP
NN
duck
PP
NP
NN
grass
IN
on, by, over
Combined trees:
NP
PP
NP
NN
grass
IN
on
NP
NN
duck
NP
PP
NP
NN
grass
IN
by
NP
NN
duck
Figure 10: Example derivation.
lect for trees that expand to the right. The right-
most noun is given a leftward directionality con-
straint, placing it as an object, and so it will only
select for trees that expand to its left. The noun in
the middle, if there is one, selects for all its local
subtrees, combining first with a noun to its right
or to its left. We now walk through the deriva-
tion process for each of the listed subtree frames.
Because we are following an overgenerate-and-
select approach, all combinations above a proba-
bility threshold ? and an observation cutoff ? are
created.
Tree 1:
Collect all NP? (DT det) (JJ adj)* (NN noun)
and NP? (JJ adj)* (NN noun) subtrees, where:
? p((JJ adj)|(NN noun)) > ? for each adj
? p((DT det)|JJ, (NN noun)) > ?, and the proba-
bility of a determiner for the head noun is higher
than the probability of no determiner.
Any number of adjectives (including none) may
be generated, and we include the presence or ab-
sence of an adjective when calculating which de-
terminer to include.
The reasoning behind the generation of these
subtrees is to automatically learn whether to treat
a given noun as a mass or count noun (not taking a
determiner or taking a determiner, respectively) or
as a given or new noun (phrases like a sky sound
unnatural because sky is given knowledge, requir-
ing the definite article the). The selection of de-
terminer is not independent of the selection of ad-
jective; a sky may sound unnatural, but a blue sky
is fine. These trees take the dependency between
determiner and adjective into account.
Trees 2 and 3:
Collect beginnings of VP subtrees headed by
(VBZ verb), (VBG verb), and (VBN verb), no-
tated here as VP{VBX verb}, where:
? p(VP{VBX verb}|NP{NN noun}=SUBJ) > ?
Tree 4:
Collect beginnings of PP subtrees headed by (IN
prep), where:
? p(PP{IN prep}|NP{NN noun}=SUBJ) > ?
Tree 5:
Collect PP subtrees headed by (IN prep) with
NP complements (OBJ) headed by (NN noun),
where:
? p(PP{IN prep}|NP{NN noun}=OBJ) > ?
Tree 6:
Collect VP subtrees headed by (VBX verb) with
embedded PP complements, where:
? p(PP{IN prep}|VP{VBX verb}=SUBJ) > ?
Tree 7:
Collect VP subtrees headed by (VBX verb) with
embedded NP objects, where:
? p(VP{VBX verb}|NP{NN noun}=OBJ) > ?
4.3 Microplanning
4.3.1 Step 6: Create Full Trees
In Microplanning, full trees are created by tak-
ing the intersection of the subtrees created in Con-
tent Determination. Because the nouns are or-
dered, it is straightforward to combine the sub-
trees surrounding a noun in position 1 with sub-
trees surrounding a noun in position 2. Two
753
VP
VP* ?
NP
NP ?CC
and
NP ?
Figure 11: Auxiliary trees for generation.
further trees are necessary to allow the subtrees
gathered to combine within the Penn Treebank
syntax. These are given in Figure 11. If two
nouns in a proposed sentence cannot be combined
with prepositions or verbs, we backoff to combine
them using (CC and).
Stepping through this process, all nouns will
have a set of subtrees selected by Tree 1. Prepo-
sitional relationships between nouns are created
by substituting Tree 1 subtrees into the NP nodes
of Trees 4 and 5, as shown in Figure 10. Verbal
relationships between nouns are created by substi-
tuting Tree 1 subtrees into Trees 2, 3, and 7. Verb
with preposition relationships are created between
nouns by substituting the VBX node in Tree 6
with the corresponding node in Trees 2 and 3 to
grow the tree to the right, and the PP node in Tree
6 with the corresponding node in Tree 5 to grow
the tree to the left. Generation of a full tree stops
when all nouns in a group are dominated by the
same node, either an S or NP.
4.4 Surface Realization
In the surface realization stage, the system se-
lects a single tree from the generated set of pos-
sible trees and removes mark-up to produce a fi-
nal string. This is also the stage where punctua-
tion may be added. Different strings may be gen-
erated depending on different specifications from
the user, as discussed at the beginning of Section
4 and shown in the online demo. To evaluate the
system against other systems, we specify that the
system should (1) not hallucinate likely verbs; and
(2) return the longest string possible.
4.4.1 Step 7: Get Final Tree, Clear Mark-Up
We explored two methods for selecting a final
string. In one method, a trigram language model
built using the Europarl (Koehn, 2005) data with
start/end symbols returns the highest-scoring de-
scription (normalizing for length). In the second
method, we limit the generation system to select
the most likely closed-class words (determiners,
prepositions) while building the subtrees, over-
generating all possible adjective combinations.
The final string is then the one with the most
words. We find that the second method produces
descriptions that seem more natural and varied
than the n-gram ranking method for our develop-
ment set, and so use the longest string method in
evaluation.
4.4.2 Step 8: Prenominal Modifier Ordering
To order sets of selected adjectives, we use the
top-scoring prenominal modifier ordering model
discussed in Mitchell et al(2011). This is an n-
gram model constructed over noun phrases that
were extracted from an automatically parsed ver-
sion of the New York Times portion of the Giga-
word corpus (Graff and Cieri, 2003). With this
in place, blue clear sky becomes clear blue sky,
wooden brown table becomes brown wooden ta-
ble, etc.
5 Evaluation
Each set of sentences is generated with ? (likeli-
hood cutoff) set to .01 and ? (observation count
cutoff) set to 3. We compare the system against
human-written descriptions and two state-of-the-
art vision-to-language systems, the Kulkarni et al
(2011) and Yang et al(2011) systems.
Human judgments were collected using Ama-
zon?s Mechanical Turk (Amazon, 2011). We
follow recommended practices for evaluating an
NLG system (Reiter and Belz, 2009) and for run-
ning a study on Mechanical Turk (Callison-Burch
and Dredze, 2010), using a balanced design with
each subject rating 3 descriptions from each sys-
tem. Subjects rated their level of agreement on
a 5-point Likert scale including a neutral mid-
dle position, and since quality ratings are ordinal
(points are not necessarily equidistant), we evalu-
ate responses using a non-parametric test. Partici-
pants that took less than 3 minutes to answer all 60
questions and did not include a humanlike rating
for at least 1 of the 3 human-written descriptions
were removed and replaced. It is important to note
that this evaluation compares full generation sys-
tems; many factors are at play in each system that
may also influence participants? perception, e.g.,
sentence length (Napoles et al 2011) and punc-
tuation decisions.
The systems are evaluated on a set of 840
images evaluated in the original Kulkarni et al
(2011) system. Participants were asked to judge
the statements given in Figure 12, from Strongly
Disagree to Strongly Agree.
754
Grammaticality Main Aspects Correctness Order Humanlikeness
Human 4 (3.77, 1.19) 4 (4.09, 0.97) 4 (3.81, 1.11) 4 (3.88, 1.05) 4 (3.88, 0.96)
Midge 3 (2.95, 1.42) 3 (2.86, 1.35) 3 (2.95, 1.34) 3 (2.92, 1.25) 3 (3.16, 1.17)
Kulkarni et al2011 3 (2.83, 1.37) 3 (2.84, 1.33) 3 (2.76, 1.34) 3 (2.78, 1.23) 3 (3.13, 1.23)
Yang et al2011 3 (2.95, 1.49) 2 (2.31, 1.30) 2 (2.46, 1.36) 2 (2.53, 1.26) 3 (2.97, 1.23)
Table 4: Median scores for systems, mean and standard deviation in parentheses. Distance between points on the
rating scale cannot be assumed to be equidistant, and so we analyze results using a non-parametric test.
GRAMMATICALITY:
This description is grammatically correct.
MAIN ASPECTS:
This description describes the main aspects of this
image.
CORRECTNESS:
This description does not include extraneous or in-
correct information.
ORDER:
The objects described are mentioned in a reasonable
order.
HUMANLIKENESS:
It sounds like a person wrote this description.
Figure 12: Mechanical Turk prompts.
We report the scores for the systems in Table
4. Results are analyzed using the non-parametric
Wilcoxon Signed-Rank test, which uses median
values to compare the different systems. Midge
outperforms all recent automatic approaches on
CORRECTNESS and ORDER, and Yang et alad-
ditionally on HUMANLIKENESS and MAIN AS-
PECTS. Differences between Midge and Kulkarni
et alare significant at p< .01; Midge and Yang et
al. at p< .001. For all metrics, human-written de-
scriptions still outperform automatic approaches
(p < .001).
These findings are striking, particularly be-
cause Midge uses the same input as the Kulka-
rni et alsystem. Using syntactically informed
word co-occurrence statistics from a large corpus
of descriptive text improves over state-of-the-art,
allowing syntactic trees to be generated that cap-
ture the variation of natural language.
6 Discussion
Midge automatically generates language that is as
good as or better than template-based systems,
tying vision to language at a syntactic/semantic
level to produce natural language descriptions.
Results are promising, but, there is more work to
be done: Evaluators can still tell a difference be-
tween human-written descriptions and automati-
cally generated descriptions.
Improvements to the generated language are
possible at both the vision side and the language
side. On the computer vision side, incorrect ob-
jects are often detected and salient objects are of-
ten missed. Midge does not yet screen out un-
likely objects or add likely objects, and so pro-
vides no filter for this. On the language side, like-
lihood is estimated directly, and the system pri-
marily uses simple maximum likelihood estima-
tions to combine subtrees. The descriptive cor-
pus that informs the system is not parsed with
a domain-adapted parser; with this in place, the
syntactic constructions that Midge learns will bet-
ter reflect the constructions that people use.
In future work, we hope to address these issues
as well as advance the syntactic derivation pro-
cess, providing an adjunction operation (for ex-
ample, to add likely adjectives or adverbs based
on language alone). We would also like to incor-
porate meta-data ? even when no vision detection
fires for an image, the system may be able to gen-
erate descriptions of the time and place where an
image was taken based on the image file alone.
7 Conclusion
We have introduced a generation system that uses
a new approach to generating language, tying a
syntactic model to computer vision detections.
Midge generates a well-formed description of an
image by filtering attribute detections that are un-
likely and placing objects into an ordered syntac-
tic structure. Humans judge Midge?s output to be
the most natural descriptions of images generated
thus far. The methods described here are promis-
ing for generating natural language descriptions
of the visual world, and we hope to expand and
refine the system to capture further linguistic phe-
nomena.
8 Acknowledgements
Thanks to the Johns Hopkins CLSP summer
workshop 2011 for making this system possible,
and to reviewers for helpful comments. This
work is supported in part by Michael Collins and
by NSF Faculty Early Career Development (CA-
REER) Award #1054133.
755
References
Amazon. 2011. Amazon mechanical turk: Artificial
artificial intelligence.
Holly P. Branigan, Martin J. Pickering, and Mikihiro
Tanaka. 2007. Contributions of animacy to gram-
matical function assignment and word order during
production. Lingua, 118(2):172?189.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram version 1.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with Amazon?s Me-
chanical Turk. NAACL 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detections. Proceed-
ings of CVPR 2005.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. Proceedings of CVPR 2009.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
Proceedings of ECCV 2010.
Pedro Felzenszwalb, David McAllester, and Deva Ra-
maman. 2008. A discriminatively trained, mul-
tiscale, deformable part model. Proceedings of
CVPR 2008.
Flickr. 2011. http://www.flickr.com. Accessed
1.Sep.11.
Kotaro Funakoshi, Satoru Watanabe, Naoko
Kuriyama, and Takenobu Tokunaga. 2004.
Generating referring expressions using perceptual
groups. Proceedings of the 3rd INLG.
Albert Gatt. 2006. Generating collective spatial refer-
ences. Proceedings of the 28th CogSci.
David Graff and Christopher Cieri. 2003. English Gi-
gaword. Linguistic Data Consortium, Philadelphia,
PA. LDC Catalog No. LDC2003T05.
Philipp Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. MT Summit.
http://www.statmt.org/europarl/.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. Proceedings of
ACL-08: HLT.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C. Berg, and Tamara
Berg. 2011. Baby talk: Understanding and gener-
ating image descriptions. Proceedings of the 24th
CVPR.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. Proceedings of the 36th ACL.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
Proceedings of CoNLL 2011.
Mitchell Marcus, Ann Bies, Constance Cooper, Mark
Ferguson, and Alyson Littman. 1995. Treebank II
bracketing guide.
George A. Miller. 1995. WordNet: A lexical
database for english. Communications of the ACM,
38(11):39?41.
Margaret Mitchell, Aaron Dunlop, and Brian Roark.
2011. Semi-supervised modeling for prenomi-
nal modifier ordering. Proceedings of the 49th
ACL:HLT.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. ACL-
HLT Workshop on Monolingual Text-To-Text Gen-
eration.
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. Proceedings of NIPS 2011.
Slav Petrov. 2010. Berkeley parser. GNU General
Public License v.2.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazon?s mechanical turk. Proceed-
ings of the NAACL HLT 2010 Workshop on Creat-
ing Speech and Language Data with Amazon?s Me-
chanical Turk.
Ehud Reiter and Anja Belz. 2009. An investiga-
tion into the validity of some metrics for automat-
ically evaluating natural language generation sys-
tems. Computational Linguistics, 35(4):529?558.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural language generation systems. Journal
of Natural Language Engineering, pages 57?87.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and
Yiannis Aloimonos. 2011. Corpus-guided sen-
tence generation of natural images. Proceedings of
EMNLP 2011.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2T: Image pars-
ing to text description. Proceedings of IEEE 2010,
98(8):1485?1508.
756
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 762?772,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detecting Visual Text
Jesse Dodge1, Amit Goyal2, Xufeng Han3, Alyssa Mensch4, Margaret Mitchell5, Karl Stratos6
Kota Yamaguchi3, Yejin Choi3, Hal Daume? III2, Alexander C. Berg3 and Tamara L. Berg3
1University of Washington, 2University of Maryland, 3Stony Brook University
4MIT, 5Oregon Health & Science University, 6Columbia University
dodgejesse@gmail.com, amit@umiacs.umd.edu, xufhan@cs.stonybrook.edu
acmensch@mit.edu, mitchmar@ohsu.edu, stratos@cs.columbia.edu
kyamagu@cs.stonybrook.edu, ychoi@cs.stonybrook.edu
me@hal3.name, aberg@cs.stonybrook.edu, tlberg@cs.stonybrook.edu
Abstract
When people describe a scene, they often in-
clude information that is not visually apparent;
sometimes based on background knowledge,
sometimes to tell a story. We aim to sepa-
rate visual text?descriptions of what is being
seen?from non-visual text in natural images
and their descriptions. To do so, we first con-
cretely define what it means to be visual, an-
notate visual text and then develop algorithms
to automatically classify noun phrases as vi-
sual or non-visual. We find that using text
alone, we are able to achieve high accuracies
at this task, and that incorporating features
derived from computer vision algorithms im-
proves performance. Finally, we show that we
can reliably mine visual nouns and adjectives
from large corpora and that we can use these
effectively in the classification task.
1 Introduction
People use language to describe the visual world.
Our goal is to: formalize what ?visual text? is (Sec-
tion 2.2); analyze naturally occurring written lan-
guage for occurrences of visual text (Section 2); and
build models that can detect visual descriptions from
raw text or from image/text pairs (Section 3). This
is a challenging problem. One challenge is demon-
strated in Figure 1, which contains two images that
contain the noun ?car? in their human-written cap-
tions. In one case (the top image), there actually is a
car in the image; in the other case, there is not: the
car refers to the state of the speaker.
The ability to automatically identify visual text is
practically useful in a number of scenarios. One can
Another dream car to
add to the list, this one
spotted in Hanbury St.
Shot out my car win-
dow while stuck in traf-
fic because people in
Cincinnati can?t drive in
the rain.
Figure 1: Two image/caption pairs, both containing the
noun ?car? but only the top one in a visual context.
imagine automatically mining image/caption data
(like that in Figure 1) to train object recognition sys-
tems. However, in order to do so reliably, one must
know whether the ?car? actually appears or not.
When building image search engines, it is common
to use text near an image as features; this is more
useful when this text is actually visual. Or when
training systems to automatically generate captions
of images (e.g., for visually impaired users), we
need good language models for visual text.
One of our goals is to define what it means for a
bit of text to be visual. As inspiration, we consider
image/description pairs automatically crawled from
Flickr (Ordonez et al, 2011). A first pass attempt
might be to say ?a phrase in the description of an
image is visual if you can see it in the corresponding
image.? Unfortunately, this is too vague to be useful;
the biggest issues are discussed in Section 2.2.
762
Based on our analysis, we settled on the follow-
ing definition: A piece of text is visual (with re-
spect to a corresponding image) if you can cut out
a part of that image, paste it into any other image,
and a third party could describe that cut-out part in
the same way. In the car example, the claim is that I
could cut out the car, put it in the middle of any other
image, and someone else might still refer to that car
as ?dream car.? The car in the bottom image in Fig-
ure 1 is not visual because there?s nothing you could
cut out that would retain car-ness.
2 Data Analysis
Before embarking on the road to building models of
visual text, it is useful to obtain a better understand-
ing of what visual text is like, and how it compares to
the more standard corpora that we are used to work-
ing with. We describe the two large data sets that we
use (one visual, one non-visual), then describe the
quantitative differences between them, and finally
discuss our annotation effort for labeling visual text.
2.1 Data sets
We use the SBU Captioned Photo Dataset (Ordonez
et al, 2011) as our primary source of image/caption
data. This dataset contains 1 million images with
user associated captions, collected in the wild by in-
telligent filtering of a huge number of Flickr pho-
tos. Past work has made use of this dataset to re-
trieve whole captions for association with a query
image (Ordonez et al, 2011). Their method first
used global image descriptors to retrieve an initial
matched set, and then applied more local estimates
of content to re-rank this (relatively small) set (Or-
donez et al, 2011). This means that content based
matching was relatively constrained by the bottle-
neck of global descriptors, and local content (e.g.,
objects) had relatively small effect on accuracy.
As an auxiliary source of information for (largely)
non-visual text, we consider a large corpus of text
obtained by concatenating ukWaC1 and the New
York Times Newswire Service (NYT) section of the
Gigaword (Graff, 2003) Corpus. The Web-derived
ukWaC is already tokenized and POS-tagged with
the TreeTagger (Schmid, 1995). NYT is tokenized,
1ukWaC is a freely available Wikipedia-derived corpus from
2009; see http://wacky.sslmit.unibo.it/doku.php.
and POS-tagged using TagChunk (Daume? III and
Marcu, 2005). This consists of 171 million sen-
tences (4 billion words). We refer to this generic
text corpus as Large-Data.
2.2 Formalizing visual text
We begin our analysis by revisiting the definition
of visual text from the introduction, and justifying
this particular definition. In order to arrive at a suf-
ficiently specific definition of ?visual text,? we fo-
cused on the applications of visual text that we care
about. As discussed in the introduction, these are:
training object detectors, building image search en-
gines and automatically generating captions for im-
ages. Our definition is based on access to image/text
pairs, but later we discuss how to talk about it purely
based on text. To make things concrete, consider an
image/text pair like that in the top of Figure 1. And
then consider a phrase in the text, like ?dream car.?
The question is: is ?dream car? visual or not?
One of the challenges in arriving at such a defi-
nition is that the description of an image in Flickr
is almost always written by the photographer of that
image. This means the descriptions often contain in-
formation that is not actually pictured in the image,
or contain references that are only relevant to the
photographer (referring to a person/pet by name).
One might think that this is an artifact of this par-
ticular dataset, but it appears to be generic to all cap-
tions, even those written by a viewer (rather than the
photographer). Figure 2 shows an image from the
Pascal dataset (Everingham et al, 2010), together
with captions written by random people collected
via crowd-sourcing (Rashtchian et al, 2010). There
is much in this caption that is clearly made-up by the
author, presumably to make the caption more inter-
esting (e.g., meta-references like ?the camera? or ?A
photo? as well as ?guesses? about the image, such as
?garage? and ?venison?).
Second, there is a question of how much inference
you are allowed to do when you say that you ?see?
something. For example, in the top image in Fig-
ure 1, the street is pictured, but does that mean that
?Hanbury St.? is visual? What if there were a street
sign that clearly read ?Hanbury St.? in the image?
This problem comes up all the time, when people
say things like ?in London? or ?in France? in their
captions. If it?s just a portrait of people ?in France,?
763
1. A distorted photo of a man cutting up a large cut of meat in a garage.
2. A man smiling at the camera while carving up meat.
3. A man smiling while he cuts up a piece of meat.
4. A smiling man is standing next to a table dressing a piece of venison.
5. The man is smiling into the camera as he cuts meat.
Figure 2: An image from the Pascal data with five captions collected via crowd-sourcing. Measurements on the
SMALL and LARGE dataset show that approximately 70% of noun phrases are visual (bolded), while the rest are
non-visual (underlined). See Section 2.4 for details.
it?s hard to say that this is visual. If you see the Eif-
fel tower in the background, this is perhaps better
(though it could be Las Vegas!), but how does this
compare to a photo taken out of an airplane window
in which you actually do see France-the-country?
This problem becomes even more challenging
when you consider things other than nouns. For in-
stance, when is a verb visual? For instance, the most
common non-copula verb in our data is ?sitting,?
which appears in roughly two usages: (1) ?Took this
shot, sitting in a bar and enjoying a Portugese beer.?
and (2) ?Lexy sitting in a basket on top of her cat
tree.? The first one is clearly not visual; the second
probably is. A more nuanced case is for ?playing,?
as in: ?Girls playing in a boat on the river bank?
(probably visual) versus ?Tuckered out from play-
ing in Nannie?s yard.? The corresponding image for
the latter description shows a sleeping cat.
Our final definition, based on cutting out the po-
tentially visual part of the image, allows us to say
that: (1) ?venison? is not visual (because you cannot
actually tell); (2) ?Hanbury St.? and ?Lexy? are not
visual (you can infer them, in the first case because
there is only one street and in the second case be-
cause there is only one cat); (3) that seeing the real
Eiffel tower in the background does not mean that
?France? is visual (but again, may be inferred); etc.
2.3 Most Pronounced Differences
To get an intuitive sense of how Flickr captions (ex-
pected to be predominantly visual) and generic text
(expected not to be so) differ, we computed some
simple statistics on sentences from these. In gen-
eral, the generic text had twice as many main verbs
as the Flickr data, four times as many auxiliaries or
light verbs, and about 50% more prepositions.
Flickr captions tended to have far more references
to physical objects (versus abstract objects) than the
generic text, according to the WordNet hierarchy.
Approximately 64% of the objects in Flickr were
physical (about 22% abstract and 14% unknown).
Whereas in the generic text, only 30% of the objects
were physical, 53% were abstract (17% unknown).
A third major difference between the corpora is
in terms of noun modifiers. In both corpora, nouns
tend not to have any modifiers, but modifiers are still
more prevalent in Flickr than in generic text. In par-
ticular, 60% of nouns in Flickr have zero modifiers,
but 70% of nouns in generic text have zero modi-
fiers. In Flickr, 30% of nouns have exactly one mod-
ifier, as compared to only 22% for generic text.
The breakdown of what those modifiers look like
is even more pronounced, even when restricted just
to physical objects (modifier types are obtained
through the bootstrapping process discussed in Sec-
tion 3.1). Almost 50% of nominal modifiers in the
Flickr data are color modifiers, whereas color ac-
counts for less than 5% of nominal modifiers in
generic text. In Flickr, 10% of modifiers talk about
beauty, in comparison to less than 5% in generic
text. On the other hand, less than 3% of modifiers
in Flickr reference ethnicity, as compared to almost
20% in generic text; and 20% of Flickr modifiers
reference size, versus 50% in generic text.
2.4 Annotating Visual Text
In order to obtain ground truth data, we rely on
crowdsourcing (via Amazon?s Mechanical Turk).
Each instance is an image, a paired caption, and a
highlighted noun phrase in that caption. The anno-
tation for this instance is a label of ?visual,? ?non-
visual? or ?error,? where the error category is re-
764
served for cases where the noun phrase segmenta-
tion was erroneous. Each worker is given five in-
stances to label and paid one cent per annotation.2
For a small amount of data (803 images contain-
ing 2339 instances), we obtained annotations from
three separate workers per instance to obtain higher
quality data. For a large amount of data (48k im-
ages), we obtained annotations from only a sin-
gle worker. Subsequently, we will refer to these
two data sets as the SMALL and LARGE data sets.
In both data sets, approximately 70% of the noun
phrases were visual, 28% were non-visual and 2%
were erroneous. For simplicity, we group erroneous
and non-visual for all learning and evaluation.
In the SMALL data set, the rate of disagreement
between annotators was relatively low. In 74% of the
annotations, there was no disagreement at all. We
reconciled the annotations using the quality manage-
ment technique of Ipeirotis et al (2010); only 14%
of the annotations need to be changed in order to ob-
tain a gold standard.
One immediate question raised in this process is
whether one needs to actually see the image to per-
form the annotation. In particular, if we expect an
NLP system to be able to classify noun phrases as
visual or non-visual, we need to know whether peo-
ple can do this task sans image. We therefore per-
formed the same annotation on the SMALL data set,
but where the workers were not shown the image.
Their task was to imagine an image for this caption
and then annotate the noun phrase based on whether
they thought it would be pictured or not. We ob-
tained three annotations as before and reconciled
them (Ipeirotis et al, 2010). The accuracy of this
reconciled version against the gold standard (pro-
duced by people who did see the image) was 91%.
This suggests that while people are able to do this
task with some reliability, seeing the image is very
important (recall that always guessing ?visual? leads
to an accuracy of 70%).
3 Visual Features from Raw Text
Our first goal is to attempt to obtain relatively large
knowledge bases of terms that are (predominantly)
visual. This is potentially useful in its own right
2Data available at http://hal3.name/dvt/, with direct links
back to the SBU Captioned Photo Dataset.
(for instance, in the context of search, to determine
which query terms are likely to be pictured). We
have explored two techniques for performing this
task, the first based on bootstrapping (Section 3.1)
and the second based on label propagation (Sec-
tion 3.2). We then use these lists to generate features
for a classifier that predicts whether a noun phrase?
in context?is visual or not (Section 4).
In addition, we consider the task of separating ad-
jectives into different visual categories (Section 3.3).
We have already used the results of this in Sec-
tion 2.3 to understand the differences between our
two corpora. It is also potentially useful for the
purpose of building new object detection systems or
even attribute detection systems, to get a vocabulary
of target detections.
3.1 Bootstrapping for Visual Text
In this section, we learn visual and non-visual nouns
and adjectives automatically based on bootstrapping
techniques. First, we construct a graph between ad-
jectives by computing distributional similarity (Tur-
ney and Pantel, 2010) between them. For comput-
ing distributional similarity between adjectives, each
target adjective is defined as a vector of nouns which
are modified by the target adjective. To be exact, we
use only those adjectives as modifiers which appear
adjacent to a noun (that is, in a JJ NN construction).
For example, in ?small red apple,? we consider only
red as a modifier for noun. We use Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1989)
to weight the contexts, and select the top 1000 PMI
contexts for each adjective.3
Next, we apply cosine similarity to find the top
10 distributionally similar adjectives with respect to
each target adjective based on our large generic cor-
pus (Large-Data from Section 2.1). This creates a
graph with adjectives as nodes and cosine similarity
as weight on the edges. Analogously, we construct a
graph with nouns as nodes (here, adjectives are used
as contexts for nouns).
We then apply bootstrapping (Kozareva et al,
2008) on the noun and adjective graphs by select-
ing 10 seeds for visual and non-visual nouns and
adjectives (see Table 1). We use in-degree (sum of
weights of incoming edges) to compute the score for
3We are interested in descriptive adjectives, which ?typi-
cally ascribe to a noun a value of an attribute? (Miller, 1998).
765
Visual car house tree horse animal
nouns man table bottle
seeds woman computer
Non-visual idea bravery deceit trust
nouns dedication anger humour luck
seeds inflation honesty
Visual brown green wooden striped
adjectives orange rectangular furry
seeds shiny rusty feathered
Non-visual public original whole righteous
adjectives political personal intrinsic
seeds individual initial total
Table 1: Example seeds for bootstrapping.
each node that has connections with known (seeds)
or automatically labeled nodes, previously exploited
to learn hyponymy relations from the web (Kozareva
et al, 2008). Intuitively, in-degree captures the pop-
ularity of new instances among instances that have
already been identified as good instances. We learn
visual and non-visual words together (known as the
mutual exclusion principle in bootstrapping (The-
len and Riloff, 2002; McIntosh and Curran, 2008)):
each word (node) is assigned to only one class.
Moreover, after each iteration, we harmonically de-
crease the weight of the in-degree associated with
instances learned in later iterations. We added 25
new instances at each iteration and ran 500 iterations
of bootstrapping, yielding 11955 visual and 11978
non-visual nouns, and 7746 visual and 7464 non-
visual adjectives.
Based on manual inspection, the learned visual
and non-visual lists look great. In the future, we
would like to do a Mechanical Turk evaluation to
directly evaluate the visual and non-visual nouns
and adjectives. For now, we show the coverage of
these classes in the Flickr data-set: Visual nouns:
53.71%; Non-visual nouns: 14.25%; Visual ad-
jectives: 51.79%; Non-visual adjectives: 14.40%.
Overall, we find more visual nouns and adjectives
are covered in the Flickr data-set, which makes
sense, since the Flickr data-set is largely visual.
Second, we show the coverage of these classes
on the large text corpora (Large-Data from Sec-
tion 2.1): Visual nouns: 26.05%; Non-visual nouns:
41.16%; Visual adjectives: 20.02%; Non-visual ad-
Visual: attend, buy, clean, comb, cook, drink, eat,
fry, pack, paint, photograph, smash, spill, steal,
taste, tie, touch, watch, wear, wipe
Non-visual: achieve, admire, admit, advocate, al-
leviate, appreciate, arrange, criticize, eradicate,
induce, investigate, minimize, overcome, pro-
mote, protest, relieve, resolve, review, support,
tolerate
Table 2: Predicates that are visual and non-visual.
Visual: water, cotton, food, pumpkin, chicken,
ring, hair, mouth, meeting, kind, filter, game, oil,
show, tear, online, face, class, car
Non-visual: problem, poverty, pain, issue, use,
symptom, goal, effect, thought, government,
share, stress, work, risk, impact, concern, obsta-
cle, change, disease, dispute
Table 3: Learned visual/non-visual nouns.
jectives: 40.00%. Overall, more non-visual nouns
and adjectives cover text data, since Large-Data is
a non-visual data-set.
3.2 Label Propagation for Visual Text
To propagate visual labels, we construct a bipartite
graph between visually descriptive predicates and
their arguments. Let VP be the set of nodes that cor-
responds to predicates, and let VA be the set of nodes
that corresponds to arguments. To learn the visually
descriptive words, we set VP to 20 visually descrip-
tive predicates shown in the top of Table 2, and VA
to all nouns that appear in the object argument posi-
tion with respect to the seed predicates. We approx-
imate this by taking nouns on the right hand side
of the predicates within a window of 4 words using
the Web 1T Google N-gram data (Brants and Franz.,
2006). For edge weights, we use conditional prob-
abilities between predicates and arguments so that
w(p? a) := pr(a|p) and w(a? p) := pr(p|a).
In order to collectively induce the visually de-
scriptive words from this graph, we apply the graph
propagation algorithm of Velikovich et al (2010),
a variant of label propagation algorithms (Zhu and
Ghahramani, 2002) that has been shown to be ef-
fective for inducing a web-scale polarity lexicon
based on word co-occurrence statistics. This algo-
766
Color purple blue maroon beige green
Material plastic cotton wooden metallic silver
Shape circular square round rectangular triangular
Size small big tiny tall huge
Surface coarse smooth furry fluffy rough
Direction sideways north upward left down
Pattern striped dotted checked plaid quilted
Quality shiny rusty dirty burned glittery
Beauty beautiful cute pretty gorgeous lovely
Age young mature immature older senior
Ethnicity french asian american greek hispanic
Table 4: Attribute Classes with their seed values
rithm iteratively updates the semantic distance be-
tween each pair of nodes in the graph, then produces
a score for each node that represents how visually
descriptive each word is. To learn the words that
are not visually descriptive, we use the predicates
shown in the bottom of Table 2 as VP instead. Ta-
ble 3 shows the top ranked nouns that are visually
descriptive and not visually descriptive.
3.3 Bootstrapping Visual Adjectives
Our goal in this section is to automatically gener-
ate comprehensive lists of adjectives for different at-
tributes, such as color, material, shape, etc. To our
knowledge, this is the first significant effort of this
type for adjectives: most bootstrapping techniques
focus exclusively on nouns, although Almuhareb
and Poesio (2005) populated lists of attributes us-
ing web-based similarity measures. We found that
in some ways adjectives are easier than nouns, but
require slightly different representations.
One might conjecture that listing attributes by
hand is difficult. Colors names are well known to
be quite varied. For instance, our bootstrapping
approach is able to discover colors like ?grayish,?
?chestnut,? ?emerald,? and ?rufous? that would be
hard to list manually (the last is a reddish-brown
color, somewhat like rust). Although perhaps not
easy to create, the Wikipedia list of colors (http:
//en.wikipedia.org/wiki/List of colors) includes all of these
except ?grayish?. On the other hand, it includes
color terms that might be difficult to make use of as
colors, such as ?bisque,? ?bone? and ?bubbles? (the
last is a very light cyan), which might over-generate
hits. For shape, we find ?oblong,? ?hemispherical,?
?quadrangular? and, our favorite, ?convex?.
We use essentially the same bootstrapping process
as described earlier in Section 3.1, but on a slightly
different data representation. The only difference is
that instead of linking adjectives to their 10 most
similar neighbors, we link them only to 25 neigh-
bors to attempt to improve recall.
We begin with seeds for each attribute class from
Table 4. We conduct a manual evaluation to di-
rectly measure the quality of attribute classes. We
recruited 3 annotators and developed annotation
guidelines that instructed each recruiter to judge
whether a learned value belongs to an attribute class
or not. The annotators assigned ?1? if a learned
value belongs to a class, otherwise ?0?.
We conduct an Information Retrieval (IR) Style
human evaluation. Analogous to an IR evaluation,
here the total number of relevant values for attribute
classes can not be computed. Therefore, we assume
the correct output of several systems as the total re-
call which can be produced by any system. Now,
with the help of our 3 manual annotators, we obtain
the correct output of several systems from the total
output produced by these systems.
First, we measured the agreement on whether
each learned value belongs to a semantic class or
not. We computed ? to measure inter-annotator
agreement for each pair of annotators. We focus
our evaluation on 4 classes: age, beauty, color, and
direction; between Human 2 and Human 3 and be-
tween Human 1 and Human 3, the ? value was 0.48;
between Human 1 and Human 2 it was 0.45. These
numbers are somewhat lower than we would like,
but not terrible. If we evaluate the classes individu-
ally, we find that age has the lowest ?. If we remove
?age,? the pairwise ?s rise to 0.59, 0.57 and 0.55.
Second, we compute Precision (Pr), Recall (Rec)
and F-measure (F1) for different bootstrapping sys-
tems (based on the number of iterations and the
number of new words added in each iteration).
Two parameter settings performed consistently bet-
ter than others (10 iterations with 25 items, and 5 it-
erations with 50 items). The former system achieves
a precision/recall/F1 of 0.53, 0.71, 0.60 against Hu-
man 2; the latter achieves scores of 0.54, 0.72, 0.62.
4 Recognizing Visual Text
We train a logistic regression (aka maximum en-
tropy) model (Daume? III, 2004) to classify text as
visual or non-visual. The features we use fall into
767
the following categories: WORDS (the actual lexi-
cal items and stems); BIGRAMS (lexical bigrams);
SPELL (lexical features such as capitalization pat-
tern, and word prefixes and suffixes); WORDNET
(set of hypernyms according to WordNet); and
BOOTSTRAP (features derived from bootstrapping
or label propagation).
For each of these feature categories, we compute
features inside the phrase being categorized (e.g.,
?the car?), before the phrase (two words to the left)
and after the phrase (two words to the right). We
additionally add a feature that computes the num-
ber of words in a phrase, and a feature that com-
putes the position of the phrase in the caption (first
fifth through last fifth of the description). This leads
to seventeen feature templates that are computed for
each example. In the SMALL data set, there are 25k
features (10k non-singletons); in the LARGE data
set, there are 191k features (79k non-singletons).
To train models on the SMALL data set, we use
1500 instances as training, 200 as development and
the remaining 639 as test data. To train models on
the LARGE data set, we use 45000 instances as train-
ing and the remaining 4401 as development. We
always test on the 639 instances from the SMALL
data, since it has been redundantly annotated. The
development data is used only to choose the regular-
ization parameter for a Gaussian prior on the logis-
tic regression model; this parameter is chosen in the
range {0.01, 0.05, 0.1, 0.5, 1, 2, 4, 8, 16, 32, 64}.
Because of the imbalanced data problem, evalu-
ating according to accuracy is not appropriate for
this task. Even evaluating by precision/recall is not
appropriate, because a baseline system that guesses
that everything is visual obtains 100% recall and
70% precision. Due to these issues, we instead
evaluate according to the area under the ROC curve
(AUC). To check statistical significance, we com-
pute standard deviations using bootstrap resampling,
and consider there to be a significant difference if a
result falls outside of two standard deviations of the
baseline (95% confidence).
Figure 3 shows learning curves for the two data
sets. The SMALL data achieves an AUC score of
71.3 in the full data setting (1700 examples); the
LARGE data needs 12k examples to achieve similar
accuracy due to noise. However, with 49k examples,
we are able to achieve a AUC score of 75.3 using the
101 102 103 104 105
0.55
0.6
0.65
0.7
0.75
0.8
Figure 3: Learning curves for training on SMALL data
(blue solid) and LARGE data (black dashed). X-axis (in
log-scale) is number of training examples; Y-axis is AUC.
large data set. By pooling the data (and weighting
the small data), this boosts results to 76.1. The con-
fidence range on these data is approximately ?1.9,
meaning that this boost is likely not significant.
4.1 Using Image Features
As discussed previously, humans are only able to
achieve 90% accuracy on the visual/non-visual task
when they are not allowed to view the image.
This potentially upper-bounds the performance of a
learned system that can only look at text. In order to
attempt to overcome this, we augment our basic sys-
tem with a number of features computed from the
corresponding images. These features are derived
from the output of state of the art vision algorithms
to detect 121 different objects, stuff and scenes.
As our object detectors, we use standard state
of the art deformable part-based models (Felzen-
szwalb et al, 2010) for 89 common object cate-
gories, including: the original 20 objects from Pas-
cal, 49 objects from Object Bank (Li-Jia Li and Fei-
Fei, 2010), and 20 from Im2Text (Ordonez et al,
2011). We additionally use coarse image parsing
to estimate background elements in each database
image. Six possible background (stuff) categories
are considered: sky, water, grass, road, tree, and
building. For this we use detectors (Ordonez et
al., 2011) which compute color, texton, HoG (Dalal
and Triggs, 2005) and Geometric Context (Hoiem
et al, 2005) as input features to a sliding win-
dow based SVM classifier. These detectors are run
on all database images, creating a large pool of
background elements for retrieval. Finally, we ob-
768
Figure 4: (Left) Highest confidence flower detected in an
image; (Right) All detections in the same image.
tain scene descriptors for each image by comput-
ing scene classification scores for 26 common scene
categories, using the features, methods and training
data from the SUN dataset (Xiao et al, 2010).
Figure 4 shows an example image on which sev-
eral detectors have been run. From each image, we
extract the following features: which object detec-
tors fired; how many times they fired; the confidence
of the most-likely firing; the percentage of the image
(in pixels) that the bounding box corresponding to
this object occupies; and the percentage of the width
(and height) of the image that it occupies.
Unfortunately, object detection is a highly noisy
process. The right image in Figure 4 shows all de-
tections for that image, which includes, for instance,
a chair detection that spans nearly the entire image,
and a person detection in the bottom-right corner.
For an average image, if a single detector (e.g., the
flower detector) fires once, it actually fires 40 times
(?? = 1.8). Moreover, of the 120 detectors, on
an average image over 22 (?? = 5.6) of them fire
at least once (though certainly in an average image
only a few objects are actually present). Exacerbat-
ing this problem, although the confidence scores for
a single detector can be compared, the scores be-
tween different detectors are not at all comparable.
In order to attenuate this problem, we include dupli-
cate copies of all the above features restricted to the
most confident object for each object type.
On the SMALL data set, this adds 400 new fea-
CATEGORY POSITION AUC
Bootstrap Phrase 65.2
+ Spell Phrase 68.6
+ Image - 69.2
+ Words Phrase 70.0
+ Length - 69.8
+ Wordnet Phrase 70.4
+ Wordnet Before 70.6
+ Spell Before 71.8
+ Words Before 72.2
+ Bootstrap Before 72.4
+ Spell After 71.5
Table 5: Results of feature ablation on SMALL data set.
Best result is in bold; results that are not statistically sig-
nificantly worse are italicized.
tures (300 of which are non-singletons4); on the
LARGE data set, this adds 500 new features (480
non-singletons). Overall, the AUC scores trained on
the small data set increase from 71.3 to 73.9 (a sig-
nificant improvement). On the large data set, the in-
crease is only from 76.1 to 76.8, which is not likely
to be significant. In general, the improvement ob-
tained by adding image features is most pronounced
in the setting of small training data, perhaps because
these features are more generic than the highly lexi-
calized features used in the textual model. But once
there is a substantial amount of text data, the noisy
image features become less useful.
4.2 Feature Ablations
In order to ascertain the degree to which each feature
template is useful, we perform an ablation study. We
first perform feature selection at the template level
using the information gain criteria, and then train
models using the corresponding subset of features.
The results on the SMALL data set are shown in
Table 5. Here, the bootstrapping features computed
on words within the phrase to be classified were
judged as the most useful, followed by spelling fea-
tures. Image features were judged third most use-
ful. In general, features in the phrase were most use-
ful (not surprisingly), and then features before the
phrase (presumably to give context, for instance as
in ?out of the window?). Features from after the
phrase were not useful.
4Non-singleton features appear more than once in the data.
769
CATEGORY POSITION AUC
Words Phrase 74.7
+ Image - 74.4
+ Bootstrap Phrase 74.3
+ Spell Phrase 75.3
+ Length - 74.7
+ Words Before 76.2
+ Wordnet Phrase 76.1
+ Spell After 76.0
+ Spell Before 76.8
+ Wordnet Before 77.0
+ Wordnet After 75.6
Table 6: Results of feature ablation on LARGE data set.
Corresponding results on the LARGE data set are
shown in Table 6. Note that the order of features
selected is different because the training data is dif-
ferent. Here, the most useful features are simply the
words in the phrase to be classified, which alone al-
ready gives an AUC score of 74.7, only a few points
off from the best performance of 77.0 once image
features, bootstrap features and spelling features are
added. As before, these features are rated as very
useful for classification performance.
Finally, we consider the effect of using Bootstrap-
based features or label-propagation-based features.
In all the above experiments, the features used
are based on the union of word lists created by
these two techniques. We perform three experi-
ments. Beginning with the system that contains all
features (SMALL=73.9, LARGE=76.8), we first re-
move the bootstrap-based features (SMALL?71.8,
LARGE?75.5) or remove the label-propagation-
based features (SMALL?71.2, LARGE?74.9) or
remove both (SMALL?70.7, LARGE?74.2). From
these results, we can see that these techniques are
useful, but somewhat redundant: if you had to
choose one, you should choose label-propagation.
5 Discussion
As connections between language and vision be-
come stronger, for instance in the contexts of ob-
ject detection (Hou and Zhang, 2007; Kim and Tor-
ralba, 2009; Sivic et al, 2008; Alexe et al, 2010;
Gu et al, 2009), attribute detection (Ferrari and Zis-
serman, 2007; Farhadi et al, 2009; Kumar et al,
2009; Berg et al, 2010), visual phrases (Farhadi and
Sadeghi, 2011), and automatic caption generation
(Farhadi et al, 2010; Feng and Lapata, 2010; Or-
donez et al, 2011; Kulkarni et al, 2011; Yang et
al., 2011; Li et al, 2011; Mitchell et al, 2012), it
becomes increasingly important to understand, and
to be able to detect, text that actually refers to ob-
served phenomena. Our results suggest that while
this is a hard problem, it is possible to leverage large
text resources and state-of-the-art computer vision
algorithms to address it with high accuracy.
Acknowledgments
T.L. Berg and K. Yamaguchi were supported in part
by NSF Faculty Early Career Development (CA-
REER) Award #1054133; A.C. Berg and Y. Choi
were partially supported by the Stony Brook Uni-
versity Office of the Vice President for Research; H.
Daume? III and A. Goyal were partially supported by
NSF Award IIS-1139909; all authors were partially
supported by a 2011 JHU Summer Workshop.
References
B. Alexe, T. Deselaers, and V. Ferrari. 2010. What is an
object? In Computer Vision and Pattern Recognition
(CVPR), 2010 IEEE Conference on, pages 73 ?80.
A. Almuhareb and M. Poesio. 2005. Finding concept at-
tributes in the web. In Corpus Linguistics Conference.
Tamara L. Berg, Alexander C. Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteriza-
tion from noisy web data. In European Conference on
Computer Vision (ECCV).
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium, ISBN: 1-
58563-397-6, Philadelphia.
K. Church and P. Hanks. 1989. Word Associa-
tion Norms, Mutual Information and Lexicography.
In Proceedings of ACL, pages 76?83, Vancouver,
Canada, June.
N. Dalal and B. Triggs. 2005. Histograms of oriented
gradients for human detection. In CVPR.
Hal Daume? III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of the In-
ternational Conference on Machine Learning (ICML).
Hal Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available
at http://pub.hal3.name/#daume04cg-bfgs, implementation
available at http://hal3.name/megam/, August.
770
M. Everingham, L. Van Gool, C. K. I. Williams,
J. Winn, and A. Zisserman. 2010. The PASCAL
Visual Object Classes Challenge 2010 (VOC2010)
Results. http://www.pascal-network.org/challenges/VOC/
voc2010/workshop/index.html.
Ali Farhadi and Amin Sadeghi. 2011. Recognition us-
ing visual phrases. In Computer Vision and Pattern
Recognition (CVPR).
A. Farhadi, I. Endres, D. Hoiem, and D.A. Forsyth. 2009.
Describing objects by their attributes. In Computer
Vision and Pattern Recognition (CVPR).
A. Farhadi, M. Hejrati, M.A. Sadeghi, P. Young,
C. Rashtchian1, J. Hockenmaier, and D.A. Forsyth.
2010. Every picture tells a story: Generating sentences
from images. In ECCV.
P. F. Felzenszwalb, R. B. Girshick, and D. McAllester.
2010. Discriminatively trained deformable part
models, release 4. http://people.cs.uchicago.edu/?pff/
latent-release4/.
Y. Feng and M. Lapata. 2010. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
V. Ferrari and A. Zisserman. 2007. Learning visual at-
tributes. In Advances in Neural Information Process-
ing Systems (NIPS).
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium, Philadelphia, PA, January.
Chunhui Gu, J.J. Lim, P. Arbelaez, and J. Malik. 2009.
Recognition using regions. In Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Confer-
ence on, pages 1030 ?1037.
Derek Hoiem, Alexei A. Efros, and Martial Hebert.
2005. Geometric context from a single image. In
ICCV.
Xiaodi Hou and Liqing Zhang. 2007. Saliency detection:
A spectral residual approach. In Computer Vision and
Pattern Recognition, 2007. CVPR ?07. IEEE Confer-
ence on, pages 1 ?8.
P. Ipeirotis, F. Provost, and J. Wang. 2010. Quality man-
agement on amazon mechanical turk. In Proceedings
of the Second Human Computation Workshop (KDD-
HCOMP).
Gunhee Kim and Antonio Torralba. 2009. Unsupervised
Detection of Regions of Interest using Iterative Link
Analysis. In Annual Conference on Neural Informa-
tion Processing Systems (NIPS 2009).
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C
Berg, and T. L Berg. 2011. Babytalk: Understanding
and generating simple image descriptions. In CVPR.
N. Kumar, A.C. Berg, P. Belhumeur, and S.K. Nayar.
2009. Attribute and simile classifiers for face verifi-
cation. In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing sim-
ple image descriptions using web-scale n-grams. In
CONLL.
Eric P. Xing Li-Jia Li, Hao Su and Li Fei-Fei. 2010. Ob-
ject bank: A high-level image representation for scene
classification and semantic feature sparsification. In
NIPS.
Tara McIntosh and James R Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop 2008, pages 97?105, December.
K.J. Miller. 1998. Modifiers in WordNet. In C. Fell-
baum, editor, WordNet, chapter 2. MIT Press.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Mensch,
Alex Berg, Tamara Berg, and Hal Daume? III. 2012.
Midge: Generating image descriptions from computer
vision detections. Proceedings of EACL 2012.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing Images Using 1 Million
Captioned Photographs. In NIPS.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using amazon?s mechanical turk. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk.
Association for Computational Linguistics.
H. Schmid. 1995. Improvements in part?of?speech tag-
ging with an application to german. In Proceedings of
the EACL SIGDAT Workshop.
J. Sivic, B.C. Russell, A. Zisserman, W.T. Freeman, and
A.A. Efros. 2008. Unsupervised discovery of visual
object class hierarchies. In Computer Vision and Pat-
tern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 ?8.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the Empirical Meth-
ods in Natural Language Processing, pages 214?221.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Artificial Intelligence Research (JAIR),
37:141.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-
nan, and Ryan McDonald. 2010. The viability of web-
derived polarity lexicons. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
771
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba.
2010. Sun database: Large-scale scene recognition
from abbey to zoo. In CVPR.
Yezhou Yang, Ching Lik Teo, Hal Daume? III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gener-
ation of natural images. In EMNLP.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
772
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?368,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Collective Generation of Natural Image Descriptions
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
We present a holistic data-driven approach
to image description generation, exploit-
ing the vast amount of (noisy) parallel im-
age data and associated natural language
descriptions available on the web. More
specifically, given a query image, we re-
trieve existing human-composed phrases
used to describe visually similar images,
then selectively combine those phrases
to generate a novel description for the
query image. We cast the generation pro-
cess as constraint optimization problems,
collectively incorporating multiple inter-
connected aspects of language composition
for content planning, surface realization
and discourse structure. Evaluation by hu-
man annotators indicates that our final
system generates more semantically cor-
rect and linguistically appealing descrip-
tions than two nontrivial baselines.
1 Introduction
Automatically describing images in natural lan-
guage is an intriguing, but complex AI task, re-
quiring accurate computational visual recogni-
tion, comprehensive world knowledge, and natu-
ral language generation. Some past research has
simplified the general image description goal by
assuming that relevant text for an image is pro-
vided (e.g., Aker and Gaizauskas (2010), Feng
and Lapata (2010)). This allows descriptions to
be generated using effective summarization tech-
niques with relatively surface level image under-
standing. However, such text (e.g., news articles
or encyclopedic text) is often only loosely related
to an image?s specific content and many natu-
ral images do not come with associated text for
summarization.
In contrast, other recent work has focused
more on the visual recognition aspect by de-
tecting content elements (e.g., scenes, objects,
attributes, actions, etc) and then composing de-
scriptions from scratch (e.g., Yao et al (2010),
Kulkarni et al (2011), Yang et al (2011), Li
et al (2011)), or by retrieving existing whole
descriptions from visually similar images (e.g.,
Farhadi et al (2010), Ordonez et al (2011)). For
the latter approaches, it is unrealistic to expect
that there will always exist a single complete de-
scription for retrieval that is pertinent to a given
query image. For the former approaches, visual
recognition first generates an intermediate rep-
resentation of image content using a set of En-
glish words, then language generation constructs
a full description by adding function words and
optionally applying simple re-ordering. Because
the generation process sticks relatively closely
to the recognized content, the resulting descrip-
tions often lack the kind of coverage, creativ-
ity, and complexity typically found in human-
written text.
In this paper, we propose a holistic data-
driven approach that combines and extends the
best aspects of these previous approaches ? a)
using visual recognition to directly predict indi-
vidual image content elements, and b) using re-
trieval from existing human-composed descrip-
tions to generate natural, creative, and inter-
359
esting captions. We also lift the restriction of
retrieving existing whole descriptions by gather-
ing visually relevant phrases which we combine
to produce novel and query-image specific de-
scriptions. By judiciously exploiting the corre-
spondence between image content elements and
phrases, it is possible to generate natural lan-
guage descriptions that are substantially richer
in content and more linguistically interesting
than previous work.
At a high level, our approach can be moti-
vated by linguistic theories about the connection
between reading activities and writing skills,
i.e., substantial reading enriches writing skills,
(e.g., Hafiz and Tudor (1989), Tsang (1996)).
Analogously, our generation algorithm attains a
higher level of linguistic sophistication by read-
ing large amounts of descriptive text available
online. Our approach is also motivated by lan-
guage grounding by visual worlds (e.g., Roy
(2002), Dindo and Zambuto (2010), Monner and
Reggia (2011)), as in our approach the mean-
ing of a phrase in a description is implicitly
grounded by the relevant content of the image.
Another important thrust of this work is col-
lective image-level content-planning, integrating
saliency, content relations, and discourse struc-
ture based on statistics drawn from a large
image-text parallel corpus. This contrasts with
previous approaches that generate multiple sen-
tences without considering discourse flow or re-
dundancy (e.g., Li et al (2011)). For example,
for an image showing a flock of birds, generating
a large number of sentences stating the relative
position of each bird is probably not useful.
Content planning and phrase synthesis can
be naturally viewed as constraint optimization
problems. We employ Integer Linear Program-
ming (ILP) as an optimization framework that
has been used successfully in other generation
tasks (e.g., Clarke and Lapata (2006), Mar-
tins and Smith (2009), Woodsend and Lapata
(2010)). Our ILP formulation encodes a rich
set of linguistically motivated constraints and
weights that incorporate multiple aspects of the
generation process. Empirical results demon-
strate that our final system generates linguisti-
cally more appealing and semantically more cor-
rect descriptions than two nontrivial baselines.
1.1 System Overview
Our system consists of two parts. For a query
image, we first retrieve candidate descriptive
phrases from a large image-caption database us-
ing measures of visual similarity (?2). We then
generate a coherent description from these can-
didates using ILP formulations for content plan-
ning (?4) and surface realization (?5).
2 Vision & Phrase Retrieval
For a query image, we retrieve relevant candi-
date natural language phrases by visually com-
paring the query image to database images from
the SBU Captioned Photo Collection (Ordonez
et al, 2011) (1 million photographs with asso-
ciated human-composed descriptions). Visual
similarity for several kinds of image content are
used to compare the query image to images from
the database, including: 1) object detections for
89 common object categories (Felzenszwalb et
al., 2010), 2) scene classifications for 26 com-
mon scene categories (Xiao et al, 2010), and
3) region based detections for stuff categories
(e.g. grass, road, sky) (Ordonez et al, 2011).
All content types are pre-computed on the mil-
lion database photos, and caption parsing is per-
formed using the Berkeley PCFG parser (Petrov
et al, 2006; Petrov and Klein, 2007).
Given a query image, we identify content el-
ements present using the above classifiers and
detectors and then retrieve phrases referring to
those content elements from the database. For
example, if we detect a horse in a query im-
age, then we retrieve phrases referring to vi-
sually similar horses in the database by com-
paring the color, texture (Leung and Malik,
1999), or shape (Dalal and Triggs, 2005; Lowe,
2004) of the detected horse to detected horses
in the database images. We collect four types of
phrases for each query image as follows:
[1] NPs We retrieve noun phrases for each
query object detection (e.g., ?the brown cow?)
from database captions using visual similar-
ity between object detections computed as an
equally weighted linear combination of L2 dis-
360
tances on histograms of color, texton (Leung and
Malik, 1999), HoG (Dalal and Triggs, 2005) and
SIFT (Lowe, 2004) features.
[2] VPs We retrieve verb phrases for each
query object detection (e.g. ?boy running?)
from database captions using the same mea-
sure of visual similarity as for NPs, but restrict-
ing the search to only those database instances
whose captions contain a verb phrase referring
to the object category.
[3] Region/Stuff PPs We collect preposi-
tional phrases for each query stuff detection (e.g.
?in the sky?, ?on the road?) by measuring visual
similarity of appearance (color, texton, HoG)
and geometric configuration (object-stuff rela-
tive location and distance) between query and
database detections.
[4] Scene PPs We also collect prepositonal
phrases referring to general image scene context
(e.g. ?at the market?, ?on hot summer days?,
?in Sweden?) based on global scene similarity
computed using L2 distance between scene clas-
sification score vectors (Xiao et al, 2010) com-
puted on the query and database images.
3 Overview of ILP Formulation
For each image, we aim to generate multiple
sentences, each sentence corresponding to a sin-
gle distinct object detected in the given image.
Each sentence comprises of the NP for the main
object, and a subset of the corresponding VP,
region/stuff PP, and scene PP retrieved in ?2.
We consider four different types of operations
to generate the final description for each image:
T1. Selecting the set of objects to describe (one
object per sentence).
T2. Re-ordering sentences (i.e., re-ordering ob-
jects).
T3. Selecting the set of phrases for each sen-
tence.
T4. Re-ordering phrases within each sentence.
The ILP formulation of ?4 addresses T1 & T2,
i.e., content-planning, and the ILP of ?5 ad-
dresses T3 & T4, i.e., surface realization.1
1It is possible to create one conjoined ILP formulation
to address all four operations T1?T4 at once. For com-
4 Image-level Content Planning
First we describe image-level content planning,
i.e., abstract generation. The goals are to (1) se-
lect a subset of the objects based on saliency and
semantically compatibility, and (2) order the se-
lected objects based on their content relations.
4.1 Variables and Objective Function
The following set of indicator variables encodes
the selection of objects and ordering:
ysk =
?
?
?
1, if object s is selected
for position k
0, otherwise
(1)
where k = 1, ..., S encodes the position (order)
of the selected objects, and s indexes one of the
objects. In addition, we define a set of variables
indicating specific pairs of adjacent objects:
yskt(k+1) =
{
1, if ysk = yt(k+1) = 1
0, otherwise
(2)
The objective function, F , that we will maxi-
mize is a weighted linear combination of these
indicator variables and can be optimized using
integer linear programming:
F =
?
s
Fs ?
S?
k=1
ysk ?
?
st
Fst ?
S?1?
k=1
yskt(k+1) (3)
where Fs quantifies the salience/confidence of
the object s, and Fst quantifies the seman-
tic compatibility between the objects s and t.
These coefficients (weights) will be described in
?4.3 and ?4.4. We use IBM CPLEX to optimize
this objective function subject to the constraints
introduced next in ?4.2.
4.2 Constraints
Consistency Constraints: We enforce consis-
tency between indicator variables for indivisual
objects (Eq. 1) and consecutive objects (Eq. 2)
so that yskt(k+1) = 1 iff ysk = 1 and yt(k+1) = 1:
?stk, yskt(k+1) ? ysk (4)
yskt(k+1) ? yt(k+1) (5)
yskt(k+1) + (1? ysk) + (1? yt(k+1)) ? 1 (6)
putational and implementation efficiency however, we opt
for the two-step approach.
361
To avoid empty descriptions, we enforce that the
result includes at least one object:
?
s
ys1 = 1 (7)
To enforce contiguous positions be selected:
?k = 2, ..., S ? 1,
?
s
ys(k+1) ?
?
s
ysk (8)
Discourse constraints: To avoid spurious de-
scriptions, we allow at most two objects of the
same type, where cs is the type of object s:
?c ? objTypes,
?
{s: cs=c}
S?
k=1
ysk ? 2 (9)
4.3 Weight Fs: Object Detection
Confidence
In order to quantify the confidence of the object
detector for the object s, we define 0 ? Fs ? 1
as the mean of the detector scores for that object
type in the image.
4.4 Weight Fst: Ordering and
Compatibility
The weight 0 ? Fst ? 1 quantifies the compat-
ibility of the object pairing (s, t). Note that in
the objective function, we subtract this quan-
tity from the function to be maximized. This
way, we create a competing tension between the
single object selection scores and the pairwise
compatibility scores, so that variable number of
objects can be selected.
Object Ordering Statistics: People have bi-
ases on the order of topic or content flow. We
measure these biases by collecting statistics on
ordering of object names from the 1 million im-
age descriptions in the SBU Captioned Dataset
(Ordonez et al, 2011). Let ford(w1, w2) be
the number of times w1 appeared before w2.
For instance, ford(window, house) = 2895 and
ford(house, window) = 1250, suggesting that
people are more likely to mention a window be-
fore mentioning a house/building2. We use these
ordering statistics to enhance content flow. We
define score for the order of objects using Z-score
for normalization as follows:
F?st =
ford(cs, ct)?mean(ford)
std dev(ford)
(10)
2We take into account synonyms.
We then transform F?st so that F?st ? [0,1], and
then set Fst = 1 ? F?st so that smaller values
correspond to better choices.
5 Surface Realization
Recall that for each image, the computer vi-
sion system identifies phrases from descriptions
of images that are similar in a variety of aspects.
The result is a set of phrases representing four
different types of information (?2). From this
assortment of phrases, we aim to select a subset
and glue them together to compose a complete
sentence that is linguistically plausible and se-
mantically truthful to the content of the image.
5.1 Variables and Objective Function
The following set of variables encodes the selec-
tion of phrases and their ordering in construct-
ing S? sentences.
xsijk =
?
?????
?????
1, if phrase i of type j
is selected
for position k
in sentence s
0, otherwise
(11)
where k = 1, ..., N encodes the ordering of the
selected phrases, and j indexes one of the four
phrases types (object-NPs, action-VPs, region-
PPs, scene-PPs), i = 1, ...,M indexes one of
the M candidate phrases of each phrase type,
and s = 1, ..., S? encodes the sentence (object).
In addition, we define indicator variables for
adjacent pairs of phrases: xsijkpq(k+1) = 1 if
xsijk = xspq(k+1) = 1 and 0 otherwise. Finally,
we define the objective function F as:
F =
?
sij
Fsij ?
N?
k=1
xsijk
?
?
sijpq
Fsijpq ?
N?1?
k=1
xsijkpq(k+1) (12)
where Fsij weights individual phrase goodness
and Fsijpq adjacent phrase goodness. All coeffi-
cients (weights) will be described in Section 5.3
and 5.4.
We optionally prepend the first sentence in a
generated description with a cognitive phrase.3
3We collect most frequent 200 phrases of length 1-
7 that start a caption from the SBU Captioned Photo
Collection.
362
ILP: I think this is a boy?s bike 
lied in saltwater for quite a 
while. 
 
HMM: I liked the way bicycles 
leaning against a wall in 
Copenhagen Denmark in a 
windy sky in a Singapore 
bathroom. Boy?s bike lied in 
saltwater for quite a while in a 
windy sky in a Singapore 
bathroom. Fruit rubbing his 
face in the encrusted snow in a 
windy sky in a Singapore 
bathroom. 
 
Human: You re nobody in 
Oxford, unless you have a old 
bike with a basket 
 
ILP:  
This is a photo of this little flower sprouted up in defiance against grass.  
Bright yellow flowers growing in a rock garden at Volcan Mombacho. 
 
HMM: These was taken on the flowers growing in a rock garden in the field in two sorts. This 
little flower sprouted up in defiance in the field in two sorts. A full open flower sprouted up in 
defiance in the field in gardens. Bright yellow flowers growing in a rock garden in the field. 
 
Human: Yellow flower in my field 
 
ILP: Found trucks parked on first avenue in the east village. 
 
HMM: This is the first cellar door left back bedroom in center 
and clothes dryer to the right to the building in the house. 
This HUGE screen hanging on the wall outside a burned down 
building in the house. My truck parked on first avenue in the 
east village by the glass buildings in the house. 
 
Human: Flat bed Chisholms truck on display at the vintage 
vehicle rall y at Astley Green Colliery near Leigh Lancs 
 
Figure 1: ILP & HMM generated captions. In HMM generated captions, underlined phrases show redundancy
across different objects (due to lack of discourse constraints), and phrases in boldface show awkward topic
flow (due to lack of content planning). Note that in the bicycle image, the visual recognizer detected two
separate bicycles and some fruits, as can be seen in the HMM result. Via collective image-level content
planning (see ?4), some of these erroneous detection can be corrected, as shown in the ILP result. Spurious
and redundant phrases can be suppressed via discourse constraints (see ?5).
These are generic constructs that are often used
to start a description about an image, for in-
stance, ?This is an image of...?. We treat these
phrases as an additional type, but omit corre-
sponding variables and constraints for brevity.
5.2 Constraints
Consistency Constraints: First we enforce
consistency between the unary variables (Eq.
11) and the pairwise variables so that xsijkpqm =
1 iff xsijk = 1 and xspqm = 1:
?ijkpqm, xsijkpqm ? xsijk (13)
xsijkpqm ? xspqm (14)
xsijkpqm + (1? xsijk) + (1? xspqm) ? 1 (15)
Next we include constraints similar to Eq. 8
(contiguous slots are filled), but omit them for
brevity. Finally, we add constraints to ensure at
least two phrases are selected for each sentence,
to promote informative descriptions.
Linguistic constraints: We include linguisti-
cally motivated constraints to generate syntacti-
cally and semantically plausible sentences. First
we enforce a noun-phrase to be selected to en-
sure semantic relevance to the image:
?s,
?
ik
xsiNPk = 1 (16)
Also, to avoid content redundancy, we allow at
most one phrase of each type:
?sj,
?
i
N?
k=1
xsijk ? 1 (17)
Discourse constraints: We allow at most
one prepositional scene phrase for the whole de-
scription to avoid redundancy:
For j = PPscene,
?
sik
xsijk ? 1 (18)
We add constraints that prevent the inclusion of
more than one phrase with identical head words:
?s, ij, pq with the same heads,
N?
k=1
xsijk +
N?
k=1
xspqk ? 1 (19)
5.3 Unary Phrase Selection
Let Msij be the confidence score for phrase
xsij given by the image?phrase matching al-
gorithm (?2). To make the scores across dif-
ferent phrase types comparable, we normalize
them using Z-score: Fsij = norm?(Msij) =
(Msij ? meanj)/devj , and then transform the
values into the range of [0,1].
5.4 Pairwise Phrase Cohesion
In this section, we describe the pairwise phrase
cohesion score Fsijpq defined for each xsijpq in
363
ILP: I like the way the clouds hanging down by 
the ground in Dupnitsa of Avikwalal. 
 
Human: Car was raised on the wall over a bridge 
facing traffic..paramedics were attending the 
driver on the ground 
ILP: This is a photo of this bird hopping 
around eating things off of the ground by 
river. 
Human: IMG_6892 Lookn up in the sky its a 
bird its a plane its ah..... you 
ILP: This is a sporty little red convertible made for 
a great day in Key West FL. This car was in the 4th 
parade of the apartment buildings. 
 
Human: Hard rock casino exotic car show in June 
ILP: Taken in front of my cat sitting in a shoe 
box. Cat likes hanging around in my recliner. 
 
Human: H happily rests his armpit on a 
warm Gatorade bottle of water (a small 
bottle wrapped in a rag) 
Figure 2: In some cases (16%), ILP generated captions were preferred over human written ones!
the objective function (Eq. 12). Via Fsijpq,
we aim to quantify the degree of syntactic and
semantic cohesion across two phrases xsij and
xspq. Note that we subtract this cohesion score
from the objective function. This trick helps the
ILP solver to generate sentences with varying
number of phrases, rather than always selecting
the maximum number of phrases allowed.
N-gram Cohesion Score: We use n-gram
statistics from the Google Web 1-T dataset
(Brants and Franz., 2006) Let Lsijpq be the set
of all n-grams (2 ? n ? 5) across xsij and xspq.
Then the n-gram cohesion score is computed as:
FNGRAMsijpq = 1?
?
l?Lsijpq
NPMI(l)
size(Lsijpq)
(20)
NPMI(ngr) =
PMI(ngr)? PMImin
PMImax ? PMImin
(21)
Where NPMI is the normalized point-wise mu-
tual information.4
Co-occurrence Cohesion Score: To cap-
ture long-distance cohesion, we introduce a co-
occurrence-based score, which measures order-
preserved co-occurrence statistics between the
head words hsij and hspq 5. Let f?(hsij , hspq)
be the sum frequency of all n-grams that start
with hsij , end with hspq and contain a prepo-
sition prep(spq) of the phrase spq. Then the
4We include the n-gram cohesion for the sentence
boundaries as well, by approximating statistics for sen-
tence boundaries with punctuation marks in the Google
Web 1-T data.
5For simplicity, we use the last word of a phrase as
the head word, except VPs where we take the main verb.
co-occurrence cohesion is computed as:
FCOsijpq =
max(f?)? f?(hsij , hspq)
max(f?)?min(f?)
(22)
Final Cohesion Score: Finally, the pairwise
phrase cohesion score Fijpq is a weighted sum of
n-gram and co-occurrence cohesion scores:
Fsijpq =
? ? FNGRAMsijpq + ? ? F
CO
sijpq
?+ ?
(23)
where ? and ? can be tuned via grid search,
and FNGRAMijpq and F
CO
ijpq are normalized ? [0, 1]
for comparability. Notice that Fsijpq is in the
range [0,1] as well.
6 Evaluation
TestSet: Because computer vision is a challeng-
ing and unsolved problem, we restrict our query
set to images where we have high confidence that
visual recognition algorithms perform well. We
collect 1000 test images by running a large num-
ber (89) of object detectors on 20,000 images
and selecting images that receive confident ob-
ject detection scores, with some preference for
images with multiple object detections to obtain
good examples for testing discourse constraints.
Baselines: We compare our ILP approaches
with two nontrivial baselines: the first is an
HMM approach (comparable to Yang et al
(2011)), which takes as input the same set of
candidate phrases described in ?2, but for de-
coding, we fix the ordering of phrases as [ NP
? VP ? Region PP ? Scene PP] and find the
best combination of phrases using the Viterbi
algorithm. We use the same rich set of pairwise
364
Hmm Hmm Ilp Ilp
cognitive phrases: with w/o with w/o
0.111 0.114 0.114 0.116
Table 1: Automatic Evaluation
ILP selection rate
ILP V.S. HMM (w/o cogn) 67.2%
ILP V.S. HMM (with cogn) 66.3%
Table 2: Human Evaluation (without images)
ILP selection rate
ILP V.S. HMM (w/o cogn) 53.17%
ILP V.S. HMM (with cogn) 54.5%
ILP V.S. Retrieval 71.8%
ILP V.S. Human 16%
Table 3: Human Evaluation (with images)
phrase cohesion scores (?5.4) used for the ILP
formulation, producing a strong baseline6.
The second baseline is a recent Retrieval
based description method (Ordonez et al, 2011),
that searches the large parallel corpus of im-
ages and captions, and transfers a caption from
a visually similar database image to the query.
This again is a very strong baseline, as it ex-
ploits the vast amount of image-caption data,
and produces a description high in linguistic
quality (since the captions were written by hu-
man annotators).
Automatic Evaluation: Automatically quan-
tifying the quality of machine generated sen-
tences is known to be difficult. BLEU score
(Papineni et al, 2002), despite its simplicity
and limitations, has been one of the common
choices for automatic evaluation of image de-
scriptions (Farhadi et al, 2010; Kulkarni et al,
2011; Li et al, 2011; Ordonez et al, 2011), as
it correlates reasonably well with human evalu-
ation (Belz and Reiter, 2006).
Table 1 shows the the BLEU @1 against the
original caption of 1000 images. We see that the
ILP improves the score over HMM consistently,
with or without the use of cognitive phrases.
6Including other long-distance scores in HMM decod-
ing would make the problem NP-hard and require more
sophisticated decoding, e.g. ILP.
Grammar Cognitive Relevance
HMM 3.40(?=.82) 3.40(?=.88) 2.25(?=1.37)
ILP 3.56(?=.90) 3.60(?=.98) 2.37(?=1.49)
Hum. 4.36(?=.79) 4.77(?=.66) 3.86(?=1.60)
Table 4: Human Evaluation: Multi-Aspect Rating
(? is a standard deviation)
Human Evaluation I ? Ranking: We com-
plement the automatic evaluation with Mechan-
ical Turk evaluation. In ranking evaluation, we
ask raters to choose a better caption between
two choices7. We do this rating with and with-
out showing the images, as summarized in Ta-
ble 2 & 3. When images are shown, raters evalu-
ate content relevance as well as linguistic quality
of the captions. Without images, raters evaluate
only linguistic quality.
We found that raters generally prefer ILP gen-
erated captions over HMM generated ones, twice
as much (67.2% ILP V.S. 32.8% HMM), if im-
ages are not presented. However the difference is
less pronounced when images are shown. There
could be two possible reasons. The first is that
when images are shown, the Turkers do not try
as hard to tell apart the subtle difference be-
tween the two imperfect captions. The second
is that the relative content relevance of ILP gen-
erated captions is negating the superiority in lin-
guistic quality. We explore this question using
multi-aspect rating, described below.
Note that ILP generated captions are exceed-
ingly (71.8 %) preferred over the Retrieval
baseline (Ordonez et al, 2011), despite the gen-
erated captions tendency to be more prone to
grammatical and cognitive errors than retrieved
ones. This indicates that the generated captions
must have substantially better content relevance
to the query image, supporting the direction of
this research. Finally, notice that as much as
16% of the time, ILP generated captions are pre-
ferred over the original human generated ones
(examples in Figure 2).
Human Evaluation II ? Multi-Aspect Rat-
ing: Table 4 presents rating in the 1?5 scale (5:
perfect, 4: almost perfect, 3: 70?80% good, 2:
7We present two captions in a randomized order.
365
Found MIT boy 
gave me this 
quizical expression. 
One of the most shirt 
in the wall of the 
house. 
Grammar Problems 
Here you can see a 
bright red flower taken 
near our apartment in 
Torremolinos the Costa 
Del Sol. 
Content Irrelevance 
This is a shoulder bag with 
a blended rainbow effect. 
Cognitive Absurdity 
Here you can see a cross 
by the frog in the sky. 
Figure 3: Examples with different aspects of prob-
lems in the ILP generated captions.
50?70% good, 1: totally bad) in three different
aspects: grammar, cognitive correctness,8 and
relevance. We find that ILP improves over HMM
in all aspects, however, the relevance score is no-
ticeably worse than scores of two other criteria.
It turns out human raters are generally more
critical against the relevance aspect, as can be
seen in the ratings given to the original human
generated captions.
Discussion with Examples: Figure 1 shows
contrastive examples of HMM vs ILP gener-
ated captions. Notice that HMM captions
look robotic, containing spurious and redundant
phrases due to lack of discourse constraints, and
often discussing an awkward set of objects due
to lack of image-level content planning. Also
notice how image-level content planning under-
pinned by language statistics helps correct some
of the erroneous vision detections. Figure 3
shows some example mistakes in the ILP gen-
erated captions.
7 Related Work & Discussion
Although not directly focused on image descrip-
tion generation, some previous work in the realm
of summarization shares the similar problem of
content planning and surface realization. There
8E.g., ?A desk on top of a cat? is grammatically cor-
rect, but cognitively absurd.
are subtle, but important differences however.
First, sentence compression is hardly the goal
of image description generation, as human writ-
ten descriptions are not necessarily succinct.9
Second, unlike summarization, we are not given
with a set of coherent text snippet to begin with,
and the level of noise coming from the visual
recognition errors is much higher than that of
starting with clean text. As a result, choosing
an additional phrase in the image description is
much riskier than it is in summarization.
Some recent research proposed very elegant
approaches to summarization using ILP for col-
lective content planning and/or surface realiza-
tion (e.g., Martins and Smith (2009), Woodsend
and Lapata (2010), Woodsend et al (2010)).
Perhaps the most important difference in our
approach is the use of negative weights in the
objective function to create the necessary ten-
sion between selection (salience) and compatibil-
ity, which makes it possible for ILP to generate
variable length descriptions, effectively correct-
ing some of the erroneous vision detections. In
contrast, all previous work operates with a pre-
defined upper limit in length, hence the ILP was
formulated to include as many textual units as
possible modulo constraints.
To conclude, we have presented a collective
approach to generating natural image descrip-
tions. Our approach is the first to systematically
incorporate state of the art computer vision
to retrieve visually relevant candidate phrases,
then produce images descriptions that are sub-
stantially more complex and human-like than
previous attempts.
Acknowledgments T. L. Berg is supported
in part by NSF CAREER award #1054133; A.
C. Berg and Y. Choi are partially supported by
the Stony Brook University Office of the Vice
President for Research. We thank K. Yam-
aguchi, X. Han, M. Mitchell, H. Daume III, A.
Goyal, K. Stratos, A. Mensch, J. Dodge for data
pre-processing and useful initial discussions.
9On a related note, the notion of saliency also differs
in that human written captions often digress on details
that might be tangential to the visible content of the
image. E.g., ?This is a dress my mom made.?, where the
picture does not show a woman making the dress.
366
References
Ahmet Aker and Robert Gaizauskas. 2010. Gen-
erating image descriptions using dependency rela-
tional patterns. In ACL.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of nlg systems.
In EACL 2006, 11st Conference of the European
Chapter of the Association for Computational Lin-
guistics, Proceedings of the Conference, April 3-7,
2006, Trento, Italy. The Association for Computer
Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In Pro-
ceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR?05) - Volume 1 - Volume 01, CVPR
?05, pages 886?893, Washington, DC, USA. IEEE
Computer Society.
Haris Dindo and Daniele Zambuto. 2010. A prob-
abilistic approach to learning a visually grounded
language model through human-robot interaction.
In IROS, pages 790?796. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences for im-
ages. In ECCV.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part based
models. tPAMI, Sept.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gen-
eration for news images. In ACL.
Fateh Muhammad Hafiz and Ian Tudor. 1989. Ex-
tensive reading and the development of language
skills. ELT Journal, 43(1):4?13.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar,
Siming Li, Yejin Choi, Alexander C Berg, and
Tamara L Berg. 2011. Babytalk: Understand-
ing and generating simple image descriptions. In
CVPR.
Thomas K. Leung and Jitendra Malik. 1999. Rec-
ognizing surfaces using three-dimensional textons.
In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Compos-
ing simple image descriptions using web-scale n-
grams. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 220?228, Portland, Oregon, USA, June.
Association for Computational Linguistics.
David G. Lowe. 2004. Distinctive image features
from scale-invariant keypoints. Int. J. Comput.
Vision, 60:91?110, November.
Andre Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 1?9, Boulder, Colorado,
June. Association for Computational Linguistics.
Derek D. Monner and James A. Reggia. 2011. Sys-
tematically grounding language through vision in
a deep, recurrent neural network. In Proceed-
ings of the 4th international conference on Arti-
ficial general intelligence, AGI?11, pages 112?121,
Berlin, Heidelberg. Springer-Verlag.
Vicente Ordonez, Girish Kulkarni, and Tamara L.
Berg. 2011. Im2text: Describing images using 1
million captioned photographs. In Neural Infor-
mation Processing Systems (NIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, com-
pact, and interpretable tree annotation. In COL-
ING/ACL.
Deb K. Roy. 2002. Learning visually-grounded
words and syntax for a scene description task.
Computer Speech and Language, In review.
Wai-King Tsang. 1996. Comparing the effects of
reading and writing on writing performance. Ap-
plied Linguistics, 17(2):210?233.
Kristian Woodsend and Mirella Lapata. 2010. Au-
tomatic generation of story highlights. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 565?
574, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella
Lapata. 2010. Title generation with quasi-
synchronous grammar. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 513?523,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
367
Jianxiong Xiao, James Hays, Krista A. Ehinger,
Aude Oliva, and Antonio Torralba. 2010. Sun
database: Large-scale scene recognition from
abbey to zoo. In CVPR.
Yezhou Yang, Ching Teo, Hal Daume III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Proceedings of the
2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 444?454, Edin-
burgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proc. IEEE, 98(8).
368
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 790?796,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generalizing Image Captions for Image-Text Parallel Corpus
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
The ever growing amount of web images
and their associated texts offers new op-
portunities for integrative models bridging
natural language processing and computer
vision. However, the potential benefits of
such data are yet to be fully realized due
to the complexity and noise in the align-
ment between image content and text. We
address this challenge with contributions
in two folds: first, we introduce the new
task of image caption generalization, for-
mulated as visually-guided sentence com-
pression, and present an efficient algo-
rithm based on dynamic beam search with
dependency-based constraints. Second,
we release a new large-scale corpus with
1 million image-caption pairs achieving
tighter content alignment between images
and text. Evaluation results show the in-
trinsic quality of the generalized captions
and the extrinsic utility of the new image-
text parallel corpus with respect to a con-
crete application of image caption transfer.
1 Introduction
The vast number of online images with accom-
panying text raises hope for drawing synergistic
connections between human language technolo-
gies and computer vision. However, subtleties and
complexity in the relationship between image con-
tent and text make exploiting paired visual-textual
data an open and interesting problem.
Some recent work has approached the prob-
lem of composing natural language descriptions
for images by using computer vision to retrieve
images with similar content and then transferring
?A house being 
pulled by a boat.? 
?I saw her in the light 
of her reading lamp 
and sneaked back to 
her door with the 
camera.? 
?Sections of the 
bridge sitting in the 
Dyer Construction 
yard south of 
Cabelas Driver.? 
Circumstantial 
information that is not 
visually present 
Visually relevant, 
but with overly 
extraneous details 
Visually truthful, 
but for an uncommon 
situation 
Figure 1: Examples of captions that are not readily
applicable to other visually similar images.
text from the retrieved samples to the query im-
age (e.g. Farhadi et al (2010), Ordonez et al
(2011), Kuznetsova et al (2012)). Other work
(e.g. Feng and Lapata (2010a), Feng and Lapata
(2010b)) uses computer vision to bias summariza-
tion of text associated with images to produce de-
scriptions. All of these approaches rely on ex-
isting text that describes visual content, but many
times existing image descriptions contain signifi-
cant amounts of extraneous, non-visual, or other-
wise non-desirable content. The goal of this paper
is to develop techniques to automatically clean up
visually descriptive text to make it more directly
usable for applications exploiting the connection
between images and language.
As a concrete example, consider the first image
in Figure 1. This caption was written by the photo
owner and therefore contains information related
to the context of when and where the photo was
taken. Objects such as ?lamp?, ?door?, ?camera?
are not visually present in the photo. The second
image shows a similar but somewhat different is-
sue. Its caption describes visible objects such as
?bridge? and ?yard?, but ?Cabelas Driver? are
overly specific and not visually detectable. The
790
Dependency Constraints with Examples Additional Dependency ConstraintsConstraints Sentence Dependency
advcl*(?) Taken when it was running... taken?running acomp*(?), advmod(?), agent*(?), attr(?)
amod(?) A wooden chair in the living room chair? wooden auxpass(?), cc*(?),complm(?), cop*(?)
aux(?) This crazy dog was jumping... jumping?was csubj*/csubjpass*(?),expl(?), mark*(?)
ccomp*(?) I believe a bear was in the box... believe?was infmod*(?), mwe(?), nsubj*/nsubjpass*(?)
prep(?) A view from the balcony view?from npadvmod(?), nn(?), conj*(?), num*(?)
det(?) A cozy street cafe... cafe?A number(?), parataxis(?),?
dobj*(?) A curious cow surveys the road... surveys?road partmod*(?), pcomp*(?), purpcl*(?)
iobj*(?) ...rock gives the water the color gives?water possessive(?), preconj*(?), predet*(?)
neg(?) Not a cloud in the sky... cloud?Not prt(?), quantmod(?), rcmod(?), ref(?)
pobj*(?) This branch was on the ground... on?ground rel*(?), tmod*(?), xcomp*(?), xsubj(?)
Table 1: Dependency-based Constraints
text of the third image, ?A house being pulled by a
boat?, pertains directly to the visual content of the
image, but is unlikely to be useful for tasks such as
caption transfer because the depiction is unusual.1
This phenomenon of information gap between the
visual content of the images and their correspond-
ing narratives has been studied closely by Dodge
et al (2012).
The content misalignment between images and
text limits the extent to which visual detectors
can learn meaningful mappings between images
and text. To tackle this challenge, we introduce
the new task of image caption generalization that
rewrites captions to be more visually relevant and
more readily applicable to other visually similar
images. Our end goal is to convert noisy image-
text pairs in the wild (Ordonez et al, 2011) into
pairs with tighter content alignment, resulting in
new simplified captions over 1 million images.
Evaluation results show both the intrinsic quality
of the generalized captions and the extrinsic util-
ity of the new image-text parallel corpus. The new
parallel corpus will be made publicly available.2
2 Sentence Generalization as Constraint
Optimization
Casting the generalization task as visually-guided
sentence compression with lightweight revisions,
we formulate a constraint optimization problem
that aims to maximize content selection and lo-
cal linguistic fluency while satisfying constraints
driven from dependency parse trees. Dependency-
based constraints guide the generalized caption
1Open domain computer vision remains to be an open
problem, and it would be difficult to reliably distinguish pic-
tures of subtle visual differences, e.g., pictures of ?a water
front house with a docked boat? from those of ?a floating
house pulled by a boat?.
2Available at http://www.cs.stonybrook.edu/
?ychoi/imgcaption/
to be grammatically valid (e.g., keeping articles
in place, preventing dangling modifiers) while re-
maining semantically compatible with respect to a
given image-text pair (e.g., preserving predicate-
argument relations). More formally, we maximize
the following objective function:
F (y;x) = ?(y;x, v) + ?(y;x)
subject to C(y;x, v)
where x = {xi} is the input caption (a sentence),
v is the accompanying image, y = {yi} is the
output sentence, ?(y;x, v) is the content selection
score, ?(y;x) is the linguistic fluency score, and
C(y;x, v) is the set of hard constraints. Let l(yi)
be the index of the word in x that is selected as the
i?th word in the output y so that xl(yi) = yi. Then,
we factorize ?(?) and ?(?) as:
?(y;x, v) =
?
i
?(yi, x, v) =
?
i
?(xl(yi), v)
?(y;x) =
?
i
?(yi, ..., yi?K)
=
?
i
?(xl(yi), ..., xl(yi?K))
where K is the size of local context.
Content Selection ? Visual Estimates:
The computer vision system used consists of 7404
visual classifiers for recognizing leaf level Word-
Net synsets (Fellbaum, 1998). Each classifier is
trained using labeled images from the ImageNet
dataset (Deng et al, 2009) ? an image database
of over 14 million hand labeled images orga-
nized according to the WordNet hierarchy. Image
similarity is represented using a Spatial Pyramid
Match Kernel (SPM) (Lazebnik et al, 2006) with
Locality-constrained Linear Coding (Wang et al,
2010) on shape based SIFT features (Lowe, 2004).
791
  (a) (b)
0 1 2 3 4 5 6 7 80
200400
600800
# of s
enten
ces (
in tho
usan
ds)
0 1 2 3 40
400
800
1200
# of s
enten
ces (
in tho
usan
ds)
Figure 2: Number of sentences (y-axis) for each
average (x-axis in (a)) and maximum (x-axis in
(b)) number of words with future dependencies
Models are linear SVMs followed by a sigmoid to
produce probability for each node.3
Content Selection ? Salient Topics:
We consider Tf.Idf driven scores to favor salient
topics, as those are more likely to generalize
across many different images. Additionally, we
assign a very low content selection score (??) for
proper nouns and numbers and a very high score
(larger then maximum idf or visual score) for the
2k most frequent words in our corpus.
Local Linguistic Fluency:
We model linguistic fluency with 3-gram condi-
tional probabilities:
?(xl(yi), xl(yi?1), xl(yi?2)) (1)
= p(xl(yi)|xl(yi?2), xl(yi?1))
We experiment with two different ngram statis-
tics, one extracted from the Google Web 1T cor-
pus (Brants and Franz., 2006), and the other com-
puted from the 1M image-caption corpus (Or-
donez et al, 2011).
Dependency-driven Constraints:
Table 1 defines the list of dependencies used
as constraints driven from the typed dependen-
cies (de Marneffe and Manning, 2009; de Marn-
effe et al, 2006). The direction of arrows indi-
cate the direction of inclusion requirements. For
example, dep(X ?? Y ), denotes that ?X? must
be included whenever ?Y ? is included. Similarly,
dep(X ?? Y ) denotes that ?X? and ?Y ? must
either be included together or eliminated together.
We determine the uni- or bi-directionality of these
constraints by manually examining a few example
sentences corresponding to each of these typed de-
pendencies. Note that some dependencies such as
det(??) would hold regardless of the particular
3Code was provided by Deng et al (2012).
Method-1 (M1) v.s. Method-2 (M2) M1 winsover M2
SALIENCY ORIG 76.34%
VISUAL ORIG 81.75%
VISUAL SALIENCY 72.48%
VISUAL VISUAL W/O CONSTR 83.76%
VISUAL NGRAM-ONLY 90.20%
VISUAL HUMAN 19.00%
Table 2: Forced Choice Evaluation (LM Corpus =
Google)
lexical items, while others, e.g., dobj(??) may
or may not be necessary depending on the context.
Those dependencies that we determine as largely
context dependent are marked with * in Table 1.
One could consider enforcing all dependency
constraints in Table 1 as hard constraints so that
the compressed sentence must not violate any of
those directed dependency constraints. Doing so
would lead to overly conservative compression
with least compression ratio however. Therefore,
we relax those that are largely context dependent
as soft constraints (marked in Table 1 with *) by
introducing a constant penalty term in the objec-
tive function. Alternatively, the dependency based
constraints can be learned statistically from the
training corpus of paired original and compressed
sentences. Since we do not have such in-domain
training data at this time, we leave this exploration
as future research.
Dynamic Programming with Dynamic Beam:
The constraint optimization we formulated corre-
sponds to an NP-hard problem. In our work, hard
constraints are based only on typed dependencies,
and we find that long range dependencies occur in-
frequently in actual image descriptions, as plotted
in Figure 2. With this insight, we opt for decoding
based on dynamic programming with dynamically
adjusted beam.4 Alternatively, one can find an ap-
proximate solution using Integer Linear Program-
ming (e.g., Clarke and Lapata (2006), Clarke and
Lapata (2007), Martins and Smith (2009)).
3 Evaluation
Since there is no existing benchmark data for im-
age caption generalization, we crowdsource evalu-
ation using Amazon Mechanical Turk (AMT). We
empirically compare the following options:
4The required beam size at each step depends on how
many words have dependency constraints involving any word
following the current one ? beam size is at most 2p, where p
is the max number of words dependent on any future words.
792
Big elm tree over 
the house is no 
their anymore. 
? Tree over the house. 
Abandonned 
houses in the 
forest. 
? Houses in the 
     forest. 
A woman paints a tree in 
bloom near the duck pond 
in the Boston Public 
Garden, April 15, 2006. 
? A tree in bloom . 
Pillbox in field 
behind a pub 
car park. 
? Pub car. 
Flowering tree in 
mixed forest at 
Wakehurst. 
? Flowering tree  
    in forest. 
The insulbrick matches 
the yard. This is outside 
of medina ohio near the 
tonka truck house. 
? The yard. This is 
     outside the house. 
Query Image Retrieved Images 
Figure 3: Example Image Caption Transfer
Method LM strict matching semantic matchingCorpus BLEU P R F BLEU P R F
ORIG N/A 0.063 0.064 0.139 0.080 0.215 0.220 0.508 0.276
SALIENCY Image Corpus 0.060 0.074 0.077 0.068 0.302 0.411 0.399 0.356
VISUAL Image Corpus 0.060 0.075 0.075 0.068 0.305 0.422 0.397 0.360
SALIENCY Google Corpus 0.064 0.070 0.101 0.074 0.286 0.337 0.459 0.340
VISUAL Google Corpus 0.065 0.071 0.098 0.075 0.296 0.354 0.457 0.350
Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.
? ORIG: original uncompressed captions
? HUMAN: compressed by humans (See ? 3.2)
? SALIENCY: linguistic fluency + saliency-based
content selection + dependency constraints
? VISUAL: linguistic fluency + visually-guided
content selection + dependency constraints
? x W/O CONSTR: method xwithout dependency
constraints
? NGRAM-ONLY: linguistic fluency only
3.1 Intrinsic Evaluation: Forced Choice
Turkers are provided with an image and two cap-
tions (produced by different methods) and are
asked to select a better one, i.e., the most relevant
and plausible caption that contains the least extra-
neous information. Results are shown in Table 2.
We observe that VISUAL (full model with visually
guided content selection) performs the best, being
selected over SALIENCY (content-selection with-
out visual information) in 72.48% cases, and even
over the original image caption in 81.75% cases.
This forced-selection experiment between VI-
SUAL and ORIG demonstrates the degree of noise
prevalent in the image captions in the wild. Of
course, if compared against human-compressed
captions, the automatic captions are preferred
much less frequently ? in 19% of the cases. In
those 19% cases when automatic captions are pre-
ferred over human-compressed ones, it is some-
times that humans did not fully remove informa-
tion that is not visually present or verifiable, and
other times humans overly compressed. To ver-
ify the utility of dependency-based constraints,
we also compare two variations of VISUAL, with
and without dependency-based constraints. As ex-
pected, the algorithm with constraints is preferred
in the majority of cases.
3.2 Extrinsic Evaluation: Image-based
Caption Retrieval
We evaluate the usefulness of our new image-text
parallel corpus for automatic generation of image
descriptions. Here the task is to produce, for a
query image, a relevant description, i.e., a visu-
ally descriptive caption. Following Ordonez et al
(2011), we produce a caption for a query image
by finding top k most similar images within the
1M image-text corpus (Ordonez et al, 2011) and
then transferring their captions to the query im-
age. To compute evaluation measures, we take the
average scores of BLEU(1) and F-score (unigram-
based with respect to content-words) over k = 5
candidate captions.
Image similarity is computed using two global
(whole) image descriptors. The first is the GIST
feature (Oliva and Torralba, 2001), an image de-
scriptor related to perceptual characteristics of
scenes ? naturalness, roughness, openness, etc.
The second descriptor is also a global image de-
scriptor, computed by resizing the image into a
?tiny image? (Torralba et al, 2008), which is ef-
fective in matching the structure and overall color
of images. To find visually relevant images, we
compute the similarity of the query image to im-
793
Huge wall of glass 
at the Conference 
Centre in 
Yohohama  Japan.  
? Wall of glass  
My footprint in a 
sand box 
? A sand box  
James the cat is 
dreaming of running 
in a wide green 
valley 
? Running in 
a valley (not 
relevant) 
This little boy was so 
cute. He was flying his 
spiderman kite all by 
himself on top of Max 
Patch  
? This little boy was so 
cute. He was flying 
(semantically odd) 
A view of the post office 
building in Manila from 
the other side of the 
Pasig River  
? A view of the post 
office building from 
the side  
Cell phone shot of 
a hat stall in the 
Northeast Market, 
Baltimore, MD.  
? Cell phone shot.  
(visually not 
verifiable) 
Figure 4: Good (left three, in blue) and bad examples (right three, in red) of generalized captions
ages in the whole dataset using an unweighted sum
of gist similarity and tiny image similarity.
Gold standard (human compressed) captions are
obtained using AMT for 1K images. The results
are shown in Table 3. Strict matching gives credit
only to identical words between the gold-standard
caption and the automatically produced caption.
However, words in the original caption of the
query image (and its compressed caption) do not
overlap exactly with words in the retrieved cap-
tions, even when they are semantically very close,
which makes it hard to see improvements even
when the captions of the new corpus are more gen-
eral and transferable over other images. Therefore,
we also report scores based on semantic matching,
which gives partial credits to word pairs based on
their lexical similarity.5 The best performing ap-
proach with semantic matching is VISUAL (with
LM = Image corpus), improving BLEU, Precision,
F-score substantially over those of ORIG, demon-
strating the extrinsic utility of our newly gener-
ated image-text parallel corpus in comparison to
the original database. Figure 3 shows an example
of caption transfer.
4 Related Work
Several recent studies presented approaches to
automatic caption generation for images (e.g.,
Farhadi et al (2010), Feng and Lapata (2010a),
Feng and Lapata (2010b), Yang et al (2011),
Kulkarni et al (2011), Li et al (2011), Kuznetsova
et al (2012)). The end goal of our work differs in
that we aim to revise original image captions into
5We take Wu-Palmer Similarity as similarity mea-
sure (Wu and Palmer, 1994). When computing BLEU with
semantic matching, we look for the match with the highest
similarity score among words that have not been matched be-
fore. Any word matched once (even with a partial credit) will
be removed from consideration when matching next words.
descriptions that are more general and align more
closely to the visual image content.
In comparison to prior work on sentence com-
pression, our approach falls somewhere between
unsupervised to distant-supervised approach (e.g.,
Turner and Charniak (2005), Filippova and Strube
(2008)) in that there is not an in-domain train-
ing corpus to learn generalization patterns directly.
Future work includes exploring more direct su-
pervision from human edited sample generaliza-
tion (e.g., Knight and Marcu (2000), McDonald
(2006)) Galley and McKeown (2007), Zhu et al
(2010)), and the inclusion of edits beyond dele-
tion, e.g., substitutions, as has been explored by
e.g., Cohn and Lapata (2008), Cordeiro et al
(2009), Napoles et al (2011).
5 Conclusion
We have introduced the task of image caption gen-
eralization as a means to reduce noise in the paral-
lel corpus of images and text. Intrinsic and extrin-
sic evaluations confirm that the captions in the re-
sulting corpus align better with the image contents
(are often preferred over the original captions by
people), and can be practically more useful with
respect to a concrete application.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. Additionally, Tamara Berg is supported
by NSF #1054133 and NSF #1161876. We thank
reviewers for many insightful comments and sug-
gestions.
794
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Association
for Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1?11, Prague, Czech Republic, June.
Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 137?144,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Joao Cordeiro, Gael Dias, and Pavel Brazdil. 2009.
Unsupervised induction of sentence compression
rules. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 15?22, Suntec, Singapore,
August. Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2009. Stanford typed dependencies manual.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Language Resources and Evaluation Conference
2006.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hi-
erarchical Image Database. In Conference on Com-
puter Vision and Pattern Recognition.
Jia Deng, Jonathan Krause, Alexander C. Berg, and
L. Fei-Fei. 2012. Hedging your bets: Optimizing
accuracy-specificity trade-offs in large scale visual
recognition. In Conference on Computer Vision and
Pattern Recognition.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daume III, Alex Berg, and
Tamara Berg. 2012. Detecting visual text. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
762?772, Montre?al, Canada, June. Association for
Computational Linguistics.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young1, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
In European Conference on Computer Vision.
Christiane D. Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Association for Computa-
tional Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Hu-
man Language Technologies.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, INLG ?08, pages 25?32,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI/IAAI, pages 703?710.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Babytalk: Understanding and gener-
ating simple image descriptions. In Conference on
Computer Vision and Pattern Recognition.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gen-
eration of natural image descriptions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 359?368, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching. In Conference on Computer Vision and
Pattern Recognition, June.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220?
228, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. Int. J. Comput. Vision,
60:91?110, November.
795
Andre Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on
Integer Linear Programming for Natural Language
Processing, pages 1?9, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Ryan T. McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL
2006, 11st Conference of the European Chapter of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, April 3-7, 2006, Trento,
Italy. The Association for Computer Linguistics.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84?90, Portland, Oregon, June.
Association for Computational Linguistics.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: a holistic representation of the
spatial envelope. International Journal of Computer
Vision.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Antonio Torralba, Rob Fergus, and William T. Free-
man. 2008. 80 million tiny images: a large dataset
for non-parametric object and scene recognition.
Pattern Analysis and Machine Intelligence, 30.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 290?297, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv,
T. Huang, and Yihong Gong. 2010. Locality-
constrained linear coding for image classification.
In Conference on Computer Vision and Pattern
Recognition (CVPR).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 444?454, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353?1361, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
796
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220?228,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Composing Simple Image Descriptions using Web-scale N-grams
Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg, and Yejin Choi
Department of Computer Science
Stony Brook University
NY 11794, USA
{silli, gkulkarni, tlberg, aberg, ychoi}@cs.stonybrook.edu
Abstract
Studying natural language, and especially how
people describe the world around them can
help us better understand the visual world. In
turn, it can also help us in the quest to generate
natural language that describes this world in a
human manner. We present a simple yet effec-
tive approach to automatically compose im-
age descriptions given computer vision based
inputs and using web-scale n-grams. Unlike
most previous work that summarizes or re-
trieves pre-existing text relevant to an image,
our method composes sentences entirely from
scratch. Experimental results indicate that it is
viable to generate simple textual descriptions
that are pertinent to the specific content of an
image, while permitting creativity in the de-
scription ? making for more human-like anno-
tations than previous approaches.
1 Introduction
Gaining a better understanding of natural language,
and especially natural language associated with im-
ages helps drive research in both computer vision
and natural language processing (e.g., Barnard et
al. (2003), Pastra et al (2003), Feng and Lapata
(2010b)). In this paper, we look at how to exploit
the enormous amount of textual data electronically
available today, web-scale n-gram data in particular,
in a simple yet highly effective approach to com-
pose image descriptions in natural language. Auto-
matic generation of image descriptions differs from
automatic image tagging (e.g., Leong et al (2010))
in that we aim to generate complex phrases or sen-
tences describing images rather than predicting in-
dividual words. These natural language descriptions
can be useful for a variety of applications, includ-
ing image retrieval, automatic video surveillance,
and providing image interpretations for visually im-
paired people.
Our work contrasts to most previous approaches
in four key aspects: first, we compose fresh sen-
tences from scratch, instead of retrieving (Farhadi et
al. (2010)), or summarizing existing text fragments
associated with an image (e.g., Aker and Gaizauskas
(2010), Feng and Lapata (2010a)). Second, we aim
to generate textual descriptions that are truthful to
the specific content of the image, whereas related
(but subtly different) work in automatic caption gen-
eration creates news-worthy text (Feng and Lapata
(2010a)) or encyclopedic text (Aker and Gaizauskas
(2010)) that is contextually relevant to the image, but
not closely pertinent to the specific content of the
image. Third, we aim to build a general image de-
scription method as compared to work that requires
domain specific hand-written grammar rules (Yao et
al. (2010)). Last, we allow for some creativity in
the generation process which produces more human-
like descriptions than a closely related, very recent
approach that drives annotation more directly from
computer vision inputs (Kulkarni et al, 2011).
In this work, we propose a novel surface realiza-
tion technique based on web-scale n-gram data. Our
approach consists of two steps: (n-gram) phrase se-
lection and (n-gram) phrase fusion. The first step
? phrase selection ? collects candidate phrases that
may be potentially useful for generating the descrip-
tion of a given image. This step naturally accom-
modates uncertainty in image recognition inputs as
220
Hairy goat under a tree 
Fluffy posturing sheep under a tree 
<furry;gray;brown,sheep>,by;near,<rusty;gray;green,tree> 
furry 
gray 
brown 
rusty 
gray 
green 
by 
near 
Figure 1: The big picture of our task to automatically
generate image description.
well as synonymous words and word re-ordering to
improve fluency. The second step ? phrase fusion
? finds the optimal compatible set of phrases us-
ing dynamic programming to compose a new (and
more complex) phrase that describes the image. We
compare the performance of our proposed approach
to three baselines based on conventional techniques:
language models, parsers, and templates.
Despite its simplicity, our approach is highly ef-
fective for composing image descriptions: it gen-
erates mostly appealing and presentable language,
while permitting creative writing at times (see Fig-
ure 5 for example results). We conclude from our
exploration that (1) it is viable to generate simple
textual descriptions that are germane to the specific
image content, and that (2) world knowledge implic-
itly encoded in natural language (e.g., web-scale n-
gram data) can help enhance image content recogni-
tion.
2 Image Recognition
Figure 1 depicts our system flow: a) an image is in-
put into our system, b) image recognition techniques
are used to extract visual content information, c) vi-
sual content is encoded as a set of triples, d) natural
language descriptions are generated.
In this section, we briefly describe the image
recognition system that extracts visual information
and encodes it as a set of triples. For a given image,
the image recognizer extracts objects, attributes and
spatial relationships among objects as follows:
1. Objects: including things (e.g., bird, bus, car)
and stuff (e.g., grass, water, sky, road) are de-
tected.
2. Visual attributes (e.g., feathered, black) are pre-
dicted for each object.
3. Spatial relationships (e.g., on, near, under) be-
tween objects are estimated.
In particular, object detectors are trained using state
of the art mixtures of multi-scale deformable parts
models (Felzenszwalb et al, 2010). Our set of
objects encompasses the 20 PASCAL 2010 object
challenge 1 categories as well as 4 additional cate-
gories for flower, laptop, tiger, and window trained
on images with associated bounding boxes from
Imagenet (Deng et al, 2009). Stuff detectors are
trained to detect regions corresponding to non-part
based object categories (sky, road, building, tree,
water, and grass) using linear SVMs trained on
the low level region features of (Farhadi et al,
2009). These are also trained on images with la-
beled bounding boxes from ImageNet and evaluated
at test time on a coarsely sampled grid of overlap-
ping square regions over whole images. Pixels in
any region with a classification probability above a
fixed threshold are treated as detections.
We select visual attribute characteristics that are
relevant to our object and stuff categories. Our at-
tribute terms include 21 visual modifiers ? adjec-
tives ? related to color (e.g. blue, gray), texture
(e.g. striped, furry), material (e.g. wooden, feath-
ered), general appearance (e.g. rusty, dirty, shiny),
and shape (e.g. rectangular) characteristics. The at-
tribute classifiers are trained on the low level fea-
tures of (Farhadi et al, 2009) using RBF kernel
SVMs. Preposition functions encoding spatial rela-
tionships between objects are hand designed to eval-
uate the spatial relationships between pairs of re-
gions in an image and provide a score for 16 prepo-
sitions (e.g., above, under, against, in etc).
1http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2010/
221
From these three types of visual output, we con-
struct a meaning representation of an image as a
set of triples (one triple for every pair of detected
objects). Each triple encodes a spatial relation be-
tween two objects in the following format: <<adj1,
obj1>, prep, <adj2, obj2>>. The generation pro-
cedure is elaborated in the following two sections.
3 Baseline Approaches to Surface
Realization
This section explores three baseline surface realiza-
tion approaches: language models (?3.1), random-
ized local search (?3.2), and template-based (?3.3).
Our best approach, phrase fusion using web-scale n-
grams follows in ?4.
3.1 Language Model Based Approach
For each triple, as described in ?2, we construct a
sentence. For instance, given the triple <<white,
cloud>, in, <blue, sky>>, we might generate
?There is a white cloud in the blue sky?.
We begin with a simple decoding scheme based
on language models. Let t be a triple, and let V t
be the set of words in t. We perform surface real-
ization by adding function words in-between words
in V t. As a concrete example, suppose we want to
determine whether to insert a function word x be-
tween a pair of words ? ? V t and ? ? V t. Then,
we need to compare the length-normalized probabil-
ity p?(?x?) with p?(??), where p? takes the n?th root
of the probability p for n-word sequences. We in-
sert the new function word x if p?(?x?) ? p?(??)
using the n-gram models, where the probability of
any given sequence w1, ..., wm is approximated by
p(w1, ..., wm) =
m?
i=1
p(wi|wi?(n?1), ..., wi?1)
Note that if we wish to reorder words in V t based on
n-gram based language models, then the decoding
problem becomes an instance of asymmetric trav-
eler?s salesman problem (NP-hard). For brevity, we
retain the original order of words in the given triple.
We later lift this restriction using the web-scale n-
gram based phrase fusion method introduced in ?4.
3.2 Randomized Local Search Approach
A much needed extension to the language model
based surface realization is incorporating parsers to
Begin Loop (until T iterations or convergence)
Choose a position i to revise at random
Choose an edit operation at random
If the edit yields a better score by LM and PCFG
Commit the edit
End Loop
Table 1: Pseudo code for a randomized local search ap-
proach. A possible edit operation includes insertion,
deletion, and replacement. The score of the current sen-
tence is determined by the multiplication LM-based prob-
ability and PCFG-based probability.
enforce long distance regularities for more gram-
matically correct generation. However, optimiz-
ing both language-model-based probabilities and
parser-based probabilities is intractable. Therefore,
we explore a randomized local search approach that
makes greedy revisions using both language models
and parsers. Randomized local search has been suc-
cessfully applied to intractable optimization prob-
lems in AI (e.g., Chisholm and Tadepalli (2002)) and
NLP (e.g., White and Cardie (2002)).
Table 1 shows the skeleton of the algorithm in our
study. Iterating through a loop, it chooses an edit
location and an edit operation (insert, delete, or re-
place) at random. If the edit yields a better score,
then we commit the edit, otherwise we jump to the
next iteration of the loop. We define the score as
score(X) = p?LM (X)p?PCFG(X)
where X is a given sentence (image description),
p?LM (X) is the length normalized probability of X
based on the language model, and p?PCFG(X) is the
length normalized probability of X based on the
probabilistic context free grammar (PCFG) model.
The loop is repeated until convergence or a fixed
number of iterations is reached. Note that this ap-
proach can be extended to simulated annealing to al-
low temporary downward steps to escape from local
maxima. We use the PCFG implementation of Klein
and Manning (2003).
3.3 Template Based Approach
The third approach is a template-based approach
with linguistic constraints, a technique that has of-
ten been used for various practical applications such
as summarization (Zhou and Hovy, 2004) and dia-
222
blue, bike  [2669]  blue, bicycle  [1365]  bike, blue  [1184]  blue, cycle  [324]  cycle, of, the, blue  [172]  cycle, blue  [158]  bicycle, blue  [154]  bike, in, blue  [98]  cycle, of, blue  [64]  bike, with, blue  [43]  
< < blue , bicycle >, near, < shiny , person > >  
bright, boy  [8092]  bright, child  [7840]  bright, girl  [6191]  bright, kid  [5873]  bright, person  [5461 ]  bright, man  [4936]  bright, woman  [2726]  bright, women  [1684]  lady, bright  [1360]  bright, men  [1050]  
person, operating, a, bicycle  [3409]  man, on, a, bicycle  [2842]  cycle, of, child  [2507]  bike, for, men  [2485]  person, riding, a, bicycle  [2118]  cycle, in, women  [1853]  bike, for, women  [1442]  boy, on, a, bicycle  [1378]  cycle, of, women  [1288]  man, on, a, bike  [1283]  
bright person operating a blue bicycle [2541 158 938 5] bright man on a blue bicycle [1914 83 72 88 0]  bright man on a blue bike [1690 24 78 07 2]  bright person riding a blue bicycle [157 881 332 70]  bright boy on a blue bicycle [1522 08 09 24 0]  blue bike for bright men [6964 08 82 50 ]  blue bike for bright women [648120 743 2]  blue cycle of bright child [6368 18 11 20 ]  blue cycle in bright women [1011 02 64 48 ]  
Figure 2: Illustration of phrase fusion composition al-
gorithm using web-scale n-grams. Numbers in square
brackets are n-gram frequencies.
logue systems (Channarukul et al, 2003). Because
the meaning representation produced by the image
recognition system has a fixed pattern of <<adj1,
obj1>, prep, <adj2, obj2>>, it can be templated as
?There is a [adj1] [obj1] [prep] the [adj2] [obj2].?
We also include templates that encode basic dis-
course constraints. For instance, the template that
generated the first sentences in Figure 3 and 4 is:
[PREFIX] [#(x1)] [x1], [#(x2)] [x2], ... and [#(xk)]
[xk], where xi is the name of an object (e.g. ?cow?),
#(xi) is the number of instances of xi (e.g. ?one?),
and PREFIX ? {?This picture shows?, ?This is a pic-
ture of?, etc}.
Although this approach can produce good looking
sentences in a limited domain, there are many limita-
tions. First, a template-based approach does not al-
low creative writing and produces somewhat stilted
prose. In particular, it cannot add interesting new
words, or replace existing content words with better
ones. In addition, such an approach does not allow
any reordering of words which might be necessary to
create a fluent sentence. Finally, hand-written rules
are domain-specific, and do not generalize well to
new domains.
4 Surface Realization by Phrase Fusion
using Web-scale N-gram
We now introduce an entirely different approach
that addresses the limitations of the conventional ap-
proaches discussed in ?3. This approach is based
on web-scale n-gram, also known as Google Web
1T data, which provides the frequency count of each
possible n-gram sequence for 1 ? n ? 5.
4.1 [Step I] ? Candidate Phrase Selection
We first define three different sets of phrases for each
given triple <<adj1, obj1>, prep, <adj2, obj2>>:
? O1 = {(x, f) | x is an n-gram phrase describ-
ing the first object using the words adj1 and
obj1, and f is the frequency of x}
? O2 = {(x, f) | x is an n-gram phrase describ-
ing the second object using the words adj2 and
obj2, and f is the frequency of x}
? R = {(x, f) | x is an n-gram describing the re-
lation between the two objects using the words
obj1 and obj2, and f is the frequency of x}
We find n-gram phrases for O1, O2, andR from the
Google Web 1T data. The search patterns for O1 is:
? [adj1] [?]n?2 [obj1]
? [obj1] [?]n?2 [adj1]
where [?] is a wildcard word, and [?]n?2 denotes
a sequence of n-2 number of wildcard words in a n-
gram sequence. For wildcards, we only allow a lim-
ited set of function words, and verbs in the gerund
form2 for reasons that will become clearer in the
next step ? phrase fusion in ?4.2.
Note that it is the second pattern that allows
interesting re-ordering of words in the final sen-
tence generation. For instance, suppose adj1=green,
obj1=person. Then it is more natural to generate
a phrase using the reverse pattern such as, ?person
in green? or ?person wearing green? than simply
concatenating adj1 and obj1 to generate ?green per-
son?. Similarly, given obj1=bicycle and obj2=man,
generating a phrase using the reverse pattern, e.g.,
?man with a bicycle? would be more natural than
?bicycle with a man?. Our hypothesis is that such
ordering preference is implicitly encoded in the
web-scale n-grams via frequencies.
It is worthwhile to note that our pattern matching
is case sensitive, and we only allow patterns that are
2We treat words with suffix ?ing? as gerund for simplicity.
223
Phrase Fusion Black bull eating fluffy grass Bright apartment in the blue sky Shaggy grass looking at the blue sky 
Local Search the black cow or even by the furry grass.  the shiny building up by the same blue sky. the furry grass be below over one  blue sky. 
Template   This picture shows one cow, one building, one grass and one sky. The black cow is by the shiny building, and by the furry grass, and by the blue sky. The shiny building is by the furry grass, and by the blue sky. The furry grass is below the blue sky.  
Simple decoding the black cow or by the furry grass. the shiny building up by the blue sky. the furry grass be below one  blue sky.  
Image Recognition Output as Tripes: <black;yellow;rusty,cow>,by;near;by,<furry;green;brown,grass>  <shiny;colorful;yellow,building>,by;near;by,<blue;clear;colorful,sky>  <furry;green;brown,grass>,below;beneath;by,<blue;clear;colorful,sky> 
Figure 3: Comparison of image descriptions
all lower-case. From our pilot study, we found that
n-grams with upper case characters are likely from
named entities, which distort the n-gram frequency
distribution that we rely on during the phrase fusion
phase. To further reduce noise, we also discard any
n-gram that contains a character that is not an alpha-
bet.
Accommodating Uncertainty We extend candi-
date phrase selection in order to cope with uncer-
tainty from the image recognition. In particular,
for each object detection obji, we include its top 3
predicted modifiers adji1, adji2, adji3 determined
by the attribute classifiers (see ?2) to expand the
set O1 and O2 accordingly. For instance, given
adji =(shiny or white) and obji = sheep, we can
consider both <shiny,sheep> and <white,sheep>
pairs to predict more compatible pairs of words.
Accommodating Synonyms Additionally, we
augment each modifier adji and each object name
obji with synonyms to further expand our sets
O1, O2, and R. These expanded sets of phrases
enable resulting generations that are more fluent
and creative.
4.2 [Step II] ? Phrase Fusion
Given the expanded sets of phrases O1, O2, and R
described above, we perform phrase fusion to gen-
erate simple image description. In this step, we find
the best combination of three phrases, (x?1, f?1) ?
O1, (x?2, f?2) ? O2, and (x?R, f?R) ? R as follows:
(x?1, x?2, x?R) = argmaxx1,x2,xRscore(x1, x2, xR) (1)
score(x1, x2, xR) = ?(x1)? ?(x2)? ?(xR) (2)
s.t. x?1 and x?R are compatible
& x?2 and x?R are compatible
Two phrases x?i and x?R are compatible if they share
the same object noun obji. We define the phrase-
level score function ?(?) as ?(xi) = fi using the
Google n-gram frequencies. The equation (2) can be
maximized using dynamic programming, by align-
ing the decision sequence as x?1 ? x?R ? x?2.
Once the best combination ? (x?1, x?2, x?R) is de-
termined, we fuse the phrases by replacing the word
obj1 in the phrase x?R with the corresponding phrase
x?1. Similarly, we replace the word obj2 in the phrase
x?R with the other corresponding phrase x?2. Because
the wildcard words ? [?] in ?4.1 allow only a lim-
ited set of function words and gerund, the resulting
phrase is highly likely to be grammatically correct.
Computational Efficiency One advantage of our
phrase fusion method is its efficiency. If we were
to attempt to re-order words with language mod-
els in a naive way, we would need to consider all
possible permutations of words ? an NP-hard prob-
lem (?3.1). However, our phrase fusion method is
clever in that it probes reordering only on selected
pairs of words, where reordering is likely to be use-
ful. In other words, our approach naturally ignores
most word pairs that do not require reordering and
has a time complexity of only O(K2n), where K is
the maximum number of candidate phrases of any
phrase type, and n is the number of phrase types in
each sentence. K can be kept as a small constant by
selecting K-best candidate phrases of each phrase
type. We set K = 10 in this paper.
5 Experimental Results
To construct the training corpus for language mod-
els, we crawled Wikipedia pages that describe our
object set. For evaluation, we use the UIUC PAS-
CAL sentence dataset3 which contains upto five
human-generated sentences that describing 1000 im-
ages. Note that all of the approaches presented in
3http://vision.cs.uiuc.edu/pascal-sentences/
224
Phrase fusion  shiny motorcycle nearby shiny motorcycle.   black women operating a shiny motorcycle.   bright boy on a shiny motorcycle.   girl showing pink on a shiny motorcycle.  
Local search the shiny motorbike or against the shiny motorbike. the shiny motorbike or by the black person. the shiny motorbike or by the shiny person. the shiny motorbike or by the pink person. 
Simple Decoding  the shiny motorbike or against the shiny motorbike. the shiny motorbike or by the black person. the shinny motorbike or by the shiny boy. the shiny motorbike or by the pink person. 
Template  This is a picture of two motorbikes, three persons, one building and one tree. The first shiny motorbike is against the second shiny motorbike, and by the first black person. The second shiny motorbike is by the first black person, and by the second shiny person, and by the third pink person. 
Image Recognition Output as Triples: < < shiny; black; rusty , motorbike >, against; by; in , < shiny; black; rusty , motorbike > > < < shiny; black; rusty , motorbike >, by; near; by , < black; shiny; rusty , person > > < < shiny; black; rusty , motorbike >, by; near; by , < pink; rusty; striped , person > > 
Figure 4: Comparison of image descriptions
Section 3 and 4 attempt to insert function words for
surface realization. In this work, we limit the choice
of function words to only those words that are likely
to be necessary in the final output.4 For instance, we
disallow function words such as ?who? or ?or?.
Before presenting evaluation results, we present
some samples of image descriptions generated by 4
different approaches in Figure 3 and 4. Notice that
only the PHRASE FUSION approach is able to in-
clude interesting and adequate verbs, such as ?eat-
ing? or ?looking? in Figure 3, and ?operating? in
Figure 4. Note that the choice of these action verbs
is based only on the co-occurrence statistics encoded
in n-grams, without relying on the vision compo-
nent that specializes in action recognition. These ex-
amples therefore demonstrate that world knowledge
implicitly encoded in natural language can help en-
hance image content recognition.
Automatic Evaluation: BLEU (Papineni et al,
2002) is a widely used metric for automatic eval-
uation of machine translation that measures the n-
gram precision of machine generated sentences with
respect to human generated sentences. Because our
task can be viewed as machine translation from im-
ages to text, BLEU (Papineni et al, 2002) may seem
4This limitation does not apply to TEMPLATE.
w/o w/ syn
LANGUAGE MODEL 0.094 0.106
TEMPLATE 0.087 0.096
LOCAL SEARCH 0.100 0.111
PHRASE FUSION (any best) 0.149 0.153
PHRASE FUSION (best w/ gerund) 0.146 0.149
Human 0.500 0.510
Table 2: Automatic Evaluation: BLEU measured at 1
Creativ. Fluency Relevan.
LANGUAGE MODEL 2.12 1.96 2.09
TEMPLATE 2.04 1.7 1.96
LOCAL SEARCH 2.21 1.96 2.04
PHRASE FUSION 1.86 1.97 2.11
Table 3: Human Evaluation: the scores range over 1 to 3,
where 1 is very good, 2 is ok, 3 is bad.
like a reasonable choice. However, there is larger
inherent variability in generating sentences from im-
ages than translating a sentence from one language
to another. In fact two people viewing the same pic-
ture may produce quite different descriptions. This
means BLEU could penalize many correctly gener-
ated sentences, and be poorly correlated with human
judgment of quality. Nevertheless we report BLEU
scores in absence of any other automatic evaluation
method that serves our needs perfectly.
The results are shown in Table 2 ? first column
shows BLEU score considering exact matches, sec-
ond column shows BLEU with full credit for syn-
onyms. To give a sense of upper bound and to see
some limitations of the BLEU score, we also com-
pute the BLEU score between human-generated sen-
tences by computing the BLEU score of the first hu-
man sentence with respect to the others.
There is one important factor to consider when in-
terpreting Table 2. The four approaches explored
in this paper are purposefully prolific writers in that
they generate many more sentences than the num-
ber of sentences in the image descriptions written by
humans (available in the UIUC PASCAL dataset).
In this work, we do not perform sentence selection
to reduce the number of sentences in the final out-
put. Rather, we focus on the quality of each gener-
ated sentence. The consequence of producing many
225
Way rusty the golden cow 
Golden cow in the golden sky 
Tree snowing black train 
Black train under the tree Rusty girl sitting at a white table White table in the clear sky 
Rusty girl living in the clear sky 
Blue path up in the clear sky 
Blue path to colored fishing boat 
Blue path up in the clear 
morning sky 
rusty chair for rusty dog.  
rusty dog under the rusty chair.  
rusty dog sitting in a rusty chair. 
Gray cat from a burning gray 
building 
Gray building with a gray cat. 
Gray building in the white sky 
 
Shaggy dog knotting hairy men 
Pink flowering plant the hairy dog 
Pink dog training shaggy dog 
Shaggy dog relaxing on a colored sofa 
 
black women hanging 
from a black tree.  
colored man in the tree. 
1 2 3 4 
5 
6 7 
8 
Figure 5: Sample image descriptions using PHRASE FUSION: some of the unexpected or poetic descriptions are
highlighted in boldface, and some of the interesting incorrect descriptions are underlined.
more sentences in our output is overall lower BLEU
scores, because BLEU precision penalizes spurious
repetitions of the same word, which necessarily oc-
curs when generating more sentences. This is not an
issue for comparing different approaches however,
as we generate the same number of sentences for
each method.
From Table 2, we find that our final approach ?
PHRASE FUSION based on web-scale n-grams per-
forms the best. Notice that there are two different
evaluations for PHRASE FUSION: the first one is
evaluated for the best combination of phrases (Equa-
tion (1)), while the second one is evaluated for the
best combination of phrases that contained at least
one gerund.
Human Evaluation: As mentioned earlier, BLEU
score has some drawbacks including obliviousness
to correctness of grammar and inability to evaluate
the creativity of a composition. To directly quantify
these aspects that could not be addressed by BLEU,
we perform human judgments on 120 instances for
the four proposed methods. Evaluators do not have
any computer vision or natural language generation
background.
We consider the following three aspects to eval-
uate the our image descriptions: creativity, fluency,
and relevance. For simplicity, human evaluators as-
sign one set of scores for each aspect per image. The
scores range from 1 to 3, where 1 is very good, 2 is
ok, and 3 is bad.5 The definition and guideline for
each aspect is:
[Creativity] How creative is the generated sen-
tence?
1 There is creativity either based on unexpected
words (in particular, verbs), or describing
things in a poetic way.
2 There is minor creativity based on re-ordering
words that appeared in the triple
3 None. Looks like a robot talking.
[Fluency] How grammatically correct is the gener-
ated sentence?
1 Mostly perfect English phrase or sentence.
2 There are some errors, but mostly comprehen-
sible.
3 Terrible.
[Relevance] How relevant is the generated descrip-
tion to the given image?
1 Very relevant.
2 Reasonably relevant.
3 Totally off.
5In our pilot study, human annotations on 160 instances
given by two evaluators were identical on 61% of the instances,
and close (difference ? 1) on 92%.
226
Table 3 shows the human evaluation results. In
terms of creativity, PHRASE FUSION achieves the
best score as expected. In terms of fluency and
relevance however, TEMPLATE achieves the best
scores, while PHRASE FUSION performs the second
best. Remember that TEMPLATE is based on hand-
engineered rules with discourse constraints, which
seems to appeal to evaluators more. It would be
straightforward to combine PHRASE FUSION with
TEMPLATE to improve the output of PHRASE FU-
SION with hand-engineered rules. However, our
goal in this paper is to investigate statistically moti-
vated approaches for generating image descriptions
that can address inherent limitations of hand-written
rules discussed in ?3.3.
Notice that the relevance score of TEMPLATE is
better than that of LANGUAGE MODEL, even though
both approaches generate descriptions that consist of
an almost identical set of words. This is presum-
ably because the output from LANGUAGE MODEL
contains grammatically incorrect sentences that are
not comprehendable enough to the evaluators. The
relevance score of PHRASE FUSION is also slightly
worse than that of TEMPLATE, presumably because
PHRASE FUSION often generates poetic or creative
expressions, as shown in Figure 5, which can be con-
sidered a deviation from the image content.
Error Analysis There are different sources of er-
rors. Some errors are due to mistakes in the origi-
nal visual recognition input. For example, in the 3rd
image in Figure 5, the color of sky is predicted to
be ?golden?. In the 4th image, the wall behind the
table is recognized as ?sky?, and in the 6th image,
the parrots are recognized as ?person?.
Other errors are from surface realization. For in-
stance, in the 8th image, PHRASE FUSION selects
the preposition ?under?, presumably because dogs
are typically under the chair rather than on the chair
according to Google n-gram statistics. In the 5th
image, an unexpected word ?burning? is selected to
make the resulting output idiosyncratic. Word sense
disambiguation sometimes causes a problem in sur-
face realization as well. In the 3rd image, the word
?way? is chosen to represent ?path? or ?street? by
the image recognizer. However, a different sense of
way ? ?very? ? is being used in the final output.
6 Related Work
There has been relatively limited work on automat-
ically generating natural language image descrip-
tions. Most work related to our study is discussed
in ?1, hence we highlight only those that are clos-
est to our work here. Yao et al (2010) present a
comprehensive system that generates image descrip-
tions using Head-driven phrase structure (HPSG)
grammar, which requires carefully written domain-
specific lexicalized grammar rules, and also de-
mands a very specific and complex meaning rep-
resentation scheme from the image processing. In
contrast, our approach handles images in the open-
domain more naturally using much simpler tech-
niques.
We use similar vision based inputs ? object detec-
tors, modifier classifiers, and prepositional functions
? to some very recent work on generating simple de-
scriptions for images (Kulkarni et al, 2011), but fo-
cus on improving the sentence generation method-
ology and produce descriptions that are more true
to human generated descriptions. Note that the
BLEU scores reported in their work of Kulkarni et
al. (2011) are not directly comparable to ours, as the
scale of the scores differs depending on the number
of sentences generated per image.
7 Conclusion
In this paper, we presented a novel surface realiza-
tion technique based on web-scale n-gram data to
automatically generate image description. Despite
its simplicity, our method is highly effective in gen-
erating mostly appealing and presentable language,
while permitting creative writing at times. We con-
clude from our study that it is viable to generate
simple textual descriptions that are germane to the
specific image content while also sometimes pro-
ducing almost poetic natural language. Furthermore,
we demonstrate that world knowledge implicitly en-
coded in natural language can help enhance image
content recognition.
Acknowledgments
This work is supported in part by NSF Faculty Early
Career Development (CAREER) Award #1054133.
227
References
A. Aker and R. Gaizauskas. 2010. Generating image
descriptions using dependency relational patterns. In
ACL.
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. JMLR, 3:1107?1135.
Songsak Channarukul, Susan W. McRoy, and Syed S.
Ali. 2003. Doghed: a template-based generator for
multimodal dialog systems targeting heterogeneous
devices. In NAACL.
Michael Chisholm and Prasad Tadepalli. 2002. Learning
decision rules by randomized iterative local search. In
ICML, pages 75?82.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-
Fei. 2009. ImageNet: A Large-Scale Hierarchical Im-
age Database. In CVPR.
A. Farhadi, I. Endres, D. Hoiem, and D. A. Forsyth.
2009. Describing objects by their attributes. In CVPR.
A. Farhadi, M Hejrati, A. Sadeghi, P. Young,
C. Rashtchian, J. Hockenmaier, and D. A. Forsyth.
2010. Every picture tells a story: generating sentences
for images. In ECCV.
P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-
manan. 2010. Object detection with discriminatively
trained part based models. tPAMI, Sept.
Y. Feng and M. Lapata. 2010a. How many words is a
picture worth? automatic caption generation for news
images. In ACL.
Yansong Feng and Mirella Lapata. 2010b. Topic models
for image annotation and text illustration. In HLT.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C Berg, and Tamara L Berg.
2011. Babytalk: Understanding and generating simple
image descriptions. In CVPR.
Chee Wee Leong, Rada Mihalcea, and Samer Hassan.
2010. Text mining for automatic image tagging. In
COLING.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation.
Katerina Pastra, Horacio Saggion, and Yorick Wilks.
2003. Nlp for indexing and retrieval of captioned pho-
tographs. In EACL.
Michael White and Claire Cardie. 2002. Selecting sen-
tences for multidocument summaries using random-
ized local search. In ACL Workshop on Automatic
Summarization.
B.Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and
Song-Chun Zhu. 2010. I2t: Image parsing to text de-
scription. Proc. IEEE, 98(8).
Liang Zhou and Eduard Hovy. 2004. Template-
filtered headline summarization. In Text Summariza-
tion Branches Out: Pr ACL-04 Wkshp, July.
228
