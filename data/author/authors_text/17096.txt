Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1670?1681, Dublin, Ireland, August 23-29 2014.
Generating Supplementary Travel Guides from Social Media
Liu Yang
1,2
, Jing Jiang
2,?
, Lifu Huang
1,2
, Minghui Qiu
2
, Lizi Liao
2,3
1
Peking University / Beijing, China, 100871
2
Singapore Management University / Singapore, Singapore, 178902
3
Beijing Institute of Technology / Beijing, China, 100081
yang.liu@pku.edu.cn, jingjiang@smu.edu.sg
{warrior.fu, minghuiqiu, liaolizi.llz}@gmail.com
Abstract
In this paper we study how to summarize travel-related information in forum threads to gener-
ate supplementary travel guides. Such summaries presumably can provide additional and more
up-to-date information to tourists. Existing multi-document summarization methods have limita-
tions for this task because (1) they do not generate structured summaries but travel guides usually
follow a certain template, and (2) they do not put emphasis on named entities but travel guides
often recommend points of interest to travelers. To overcome these limitations, we propose to
use a latent variable model to align forum threads with the section structure of well-written travel
guides. The model also assigns section labels to named entities in forum threads. We then
propose to modify an ILP-based summarization method to generate section-specific summaries.
Evaluation on threads from Yahoo! Answers shows that our proposed method is able to generate
better summaries compared with a number of baselines based on ROUGE scores and coverage
of named entities.
1 Introduction
Online forums and community question answering (CQA) sites contain much useful information from
ordinary users, such as their personal experience, opinions, suggestions and recommendations. Extract-
ing and summarizing information from these rich information sources has a wide range of applications.
In this work, we study how to tap into user-generated content in forums such as Yahoo! Answers to
generate supplementary city travel guides. Travel guides published by well-known publishers such as
Lonely Planet are written by a small number of authors based on their travel experience. Presumably
if we could summarize the large amount of information given by ordinary users about a city, such a
summary could supplement the official travel guide and cover more up-to-date information.
However, social media content is diverse and noisy because it is contributed by many different au-
thors. Directly applying existing multi-document summarization methods to forum and CQA threads
may not produce good travel guides for the following reasons: (1) Summaries produced by standard
summarization methods are not structured, but travel guides usually follow a template structure. (2)
Travel guides put much emphasis on points of interest, which are usually location entities, but standard
text summarization methods are not entity-oriented.
To illustrate our points, in Table 1 we show (i) the overall structure of a travel guide for Sydney from
Lonely Planet, (ii) an excerpt from a summary generated by a state-of-the-art ILP-based summarization
method (Gillick and Favre, 2009) from a set of threads related to Sydney, and (iii) excerpts of a structured
summary generated by our proposed method. The comparison shows that the summary generated by
the standard ILP method mixes information on different topics together and does not mention many
* Corresponding author.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1670
Travel Guide from Lonely Planet (http://www.lonelyplanet.com/australia/sydney/)
Restaurants:
Sepia:There?s nothing washed out or brown-tinged about Sepia?s food: Martin Benn?s picture-perfect creations are presented in . . .
Icebergs Dining Room: Poised above the famous Icebergs swimming pool, Icebergs views sweep across the Bondi Beach arc to . . .
Shopping:
Strand Arcade: Constructed in 1891, the Strand rivals the QVB in the ornateness stakes. Three floors of designer fashions . . .
Westfield Sydney: The city?s newest shopping mall is a bafflingly large complex gobbling up Sydney Tower and a fair chunk of . . .
Transport:
Sydney Airport: Sydneys Kingsford Smith Airport , 10km south of the city centre, is Australias busiest airport, handling flights . . .
Water Taxis Combined:Fares based on up to four passengers; add $10 per person for additional passengers. Sample fares . . .
Yahoo! Answers Summary Generated by Standard ILP Method Yahoo! Answers Summary Generated by Our Method
It ?s not too far from Sydney . Sydney is the most expensive
place in Australia . They are a little lame ... Then you can go to
Darling Harbour, a beautiful habour which is a 10-minute walk
from town hall station . Make sure , if you are up to it to do the
bridge climb , this is a real treat . There are lots of interesting
things to see and do in and around Sydney . The suburbs-much
cheaper than the CBD. It was in the basement of a big shopping
mall . The only way to do that is to drive . Got to walk on top of
the Sydney harbour bridge and go up centre point tower ! Walk
around the street and see the beach . I would like to stay at a nice
hotel . My friend and I are wanting to take a trip to Sydney for
the summer . But you ?ll need to get there by taxi . Sydney is so
pretty, so you should be able to find stuff to do . And they have
many facilities . Good luck and have fun . Public transport is not
very good . Depending on what you ?re in Sydney to do it ?s hard
to say . . .
Restaurants:
Go to the two major restaurant areas close to the city Dar-
linghurst , along Oxford Street , and Newtown , along King
Street . Chinatown which is off George St. in the city look up
Dixon st. is a great place to get a cheap Chinese meal . . .
Shopping:
Queen Victoria Building and Pitt St Mall , World Square and
the Strand are good ideas to check out . Hair driers you can get
in many places , but the main places would be the department
stores such as Target , Big W , K-Mart , Myer, David Jones . . .
Transport:
The CBD is about 15 minutes by train from the airport and there
is a station at Circular Quay , right on the Harbour with access
to the bridge and the Opera House . You can catch an intercity
train with Cityrail from just about anywhere in Sydney . . .
Table 1: Comparison of different travel guides about Sydney. Top: excerpts from Lonely Planet. Bottom left: excerpt from
a summary generated by standard ILP. Bottom right: excerpts from summary generated by our method. Named entities are
highlighted in bold font.
interesting places to visit. The summary by our proposed method, in contrast, organizes the information
into sections and has a high coverage of places a tourist can visit.
To generate the kind of summaries as shown in the bottom right of Table 1, we propose to first leverage
the section structure of well-written travel guides and use a latent variable model to align forum threads
with the different sections from these travel guides. Moreover, observing that points of interest are orga-
nized by sections in these travel guides, we also identify location names from user-generated content and
try to uncover their underlying section labels. We then treat the remaining problem as a multi-document
summarization task. We modify an Integer Linear Programming (ILP)-based extractive summarization
framework (Gillick and Favre, 2009) to select sentences from forum threads to generate section-specific
summaries, where we specifically emphasize the inclusion of potential points of interest for each sec-
tion. Experiments using threads from Yahoo! Answers show that our proposed method generates better
summaries than a number of baselines in terms of ROUGE scores and coverage of named entities.
Our work makes the following contributions. First, we study a new problem of summarizing multiple
forum threads to generate city travel guides based on known template structure from well-written travel
guides. Second, we propose a principled approach based on latent variable models and Integer Linear
Programming. Third, we evaluate our method using real forum threads and human generated model
summaries, and the results are positive.
2 Overview of Our Method
Our task is to summarize travel-related information from forum threads for potential tourists. In order
to inject some structure into the generated summaries, we assume that we have a set of I well-written
travel guides that correspond to I different cities and have the same structure. We refer to these travel
guides as official travel guides. Each official travel guide consists of a fixed set of S sections such as
restaurants and shopping, and this section structure will be used to organize our generated summaries.
We further assume that each section of an official travel guide consists of a list of points of interest, each
with a name and a short description, as illustrated in Figure 1. We believe that this is a fairly common
structure followed by many if not all travel guides.
Given a target city, we assume that we can collect a set of threads about this city from travel-related
forums. In this paper we use threads from Yahoo! Answers, but our solution does not use any CQA
properties of the threads, so threads from other general forums can also be used. Our goal is to generate
a text summary with S sections from these threads, where each section has a length limit.
1671
As we have mentioned, we treat the problem as a multi-document summarization task. However,
different from standard text summarization, our generated summaries should contain S sections. To
achieve this goal, we first select a set of relevant threads for each section and then perform section-
specific summarization from the selected threads.
Thread selection: To select relevant threads given a section, a naive solution is to rank the threads based
on their relevance to the section, where relevance can be measured by, for example, cosine similarity
between a thread and all the text in the given travel guides belonging to the section. But we observe that
the language used in forum threads could be very different from that in the official travel guides, making
it hard to measure relevance purely based on lexical overlap. For example, in the entertainment section,
forum threads may contain words such as ?djs,? ?Xmas,? ?b?day? and ?anni.?, but these words do not
occur in the official travel guides. To overcome this difficulty, we propose to use a latent variable model
that jointly models official travel guides and forum threads. We treat the S sections as S latent factors
that govern the generation of the forum threads. With the latent factors observed in the official travel
guides, we receive some supervision; and yet by jointly modeling both the official travel guides and the
forum threads, we allow the latent factors to adapt to the lexical variations in user-generated content. In
the end, the learned latent factors can help us align forum threads with the sections and subsequently
select the most relevant ones for each section.
Section-specific summarization: Given the selected relevant threads for a section, we adopt an ILP-
based extractive summarization framework that has been shown to be effective (Gillick and Favre, 2009).
We modify the objective function in this framework to consider two factors: (1) Since not every sentence
in the selected threads is highly relevant to the section, we want to give preference to those more relevant
sentences in the objective function, where relevance can be measured using word distributions learned
by the latent variable model. (2) Since travel guides are expected to recommend points of interest to
readers, we try to maximize the coverage of section-specific location entities in the objective function.
3 Joint City Section Model
3.1 Model
In this section we present our Joint City Section Model (JCSM), which links official travel guides and
forum threads. The model is a typical extension of LDA, where a number of latent topics (i.e. latent
factors) are assumed to have generated the observed text. First of all, for each pre-defined section there
is a latent topic. These explain words such as ?food? and ?menu? for restaurants and ?store? and ?mall?
for shopping. In addition, in both travel guides and forum threads, some words are more related to the
city being discussed than any specific section. For example, when New York City is being discussed,
words such as ?NYC? and ?Manhattan? may frequently show up in any section. We therefore further
assume that for each city there is a city-specific topic. A switch variable is used to determine whether a
word comes from a city-specific or section-specific topic.
A special design of our model that differs from many existing LDA extensions is the treatment of
named entities. We first use a named entity recognizer to identify potential names of locations from
forum threads. We assume that each of these entities belongs to a section, which is indicated by a latent
variable. We then assume that the section labels of the non-entity words in forum threads are dependent
on the section labels of these entities. By doing so, we emphasize the importance of associating potential
points of interest with sections, which will be useful when we generate summaries.
We now formally present JCSM. To simplify the model description, we assume that we work with
I cities, each of which has a given, well-written travel guide and a set of forum threads. Note that in
practice this model can be easily extended such that a target city with forum threads does not need to have
a given travel guide to begin with. Let ?
i
denote the word distribution for the city-specific latent topic
associated with city i. Let ?
s
denote the word distribution for the section-specific latent topic for section
s. Let d
i,s,n
denote the n-th word in the s-th section of the i-th city?s travel guide. Here 1 ? d
i,s,n
? V is
an index into the vocabulary with size V . Let x
i,s,n
be a switch variable associated with d
i,s,n
to indicate
whether this word is city-specific or section-specific. For the j-th forum thread related to the i-th city, we
assume there is a distribution over sections, denoted as ?
i,j
. For the l-th location entity in the k-th post
1672
of this thread, we assume a latent variable c
i,j,k,l
(1 ? c
i,j,k,l
? S) that indicates the section label of this
entity. Then for the m-th word in this post, we first use a switch variable y
i,j,k,m
to determine whether
the word is city-specific or section-specific. If it is section-specific, we then choose one of the entities in
the same post, denoted as z
i,j,k,m
, and its corresponding section label as the section for this word.
All the binary switch variables follow a global Bernoulli distribution parameterized by pi. There are
hyperparameters ?, ?, ?
?
and ? that define the prior distributions. The complete model is depicted in
Figure 1. The generative process of JCSM is also described as follows.
S
N M L
K
J I
 d   w c ?
x y z
?
Figure 1: The plate notation of the Joint City Section Model (JCSM). Dashed variables will be integrated out in Gibbs sampling.
For clarity, the Dirichlet and Beta priors are omitted. The arrow pointing to z indicates that z is drawn from a uniform
distribution over the integers from 1 to L.
? For each city i, (i = 1, 2, ? ? ? , I), draw a city-specific word distribution ?
i
? Dir(?
?
)
? For each section s, (s = 1, 2, ? ? ? , S), draw a section-specific word distribution ?
s
? Dir(?)
? Draw a switch distribution pi ? Beta(?)
? For each city i (i = 1, 2, ? ? ? , I)
? For each section s (s = 1, 2, ? ? ? , S)
? For the n-th word in the given travel guide
- Draw x
i,s,n
? Bernoulli(pi)
- If x
i,s,n
= 1, draw d
i,s,n
? Multi(?
s
); otherwise, draw d
i,s,n
? Multi(?
i
).
? For the j-th thread
? Draw a thread specific section distribution ?
j
? Dir(?)
? For the k-th post
- For the l-th entity, draw c
i,j,k,l
? Multi(?
j
)
- For the m-th word, draw y
i,j,k,m
? Bernoulli(pi). If y
i,j,k,m
= 1, draw z
i,j,k,m
? Uniform(1, ? ? ? , L
i,j,k
)
and then draw w
i,j,k,m
? Multi(?
c
i,j,k,z
i,j,k,m
); otherwise, draw w
i,j,k,m
? Multi(?
i
).
3.2 Inference
We use collapsed Gibbs sampling to estimate the parameters in the model. The problem is to compute
the Gibbs update rules for sampling x
i,s,n
, c
i,j,k,l
, z
i,j,k,m
, y
i,j,k,m
.
Sample entity topic c
i,j,k,l
Let b denote {i, j, k, l} and u denote {i, j, k}. We can derive the Gibbs update rule for sampling entity
topic c
i,j,k,l
as follows:
p(c
b
= s|C
?
b
,W,D,X,Y,Z) =
n
s
i,j,?
b
+ ?
?
S
s
?
=1
n
s
?
i,j,?
b
+ S?
?
?
V
w=1
?
n
w
u,y=1,z=l
i
?
=1
(n
w
y=1,z=l,?
u
+ ? + i
?
? 1)
?
n
w
y=1,z=l,u
j
?
=1
(
?
V
w=1
n
w
y=1,z=l,?
u
+ V ? + j
?
? 1)
,
where n
s
i,j,?
b
denotes the number of entities whose topic assignments are s in thread {i, j} without
consideration of entity {i, j, k, l}. n
w
u,y=1,z=l
denotes the number of times term w occurs in the post
{i, j, k} with the constraint that y = 1 and z = l. n
w
y=1,z=l,?
u
is the number of times term w occurs in
all posts except the post {i, j, k} with the constraint that y = 1 and z = l.
Sample switch label x
i,s,n
We can derive the Gibbs update rule for sampling x
i,s,n
in a similar way. Note that the sampling of
x
i,s,n
is in travel guide word level. Let g denote{i, s, n}, the Gibbs update rule for sampling x
i,s,n
is as
follows:
1673
p(x
g
= 0|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=0
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=0,i,?
g
+ ?
?
?
V
w=1
n
w
x=0,i,?
g
+ V ?
?
p(x
g
= 1|C,W,D
?
g
,X
?
g
,Y,Z) =
n
x=1
?
g
+ ?
?
1
x=0
n
x
?
g
+ 2?
?
n
w
g
x=1,s,?
g
+ ?
?
V
w=1
n
w
x=1,s,?
g
+ V ?
Sample post word topic z
i,j,k,m
and switch label y
i,j,k,m
For words in the thread posts, We can derive the Gibbs update rule for sampling post word topic z
i,j,k,m
and switch label y
i,j,k,m
. Note that the sampling of z
i,j,k,m
and y
i,j,k,m
is in post word level. Let f
denote{i, j, k,m}. The Gibbs update rule for sampling z
i,j,k,m
and y
i,j,k,m
is as follows:
p(z
f
= s|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
?
1
L
i,j,k
p(y
f
= 0|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=0
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=0,i,?
f
+ ?
?
?
V
w=1
n
w
y=0,i,?
f
+ V ?
?
p(y
f
= 1|C,W
?
f
,D,X,Y
?
f
,Z
?
f
) =
n
y=1
?
f
+ ?
?
1
y=0
n
y
?
f
+ 2?
?
n
w
f
y=1,s
?
,?
f
+ ?
?
V
w=1
n
w
y=1,s
?
,?
f
+ V ?
where s
?
= c
i,j,k,l
which is the topic index of the associated entity of this word.
Parameter estimation
After Gibbs Sampling, we can make the following parameter estimation:
?
i,j,s
=
n
s
i,j
+ ?
?
S
s
?
=1
n
s
?
i,j
+ S?
. thread-section distribution.
?
s,w
=
n
w
s,y=1
+ ?
?
V
w
?
=1
n
w
?
s,y=1
+ V ?
. section-word distribution.
?
i,w
=
n
w
i,y=0
+ ?
?
?
V
w
?
=1
n
w
?
i,y=0
+ V ?
?
. city-word distribution.
pi
y
=
n
y
(.)
+ ?
?
1
y
?
=0
n
y
?
(.)
+ 2?
. switch distribution.
4 Generating Section-specific Summaries
With the JCSM model presented in the last section, we can learn a word distribution for each section,
which can help us find more relevant content for the section. For each section, we rank the forum
threads by how likely the words inside a thread is generated from the corresponding section-specific word
distribution. We select the top-K threads for each section to perform section-specific summarization.
Extractive summarization has been well studied and many algorithms have been proposed. We choose
to build our solution on top of an ILP-based framework proposed by Gillick and Favre (2009), partly
because our experiments comparing this ILP framework and other existing methods show its advantage
on our data sets (see Section 5). Below we first briefly review this ILP-based summarization framework
and then present our proposed improvements.
The idea behind the ILP framework by Gillick and Favre (2009) is to maximize the coverage of so-
called ?concepts? from the original corpus in the generated summary. In practice, bigrams are used as
concepts. Specifically, let us use i to index all the concepts from the original corpus. Let w
i
denote
the weight of the i-th concept computed based on its frequency and b
i
? {0, 1} denote the absence or
1674
presence of the concept. The framework aims to maximize
?
i
w
i
b
i
, i.e. the total weighted coverage of
the concepts, subject to the following constraints:
?
j
l
j
s
j
? L, (l
j
is the length of the j-th sentence in terms of words, and L is the length limit of the summary.)
?i, j : s
j
o
i,j
? b
i
, (s
j
? {0, 1} denotes the absence or presence of the j-th sentence.)
?i :
?
j
s
j
o
i,j
? b
i
. (o
i,j
? {0, 1} denotes whether concept i occurs in sentence j.)
Although this framework works well for standard summarization, our task is different. We propose the
following changes to this framework:
Favoring relevant sentences: Recall that although we select presumably the most relevant threads for
each section, we cannot guarantee that each sentence in these threads is related to the section. For
example, we observe that the things-to-do section is often mixed with content from restaurants, sights,
transport and entertainment sections. Also, some sentences are less relevant to the target city than
others. In order to select the more relevant sentences in the summary, we propose to add the second term
in Eqn. 1 below. Here j is used to index all the candidate sentences and u
j
is a weight for sentence j
based on its relevance.
We measure relevance with respect to both the city and the section. Let LL(j, ?) denote the log like-
lihood of generating sentence j from the section-specific topic ? and LL(j, ?) denote the log likelihood
of generating sentence j from the city-specific topic ?. We define u
j
as follows:
u
j
? exp (?LL(j, ?) + (1? ?)LL(j, ?)) .
u
j
are then normalized to be between 0 and 1. Note that here ? is a manually defined parameter used to
control the tradeoff between city-specific relevance and section-specific relevance. As we will show in
Section 5, both relevance factors turn out to be useful.
Covering section-specific points of interest: We hypothesize that a good summary travel guide should
mention potential points of interest to the reader. To this end, the last term in Eqn. 1 is added. Specifically,
k is an index for unique location names we find that have been labeled as belonging to section s according
to the JCSM model. e
k
? {0, 1} denotes whether the k-th entity is present in the selected sentences, and
v
k
denotes the weight for this entity based on its frequency.
Eventually, the summarization task is formulated as the following optimization problem:
Maximize: ?
1
?
i
w
i
b
i
+ ?
2
?
j
u
j
s
j
+ (1? ?
1
? ?
2
)
?
k
v
k
e
k
(1)
Subject to:
?
j
l
j
s
j
? L,
?i :
?
j
s
j
o
i,j
? b
i
, ?i, j : s
j
o
i,j
? b
i
,
?j :
?
k
s
j
p
j,k
? e
k
, ?j, k : s
j
p
j,k
? e
k
.
Here o
i,j
denotes whether concept i occurs in sentence j, and p
j,k
denotes whether entity k occurs in
sentence j. For the weights w
i
and v
k
, we normalize them using the total occurrences of bigrams/entities
to ensure their values are between 0 and 1. We solve the above optimization problem using the IBM
ILOG CPLEX Optimizer
1
.
5 Experiments
5.1 Data and Experimental Setup
We use real data from Yahoo! Answers and Lonely Planet for evaluation. We first crawl the travel guides
for 10 cities from Lonely Planet, where each travel guide has 8 sections. We then crawl the top 60000
Q&A threads ranked by number of posts related to these 10 cities (6000 for each city) from Yahoo!
Answers under the ?travel? category where all questions have been grouped by cities. We filter out
trivial factoid questions using features used by Tomasoni and Huang (2010). We then use the Stanford
1
http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/
1675
Method Singapore Sydney New York City Los Angeles Overall Average
R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4
Random 0.4091 0.1046 0.1576 0.4496 0.1100 0.1925 0.4442 0.1192 0.1858 0.4154 0.1130 0.1693 0.4309 0.1115 0.1771
Centroid 0.4029 0.0993 0.1484 0.4228 0.1100 0.1764 0.4235 0.1192 0.1722 0.3763 0.0787 0.1386 0.4133 0.1077 0.1640
LexRank 0.4396 0.1451 0.1891 0.4406 0.1296 0.1955 0.4304 0.1397 0.1859 0.4032 0.0992 0.1661 0.4350 0.1331 0.1894
DivRank 0.4534 0.1504 0.1888 0.4473 0.1161 0.1925 0.4391 0.1167 0.1804 0.4275 0.1180 0.1733 0.4487 0.1317 0.1888
GMDS 0.3918 0.0890 0.1415 0.4339 0.1066 0.1784 0.4064 0.0845 0.1576 0.3846 0.0809 0.1413 0.4045 0.0916 0.1553
ILP-BL 0.4635 0.1650 0.2000 0.4948 0.1731 0.2333 0.4691 0.1613 0.2073 0.4545 0.1445 0.1981 0.4755 0.1654 0.2136
Our Method 0.4723 0.1655 0.2035 0.5078 0.1787 0.2397 0.4716 0.1713 0.2086 0.4543 0.1565 0.1945 0.4804
?
0.1715
?
0.2144
?
Table 2: Comparison of the summarization results.
?
means the result is better than others except ILP-BL in the same column
at 5% significance level measured by Wilcoxon signed rank test. Note that only the average scores are tested for statistical
significance based on the 32 summarization tasks in total.
NER tool to recognize named entities in these threads. Since we notice that sometimes entities tagged as
PER are also possible points of interest, we include all entities of LOC, ORG and PER types. In order
to use higher quality threads for evaluation, for each city we pick the top 600 threads that have the most
overlapping points of interest with the Lonely Planet travel guides. On average, each thread contains 5.0
posts and 618.1 words. These 600? 10 threads are used to train the JCSM model.
We need human generated model summaries for evaluation. Since it is too time consuming to ask
human annotators to look through 600 threads and generate structured summaries, we instead opt to
first retrieve the top 30 relevant threads per section per city based on the JCSM results and then ask
human annotators to summarize these 30 threads to generate a section-specific summary. Our summa-
rization method as well as the baselines are also applied to these 30 threads per section per city for fair
comparison. We randomly select 4 cities for human annotation, giving us 8 ? 4 = 32 section-specific
summarization tasks. For each task, we ask four annotators to read all 30 threads and write a summary
as model summaries in our experiments
2
.
We use the following baseline algorithms for comparison: (1) Random, which randomly picks sum-
mary sentences. (2) Centroid (Radev et al., 2004), which selects sentences according to several features
like tfidf, cluster centroid and position. (3) LexRank (Erkan and Radev, 2004b)., which applies a graph-
based algorithm . (4) DivRank (Mei et al., 2010), which employs a time-variant random walk to enhance
diversity. (5) GMDS (Wan, 2008), which incorporates the document-level information and the sentence-
to-document relationship into the ranking process. (6) ILP-BL, which is the method proposed by Gillick
and Favre (2009).
We empirically set Dirichlet hyperparameters ? = 0.5, ? = 0.01, ? = 0.01, ?
?
= 0.1. We run JCSM
with 400 iterations of Gibbs sampling. For the weight parameters in the ILP model, we empirically set
?
1
= 0.7, ?
2
= 0.1, ? = 0.7 after we conduct multiple experiments to determine the best values of them
from 0.1 to 0.9.
5.2 Summarization Results
To compare the summaries generated by our method with those generated by the baselines, we first
compute their ROUGE scores against the human generated model summaries. ROUGE scores have
been widely used for evaluation of summarization systems (Lin and Hovy, 2003). We use the ROUGE
toolkit
3
, which provides multiple kinds of ROUGE metrics including ROUGE-N, ROUGE-L, ROUGE-
W and ROUGE-SU4. In the experiment results we report three ROUGE F-measure scores, namely,
ROUGE-1, ROUGE-2 and ROUGE-SU4. The higher the ROUGE scores, the better a summary is.
In Table 2 we show the summarization results of our method (with the optimal parameter setting) and
the baseline methods. For each city, the scores we show are averaged over the 8 sections. The overall
average scores on the right hand side are averaged over the 4 cities. We have the following findings from
the table: (1) Compared with the other baselines, the ILP-based baseline clearly shows its advantage,
justifying our our design choice of adopting an ILP-based framework as the basis of our method. (2)
Our method performs slightly better than ILP-BL based on the overall scores, but the difference is not
statistically significant.
2
The summary dataset can be found at https://sites.google.com/site/liuyang198908/code-data.
3
http://www.isi.edu/licensed-sw/see/rouge/
1676
section Singapore Sydney New York City Los Angeles
ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method ILP-BL Our Method
restaurants 0.3750 0.5417 0.5714 0.7143 0.2500 0.3750 0.1053 0.2105
hotels 0.4091 0.4091 0.0000 0.5000 0.3636 0.5000 0.4500 0.5500
shopping 0.1429 0.5357 0.3750 0.3750 0.1905 0.1905 0.0455 0.1818
sights 0.5000 0.5789 0.3846 0.4615 0.3636 0.6364 0.1143 0.2571
entertainment 0.1304 0.2174 0.2500 0.7500 0.0909 0.2273 0.2500 0.4167
activities 0.4167 0.5833 0.2500 0.2500 0.1250 0.5000 0.2069 0.2759
transport 0.3889 0.5556 0.7500 0.7500 0.6000 0.8000 0.3158 0.7368
things-to-do 0.2105 0.2632 0.2500 0.5500 0.4583 0.5833 0.0000 0.2000
average 0.3217 0.4606 0.3539 0.5439 0.3052 0.4766 0.1860 0.3536
Table 3: Comparison of the recall of named entities of ILP-BL and our method.
Method Our Complete Model ?EC ?SR ?SecRel ?CityRel
R-1 0.4804 0.4520 0.4657 0.4672 0.4796
R-2 0.1715 0.1430 0.1669 0.1652 0.1685
RSU4 0.2144 0.1987 0.2028 0.2039 0.2120
Table 4: Summarization results of the degenerate versions of our method. ??? means removing this component from our
complete method. The table shows the average results over data sets of all cites.
Considering that an importance difference between our method and ILP-BL is our focus on points
of interest, we further compared ILP-BL and our method using a different metric. The objective is to
test the coverage of points of interest in our generated summaries versus the summaries generated by
ILP-BL. To this end, we first identify all the named entities in the model summaries using the Stanford
NER tool. We then check the percentage of these named entities covered in the generated summaries and
report these recall scores in Table 3. We can see that for majority of the 32 section-specific summaries,
our method clearly has a higher recall score than ILP-BL, showing that our method generates summaries
with more potential points of interest.
To further understand whether all the components of our improved ILP method have contributed to
the performance improvement, we compare our overall method with a few degenerate versions of our
method. In each degenerate version, we remove a single component of the objective function. The results
are shown in Table 4, where?EC removes the consideration of entity coverage (i.e. setting ?
1
+?
2
= 1),
?SR removes the consideration of sentence relevance (i.e. setting ?
2
= 0), ?SecRel removes only the
section-specific relevance of the sentences (i.e. setting ? = 0), and ?CityRel removes only the city-
specific relevance of the sentences (i.e. setting ? = 1). We can see that each degenerate version of our
method performs worse than the complete method, which shows that all components of the objective
function are useful. In particular, entity coverage and section-specific relevance seem to be the more
important components.
 0
 0.2
 0.4
 0.6
 0.8
 1 0
 0.2
 0.4
 0.6
 0.8
 1
 0.4
 0.5
 0.6
lambda2
lambda1
 0.445 0.45
 0.455 0.46
 0.465 0.47
 0.475 0.48
 0.485 0.49
 0.495
(a) R-1(?
1
, ?
2
)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
rho
R-1
R-2
RSU4
(b) ?
Figure 2: Summarization performance of our method by varying the value of the parameters ?
1
, ?
2
and ?.
1677
5.3 Analysis of Topic Words
We show some further analysis of our results. To begin with, we analyze the learning results of JCSM.
The top words in city-specific word distributions and section-specific word distributions learnt by JCSM
are presented in Table 5 and Table 6. Generally we observe clean top words for each city and each
section. For each city, city-specific words are those associated with the corresponding city. For example,
for Singapore, we see words such as ?s$? (Singapore dollars), ?sentosa? (an island resort in Singapore),
?orchard? (a boulevard that is the retail and entertainment hub of Singapore) and ?bugis? (a popular
shopping place). For New York City, we see ?square?, ?times? and ?manhattan?. For each section,
section-specific words are those words which frequently appear when people discuss about this section,
such as ?menu?, ?dishes? and ? seafood? for the restaurant section and ?train?, ?bus? and ?station? for
the transport section.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9 Topic 10
Singapore SFO Chicago Boston LA NYC Seattle Pairs London Sydney
singapore sf chicago boston beach york downtown paris london sydney
s$ san downtown end hollywood nyc seattle de tube harbour
centre francisco park north los park needle metro underground beach
food gate city downtown angeles central space eiffel central manly
shopping golden neighborhood fenway la square market french centre beaches
sentosa bay north bay downtown times rain la british house
road bart lake harvard drive manhattan place du palace opera
orchard union mile place california broadway pike tower thames quay
chinese wharf loop city miles city center des end australian
mrt muni ave college hills street waterfront rue kensington rocks
bugis square field subway long east area le station bridge
Table 5: Top city specific words discovered by JCSM.
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8
restaurants hotels shopping sights entertainment activities transport thingsToDo
food hotel shop museum bar visit train bar
restaurant rooms store city music park bus place
menu free stores park club tour station tour
dishes wi-fi shopping art place fun airport city
place walk shops building night city time food
bar located find built dance walk line art
chicken offers clothes world beer day car day
fish station wear place clubs time walk including
fresh features mall house crowd shopping minutes music
seafood tv place area bars museum hours restaurant
Table 6: Top section specific words discovered by JCSM.
5.4 Parameter Sensitivity Analysis
We further give parameter sensitivity analysis for our proposed method. We show how sensitive our
results are with respect to the parameters ?
1
, ?
2
and ?. We choose the Sydney data set to perform
parameter sensitivity analysis. In Figure 2(a), we show how ROUGE-1 varies with respect to ?
1
and ?
2
.
We can see that the performance fluctuates within a limited range as we vary ?
1
and ?
2
. We find the
trend for ROUGE-2 and ROUGE-SU4 is similar so we leave out the figures for them. In Figure 2(b) we
see that the performance is pretty stable as we vary ?.
5.5 Sample Output and Case Study
Finally, we show a sample travel guide our method generates for Sydney in Table 7. We can see that first
of all the sentences selected by our method have high relevance to the corresponding sections. Second,
through observation we find that humans tend to select sentences containing more points of interest as
summary. Our summary sentences contain many points of interest as highlighted, showing the advantage
of our method.
1678
Sample Summary Sentences Generated from Yahoo! Answers by Our Method for Sydney
Hotel
Sorry can not recommend you a hotels as I have no idea of pricing , but if you want a nice area , check hotels in Bondi and Manly Beaches .
As for the Acer Arena , that is in the Homebush Olympic Park and you can choose to live in either Parramatta or the city .
You need to live in one of the surrounding residential suburbs , close to a train line . Try Alexandria , Newtown , Surry hills for inner suburbs . . . .
Sights
You can walk around the harbor area to the Opera House and you can see the beautiful Harbor Bridge .
All this is apart from the Opera House and the Botanical Gardens . Visit the Custom House Circular Quay and see a model of Sydney. You
must also do a day trip to the Blue Mountains . Harbour Wedding is one of the major attraction in Sydney . . . .
Entertainment
George Street has a number of bars . All the bars around the harbour are really good day and night . If you want to stay in a hotel where there is
entertainment at night , you could look at Woolloomooloo , Darlinghurst , Surry Hills , Kings Cross or Potts Point . Newtown is good for bars .
Get them to see a theatre show or something at the Opera House . . . .
Things-to-do
If you are going out for the day , starting with a walk to the city will be most enjoyable . Take a public ferry from Circular Quay to Darling
Harbour , about 15 minutes across the harbour and under the bridge , when you get to Darling Harbour go and see the Chinese Gardens . There
are lots of interesting things to see and do in and around Sydney . . . .
Activities
They have good markets at the weekend and great views of the Opera House . The Opera House is free to have a look at , if you like art then walk
through the Botanical Gardens and go and see the art gallery . If you ?re feeling brave , you can do a Harbour Bridge walk , though I think it may
be a little pricey . . . .
Table 7: Excerpts from the summary generated from Yahoo! Answers by our method for Sydney. We show summaries for the
5 sections other than the 3 sections shown in Table1. Named entities are highlighted in bold font.
6 Related Work
Multi-document summarization is a process to generate a text summary by reducing documents in size
while retaining the main points of the original documents. It has been extensively studied in the NLP
community, with most efforts on extractive summarization. Our work is also based on extractive sum-
marization. Extractive summarization essentially selects a set of sentences from the original documents
to form a summary.
To select sentences, different features and ranking strategies have been studied. Early work focuses
on finding good features to select summary sentences. Radev et al. (2004) proposed a centroid-based
summarizer which combines several pre-defined features like tfidf, cluster centroid and position to score
sentences. Lin and Hovy (2002) built the NeATS multi-document summarization system using term fre-
quency, sentence position, stigma words and simplified Maximal Marginal Relecvance (MMR). Nenkova
et al. (2006) proved that high-frequency words were significant in reflecting the focus of documents.
Ouyang et al. (2010) studied the influence of different word positions in summarization. Later, graph-
based ranking algorithms have been successfully applied to summarization. LexPageRank (Erkan and
Radev, 2004a) is a representative one based on the PageRank algorithm (Page et al., 1999). Later exten-
sions include ToPageRank (Pei et al., 2012), which incorporates topic information into the propagation
mechanism, the manifold-ranking based method for topic-focused summarization (Wan et al., 2007) and
DivRank (Mei et al., 2010), which introduces a time-variant matrix into a reinforced random walk to
balance prestige and diversity.
More recently, Integer Linear Programming (ILP) based framework was introduced as a global infer-
ence algorithm for multi-document summarization by McDonald (2007), which considers information
and redundancy at the sentence level. Gillick and Favre (2009) studied information and redundancy at a
sub-sentence, ?concept? level, modeling the value of a summary as a function of the concepts it covers.
In our work we also model concept level coverage of the summaries. Li et al. (2013) proposed a re-
gression model to estimate the frequency of bigrams in the reference summary and analyzed the impact
of bigram selection, weight estimation and ILP setup. Haghighi and Vanderwende (2009) constructed
a sequence of generative probabilistic models for multi-document summarization, exhibiting ROUGE
gains along the way. Sauper and Barzilay (2009) investigated an approach for creating a comprehensive
textual overview of subject composed of information drawn from the Internet and applied ILP to opti-
mize both local fit of information into each topic and global coherence across the entire overview. Li
et al. (2011) developed an entity-aspect LDA model to cluster sentences into aspects and then extend
LexRank algorithm to rank sentences. Hu and Wan (2013) proposed to use SVR model and ILP method
to generate presentation slides for academic papers.
Our work is different from standard ILP-based multi-document summarization. We designed a latent
variable model to first separate the threads to be summarized into sections based on model gravel guides.
1679
We also emphasized the inclusion of potential points of interest in formulating the ILP optimization
problem.
Our work is also closely related to previous work on answer summarization in community-based
QA sites. Previous work on summarizing answers is mainly based on query focused multi-document
summarization techniques to summarize multiple answer documents given a single question. Liu et al.
(2008) proposed a CQA question taxonomy to classify questions in CQA and question-type oriented
answer summarization for better reuse of answers. Tomasoni and Huang (2010) proposed two concept-
scoring functions to combine quality, coverage, relevance and novelty measures for answer summary
in response to a question and showed that their summarized answers constitute a solid complement to
best answers voted by CQA users. Chan et al. (2012) presented an answer summarization method for
complex multi-sentence questions. For our work, we study a new problem of summarizing multiple
threads to automatically generate city travel guides based on known template structure from well-written
travel guides, which is different from the setting of single Q&A thread summarization in the previous
related studies.
7 Conclusion and Future Work
In this paper we proposed a summarization framework to generate well structured supplementary travel
guides from social media based on a latent variable model and integer linear programming. The la-
tent variable model could align forum threads with the section structure of well-written travel guides.
Compared to standard concept based ILP methods, our method additionally tries to cover more named
entities as points of interest and maximizes sentence relevance scores measured by section-specific and
city-specific word distributions learnt by the latent variable model. Extensive experiments with real data
from Yahoo! Answers show that our proposed method is able to generate better summaries compared
with a number of multi-document summarization baselines measured by ROUGE scores.
Currently our generated summaries may have overlap with the well-written model travel guides. In the
future, we plan to improve our method to emphasize the selection of additional information from social
media compared with the model travel guides. We will also look into the problem of how to summarize
information that does not fit into the template structure derived from model travel guides.
Acknowledgments
This work was done during Liu Yang?s visit to Singapore Management University. The authors would
like to thank the reviewers for their valuable comments on this work.
References
Wen Chan, Xiangdong Zhou, Wei Wang, and Tat-Seng Chua. 2012. Community answer summarization for multi-
sentence question with group l1 regularization. In Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers - Volume 1, ACL ?12, pages 582?591, Stroudsburg, PA, USA.
Association for Computational Linguistics.
G?unes Erkan and Dragomir R. Radev. 2004a. Lexpagerank: Prestige in multi-document text summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4 of EMNLP
?04.
G?unes Erkan and Dragomir R. Radev. 2004b. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. J. Artif. Int. Res., 22(1):457?479, December.
Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Langauge Processing, ILP ?09, pages 10?18, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, NAACL ?09, pages 362?370, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
1680
Yue Hu and Xiaojun Wan. 2013. Ppsgen: Learning to generate presentation slides for academic papers. In
Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI ?13, pages 2099?2105.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011. Generating aspect-oriented multi-document summariza-
tion with event-aspect model. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 1137?1146, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chen Li, Xian Qian, and Yang Liu. 2013. Using supervised bigram-based ilp for extractive summarization. In
Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, ACL ?13, pages
1004?1013, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2002. From single to multi-document summarization: A prototype system and
its evaluation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL
?02, pages 457?464, Stroudsburg, PA, USA. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology - Volume 1, NAACL ?03, pages 71?78, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and sum-
marizing answers in community-based question answering services. In Proceedings of the 22Nd International
Conference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on IR Research, ECIR?07, pages 557?564, Berlin, Heidelberg. Springer-
Verlag.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Divrank: The interplay of prestige and diversity in informa-
tion networks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ?10, pages 1009?1018, New York, NY, USA. ACM.
Ani Nenkova, Lucy Vanderwende, and Kathleen McKeown. 2006. A compositional context sensitive multi-
document summarizer: Exploring the factors that influence summarization. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ?06, pages
573?580, New York, NY, USA. ACM.
You Ouyang, Wenjie Li, Qin Lu, and Renxian Zhang. 2010. A study on position information in document
summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,
COLING ?10, pages 919?927, Stroudsburg, PA, USA. Association for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web. Technical Report 1999-66, Stanford InfoLab, November. Previous number = SIDL-WP-
1999-0120.
Yulong Pei, Wenpeng Yin, and Lian?en Huang. 2012. Generic multi-document summarization using topic-oriented
information. In Proceedings of the 12th Pacific Rim International Conference on Trends in Artificial Intelli-
gence, PRICAI?12, pages 435?446, Berlin, Heidelberg. Springer-Verlag.
Dragomir R. Radev, Hongyan Jing, Malgorzata Sty?s, and Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Inf. Process. Manage., 40(6):919?938, November.
Christina Sauper and Regina Barzilay. 2009. Automatically generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
208?216, Stroudsburg, PA, USA. Association for Computational Linguistics.
Mattia Tomasoni and Minlie Huang. 2010. Metadata-aware measures for answer summarization in community
question answering. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA. Association for Computational Linguistics.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multi-document
summarization. In Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI?07,
pages 2903?2908, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Xiaojun Wan. 2008. An exploration of document impact on graph-based multi-document summarization. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages
755?762, Stroudsburg, PA, USA. Association for Computational Linguistics.
1681
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1858?1868,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Learning Topics and Positions from Debatepedia
Swapna Gottipati? Minghui Qiu? Yanchuan Sim? Jing Jiang? Noah A. Smith?
?School of Information Systems, Singapore Management University, Singapore
?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?{swapnag.2010,minghui.qiu.2010,jingjiang}@smu.edu.sg
?{ysim,nasmith}@cs.cmu.edu
Abstract
We explore Debatepedia, a community-
authored encyclopedia of sociopolitical de-
bates, as evidence for inferring a low-
dimensional, human-interpretable representa-
tion in the domain of issues and positions. We
introduce a generative model positing latent
topics and cross-cutting positions that gives
special treatment to person mentions and opin-
ion words. We evaluate the resulting repre-
sentation?s usefulness in attaching opinionated
documents to arguments and its consistency
with human judgments about positions.
1 Introduction
The social web has evolved into a forum for large
portions of the population to discuss and debate
complex issues of societal importance. Websites like
Debatepedia,1 an online, community-authored ency-
clopedia of debates (?2), seek to organize some of
this exchange into structured information resources
that summarize arguments and link externally to
texts (editorials, blog posts, etc.) that express and
evoke them. Empirical NLP, we propose, has a
role to play in creating a more compact and easily-
interpretable way to understand the opinion space.
In particular, we envision applications to computa-
tional journalism, where there is high demand for
transformation of and pattern discovery in unman-
ageable, unstructured, evolving data (including text)
to inform the public (Cohen et al, 2011).
In this paper, we develop a generative model for
discovering such a representation (?3), using De-
batepedia as a corpus of evidence. We draw in-
spiration from Lin et al (2008) and Ahmed and
1http://dbp.idebate.org
Xing (2010), who used generative models to infer
topics?distributions over words?and other word-
associated variables representing perspectives or
ideologies. We view topics as lexicons, and propose
that grounding a topic model with evidence beyond
bags of words can lead to more lexicon-like repre-
sentations. Specifically, our generative topic model
grounds topics using the hierarchical organization
of arguments within Debatepedia. Further, we use
named entity recognition as a preprocessing step, an
existing sentiment lexicon to construct an informed
prior, and we incorporate a latent, discrete position
variable that cuts across debates.2
We evaluate the model informally and formally
(?4). Subjectively, the model identifies reasonable
topic and perspective terms, and it associates topics
sensibly with important public figures. In quanti-
tative evaluations, we find the model?s representa-
tion superior to topics from vanilla latent Dirichlet
allocation (Blei et al, 2003) and the joint sentiment
topic model (Lin and He, 2009) in matching external
texts to debates. Further, the position variables can
be used to infer the side of an argument within a de-
bate; our model performs with an accuracy of 86%
on position prediction of the debate argument. The
cross-cutting position variable is not especially con-
sistent with human judgments, suggesting that fur-
ther knowledge sources may be required to improve
interpretability across issues.
2 Data
Debatepedia, like Wikipedia, is constructed by vol-
unteer contributors and has a system of community
2This variable might serve to cluster debate sides according
to ?abstract beliefs commonly shared by a group of people,?
sometimes called ideologies (Van Dijk, 1998). We do not claim
that our model infers ideologies (see ?4).
1858
Debate: Gun control; should laws be passed to limit gun ownership further?
Question: Self-defense ? Is self-defense a good reason for gun ownership?
Side: Yes Side: No
Argument: A citizen has a ?right? to guns as a means
to self-defense: Many groups argue that a citizen
should have the ?right? to defend themselves, and that
a gun is frequently the . . .
Argument: The protection of property is not a good
justification for yielding a lethal weapon. While peo-
ple have a right to their property, this should not justify
wielding a lethal . . .
Argument: Gun restrictions and bans disadvantage cit-
izens against armed criminals. Citizens that are not al-
lowed to carry guns are disadvantaged against lawless
criminals that . . .
Argument: Robert F. Drinan, Former Democratic US
Congressman, ?Gun Control: The Good Outweighs
the Evil?, 1976 ? ?These graphic examples of individ-
ual instances of . . .
Question: Economic benefits ? Is gun control economically beneficial?
Side: Yes Side: No
Argument: Lax gun control laws are economically
costly. The Coalition for Gun Control claims that, ?in
Canada, the costs of firearms death and injury alone
have been estimated at . . .
Argument: Gun sports have economic benefits. Field
sports bring money into poor rural economies and pro-
vide a motivation for landowners to value environmen-
tal protection.
Table 1: An example of a Debatepedia debate on the topic ?Gun control.?
moderation. Many of the debate issues covered are
controversial and salient in current public discourse.
Because it is primarily expressed as text, Debatepe-
dia is a corpus of debate topics, but it is organized
hierarchically, with multiple issues in each debate
topic, questions within each issue, and arguments on
two sides of each question. An important feature of
the corpus is the widespread quotation and linking to
external articles on the web, including news stories,
blog postings, wiki pages, and social media forums;
here we use these external articles in evaluation (?4).
Table 1 shows excerpts from a debate page3 from
Debatepedia. Each debate contains ?questions,?
which reflect the different aspects of a debate. In this
particular debate, there are 13 questions (2 shown),
ranging from economic benefits to enforceability to
social impacts. For each question, there are two dis-
tinct sides, each with its own set of supporting argu-
ments. Many of these arguments also contains links
to online articles where the quotes are extracted from
(not shown in Table 1). For example, in the second
argument on the ?No? side, there is an inline link to
the article written by Congressman Drinan.4
Within a debate topic, the sides cut across differ-
ent questions, aligning arguments together. In gen-
3http://dbp.idebate.org/en/index.php/
Debate:_Gun_control
4http://www.saf.org/LawReviews/Drinan1.
html
Debates 1,303
Arguments 33,556
Articles linked by exactly one argument 3,352
Tokens 1,710,814
Types (excluding NE mentions) 59,601
Person named entity mentions 9,496
Table 2: Debatepedia corpus statistics. Types and tokens
include unigrams, bigrams and person named entities.
eral, the questions are phrased so that a consistent
?pro? and ?con? structure is apparent throughout
each debate, aligned to a high-level question (i.e.,
the ?Yes? sides of all the questions are consistent
with the same side of the larger debate). The ex-
ample of Table 1 deviates from this pattern, with the
self-defense ?Yes? arguing ?no? to the high-level de-
bate question?Should laws be passed to limit gun
ownership further??and the economic ?Yes? argu-
ing ?yes? to the high-level question.
Table 2 presents statistics of our corpus.
2.1 Preprocessing
We scraped the Debatepedia website and extracted
the debate, question, argument, and side structure
of the debate topics. We crawled the external
web articles that were linked from the Debatepe-
dia arguments. For the web articles, we extracted
the main text content (ignoring boilerplate elements
such as navigation and advertisments) using Boil-
1859
erpipe (Kohlschu?tter et al, 2010).5 We tokenized
the text and filtered stopwords.6 We considered both
unigrams and bigrams in our model, keeping all uni-
grams and removing bigram types that appeared less
than 5 times in the corpus. Although our modeling
approach ultimately treats texts as bags of terms (un-
igrams and bigrams), one important preprocessing
step was taken to further improve the interpretabil-
ity of the inferred representation: named entity men-
tions of persons. We identified these mentions of
persons using Stanford NER (Finkel et al, 2005)
and treated each person mention as a single token. In
our qualitative analysis of the model (?4.2), we will
show how this special treatment of person mentions
enables the association of well-known individuals
with debate topics. Though not part of our exper-
imental evaluation in this paper, such associations
are, we believe, an interesting direction for future
applications of the model.
3 Model
Our model defines a probability distribution over
terms7 that are observed in the corpus. Each term
occurs in a context defined by the tuple ?d, q, s, a?
(respectively, a debate, a question within the debate,
a side within the debate, and an argument). At each
level of the hierarchy is a different latent variable:
? Each question q within debate d is associated
with a distribution over topics, denoted ?d,q.8
? Each side s of the debate d is associated with a
position, denoted id,s and we posit a global dis-
tribution ? that cuts across different questions
and arguments. In our experiments, there are
two positions, and the two sides of a debate
are constrained to associate with opposing po-
sitions. As illustrated by Table 1, this assump-
5http://code.google.com/p/boilerpipe
6www.ranks.nl/resources/stopwords.html
7Recall that our model includes bigrams. We treat each un-
igram and bigram token (after filtering discussed in ?2.1) as a
separate term.
8In future work, more sharing across questions within a
debate, or more differentiation among the topic distributions
for arguments under a question, might be explored. Wallach
(2006) describes suitable techniques using hierarchical Dirich-
let draws, and Eisenstein et al (2011) suggests the use of sparse
shocks to log-odds at different levels. Here we work on the
assumption that Debatepedia?s questions are the most topically
coherent level, and work with a single topic mixture at this level.
wz y
Nd,q,s,a
Ad,q,s
?
?
Qd
?
?
?tt?
o
i,t?ii ?et
K TKT
?b
?i ?o ?t ?e
?b
i
?
?
Sd
D
Figure 1: Plate diagram. K is the number of positions,
and T is number of topics. The shaded variables are ob-
served and dashed variables are marginalized. ?,?,?
and all ? are fixed hyperparameters (?3.1).
tion is not always correct, though it tends to
hold most of the time.
? Each term wd,q,s,a,n (n is the position index
of the term within an argument) is associated
with one of five functional term types, denoted
yd,q,s,a,n. This variable is latent, except when it
takes the value ?entity? (e) for terms marked as
named entity mentions. When it is not an en-
tity, it takes one of the other four values: ?gen-
eral position? (i), ?topic-specific position? (o),
?topic? (t), or ?background? (b). Thus, every
term w is drawn from one of these 5 types of
bags, and y acts as a switching variable to se-
lect the type of bag.
? For some term types (the ones where y ?
{o, t}), each term wd,q,s,a,n is associated with
one of T discrete topics, as indexed by
zd,q,s,a,n.
Figure 1 illustrates the plate diagram for the
graphical model underlying our approach. The gen-
erative story is given in Figure 2.
3.1 Priors
Typical probabilistic topic models assume a sym-
metric Dirichlet prior over its term distributions or
1860
1. ? topics t, draw topic-term distribution ?tt ? Dirichlet(?t) and topic-entity distribution ?et ? Dirichlet(?e).
2. ? positions i, draw position-term distribution ?ii ? Dirichlet(?i).
3. ? topics t, ? positions i, draw topic-position term distribution ?oi,t ? Dirichlet(?o).
4. Draw background term distribution ?b ? Dirichlet(?b).
5. Draw functional term type distribution ? ? Dirichlet(?).
6. Draw position distribution ? ? Dirichlet(?).
7. ? debates d:
a. Draw id,1, id,2 ? Multinomial(?), assigning each of the two sides to a position.
b. ? questions q in d:
i. Draw topic mixture proportions ?d,q ? Dirichlet(?).
ii. ? arguments a under question q and term positions n in a:
A. Draw topic label zd,q,s,a ? Multinomial(?d,q).
B. Draw functional term type yd,q,s,a ? Multinomial(?).
C. Draw term wd,q,s,a ? Multinomial (?yd,q,s,a | id,1, id,2, zd,q,s,a).
Figure 2: Generative story for our model of Debatepedia.
apply empirical Bayesian techniques to estimate the
hyperparameters. Motivated by past efforts to ex-
ploit prior knowledge (Zhao et al, 2010; Lin and
He, 2009), we use the OpinionFinder sentiment lex-
icon9 (Wilson et al, 2005) to construct ?i and ?o.
Specifically, terms w in the lexicon were given pa-
rameters ?iw = ?ow = 0.01, and other terms were
given ?iw = ?ow = 0.001, capturing our prior belief
that opinion-expressing terms are likely to be used
in expressing positions. 5,451 types were given a
?boost? through this prior.
Information retrieval has long exploited the ob-
servation that a term?s document frequency (i.e., the
number of documents a term occurs in) is inversely
related its usefulness in retrieval (Jones, 1972). We
encode this in ?b, the prior over the background
term distribution, by setting each value to the log-
arithm of the term?s argument frequency.
The other priors were set to be symmetric: ?e =
0.01 (entity topics), ?t = 0.001 (topics), ? =
50/T = 1.25 (topic mixture coefficients), ? = 0.01
(positions), and ? = 0.01 (functional term types).
Preliminary tests showed that final topics are rela-
tively insensitive to the values of the hyperparame-
ters.
3.2 Inference and Parameter Estimation
Exact inference under this model, like most latent-
variable topic models, is intractable. We apply col-
lapsed Gibbs sampling, a standard approach for such
9http://mpqa.cs.pitt.edu/lexicons/subj_
lexicon/
models (Griffiths and Steyvers, 2004).10 The no-
table deviations from typical uses of collapsed Gibbs
sampling are: (i) we jointly sample id,1 and id,2 to
respect the constraint that they differ; and (ii) we
fix the priors, in some cases to be asymmetric, as
discussed in ?3.1. We perform Gibbs sampling for
2,000 iterations over the dataset, discarding the first
500 iterations for burn-in, and averaging over every
10th iteration thereafter to get estimates for our term
distributions.
3.3 T andK
In all experiments, we use T = 40 topics andK = 2
positions. We did not extensively explore different
values for T and K; preliminary exploration sug-
gested that interpretability, gauged informally by the
authors, degraded for higher values of either.
4 Evaluation
Recall that the aim of this work is to infer a low-
dimensional representation of debate text. We esti-
mated our model on the Debatepedia debates (not in-
cluding hyperlinked articles), and conducted several
evaluations of the model, each considering a differ-
ent aspect of the goal. We exploit external articles
hyperlinked from Debatepedia described in ?2 as
supporting texts for arguments, treating each one?s
association to an argument as variable to be pre-
dicted. Firstly, we evaluate our model on the article
associating task. Secondly, we evaluate our model
on the position prediction task. Then, we compare
10Because this technique is well known in NLP, details are
relegated to supplementary material.
1861
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
N
o 
of
 A
rti
cle
s
JS Divergence
LDA
JST
Our Model
Figure 3: The distribution over Jensen-Shannon diver-
gences between a hyperlinked article and the correspond-
ing Debatepedia argument, n = 3, 352.
our model?s positional assignment of arguments to
human annotated clusterings. Finally, we present
qualitative discussion.
4.1 Quantitative Evaluation
4.1.1 Topics
As described in ?2, our corpus includes 3,352 ar-
ticles hyperlinked by Debatepedia arguments.11 Our
model can be used to infer the posterior over top-
ics associated with such an article, and we compare
that distribution to that of the Debatepedia article
that links to it. Calculating the similarity of these
distributions, we get an estimate of how closely our
model can associate text related to a debate with the
specific argument that linked to it. We compare with
LDA (Blei et al, 2003), which ignores sentiment,
and the joint sentiment topic (JST) model (Lin and
He, 2009), an unsupervised model that jointly cap-
tures sentiment and topic.12 Using Jensen-Shannon
divergence, we find that our approach embeds these
pairs significantly closer than LDA and JST (also
trained with 40 topics), under a Wilcoxon signed
rank test (p < 0.001). Figure 3 shows the histogram
of divergences between our model, JST, and LDA.
Associating external articles. More challenging,
of course, is selecting the argument to which an
external article should be associated. We used the
Jensen-Shannon divergence between topic distribu-
tions of articles and arguments to rank the latter,
for each article. The mean reciprocal rank scores
(Voorhees, 1999) for LDA, JST, and our model were
11We consider only those articles linked by a single Debate-
pedia argument.
12JST multiplies topics out by the set of sentiment labels, as-
signing each token to both a topic and a sentment. We use the
OpinionFinder lexicon in JST?s prior in the same way it is used
in our model.
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
5 10 15 20 25 inf
M
R
R
K
LDA
JST
Our Model
Figure 4: Mean reciprocal ranks for the association task.
0.1272, 0.1421, and 0.1507, respectively; the differ-
ence is significant (Wilcoxon signed rank test, p <
0.001). We found the same pattern for MRR@k,
k ? {5, 10, 15, 20, 25,?}, as shown in Figure 4.
It is likely possible to engineer more accurate
models for attaching articles to arguments, but the
attachment task is our aim only insofar as it con-
tributes to an overall assessment of an inferred rep-
resentation?s quality.
4.1.2 Positions
Positional distance by topic. We next consider
the JS divergences of position term distributions by
topic; for each topic t, we consider the divergence
between inferred values for ?o1,t and ?o2,t. Figure 5
shows these measurements sorted from most to least
different; these might be taken as evidence for which
issue areas? arguments are more lexically distin-
guishable by side, perhaps indicating less common
ground in discourse or (more speculatively) greater
controversy. For example, our model suggests that
debates relating to topics like presidential politics,
foreign policy, teachers, women?s health, religion,
and Israel/Palestine are more heated (within the De-
batepedia community at the time the debates took
place) than those about the minimum wage, Iran as
a nuclear threat, or immigration.
Predicting positions for arguments. We tested
our model?s ability to infer the positions of argu-
ments. In this experiment (only), we held out 3,000
arguments during parameter estimation. The held-
out arguments were selected so that every debate
side maintained at least one argument whose in-
ferred side could serve as the correct answer for the
held-out argument. We then inferred i for each held-
out argument from debate d and side s, given the
parameters, and compared it with the value of id,s
inferred during parameter estimation. The model
achieved 86% accuracy (Table 3 shows the confu-
1862
sion matrix). Note that JST does not provide a base-
line for comparison, since it does not capture debate
sides.
i = 1 i = 2
i? = 1 1,272 216
i? = 2 199 1,313
Table 3: Confusion matrix for position prediction on
held-out arguments.
Predicting positions for external articles. We
can also use the model to predict the position
adopted in an external text. For articles linked from
within Debatepedia, we have a gold standard: from
which side of a debate was it linked? After using
the model to infer a position variable for such a text,
we can check whether the inferred position variable
matches that of the argument that links to it. Table 4
shows that our model does not successfully com-
plete this task, assigning about 60% of both kinds
of articles i = 1.
i = 1 i = 2
i? = 1 1,042 623
i? = 2 1,043 644
Table 4: Confusion matrix for position prediction on hy-
perlinked articles.
Genre. We manually labeled 500 of these articles
into six genre categories. We had two annotators for
this task (Cohen?s ? = 0.856). These categories,
in increasing order of average Jensen-Shannon di-
vergence, are: blogs, editorials, wiki pages, news,
other, and government. Figure 6 shows the results.
While the only difference between the first and last
groups are surprising by chance, we are encouraged
by our model?s suggestion that blogs and editori-
als may be more ?Debatepedia argument-like? than
news and government articles.
Note that our model is learned only from text
within Debatepedia; it does not observe the text of
external linked articles. Future work might incorpo-
rate this text as additional evidence in order to cap-
ture effects on language stemming from the interac-
tion of position and genre.
0.1 0.2 0.3 0.4 0.5comment, minimum, wage, poverty, capitalism
nuclear, weapons, iran, states, threatparty, vote, republican, political, voters
energy, gas, power, fuel, windtax, economic, trade, cost, percent
immigration, cameras, police, immigrants, crimepeople, dont, time, lot, make
food, consumers, products, calorie, informationdeath, crime, punishment, penalty, justice
marijuana, drug, drugs, alcohol, agemarriage, gay, mars, space, moon
rights, law, people, individual, amendmentsouth, kosovo, independence, state, republic
human, rights, animals, life, animalchildren, child, sex, parents, sexual
school, schools, students, education, publicchina, tibet, chinese, people, tibetan
global, emissions, climate, carbon, warminginternational, court, war, crimes, icc
english, language, violence, people, videoorleans, euthanasia, city, suicide, priests
speech, corporations, corporate, public, moneyhealth, care, insurance, public, private
circumcision, men, sexual, circumcised, foreskininformation, torture, science, evidence, wikipedia
companies, market, industry, business, bailoutlaw, workers, union, rights, legal
college, cloning, game, football, incesttimes, york, ban, june, january
countries, eu, european, international, statesoil, water, production, ethanol, environmental
military, war, iraq, forces, marcheconomy, financial, spending, economic, government
government, social, governments, state, programsisrael, gaza, hamas, israeli, palestinian
women, religious, abortion, god, lifeteachers, pay, test, left, merit
peace, state, west, united, actionunited, states, president, administration, foreign
president, washington, obama, american, america
Figure 5: Jensen-Shannon divergences between topic-
specific positional term distributions, for each topic. Top-
ics are labeled by their most frequent terms from ?t.
4.1.3 Comparison to Human Judgments of
Positions
We compared our model?s inferred positions to
human judgments. For each of the 11 topics in Ta-
ble 8, we selected two associated debates with more
arguments than average (24.99). The debates were
provided to each of three human annotators,13 who
13All were native English-speaking American graduate stu-
dents not otherwise involved in this research. Each is known
by the authors to have basic literacy with issues and debates in
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
blog(12) edit(14) wiki(11) news(33) other(18) gov(12)
JS
 D
ive
rge
nc
e S
co
re
Article type(% of articles)
Figure 6: Position prediction on 500 hyperlinked articles
by genre.
1863
?Israel-Palestine? ?Same-sex marriage? ?Drugs? ?Healthcare? ?Death penalty? ?Abortion?
i1
pre emptive same sex hands free single payer anti death pro choice
israeli palestinian long term performance enhancing so called non violent pro life
open and shut second class in depth self sustaining african american non muslim
i2
two state opposite sex long term government run semi automatic would be
long term well intentioned high speed government approved high profile full time
self destructive day time short term high risk hate crime late term
a. Our model: topic-specific position bigrams associated with six selected topics.
?
war large illegal support death power
assault possibility abuse force penalty limit
disproportionate problems high threat murder civil
+
peace civil disease care power care
independence rights nature universal clean suicide
self-determination affirmative potential uninsured waste death
b. JST: sentiments associated with six selected topics manually aligned to our model?s topics.
Table 6: Terms associated with selected topics. The labels and alignments between the two models? topics were
assigned manually. (a.) Our model: topic-specific position bigrams which are ranked by comparing the log odds
conditioned on the position and topic: log ?oi1,t,w? log ?oi2,t,w. We show the top three terms for each position (b.) JST:we show the top three terms for each sentiment (negative and positive).
A1 (11) A2 (5) A3 (16)
Model (2) 3.21 2.58 3.45
A1 (11) 2.15 2.15
A2 (5) 2.63
Table 5: Variation of information scores for each pairing
of annotators and model.
were instructed to group the 44 sides of the debates.
The instructions stated:
Our goal is to see what you think about how
the different sides of different debates can be
lined up. You might find it convenient to
think of these in terms of political philoso-
phies, contemporary political party platforms,
or something else. Any of these is fine; we
want you to tell us the grouping you find most
reasonable.
All three annotators (hereafter denoted A1, A2, and
A3) used fairly involved labeling schemes; the an-
notators used 37, 30, and 16 unique labels, respec-
tively.14 A1 used keyword lists to label items; we
coarsened his labels manually by removing or merg-
ing less common keywords (resulting in: Republi-
can, Democrat, science/environment, nanny, politi-
cal reform, fiscal liberal, fiscal conservative, liber-
tarian, Israel, Palestine, and one unlabeled side).
A2 provided a coarse annotation along with each
American politics.
14In a small number of cases, an annotator declined to label
a side. Each unlabeled item received its own cluster.
fine-grained one (liberal, conservative, ?, and two
unlabeled sides). We used 100 samples from our
Gibbs sampler to estimate posteriors for each id,s;
these were always 99% or more in agreement, so we
mapped each debate side into its single most proba-
ble cluster. Recall that the two sides of each debate
must be in different clusters.
Table 5 shows the variation of information mea-
sure (Meila, 2003) for each pairing among the three
annotators and our model. The model agrees with
A2?s coarse clustering most closely, and in fact is
closer to A2?s clustering than A2 is to A3?s; it also
agrees with A2?s coarse clustering better than A2?s
coarse and fine clusterings agree (3.36, not shown
in the table). This is promising, but we do not
have confidence that the positional dimension is be-
ing captured especially well in this model; for those
debate-sides labeled liberal or conservative by A2,
the best match of our two positions was still only in
agreement only about 60% of the time, and agree-
ment with each human annotator is within the inter-
val of what would be expected if each debate?s sides
were assigned uniformly at random to positions.15
Remarks. Within debates and within topics, the
model uses the position variable to distinguish sides
well. For external text, the model performs well
on articles such as blogs and editorials but on oth-
ers the positional categories do not seem meaning-
15This was determined using a Monte Carlo simulation with
1,000 samples.
1864
Topic i = 1 i = 2
None (?i) vice president, c sections, twenty four, cross pressures,
pre dates, anti ballistic, cost effectiveness, anti land-
mine, court appointed, child poverty
cross examination, under runs, hand outs, half million,
non christians, break down, counter argument, seventy
five, co workers, run up
?Israel-
Palestine?
pre emptive, israeli palestinian, open and shut, first
time, hamas controlled, democratically elected
two state, long term, self destructive, secretary general,
right wing, all out, near daily, short term
?Same-sex
marriage?
same sex, long term, second class, blankenhorn rauch,
wrong headed, self denial, left handed
opposite sex, well intentioned, day time, planet wide,
day night, child rearing, low earth, one way, one third
?Drugs? hands free, performance enhancing, in depth, hand
held, best kept, non pharmaceutical, anti marijuana
long term, high speed, short term, peer reviewed, alco-
hol related, mind altering, inner city, long lasting
?Healthcare? single payer, so called, self sustaining, public private,
for profit, long run, high cost, multi payer
government run, government approved, high risk, two
tier, government appointed, low cost, set up
?Death
penalty?
anti death, non violent, african american, self help, cut
and cover, heavy handed, dp equivalent
semi automatic, high profile, hate crime, assault
weapons, military style, high dollar, self protective
?Abortion? pro choice, pro life, non muslim, well educated, anti
abortion, much needed, church state, birth control
would be, full time, late term, judeo christian, life
style, day to day, non christian, child bearing
Table 7: General position (first row) and topic-specific position bigrams associated with six selected topics.
Topic Terms Person entity mentions
?Israel-
Palestine?
israel, gaza, hamas, israeli, pales-
tinian
Benjamin Netanyahu, Al Jazeera, Mavi Marmara, Nicholas Kristoff,
Steven R. David
?Same-sex
marriage?
marriage, gay, mars, space, moon Buzz Aldrin, Andrew Sullivan, Moon Base, Scott Bidstrup, Ted Olson
?Drugs? marijuana, drug, drugs, alcohol, age Four Loko, Evo Morales, Toni Meyer, Sean Flynn, Robert Hahn
?Healthcare? health, care, insurance, public, pri-
vate
Kent Conrad, Paul Hsieh, Paul Krugman, Ezra Klein, Jacob Hacker
?Death
penalty?
death, crime, punishment, penalty,
justice
Adam Bedau, Thomas R. Eddlem, Jeff Jacoby, John Baer, Peter Bronson
?Abortion? women, religious, abortion, god, life Ronald Reagan, John Paul II, Sara Malkani, Mother Teresa, Marcella
Alsan
Table 8: For 6 selected topics (labels assigned manually), top terms (?t) and person entities (?e). Bigrams were
included but did not rank in the top five for these topics. The model has conflated debates relating to same-sex
marriage with the space program.
ful, perhaps due to the less argumentative nature
of other kinds of articles. Noting the vast litera-
ture focusing on ideological positions expressed in
text, we believe this failure suggests (i) that broad-
based positions that hold across many topics may
require richer textual representations (see, e.g., the
?syntactic priming? of Greene and Resnik, 2009),
or (ii) that an alternative representation of positions,
such as the spatial models favored by political sci-
entists (Poole and Rosenthal, 1991), may be more
discoverable. Aside from those issues, a stronger
theory of positions may be required. Such a the-
ory could be encoded in a more informative prior or
weaker independence assumptions across debates.
Finally, exploiting explicitly ideological texts along-
side the moderated arguments of Debatepedia might
also help to identify textual associations with gen-
eral positions (Sim et al, 2013). We leave these di-
rections to future work.
4.2 Qualitative Analysis
Of the T = 40 topics our model inferred, we subjec-
tively judged 37 to be coherent; a glimpse of each is
given in Figure 5. We manually selected six of the
most interpretable topics for further evaluation.
As a generative modeling approach, our model
was designed for the purpose of reducing the dimen-
sionality of the sociopolitical debate space, as evi-
denced by Debatepedia. It is like other topic models
in this regard, but we believe that some effects of our
design choices are noteworthy. Table 6 compares the
positional bigrams of our model to the sentiments in-
ferred by JST. We observe the benefit of our model
in identifying terms associated with positions on so-
cial issues, while JST selects more general sentiment
terms.
1865
Table 7 shows bigrams most strongly associated
with general position distributions ?i and selected
topic-position distributions ?o.16 We see the poten-
tial benefit of multiword expressions. Although we
have used frequent bigrams as a poor man?s approx-
imation to multiword expression analysis, we find
the topic-specific positions terms to be subjectively
evocative. While somewhat internally coherent, we
do not observe consistent alignment across topics,
and the general distributions ?i are not suggestive.
The separation of personal name mentions into
their own distributions, shown for some topics in
Table 8, gives a distinctive characterization of top-
ics based on relevant personalities. Subjectively, the
top individuals are relevant to the subject matter as-
sociated with each topic (though the topics are not
always pure; same-sex marriage and the space pro-
gram are merged, for example).
5 Related Work
Insofar as debates are subjective, our study is related
to opinion mining. Subjective text classification
(Wiebe and Riloff, 2005) leads to opinion mining
tasks such as opinion extraction (Dave et al, 2003),
positive and negative polarity classification (Pang et
al., 2002), sentiment target detection (Hu and Liu,
2004; Ganapathibhotla and Liu, 2008), and feature-
opinion extraction (Wu et al, 2009). The above
studies are conducted mostly on product reviews, a
domain with a simpler opinion landscape and more
concrete rationales for those opinions, compared to
sociopolitical debates.
Generative topic models have been successfully
implemented in opinion mining tasks such as feature
identification (Titov and McDonald, 2008), entity-
topic extraction (Newman et al, 2006), mining con-
tentious expressions and interactions (Mukherjee
and Liu, 2012) and specific aspect-opinion word ex-
traction from labeled data (Zhao et al, 2010). Most
relevant to this research is work on feature-sentiment
extraction (Lin and He, 2009; Mei et al, 2007). Mei
et al (2007) built on PLSI, which is problematic
for generalizing beyond the training sample. The
JST model of Lin and He (2009) is an LDA-based
topic model in which each word token is assigned
both a sentiment and a topic; they exploited a sen-
16For more topics, please refer to the supplementary notes.
timent lexicon in the prior distribution. Our model
is closely related, but introduces a switching vari-
able that assigns some tokens to positions, some to
topics, and some to both. Unlike Lin and He?s senti-
ments, our model?s positions are associated with the
two sides of a debate, and we incorporate topics at
the level of questions within debates.
Some studies have specifically analyzed con-
trastive viewpoints or stances in general discussion
text.Agrawal et al (2003) used graph mining based
method to classify authors in to opposite camps for
a given topic. Paul et al (2010) developed an unsu-
pervised method for summarizing contrastive opin-
ions from customer reviews. Abu-Jbara et al (2012)
and Dasigi et al (2012) developed techniques to ad-
dress the problem of automatically detecting sub-
groups of people holding similar stances in a dis-
cussion thread.
Several prior studies have considered debates.
Cabrio and Villata (2012) developed a system based
on argumentation theory which recognizes the en-
tailment and contradiction relationships between
two texts. Awadallah et al (2011) used a debate
corpus as a seed for extracting person-opinion-topic
tuples from news and other web documents and in
later work classified the quotations to specific top-
ics and polarity using language models (Awadal-
lah et al, 2012). Somasundaran and Wiebe (2009)
and Anand et al (2011) were interested in ideolog-
ical content in debates, relying on discourse struc-
ture and leveraging sentiment lexicons to recognize
stances.
Closer to the methodology we describe, Lin et
al. (2008) presented a statistical model for politi-
cal discourse that incorporates both topics and ide-
ologies; they used debates on the Israeli-Palestinian
conflict. Fortuna et al (2009) showed that it is pos-
sible to isolate a subset of terms from media content
that are informative of a news organization?s bias to-
wards a particular issue. Ahmed and Xing (2010) in-
troduced multi-level latent Dirichlet alocation, and
Eisenstein et al (2011) introduced sparse additive
generative models, both conceived as extensions to
well-established probabilistic modeling techniques
(Blei et al, 2003); these were applied to debates
and political blog datasets. Our approach builds on
these models (especially the switching variables of
Ahmed and Xing). We go farther in jointly modeling
1866
text across many debates evidenced by the structure
of Debatepedia, thus grounding our models more
solidly in familiar sociopolitical issues, and in mak-
ing extensive use of existing NLP resources.
6 Conclusion
Using text from Debatepedia, we inferred topics and
position term lexicons in the domain of sociopoliti-
cal debates. Our approach brings together tools from
information extraction and sentiment analysis into a
latent-variable topic model and exploits the hierar-
chical structure of the dataset. Our qualitative and
quantitative evaluations show the model?s strengths
and weaknesses.
Acknowledgments
The authors thank several anonymous reviewers,
Justin Gross, David Kaufer, and members of the
ARK group at CMU for helpful feedback on this
work and gratefully acknowledge the assistance of
the annotators. This research is supported by the
Singapore National Research Foundation under its
International Research Centre@Singapore Funding
Initiative and administered by the IDM Programme
Office, by an A?STAR fellowship to Y.S., and by
Google?s support of the Reading is Believing project
at CMU.
References
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of ACL.
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In WWW
?03.
Amr Ahmed and Eric P. Xing. 2010. Staying in-
formed: supervised and semi-supervised multi-view
topical analysis of ideological perspective. In Pro-
ceedings of EMNLP.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox
Tree, Robeson Bowmani, and Michael Minor. 2011.
Cats rule and dogs drool!: classifying stance in online
debate. In Proceedings of the Second Workshop on
Computational Approaches to Subjectivity and Senti-
ment Analysis.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2011. OpinioNetIt: Understanding the
opinions-people network for politically controversial
topics. In Proceedings of CIKM.
Rawia Awadallah, Maya Ramanath, and Gerhard
Weikum. 2012. PolariCQ: Polarity classification of
political quotations. In Proceedings of CIKM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elena Cabrio and Serena Villata. 2012. Combining
textual entailment and argumentation theory for sup-
porting online debates interactions. In Proceedings of
ACL.
Sarah Cohen, James T. Hamilton, and Fred Turner. 2011.
Computational journalism. Communications of the
ACM, 54(10):66?71.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: a pilot study of implicit attitude using
latent textual semantics. In Proceedings of ACL.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Proceed-
ings of ICML.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Blaz Fortuna, Carolina Galleguillos, and Nello Cristian-
ini. 2009. Detecting the bias in media with statis-
tical learning methods. In Ashok N . Srivastava and
Mehran Sahami, editors, Text Mining: Classification,
Clustering, and Applications, pages 27?50. Chapman
& Hall/CRC.
Murthy Ganapathibhotla and Bing Liu. 2008. Mining
opinions in comparative sentences. In Proceedings of
COLING.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment. In
Proceedings of HLT-NAACL.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(Suppl. 1):5228?5235.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of CIKM.
Karen Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of documentation, 28(1):11?21.
Christian Kohlschu?tter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of WSDM.
1867
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic
model for sentiment analysis. In Proceedings of
CIKM.
Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.
2008. A joint topic and perspective model for ideo-
logical discourse. In Proceedings of ECML-PKDD.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW.
Marina Meila. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, Learning Theory and Kernel
Machines, volume 2777 of Lecture Notes in Computer
Science, pages 173?187. Springer.
Arjun Mukherjee and Bing Liu. 2012. Mining con-
tentions from discussions and debates. In Proceedings
of KDD.
David Newman, Chaitanya Chemudugunta, and Padhraic
Smyth. 2006. Statistical entity-topic models. In Pro-
ceedings of KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of EMNLP.
Keith Poole and Howard Rosenthal. 1991. Patterns of
congressional voting. American Journal of Political
Science, pages 118?178.
Yanchuan Sim, Brice Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological propor-
tions in political speeches. In Proceedings of EMNLP.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings of
ACL.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceedings
of WWW.
Teun A. Van Dijk. 1998. Ideology: A Multidisciplinary
Approach. Sage Publications Limited.
Ellen M. Voorhees. 1999. The trec-8 question answering
track report. In Proceedings of TREC.
Hanna M. Wallach. 2006. Topic modeling: beyond bag-
of-words. In Proceedings of ICML.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of CICLing.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT-EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming
Li. 2010. Jointly modeling aspects and opinions with
a maxent-lda hybrid. In Proceedings of EMNLP.
1868
Proceedings of NAACL-HLT 2013, pages 401?410,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Mining User Relations from Online Discussions using Sentiment Analysis
and Probabilistic Matrix Factorization
Minghui Qiu?, Liu Yang?,?, Jing Jiang?
? School of Information Systems, Singapore Management University, Singapore
? School of Software and Microelectronics, Peking University, China
{minghui.qiu.2010,jingjiang}@smu.edu.sg, yang.liu@pku.edu.cn
Abstract
Advances in sentiment analysis have enabled
extraction of user relations implied in online
textual exchanges such as forum posts. How-
ever, recent studies in this direction only con-
sider direct relation extraction from text. As
user interactions can be sparse in online dis-
cussions, we propose to apply collaborative
filtering through probabilistic matrix factor-
ization to generalize and improve the opinion
matrices extracted from forum posts. Exper-
iments with two tasks show that the learned
latent factor representation can give good per-
formance on a relation polarity prediction task
and improve the performance of a subgroup
detection task.
1 Introduction
The fast growth of the social Web has led to a large
amount of interest in online social network analysis.
Most existing work on social network analysis re-
lies on explicit links among users such as undirected
friendship relations (Liben-Nowell and Kleinberg,
2003), directed following relations (Hopcroft et al,
2011) and trust/distrust relations (Leskovec et al,
2010). However, besides these explicit social rela-
tions, the various kinds of interactions between on-
line users often suggest other implicit relations. In
particular, in online discussion forums, users inter-
act through textual posts and these exchanged texts
often reveal whether two users are friends or foes, or
whether two users share the same viewpoint towards
a given issue.
To uncover such implicit relations requires text
analysis and particularly sentiment analysis. Re-
cently, Hassan et al (2012) studied predicting the
polarity of user interactions in online discussions
based on textual exchanges. They found that the au-
tomatically predicted signed relations had an accu-
racy above 80%. The extracted signed network was
further used to detect ideological subgroups. This is
a piece of pioneering work that extracts online social
relations based on text analysis.
In this paper, we further extend the idea of mining
social relations from online forum posts by incorpo-
rating collaborative filtering. Our work is motivated
by the observation that direct textual exchanges be-
tween users are sparse. For example, in the data set
we use, only around 13% of user-user pairs have di-
rect interactions. Collaborative filtering is a com-
monly used technique in recommender systems to
predict missing ratings. The key assumption is that
if two people have the same opinion on an item A,
they are likely to also have the same opinion on a
different item B. In online discussion forums, users
express their opinions about each other as well as
the various aspects of the topic under discussion, but
not every user comments on every aspect or every
other user. Collaborative filtering allows us to iden-
tify users with the same opinion even if they have not
directly interacted with each other or commented on
any common aspect.
Our method starts with extracting opinions on
users and topic aspects from online posts using sen-
timent analysis. The results are two matrices indi-
cating the sentiment polarity scores between pairs
of users and pairs of a user and an aspect. To in-
corporate collaborative filtering, we choose proba-
bilistic matrix factorization (PMF) (Salakhutdinov
401
and Mnih, 2008), a technique that has been success-
fully applied for collaborative filtering-based recom-
mendation problems. PMF automatically discovers
a low-rank representation for both users and items
based on observed rating data. In our problem, the
predicted sentiment polarity scores are treated as rat-
ing data, and the results of PMF are low-rank vectors
representing each user in online discussions.
We evaluate our method on two tasks. The first
is to predict the polarity of interactions between two
users not from their own textual exchanges but from
their interactions with other users or comments on
topic aspects. The second is to use the latent vectors
to group users based on viewpoints. We find that the
latent factor representation can produce good predic-
tion results for the first task and improve the cluster-
ing results of the second task compared with a num-
ber of baselines, showing the effectiveness of col-
laborative filtering for mining social relations from
online discussions.
2 Related Work
Our work is closely related to recent studies on
detecting subgroups from online discussions (Abu-
Jbara et al, 2012; Dasigi et al, 2012; Hassan et
al., 2012). Abu-Jbara et al (2012) proposed to
build discussant attitude profiles (DAP) from on-
line posts and use these profiles to cluster users into
subgroups. A DAP is a vector that contains the
attitudes of a discussant towards other discussants
and a set of opinion targets. We also extract opin-
ions of users towards other users and opinion tar-
gets from posts, which are similar to DAPs. The
difference is that we further apply probabilistic ma-
trix factorization to derive a low-rank representation
from the raw opinion scores. Our comparison with
DAP-based clustering shows that probabilistic ma-
trix factorization can improve subgroup detection.
Hassan et al (2012) proposed to predict the polar-
ity of interactions between users based on their tex-
tual exchanges. They defined a set of interaction
features using sentiment analysis and applied super-
vised learning for polarity prediction. In compari-
son, our work is unsupervised, that is, we do not use
any ground truth of interaction polarity for training.
Probabilistic matrix factorization was proposed
by Salakhutdinov and Mnih (2008) as a collabo-
rative filtering method for recommender systems.
It has attracted much attention and been extended
by Ma et al (2008) and Wang and Blei (2011).
In particular, Ma et al (2008) proposed a SocRec
model that combines social network information
with rating data using the PMF framework to per-
form social recommendation. Our model bears sim-
ilarity to SocRec in that we also consider two types
of interactions, i.e. user-user interactions and user-
aspect interactions. However, different from Ma et
al. (2008), we predict both the user-user and user-
aspect scores from textual posts using sentiment
analysis, and the user-user opinion polarity scores
are symmetric.
Part of our method uses sentiment analysis to ex-
tract opinions from text. This is built on top of a
large body of existing work on opinion extraction,
e.g. Choi et al (2006) and Wu et al (2009). As the
sentiment analysis component is not our main con-
tribution, we do not review existing work along this
direction in detail here. Interested readers can refer
to Pang and Lee (2008).
The idea of incorporating sentiment analysis into
collaborative filtering algorithms has been explored
by Kawamae (2011), Moshfeghi et al (2011) and
Leung et al (2011). While their work also com-
bines sentiment analysis with collaborative filtering,
the purpose is to improve the accuracy of item rec-
ommendation. In contrast, we borrow the idea and
technique of collaborative filtering to improve user
relation mining from online text.
3 Method Overview
In this section, we provide an overview of our
method. We first introduce some concepts.
User: We use user to refer to a discussant in an on-
line discussion. Each user has an online ID, which
can be used by other users to refer to him/her in a
post. Users are both opinion holders and opinion
targets. For example, User 1 below expresses a neg-
ative opinion towards another user in the following
snippet.
User 1: Actually, I have to disagree with you.
Aspect: We use topic aspect or aspect to refer to an
opinion target that is related to the topic under dis-
cussion. For example, when debating about whether
one should vote for Obama, people may express
402
opinions on targets such as ?President Obama? and
?Republican party,? as shown in the following snip-
pets. These aspects are all related to Obama?s pres-
idential campaign. As we will explain later, the as-
pects we consider are named entities and frequent
noun phrases.
User 2: Americans should vote for President Obama be-
cause he picks good corporations as winners.
User 3: I simply point out how absolutely terrible the Re-
publican party is.
Polarity Score: A sentiment polarity score is a
real number between 0 and 1, where 0 indicates a
completely negative opinion and 1 indicates a com-
pletely positive opinion.
User-User Opinion Matrix: The opinions ex-
tracted from posts between users are represented by
a user-user opinion matrix S, where entry si,j is a
polarity score between the i-th user and the j-th user.
We assume that the polarity scores are symmetric.
User-Aspect Opinion Matrix: The opinions held
by different users on the various topic aspects are
represented by a user-aspect opinion matrix R,
where entry ri,k is a polarity score indicating the i-th
user?s opinion towards the k-th aspect.
Given the matrices S and R, we perform proba-
bilistic matrix factorization to derive a low-rank vec-
tor representation for users and aspects such that if
the polarity score between two users or a user and
an aspect is high, the dot product between the corre-
sponding two vectors is also high.
In Section 4, we will explain in detail how we
identify topic aspects from a discussion thread and
how we obtain polarity scores from posts. In Sec-
tion 5, we will present the details of our probabilistic
matrix factorization model.
4 Construction of Opinion Matrices
The opinion matrices are constructed from a single
forum thread discussing some controversial topic.
4.1 Aspect Identification
As we have pointed out, there are two kinds of opin-
ion targets, namely users and aspects. Users are
clearly defined and can often be identified in posts
by their IDs or second person pronouns. For aspects,
however, there is not a pre-defined set. We observe
that these topic aspects are usually named entities
or noun phrases frequently mentioned. We therefore
use the OpenNLP toolkit1 to perform chunking and
obtain noun phrases and the Standford NER tagger2
to identify named entities from the posts.
Some of the candidate aspect phrases identified
above actually refer to the same actual aspect, e.g.
?Obama voter,? ?Obama voters? and ?the Obama
voter.? We remove stop words from each candidate
phrase and use the WordNet by Miller (1995) to ob-
tain the lemma of each word such that we can nor-
malize the candidate aspect phases to some extent.
Finally, to select salient aspects for a given discus-
sion topic, we count the number of times each candi-
date aspect has been expressed a positive or negative
opinion on by all users, and select those candidate
aspects which have opinion expressions from at least
M users. We set M to 2 in our experiments. Fig-
ure 1 shows the top salient aspects for the thread on
?Will you vote for Obama?? We acknowledge there
are still duplicate aspects in the results like ?Repub-
lican Party? and ?GOP?. To normalize these aspects,
some additional information such as Wikipedia en-
tries and Google snippets may be considered. We
will study this problem in our future work.
4.2 Opinion Expression Identification
Our next step is to identify candidate opinion expres-
sions. This problem has been studied in Hu and Liu
(2004), Popescu and Etzioni (2005), and Hassan
and Radev (2010). Based on previous work, we do
the following. We first combine three popular sen-
timent lexicons to form a single sentiment lexicon:
the lexicon used in Hu and Liu (2004), MPQA Sub-
jectivity Lexicon by Wilson et al (2005) and Senti-
WordNet by Baccianella et al (2010). Our final sen-
timent lexicon contains 15,322 negative expressions
and 10,144 positive expressions. We then identify
candidate opinion expressions by searching for oc-
currences of words in this lexicon in the posts.
4.3 Opinion Relation Extraction
Given a post that contains an aspect and an opin-
ion expression, we still need to determine whether
the opinion expression is used to describe the as-
pect. This is a relation extraction problem. We use a
supervised learning approach based on dependency
1http://opennlp.apache.org/
2http://nlp.stanford.edu/ner/index.shtml
403
0
20
40
60
80
100
OBAM
A BUSHAMER
ICA PALIN
REPU
BLICA
N
CONG
RESSTAX_
CUT
AME
RICA
N
CLIN
TONMCC
AIN
TEA_
PART
Y IRAQ
SARA
H_PA
LIN
PRES
IDEN
T_OB
AMAREAG
AN
RON_
PAUL
ECON
OMIC
_POL
ICY
AFGH
ANIS
TANCART
ER FOX
HEAL
TH_C
ARE
NATI
ONAL
_DEB
T
DEM
OCRA
T GOP
MIDD
LE_C
LASS
OBAM
A_AD
MINIS
TRAT
ION
REPU
BLICA
N_PA
RTY
TAX_
BREA
K
WAS
HING
TON
FEDE
RAL_
GOVE
RNM
ENT
HEAL
TH_C
ARE_
REFO
RMHITLE
R
IRAQ
_WAR
WAL
L_ST
REET
Figure 1: Salient aspects and number of users who express opinions on them in the thread ?Will you vote for Obama??
ID Dependency path rule Example
R1 ADJOP ? amod? NTR I simply point out how terrible REPUBLICAN PARTY is.
R2 ADJOP ? nsubj ? NTR BUSH is even more reasonable for tax hike than Obama.
R3 VOP ? dobj ? NTR I would never support OBAMA.
R4 VOP ? prep ? ? NTR I?ll vote for OBAMA.
R5 VOP ? nsubjpass? NTR DEMOCRATIC PARTY are ultimately corrupted by love of money.
R6 NOP ? dobj ? V ? nsubj ? NTR PAKISTAN is increasing terrorist threat.
R7 ADJOP ? amod? N ? nsubj ? NTR OBAMA was a top scorer for occidental college.
R8 ADVOP ? advmod? V ? nsubj ? NTR OBAMA is smarter than people.
Table 1: Examples of frequent dependency path rules in our training data. OP and TR refer to the opinion and the
target. The opinion words are in italic and the aspect words are in uppercase.
paths. Previous work by Mintz et al (2009), and Qiu
et al (2009) has shown that the shortest path be-
tween a candidate opinion aspect and a candidate
opinion expression in the dependency parse tree can
be effective in extracting opinion relations. We use
the Stanford Parser from Klein and Manning (2003)
to obtain the dependency parse trees for each sen-
tence in the posts and then get the dependency paths
between each pair of candidate aspect and opinion
expression. We use dependency relations and POS
tags of nodes along the path to represent a depen-
dency path. Given a set of training sentences (we
use the one from Wu et al (2009)), we can get a set
of dependency path rules based on their frequencies
in the training data. Table 1 shows the frequent de-
pendency path rules in our training data.
When a pair of aspect and opinion expression is
identified to be related, we use the polarity of the
opinion expression to label the relation. Finally,
given a pair of users, we use the percentage of pos-
itive interactions between them over all subjective
interactions (i.e. interactions with either positive or
negative opinions) as extracted from their exchanged
posts as the sentiment polarity score between the
two users, regardless of the reply-to direction of
the posts. Similarly, given a user and an aspect,
we also use the percentage of positive opinion re-
lations extracted as the sentiment polarity score be-
tween them. Thus the user-user opinion matrix and
the user-aspect opinion matrix are constructed. If
there is no subjective interaction detected between
two users or between a user and an aspect, the cor-
responding entry in the matrix is left empty. We will
see later that empty entries in the matrices are not
used in the probabilistic matrix factorization step.
5 Probabilistic Matrix Factorization
As we have pointed out earlier, a problem with the
matrices extracted as described in Section 4 is that
the matrices are sparse, i.e. many entries are empty.
For the data set we use, we find that around 87% of
entries in the user-user opinion matrix and around
90% of entries in the user-aspect opinion matrix are
empty. In this section, we describe how we use
Probabilistic Matrix Factorization (PMF) to repre-
sent users and aspects in a latent factor space and
thus generalize the user preferences.
Our model is almost a direct application of proba-
404
bilistic matrix factorization from Salakhutdinov and
Mnih (2008), originally proposed for recommender
systems. The main difference is that the user-user
opinion polarity scores are symmetric. Our model is
also similar to the one used by Ma et al (2008). We
describe our model as follows.
We assume that there are K latent factors with
which both users and aspects can be represented. Let
ui ? RK denote the vector in the latent factor space
for the i-th user, and ak the vector for the k-th aspect.
Recall that the opinions extracted from posts be-
tween users are represented by a user-user opinion
matrix S, and the opinions held by different users on
the various topic aspects are represented by a user-
aspect opinion matrix R. We assume that the polar-
ity scores si,j between the i-th and the j-th users and
ri,k between the i-th user and the k-th aspect in the
two matrices S and R are generated in the following
way:
p(si,j |ui, uj , ?
2
1) = N (si,j |g(u
T
i uj), ?
2
1),
p(ri,k|ui, ak, ?
2
2) = N (ri,k|g(u
T
i ak), ?
2
2),
where ?21 and ?
2
2 are variance parameters, g(?) the
logistic function, and N (?|?, ?2) is the normal dis-
tribution with mean ? and variance ?2.
We can see that with this generative assumption,
if two users are similar in terms of their dot product
in the latent factor space, then they are more likely
to have positive interactions as extracted from their
textual exchanges. Similarly, if a user and an aspect
are similar, then the user is more likely to express a
positive opinion on the aspect in his/her posts. The
latent factors can therefore encode user preferences
and similarity between two users in the latent factor
space reflects whether they share similar viewpoints.
We also place the following prior over ui and ak:
p(ui|?
2
U ) = N (ui|~0, ?
2
UI),
p(ak|?
2
A) = N (ak|~0, ?
2
AI),
where ?2U and ?
2
A are two variance parameters for
users and aspects, respectively, and I is the identify
matrix.
Figure 2 shows the plate notation for the genera-
tive model.
Let U be aK?U matrix containing the vectors ui
for allU users, andA be anK?Amatrix containing
Figure 2: Probabilistic matrix factorization model on
opinion matrices.
the vectors ak for all A aspects. To automatically
learn U andA, we minimize the following objective
function:
L(U ,A,S,R)
=
1
2
U?
i=1
A?
k=1
I(ri,k)(ri,k ? g(uTi ak))2
+
?1
2
U?
i=1
U?
j=1
I(si,j)(si,j ? g(uTi uj))2
+
?U
2
||U||2F +
?A
2
||A||2F , (1)
where ? = ?
2
1
?22
, ?U =
?21
?2U
, and ?A =
?21
?2A
, I(s) is
an indicator function which equals 1 when s is not
empty and otherwise 0.
To optimize the objective function above, we can
perform gradient descent on U and A to find a local
optimum point. The derivation is similar to Ma et al
(2008).
Degenerate Versions of the Model
We refer to the complete model described above
as PMF-UOM (PMF model based on User Opinion
Matrices). PMF-UOM has the following two degen-
erate versions by considering either only the user-
user opinion matrix or only the user-aspect opinion
matrix.
PMF-UU: In this degenerate version of the model,
we use only the user-user opinion matrix to learn the
latent factor representation. Specifically, the objec-
tive function is modified such that we drop the sum
405
of the square errors involving R and the regularizer
on A.
PMF-UA: In this degenerate version of the model,
we use only the user-aspect opinion matrix to learn
the latent factor representation. Specifically, the ob-
jective function is modified such that we drop the
sum of the square errors involving S.
6 Experiments
In this section, we present our experiments that eval-
uate our model.
6.1 Data Set and Experiment Settings
The data set we use comes from Abu-Jbara et al
(2012) and Hassan et al (2012). The data set
contains a set of discussion threads collected from
two political forums (Createdebate3 and Politicalfo-
rum4) and one Wikipedia discussion session. We
randomly select 6 threads from the original data set
to evaluate our model. Some details of the data we
use are listed in Table 2.
ID topic #sides #sentences #users
DS1 Vote for Obama 2 12492 197
DS2 Abortion Banned 6 3844 70
DS3 Profile Muslims 4 2167 69
DS4 England and USA 6 2030 62
DS5 Tax Cuts 2 1193 26
DS6 Political Spectrum 7 1130 50
Table 2: Some statistics of the data sets.
In our experiments, for the PMF-based methods,
we set the number of latent factors to be 10 as we
do not observe big difference when vary the latent
factor size from 10 to 50. For the other parame-
ters, we select the optimal setting for each thread
based on the average of 50 runs. ?U is chosen
from {0.1, 0.01}, ?A from {0.01, 0.001} and ? from
{1, 0.1}.
6.2 Relation Polarity Prediction
The first task we use to evaluate our model is to pre-
dict the polarity of interactions between two users.
Different from Hassan et al (2012), however, we
are not using this task to evaluate the accuracy of
sentiment analysis from text. Our experimental set-
ting is completely different in that we do not make
3www.createdebate.com
4www.politicalforum.com
use of the text exchanges between the two users but
instead use their interactions with other users or as-
pects. The purpose is to test the effectiveness of col-
laborative filtering.
Experimental Setting: The experiments are set up
in the following way. Given a pair of users i and j
who have directly exchanged posts, i.e. si,j is not
empty, we first hide the value of si,j in the matrix S.
Let the altered matrix be S?(i,j). We then use S?(i,j)
instead of S in the learning process as described in
Section 5 to learn the latent factor representation.
Let u?i and u?j denote the learned latent vectors for
user i and user j. We predict the polarity of relation
between i and j as follows:
s?i,j =
{
1 if g(u?Ti u?j) > 0.5,
0 otherwise,
where g(?) is the logistic function to convert the dot
product into a value between 0 and 1.
To judge the quality of the predicted polarity s?i,j ,
we could compare it with si,j . But since si,j itself is
predicted from the textual exchanges between i and
j, it is not the ground truth. Instead, we ask two hu-
man annotators to assign the true polarity label for
user i and user j by reading the textual exchanges
between them and judging whether they are friends
or foes in the discussion thread. The annotators are
asked to assign a score of 0 (indicating a negative
relation), 0.5 (indicating a neutral relation) or 1 (in-
dicating a positive relation). The lowest agreement
score based on Cohen?s kappa coefficient among the
6 threads we use is 0.56, showing fair to good agree-
ment. As ground truth, we set the final polarity score
to 1 if the average score of the two annotators is
larger than 0.5 and 0 otherwise.
We compare the PMF-based methods with two
majority baselines: MBL-0 always predicts negative
relations for all the user pairs (assuming most rela-
tions are negative) and MBL-1 always predicts posi-
tive relations (assuming most relations are positive).
We use MAE (mean absolute error) and RMSE
(root mean square error) as defined below as perfor-
mance metrics:
MAE =
?
i,j |s?i,j ? li,j |
N
,
RMSE =
??
i,j(s?i,j ? li,j)
2
N
,
406
0.2
0.4
0.6
0.8
1.0
DS1 DS2 DS3 DS4 DS5 DS6
MA
E
MB-1
MB-0
PMF-UU
PMF-UA
PMF-UOM
Figure 3: Comparing all the methods in terms of MAE.
0.2
0.4
0.6
0.8
1.0
DS1 DS2 DS3 DS4 DS5 DS6
RM
SE
MB-1
MB-0
PMF-UU
PMF-UA
PMF-UOM
Figure 4: Comparing all the methods in terms of RMSE.
where N is the total number of user pairs we test,
and li,j is the ground truth polarity score between
user i and user j.
Results: We show the results of our model and of
PMF-UU and PMF-UA in terms of MAE in Figure 3
and RMSE in Figure 4. The MAE values range be-
tween 0.31 and 0.44 except for DS5, which has a
higher error rate of 0.53. The results show that even
without knowing the textual exchanges between two
users, from their interactions with other users and/or
with topic aspects, we can still infer the polarity of
their relation with decent accuracy most of the time.
The results also show the comparison between our
model and the competing methods. We can see that
overall the complete model (PMF-UOM) performs
better than the two degenerate models (PMF-UU
and PMF-UA). The differences are statistically sig-
nificant at the 5% level without considering DS5, as
indicated by a 2-tailed paired t-test. Comparing to
the majority baselines, our model significantly out-
performs MBL-1 at 1% significance level while out-
performs MBL-0 on all the data sets except DS5. A
close examinations shows DS5 has very unbalanced
relations (around 83% of relations are negative). Ex-
cept for the unbalanced data set, our model has rea-
sonably good performance.
6.3 Subgroup Detection
The second task we study is the problem of detecting
ideological subgroups from discussion threads. The
original data set has been labeled with the ground
truth for this problem, that is, for each thread the
number of viewpoints is known and the viewpoint
held by each user is labeled. A subgroup is defined
as a set of users holding the same viewpoint.
Experimental Setting: Through this second exper-
iment, we would like to verify the hypothesis that
using the learned latent factor representation U for
users, we can better detect subgroups than directly
using the opinion matrices S and R. For all the
methods we compare, we first construct a feature
vector representation for each user. We then apply
K-means clustering to group users. The number of
clusters is set to be the true number of viewpoints
for each thread. The different methods are described
below:
? PMF-based methods: We simply use the
learned latent vectors u?i after optimizing the
objective function as the feature vectors to rep-
resent each user.
? BL-1: This is our own implementation to sim-
ulate the method by Abu-Jbara et al (2012).
Here each user is represented by a (3 ? (U +
A))-dimensional vector, where U is the num-
ber of users and A is the number of aspects,
i.e. (U +A) is the total number of opinion tar-
gets. For each opinion target, there are 3 di-
mensions in the feature vector, corresponding
to the number of positive, neutral and negative
opinion expressions towards the target from the
online posts.
? BL-2: BL-2 is similar to BL-1 except that we
only use a (U+A)-dimensional vector to repre-
407
sent each user. Here for each opinion target, we
directly use the corresponding sentiment polar-
ity score si,j or ri,j from the matrix S orR. For
empty entries in S andR, we use a score of 0.5.
We use Purity (the higher the better), Entropy (the
lower the better) and Rand Index (the higher the bet-
ter) to evaluate the performance of subgroup detec-
tion (Manning et al, 2008). We further use Accuracy
obtained by choosing the best alignment of clusters
with the ground truth class labels and computing the
percentage of users that are ?classified? correctly.
Results: We first give an overview of the perfor-
mance of all the methods on the task. We show the
average performance of the methods on all the data
sets in Figure 5. Overall, our model has a better per-
formance than all the competing methods.
0.4
0.6
0.8
1.0
Purity Entropy Accuracy RandIndex
BL-1
BL-2
PMF-UU
PMF-UA
PMF-UOM
Figure 5: An overview of the average performance of all
the methods on the 6 threads.
We present all the results in Table 3. We per-
form 2-tailed paired t-test on the results. We find
that PMF-UOM outperforms all the other methods
in terms of RandIndex at 5% significance level and
outperforms other methods in terms of Purity and
Entropy at 10% significance level. Furthermore,
the PMF-UOM model outperforms its degenerative
models PMF-UU and PMF-UA at 10% significance
level in terms of all the measures.
We observe that PMF-UOM achieves the best per-
formance in terms of all the measures for almost
all threads. In particular, comparison with BL-1
and BL-2 shows that collaborative filtering can gen-
eralize the user preferences and help better group
the users based on their viewpoints. The fact that
PMF-UOM outperforms both PMF-UU and PMF-
UA shows that it is important to consider both user-
user interactions and user-aspect interactions.
The Effects of Cluster Size: To test the effect of the
number of clusters on the experiment result, we vary
the number of clusters from 2 to 10 in all methods.
We find that all methods tend to achieve better re-
sults when the number of clusters equals the ground
truth cluster size. Overall, our method PMF-UOM
shows a better performance than the other four meth-
ods when the number of clusters changes, which in-
dicates the robustness of our method.
BL-1 BL-2 PMF-UU PMF-UA PMF-UOM
DS1
P 0.61 0.61 0.61 0.61 0.62
E 0.96 0.96 0.94 0.95 0.94
A 0.59 0.59 0.55 0.57 0.60
R 0.51 0.51 0.50 0.51 0.52
DS2
P 0.53 0.63 0.64 0.61 0.68
E 1.17 1.22 1.14 1.09 0.99
A 0.47 0.53 0.48 0.47 0.50
R 0.50 0.50 0.56 0.56 0.58
DS3
P 0.66 0.68 0.62 0.60 0.68
E 1.05 1.01 1.06 1.07 0.94
A 0.61 0.63 0.48 0.47 0.58
R 0.50 0.52 0.53 0.53 0.57
DS4
P 0.64 0.64 0.66 0.65 0.70
E 0.92 0.94 0.90 0.91 0.85
A 0.59 0.64 0.62 0.62 0.68
R 0.49 0.52 0.52 0.51 0.56
DS5
P 0.86 0.86 0.86 0.86 0.86
E 0.56 0.56 0.49 0.48 0.38
A 0.70 0.70 0.57 0.60 0.71
R 0.52 0.52 0.43 0.45 0.56
DS6
P 0.50 0.50 0.60 0.60 0.68
E 1.35 1.35 1.03 1.04 0.79
A 0.40 0.30 0.53 0.54 0.64
R 0.53 0.53 0.68 0.68 0.74
Table 3: Results on subgroup detection on all the 6
threads. P, E, A and R refer to Purity, Entropy, Accuracy
and RandIndex, respectively.
7 Conclusions
In this paper, we studied how to use probabilistic
matrix factorization, a common technique for col-
laborative filtering, to improve relation mining from
online discussion forums. We first applied senti-
ment analysis to extract user-user opinions and user-
aspect opinions from forum posts. The extracted
opinions form two opinion matrices. We then ap-
plied probabilistic matrix factorization using these
408
two matrices to discover a low-rank latent factor
space which aims to better generalize the users? un-
derlying preferences and indicate user similarities
based on their viewpoints. Using a data set with 6
discussion threads, we showed that the learned la-
tent vectors can be used to predict the polarity of
user relations well without using the users? direct
interaction data, demonstrating the effectiveness of
collaborative filtering. We further found that for the
task of subgroup detection, the latent vectors gave
better performance than using the directly extracted
opinion data, again showing that collaborative fil-
tering through probabilistic matrix factorization can
help address the sparseness problem in the extracted
opinion matrices and help improve relation mining.
Our current work mainly focuses on the user opin-
ion matrices. As future work, we would like to ex-
plore how to incorporate textual contents without
opinionated expressions. One possible way is to
consider the combination of matrix factorization and
topic modeling as studied by Wang and Blei (2011)
where we can use topic modeling to study textual
contents.
Acknowledgments
We thank the reviewers for their valuable comments
on this work.
References
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir R. Radev. 2012. Subgroup detection in
ideological discussions. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 399?409.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?06, pages 431?439, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Pradeep Dasigi, Weiwei Guo, and Mona T. Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude using
textual latent semantics. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 65?69.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ?10, pages 395?403,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by
modeling positive and negative relations among par-
ticipants. In Proceedings of the 2012 EMNLP, pages
59?70.
John Hopcroft, Tiancheng Lou, and Jie Tang. 2011. Who
will follow you back?: reciprocal relationship predic-
tion. In Proceedings of the 20th ACM international
conference on Information and knowledge manage-
ment, pages 1137?1146.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the 10th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177.
Noriaki Kawamae. 2011. Predicting future reviews: sen-
timent analysis models for collaborative filtering. In
Proceedings of the fourth ACM international confer-
ence on Web search and data mining, WSDM ?11,
pages 605?614.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, ACL ?03, pages 423?430, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010. Predicting positive and negative links in online
social networks. In Proceedings of the 19th interna-
tional conference on World wide web, pages 641?650.
Cane Wing-Ki Leung, Stephen Chi-Fai Chan, Fu-Lai
Chung, and Grace Ngai. 2011. A probabilistic rat-
ing inference framework for mining user preferences
from reviews. World Wide Web, 14(2):187?215.
David Liben-Nowell and Jon Kleinberg. 2003. The link
prediction problem for social networks. In Proceed-
ings of the twelfth international conference on Infor-
mation and knowledge management.
Hao Ma, Haixuan Yang, Michael R. Lyu, and Irwin King.
2008. Sorec: Social recommendation using proba-
bilistic matrix factorization. In Proc. of ACM interna-
tional conference on Information and knowledge man-
agement.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, July.
409
George A. Miller. 1995. Wordnet: A lexical database
for english. Communications of the ACM, Vol. 38, No.
11:39?41.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2, ACL
?09, pages 1003?1011, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Yashar Moshfeghi, Benjamin Piwowarski, and Joe-
mon M. Jose. 2011. Handling data sparsity in collabo-
rative filtering using emotion and semantic based fea-
tures. In Proceedings of the 34th international ACM
SIGIR conference on Research and development in In-
formation Retrieval, SIGIR ?11, pages 625?634.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?
135, January.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 339?346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1199?1204, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Ruslan Salakhutdinov and Andriy Mnih. 2008. Prob-
abilistic matrix factorization. In Advances in Neural
Information Processing Systems, volume 20.
Chong Wang and David M. Blei. 2011. Collaborative
topic modeling for recommending scientific articles.
In Proceedings of the 17th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 448?456.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3
- Volume 3, EMNLP ?09, pages 1533?1541, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
410
Proceedings of NAACL-HLT 2013, pages 1031?1040,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Latent Variable Model for Viewpoint Discovery from
Threaded Forum Posts
Minghui Qiu
School of Information Systems
Singapore Management University
Singapore
minghui.qiu.2010@smu.edu.sg
Jing Jiang
School of Information Systems
Singapore Management University
Singapore
jingjiang@smu.edu.sg
Abstract
Threaded discussion forums provide an im-
portant social media platform. Its rich user
generated content has served as an important
source of public feedback. To automatically
discover the viewpoints or stances on hot is-
sues from forum threads is an important and
useful task. In this paper, we propose a novel
latent variable model for viewpoint discov-
ery from threaded forum posts. Our model is
a principled generative latent variable model
which captures three important factors: view-
point specific topic preference, user identity
and user interactions. Evaluation results show
that our model clearly outperforms a number
of baseline models in terms of both clustering
posts based on viewpoints and clustering users
with different viewpoints.
1 Introduction
Threaded discussion forums provide an important
social media platform that allows netizens to express
their opinions, to ask for advice, and to form on-
line communities. In particular, responses to major
sociopolitical events and issues can often be found
in discussion forums, which serve as an impor-
tant source of public feedback. In such discussion
threads, we often observe heated debates over a con-
troversial issue, with different sides defending their
viewpoints with different arguments. For example,
after the presidential debate between Barack Obama
and Mitt Romney, there were heated discussions in
online forums like CreateDebate1 where some peo-
ple expressed their support for Obama while some
1http://www.createdebate.com/
others have their opposition to him. For a user who
is not closely following an event or issue, instead of
going through all the existing posts in a long thread,
she may want to quickly get an overview of the ma-
jor viewpoints and arguments given by the different
sides. For policy makers who want to obtain pub-
lic feedback on social issues from social media, it is
also desirable to automatically summarize the view-
points on an issue from relevant threads. In this pa-
per, we study the problem of modeling and discov-
ering different viewpoints in forum threads.
Recently there has been some work on finding
contrastive viewpoints from text. The model pro-
posed by Paul et al (2010) assumes viewpoints and
topics are orthogonal dimensions. Another model
proposed by Fang et al (2012) assumes that docu-
ments are already grouped by viewpoints and it fo-
cus on identifying contrastive viewpoint words un-
der the same topic. However, these existing stud-
ies are not based on interdependent documents like
threaded forum posts. As a result, at least two im-
portant characteristics of threaded forum data are not
considered in these models. (1) User identity: The
user or publisher of each forum post is known, and
a user may publish several posts in the same thread.
Since the same user?s opinion on an issue usually re-
mains unchanged, posts published by the same user
are likely to contain the same viewpoint. (2) User
interactions. A thread is like a conversation, where
users not only directly comment on the issue under
discussion but also comment on each other?s posts.
Users having different viewpoints may express their
disagreement or even attack each other while users
having the same viewpoint often support each other.
1031
The interaction expressions in forum posts may help
us infer the relation between two users and subse-
quently infer the viewpoints of the corresponding
posts.
In this paper, we propose a novel latent variable
model for viewpoint discovery from threaded forum
posts. Our model is based on the following obser-
vations: First, posts with different viewpoints tend
to focus on different topics. To illustrate this point,
we first apply the Latent Dirichlet Allocation (LDA)
model (Blei et al, 2003) on a thread about ?will
you vote Obama? and obtain a set of topics. This
thread comes from a data set that has each user?s
viewpoint annotated. Using the ground truth view-
point labels, we group all posts published by users
with viewpoint 1 (or viewpoint 2) and compute the
topic proportions. The two topic distributions are
shown in Figure 1. We can see that indeed the two
viewpoints each have some dominating topics. Our
second observations is that the same user tends to
hold the same viewpoint. In our model, we use a
user-level viewpoint distribution to capture this ob-
servation, and the experiments show that it works
better than assuming a global viewpoint distribution.
Third, users with the same viewpoint are likely to
have positive interactions while users with different
viewpoints tend to have negative interactions. Using
a sentiment lexicon, we can first predict the polarity
of interaction expressions. We then propose a novel
way to incorporate this information into the latent
variable model. In summary, we capture the three
observations above in a principled generative latent
variable model. We present the details of our model
in Section 3.
Figure 1: Topic distributions of two viewpoints for the
thread ?will you vote Obama?? The dotted line is the
average topic probability.
We use two tasks to evaluate our model. In the
first task, we evaluate how well posts with differ-
ent viewpoints are separated. In the second task, we
evaluate how well our model is able to group users
with different viewpoints. For both tasks, we com-
pare our model with an existing model as well as
a few degenerate versions of our model. The re-
sults show that our model can clearly outperform the
baselines in terms of three evaluation metrics. The
experiments are presented in Section 5.
The contributions of our work are threefold: (1)
We identify the importance of using user interac-
tions to help infer viewpoints in forum posts. (2) We
propose a principled latent variable model to jointly
model topics, viewpoints and user interactions. (3)
We empirically verify the validity of the three as-
sumptions in our model using real data sets.
2 Related Work
There are a few different lines of work that are re-
lated to our work. For discovering different view-
points from general text, Paul et al (2010) used the
model proposed by Paul and Girju (2010) to jointly
model topics and viewpoints. They assume these
two concepts are orthogonal and they do not con-
sider user identity. In comparison, our model has
the notion of topics and viewpoints, but we explicitly
model the dependency of topics on viewpoints, i.e.
we assume each viewpoint has a topic distribution.
We also consider author identities as an important
factor of our model. Fang et al (2012) proposed a
model that also combines topics and viewpoints. But
they assume that documents are already grouped by
viewpoints, which is not the case for forum posts.
Therefore, their model cannot be directly applied to
forum posts.
There has also been some work on finding view-
points from social media. Somasundaran and Wiebe
(2010) studied how to identify stances in online de-
bates. They used a supervised approach for classi-
fying stances in ideological debates. In comparison,
our model is an unsupervised method. The same au-
thors proposed an unsupervised method which relies
on associations of aspects with topics indicative of
stances mined from the Web for the task (Somasun-
daran and Wiebe, 2009). In contrast, our model is
also an unsupervised one but we do not rely on any
external knowledge.
Part of our work is related to detecting agree-
ment/disagreement from text. For this task, nor-
1032
mally supervised methods are used (Galley et al,
2004; Abbott et al, 2011), which require sufficient
labeled training data. In our work, since we deal
with different languages, we use a lexicon-based
approach that does not need training data. Re-
cently, Mukherjee and Liu (2012) proposed an un-
supervised model to extract different types of ex-
pressions including agreement/disagreement expres-
sions. However, our focus is not to detect agree-
ment/disagreement expressions but to model the
interplay between agreement/disagreement expres-
sions and viewpoints. The work by Mukherjee and
Liu (2012) can potentially be combined with our
model.
Another line of related work is subgroup detec-
tion, which aims to separate users holding different
viewpoints. This problem has recently been stud-
ied by Abu-Jbara and Radev (2012), Dasigi et al
(2012), Abu-Jbara et al (2012) and Hassan et al
(2012), where a clustering based approach is used.
Lu et al (2012) studied both textual content and
social interactions to find opposing network from
online forums. In our experiments we show that
our model can also be used for subgroup detection,
but meanwhile we also directly identify viewpoints,
which is not the goal of existing work on subgroup
finding or opposing network extraction.
3 Model
3.1 Motivation
Before we formally present our latent variable
model for viewpoint discovery, let us first look at the
assumptions we would like to capture in the model.
Viewpoint-based topic distribution: The first as-
sumption we have is that different viewpoints tend
to touch upon different topics. This is because to
support a viewpoint, users need to provide evidence
and arguments, and for different viewpoints the ar-
guments are likely different. To capture this assump-
tion, in our model, we let each viewpoint have its
own distribution of topics. Given the viewpoint of
a post, the hidden topic of each word in the post is
chosen according to the corresponding topic distri-
bution associated with that viewpoint.
User identify: The second assumption we have
is that the same user tends to talk from the same
viewpoint, although there are also users who do not
clearly have a viewpoint. In our model, we assume
that there is a user-level viewpoint distribution. For
each post by a user, its viewpoint is drawn from the
corresponding viewpoint distribution.
User interaction: An important difference between
threaded forum posts and regular document collec-
tions such as news articles is that posts in the same
thread form a tree structure via the ?reply-to? re-
lations. Many reply posts start with an expression
that comments on a previous post or directly ad-
dresses another user. These interaction expressions
may carry positive or negative sentiment, indicating
an agreement or a disagreement. For example, Ta-
ble 1 shows the interaction expressions from a few
sample posts with words such as ?correct,? ?agree,?
and ?delusional,? implying the polarity of the inter-
action expressions. The polarity of these interaction
expressions can help us infer whether two posts or
two users hold the same viewpoint or not. In our
model, we assume that the polarity of each interac-
tion expression can be detected. Details of how we
perform this detection are in Section 3.4.
Post
+
You are correct. Obama got into office w/ everything ? ? ?
I agree with your post Dan. Obama is so ? ? ?
?
Most of your post is delusional, especially the part ? ? ?
Are you freaking nutz? Palin is a BIMBO!
Table 1: Sample posts with positive (+) and negative(?)
interactions.
While the way to capture the first two assump-
tions discussed above is fairly standard, modeling
user interactions is something new. In our model,
we assume that the polarity of an interaction expres-
sion is generated based on the viewpoint of the cur-
rent post and the viewpoint of post(s) that the current
post replies to. The intuition is that if the viewpoints
are the same, we are more likely to see a positive in-
teraction whereas if the viewpoints are different we
are more likely to see a negative interaction.
3.2 Model description
We use the following notation to represent our data.
We consider a set of forum posts published by U dif-
ferent users on the same event or issue, where user
u (1 ? u ? U ) has published Nu posts. Let wu,n,l
(1 ? l ? Lu,n ) denote the l-th word in the n-th
post by user u, where Lu,n is the number of words
1033
in the n-th post by user u. wu,n,l is represented by
an index between 1 and V where V is the vocabu-
lary size. Furthermore, we assume that some of the
posts have user interaction expressions, where the
polarity of the expression is known. Without loss of
generality, let su,n ? {0, 1} denote the polarity of
the interaction expression of the n-th post by user
u. In addition, for each post that has an interaction
expression, we assume we also know the previous
post(s) it replies to. (In the case when the current
post replies to a user, we assume all that user?s ex-
isting posts are being replied to.) We refer to these
posts as the parent posts of the current post.
We assume that there are T topics where each
topic is essentially a word distribution, denoted as
?t. We also assume that there are Y different view-
points expressed in the collection of posts. For most
controversial issues, Y can be set to 2. Each view-
point y has a topic distribution ?y over the T top-
ics. While these T topics are meant to capture the
topical differences between viewpoints, since these
viewpoints are all about the same issue, there are
also some words commonly used by different view-
points. We therefore introduce a background topic
?B to capture these words. Finally, each user u has
a distribution over the Y viewpoints, denoted as ?u.
Figure 2: Plate notation of the Joint Viewpoint-Topic
Model with User Interaction (JVTM-UI). The dotted cir-
cle for Y means the variables represented by Y are not
new variables but a subset of the y variables.
Figure 2 shows the plate notation of the complete
model. We assume the following generation process
in our model. When user u generates her n-th post,
she first samples a viewpoint from ?u. Let this view-
point be represented by a hidden variable yu,n. For
the l-th word in this post, she first samples an in-
dicator variable xu,n,l from a Bernoulli distribution
parameterized by pi. If xu,n,l = 0, then she draws
wu,n,l from ?B . Otherwise, she first samples a topic,
denoted as zu,n,l, according to ?yu,n , and then draws
wu,n,l from ?zu,n,l .
Furthermore, if this post is a reply to a previous
post or another user, she may first comment on the
parent post(s). The polarity of the interaction ex-
pression in the post is dependent on the viewpoint
yu,n and the viewpoints of the previous post(s). Let
us use Yu,n to denote the set of y variables associ-
ated with the parent posts of the current post. The
user draws su,n according to following distribution:
p(su,n = 1|yu,n,Yu,n, ?) =
?
y??Yu,n
I(yu,n == y?) + ?
|Yu,n|+ 2?
,
p(su,n = 0|yu,n,Yu,n, ?) = 1? p(su,n = 1|yu,n,Yu,n, ?), (1)
where I(?) is 1 if the statement inside is true and 0
otherwise, and ? > 0 is a smoothing parameter.
Finally, we assume that ?B , ?t, ?u, ?y and pi all
have some uniform Dirichlet priors.
3.3 Inference
We use collapsed Gibbs sampling to estimate the
model parameters. In the initialization stage of
Gibbs sampling, for a reply post to a recipient, we
initialize its corresponding reply polarity s accord-
ing to all the labeled polarity of interaction words.
Specifically, if the majority of labeled interaction
words are positive, we set s = 1, otherwise we set
s = 0.
Let Y denote the set of all y variables, and
Y?(u,n) denote Y excluding yu,n. Similar notation
is used for the other variables. We sample yu,n using
the following formula.
p(yu,n = k|Y?(u,n),Z,S,X, ?, ?, ?)
?
p(yu,n = k,Y?(u,n)|?)
p(Y?(u,n)|?)
?
p(Z|yu,n = k,Y?(u,n),X, ?)
p(Z?(u,n)|Y?(u,n),X?(u,n), ?)
?p(S|yu,n = k,Y?(u,n), ?)
=
Cku,?n + ?
C(?)u,?n + Y ?
?
?T
t=1
?Ctu,n?1
a=0 (C
t
k,?(u,n) + ? + a)
?C(?)u,n?1
b=0 (C
(?)
k,?(u,n) + T? + b)
?p(S|yu,n = k,Y?(u,n), ?). (2)
Here all Cs are counters. Cku,?n is the number of
times we observe the viewpoint k from u?s posts,
excluding the n-th post, based on Y?(u,n). C
t
u,n is
1034
the number of times we observe topic t from user
u?s n-th post, based on Zu,n. And Ctk,?(u,n) is the
number of times we observe topic t associated with
viewpoint k, excluding user u?s n-th post. Note that
we need X to know which words are assigned to
the background topic so we can exclude them for
Ctu,n and C
t
k,?(u,n). C
(?)
u,?n is the number of times we
observe any viewpoint from u?s posts, excluding the
n-th post. C(?)u,n and C
(?)
k,?(u,n) are defined similarly.
The last term is further expanded as follows:
p(S|yu,n = k,Y?(u,n), ?) = p(su,n|yu,n = k,Yu,n, ?)
?p(S?(u,n)|yu,n = k,Y?(u,n), ?). (3)
Here p(su,n|yu,n = k,Yu,n, ?) is computed ac-
cording to Eqn. (1). For the latter term, we need to
consider posts which reply to user u?s n-th post be-
cause the value of yu,n affects these posts.
p(S?(u,n)|yu,n = k,Y?(u,n), ?)
?
?
(u?,n?):yu,n?Yu?,n?
p(su?,n? |yu?,n? ,Yu?,n? , ?). (4)
Next, we show how we jointly sample xu,n,l
and zu,n,l. We jointly sample them because when
xu,n,l = 0, zu,n,l does not need a value. We have the
following formulas:
p(xu,n,l = 1, zu,n,l = t|X?(u,n,l),Z?(u,n,l),Y,W, ?, ?, ?, ?
B)
?
C1?(u,n,l) + ?
C(?)
?(u,n,l)
+ 2?
?
Ctyu,n,l,?(u,n,l)
+ ?
C(?)
yu,n,l,?(u,n,l)
+ T?
?
C
wu,n,l
t,?(u,n,l)
+ ?
C(?)
t,?(u,n,l)
+ V ?
, (5)
p(xu,n,l = 0|X?(u,n,l),Z?(u,n,l),Y,W, ?, ?, ?, ?
B)
?
C0?(u,n,l) + ?
C(?)
?(u,n,l)
+ 2?
?
C
wu,n,l
B,?(u,n,l)
+ ?B
C(?)
B,?(u,n,l)
+ V ?B
. (6)
Here again the Cs are counters defined in similar
ways as before. For example, C1?(u,n,l) is the num-
ber of times we observe 1 assigned to an x variable,
excluding xu,n,l.
3.4 Interaction polarity prediction
The problem of detecting agreement and disagree-
ment from forum posts is relatively new. One pos-
sible solution is to use supervised learning, which
requires training data (Galley et al, 2004; Abbott et
al., 2011; Andreas et al, 2012). However, training
data are also likely domain and language dependent,
which makes them hard for re-use. For our task, we
take a simpler approach and use a sentiment lexicon
together with some heuristics to predict the polar-
ity of interaction expressions. Specifically, we first
identify interaction sentences following the strate-
gies from Hassan et al (2012). We assume sentences
containing mentions of the recipient of a post are in-
teraction sentences. Next, we consider words within
a text window of 8 words surrounding these men-
tions. We then use a subjectivity lexicon to label
these words. To form an English lexicon, we com-
bine three popular lexicons: the sentiment lexicon
used by Hu and Liu (2004), Multi-Perspective Ques-
tion Answering Subjectivity Lexicon by Wilson et
al. (2005) and SentiWordNet by Baccianella et al
(2010). Since we also work with a Chinese data set,
to form the Chinese sentiment lexicon, we use opin-
ion words from HowNet2 and NTUSD by Ku et al
(2007). To predict the polarity of an interaction ex-
pression, we simply check whether there are more
positive sentiment words or more negative sentiment
words in the expression, and label the interaction ex-
pression accordingly.
We would like to stress that since this interaction
classification step is independent of the latent vari-
able model, we can always apply a more accurate
method, but this is not the focus of this work.
4 Models for Comparison
In our experiments, we compare our model,
Joint Viewpoint-Topic Model with User Interaction
(JVTM-UI), with the following baseline models.
JVTM: The model is shown in Figure 3(a), a variant
of JVTM-UI that does not consider user interaction.
Through comparison with it, we can check the effect
of modeling user interactions.
JVTM-G: We consider JVTM-G in Figure 3(b), a
variant of JVTM which assumes a global viewpoint
distribution. Comparison with it allows us to check
the usefulness of user identity in the task.
UIM: The third model we consider is a User Interac-
tion Model (UIM) in Figure 3(c), where we rely on
only the users? interactions to infer the viewpoints.
We use it to check how well viewpoints can be dis-
covered from only user interaction expressions.
2http://www.keenage.com/html/e_index.html
1035
Figure 3: (a) JVTM: Joint Viewpoint-Topic Model. (b) JVTM-G: JVTM with a global viewpoint distribution. (c)
UIM: User-Interaction Model.
TAM: The last model we consider is the one by Paul
et al (2010). As TAM is applied at document collec-
tions, we first concatenate all the posts by the same
user into a pseudo document and then apply TAM.
5 Experiments and Analysis
In this section, we evaluate our model with a set of
baseline models using two data sets.
Name Issue #Posts #Users
EDS1 Vote for Obama 2599 197
EDS2 Arizona Immigration Law 738 59
EDS3 Tax Cuts 276 26
CDS1 Tencent and Qihoo dispute 30137 2507
CDS2 Fang Zhouzi questions Han Han 76934 1769
CDS3 Liu Xiang in London Olympics 29486 2774
Table 2: Some statistics of the data set.
5.1 Data Sets and Experimental Settings
We focus our work on finding users? viewpoints on
a controversial issue, where we assume that there
are two contradictory viewpoints. We use two data
sets on controversial issues. The first data set comes
from Abu-Jbara et al (2012) and Hassan et al
(2012). This data set originally was used for finding
subgroups of users, so the annotations were done at
user level, i.e. for each user there is a label indicat-
ing which subgroup he/she belongs to. We use the
top-3 mostly discussed threads with two subgroups
for our study.
In reality, controversial issues are often discussed
across threads. We thus constructed another large
data set which contains more than one thread for
each issue. We chose three hot issues from one of
the most popular Chinese online forums ? TianYa
Club3. The three issues are ?Fang Zhouzi questions
Han Han?4, ?Tencent and Qihoo dispute?5, and ?Liu
Xiang in London Olympics?6. All these issues trig-
gered heated discussions on the forum and we found
that most of the users were divided into two different
groups.
We crawled the data set using the TianYa API7.
The API allows users to issue queries and get threads
most related to the queries. For each issue, we used
entities involved in the event as queries and obtained
750 threads for each query. We then extracted all the
posts in the threads. As there are users who posted
irrelevant posts in the forum, we then filtered out
those users who did not mention the entities or had
fewer than 4 posts.
We refer to the first set of data in English as EDS1,
EDS2 and EDS3, and the second set of data in Chi-
nese as CDS1, CDS2 and CDS3. Some statistics of
the resulting data set are shown in Table 2.
For all the models, we set Y = 2. We set T = 10
for the English data sets and T = 40 for the Chinese
data sets. We run 400 iterations of Gibbs sampling
as burn-in iterations and then take 100 samples with
a gap of 5 to obtain our final results. We empirically
set ? = 0.01, ?B = 0.1, ? = 10 and ? = 0.1 for our
model on all the data sets. ? and ? are set through
grid search where they take values in {0.01, 0.001}.
For each data set, we choose the best setting for each
model and report the corresponding results.
3http://en.wikipedia.org/wiki/Tianya_Club
4http://en.wikipedia.org/wiki/Fang_Zhouzi
5http://en.wikipedia.org/wiki/360_v._Tencent
6http://en.wikipedia.org/wiki/Liu_Xiang
7http://open.tianya.cn/index.php
1036
5.2 Identification of viewpoints
We first evaluate the models on the task of identi-
fying viewpoints. For fair comparison, each model
will output a viewpoint label for each post. For
JVTM-UI, JVTM, JVTM-G and UIM, after we learn
the model, each post will directly have a viewpoint
assignment. For TAM we cannot directly get each
post?s viewpoint as the model assumes a document-
level viewpoint distribution. To estimate each post?s
viewpoint in this model, we use viewpoint assign-
ment at the word level learnt from the model. Then
for each post, we label its viewpoint as the viewpoint
that has the majority count in the post.
Ideally, we would like to manually label all the
posts to obtain the ground truth for evaluation. Since
there are too many posts, we only labeled a sample
of them. For each issue, we randomly selected 150
posts to label their viewpoints. For each post, we
asked two different annotators to label its viewpoint.
We made sure that the annotators understand the is-
sue and the two major viewpoints before they anno-
tated the posts. Specifically, as the Chinese data sets
are about some controversial issues around the enti-
ties involved, we then defined two major viewpoints
as support and not support the entity who initiated
the event. The entities of data set CDS1, CDS2 and
CDS3 are Fang Zhouzi, Tencent and Liu Xiang re-
spectively. For each given post, the annotators were
asked to judge whether the post has expressed view-
points and if so, what is its corresponding view-
point. We measure the agreement score using Co-
hen?s kappa coefficient. The lowest agreement score
for an issue is 0.61 in the data set, showing good
agreement. We then used the set of posts that were
labeled with the same viewpoint by the two annota-
tors as our evaluation data for all the models.
Since our task is essentially a clustering problem,
we use purity and entropy to measure the perfor-
mance (Manning et al, 2008). Furthermore, we also
use accuracy where we choose the better alignment
of clusters with ground truth class labels and com-
pute the percentage of posts that are ?classified? cor-
rectly. For purity and accuracy, the higher the mea-
sure is the better the performance. For entropy, the
lower the measure is the better the performance.
We give an overview of the all the averaged model
results on the data sets in Figure 4. We observed that
0.4
0.6
0.8
1.0
Purity Entropy Accuracy
JVTM-GTAMJVTMUIMJVTM-UI
Figure 4: Averaged results of the models in identification
of viewpoints.
UIM performs relatively better than other methods
except our model. This shows user interactions are
important features to identify post viewpoints. Over-
all, our model has a better performance as it is with
higher purity and accuracy, and lower entropy.
JVTM-UI UIM JVTM TAM JVTM-G
EDS1
P 0.77 0.74 0.64 0.65 0.63
E 0.72 0.76 0.90 0.92 0.94
A 0.77 0.74 0.61 0.60 0.57
EDS2
P 0.82 0.78 0.68 0.65 0.64
E 0.69 0.73 0.79 0.86 0.90
A 0.81 0.78 0.68 0.68 0.65
EDS3
P 0.79 0.73 0.65 0.64 0.62
E 0.67 0.79 0.88 0.89 0.87
A 0.79 0.73 0.65 0.64 0.62
CDS1
P 0.87 0.83 0.83 0.82 0.82
E 0.61 0.64 0.65 0.66 0.64
A 0.60 0.58 0.59 0.58 0.57
CDS2
P 0.71 0.65 0.61 0.63 0.60
E 0.80 0.85 0.92 0.95 0.96
A 0.71 0.65 0.61 0.61 0.59
CDS3
P 0.78 0.78 0.78 0.78 0.78
E 0.73 0.75 0.70 0.72 0.73
A 0.67 0.59 0.67 0.66 0.63
Table 3: Results on viewpoint identification on the all
data sets.
Table 3 shows the detailed results on the data
sets. We perform the 2-tailed paired t-test as used
by Abu-Jbara et al (2012) on the results. All the re-
sult differences are at 10% significance level if not
with further clarification. First, JVTM has a better
performance over JVTM-G, which shows it is im-
portant to consider user identity in the task. Sec-
ond, JVTM and TAM have similar performance on
1037
EDS1 and CDS2, but JVTM has a relatively bet-
ter performance on EDS2, EDS3, CDS1 and CDS3.
This shows it is helpful to consider each viewpoint?s
topic preference. Although as studied by Paul et
al. (2010), by only using unigram features, TAM
may not be able to cluster viewpoints accurately,
our study shows that the results can be improved
when adding each viewpoint?s topic focus. Third,
UIM has relatively better performance than the other
models, which demonstrates that user interactions
alone can do a decent job in inferring viewpoints. Fi-
nally, our proposed model has the best performance
across the board in terms of all three evaluation met-
rics. Note that, our proposed model significantly
outperforms other methods at 5% significance level
except at 10% significance level over JVTM model.
This shows by jointly modeling topics, viewpoints
and user interactions, our model can better identify
posts with different viewpoints.
5.3 Identification of user groups
We also use another task to evaluate our model.
The task here is finding each user?s viewpoint and
subsequently grouping users by their viewpoints.
This task has been studied by Abu-Jbara and Radev
(2012), Dasigi et al (2012), Abu-Jbara et al (2012)
and Hassan et al (2012). For the English data set,
the user-level group labels are provided by the orig-
inal data set. For the Chinese data set, we randomly
selected 150 users for each issue and manually la-
beled them according to their viewpoints as reflected
by their posts. If a user?s posts do not clearly suggest
a viewpoint, we label her as neutral. Again we asked
two human judges to do annotation. The agreement
scores are above 0.70 for all issues, showing sub-
stantial agreement. This score is higher than view-
point identification, which suggests that it is easier
to judge a user?s viewpoint than a single post?s view-
point. We use the set of users who have got the same
labels by the two human judges for our experiments.
Similarly we compute purity, entropy and accuracy
to evaluate the clustering results.
Figure 5 shows the averaged results of all the
models. Similar to previous experiment, our model
has a better performance compared to the competing
models.
The results on the each data set are shown in Ta-
ble 4. The tables show that similar trends can be
0.4
0.6
0.8
1.0
Purity Entropy Accuracy
JVTM-GTAMJVTMUIMJVTM-UI
Figure 5: Averaged results of the models in identification
of user groups.
JVTM-UI UIM JVTM TAM JVTM-G
EDS1
P 0.67 0.67 0.67 0.67 0.67
E 0.85 0.88 0.89 0.89 0.91
A 0.63 0.59 0.58 0.59 0.57
EDS2
P 0.77 0.77 0.77 0.77 0.77
E 0.72 0.76 0.74 0.75 0.76
A 0.62 0.59 0.60 0.58 0.59
EDS3
P 0.68 0.63 0.61 0.61 0.58
E 0.90 0.92 0.95 0.96 0.97
A 0.68 0.63 0.61 0.58 0.57
CDS1
P 0.64 0.60 0.61 0.61 0.60
E 0.91 0.97 0.96 0.96 0.97
A 0.61 0.55 0.55 0.56 0.53
CDS2
P 0.69 0.69 0.69 0.69 0.69
E 0.83 0.89 0.85 0.89 0.89
A 0.62 0.57 0.56 0.58 0.54
CDS3
P 0.67 0.63 0.64 0.60 0.60
E 0.89 0.91 0.92 0.93 0.96
A 0.64 0.62 0.60 0.56 0.54
Table 4: Results on identification of user groups on the
all the data sets.
observed for the task of user group identification.
We also perform the 2-tailed paired t-test on the re-
sults. We find our model significantly outperforms
other models in terms of accuracy at 5% significance
level, and purity and entropy at 10% significance
level. Overall speaking, our joint model performed
the best among all the models for this task for all
three metrics. This shows that it is important to con-
sider the topical preference of individual viewpoint,
user?s identify as well as the interactions between
users.
1038
Figure 6: The user interaction network in a discussion
thread about ?will you vote obama.? Green (left) and
white (right) nodes represent users with two different
viewpoints. Red (thin) and blue(thick) edges represent
negative and positive interactions.
5.4 User interaction network
To gain some direct insight into our results, we show
the user interaction network from one thread in Fig-
ure 6. Here each node denotes a user, and its color
denotes the predicted viewpoint of that user. A link
between a pair of users means these users have in-
teractions and the interaction types have a dominant
polarity. The polarities of these links are predicted
using the interaction expressions and a sentiment
lexicon, whereas the viewpoints of different users
are learned by JVTM-UI, making use of the inter-
action polarities. The figure shows that clearly there
are mostly positive interactions between users with
the same viewpoint and mostly negative interactions
between users with different viewpoints. Note that,
our method to identify user interaction polarity is
rule-based. As this step serves as a preprocessing
step for our latent variable model, we can always
use a more accurate method to improve the perfor-
mances.
6 Conclusion
In this work, we proposed a novel latent variable
model for viewpoint discovery from threaded forum
posts. Our model is based on the three important fac-
tors: viewpoint specific topic preference, user iden-
tity and user interactions. Our proposed model cap-
tures these observations in a principled way. In par-
ticular, to incorporate the user interaction informa-
tion, we proposed a novel generative process. Em-
pirical evaluation on the real forum data sets showed
that our model could cluster both posts and users
with different viewpoints more accurately than the
baseline models we consider. To the best of our
knowledge, our work is the first to incorporate user
interaction polarity into a generative model to dis-
cover viewpoints.
In this work, we only considered unigrams. As
some previous work has shown, more complex lexi-
cal units such as n-grams (Mukherjee and Liu, 2012)
and dependency triplets (Paul et al, 2010) may im-
prove the performance of topic models. We will con-
sider these strategies in our future work. Currently
we use a simple heuristic-based classifier to predict
interaction polarity. In our further work, we plan
to consider more accurate methods using deeper lin-
guistic analysis. We did not study how to summarize
the discovered viewpoints in this work, which is also
something we will look into in our future work.
Acknowledgments
We thank the reviewers for their valuable comments
on this work. We also thank Shuang Xia for his help
on processing and labeling the data sets.
References
Rob Abbott, Marilyn Walker, Pranav Anand, Jean E.
Fox Tree, Robeson Bowmani, and Joseph King. 2011.
How can you say such things?!?: Recognizing dis-
agreement in informal political argument. In Proceed-
ings of the Workshop on Language in Social Media
(LSM 2011), pages 2?11.
Amjad Abu-Jbara and Dragomir R. Radev. 2012. Sub-
group detector: A system for detecting subgroups in
online discussions. In ACL (System Demonstrations),
pages 133?138.
Amjad Abu-Jbara, Pradeep Dasigi, Mona Diab, and
Dragomir R. Radev. 2012. Subgroup detection in
ideological discussions. In Proceedings of ACL 2012,
pages 399?409.
Jacob Andreas, Sara Rosenthal, and Kathleen McKe-
own. 2012. Annotating agreement and disagreement
in threaded discussion. In Proceedings of LREC?12.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
LREC.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Pradeep Dasigi, Weiwei Guo, and Mona T. Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude using
1039
textual latent semantics. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics, pages 65?69.
Yi Fang, Luo Si, Naveen Somasundaram, and Zhengtao
Yu. 2012. Mining contrastive opinions on political
texts using cross-perspective topic model. In WSDM,
pages 63?72.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL?04, Main Volume, pages 669?
676.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Detecting subgroups in online discussions by
modeling positive and negative relations among par-
ticipants. In Proceedings of the 2012 EMNLP, pages
59?70.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the 10th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177.
Lun-wei Ku, Yong-sheng Lo, and Hsin-hsi Chen. 2007.
Using polarity scores of words for sentence-level opin-
ion extraction. In Proc. of the NTCIR-6 Workshop
Meeting, pages 316?322.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM international conference on
Information and knowledge management, CIKM ?12,
pages 1642?1646, New York, NY, USA. ACM.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, July.
Arjun Mukherjee and Bing Liu. 2012. Modeling review
comments. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics,
pages 320?329.
Michael J. Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In AAAI.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In EMNLP, pages 66?76.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226?234.
Swapna Somasundaran and Janyce Wiebe. 2010. Recog-
nizing stances in ideological on-line debates. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, pages 116?124.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
1040
