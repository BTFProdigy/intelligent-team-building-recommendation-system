Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 155?163,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Text Mining Techniques for Leveraging Positively Labeled Data 
Lana Yeganova*, Donald C. Comeau, Won Kim, W. John Wilbur 
National Center for Biotechnology Information, NLM, NIH, Bethesda, MD 20894 USA 
{yeganova, comeau, wonkim, wilbur}@mail.nih.gov 
* Corresponding author. Tel.:+1 301 402 0776 
Abstract 
Suppose we have a large collection of 
documents most of which are unlabeled. Suppose 
further that we have a small subset of these 
documents which represent a particular class of 
documents we are interested in, i.e. these are 
labeled as positive examples. We may have reason 
to believe that there are more of these positive 
class documents in our large unlabeled collection. 
What data mining techniques could help us find 
these unlabeled positive examples? Here we 
examine machine learning strategies designed to 
solve this problem. We find that a proper choice of 
machine learning method as well as training 
strategies can give substantial improvement in 
retrieving, from the large collection, data enriched 
with positive examples. We illustrate the principles 
with a real example consisting of multiword 
UMLS phrases among a much larger collection of 
phrases from Medline. 
 
1 Introduction 
Given a large collection of documents, a few of 
which are labeled as interesting, our task is to 
identify unlabeled documents that are also 
interesting. Since the labeled data represents the 
data we are interested in, we will refer to it as the 
positive class and to the remainder of the data as 
the negative class. We use the term negative class, 
however, documents in the negative class are not 
necessarily negative, they are simply unlabeled and 
the negative class may contain documents relevant 
to the topic of interest. Our goal is to retrieve these 
unknown relevant documents. 
A na?ve approach to this problem would simply 
take the positive examples as the positive class and 
the rest of the collection as the negative class and 
apply machine learning to learn the difference and 
rank the negative class based on the resulting 
scores. It is reasonable to expect that the top of this 
ranking would be enriched for the positive class. 
But an appropriate choice of methods can improve 
over the na?ve approach.  
One issue of importance would be choosing the 
most appropriate machine learning method. Our 
problem can be viewed from two different 
perspectives: the problem of learning from 
imbalanced data as well as the problem of 
recommender systems. In terms of learning from 
imbalanced data, our positive class is significantly 
smaller than the negative, which is the remainder 
of the collection. Therefore we are learning from 
imbalanced data. Our problem is also a 
recommender problem in that based on a few 
examples found of interest to a customer we seek 
similar positive examples amongst a large 
collection of unknown status. Our bias is to use 
some form of wide margin classifier for our 
problem as such classifiers have given good 
performance for both the imbalanced data problem 
and the recommender problem (Zhang and Iyengar 
2002; Abkani, Kwek et al 2004; Lewis, Yang et 
al. 2004).  
Imbalanced data sets arise very frequently in 
text classification problems. The issue with 
imbalanced learning is that the large prevalence of 
negative documents dominates the decision 
process and harms classification performance. 
Several approaches have been proposed to deal 
with the problem including sampling methods and 
cost-sensitive learning methods and are described 
in (Chawla, Bowyer et al 2002; Maloof 2003; 
Weiss, McCarthy et al 2007). These studies have 
shown that there is no clear advantage of one 
approach versus another. Elkan (2001) points out 
that cost-sensitive methods and sampling methods 
are related in the sense that altering the class 
distribution of training data is equivalent to 
altering misclassification cost. Based on these 
studies we examine cost-sensitive learning in 
which the cost on the positive set is increased, as a 
useful approach to consider when using an SVM.  
In order to show how cost-sensitive learning for 
an SVM is formulated, we write the standard 
equations for an SVM following (Zhang 2004). 
155
Given training data ? ?? ?,i ix y  where iy  is 1 or ?1 
depending on whether the data point 
ix  is 
classified as positive or negative, an SVM seeks 
that vector 
iw  which minimizes  
? ? 2( )     (1)2i ii h y x w w??? ? ??
 
 
where the loss function is defined by  
? ? 1 ,  1           (2) 0,         z>1.
z zh z ? ??? ??
 
The cost-sensitive version modifies (1) to become  
 
 
and now we can choose r?  and r?  to magnify the 
losses appropriately. Generally we take r?  to be 1, 
and r?  to be some factor larger than 1. We refer to 
this formulation as CS-SVM. Generally, the same 
algorithms used to minimize (1) can be used to 
minimize (3). 
Recommender systems use historical data on 
user preferences, purchases and other available 
data to predict items of interest to a user. Zhang 
and Iyengar (2002) propose a wide margin 
classifier with a quadratic loss function as very 
effective for this purpose (see appendix). It is used 
in (1) and requires no adjustment in cost between 
positive and negative examples. It is proposed as a 
better method than varying costs because it does 
not require searching for the optimal cost 
relationship between positive and negative 
examples. We will use for our wide margin 
classifier the modified Huber loss function (Zhang 
2004).  The modified Huber loss function is 
quadratic where this is important and has the form  
? ? ? ?2
4 ,     1
                    (4)1 ,  -1 1
0,     z>1.
z z
h z z z
? ? ? ??
?
? ? ? ??
?
?
 
We also use it in (1). We refer to this approach as 
the Huber method (Zhang 2004) as opposed to 
SVM. We compare it with SVM and CS-SVM. We 
used our own implementations for SVM, CS-SVM, 
and Huber that use gradient descent to optimize the 
objective function. 
The methods we develop are related to semi-
supervised learning approaches (Blum and 
Mitchell 1998; Nigam, McCallum et al 1999) and 
active learning (Roy and McCallum 2001; Tong 
and Koller 2001). Our method differs from active 
learning in that active learning seeks those 
unlabeled examples for which labels prove most 
informative in improving the classifier. Typically 
these examples are the most uncertain. Some semi-
supervised learning approaches start with labeled 
examples and iteratively seek unlabeled examples 
closest to already labeled data and impute the 
known label to the nearby unlabeled examples. Our 
goal is simply to retrieve plausible members for the 
positive class with as high a precision as possible. 
Our method has value even in cases where human 
review of retrieved examples is necessary. The 
imbalanced nature of the data and the presence of 
positives in the negative class make this a 
challenging problem. 
In Section 2 we discuss additional strategies 
proposed in this work, describe the data used and 
design of experiments, and provide the evaluation 
measure used. In Section 3 we present our results, 
in Sections 4 and 5 we discuss our approach and 
draw conclusions.  
 
2 Methods 
2.1 Cross Training 
Let D  represent our set of documents, and C?  
those documents that are known positives in D . 
Generally C?  would be a small fraction of D  and 
for the purposes of learning we assume that 
\C D C? ?? . 
 We are interested in the case when some of the 
negatively labeled documents actually belong to 
the positive class. We will apply machine learning 
to learn the difference between the documents in 
the class C?  and documents in the class C?  and 
use the weights obtained by training to score the 
documents in the negative class C? . The highest 
scoring documents in set C?  are candidate 
mislabeled documents. However, there may be a 
problem with this approach, because the classifier 
is based on partially mislabeled data. Candidate 
? ? ? ? 2( ) ( )  (3)2i i i ii C i Cr h y x w r h y x w w
?? ?
? ?
? ?? ?
? ? ? ? ? ? ? ?? ?
156
mislabeled documents are part of the C?  class. In 
the process of training, the algorithm purposely 
learns to score them low. This effect can be 
magnified by any overtraining that takes place. It 
will also be promoted by a large number of 
features, which makes it more likely that any 
positive point in the negative class is in some 
aspect different from any member of C? . 
Another way to set up the learning is by 
excluding documents from directly participating in 
the training used to score them. We first divide the 
negative set into disjoint pieces 
1 2C Z Z? ? ?  
Then train documents in C?  versus documents in 
1Z  to rank documents in 2Z  and train documents 
in C?  versus documents in 2Z  to rank documents 
in
1Z . We refer to this method as cross training 
(CT). We will apply this approach and show that it 
confers benefit in ranking the false negatives in 
C? .  
2.2 Data Sources and Preparation 
The databases we studied are MeSH25, Reuters, 
20NewsGroups, and MedPhrase. 
MeSH25.   We selected 25 MeSH? terms with 
occurrences covering a wide frequency range: from 
1,000 to 100,000 articles. A detailed explanation of 
MeSH can be found at 
http://www.nlm.nih.gov/mesh/. 
For a given MeSH term m, we treat the records 
assigned that MeSH term m as positive. The 
remaining MEDLINE? records do not have m 
assigned as a MeSH and are treated as negative. 
Any given MeSH term generally appears in a small 
minority of the approximately 20 million MEDLINE 
documents making the data highly imbalanced for 
all MeSH terms.  
Reuters. The data set consists of 21,578 Reuters 
newswire articles in 135 overlapping topic 
categories. We experimented on the 23 most 
populated classes. 
For each of these 23 classes, the articles in the 
class of interest are positive, and the rest of 21,578 
articles are negative. The most populous positive 
class contains 3,987 records, and the least 
populous class contains 112 records.  
 20NewsGroups. The dataset is a collection of 
messages from twenty different newsgroups with 
about one thousand messages in each newsgroup. 
We used each newsgroup as the positive class and 
pooled the remaining nineteen newsgroups as the 
negative class. 
Text in the MeSH25 and Reuters databases has 
been preprocessed as follows: all alphabetic 
characters were lowercased, non-alphanumeric 
characters replaced by blanks, and no stemming 
was done. Features in the MeSH25 dataset are all 
single nonstop terms and all pairs of adjacent 
nonstop terms that are not separated by 
punctuation. Features in the Reuters database are 
single nonstop terms only. Features in the 
20Newsgroups are extracted using the Rainbow 
toolbox (McCallum 1996).  
MedPhrase. We process MEDLINE to extract all 
multiword UMLS? 
(http://www.nlm.nih.gov/research/umls/) phrases 
that are present in MEDLINE. From the resulting 
set of strings, we drop the strings that contain 
punctuation marks or stop words. The remaining 
strings are normalized (lowercased, redundant 
white space is removed) and duplicates are 
removed. We denote the resulting set of 315,679 
phrases by 
phrasesU .  
For each phrase in ,phrasesU  we randomly 
sample, as available, up to 5 MEDLINE sentences 
containing it. We denote the resulting set of 
728,197 MEDLINE sentences by 
phrasesS . From
phrasesS  we extract all contiguous multiword 
expressions that are not present in 
phrasesU . We 
call them n-grams, where n>1. N-grams containing 
punctuation marks and stop words are removed 
and remaining n-grams are normalized and 
duplicates are dropped. The result is 8,765,444 n-
grams that we refer to as .ngramM  We believe that 
ngramM contains many high quality biological 
phrases. We use 
phrasesU  , a known set of high 
quality biomedical phrases, as the positive class, 
and 
ngramM  
as the negative class. 
In order to apply machine learning we need to 
define features for each n-gram. Given an n-gram 
grm that is composed of n  words,
1 2 ngrm w w w? , we extract a set of 11 numbers 
157
? ?111i if ?  associated with the n-gram grm. These are 
as follows: 
f1: number of occurrences of grm throughout 
Medline; 
f2: -(number of occurrences of w2?wn not 
following w1 in documents that contain grm)/ f1; 
f3: -(number of occurrences of w1?wn-1 not 
preceding wn in documents that contain grm)/ f1; 
f4: number of occurrences of (n+1)-grams of the 
form xw1?wn throughout Medline; 
f5: number of occurrences of (n+1)-grams  of 
the form w1?wn x throughout Medline; 
f6: ? ? ? ?? ?
? ?? ? ? ?
1 2 1 2
1 2 1 2
| 1 |log 1 | |
p w w p w w
p w w p w w
? ?? ?
? ?? ?? ?? ?
 
f7: mutual information between w1 and w2; 
f8: ? ? ? ?? ?
? ?? ? ? ?
1 1
1 1
| 1 |log 1 | |
n n n n
n n n n
p w w p w w
p w w p w w
? ?
? ?
? ?? ?
? ?? ?? ?
 
f9: mutual information between wn-1 and wn; 
f10: -(number of different multiword expressions 
beginning with w1 in Medline); 
f11: -(number of different multiword expressions 
ending with wn in Medline).   
We discretize the numeric values of the ? ?111i if ?  
into categorical values.  
In addition to these features, for every n-gram 
grm, we include the part of speech tags predicted 
by the MedPost tagger (Smith, Rindflesch et al 
2004).  To obtain the tags for a given n-gram grm 
we randomly select a sentence from 
phrasesS  
containing grm, tag the sentence, and consider the 
tags 
0 1 2 1 1n n nt t t t t t? ?  where 0t is the tag of the 
word preceding word 
1w in n-gram grm, 1t  is the 
tag of word 
1w  in n-gram grm, and so on. We 
construct the features  
  
 
These features emphasize the left and right ends of 
the n-gram and include parts-of-speech in the 
middle without marking their position. The 
resulting features are included with ? ?111i if ?  to 
represent the n-gram. 
2.3 Experimental Design  
A standard way to measure the success of a 
classifier is to evaluate its performance on a 
collection of documents that have been previously 
classified as positive or negative. This is usually 
accomplished by randomly dividing up the data 
into training and test portions which are separate. 
The classifier is then trained on the training 
portion, and is tested on test portion. This can be 
done in a cross-validation scheme or by randomly 
re-sampling train and test portions repeatedly.   
We are interested in studying the case where 
only some of the positive documents are labeled. 
We simulate that situation by taking a portion of 
the positive data and including it in the negative 
training set. We refer to that subset of positive 
documents as tracer data (Tr). The tracer data is 
then effectively mislabeled as negative. By 
introducing such an artificial supplement to the 
negative training set we are not only certain that 
the negative set contains mislabeled positive 
examples, but we know exactly which ones they 
are. Our goal is to automatically identify these 
mislabeled documents in the negative set and 
knowing their true labels will allow us to measure 
how successful we are. Our measurements will be 
carried out on the negative class and for this 
purpose it is convenient to write the negative class 
as composed of true negatives and tracer data 
(false negatives) 
'C C Tr? ?? ? . 
 
When we have trained a classifier, we evaluate 
performance by ranking 'C?  and measuring how 
well tracer data is moved to the top ranks. The 
challenge is that Tr appears in the negative class 
and will interact with the training in some way.  
2.4 Evaluation 
We evaluate performance using Mean Average 
Precision (MAP) (Baeza-Yates and Ribeiro-Neto 
1999). The mean average precision is the mean 
value of the average precisions computed for all 
topics in each of the datasets in our study. Average 
precision is the average of the precisions at each 
rank that contains a true positive document. 
 
? ? ? ?? ?? ?? ?
? ? ? ?? ?? ?? ?
0 1 1 2 1
0 1 1
if 2 :  ,1 , , 2 ,3 ,4 , ,...,  
otherwise: ,1 , , 2 ,3 ,4 .
n n
n n
n t t t t t t
t t t t
? ?
?
? ???
??
158
Table 1: MAP scores trained with three levels of tracer data introduced to the negative training set. 
 
No Cross Training No Tracer Data Tr20 in training Tr50 in training 
MeSH Terms Huber SVM Huber SVM Huber SVM 
celiac disease 0.694 0.677 0.466 0.484 0.472 0.373 
lactose intolerance 0.632 0.635 0.263 0.234 0.266 0.223 
myasthenia gravis 0.779 0.752 0.632 0.602 0.562 0.502 
carotid stenosis 0.466 0.419 0.270 0.245 0.262 0.186 
diabetes mellitus 0.181 0.181 0.160 0.129 0.155 0.102 
rats, wistar 0.241 0.201 0.217 0.168 0.217 0.081 
myocardial infarction 0.617 0.575 0.580 0.537 0.567 0.487 
blood platelets 0.509 0.498 0.453 0.427 0.425 0.342 
serotonin 0.514 0.523 0.462 0.432 0.441 0.332 
state medicine 0.158 0.164 0.146 0.134 0.150 0.092 
urinary bladder 0.366 0.379 0.312 0.285 0.285 0.219 
drosophila melanogaster 0.553 0.503 0.383 0.377 0.375 0.288 
tryptophan 0.487 0.480 0.410 0.376 0.402 0.328 
laparotomy 0.186 0.173 0.138 0.101 0.136 0.066 
crowns 0.520 0.497 0.380 0.365 0.376 0.305 
streptococcus mutans 0.795 0.738 0.306 0.362 0.218 0.306 
infectious mononucleosis 0.622 0.614 0.489 0.476 0.487 0.376 
blood banks 0.283 0.266 0.170 0.153 0.168 0.115 
humeral fractures 0.526 0.495 0.315 0.307 0.289 0.193 
tuberculosis, lymph node 0.385 0.397 0.270 0.239 0.214 0.159 
mentors 0.416 0.420 0.268 0.215 0.257 0.137 
tooth discoloration 0.499 0.499 0.248 0.215 0.199 0.151 
pentazocine 0.710 0.716 0.351 0.264 0.380 0.272 
hepatitis e 0.858 0.862 0.288 0.393 0.194 0.271 
genes, p16 0.278 0.313 0.041 0.067 0.072 0.058 
Avg 0.491 0.479 0.321 0.303 0.303 0.238 
 
3 Results 
3.1 MeSH25, Reuters, and 20NewsGroups 
We begin by presenting results for the MeSH25 
dataset. Table 1 shows the comparison between 
Huber and SVM methods. It also compares the 
performance of the classifiers with different levels 
of tracer data in the negative set. We set aside 50% 
of C?  to be used as tracer data and used the 
remaining 50% of C?  as the positive set for 
training. We describe three experiments where we 
have different levels of tracer data in the negative 
set at training time. These sets are ,C?  20 ,C Tr? ?  
and 
50  C Tr? ? representing no tracer data, 20% of 
C?  as tracer data and 50% of C?  as tracer data, 
respectively. The test set 
20C Tr? ?  is the same for 
all of these experiments. Results indicate that on 
average Huber outperforms SVM on these highly 
imbalanced datasets. We also observe that 
performance of both methods deteriorates with 
increasing levels of tracer data.   
Table 2 shows the performance of Huber and 
SVM methods on negative training sets with tracer 
data 
20C Tr? ?  and 50  C Tr? ? as in Table 1, but 
with cross training. As mentioned in the Methods 
section, we first divide each negative training set 
into two disjoint pieces
1Z  and 2Z . We then train 
documents in the positive training set versus 
documents in 
1Z  to score documents in 2Z  and 
train documents in the positive training set versus  
documents in 
2Z  to score documents in 1Z . We 
then merge 
1Z  and 2Z  as scored sets and report 
measurements on the combined ranked set of 
documents. Comparing with Table 1, we see a 
significant improvement in the MAP when using 
cross training. 
 
159
 
Table 2: MAP scores for Huber and SVM trained with two levels of tracer data introduced to the 
negative training set using cross training technique. 
 
2-fold Cross Training Tr20 in training Tr50 in training 
MeSH Terms Huber SVM Huber SVM 
celiac disease 0.550 0.552 0.534 0.521 
lactose intolerance 0.415 0.426 0.382 0.393 
myasthenia gravis 0.652 0.643 0.623 0.631 
carotid stenosis 0.262 0.269 0.241 0.241 
diabetes mellitus 0.148 0.147 0.144 0.122 
rats, wistar 0.212 0.186 0.209 0.175 
myocardial infarction 0.565 0.556 0.553 0.544 
blood platelets 0.432 0.435 0.408 0.426 
serotonin 0.435 0.447 0.417 0.437 
state medicine 0.135 0.136 0.133 0.132 
urinary bladder 0.295 0.305 0.278 0.280 
drosophila melanogaster 0.426 0.411 0.383 0.404 
tryptophan 0.405 0.399 0.390 0.391 
laparotomy 0.141 0.128 0.136 0.126 
crowns 0.375 0.376 0.355 0.353 
streptococcus mutans 0.477 0.517 0.448 0.445 
infectious mononucleosis 0.519 0.514 0.496 0.491 
blood banks 0.174 0.169 0.168 0.157 
humeral fractures 0.335 0.335 0.278 0.293 
tuberculosis, lymph node 0.270 0.259 0.262 0.244 
mentors 0.284 0.278 0.275 0.265 
tooth discoloration 0.207 0.225 0.209 0.194 
pentazocine 0.474 0.515 0.495 0.475 
hepatitis e 0.474 0.499 0.482 0.478 
genes, p16 0.102 0.101 0.083 0.093 
Avg 0.350 0.353 0.335 0.332 
 
 
We performed similar experiments with the 
Reuters and 20NewsGroups datasets, where 20%  
and 50% of the good set is used as tracer data. We 
report MAP scores for these datasets in Tables 3 
and 4. 
 
3.2 Identifying high quality biomedical 
phrases in the MEDLINE Database 
We illustrate our findings with a real example 
of detecting high quality biomedical phrases 
among ,ngramM a large collection of multiword 
expressions from Medline. We believe that 
ngramM  
contains many high quality biomedical phrases. 
These examples are the counterpart of the 
mislabeled positive examples (tracer data) in the 
previous tests. 
  
Table 3: MAP scores for Huber and SVM 
trained with 20% and 50% tracer data introduced to 
the negative training set for Reuters dataset. 
 
Reuters 
Tr20 in training Tr50 in training 
Huber SVM Huber SVM 
No CT 0.478 0.451 0.429 0.403 
2-Fold CT 0.662 0.654 0.565 0.555 
 
Table 4: MAP scores for Huber and SVM 
trained with 20% and 50% tracer data introduced to 
the negative training set for 20NewsGroups dataset. 
 
20News 
Groups 
Tr20 in training Tr50 in training 
Huber SVM Huber SVM 
No CT 0.492 0.436 0.405 0.350 
2-Fold CT 0.588 0.595 0.502 0.512 
160
To identify these examples, we learn the 
difference between the phrases in 
phrasesU  
 and 
.ngramM  Based on the training we rank the n-grams 
in .ngramM  
We expect the n-grams that cannot be 
separated from UMLS phrases are high quality 
biomedical phrases. In our experiments, we 
perform 3-fold cross validation for training and 
testing. This insures we obtain any possible benefit 
from cross training. The results shown in figure 1 
are MAP values for these 3 folds.  
 
Figure 1. Huber, CS-SVM, and na?ve Bayes 
classifiers applied to the MedPhrase dataset. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
We trained na?ve Bayes, Huber, and CS-SVM 
with a range of different cost factors. The results 
are presented in Figure 1. We observe that the 
Huber classifier performs better than na?ve Bayes. 
CS-SVM with the cost factor of 1 (standard SVM) 
is quite ineffective. As we increase the cost factor, 
the performance of CS-SVM improves until it is 
comparable to Huber. We believe that the quality 
of ranking is better when the separation of 
phrasesU  
from 
ngramM  is better.  
Because we have no tracer data we have no 
direct way to evaluate the ranking of .ngramM  
However, we selected a random set of 100 n-grams 
from ,ngramM  which score as high as top-scoring 
10% of phrases in 
phrasesU . Two reviewers 
manually reviewed that list and identified that 99 
of these 100 n-grams were high quality biomedical 
phrases. Examples are: aminoshikimate pathway, 
berberis aristata, dna hybridization, subcellular 
distribution, acetylacetoin synthase, etc. One false-
positive example in that list was congestive heart.  
 
 
4 Discussion 
We observed that the Huber classifier performs 
better than SVM on imbalanced data with no cross 
training (see appendix). The improvement of 
Huber over SVM becomes more marked as the 
percentage of tracer data in the negative training 
set is increased. However, the results also show 
that cross training, using either SVM or Huber 
(which are essentially equivalent), is better than 
using Huber without cross training. This is 
demonstrated in our experiments using the tracer 
data. The results are consistent over the range of 
different data sets. We expect cross training to 
have benefit in actual applications.  
Where does cost-sensitive learning fit into this 
picture? We tested cost-sensitive learning on all of 
our corpora using the tracer data. We observed 
small and inconsistent improvements (data not 
shown). The optimal cost factor varied markedly 
between cases in the same corpus. We could not 
conclude this was a useful approach and instead 
saw better results simply using Huber. This 
conclusion is consistent with (Zhang and Iyengar 
2002) which recommend using a quadratic loss 
function. It is also consistent with results reported 
in (Lewis, Yang et al 2004) where CS-SVM is 
compared with SVM on multiple imbalanced text 
classification problems and no benefit is seen using 
CS-SVM. Others have reported a benefit with CS-
SVM (Abkani, Kwek et al 2004; Eitrich and Lang 
2005). However, their datasets involve relatively 
few features and we believe this is an important 
aspect where cost-sensitive learning has proven 
effective. We hypothesize that this is the case 
because with few features the positive data is more 
likely to be duplicated in the negative set. In our 
case, the MedPhrase dataset involves relatively 
few features (410) and indeed we see a dramatic 
improvement of CS-SVM over SVM. 
One approach to dealing with imbalanced data 
is the artificial generation of positive examples as 
seen with the SMOTE algorithm (Chawla, Bowyer 
et al 2002). We did not try this method and do not 
know if this approach would be beneficial for 
1 11 21 31 41
Cost  Factor r
+
0.20
0.22
0.24
0.26
0.28
A
v
e
r
a
g
e
 
P
r
e
c
i
s
i
o
n
Comparison of different Machine Learning Methods
Huber
CS-SVM
Bayes
161
textual data or data with many features. This is an 
area for possible future research. 
Effective methods for leveraging positively 
labeled data have several potential applications:  
? Given a set of documents discussing a 
particular gene, one may be interested in 
finding other documents that talk about the 
same gene but use an alternate form of the 
gene name.  
? Given a set of documents that are indexed with 
a particular MeSH term, one may want to find 
new documents that are candidates for being 
indexed with the same MeSH term. 
? Given a set of papers that describe a particular 
disease, one may be interested in other 
diseases that exhibit a similar set of symptoms. 
? One may identify incorrectly tagged web 
pages.  
These methods can address both removing 
incorrect labels and adding correct ones. 
 
5 Conclusions 
Given a large set of documents and a small set 
of positively labeled examples, we study how best 
to use this information in finding additional 
positive examples. We examine the SVM and 
Huber classifiers and conclude that the Huber 
classifier provides an advantage over the SVM 
classifier on such imbalanced data. We introduce a 
technique which we term cross training. When this 
technique is applied we find that the SVM and 
Huber classifiers are essentially equivalent and 
superior to applying either method without cross 
training.  We confirm this on three different 
corpora. We also analyze an example where cost-
sensitive learning is effective. We hypothesize that 
with datasets having few features, cost-sensitive 
learning can be beneficial and comparable to using 
the Huber classifier.  
 
Appendix: Why Huber Loss Function works 
better for problems with Unbalanced Class 
Distributions. 
The drawback of the standard SVM for the 
problem with an unbalanced class distribution 
results from the shape of ( )h z  in (2). Consider the 
initial condition at 0w ?  and also imagine that there is 
a lot more C?  training data than C?  training data.  In 
this case, by choosing 1? ? ? , we can achieve the 
minimum value of the loss function in (1) for the initial 
condition 0w ? . Under these conditions, all C?  points 
yield 1z ?  and ( ) 0h z ? and all C?  points yield 
1z ? ?  and ( ) 2h z ? . The change of the loss function 
( )h z?  in (2) with a change w?  is given by 
 
 
 
In order to reduce the loss at a C?  
data point ( , )i ix y , 
we must choose w?  such that 0.ix w?? ?   But we 
assume that there are significantly more C?  class 
data points than C?  and many such points x? are 
mislabeled and close to
ix  such that 0.x w? ?? ?  
Then ( )h z  is likely be increased by ( 0)x w? ?? ?  
for these mislabeled points. Clearly, if there are 
significantly more C?  class data than those of  C?
class and the C? set  contains a lot of mislabeled 
points, it may be difficult to find w?  that can 
result in a net effect of decreasing the right hand 
side of (2). The above analysis shows why the 
standard support vector machine formulation in (2) 
is vulnerable to an unbalanced and noisy training 
data set. The problem is clearly caused by the fact 
that the SVM loss function ( )h z  in (2) has a 
constant slope for 1z ? . In order to alleviate this 
problem, Zhang and Iyengar (2002) proposed the 
loss function 2 ( )h z  which is a smooth non-
increasing function with slope 0 at 1z ? . This 
allows the loss to decrease while the positive 
points move a small distance away from the bulk 
of the negative points and take mislabeled points 
with them. The same argument applies to the 
Huber loss function defined in (4). 
 
Acknowledgments 
This research was supported by the Intramural Research 
Program of the NIH, National Library of Medicine. 
  
? ? ( )      (5).w i idh zh z z w y x wdz? ? ? ?? ? ? ??
162
References  
 
Abkani, R., S. Kwek, et al (2004). Applying Support 
Vector Machines to Imballanced Datasets. ECML. 
  
Baeza-Yates, R. and B. Ribeiro-Neto (1999). Modern 
Information Retrieval. New York, ACM Press. 
  
Blum, A. and T. Mitchell (1998). "Combining Labeled 
and Unlabeled Data with Co-Training." COLT: 
Proceedings of the Workshop on Computational 
Learning Theory: 92-100. 
  
Chawla, N. V., K. W. Bowyer, et al (2002). "SMOTE: 
Synthetic Minority Over-sampling Technique." Journal 
of Artificial Intelligence Research 16: 321-357. 
  
Eitrich, T. and B. Lang (2005). "Efficient optimization 
of support vector machine learning parameters for 
unbalanced datasets." Journal of Computational and 
Applied Mathematics 196(2): 425-436. 
  
Elkan, C. (2001). The Foundations of Cost Sensitive 
Learning. Proceedings of the Seventeenth International 
Joint Conference on Artificial Intelligence. 
  
Lewis, D. D., Y. Yang, et al (2004). "RCV1: A New 
Benchmark Collection for Text Categorization 
Research." Journal of Machine Learning Research 5: 
361-397. 
  
Maloof, M. A. (2003). Learning when data sets are 
imbalanced and when costs are unequal and unknown. 
ICML 2003, Workshop on Imballanced Data Sets. 
  
McCallum, A. K. (1996). "Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. http://www.cs.cmu.edu/~mccallum/bow/." 
  
Nigam, K., A. K. McCallum, et al (1999). "Text 
Classification from Labeled and Unlabeled Documents 
using EM." Machine Learning: 1-34. 
  
Roy, N. and A. McCallum (2001). Toward Optimal 
Active Learning through Sampling Estimation of Error 
Reduction. Eighteenth International Conference on 
Machine Learning. 
  
Smith, L., T. Rindflesch, et al (2004). "MedPost: A part 
of speech tagger for biomedical text." Bioinformatics 
20: 2320-2321. 
  
Tong, S. and D. Koller (2001). "Support vector machine 
active learning with applications to text classification." 
Journal of Machine Learning Research 2: 45-66. 
  
Weiss, G., K. McCarthy, et al (2007). Cost-Sensitive 
Learning vs. Sampling: Which is Best for Handling 
Unbalanced Classes with Unequal Error Costs? 
Proceedings of the 2007 International Conference on 
Data Mining. 
  
Zhang, T. (2004). Solving large scale linear prediction 
problems using stochastic gradient descent algorithms. 
Twenty-first International Conference on Machine 
learning, Omnipress. 
  
Zhang, T. and V. S. Iyengar (2002). "Recommender 
Systems Using Linear Classifiers." Journal of Machine 
Learning Research 2: 313-334. 
  
 
 
163
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 185?192,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Classifying Gene Sentences in Biomedical Literature by 
Combining High-Precision Gene Identifiers 
 
 
Sun Kim, Won Kim, Don Comeau, and W. John Wilbur 
National Center for Biotechnology Information 
National Library of Medicine, National Institutes of Health 
Bethesda, MD 20894, USA 
{sun.kim,won.kim,donald.comeau,john.wilbur}@nih.gov 
 
 
 
 
 
 
Abstract 
Gene name identification is a fundamental 
step to solve more complicated text mining 
problems such as gene normalization and pro-
tein-protein interactions. However, state-of-
the-art name identification methods are not 
yet sufficient for use in a fully automated sys-
tem. In this regard, a relaxed task, 
gene/protein sentence identification, may 
serve more effectively for manually searching 
and browsing biomedical literature. In this pa-
per, we set up a new task, gene/protein sen-
tence classification and propose an ensemble 
approach for addressing this problem. Well-
known named entity tools use similar gold-
standard sets for training and testing, which 
results in relatively poor performance for un-
known sets. We here explore how to combine 
diverse high-precision gene identifiers for 
more robust performance. The experimental 
results show that the proposed approach out-
performs BANNER as a stand-alone classifier 
for newly annotated sets as well as previous 
gold-standard sets. 
1 Introduction 
With the rapidly increasing biomedical literature, 
text mining has become popular for finding bio-
medical information in text. Among others, named 
entity recognition (NER) for bio-entities such as 
genes and proteins is a fundamental task because 
extracting biological relationships begins with enti-
ty identification. However, NER in biomedical 
literature is challenging due to the irregularities 
and ambiguities in bio-entities nomenclature (Yang 
et al, 2008). In particular, compound entity names 
make this problem difficult because it also requires 
deciding word boundaries. 
Recent bio-text competitions such as JNLPBA 
(Kim et al, 2004) and BioCreative (Lu et al, 2011; 
Smith et al, 2008) have evaluated NER systems 
for gene mentions. Even though progress has been 
made in several areas, gene identification methods 
are not yet sufficient for real-world use without 
human interaction (Arighi et al, 2011). Thus, at 
the present, a realistic suggestion is to use these 
algorithms as an aid to human curation and infor-
mation retrieval (Altman et al, 2008). 
In this paper, we define a new task, gene/protein 
sentence classification. A gene or protein sentence 
means a sentence including at least one specific 
gene or protein name. This new task has ad-
vantages over gene mention identification. First, 
gene name boundaries are not important at the sen-
tence level and human judges will agree more in 
their judgments. Second, highlighting gene sen-
tences may be more useful in manual search and 
browsing environments since this can be done 
more accurately and with less distraction from in-
correct annotations. 
To classify gene/protein sentences, we here pro-
pose an ensemble approach to combine different 
NER identifiers. Previous NER approaches are 
mostly developed on a small number of gold-
185
standard sets including GENIA (Kim et al, 2003) 
and BioCreative (Smith et al, 2008) corpora. The-
se sets help to find regular name patterns in a lim-
ited set of articles, but also limit the NER 
performance for real-world use. In the proposed 
approach, we use a Semantic Model and a Priority 
Model along with BANNER (Leaman and 
Gonzalez, 2008). The Semantic and Priority Mod-
els are used to provide more robust performance on 
gene/protein sentence classification because they 
utilize larger resources such as SemCat and Pub-
Med?R  to detect gene names. 
For experiments, we created three new gold-
standard sets to include cases appearing in the most 
recent publications. The experimental results show 
that our approach outperforms machine learning 
classifiers using unigrams and substring features as 
well as stand-alone BANNER classification on five 
gold-standard datasets. 
The paper is organized as follows. In Section 2, 
the ensemble approach for gene/protein sentence 
classification is described. Section 3 explains the 
gold-standard sets used for our experiments. Sec-
tion 4 presents and discusses the experimental re-
sults. Conclusions are drawn in Section 5. 
2 Methods 
 Figure 1. Method Overview. 
 
Figure 1 shows the overall framework for our pro-
posed approach. We basically assume that a main 
NER module works as a strong predictor, i.e., the 
majority of outputs obtained from this module are 
correct. We here use BANNER (Leaman and 
Gonzalez, 2008) as the main NER method because 
it adopts features and methods which are generally 
known to be effective for gene name recognition. 
While BANNER shows good performance on 
well-known gold-standard sets, it suffers from rela-
tively poor performance on unknown examples. To 
overcome this problem, we combine BANNER 
with two other predictors, a Sematic Model and a 
Priority Model. First, the Semantic Model and the 
Priority Model do not use previous gold-standard 
sets for training. Second, these two models learn 
name patterns in different ways, i.e., semantic rela-
tionships for the Semantic Model and positional 
and lexical information for the Priority Model. 
This combination of a strong predictor and two 
weaker but more general predictors can respond 
better to unknown name patterns. 
As described above, the proposed method main-
ly relies on outputs from different NER methods, 
whereas word features can still provide useful evi-
dence for discriminating gene and non-gene sen-
tences. Hence, we alternatively utilize word 
features such as unigrams and substrings along 
with NER features. For NER features only, the 
output is the sum of binary decisions from three 
NER modules. For word and NER features, the 
Huber classifier (Kim and Wilbur, 2011) is trained 
to combine the features. The parameter set in the 
Huber classifier is optimized to show the best clas-
sification performance on test sets. The following 
subsections describe each feature type used for 
gene sentence classification. 
2.1 Word Features 
Unigrams are a set of words obtained from to-
kenizing sentences on white space. All letters in 
unigrams are converted to lower case.  
Substrings are all contiguous substrings of a sen-
tence, sized three to six characters. This substring 
feature may help reduce the difference between 
distributions on training and test sets (Huang et al, 
2008). Substrings encode the roots and morpholo-
gy of words without identifying syllables or stems. 
They also capture neighboring patterns between 
words. 
2.2 BANNER 
BANNER is a freely available tool for identifying 
gene mentions. Due to its open-source policy and 
Java implementation, it has become a popular tool. 
BANNER uses conditional random fields (CRF) 
as a discriminative method and utilizes a set of fea-
ture types that are known to be good for identify-
ing gene names. The feature sets used are 
186
orthographic, morphological and shallow syntax 
features (Leaman and Gonzalez, 2008): 
 
(1) The part of speech (POS) of a token in a sen-
tence. 
(2) The lemma of a word. 
(3) 2, 3 and 4-character prefixes and suffixes. 
(4) 2 and 3 character n-grams including start-of-
token and end-of-token indicators. 
(5) Word patterns by converting upper-case letters, 
lower-case letters and digits to their corresponding 
representative characters (A, a, 0). 
(6) Numeric normalization by converting digits to 
?0?s. 
(7) Roman numerals. 
(8) Names of Greek letters. 
 
Even though BANNER covers most popular 
feature types, it does not apply semantic features or 
other post-processing procedures such as abbrevia-
tion processing. However, these features may not 
have much impact for reducing performance since 
our goal is to classify gene sentences, not gene 
mentions. 
2.3 Semantic Model  
The distributional approach to semantics (Harris, 
1954) has become more useful as computational 
power has increased, and we have found this ap-
proach helpful in the attempt to categorize entities 
found in text. We use a vector space approach to 
modeling semantics (Turney and Pantel, 2010) and 
compute our vectors as described in (Pantel and 
Lin, 2002) except we ignore the actual mutual in-
formation and just include a component of 1 if the 
dependency relation occurs at all for a word, else 
the component is set to 0. We constructed our vec-
tor space from all single tokens (a token must have 
an alphabetic character) throughout the titles and 
abstracts of the records in the whole of the Pub-
Med database based on a snapshot of the database 
taken in January 2012. We included only tokens 
that occurred in the data sufficient to accumulate 
10 or more dependency relations. There were just 
over 750 thousand token types that satisfied this 
condition and are represented in the space. We de-
note this space by h. We then took all the single 
tokens and all head words from multi-token strings 
in the categories ?chemical?, ?disease?, and 
?gene/protein? from an updated version of the  
SemCat database (Tanabe et al, 2006) and placed 
all the other SemCat categories similarly processed 
into a category we called ?other?. We consider on-
ly the tokens in these categories that also occur in 
our semantic vector space h and refer to these sets 
as Chemicalh , Diseaseh , inGene/Proteh , Otherh . Table 1 shows 
the size of overlaps between sets. 
 
 Chemicalh Diseaseh  inGene/Proteh  Otherh
Chemicalh 54478 209 4605 5495 
Diseaseh  8801 1139 169 
inGene/Proteh   76440 9466 
Otherh    127337 
Table 1. Pairwise overlap between sets representing the 
different categories. 
 
Class 'Chemicalh 'Diseaseh  ' inGene/Proteh  'OtherhStrings 49800 7589 70832 113815 
Ave. Prec. 0.8680 0.7060 0.9140 0.9120 
Table 2. Row two contains the number of unique strings 
in the four different semantic classes studied. The last 
row shows the mean average precisions from a 10-fold 
cross validation to learn how to distinguish each class 
from the union of the other three. 
 
In order to remove noise or ambiguity in the 
training set, we removed the tokens that appeared 
in more than one semantic class as follows. 
 ? ?
? ?
? ?
? ?inGene/ProteDiseaseChemicalOther'Other
DiseaseChemicalinGene/Prote
'
inGene/Prote
inGene/ProteChemicalDisease
'
Disease
inGene/ProteDiseaseChemical
'
Chemical
hhhhh
hhhh
hhhh
hhhh
????
???
???
???
       
   (1)
  
We then applied Support Vector Machine learn-
ing to the four resulting disjoint semantic classes in 
a one-against-all strategy to learn how to classify 
into the different classes. We used 31064.1 ??C  
based upon the size of the training set. As a test of 
this process we applied this same learning with 10-
fold cross validation on the training data and the 
results are given in the last row of Table 2. 
This Semantic Model is an efficient and general 
way to identify words indicating gene names. Un-
like other NER approaches, this model decides a 
target class solely based on a single word. Howev-
er, evaluating all tokens from sentences may in-
crease incorrect predictions. A dependency parser 
analyzes a sentence as a set of head- and depend-
187
ent-word combinations. Since gene names likely 
appear in describing a relationship with other enti-
ties, a name indicating a gene mention will be 
mostly placed in a dependent position. Thus, we 
first apply the C&C CCG parser (Curran et al, 
2007), and evaluate words in dependent positions 
only. 
 
2.4 Priority Model 
The Semantic Model detects four different catego-
ries for a single word. However, the Priority Model 
captures gene name patterns by analyzing the order 
of words and the character strings making up 
words. Since gene names are noun phrases in gen-
eral, we parse sentences and identify noun phrases 
first. These phrases are then evaluated using the 
Priority Model. 
The Priority Model is a statistical language 
model for named entity recognition (Tanabe and 
Wilbur, 2006). For named entities, a word to the 
right is more likely to be the word determining the 
nature of the entity than a word to the left in gen-
eral.  
Let T1 be the set of training data for class C1 and 
T2 for class C2. Let ? ? At ???  denote the set of all to-
kens used in names contained in 21 TT ? . For each 
token t?, A?? , it is assumed that there are associ-
ated two probabilities p? and q?, where p? is the 
probability that the appearance of the token t?  in a 
name indicates that name belongs to class C1 and 
q? is the probability that t? is a more reliable indi-
cator of the class of a name than any token to its 
left. Let )()2()1( ktttn ??? ??  be composed of the 
tokens on the right in the given order. Then the 
probability of n belonging to class C1 can be com-
puted as follows. 
 
? ? ? ? ? ?? ??
? ???
????
k
i
k
ij
jii
k
j
j qpqqpnCp
2 1
)()()(
2
)()1(1 11| ?????  (2) 
 
A limited memory BFGS method (Nash and 
Nocedal, 1991) and a variable order Markov model 
(Tanabe and Wilbur, 2006) are used to obtain p?   
and q?. An updated version of SemCat (Tanabe and 
Wilbur, 2006) was used to learn gene names. 
2.5 Semantic and Priority Models for High-
Precision Scores 
The Semantic and Priority Models learn gene 
names and other necessary information from the 
SemCat database, where names are semantically 
categorized based on UMLS?R  (Unified Medical 
Language System) Semantic Network. Even 
though the Semantic and Priority Models show 
good performance on names in SemCat, they can-
not avoid noise obtained from incorrect pre-
processing, e.g., parsing errors. The use of a gen-
eral category for training may also limit perfor-
mance. To obtain high-precision scores for our 
ensemble approach, it is important to reduce the 
number of false positives from predictions. Hence, 
we apply the Semantic and Priority Models on 
training sets, and mark false positive cases. These 
false positives are automatically removed from 
predictions on test sets. These false positive cases 
tend to be terms for entities too general to warrant 
annotation. 
Table 3 shows the classification performance 
with and without false positive corrections on 
training data. For both Semantic and Priority Mod-
els, precision rates are increased by removing false 
positives. Even though recall drops drastically, this 
does not cause a big problem in our setup since 
these models try to detect gene names which are 
not identified by BANNER. 
 
 SEM SEMFP PM PMFP
Accuracy 0.7907 0.7773 0.7805 0.8390 
Precision 0.7755 0.8510 0.7405 1.0000 
Recall 0.8323 0.6852 0.8799 0.6856 
F1 0.8029 0.7592 0.8042 0.8135 
Table 3. Performance changes on training set for the 
Semantic Model (SEM) and the Priority Model (PM). 
FP indicates that learned false positives were removed 
from predictions. 
3 Datasets 
For experiments, we rigorously tested the proposed 
method on gene mention gold-standard sets and 
newly annotated sets. GENETAG (Smith et al, 
2008) is the dataset released for BioCreative I and 
BioCreative II workshops. Since it is well-known 
for a gene mention gold-standard set, we used 
GENETAG as training data. 
For test data, two previous gold-standard sets 
were selected and new test sets were also built for 
gene sentence classification. YAPEX (Franzen et 
al., 2002) and JNLPBA (Kim et al, 2004) are con-
sidered of moderate difficulty because they are 
188
both related to GENIA corpus, a well-known gold-
standard set. However, Disease, Cell Line and 
Reptiles are considered as more difficult tasks be-
cause they represent new areas and contain recent-
ly published articles. The annotation guideline for 
new test sets basically followed those used in 
GENETAG (Tanabe et al, 2005), however do-
mains, complexes, subunits and promoters were 
not included in new sets. 
 
(1) ?Disease? Set: This set of 60 PubMed docu-
ments was obtained from two sources. Fifty of the 
documents were obtained from the 793 PubMed 
documents used to construct the AZDC (Leaman et 
al., 2009). They are the fifty most recent among 
these records. In addition to these fifty documents, 
ten documents were selected from PubMed on the 
topic of maize to add variety to the set and because 
one of the curators who worked with the set had 
experience studying the maize genome.  These ten 
were chosen as recent documents as of early March 
2012 and which contained the text word maize and 
discussed genetics.  The whole set of 60 docu-
ments were annotated by WJW to produce a gold 
standard. 
 
(2) ?CellLine? Set: This set comprised the most 
recent 50 documents satisfying the query ?cell 
line[MeSH]? in PubMed on March 15, 2012. This 
query was used to obtain documents which discuss 
cell lines, but most of these documents also discuss 
genes and for this reason the set was expected to be 
challenging. The set was annotated by WJW and 
DC and after independently annotating the set they 
reconciled differences to produce a final gold 
standard. 
 
(3) ?Reptiles? Set: This set comprised the most 
recent 50 documents satisfying the query ?reptiles 
AND genes [text]? in PubMed on March 15, 2012. 
This set was chosen because it would have little 
about human or model organisms and for this rea-
son it was expected to be challenging.  The set was 
annotated by WJW and DC and after independent-
ly annotating the set they reconciled differences to 
produce a final gold standard. 
 
For both ?CellLine? and ?Reptiles? Sets, the 
most recent data was chosen in an effort to make 
the task more challenging. Presumably such docu-
ments will contain more recently created names 
and phrases that do not appear in the older training 
data. This will then pose a more difficult test for 
NER systems. 
Table 4 shows all datasets used for training and 
testing. The new sets, ?Disease?, ?CellLine? and 
?Reptiles? are also freely available at 
http://www.ncbi.nlm.nih.gov/CBBresearch/Wilbur/
IRET/bionlp.zip 
 
 Positives Negatives Total 
GENETAG 10245 9755 20000 
YAPEX 1298 378 1676 
JNLPBA 17761 4641 22402 
Disease 345 251 596 
CellLine 211 217 428 
Reptiles 179 328 507 
Table 4. Datasets. ?GENETAG? was used for training 
data and others were used for test data. ?YAPEX? and 
?JNLPBA? were selected from previous gold-standard 
corpora. ?Disease?, ?Cell Line? and ?Reptiles? are new-
ly created from recent publications and considered as 
difficult sets. 
4 Results and Discussion  
In this paper, our goal is to achieve higher-
prediction performance on a wide range of gene 
sentences by combining multiple gene mention 
identifiers. The basic assumption here is that there 
is a strong predictor that performs well for previ-
ously known gold-standard datasets. For this 
strong predictor, we selected BANNER since it 
includes basic features that are known to give good 
performance. 
 
 Accuracy Precision Recall F1 
GENETAG 0.9794 0.9817 0.9779 0.9799 
YAPEX 0.9051 0.9304 0.9483 0.9392 
JNLPBA 0.8693 0.9349 0.8976 0.9159 
Disease 0.8591 0.9223 0.8261 0.8716 
Cell Line 0.8925 0.9146 0.8626 0.8878 
Reptiles 0.8994 0.8478 0.8715 0.8595 
Table 5. Performance of BANNER on training and test 
datasets. 
 
Table 5 presents the gene sentence classification 
performance of BANNER on training and test sets. 
We emphasize that performance here means that if 
BANNER annotates a gene/protein name in a sen-
tence, that sentence is classified as positive, other-
wise it is classified as negative. BANNER used 
GENETAG as training data, hence it shows excel-
lent classification performance on the same set. 
189
 Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+Ensemble
YAPEX 0.9414 0.9491 0.9685 0.9704 0.9624 0.9678 
JNLPBA 0.9512 0.9504 0.9584 0.9651 0.9625 0.9619 
Disease 0.8255 0.8852 0.9238 0.9501 0.9573 0.9610 
CellLine 0.8174 0.9004 0.9281 0.9539 0.9429 0.9496 
Reptiles 0.6684 0.7360 0.8696 0.9049 0.9001 0.8937 
Table 6. Average precision results on test sets for different feature combinations. 
 
 Unigrams Substrings BANNER Ensemble Uni+Ensemble Sub+Ensemble
YAPEX 0.8735 0.8819 0.9321 0.9196 0.9298 0.9336 
JNLPBA 0.8902 0.8938 0.9111 0.9197 0.9262 0.9264 
Disease 0.7449 0.7884 0.8479 0.8894 0.8957 0.9043 
CellLine 0.7346 0.8057 0.8698 0.9017 0.9052 0.8957 
Reptiles 0.6257 0.6816 0.8499 0.8199 0.8547 0.8547 
Table 7. Breakeven results on test sets for different feature combinations. 
 
 
? Just one fiber gene was revealed in this strain. 
 
? This transcription factor family is characterized by 
a DNA-binding alpha-subunit harboring the Runt 
domain and a secondary subunit, beta, which binds 
to the Runt domain and enhances its interaction 
with DNA.  
  
Figure 2. False positive examples including misleading 
words. 
 
YAPEX and JNLPBA are gold-standard sets that 
partially overlap the GENIA corpus. Since 
BANNER utilizes features from previous research 
on GENETAG, YAPEX and JNLPBA, we expect 
good performance on these data sets. For that rea-
son, we created the three additional gold-standard 
sets to use in this study, and we believe the per-
formance on these sets is more representative of 
what could be expected when our method is ap-
plied to cases recently appearing in the literature. 
Table 6 show average precision results for the 
different methods and all the test sets. GENETAG 
is left out because BANNER is trained on 
GENETAG. We observe improved performance of 
the ensemble methods over unigrams, substrings 
and BANNER. The improvement is small on 
YAPEX and JNLPBA, but larger for Disease, 
CellLine and Reptiles. We see that unigrams and 
substrings tend to add little to the plain ensemble. 
The MAP (Mean Average Precision) values in 
Table 6 are in contrast to the breakeven results in 
Table 7, where we see that unigrams and sub-
strings included with the ensemble generally give 
improved results.  Some of the unigrams and sub-
strings are specific enough to detect gene/protein 
names with high accuracy, and improve precision 
in top ranks in a way that cannot be duplicated by 
the annotations coming from Semantic or Priority 
Models or BANNER. In addition, substrings may 
capture more information than unigrams because 
of their greater generality. 
Some of our errors are due to false positive NER 
identifications. By this we mean a token was clas-
sified as a gene/protein by BANNER or the Se-
mantic or Priority Models. This often happens 
when the name indeed represents a gene/protein 
class, which is too general to be marked positive 
(Figure 2). A general way in which this problem 
could be approached is to process a large amount 
of literature discussing genes or proteins and look 
for names that are marked as positives by one of 
the NER identifiers, and which appear frequently 
in plural form as well as in the singular. Such 
names are likely general class names, and have a 
high probability to be false positives. 
Another type of error will arise when unseen to-
kens are encountered. If such tokens have string 
similarity to gene/protein names already encoun-
tered in the SemCat data, they may be recognized 
by the Priority Model. But there will be completely 
new strings. Then one must rely on context and 
this may not be adequate. We think there is little 
that can be done to solve this short of better lan-
guage understanding by computers. 
There is a benefit in considering whole sentenc-
es as opposed to named entities. By considering 
whole sentences, name boundaries become a non-
issue. For this reason, one can expect training data 
to be more accurate, i.e., human judges will tend to 
agree more in their judgments. This may allow for 
improved training and testing performance of ma-
190
chine learning methods. We believe it beneficial 
that human users are directed to sentences that con-
tain the entities they seek without necessity of 
viewing the less accurate entity specific tagging 
which they may then have to correct. 
5 Conclusions 
We defined a new task for classifying gene/protein 
sentences as an aid to human curation and infor-
mation retrieval. An ensemble approach was used 
to combine three different NER identifiers for im-
proved gene/protein sentence recognition. Our ex-
periments show that one can indeed find improved 
performance over a single NER identifier for this 
task. An additional advantage is that performance 
at this task is significantly more accurate than 
gene/protein NER. We believe this improved accu-
racy may benefit human users of this technology. 
We also make available to the research community 
three gold-standard gene mention sets, and two of 
these are taken from the most recent literature ap-
pearing in PubMed. 
Acknowledgments 
This work was supported by the Intramural Re-
search Program of the National Institutes of 
Health, National Library of Medicine. 
References  
R. B. Altman, C. M. Bergman, J. Blake, C. Blaschke, A. 
Cohen, F. Gannon, L. Grivell, U. Hahn, W. Hersh, L. 
Hirschman, L. J. Jensen, M. Krallinger, B. Mons, S. 
I. O'donoghue, M. C. Peitsch, D. Rebholz-
Schuhmann, H. Shatkay, and A. Valencia. 2008. Text 
mining for biology - the way forward: opinions from 
leading scientists. Genome Biol, 9 Suppl 2:S7. 
C. N. Arighi, Z. Lu, M. Krallinger, K. B. Cohen, W. J. 
Wilbur, A. Valencia, L. Hirschman, and C. H. Wu. 
2011. Overview of the BioCreative III workshop. 
BMC Bioinformatics, 12 Suppl 8:S1. 
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically 
motivated large-scale NLP with C&C and boxer. In 
Proceedings of the 45th Annual Meeting of the ACL 
on Interactive Poster and Demonstration Sessions, 
pages 33-36. 
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden, 
and J. Coster. 2002. Protein names and how to find 
them. Int J Med Inform, 67:49-61. 
Z. S. Harris. 1954. Distributional structure. Word, 
10:146-162. 
M. Huang, S. Ding, H. Wang, and X. Zhu. 2008. 
Mining physical protein-protein interactions from the 
literature. Genome Biol, 9 Suppl 2:S12. 
J.-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. 
GENIA corpus - semantically annotated corpus for 
bio-textmining. Bioinformatics, 19 Suppl 1:i180-
i182. 
J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. 
Collier. 2004. Introduction to the bio-entity 
recognition task at JNLPBA. In Proceedings of the 
International Joint Workshop on Natural Language 
Processing in Biomedicine and its Applications, 
pages 70-75. 
S. Kim and W. J. Wilbur. 2011. Classifying protein-
protein interaction articles using word and syntactic 
features. BMC Bioinformatics, 12 Suppl 8:S9. 
R. Leaman and G. Gonzalez. 2008. BANNER: an 
executable survey of advances in biomedical named 
entity recognition. In Proceedings of the Pacific 
Symposium on Biocomputing, pages 652-663. 
R. Leaman, C. Miller, and G. Gonzalez. 2009. Enabling 
recognition of diseases in biomedical text with 
machine learning: corpus and benchmark. In 2009 
Symposium on Languages in Biology and Medicine. 
Z. Lu, H. Y. Kao, C. H. Wei, M. Huang, J. Liu, C. J. 
Kuo, C. N. Hsu, R. T. Tsai, H. J. Dai, N. Okazaki, H. 
C. Cho, M. Gerner, I. Solt, S. Agarwal, F. Liu, D. 
Vishnyakova, P. Ruch, M. Romacker, F. Rinaldi, S. 
Bhattacharya, P. Srinivasan, H. Liu, M. Torii, S. 
Matos, D. Campos, K. Verspoor, K. M. Livingston, 
and W. J. Wilbur. 2011. The gene normalization task 
in BioCreative III. BMC Bioinformatics, 12 Suppl 
8:S2. 
S. G. Nash and J. Nocedal. 1991. A numerical study of 
the limited memory BFGS method and the truncated-
Newton method for large scale optimization. SIAM 
Journal on Optimization, 1:358-372. 
P. Pantel and D. Lin. 2002. Discovering word senses 
from text. In Proceedings of the Eighth ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining, pages 613-619. 
L. Smith, L. K. Tanabe, R. J. Ando, C. J. Kuo, I. F. 
Chung, C. N. Hsu, Y. S. Lin, R. Klinger, C. M. 
Friedrich, K. Ganchev, M. Torii, H. Liu, B. Haddow, 
C. A. Struble, R. J. Povinelli, A. Vlachos, W. A. 
Baumgartner, Jr., L. Hunter, B. Carpenter, R. T. Tsai, 
H. J. Dai, F. Liu, Y. Chen, C. Sun, S. Katrenko, P. 
Adriaans, C. Blaschke, R. Torres, M. Neves, P. 
Nakov, A. Divoli, M. Mana-Lopez, J. Mata, and W. 
J. Wilbur. 2008. Overview of BioCreative II gene 
mention recognition. Genome Biol, 9 Suppl 2:S2. 
L. Tanabe, L. H. Thom, W. Matten, D. C. Comeau, and 
W. J. Wilbur. 2006. SemCat: semantically 
categorized entities for genomics. In AMIA Annu 
Symp Proc, pages 754-758. 
191
L. Tanabe and W. J. Wilbur. 2006. A priority model for 
named entities. In Proceedings of the Workshop on 
Linking Natural Language Processing and Biology: 
Towards Deeper Biological Literature Analysis, 
pages 33-40. 
L. Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J. 
Wilbur. 2005. GENETAG: a tagged corpus for 
gene/protein named entity recognition. BMC 
Bioinformatics, 6 Suppl 1:S3. 
P. D. Turney and P. Pantel. 2010. From frequency to 
meaning: vector space models of semantics. Journal 
of Artificial Intelligence Research, 37:141-188. 
Z. Yang, H. Lin, and Y. Li. 2008. Exploiting the 
contextual cues for bio-entity name recognition in 
biomedical literature. J Biomed Inform, 41:580-587. 
 
 
192
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 76?85,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Generalizing an Approximate Subgraph Matching-based System to Extract
Events in Molecular Biology and Cancer Genetics
Haibin Liu
haibin.liu@nih.gov
NCBI, Bethesda, MD, USA
Karin Verspoor
karin.verspoor@nicta.com.au
NICTA, Melbourne, VIC, Australia
Donald C. Comeau
comeau@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Andrew MacKinlay
andrew.mackinlay@nicta.com.au
NICTA, Melbourne, VIC, Australia
W. John Wilbur
wilbur@ncbi.nlm.nih.gov
NCBI, Bethesda, MD, USA
Abstract
We participated in the BioNLP 2013 shared
tasks, addressing the GENIA (GE) and the Can-
cer Genetics (CG) event extraction tasks. Our
event extraction is based on the system we re-
cently proposed for mining relations and events
involving genes or proteins in the biomedical
literature using a novel, approximate subgraph
matching-based approach. In addition to han-
dling the GE task involving 13 event types uni-
formly related to molecular biology, we gener-
alized our system to address the CG task tar-
geting a challenging set of 40 event types re-
lated to cancer biology with various arguments
involving 18 kinds of biological entities. More-
over, we attempted to integrate a distributional
similarity model into our system to extend the
graph matching scheme for more events. In ad-
dition, we evaluated the impact of using paths of
all possible lengths among event participants as
key contextual dependencies to extract potential
events as compared to using only the shortest
paths within the framework of our system.
We achieved a 46.38% F-score in the CG task
and a 48.93% F-score in the GE task, ranking
3rd and 4th respectively. The consistent perfor-
mance confirms that our system generalizes well
to various event extraction tasks and scales to
handle a large number of event and entity types.
1 Introduction
Understanding the sophisticated interactions between
various components of biological systems and conse-
quences of these biological processes on the function
and behavior of the systems provides profound im-
pacts on translational biomedical research, leading to
more rapid development of new therapeutics and vac-
cines for combating diseases. For the past five years,
the BioNLP shared task series has served as an in-
strumental platform to promote the development of
text mining methodologies and resources for the au-
tomatic extraction of semantic events involving genes
or proteins such as gene expression, binding, or reg-
ulatory events from the biomedical literature (Kim et
al., 2009; Kim et al, 2011). An event typically cap-
tures the association of multiple participants of vary-
ing numbers and with diverse semantic roles (Anani-
adou et al, 2010). Since events often serve as partic-
ipants in other events, the extraction of such nested
event structures provides an integrated, network view
of these biological processes.
Previous shared tasks focused exclusively on
events at the molecular and sub-cellular level. How-
ever, biological processes at higher levels of organi-
zation are equally important, such as cell prolifer-
ation, organ growth and blood vessel development.
While preserving the classic event extraction tasks
such as the GE task, the BioNLP-ST 2013 broad-
ens the scope of application domains by introducing
many new issues in biology such as cancer genetics
and pathway curation. On behalf of NCBI (National
Center for Biotechnology Information), our team par-
ticipated in the GENIA (GE) task and the Cancer Ge-
netics (CG) task. Compared to the GE task that aims
for 13 types of events concerning the protein NF-?B,
the CG task targets a challenging set of 40 types of
biological processes related to the development and
progression of cancer involving 18 entity types. This
additionally requires that event extraction systems be
able to associate entities and events at the molecular
level with anatomy level effects and organism level
outcomes of cancer biology.
Our event extraction is based on the system we re-
cently proposed for mining relations and events in-
volving genes or proteins in the biomedical litera-
ture using a novel, Approximate Subgraph Matching-
based (ASM) approach (Liu et al, 2013a). When
evaluated on the GE task of the BioNLP-ST 2011, its
performance is comparable to the top systems in ex-
tracting 9 types of biological events. In the BioNLP-
76
ST 2013, we generalized our system to investigate
both CG and GE tasks. Moreover, we attempted to in-
tegrate a distributional similarity model into the sys-
tem to extend the graph matching scheme for more
events. The graph representation that considers paths
of all possible lengths (all-paths) between any two
nodes has been encoded in graph kernels used in
conjunction with Support Vector Machines (SVM),
and led to state-of-the-art performance in extracting
protein-protein (Airola et al, 2008) and drug-drug in-
teractions (Zhang et al, 2012). Borrowing from the
idea of the all-paths representation, in addition, we
evaluated the impact of using all-paths among event
participants as key contextual dependencies to extract
potential events as compared to using only the short-
est paths within the framework of our system.
The rest of the paper is organized as follows: In
Section 2, we briefly introduce our ASM-based event
extraction system. Section 3 describes our experi-
ments aiming to extend our system. Section 4 elab-
orates some implementation details and Section 5
presents our results and discussion. Finally, Section
6 summarizes the paper and introduces future work.
2 ASM-based Event Extraction
The underlying assumption of our event extraction
approach is that the contextual dependencies of each
stated biological event represent a typical context for
such events in the biomedical literature. Our ap-
proach falls into the machine learning category of
instance-based reasoning (Alpaydin, 2004). Specif-
ically, the key contextual structures are learned from
each labeled positive instance in a set of train-
ing data and maintained as event rules in the form
of subgraphs. Extraction of events is performed
by searching for an approximate subgraph isomor-
phism between key dependencies and input sen-
tence graphs using an approximate subgraph match-
ing (ASM) algorithm designed for literature-based
relational knowledge extraction (Liu et al, 2013a).
By introducing error tolerance into the graph match-
ing process, our approach is capable of retrieving
events encoded within complex dependency contexts
while maintaining the extraction precision at a high
level. The ASM algorithm has been released as open
source software1. See (Liu et al, 2013a) for more de-
tails on the ASM algorithm, its complexity and the
comparison with existing graph distance metrics.
Figure 1 illustrates the overall architecture of our
ASM-based system with three core components high-
1http://asmalgorithm.sourceforge.net
lighted: rule induction, sentence matching and rule
set optimization. Our approach focuses on extract-
ing events expressed within the boundaries of a single
sentence. It is also assumed that entities involved in
the target event have been annotated. Next, we briefly
describe the core components of the system.
Rule Induction
Preprocessing
Sentence Matching
Postprocessing
Training data Testing data
Rule Set
Optimization
Figure 1: ASM-based Event Extraction Framework
2.1 Rule Induction
Event rules are learned automatically using the fol-
lowing method. Starting with the dependency graph
of each training sentence, for each annotated event,
the shortest dependency path connecting the event
trigger to each event argument in the undirected ver-
sion of the graph is selected. While additional in-
formation such as individual words in each sentence
(bag-of-words), sequences of words (n-grams) and
semantic concepts is typically used in the state-of-
the-art supervised learning-based systems to cover a
broader context (Airola et al, 2008; Buyko et al,
2009; Bjo?rne et al, 2012), the shortest path be-
tween two tokens in the dependency graph is par-
ticularly likely to carry the most valuable informa-
tion about their mutual relationship (Bunescu and
Mooney, 2005a; Thomas et al, 2011b; Rinaldi et
al., 2010). In case there exists more than one short-
est path, all of them are considered. For multi-token
event triggers, the shortest path connecting every trig-
ger token to each event argument is extracted, and the
union of the paths is then computed for each trigger.
For regulatory events that take a sub-event as an ar-
gument, the shortest path is extracted so as to connect
the trigger of the main event to that of the sub-event.
For complex events that involve multiple argu-
ments, we computed the dependency path union of
all shortest paths from trigger to each event argument,
resulting in a graph in which all event participants are
jointly depicted. Individual dependency paths con-
necting triggers to each argument are also considered
to determine event arguments independently. If the
77
resulting arguments share the same event trigger, they
are grouped together to form a potential event. In our
approach, the individual paths aim to retrieve more
potential events while the path unions retain the pre-
cision advantage of joint inference.
While the dependencies of such paths are used as
the graph representation of the event, a detailed de-
scription records the participants of the event, their
semantic role labels and the associated nodes in the
graph. All participating biological entities are re-
placed with a tag denoting their entity type, e.g. ?Pro-
tein? or ?Organism?, to ensure generalization of the
learned rules. As a result, each annotated event is
generalized and transformed into a generic graph-
based rule. The resulting event rules are categorized
into different target event types.
2.2 Sentence Matching
Event extraction is achieved by matching the induced
rules to each testing sentence and applying the de-
scriptions of rule tokens (e.g. role labels) to the cor-
responding sentence tokens. Since rules and sentence
parses all possess a graph representation, event recog-
nition becomes a subgraph matching problem. We
introduced a novel approximate subgraph matching
(ASM) algorithm (Liu et al, 2013a) to identify a sub-
graph isomorphic to a rule graph within the graph of
a testing sentence. The ASM problem is defined as
follows.
Definition 1. An event rule graph Gr =
(Vr, Er) is approximately isomorphic to a subgraph
Ss of a sentence graph Gs = (Vs, Es), denoted
by Gr ?=t Ss ? Gs, if there is an injective
mapping f : Vr ? Vs such that, for a given
threshold t, t ? 0, the subgraph distance be-
tween Gr and Gs satisfies 0 ? subgraphDistf (Gr,
Gs) ? t, where subgraphDistf (Gr, Gs) = ws ?
structDistf (Gr, Gs) + wl ? labelDistf (Gr, Gs) +
wd ? directionalityDistf (Gr, Gs).
The subgraph distance is proposed to be the
weighted summation of three penalty-based measures
for a candidate match between the two graphs. The
measure structDist compares the distance between
each pair of matched nodes in one graph to the
distance between corresponding nodes in the other
graph, and accumulates the structural differences.
The distance in rule graphs is defined as the length
of the shortest path between two nodes. The distance
in sentence graphs is defined as the length of the path
between corresponding nodes that leads to minimum
structural difference with the distance in rule graphs.
Because dependency graphs are edge-labeled, ori-
ented graphs, the measures labelDist and direction-
alityDist evaluate respectively the overall differences
in edge labels and directionalities on the compared
path between each pair of matched nodes in the two
graphs. The real numbers ws, wl and wd are non-
negative weights associated with the measures.
The weights ws, wl and wd are defaulted to be
equal but can be tuned to change the emphasis of the
overall distance function. The distance threshold t
controls the isomorphism quality of the retrieved sub-
graphs from sentences. A smaller t allows only lim-
ited variations and always looks for a sentence sub-
graph as closely isomorphic to the rule graph as pos-
sible. A larger t enables the extraction of events de-
scribed in complicated dependency contexts, thus in-
creasing the chance of retrieving more events. How-
ever, it can incur a bigger search cost due to the eval-
uation of more potential solutions.
An iterative, bottom-up matching process is used
to ensure the extraction of complex and nested events.
Starting with the extraction of simple events, simple
event rules are first matched with a testing sentence.
Next, as potential arguments of higher level events,
obtained simple events continue to participate in the
subsequent matching process between complex event
rules and the sentence to initiate the iterative process
for detecting complex events with nested structures.
The process terminates when no new candidate event
is generated for the testing sentence.
During the matching phase we relax the event
rules that contain sub-event arguments such that any
matched event can substitute for the sub-event. We
believe that the contextual structures linking anno-
tated sub-events of a certain type are generalizable
to other event types. This relaxation increases the
chance of extracting complex events with nested
structures but still takes advantage of the contextual
constraints encoded in the rule graphs.
2.3 Rule Set Optimization
Typical of instance-based reasoners, the accuracy of
rules with which to compare an unseen sentence is
crucial to the success of our approach. For instance, a
Transcription rule encoding a noun compound mod-
ification dependency between ?TNF? and ?mRNA?
derived from an event context ?expression of TNF
mRNA? should not produce a Transcription event
for the general phrase ?level of TNF mRNA? even
though they share a matchable dependency. Such
matches result in false positive events.
78
Therefore, we measured the accuracy of each rule
ri in terms of its prediction result via Eq.(1). For rules
that produce at least one prediction, we ranked them
byAcc(ri) and excluded the ones with aAcc(ri) ratio
lower than an empirical threshold, e.g. 1:4.
Acc(ri) =
#correct predictions by ri
#total predictions by ri
(1)
Because of nested event structures, the removal
of some rules might incur a propagating effect on
rules relying on them to produce arguments for the
extraction of higher order events. Therefore, an it-
erative rule set optimization process, in which each
iteration performs sentence matching, rule ranking
and rule removal sequentially, is conducted, lead-
ing to a converged, optimized rule set. While the
ASM algorithm aims to extract more potential events,
this performance-based evaluation component en-
sures the precision of our event extraction framework.
3 Extensions to Event Extraction System
In the BioNLP-ST 2013, we attempted two different
ways to extend the current event extraction system:
(1) integrate a distributional similarity model into the
system to extend the graph matching scheme for more
events; (2) use paths of all possible lengths (all-paths)
among event participants as key contextual depen-
dencies to extract events. We next elaborate these
system extensions in detail.
3.1 Integrating Distributional Similarity Model
The proposed subgraph distance measure of the ASM
algorithm focuses on capturing differences in the
overall graph structure, edge labels and directional-
ities. However, when determining the injective node
mapping between graphs, the matching remains at the
surface word level.
In the current setting, various node features can be
considered when comparing two graph nodes, result-
ing in different matching criteria. The features in-
clude POS tags (P), event trigger (T), token lemmas
(L) and tokens themselves (A). For instance, a match-
ing criterion, ?P*+L?, requires that the relaxed POS
tags (P*) and the lemmatized form (L) of tokens be
identical for each rule node to match with a sentence
node. The relaxed POS allows the plural form of
nouns to match with the singular form, and the con-
jugations of verbs to match with each other. How-
ever, the inability to go beyond surface level match-
ing prevents node tokens that share similar meaning
but possess distinct orthography from matching with
each other. For instance, a mismatch between rule
token ?crucial? and a sentence token ?critical? could
lead to an undiscovered Positive regulation event.
We attempted to use only POS information in the
node matching scheme and observed a nearly 14%
increase in recall (Liu et al, 2013b). However, the
precision drops sharply, resulting in an undesirable
F-score. This indicates that the lexical information
is a critical supplement to the contextual dependency
constraints in accurately capturing events within the
framework of our system. Moreover, we attempted to
extend the node matching using the synsets of Word-
Net (Fellbaum, 1998) to allow tokens to match with
their synonyms (Liu et al, 2011). However, since
WordNet is developed for the general English lan-
guage, it relates biomedical terms e.g., ?expression?
with general words such as ?aspect? and ?face?, thus
leading to incorrect events.
In this work, we integrated a distributional simi-
larity model (DSM) into our node matching scheme
to further improve the generalization of event rules.
A distributional similarity model is constructed
based on the distributional hypothesis (Harris, 1954):
words that occur in the same contexts tend to share
similar meanings. We expect that the incorporation
of DSM will enable our system to capture matching
tokens in testing sentences that do not appear in the
training data while maintaining the extraction pre-
cision at a high level. There have been many ap-
proaches to compute the similarity between words
based on their distribution in a corpus (Landauer and
Dumais, 1997; Pantel and Lin, 2002). The output is a
ranked list of similar words to each word. We reim-
plemented the model proposed by (Pantel and Lin,
2002) in which each word is represented by a fea-
ture vector and each feature corresponds to a context
where the word appears. The value of the feature
is the pointwise mutual information (Manning and
Schu?tze, 1999) between the feature and the word. Let
c be a context and Fc(w) be the frequency count of a
word w occurring in context c. The pointwise mutual
information, miw,c between c and w is defined as:
miw,c =
Fc(w)
N?
i
Fi(w)
N ?
?
j
Fc(j)
N
(2)
where N =
?
i
?
j
Fi(j) is the total frequency count
of all words and their contexts.
Since mutual information is known to be biased
towards infrequent words/features, the above mutual
79
information value is multiplied by a discounting fac-
tor as described in (Pantel and Lin, 2002). The simi-
larity between two words is then computed using the
cosine coefficient (Salton and McGill, 1986) of their
mutual information vectors.
We experimented with two different approaches to
integrate the DSM into our event extraction system.
First, the model is directly embedded into the node
matching scheme. Once a match cannot be deter-
mined by surface tokens, the DSM is invoked to allow
a match if the sentence token appears in the list of the
top M most similar words to the rule token. Sec-
ond, additional event rules are generated by replac-
ing corresponding rule tokens with their top M most
similar words, rather than allow DSM to participate
in the node matching. While the first method mea-
sures the consolidated extraction ability of an event
rule by combining its DSM-generalized performance,
the second approach provides a chance to evaluate the
impact of each DSM-introduced similar word indi-
vidually on event extraction.
3.2 Adopting All-paths for Event Rules
Airola et al proposed an all-paths graph (APG) ker-
nel for extracting protein-protein interactions (PPI),
in which the kernel function counts weighted shared
dependency paths of all possible lengths (Airola et
al., 2008). Thomas et al adopted this kernel as
one of the three models used in the ensemble learn-
ing for extracting drug-drug interactions (Thomas et
al., 2011a) and won the recent DDIExtraction 2011
challenge (Segura-Bedmar et al, 2011). The JULIE
lab adapted the APG kernel to event extraction us-
ing syntactically pruned and semantically enriched
dependency graphs (Buyko et al, 2009).
The graph representation of the kernel consists of
two sub-representations: the full dependency parse
and the surface word sequence of the sentence where
a pair of interacting entities occurs. At the expense
of computational complexity, this representation en-
ables the kernel to explore broader contexts of an
interaction, thus taking advantage of the entire de-
pendency graph of the sentence. When comparing
two interaction instances, instead of using only the
shortest path that might not always provide suffi-
cient syntactic information about relations, the ker-
nel considers paths of all possible lengths between
any two nodes. More recently, a hash subgraph pair-
wise (HSP) kernel-based approach was also proposed
for drug-drug interactions and adopts the same graph
representation as the APG kernel (Zhang et al, 2012).
In contrast, the graph representation that our ASM
algorithm searches in a sentence is inherently re-
stricted to the shortest path among target entities in
event rules, as described in Section 2.2. Borrowing
from the idea of the all-path graph representation, in
this work we attempted to explore contexts beyond
the shortest paths to enrich our rule set. We evalu-
ated within the framework of our system the impact
of using acyclic paths of all possible lengths among
event participants as key contextual dependencies to
populate the event rule set as compared to using only
the shortest paths in the current system setting.
4 Implementation
4.1 Preprocessing
We employed the preprocessed data in the
BioC (Comeau et al, 2013) compliant XML format
provided by the shared task organizers as supporting
resources. The BioC project attempts to address
the interoperability among existing natural language
processing tools by providing a unified BioC XML
format. The supporting analyses include tokeniza-
tion, sentence segmentation, POS tagging and
lemmatization. Different syntactic parsers analyze
text based on different underlying methodologies, for
instances, the Stanford parser (Klein and Manning,
2003) performs joint inference over the product of an
unlexicalized Probabilistic Context-Free Grammar
(PCFG) parser and a lexicalized dependency parser
while the McClosky-Charniak-Johnson (Charniak)
parser (McClosky and Charniak, 2008) is based on
N -best parse reranking over a lexicalized PCFG
model. In order to take advantage of multiple aspects
of structural analysis of sentences, both Stanford
parser and Charniak parser, which are among the best
performing parsers trained on the GENIA Treebank
corpus, are used to parse the training sentences and
produce dependency graphs for learning event rules.
Only the Charniak parser is used on the testing
sentences in the event extraction phase.
4.2 ASM Parameter Setting
The GE task includes 13 different event types. Since
each type possesses its own event contexts, an indi-
vidual threshold te is assigned to each type. Together
with the 3 distance function weights ws, wl and wd,
the ASM requires 16 parameters for the GE event ex-
traction task. Similarly, the ASM requires 43 param-
eters to cater to the 40 diverse event types of the CG
task. As reported in (Liu et al, 2013a), we used a
genetic algorithm (GA) (Cormen et al, 2001) to au-
80
tomatically determine values of the 12 ASM param-
eters for the 2011 GE task using the training data.
We inherited these previously determined parameters
and adapted them into the 2013 tasks according to
the event type and its argument configuration. For in-
stance, ?Pathway? events in the CG task is assigned
the same te as the ?Binding? events in the GE task as
they possess similar argument configurations.
Table 1 shows the parameter setting for the 2013
GE task with the equal weights ws = wl = wd con-
straint. The graph node matching criterion ?P*+L?
that requires the relaxed POS tags and the token lem-
mas to be identical is used in the ASM.
Parameter Value Parameter Value
tGene expression 8 tUbiquitination 3
tTranscription 7 tBinding 7
tProtein catabolism 10 tRegulation 3
tPhosphorylation 8 tPositive regulation 3
tLocalization 8 tNegative regulation 3
tAcetylation 3 ws 10
tDeacetylation 3 wl 10
tProteinmodification 3 wd 10
Table 1: ASM parameter setting for the 2013 GE task
4.3 Distributional Similarity Model
In our implementation, we made following improve-
ments to the original Pantel model (Pantel and Lin,
2002): (1) lemmas of words generated by the Bi-
oLemmatizer (Liu et al, 2012) are used to achieve
generalization. The POS information is combined
with each lemmatized word to disambiguate its cat-
egory. (2) instead of the linear context where a
word occurs, we take advantage of dependency con-
texts inferred from dependency graphs. For instance,
?toxicity?amod? is extracted as a feature of the to-
ken ?nonhematopoietic JJ?. It captures the dependent
token, the type and the directionality of the depen-
dency. (3) the resulting miw,c is scaled into the [0, 1]
range by
? ?miw,c
1 + ? ?miw,c
to avoid greater miw,c values
dominating the similarity calculation between words.
An empirical ? = 0.01 is used. (4) while only the
immediate dependency contexts of a word are used
in our model, our implementation is flexible so that
contexts of various dependency depths could be taken
into consideration.
In order to cover a wide range of words and capture
the diverse usages of them in biomedical texts, in-
stead of resorting to an existing corpus, our distribu-
tional similarity model is built based on a random se-
lection of 5 million abstracts from the entire PubMed.
When computing miw,c, we filtered out contexts of
each word where the word occurs less than 5 times.
Eventually, the model contains 2.8 million distinct to-
kens and 0.4 million features. When it is queried with
an amino acid, e.g, ?lysine?, the top 15 tokens in the
resulting ranked list are all correct amino acid names.
5 Results and Discussion
This section reports our results on the GE and the CG
tasks respectively, including the attempted extensions
to our ASM-based event extraction system.
5.1 GE task
5.1.1 Datasets
The 2013 GE task dataset is composed of full-text
articles from PubMed Central, which are divided into
smaller segments by the task organizers according to
various sections of the articles. Table 2 presents some
statistics of the GE dataset.
Attributes Counted Training Development Testing
Full article segments 222 249 305
Proteins 3,571 4,138 4,359
Annotated events 2,817 3,199 3,301
Table 2: Statistics of BioNLP-ST 2013 GE dataset
As distributed, the development set is bigger than
the training set. For better system generalization, we
randomly reshuffled the data and created a 353/118
training/development division, a roughly 3:1 ratio
consistent with the settings in previous GE tasks.
The results reported on the training/development data
thereafter are based on our new data partition.
5.1.2 GE Results on Development Set
Table 3 shows the event extraction results on the 118
development documents based on event rules derived
from different parsers. Only the numbers of unique,
optimized rules are reported and those that possess
isomorphic graph representations determined by an
Exact Subgraph Matching (ESM) algorithm (Liu et
al., 2013b) are removed. The ensemble rule set com-
bines rules derived from both parsers and achieves
a better performance than that of using individual
parsers. It makes sense that the Charniak parser is
favored and leads to a performance close to the en-
semble performance because sentences from which
events are extracted are parsed by the Charniak parser
as well. However, we retained the additional rules
from the Stanford parser in the hope that they may
contribute to the testing data.
When embedding the distributional similarity
model (DSM) directly into the graph node matching
81
Parser Type Event Rule Recall Precision F-score
Charniak 2,923 47.01% 66.01% 54.91%
Stanford 3,305 43.66% 67.67% 53.08%
Ensemble 4,617 47.45% 65.65% 55.09%
Table 3: Performance of using different parsers
scheme, we performed the DSM on all rule tokens ex-
cept biological entities, meaning that for each rule to-
ken, if a match will be granted if a rule token appears
in the top M most similar word list of a sentence to-
ken, e.g., ?DSM 3? denotes the top 3 similar words
determined by the DSM. We further performed DSM
only on trigger tokens for comparison, as presented
in Table 4.
All Tokens Recall Precision F-score
DSM 1 47.98% 52.56% 50.17%
DSM 3 48.68% 35.07% 40.77%
DSM 10 53.43% 19.38% 28.44%
Trigger Tokens Recall Precision F-score
DSM 1 48.06% 54.22% 50.95%
DSM 3 48.59% 37.00% 42.01%
DSM 10 53.35% 24.65% 33.72%
Table 4: Performance of integrated DSM
Even though the DSM helps to substantially in-
crease the recall to 53.43%, we observed a significant
precision drop which leads to an inferior F-score to
the ensemble baseline in Table 3. A close evaluation
of the generated graph matches reveals that antonyms
produced by the DSM contributes to most of the false
positive events. For instance, the most similar words
for the verb ?increase? and the adjective ?high? re-
turned by the model are ?decrease? and ?low? be-
cause they tend to occur in the same contexts. Fur-
ther investigation is needed to automatically filter out
the antonyms. When generating additional rules us-
ing the top M most similar words from the DSM,
since all the rules undergo the optimization process,
the event extraction precision is ensured. However,
the recall increase from simple events is diluted by
the counter effect of the introduced false positives in
detecting regulation-related complex events, result-
ing in a comparable performance to the baseline.
Table 5 gives the performance comparison of us-
ing all-paths and the shortest paths in our event ex-
traction system. Using all-paths does not bring in a
significant improvement in F-score but takes 27 it-
erations to optimize as compared to the 5-iteration
optimization on shortest paths. Most of the rules in-
duced from all-paths are eventually discarded by the
optimization process. The all-paths graph represen-
tation was motivated by the observation that short-
est paths between candidate entities often exclude
relation-signaling words when detecting binary re-
lationships (Airola et al, 2008). Exploring broader
contexts ensures such words to be considered. In the
event extraction task, however, since triggers have
been annotated, they are naturally incorporated into
the shortest paths connecting trigger to each event ar-
gument. This in part explains why contexts beyond
shortest paths did not bring in an appreciable benefit.
All Tokens Recall Precision F-score
All-paths 48.77% 64.64% 55.59%
Shortest paths 47.45% 65.65% 55.09%
Table 5: Performance of using all-paths
5.1.3 GE Results on Testing Set
Since integrating the DSM and all-paths do not pro-
vide significant performance improvements to our
system, we decided to retain the original settings in
the ASM when extracting events from the testing
data. While most of the 2011 shared task datasets are
composed of PubMed abstracts compared to full-text
articles in the 2013 GE task, our system focuses on
extracting events expressed within the boundaries of
a single sentence. Therefore, in order to take advan-
tage of existing annotated resources, we incorporated
the annotated data of 2011 GE task and EPI (Epi-
genetics and Post-translational Modifications) task to
enrich the training instances of corresponding event
types of the 2013 GE task. Eventually, we obtained a
total of 14,448 rules of different event types from our
training data. In practice, it takes the ASM less than a
second to match the entire rule set with one document
and return results.
Our submitted system achieves a 48.93% F-score
on the 305 testing documents of the GE task, ranking
4th among 12 participating teams. Table 6 presents
the performance of the top eight systems.
System Recall Precision F-score
EVEX 45.44% 58.03% 50.97%
TEES 2.1 46.17% 56.32% 50.74%
BioSEM 42.47% 62.83% 50.68%
NCBI 40.53% 61.72% 48.93%
DlutNLP 40.81% 57.00% 47.56%
HDS4NLP 37.11% 51.19% 43.03%
NICTANLM 36.99% 50.68% 42.77%
USheff 31.69% 63.28% 42.23%
Table 6: Performance of top 8 systems in GE task
Our performance is within a reasonable mar-
gin from the best-performing system ?EVEX?, and
shows an overall superior precision over most partic-
ipating teams; only two of the top 5 systems obtained
82
a precision in the 60% range. Particularly for the
regulation-related complex events, we are the only
team that achieved a precision over 55% among all
12 participating systems. This indicates that event
rules automatically learned and optimized over train-
ing data generalize well to the unseen text, and have
the ability to identify precisely corresponding events.
We further evaluated the impact of the additonal
training instances from 2011 tasks and the ensemble
rule set derived from different parsers as presented
in Table 7. With the help from the 2011 data, our
F-score is increased by 3% and we became the only
team that detected ?Ubiquitination? events from test-
ing data. In addition, rules derived from the Stanford
parser do not provide additional benefits on the test-
ing data compared to using the Charniak parser alone.
System Attribute Recall Precision F-score
Ensemble 2013 + 2011 data 40.53% 61.72% 48.93%
Ensemble 2013 data 35.63% 63.91% 45.75%
Charniak 2013 data 35.29% 65.71% 45.92%
Table 7: Impact of 2011 data and ensemble rule set
5.2 CG task
5.2.1 Datasets
The CG task dataset is prepared based on a previ-
ously released corpus of angiogenesis domain ab-
stracts (Wang et al, 2011). It targets a challenging
set of 40 types of biological processes related to the
development and progression of cancer involving 18
entity types (Pyysalo et al, 2012). Table 8 presents
some statistics of the CG dataset.
Attributes Counted Training Development Testing
Abstracts 300 100 200
Entities 10,935 3,634 6,955
Annotated events 8,803 2,915 5,972
Table 8: Statistics of BioNLP-ST 2013 CG dataset
5.2.2 CG Results on Testing Set
We generalized our event extraction system to the CG
task and the corresponding annotated data of the 2011
tasks is also incorporated in the training phase to ob-
tain the optimized event rule set. Due to time con-
straints, the impact of integrating the DSM and all-
paths is not evaluated on the CG task. We achieved
a 46.38% F-score on the 200 testing documents of
the CG task, ranking 3rd among the 6 participating
teams. Table 9 gives the primary evaluation results of
the 6 participating teams; only ?TEES-2.1? and we
participated in both GE and CG tasks. The detailed
results of each of the targeted 40 event types is avail-
able from the official CG task website.
Team Recall Precision F-score
TEES-2.1 48.76% 64.17% 55.41%
NaCTeM 48.83% 55.82% 52.09%
NCBI 38.28% 58.84% 46.38%
RelAgent 41.73% 49.58% 45.32%
UET-NII 19.66% 62.73% 29.94%
ISI 16.44% 47.83% 24.47%
Table 9: Performance of all systems in 2013 CG task
Inconsistent with other biological entities, the en-
tity annotation for the optional ?Site? argument in-
volved in events such as ?Binding?, ?Mutation? and
?Phosphorylation? are not provided by the task orga-
nizers. We consider that detecting ?Site? entities is
related to entity detection and we would like to focus
our system on the event extraction itself. Thus, we
decided to ignore the ?Site? argument in our system.
However, a problem will arise that even though the
other arguments are correctly identified for an event,
it might still be evaluated as false positive if a ?Site?
argument is not detected. This results in both false
positive and false negative events. In addition, since
we did not perform the secondary task which requires
us to detect modifications of the predicted events, in-
cluding negation and speculation, about 7.5% anno-
tated instances in the testing data are thus missed,
causing damage to our recall in the overall evalua-
tion. The organizers have agreed to issue an additonal
evaluation that will focus on core event extraction tar-
gets excluding optional arguments such as ?Site? and
the secondary task. We will conduct more detailed
analysis on the results once they are made available.
6 Conclusion and Future Work
In the BioNLP-ST 2013, we generalized our ASM-
based system to address both GE and CG tasks.
We attempted to integrate a distributional similarity
model into our system to extend the graph match-
ing scheme. We also evaluated the impact of using
paths of all possible lengths among event participants
as key contextual dependencies to extract potential
events as compared to using only the shortest paths
within the framework of our system.
We achieved a 46.38% F-score in the CG task and
a 48.93% F-score in the GE task, ranking 3rd and
4th respectively. While the distributional similarity
model did not improve the overall performance of our
system in the tasks, we would like to further investi-
gate the antonym problem introduced by the model in
our future work.
83
Acknowledgments
This research was supported by the Intramural Re-
search Program of the NIH, NLM.
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski1. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9 Suppl 11:s2.
Ethem Alpaydin. 2004. Introduction to Machine Learn-
ing. MIT Press.
Sophia Ananiadou, Sampo Pyysalo, Jun?ichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for systems
biology by text mining the literature. Trends in Biotech-
nology, 28(7):381?390.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012. Uni-
versity of turku in the BioNLP?11 shared task. BMC
Bioinformatics, 13 Suppl 11:S4.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing, pages 724?731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Information
Processing Systems (NIPS). Vancouver, BC, December.
Ekaterina Buyko, Erik Faessler, Joachim Wermter, and
Udo Hahn. 2009. Event extraction from trimmed de-
pendency graphs. In BioNLP ?09: Proceedings of the
Workshop on BioNLP, pages 19?27, Morristown, NJ,
USA. Association for Computational Linguistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Verspoor,
Thomas C. Wiegers, Cathy H. Wu, and W. John Wilbur.
2013. BioC: A minimalist approach to interoperability
for biomedical text processing. submitted.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. The MIT Press.
Christiane Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. Bradford Books.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of BioNLP Shared Task 2009 Workshop, pages
1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of BioNLP shared task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop, pages 1?6. As-
sociation for Computational Linguistics, June.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computational
Linguistics, pages 423?430. Association for Computa-
tional Linguistics.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, 104(2):211?240.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching ap-
proach for information extraction from biomedical text.
In Proceedings of BioNLP Shared Task 2011 Work-
shop, pages 164?172. Association for Computational
Linguistics, June.
Haibin Liu, Tom Christiansen, William A Baumgartner,
and Karin Verspoor. 2012. Biolemmatizer: a lemmati-
zation tool for morphological processing of biomedical
text. Journal of Biomedical Semantics, 3:3.
Haibin Liu, Lawrence Hunter, Vlado Keselj, and Karin
Verspoor. 2013a. Approximate subgraph matching-
based literature mining for biomedical events and re-
lations. PLOS ONE, 8:4 e60954.
Haibin Liu, Vlado Keselj, and Christian Blouin. 2013b.
Exploring a subgraph matching approach for extracting
biological events from literature. Computational Intel-
ligence.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language processing.
MIT Press, Cambridge, MA, USA.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of the
Association for Computational Linguistics, pages 101?
104, Columbus, Ohio. The Association for Computer
Linguistics.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, KDD ?02, pages 613?619,
New York, NY, USA. ACM.
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol
Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012.
Event extraction across multiple levels of biological or-
ganization. Bioinformatics, 28:i575?i581.
Fabio Rinaldi, Gerold Schneider, Kaarel Kaljurand, Simon
Clematide, Thrse Vachon, and Martin Romacker. 2010.
Ontogene in BioCreative II.5. IEEE/ACM Trans. Com-
put. Biology Bioinform., 7(3):472?480.
84
Gerard Salton and Michael J. McGill. 1986. Introduction
to Modern Information Retrieval. McGraw-Hill, Inc.,
New York, NY, USA.
Isabel Segura-Bedmar, Paloma Martinez, and Daniel
Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011
Challenge Task: Extraction of Drug-Drug Interactions
from Biomedical Texts. In Proceedings of the 1st Chal-
lenge Task on Drug-Drug Interaction Extraction 2011,
pages 1?9.
Philippe Thomas, Mariana Neves, Illes Solt, Domonkos
Tikk, and Ulf Leser. 2011a. Relation extraction for
drug-drug interactions using ensemble learning. In Pro-
ceedings of DDIExtraction-2011 challenge task, pages
11?18.
Philippe Thomas, Stefan Pietschmann, Ille?s Solt,
Domonkos Tikk, and Ulf Leser. 2011b. Not all
links are equal: Exploiting dependency types for the
extraction of protein-protein interactions from text. In
Proceedings of BioNLP 2011 Workshop, pages 1?9.
Association for Computational Linguistics, June.
Domonkos Tikk, Philippe Thomas, Peter Palaga, Jo?rg
Hakenberg, and Ulf Leser. 2010. A comprehensive
benchmark of kernel methods to extract protein?protein
interactions from literature. PLoS Computational Biol-
ogy, 6:e1000837, July.
Xinglong Wang, Iain McKendrick, Ian Barrett, Ian Dix,
Tim French, Jun?ichi Tsujii, and Sophia Ananiadou.
2011. Automatic extraction of angiogenesis bioprocess
from text. Bioinformatics, 27(19):2730?2737.
Yijia Zhang, Hongfei Lin, Zhihao Yang, Jian Wang, and
Yanpeng Li. 2012. A single kernel-based approach to
extract drug-drug interactions from biomedical litera-
ture. PLOS ONE, 7(11): e48901.
85
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 99?103,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013: Supporting Resources
Pontus Stenetorp 1 Wiktoria Golik 2 Thierry Hamon 3
Donald C. Comeau 4 Rezarta Islamaj Dog?an 4 Haibin Liu 4 W. John Wilbur 4
1 National Institute of Informatics, Tokyo, Japan
2 French National Institute for Agricultural Research (INRA), Jouy-en-Josas, France
3 University Paris 13, Paris, France
4 National Center for Biotechnology Information, National Library of Medicine,
National Institutes of Health, Bethesda, MD, USA
pontus@nii.ac.jp wiktoria.golik@jouy.inra.fr thierry.hamon@univ-paris13.fr
{comeau,islamaj,liuh11,wilbur}@ncbi.nlm.nih.gov
Abstract
This paper describes the technical con-
tribution of the supporting resources pro-
vided for the BioNLP Shared Task 2013.
Following the tradition of the previous
two BioNLP Shared Task events, the task
organisers and several external groups
sought to make system development easier
for the task participants by providing auto-
matically generated analyses using a vari-
ety of automated tools. Providing analy-
ses created by different tools that address
the same task also enables extrinsic evalu-
ation of the tools through the evaluation of
their contributions to the event extraction
task. Such evaluation can improve under-
standing of the applicability and benefits
of specific tools and representations. The
supporting resources described in this pa-
per will continue to be publicly available
from the shared task homepage
http://2013.bionlp-st.org/
1 Introduction
The BioNLP Shared Task (ST), first organised in
2009, is an ongoing series of events focusing on
novel challenges in biomedical domain informa-
tion extraction. In the first BioNLP ST, the or-
ganisers provided the participants with automat-
ically generated syntactic analyses from a variety
of Natural Language Processing (NLP) tools (Kim
et al, 2009) and similar syntactic analyses have
since then been a key component of the best per-
forming systems participating in the shared tasks.
This initial work was followed up by a similar ef-
fort in the second event in the series (Kim et al,
2011), extended by the inclusion of software tools
and contributions from the broader BioNLP com-
munity in addition to task organisers (Stenetorp et
al., 2011).
Although no formal study was carried out to es-
timate the extent to which the participants utilised
the supporting resources in these previous events,
we note that six participating groups mention us-
ing the supporting resources in published descrip-
tions of their methods (Emadzadeh et al, 2011;
McClosky et al, 2011; McGrath et al, 2011;
Nguyen and Tsuruoka, 2011; Bjo?rne et al, 2012;
Vlachos and Craven, 2012). These resources have
been available also after the original tasks, and
several subsequent studies have also built on the
resources. Van Landeghem et al (2012) applied a
visualisation tool that was made available as a part
of the supporting resources, Vlachos (2012) em-
ployed the syntactic parses in a follow-up study
on event extraction, Van Landeghem et al (2013)
used the parsing pipeline created to produce the
syntactic analyses, and Stenetorp et al (2012) pre-
sented a study of the compatibility of two different
representations for negation and speculation anno-
tation included in the data.
These research contributions and the overall
positive reception of the supporting resources
prompted us to continue to provide supporting re-
sources for the BioNLP Shared Task 2013. This
paper presents the details of this technical contri-
bution.
2 Organisation
Following the practice established in the
BioNLP ST 2011, the organisers issued an
open call for supporting resources, welcoming
contributions relevant to the task from all authors
of NLP tools. In the call it was mentioned that
points such as availability for research purposes,
support for well-established formats and access
99
Name Annotations Availability
BioC Lemmas and syntactic constituents Source
BioYaTeA Terms, lemmas, part-of-speech and syntactic constituencies Source
Cocoa Entities Web API
Table 1: Summary of tools/analyses provided by external groups.
to technical documentation would considered
favourable (but not required) and each supporting
resource provider was asked to write a brief
description of their tools and how they could
potentially be applied to aid other systems in the
event extraction task. This call was answered
by three research groups that offered to provide
a variety of semantic and syntactic analyses.
These analyses were provided to the shared
task participants along with additional syntactic
analyses created by the organisers.
However, some of the supporting resource
providers were also participants in the main event
extraction tasks, and giving them advance access
to the annotated texts for the purpose of creating
the contributed analyses could have given those
groups an advantage over others. To address this
issue, the texts were made publicly available one
week prior to the release of the annotations for
each set of texts. During this week, the supporting
analysis providers annotated the texts using their
automated tools and then handed the analyses to
the shared task organisers, who made them avail-
able to the task participants via the shared task
homepage.
3 Analyses by External Groups
This section describes the tools that were applied
to create supporting resources by the three exter-
nal groups. These contributions are summarised in
Table 1.
BioC Don Comeau, Rezarta Islamaj, Haibin
Liu and John Wilbur of the National Center for
Biotechnology Information provided the output of
the shallow parser MedPost (Smith et al, 2004)
and the BioLemmatizer tool (Liu et al, 2012),
supplied in the BioC XML format1 for annota-
tion interchange (Comeau et al, 2013). The BioC
format address the problem of interoperability be-
tween different tools and platforms by providing a
unified format for use by various tools. Both Med-
Post and BioLemmatizer are specifically designed
1http://bioc.sourceforge.net/
for biomedical texts. The former annotates parts-
of-speech and performs sentence splitting and to-
kenisation, while the latter performs lemmatisa-
tion. In order to make it easier for participants
to get started with the BioC XML format, the
providers also supplied example code for parsing
the format in both the Java and C++ programming
languages.
BioYaTeA Wiktoria Golik of the French Na-
tional Institute for Agricultural Research (INRA)
and Thierry Hamon of University Paris 13 pro-
vided analyses created by BioYaTeA2 (Golik et
al., 2013). BioYaTeA is a modified version of the
YaTeA term extraction tool (Aubin and Hamon,
2006) adapted to the biomedical domain. Working
on a noun-phrase level, BioYaTeA provides anno-
tations such as lemmas, parts-of-speech, and con-
stituent analysis. The output formats used were a
simple tabular format as well as BioYaTeA-XML,
an XML representation specific to the tool.
Cocoa S. V. Ramanan of RelAgent Private Ltd
provided the output of the Compact cover anno-
tator (Cocoa) for biological noun phrases.3 Co-
coa provides noun phrase-level entity annotations
for over 20 different semantic categories such as
macromolecules, chemicals, proteins and organ-
isms. These annotations were made available for
the annotated texts for the shared task along with
the opportunity for the participants to use the Co-
coa web API to annotate any text they may con-
sider beneficial for their system. The data format
used by Cocoa is a subset of the standoff format
used for the shared task entity annotations, and it
should thus be easy to integrate into existing event
extraction systems.
4 Analyses by Task Organisers
This section describes the syntactic parsers ap-
plied by the task organisers and the pre-processing
2http://search.cpan.org/?bibliome/
Lingua-BioYaTeA/
3http://npjoint.com/
100
Name Model Availability
Enju Biomedical Binary
Stanford Combination Binary, Source
McCCJ Biomedical Source
Table 2: Parsers used for the syntactic analyses.
and format conversions applied to their output.
The applied parsers are listed in Table 2.
4.1 Syntactic Parsers
Enju Enju (Miyao and Tsujii, 2008) is a deep
parser based on the Head-Driven Phrase Struc-
ture Grammar (HPSG) formalism. Enju analyses
its input in terms of phrase structure trees with
predicate-argument structure links, represented in
a specialised XML-format. To make the analyses
of the parser more accessible to participants, we
converted its output into the Penn Treebank (PTB)
format using tools included with the parser. The
use of the PTB format also allow for its output to
be exchanged freely for that of the other two syn-
tactic parsers and facilitates further conversions
into dependency representations.
McCCJ The BLLIP Parser (Charniak and John-
son, 2005), also variously known as the Charniak
parser, the Charniak-Johnson parser, or the Brown
reranking parser, has been applied in numerous
biomedical domain NLP efforts, frequently using
the self-trained biomedical model of McClosky
(2010) (i.e. the McClosky-Charniak-Johnson or
McCCJ parser). The BLLIP Parser is a con-
stituency (phrase structure) parser and the applied
model produces PTB analyses as its native out-
put. These analyses were made available to par-
ticipants without modification.
Stanford The Stanford Parser (Klein and Man-
ning, 2002) is a widely used publicly available
syntactic parser. As for the Enju and BLLIP
parsers, a model trained on a dataset incorporating
biomedical domain annotations is available also
for the Stanford parser. Like the BLLIP parser,
the Stanford parser is constituency-based and pro-
duces PTB analyses, which were provided to task
participants. The Stanford tools additionally in-
corporate methods for automatic conversion from
this format to other representations, discussed fur-
ther below.
4.2 Pre-processing and Conversions
To create the syntactic analyses from the Enju,
BLLIP and Stanford Parser systems, we first ap-
plied a uniform set of pre-processing steps in order
to normalise over differences in e.g. tokenisation
and thus ensure that the task participants can eas-
ily swap the output of one system for another. This
pre-processing was identical to that applied in the
BioNLP 2011 Shared Task, and included sentence
splitting of the annotated texts using the Genia
Sentence Splitter,4 the application of a set of post-
processing heuristics to correct frequently occur-
ring sentence splitting errors, and Genia Treebank-
like tokenisation (Tateisi et al, 2004) using a to-
kenisation script created by the shared task organ-
isers. 5
Since several studies have indicated that repre-
sentations of syntax and aspects of syntactic de-
pendency formalism differ in their applicability to
support information extraction tasks (Buyko and
Hahn, 2010; Miwa et al, 2010; Quirk et al, 2011),
we further converted the output of each of the
parsers from the PTB representation into three
other representations: CoNNL-X, Stanford De-
pendencies and Stanford Collapsed Dependencies.
For the CoNLL-X format we employed the con-
version tool of Johansson and Nugues (2007), and
for the two Stanford Dependency variants we used
the converter provided with the Stanford CoreNLP
tools (de Marneffe et al, 2006). These analyses
were provided to participants in the output for-
mats created by the respective tools, i.e. the TAB-
separated column-oriented format CoNLL and the
custom text-based format of the Stanford Depen-
dencies.
5 Results and Discussion
Just like in previous years the supporting resources
were well-received by the shared task participants
and as many as five participating teams mentioned
utilising the supporting resources in their initial
submissions (at the time of writing, the camera-
ready versions were not yet available). This level
of usage of the supporting resources by the partici-
pants is thus comparable to what was observed for
the 2011 shared task.
Following in the tradition of the 2011 support-
4https://github.com/ninjin/geniass
5https://github.com/ninjin/bionlp_
st_2013_supporting/blob/master/tls/
GTB-tokenize.pl
101
ing resources, to aim for reproducibility, the pro-
cessing pipeline containing pre/post-processing
and conversion scripts for all the syntactic parses
has been made publicly available under an open
licence.6 The repository containing the pipeline
also contains detailed instructions on how to re-
produce the output and how it can potentially be
applied to other texts.
Given the experience of the organisers in
analysing medium-sized corpora with a variety of
syntactic parsers, many applied repeatedly over
several years, we are also happy to report that the
robustness of several publicly available parsers has
recently improved noticeably. Random crashes,
corrupt outputs and similar failures appear to be
transitioning from being expected to rare occur-
rences.
In this paper, we have introduced the supporting
resources provided for the BioNLP 2013 Shared
Task by the task organisers and external groups.
These resources included both syntactic and se-
mantic annotations and were provided to allow the
participants to focus on the various novel chal-
lenges of constructing event extraction systems by
minimizing the need for each group to separately
perform standard processing steps such as syntac-
tic analysis.
Acknowledgements
We would like to give special thanks to Richard
Johansson for providing and allowing us to dis-
tribute an improved and updated version of his for-
mat conversion tool.7 We would also like to ex-
press our appreciation to the broader NLP com-
munity for their continued efforts to improve the
availability of both code and data, thus enabling
other researchers to stand on the shoulders of gi-
ants.
This work was partially supported by the
Quaero programme funded by OSEO (the French
agency for innovation). The research of Donald
C. Comeau, Rezarta Islamaj Dog?an, Haibin Liu
and W. John Wilbur was supported by the Intra-
mural Research Program of the National Institutes
of Health (NIH), National Library of Medicine
(NLM).
6https://github.com/ninjin/bionlp_st_
2013_supporting
7https://github.com/ninjin/
pennconverter
References
Sophie Aubin and Thierry Hamon. 2006. Improving
term extraction with terminological resources. In
Advances in Natural Language Processing, pages
380?387. Springer.
Jari Bjo?rne, Filip Ginter, and Tapio Salakoski. 2012.
University of Turku in the BioNLP?11 Shared Task.
BMC Bioinformatics, 13(Suppl 11):S4.
Ekaterina Buyko and Udo Hahn. 2010. Evaluating
the Impact of Alternative Dependency Graph Encod-
ings on Solving Event Extraction Tasks. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 982?992,
Cambridge, MA, October.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180. Association for Computational Lin-
guistics.
Donald C. Comeau, Rezarta Islamaj Dog?an, Paolo Ci-
ccarese, Kevin Bretonnel Cohen, Martin Krallinger,
Florian Leitner, Zhiyong Lu, Yifan Peng, Fabio Ri-
naldi, Manabu Torii, Alfonso Valencia, Karin Ver-
spoor, Thomas C. Wiegers, Cathy H. Wu, and
W. John Wilbur. 2013. BioC: A minimalist ap-
proach to interoperability for biomedical text pro-
cessing. submitted.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double layered learning for bio-
logical event extraction from text. In Proceedings
of the BioNLP Shared Task 2011 Workshop, pages
153?154. Association for Computational Linguis-
tics.
Wiktoria Golik, Robert Bossy, Zorana Ratkovic, and
Claire Ne?dellec. 2013. Improving Term Extraction
with Linguistic Analysis in the Biomedical Domain.
In Special Issue of the journal Research in Comput-
ing Science, Samos, Greece, March. 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of the 16th Nordic Conference
on Computational Linguistics (NODALIDA), pages
105?112.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction. In
Proceedings of the BioNLP 2009 Workshop Com-
panion Volume for Shared Task, pages 1?9, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
102
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of Genia Event
Task in BioNLP Shared Task 2011. In Proceedings
of BioNLP Shared Task 2011 Workshop.
Dan Klein and Christopher D Manning. 2002. Fast ex-
act inference with a factored model for natural lan-
guage parsing. Advances in neural information pro-
cessing systems, 15(2003):3?10.
Haibin Liu, Tom Christiansen, William Baumgartner,
and Karin Verspoor. 2012. BioLemmatizer: a
lemmatization tool for morphological processing of
biomedical text. Journal of Biomedical Semantics,
3(1):3.
David McClosky, Mihai Surdeanu, and Christopher D
Manning. 2011. Event Extraction as Dependency
Parsing for BioNLP 2011. In Proceedings of the
BioNLP Shared Task 2011 Workshop, pages 41?45.
Association for Computational Linguistics.
David McClosky. 2010. Any domain parsing: Auto-
matic domain adaptation for natural language pars-
ing. Ph.D. thesis, Brown University.
Liam R McGrath, Kelly Domico, Courtney D Cor-
ley, and Bobbie-Jo Webb-Robertson. 2011. Com-
plex biological event extraction from full text us-
ing signatures of linguistic and semantic features.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 130?137. Association for Compu-
tational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010. Evaluating Dependency Rep-
resentations for Event Extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 779?787,
Beijing, China, August.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34(1):35?80.
Nhung TH Nguyen and Yoshimasa Tsuruoka. 2011.
Extracting bacteria biotopes with semi-supervised
named entity recognition and coreference resolution.
In Proceedings of the BioNLP Shared Task 2011
Workshop, pages 94?101. Association for Compu-
tational Linguistics.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwende. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 155?
163, Portland, Oregon, USA, June.
Larry Smith, Thomas Rindflesch, and W. John Wilbur.
2004. MedPost: a part-of-speech tagger for bio
medical text. Bioinformatics, 20(14):2320?2321.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo,
Tomoko Ohta, Jin-Dong Kim, and Jun?ichi Tsujii.
2011. BioNLP Shared Task 2011: Supporting Re-
sources. In Proceedings of BioNLP Shared Task
2011 Workshop, pages 112?120, Portland, Oregon,
USA, June.
Pontus Stenetorp, Sampo Pyysalo, Tomoko Ohta,
Sophia Ananiadou, and Jun?ichi Tsujii. 2012.
Bridging the gap between scope-based and event-
based negation/speculation annotations: a bridge not
too far. In Proceedings of the Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 47?56. Association for Computa-
tional Linguistics.
Y Tateisi, T Ohta, and J Tsujii. 2004. Annotation of
predicate-argument structure on molecular biology
text. Proceedings of the Workshop on the 1st In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-04).
Sofie Van Landeghem, Kai Hakala, Samuel Ro?nnqvist,
Tapio Salakoski, Yves Van de Peer, and Filip Gin-
ter. 2012. Exploring biomolecular literature with
EVEX: connecting genes through events, homology,
and indirect associations. Advances in Bioinformat-
ics, 2012.
Sofie Van Landeghem, Jari Bjo?rne, Chih-Hsuan Wei,
Kai Hakala, Sampo Pyysalo, Sophia Ananiadou,
Hung-Yu Kao, Zhiyong Lu, Tapio Salakoski, Yves
Van de Peer, et al 2013. Large-scale event extrac-
tion from literature with multi-level gene normaliza-
tion. PloS one, 8(4):e55814.
Andreas Vlachos and Mark Craven. 2012. Biomedical
event extraction from abstracts and full papers using
search-based structured prediction. BMC Bioinfor-
matics, 13(Suppl 11):S5.
Andreas Vlachos. 2012. An investigation of imita-
tion learning algorithms for structured prediction. In
Workshop on Reinforcement Learning, page 143.
103
