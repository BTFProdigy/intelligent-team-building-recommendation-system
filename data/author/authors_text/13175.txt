Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 430?438, Prague, June 2007. c?2007 Association for Computational Linguistics
Smooth Bilingual N-gram Translation
Holger Schwenk Marta R. Costa-jussa` and Jose? A.R. Fonollosa
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
schwenk@lismi.fr
UPC - TALP
Barcelona 08034, Spain
{mruiz,adrian}@gps.tsc.upc.edu
Abstract
We address the problem of smoothing trans-
lation probabilities in a bilingual N-gram-
based statistical machine translation system.
It is proposed to project the bilingual tuples
onto a continuous space and to estimate the
translation probabilities in this representa-
tion. A neural network is used to perform the
projection and the probability estimation.
Smoothing probabilities is most important
for tasks with a limited amount of training
material. We consider here the BTEC task
of the 2006 IWSLT evaluation. Improve-
ments in all official automatic measures are
reported when translating from Italian to En-
glish. Using a continuous space model for
the translation model and the target language
model, an improvement of 1.5 BLEU on the
test data is observed.
1 Introduction
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Among all possible target language sen-
tences the one with the highest probability is chosen:
e? = arg max
e
Pr(e|f) = arg max
e
Pr(f |e) Pr(e)
where Pr(f |e) is the translation model and Pr(e)
is the target language model. This approach is
usually referred to as the noisy source-channel ap-
proach in statistical machine translation (Brown et
al., 1993).
During the last few years, the use of context
in SMT systems has provided great improvements
in translation. SMT has evolved from the origi-
nal word-based approach to phrase-based translation
systems (Och et al, 1999; Koehn et al, 2003). A
phrase is defined as a group of source words f? that
should be translated together into a group of target
words e?. The translation model in phrase-based sys-
tems includes the phrase translation probabilities in
both directions, i.e. P (e?|f?) and P (f? |e?).
The use of a maximum entropy approach simpli-
fies the introduction of several additional models ex-
plaining the translation process :
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models and
the ?i weights are typically optimized to maximize
a scoring function on a development set (Och and
Ney, 2002).
The phrase translation probabilities P (e?|f? ) and
P (f? |e?) are usually obtained using relative frequency
estimates. Statistical learning theory, however, tells
us that relative frequency estimates have several
drawbacks, in particular high variance and low bias.
Phrase tables may contain several millions of en-
tries, most of which appear only once or twice,
which means that we are confronted with a data
sparseness problem. Surprisingly, there seems to be
little work addressing the issue of smoothing of the
phrase table probabilities.
On the other hand, smoothing of relative fre-
quency estimates was extensively investigated in the
430
area of language modeling. A systematic compari-
son can be for instance found in (Chen and Good-
man, 1999). Language models and phrase tables
have in common that the probabilities of rare events
may be overestimated. However, in language mod-
eling probability mass must be redistributed in order
to account for the unseen n-grams. Generalization
to unseen events is less important in phrase-based
SMT systems since the system searches only for the
best segmentation and the best matching phrase pair
among the existing ones.
We are only aware of one work that performs a
systematic comparison of smoothing techniques in
phrase-based machine translation systems (Foster et
al., 2006). Two types of phrase-table smoothing
were compared: black-box and glass-box methods.
Black-methods do not look inside phrases but in-
stead treat them as atomic objects. By these means,
all the methods developed for language modeling
can be used. Glass-box methods decompose P (e?|f?)
into a set of lexical distributions P (e|f? ). For in-
stance, it was suggested to use IBM-1 probabili-
ties (Och et al, 2004), or other lexical translation
probabilities (Koehn et al, 2003; Zens and Ney,
2004). Some form of glass-box smoothing is now
used in all state-of-the-art statistical machine trans-
lation systems.
Another approach related to phrase table smooth-
ing is the so-called N-gram translation model
(Marin?o et al, 2006). In this model, bilingual tu-
ples are used instead of the phrase pairs and n-gram
probabilities are considered rather than relative fre-
quencies. Therefore, smoothing is obtained us-
ing the standard techniques developed for language
modeling. In addition, a context dependence of the
phrases is introduced. On the other hand, some
restrictions on the segmentation of the source sen-
tence must be used. N-gram-based translation mod-
els were extensively compared to phrase-based sys-
tems on several tasks and typically achieve compa-
rable performance.
In this paper we propose to investigate improved
smoothing techniques in the framework of the N-
gram translation model. Despite the undeniable suc-
cess of n-graam back-off models, these techniques
have several drawbacks from a theoretical point of
view: the words are represented in a discrete space,
the vocabulary. This prevents ?true interpolation? of
the probabilities of unseen n-grams since a change
in this word space can result in an arbitrary change
of the n-gram probability. An alternative approach
is based on a continuous representation of the words
(Bengio et al, 2003). The basic idea is to convert
the word indices to a continuous representation and
to use a probability estimator operating in this space.
Since the resulting distributions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. Prob-
ability estimation and interpolation in a continuous
space is mathematically well understood and numer-
ous powerful algorithms are available that can per-
form meaningful interpolations even when only a
limited amount of training material is available. This
approach was successfully applied to language mod-
eling in large vocabulary continuous speech recogni-
tion (Schwenk, 2007) and to language modeling in
phrase-based SMT systems (Schwenk et al, 2006).
In this paper, we investigate whether this ap-
proach is useful to smooth the probabilities involved
in the bilingual tuple translation model. Reliable es-
timation of unseen n-grams is very important in this
translation model. Most of the trigram tuples en-
countered in the development or test data were never
seen in the training data. N-gram hit rates are re-
ported in the results section of this paper. We report
experimental results for the BTEC corpus as used
in the 2006 evaluations of the international work-
shop on spoken language translation IWSLT (Paul,
2006). This task provides a very limited amount
of resources in comparison to other tasks like the
translation of journal texts (NIST evaluations) or of
parliament speeches (TC-STAR evaluations). There-
fore, new techniques must be deployed to take the
best advantage of the limited resources. Among the
language pairs tested in this years evaluation, Ital-
ian to English gave the best BLEU results in this
year evaluation. The better the translation quality is,
the more it is challenging to outperform it without
adding more data. We show that a new smoothing
technique for the translation model achieves a sig-
nificant improvement in the BLEU score for a state-
of-the-art statistical translation system.
This paper is organized as follows. In the next
section we first describe the baseline statistical ma-
chine translation systems. Section 3 presents the ar-
chitecture and training algorithms of the continuous
431
space translation model and section 4 summarizes
the experimental evaluation. The paper concludes
with a discussion of future research directions.
2 N-gram-based Translation Model
The N -gram-based translation model has been de-
rived from the finite-state perspective; more specif-
ically, from the work of Casacuberta (2001). How-
ever, different from it, where the translation model
is implemented by using a finite-state transducer,
the N -gram-based system implements a bilingual
N -gram model. It actually constitutes a language
model of bilingual units, referred to as tuples, which
approximates the joint probability between source
and target languages by using N -grams, such as de-
scribed by the following equation:
p(e, f) ?
K
?
k=1
p((e, f)k|(e, f)k?1, . . . , (e, f)k?4)
(2)
where e refers to target, f to source and (e, f)k to
the kth tuple of a given bilingual sentence pair.
Bilingual units (tuples) are extracted from any
word-to-word alignment according to the following
constraints:
? a monotonic segmentation of each bilingual
sentence pairs is produced,
? no word inside the tuple is aligned to words
outside the tuple, and
? no smaller tuples can be extracted without vio-
lating the previous constraints.
As a consequence of these constraints, only one
segmentation is possible for a given sentence pair.
Two important issues regarding this translation
model must be considered. First, it often occurs that
a large number of single-word translation probabil-
ities are left out of the model. This happens for all
words that are always embedded in tuples contain-
ing two or more words, then no translation probabil-
ity for an independent occurrence of these embed-
ded words will exist. To overcome this problem, the
tuple trigram model is enhanced by incorporating
1-gram translation probabilities for all the embed-
ded words detected during the tuple extraction step.
These 1-gram translation probabilities are computed
from the intersection of both the source-to-target and
the target-to-source alignments.
The second issue has to do with the fact that some
words linked to NULL end up producing tuples with
NULL source sides. Since no NULL is actually ex-
pected to occur in translation inputs, this type of tu-
ple is not allowed. Any target word that is linked to
NULL is attached either to the word that precedes
or the word that follows it. To determine this, an ap-
proach based on the IBM1 probabilities was used, as
described in (Marin?o et al, 2006).
2.1 Additional features
The following feature functions were used in the N-
gram-based translation system:
? A target language model. In the baseline sys-
tem, this feature consists of a 4-gram back-off
model of words, which is trained from the tar-
get side of the bilingual corpus.
? A source-to-target lexicon model and a
target-to-source lexicon model. These fea-
ture, which are based on the lexical parameters
of the IBM Model 1, provide a complementary
probability for each tuple in the translation ta-
ble.
? A word bonus function. This feature intro-
duces a bonus based on the number of target
words contained in the partial-translation hy-
pothesis. It is used to compensate for the sys-
tem?s preference for short output sentences.
All these models are combined in the de-
coder. Additionally, the decoder allows for a
non-monotonic search with the following distorsion
model.
? A word distance-based distorsion model.
P (tK1 ) = exp(?
K
?
k=1
dk)
where dk is the distance between the first word
of the kth tuple (unit), and the last word+1 of
the (k ? 1)th tuple.
432
Figure 1: Comparing regular and unfolded tuples.
Distance are measured in words referring to the units
source side.
To reduce the computational cost we place lim-
its on the search using two parameters: the distor-
tion limit (the maximum distance measured in words
that a tuple is allowed to be reordered, m) and the
reordering limit (the maximum number of reorder-
ing jumps in a sentence, j). Tuples need to be ex-
tracted by an unfolding technique (Marin?o et al,
2006). This means that the tuples are broken into
smaller tuples, and these are sequenced in the order
of the target words. In order not to lose the infor-
mation on the correct order, the decoder performs a
non-monotonic search. Figure 1 shows an example
of tuple unfolding compared to the monotonic ex-
traction. The unfolding technique produces a differ-
ent bilingual n-gram language model with reordered
source words.
In order to combine the models in the decoder
suitably, an optimization tool based on the Simplex
algorithm is used to compute log-linear weights for
each model.
3 Continuous Space N-gram Models
The architecture of the neural network n-gram
model is shown in Figure 2. A standard
fully-connected multi-layer perceptron is
used. The inputs to the neural network are
the indices of the n?1 previous units (words
or tuples) in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the poste-
rior probabilities of all units of the vocabulary:
projection
layer hidden
layer
output
layerinput
projections
shared
LM probabilities
for all words
probability estimation
Neural Network
discrete
representation:
indices in wordlist
continuous
representation:
P dimensional vectors
N
wj?1 P
H
N
P (wj =1|hj)
wj?n+1
wj?n+2
P (wj =i|hj)
P (wj =N|hj)
cl
oiM
Vdj
p1 =
pN =
pi =
Figure 2: Architecture of the continuous space LM.
hj denotes the context wj?n+1, . . . , wj?1. P is the
size of one projection and H ,N is the size of the
hidden and output layer respectively. When short-
lists are used the size of the output layer is much
smaller than the size of the vocabulary.
P (wj = i|hj) ?i ? [1,N ] (3)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith unit of
the vocabulary is coded by setting the ith element of
the vector to 1 and all the other elements to 0. The
ith line of the N ?P dimensional projection matrix
corresponds to the continuous representation of the
ith unit. Let us denote cl these projections, dj the
hidden layer activities, oi the outputs, pi their soft-
max normalization, and mjl, bj , vij and ki the hid-
den and output layer weights and the corresponding
biases. Using these notations, the neural network
performs the following operations:
dj = tanh
(
?
l
mjl cl + bj
)
(4)
oi =
?
j
vij dj + ki (5)
pi = eoi /
N
?
r=1
eor (6)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj).
433
Training is performed with the standard back-
propagation algorithm minimizing the following er-
ror function:
E =
N
?
i=1
ti log pi + ?
?
?
?
jl
m2jl +
?
ij
v2ij
?
? (7)
where ti denotes the desired output, i.e., the proba-
bility should be 1.0 for the next unit in the training
sentence and 0.0 for all the other ones. The first part
of this equation is the cross-entropy between the out-
put and the target probability distributions, and the
second part is a regularization term that aims to pre-
vent the neural network from over-fitting the train-
ing data (weight decay). The parameter ? has to be
determined experimentally. Training is done using
a re-sampling algorithm as described in (Schwenk,
2007).
It can be shown that the outputs of a neural net-
work trained in this manner converge to the posterior
probabilities. Therefore, the neural network directly
minimizes the perplexity on the training data. Note
also that the gradient is back-propagated through the
projection-layer, which means that the neural net-
work learns the projection of the units onto the con-
tinuous space that is best for the probability estima-
tion task.
In general, the complexity to calculate one prob-
ability with this basic version of the neural network
n-gram model is dominated by the dimension of the
output layer since the size of the vocabulary (10k
to 64k) is usually much larger than the dimension of
the hidden layer (200 to 500). Therefore, in previous
applications of the continuous space n-gram model,
the output was limited to the s most frequent units, s
ranging between 2k and 12k (Schwenk, 2007). This
is called a short-list.
Sents Words
Train (bitexts) 20k 155.4/166.3k
Dev 489 5.2k
Eval 500 6k
Table 1: Available data in the supplied resources of
the 2006 IWSLT evaluation.
4 Experimental Evaluation
In this work we report results on the Basic Travel-
ing Expression Corpus (BTEC) as used in the 2006
evaluations of the international workshop on spoken
language translation (IWSLT). This corpus consists
of typical sentences from phrase books for tourists in
several languages (Takezawa et al, 2002). We report
results on the supplied development corpus of 489
sentences and the official test set of the IWSLT?06
evaluation. The main measure is the BLEU score,
using seven reference translations. The scoring is
case insensitive and punctuations are ignored. De-
tails on the available data are summarized in Table 1.
We concentrated first on the translation from Ital-
ian to English. All participants in the IWSLT evalua-
tion achieved much better performances for this lan-
guage pair than for the other considered translation
directions. This makes it more difficult to achieve
additional improvements.
A non-monotonic search was performed follow-
ing a local reordering named in Section 2, setting
m = 5 and j = 3. Also we used histogram prun-
ing in the decoder, i.e. the maximum number of hy-
potheses in a stack is limited to 50.
4.1 Language-dependent preprocessing
Italian contracted prepositions have been separated
into preposition + article, such as ?alla???a la?,
?degli???di gli? or ?dallo???da lo?, among others.
4.2 Model training
The training and development data for the bilingual
back-off and neural network translation model were
created as follows. Given the alignment of the train-
ing parallel corpus, we perform a unique segmenta-
tion of each parallel sentence following the criterion
of unfolded segmentation seen in Section 2. This
segmentation is used in a sequence as training text
for building the language model. As an example,
given the alignment and the unfold extraction of Fig-
ure 1, we obtain the following training sentence:
<s> how long#cua?nto does#NULL last#dura
the#el flight#vuelo </s>
The reference bilingual trigram back-off transla-
tion model was trained on these bilingual tuples us-
434
ing the SRI LM toolkit (Stolcke, 2002). Different
smoothing techniques were tried, and best results
were obtained using Good-Turing discounting.
The neural network approach was trained on ex-
actly the same data. A context of two tuples was
used (trigram model). The training corpus contains
about 21,500 different bilingual tuples. We decided
to limit the output of the neural network to the 8k
most frequent tuples (short-list). This covers about
90% of the requested tuple n-grams in the training
data.
Similar to previous applications, the neural net-
work is not used alone but interpolation is performed
to combine several n-gram models. First of all, the
neural network and the reference back-off model are
interpolated together - this always improved perfor-
mance since both seem to be complementary. Sec-
ond, four neural networks with different sizes of the
continuous representation were trained and interpo-
lated together. This usually achieves better general-
ization behavior than training one larger neural net-
work. The interpolation coefficients were calculated
by optimizing perplexity on the development data,
using an EM procedure. The obtained values are
0.33 for the back-off translation model and about
0.16 for each neural network model respectively.
This interpolation is used in all our experiments. For
the sake of simplicity we will still call this the con-
tinuous space translation model.
Each network was trained independently using
early stopping on the development data. Conver-
gence was achieved after about 10 iterations through
the training data (less than 20 minutes of processing
on a standard Linux machine). The other parameters
are as follows:
? Context of two tuples (trigram)
? The dimension of the continuous representation
of the tuples were c =120,140,150 and 200,
? The dimension of the hidden layer was set to
P = 200,
? The initial learning rate was 0.005 with an ex-
ponential decay,
? The weight decay coefficient was set to ? =
0.00005.
N-gram models are usually evaluated using per-
plexity on some development data. In our case, i.e.
using bilingual tuples as basic units (?words?), it is
less obvious if perplexity is a useful measure. Nev-
ertheless, we provide these numbers for complete-
ness. The perplexity on the development data of the
trigram back-off translation model is 227.0. This
could be reduced to 170.4 using the neural network.
It is also very informative to analyze the n-gram
hit-rates of the back-off model on the development
data: 10% of the probability requests are actually a
true trigram, 40% a bigram and about 49% are fi-
nally estimated using unigram probabilities. This
means that only a limited amount of phrase con-
text is used in the standard N-gram-based translation
model. This makes this an ideal candidate to ap-
ply the continuous space model since probabilities
are interpolated for all possible contexts and never
backed-up to shorter contexts.
4.3 Results and analysis
The incorporation of the neural translation model
is done using n-best list. Each hypothesis is com-
posed of a sequence of bilingual tuples and the cor-
responding scores of all the feature functions. Fig-
ure 3 shows an example of such an n-best list. The
neural trigram translation model is used to replace
the scores of the trigram back-off translation model.
This is followed by a re-optimization of the coef-
ficients of all feature functions, i.e. maximization
of the BLEU score on the development data using
the numerical optimization tool CONDOR (Berghen
and Bersini, 2005). An alternative would be to add
a feature function and to combine both translation
models under the log-linear model framework, us-
ing maximum BLEU training.
Another open question is whether it might by
better to already use the continuous space transla-
tion model during decoding. The continuous space
model has a much higher complexity than a back-
off n-gram. However, this can be heavily optimized
when rescoring n-best lists, i.e. by grouping to-
gether all calls in the whole n-best list with the same
context, resulting in only one forward pass through
the neural network. This is more difficult to per-
form when the continuous space translation model
is used during decoding. Therefore, this was not in-
vestigated in this work.
435
spiacente#sorry tutto occupato#it ?s full
spiacente#i ?m sorry tutto occupato#it ?s full
spiacente#i ?m afraid tutto occupato#it ?s full
spiacente#sorry tutto#all occupato#busy
spiacente#sorry tutto#all occupato#taken
Figure 3: Example of sentences in the n-best list of
bilingual tuples. The special character ?#? is used to
separate the source and target sentence words. Sev-
eral words in one tuple a grouped together using ? .?
In all our experiments 1000-best lists were used.
In order to evaluate the quality of these n-best lists,
an oracle trigram back-off translation model was
build on the development data. Rescoring the n-
best lists with this translation model resulted in an
increase of the BLEU score of about 10 points (see
Table 2). While there is an decrease of about 6%
for the position dependent word error rate (mWER),
a smaller change in the position independent word
error rate was observed (mPER). This suggests that
most of the alternative translation hypothesis re-
sult in word reorderings and not in many alternative
word choices. This is one of the major drawbacks
of phrase- and N-gram-based translation systems:
only translations observed in the training data can
be used. There is no generalization to new phrase
pairs.
Back-off Oracle Neural
BLEU 42.34 52.45 43.87
mWER 41.6% 35.6% 40.3%
mPER 31.5% 28.2% 30.7%
Table 2: Comparison of different N-gram-
translation models on the development data.
When the 1000-best lists are rescored with the
neural network translation model the BLEU score
increases by 1.5 points (42.34 to 43.87). Similar im-
provements were observed in the word error rates
(see Table 2). For comparison, a 4-gram back-off
translation model was also built, but no change of
the BLEU score was observed. This suggests that
careful smoothing is more important than increasing
the context when estimating the translation probabil-
ities in an N-gram-based statistical machine transla-
tion system.
In previous work, we have investigated the use of
the neural network approach to modeling the target
language for the IWSLT task (Schwenk et al, 2006).
We also applied this technique to this improved N-
gram-based translation system. In our implemen-
tation, the neural network target 4-gram language
model gives an improvement of 1.3 points BLEU
on the development data (42.34 to 43.66), in com-
parison to 1.5 points for the neural translation model
(see Table 3).
Back-off neural neural neural
TM+LM TM LM TM+LM
BLEU 42.34 43.87 43.66 44.83
Table 3: Combination of a neural translation model
(TM) and a neural language model (LM). BLEU
scores on the development data.
The neural translation and target language model
were also applied to the test data, using of course the
same feature function coefficients as for the devel-
opment data. The results are given in Table 4 for all
the official measures of the IWSLT evaluation. The
new smoothing method of the translation probabili-
ties achieves improvement in all measures. It gives
also an additional gain (again in all measures) when
used together with a neural target language model.
Surprisingly, neural TM and neural LM improve-
ments almost add up: when both techniques are used
together, the BLEU scores increases by 1.5 points
(36.97 ? 38.50). Remember that the reference N-
gram-based translation system already uses a local
reordering approach.
Back-off neural neural neural
TM+LM TM LM TM+LM
BLEU 36.97 37.21 38.04 38.50
mWER 48.10 47.42 47.83 47.61
mPER 38.21 38.07 37.26 37.12
NIST 8.3 8.3 8.6 8.7
Meteor 63.16 63.40 64.70 65.20
Table 4: Test set scores for the combination of a
neural translation model (TM) and a neural language
model (LM).
436
5 Discussion
Phrase-based approaches are the de-facto standard
in statistical machine translation. The phrases are
extracted automatically from the word alignments
of parallel texts, and the different possible transla-
tions of a phrase are weighted using relative fre-
quency. This can be problematic when the data is
sparse. However, there seems to be little work on
possible improvements of the relative frequency es-
timates by some smoothing techniques. It is today
common practice to use additional feature functions
like IBM-1 scores to obtain some kind of smoothing
(Och et al, 2004; Koehn et al, 2003; Zens and Ney,
2004), but better estimation of the phrase probabili-
ties is usually not addressed.
An alternative way to represent phrases is to de-
fine bilingual tuples. Smoothing, and context de-
pendency, is obtained by using an n-gram model on
these tuples. In this work, we have extended this
approach by using a new smoothing technique that
operates on a continuous representation of the tu-
ples. Our method is distinguished by two charac-
teristics: better estimation of the numerous unseen
n-grams, and a discriminative estimation of the tu-
ple probabilities. Results are provided on the BTEC
task of the 2006 IWSLT evaluation for the translation
direction Italian to English. This task provides very
limited amount of resources in comparison to other
tasks. Therefore, new techniques must be deployed
to take the best advantage of the limited resources.
We have chosen the Italian to English task because it
is challenging to enhance a good quality translation
task (over 40 BLEU percentage). Using the continu-
ous space model for the translation and target lan-
guage model, an improvement of 2.5 BLEU on the
development data and 1.5 BLEU on the test data was
observed.
Despite these encouraging results, we believe that
additional research on improved estimation of prob-
abilities in N-gram- or phrase-based statistical ma-
chine translation systems is needed. In particu-
lar, the problem of generalization to new trans-
lations seems to be promising to us. This could
be addressed by the so-called factored phrase-based
model as implemented in the Moses decoder (Koehn
et al, 2007). In this approach words are decom-
posed into several factors. These factors are trans-
lated and a target phrase is generated. This model
could be complemented by a factored continuous
tuple N-gram. Factored word language models
were already successfully used in speech recogni-
tion (Bilmes and Kirchhoff, 2003; Alexandrescu and
Kirchhoff, 2006) and an extension to machine trans-
lation seems to be promising.
The described smoothing method was explicitly
developed to tackle the data sparseness problem in
tasks like the BTEC corpus. It is well known from
language modeling that careful smoothing is less im-
portant when large amounts of data are available.
We plan to investigate whether this also holds for
smoothing of the probabilities in phrase- or tuple-
based statistical machine translation systems.
6 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR (IST-
2002-FP6-506738), by the French Government un-
der the project INSTAR (ANR JCJC06 143038) and
the the Spanish government under a FPU grant and
the project AVIVAVOZ (TEC2006-13964-C03).
References
A. Alexandrescu and K. Kirchhoff. 2006. Factored neu-
ral language models. In HLT-NAACL.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3(2):1137?1155.
F. Vanden Berghen and H. Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175.
J. A. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized backoff. In HLT-NAACL.
P. Brown, S. Della Pietra, V. J. Della Pietra, and R: Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
311.
F. Casacuberta, D. Llorens, C. Mart??nez, S. Molau,
F. Nevado, H. Ney, M. Pastor, D. Pico?, A. Sanchis,
E. Vidal, and J.M. Vilar. 2001. Speech-to-speech
translation based on finite-state transducers. Interna-
tional Conference on Acoustic, Speech and Signal Pro-
cessing, 1.
437
S. F. Chen and J. T. Goodman. 1999. An empirical study
of smoothing techniques for language modeling. CSL,
13(4):359?394.
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
smoothing for statistical machine translation. In
EMNLP06, pages 53?61.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrased-based machine translation. In Human Lan-
guage Technology Conference (HLT-NAACL), pages
127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, demonstration session.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M. R. Costa-jussa`.
2006. Bilingual n-gram statistical machine transla-
tion. Computational Linguistics, 32(4):527?549, De-
cember.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL, pages 295?302.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Copora,
pages 20?28.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL, pages 161?168.
M. Paul. 2006. Overview of the IWSLT 2006 campaign.
In IWSLT, pages 1?15.
H. Schwenk, M. R. Costa-jussa`, and J. A. R. Fonollosa.
2006. Continuous space language models for the iwslt
2006 task. IWSLT, pages 166?173.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In ICSLP, pages II: 901?904.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a borad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In LREC, pages 147?152.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In HLT/NACL,
pages 257?264.
438
N-gram-based Machine Translation
Jose? B. Marin?o?
Rafael E. Banchs?
Josep M. Crego?
Adria` de Gispert?
Patrik Lambert?
Jose? A. R. Fonollosa?
Marta R. Costa-jussa`?
Universitat Polite`cnica de Catalunya
This article describes in detail an n-gram approach to statistical machine translation. This ap-
proach consists of a log-linear combination of a translation model based on n-grams of bilingual
units, which are referred to as tuples, along with four specific feature functions. Translation
performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English
and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS).
1. Introduction
The beginnings of statistical machine translation (SMT) can be traced back to the early
fifties, closely related to the ideas from which information theory arose (Shannon and
Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during
World War II. According to this view, machine translation was conceived as the problem
of finding a sentence by decoding a given ?encrypted? version of it (Weaver 1955).
Although the idea seemed very feasible, enthusiasm faded shortly afterward because of
the computational limitations of the time (Hutchins 1986). Finally, during the nineties,
two factors made it possible for SMT to become an actual and practical technology:
first, significant increment in both the computational power and storage capacity of
computers, and second, the availability of large volumes of bilingual data.
The first SMT systems were developed in the early nineties (Brown et al 1990, 1993).
These systems were based on the so-called noisy channel approach, which models the
probability of a target language sentence T given a source language sentence S as the
product of a translation-model probability p(S|T), which accounts for adequacy of trans-
lation contents, times a target language probability p(T), which accounts for fluency
of target constructions. For these first SMT systems, translation-model probabilities at
the sentence level were approximated from word-based translation models that were
trained by using bilingual corpora (Brown et al 1993). In the case of target language
probabilities, these were generally trained from monolingual data by using n-grams.
Present SMT systems have evolved from the original ones in such a way that
mainly differ from them in two respects: first, word-based translation models have been
? Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain.
Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for
publication: 5 July 2006
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 4
replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och,
and Marcu 2003) which are directly estimated from aligned bilingual corpora by consid-
ering relative frequencies, and second, the noisy channel approach has been expanded
to a more general maximum entropy approach in which a log-linear combination of
multiple feature functions is implemented (Och and Ney 2002).
As an extension of the machine translation problem, technological advances in the
fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it
possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron,
and Norvig 1992). According to this, SMT has also been approached from a finite-state
point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini,
and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi
2000). In this SMT approach, translation models are implemented by means of finite-
state transducers for which transition probabilities are learned from bilingual data.
As opposed to phrase-based translation models, which consider probabilities between
target and source units referred to as phrases, finite-state translation models rely on
probabilities among sequences of bilingual units, which are defined by the transitions
of the transducer.
The translation system described in this article implements a translation model that
has been derived from the finite-state perspective?more specifically, from the work of
Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier
work the translation model is implemented by using a finite-state transducer, in the sys-
tem presented here the translation model is implemented by using n-grams. In this way,
the proposed translation system can take full advantage of the smoothing and consist-
ency provided by standard back-off n-gram models. The translation model presented
here actually constitutes a language model of a sort of ?bilanguage? composed of bilin-
gual units, which will be referred to as tuples (de Gispert and Marin?o 2002). An alterna-
tive approach, which relies on bilingual-unit unigram probabilities, was developed by
Tillmann and Xia (2003); in contrast, the approach presented here considers bilingual-
unit n-gram probabilities. In addition to the tuple n-gram translation model, the
translation system presented here implements four specific feature functions that are
log-linearly combined along with the translation model for performing the decoding
(Marin?o et al 2005).
This article is intended to provide a detailed description of the n-gram-based
translation system, as well as to demonstrate the system performance in a wide-
domain, large-vocabulary translation task. The article is structured as follows. First,
Section 2 presents a complete description of the n-gram-based translation model. Then,
Section 3 describes in detail the additional feature functions that, along with the trans-
lation model, compose the n-gram-based SMT system implemented. Section 4 describes
the European Parliament Plenary Session (EPPS) data, as well as the most relevant
details about the translation tasks considered. Section 5 presents and discusses the
translation experiments and their results. Finally, Section 6 presents some conclusions
and intended further work.
2. The Tuple N-gram Model
This section describes in detail the tuple n-gram translation model, which constitutes
the core model implemented by the n-gram-based SMT system. First, the bilingual unit
definition and model computation are presented in Section 2.1. Then, some important
refinements to the basic translation model are provided and discussed in Section 2.2.
Finally, Section 2.3 discusses issues related to n-gram-based decoding.
528
Marin?o et al N-gram-based Machine Translation
2.1 Tuple Extraction and Model Computation
As already mentioned, the translation model implemented by the described SMT sys-
tem is based on bilingual n-grams. This model actually constitutes a language model of
a particular bilanguage composed of bilingual units that are referred to as tuples. In this
way, the translation model probabilities at the sentence level are approximated by using
n-grams of tuples, such as described by the following equation:
p(T, S) ?
K
?
k=1
p((t, s)k|(t, s)k?1, (t, s)k?2, . . . , (t, s)k?n+1) (1)
where t refers to target, s to source, and (t, s)k to the kth tuple of a given bilingual
sentence pair. It is important to note that since both languages are linked up in tuples,
the context information provided by this translation model is bilingual.
Tuples are extracted from a word-to-word aligned corpus in such a way that a
unique segmentation of the bilingual corpus is achieved. Although in principle any
Viterbi alignment should allow for tuple extraction, the resulting tuple vocabulary
depends highly on the particular alignment set considered, and this impacts the trans-
lation results. According to our experience, the best performance is achieved when
the union of the source-to-target and target-to-source alignment sets (IBM models;
Brown et al [1993]) is used for tuple extraction (some experimental results regarding
this issue are presented in Section 4.2.2). Additionally, the use of the union can also
be justified from a theoretical point of view by considering that the union set typically
exhibits higher recall values than do other alignment sets such as the intersection and
source-to-target.
In this way, as opposed to other implementations, where one-to-one (Bangalore
and Riccardi 2000) or one-to-many (Casacuberta and Vidal 2004) alignments are used,
tuples are extracted from many-to-many alignments. This implementation produces
a monotonic segmentation of bilingual sentence pairs, which allows for simulta-
neously capturing contextual and reordering information into the bilingual translation
unit structures. This segmentation also allows for estimating the n-gram probabil-
ities appearing in (1). In order to guarantee a unique segmentation of the corpus,
tuple extraction is performed according to the following constraints (Crego, Marin?o,
and de Gispert 2004):
 a monotonic segmentation of each bilingual sentence pair is produced,
 no word inside the tuple is aligned to words outside the tuple, and
 no smaller tuples can be extracted without violating the previous
constraints.
Notice that, according to this, tuples can be formally defined as the set of shortest
phrases that provides a monotonic segmentation of the bilingual corpus. Figure 1
presents a simple example illustrating the unique tuple segmentation for a given pair of
sentences, as well as the complete phrase set.
The first important observation from Figure 1 is related to the possible occurrence
of tuples containing unaligned elements on the target side. This is the case for tuple 1.
Tuples of this kind should be handled in an alternative way for the system to be able
to provide appropriate translations for such unaligned elements. The problem of how
529
Computational Linguistics Volume 32, Number 4
Figure 1
Example of tuple extraction. Tuples are extracted from Viterbi alignments in such a way that the
set of shortest bilingual units that provide a monotonous segmentation of the bilingual sentence
pair is achieved.
to handle this kind of situation, which we refer to as involving source-nulled tuples, is
discussed in detail in Section 2.2.2.
Also, as observed from Figure 1, the total number of tuples is significantly lower
than the total number of phrases, and, in most of the cases, longer phrases can be
constructed by considering tuple n-grams, which is the case for phrases 2, 6, 7, 9, 10,
and 11. However, phrases 4 and 5 cannot be generated from tuples. In general, the tuple
representation is not able to provide translations for individual words that appear tied
to other words unless they occur alone in some other tuple. This problem, which we
refer to as embedded words, is discussed in detail in Section 2.2.1.
Another important observation from Figure 1 is that each tuple length is implicitly
defined by the word links in the alignment. As opposed to phrase-extraction proce-
dures, for which a maximum phrase length should be defined to avoid a vocabulary
explosion, tuple extraction procedures do not have any control over tuple lengths.
According to this, the tuple approach will strongly benefit from the structural similarity
between the languages under consideration. Then, for close language pairs, tuples are
expected to successfully handle those short reordering patterns that are included in
the tuple structure, as in the case of ?traducciones perfectas : perfect translations?
presented in Figure 1. On the other hand, in the case of distant pairs of languages, for
which a large number of long tuples are expected to occur, the approach will more easily
fail to provide a good translation model due to tuple sparseness.
2.2 Translation Model Refinements
The basic n-gram translation model, as defined in the previous section, exhibits some
important limitations that can be easily overcome by incorporating specific changes in
530
Marin?o et al N-gram-based Machine Translation
either the tuple vocabulary or the n-gram model. This section describes such limitations
and provides a detailed description of the implemented refinements.
2.2.1 Embedded Words. The first issue regarding the n-gram translation model is related
to the already mentioned problem of embedded words, which refers to the fact that
the tuple representation is not able to provide translations for individual words all the
time. Embedded words can become a serious drawback when they occur in relatively
significant numbers in the tuple vocabulary.
Consider for example the word translations in Figure 1. As seen from the figure, this
word appears embedded into tuple ?traducciones perfectas : perfect translations.? If a
similar situation is encountered for all other occurrences of that word in the training
corpus, then no translation probability for an independent occurrence of that word
will exist. A more relevant example would be the case of the embedded word perfect
since this adjective always moves relative to the noun it is modifying. In this case,
providing the translation system with a word-to-word translation probability for ?per-
fectas : perfect? only guarantees that the decoder will have a translation option for an
isolated occurrence of such words but does not guarantee anything about word order.
So, certainly, any adjective?noun combination including the word perfect, which has not
been seen during the training stage, will be translated in the wrong order. Accordingly,
the problem resulting from embedded words can be partially solved by incorporating a
bilingual dictionary able to provide word-to-word translation when required by the
translation system. A more complete treatment for this problem must consider the
implementation of a word-reordering strategy for the proposed SMT approach (as will
be discussed in Section 6, this constitutes one of the main concerns for our further
research).
In our n-gram-based SMT implementation, the following strategy for handling em-
bedded words is considered. First, one-word tuples for each detected embedded word
are extracted from the training data and their corresponding word-to-word translation
probabilities are computed by using relative frequencies. Then, the tuple n-gram model
is enhanced by including all embedded-word tuples as unigrams into the model. Since
a high-precision alignment set is desirable for extracting such one-word tuples and
estimating their probabilities, the intersection of both alignments, source to target and
target-to-source, is used instead of the union.
In the particular case of the EPPS tasks considered in this work, embedded words
do not constitute a real problem because of the great amount of training material and
the reduced size of the test data set (see Section 4.1 for a detailed description of the
EPPS data set). On the contrary, in other translation tasks with less available training
material, the embedded-word handling strategy described above has been very useful
(de Gispert, Marin?o, and Crego 2004).
2.2.2 Tuples with Empty Source Sides. The second important issue regarding the
n-gram translation model is related to tuples with empty source sides, hereinafter
referred to as source-nulled tuples. In the tuple n-gram model implementation, it fre-
quently happens that some target words linked to NULL end up producing tuples with
NULL source sides. Consider, for example, the first tuple of the example presented in
Figure 1. In this example, ?NULL : we? is a source-nulled tuple if Spanish is considered
to be the source language. Notice that tuples of this kind cannot be allowed since no
NULL is expected to occur in a translation input.
The classical solution to this problem in the finite-state transducer framework is
the inclusion of epsilon arcs (Knight and Al-Onaizan 1998; Bangalore and Riccardi
531
Computational Linguistics Volume 32, Number 4
2000). However, epsilon arcs significantly increase decoding complexity. In our n-gram
system implementation, this problem is easily solved by preprocessing the union set of
alignments before extracting tuples, in such a way that any target word that is linked
to NULL is attached to either its preceding word or its following word. In this way, no
target word remains linked to NULL, and source-nulled tuples will not occur during
tuple extraction.
Some different strategies for handling target words aligned to NULL have been
considered. In the simplest strategy, which will be referred to as the attach-to-right strat-
egy, target words aligned to NULL are always attached to their following word. This
simple strategy happens to provide better results, for English-to-Spanish and Spanish-
to-English translations, than the opposite one (attachment to the previous word), and
also better than a more sophisticated strategy that considers bigram probabilities for
deciding whether a given word should be attached to the following or to the pre-
vious one.
Notice that in the particular cases of Spanish and English, the attach-to-right strat-
egy can be justified heuristically. Indeed, when translating from Spanish to English,
most of the source-nulled tuples result from omitted verbal subjects, which is a very
common situation in Spanish. This is the case for the first tuple in Figure 1. Suppose,
for instance, that the attach-to-right strategy is used in Figure 1; in such a case, the
tuple ?quisie?ramos : would like? will be replaced by the new tuple ?quisie?ramos : we
would like,? which actually makes a better translation unit, at least from a grammatical
point of view. Similarly, some common situations can be identified for translations in
the English-to-Spanish direction, such as omitted determiners (e.g., ?I want information
about European countries : quiero informacio?n sobre los pa??ses Europeos?). Again,
the attach-to-right strategy for the unaligned Spanish determiner los seems to be the
best one.
Experimental results comparing the attach-to-right strategy to an additional strat-
egy based on a statistical translation lexicon are provided in Section 5.1.3.
2.2.3 Tuple Vocabulary Pruning. The third and last issue regarding the n-gram transla-
tion model is related to the computational costs resulting from the tuple vocabulary size
during decoding. The idea behind this refinement is to reduce both computation time
and storage requirements without degrading translation performance. In our n-gram-
based SMT system implementation, the tuple vocabulary is pruned by using histogram
counts. This pruning is performed by keeping the N most frequent tuples with common
source sides.
Notice that such a pruning, because it is performed before computing tuple n-gram
probabilities, has a direct impact on the translation model probabilities and then on
the overall system performance. For this reason, the pruning parameter N is critical
for efficient usage of the translation system. While a low value of N will significantly
decrease translation quality, on the other hand, a large value of N will provide the
same translation quality than a more adequate N, but with a significant increment in
computational costs. The optimal value for this parameter depends on data and should
be adjusted empirically for each considered translation task.
2.3 N-gram-based Decoding
Decoding for the n-gram-based translation model is slightly different from phrase-
based decoding. For this reason, a specific decoding tool had to be implemented. This
532
Marin?o et al N-gram-based Machine Translation
section briefly describes MARIE, the n-gram based search engine developed for our
SMT system (Crego, Marin?o, and de Gispert 2005a).
MARIE implements a beam-search strategy based on dynamic programming. The
decoding is performed monotonically and is guided by the source. During decoding,
partial-translation hypotheses are arranged into different stacks according to the total
number of source words they cover. In this way, a given hypothesis only competes with
those hypotheses that provide the same source-word coverage. At every translation
step, stacks are pruned to keep decoding tractable. MARIE allows for two different
pruning methods:
 Threshold pruning: for which all partial-translation hypotheses scoring
below a predetermined threshold value are eliminated.
 Histogram pruning: for which the maximum number of partial-translation
hypotheses to be considered is limited to the K-best ranked ones.
Additionally, MARIE allows for hypothesis recombination, which provides a more
efficient search. In the implemented algorithm, partial-translation hypotheses are re-
combined if they coincide exactly in both the present tuple and the tuple trigram history.
MARIE also allows for considering additional feature functions during decoding.
All these models are taken into account simultaneously, along with the n-gram trans-
lation model. In our SMT system implementation, four additional feature functions are
considered. These functions are described in detail in Section 3.2.
3. Feature Functions for the N-gram-based SMT System
This section describes in detail some feature functions that are implemented along with
the n-gram translation model for the complete translation system. First, in subsection
3.1, the log-linear combination framework and the implemented optimization proce-
dure are discussed. Then, four specific feature functions that constitute our SMT system
are detailed in Section 3.2.
3.1 Log-linear Combination Framework
As mentioned in the Introduction, in recent translation systems the noisy channel ap-
proach has been replaced by a more general approach, which is founded on the princi-
ples of maximum entropy (Berger, Della Pietra, and Della Pietra 1996). In this approach,
the corresponding translation for a given source language sentence S is defined by the
target language sentence that maximizes a log-linear combination of multiple feature
functions hi(S, T) (Och and Ney 2002), such as described by the following equation:
argmax
T
?
m
?mhm(S, T) (2)
where ?m represents the coefficient of the mth feature function hm(S, T), which ac-
tually corresponds to a log-scaled version of the mth-model probabilities. Optimal
values for the ?m coefficients are estimated via an optimization procedure by using a
development data set.
533
Computational Linguistics Volume 32, Number 4
3.2 Translation System Features
In addition to the tuple n-gram translation model, our n-gram-based SMT system
implements four feature functions: a target-language model, a word-bonus model, and
two lexicon models. These system features are described next.
3.2.1 Target-language Model. This feature provides information about the target lan-
guage structure and fluency. It favors those partial-translation hypotheses that are more
likely to constitute correctly structured target sentences over those that are not. The
model is implemented by using a word n-gram model of the target language, which is
computed according to the following expression:
hTL(T, S) = hTL(T) = log
K
?
k=1
p(wk|wk?1, wk?2, . . . , wk?n+1) (3)
where wk refers to the kth word in the considered partial-translation hypothesis. Notice
that this model only depends on the target side of the data, and can in fact be trained by
including additional information from other available monolingual corpora.
3.2.2 Word-bonus Model. This feature introduces a bonus that depends on the partial-
translation hypothesis length. This is done to compensate for the system preference for
short translations over large ones. The model is implemented through a bonus factor
that directly depends on the total number of words contained in the partial-translation
hypothesis, and it is computed as follows:
hWP(T, S) = hWP(T) = M (4)
where M is the number of words contained in the partial-translation hypothesis.
3.2.3 Source-to-Target Lexicon Model. This feature actually constitutes a complemen-
tary translation model. This model provides, for a given tuple, a translation probability
estimate between its source and target sides. This feature is implemented by using the
IBM-1 lexical parameters (Brown et al 1993; Och et al 2004). Accordingly, the source-
to-target lexicon probability is computed for each tuple according to the following
equation:
hLF(T, S) = log 1(I + 1)J
J
?
j=1
I
?
i=0
q(tnj |sni ) (5)
where sni and t
n
j are the ith and jth words in the source and target sides of tuple (t, s)n,
with I and J the corresponding total number of words in each side. In the equation,
q(.) refers to IBM-1 lexical parameters, which are estimated from alignments computed
in the source-to-target direction.
3.2.4 Target-to-Source Lexicon Model. Similar to the previous feature, this feature
function constitutes a complementary translation model too. It is computed in ex-
534
Marin?o et al N-gram-based Machine Translation
actly the same way the previous model is, with the only difference that IBM-1 lexical
parameters are estimated from alignments computed in the target-to-source direction
instead.
4. EPPS Translation Task
This section describes in detail the most relevant issues about the translation tasks con-
sidered. Section 4.1 describes the EPPS data set that is used, and Section 4.2 presents the
overall implementation details in regard to preprocessing, training, and optimization.
4.1 Corpus Description
The EPPS data set is composed of the official plenary session transcriptions of the Eu-
ropean Parliament, which are currently available in eleven different languages (Koehn
2002). However, in the case of the results presented here, we have used the Spanish and
English versions of the EPPS data that have been prepared by RWTH Aachen University
in the context of the European Project TC-STAR. The training, development, and test
data used include session transcriptions from April 1996 until September 2004, from
October 21 until October 28, 2004, and from November 15 until November 18, 2004,
respectively.
Table 1 presents the basic statistics for the training, development, and test data sets
for each considered language. More specifically, the statistics shown in Table 1 are the
number of sentences, the number of words, the vocabulary size (or number of distinct
words), the average sentence length in number of words, and the number of available
translation references.
As seen from Table 1, although the total number of words in the training set is
very similar for both languages, vocabulary sizes are substantially different. Indeed,
the Spanish vocabulary is approximately 60% larger than the English vocabulary. This
can be explained by the more inflected nature of Spanish, which is particularly evident
in the case of nouns, adjectives, and verbs, which may have many different forms de-
pending on gender, number, tense, and mode. As will be seen from results presented in
Section 5, this difference in vocabulary size has important consequences in translation
quality for the English-to-Spanish direction.
Regarding the development data set, only 1, 008 sentences were considered. Notice
from Table 1 that in this case, the Spanish vocabulary is 20% larger than the English
Table 1
Basic statistics for the training, development, and test data sets (M and k stand for millions and
thousands, respectively; Lmean refers to the average sentence length in number of words, and
Ref. to the number of available translation references).
Set Language Sentences Words Vocabulary Lmean Ref.
Train English 1.22 M 33.4 M 105 k 23.7 1
Spanish 1.22 M 34.8 M 169 k 28.4 1
Dev. English 1008 26.0 k 3.2 k 25.8 3
Spanish 1008 25.7 k 3.9 k 25.5 3
Test English 1094 26.8 k 3.9 k 24.5 2
Spanish 840 22.7 k 4.0 k 27.0 2
535
Computational Linguistics Volume 32, Number 4
vocabulary. Another important issue regarding the development data set is the number
of unseen words, that is, those words present in the development data that are not
present in the training data. In this case, 35 words (0.13%) out of the total number of
words in the English development set did not occur in the training data. From these 35
words, only 30 corresponded to different words. Similarly, 61 words (0.24%) out of the
total number of words in the Spanish development set were not in the training data. In
this case, 57 different words occurred.
Notice also in Table 1 that a different test set was used for each translation direction,
and although a different number of sentences is considered in each case, vocabulary
sizes are almost equivalent. Regarding unseen words, in this case, 112 words (0.42%) out
of the total number of words in the English test set did not occur in the training data.
From these 112 words, only 81 corresponded to different words. Similarly, 46 words
(0.20%) out of the total number of words in the Spanish test were not in the training
data. In this case, 40 different words occurred.
4.2 Preprocessing, Training, and System Optimization
This section presents the overall implementation details in regard to preprocessing,
training, and optimization of the translation system. Two languages, English and Span-
ish, and both translation directions between them are considered for several different
system configurations.
4.2.1 Preprocessing and Alignment. The training data are preprocessed by using stan-
dard tools for tokenizing and filtering. In the filtering stage, some sentence pairs are
removed from the training data to allow for a better performance of the alignment tool.
Sentence pairs are removed according to the following two criteria:
 Fertility filtering: removes sentence pairs with a word ratio larger than a
predefined threshold value.
 Length filtering: removes sentence pairs with at least one sentence of more
than 100 words in length. This helps to maintain bounded alignment
computational times.
After preprocessing, word-to-word alignments are performed in both directions,
source-to-target and target-to-source. In our system implementation, GIZA++ (Och and
Ney 2000) is used for computing the alignments. A total of five iterations for models
IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed.
Then, the obtained alignment sets are used for computing the intersection and the
union of alignments from which tuples and embedded-word tuples are extracted,
respectively.
4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is ex-
tracted from the union set of alignments while avoiding source-nulled tuples by using
the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are
pruned according to the procedure described in Section 2.2.3. In the case of the EPPS
data under consideration, pruning parameter values of N = 20 and N = 30 are used for
Spanish-to-English and English-to-Spanish, respectively.
In order to better justify such alignment set and pruning parameter selections,
Tables 2 and 3 present model sizes and translation accuracies for the tuple n-gram model
536
Marin?o et al N-gram-based Machine Translation
Table 2
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy when tuples are extracted from different alignment sets. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model alone.
Direction Alignment set Tuple voc. Bigrams Trigrams BLEU
ES ? EN Source-to-target 1.920 6.426 2.353 0.4424
union 2.040 6.009 1.798 0.4745
refined 2.111 6.851 2.398 0.4594
EN ? ES Source-to-target 1.813 6.263 2.268 0.4152
union 2.023 6.092 1.747 0.4276
refined 2.081 6.920 2.323 0.4193
when tuples are extracted from different alignment sets and when different pruning
parameters are used, respectively. Translation accuracy is measured in terms of the
BLEU score (Papineni et al 2002), which is computed here for translations generated
by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2,
in the case of Table 3. Both translation directions, Spanish to English (ES ? EN) and
English to Spanish (EN ? ES), are considered in each table.
In the case of Table 2, model size and translation accuracy are evaluated against
the type of alignment set used for extracting tuples. Three different alignment sets are
considered: source-to-target, the union of source-to-target and target-to-source, and the
?refined? alignment method described by Och and Ney (2003). For the results presented
in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English
direction, while a value of N = 30 was used for the English-to-Spanish direction.
As can be clearly seen in Table 2, the union alignment set happens to be the most
favorable one for extracting tuples in both translation directions since it provides a
significantly better translation accuracy, in terms of BLEU score, than the other two
alignment sets considered. Notice also in Table 2 that the union set is the one providing
the smallest model sizes according to the number of bigrams and trigrams. This might
explain the improvement observed in translation accuracy, with respect to the other two
cases, in terms of model sparseness.
Table 3
Tuple vocabulary sizes and their corresponding number of n-grams (in millions), and
translation accuracy for different pruning values and both translation directions. Notice that
BLEU measurements in this table correspond to translations computed by using the tuple
n-gram model along with the additional four feature functions described in Section 3.2.
Direction Pruning Tuple voc. Bigrams Trigrams BLEU
ES ? EN N = 30 2.109 6.233 1.805 0.5440
N = 20 2.040 6.009 1.798 0.5434
N = 10 1.921 5.567 1.759 0.5399
EN ? ES N = 30 2.023 6.092 1.747 0.4688
N = 20 1.956 5.840 1.733 0.4671
N = 10 1.843 5.342 1.677 0.4595
537
Computational Linguistics Volume 32, Number 4
In the case of Table 3, model size and translation accuracy are compared for three
different pruning conditions: N = 30, N = 20, and N = 10. For all the cases presented in
the table, tuples were extracted from the union set of alignments.
Notice in Table 3 how translation accuracy is clearly affected by pruning. In the
case of Spanish to English, values of N = 20 and N = 10, while providing tuple vo-
cabulary reductions of 3.27% and 8.91% with respect to N = 30, respectively, produce
a translation BLEU score reductions of 0.11% and 0.75%. On the other hand, in the
case of English to Spanish, values of N = 20 and N = 10 provide tuple vocabulary
reductions of 3.31% and 8.89% and a translation BLEU score reductions of 0.36% and
1.98% with respect to N = 30, respectively. According to these results, a similar tuple
vocabulary reduction seems to affect English-to-Spanish translations more than it af-
fects Spanish-to-English translations. For this reason, we finally adopted N = 20 and
N = 30 as the pruning parameter values for Spanish to English and English to Spanish,
respectively.
Another important observation derived from Table 3 is the higher BLEU score
values with respect to the ones presented in Table 2. This is because, as mentioned
above, the results presented in Table 3 were obtained by considering a full translation
system that implements the tuple n-gram model along with the additional four feature
functions described in Section 3.2. The relative impact of the described feature functions
on translation accuracy is studied in detail in Section 5.1.1.
4.2.3 Translation Model and Feature Function Training. After pruning, a tuple n-gram
model is trained for each translation direction by using the SRI Language Modeling
toolkit (Stolcke 2002). The options for Kneser?Ney smoothing (Kneser and Ney 1995)
and interpolation of higher and lower n-grams are used in these trainings. Then, each
tuple n-gram translation model is finally enhanced by including the unigram probabil-
ities for the embedded-word tuples such as described in Section 2.2.2.
Similarly, a word n-gram target language model is trained for each translation
direction by using the SRI Language Modeling toolkit. Again, as in the case of the
tuple n-gram model, Kneser?Ney smoothing and interpolation of higher and lower
n-grams are used. Extended target language models might also be obtained by adding
additional information from other available monolingual corpora. However, in the
translation tasks described here, target language models are estimated by using only
the information contained in the target side of the training data set.
In our SMT system implementation, trigram models are considered for both the
tuple translation model and the target language model. This selection is based on
perplexity measurements (over the development data set) obtained for n-gram models
computed from the EPPS training data by using different n-gram sizes. Table 4 presents
Table 4
Perplexity measurements for translation and target language models of different n-gram sizes.
Type of model Language Bigram Trigram 4-gram 5-gram
Translation ES ? EN 201.75 161.26 156.88 157.24
Translation EN ? ES 223.94 179.12 174.10 174.49
Language Spanish 81.98 52.49 48.03 47.54
Language English 78.91 50.59 46.22 45.59
538
Marin?o et al N-gram-based Machine Translation
perplexity values obtained for translation and target language models with different
n-gram sizes.
Although our system implements trigram models, the performance of translation
systems using different n-gram sized models is also evaluated. These results are pre-
sented and discussed in Section 5.1.2.
Finally, the source-to-target and target-to-source lexicon models are computed for
each translation direction according to the procedure described in Section 3.2.3. For each
considered lexicon model, either the alignment set in the source-to-target direction or
the alignment set in the target-to-source direction is used, accordingly.
4.2.4 System Optimization. Once the models are computed, a set of optimal log-linear
coefficients is estimated for each translation direction and system configuration via
an optimization procedure, which is described as follows. First, a development data
set that does not overlap either the training set or the test set is required. Then, trans-
lation quality over the development set is maximized by iteratively varying the set of
coefficients. In our SMT system implementation, this optimization procedure is per-
formed by using a tool developed in-house, which is based on a simplex method (Press
et al 2002), and the BLEU score (Papineni et al 2002) is used as a translation quality
measurement.
As will be described in the next section, several different system configurations
are considered in the experiments. For all these optimizations, the development data
described in Table 1 are used. As presented in the table, the development data included
three translation references for both English and Spanish, which are used to compute
the BLEU score at each iteration of the optimization procedures.
The same decoder settings are used for all system optimizations. These settings are
the following:
 decoding is performed monotonically, that is, no reordering capabilities
are used,
 decoding is guided by the source sentence to be translated,
 although available in the decoder, threshold pruning is not used, and
 a value of K = 50 for during-decoding histogram pruning is used.
5. Translation Experiments and Error Analysis
This section presents all translation experiments performed and a brief error analysis
of the obtained results. In order to evaluate the relative contributions of different
system elements to the overall performance of the n-gram-based translation system,
three different experimental settings are considered. The experiments and their re-
sults are described in Section 5.1, and a brief error analysis of results is presented in
Section 5.2. Finally, a comparison between n-gram-based SMT and state-of-the-art
phrase-based translation systems is presented in Section 5.3.
5.1 Translation Experiments and Results
As already mentioned, three experimental settings are considered. For each setting,
the impact on translation quality of a different system parameter is evaluated, namely,
539
Computational Linguistics Volume 32, Number 4
feature function, n-gram size, and the source-nulled tuple strategy. Evaluations in all
three experimental settings are performed with respect to the same standard system
configuration, which is defined in terms of the following parameters:
 Alignment set used for tuple extraction: UNION
 Tuple vocabulary pruning parameter: N = 20 for Spanish to English, and
N = 30 for English to Spanish
 N-gram size used in translation model: 3
 N-gram size used in target language model: 3
 Expanded translation model with embedded-word tuples: YES
 Source-nulled tuple handling strategy: attach-to-right
 Feature functions considered: target language, word-bonus,
source-to-target lexicon, and target-to-source lexicon
In the three experimental settings considered, which are presented in the following
subsections, a total of seven different system configurations are evaluated in both
translation directions, English to Spanish and Spanish to English. Thus, a total of 14
different translation experiments are performed. For each of these cases, the corre-
sponding test set is translated by using the corresponding estimated models and set
of optimal coefficients. The same decoder settings (which were previously described in
Section 4.2.4) that were used during the optimizations are used for all translation
experiments. Translation results are evaluated in terms of mWER and BLEU by using
the two references available for each language test set.
5.1.1 Feature Function Contributions. This experiment is designed to evaluate the
relative contribution of feature functions to the overall system performance. In this
section, four different systems are evaluated. These systems are:
 System A. This constitutes the basic n-gram translation system, which
implements the tuple trigram translation model alone, that is, no
additional feature function is used.
 System B. This is a target-reinforced system. In this system, the translation
model is used along with the target-language and word-bonus models.
 System C. This is a lexicon-reinforced system. In this system, the
translation model is used along with the source-to-target and
target-to-source lexicon models.
 System D. This constitutes the full system, that is, the translation model is
used along with all four additional feature functions. This system
corresponds to the standard system configuration that was defined at the
beginning of Section 5.1.
Table 5 summarizes the results of this evaluation, in terms of BLEU and mWER, for
the four systems considered. As can be seen from the table, both translation directions,
540
Marin?o et al N-gram-based Machine Translation
Table 5
Evaluation results for experiments on feature function contribution.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN A ? ? ? ? 39.71 0.4745
B 0.29 0.31 ? ? 39.51 0.4856
C ? ? 0.77 0.08 35.77 0.5356
D 0.49 0.30 0.94 0.25 34.94 0.5434
EN ? ES A ? ? ? ? 44.46 0.4276
B 0.33 0.27 ? ? 44.67 0.4367
C ? ? 0.29 0.15 41.69 0.4482
D 0.66 0.73 0.32 0.47 40.34 0.4688
Spanish to English and English to Spanish, are considered. Table 5 also presents the
optimized log-linear coefficients associated with the features considered in each system
configuration (the log-linear weight of the translation model has been omitted from the
table because its value is fixed to 1 in all cases).
As can be observed in Table 5, the inclusion of the four feature functions into
the translation system definitively produces a significant improvement in translation
quality in both translation directions. In particular, it becomes evident that the features
with the most impact on translation quality are the lexicon models. The target language
model and the word bonus also contribute to improving translation quality, but to a
lesser degree.
Also, although it is more evident in the English-to-Spanish direction than in the
opposite one, it can be noticed from the presented results that the contribution of
target-language and word-bonus models is more relevant when the lexicon mod-
els are used (full system). In fact, as seen from the ?lm values in Table 5, when
the lexicon models are not included, the target-language model contribution to the
overall translation system becomes much less significant. A comparative analysis of
the resulting translations suggests that including the lexicon models tends to favor
short tuples over long ones, so the target-language model becomes more important
for providing target context information when the lexicon models are used. How-
ever, more experimentation and research are required for fully understanding this
interesting result.
Another important observation, which follows from comparing results between
both translation directions, is that in all cases the Spanish-to-English translations are
consistently and significantly better than the English-to-Spanish translations. This is
clearly due to the more inflected nature of Spanish vocabulary. For example, the single
English word the can generate any of the four Spanish words el, la, los, and las. Similar
situations occur with nouns, adjectives, and verbs that may have many different forms
in Spanish. This would suggest that the English-to-Spanish translation task is more
difficult than the Spanish-to-English task.
5.1.2 Translation and Language N-gram Size. This experiment is designed to evaluate
the impact of translation- and language-model n-gram sizes on overall system perform-
ance. In this section, the full system (System D in the previous experiment) is com-
pared with two similar systems for which 4-grams are used for training the translation
541
Computational Linguistics Volume 32, Number 4
model and/or the target language model. More specifically, the three systems compared
in this experiment are:
 System D, which implements a tuple trigram translation model and a word
trigram target language model. This system corresponds to the standard
system configuration that was defined at the beginning of Section 5.1.
 System E, which implements a tuple trigram translation model and a word
4-gram target language model.
 System F, which implements a tuple 4-gram translation model and a word
4-gram target language model.
Table 6 summarizes the results of this evaluation for Systems E, F, and D. Again, both
translation directions are considered and the optimized coefficients associated with the
four feature functions are also presented for each system configuration.
As can be seen in Table 6, the use of 4-grams for model computation does not
provide a clear improvement in translation quality. This is more evident in the English-
to-Spanish direction for which System F happens to be the worst ranked one, while
System D is the one obtaining the best mWER score and system E is the one obtaining
the best BLEU score. On the other hand, in the Spanish-to-English direction, it seems
that a little improvement with respect to System D is achieved by using 4-grams.
However, it is not clear which system performs the best since System E obtains the
best BLEU score while System F obtains the best mWER score.
According to these results, more experimentation and research are required to fully
understand the interaction between the n-gram sizes of translation and target language
models. Notice that in the particular case of the n-gram SMT system described here,
such an interaction is not evident at all since the n-gram-based translation model itself
contains some of the target language model information.
5.1.3 Source-nulled Tuple Strategy Comparison. This experiment is designed to eval-
uate a different strategy for handling source-nulled tuples. In this section, the standard
system configuration (System D) presented at the beginning of Section 5.1, which imple-
ments the attach-to-right strategy described in Section 2.2.2, is compared with a similar
system (referred to as System G) implementing a more complex strategy for handling
those tuples with NULL source sides. More specifically, the latter system uses the
IBM-1 lexical parameters (Brown et al 1993) for computing the translation probabilities
of two possible new tuples: the one resulting when the null-aligned-word is attached to
Table 6
Evaluation results for experiments on n-gram size incidence.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
E 0.50 0.54 0.66 0.45 34.66 0.5483
F 0.66 0.50 1.01 0.57 34.59 0.5464
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
E 0.57 0.45 0.51 0.26 40.55 0.4714
F 1.24 1.07 0.99 0.57 40.91 0.4688
542
Marin?o et al N-gram-based Machine Translation
the previous word and the one resulting when it is attached to the following one. Then,
the attachment direction is selected according to the tuple with the highest translation
probability.
Table 7 summarizes the results of evaluation Systems D and G. Again, both trans-
lation directions are considered and the optimized coefficients associated with the four
feature functions are also presented for each system configuration.
As can be seen in Table 7, consistently better results are obtained in both translation
tasks when using IBM-1 lexicon probabilities to handle tuples with a NULL source
side. Even though slight improvements are achieved in both cases, especially with
the English-to-Spanish translation task, the results show how the initial attach-to-right
strategy is easily improved by making use of some bilingual knowledge.
5.2 Error Analysis
In this last section, we present a brief description of an error analysis performed
on some of the outputs provided by the standard system configuration that was de-
scribed in Section 5.1 (system D). More specifically, a detailed review of 100 trans-
lated sentences and their corresponding source sentences, in each direction, was
conducted. This analysis was very useful since it allowed us to identify the most com-
mon errors and problems related to our n-gram based SMT system in each translation
direction.
A detailed analysis of all the reviewed translations reveals that most translation
problems encountered are typically related to four basic different types of errors:
 Verbal forms: A significant number of wrong verbal tenses and auxiliary
forms were detected. This problem turned out to be the most common
one, reflecting the difficulty of the current statistical approach to capture
the linguistic phenomena that shape head verbs, auxiliary verbs, and
pronouns into full verbal forms in each language, especially given the
inflected nature of the Spanish language.
 Omitted translations: A large number of translations involving tuples with
NULL target sides were detected. Although in some cases these situations
corresponded to correct translations, most of the time they resulted in
omitted-word errors.
 Reordering problems: The two specific situations that most commonly
occurred were problems related to adjective?noun and subject?verb
structures.
Table 7
Evaluation results for experiments on strategies for handling source-nulled tuples.
Direction System ?lm ?wb ?s2t ?t2s mWER BLEU
ES ? EN D 0.49 0.30 0.94 0.25 34.94 0.5434
G 0.49 0.45 0.78 0.39 34.15 0.5451
EN ? ES D 0.66 0.73 0.32 0.47 40.34 0.4688
G 0.96 0.93 0.53 0.44 40.12 0.4694
543
Computational Linguistics Volume 32, Number 4
 Concordance problems: Inconsistencies related to gender and number
were the most commonly found.
Table 8 presents the relative number of occurrences for each of the four types of errors
identified in both translation directions.
Notice in Table 8 that the most common errors in both translation directions are
those related to verbal forms. However, it is important to mention that 29.5% of verbal-
form errors in the English-to-Spanish direction actually correspond to verbal omissions.
Similarly, 12.8% of verbal-form errors in the Spanish-to-English direction are verbal
omissions. According to this, if errors due to omitted translations and to omitted verbal
forms are considered together, it is evident that errors involving omissions constitute
the most important group, especially in the case of English-to-Spanish translations. It
is also interesting to note that the Spanish-to-English direction exhibits more omitted-
translation errors that are not related to verbal forms than the English-to-Spanish
direction.
Also in Table 8, it can be seen that concordance errors affect more than twice as many
English-to-Spanish translations as Spanish-to-English ones. This result can be explained
by the more inflected nature of Spanish.
Finally, as an illustrative example, three Spanish-to-English translation outputs are
presented below. For each presented example, errors have been boldfaced and correct
translations are provided in brackets:
Example 1
The policy of the European Union on Cuba NULL must [must not] change.
Example 2
To achieve these purposes, it is necessary NULL for the governments to be allocated
[to allocate], at least, 60,000 million NULL dollars a year . . .
Example 3
In the UK we have NULL [already] laws enough [enough laws], but we want to encourage
NULL other States . . .
5.3 N-gram-based SMT Compared with Phrase-Based SMT
The n-gram-based translation system here described has been also evaluated and com-
pared to other phrase-based translation systems in the context of the European Project
Table 8
Percentage of occurrence for each type of error in English-to-Spanish and Spanish-to-English
translations that were studied.
Type of error English-to-Spanish Spanish-to-English
Verbal forms 31.3% 29.9%
Omitted translations 22.0% 26.1%
Reordering problems 15.9% 19.7%
Concordance problems 10.8% 4.6%
Other errors 20.0% 19.7%
544
Marin?o et al N-gram-based Machine Translation
TC-STAR. A detailed description of the first evaluation campaign (including the main
characteristics of every system) is available through the consortium?s Web site as a
progress report (Ney et al 2005).
Table 9 presents the four best BLEU results for the EPPS translation task in the
first TC-STAR?s evaluation campaign, where the results corresponding to our n-gram-
based translation system are provided in brackets. A total of six systems were evaluated
in this evaluation campaign. The task consisted of two translation directions: English
to Spanish and Spanish to English, and three different evaluation conditions: final
text edition, verbatim, and ASR output. The final text edition condition corresponds
to the official transcripts of the EPPS, so it is actually a written-language translation
condition. On the other hand, the other two conditions are spoken-language transla-
tion conditions. More specifically, the verbatim condition corresponds to literal tran-
scriptions of parliamentary speeches, which include hesitations, repeated words, and
other spontaneous speech effects; and the ASR output condition corresponds to the
output of an automatic speech recognition system, so it additionally includes speech-
recognition errors.
As can be seen in Table 9, performance of the n-gram-based translation system is
among the three best systems for the translation directions and conditions considered
in the first TC-STAR evaluation campaign.
Another independent comparison of the translation system proposed here with
other phrase-based translation systems is available through the results of the second
shared task of the ACL 2005 workshop on ?Building and using parallel texts: Data-
driven machine translation and beyond.? In this shared task, which was entitled ?Ex-
ploiting Parallel Texts for Statistical Machine Translation,? our n-gram-based translation
system was evaluated in four different translation directions: Spanish to English, French
to English, German to English, and Finish to English (Banchs et al 2005). The domain
of this task was also the European Parliament; however, the data set considered in this
evaluation was different from the one used in TC-STAR?s evaluation campaign. The
final text edition condition (official transcripts) was the only one considered here. A total
of twelve different systems participated in this shared task. Table 10 presents the four
best BLEU results for each of the four translation directions considered in the shared
task. Again, results corresponding to our n-gram-based translation system are provided
in brackets.
As can be seen in Table 10, the performance of the n-gram-based translation system
is among the three best systems for the four translation directions considered in the
ACL 2005 workshop shared task. The third system in Table 10 for ES to EN translation
Table 9
The four best BLEU results for the EPPS translation task in TC-STAR?s first evaluation campaign.
N-gram based system results are provided in brackets. All BLEU values presented here have
been taken from TC-STAR?s SLT Progress Report, available at: http://www.tc-star.org/.
Direction Condition First Second Third Fourth
ES ? EN Final text edition [53.3] 53.1 47.5 46.1
Verbatim 45.9 44.1 [42.1] 38.1
ASR output 41.5 39.7 [37.7] 34.7
EN ? ES Final text edition [46.2] 45.2 38.9 37.6
Verbatim 42.5 [38.1] 36.8 33.4
ASR output 38.7 34.3 [33.8] 33.0
545
Computational Linguistics Volume 32, Number 4
Table 10
The four best BLEU results for the four translation directions considered in the shared task
?Exploiting Parallel Texts for Statistical Machine Translation? (ACL 2005 workshop on
?Building and using parallel texts: Data-driven machine translation and beyond?). N-gram-
based system results are provided in brackets. All BLEU values presented here have been
taken from the shared task?s Web site: http://www.statmt.org/wpt05/mt-shared-task/.
Direction Condition First Second Third Fourth
FR ? EN Final text edition 30.27 [30.20] 29.53 28.89
ES ? EN Final text edition 30.95 [30.07] 29.84 29.08
DE ? EN Final text edition 24.77 [24.26] 23.21 22.91
FI ? EN Final text edition 22.01 20.95 [20.31] 18.87
deserves some comment. This system is a conventional phrase-based system sharing
the same decoder MARIE, IBM features, word bonus, and target-language model as the
n-gram-based system. The specific characteristics of the phrase-based system are direct
and inverse phrase conditional probabilities and phrase penalty. Additional compar-
isons between an n-gram system and a phrase-based system sharing a common decoder
and training and test framework can be found in Crego et al (2005c).
6. Conclusions and Further Work
As can be concluded from the results presented, the tuple n-gram translation model,
when used along with additional feature functions, provides state-of-the-art transla-
tions for the considered translation directions.
Another important result is that the quality of Spanish-to-English translations is
significantly and consistently better than those obtained in English-to-Spanish transla-
tions. Consequently, significant efforts should be dedicated towards properly exploiting
morphological analysis and synthesis methods for improving English-to-Spanish trans-
lation quality.
Additionally, four commonly occurring types of translation errors were identified
by reviewing a significant number of translated sentence pairs. This analysis has pro-
vided us with useful hints for future research and improvement of our SMT system.
However, more evaluation and discussion are required in this area in order to fully
understand these common translation failures and then implementing appropriate
solutions.
All the experiments presented in this work were performed using monotone de-
coding, and no reordering strategies were implemented. Although this system con-
figuration proved to provide state-of-the-art translations for the tasks presented, this
may not hold for tasks involving more distant language pairs for which reordering
capabilities must be implemented. Accordingly, along with other results obtained in
the present work, we consider that further research on n-gram SMT should focus on the
following issues:
 Reordering strategies, as well as non-monotonous decoding schemes, for
the proposed SMT system must be developed and tested. As mentioned
before, reordering problems specifically related to adjective?noun and
subject?verb structures occur very often in Spanish-to-English and
546
Marin?o et al N-gram-based Machine Translation
English-to-Spanish translations. Preliminary results concerning the use of
word class deterministic reordering and POS-tag-based reordering
patterns can be found in Costa-jussa`, Fonollosa, and Monte (2006) and
Crego and Marin?o (2006), respectively.
 An effective long-tuple unfolding strategy must be developed to avoid
the occurrence of long tuples resulting from long alignment links, which
happens to be a common situation when dealing with translations
between distant pairs of languages. This problem is closely related to
reordering, and some preliminary results have been presented by Crego,
Marin?o, and de Gispert (2005b).
 The definition of the tuple as a bilingual pair will be revised in order to
better handle unaligned words in both the source and the target sides. As
mentioned above, a better strategy for dealing with target words aligned
to NULL is required. Similarly, a better handling of NULLs in the target
side will result in fewer omitted-translation errors.
 The extension of the embedded-word concept to the more general idea of
embedded n-grams should be evaluated and implemented. Accordingly, a
translation probability should be estimated for those groups of words
that always occur embedded in tuples. This would guarantee that the
decoder will always have a translation option for any given word or word
combination previously seen in the training data. Further work is required
to determine the relative impact of these embedded n-grams on the
translation model, and the most appropriate strategy for handling them.
 Linguistic information must be used to cope with the observed
morphological problems in the English-to-Spanish translation direction,
as well as the more general problem of incorrect verbal form translations.
In this regard, ongoing research on linguistic tuples classification is
being done in order to improve translation results. Preliminary results
on detecting and classifying verb forms have been presented by
de Gispert (2005).
 A more detailed error analysis than the one presented in Section 5.2 is
required to fully understand the n-gram SMT system behavior and the
specific causes of each resulting type of error. It would be very useful for
improving our translation system performance to clearly identify whether
these errors are due to unseen information while training, to modeling
problems, or to decoding errors.
Acknowledgments
This work has been partly funded by the
European Union under the integrated project
TC-STAR (Technology and Corpora for
Speech to Speech Translation) (IST-2002-
FP6-506738, http://www.tc-star.org), the
Spanish Department of Education and
Science (MEC), the Department of
Universities, Research and Information
Society (Generalitat de Catalunya), and
the Universitat Polite`cnica de Catalunya.
References
Banchs, Rachel E., Josep Maria Crego,
Adria` de Gispert, Patrik Lambert, and
Jose? Bernardo Marin?o. 2005. Statistical
machine translation of Euparl data by
using bilingual n-grams. In ACL Workshop
on Data-Driven Machine Translation and
Beyond, pages 133?136, Ann Arbor, MI.
Bangalore, Srinivas and Giuseppe Riccardi.
2000. Stochastic finite-state models for
spoken language machine translation.
547
Computational Linguistics Volume 32, Number 4
In Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Berger, Adam, Stephen Della Pietra, and
Vincent Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Brown, Peter, John Cocke, Stephen Della
Pietra, Vincent Della Pietra, Frederick
Jelinek, John Lafferty, Robert Mercer, and
Paul S. Roossin. 1990. A statistical
approach to machine translation.
Computational Linguistics, 16(2):79?85.
Brown, Peter, Stephen Della Pietra, Vincent
Della Pietra, and Robert Mercer. 1993.
The mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Casacuberta, Francisco. 2001. Finite-state
transducers for speech input translation. In
Proceedings IEEE ASRU, pages 375?380,
Madonna di Campiglio, Italy.
Casacuberta, Francisco and Enrique Vidal.
2004. Machine translation with inferred
stochastic finite-state transducers.
Computational Linguistics, 30(2):205?225.
Costa-jussa`, Marta Ruiz, Jose? Adria?n
Rodriguez Fonollosa, and Enric Monte.
2006. Using reordering in statistical
machine translation based on alignment
block classification. Internal Report.
http://gps-tsc.upc.es/veu/personal/
mruiz/docs/br06.pdf.
Crego, Josep Maria, Jose? Bernardo
Marin?o, and Adria` de Gispert. 2004.
Finite-state-based and phrase-based
statistical machine translation. In
Proceedings of the 8th International
Conference on Spoken Language
Processing, pages 37?40, Jeju, Korea.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005a. An
Ngram-based statistical machine
translation decoder. In INTERSPEECH
2005, pages 3185?3188, Lisbon, Portugal.
Crego, Josep Maria, Jose? Bernardo Marin?o,
and Adria` de Gispert. 2005b. Reordered
search and tuple unfolding for Ngram-
based SMT. Proceedings of the Tenth
Machine Translation Summit, pages 283?289,
Phuket, Thailand.
Crego, Josep Maria, Marta Ruiz Costa-jussa`,
Jose? Bernardo Marin?o, and Jose? Adria?n
Rodriguez Fonollosa. 2005c. Ngram-
based versus phrase-based statistical
machine translation. In Proceedings of the
International Workshop on Spoken Language
Translation, pages 177?184, Pittsburgh, PA.
Crego, Josep Maria and Jose? Bernardo
Marin?o. 2006. Integration of POStag-based
source reordering into SMT decoding by
an extended search graph. In Proceedings of
the 7th Biennial Conference of the Association
for Machine Translation in the Americas,
Boston, MA.
de Gispert, Adria` and Jose? Bernardo Marin?o.
2002. Using X-grams for speech-to-
speech translation. In Proceedings of the
7th International Conference on Spoken
Language Processing, pages 1885?1888,
Denver, CO.
de Gispert, Adria`, Jose? Bernardo Marin?o, and
Josep Maria Crego. 2004. TALP:
Xgram-based spoken language translation
system. In Proceedings of the International
Workshop on Spoken Language Translation,
pages 85?90, Kyoto, Japan.
de Gispert, Adria`. 2005. Phrase linguistic
classification and generalization for
improving statistical machine translation.
In ACL?05 Student Workshop, pages 67?72,
Ann Arbor, MI.
Hutchins, John. 1986. Machine Translation:
Past, Present and Future. Ellis Horwood,
Chichester, England.
Kay, Martin, Jean Mark Gawron, and Peter
Norvig. 1992. Verbmobil: A Translation
System for Face-to-Face Dialog. CSLI.
Kneser, Reinhard and Hermann Ney. 1995.
Improved backing-off for m-gram
language modeling. In IEEE International
Conference on Acoustics, Speech and Signal
Processing, pages 49?52, Detroit, MI.
Knight, Kevin and Yaser Al-Onaizan.
1998. Translation with finite-state
devices. In AI Lecture Notes in Artificial
Intelligence, volume 1529, Springer-Verlag,
pages 421?437.
Koehn, Philippe, Franz Joseph Och,
and Daniel Marcu. 2003. Statistical
phrase-based translation. In Proceedings
of the 2003 Meeting of the North American
chapter of the ACL, pages 48?54, Edmonton,
Alberta, Canada.
Koehn, Philippe. 2002. Europarl: A
multilingual corpus for evaluation
of machine translation. Available
online at: http://people.csail.mit.edu/
people/koehn/publications/europarl/.
Marin?o, Jose? Bernardo, Rafael E. Banchs,
Josep Maria Crego, Adria` de Gispert,
Patrik Lambert, Jose? Adria?n Rodriguez
Fonollosa, and Marta Ruiz. 2005. Bilingual
N-gram statistical machine translation.
In Proceedings of the Tenth Machine
Translation Summit, pages 275?282,
Phuket, Thailand.
548
Marin?o et al N-gram-based Machine Translation
Ney, Hermann, Volker Steinbiss, Richard
Zens, Evgeny Matusov, Jorge Gonza?lez,
Young-suk Lee, Salim Roukos, Marcello
Federico, Muntsin Kolss, and Rafael
Banchs. 2005. SLT progress report.
TC-STAR Deliverable D5, European
Community project no. FP6-506738.
Available online at: http://www.
tc-star.org/pages/f documents.htm.
Och, Franz Joseph and Hermann Ney.
2000. Improved statistical alignment
models. In Proceedings of the 38th Annual
Meeting of the ACL, pages 440?447,
Hong Kong, China.
Och, Franz Joseph and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 295?302,
Philadelphia, PA.
Och, Franz Joseph and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Och, Franz Joseph, Daniel Gildea, Sanjeev
Khudanpur, Anoop Sarkar, Kenji Yamada,
Alexander Fraser, Shankar Kumar, Libin
Shen, David Smith, Katharine Eng, Viren
Jain, Zhen Jin, and Dragomir Radev. 2004.
A smorgasbord of features for statistical
machine translation. In Proceedings of the
Human Language Technology Conference
NAACL, pages 161?168, Boston, MA, May.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu:
A method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Conference of the ACL,
pages 311?318, Philadelphia, PA.
Press, William H., Saul Teukolsky, William
Vetterling, and Brian P. Flannery.
2002. Numerical Recipes in C++: The
Art of Scientific Computing, Cambridge
University Press.
Riccardi, Giuseppe, Roberto Pieraccini, and
Enrico Bocchieri. 1996. Stochastic automata
for language modeling. Computer Speech
and Language, 10(4):265?293.
Shannon, Claude E. 1949. Communication
theory of secrecy systems. Bell System
Technical Journal, 28:656?715.
Shannon, Claude E. 1951. Prediction and
entropy of printed English. Bell System
Technical Journal, 30:50?64.
Shannon, Claude E. and Warren Weaver.
1949. The Mathematical Theory of
Communication, University of Illinois
Press, Urbana, IL.
Stolcke, Andreas 2002. SRLIM: An extensible
language modeling toolkit. In Proceedings
of the International Conference on Spoken
Language Processing, pages 901?904,
Denver, CO.
Tillmann, Christoph and Fei Xia. 2003. A
phrase-based unigram model for statistical
machine translation. In Proceedings of
HLT-NAACL - Short Papers, pages 106?108,
Edmonton, Alberta, Canada.
Vidal, Enrique. 1997. Finite-state speech-to-
speech translation. In Proceedings of 1997
IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 111?114,
Munich, Germany.
Weaver, Warren. 1955. Translation. In
William Locke and A. Donald Booth,
editors, Machine Translation of Languages:
Fourteen Essays. John Wiley & Sons, New
York, pages 15?23.
Zens, Richard, Franz Joseph Och, and
Hermann Ney. 2002. Phrase-based
statistical machine translation. In
25th German Conference on Artificial
Intelligence, pages 18?32, September.
Aachen, Springer Verlag.
549

Proceedings of NAACL HLT 2007, Companion Volume, pages 137?140,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Analysis and System Combination of Phrase- and N -gram-based
Statistical Machine Translation Systems
Marta R. Costa-jussa`1, Josep M. Crego1, David Vilar2
Jose? A. R. Fonollosa1, Jose? B. Marin?o1 and Hermann Ney2
1TALP Research Center (UPC), Barcelona 08034, Spain
{mruiz,jmcrego,adrian,canton}@gps.tsc.upc.edu
2RWTH Aachen University, Aachen D-52056, Germany
{vilar,ney}@i6.informatik.rwth-aachen.de
Abstract
In the framework of the Tc-Star project,
we analyze and propose a combination of
two Statistical Machine Translation sys-
tems: a phrase-based and an N -gram-based
one. The exhaustive analysis includes a
comparison of the translation models in
terms of efficiency (number of translation
units used in the search and computational
time) and an examination of the errors in
each system?s output. Additionally, we
combine both systems, showing accuracy
improvements.
1 Introduction
Statistical machine translation (SMT) has evolved
from the initial word-based translation models to
more advanced models that take the context sur-
rounding the words into account. The so-called
phrase-based and N -gram-based models are two ex-
amples of these approaches (Zens and Ney, 2004;
Marin?o et al, 2006).
In current state-of-the-art SMT systems, the
phrase-based or the N -gram-based models are usu-
ally the main features in a log-linear framework, rem-
iniscent of the maximum entropy modeling approach.
Two basic issues differentiate the N -gram-based
system from the phrase-based one: the training data
is sequentially segmented into bilingual units; and
the probability of these units is estimated as a bilin-
gual N -gram language model. In the phrase-based
model, no monotonicity restriction is imposed on the
segmentation and the probabilities are normally es-
timated simply by relative frequencies.
This paper extends the analysis of both systems
performed in (Crego et al, 2005a) by additionally
performing a manual error analysis of both systems,
which were the ones used by UPC and RWTH in the
last Tc-Star evaluation.
Furthermore, we will propose a way to combine
both systems in order to improve the quality of trans-
lations.
Experiments combining several kinds of MT sys-
tems have been presented in (Matusov et al, 2006),
based only on the single best output of each system.
Recently, a more straightforward approach of both
systems has been performed in (Costa-jussa` et al,
2006) which simply selects, for each sentence, one of
the provided hypotheses.
This paper is organized as follows. In section 2,
we briefly describe the phrase and the N -gram-based
baseline systems. In the next section we present the
evaluation framework. In Section 4 we report a struc-
tural comparison performed for both systems and, af-
terwards, in Section 5, we analyze the errors of both
systems. Finally, in the last two sections we rescore
and combine both systems, and the obtained results
are discussed.
2 Baseline Systems
2.1 Phrase-based System
The basic idea of phrase-based translation is to seg-
ment the given source sentence into units (here called
phrases), then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions.
In order to train these phrase-based models, an
alignment between the source and target training
sentences is found by using the standard IBM mod-
els in both directions (source-to-target and target-
to-source) and combining the two obtained align-
ments. Given this alignment an extraction of con-
tiguous phrases is carried out, specifically we extract
all phrases that fulfill the following restrictions: all
source (target) words within the phrase are aligned
only to target (source) words within the phrase.
The probability of these phrases is normally esti-
mated by relative frequencies, normally in both di-
rections, which are then combined in a log-linear way.
137
2.2 N-gram-based System
In contrast with standard phrase-based approaches,
the N -gram translation model uses tuples as bilin-
gual units whose probabilities are estimated as an
N -gram language model (Marin?o et al, 2006). This
model approximates the joint probability between
the source and target languages by using N -grams.
Given a word alignment, tuples define a unique
and monotonic segmentation of each bilingual sen-
tence, building up a much smaller set of units
than with phrases and allowing N -gram estimation
to account for the history of the translation pro-
cess (Marin?o et al, 2006).
2.3 Feature functions
Both baseline systems are combined in a log-linear
way with several additional feature functions: a tar-
get language model, a forward and a backward lex-
icon model and a word bonus are common features
for both systems. The phrase-based system also in-
troduces a phrase bonus model.
3 Evaluation framework
The translation models presented so far were the ones
used by UPC and RWTH in the second evaluation
campaign of the Tc-Star project. The goal of this
project is to build a speech-to-speech translation sys-
tem that can deal with real life data.
The corpus consists of the official version of the
speeches held in the European Parliament Plenary
Sessions (EPPS), as available on the web page of the
European Parliament. Table 1 shows some statistics.
The following tools have been used for building
both systems: Word alignments were computed us-
ing GIZA++ (Och, 2003), language models were es-
timated using the SRILM toolkit (Stolcke, 2002), de-
coding was carried out by the free available MARIE
decoder (Crego et al, 2005b) and the optimization
was performed through an in-house implementation
of the simplex method (Nelder and Mead, 1965).
Spanish English
Train Sentences 1.2M
Words 32M 31M
Vocabulary 159K 111K
Dev Sentences 1 122 699
Words 26K 21K
Test Sentences 1 117 894
Words 26K 26K
Table 1: Statistics of the EPPS Corpora.
4 Structural comparison
Both approaches aim at improving accuracy by in-
cluding word context in the model. However, the
implementation of the models are quite different and
may produce variations in several aspects.
Table 2 shows the effect on decoding time intro-
duced through different settings of the beam size.
Additionally, the number of available translation
units is shown, corresponding to number of avail-
able phrases for the phrase-based system and 1gram,
2gram and 3gram entries for the N -gram-based sys-
tem. Results are computed on the development set.
Task Beam Time(s) Units
50 2,677
es?en 10 852 537k
5 311
50 2,689
en?es 10 903 594k
5 329
50 1,264
es?en 10 281 104k 288k 145k
5 138
50 1,508
en?es 10 302 118k 355k 178k
5 155
Table 2: Impact on efficiency of the beam size in PB
(top) and NB system (bottom).
As it can be seen, the number of translation units
is similar in both tasks for both systems (537k ?
537k for Spanish to English and 594k ? 651k for
English to Spanish) while the time consumed in de-
coding is clearly higher for the phrase-based system.
This can be explained by the fact that in the phrase-
based approach, the same translation can be hypoth-
esized following several segmentations of the input
sentence, as phrases appear (and are collected) from
multiple segmentations of the training sentence pairs.
In other words, the search graph seems to be over-
populated under the phrase-based approach.
Table 3 shows the effect on translation accuracy
regarding the size of the beam in the search. Results
are computed on the test set for the phrase-based
and N -gram-based systems.
Results of the N -gram-based system show that de-
creasing the beam size produces a clear reduction
of the accuracy results. The phrase-based system
shows that accuracy results remain very similar un-
der the different settings. The reason is found on
how translation models are used in the search. In
the phrase-based approach, every partial hypothesis
138
Task Beam BLEU NIST mWER
50 51.90 10.53 37.54
es?en 10 51.93 10.54 37.49
5 51.87 10.55 37.47
50 47.75 9.94 41.20
en?es 10 47.77 9.96 41.09
5 47.86 10.00 40.74
50 51.63 10.46 37.88
es?en 10 51.50 10.45 37.83
5 51.39 10.45 37.85
50 47.73 10.08 40.50
en?es 10 46.82 9.97 41.04
5 45.59 9.83 41.04
Table 3: Impact on accuracy of the beam size in PB
(top) and NB system (bottom).
is scored uncontextualized, hence, a single score is
used for a given partial hypothesis (phrase). In the
N -gram-based approach, the model is intrinsically
contextualized, which means that each partial hy-
pothesis (tuple) depends on the preceding sequence
of tuples. Thus, if a bad sequence of tuples (bad
scored) is composed of a good initial sequence (well
scored), it is placed on top of the first stacks (beam)
and may cause the pruning of the rest of hypotheses.
5 Error analysis
In order to better asses the quality and the differ-
ences between the two systems, a human error anal-
ysis was carried out. The guidelines for this error
analysis can be found in (Vilar et al, 2006). We
randomly selected 100 sentences, which were evalu-
ated by bilingual judges.
This analysis reveals that both systems produce
the same kind of errors in general. However some dif-
ferences were identified. For the English to Spanish
direction the greatest problem is the correct genera-
tion of the right tense for verbs, with around 20% of
all translation errors being of this kind. Reordering
also poses an important problem for both phrase and
N-gram-based systems, with 18% or 15% (respec-
tively) of the errors falling into this category. Miss-
ing words is also an important problem. However,
most of them (approximately two thirds for both sys-
tems) are filler words (i.e. words which do not con-
vey meaning), that is, the meaning of the sentence
is preserved. The most remarkable difference when
comparing both systems is that the N -gram based
system produces a relatively large amount of extra
words (approximately 10%), while for the phrase-
based system, this is only a minor problem (2% of
the errors). In contrast the phrase-based system has
more problems with incorrect translations, that is
words for which a human can find a correspondence
in the source text, but the translation is incorrect.
Similar conclusions can be drawn for the inverse di-
rection. The verb generating problem is not so acute
in this translation direction due to the much simpli-
fied morphology of English. An important problem
is the generation of the right preposition.
The N -gram based system seems to be able to pro-
duce more accurate translations (reflected by a lower
percentage of translation errors). However, it gener-
ates too many additional (and incorrect words) in
the process. The phrase-based system, in contrast,
counteracts this effect by producing a more direct
correspondence with the words present in the source
sentence at the cost of sometimes not being able to
find the exact translation.
6 System Rescoring and
Combination
Integration of both output translations in the search
procedure is a complex task. Translation units of
both models are quite different and generation his-
tories pose severe implementation difficulties. We
propose a method for combining the two systems at
the level of N -best lists.
Some features that are useful for SMT are too com-
plex for including them directly in the search pro-
cess. A clear example are the features that require
the entire target sentence to be evaluated, as this is
not compatible with the pruning and recombination
procedures that are necessary for keeping the target
sentence generation process manageable. A possible
solution for this problem is to apply sentence level
re-ranking by using N -best lists.
6.1 Rescoring Criteria
The aim of the rescoring procedure is to choose the
best translation candidate out of a given set of N
possible translations. In our approach this transla-
tion candidates are produced independently by both
of the systems and then combined by a simple con-
catenation1. In order for the hypothesis to have a
comparable set of scores, we perform an additional
?cross-rescoring? of the lists.
Given an N -best list of the phrase-based (N -gram-
based) system, we compute the cost of each target
sentence of this N -best list for the N -gram-based
(phrase-based) system. However this computation
is not possible in all cases. Table 4 shows the per-
centage of target sentences that the N -gram-based
1With removal of duplicates.
139
(phrase-based) system is able to produce given an N -
best list of target sentences computed by the phrase-
based (N -gram-based) system. This percentage is
calculated on the development set.
The vocabulary of phrases is bigger than the vo-
cabulary of tuples, due to the fact that phrases are
extracted from multiple segmentations of the train-
ing sentence pairs. Hence, the number of sentences
reproduced by the N -gram-based system is smaller
than the number of sentences reproduced by the
phrase-based system. Whenever a sentence can not
be reproduced by a given system, the cost of the
worst sentence in the N -best list is assigned to it.
Task N -best % NB % PB
es?en 1000 37.5 57.5
en?es 1000 37.2 48.6
Table 4: Sentences (%) produced by each system.
6.2 Results
Table 5 shows results of the rescoring and system
combination experiments on the test set. The first
two rows include results of systems non-rescored and
PB (NB) rescored by NB (PB). The third row corre-
sponds to the system combination. Here, PB (NB)
rescored by NB (PB) are simply merged and ranked
by rescored score.
System N -best BLEU NIST mWER
Spanish-to-English
PB 1 51.90 10.54 37.50
PB 1000 52.55 10.61 37.12
NB 1 51.63 10.46 37.88
NB 1000 52.25 10.55 37.43
PB+NB 2 51.77 10.49 37.68
PB+NB 2000 52.31 10.56 37.32
English-to-Spanish
PB 1 47.75 9.94 41.2
PB 1000 48.46 10.13 39.98
NB 1 47.73 10.09 40.50
NB 1000 48.33 10.15 40.13
PB+NB 2 48.26 10.05 40.61
PB+NB 2000 48.54 10.16 40.00
Table 5: Rescoring and system combination results.
7 Discussion
The structural comparison has shown on the one
hand that the N -gram-based system outperforms
the phrase-based in terms of search time efficiency
by avoiding the overpopulation problem presented
in the phrase-based approach. On the other hand
the phrase-based system shows a better performance
when decoding under a highly constrained search.
A detailed error analysis has also been carried out
in order to better determine the differences in per-
formance of both systems. The N -gram based sys-
tem produced more accurate translations, but also a
larger amount of extra (incorrect) words when com-
pare to the phrase-based translation system.
In section 6 we have presented a system combina-
tion method using a rescoring feature for each SMT
system, i.e. the N -gram-based feature for the phrase-
based system and vice-versa. For both systems, con-
sidering the feature of the opposite system leads to
an improvement of BLEU score.
References
M.R. Costa-jussa`, J.M. Crego, A. de Gispert,
P. Lambert, M. Khalilov J.A.R. Fonollosa, J.B.
Marin?o, and R. Banchs. 2006. Talp phrase-based
statistical machine translation and talp system
combination the iwslt 2006. IWSLT06.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005a. N-gram-based versus phrase-
based statistical machine translation. IWSLT05,
October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005b.
An Ngram-based statistical machine translation
decoder. ICSLP05, April.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gis-
pert, P. Lambert, J.A.R. Fonollosa, and M.R.
Costa-jussa`. 2006. N-gram based machine trans-
lation. Computational Linguistics, 32(4):527?549.
E. Matusov, N. Ueffing, and H. Ney. 2006. Com-
puting consensus translation from multiple ma-
chine translation systems using enhanced hypothe-
ses alignment. EACL06, pages 33?40.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language
modeling toolkit. Proc. of the 7th Int. Conf. on
Spoken Language Processing, ICSLP?02, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D?Haro, and
Hermann Ney. 2006. Error Analysis of Machine
Translation Output. In LREC06, pages 697?702,
Genoa, Italy, May.
Richard Zens and Hermann Ney. 2004. Improve-
ments in phrase-based statistical machine transla-
tion. In HLT04, pages 257?264, Boston, MA, May.
140
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 149?154,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Improving Phrase-Based Statistical Translation by modifying
phrase extraction and including several features
Marta Ruiz Costa-jussa` and Jose? A. R. Fonollosa
TALP Research Center
Universitat Polite`cnica de Catalunya
{mruiz,adrian}@gps.tsc.upc.edu
Abstract
Nowadays, most of the statistical translation sys-
tems are based on phrases (i.e. groups of words).
In this paper we study different improvements to
the standard phrase-based translation system. We
describe a modified method for the phrase extrac-
tion which deals with larger phrases while keeping
a reasonable number of phrases. We also propose
additional features which lead to a clear improve-
ment in the performance of the translation. We
present results with the EuroParl task in the direc-
tion Spanish to English and results from the evalu-
ation of the shared task ?Exploiting Parallel Texts
for Statistical Machine Translation? (ACL Work-
shop on Parallel Texts 2005).
1 Introduction
Statistical Machine Translation (SMT) is based on
the assumption that every sentence e in the target
language is a possible translation of a given sen-
tence f in the source language. The main difference
between two possible translations of a given sen-
tence is a probability assigned to each, which has
to be learned from a bilingual text corpus. Thus,
the translation of a source sentence f can be for-
mulated as the search of the target sentence e that
maximizes the translation probability P (e|f),
e? = argmax
e
P (e|f) (1)
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project).
If we use Bayes rule to reformulate the transla-
tion probability, we obtain,
e? = argmax
e
P (f |e)P (e) (2)
This translation model is known as the source-
channel approach [1] and it consists on a lan-
guage model P (e) and a separate translation model
P (f |e) [5].
In the last few years, new systems tend to use
sequences of words, commonly called phrases [8],
aiming at introducing word context in the transla-
tion model. As alternative to the source-channel
approach the decision rule can be modeled through
a log-linear maximum entropy framework.
e? = argmax
e
{ M
?
m=1
?mhm(e, f)
}
(3)
The features functions, hm, are the system mod-
els (translation model, language model and others)
and weigths, ?i, are typically optimized to max-
imize a scoring function. It is derived from the
Maximum Entropy approach suggested by [13] [14]
for a natural language understanding task. It has
the advantatge that additional features functions
can be easily integrated in the overall system.
This paper addresses a modification of the
phrase-extraction algorythm in [11]. It also com-
bines several interesting features and it reports an
important improvement from the baseline. It is or-
ganized as follows. Section 2 introduces the base-
line; the following section explains the modification
in the phrase extraction; section 4 shows the differ-
ent features which have been taken into account;
section 5 presents the evaluation framework; and
149
the final section shows some conclusions on the ex-
periments in the paper and on the results in the
shared task.
2 Baseline
The baseline is based on the source-channel ap-
proach, and it is composed of the following models
which later will be combined in the decoder.
The Translation Model. It is based on bilin-
gual phrases, where a bilingual phrase (BP ) is
simply two monolingual phrases (MP ) in which
each one is supposed to be the translation of each
other. A monolingual phrase is a sequence of words.
Therefore, the basic idea of phrase-based transla-
tion is to segment the given source sentence into
phrases, then translate each phrase and finally com-
pose the target sentence from these phrase transla-
tions [17].
During training, the system has to learn a dictio-
nary of phrases. We begin by aligning the training
corpus using GIZA++ [6], which is done in both
translation directions. We take the union of both
alignments to obtain a symmetrized word align-
ment matrix. This alignment matrix is the starting
point for the phrase based extraction.
Next, we define the criterion to extract the set of
BP of the sentence pair (f j2j1 ; e
i2
i1) and the alignment
matrix A ? J?I , which is identical to the alignment
criterion described in [11].
BP (fJ1 , eI1, A) = {(f j2j1 , e
i2
i1) :
?(j, i)?A : j1 ? j ? j2 ? i1 ? i ? i2
??(j, i)?A : j1 ? j ? j2 ? i1 ? i ? i2}
The set of BP is consistent with the alignment
and consists of all BP pairs where all words within
the foreign language phrase are only aligned to the
words of the English language phrase and viceversa.
At least one word in the foreign language phrase has
to be aligned with at least one word of the English
language. Finally, the algorithm takes into account
possibly unaligned words at the boundaries of the
foreign or English language phrases.
The target language model. It is combined
with the translation probability as showed in equa-
tion (2). It gives coherence to the target text ob-
tained by the concatenated phrases.
3 Phrase Extraction
Motivation. The length of a MP is defined as
its number of words. The length of a BP is the
greatest of the lengths of its MP .
As we are working with a huge amount of data
(see corpus statistics), it is unfeasible to build a
dictionary with all the phrases longer than length
4. Moreover, the huge increase in computational
and storage cost of including longer phrases does
not provide a significant improve in quality [8].
X-length In our system we considered two length
limits. We first extract all the phrases of length 3
or less. Then, we also add phrases up to length
5 if they cannot be generated by smaller phrases.
Empirically, we chose 5, as the probability of reap-
pearence of larger phrases decreases.
Basically, we select additional phrases with
source words that otherwise would be missed be-
cause of cross or long alignments. For example,
from the following sentence,
Cuando el Parlamento Europeo , que tan fre-
cuentemente insiste en los derechos de los traba-
jadores y en la debida proteccio?n social , (...)
NULL ( ) When ( 1 ) the ( 2 ) European ( 4
) Parliament ( 3 4 ) , ( 5 ) that ( 6 ) so ( 7 )
frequently ( 8 ) insists ( 9 ) on ( 10 ) workers ( 11
15 ) ? ( 14 ) rights ( 12 ) and ( 16 ) proper ( 19 )
social ( 21 ) protection ( 20 ) , ( 22 ) (...)
where the number inside the clauses is the
aligned word(s). And the phrase that we are look-
ing for is the following one.
los derechos de los trabajadores # workers ?
rights
which only could appear in the case the maximum
length was 5.
150
4 Phrase ranking
4.1 Conditional probability P (f |e)
Given the collected phrase pairs, we estimated the
phrase translation probability distribution by rela-
tive frecuency.
P (f |e) = N(f, e)N(e) (4)
where N(f,e) means the number of times the phrase
f is translated by e. If a phrase e has N > 1
possible translations, then each one contributes as
1/N [17].
Note that no smoothing is performed, which may
cause an overestimation of the probability of rare
phrases. This is specially harmful given a BP
where the source part has a big frecuency of ap-
pearence but the target part appears rarely. For
example, from our database we can extract the fol-
lowing BP : ?you # la que no?, where the English
is the source language and the Spanish, the tar-
get language. Clearly, ?la que no? is not a good
translation of ?you?, so this phrase should have a
low probability. However, from our aligned training
database we obtain,
P (f |e) = P (you|la que no) = 0.23
This BP is clearly overestimated due to sparse-
ness. On the other, note that ?la que no? can-
not be considered an unusual trigram in Spanish.
Hence, the language model does not penalise this
target sequence either. So, the total probability
(P (f |e)P (e)) would be higher than desired.
In order to somehow compensate these unreili-
able probabilities we have studied the inclusion of
the posterior [12] and lexical probabilities [1] [10]
as additional features.
4.2 Feature P (e|f)
In order to estimate the posterior phrase probabil-
ity, we compute again the relative frequency but re-
placing the count of the target phrase by the count
of the source phrase.
P (e|f) = N
?(f, e)
N(f) (5)
where N?(f,e) means the number of times the
phrase e is translated by f. If a phrase f has N > 1
possible translations, then each one contributes as
1/N.
Adding this feature function we reduce the num-
ber of cases in which the overall probability is over-
estimated. This results in an important improve-
ment in translation quality.
4.3 IBM Model 1
We used IBM Model 1 to estimate the probability
of a BP . As IBM Model 1 is a word translation and
it gives the sum of all possible alignment probabil-
ities, a lexical co-ocurrence effect is expected. This
captures a sort of semantic coherence in transla-
tions.
Therefore, the probability of a sentence pair is
given by the following equation.
P (f |e; M1) = 1(I + 1)J
J
?
j=1
I
?
i=0
p(fj |ei) (6)
The p(fj |ei) are the source-target IBM Model 1
word probabilities trained by GIZA++. Because
the phrases are formed from the union of source-to-
target and target-to-source alignments, there can
be words that are not in the P (fj |ei) table. In this
case, the probability was taken to be 10?40.
In addition, we have calculated the IBM?1 Model
1.
P (e|f ; M1) = 1(J + 1)I
I
?
I=1
J
?
j=0
p(ei|fj) (7)
4.4 Language Model
The English language model plays an important
role in the source channel model, see equation (2),
and also in its modification, see equation (3). The
English language model should give an idea of the
sentence quality that is generated.
As default language model feature, we use a stan-
dard word-based trigram language model generated
with smoothing Kneser-Ney and interpolation (by
using SRILM [16]).
4.5 Word and Phrase Penalty
To compensate the preference of the target lan-
guage model for shorter sentences, we added two
151
Spanish English
Train Sentences 1223443 1223443
Words 34794006 33379333
Vocabulary 168685 104975
Dev Sentences 504 504
Words 15353 15335
OOV 25 16
Test Sentences 504 504
Words 10305 10667
OOV 36 19
Table 1: Statistics of training and test corpus
simple features which are widely used [17] [7]. The
word penalty provides means to ensure that the
translations do not get too long or too short. Neg-
ative values for the word penalty favor longer out-
put, positive values favor shorter output [7].
The phrase penalty is a constant cost per pro-
duced phrase. Here, a negative weight, which
means reducing the costs per phrase, results in a
preference for adding phrases. Alternatively, by us-
ing a positive scaling factors, the system will favor
less phrases.
5 Evaluation framework
5.1 Corpus Statistics
Experiments were performed to study the effect
of our modifications in the phrases. The training
material covers the transcriptions from April 1996
to September 2004. This material has been dis-
tributed by the European Parlament. In our ex-
periments, we have used the distribution of RWTH
of Aachen under the project of TC-STAR 1. The
test material was used in the first evaluation of the
project in March 2005. In our case, we have used
the development divided in two sets. This mate-
rial corresponds to the transcriptions of the sessions
from October the 21st to October the 28th. It has
been distributed by ELDA2. Results are reported
for Spanish-to-English translations.
1http://www.tcstar.org/
2http://www.elda.org/
5.2 Experiments
The decoder used for the presented translation sys-
tem is reported in [2]. This decoder is called
MARIE and it takes into account simultaneously
all the 7 features functions described above. It im-
plements a beam-search strategy.
As evaluation criteria we use: the Word Error
Rate (WER), the BLEU score [15] and the NIST
score [3].
As follows we report the results for several ex-
periments that show the performance of: the base-
line, adding the posterior probability, IBM Model
1 and IBM1?1, and, finally, the modification of the
phrases extraction.
Optimisation. Significant improvements can be
obtained by tuning the parameters of the features
adequately. In the complet system we have 7 pa-
rameters to tune: the relatives frecuencies P (f |e)
and P (e|f), IBM Model 1 and its inverse, the word
penalty, the phrase penalty and the weight of the
language model. We applied the widely used algo-
rithm SIMPLEX to optimise [9]. In Table 2 (line
5th), we see the final results.
Baseline. We report the results of the baseline.
We use the union alignment and we extract the
BP of length 3. As default language model fea-
ture, we use the standard trigram with smoothing
Kneser-Ney and interpolation. Also we tune the
parameters (only two parameters) with the SIM-
PLEX algorithm (see Table 2).
Posterior probability. Table 2 shows the effect
of using the posterior probability: P (e|f). We use
all the features but the P (e|f) and we optimise the
parameters. We see the results without this feature
decrease around 1.1 points both in BLEU and WER
(see line 2rd and 5th in Table 2).
IBM Model 1. We do the same as in the para-
graph above, we do not consider the IBM Model
1 and the IBM1?1. Under these conditions, the
translation?s quality decreases around 1.3 points
both in BLEU and WER (see line 3th and 5th in
Table 2).
152
Modification of the Phrase Extraction. Fi-
nally, we made an experiment without modification
of the phrases? length. We can see the comparison
between: (1) the phrases of fixed maximum length
of 3; and (2) including phrases with a maximum
length of 5 which can not be generated by smaller
phrases. We can see it in Table 2 (lines 4th and
5th). We observe that there is no much difference
between the number of phrases, so this approach
does not require more resources. However, we get
slightly better scores.
5.3 Shared Task
This section explains the participation of ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion?. We used the EuroParl data provided for this
shared task [4]. A word-to-word alignment was per-
formed in both directions as explained in section
2. The phrase-based translation system which has
been considered implements a total of 7 features
(already explained in section 4). Notice that the
language model has been trained with the training
provided in the shared task. However, the opti-
mization in the parameters has not been repeated,
and we used the parameters obtained in the sub-
section above. We have obtained the results in the
Table 3.
6 Conclusions
We reported a new method to extract longer
phrases without increasing the quantity of phrases
(less than 0.5%).
We also reported several features as P (e|f)
which in combination with the functions of the
source-channel model provides significant improve-
ment. Also, the feature IBM1 in combination with
IBM1?1 provides improved scores, too.
Finally, we have optimized the parameters, and
we provided the final results which have been pre-
sented in the Shared Task: Exploiting Parallel
Texts for Statistical Machine Translation (June 30,
2005) in conjunction with ACL 2005 in Ann Arbor,
Michigan.
7 Acknowledgements
The authors want to thank Jose? B. Marin?o, Adria`
de Gispert, Josep M. Crego, Patrik Lambert and
Rafael E. Banchs (members of the TALP Research
Center) for their contribution to this work.
References
[1] P.F. Brown, J. Cocke, S.A. Della Pietra,
V.J. Della Pietra, F. Jelinek, J.D. Lafferty,
R.L. Mercer, and P.S. Rossin. A statistical ap-
proach to machine translation. Computational
Linguistics, 16(2):79?85, June 1990.
[2] Josep M. Crego, Jose? B. Marin?o, and Adria`
de Gispert. An Ngram-based Statistical Ma-
chine Translation Decoder. In Draft, 2005.
[3] G. Doddington. Automatic evaluation ma-
chine translation quality using n-gram co-
ocurrence statistics. In Proc. ARPA Workshop
on Human Language Technology, 2002.
[4] EuroParl: European Parliament Proceed-
ings Parallel Corpus. Available on-line at:
http://people.csail.mit.edu/koehn/publica-
tions/europarl/. 1996-2003.
[5] I. Garc??a-Varea. Traduccio?n Automa?tica es-
tad??stica: Modelos de Traduccio?n basados en
Ma?xima Entrop??a y Algoritmos de Bu?squeda .
UPV, Diciembre 2003.
[6] Giza++. http://www-i6.informatik.rwth-
aachen.de/?och/software/giza++.html/,
1999.
[7] P. Koehn. A Beam Search Decoder for Phrase-
Based Statistical Machine Translation Models.
2003.
[8] P. Koehn, F. J. Och, and D. Marcu. Statisti-
cal phrase-based translation. In Proceedings of
the Human Language Technology Conference
(HLT-NAACL), pages 127?133, May 2003.
[9] J.A. Nelder and R. Mead. A simplex method
for function minimization. The Computer
Journal, 7:308?313, 1965.
153
Phr Length ?LM ?p(f |e) ?p(e|f) ?IBM1 ?IBM1?1 ?PP ?WP WER BLEU NIST # frases
3 0.788 0.906 0 0 0 0 0 33.98 57.44 10.11 67.7M
3+5length 0.788 0.941 0 0.771 0.200 3.227 0.448 28.97 64.71 11.07 68M
3+5length 0.788 0.824 0.820 0 0 3.430 -0.083 29.17 64.59 10.99 68M
3 0.746 0.515 0.979 0.514 0.390 1.537 -1.264 27.94 65.70 11.18 67.7M
3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 27.88 65.82 11.23 68M
Table 2: Results for the different experiments with optimized parameters in the direction SPA->ENG
Phr Length ?LM ?p(f |e) ?p(e|f) ?IBM1 ?IBM1?1 ?PP ?WP BLEU # frases
3+5length 0.788 0.617 0.810 0.635 0.101 1.995 -0.296 29.84 34.8M
Table 3: Results for the ACL training and ACL test (SPA->ENG)
[10] F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar,
K. Yamada, A. Fraser, S. Kumar, L. Shen,
D. Smith, K. Eng, V. Jain, Z. Jin, and
D. Radev. A Smorgasbord of Features for Sta-
tistical Machine Translation. In Proceedings of
the Human Language Technology Conference
(HLT-NAACL), May 2004.
[11] F. J. Och and H. Ney. The Alignment Tem-
plate Approach to Statistical Machine Trans-
lation. Computational linguistics, 30:417?449,
December 2004.
[12] Franz Josef Och and Hermann Ney. Discrimi-
native Training and Maximum Entropy Mod-
els for Statistical Machine Translation. In
ACL, pages pages 295?302, July 2002.
[13] Papineni, S.Roukos, and R.T. Ward. Feature-
based language understanding. In European
Conf. on Speech Communication and Technol-
ogy, pages 1435?1438, September 1997.
[14] Papineni, S.Roukos, and R.T. Ward. Maxi-
mum likelihood and discriminative training of
direct translation models. In Proc. Int. Conf.
on Acoustics, Speech, and Signal Proceedings,
pages 189?192, May 1998.
[15] K.A. Papineni, S. Roukos, T. Ward, and W.J.
Zhu. Bleu: a method for automatic evaluation
of machine translation. In Technical Report
RC22176 (W0109-022), IBM Research Divi-
sion, 2001.
[16] A. Stolcke. SRILM - An Extensible Language
Modeling Toolkit. In Proceedings Intl. Confer-
ence Spoken Language Processing, September
2002.
[17] R. Zens and H. Ney. Improvements in Phrase-
Based Statistical Machine Translation. In
Proceedings of the Human Language Technol-
ogy Conference (HLT-NAACL), pages 257?
264, May 2004.
154
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 70?76,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Statistical Machine Reordering
Marta R. Costa-jussa` and Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,adrian)@gps.tsc.upc.edu
Abstract
Reordering is currently one of the most
important problems in statistical machine
translation systems. This paper presents
a novel strategy for dealing with it: sta-
tistical machine reordering (SMR). It con-
sists in using the powerful techniques de-
veloped for statistical machine translation
(SMT) to translate the source language
(S) into a reordered source language (S?),
which allows for an improved translation
into the target language (T). The SMT task
changes from S2T to S?2T which leads to a
monotonized word alignment and shorter
translation units. In addition, the use of
classes in SMR helps to infer new word
reorderings. Experiments are reported in
the EsEn WMT06 tasks and the ZhEn
IWSLT05 task and show significant im-
provement in translation quality.
1 Introduction
During the last few years, SMT systems
have evolved from the original word-based ap-
proach (Brown et al, 1993) to phrase-based trans-
lation systems (Koehn et al, 2003). In parallel
to the phrase-based approach, the use of bilin-
gual n-grams gives comparable results, as shown
by Crego et al (2005a). Two basic issues differ-
entiate the n-gram-based system from the phrase-
based: training data are monotonously segmented
into bilingual units; and, the model considers n-
gram probabilities rather than relative frequencies.
This translation approach is described in detail by
Marin?o et al (2005). The n-gram-based system
follows a maximum entropy approach, in which a
log-linear combination of multiple models is im-
plemented (Och and Ney, 2002), as an alternative
to the source-channel approach.
In both systems, introducing reordering capabil-
ities is of crucial importance for certain language
pairs. Recently, new reordering strategies have
been proposed in the literature on SMT such as the
reordering of each source sentence to match the
word order in the corresponding target sentence,
see Kanthak et al (2005) and Crego et al (2005b).
Similarly, Matusov et al (2006) describe a method
for simultaneously aligning and monotonizing the
training corpus. The main problems of these ap-
proaches are: (1) the fact that the proposed mono-
tonization is based on the alignment and cannot be
applied to the test sets, and (2) the lack of reorder-
ing generalization.
This paper presents a reordering approach
called statistical machine reordering (SMR) which
improves the reordering capabilities of SMT sys-
tems without incurring any of the problems men-
tioned above. SMR is a first-pass translation
performed on the source corpus, which converts
it into an intermediate representation, in which
source-language words are presented in an order
that more closely matches that of the target lan-
guage. SMR and SMT are performed using the
same modeling tools as n-gram-based systems but
using different statistical log-linear models.
In order to be able to infer new reorderings we
use word classes instead of words themselves as
the input to the SMR system. In fact, the use of
classes to help in the reordering is a key difference
between our approach and standard SMT systems.
This paper is organized as follows: Section 2
outlines the baseline system. Section 3 describes
the reordering strategy in detail. Section 4 presents
and discusses the results, and Section 5 presents
our conclusions and suggestions for further work.
70
2 N-gram-based SMT System
This section briefly describes the n-gram-based
SMT which uses a translation model based on
bilingual n-grams. It is actually a language model
of bilingual units, referred to as tuples, which ap-
proximates the joint probability between source
and target languages by using bilingual n-grams
(de Gispert and Marin?o, 2002).
Bilingual units (tuples) are extracted from any
word alignment according to the following con-
straints:
1. a monotonous segmentation of each bilingual
sentence pairs is produced,
2. no word inside the tuple is aligned to words
outside the tuple, and
3. no smaller tuples can be extracted without vi-
olating the previous constraints.
As a result of these constraints, only one seg-
mentation is possible for a given sentence pair.
Figure 1 presents a simple example which illus-
trates the tuple extraction process.
I would like NULL to eat a huge ice-cream
NULL quisiera ir a comer un helado gigante
t1 t2 t3 t4 t5 t6
Figure 1: Example of tuple extraction from an
aligned bilingual sentence pair.
Two important issues regarding this translation
model must be considered. First, it often occurs
that large number of single-word translation prob-
abilities are left out of the model. This happens
for all words that are always embedded in tuples
containing two or more words. Consider for ex-
ample the word ?ice-cream? in Figure 1. As seen
from the Figure, ?ice-cream? is embedded into tu-
ple t6. If a similar situation is encountered for all
occurrences of ?ice-cream? in the training corpus,
then no translation probability for an independent
occurrence of this word will exist.
To overcome this problem, the tuple 4-gram
model is enhanced by incorporating 1-gram trans-
lation probabilities for all the embedded words de-
tected during the tuple extraction step. These 1-
gram translation probabilities are computed from
the intersection of both, the source-to-target and
the target-to-source alignments.
The second issue has to do with the fact that
some words linked to NULL end up producing tu-
ples with NULL source sides. Consider for exam-
ple the tuple t3 in Figure 1. Since no NULL is ac-
tually expected to occur in translation inputs, this
type of tuple is not allowed. Any target word that
is linked to NULL is attached either to the word
that precedes or the word that follows it. To de-
termine this, we use the IBM1 probabilities, see
Crego et al (2005a).
In addition to the bilingual n-gram transla-
tion model, the baseline system implements a
log-linear combination of four feature functions,
which are described as follows:
? A target language model. This feature con-
sists of a 4-gram model of words, which is
trained from the target side of the bilingual
corpus.
? A word bonus function. This feature intro-
duces a bonus based on the number of target
words contained in the partial-translation hy-
pothesis. It is used to compensate for the sys-
tem?s preference for short output sentences.
? A source-to-target lexicon model. This fea-
ture, which is based on the lexical param-
eters of the IBM Model 1 (Brown et al,
1993), provides a complementary probabil-
ity for each tuple in the translation table.
These lexicon parameters are obtained from
the source-to-target algnments.
? A target-to-source lexicon model. Similarly
to the previous feature, this feature is based
on the lexical parameters of the IBM Model
1 but, in this case, these parameters are ob-
tained from target-to-source alignments.
All these models are combined in the de-
coder. Additionally, the decoder allows for a non-
monotonous search with the following distorsion
model.
71
? A word distance-based distorsion model.
P (tK1 ) = exp(?
K
?
k=1
dk)
where dk is the distance between the first
word of the kth tuple (unit), and the last
word+1 of the (k ? 1)th tuple. Distance
are measured in words referring to the units
source side.
To reduce the computational cost we place lim-
its on the search using two parameters: the dis-
tortion limit (the maximum distance measured in
words that a tuple is allowed to be reordered, m)
and the reordering limit (the maximum number of
reordering jumps in a sentence, j). This feature is
independent of the reordering approach presented
in this paper, so they can be used simultaneously.
In order to combine the models in the decoder
suitably, an optimization tool is needed to compute
log-linear weights for each model.
3 Statistical Machine Reordering
As mentioned in the introduction, SMR and SMT
are based on the same principles. Here, we give
a detailed description of the SMR reordering ap-
proach proposed.
3.1 Concept
The aim of SMR consists in using an SMT sys-
tem to deal with reordering problems. Therefore,
the SMR system can be seen as an SMT system
which translates from an original source language
(S) to a reordered source language (S?), given a
target language (T). Then, the translation tasks
changes from S2T to S?2T. The main difference
between the two tasks is that the latter allows for:
(1) monotonized word alignment, and (2) higher
quality monotonized translation.
3.2 Description
Figure 2 shows the SMR block diagram. The in-
put is the initial source sentence (S) and the output
is the reordered source sentence (S?). There three
blocks inside SMR: (1) class replacing ; (2) the de-
coder, which requires the translation model; and,
(3) the block which reorders the original sentence
using the indexes given by the decoder. The fol-
lowing example specifies the input and output of
each block inside the SMR.
Figure 2: SMR block diagram.
1. Source sentence (S):
El compromiso s?olo podr?a mejorar
2. Source sentence classes (S-c):
C38 C43 C49 C42 C22
3. Decoder output (translation, T ):
C38#0 | C43 C49 C42#1 2 0 | C22#0
where | indicates the segmentation into trans-
lation units and # divides the source and tar-
get. The source part is composed of word
classes and the target part is composed of
the new positions of the source word classes,
starting at 0.
4. SMR output (S?). The reordering information
inside each translation unit of the decoder
output (T ) is applied to the original source
sentence (S):
El s?olo podr?a compromiso mejorar
3.3 Training
For the reordering translation, we used an n-gram-
based SMT system (and considered only the trans-
lation model). Figure 3 shows the block diagram
of the training process of the SMR translation
model, which is a bilingual n-gram-based model.
The training process uses the training source and
target corpora and consists of the following steps:
1. Determine source and target word classes.
2. Align parallel training sentences at the word
level in both translation directions. Compute
the union of the two alignments to obtain a
symmetrized many-to-many word alignment.
3. Extract reordering tuples, see Figure 4.
(a) From union word alignment, extract
bilingual S2T tuples (i.e. source and
target fragments) while maintaining the
72
Figure 3: Block diagram of the training process of the SMR translation model.
Figure 4: Example of the extraction of reordering
tuples (step 3).
alignment inside the tuple. As an ex-
ample of a bilingual S2T tuple consider:
only possible compromise # compromiso
s?olo podr?a # 0-1 1-1 1-2 2-0, as shown
in Figure 4, where the different fields are
separated by # and correspond to: (1)
the target fragment; (2) the source frag-
ment; and (3) the word alignment (in
this case, the fields that respectively cor-
respond to a target and source word are
separated by ?).
(b) Modify the many-to-many word align-
ment from each tuple to many-to-one.
If one source word is aligned to two or
more target words, the most probable
link given IBM Model 1 is chosen, while
the other are omitted (i.e. the num-
ber of source words is the same before
and after the reordering translation). In
the above example, the tuple would be
changed to: only possible compromise
# compromiso s?olo podr?a # 0-1 1-2 2-
0, as Pibm1(only, so?lo) is higher than
Pibm1(possible, so?lo).
(c) From bilingual S2T tuples (with many-
to-one inside alignment), extract bilin-
gual S2S? tuples (i.e. the source frag-
ment and its reordering). As in the ex-
ample: compromiso s?olo podr?a # 1 2 0,
where the first field is the source frag-
ment, and the second is the reordering
of these source words.
(d) Eliminate tuples whose source fragment
consists of the NULL word.
(e) Replace the words of each tuple source
fragment with the classes determined in
Step 1.
4. Compute the bilingual language model of the
bilingual S2S? tuple sequence composed of
the source fragment (in classes) and its re-
order.
Once the translation model is built, the origi-
nal source corpus S is translated into the reordered
source corpus S? with the SMR system, see Fig-
ure 2. The reordered training source corpus and
the original training target corpus are used to train
the SMT system (as explained in Section 2). Fi-
nally, with this system, the reordered test source
corpus is translated.
4 Evaluation Framework
In this section, we present experiments carried out
using the EsEn WMT06 and the ZhEn IWSLT05
parallel corpus. We detail the tools which have
been used and the corpus statistics.
73
EuroParl Spanish English
Training Sentences 727.1 k 727.1 k
Words 15.7 M 15.2 M
Vocabulary 108.7 k 72.3 k
Development Sentences 500 500
Words 15.2 k 14.8 k
Vocabulary 3.6 k 3 k
Test Sentences 3064 3064
Words 91.9 k 85.2 k
Vocabulary 11.1 k 9.1 k
Table 1: Spanish to English task. EuroParl cor-
pus: training, development and test data sets.
4.1 Tools
? The word alignments were computed using
the GIZA++ tool (Och, 2003).
? The word classes were determined us-
ing ?mkcls?, a freely-available tool with
GIZA++.
? The language model was estimated using the
SRILM toolkit (Stolcke, 2002).
? We used MARIE as a decoder (Crego et al,
2005b).
? The optimization tool used for computing
log-linear weights (see Section 2) is based
on the simplex method (Nelder and Mead,
1965).
4.2 Corpus Statistics
Experiments were carried out on the Spanish and
English task of the WMT06 evaluation1 (EuroParl
Corpus) and on the Chinese to English task of the
IWSLT05 evaluation2 (BTEC Corpus). The for-
mer is a large corpus, whereas the latter is a small
corpus translation task. Table 1 and 2 show the
main statistics of the data used, namely the number
of sentences, words, vocabulary, and mean sen-
tence lengths for each language.
4.3 Units
In this section different statistics units of both ap-
proaches (S2T and S?2T) are shown (using the
ZhEn task). All the experiments in this section
were carried out using 100 classes in the SMR
step.
1www.statmt.org/wmt06/shared-task/
2www.slt.atr.jp/IWSLT2005
BTEC Chinese English
Training Sentences 20 k 20 k
Words 176.2 k 182.3 k
Vocabulary 8.7 k 7.3 k
Development Sentences 506 506
Words 3.5 k 3.3 k
Vocabulary 870 799
Test Sentences 506 506
Words 4 k 3 k
Vocabulary 916 818
Table 2: Chinese to English task. BTEC corpus:
training, development and test data sets. Develop-
ment and test data sets have 16 references.
Table 3 shows the vocabulary of bilingual n-
grams and embedded words in the translation
model. Once the reordering translation has been
computed, alignment becomes more monotonic. It
is commonly known that non-monotonicity poses
difficulties for word alignments. Therefore, when
the alignment becomes more monotonic, we ex-
pect an improvement in the alignment, and, there-
fore in the translation. Here, we can observe a
significant enlargement of the number of transla-
tion units, which leads to a growth of the transla-
tion vocabulary. We also observe a decrease in the
number of embedded words (around 20%). From
Section 2, we know that the probability of embed-
ded words is estimated independently of the trans-
lation model. Reducing embedded words allows
for a better estimation of the translation model.
Figure 5 shows the histogram of the tuple size in
the two approaches. We observe that the number
of tuples is similar over length 5. However, there
are a greater number of shorter units in the case of
SMR+NB (shorter units lead to a reduction in data
sparseness).
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 0  2  4  6  8  10  12  14
NB
SMR + NB
Figure 5: Comparison of the histogram of the tuple
size in the two approaches (NB and SMR+NB).
74
System 1gr 2gr 3gr 4gr Embedded
NB 34487 57597 3536 1918 5735
SMR + NB 35638 70947 5894 3412 4632
Table 3: Vocabulary of n-grams and embedded words in the translation model.
System Total Vocabulary
NB 4460 959
SMR + NB 4628 1052
Table 4: Tuples used to translate the test set (total
number and vocabulary).
Table 4 shows the tuples used to translate the
test set (total number and vocabulary). Note that
the number of tuples and vocabulary used to trans-
late the test set is significantly greater after the re-
ordering translation.
4.4 Results
Here, we introduce the experiments that were car-
ried out in order to evaluate the influence of the
SMR approach in both tasks EsEn and ZhEn. The
log-linear translation model was optimized with
the simplex algorithm by maximizing over the
BLEU score. The evaluation was carried out us-
ing references and translation in lowercase and, in
the ZhEn task, without punctuation marks.
We studied the influence of the proposed SMR
approach on the n-gram-based SMT system de-
scribed using a monotonous search (NBm or
monotonous baseline configuration) in the two
tasks and a non-monotonous search (NBnm or
non-monotonous baseline configuration) in the
ZhEn task. In allowing for reordering in the SMT
decoder, the distortion limit (m) and reordering
limit (j) (see Section 2) were empirically set to
5 and 3, as they showed a good trade-off between
quality and efficiency. Both systems include the
four features explained in Section 2: the language
model, the word bonus, and the source-to-target
and target-to-source lexicon models.
Tables 5 and 6 show the results in the test set.
The former corresponds to the influence of the
SMR system on the EsEn task (NBm), whereas
the latter corresponds to the influence of the SMR
system on the ZhEn task (NBm and NBnm).
4.5 Discussion
Both BLEU and NIST coherently increase after
the inclusion of the SMR step when 100 classes
are used. The improvement in translation quality
can be explained as follows:
? SMR takes advantage of the use of classes
and correctly captures word reorderings that
are missed in the standard SMT system. In
addition, the use of classes allows new re-
orderings to be inferred.
? The new task S?2T becomes more
monotonous. Therefore, the translation
units tend to be shorter and SMT systems
perform better.
The gain obtained in the SMR+NBnm case indi-
cates that the reordering provided by SMR system
and the non-monotonous search are complemen-
tary. It means that the output of the SMR could
still be further monotonized. Note that the ZhEn
task has complex word reorderings.
These preliminary results also show that SMR
itself provides further improvements to those pro-
vided by the non-monotonous search.
5 Conclusions and Further Research
In this paper we have mainly dealt with the re-
ordering problem for an n-gram-based SMT sys-
tem. However, our approach could be used sim-
ilarly for a phrase-based system. We have ad-
dressed the reordering problem as a translation
from the source sentence to a monotonized source
sentence. The proposed SMR system is applied
before a standard SMT system. The SMR and
SMT systems are based on the same principles and
share the same type of decoder.
In extracting bilingual units, the change of order
performed in the source sentence has allowed the
modeling of the translation units to be improved
(shorter units mean a reduction in data sparse-
ness). Also, note that the SMR approach allows
the coherence between the change of order in the
training and test source corpora to be maintained.
75
System Classes BLEU NIST WER PER
NBm - 27.69 7.31 61.6 45.34
SMR + NBm - 28.60 7.53 59.89 43.53
SMR + NBm 100 30.89 7.75 55.77 42.85
Table 5: Results in the test set of the EsEn task using a monotonous search.
System Classes BLEU NIST WER PER
NBm - 42.42 8.3 42.87 33.44
NBnm - 43.58 8.9 43.89 34.05
SMR + NBm 100 43.75 8.49 42.45 33.85
SMR + NBnm 100 45.97 9.0 40.92 32.32
Table 6: Results in the test set of the ZhEn task using a monotonous and a non-monotonous search.
Performing reordering as a preprocessing step
and independently from the SMT system allows
for a more efficient final system implementation
and a quicker translation. Additionally, using
word classes helps to infer unseen reorderings.
These preliminary results show consistent and sig-
nificant improvements in translation quality.
As further research, we would like to add extra
features to the SMR system, and study new types
of classes for the reordering task.
6 Acknowledgments
This work has been partially funded by the Eu-
ropean Union under the integrated project TC-
STAR - Technology and Corpora for Speech
to Speech Translation - (IST-2002-FP6-506738,
http://www.tc-star.org) and the Spanish govern-
ment under a FPU grant.
References
E. Matusov A. Mauser and H. Ney. 2006. Train-
ing a statistical machine translation system without
giza++. 5th Int. Conf. on Language Resources and
Evaluation, LREC?06, May.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
311.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005a. Ngram-based versus phrase-
based statistical machine translation. Proc. of the
Int. Workshop on Spoken Language Translation,
IWSLT?05, October.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005b.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
A. de Gispert and J. Marin?o. 2002. Using X-grams for
speech-to-speech translation. Proc. of the 7th Int.
Conf. on Spoken Language Processing, ICSLP?02,
September.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M. Ruiz. 2005.
Bilingual n-gram statistical machine translation. In
Proc. of the MT Summit X, pages 275?82, Pukhet
(Thailand), May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical ma-
chine translation. 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 295?
302, July.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
76
Proceedings of the Workshop on Statistical Machine Translation, pages 142?145,
New York City, June 2006. c?2006 Association for Computational Linguistics
TALP Phrase-based statistical translation system for European language
pairs
Marta R. Costa-jussa`
Patrik Lambert
Jose? B. Marin?o
Josep M. Crego
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,agispert,lambert,khalilov,canton,adrian, rbanchs)@gps.tsc.upc.edu
Adria` de Gispert
Rafael E. Banchs
Abstract
This paper reports translation results for
the ?Exploiting Parallel Texts for Statis-
tical Machine Translation? (HLT-NAACL
Workshop on Parallel Texts 2006). We
have studied different techniques to im-
prove the standard Phrase-Based transla-
tion system. Mainly we introduce two re-
ordering approaches and add morphologi-
cal information.
1 Introduction
Nowadays most Statistical Machine Translation
(SMT) systems use phrases as translation units. In
addition, the decision rule is commonly modelled
through a log-linear maximum entropy framework
which is based on several feature functions (in-
cluding the translation model), hm. Each feature
function models the probability that a sentence e in
the target language is a translation of a given sen-
tence f in the source language. The weights, ?i,
of each feature function are typically optimized to
maximize a scoring function. It has the advantage
that additional features functions can be easily in-
tegrated in the overall system.
This paper describes a Phrase-Based system
whose baseline is similar to the system in Costa-
jussa` and Fonollosa (2005). Here we introduce
two reordering approaches and add morphological
information. Translation results for all six trans-
lation directions proposed in the shared task are
presented and discussed. More specifically, four
different languages are considered: English (en),
Spanish (es), French (fr) and German (de); and
both translation directions are considered for the
pairs: EnEs, EnFr, and EnDe. The paper is orga-
nized as follows: Section 2 describes the system;
0This work has been supported by the European Union
under grant FP6-506738 (TC-STAR project) and the TALP
Research Center (under a TALP-UPC-Recerca grant).
Section 3 presents the shared task results; and, fi-
nally, in Section 4, we conclude.
2 System Description
This section describes the system procedure fol-
lowed for the data provided.
2.1 Alignment
Given a bilingual corpus, we use GIZA++ (Och,
2003) as word alignment core algorithm. During
word alignment, we use 50 classes per language
estimated by ?mkcls?, a freely-available tool along
with GIZA++. Before aligning we work with low-
ercase text (which leads to an Alignment Error
Rate reduction) and we recover truecase after the
alignment is done.
In addition, the alignment (in specific pairs of
languages) was improved using two strategies:
Full verb forms The morphology of the verbs
usually differs in each language. Therefore, it is
interesting to classify the verbs in order to address
the rich variety of verbal forms. Each verb is re-
duced into its base form and reduced POS tag as
explained in (de Gispert, 2005). This transforma-
tion is only done for the alignment, and its goal
is to simplify the work of the word alignment im-
proving its quality.
Block reordering (br) The difference in word
order between two languages is one of the most
significant sources of error in SMT. Related works
either deal with reordering in general as (Kanthak
et al, 2005) or deal with local reordering as (Till-
mann and Ney, 2003). We report a local reorder-
ing technique, which is implemented as a pre-
processing stage, with two applications: (1) to im-
prove only alignment quality, and (2) to improve
alignment quality and to infer reordering in trans-
lation. Here, we present a short explanation of the
algorithm, for further details see Costa-jussa` and
Fonollosa (2006).
142
Figure 1: Example of an Alignment Block, i.e. a
pair of consecutive blocks whose target translation
is swapped
This reordering strategy is intended to infer the
most probable reordering for sequences of words,
which are referred to as blocks, in order to mono-
tonize current data alignments and generalize re-
ordering for unseen pairs of blocks.
Given a word alignment, we identify those pairs
of consecutive source blocks whose translation is
swapped, i.e. those blocks which, if swapped,
generate a correct monotone translation. Figure 1
shows an example of these pairs (hereinafter called
Alignment Blocks).
Then, the list of Alignment Blocks (LAB) is
processed in order to decide whether two consec-
utive blocks have to be reordered or not. By using
the classification algorithm, see the Appendix, we
divide the LAB in groups (Gn, n = 1 . . . N ). In-
side the same group, we allow new internal com-
bination in order to generalize the reordering to
unseen pairs of blocks (i.e. new Alignment Blocks
are created). Based on this information, the source
side of the bilingual corpora are reordered.
In case of applying the reordering technique for
purpose (1), we modify only the source training
corpora to realign and then we recover the origi-
nal order of the training corpora. In case of using
Block Reordering for purpose (2), we modify all
the source corpora (both training and test), and we
use the new training corpora to realign and build
the final translation system.
2.2 Phrase Extraction
Given a sentence pair and a corresponding word
alignment, phrases are extracted following the cri-
terion in Och and Ney (2004). A phrase (or
bilingual phrase) is any pair of m source words
and n target words that satisfies two basic con-
straints: words are consecutive along both sides
of the bilingual phrase, and no word on either side
of the phrase is aligned to a word out of the phrase.
We limit the maximum size of any given phrase to
7. The huge increase in computational and storage
cost of including longer phrases does not provide
a significant improvement in quality (Koehn et al,
2003) as the probability of reappearance of larger
phrases decreases.
2.3 Feature functions
Conditional and posterior probability (cp, pp)
Given the collected phrase pairs, we estimate the
phrase translation probability distribution by rela-
tive frequency in both directions.
The target language model (lm) consists of an
n-gram model, in which the probability of a trans-
lation hypothesis is approximated by the product
of word n-gram probabilities. As default language
model feature, we use a standard word-based 5-
gram language model generated with Kneser-Ney
smoothing and interpolation of higher and lower
order n-grams (Stolcke, 2002).
The POS target language model (tpos) con-
sists of an N-gram language model estimated over
the same target-side of the training corpus but us-
ing POS tags instead of raw words.
The forward and backwards lexicon mod-
els (ibm1, ibm1?1) provide lexicon translation
probabilities for each phrase based on the word
IBM model 1 probabilities. For computing the
forward lexicon model, IBM model 1 probabili-
ties from GIZA++ source-to-target algnments are
used. In the case of the backwards lexicon model,
target-to-source alignments are used instead.
The word bonus model (wb) introduces a sen-
tence length bonus in order to compensate the sys-
tem preference for short output sentences.
The phrase bonus model (pb) introduces a con-
stant bonus per produced phrase.
2.4 Decoding
The search engine for this translation system is de-
scribed in Crego et al (2005) which takes into ac-
count the features described above.
Using reordering in the decoder (rgraph) A
highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
143
extend the monotone search graph with additional
arcs. See the details in Crego et al (2006).
2.5 Optimization
It is based on a simplex method (Nelder and
Mead, 1965). This algorithm adjusts the log-
linear weights in order to maximize a non-linear
combination of translation BLEU and NIST: 10 ?
log10((BLEU ? 100) + 1) + NIST. The max-
imization is done over the provided development
set for each of the six translation directions under
consideration. We have experimented an improve-
ment in the coherence between all the automatic
figures by integrating two of these figures in the
optimization function.
3 Shared Task Results
3.1 Data
The data provided for this shared task corresponds
to a subset of the official transcriptions of the
European Parliament Plenary Sessions, and it
is available through the shared task website at:
http://www.statmt.org/wmt06/shared-task/.
The development set used to tune the system
consists of a subset (500 first sentences) of the
official development set made available for the
Shared Task.
We carried out a morphological analysis of the
data. The English POS-tagging has been carried
out using freely available TNT tagger (Brants,
2000). In the Spanish case, we have used the
Freeling (Carreras et al, 2004) analysis tool
which generates the POS-tagging for each input
word.
3.2 Systems configurations
The baseline system is the same for all tasks and
includes the following features functions: cp, pp,
lm, ibm1, ibm1?1, wb, pb. The POStag target
language model has been used in those tasks for
which the tagger was available. Table 1 shows the
reordering configuration used for each task.
The Block Reordering (application 2) has been
used when the source language belongs to the Ro-
manic family. The length of the block is lim-
ited to 1 (i.e. it allows the swapping of single
words). The main reason is that specific errors are
solved in the tasks from a Romanic language to
a Germanic language (as the common reorder of
Noun + Adjective that turns into Adjective +
Noun). Although the Block Reordering approach
Task Reordering Configuration
Es2En br2
En2Es br1 + rgraph
Fr2En br2
En2Fr br1 + rgraph
De2En -
En2De -
Table 1: Additional reordering models for each
task: br1 (br2) stands for Block Reordering ap-
plication 1 (application 2); and rgraph refers to
the reordering integrated in the decoder
does not depend on the task, we have not done
the corresponding experiments to observe its ef-
ficiency in all the pairs used in this evaluation.
The rgraph has been applied in those cases
where: we do not use br2 (there is no sense in
applying them simultaneously); and we have the
tagger for the source language model available.
In the case of the pair GeEn, we have not exper-
imented any reordering, we left the application of
both reordering approaches as future work.
3.3 Discussion
Table 2 presents the BLEU scores evaluated on the
test set (using TRUECASE) for each configuration.
The official results were slightly better because a
lowercase evaluation was used, see (Koehn and
Monz, 2006).
For both, Es2En and Fr2En tasks, br helps
slightly. The improvement of the approach de-
pends on the quality of the alignment. The better
alignments allow to extract higher quality Align-
ment Blocks (Costa-jussa` and Fonollosa, 2006).
The En2Es task is improved when adding both
br1 and rgraph. Similarly, the En2Fr task seems to
perform fairly well when using the rgraph. In this
case, the improvement of the approach depends on
the quality of the alignment patterns (Crego et al,
2006). However, it has the advantage of delay-
ing the final decision of reordering to the overall
search, where all models are used to take a fully
informed decision.
Finally, the tpos does not help much when trans-
lating to English. It is not surprising because it was
used in order to improve the gender and number
agreement, and in English there is no need. How-
ever, in the direction to Spanish, the tpos added
to the corresponding reordering helps more as the
Spanish language has gender and number agree-
ment.
144
Task Baseline +tpos +rc +tpos+rc
Es2En 29.08 29.08 29.89 29.98
En2Es 27.73 27.66 28.79 28.99
Fr2En 27.05 27.06 27.43 27.23
En2Fr 26.16 - 27.80 -
De2En 21.59 21.33 - -
En2De 15.20 - - -
Table 2: Results evaluated using TRUECASE on
the test set for each conguration: rc stands for
Reordering Conguration and refers to Table 1.
The bold results were the congurations submit-
ted.
4 Conclusions
Reordering is important when using a Phrase-
Based system. Although local reordering is sup-
posed to be included in the phrase structure, per-
forming local reordering improves the translation
quality. In fact, local reordering, provided by the
reordering approaches, allows for those general-
izations which phrases could not achieve. Re-
ordering in the DeEn task is left as further work.
References
T. Brants. 2000. Tnt - a statistical part-of-speech tag-
ger. Proceedings of the Sixth Applied Natural Lan-
guage Processing.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyz-
ers. 4th Int. Conf. on Language Resources and Eval-
uation, LREC?04.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2005. Im-
proving the phrase-based statistical translation by
modifying phrase extraction and including new fea-
tures. Proceedings of the ACL Workshop on Build-
ing and Using Parallel Texts: Data-Driven Machine
Translation and Beyond.
M. R. Costa-jussa` and J.A.R. Fonollosa. 2006. Using
reordering in statistical machine translation based on
alignment block classification. Internal Report.
J.M. Crego, J. Marin?o, and A. de Gispert. 2005.
An Ngram-based statistical machine translation de-
coder. Proc. of the 9th Int. Conf. on Spoken Lan-
guage Processing, ICSLP?05.
J. M. Crego, A. de Gispert, P. Lambert, M. R.
Costa-jussa`, M. Khalilov, J. Marin?o, J. A. Fonol-
losa, and R. Banchs. 2006. Ngram-based smt
system enhanced with reordering patterns. HLT-
NAACL06 Workshop on Building and Using Paral-
lel Texts: Data-Driven Machine Translation and Be-
yond, June.
A. de Gispert. 2005. Phrase linguistic classification for
improving statistical machine translation. ACL 2005
Students Workshop, June.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H.
Ney. 2005. Novel reordering approaches in phrase-
based statistical machine translation. Proceedings
of the ACL Workshop on Building and Using Par-
allel Texts: Data-Driven Machine Translation and
Beyond, pages 167?174, June.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between european
languages. June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. Proc. of the Human Lan-
guage Technology Conference, HLT-NAACL?2003,
May.
J.A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7:308?313.
F.J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 30(4):417?449, December.
F.J. Och. 2003. Giza++ software. http://www-
i6.informatik.rwth-aachen.de/?och/ soft-
ware/giza++.html.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. Proc. of the 7th Int. Conf. on Spoken
Language Processing, ICSLP?02, September.
C. Tillmann and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133, March.
A Appendix
Here we describe the classification algorithm used
in Section 1.
1. Initialization: set n? 1 and LAB ? ? LAB.
2. Main part: while LAB ? is not empty do
? Gn = {(?k, ?k)} where (?k, ?k) is any
element of LAB ?, i.e. ?k is the first
block and ?k is the second block of the
Alignment Block k of the LAB ?.
? Recursively, move elements (?i, ?i)
from LAB? to Gn if there is an element
(?j , ?j) ? Gn such that ?i = ?j or
?i = ?j
? Increase n (i.e. n? n + 1)
3. Ending: For each Gn, construct the two sets
An and Bn which consists on the first and
second element of the pairs in Gn, respec-
tively.
145
Proceedings of the Workshop on Statistical Machine Translation, pages 162?165,
New York City, June 2006. c?2006 Association for Computational Linguistics
N-gram-based SMT System Enhanced with Reordering Patterns
Josep M. Crego
Marta R. Costa-jussa`
Jose? B. Marin?o
Adria` de Gispert
Maxim Khalilov
Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
{jmcrego,agispert,lambert,mruiz,khalilov,rbanchs,canton,adrian}@gps.tsc.upc.edu
Patrik Lambert
Rafael E. Banchs
Abstract
This work presents translation results for
the three data sets made available in the
shared task ?Exploiting Parallel Texts for
Statistical Machine Translation? of the
HLT-NAACL 2006 Workshop on Statisti-
cal Machine Translation. All results pre-
sented were generated by using the N-
gram-based statistical machine translation
system which has been enhanced from the
last year?s evaluation with a tagged target
language model (using Part-Of-Speech
tags). For both Spanish-English transla-
tion directions and the English-to-French
translation task, the baseline system al-
lows for linguistically motivated source-
side reorderings.
1 Introduction
The statistical machine translation approach used
in this work implements a log-linear combination
of feature functions along with a translation model
which is based on bilingual n-grams (de Gispert and
Marin?o, 2002).
This translation model differs from the well
known phrase-based translation approach (Koehn
et al, 2003) in two basic issues: first, training data
is monotonously segmented into bilingual units; and
second, the model considers n-gram probabilities in-
stead of relative frequencies. This translation ap-
proach is described in detail in (Marin?o et al, 2005).
For those translation tasks with Spanish or En-
glish as target language, an additional tagged (us-
ing POS information) target language model is used.
Additionally a reordering strategy that includes POS
information is described and evaluated.
Translation results for all six translation directions
proposed in the shared task are presented and dis-
cussed. Both translation directions are considered
for the pairs: English-Spanish, English-French,
and English-German.
The paper is structured as follows: Section 2
briefly outlines the baseline system. Section 3 de-
scribes in detail the implemented POS-based re-
ordering strategy. Section 4 presents and discusses
the shared task results and, finally, section 5 presents
some conclusions and further work.
2 Baseline N-gram-based SMT System
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units, referred
to as tuples, which approximates the joint probabil-
ity between source and target languages by using
bilingual n-grams (de Gispert and Marin?o, 2002).
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, no smaller tuples can be extracted without vi-
olating the previous constraint. See (Crego et al,
2004) for further details.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tu-
ples. In addition to this bilingual n-gram translation
model, the baseline system implements a log linear
combination of five feature functions.
162
These five additional models are:
? A target language model. 5-gram of the target
side of the bilingual corpus.
? A word bonus. Based on the number of tar-
get words in the partial-translation hypothesis,
to compensate the LM preference for short sen-
tences.
? A Source-to-target lexicon model. Based on
IBM Model 1 lexical parameters(Brown et al,
1993), providing a complementary probability
for each tuple in the translation table. These
parameters are obtained from source-to-target
alignments.
? A Target-to-source lexicon model. Analo-
gous to the previous feature, but obtained from
target-to-source alignments.
? A Tagged (POS) target language model. This
feature implements a 5-gram language model
of target POS-tags. In this case, each trans-
lation unit carried the information of its target
side POS-tags, though this is not used for trans-
lation model estimation (only in order to eval-
uate the target POS language model at decod-
ing time). Due to the non-availability of POS-
taggers for French and German, it was not pos-
sible to incorporate this feature in all transla-
tion tasks considered, being only used for those
translation tasks with Spanish and English as
target languages.
The search engine for this translation system is
described in (Crego et al, 2005) and implements
a beam-search strategy based on dynamic program-
ming, taking into account all feature functions de-
scribed above, along with the bilingual n-gram trans-
lation model. Monotone search is performed, in-
cluding histogram and threshold pruning and hy-
pothesis recombination.
An optimization tool, which is based on a down-
hill simplex method was developed and used for
computing log-linear weights for each of the feature
functions. This algorithm adjusts the weights so that
a non-linear combination of BLEU and NIST scores
is maximized over the development set for each of
the six translation directions considered.
This baseline system is actually very similar to
the system used for last year?s shared task ?Exploit-
ing Parallel Texts for Statistical Machine Transla-
tion? of ACL?05 Workshop on Building and Us-
ing Parallel Texts: Data-Driven Machine Translation
and Beyond (Banchs et al, 2005), whose results
are available at: http://www.statmt.org/wpt05/
mt-shared-task/. A more detailed description of
the system can be found in (2005).
The tools used for POS-tagging were Freel-
ing (Carreras et al, 2004) for Spanish and
TnT (Brants, 2000) for English. All language mod-
els were estimated using the SRI language mod-
eling toolkit. Word-to-word alignments were ex-
tracted with GIZA++. Improvements in word-to-
word alignments were achieved through verb group
classification as described in (de Gispert, 2005).
3 Reordering Framework
In this section we outline the reordering framework
used for the experiments (Crego and Marin?o, 2006).
A highly constrained reordered search is performed
by means of a set of reordering patterns (linguisti-
cally motivated rewrite patterns) which are used to
extend the monotone search graph with additional
arcs.
To extract patterns, we use the word-to-word
alignments (the union of both alignment directions)
and source-side POS tags. The main procedure con-
sists of identifying all crossings produced in the
Figure 1: Reordering patterns are extracted using
word-to-word alignments. The generalization power
is achieved through the POS tags. Three instances of
different patterns are extracted using the sentences
in the example.
163
word-to-word alignments. Once a crossing has been
detected, its source POS tags and alignments are
used to account for a new instance of pattern. The
target side of a pattern (source-side positions after
reordering), is computed using the original order
of the target words to which the source words are
aligned. See figure 1 for a clarifying example of
pattern extraction.
The monotone search graph is extended with re-
orderings following the patterns found in training.
The procedure identifies first the sequences of words
in the input sentence that match any available pat-
tern. Then, each of the matchings implies the ad-
dition of an arc into the search graph (encoding the
reordering learnt in the pattern). However, this ad-
dition of a new arc is not performed if a translation
unit with the same source-side words already exists
in the training. Figure 2 shows an example of the
procedure.
Figure 2: Three additional arcs have been added
to the original monotone graph (bold arcs) given
the reordering patterns found matching any of the
source POS tags sequence.
Once the search graph is built, the decoder tra-
verses the graph looking for the best translation.
Hence, the winner hypothesis is computed using
all the available information (the whole SMT mod-
els). The reordering strategy is additionally sup-
ported by a 5-gram language model of reordered
source POS-tags. In training, POS-tags are re-
ordered according with the extracted reordering pat-
terns and word-to-word links. The resulting se-
quence of source POS-tags are used to train the n-
gram LM.
Notice that this reordering framework has only
been used for some translation tasks (Spanish-
to-English, English-to-Spanish and English-to-
French). The reason is double: first, because we
did not have available a French POS-tagger. Second,
because the technique used to learn reorderings (de-
tailed below) does not seem to apply for language
pairs like German-English, because the agglutina-
tive characteristic of German (words are formed by
joining morphemes together).
Table 1: BLEU, NIST and mWER scores (com-
puted using two reference translations) obtained for
both translation directions (Spanish-to-English and
English-to-Spanish).
Conf BLEU NIST mWER
Spanish-to-English
base 55.23 10.69 34.40
+rgraph 55.59 10.70 34.23
+pos 56.39 10.75 33.75
English-to-Spanish
base 48.03 9.84 41.18
+rgraph 48.53 9.81 41.15
+pos 48.91 9.91 40.29
Table 1 shows the improvement of the original
baseline system described in section 2 (base), en-
hanced using reordering graphs (+rgraph) and pro-
vided the tagged-source language model (+pos).
The experiments in table 1 were not carried out over
the official corpus of this shared task. The Spanish-
English corpus of the TC-Star 2005 Evaluation was
used. Due to the high similarities between both cor-
pus (this shared task corpus consists of a subset of
the whole corpus used in the TC-Star 2005 Evalua-
tion), it makes sense to think that comparable results
would be obtained.
It is worth mentioning that the official corpus of
the shared task (HLT-NAACL 2006) was used when
building and tuning the present shared task system.
4 Shared Task Results
The data provided for this shared task corresponds
to a subset of the official transcriptions of the Euro-
pean Parliament Plenary Sessions. The development
set used to tune the system consists of a subset (500
first sentences) of the official development set made
available for the Shared Task.
164
Table 2 presents the BLEU, NIST and mWER
scores obtained for the development-test data set.
The last column shows whether the target POS lan-
guage model feature was used or not. Computed
scores are case sensitive and compare to one refer-
ence translation. Tasks in bold were conducted al-
lowing for the reordering framework. For French-
to-English task, block reordering strategy was used,
which is described in (Costa-jussa` et al, 2006). As it
can be seen, for the English-to-German task we did
not use any of the previous enhancements.
Table 2: Translation results
Task BLEU NIST mWER tPOS
en ? es 29.50 7.32 58.95 yes
es ? en 30.29 7.51 57.72 yes
en ? fr 30.23 7.40 59.76 no
fr ? en 30.21 7.61 56.97 yes
en ? de 17.40 5.61 71.18 no
de ? en 23.78 6.70 65.83 yes
Important differences can be observed between
the German-English and the rest of translation tasks.
They result from the greater differences in word
order present in this language pair (the German-
English results are obtained under monotone decod-
ing conditions). Also because the greater vocabulary
of words of German, which increases sparseness in
any task where German is envolved. As expected,
differences in translation accuracy between Spanish-
English and French-English are smaller.
5 Conclusions and Further Work
As it can be concluded from the presented results,
although in principle some language pairs (Spanish-
English-French) seem to have very little need for re-
orderings (due to their similar word order), the use
of linguistically-based reorderings proves to be use-
ful to improve translation accuracy.
Additional work is to be conducted to allow for
reorderings when translating from/to German.
6 Acknowledgments
This work was partly funded by the European Union
under the integrated project TC-STAR1: Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738) and the European Social Fund.
1http://www.tc-star.org
References
R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, and
J. B. Marin?o. 2005. Statistical machine translation of
euparl data by using bilingual n-grams. Proc. of the
ACL Workshop on Building and Using Parallel Texts
(ACL?05/Wkshp), pages 67?72, June.
T. Brants. 2000. TnT ? a statistical part-of-speech tag-
ger. In Proc. of the Sixth Applied Natural Language
Processing (ANLP-2000), Seattle, WA.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
Freeling: An open-source suite of language analyzers.
4th Int. Conf. on Language Resources and Evaluation,
LREC?04, May.
M.R. Costa-jussa`, J.M. Crego, A. de Gispert, P. Lam-
bert, M. Khalilov, R. Banchs, J.B. Marin?o, and J.A.R.
Fonollosa. 2006. Talp phrase-based statistical transla-
tion system for european language pairs. Proc. of the
HLT/NAACL Workshop on Statistical Machine Trans-
lation, June.
J. M. Crego and J. Marin?o. 2006. A reordering frame-
work for statistical machine translation. Internal Re-
port.
J. M. Crego, J. Marin?o, and A. de Gispert. 2004. Finite-
state-based and phrase-based statistical machine trans-
lation. Proc. of the 8th Int. Conf. on Spoken Language
Processing, ICSLP?04, pages 37?40, October.
J. M. Crego, J. Marin?o, and A. Gispert. 2005. An ngram-
based statistical machine translation decoder. Proc. of
the 9th European Conference on Speech Communica-
tion and Technology, Interspeech?05, September.
A. de Gispert and J. Marin?o. 2002. Using X-grams
for speech-to-speech translation. Proc. of the 7th
Int. Conf. on Spoken Language Processing, ICSLP?02,
September.
A. de Gispert. 2005. Phrase linguistic classification and
generalization for improving statistical machine trans-
lation. Proc. of the ACL Student Research Workshop
(ACL?05/SRW), June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
May.
J.B. Marin?o, R Banchs, J.M. Crego, A. de Gispert,
P. Lambert, M. R. Costa-jussa`, and J.A.R. Fonollosa.
2005. Bilingual n?gram statistical machine transla-
tion. Proc. of the MT Summit X, September.
165
Proceedings of the Second Workshop on Statistical Machine Translation, pages 167?170,
Prague, June 2007. c?2007 Association for Computational Linguistics
Ngram-based statistical machine translation enhanced with multiple
weighted reordering hypotheses
Marta R. Costa-jussa`, Josep M. Crego, Patrik Lambert, Maxim Khalilov
Jose? A. R. Fonollosa, Jose? B. Marin?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.edu
Abstract
This paper describes the 2007 Ngram-based sta-
tistical machine translation system developed at
the TALP Research Center of the UPC (Uni-
versitat Polite`cnica de Catalunya) in Barcelona.
Emphasis is put on improvements and extensions
of the previous years system, being highlighted
and empirically compared. Mainly, these include
a novel word ordering strategy based on: (1) sta-
tistically monotonizing the training source cor-
pus and (2) a novel reordering approach based
on weighted reordering graphs. In addition, this
system introduces a target language model based
on statistical classes, a feature for out-of-domain
units and an improved optimization procedure.
The paper provides details of this system par-
ticipation in the ACL 2007 SECOND WORK-
SHOP ON STATISTICAL MACHINE TRANSLA-
TION. Results on three pairs of languages are
reported, namely from Spanish, French and Ger-
man into English (and the other way round) for
both the in-domain and out-of-domain tasks.
1 Introduction
Based on estimating a joint-probability model between
the source and the target languages, Ngram-based SMT
has proved to be a very competitive alternatively to
phrase-based and other state-of-the-art systems in previ-
ous evaluation campaigns, as shown in (Koehn and Monz,
2005; Koehn and Monz, 2006).
Given the challenge of domain adaptation, efforts have
been focused on improving strategies for Ngram-based
SMT which could generalize better. Specifically, a novel
reordering strategy is explored. It is based on extending
the search by using precomputed statistical information.
Results are promising while keeping computational ex-
penses at a similar level as monotonic search. Addition-
ally, a bonus for tuples from the out-of-domain corpus is
introduced, as well as a target language model based on
statistical classes. One of the advantages of working with
statistical classes is that they can easily be used for any
pair of languages.
This paper is organized as follows. Section 2 briefly
reviews last year?s system, including tuple definition and
extraction, translation model and feature functions, de-
coding tool and optimization criterion. Section 3 delves
into the word ordering problem, by contrasting last year
strategy with the novel weighted reordering input graph.
Section 4 focuses on new features: both tuple-domain
bonus and target language model based on classes. Later
on, Section 5 reports on all experiments carried out for
WMT 2007. Finally, Section 6 sums up the main conclu-
sions from the paper and discusses future research lines.
2 Baseline N-gram-based SMT System
The translation model is based on bilingual n-grams. It
actually constitutes a language model of bilingual units,
referred to as tuples, which approximates the joint proba-
bility between source and target languages by using bilin-
gual n-grams.
Tuples are extracted from a word-to-word aligned cor-
pus according to the following two constraints: first, tu-
ple extraction should produce a monotonic segmentation
of bilingual sentence pairs; and second, no smaller tuples
can be extracted without violating the previous constraint.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tuples.
In addition to this bilingual n-gram translation model, the
baseline system implements a log linear combination of
four feature functions. These four additional models are:
a target language model (a 5-gram model of words);
a word bonus; a source-to-target lexicon model and a
target-to-source lexicon model, both features provide a
complementary probability for each tuple in the transla-
tion table.
The decoder (called MARIE) for this translation sys-
167
tem is based on a beam search 1.
This baseline system is actually the same system used
for the first shared task ?Exploiting Parallel Texts for Sta-
tistical Machine Translation? of the ACL 2005 Work-
shop on Building and Using Parallel Texts: Data-Driven
Machine Translation and Beyond. A more detailed de-
scription of the system can be found in (Marin?o et al,
2006).
3 Baseline System Enhanced with a
Weighted Reordering Input Graph
This section briefly describes the statistical machine re-
ordering (SMR) technique. Further details on the archi-
tecture of SMR system can be found on (Costa-jussa` and
Fonollosa, 2006).
3.1 Concept
The SMR system can be seen as a SMT system which
translates from an original source language (S) to a re-
ordered source language (S?), given a target language
(T). The SMR technique works with statistical word
classes (Och, 1999) instead of words themselves (partic-
ularly, we have used 200 classes in all experiments).
Figure 1: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
3.2 Using SMR technique to improve SMT training
The original source corpus S is translated into the re-
ordered source corpus S? with the SMR system. Fig-
ure 1 (A) shows the corresponding block diagram. The
reordered training source corpus and the original training
target corpus are used to build the SMT system.
The main difference here is that the training is com-
puted with the S?2T task instead of the S2T original task.
Figure 2 (A) shows an example of the alignment com-
puted on the original training corpus. Figure 2 (B) shows
the same links but with the source training corpus in a
different order (this training corpus comes from the SMR
output). Although, the quality in alignment is the same,
the tuples that can be extracted change (notice that the
tuple extraction is monotonic). We are able to extract
1http://gps-tsc.upc.es/veu/soft/soft/marie/
smaller tuples which reduces the translation vocabulary
sparseness. These new tuples are used to build the SMT
system.
Figure 2: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
3.3 Using SMR technique to generate multiple
weighted reordering hypotheses
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 1 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear framework. Figure 3 shows the
weighted reordering graph.
The main difference with the reordering technique for
WMT06 (Crego et al, 2006) lies in (1) the tuples are ex-
tracted from the word alignment between the reordered
source training corpus and the given target training cor-
pus and (2) the graph structure: the SMR graph provides
weights for each reordering path.
4 Other features and functionalities
In addition to the novel reordering strategy, we consider
two new features functions.
4.1 Target Language Model based on Statistical
Classes
This feature implements a 5-gram language model of tar-
get statistical classes (Och, 1999). This model is trained
by considering statistical classes, instead of words, for
168
Figure 3: Weighted reordering input graph for SMT sys-
tem.
the target side of the training corpus. Accordingly, the tu-
ple translation unit is redefined in terms of a triplet which
includes: a source string containing the source side of
the tuple, a target string containing the target side of the
tuple, and a class string containing the statistical classes
corresponding to the words in the target strings.
4.2 Bonus for out-of-domain tuples
This feature adds a bonus to those tuples which comes
from the training of the out-of-domain task. This feature
is added when optimizing with the development of the
out-of-domain task.
4.3 Optimization
Finally, a n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/. This procedure
allows for a faster and more efficient adjustment of model
weights by means of a double-loop optimization, which
provides significant reduction of the number of transla-
tions that should be carried out. The current optimization
procedure uses the Simplex algorithm.
5 Shared Task Framework
5.1 Data
The data provided for this shared task corresponds to a
subset of the official transcriptions of the European Par-
liament Plenary Sessions 2. Additionally, there was avail-
able a smaller corpus called News-Commentary. For all
tasks and domains, our training corpus was the catenation
of both.
2http://www.statmt.org/wmt07/shared-task/
5.2 Processing details
Word Alignment. The word alignment is automati-
cally computed by using GIZA++ 3 in both directions,
which are symmetrized by using the union operation. In-
stead of aligning words themselves, stems are used for
aligning. Afterwards case sensitive words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al are splited into de el or a
el. As a post-processing, in the En2Es direction we used
a POS target language model as a feature (instead of the
target language model based on classes) that allowed to
recover the segmentations (de Gispert, 2006).
Language Model Interpolation. In other to better
adapt the system to the out-of-domain condition, the
target language model feature was built by combining
two 5-gram target language models (using SRILM 4).
One was trained from the EuroParl training data set, and
the other from the available, but much smaller, news-
commentary data set. The combination weights for the
EuroParl and news-commentary language models were
empirically adjusted by following a minimum perplexity
criterion. A relative perplexity reduction around 10-15%
respect to original EuroParl language model was achieved
in all the tasks.
5.3 Experiments and Results
The main difference between this year?s and last year?s
systems are: the amount of data provided; the word align-
ment; the Spanish morphology reduction; the reordering
technique; the extra target language model based on sta-
tistical classes (except for the En2Es); and the bonus for
the out-of-domain task (only for the En2Es task).
Among them, the most important is the reordering
technique. That is why we provide a fair comparison be-
tween the reordering patterns (Crego and Marin?o, 2006)
technique and the SMR reordering technique. Table 1
shows the system described above using either reorder-
ing patterns or the SMR technique. The BLEU calcula-
tion was case insensitive and sensitive to tokenization.
Table 2 presents the BLEU score obtained for the 2006
test data set comparing last year?s and this year?s systems.
The computed BLEU scores are case insensitive, sensi-
tive to tokenization and uses one translation reference.
The improvement in BLEU results shown from UPC-jm
3http://www.fjoch.com/GIZA++.html
4http://www.speech.sri.com/projects/srilm/
169
Task Reordering patterns SMR technique
es2en 31.21 33.34
en2es 31.67 32.33
Table 1: BLEU comparison: reordering patterns vs. SMR
technique.
Task UPC-jm 2006 UPC 2007
in-d out-d in-d out-d
es2en 31.01 27.92 33.34 32.85
en2es 30.44 25.59 32.33 33.07
fr2en 30.42 21.79 32.44 26.93
en2fr 31.75 23.30 32.30 27.03
de2en 24.43 17.57 26.54 21.63
en2de 17.73 10.96 19.74 15.06
Table 2: BLEU scores for each of the six translation di-
rections considered (computed over 2006 test set) com-
paring last year?s and this year?s system results (in-
domain and out-domain).
2006 Table 2 and reordering patterns Table 1 in the En-
glish/Spanish in-domain task comes from the combina-
tion of: the additional corpora, the word alignment, the
Spanish morphology reduction and the extra target lan-
guage model based on classes (only in the Es2En direc-
tion).
6 Conclusions and Further Work
This paper describes the UPC system for the WMT07
Evaluation. In the framework of Ngram-based system, a
novel reordering strategy which can be used for any pair
of languages has been presented and it has been showed
to significantly improve translation performance. Ad-
ditionally two features has been added to the log-lineal
scheme: the target language model based on classes and
the bonus for out-of-domain translation units.
7 Acknowledgments
This work has been funded by the European Union un-
der the TC-STAR project (IST-2002-FP6-506738) and
the Spanish Government under grant TEC2006-13964-
C03 (AVIVAVOZ project).
References
M.R. Costa-jussa` and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In EMNLP, pages 71?77, Sydney,
July. ACL.
J.M. Crego and J.B. Marin?o. 2006. Reordering experi-
ments for n-gram-based smt. In SLT, pages 242?245,
Aruba.
Josep M. Crego, Adria` de Gispert, Patrik Lambert,
Marta R. Costa-jussa`, Maxim Khalilov, Rafael Banchs,
Jose? B. Marin?o, and Jose? A. R. Fonollosa. 2006. N-
gram-based smt system enhanced with reordering pat-
terns. In WMT, pages 162?165, New York City, June.
ACL.
Adria` de Gispert. 2006. Introducing Linguistic Knowl-
edge in Statistical Machine Translation. Ph.D. thesis,
Universitat Polite`cnica de Catalunya, December.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between european lan-
guages. In WMT, pages 119?124, Michigan, June.
ACL.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In WMT, pages 102?121, New
York City, June. ACL.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549, December.
F.J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In EACL, pages 71?76,
Bergen, Norway, June.
170
Proceedings of the Second Workshop on Statistical Machine Translation, pages 171?176,
Prague, June 2007. c?2007 Association for Computational Linguistics
Analysis of statistical and morphological classes to generate weighted
reordering hypotheses on a Statistical Machine Translation system
Marta R. Costa-jussa` and Jose? A. R. Fonollosa
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(mruiz,adrian)@gps.tsc.upc.edu
Abstract
One main challenge of statistical machine trans-
lation (SMT) is dealing with word order. The
main idea of the statistical machine reordering
(SMR) approach is to use the powerful tech-
niques of SMT systems to generate a weighted
reordering graph for SMT systems. This tech-
nique supplies reordering constraints to an SMT
system, using statistical criteria.
In this paper, we experiment with different graph
pruning which guarantees the translation quality
improvement due to reordering at a very low in-
crease of computational cost.
The SMR approach is capable of generalizing re-
orderings, which have been learned during train-
ing, by using word classes instead of words
themselves. We experiment with statistical and
morphological classes in order to choose those
which capture the most probable reorderings.
Satisfactory results are reported in the WMT07
Es/En task. Our system outperforms in terms of
BLEU the WMT07 Official baseline system.
1 Introduction
Nowadays, statistical machine translation is mainly based
on phrases (Koehn et al, 2003). In parallel to this phrase-
based approach, the use of bilingual n-grams gives com-
parable results, as shown by Crego et al (2005). Two
basic issues differentiate the n-gram-based system from
the phrase-based: training data is monotonically seg-
mented into bilingual units; and, the model considers n-
gram probabilities rather than relative frequencies. The
n-gram-based system follows a maximum entropy ap-
proach, in which a log-linear combination of multiple
models is implemented (Marin?o et al, 2006), as an al-
ternative to the source-channel approach.
Introducing reordering capabilities is important in both
systems. Recently, new reordering strategies have been
proposed such as the reordering of each source sentence
to match the word order in the corresponding target sen-
tence, see Kanthak et al (2005) and Marin?o et al (2006).
These approaches are applied in the training set and they
lack of reordering generalization.
Applied both in the training and decoding step, Collins
et al (2005) describe a method for introducing syntac-
tic information for reordering in SMT. This approach is
applied as a pre-processing step.
Differently, Crego et al (2006) presents a reordering
approach based on reordering patterns which is coupled
with decoding. The reordering patterns are learned di-
rectly from word alignment and all reorderings have the
same probability.
In our previous work (Costa-jussa` and Fonollosa,
2006) we presented the SMR approach which is based
on using the powerful SMT techniques to generate a re-
ordered source input for an SMT system both in train-
ing and decoding steps. One step further, (Costa-jussa`
et al, 2007) shows how the SMR system can generate a
weighted reordering graph, allowing the SMT system to
make the final reordering decision.
In this paper, the SMR approach is used to train the
SMT system and to generate a weighted reordering graph
for the decoding step. The SMR system uses word classes
instead of words themselves and we analyze both statisti-
cal and morphological classes. Moreover, we present ex-
periments regarding the reordering graph efficiency: we
analyze different graph pruning and we show the very low
increase in computational cost (compared to a monotonic
translation). Finally, we compare the performance our
system in terms of BLEU with the WMT07 baseline sys-
tem.
This paper is organized as follows. The first two sec-
tions explain the SMT and the SMR baseline systems,
respectively. Section 4 reports the study of statistical and
171
morphological classes. Section 5 describes the experi-
mental framework and discusses the results. Finally, Sec-
tion 6 presents the conclusions and some further work.
2 Ngram-based SMT System
This section briefly describes the Ngram-based SMT (for
further details see (Marin?o et al, 2006)). The Ngram-
based SMT system uses a translation model based on
bilingual n-grams. It is actually a language model of
bilingual units, referred to as tuples, which approxi-
mates the joint probability between source and target lan-
guages by using bilingual n-grams. Tuples are extracted
from any word alignment according to the following con-
straints:
1. a monotonic segmentation of each bilingual sen-
tence pairs is produced,
2. no word inside the tuple is aligned to words outside
the tuple, and
3. no smaller tuples can be extracted without violating
the previous constraints.
As a result of these constraints, only one segmentation
is possible for a given sentence pair.
In addition to the bilingual n-gram translation model,
the baseline system implements a log-linear combination
of feature functions, which are described as follows:
? A target language model. This feature consists of
a 4-gram model of words, which is trained from the
target side of the bilingual corpus.
? A class target language model. This feature con-
sists of a 5-gram model of words classes, which is
trained from the target side of the bilingual corpus
using the statistical classes from (Och, 1999).
? A word bonus function. This feature introduces
a bonus based on the number of target words con-
tained in the partial-translation hypothesis. It is used
to compensate for the system?s preference for short
output sentences.
? A source-to-target lexicon model. This feature,
which is based on the lexical parameters of the IBM
Model 1 (Brown et al, 1993), provides a comple-
mentary probability for each tuple in the translation
table. These lexicon parameters are obtained from
the source-to-target algnments.
? A target-to-source lexicon model. Similarly to the
previous feature, this feature is based on the lexical
parameters of the IBM Model 1 but, in this case,
these parameters are obtained from target-to-source
alignments.
Figure 1: SMR block diagram.
3 SMR Baseline System
As mentioned in the introduction, SMR and SMT are
based on the same principles.
3.1 Concept
The aim of SMR consists in using an SMT system to deal
with reordering problems. Therefore, the SMR system
can be seen as an SMT system which translates from an
original source language (S) to a reordered source lan-
guage (S?), given a target language (T).
3.2 Description
Figure 1 shows the SMR block diagram and an exam-
ple of the input and output of each block inside the
SMR system. The input is the initial source sentence
(S) and the output is the reordered source sentence (S?).
There are three blocks inside SMR: (1) the class replac-
ing block; (2) the decoder, which requires an Ngram
model containing the reordering information; and, (3) the
post-processing block which either reorders the source
sentence given the indexes of the decoder output 1-best
(training step) or transforms the decoder output graph to
an input graph for the SMT system (decoding step).
The decoder in Figure 1 requires a translation model
which is an Ngram model. Given a training parallel cor-
pus this model has been built following the next steps:
1. Select source and target word classes.
2. Align parallel training sentences at the word level in
both translation directions. Compute the union of
the two alignments to obtain a symmetrized many-
to-many word alignment.
3. Use the IBM1 Model to obtain a many-to-one word
alignment from the many-to-many word alignment.
4. Extract translation units from the computed many-
to-one alignment. Replace source words by their
172
Figure 2: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
classes and target words by the index of the linked
source word. An example of a translation unit here
is: C61 C28 C63#2 0 1, where # divides source
(word classes) and target (positions).
5. Compute the sequence of the above units and learn
the language model
For further information about the SMR training proce-
dure see (Costa-jussa` and Fonollosa, 2006).
3.3 Improving SMT training
Figure 2 (A) shows the corresponding block diagram
for the training corpus: first, the given training corpus
S is translated into the reordered training source corpus
S? with the SMR system. Then, this reordered training
source corpus S? and the given training target corpus T
are used to build the SMT system
The main difference here is that the training is com-
puted with the S?2T task instead of the S2T given task.
Figure 3 (A) shows an example of the word alignment
computed on the given training parallel corpus S2T. Fig-
ure 3 (B) shows the same links but with the reordered
source training corpus S?. Although the quality in align-
ment is the same, the tuples that can be extracted change
(notice that tuple extraction is monotonic). We now are
able to extract smaller tuples which reduce the transla-
tion vocabulary sparseness. These new tuples are used to
build the SMT system.
3.4 Generation of multiple weighted reordering
hypotheses
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 2 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
Figure 3: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear model.
4 Morphological vs Statistical Classes
Previous SMR studies (Costa-jussa` and Fonollosa,
2006) (Costa-jussa` et al, 2007) considered only statisti-
cal classes. On the one hand, these statistical classes per-
formed fairly well and had the advantage of being suit-
able for any language. On the other hand, it should be
taken into account the fact of training them in the train-
ing set alows for unknown words in the development or
in the test set. Additionally, they do not have any reorder-
ing information because they are trained on a monolin-
gual set.
The first problem, unknown words which appear in
the development or in the test set, may be solved by us-
ing a disambiguation technique. Unknown words can be
assigned to one class by taking into account their own
context. The second problem, incorporating information
about order, might be solved by training classes in the
reordered training source corpus. In other words, we
monotonized the training corpus with the alignment in-
formation (i.e. reorder the source corpus in the way that
matches the target corpus under the alignment links cri-
terion). After that, we train the statistical classes, here-
inafter, called statistical reordered classes.
In some pair of languages, as for example En-
glish/Spanish, the reordering that may be performed is
related to word?s morphology (i.e. TAGS). Some TAGS
rules (with some lexical exceptions) can be extracted as
in (Popovic and Ney, 2006) where they were applied
with reordering purposes as a preprocessing step. An-
other approach that has related TAGS and reordering was
presented in (Crego and Marin?o, 2006) where instead of
rules, they learned reordering patterns based on TAGS as
named in this paper?s introduction. Hence, the SMR tech-
173
Spanish English
Train Sentences 1,3M
Words 37,9M 35,5M
Vocabulary 138,9k 133k
Dev Sentences 2 000 2 000
Words 60.5k 58.7k
Vocabulary 8.1k 6.5k
Test Sentences 2 000 2 000
Words 60,2k 58k
Vocabulary 8,2k 6,5k
Table 1: Corpus Statistics.
nique may take advantage of the morphological informa-
tion. Notice that an advantage is that there is a TAG for
each word, hence there are not unknown words.
5 Evaluation Framework
5.1 Corpus Statistics
Experiments were carried out using the data in the second
evaluation campaign of the WMT07 1.
This corpus consists in the official version of the
speeches held in the European Parliament Plenary Ses-
sions (EPPS), as available on the web page of the Eu-
ropean Parliament. Additionally, there was available a
smaller corpus (News-Commentary). Our training cor-
pus was the catenation of both. Table 1 shows the corpus
statistics.
5.2 Tools and preprocessing
The system was built similarly to (Costa-jussa` et al,
2007). The SMT baseline system uses the Ngram-
based approach, which has been explained in Section 2.
Tools used are defined as follows: word alignments were
computed using GIZA++ 2; language model was esti-
mated using SRILM 3; decoding was carried out with
MARIE4; an n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/ using the simplex
method (Nelder and Mead, 1965) and BLEU as a loss
function.
The SMT system we use a 4gram translation language
model, a 5gram target language model and a 5gram class
target language model.
Spanish data have been processed so that the pronouns
which are attached to verbs are split up. Additionally,
several article and prepositions words are separated (i.e.
1http://www.statmt.org/wmt07/
2http://www.fjoch.com/GIZA++.html
3http://www.speech.sri.com/projects/srilm/
4http://gps-tsc.upc.es/veu/soft/soft/marie/
Figure 5: Perplexity over the manually aligned test set
given the SMR Ngram length.
del goes into de el). This preprocessing was performed
using Freeling software (Atserias et al, 2006). Training
and evaluation were both true-case.
5.3 Classes and Ngram length Study for the
SMR-Graph generation
This section evaluates several types of classes and n-gram
lengths in the SMR model in order to choose the SMR
configuration which provides the best results in trans-
lation in terms of quality. To accomplish this evalua-
tion, we have designed the following experiment. Given
500 manually aligned parallel sentences of the EPPS cor-
pora (Lambert et al, 2006), we order the source test in
the way that better matches the target set. This ordered
source set is considered our reference as it is based on
manual alignments. On the other hand, the 500 sen-
tences set is translated using the SMR configurations to
be tested. Finally, the Word Error Rate (WER) is used as
quality measure.
Figure 4 shows the WER behavior given different types
of classes. As statistical classes (cl50,cl100,cl200) we
used the Och monolingual classes (Och, 1999), which
can be performed using ?mkcls? (a tool available with
GIZA). Also we used the statistical reordered classes
(cl100mono) which were explained in Section 4. Both
statistical and statistical reordered classes used the dis-
amb tool of SRILM in order to classify unknown words.
As morphological classes we used the TAGS provided by
Freeling. Clearly, statistical classes perform better than
TAGS and best results can be achieved with 100 and 200
classes and an n-gram length of 5.
For the sake of completeness, we have evaluated the
perplexity of the SMR Ngram model over the aligned test
set above and choosing 200 classes. Figure 5 is coherent
with the WER results above and it shows that perplexity
is not reduced for an n-gram length greater than 5.
174
Figure 4: WER over the reference given various sets of classes and Ngram lengths.
5.4 Graph pruning
The more complex is the reordering graph, the less effi-
cient is the decoding. That is why, in this section, we ex-
periment with several ways of graph pruning. Addition-
ally, for each pruning we see the influence of considering
the graph weights (i.e. reordering feature importance).
Given that the reordering graph is the output of a beam
search decoder, we can consider pruning the reordering
graph by limiting the SMR beam, i.e. limiting the size of
hypothesis stacks.
Given a reordering graph, another option is to prune
states and arches only used in paths s times worse than
the best path.
Table 2 gives the results of the proposed pruning. Note
that computational time is given in terms of the mono-
tonic translation time (and it is the same for both direc-
tions). It is shown that graph pruning guarantees the effi-
ciency of the system and even increases the translation?s
quality. Similar results are obtained in terms of BLEU for
both types of pruning. In this task and for both translation
directions, it seems more appropriate to limit directly the
beam search in the SMR step to 5.
As expected, the influence of the reordering feature,
which takes into account the graph weights, tends to be
more important as pruning decreases (i.e. when the graph
has more paths).
Pruning Wr BLEUEn2Es BLEUEs2En TIME
b5 yes 31.32 32.64 2.4Tm
b5 no 31.25 31.82 2.5Tm
b50 yes 30.95 32.28 5.3Tm
b50 no 30.90 27.44 4.8Tm
b50 s10 yes 31.19 32.20 1.5Tm
b50 s10 no 31.07 32.41 1.4Tm
Table 2: Performance in BLEU in the test set of different
graph pruning (b stands for beam and s for states); the
use of reordering feature function (Wr indicates its use);
and the time increase related to Tm (monotonic transla-
tion time).
5.5 Results and discussion
Table 3 shows the performance of our Ngram-
based system using the SMR technique. First
row is the WMT07 baseline system which can
be reproduced following the instructions in
http://www.statmt.org/wmt07/baseline.html. This
baseline system uses a non-monotonic search. Second
row shows the results of the Ngram-based system
presented in section 2 using the weighted reordering
graph trained with the best configuration found in the
above section (200 statistical classes and an Ngram of
length 5).
175
System BLEUes2en BLEUen2es
WMT07 Of. Baseline 31.21 30.74
Ngram-based 32.64 31.32
Table 3: BLEU Results.
6 Conclusions and further work
The proposed SMR technique can be used both in training
and test steps in a SMT system. Applying the SMR tech-
nique in the training step reduces the sparseness in the
translation vocabulary. Applying SMR technique in the
test step allows to generate a weighted reordering graph
for SMT system.
The use of classes plays an important role in the SMR
technique, and experiments have shown that statistical
classes are better than morphological ones.
Moreover, we have experimented with different graph
pruning showing that best translation results can be
achieved at a very low increase of computational cost
when comparing to the monotonic translation computa-
tional cost.
Finally, we have shown that our translation system us-
ing the SMR technique outperforms the WMT07 Official
baseline system (which uses a non-monotonic search) in
terms of BLEU.
As further work, we want to introduce the SMR tech-
nique in a state-of-the-art phrase-based system.
7 Acknowledgments
This work has been funded by the European Union under
the TC-STAR project (IST- 2002-FP6-506738) and the
Spanish Government under grant TEC2006-13964-C03
(AVIVAVOZ project).
References
J. Atserias, B. Casas, E. Comelles, M. Gonza?lez,
L. Padro?, and M. Padro?. 2006. Freeling 1.3: Syntactic
and semantic services in an open-source nlp library. In
5th Int. Conf. on Language Resource and Evaluation
(LREC), pages 184?187.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine transla-
tion. Computational Linguistics, 19(2):263?311.
M. Collins, P. Koehn, and I. Kucerova?. 2005. Clause
restructuring for statistical machine translation. In 43st
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531 ? 540, Michigan.
M.R. Costa-jussa` and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In Empirical Methods in Natural
Language Processing (EMNLP), pages 71?77, Sydney.
M. R. Costa-jussa`, P. Lambert, J.M. Crego, M. Khalilov,
J.A.R. Fonollosa, J.B. Marin?o, and R. Banchs. 2007.
Ngram-based statistical machine translation enhanced
with multiple weighted reordering hypotheses. In
ACL: Workshop of Statistical Machine Translation
(WMT07), Prague.
J.M. Crego and J.B. Marin?o. 2006. Reordering exper-
iments for n-gram-based smt. Ist IEEE/ACL Inter-
national Workshop on Spoken Language Technology
(SLT?06), pages 242?245.
J. M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.
Fonollosa. 2005. Ngram-based versus phrase-
based statistical machine translation. In Proc. of
the Int. Workshop on Spoken Language Translation,
IWSLT?05, pages 177?184, Pittsburgh, October.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of the
ACL Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
167?174, Ann Arbor, MI, June.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proc. of the Human
Language Technology Conference, HLT-NAACL?2003,
pages 48 ? 54, Edmonton, Canada, May.
P. Lambert, A. de Gispert, R. Banchs, and J. Marin?o.
2006. Guidelines for word alignment and man-
ual alignment. Language Resources and Evaluation,
39(4):267?285.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527?549.
J.A. Nelder and R. Mead. 1965. A simplex method for
function minimization. The Computer Journal, 7:308?
313.
F.J. Och. 1999. An efficient method for determining
bilingual word classes. In 9th Conf. of the European
Chapter of the Association for Computational Linguis-
tics (EACL), pages 71?76, June.
M. Popovic and H. Ney. 2006. Pos-based word reorder-
ings for statistical machine translation. In 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), pages 1278?1283, Genova, May.
176
Proceedings of the Third Workshop on Statistical Machine Translation, pages 127?130,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The TALP-UPC Ngram-based statistical machine translation system for
ACL-WMT 2008
Maxim Khalilov, Adolfo Hern?ndez H., Marta R. Costa-juss?,
Josep M. Crego, Carlos A. Henr?quez Q., Patrik Lambert,
Jos? A. R. Fonollosa, Jos? B. Mari?o and Rafael E. Banchs
Department of Signal Theory and Communications
TALP Research Center (UPC)
Barcelona 08034, Spain
(khalilov, adolfohh, mruiz, jmcrego, carloshq, lambert, adrian, canton, rbanchs)@gps.tsc.upc.edu
Abstract
This paper reports on the participation of the TALP
Research Center of the UPC (Universitat Polit?cnica
de Catalunya) to the ACL WMT 2008 evaluation
campaign.
This year?s system is the evolution of the one we em-
ployed for the 2007 campaign. Main updates and
extensions involve linguistically motivated word re-
ordering based on the reordering patterns technique.
In addition, this system introduces a target language
model, based on linguistic classes (Part-of-Speech),
morphology reduction for an inflectional language
(Spanish) and an improved optimization procedure.
Results obtained over the development and test sets
on Spanish to English (and the other way round)
translations for both the traditional Europarl and
a challenging News stories tasks are analyzed and
commented.
1 Introduction
Over the past few years, the Statistical Machine Transla-
tion (SMT) group of the TALP-UPC has been develop-
ing the Ngram-based SMT system (Mari?o et al, 2006).
In previous evaluation campaigns the Ngram-based ap-
proach has proved to be comparable with the state-of-
the-art phrase-based systems, as shown in Koehn and
Monz(2006), Callison-Burch et al (2007).
We present a summary of the TALP-UPC Ngram-
based SMT system used for this shared task. We dis-
cuss the system configuration and novel features, namely
linguistically motivated reordering technique, which is
applied on the decoding step. Additionally, the reorder-
ing procedure is supported by an Ngram language model
(LM) of reordered source Part-of-Speech tags (POS).
In this year?s evaluation we submitted systems for
Spanish-English and English-Spanish language pairs for
the traditional (Europarl) and challenging (News) tasks.
In each case, we used only the supplied data for each lan-
guage pair for models training and optimization.
This paper is organized as follows. Section 2 briefly
outlines the 2008 system, including tuple definition and
extraction, translation model and additional feature mod-
els, decoding tool and optimization procedure. Section 3
describes the word reordering problem and presents the
proposed technique of reordering patterns learning and
application. Later on, Section 4 reports on the experi-
mental setups of the WMT 2008 evaluation campaign. In
Section 5 we sum up the main conclusions from the pa-
per.
2 Ngram-based SMT System
Our translation system implements a log-linear model in
which a foreign language sentence fJ1 = f1, f2, ..., fJ
is translated into another language eI1 = f1, f2, ..., eI by
searching for the translation hypothesis e?I1 maximizing a
log-linear combination of several feature models (Brown
et al, 1990):
e?I1 = argmax
eI1
{ M
?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system models
and the set of ?m refers to the weights corresponding to
these models.
The core part of the system constructed in that way
is a translation model, which is based on bilingual n-
grams. It actually constitutes an Ngram-based LM of
bilingual units (called tuples), which approximates the
joint probability between the languages under consider-
ation. The procedure of tuples extraction from a word-
to-word alignment according to certain constraints is ex-
plained in detail in Mari?o et al (2006).
The Ngram-based approach differs from the phrase-
based SMT mainly by distinct representating of the bilin-
gual units defined by word alignment and using a higher
127
order HMM of the translation process. While regular
phrase-based SMT considers context only for phrase re-
ordering but not for translation, the N-gram based ap-
proach conditions translation decisions on previous trans-
lation decisions.
The TALP-UPC 2008 translation system, besides the
bilingual translation model, which consists of a 4-gram
LM of tuples with Kneser-Ney discounting (estimated
with SRI Language Modeling Toolkit1), implements a
log-linear combination of five additional feature models:
? a target language model (a 4-gram model of words,
estimated with Kneser-Ney smoothing);
? a POS target language model (a 4-gram model of
tags with Good-Turing discounting (TPOS));
? a word bonus model, which is used to compensate
the system?s preference for short output sentences;
? a source-to-target lexicon model and a target-to-
source lexicon model, these models use word-to-
word IBM Model 1 probabilities (Och and Ney,
2004) to estimate the lexical weights for each tuple
in the translation table.
Decisions on the particular LM configuration and
smoothing technique were taken on the minimal-
perplexity and maximal-BLEU bases.
The decoder (called MARIE), an open source tool2,
implementing a beam search strategy with distortion ca-
pabilities was used in the translation system.
Given the development set and references, the log-
linear combination of weights was adjusted using a sim-
plex optimization method (with the optimization criteria
of the highest BLEU score ) and an n-best re-ranking
just as described in http://www.statmt.org/jhuws/. This
strategy allows for a faster and more efficient adjustment
of model weights by means of a double-loop optimiza-
tion, which provides significant reduction of the number
of translations that should be carried out.
3 Reordering framework
For a great number of translation tasks a certain reorder-
ing strategy is required. This is especially important
when the translation is performed between pairs of lan-
guages with non-monotonic word order. There are var-
ious types of distortion models, simplifying bilingual
translation. In our system we use an extended monotone
reordering model based on automatically learned reorder-
ing rules. A detailed description can be found in Crego
and Mari?o (2006).
1http://www.speech.sri.com/projects/srilm/
2http://gps-tsc.upc.es/veu/soft/soft/marie/
Apart from that, tuples were extracted by an unfold-
ing technique: this means that the tuples are broken into
smaller tuples, and these are sequenced in the order of the
target words.
3.1 Reordering patterns
Word movements are realized according to the reordering
rewrite rules, which have the form of:
t1, ..., tn 7? i1, ..., in
where t1, ..., tn is a sequence of POS tags (relating a
sequence of source words), and i1, ..., in indicates which
order of the source words generate monotonically the tar-
get words.
Patterns are extracted in training from the crossed links
found in the word alignment, in other words, found in
translation tuples (as no word within a tuple can be linked
to a word out of it (Crego and Mari?o, 2006)).
Having all the instances of rewrite patterns, a score for
each pattern on the basis of relative frequency is calcu-
lated as shown below:
p(t1, ..., tn 7? i1, ..., in) =
N(t1, ..., tn 7? i1, ..., in)
NN(t1, ..., tn)
3.2 Search graph extension and source POS model
The monotone search graph is extended with reorderings
following the patterns found in training. Once the search
graph is built, the decoder traverses the graph looking for
the best translation. Hence, the winning hypothesis is
computed using all the available information (the whole
SMT models).
Figure 1: Search graph extension. NC, CC and AQ stand re-
spectively for name, conjunction and adjective.
The procedure identifies first the sequences of words
in the input sentence that match any available pattern.
Then, each of the matchings implies the addition of an arc
into the search graph (encoding the reordering learned in
the pattern). However, this addition of a new arc is not
128
Task BL BL+SPOS
Europarl News Europarl News
es2en 32.79 36.09 32.88 36.36
en2es 32.05 33.91 32.10 33.63
Table 1: BLEU comparison demonstrating the impact of the
source-side POS tags model.
performed if a translation unit with the same source-side
words already exists in the training. Figure 1 shows how
two rewrite rules applied over an input sentence extend
the search graph given the reordering patterns that match
the source POS tag sequence.
The reordering strategy is additionally supported by
a 4-gram language model (estimated with Good-Turing
smoothing) of reordered source POS tags (SPOS). In
training, POS tags are reordered according with the ex-
tracted reordering patterns and word-to-word links. The
resulting sequence of source POS tags is used to train the
Ngram LM.
Table 1 presents the effect of the source POS LM in-
troduction to the reordering module of the Ngram-based
SMT. As it can be seen, the impactya le h of the source-
side POS LM is minimal, however we decided to consider
the model aiming at improving it in future. The reported
results are related to the Europarl and News Commen-
tary (News) development sets. BLEU calculation is case
insensitive and insensitive to tokenization. BL (baseline)
refers to the presented Ngram-based system considering
all the features, apart from the target and source POS
models.
4 WMT 2008 Evaluation Framework
4.1 Corpus
An extraction of the official transcriptions of the 3rd re-
lease of the European Parliament Plenary Sessions3 was
provided for the ACL WMT 2008 shared translation task.
About 40 times smaller corpus from news domain (called
News Commentary) was also available. For both tasks,
our training corpus was the catenation of the Europarl and
News Commentary corpora.
TALP UPC participated in the constraint to the
provided training data track for Spanish-English and
English-Spanish translation tasks. We used the same
training material for the traditional and challenging tasks,
while the development sets used to tune the system were
distinct (2000 sentences for Europarl task and 1057
for News Commentary, one reference translation for
each of them). A brief training and development corpora
statistics is presented in Table 2.
3http://www.statmt.org/wmt08/shared-task.html
Spanish English
Train
Sentences 1.3 M 1.3 M
Words 38.2 M 35.8 K
Vocabulary 156 K 120 K
Development Europarl
Sentences 2000 2000
Words 61.8 K 58.7 K
Vocabulary 8 K 6.5 K
Development News Commentary
Sentences 1057 1057
Words 29.8 K 25.8 K
Vocabulary 5.4 K 4.9 K
Table 2: Basic statistics of ACL WMT 2008 corpus.
4.2 Processing details
The training data was preprocessed by using provided
tools for tokenizing and filtering.
POS tagging. POS information for the source and the
target languages was considered for both translation tasks
that we have participated. The software tools available
for performing POS-tagging were Freeling (Carreras et
al., 2004) for Spanish and TnT (Brants, 2000) for En-
glish. The number of classes for English is 44, while
Spanish is considered as a more inflectional language,
and the tag set contains 376 different tags.
Word Alignment. The word alignment is automati-
cally computed by using GIZA++4(Och and Ney, 2000)
in both directions, which are symmetrized by using the
union operation. Instead of aligning words themselves,
stems are used for aligning. Afterwards case sensitive
words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al were splitted into de el or
a el. As a post-processing, in the En2Es direction we
used a POS target LM as a feature (instead of the target
language model based on classes) that allowed to recover
the segmentations (de Gispert, 2006).
4.3 Experiments and Results
In contrast to the last year?s system where statistical
classes were used to train the target-side tags LM, this
year we used linguistically motivated word classes
4http://code.google.com/p/giza-pp/
129
Task BL+SPOS BL+SPOS+TPOS
(UPC 2008)
Europarl News Europarl News
es2en 32.88 36.36 32.89 36.31
en2es 31.52 34.13 30.72 32.72
en2es "clean"5 32.10 33.63 32.09 35.04
Table 3: BLEU scores for Spanish-English and English-Spanish
2008 development corpora (Europarl and News Commentary).
Task UPC 2008
Europarl News
es2en 32.80 19.61
en2es 31.31 19.28
en2es "clean"5 32.34 20.05
Table 4: BLEU scores for official tests 2008.
(POS) which were considered to train the POS target LM
and extract the reordering patterns. Other characteristics
of this year?s system are:
? reordering patterns technique;
? source POS model, supporting word reordering;
? no LM interpolation. For this year?s evaluation, we
trained two separate LMs for each domain-specific
corpus (i.e., Europarl and News Commentary tasks).
It is important to mention that 2008 training material is
identical to the one provided for the 2007 shared transla-
tion task.
Table 3 presents the BLEU score obtained for the 2008
development data sets and shows the impact of the target-
side POS LM introduction, which can be characterized as
highly corpus- and language-dependent feature. BL refers
to the same system configuration as described in subsec-
tion 3.2. The computed BLEU scores are case insensitive,
insensitive to tokenization and use one translation refer-
ence.
After submitting the systems we discovered a bug re-
lated to incorrect implementation of the target LMs of
words and tags for Spanish, it caused serious reduction
of translation quality (1.4 BLEU points for development
set in case of English-to-Spanish Europarl task and 2.3
points in case of the corresponding News Commentary
task). The last raw of table 3 (en2es "clean") repre-
sents the results corresponding to the UPC 2008 post-
evaluation system, while the previous one (en2es) refers
to the "bugged" system submitted to the evaluation.
The experiments presented in Table 4 correspond to the
2008 test evaluation sets.
5Corrected post-evaluation results (see subsection 4.3.)
5 Conclusions
In this paper we introduced the TALP UPC Ngram-based
SMT system participating in the WMT08 evaluation.
Apart from briefly summarizing the decoding and opti-
mization processes, we have presented the feature mod-
els that were taken into account, along with the bilingual
Ngram translation model. A reordering strategy based on
linguistically-motivated reordering patterns to harmonize
the source and target word order has been presented in
the framework of the Ngram-based system.
6 Acknowledgments
This work has been funded by the Spanish Government
under grant TEC2006-13964-C03 (AVIVAVOZ project).
The authors want to thank Adri? de Gispert (Cambridge
University) for his contribution to this work.
References
T. Brants. 2000. TnT ? a statistical part-of-speech tagger. In
Proceedings of the 6th Applied Natural Language Processing
(ANLP-2000).
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek,
J. D. Lafferty, R. Mercer, and P. S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational Lin-
guistics, 16(2):79?85.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine trans-
lation. In Proceedings of the ACL 2007 Workshop on Statis-
tical and Hybrid methods for Machine Translation (WMT),
pages 136?158.
X. Carreras, I. Chao, L. Padr?, and M. Padr?. 2004. Freeling:
An open-source suite of language analyzers. In Proceedings
of the 4th Int. Conf. on Language Resources and Evaluation
(LREC?04).
J. M. Crego and J. B. Mari?o. 2006. Improving statistical MT
by coupling reordering and decoding. Machine Translation,
20(3):199?215.
A. de Gispert. 2006. Introducing linguistic knowledge into
statistical machine translation. Ph.D. thesis, Universitat
Polit?cnica de Catalunya, December.
P. Koehn and C. Monz. 2006. Manual and automatic eval-
uation of machine translation between european languages.
In Proceedings of the ACL 2006 Workshop on Statistical and
Hybrid methods for Machine Translation (WMT), pages 102?
121.
J. B. Mari?o, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lam-
bert, J. A. R. Fonollosa, and M. R. Costa-juss?. 2006. N-
gram based machine translation. Computational Linguistics,
32(4):527?549, December.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the the 38th Annual Meeting
on Association for Computational Linguistics (ACL), pages
440?447.
F. Och and H. Ney. 2004. The alignment template approach to
statistical machine translation. 30(4):417 ? 449, December.
130
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 85?89,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The TALP-UPC phrase-based translation system for EACL-WMT 2009
Jos? A.R. Fonollosa and Maxim Khalilov and Marta R. Costa-juss? and
Jos? B. Mari?o and Carlos A. Henr?quez Q. and Adolfo Hern?ndez H. and
Rafael E. Banchs
TALP Research Center
Universitat Polit?cnica de Catalunya, Barcelona 08034
{adrian,khalilov,mruiz,canton,carloshq,adolfohh,rbanchs}@talp.upc.edu
Abstract
This study presents the TALP-UPC sub-
mission to the EACL Fourth Worskhop
on Statistical Machine Translation 2009
evaluation campaign. It outlines the ar-
chitecture and configuration of the 2009
phrase-based statistical machine transla-
tion (SMT) system, putting emphasis on
the major novelty of this year: combina-
tion of SMT systems implementing differ-
ent word reordering algorithms.
Traditionally, we have concentrated on
the Spanish-to-English and English-to-
Spanish News Commentary translation
tasks.
1 Introduction
TALP-UPC (Center of Speech and Language
Applications and Technology at the Universitat
Polit?cnica de Catalunya) is a permanent par-
ticipant of the ACL WMT shared translations
tasks, traditionally concentrating on the Spanish-
to-English and vice versa language pairs. In this
paper, we describe the 2009 system?s architecture
and design describing individual components and
distinguishing features of our model.
This year?s system stands aside from the
previous years? configurations which were per-
formed following an N -gram-based (tuple-based)
approach to SMT. By contrast to them, this
year we investigate the translation models (TMs)
interpolation for a state-of-the-art phrase-based
translation system. Inspired by the work pre-
sented in (Schwenk and Est?ve, 2008), we attack
this challenge using the coefficients obtained for
the corresponding monolingual language models
(LMs) for TMs interpolation.
On the second step, we have performed
additional word reordering experiments, com-
paring the results obtained with a statisti-
cal method (R. Costa-juss? and R. Fonollosa,
2009) and syntax-based algorithm (Khalilov and
R. Fonollosa, 2008). Further the outputs of
the systems were combined selecting the trans-
lation with the Minimum Bayes Risk (MBR) al-
gorithm (Kumar, 2004) that allowed significantly
outperforming the baseline configuration.
The remainder of this paper is organized as
follows: Section 2 presents the TALP-UPC?09
phrase-based system, along with the translation
models interpolation procedure and other minor
novelties of this year. Section 3 reports on the ex-
perimental setups and outlines the results of the
participation in the EACL WMT 2009 evaluation
campaign. Section 4 concludes the paper with dis-
cussions.
2 TALP-UPC phrase-based SMT
The system developed for this year?s shared
task is based on a state-of-the-art SMT sys-
tem implemented within the open-source MOSES
toolkit (Koehn et al, 2007). A phrase-based trans-
lation is considered as a three step algorithm:
(1) the source sequence of words is segmented
in phrases, (2) each phrase is translated into tar-
get language using translation table, (3) the target
phrases are reordered to be inherent in the target
language.
A bilingual phrase (which in the context of SMT
do not necessarily coincide with their linguistic
analogies) is any pair of m source words and n
target words that satisfies two basic constraints:
(1) words are consecutive along both sides of the
bilingual phrase and (2) no word on either side of
the phrase is aligned to a word outside the phrase.
Given a sentence pair and a corresponding word-
to-word alignment, phrases are extracted follow-
ing the criterion in (Och and Ney, 2004). The
probability of the phrases is estimated by relative
frequencies of their appearance in the training cor-
pus.
85
Classically, a phrase-based translation system
implements a log-linear model in which a foreign
language sentence fJ1 = f1, f2, ..., fJ is trans-
lated into another language eI1 = e1, e2, ..., eI by
searching for the translation hypothesis e?I1 maxi-
mizing a log-linear combination of several feature
models (Brown et al, 1990):
e?I1 = argmaxeI1
{ M?
m=1
?mhm(eI1, fJ1 )
}
where the feature functions hm refer to the system
models and the set of ?m refers to the weights cor-
responding to these models.
2.1 Translation models interpolation
We implemented a TM interpolation strategy fol-
lowing the ideas proposed in (Schwenk and Es-
t?ve, 2008), where the authors present a promis-
ing technique of target LMs linear interpolation;
in (Koehn and Schroeder, 2007) where a log-linear
combination of TMs is performed; and specifi-
cally in (Foster and Kuhn, 2007) where the authors
present various ways of TM combination and ana-
lyze in detail the TM domain adaptation.
In the framework of the evaluation campaign,
there were two Spanish-to-English parallel train-
ing corpora available: Europarl v.4 corpus (about
50M tokens) and News Commentary (NC) corpus
(about 2M tokens). The test dataset provided by
the organizers this year was from the news do-
main, so we considered the Europarl training cor-
pus as "out-of-domain" data and the News Com-
mentary as "in-domain" training material. Unfor-
tunately, the in-domain corpus is much smaller in
size, however the Europarl corpus can be also used
to increase the final translation and reordering ta-
bles in spite of its different nature.
A straightforward approach to the TM interpo-
lation would be an iterative TM reconstruction ad-
justing scale coefficients on each step of the loop
with use of the highest BLEU score as a maxi-
mization criterion.
However, we did not expect a significant gain
from this time-consumption strategy and we de-
cided to follow a simpler approach. In the pre-
sented results, we obtained the best interpola-
tion weight following the standard entropy-based
optimization of the target-side LM. We adjust
the weight coefficient ?Europarl (?NC = 1 ?
?Europarl) of the linear interpolation of the target-
side LMs:
P (w) = ?Europarl ? PwEuroparl + ?NC ? PwNC (1)
where PwEuroparl and PwNC are probabilities as-
signed to the word sequence w by the LM esti-
mated on Europarl and NC data, respectively.
The scale factor values are automatically opti-
mized to obtain the lowest perplexity ppl(w) pro-
duced by the interpolated LM P (w). We used the
standard script compute ? best ? mix from the
SRI LM package (Stolcke, 2002) for optimization.
On the next step, the optimized coefficients
?Europarl and ?NC are generalized on the interpo-
lated translation and reordering models. In other
words, reordering and translation models are in-
terpolated using the same weights which yield the
lowest perplexity for LM interpolation.
The word-to-word alignment was obtained from
the joint (merged) database (Europarl + NC).
Then, we separately computed the translation and
reordering tables corresponding to the in- and out-
of-domain parts of the joint alignment. The final
tables, as well as the final target LM were obtained
using linear interpolation. The weights were se-
lected using a minimum perplexity criterion esti-
mated on the corresponding interpolated combina-
tion of the target-side LMs.
The optimized coefficient values are: for Span-
ish: NC weight = 0.526, Europarl weight = 0.474;
for English: NC weight = 0.503, Europarl weight
= 0.497. The perplexity results obtained using
monolingual LMs and the 2009 development set
(English and Spanish references) can be found in
Table 1, while the corresponding improvement in
BLEU score is presented in Section 3.3 and sum-
mary of the obtained results (Table 4).
Europarl NC Interpolated
English 463.439 489.915 353.305
Spanish 308.802 347.092 246.573
Table 1: Perplexity results obtained on the Dev
2009 corpus and the monolingual LMs.
Note that the corresponding reordering models
are interpolated with the same weights.
2.2 Statistical Machine Reordering
The idea of the Statistical Machine Reordering
(SMR) stems from the idea of using the power-
ful techniques developed for SMT and to translate
86
the source language (S) into a reordered source
language (S?), which more closely matches the
order of the target language. To infer more re-
orderings, it makes use of word classes. To cor-
rectly integrate the SMT and SMR systems, both
are concatenated by using a word graph which of-
fers weighted reordering hypotheses to the SMT
system. The details are described in (?).
2.3 Syntax-based Reordering
Syntax-based Reordering (SBR) approach deals
with the word reordering problem and is based on
non-isomorphic parse subtree transfer as described
in details in (Khalilov and R. Fonollosa, 2008).
Local and long-range word reorderings are
driven by automatically extracted permutation pat-
terns operating with source language constituents.
Once the reordering patterns are extracted, they
are further applied to monotonize the bilingual
corpus in the same way as shown in the previ-
ous subsection. The target-side parse tree is con-
sidered as a filter constraining reordering rules to
the set of patterns covered both by the source- and
target-side subtrees.
2.4 System Combination
Over the past few years the MBR algorithm uti-
lization to find the best consensus outputs of dif-
ferent translation systems has proved to improve
the translation accuracy (Kumar, 2004). The sys-
tem combination is performed on the 200-best
lists which are generated by the three systems:
(1) MOSES-based system without pre-translation
monotonization (baseline), (2) MOSES-based
SMT enhanced with SMR monotonization and (3)
MOSES-based SMT augmented with SBR mono-
tonization. The results presented in Table 4 show
that the combined output significantly outperforms
the baseline system configuration.
3 Experiments and results
We followed the evaluation baseline instructions 1
to train the MOSES-based translation system.
In some experiments we used MBR decod-
ing (Kumar and Byrne, 2004) with the smoothed
BLEU score as a similarity criteria, that al-
lowed gaining 0.2 BLEU points comparing to the
standard procedure of outputting the translation
with the highest probability (HP). We applied the
Moses implementation of this algorithm to the list
1http://www.statmt.org/wmt09/baseline.html
of 200 best translations generated by the TALP-
UPC system. The results obtained over the official
2009 Test dataset can be found in Table 2.
Task HP MBR
EsEn 24.48 24.62
EnEs 23.46 23.64
Table 2: MBR versus MERT decoding.
The "recase" script provided within the base-
line was supplemented with and additional mod-
ule, which restore the original case for unknown
words (many of them are proper names and loos-
ing of case information leads to a significant per-
formance degradation).
3.1 Language models
The target-side language models were estimated
using the SRILM toolkit (Stolcke, 2002). We tried
to use all the available in-domain training mate-
rial: apart from the corresponding portions of the
bilingual NC corpora we involved the following
monolingual corpora:
? News monolingual corpus (49M tokens for
English and 49M for Spanish)
? Europarl monolingual corpus (about 504M
tokens for English and 463M for Spanish)
? A collection of News development and test
sets from previous evaluations (151K tokens
for English and 175K for Spanish)
? A collection of Europarl development and
test sets from previous evaluations (295K to-
kens for English and 311K for Spanish)
Five LMs per language were estimated on the
corresponding datasets and interpolated follow-
ing the maximum perplexity criteria. Hence, the
larger LMs incorporating in- and out-of-domain
data were used in decoding.
3.2 Spanish enclitics separation
For the Spanish portion of the corpus we imple-
mented an enclitics separation procedure on the
preprocessing step, i.e. the pronouns attached to
the verb were separated and contractions as del
or al were splitted into de el or a el. Conse-
quently, training data sparseness due to Spanish
morphology was reduced improving the perfor-
mance of the overall translation system. As a
87
post-processing, the segmentation was recovered
in the English-to-Spanish direction using target-
side Part-of-Speech tags (de Gispert, 2006).
3.3 Results
The automatic scores provided by the WMT?09
organizers for TALP-UPC submissions calculated
over the News 2009 dataset can be found in Ta-
ble 3. BLEU and NIST case-insensitive (CI) and
case-sensitive (CS) metrics are considered.
Task Bleu CI Bleu CS NIST CI NIST CS
EsEn 25.93 24.54 7.275 7.017
EnEs 24.85 23.37 6.963 6.689
Table 3: BLEU and NIST scores for preliminary
official test dataset 2009 (primary submission)
with 500 sentences excluded.
The TALP-UPC primary submission was
ranked the 3rd among 28 presented translations
for the Spanish-to-English task and the 4th for the
English-to-Spanish task among 9 systems.
The following system configurations and the in-
ternal results obtained are reported:
? Baseline: Moses-based SMT, as proposed
on the web-page of the evaluation campaign
with Spanish enclitics separation and modi-
fied version of ?recase? tool,
? Baseline+TMI: Baseline enhanced with TM
interpolation as described in subsection 2.1,
? Baseline+TMI+MBR: the same as the latter
but with MBR decoding,
? Baseline+TMI+SMR: the same as Base-
line+TMI but with SMR technique applied to
monotonize the source portion of the corpus,
as described in subsection 2.2,
? Baseline+SBR: the same as Baseline but with
SBR algorithm applied to monotonize the
source portion of the corpus, as described in
subsection 2.3,
? System Combination: a combined output of
the 3 previous systems done with the MBR
algorithm, as described in subsection 2.4.
Impact of TM interpolation and MBR decod-
ing is more significant for the English-to-Spanish
translation task, for which the target-side mono-
lingual corpus is smaller than for the Spanish-to-
English translation.
We did not have time to meet the evalua-
tion deadline for providing the system combi-
nation output. Nevertheless, during the post-
evaluation period we performed the experiments
reported in the last three lines of Table 4 (Base-
line+TMI+SMR, Baseline+SBR and System com-
bination).
Note that the results presented in Table 4 differ
from the ones which can be found the Table 3 due
to selective conditions of preliminary evaluation
done by the Shared Task organizers.
System News 2009 Test CI News 2009 Test CS
Spanish-to-English
Baseline 25.82 24.37
Baseline+TMI 25.84 24.47
Baseline+TMI+MBR (Primary) 26.04 24.62
Baseline+SMR 24.95 23.62
Baseline+SBR 24.24 22.89
System combination 26.44 25.00
English-to-Spanish
Baseline 24.56 23.05
Baseline+TMI 25.01 23.41
Baseline+TMI+MBR (Primary) 25.16 23.64
Baseline+SMR 24.09 22.65
Baseline+SBR 23.52 22.05
System combination 25.39 23.86
Table 4: Experiments summary.
88
4 Conclusions
In this paper, we present the TALP-UPC phrase-
based translation system developed for the EACL-
WMT 2009 evaluation campaign. The major nov-
elties of this year are translation models interpola-
tion done in linear way and combination of SMT
systems implementing different word reordering
algorithms. The system was ranked pretty well for
both translation tasks in which our institution has
participated.
Unfortunately, the promising reordering tech-
niques and the combination of their outputs were
not applied within the evaluation deadline, how-
ever we report the obtained results in the paper.
5 Acknowledgments
This work has been funded by the Spanish Gov-
ernment under grant TEC2006-13964-C03 (AVI-
VAVOZ project).
References
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J.D. Lafferty, R. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?
85.
A. de Gispert. 2006. Introducing linguistic knowledge
into Statistical Machine Translation. Ph.D. thesis,
Universitat Polit?cnica de Catalunya, December.
G. Foster and R. Kuhn. 2007. Mixture-model adap-
tation for SMT. In In Annual Meeting of the Asso-
ciation for Computational Linguistics: Proc. of the
Second Workshop on Statistical Machine Transla-
tion (WMT), pages 128?135, Prague, Czech Repub-
lic, June.
M. Khalilov and J. R. Fonollosa. 2008. A new subtree-
transfer approach to syntax-based reordering for sta-
tistical machine translation. Technical report, Uni-
versitat Polit?cnica de Catalunya.
Ph. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In In Annual Meeting of the Association for Compu-
tational Linguistics: Proc. of the Second Workshop
on Statistical Machine Translation (WMT), pages
224?227, Prague, Czech Republic, June.
Ph. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open-source
toolkit for statistical machine translation. In Pro-
ceedings of the Association for Computational Lin-
guistics (ACL) 2007, pages 177?180.
Sh. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In In
HLTNAACL?04, pages 169?176.
Sh. Kumar. 2004. Minimum Bayes-Risk Techniques in
Automatic Speech Recognition and Statistical Ma-
chine Translation. Ph.D. thesis, Johns Hopkins Uni-
versity.
F. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Compu-
tational Linguistics, 3(4):417?449, December.
M. R. Costa-juss? and J. R. Fonollosa. 2009. An
Ngram reordering model. Computer Speech and
Language. ISSN 0885-2308, accepted for publica-
tion.
H. Schwenk and Y. Est?ve. 2008. Data selection and
smoothing in an open-source system for the 2008
nist machine translation evaluation. In Proceedings
of the Interspeech?08, pages 2727?2730, Brisbane,
Australia, September.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proceedings of the Int. Conf.
on Spoken Language Processing, pages 901?904.
89
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 114?121,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Opinion Mining of Spanish Customer Comments with Non-Expert
Annotations on Mechanical Turk
Bart Mellebeek, Francesc Benavent, Jens Grivolla,
Joan Codina, Marta R. Costa-jussa` and Rafael Banchs
Barcelona Media Innovation Center
Av. Diagonal, 177, planta 9
08018 Barcelona, Spain
{bart.mellebeek|francesc.benavent|jens.grivolla|joan.codina|
marta.ruiz|rafael.banchs}@barcelonamedia.org
Abstract
One of the major bottlenecks in the develop-
ment of data-driven AI Systems is the cost of
reliable human annotations. The recent ad-
vent of several crowdsourcing platforms such
as Amazon?s Mechanical Turk, allowing re-
questers the access to affordable and rapid re-
sults of a global workforce, greatly facilitates
the creation of massive training data. Most
of the available studies on the effectiveness of
crowdsourcing report on English data. We use
Mechanical Turk annotations to train an Opin-
ion Mining System to classify Spanish con-
sumer comments. We design three different
Human Intelligence Task (HIT) strategies and
report high inter-annotator agreement between
non-experts and expert annotators. We evalu-
ate the advantages/drawbacks of each HIT de-
sign and show that, in our case, the use of
non-expert annotations is a viable and cost-
effective alternative to expert annotations.
1 Introduction
Obtaining reliable human annotations to train data-
driven AI systems is often an arduous and expensive
process. For this reason, crowdsourcing platforms
such as Amazon?s Mechanical Turk1, Crowdflower2
and others have recently attracted a lot of attention
from both companies and academia. Crowdsourc-
ing enables requesters to tap from a global pool of
non-experts to obtain rapid and affordable answers
to simple Human Intelligence Tasks (HITs), which
1https://www.mturk.com
2http://crowdflower.com/
can be subsequently used to train data-driven appli-
cations.
A number of recent papers on this subject point
out that non-expert annotations, if produced in a suf-
ficient quantity, can rival and even surpass the qual-
ity of expert annotations, often at a much lower cost
(Snow et al, 2008), (Su et al, 2007). However, this
possible increase in quality depends on the task at
hand and on an adequate HIT design (Kittur et al,
2008).
In this paper, we evaluate the usefulness of MTurk
annotations to train an Opinion Mining System to
detect opinionated contents (Polarity Detection) in
Spanish customer comments on car brands. Cur-
rently, a large majority of MTurk tasks is designed
for English speakers. One of our reasons for partic-
ipating in this shared task was to find out how easy
it is to obtain annotated data for Spanish. In addi-
tion, we want to find out how useful these data are
by comparing them to expert annotations and using
them as training data of an Opinion Mining System
for polarity detection.
This paper is structured as follows. Section 2 con-
tains an explanation of the task outline and our goals.
Section 3 contains a description of three different
HIT designs that we used in this task. In Section
4, we provide a detailed analysis of the retrieved
HITs and focus on geographical information of the
workers, the correlation between the different HIT
designs, the quality of the retrieved answers and on
the cost-effectiveness of the experiment. In Section
5, we evaluate the incidence of MTurk-generated an-
notations on a polarity classification task using two
different experimental settings. Finally, we conclude
114
in Section 6.
2 Task Outline and Goals
We compare different HIT design strategies by eval-
uating the usefulness of resulting Mechanical Turk
(MTurk) annotations to train an Opinion Mining
System on Spanish consumer data. More specifi-
cally, we address the following research questions:
(i) Annotation quality: how do the different
MTurk annotations compare to expert annotations?
(ii) Annotation applicability: how does the per-
formance of an Opinion Mining classifier vary after
training on different (sub)sets of MTurk and expert
annotations?
(iii) Return on Investment: how does the use of
MTurk annotations compare economically against
the use of expert annotations?
(iv) Language barriers: currently, most MTurk
tasks are designed for English speakers. How easy
is it to obtain reliable MTurk results for Spanish?
3 HIT Design
We selected a dataset of 1000 sentences contain-
ing user opinions on cars from the automotive sec-
tion of www.ciao.es (Spanish). This website was
chosen because it contains a large and varied pool
of Spanish customer comments suitable to train an
Opinion Mining System and because opinions in-
clude simultaneously global numeric and specific
ratings over particular attributes of the subject mat-
ter. Section 5.1 contains more detailed information
about the selection of the dataset. An example of a
sentence from the data set can be found in (1):
(1) ?No te lo pienses ma?s, co?mpratelo!?
(= ?Don?t think twice, buy it!?)
The sentences in the dataset were presented to
the MTurk workers in three different HIT designs.
Each HIT design contains a single sentence to be
evaluated. HIT1 is a simple categorization scheme
in which workers are asked to classify the sentence
as being either positive, negative or neutral, as is
shown in Figure 1b. HIT2 is a graded categorization
template in which workers had to assign a score be-
tween -5 (negative) and +5 (positive) to the example
sentence, as is shown in Figure 1c. Finally, HIT3 is
a continuous triangular scoring template that allows
Figure 1: An example sentence (a) and the three HIT
designs used in the experiments: (b) HIT1: a simple
categorization scheme, (c) HIT2: a graded categoriza-
tion scheme, and (d) HIT3: a continuous triangular scor-
ing scheme containing both a horizontal positive-negative
axis and a vertical subjective-objective axis.
workers to use both a horizontal positive-negative
axis and a vertical subjective-objective axis by plac-
ing the example sentence anywhere inside the trian-
gle. The subjective-objective axis expresses the de-
gree to which the sentence contains opinionated con-
tent and was earlier used by (Esuli and Sebastiani,
2006). For example, the sentence ?I think this is a
wonderful car? clearly marks an opinion and should
be positioned towards the subjective end, while the
sentence ?The car has six cilinders? should be lo-
cated towards the objective end. Figure 1d contains
an example of HIT3. In order not to burden the
workers with overly complex instructions, we did
not mention this subjective-objective axis but asked
them instead to place ambiguous sentences towards
the center of the horizontal positive-negative axis
and more objective, non-opinionated sentences to-
wards the lower neutral tip of the triangle.
115
For each of the three HIT designs, we speci-
fied the requirement of three different unique as-
signments per HIT, which led to a total amount of
3 ? 3 ? 1000 = 9000 HIT assignments being up-
loaded on MTurk. Mind that setting the requirement
of unique assigments ensures a number of unique
workers per individual HIT, but does not ensure a
consistency of workers over a single batch of 1000
HITs. This is in the line with the philosophy of
crowdsourcing, which allows many different people
to participate in the same task.
4 Annotation Task Results and Analysis
After designing the HITs, we uploaded 30 random
samples for testing purposes. These HITs were com-
pleted in a matter of seconds, mostly by workers in
India. After a brief inspection of the results, it was
obvious that most answers corresponded to random
clicks. Therefore, we decided to include a small
competence test to ensure that future workers would
possess the necessary linguistic skills to perform the
task. The test consists of six simple categorisation
questions of the type of HIT1 that a skilled worker
would be able to perform in under a minute. In order
to discourage the use of automatic translation tools,
a time limit of two minutes was imposed and most
test sentences contain idiomatic constructions that
are known to pose problems to Machine Translation
Systems.
4.1 HIT Statistics
Table 1 contains statistics on the workers who com-
pleted our HITs. A total of 19 workers passed the
competence test and submitted at least one HIT. Of
those, four workers completed HITs belonging to
two different designs and six submitted HITs in all
three designs. Twelve workers are located in the US
(64%), three in Spain (16%), one in Mexico (5%),
Equador (5%), The Netherlands (5%) and an un-
known location (5%).
As to a comparison of completion times, it took
a worker on average 11 seconds to complete an in-
stance of HIT1, and 9 seconds to complete an in-
stance of HIT2 and HIT3. At first sight, this result
might seem surprising, since conceptually there is an
increase in complexity when moving from HIT1 to
HIT2 and from HIT2 to HIT3. These results might
Overall HIT1 HIT2 HIT3
ID C % # sec. # sec. # sec.
1 mx 29.9 794 11.0 967 8.6 930 11.6
2 us 27.6 980 8.3 507 7.8 994 7.4
3 nl 11.0 85 8.3 573 10.9 333 11.4
4 us 9.5 853 16.8 - - - -
5 es 9.4 - - 579 9.1 265 8.0
6 ec 4.1 151 9.4 14 16.7 200 13.0
7 us 3.6 3 15.7 139 8.5 133 11.6
8 us 2.2 77 8.2 106 7.3 11 10.5
9 us 0.6 - - - - 50 11.2
10 us 0.5 43 5.3 1 5 - -
11 us 0.4 - - 38 25.2 - -
12 us 0.4 - - 10 9.5 27 10.8
13 es 0.4 - - - - 35 15.1
14 es 0.3 - - 30 13.5 - -
15 us 0.3 8 24.7 18 21.5 - -
16 us 0.2 - - - - 22 8.9
17 us 0.2 - - 17 16.5 - -
18 ? 0.1 6 20 - - - -
19 us 0.1 - - 1 33 - -
Table 1: Statistics on MTurk workers for all three HIT
designs: (fictional) worker ID, country code, % of total
number of HITs completed, number of HITs completed
per design and average completion time.
suggest that users find it easier to classify items
on a graded or continuous scale such as HIT2 and
HIT3, which allows for a certain degree of flexibil-
ity, than on a stricter categorical template such as
HIT1, where there is no room for error.
4.2 Annotation Distributions
In order to get an overview of distribution of the re-
sults of each HIT, a histogram was plotted for each
different task. Figure 2a shows a uniform distribu-
tion of the three categories used in the simple cat-
egorization scheme of HIT1, as could be expected
from a balanced dataset.
Figure 2b shows the distribution of the graded cat-
egorization template of HIT2. Compared to the dis-
tribution in 2a, two observations can be made: (i)
the proportion of the zero values is almost identical
to the proportion of the neutral category in Figure
2a, and (ii) the proportion of the sum of the positive
values [+1,+5] and the proportion of the sum of the
negative values [-5,-1] are equally similar to the pro-
portion of the positive and negative categories in 2a.
This suggests that in order to map the graded annota-
tions of HIT2 to the categories of HIT1, an intuitive
partitioning of the graded scale into three equal parts
should be avoided. Instead, a more adequate alterna-
tive would consist of mapping [-5,-1] to negative, 0
116
Figure 2: Overview of HIT results: a) distribution of the three categories used in HIT1, b) distribution of results in the
scaled format of HIT2, c) heat map of the distribution of results in the HIT3 triangle, d) distribution of projection of
triangle data points onto the X-axis (positive/negative).
to neutral and [+1,+5] to positive. This means that
even slightly positive/negative grades correspond to
positive/negative categories.
Figure 2c shows a heat map that plots the distri-
bution of the annotations in the triangle of HIT3. It
appears that worker annotations show a spontaneous
tendency of clustering, despite the continuous nature
of the design. This suggests that this HIT design,
originally conceived as continuous, was transformed
by the workers as a simpler categorization task using
five labels: negative, ambiguous and positive at the
top, neutral at the bottom, and other in the center.
Figure 2d shows the distribution of all data-
points in the triangle of Figure 2c, projected onto
the X-axis (positive/negative). Although similar to
the graded scale in HIT2, the distribution shows a
slightly higher polarization.
These results suggest that, out of all three HIT de-
signs, HIT2 is the one that contains the best balance
between the amount of information that can be ob-
tained and the simplicity of a one-dimensional an-
notation.
4.3 Annotation Quality
The annotation quality of MTurk workers can be
measured by comparing them to expert annotations.
This is usually done by calculating inter-annotator
agreement (ITA) scores. Note that, since a single
HIT can contain more than one assignment and each
assignment is typically performed by more than one
annotator, we can only calculate ITA scores between
batches of assignments, rather than between individ-
ual workers. Therefore, we describe the ITA scores
in terms of batches. In Table 4.4, we present a com-
parison of standard kappa3 calculations (Eugenio
and Glass, 2004) between batches of assignments in
HIT1 and expert annotations.
We found an inter-batch ITA score of 0.598,
which indicates a moderate agreement due to fairly
consistent annotations between workers. When
comparing individual batches with expert annota-
tions, we found similar ITA scores, in the range be-
tween 0.628 and 0.649. This increase with respect
to the inter-batch score suggests a higher variability
among MTurk workers than between workers and
experts. In order to filter out noise in worker annota-
tions, we applied a simple majority voting procedure
in which we selected, for each sentence in HIT1, the
most voted category. This results in an additional
3In reality, we found that fixed and free margin Kappa values
were almost identical, which reflects the balanced distribution
of the dataset.
117
batch of annotations. This batch, refered in Table
4.4 as Majority, produced a considerably higher ITA
score of 0.716, which confirms the validity of the
majority voting scheme to obtain better annotations.
In addition, we calculated ITA scores between
three expert annotators on a separate, 500-sentence
dataset, randomly selected from the same corpus as
described at the start of Section 3. This collection
was later used as test set in the experiments de-
scribed in Section 5. The inter-expert ITA scores
on this separate dataset contains values of 0.725 for
?1 and 0.729 for ?2, only marginally higher than the
Majority ITA scores. Although we are comparing
results on different data sets, these results seem to
indicate that multiple MTurk annotations are able to
produce a similar quality to expert annotations. This
might suggest that a further increase in the number
of HIT assignments would outperform expert ITA
scores, as was previously reported in (Snow et al,
2008).
4.4 Annotation Costs
As explained in Section 3, a total amount of 9000
assignments were uploaded on MTurk. At a reward
of .02$ per assignment, a total sum of 225$ (180$
+ 45$ Amazon fees) was spent on the task. Work-
ers perceived an average hourly rate of 6.5$/hour for
HIT1 and 8$/hour for HIT2 and HIT3. These fig-
ures suggest that, at least for assignments of type
HIT2 and HIT3, a lower reward/assignment might
have been considered. This would also be consis-
tent with the recommendations of (Mason and Watts,
2009), who claim that lower rewards might have an
effect on the speed at which the task will be com-
pleted - more workers will be competing for the task
at any given moment - but not on the quality. Since
we were not certain whether a large enough crowd
existed with the necessary skills to perform our task,
we explicitly decided not to try to offer the lowest
possible price.
An in-house expert annotator (working at approx-
imately 70$/hour, including overhead) finished a
batch of 1000 HIT assignments in approximately
three hours, which leads to a total expert annotator
cost of 210$. By comparing this figure to the cost
of uploading 3 ? 1000 HIT assignments (75$), we
saved 210 ? 75 = 135$, which constitutes almost
65% of the cost of an expert annotator. These figures
do not take into account the costs of preparing the
data and HIT templates, but it can be assumed that
these costs will be marginal when large data sets are
used. Moreover, most of this effort is equally needed
for preparing data for in-house annotation.
?1 ?2
Inter-batch 0.598 0.598
Batch 1 vs. Expert 0.628 0.628
Batch 2 vs. Expert 0.649 0.649
Batch 3 vs. Expert 0.626 0.626
Majority vs. Expert 0.716 0.716
Experts4 0.725 0.729
Table 2: Interannotation Agreement as a measure of qual-
ity of the annotations in HIT1. ?1 = Fixed Margin
Kappa. ?2 = Free Margin Kappa.
5 Incidence of annotations on supervised
polarity classification
This section intends to evaluate the incidence of
MTurk-generated annotations on a polarity classifi-
cation task. We present two different evaluations.
In section 5.2, we compare the results of training
a polarity classification system with noisy available
metadata and with MTurk generated annotations of
HIT1. In section 5.3, we compare the results of
training several polarity classifiers using different
training sets, comparing expert annotations to those
obtained with MTurk.
5.1 Description of datasets
As was mentioned in Section 3, all sentences were
extracted from a corpus of user opinions on cars
from the automotive section of www.ciao.es
(Spanish). For conducting the experimental evalu-
ation, the following datasets were used:
1. Baseline: constitutes the dataset used for train-
ing the baseline or reference classifiers in Ex-
periment 1. Automatic annotation for this
dataset was obtained by using the following
naive approach: those sentences extracted from
comments with ratings5 equal to 5 were as-
signed to category ?positive?, those extracted
5The corpus at www.ciao.es contains consumer opinions
marked with a score between 1 (negative) and 5 (positive).
118
from comments with ratings equal to 3 were
assigned to ?neutral?, and those extracted from
comments with ratings equal to 1 were assigned
to ?negative?. This dataset contains a total of
5570 sentences, with a vocabulary coverage of
11797 words.
2. MTurk Annotated: constitutes the dataset that
was manually annotated by MTurk workers in
HIT1. This dataset is used for training the con-
trastive classifiers which are to be compared
with the baseline system in Experiment 1. It
is also used in various ways in Experiment 2.
The three independent annotations generated
by MTurk workers for each sentence within this
dataset were consolidated into one unique an-
notation by majority voting: if the three pro-
vided annotations happened to be different6,
the sentence was assigned to category ?neutral?;
otherwise, the sentence was assigned to the cat-
egory with at least two annotation agreements.
This dataset contains a total of 1000 sentences,
with a vocabulary coverage of 3022 words.
3. Expert Annotated: this dataset contains the
same sentences as the MTurk Annotated one,
but with annotations produced internally by
known reliable annotators7. Each sentence re-
ceived one annotation, while the dataset was
split between a total of five annotators.
4. Evaluation: constitutes the gold standard used
for evaluating the performance of classifiers.
This dataset was manually annotated by three
experts in an independent manner. The gold
standard annotation was consolidated by using
the same criterion used in the case of the pre-
vious dataset8. This dataset contains a total of
500 sentences, with a vocabulary coverage of
2004 words.
6This kind of total disagreement among annotators occurred
only in 13 sentences out of 1000.
7While annotations of this kind are necessarily somewhat
subjective, these annotations are guaranteed to have been pro-
duced in good faith by competent annotators with an excellent
understanding of the Spanish language (native or near-native
speakers)
8In this case, annotator inter-agreement was above 80%, and
total disagreement among annotators occurred only in 1 sen-
tence out of 500
Baseline Annotated Evaluation
Positive 1882 341 200
Negative 1876 323 137
Neutral 1812 336 161
Totals 5570 1000 500
Table 3: Sentence-per-category distributions for baseline,
annotated and evaluation datasets.
These three datasets were constructed by ran-
domly extracting sample sentences from an origi-
nal corpus of over 25000 user comments contain-
ing more than 1000000 sentences in total. The sam-
pling was conducted with the following constraints
in mind: (i) the three resulting datasets should not
overlap, (ii) only sentences containing more than
3 tokens are considered, and (iii) each resulting
dataset must be balanced, as much as possible, in
terms of the amount of sentences per category. Table
3 presents the distribution of sentences per category
for each of the three considered datasets.
5.2 Experiment one: MTurk annotations vs.
original Ciao annotations
A simple SVM-based supervised classification ap-
proach was considered for the polarity detection task
under consideration. According to this, two dif-
ferent groups of classifiers were used: a baseline
or reference group, and a contrastive group. Clas-
sifiers within these two groups were trained with
data samples extracted from the baseline and anno-
tated datasets, respectively. Within each group of
classifiers, three different binary classification sub-
tasks were considered: positive/not positive, nega-
tive/not negative and neutral/not neutral. All trained
binary classifiers were evaluated by computing pre-
cision and recall for each considered category, as
well as overall classification accuracy, over the eval-
uation dataset.
A feature space model representation of the data
was constructed by considering the standard bag-of-
words approach. In this way, a sparse vector was ob-
tained for each sentence in the datasets. Stop-word
removal was not conducted before computing vec-
tor models, and standard normalization and TF-IDF
weighting schemes were used.
Multiple-fold cross-validation was used in all
conducted experiments to tackle with statistical vari-
119
classifier baseline annotated
positive/not positive 59.63 (3.04) 69.53 (1.70)
negative/not negative 60.09 (2.90) 63.73 (1.60)
neutral/not neutral 51.27 (2.49) 62.57 (2.08)
Table 4: Mean accuracy over 20 independent simula-
tions (with standard deviations provided in parenthesis)
for each classification subtasks trained with either the
baseline or the annotated dataset.
ability of the data. In this sense, twenty independent
realizations were actually conducted for each exper-
iment presented and, instead of individual output re-
sults, mean values and standard deviations of evalu-
ation metrics are reported.
Each binary classifier realization was trained with
a random subsample set of 600 sentences extracted
from the training dataset corresponding to the clas-
sifier group, i.e. baseline dataset for reference sys-
tems, and annotated dataset for contrastive systems.
Training subsample sets were always balanced with
respect to the original three categories: ?positive?,
?negative? and ?neutral?.
Table 4 presents the resulting mean values of
accuracy for each considered subtask in classifiers
trained with either the baseline or the annotated
dataset. As observed in the table, all subtasks ben-
efit from using the annotated dataset for training
the classifiers; however, it is important to mention
that while similar absolute gains are observed for
the ?positive/not positive? and ?neutral/not neutral?
subtasks, this is not the case for the subtask ?neg-
ative/not negative?, which actually gains much less
than the other two subtasks.
After considering all evaluation metrics, the bene-
fit provided by human-annotated data availability for
categories ?neutral? and ?positive? is evident. How-
ever, in the case of category ?negative?, although
some gain is also observed, the benefit of human-
annotated data does not seem to be as much as for
the two other categories. This, along with the fact
that the ?negative/not negative? subtask is actually
the best performing one (in terms of accuracy) when
baseline training data is used, might suggest that
low rating comments contains a better representa-
tion of sentences belonging to category ?negative?
than medium and high rating comments do with re-
spect to classes ?neutral? and ?positive?.
In any case, this experimental work only verifies
the feasibility of constructing training datasets for
opinionated content analysis, as well as it provides
an approximated idea of costs involved in the gener-
ation of this type of resources, by using MTurk.
5.3 Experiment two: MTurk annotations vs.
expert annotations
In this section, we compare the results of training
several polarity classifiers on six different training
sets, each of them generated from the MTurk anno-
tations of HIT1. The different training sets are: (i)
the original dataset of 1000 sentences annotated by
experts (Experts), (ii) the first set of 1000 MTurk re-
sults (Batch1), (iii) the second set of 1000 MTurk
results (Batch2), (iv) the third set of 1000 MTurk
results (Batch3), (v) the batch obtained by major-
ity voting between Batch1, Batch2 and Batch3 (Ma-
jority), and (vi) a batch of 3000 training instances
obtained by aggregating Batch1, Batch2 and Batch3
(All). We used classifiers as implemented in Mal-
let (McCallum, 2002) and Weka (Hall et al, 2009),
based on a simple bag-of-words representation of
the sentences. As the objective was not to obtain
optimum performance but only to evaluate the dif-
ferences between different sets of annotations, all
classifiers were used with their default settings.
Table 5 contains results of four different clas-
sifiers (Maxent, C45, Winnow and SVM), trained
on these six different datasets and evaluated on the
same 500-sentence test set as explained in Section
5.1. Classification using expert annotations usu-
ally outperforms classification using a single batch
(one annotation per sentence) of annotations pro-
duced using MTurk. Using the tree annotations per
sentence available from MTurk, all classifiers reach
similar or better performance compared to the sin-
gle set of expert annotations, at a much lower cost
(as explained in section 4.4).
It is interesting to note that most classifiers bene-
fit from using the full 3000 training examples (1000
sentences with 3 annotations each), which intu-
itively makes sense as the unanimously labeled ex-
amples will have more weight in defining the model
of the corresponding class, whereas ambiguous or
unclear cases will have their impact reduced as their
characteristics are attributed to various classes.
On the contrary, Support Vector Machines show
120
System
E
xp
er
ts
B
at
ch
1
B
at
ch
2
B
at
ch
3
M
aj
or
it
y
A
ll
Winnow 44.2 43.6 40.4 47.6 46.2 50.6
SVM 57.6 53.0 55.4 54.0 57.2 52.8
C45 42.2 33.6 42.0 41.2 41.6 45.0
Maxent 59.2 55.8 57.6 54.0 57.6 58.6
Table 5: Accuracy figures of four different classifiers
(Winnow, SVM, C45 and Maxent) trained on six different
datasets (see text for details).
an important drop in performance when using mul-
tiple annotations, but perform well when using the
majority vote. As a first intuition, this may be due to
the fact that SVMs focus on detecting class bound-
aries (and optimizing the margin between classes)
rather than developing a model of each class. As
such, having the same data point appear several
times with the same label will not aid in finding ap-
propriate support vectors, whereas having the same
data point with conflicting labels may have a nega-
tive impact on the margin maximization.
Having only evaluated each classifier (and train-
ing set) once on a static test set it is unfortunately not
possible to reliably infer the significance of the per-
formance differences (or determine confidence in-
tervals, etc.). For a more in-depth analysis it might
be interesting to use bootstrapping or similar tech-
niques to evaluate the robustness of the results.
6 Conclusions
In this paper we have examined the usefulness of
non-expert annotations on Amazon?s Mechanical
Turk to annotate the polarity of Spanish consumer
comments. We discussed the advantages/drawbacks
of three different HIT designs, ranging from a sim-
ple categorization scheme to a continous scoring
template. We report high inter-annotator agree-
ment scores between non-experts and expert anno-
tators and show that training an Opinion Mining
System with non-expert MTurk annotations outper-
forms original noisy annotations and obtains com-
petitive results when compared to expert annotations
using a variety of classifiers. In conclusion, we
found that, in our case, the use of non-expert anno-
tations through crowdsourcing is a viable and cost-
effective alternative to the use of expert annotations.
In the classification experiments reported in this
paper, we have relied exclusively on MTurk anno-
tations from HIT1. Further work is needed to fully
analyze the impact of each of the HIT designs for
Opinion Mining tasks. We hope that the added rich-
ness of annotation of HIT2 and HIT3 will enable us
to use more sophisticated classification methods.
References
A. Esuli and F. Sebastiani. 2006. SentiWordNet: a pub-
licly available lexical resource for opinion mining. In
Proceedings of LREC, volume 6.
B. D Eugenio and M. Glass. 2004. The kappa statistic: A
second look. Computational linguistics, 30(1):95101.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
A. Kittur, E. H Chi, and B. Suh. 2008. Crowdsourcing
user studies with mechanical turk.
W. Mason and D. J Watts. 2009. Financial incentives
and the performance of crowds. In Proceedings of
the ACM SIGKDD Workshop on Human Computation,
pages 77?85.
A. K. McCallum. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y Ng. 2008.
Cheap and fastbut is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263.
Q. Su, D. Pavlov, J. H Chow, and W. C Baker. 2007.
Internet-scale collection of human-reviewed data. In
Proceedings of the 16th international conference on
World Wide Web, pages 231?240.
121
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 98?102,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Using collocation segmentation to augment the phrase table
Carlos A. Henr?quez Q.
?
, Marta R. Costa-juss?
?
, Vidas Daudaravicius?
Rafael E. Banchs
?
, Jos? B. Mari?o
?
?
TALP Research Center, Universitat Polit?cnica de Catalunya, Barcelona, Spain
{carlos.henriquez,jose.marino}@upc.edu
?
Barcelona Media Innovation Center, Barcelona, Spain
{marta.ruiz,rafael.banchs}@barcelonamedia.org
?Faculty of Informatics, Vytautas Magnus University, Kaunas, Lithuania
vidas@donelaitis.vdu.lt
Abstract
This paper describes the 2010 phrase-based
statistical machine translation system de-
veloped at the TALP Research Center of
the UPC
1
in cooperation with BMIC
2
and
VMU
3
. In phrase-based SMT, the phrase
table is the main tool in translation. It is
created extracting phrases from an aligned
parallel corpus and then computing trans-
lation model scores with them. Performing
a collocation segmentation over the source
and target corpus before the alignment
causes that different and larger phrases
are extracted from the same original doc-
uments. We performed this segmentation
and used the union of this phrase set with
the phrase set extracted from the non-
segmented corpus to compute the phrase
table. We present the configurations con-
sidered and also report results obtained
with internal and official test sets.
1 Introduction
The TALP Research Center of the UPC
1
in coop-
eration with BMIC
2
and VMU
3
participated in the
Spanish-to-English WMT task. Our primary sub-
mission was a phrase-based SMT system enhanced
with POS tags and our contrastive submission was
an augmented phrase-based system using colloca-
tion segmentation (Costa-juss? et al, 2010), which
mainly is a way of introducing new phrases in the
translation table. This paper presents the descrip-
tion of both systems together with the results that
we obtained in the evaluation task and is organized
as follows: first, Section 2 and 3 present a brief de-
scription of a phrase-based SMT, followed by a gen-
eral explanation of collocation segmentation. Sec-
tion 4 presents the experimental framework, corpus
used and a description of the different systems built
for the translation task; the section ends showing
the results we obtained over the official test set. Fi-
nally, section 5 presents the conclusions obtained
from the experiments.
1
Universitat Polit?cnica de Catalunya
2
Barcelona Media Innovation Center
3
Vytautas Magnus University
2 Phrase-based SMT
This approach to SMT performs the translation
splitting the source sentence in segments and as-
signing to each segment a bilingual phrase from
a phrase-table. Bilingual phrases are translation
units that contain source words and target words,
e.g. < unidad de traduccio?n | translation unit >,
and have different scores associated to them. These
bilingual phrases are then sorted in order to max-
imize a linear combination of feature functions.
Such strategy is known as the log-linear model
(Och and Ney, 2003) and it is formally defined as:
e? = arg max
e
[
M?
m=1
?mhm (e, f)
]
(1)
where hm are different feature functions with
weights ?m. The two main feature functions
are the translation model (TM) and the target
language model (LM). Additional models include
POS target language models, lexical weights, word
penalty and reordering models among others.
3 Collocation segmentation
Collocation segmentation is the process of de-
tecting boundaries between collocation segments
within a text (Daudaravicius and Marcinkeviciene,
2004). A collocation segment is a piece of text be-
tween boundaries. The boundaries are established
in two steps using two different measures: the Dice
score and a Average Minimum Law (AML).
The Dice score is used to measure the associa-
tion strength between two words. It has been used
before in the collocation compiler XTract (Smadja,
1993) and in the lexicon extraction system Cham-
pollion (Smadja et al, 1996). It is defined as fol-
lows:
Dice (x; y) =
2f (x, y)
f (x) + f (y)
(2)
where f (x, y) is the frequency of co-occurrence of
x and y, and f (x) and f (y) the frequencies of
occurrence of x and y anywhere in the text. It gives
high scores when x and y occur in conjunction.
The first step then establishes a boundary between
98
two adjacent words when the Dice score is lower
than a threshold t = exp (?8). Such a threshold
was established following the results obtained in
(Costa-juss? et al, 2010), where an integration of
this technique and a SMT system was performed
over the Bible corpus.
The second step of the procedure uses the AML.
It defines a boundary between words xi?1 and xi
when:
Dice (xi?2;xi?1) +Dice (xi;xi+1)
2
> Dice (xi?1;xi)
(3)
That is, the boundary is set when the Dice value
between words xi and xi?1 is lower than the aver-
age of preceding and following values.
4 Experimental Framework
All systems were built using Moses (Koehn et al,
2007), a state-of-the-art software for phrase-based
SMT. For preprocessing Spanish, we used Freeling
(Atserias et al, 2006), an open source library of
natural language analyzers. For English, we used
TnT (Brants, 2000) and Moses' tokenizer. The
language models were built using SRILM (Stolcke,
2002).
4.1 Corpus
This year, the translation task provided four dif-
ferent sources to collect corpora for the Spanish-
English pair. Bilingual corpora included version 5
of the Europarl Corpus (Koehn, 2005), the News
Commentary corpus and the United Nations cor-
pus. Additional English corpora was available from
the News corpus. The organizers also allowed the
use of the English Gigaword Third and Fourth Edi-
tion, released by the LDC. As for development
and internal test, the test sets from 2008 and 2009
translation tasks were available.
For our experiments, we selected as training data
the union of the Europarl and the News Commen-
tary. Development was performed with a section
of the 2008 test set and the 2009 test set was se-
lected as internal test. We deleted all empty lines,
removed pairs that were longer than 40 words, ei-
ther in Spanish or English; and also removed pairs
whose ratio between number of words were bigger
than 3.
As a preprocess, all corpora were lower-cased
and tokenized. The Spanish corpus was tokenized
and POS tags were extracted using Freeling, which
split clitics from verbs and also separated words
like del into de el. In order to build a POS tar-
get language model, we also obtained POS tags
from the English corpus using the TnT tagger.
Statistics of the selected corpus can be seen in Ta-
ble 1.
Corpora Spanish English
Training sent 1, 180, 623 1, 180, 623
Running words 26, 454, 280 25, 291, 370
Vocabulary 118, 073 89, 248
Development sent 1, 729 1, 729
Running words 37, 092 34, 774
Vocabulary 7, 025 6, 199
Internal test sent 2, 525 2, 525
Running words 69, 565 65, 595
Vocabulary 10, 539 8, 907
Official test sent 2, 489 -
Running words 66, 714 -
Vocabulary 10, 725 -
Table 1: Statistics for the training, development
and test sets.
Internal test Official test
Adjectives 137 72
Common nouns 369 188
Proper nouns 408 2, 106
Verbs 213 128
Others 119 168
Total 1246 2662
Table 2: Unknown words found in internal and
official test sets
It is important to notice that neither the United
Nations nor the Gigaword corpus were used for
bilingual training. Nevertheless, the English part
from the United Nations and the monolingual
News corpus were used to build the language model
of our systems.
4.1.1 Unknown words
We analyzed the content from the internal and of-
ficial test and realized that they both contained
many words that were not seen in the training data.
Table 2 shows the number of unknown words found
in both sets, classified according to their POS.
In average, we may expect an unknown word
every two sentences in the internal test and more
than one per sentence in the official test set. It can
also be seen that most of those unknown words are
proper nouns, representing 32% and 79% of the
unknown sets, respectively. Common nouns were
the second most frequent type of unknown words,
followed by verbs and adjectives.
4.2 Systems
We submitted two different systems for the trans-
lation task. First a baseline using the training data
mentioned before; and then an augmented system,
where the baseline-extracted phrase list was ex-
tended with additional phrases coming from a seg-
mented version of the training corpus.
We also considered an additional system built
99
with two different decoding path, a standard path
from words to words and POS and an alternative
path from stems to words and POS in the target
side. At the end, we did not submit this system
to the translation task because it did not provide
better results than the previous two in our internal
test.
The set of feature functions used include: source-
to-target and target-to-source relative frequen-
cies, source-to-target and target-to-source lexical
weights, word and phrase penalties, a target lan-
guage model, a POS target language model, and a
lexicalized reordering model (Tillman, 2004).
4.2.1 Considering stems as an alternate
decoding path.
Using Moses' framework for factored translation
models we defined a system with two decoding
paths: one decoding path using words and the
other decoding path using stems in the source lan-
guage and words in the target language. Both de-
coding paths only had a single translation step.
The possibility of using multiple alternative decod-
ing path was developed by Birch et. al. (2007).
This system tried to solve the problem with the
unknown words. Because Spanish is morphologi-
cally richer than English, this alternative decoding
path allowed the decoder translate words that were
not seen in the training data and shared the same
root with other known words.
4.2.2 Expanding the phrase table using
collocation segmentation.
In order to build the augmented phrase table with
the technique mentioned in section 3, we seg-
mented each language of the bilingual corpus in-
dependently and then, using the collocation seg-
ments as words, we aligned the corpus and ex-
tracted the phrases from it. Once the phrases were
extracted, the segments of each phrase were split
again in words to have standard phrases. Finally,
we use the union of this phrases and the phrases
extracted from the baseline system to compute the
final phrase table. A diagram of the whole proce-
dure can be seen in figure 1.
The objective of this integration is to add new
phrases in the translation table and to enhance
the relative frequency of the phrases that were ex-
tracted from both methods.
4.2.3 Language model interpolation.
Because SMT systems are trained with a bilingual
corpus, they ended highly tied to the domain the
corpus belong to. Therefore, when the documents
we want to translate belong to a different domain,
additional domain adaptation techniques are rec-
ommended to build the system. Those techniques
usually employ additional corpora that correspond
to the domain we want to translate from.
internal test
baseline 24.25
baseline+stem 23.45
augmented 23.9
Table 3: Internal test results.
test testcased?detok
baseline 26.1 25.1
augmented 26.1 25.1
Table 4: Results from translation task
The test set for this translation task comes from
the news domain, but most of our bilingual cor-
pora belonged to a political domain, the Europarl.
Therefore we use the additional monolingual cor-
pus to adapt the language model to the news do-
main.
The strategy used followed the experiment per-
formed last year in (R. Fonollosa et al, 2009).
We used SRILM during the whole process. All
language models were order five and used modi-
fied Kneser-Ney discount and interpolation. First,
we build three different language models accord-
ing to their domain: Europarl, United Nations and
news; then, we obtained the perplexity of each lan-
guage model over the News Commentary develop-
ment corpus; next, we used compute-best-mix to
obtain weights for each language model that di-
minish the global perplexity. Finally, the models
were combined using those weights.
In our experiments all systems used the resulting
language model, therefore the difference obtained
in our results were cause only by the translation
model.
4.3 Results
We present results from the three systems devel-
oped this year. First, the baseline, which included
all the features mentioned in section 4.2; then, the
system with an alternative decoding path, called
baseline+stem; and finally the augmented system,
which integrated collocation segmentation to the
baseline. Internal test results can be seen in table
3. Automatic scores provided by the WMT 2010
organizers for the official test can be found in ta-
ble 4. All BLEU scores are case-insensitive and
tokenized except for the official test set which also
contains case-sensitive and non-tokenized score.
We obtained a BLEU score of 26.1 and 25.1 for
our case-insensitive and sensitive outputs, respec-
tively. The highest score was obtained by Uni-
versity of Cambridge, with 30.5 and 29.1 BLEU
points.
100
Figure 1: Example of the expansion of the phrase table using collocation segmentation. New phrases
added by the collocation-based system are marked with a ??.
4.3.1 Comparing systems
Once we obtained the translation outputs from the
baseline and the augmented system, we performed
a manual comparison of them. Even though we
did not find any significant advantages of the aug-
mented system over the baseline, the collocation
segmentation strategy chose a better morphologi-
cal structures in some cases as can be seen in Table
5 (only sentence sub-segments are shown):
5 Conclusion
We presented two different submissions for the
Spanish-English language pair. The language
model for both system was built interpolating two
big out-of-domain language models and one smaller
in-domain language model. The first system was a
baseline with POS target language model; and the
second one an augmented system, that integrates
the baseline with collocation segmentation. Re-
sults over the official test set showed no difference
in BLEU between these two, even though internal
results showed that the baseline obtained a better
score.
We also considered adding an additional decod-
ing path from stems to words in the baseline but
internal tests showed that it did not improve trans-
lation quality either. The high number of unknown
words found in Spanish suggested us that consider-
ing in parallel the simple form of stems could help
us achieve better results. Nevertheless, a deeper
study of the unknown set showed us that most
of those words were proper nouns, which do not
have inflection and therefore cannot benefited from
stems.
Finally, despite that internal test did not showed
an improvement with the augmented system, we
submitted it as a secondary run looking for the
effect these phrases could have over human evalu-
ation.
Acknowledgment
The research leading to these results has received
funding from the European Community's Seventh
Framework Programme (FP7/2007-2013) under
grant agreement number 247762, from the Span-
ish Ministry of Science and Innovation through the
Buceador project (TEC2009-14094-C04-01) and
the Juan de la Cierva fellowship program. The
authors also wants to thank the Barcelona Media
Innovation Centre for its support and permission
to publish this research.
References
Jordi Atserias, Bernardino Casas, Elisabet
Comelles, Meritxell Gonz?lez, Llu?s Padr?, and
Muntsa Padr?. 2006. FreeLing 1.3: Syntactic
and semantic services in an open-source NLP
101
Original: sabiendo que est? recibiendo el premio
Baseline: knowing that it receive the prize
Augmented: knowing that he is receiving the prize
Original: muchos de mis amigos prefieren no separarla.
Baseline: many of my friends prefer not to separate them.
Augmented: many of my friends prefer not to separate it.
Original: Los estadounidenses contar?n con un tel?fono m?vil
Baseline: The Americans have a mobile phone
Augmented: The Americans will have a mobile phone
Original: es plenamente consciente del camino m?s largo que debe emprender
Baseline: is fully aware of the longest journey must undertake
Augmented: is fully aware of the longest journey that need to be taken
Table 5: Comparison between baseline and augmented outputs
library. In Proceedings of the fifth interna-
tional conference on Language Resources and
Evaluation (LREC 2006), ELRA, Genoa, Italy,
May.
Alexandra Birch, Miles Osborne, and Philipp
Koehn. 2007. Ccg supertags in factored statis-
tical machine translation. In StatMT '07: Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, pages 916, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Thorsten Brants. 2000. TnT  a statistical part-
of-speech tagger. In Proceedings of the Sixth
Applied Natural Language Processing (ANLP-
2000), Seattle, WA.
Marta R. Costa-juss?, Vidas Daudaravicius, and
Rafael E. Banchs. 2010. Integration of statisti-
cal collocation segmentations in a phrase-based
statistical machine translation system. In 14th
Annual Conference of the European Association
for Machine Translation.
Vidas Daudaravicius and Ruta Marcinkeviciene.
2004. Gravity counts for the boundaries of col-
locations. International Journal of Corpus Lin-
guistics, 9:321348(28).
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In ACL '07: Proceedings of
the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions, pages
177180, Morristown, NJ, USA. Association for
Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Machine
Translation Summit.
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Align-
ment Models. Computational Linguistics, 29:19
51.
Jos? A. R. Fonollosa, Maxim Khalilov, Marta R.
Costa-juss?, Jos? B. Mari?o, Carlos A. Hen-
r?quez Q., Adolfo Hern?ndez H., and Rafael E.
Banchs. 2009. The TALP-UPC phrase-based
translation system for EACL-WMT 2009. In
Proceedings of the Fourth Workshop on Statis-
tical Machine Translation, pages 8589, Athens,
Greece, March. Association for Computational
Linguistics.
Frank A. Smadja, Kathleen McKeown, and
Vasileios Hatzivassiloglou. 1996. Translating
collocations for bilingual lexicons: A statistical
approach. Computational Linguistics, 22(1):1
38.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Comput. Linguist., 19(1):143177.
Andreas Stolcke. 2002. SRILM  an extensible
language modeling toolkit. pages 901904.
Christoph Tillman. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In
HLT-NAACL.
102
Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 126?134,
ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational Linguistics
A Semantic Feature for Statistical Machine Translation 
 
 
Rafael E. Banchs Marta R. Costa-juss? 
Institute for Infocomm Research Barcelona Media Innovation Centre 
1 Fusionopolis Way, 21-01, Singapore 138632 Av. Diagonal 177, planta 9, 08018 Barcelona 
rembanchs@i2r.a-star.edu.sg marta.ruiz@barcelonamedia.org 
 
 
 
 
Abstract 
A semantic feature for statistical machine trans-
lation, based on Latent Semantic Indexing, is 
proposed and evaluated. The objective of the 
proposed feature is to account for the degree of 
similarity between a given input sentence and 
each individual sentence in the training dataset. 
This similarity is computed in a reduced vector-
space constructed by means of the Latent Se-
mantic Indexing decomposition. The computed 
similarity values are used as an additional fea-
ture in the log-linear model combination ap-
proach to statistical machine translation. In our 
implementation, the proposed feature is dy-
namically adjusted for each translation unit in 
the translation table according to the current in-
put sentence to be translated. This model aims 
at favoring those translation units that were ex-
tracted from training sentences that are seman-
tically related to the current input sentence 
being translated. Experimental results on a 
Spanish-to-English translation task on the Bible 
corpus demonstrate a significant improvement 
on translation quality with respect to a baseline 
system. 
1 Introduction  
In recent years, the statistical approach to machine 
translation has gained a lot of attention from both 
the scientific and the commercial perspective. This 
has basically been a consequence of the increasing 
availability of bilingual training material as well as 
the increasing storage and processing capabilities 
of current computational systems, which have al-
lowed for the construction of machine translation 
systems with general-public acceptance quality. 
For several reasons, the most prominent statisti-
cal machine translation paradigm currently used is 
the phrase-based approach (Koehn et al, 2003), 
which has been derived from the IBM?s word-
based approach originally proposed in the early 
90?s (Brown et al, 1993). This original approach 
was heavily rooted on the noisy-channel model 
framework, which, in our view, continues to play 
an important role in the fundamental conception of 
current statistical machine translation. 
While one of the major assumptions of the 
noisy-channel model approach is the independence 
between decoding and source language probabili-
ties, there exists strong evidence on the important 
role played by source language structure and con-
text within the task of human translation (Padilla & 
Bajo, 1998). In this sense, the inability of main-
stream statistical machine translation to tackle with 
source-context information in a reliable way has 
been already recognized as a major drawback of 
the statistical approach, whereas the use of source-
context information has been proven to be effec-
tive in the case of example-based machine transla-
tion (Carl & Way, 2003). In this regard, attempts 
for incorporating source-context information into 
the phrase-based machine translation framework 
have been already reported (Carpuat & Wu, 2007; 
Carpuat & Wu, 2008; Haque et al, 2009; Espa?a-
Bonet et al, 2009; Haque et al, 2010; Costa-juss? 
& Banchs, 2010). However, as far as we know, no 
transcendental improvements in performance have 
been achieved or, at least, reported yet. 
In this work, we elaborate deeper on the ideas 
we have recently presented and discussed in Costa-
juss? & Banchs (2010), where we used a similarity 
metric between the source sentence to be translated 
and all the sentences in the training set as an addi-
126
tional feature in the log-linear combination (Och & 
Ney, 2002) of models of a phrase-based translation 
system. Such a feature, which is dynamic in the 
sense that depends on the input sentence to be 
translated, is intended to favor those translation 
units which were extracted from training sentences 
that are similar to the current input sentence over 
those translation units which were extracted from 
different or unrelated sentences. Different from our 
original methodology, where sentence similarities 
were assessed over a term-document matrix repre-
sentation for words and statistical classes of words, 
here we compute sentence similarities in a low-
dimensional vector space constructed by means of 
Latent Semantic Indexing (Landauer et al, 1998). 
The rest of the paper is organized as follows. 
Section 2 presents an overview of some recent ap-
proaches attempting to introduce source-context 
information into the statistical machine translation 
framework. Then, section 3 introduces the meth-
odology that is proposed and evaluated in this 
work, and section 4 focuses on some implementa-
tion issues. Section 5 describes the experimental 
settings and results. Section 6 presents a manual 
evaluation of a selected sample of system transla-
tions and discusses the most relevant findings and 
observations. Finally, section 7 presents the most 
relevant conclusions of this work and provides 
guidelines for further research in this area. 
2 Related Work  
Several attempts for incorporating source-context 
information into the statistical machine translation 
framework have been reported in the literature dur-
ing the last few years. Without attempting to be 
comprehensive, we provide a brief overlook of 
some of the most sounded recent works within this 
area which are relevant to the phrase-based statisti-
cal machine translation approach. For a more com-
prehensive review of the state-of-the-art, the reader 
can refer to Haque et al (2010). 
On the one hand, there are some semantic ap-
proaches. In Carpuat & Wu (2007), for instance, 
word sense disambiguation techniques are intro-
duced into statistical machine translation; and in 
Carpuat & Wu (2008), dynamically-built context-
dependant phrasal translation lexicons are shown 
to be more useful for phrase-based machine trans-
lation than conventional static phrasal translation 
lexicons, which ignore all contextual information. 
On the other hand, there are approaches which 
use machine learning techniques. In Haque et al 
(2009), different syntactic and lexical features are 
proposed for incorporating information about the 
neighbouring words; and in Espa?a-Bonet et al 
(2009), local classifiers are trained, using linguistic 
and context information, to translate a phrase. 
Finally, our recent approach, which is inspired 
on information retrieval techniques for measuring 
the source-context similarity between the input 
sentence to be translated and the original training 
material, was presented in Costa-juss? & Banchs 
(2010). As our present methodology is closely re-
lated to this approach, more details are provided in 
the following section. 
3 Proposed Methodology  
As already mentioned, the methodology proposed 
and evaluated in this work is based on the source-
context similarity approach we presented in Costa-
juss? & Banchs (2010). Different from that work, 
here we introduce the use Latent Semantic Index-
ing (Landauer et al, 1998) to construct a vector-
space model representation of the data collection in 
a reduced-dimensionality space before computing 
source sentence similarities. First, in subsection 
3.1, we review the source-context similarity ap-
proach. Then, in subsection 3.2 we present the ba-
sics of Latent Semantic Indexing.  
3.1 The Source-Context Similarity Approach 
The method we proposed in Costa-juss? & Banchs 
(2010) introduces and extended concept of transla-
tion unit or phrase by defining a tuple of three ele-
ments: phrase-source-side, phrase-target-side, and 
source-context: 
 
TU = {PSS ||| PTS ||| SC} . (1) 
    
In the most simplistic approach, the source-
context element of a given translation unit can be 
approximated by the complete source sentence the 
translation unit was originally extracted from. To 
illustrate this point, consider the following conven-
tional translation unit {vino|||wine} which has been 
extracted from the training sentence sus ojos est?n 
brillantes por el vino y sus dientes blancos por la 
leche (his eyes shall be red with wine and his teeth 
white with milk). According to (1), the extended 
translation unit TU is defined as {vino|||wine|||sus 
127
ojos est?n brillantes por el vino y sus dientes blan-
cos por la leche}. Notice that, from this definition, 
identical source-target phrase pairs that have been 
extracted from different training sentences are re-
garded as different translation units! 
According to this definition, the relatedness of 
contexts between any translation unit and an input 
sentence to be translated can be computed by 
means of some distance or similarity metric over a 
semantic space representation for sentences. This 
idea is implemented in practice by means of the 
following dynamic feature function: 
 
F(TU,IN) = SIM(TU,IN) = SIM(SC,IN) , (2) 
 
where TU refers to a given translation unit, IN re-
fers to the input sentence to be translated, SC refers 
to the source-context component of translation unit 
TU (which in our implementation is the source 
training sentence which the translation unit was 
extracted from), and SIM is a similarity metric over 
a given model space.  
As implied in (2), the source-context feature to 
be implemented consists of a similarity measure-
ment between the input sentence to be translated 
IN and the source-context component SC of the 
available translation units.  
In Costa-juss? & Banchs (2010), we used the 
cosine of the angle between vectors in a term-
sentence matrix representation (Salton et al, 1975) 
for computing the source-context similarity feature 
described in (2). In this work, we use Latent Se-
mantic Indexing (Landauer et al, 1998) for pro-
jecting the term-sentence matrix representation 
into a low-dimensional space and use the cosine of 
the angle between vectors in the resulting reduced 
space for computing the source-context similarity 
feature. With this, we expect to reduce the noise 
resulting from data sparseness problems in the 
original full-dimensional representation. 
To better illustrate the concepts discussed here, 
let us consider the Spanish word vino and the cor-
responding English translations for its two senses: 
wine and came. Both translations can be automati-
cally inferred from training data; and Table 1 illus-
trates the resulting probability values derived for 
both senses of the Spanish word vino from the ac-
tual training dataset used in this work (a detailed 
description of the dataset is given in section 5).  
Notice from the table, how in general the most 
probable sense of vino in our considered dataset is 
wine. This actually happens because the English 
word wine is always related to the Spanish word 
vino, whereas the English word came can refer to 
many different inflections of the same Spanish 
word: vine, viniste, vino, vinimos, vinieron, etc. 
 
phrase ?(f|e) lex(f|e) ?(e|f) lex(e|f) 
{vino|||wine} 0.665198 0.721612 0.273551 0.329431
{vino|||came} 0.253568 0.131398 0.418478 0.446488
 
Table 1: Actual probability values for the two pos-
sible translations of the Spanish word vino. 
 
The idea of the proposed source-context feature 
is to use the contextual similarity between the input 
sentence to be translated and the sentences in the 
training dataset as an additional source of informa-
tion that should be helpful during decoding.  
Consider for instance the following two sen-
tences corresponding to the wine sense of vino:  
 
SC1: No hab?is comido pan ni tomado vino ni licor , para que se-
p?is que yo soy Jehovah vuestro Dios . (Ye have not eaten bread , 
neither have ye drunk wine or strong drink : that ye might know 
that I am the Lord your God .) 
 
SC2: Cuando fue divulgada esta orden , los hijos de Israel dieron 
muchas primicias de grano , vino nuevo , aceite , miel y de todos 
los frutos de la tierra . (And as soon as the commandment came 
abroad , the children of Israel brought in abundance the firstfruits 
of corn , wine , and oil , and honey , and of all the increase of the 
field .) 
 
and the following two sentences corresponding to 
the came sense of vino: 
 
SC3: Al tercer d?a vino Jeroboam con todo el pueblo a Roboam , 
como el rey hab?a hablado diciendo : Volved a m? al tercer d?a . 
(So Jeroboam and all the people came to Rehoboam the third day , 
as the king had appointed , saying , Come to me again the third 
day .) 
 
SC4: Ella vino y ha estado desde la ma?ana hasta ahora . No ha 
vuelto a casa ni por un momento . (She came , and hath continued 
even from the morning until now , that she tarried a little in the 
house .) 
 
As the context for a given word is generally de-
termined by its surrounding words, we should be 
able to infer the correct sense for the word vino in 
a new Spanish sentence by considering its similar-
ity to sentences SC1, SC2, SC3 and SC4. Now, sup-
pose we want to translate the following two input 
sentences into English: 
 
IN1: Hasta que yo venga y os lleve a una tierra como la vuestra , 
tierra de grano y de vino , tierra de pan y de vi?as , tierra de aceite 
de olivo y de miel . (Until I come and take you away to a land like 
your own land , a land of corn and wine , a land of bread and 
vineyards , a land of oil olive and of honey .) 
128
IN2: Cuando amanec?a , la mujer vino y cay? delante de la puerta 
de la casa de aquel hombre donde estaba su se?or , hasta que fue 
de d?a . (Then came the woman in the dawning of the day , and fell 
down at the door of the man 's house where her lord was , till it 
was light .) 
 
We can select the appropriate sense for vino in 
each case by considering the sentence similarity 
between each of these two sentences and ?training? 
sentences SC1, SC2, SC3 and SC4. The actual similar-
ity values are presented in Table 2. 
 
 SC1 SC2 SC3 SC4 
sense {vino|||wine} {vino|||came} 
IN1 0.0636 0.2666 0.0351 0.0310 
IN2 0.0023 0.0513 0.0888 0.0774 
 
Table 2: Actual similarity values between input 
and training sentences containing the word vino. 
 
As seen from the table, the source-context simi-
larity feature is actually giving preference to the 
phrase pair {vino|||wine} in the case of input sen-
tence IN1 and to {vino|||came} in the case of IN2. 
Notice that more than one similarity value is gen-
erally available for each phrase pair. In our pro-
posed implementation, the largest similarity value 
is the one that is retained. More details on how we 
compute these sentence similarities are given in the 
following subsection.   
3.2 Latent Semantic Indexing 
Latent Semantic Indexing (Landauer et al, 1998) 
can be regarded as the text mining equivalent of 
Principal Component Analysis (Pearson, 1901). 
Both methods are based on the singular value de-
composition (SVD) of a matrix (Golub & Kahan, 
1965), according to which a rectangular matrix X 
of dimensions MxN can be factorized as follows: 
 
X = U ? VT , (3) 
 
where U and V are unitary matrices of dimensions 
MxM and NxN, respectively, and ? is a diagonal 
matrix containing the singular values associated to 
the decomposition.  
According to Landauer et al (1998), a low-
dimensional representation of a given document 
vector x can be obtained by means of the SVD de-
composition depicted in (3) as follows: 
 
yT = xT UMxL , (4) 
where y is the L-dimensional document vector cor-
responding to the projection of an M-dimensional 
document vector x, and UMxL is a matrix contain-
ing the L first column vectors of the unitary matrix 
U obtained from (3). 
Finally, the feature F(TU,IN) described in (2) is 
implemented as the internal product between nor-
malized versions of the vector projections obtained 
in (4). In our case, a vector-space model represen-
tation is constructed for sentences, instead of 
documents, and the source-context similarity val-
ues between translation units and input sentences 
are computed accordingly: 
 
F (TU, IN) = (5)  
<scT UMxL / |scTUMxL| , inT UMxL / |inTUMxL|> 
 
While the value of M is given by the vocabulary 
size in the data collection under consideration, se-
veral implementation questions arise regarding the 
most appropriate values for N (amount of sen-
tences to be used for estimating the projection op-
erator U) and L (the dimensionality of the reduced 
space). These and other implementation issues are 
discussed in detail in the following section.  
4 Implementation Issues  
This section discusses some important implemen-
tation issues that have to be dealt with in order to 
implement and evaluate the proposed approach. 
First, in subsection 4.1, the problem of implement-
ing a dynamic feature in a standard phrase-based 
machine translation framework is discussed. Then, 
in subsections 4.2 and 4.3, the problems of deter-
mining the amount of data required for estimating 
the Latent Semantic Indexing projection operator 
and the most appropriate dimensionality size for 
the reduced space representation are discussed.  
4.1 Implementing a Dynamic Feature 
As defined in (2), the value of the proposed source-
context similarity feature depends on each individ-
ual input sentence to be translated by the system. 
This definition implies a major difference between 
this feature and other conventional phrase-based 
translation features: it is a dynamic feature in the 
sense that it cannot be computed in advance before 
the input sentences to be translated are known. 
This on-the-fly requirement, along with the ex-
tended translation unit definition presented in (1), 
129
makes it not possible to directly implement the 
proposed methodology within a standard phrase-
based machine translation framework such as 
MOSES (Koehn et al, 2007). As it is not our in-
tention to develop a customized decoding tool for 
implementing and testing our proposed feature, we 
followed or previous implementation of an off-line 
version of the proposed methodology (Costa-juss? 
& Banchs, 2010), which, although very inefficient 
in the practice, allows us to evaluate the impact of 
the source-context feature on a state-of-the-art 
phrase-based translation system.  
According to this, our practical implementation 
is a follows: 
? Two sentence similarity matrices are com-
puted: one between sentences in the devel-
opment and training sets, and the other 
between sentences in the test and training 
datasets.  
? Each matrix entry mij should contain the 
similarity score between the ith sentence in 
the training set and the jth sentence in the 
development (or test) set. 
? For each sentence s in the test and develop-
ment sets, a phrase list LS of all potential 
phrases that can be used during decoding is 
extracted from the aligned training set. 
? The corresponding source-context similarity 
values are assigned to each phrase in lists LS 
according to values in the corresponding 
similarity matrices. 
? Each phrase list LS is collapsed into a phrase 
table TS by removing repetitions (when re-
moving repeated entries in the list, the larg-
est value of the source-context similarity 
feature is retained). 
? Each phrase table is completed by adding 
standard feature values (which are computed 
in the standard manner).  
? MOSES is used on a sentence-per-sentence 
basis, using a different translation table for 
each development (or test) sentence.  
4.2 Dataset for Latent Semantic Indexing 
Another important implementation issue that re-
quires attention is the computation of the Singular 
Value Decomposition described in (3). Ideally, the 
term-sentence matrix X to be decomposed should 
include all available data, i.e. training, develop-
ment and test sentences; however, in the practice, 
this is not possible because of two reasons. First, 
the sizes of typical datasets and vocabularies used 
in statistical machine translation systems are large 
enough to make Singular Value Decomposition 
unfeasible from a computational point of view 1 . 
Second, in a practical application system, the ?test 
set? is actually unknown during the system con-
struction and training phases. In this way, a realis-
tic implementation should be able to work with 
previously unseen data. 
In order to overcome the problem of applying 
the Singular Value Decomposition described in (3) 
to the full term-sentence matrix of all available 
data, we implemented an approximated procedure. 
In our approximation, we compute the similarity 
matrix between two set of sentences as the average 
of several similarity matrices that are computed 
over reduced space projections estimated with dif-
ferent random samples of the training data sen-
tences. In this way, our source-context similarity 
feature, previously defined in (5), becomes: 
 
F (TU, IN) ? (6)  
    1/K ?k <scTUkMxL/|scTUkMxL| , inTUkMxL/|inTUkMxL|>  
 
where UkMxL refers to a projection operator that has 
been computed by means of the Singular Valued 
Decomposition of a term-sentence matrix Xk con-
structed with a random sample of N sentences. 
Note that a total of K different similarity scores are 
averaged in (6). 
In order to evaluate the variability of the similar-
ity values estimated by this approximation, several 
experiments were conducted for different values of 
N and L, where the variance of the estimates over 
K=10 different realizations were computed. Figure 
1 shows the resulting standard deviations for simi-
larity values estimated for different values of L 
when varying N (upper panel), and for different 
values of N when varying L (lower panel). 
As seen from the figure, the range 500<N<1000 
seems to constitute a good compromise between 
the size of selected random sentence sets and the 
observed variability for similarity value estimates, 
as it provides a significant reduction in the com-
puted standard deviations with respect to N=100, 
and not important improvement is observed when 
                                                          
1 Even in the case of a small dataset such as the one consid-
ered here (see details in section 5) the Singular Value Decom-
position of the full term-sentence matrix can take several 
weeks to be completed in and standard Linux-based server. 
130
N>1000. According to this, we selected N=1000 
for our proposed approximation described in (6). 
 
 
 
Figure 1: Standard deviations (STD) for similarity 
values between development and test datasets (de-
scribed in section 5) estimated for different values 
of L when varying N (upper panel), and for differ-
ent values of N when varying L (lower panel). In 
all cases K=10. 
4.3 Reduced Space Dimensionality 
The third and final implementation issue to be dis-
cussed is the selection of the reduced space dimen-
sionality. It have been reported in the literature that 
dimensionality reduction, by means of Latent Se-
mantic Indexing, into the range between 100 and 
1000 provides good space representations for word 
and sentence association applications (Landauer et 
al., 1998). Although it is reasonable to assume this 
condition to be valid also for the application under 
consideration, we conducted a more detailed ex-
ploratory analysis for selecting the dimensionality 
L to be used in our experiments. 
First, we studied the distributions of context-
similarity values computed according to (6) over 
the available data. Figure 2 shows the average dis-
tributions of similarities between sentences in the 
development and training datasets (see data de-
scription in section 5) at different dimensionality 
values. As can be seen from the figure, a dimen-
sionality value of L=100 exhibits a very nice dis-
tribution of similarity values; however, according 
to the results depicted in Figure 1 (lower panel), 
the variability of estimates for such a low dimen-
sionality is relatively high. On the other hand, no-
tice again from Figure 2, how a much larger 
dimensionality value such as L=5000 already starts 
to exhibit a distribution of similarities that is heav-
ily biased towards the low similarity region. Ac-
cording to this result, and taking also into account 
the results in Figure 1, we finally decided setting 
the dimensionality of the reduced space to L=500. 
 
 
 
Figure 2: Average distributions of similarity values 
between development and training sentences com-
puted at different dimensionality values. For all 
cases presented here N=500 and K=10. 
5 Experimental Work     
This section describes the experimental work con-
ducted to evaluate the incidence of the proposed 
source-context similarity feature on translation 
quality for a state-of-the-art phrase-based statistical 
machine translation. First, subsection 5.1 describes 
the dataset and experimental setting. Then, subsec-
tion 5.2 presents and discusses the results. 
5.1 Experimental Setting 
The proposed methodology is evaluated on the Bi-
ble dataset (Chew et al, 2006) Spanish-to-English 
translation task, using the MOSES framework as 
baseline phrase-based statistical machine transla-
tion system (Koehn et al, 2007). Table 3 presents 
the main statistics of the bilingual corpus used. 
 
dataset lang. sentences tokens vocab av. lenght 
Train Spa 28,887 781,113 28,178 27 
Train Eng 28,887 848,776 13,126 29 
Test Spa 500 13,312 2,879 27 
Test Eng 500 14,562 2,156 29 
Dev Spa 500 13,170 2,862 26 
Dev Eng 500 14,537 2,095 29 
 
Table 3: Main statistics of the bilingual corpus un-
der consideration (number of sentences, tokens, 
vocabulary, and average sentence length) 
 
Regarding the baseline system, we used the de-
fault parameters of MOSES, which include the 
131
grow-final-diagonal alignment symmetrisation, the 
lexicalized reordering, a 5-gram language model 
using Kneser-Ney smoothing, and phrases up to 
length 10, among others. The optimization was 
done using the standard MERT procedure (Och & 
Ney, 2002). 
5.2 Experimental Results 
Table 4 presents the translation BLEU, measured 
over the development and test sets, for three differ-
ent system implementations: the baseline system, a 
second system implementing the source-context 
similarity feature over the full-dimensional vector 
space (FVS), just as we implemented it in Costa-
juss? & Banchs (2010), and a third system imple-
menting the source-context similarity feature based 
on Latent Semantic Indexing (LSI). 
 
 Development Test 
Baseline 39.92 38.92 
Source-context (FVS) 40.61 39.43 
Source-context (LSI) 40.80 39.86 
 
Table 4: BLEU scores over development and test 
datasets corresponding to three system implemen-
tations: baseline, and source-context similarity fea-
ture at full-dimensional vector space (FVS) and by 
means of Latent Semantic Indexing (LSI).   
 
As seen from the table, the system implementing 
the Latent Semantic Indexing based source-context 
similarity feature outperforms the baseline system 
by almost one absolute BLEU point, and the full-
dimensional vector space system by some less than 
a half absolute BLEU point. An analysis of signifi-
cance (Koehn, 2004) showed that the differences 
among the systems are statistically significant.  
A more comprehensive manual analysis of both 
the baseline and source-context LSI system outputs 
was required to better asses the incidence of the 
implemented source-context similarity feature on 
the generated translations. The result of this analy-
sis is presented in the following section. 
6 Manual Evaluation 
This section presents and discusses the results of a 
manual evaluation that was conducted over a sam-
ple set of translations. Previous to the manual 
evaluation, we performed a sentence-based auto-
matic evaluation using BLEU for the 500 sen-
tences in the test dataset. We obtained that our 
proposed approach is better than the baseline sys-
tem in 208 sentences, while the baseline is better 
than our system in 173 sentences and the remain-
ing 119 had the same BLEU scores. 
Some output sentences were randomly selected, 
regardless of which system performed better, for 
conducting a manual inspection. From these sen-
tences, we have extracted some segments that illus-
trate specific cases in which our proposed source-
context feature is actually helping to select a better 
translation unit according to the context of the in-
put sentence being translated. Five of these seg-
ments are presented in Table 5, where the relevant 
fragments within the segments are shown in bold.  
 
Example 1  
source No des sue?o a tus ojos ni dejes dormitar tus p?rpa-
dos . 
reference Give not sleep to thine eyes , nor slumber to thine 
eyelids . 
baseline Not sleep in thy sight , Let neither slumber thy 
eyelids . 
LSI-context
 
Give not sleep to thine eyes neither slumber , Let 
thine eyelids . 
Example 2  
source Entonces ellos se acercaron , echaron mano a Jes?s 
y le prendieron ? 
reference Then came they , and laid hands on Jesus , and took 
him ? 
baseline And they came near , and cast hand to Jesus , and 
took him ? 
LSI-context
 
And they came near , and laid hands on Jesus , and 
took him ? 
Example 3  
source Y al tercer d?a , he aqu? que un hombre vino del 
campamento de Sa?l ? 
reference It came even to pass on the third day , that , behold , a 
man came out of the camp from Saul ? 
baseline And the third day , behold , a man wine of the camp 
of Saul ? 
LSI-context And the third day , behold , there came a man of the 
camp of Saul ? 
Example 4  
source ? sed confortados ; sed de un mismo sentir ? 
reference ? be of good comfort , be of one mind ? 
baseline ? thirst confortados ; thirst of one mind 's sake ? 
LSI-context ? be ye confortados : be ye of one mind 's sake ? 
Example 5  
source ? seg?n sus familias , seg?n sus idiomas , en sus 
territorios y en sus naciones . 
reference ? after their families , after their tongues , in their 
countries , and in their nations . 
baseline ? according to their families , after their tongues , in 
their coasts , and in their nations . 
LSI-context ? after their families , after their tongues , in their 
lands , and in their nations . 
 
Table 5: Sample segments where the LSI-based 
source-context feature has helped to accomplish 
better translation unit selections. 
132
As seen from the table, the LSI-based source-
context system is clearly accomplishing more ap-
propriate unit selections. However, in most of the 
cases this does not imply either a better overall 
translation or a closer match to the available refer-
ence translation. This can explain the relative low 
BLEU gain achieved by the method.  
Similarly, we also extracted some segments that 
illustrate specific cases in which our proposed 
source-context feature fails in helping to select a 
better translation unit. Table 6 presents four of 
these cases. 
 
Example 1  
source ? yo he sido enviado con malas noticias para ti . 
reference ? for I am sent to thee with heavy tidings . 
baseline ? for I have sent with evil tidings unto thee . 
LSI-context ? I am sent with evil tidings unto thee . 
Example 2  
source ? heredad de Jehovah son los hijos ; recompensa es 
el fruto del vientre . 
reference ? children are an heritage of the Lord : and the fruit 
of the womb is his reward . 
baseline ? the inheritance of the Lord , are the children ; 
reward is the fruit of the belly . 
LSI-context ? the inheritance of the Lord are the children , and 
reward is the fruit of the belly . 
Example 3  
source ? y que hab?a enaltecido su reino por amor a su 
pueblo Israel . 
reference ? and that he had exalted his kingdom for his 
people Israel 's sake . 
baseline ? and for his kingdom was lifted up his people 
Israel . 
LSI-context ? and for his kingdom was lifted up unto his 
people Israel . 
Example 4  
source Y suceder? que a causa de la abundancia de leche , 
comer? leche cuajada ? 
reference And it shall come to pass , for the abundance of 
milk that he shall eat butter ? 
baseline And it shall come to pass , that by reason of the 
multitude of milk , shall eat with milk cuajada ? 
LSI-context And it shall come to pass by reason of the multitude 
of milk , and shall eat with milk cuajada ? 
 
Table 6: Sample segments where the LSI-based 
source-context feature has failed to accomplish 
better translation unit selections. 
 
In the latter examples in Table 6, the proposed 
source-context feature is clearly failing to provide 
better lexical selections. In some cases, this seems 
to be due to the lack of enough source-context in-
formation in the input sentence to be translated. 
However, in other cases, it is because the source-
context feature alone is not able to compensate the 
system?s bias towards more frequent translations.   
7 Conclusions and Future Work  
A new semantically-motivated feature for statisti-
cal machine translation based on Latent Semantic 
Indexing has been proposed and evaluated. The 
objective of the proposed feature is to account for 
the degree of similarity between a given input sen-
tence and each individual sentence in the training 
dataset. This similarity is computed in a reduced 
vector-space constructed by means of the Latent 
Semantic Indexing decomposition.  
The computed similarity values are used as an 
additional feature in the log-linear model combina-
tion approach to statistical machine translation. In 
our implementation, the proposed feature is dy-
namically adjusted for each translation unit in the 
translation table according to the current input sen-
tence to be translated. 
Experimental results on a Spanish-to-English 
translation task on the Bible corpus showed sig-
nificant improvements of almost 1 and 0.5 absolute 
BLEU points with respect to a baseline system and 
a similar system evaluating sentence similarity at 
the full-dimensional vector space, respectively. A 
manual evaluation revealed that the proposed fea-
ture is actually helping the translation system to 
perform a better selection of translation units on a 
semantic basis.  
As future work, we intend to evaluate different 
association and distance metrics, as well as to ex-
tend the current notion of source-context from the 
input sentence to be translated to any other kind of 
available information beyond the input sentence 
limits. Similarly, different paradigms of semantic 
space representations, including those statistically 
motivated, will be studied and evaluated.  
Implementation issues are also to be revisited 
for better evaluating the impact of both the amount 
of training data and the dimensionality of the re-
duced space on the method?s performance. Finally, 
an on-line version of the method must be imple-
mented in order to be able to evaluate the proposed 
methodology over larger data collections.  
 
Acknowledgments 
The authors would like to thank the Institute for 
Infocomm Research, as well as Barcelona Media 
Innovation Centre and the Juan de la Cierva fel-
lowship program, for their support and permission 
to publish this work. 
133
References  
Brown, P., Della-Pietra, S., Della-Pietra, V., Mercer, R. 
(1993) The Mathematics of Statistical Machine 
Translation: Computational Linguistics 19(2), 263--
311 
Carl, M., Way, A. (2003) Recent Advances in Example-
Based Machine Translation. Kluwer Academic 
Carpuat, M., Wu, D. (2007) How Phrase Sense Disam-
biguation Outperforms Word Sense Disambiguation 
for Statistical Machine Translation. In: 11th Interna-
tional Conference on Theoretical and Methodological 
Issues in Machine Translation. Skovde 
Carpuat, M., Wu, D. (2008) Evaluation of Context-
Dependent Phrasal Translation Lexicons for Statisti-
cal Machine Translation. In: 6th International Con-
ference on Language Resources and Evaluation 
(LREC). Marrakech 
Chew, P. A., Verzi, S. J., Bauer, T. L., McClain, J. T. 
(2006) Evaluation of the Bible as a Resource for 
Cross-Language Information Retrieval. In: Workshop 
on Multilingual Language Resources and Interopera-
bility, pp. 68--74, Sydney 
Costa-juss?, M. R., Banchs, R.E. (2010) A Vector-
Space Dynamic Feature for Phrase-Based Statistical 
Machine Translation. Journal of Intelligent Informa-
tion Systems 
Espa?a-Bonet, C., Gimenez, J., Marquez, L. (2009) 
Discriminative Phrase-Based Models for Arabic Ma-
chine Translation. ACM Transactions on Asian Lan-
guage Information Processing Journal (Special Issue 
on Arabic Natural Language Processing)  
Golub, G. H., Kahan, W. (1965) Calculating the Singu-
lar Values and Pseudo-Inverse of a Matrix. Journal of 
the Society for Industrial and Applied Mathematics: 
Numerical Analysis 2(2), 205--224  
Haque, R., Naskar, S. K., Ma, Y., Way, A. (2009) Using 
Supertags as Source Language Context in SMT. In: 
13th Annual Conference of the European Association 
for Machine Translation, pp. 234--241. Barcelona  
Haque, R., Naskar, S. K., van den Bosh, A., Way, A. 
(2010) Supertags as Source Language Context in Hi-
erarchical Phrase-Based SMT. In: 9th Conference of 
the Association for Machine Translation in the 
Americas (AMTA) 
Koehn, P., Och, F. J., Marcu, D. (2003) Statistical 
Phrase-Based Translation. In: Human Language 
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing (HLT-
EMNLP), pp. 48--54. Edmonton 
 
Koehn, P. (2004) Statistical Significance Test for Ma-
chine Translation Evaluation. In: Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP) 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., 
Federico, M., Bertoldi, N., Cowan, B., Shen, W., 
Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, 
A., Herbst, E. (2007) Moses: Open Source Toolkit 
for Statistical Machine Translation. In: 45th Annual 
Metting of the Association for Computational Lin-
guistics, pp. 177--180. Prague 
Landauer, T. K., Laham, D., Foltz, P. (1998) Learning 
Human-Like Knowledge by Singular Value Decom-
position: A Progress Report. In: Conference on Ad-
vances in Neural Information Processing Systems, 
pp. 45--51. Denver 
Landauer, T. K., Foltz, P.W., Laham, D. (1998) Intro-
duction to Latent Semantic Analysis. Discourse 
Processes 25, 259--284  
Och, F. J., Ney, H. (2002) Discriminative Training and 
Maximum Entropy Models for Statistical Machine 
Translation. In: 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 295--302 
Padilla, P., Bajo, T. (1998) Hacia un Modelo de Memo-
ria y Atenci?n en la Interpretaci?n Simult?nea. Quad-
erns: Revista de Traducci? 2, 107--117  
Pearson, K. (1901) On Lines and Planes of Closest Fit 
to Systems of Points in Space. Philosophical Maga-
zine 2(6), 559--572  
Salton, G., Wong, A., Yang, C. S. (1975) A Vector 
Space Model for Automatic Indexing. Communica-
tions of the ACM 18(11), 613--620  
 
 
134
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 452?456,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The BM-I2R Haitian-Cre?ole-to-English translation system description
for the WMT 2011 evaluation campaign
Marta R. Costa-jussa`
Barcelona Media Innovation Center
Av Diagonal, 177, 9th floor
08018 Barcelona
marta.ruiz@barcelonamedia.org
Rafael E. Banchs
Institute for Infocomm Research
1 Fusionopolis Way 21-01
Singapore 138632
rembanchs@i2r.a-star.edu.sg
Abstract
This work describes the Haitian-Cre?ole to En-
glish statistical machine translation system
built by Barcelona Media Innovation Center
(BM) and Institute for Infocomm Research
(I2R) for the 6th Workshop on Statistical Ma-
chine Translation (WMT 2011). Our sys-
tem carefully processes the available data and
uses it in a standard phrase-based system en-
hanced with a source context semantic feature
that helps conducting a better lexical selection
and a feature orthogonalization procedure that
helps making MERT optimization more reli-
able and stable. Our system was ranked first
(among a total of 9 participant systems) by the
conducted human evaluation.
1 Introduction
During years there has been a big effort to produce
natural language processing tools that try to under-
stand well written sentences, but the question is how
well do these tools work to analyze the contents
of SMS. For example, not even syntactic tools like
stemming can bring to common stems words that
have been shortened (like Xmas or Christmas).
This paper describes our participation on the 6th
Workshop on Statistical Machine Translation (WMT
2011). The featured task from the workshop was
to translate Haitian-Cre?ole SMS messages into En-
glish. According to the WMT 2011 organizers, these
text messages (SMS) were sent by people in Haiti in
the aftermath of the January 2010 earthquake. Our
objective in this featured task is to translate from
Haitian-Cre?ole into English either using raw or clean
data.
We propose to build an SMT system which could
be used for both raw and clean data. Our base-
line system is an standard phrase-based SMT sys-
tem built with Moses (Koehn et al, 2007). Starting
from this system we propose to introduce a semantic
feature function based on latent semantic indexing
(Landauer et al, 1998). Additionally, as a total dif-
ferent approximation, we propose to orthogonalize
the standard feature functions of the phrase-based
table using the Gram-Schmidt methodology (Greub,
1975). Then, we experimentally combine both en-
hancements.
The only difference among the raw and clean
SMT system were the training sentences. In order
to translate the clean data, we propose to normalize
the corpus of short messages given very scarce re-
sources. We only count with a small set of parallel
corpus at the level of sentence of chat and standard
language. A nice normalization methodology can
allow to make the task of communication easier. We
propose a statistical normalization technique using
the scarce resources we have based on a combina-
tion of statistical machine translation techniques.
The rest of this paper is organized as follows. Sec-
tion 2 briefly describes the phrase-based SMT sys-
tem which is used as a reference system. Next, sec-
tion 3 describes our approximation to introduce se-
mantics in the baseline system. Section 4 reports our
idea of orthogonalizing the feature functions in the
translation table. Section 5 details the data process-
ing and the data conversion from raw to clean. As
follows, section 6 shows the translation results. Fi-
nally, section 7 reports most relevant conclusions of
this work.
452
2 Phrase-based SMT baseline system
The phrase-based approach to SMT performs the
translation splitting the source sentence in segments
and assigning to each segment a bilingual phrase
from a phrase-table. Bilingual phrases are trans-
lation units that contain source words and target
words, e.g. unite? de traduction ? translation unit,
and have different scores associated to them. These
bilingual phrases are then selected in order to maxi-
mize a linear combination of feature functions. Such
strategy is known as the log-linear model (Och,
2003) and it is formally defined as:
e? = argmax
e
[
M?
m=1
?mhm (e, f)
]
(1)
where hm are different feature functions with
weights ?m. The two main feature functions are
the translation model (TM) and the target lan-
guage model (LM). Additional models include lexi-
cal weights, phrase and word penalty and reordering.
3 Semantic feature function
Source context information is generally disregarded
in phrase-based systems given that all training sen-
tences contribute equally to the final translation.
The main objective in this section is to motivate
the use of a semantic feature function we have re-
cently proposed (Banchs and Costa-jussa`, 2011) for
incorporating source context information into the
phrase-based statistical machine translation frame-
work. Such a feature is based on the use of a sim-
ilarity metric for assessing the degree of similarity
between the sentences to be translated and the sen-
tences in the original training dataset.
The measured similarity is used to favour those
translation units that have been extracted from train-
ing sentences that are similar to the current sen-
tence to be translated and to penalize those trans-
lation units than have been extracted from unrelated
or dissimilar training sentences. In the proposed fea-
ture, sentence similarity is measured by means of the
cosine distance in a reduced dimension vector-space
model, which is constructed by using Latent Seman-
tic Indexing (Landauer et al, 1998), a well know
dimensionality reduction technique that is based on
the singular value decomposition of a matrix (Golub
and Kahan, 1965).
The main motivation of this semantic feature is
the fact that source context information is actually
helpful for disambiguating the sense of a given word
during the translation process. Consider for instance
the Spanish word banco which can be translated into
English as either bank or bench depending on the
specific context it occurs. By comparing a given
input sentence containing the Spanish word banco
with all training sentences from which phrases in-
cluding this word where extracted, we can figure
out which is the most appropriated sense for this
word in the given sentence. This is because for the
sense bank the Spanish word banco will be more
like to co-occur with words such as dinero (money),
cuenta (account), intereses (interest), etc., while for
the sense bench it would be more likely to co-occur
with words such as plaza (square), parque (park),
mesa (table), etc; and the chances are high for such
disambiguating words to appear in one or more of
the training sentences from which bilingual phrases
containing banco has been extracted.
In the particular case of translation tasks where
multi-domain corpora is used for training machine
translation systems, such as the Haitian-Creole-to-
English task considered here, the proposed seman-
tic feature has proven to contribute to a better lexi-
cal selection during the decoding process. However,
in tasks considering mono-domain corpora the se-
mantic feature does not improves translation quality
as the most frequent translation pairs learned by the
system are actually the correct ones.
Another important issue related to the semantic
feature discussed here is that it is a dynamic feature
in the sense that it is computed for each potential
translation unit according to the current input sen-
tence being translated. This makes the implementa-
tion of this semantic feature very expensive from a
computational point of view. At this moment, we do
not have an efficient implementation, which makes it
unfeasible in the practice to apply this methodology
to large training corpora.
As the training corpus available for the Haitian-
Creole-to-English is both small in size and multi-
domain in nature, it constitutes the perfect scenario
for experimenting with the recently proposed source
context semantic feature. For more details about im-
453
plementation and performance of this methodology
in a different translation task, the reader should refer
to (Banchs and Costa-jussa`, 2011).
4 Heuristic enhacement
The phrase-based SMT baseline system contains,
by default, 5 feature functions which are the con-
ditional and posterior probabilities, the direct and
indirect lexical scores and the phrase penalty. Usu-
ally, these feature functions are not statistical inde-
pendent from each other. Based on the analogy be-
tween the statistical and geometrical concepts of in-
dependence and orthogonality, and given that, dur-
ing MERT, the optimization of feature combination
is conducted on log-probability space; we decided
to explore the effect of using a set of orthogonal fea-
tures during MERT optimization.
It is well know in both spectral analysis and vec-
tor space decomposition that orthogonal bases allow
for optimal representations of signals and variables,
as they allow for each individual natural component
to be represented independently of the others. In
linear lattice predictors, for instance, each filter co-
efficient can be optimized independently from the
others while convergence to the optimal solution is
guarantied (Haykin, 1996). In the case of statisti-
cal machine translation, the linear nature of feature
combination in log-probability space suggested us
that transforming the features into a set of orthog-
onal features could make MERT optimization more
robust and efficient.
According to this, we used Gram-Schmidt
(Greub, 1975) to transform all available feature
functions into an orthogonal set of feature func-
tions. This orthogonalization process was con-
ducted directly over the log-probability space, i.e,
given the five vectors representing the feature
functions h1, h2, h3, h4, h5, we used the Gram-
Schmidt algorithm to construct an orthogonal basis
v1, v2, v3, v4, v5. The resulting set of features con-
sisted of 5 vectors that form an orthogonal basis.
This new orthogonal set of features was used for
MERT optimization and decoding.
5 Experimental framework
In this section we report the details of the used data
preprocessing and raw to clean data conversion.
5.1 Data preprocessing
The WMT evaluation provided a high variety of
data. Our preprocessing consisted of the following:
? Lowercase and tokenize all files using the
scripts from Moses.
? In the case of the haitian-Creole side of the
data, replace all stressed vowels by their plain
forms.
? Filter out those sentences which had no words
or more than 120.
Table 1 shows the data statistics of the different
sources before and after this preprocessing. The dif-
ferent sources of the table include: in-domain SMS
data (SMS); medical domain (medical); newswire
domain (newswire); united nations (un); state de-
partment (state depart.); guidelines for approapriate
international disaster donations (guidelines); kren-
gle senetences (krengle) and a glossary includes
wikipedia name entities and haitisurf dictionary.
The sources of this material are specified in the web
page of the workshop.
All data from table 1 was concatenated and used
as training corpus. The English part of this data was
used to build the language model. As development
and test corpus we used the data provided by the
organization. Both development and test contained
900 sentences.
Finally, in the evaluation, we included develop-
ment and tests as part of the training corpus, and
then, we translated the evaluation set.
5.2 Raw to clean data conversion
This featured task contained two subtasks. One was
to translate raw data and the other was to translate
clean data. Therefore, we have to build two sys-
tems. Our raw data system was built using the train-
ing data from table 1. The clean data system was
built using all training data from table 1 except in-
domain SMS data. Particularly, a modified version
of the in-domain SMS data was included in the clean
data system. The modification consisted in cleaning
the original in-domain SMS data using an standard
Moses SMT system. We built an SMT system to
translate from raw data to clean data. This SMT sys-
tem was built with the development, test and evalu-
ation data which in total were 2700 sentences. We
454
Statistics
before after
SMS
sentences 17,192 16,594
words 386.0k 383.0k
medical
sentences 1,619 1,619
words 10.4k 10.4k
newswire
sentences 13,517 13,508
words 326.9k 326.7k
wikipedia
sentences 8,476 8,476
words 113.9k 113.9k
un
sentences 91 91
words 1,906 1,906
state depart.
sentences 56 14
words 450 355
guidelines
sentences 60 9
words 795 206
krengle
sentences 658 655
words 4.2k 4.2k
bible
sentences 30,715 30,677
words 946k 944k
glossary
sentences 49,990 49,980
words 126.4k 126.3k
Table 1: Data Statistics before and after training prepro-
cessing. Number of words are from the English side.
used 2500 sentences as training data and 200 sen-
tences for development to adjust weights. The raw
and clean systems were tuned with their respective
developments and tested on their respective tests.
6 Experimental results
In this section we report the results of the approaches
proposed in previous sections. Table 2 and 3 report
the results on the development and test sets on the
raw and clean subtask, respectively.
First row on both tables report the results of the
baseline system briefly described in section 2. Sec-
ond row and third row on both tables report the per-
formance of the semantic feature function and on the
heuristic approach of orthogonalization (orthofea-
tures) respectively. Finally, the last row on both
tables report the performance of both semantic and
heuristic features when combined.
Results shown in tables 2 and 3 do not show
coherent improvements when introducing the new
System Dev Test
baseline 32.00 31.01
+semanticfeature 32.34 30.68
+orthofeatures 31.63 29.90
+semanticfeature+orthofeatures 32.21 30.34
Table 2: BLEU results for the raw data. Best results in
bold.
System Dev Test
baseline 35.86 33.78
+semanticfeature 35.98 33.90
+orthofeatures 35.57 34.10
+semanticfeature+orthofeatures 36.28 33.53
Table 3: BLEU results for the clean data. Best results in
bold.
methodologies proposed. The clean data seems to
benefit from the semantic features and the orthofea-
tures separately. However, the raw data seems not to
benefit from the orthofeatures and keep the similar
performance to the baseline system when using the
semantic feature. Although, this trend is clear, the
results are not conclusive. Therefore, we decided to
participate in the evaluation with the full system (in-
cluding the semantic features and orthofeatures) in
the clean track and with the system including the se-
mantic feature in the raw track. Actually, we used
those systems that performed best in the develop-
ment set. Additionally, results with the semantic
feature may not be significantly better than the base-
line system, but we have seen it actually heps to im-
prove lexical selection in practice in previous works
(Banchs and Costa-jussa`, 2011).
7 Conclusions
This paper reports the BM-I2R system description in
the Haitian-Cre?ole to English translation task. This
system was ranked first in the WMT 2011 by the
conducted human evaluation. The translation sys-
tem uses a PBSMT system enhanced with two dif-
ferent methodologies. First, we experiment with the
introduction of a semantic feature which is capa-
ble of introducing source context information. Sec-
ond, we propose to transform the five standard fea-
ture functions used in the translation model of the
PBSMT system into five orthogonal feature func-
455
tions using the Gram-Schmidt methodology. Results
show that the first methodology can be used for both
raw and clean data. Whereas the second seems to
only benefit clean data.
Acknowledgments
The research leading to these results has received
funding from the Spanish Ministry of Science and
Innovation through the Juan de la Cierva fellowship
program. The authors would like to thank Barcelona
Media Innovation Center and Institute for Infocomm
Research for their support and permission to publish
this research.
References
R. Banchs and M.R. Costa-jussa`. 2011. A semantic fea-
ture for statistical machine translation. In 5th Work-
shop on Syntax, Semantics and Structure in Statistical
Translation (at ACL HLT 2011), Portland.
G. H. Golub and W. Kahan. 1965. Calculating the sin-
gular values and pseudo-inverse of a matrix. journal of
the society for industrial and applied mathematics. In
Numerical Analysis 2(2), pages 205?224.
W. Greub. 1975. Linear Algebra. Springer.
S. Haykin. 1996. Adaptive Filter Theory. Prentice Hall.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of the 45th
Annual Meeting of the Association for Computational
Linguistics, pages 177?180, Prague, Czech Republic,
June.
T. K. Landauer, D. Laham, and P. Foltz. 1998. Learning
human-like knowledge by singular value decomposi-
tion: A progress report. In Conference on Advances in
Neural Information Processing Systems, pages 45?51,
Denver.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of the 41th Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, July.
456
