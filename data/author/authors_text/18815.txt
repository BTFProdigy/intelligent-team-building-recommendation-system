Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059?1069,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Efficient Non-parametric Estimation of
Multiple Embeddings per Word in Vector Space
Arvind Neelakantan
*
, Jeevan Shankar
*
, Alexandre Passos, Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
Amherst, MA, 01003
{arvind,jshankar,apassos,mccallum}@cs.umass.edu
Abstract
There is rising interest in vector-space
word embeddings and their use in NLP,
especially given recent methods for their
fast estimation at very large scale. Nearly
all this work, however, assumes a sin-
gle vector per word type?ignoring poly-
semy and thus jeopardizing their useful-
ness for downstream tasks. We present
an extension to the Skip-gram model that
efficiently learns multiple embeddings per
word type. It differs from recent related
work by jointly performing word sense
discrimination and embedding learning,
by non-parametrically estimating the num-
ber of senses per word type, and by its ef-
ficiency and scalability. We present new
state-of-the-art results in the word similar-
ity in context task and demonstrate its scal-
ability by training with one machine on a
corpus of nearly 1 billion tokens in less
than 6 hours.
1 Introduction
Representing words by dense, real-valued vector
embeddings, also commonly called ?distributed
representations,? helps address the curse of di-
mensionality and improve generalization because
they can place near each other words having sim-
ilar semantic and syntactic roles. This has been
shown dramatically in state-of-the-art results on
language modeling (Bengio et al, 2003; Mnih and
Hinton, 2007) as well as improvements in other
natural language processing tasks (Collobert and
Weston, 2008; Turian et al, 2010). Substantial
benefit arises when embeddings can be trained on
large volumes of data. Hence the recent consider-
able interest in the CBOW and Skip-gram models
*
The first two authors contributed equally to this paper.
of Mikolov et al (2013a); Mikolov et al (2013b)?
relatively simple log-linear models that can be
trained to produce high-quality word embeddings
on the entirety of English Wikipedia text in less
than half a day on one machine.
There is rising enthusiasm for applying these
models to improve accuracy in natural language
processing, much like Brown clusters (Brown et
al, 1992) have become common input features
for many tasks, such as named entity extraction
(Miller et al, 2004; Ratinov and Roth, 2009) and
parsing (Koo et al, 2008; T?ackstr?om et al, 2012).
In comparison to Brown clusters, the vector em-
beddings have the advantages of substantially bet-
ter scalability in their training, and intriguing po-
tential for their continuous and multi-dimensional
interrelations. In fact, Passos et al (2014) present
new state-of-the-art results in CoNLL 2003 named
entity extraction by directly inputting continuous
vector embeddings obtained by a version of Skip-
gram that injects supervision with lexicons. Sim-
ilarly Bansal et al (2014) show results in depen-
dency parsing using Skip-gram embeddings. They
have also recently been applied to machine trans-
lation (Zou et al, 2013; Mikolov et al, 2013c).
A notable deficiency in this prior work is that
each word type (e.g. the word string plant) has
only one vector representation?polysemy and
hononymy are ignored. This results in the word
plant having an embedding that is approximately
the average of its different contextual seman-
tics relating to biology, placement, manufactur-
ing and power generation. In moderately high-
dimensional spaces a vector can be relatively
?close? to multiple regions at a time, but this does
not negate the unfortunate influence of the triangle
inequality
2
here: words that are not synonyms but
are synonymous with different senses of the same
word will be pulled together. For example, pollen
and refinery will be inappropriately pulled to a dis-
2
For distance d, d(a, c) ? d(a, b) + d(b, c).
1059
tance not more than the sum of the distances plant?
pollen and plant?refinery. Fitting the constraints of
legitimate continuous gradations of semantics are
challenge enough without the additional encum-
brance of these illegitimate triangle inequalities.
Discovering embeddings for multiple senses per
word type is the focus of work by Reisinger and
Mooney (2010a) and Huang et al (2012). They
both pre-cluster the contexts of a word type?s to-
kens into discriminated senses, use the clusters to
re-label the corpus? tokens according to sense, and
then learn embeddings for these re-labeled words.
The second paper improves upon the first by em-
ploying an earlier pass of non-discriminated em-
bedding learning to obtain vectors used to rep-
resent the contexts. Note that by pre-clustering,
these methods lose the opportunity to jointly learn
the sense-discriminated vectors and the cluster-
ing. Other weaknesses include their fixed num-
ber of sense per word type, and the computational
expense of the two-step process?the Huang et
al (2012) method took one week of computation
to learn multiple embeddings for a 6,000 subset
of the 30,000 vocabulary on a corpus containing
close to billion tokens.
3
This paper presents a new method for learn-
ing vector-space embeddings for multiple senses
per word type, designed to provide several ad-
vantages over previous approaches. (1) Sense-
discriminated vectors are learned jointly with the
assignment of token contexts to senses; thus we
can use the emerging sense representation to more
accurately perform the clustering. (2) A non-
parametric variant of our method automatically
discovers a varying number of senses per word
type. (3) Efficient online joint training makes
it fast and scalable. We refer to our method as
Multiple-sense Skip-gram, or MSSG, and its non-
parametric counterpart as NP-MSSG.
Our method builds on the Skip-gram model
(Mikolov et al, 2013a), but maintains multiple
vectors per word type. During online training
with a particular token, we use the average of its
context words? vectors to select the token?s sense
that is closest, and perform a gradient update on
that sense. In the non-parametric version of our
method, we build on facility location (Meyerson,
2001): a new cluster is created with probability
proportional to the distance from the context to the
3
Personal communication with authors Eric H. Huang and
Richard Socher.
nearest sense.
We present experimental results demonstrating
the benefits of our approach. We show quali-
tative improvements over single-sense Skip-gram
and Huang et al (2012), comparing against word
neighbors from our parametric and non-parametric
methods. We present quantitative results in three
tasks. On both the SCWS and WordSim353 data
sets our methods surpass the previous state-of-
the-art. The Google Analogy task is not espe-
cially well-suited for word-sense evaluation since
its lack of context makes selecting the sense dif-
ficult; however our method dramatically outper-
forms Huang et al (2012) on this task. Finally
we also demonstrate scalabilty, learning multiple
senses, training on nearly a billion tokens in less
than 6 hours?a 27x improvement on Huang et al.
2 Related Work
Much prior work has focused on learning vector
representations of words; here we will describe
only those most relevant to understanding this pa-
per. Our work is based on neural language mod-
els, proposed by Bengio et al (2003), which extend
the traditional idea of n-gram language models by
replacing the conditional probability table with a
neural network, representing each word token by
a small vector instead of an indicator variable, and
estimating the parameters of the neural network
and these vectors jointly. Since the Bengio et al
(2003) model is quite expensive to train, much re-
search has focused on optimizing it. Collobert and
Weston (2008) replaces the max-likelihood char-
acter of the model with a max-margin approach,
where the network is encouraged to score the cor-
rect n-grams higher than randomly chosen incor-
rect n-grams. Mnih and Hinton (2007) replaces
the global normalization of the Bengio model with
a tree-structured probability distribution, and also
considers multiple positions for each word in the
tree.
More relevantly, Mikolov et al (2013a) and
Mikolov et al (2013b) propose extremely com-
putationally efficient log-linear neural language
models by removing the hidden layers of the neu-
ral networks and training from larger context win-
dows with very aggressive subsampling. The
goal of the models in Mikolov et al (2013a) and
Mikolov et al (2013b) is not so much obtain-
ing a low-perplexity language model as learn-
ing word representations which will be useful in
1060
downstream tasks. Neural networks or log-linear
models also do not appear to be necessary to
learn high-quality word embeddings, as Dhillon
and Ungar (2011) estimate word vector repre-
sentations using Canonical Correlation Analysis
(CCA).
Word vector representations or embeddings
have been used in various NLP tasks such
as named entity recognition (Neelakantan and
Collins, 2014; Passos et al, 2014; Turian et al,
2010), dependency parsing (Bansal et al, 2014),
chunking (Turian et al, 2010; Dhillon and Ungar,
2011), sentiment analysis (Maas et al, 2011), para-
phrase detection (Socher et al, 2011) and learning
representations of paragraphs and documents (Le
and Mikolov, 2014). The word clusters obtained
from Brown clustering (Brown et al, 1992) have
similarly been used as features in named entity
recognition (Miller et al, 2004; Ratinov and Roth,
2009) and dependency parsing (Koo et al, 2008),
among other tasks.
There is considerably less prior work on learn-
ing multiple vector representations for the same
word type. Reisinger and Mooney (2010a) intro-
duce a method for constructing multiple sparse,
high-dimensional vector representations of words.
Huang et al (2012) extends this approach incor-
porating global document context to learn mul-
tiple dense, low-dimensional embeddings by us-
ing recursive neural networks. Both the meth-
ods perform word sense discrimination as a pre-
processing step by clustering contexts for each
word type, making training more expensive.
While methods such as those described in Dhillon
and Ungar (2011) and Reddy et al (2011) use
token-specific representations of words as part
of the learning algorithm, the final outputs are
still one-to-one mappings between word types and
word embeddings.
3 Background: Skip-gram model
The Skip-gram model learns word embeddings
such that they are useful in predicting the sur-
rounding words in a sentence. In the Skip-gram
model, v(w) ? R
d
is the vector representation of
the word w ? W , where W is the words vocabu-
lary and d is the embedding dimensionality.
Given a pair of words (w
t
, c), the probability
that the word c is observed in the context of word
w
t
is given by,
P (D = 1|v(w
t
), v(c)) =
1
1 + e
?v(w
t
)
T
v(c)
(1)
The probability of not observing word c in the con-
text of w
t
is given by,
P (D = 0|v(w
t
), v(c)) =
1? P (D = 1|v(w
t
), v(c))
Given a training set containing the sequence of
word types w
1
, w
2
, . . . , w
T
, the word embeddings
are learned by maximizing the following objective
function:
J(?) =
?
(w
t
,c
t
)?D
+
?
c?c
t
logP (D = 1|v(w
t
), v(c))
+
?
(w
t
,c
?
t
)?D
?
?
c
?
?c
?
t
logP (D = 0|v(w
t
), v(c
?
))
where w
t
is the t
th
word in the training set, c
t
is the set of observed context words of word w
t
and c
?
t
is the set of randomly sampled, noisy con-
text words for the word w
t
. D
+
consists of
the set of all observed word-context pairs (w
t
, c
t
)
(t = 1, 2 . . . , T ). D
?
consists of pairs (w
t
, c
?
t
)
(t = 1, 2 . . . , T ) where c
?
t
is the set of randomly
sampled, noisy context words for the word w
t
.
For each training word w
t
, the set of context
words c
t
= {w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
}
includesR
t
words to the left and right of the given
word as shown in Figure 1. R
t
is the window size
considered for the word w
t
uniformly randomly
sampled from the set {1, 2, . . . , N}, where N is
the maximum context window size.
The set of noisy context words c
?
t
for the word
w
t
is constructed by randomly sampling S noisy
context words for each word in the context c
t
. The
noisy context words are randomly sampled from
the following distribution,
P (w) =
p
unigram
(w)
3/4
Z
(2)
where p
unigram
(w) is the unigram distribution of
the words and Z is the normalization constant.
4 Multi-Sense Skip-gram (MSSG) model
To extend the Skip-gram model to learn multiple
embeddings per word we follow previous work
(Huang et al, 2012; Reisinger and Mooney, 2010a)
1061
Word 
Vector
word w
t
v(w
t+2
)
Context   
Vectors
v(w
t+1
)
v(w
t-1
)
v(w
t-2
)
v(w
t
)
Figure 1: Architecture of the Skip-gram model
with window size R
t
= 2. Context c
t
of word
w
t
consists of w
t?1
, w
t?2
, w
t+1
, w
t+2
.
and let each sense of word have its own embed-
ding, and induce the senses by clustering the em-
beddings of the context words around each token.
The vector representation of the context is the av-
erage of its context words? vectors. For every word
type, we maintain clusters of its contexts and the
sense of a word token is predicted as the cluster
that is closest to its context representation. After
predicting the sense of a word token, we perform
a gradient update on the embedding of that sense.
The crucial difference from previous approaches
is that word sense discrimination and learning em-
beddings are performed jointly by predicting the
sense of the word using the current parameter es-
timates.
In the MSSG model, each word w ? W is
associated with a global vector v
g
(w) and each
sense of the word has an embedding (sense vec-
tor) v
s
(w, k) (k = 1, 2, . . . ,K) and a context clus-
ter with center ?(w, k) (k = 1, 2, . . . ,K). The K
sense vectors and the global vectors are of dimen-
sion d and K is a hyperparameter.
Consider the word w
t
and let c
t
=
{w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
the context. Let v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
be the vector representation of the context c
t
. We
use the global vectors of the context words instead
of its sense vectors to avoid the computational
complexity associated with predicting the sense
of the context words. We predict s
t
, the sense
Word 6ense 
Vectors
v(w
t
2)
v
J
(w
t+2
)
Context   
Vectors
v
J
(w
t+1
)
 v
J
(w
t-1
)
v
J
(w
t-2
)
$verDJe Context 
Vector
Context COXster 
Centers
v(w
t
1)
v(w
t
)
3redLcted 
6ense s
t
?(w
t
1)
v
context
(c
t
)
 
 
 
?(w
t
2)
?(w
t
)
 
Context   
Vectors
v
J
(w
t+2
)
v
J
(w
t+1
)
v
J
(w
t-1
)
v
J
(w
t-2
)
Figure 2: Architecture of Multi-Sense Skip-gram
(MSSG) model with window size R
t
= 2 and
K = 3. Context c
t
of word w
t
consists of
w
t?1
, w
t?2
, w
t+1
, w
t+2
. The sense is predicted by
finding the cluster center of the context that is clos-
est to the average of the context vectors.
of word w
t
when observed with context c
t
as
the context cluster membership of the vector
v
context
(c
t
) as shown in Figure 2. More formally,
s
t
= argmax
k=1,2,...,K
sim(?(w
t
, k), v
context
(c
t
)) (3)
The hard cluster assignment is similar to the k-
means algorithm. The cluster center is the aver-
age of the vector representations of all the contexts
which belong to that cluster. For sim we use co-
sine similarity in our experiments.
Here, the probability that the word c is observed
in the context of word w
t
given the sense of the
word w
t
is,
P (D = 1|s
t
,v
s
(w
t
, 1), . . . , v
s
(w
t
,K), v
g
(c))
= P (D = 1|v
s
(w
t
, s
t
), v
g
(c))
=
1
1 + e
?v
s
(w
t
,s
t
)
T
v
g
(c)
The probability of not observing word c in the con-
text of w
t
given the sense of the word w
t
is,
P (D = 0|s
t
,v
s
(w
t
, 1), . . . , v
s
(w
t
,K), v
g
(c))
= P (D = 0|v
s
(w
t
, s
t
), v
g
(c))
= 1? P (D = 1|v
s
(w
t
, s
t
), v
g
(c))
Given a training set containing the sequence of
word types w
1
, w
2
, ..., w
T
, the word embeddings
are learned by maximizing the following objective
1062
Algorithm 1 Training Algorithm of MSSG model
1: Input: w
1
, w
2
, ..., w
T
, d, K, N .
2: Initialize v
s
(w, k) and v
g
(w), ?w ? W,k ?
{1, . . . ,K} randomly, ?(w, k) ?w ? W,k ?
{1, . . . ,K} to 0.
3: for t = 1, 2, . . . , T do
4: R
t
? {1, . . . , N}
5: c
t
= {w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
}
6: v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
7: s
t
= argmax
k=1,2,...,K
{
sim(?(w
t
, k), v
context
(c
t
))}
8: Update context cluster center ?(w
t
, s
t
)
since context c
t
is added to context cluster s
t
of word w
t
.
9: c
?
t
= Noisy Samples(c
t
)
10: Gradient update on v
s
(w
t
, s
t
), global vec-
tors of words in c
t
and c
?
t
.
11: end for
12: Output: v
s
(w, k), v
g
(w) and context cluster
centers ?(w, k), ?w ?W,k ? {1, . . . ,K}
function:
J(?) =
?
(w
t
,c
t
)?D
+
?
c?c
t
logP (D = 1|v
s
(w
t
, s
t
), v
g
(c))+
?
(w
t
,c
?
t
)?D
?
?
c
?
?c
?
t
logP (D = 0|v
s
(w
t
, s
t
), v
g
(c
?
))
where w
t
is the t
th
word in the sequence, c
t
is the
set of observed context words and c
?
t
is the set of
noisy context words for the word w
t
. D
+
and D
?
are constructed in the same way as in the Skip-
gram model.
After predicting the sense of word w
t
, we up-
date the embedding of the predicted sense for
the word w
t
(v
s
(w
t
, s
t
)), the global vector of the
words in the context and the global vector of the
randomly sampled, noisy context words. The con-
text cluster center of cluster s
t
for the word w
t
(?(w
t
, s
t
)) is updated since context c
t
is added to
the cluster s
t
.
5 Non-Parametric MSSG model
(NP-MSSG)
The MSSG model learns a fixed number of senses
per word type. In this section, we describe a
non-parametric version of MSSG, the NP-MSSG
model, which learns varying number of senses per
word type. Our approach is closely related to
the online non-parametric clustering procedure de-
scribed in Meyerson (2001). We create a new clus-
ter (sense) for a word type with probability propor-
tional to the distance of its context to the nearest
cluster (sense).
Each wordw ?W is associated with sense vec-
tors, context clusters and a global vector v
g
(w) as
in the MSSG model. The number of senses for a
word is unknown and is learned during training.
Initially, the words do not have sense vectors and
context clusters. We create the first sense vector
and context cluster for each word on its first occur-
rence in the training data. After creating the first
context cluster for a word, a new context cluster
and a sense vector are created online during train-
ing when the word is observed with a context were
the similarity between the vector representation of
the context with every existing cluster center of the
word is less than ?, where ? is a hyperparameter
of the model.
Consider the word w
t
and let c
t
=
{w
t?R
t
, . . . , w
t?1
, w
t+1
, . . . , w
t+R
t
} be the
set of observed context words. The vector repre-
sentation of the context is defined as the average
of the global vector representation of the words in
the context. Let v
context
(c
t
) =
1
2?R
t
?
c?c
t
v
g
(c)
be the vector representation of the context c
t
. Let
k(w
t
) be the number of context clusters or the
number of senses currently associated with word
w
t
. s
t
, the sense of word w
t
when k(w
t
) > 0 is
given by
s
t
=
?
?
?
?
?
k(w
t
) + 1, ifmax
k=1,2,...,k(w
t
)
{sim
(?(w
t
, k), v
context
(c
t
))} < ?
k
max
, otherwise
(4)
where ?(w
t
, k) is the cluster center of
the k
th
cluster of word w
t
and k
max
=
argmax
k=1,2,...,k(w
t
)
sim(?(w
t
, k), v
context
(c
t
)).
The cluster center is the average of the vector
representations of all the contexts which belong to
that cluster. If s
t
= k(w
t
) + 1, a new context
cluster and a new sense vector are created for the
word w
t
.
The NP-MSSG model and the MSSG model
described previously differ only in the way word
sense discrimination is performed. The objec-
tive function and the probabilistic model associ-
ated with observing a (word, context) pair given
the sense of the word remain the same.
1063
Model Time (in hours)
Huang et al 168
MSSG 50d 1
MSSG-300d 6
NP-MSSG-50d 1.83
NP-MSSG-300d 5
Skip-gram-50d 0.33
Skip-gram-300d 1.5
Table 1: Training Time Results. First five model
reported in the table are capable of learning mul-
tiple embeddings for each word and Skip-gram
is capable of learning only single embedding for
each word.
6 Experiments
To evaluate our algorithms we train embeddings
using the same corpus and vocabulary as used in
Huang et al (2012), which is the April 2010 snap-
shot of the Wikipedia corpus (Shaoul and West-
bury, 2010). It contains approximately 2 million
articles and 990 million tokens. In all our experi-
ments we remove all the words with less than 20
occurrences and use a maximum context window
(N ) of length 5 (5 words before and after the word
occurrence). We fix the number of senses (K) to
be 3 for the MSSG model unless otherwise speci-
fied. Our hyperparameter values were selected by
a small amount of manual exploration on a vali-
dation set. In NP-MSSG we set ? to -0.5. The
Skip-gram model, MSSG and NP-MSSG models
sample one noisy context word (S) for each of the
observed context words. We train our models us-
ing AdaGrad stochastic gradient decent (Duchi et
al, 2011) with initial learning rate set to 0.025.
Similarly to Huang et al (2012), we don?t use a
regularization penalty.
Below we describe qualitative results, display-
ing the embeddings and the nearest neighbors of
each word sense, and quantitative experiments in
two benchmark word similarity tasks.
Table 1 shows time to train our models, com-
pared with other models from previous work. All
these times are from single-machine implementa-
tions running on similar-sized corpora. We see
that our model shows significant improvement in
the training time over the model in Huang et
al (2012), being within well within an order-of-
magnitude of the training time for Skip-gram mod-
els.
APPLE
Skip-gram blackberry, macintosh, acorn, pear, plum
MSSG
pear, honey, pumpkin, potato, nut
microsoft, activision, sony, retail, gamestop
macintosh, pc, ibm, iigs, chipsets
NP-MSSG
apricot, blackberry, cabbage, blackberries, pear
microsoft, ibm, wordperfect, amiga, trs-80
FOX
Skip-gram abc, nbc, soapnet, espn, kttv
MSSG
beaver, wolf, moose, otter, swan
nbc, espn, cbs, ctv, pbs
dexter, myers, sawyer, kelly, griffith
NP-MSSG
rabbit, squirrel, wolf, badger, stoat
cbs,abc, nbc, wnyw, abc-tv
NET
Skip-gram profit, dividends, pegged, profits, nets
MSSG
snap, sideline, ball, game-trying, scoring
negative, offset, constant, hence, potential
pre-tax, billion, revenue, annualized, us$
NP-MSSG
negative, total, transfer, minimizes, loop
pre-tax, taxable, per, billion, us$, income
ball, yard, fouled, bounced, 50-yard
wnet, tvontorio, cable, tv, tv-5
ROCK
Skip-gram glam, indie, punk, band, pop
MSSG
rocks, basalt, boulders, sand, quartzite
alternative, progressive, roll, indie, blues-rock
rocks, pine, rocky, butte, deer
NP-MSSG
granite, basalt, outcropping, rocks, quartzite
alternative, indie, pop/rock, rock/metal, blues-rock
RUN
Skip-gram running, ran, runs, afoul, amok
MSSG
running, stretch, ran, pinch-hit, runs
operated , running, runs, operate, managed
running, runs, operate, drivers, configure
NP-MSSG
two-run, walk-off, runs, three-runs, starts
operated, runs, serviced, links, walk
running, operating, ran, go, configure
re-election, reelection, re-elect, unseat, term-limited
helmed, longest-running, mtv, promoted, produced
Table 2: Nearest neighbors of each sense of each
word, by cosine similarity, for different algo-
rithms. Note that the different senses closely cor-
respond to intuitions regarding the senses of the
given word types.
6.1 Nearest Neighbors
Table 2 shows qualitatively the results of dis-
covering multiple senses by presenting the near-
est neighbors associated with various embeddings.
The nearest neighbors of a word are computed by
comparing the cosine similarity between the em-
bedding for each sense of the word and the context
embeddings of all other words in the vocabulary.
Note that each of the discovered senses are indeed
semantically coherent, and that a reasonable num-
ber of senses are created by the non-parametric
method. Table 3 shows the nearest neighbors of
the word plant for Skip-gram, MSSG , NP-MSSG
and Haung?s model (Huang et al, 2012).
1064
Skip-
gram
plants, flowering, weed, fungus, biomass
MS
-SG
plants, tubers, soil, seed, biomass
refinery, reactor, coal-fired, factory, smelter
asteraceae, fabaceae, arecaceae, lamiaceae, eri-
caceae
NP
MS
-SG
plants, seeds, pollen, fungal, fungus
factory, manufacturing, refinery, bottling, steel
fabaceae, legume, asteraceae, apiaceae, flowering
power, coal-fired, hydro-power, hydroelectric, re-
finery
Hua
-ng
et al
insect, capable, food, solanaceous, subsurface
robust, belong, pitcher, comprises, eagles
food, animal, catching, catch, ecology, fly
seafood, equipment, oil, dairy, manufacturer
facility, expansion, corporation, camp, co.
treatment, skin, mechanism, sugar, drug
facility, theater, platform, structure, storage
natural, blast, energy, hurl, power
matter, physical, certain, expression, agents
vine, mute, chalcedony, quandong, excrete
Table 3: Nearest Neighbors of the word plant
for different models. We see that the discovered
senses in both our models are more semantically
coherent than Huang et al (2012) and NP-MSSG
is able to learn reasonable number of senses.
6.2 Word Similarity
We evaluate our embeddings on two related
datasets: the WordSim-353 (Finkelstein et al,
2001) dataset and the Contextual Word Similari-
ties (SCWS) dataset Huang et al (2012).
WordSim-353 is a standard dataset for evaluat-
ing word vector representations. It consists of a
list of pairs of word types, the similarity of which
is rated in an integral scale from 1 to 10. Pairs
include both monosemic and polysemic words.
These scores to each word pairs are given with-
out any contextual information, which makes them
tricky to interpret.
To overcome this issue, Stanford?s Contextual
Word Similarities (SCWS) dataset was developed
by Huang et al (2012). The dataset consists of
2003 word pairs and their sentential contexts. It
consists of 1328 noun-noun pairs, 399 verb-verb
pairs, 140 verb-noun, 97 adjective-adjective, 30
noun-adjective, 9 verb-adjective, and 241 same-
word pairs. We evaluate and compare our embed-
dings on both WordSim-353 and SCWS word sim-
ilarity corpus.
Since it is not trivial to deal with multiple em-
beddings per word, we consider the following sim-
ilarity measures between words w and w
?
given
their respective contexts c and c
?
, where P (w, c, k)
is the probability that w takes the k
th
sense given
the context c, and d(v
s
(w, i), v
s
(w
?
, j)) is the sim-
ilarity measure between the given embeddings
v
s
(w, i) and v
s
(w
?
, j).
The avgSim metric,
avgSim(w,w
?
)
=
1
K
2
K
?
i=1
K
?
j=1
d (v
s
(w, i), v
s
(w
?
, j)) ,
computes the average similarity over all embed-
dings for each word, ignoring information from
the context.
To address this, the avgSimC metric,
avgSimC(w,w
?
) =
K
?
j=1
K
?
i=1
P (w, c, i)P (w
?
, c
?
, j)
? d (v
s
(w, i), v
s
(w
?
, j))
weighs the similarity between each pair of senses
by how well does each sense fit the context at
hand.
The globalSim metric uses each word?s global
context vector, ignoring the many senses:
globalSim(w,w
?
) = d (v
g
(w), v
g
(w
?
)) .
Finally, localSim metric selects a single sense
for each word based independently on its context
and computes the similarity by
localSim(w,w
?
) = d (v
s
(w, k), v
s
(w
?
, k
?
)) ,
where k = argmax
i
P (w, c, i) and k
?
=
argmax
j
P (w
?
, c
?
, j) and P (w, c, i) is the prob-
ability that w takes the i
th
sense given context c.
The probability of being in a cluster is calculated
as the inverse of the cosine distance to the cluster
center (Huang et al, 2012).
We report the Spearman correlation between a
model?s similarity scores and the human judge-
ments in the datasets.
Table 5 shows the results on WordSim-353
task. C&W refers to the language model by Col-
lobert and Weston (2008) and HLBL model is the
method described in Mnih and Hinton (2007). On
WordSim-353 task, we see that our model per-
forms significantly better than the previous neural
network model for learning multi-representations
per word (Huang et al, 2012). Among the meth-
ods that learn low-dimensional and dense repre-
sentations, our model performs slightly better than
Skip-gram. Table 4 shows the results for the
SCWS task. In this task, when the words are
1065
Model globalSim avgSim avgSimC localSim
TF-IDF 26.3 - - -
Collobort & Weston-50d 57.0 - - -
Skip-gram-50d 63.4 - - -
Skip-gram-300d 65.2 - - -
Pruned TF-IDF 62.5 60.4 60.5 -
Huang et al-50d 58.6 62.8 65.7 26.1
MSSG-50d 62.1 64.2 66.9 49.17
MSSG-300d 65.3 67.2 69.3 57.26
NP-MSSG-50d 62.3 64.0 66.1 50.27
NP-MSSG-300d 65.5 67.3 69.1 59.80
Table 4: Experimental results in the SCWS task. The numbers are Spearmans correlation ? ? 100
between each model?s similarity judgments and the human judgments, in context. First three models
learn only a single embedding per model and hence, avgSim, avgSimC and localSim are not reported
for these models, as they?d be identical to globalSim. Both our parametric and non-parametric models
outperform the baseline models, and our best model achieves a score of 69.3 in this task. NP-MSSG
achieves the best results when globalSim, avgSim and localSim similarity measures are used. The best
results according to each metric are in bold face.
Model ?? 100
HLBL 33.2
C&W 55.3
Skip-gram-300d 70.4
Huang et al-G 22.8
Huang et al-M 64.2
MSSG 50d-G 60.6
MSSG 50d-M 63.2
MSSG 300d-G 69.2
MSSG 300d-M 70.9
NP-MSSG 50d-G 61.5
NP-MSSG 50d-M 62.4
NP-MSSG 300d-G 69.1
NP-MSSG 300d-M 68.6
Pruned TF-IDF 73.4
ESA 75
Tiered TF-IDF 76.9
Table 5: Results on the WordSim-353 dataset.
The table shows the Spearmans correlation ? be-
tween the model?s similarities and human judg-
ments. G indicates the globalSim similarity mea-
sure and M indicates avgSim measure.The best
results among models that learn low-dimensional
and dense representations are in bold face. Pruned
TF-IDF (Reisinger and Mooney, 2010a), ESA
(Gabrilovich and Markovitch, 2007) and Tiered
TF-IDF (Reisinger and Mooney, 2010b) construct
spare, high-dimensional representations.
Figure 3: The plot shows the distribution of num-
ber of senses learned per word type in NP-MSSG
model
given with their context, our model achieves new
state-of-the-art results on SCWS as shown in the
Table-4. The previous state-of-art model (Huang
et al, 2012) on this task achieves 65.7% using
the avgSimC measure, while the MSSG model
achieves the best score of 69.3% on this task. The
results on the other metrics are similar. For a
fixed embedding dimension, the model by Huang
et al (2012) has more parameters than our model
since it uses a hidden layer. The results show
that our model performs better than Huang et al
(2012) even when both the models use 50 dimen-
sional vectors and the performance of our model
improves as we increase the number of dimensions
to 300.
We evaluate the models in a word analogy task
1066
(a) (b)
Figure 4: Figures (a) and (b) show the effect of varying embedding dimensionality and number of senses
respectively of the MSSG Model on the SCWS task.
Model Task Sim ?? 100
Skip-gram WS-353 globalSim 70.4
MSSG WS-353 globalSim 68.4
MSSG WS-353 avgSim 71.2
NP MSSG WS-353 globalSim 68.3
NP MSSG WS-353 avgSim 69.66
MSSG SCWS localSim 59.3
MSSG SCWS globalSim 64.7
MSSG SCWS avgSim 67.2
MSSG SCWS avgSimC 69.2
NP MSSG SCWS localSim 60.11
NP MSSG SCWS globalSim 65.3
NP MSSG SCWS avgSim 67
NP MSSG SCWS avgSimC 68.6
Table 6: Experiment results on WordSim-353 and
SCWS Task. Multiple Embeddings are learned for
top 30,000 most frequent words in the vocabulary.
The embedding dimension size is 300 for all the
models for this task. The number of senses for
MSSG model is 3.
introduced by Mikolov et al (2013a) where both
MSSG and NP-MSSG models achieve 64% accu-
racy compared to 12% accuracy by Huang et al
(2012). Skip-gram which is the state-of-art model
for this task achieves 67% accuracy.
Figure 3 shows the distribution of number of
senses learned per word type in the NP-MSSG
model. We learn the multiple embeddings for the
same set of approximately 6000 words that were
used in Huang et al (2012) for all our experiments
to ensure fair comparision. These approximately
6000 words were choosen by Huang et al. mainly
from the top 30,00 frequent words in the vocab-
ulary. This selection was likely made to avoid
the noise of learning multiple senses for infre-
quent words. However, our method is robust to
noise, which can be seen by the good performance
of our model that learns multiple embeddings for
the top 30,000 most frequent words. We found
that even by learning multiple embeddings for the
top 30,000 most frequent words in the vocubu-
lary, MSSG model still achieves state-of-art result
on SCWS task with an avgSimC score of 69.2 as
shown in Table 6.
7 Conclusion
We present an extension to the Skip-gram model
that efficiently learns multiple embeddings per
word type. The model jointly performs word
sense discrimination and embedding learning, and
non-parametrically estimates the number of senses
per word type. Our method achieves new state-
of-the-art results in the word similarity in con-
text task and learns multiple senses, training on
close to billion tokens in less than 6 hours. The
global vectors, sense vectors and cluster centers of
our model and code for learning them are avail-
able at https://people.cs.umass.edu/
?
arvind/emnlp2014wordvectors. In fu-
ture work we plan to use the multiple embeddings
per word type in downstream NLP tasks.
1067
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
DARPA under agreement number FA8750-13-2-
0020. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright notation
thereon. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
those of the sponsor.
References
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring Continuous Word Representations
for Dependency Parsing. Association for Computa-
tional Linguistics (ACL).
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search (JMLR).
Peter F. Brown, Peter V. Desouza, Robert L. Mercer,
Vincent J. Della Pietra, and Jenifer C. Lai. 1992.
Class-based N-gram models of natural language
Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Process-
ing: Deep Neural Networks with Multitask Learn-
ing. International Conference on Machine learning
(ICML).
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.
2011. Multi-View Learning of Word Embeddings via
CCA. Advances in Neural Information Processing
Systems (NIPS).
John Duchi, Elad Hazan, and Yoram Singer 2011.
Adaptive sub- gradient methods for online learn-
ing and stochastic optimization. Journal of Machine
Learning Research (JMLR).
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: the con-
cept revisited. International Conference on World
Wide Web (WWW).
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. International Joint
Conference on Artificial Intelligence (IJCAI).
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. Association of Computational
Linguistics (ACL).
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Parsing.
Association for Computational Linguistics (ACL).
Quoc V. Le and Tomas Mikolov. 2014 Distributed
Representations of Sentences and Documents. Inter-
national Conference on Machine Learning (ICML)
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher Potts.
2011 Learning Word Vectors for Sentiment Analysis
Association for Computational Linguistics (ACL)
Adam Meyerson. 2001 IEEE Symposium on Foun-
dations of Computer Science. International Confer-
ence on Machine Learning (ICML)
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient Estimation of Word
Representations in Vector Space. Workshop at In-
ternational Conference on Learning Representations
(ICLR).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed Repre-
sentations of Words and Phrases and their Composi-
tionality. Advances in Neural Information Process-
ing Systems (NIPS).
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013c. Exploiting Similarities among Languages
for Machine Translation. arXiv.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies (NAACL-HLT).
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical language mod-
elling. International Conference on Machine learn-
ing (ICML).
Arvind Neelakantan and Michael Collins. 2014.
Learning Dictionaries for Named Entity Recogni-
tion using Minimal Supervision. European Chap-
ter of the Association for Computational Linguistics
(EACL).
Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon Infused Phrase Embeddings for
Named Entity Resolution. Conference on Natural
Language Learning (CoNLL).
Lev Ratinov and Dan Roth. 2009. Design Chal-
lenges and Misconceptions in Named Entity Recog-
nition. Conference on Natural Language Learning
(CoNLL).
Siva Reddy, Ioannis P. Klapaftis, and Diana McCarthy.
2011. Dynamic and Static Prototype Vectors for Se-
mantic Composition. International Joint Conference
on Artificial Intelligence (IJCNLP).
1068
Joseph Reisinger and Raymond J. Mooney. 2010a.
Multi-prototype vector-space models of word mean-
ing. North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies (NAACL-HLT)
Joseph Reisinger and Raymond Mooney. 2010b. A
mixture model with sharing for lexical semantics.
Empirical Methods in Natural Language Processing
(EMNLP).
Cyrus Shaoul and Chris Westbury. 2010. The Westbury
lab wikipedia corpus.
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011
Dynamic Pooling and Unfolding Recursive Autoen-
coders for Paraphrase Detection. Advances in Neu-
ral Information Processing Systems (NIPS).
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word Representations: A Simple and General
Method for Semi-Supervised Learning. Association
for Computational Linguistics (ACL).
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
dings for Phrase-Based Machine Translation. Em-
pirical Methods in Natural Language Processing.
1069
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 593?602,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Soft Linear Constraints with Application to Citation Field
Extraction
Sam Anzaroot Alexandre Passos David Belanger Andrew McCallum
Department of Computer Science
University of Massachusetts, Amherst
{anzaroot, apassos, belanger, mccallum}@cs.umass.edu
Abstract
Accurately segmenting a citation string
into fields for authors, titles, etc. is a chal-
lenging task because the output typically
obeys various global constraints. Previous
work has shown that modeling soft con-
straints, where the model is encouraged,
but not require to obey the constraints, can
substantially improve segmentation per-
formance. On the other hand, for impos-
ing hard constraints, dual decomposition
is a popular technique for efficient predic-
tion given existing algorithms for uncon-
strained inference. We extend dual decom-
position to perform prediction subject to
soft constraints. Moreover, with a tech-
nique for performing inference given soft
constraints, it is easy to automatically gen-
erate large families of constraints and learn
their costs with a simple convex optimiza-
tion problem during training. This allows
us to obtain substantial gains in accuracy
on a new, challenging citation extraction
dataset.
1 Introduction
Citation field extraction, an instance of informa-
tion extraction, is the task of segmenting and la-
beling research paper citation strings into their
constituent parts, including authors, editors, year,
journal, volume, conference venue, etc. This task
is important because citation data is often pro-
vided only in plain text; however, having an ac-
curate structured database of bibliographic infor-
mation is necessary for many scientometric tasks,
such as mapping scientific sub-communities, dis-
covering research trends, and analyzing networks
of researchers. Automated citation field extrac-
tion needs further research because it has not yet
reached a level of accuracy at which it can be prac-
tically deployed in real-world systems.
Hidden Markov models and linear-chain condi-
tional random fields (CRFs) have previously been
applied to citation extraction (Hetzner, 2008; Peng
and McCallum, 2004) . These models support ef-
ficient dynamic-programming inference, but only
model local dependencies in the output label se-
quence. However citations have strong global reg-
ularities not captured by these models. For exam-
ple many book citations contain both an author
section and an editor section, but none have two
disjoint author sections. Since linear-chain mod-
els are unable to capture more than Markov depen-
dencies, the models sometimes mislabel the editor
as a second author. If we could enforce the global
constraint that there should be only one author
section, accuracy could be improved.
One framework for adding such global con-
straints into tractable models is constrained infer-
ence, in which at inference time the original model
is augmented with restrictions on the outputs such
that they obey certain global regularities. When
hard constraints can be encoded as linear equa-
tions on the output variables, and the underlying
model?s inference task can be posed as linear opti-
mization, one can formulate this constrained infer-
ence problem as an integer linear program (ILP)
(Roth and Yih, 2004). Alternatively, one can em-
ploy dual decomposition (Rush et al, 2010). Dual
decompositions?s advantage over ILP is is that it
can leverage existing inference algorithms for the
original model as a black box. Such a modular
algorithm is easy to implement, and works quite
well in practice, providing certificates of optimal-
ity for most examples.
The above two approaches have previously been
applied to impose hard constraints on a model?s
output. On the other hand, recent work has demon-
strated improvements in citation field extraction
by imposing soft constraints (Chang et al, 2012).
Here, the model is not required obey the global
constraints, but merely pays a penalty for their vi-
593
4 .
ref-marker
[ J.
first
D.
middle
Monk ,
last person
]
authors
[ Cardinal Functions on Boolean Algebra , ]
title
[ Lectures in Mathematics
, ETH Zurich ,
series
Birkhause Verlag ,
publisher
Basel , Boston , Berlin ,
address
1990 .
year date
]
venue
Figure 1: Example labeled citation
olation.
This paper introduces a novel method for im-
posing soft constraints via dual decomposition.
We also propose a method for learning the penal-
ties the prediction problem incurs for violating
these soft constraints. Because our learning
method drives many penalties to zero, it allows
practitioners to perform ?constraint selection,? in
which a large number of automatically-generated
candidate global constraints can be considered and
automatically culled to a smaller set of useful con-
straints, which can be run quickly at test time.
Using our new method, we are able to incor-
porate not only all the soft global constraints of
Chang et al (2012), but also far more com-
plex data-driven constraints, while also provid-
ing stronger optimality certificates than their beam
search technique. On a new, more broadly rep-
resentative, and challenging citation field extrac-
tion data set, we show that our methods achieve a
17.9% reduction in error versus a linear-chain con-
ditional random field. Furthermore, we demon-
strate that our inference technique can use and
benefit from the constraints of Chang et al (2012),
but that including our data-driven constraints on
top of these is beneficial. While this paper fo-
cusses on an application to citation field extrac-
tion, the novel methods introduced here would
easily generalize to many problems with global
output regularities.
2 Background
2.1 Structured Linear Models
The overall modeling technique we employ is to
add soft constraints to a simple model for which
we have an existing efficient prediction algorithm.
For this underlying model, we employ a chain-
structured conditional random field (CRF), since
CRFs have been shown to perform better than
other simple unconstrained models like hidden
markov models for citation extraction (Peng and
McCallum, 2004). We produce a prediction by
performing MAP inference (Koller and Friedman,
2009).
The MAP inference task in a CRF be can ex-
pressed as an optimization problem with a lin-
ear objective (Sontag, 2010; Sontag et al, 2011).
Here, we define a binary indicator variable for
each candidate setting of each factor in the graph-
ical model. Each of these indicator variables is
associated with the score that the factor takes on
when it has the indictor variable?s corresponding
value. Since the log probability of some y in the
CRF is proportional to sum of the scores of all the
factors, we can concatenate the indicator variables
as a vector y and the scores as a vectorw and write
the MAP problem as
max. ?w, y?
s.t. y ? U ,
(1)
where the set U represents the set of valid config-
urations of the indicator variables. Here, the con-
straints are that all neighboring factors agree on
the components of y in their overlap.
Structured Linear Models are the general fam-
ily of models where prediction requires solving a
problem of the form (1), and they do not always
correspond to a probabilistic model. The algo-
rithms we present in later sections for handling
soft global constraints and for learning the penal-
ties of these constraints can be applied to gen-
eral structured linear models, not just CRFs, pro-
vided we have an available algorithm for perform-
ing MAP inference.
2.2 Dual Decomposition for Global
Constraints
In order to perform prediction subject to various
global constraints, we may need to augment the
problem (1) with additional constraints. Dual De-
composition is a popular method for performing
MAP inference in this scenario, since it lever-
ages known algorithms for MAP in the base prob-
lem where these extra constraints have not been
added (Komodakis et al, 2007; Sontag et al,
2011; Rush and Collins, 2012). In this case, the
MAP problem can be formulated as a structured
linear model similar to equation (1), for which we
have a MAP algorithm, but where we have im-
posed some additional constraints Ay ? b that
no longer allow us to use the algorithm. In other
594
Algorithm 1 DD: projected subgradient for dual
decomposition with hard constraints
1: while has not converged do
2: y
(t)
= argmax
y?U
?
w +A
T
?, y
?
3: ?
(t)
= ?
0??
[
?
(t?1)
? ?
(t)
(Ay ? b)
]
words, we consider the problem
max. ?w, y?
s.t. y ? U
Ay ? b,
(2)
for an arbitrary matrix A and vector b. We can
write the Lagrangian of this problem as
L(y, ?) = ?w, y?+ ?
T
(Ay ? b). (3)
Regrouping terms and maximizing over the primal
variables, we have the dual problem
min.
?
D(?) = max
y?U
?
w +A
T
?, y
?
? ?
T
b. (4)
For any ?, we can evaluate the dual objective
D(?), since the maximization in (4) is of the same
form as the original problem (1), and we assumed
we had a method for performing MAP in this. Fur-
thermore, a subgradient ofD(?) isAy
?
?b, for an
y
?
which maximizes this inner optimization prob-
lem. Therefore, we can minimize D(?) with the
projected subgradient method (Boyd and Vanden-
berghe, 2004), and the optimal y can be obtained
when evaluating D(?
?
). Note that the subgradient
of D(?) is the amount by which each constraint is
violated by ? when maximizing over y.
Algorithm 1 depicts the basic projected subgra-
dient descent algorithm for dual decomposition.
The projection operator ? consists of truncating
all negative coordinates of ? to 0. This is neces-
sary because ? is a vector of dual variables for in-
equality constraints. The algorithm has converged
when each constraint is either satisfied by y
(t)
with
equality or its corresponding component of ? is 0,
due to complimentary slackness (Boyd and Van-
denberghe, 2004).
3 Soft Constraints in Dual
Decomposition
We now introduce an extension of Algorithm 1
to handle soft constraints. In our formulation, a
soft-constrained model imposes a penalty for each
unsatisfied constraint, proportional to the amount
by which it is violated. Therefore, our derivation
parallels how soft-margin SVMs are derived from
hard-margin SVMs by introducing auxiliary slack
variables (Cortes and Vapnik, 1995). Note that
when performing MAP subject to soft constraints,
optimal solutions might not satisfy some con-
straints, since doing so would reduce the model?s
score by too much.
Consider the optimization problems of the
form:
max. ?w, y? ? ?c, z?
s.t. y ? U
Ay ? b ? z
?z ? 0,
(5)
For positive c
i
, it is clear that an optimal z
i
will
be equal to the degree to which a
T
i
y ? b
i
is vio-
lated. Therefore, we pay a cost c
i
times the degree
to which the ith constraint is violated, which mir-
rors how slack variables are used to represent the
hinge loss for SVMs. Note that c
i
has to be pos-
itive, otherwise this linear program is unbounded
and an optimal value can be obtained by setting z
i
to infinity.
Using a similar construction as in section 2.2 we
write the Lagrangian as:
(6)
L(y, z, ?, ?) = ?w, y? ? ?c, z?
+ ?
T
(Ay ? b? z) + ?
T
(?z).
The optimality constraints with respect to z tell us
that ?c? ?? ? = 0, hence ? = ?c? ?. Substi-
tuting, we have
L(y, ?) = ?w, y?+ ?
T
(Ay ? b), (7)
except the constraint that ? = ?c? ? implies that
for ? to be positive ? ? c.
Since this Lagrangian has the same form as
equation (3), we can also derive a dual problem,
which is the same as in equation (4), with the ad-
ditional constraint that each ?
i
can not be bigger
than its cost c
i
. In other words, the dual problem
can not penalize the violation of a constraint more
than the soft constraint model in the primal would
penalize you if you violated it.
This optimization problem can still be solved
with projected subgradient descent and is depicted
in Algorithm 2. The only modifications to Al-
gorithm 1 are replacing the coordinate-wise pro-
jection ?
0??
with ?
0???c
and how we check for
convergence. Now, we check for the KKT con-
ditions of (5), where for every constraint i, either
the constraint is satisfied with equality, ?
i
= 0, or
?
i
= c
i
.
595
Algorithm 2 Soft-DD: projected subgradient for
dual decomposition with soft constraints
1: while has not converged do
2: y
(t)
= argmax
y?U
?
w +A
T
?, y
?
3: ?
(t)
= ?
0???c
[
?
(t?1)
? ?
(t)
(Ay ? b)
]
Therefore, implementing soft-constrained dual
decomposition is as easy as implementing hard-
constrained dual decomposition, and the per-
iteration complexity is the same. We encourage
further applications of soft-constraint dual decom-
position to existing and new NLP problems.
3.1 Learning Penalties
One consideration when using soft v.s. hard con-
straints is that soft constraints present a new train-
ing problem, since we need to choose the vector
c, the penalties for violating the constraints. An
important property of problem (5) in the previous
section is that it corresponds to a structured lin-
ear model over y and z. Therefore, we can apply
known training algorithms for estimating the pa-
rameters of structured linear models to choose c.
All we need to employ the structured perceptron
algorithm (Collins, 2002) or the structured SVM
algorithm (Tsochantaridis et al, 2004) is a black-
box procedure for performing MAP inference in
the structured linear model given an arbitrary cost
vector. Fortunately, the MAP problem for (5) can
be solved using Soft-DD, in Algorithm 2.
Each penalty c
i
has to be non-negative; other-
wise, the optimization problem in equation (5) is
ill-defined. This can be ensured by simple mod-
ifications of the perceptron and subgradient de-
scent optimization of the structured SVM objec-
tive simply by truncating c coordinate-wise to be
non-negative at every learning iteration.
Intuitively, the perceptron update increases the
penalty for a constraint if it is satisfied in the
ground truth and not in an inferred prediction, and
decreases the penalty if the constraint is satisfied
in the prediction and not the ground truth. Since
we truncate penalties at 0, this suggests that we
will learn a penalty of 0 for constraints in three cat-
egories: constraints that do not hold in the ground
truth, constraints that hold in the ground truth but
are satisfied in practice by performing inference
in the base CRF model, and constraints that are
satisfied in practice as a side-effect of imposing
non-zero penalties on some other constraints . A
similar analysis holds for the structured SVM ap-
proach.
Therefore, we can view learning the values of
the penalties not just as parameter tuning, but as a
means to perform ?constraint selection,? since con-
straints that have a penalty of 0 can be ignored.
This property allows us to consider large families
of constraints, from which the useful ones are au-
tomatically identified.
We found it beneficial, though it is not theoreti-
cally necessary, to learn the constraints on a held-
out development set, separately from the other
model parameters, as during training most con-
straints are satisfied due to overfitting, which leads
to an underestimation of the relevant penalties.
4 Citation Extraction Data
We consider the UMass citation dataset, first intro-
duced in Anzaroot and McCallum (2013). It has
over 1800 citation from many academic fields, ex-
tracted from the arXiv. This dataset contains both
coarse-grained and fine-grained labels; for exam-
ple it contains labels for the segment of all authors,
segments for each individual author, and for the
first and last name of each author. There are 660
citations in the development set and 367 citation
in the test set.
The labels in the UMass dataset are a con-
catenation of labels from a hierarchically-defined
schema. For example, a first name of an author is
tagged as: authors/person/first. In addition, indi-
vidual tokens are labeled using a BIO label schema
for each level in the hierarchy. BIO is a commonly
used labeling schema for information extraction
tasks. BIO labeling allows individual labels on
tokens to label segmentation information as well
as labels for the segments. In this schema, labels
that begin segments are prepended with a B, la-
bels that continue a segment are prepended with
an I, and tokens that don?t have a labeling in this
schema are given an O label. For example, in a hi-
erarchical BIO label schema the first token in the
first name for the second author may be labeled as:
I-authors/B-person/B-first.
An example labeled citation in this dataset can
be viewed in figure 1.
5 Global Constraints for Citation
Extraction
5.1 Constraint Templates
We now describe the families of global constraints
we consider for citation extraction. Note these
596
constraints are all linear, since they depend only
on the counts of each possible conditional ran-
dom field label. Moreover, since our labels are
BIO-encoded, it is possible, by counting B tags,
to count how often each citation tag itself appears
in a sentence. The first two families of constraints
that we describe are general to any sequence la-
beling task while the last is specific to hierarchical
labeling such as available in the UMass dataset.
Our sequence output is denoted as y and an ele-
ment of this sequence is y
k
.
We denote [[y
k
= i]] as the function that outputs
1 if y
k
has a 1 at index i and 0 otherwise. Here, y
k
represents an output tag of the CRF, so if [[y
k
= i]]
= 1, then we have that y
k
was given a label with
index i.
5.2 Singleton Constraints
Singleton constraints ensure that each label can
appear at most once in a citation. These are same
global constraints that were used for citation field
extraction in Chang et al (2012). We define s(i)
to be the number of times the label with index i is
predicted in a citation, formally:
s(i) =
?
y
k
?y
[[y
k
= i]]
The constraint that each label can appear at
most once takes the form:
s(i) <= 1
5.3 Pairwise Constraints
Pairwise constraints are constraints on the counts
of two labels in a citation. We define z
1
(i, j) to be
z
1
(i, j) =
?
y
k
?y
[[y
k
= i]] +
?
y
k
?y
[[y
k
= j]]
and z
2
(i, j) to be
z
2
(i, j) =
?
y
k
?y
[[y
k
= i]]?
?
y
k
?y
[[y
k
= j]]
We consider all constraints of the forms:
z(i, j) ? 0, 1, 2, 3 and z(i, j) ? 0, 1, 2, 3.
Note that some pairs of these constraints are re-
dundant or logically incompatible. However, we
are using them as soft constraints, so these con-
straints will not necessarily be satisfied by the out-
put of the model, which eliminates concern over
enforcing logically impossible outputs. Further-
more, in section 3.1 we described how our proce-
dure for learning penalties will drive some penal-
ties to 0, which effectively removes them from our
set of constraints we consider. It can be shown, for
example, that we will never learn non-zero penal-
ties for certain pairs of logically incompatible con-
straints using the perceptron-style algorithm de-
scribed in section 3.1 .
5.4 Hierarchical Equality Constraints
The labels in the citation dataset are hierarchical
labels. This means that the labels are the concate-
nation of all the levels in the hierarchy. We can
create constraints that are dependent on only one
or couple of elements in the hierarchy.
We define C(x, i) as the function that returns 1
if the output x contains the label i in the hierarchy
and 0 otherwise. We define e(i, j) to be
e(i, j) =
?
y
k
?y
[[C(y
k
, i)]]?
?
y
k
?y
[[C(y
k
, j)]]
Hierarchical equality constraints take the forms:
e(i, j) ? 0 (8)
e(i, j) ? 0 (9)
5.5 Local constraints
We constrain the output labeling of the chain-
structured CRF to be a valid BIO encoding.
This both improves performance of the underly-
ing model when used without global constraints,
as well as ensures the validity of the global con-
straints we impose, since they operate only on
B labels. The constraint that the labeling is
valid BIO can be expressed as a collection of
pairwise constraints on adjacent labels in the se-
quence. Rather than enforcing these constraints
using dual decomposition, they can be enforced
directly when performing MAP inference in the
CRF by modifying the dynamic program of the
Viterbi algorithm to only allow valid pairs of adja-
cent labels.
5.6 Constraint Pruning
While the techniques from section 3.1 can easily
cope with a large numbers of constraints at train-
ing time, this can be computationally costly, spe-
cially if one is considering very large constraint
families. This is problematic because the size
597
Unconstrained
[17]
ref-marker
[ D.
first
Sivia ,
last person
J.
first
Skilling ,
last person
]
authors
[ Data Analysis : A Bayesian Tutorial
,
booktitle
Oxford University Press ,
publisher
2006
year date
]
venue
Constrained
[17]
ref-marker
[ D.
first
Sivia ,
last person
J.
first
Skilling ,
last person
]
authors
Data Analysis : A Bayesian Tutorial
,
title
[ Oxford University Press ,
publisher
2006
year date
]
venue
Unconstrained
[ Sobol? ,
last
I.
first
M.
middle person
]
authors
[ (1990) .
year
]
date
[On sensitivity estimation for nonlinear mathe-
matical models .]
title
[ Matematicheskoe Modelirovanie ,
journal
2
volume
(1) :
number
112?118 .
pages
( In Russian
) .
status
]
venue
Constrained
[ Sobol? ,
last
I.
first
M.
middle person
]
authors
[ (1990) .
year
]
date
[On sensitivity estimation for nonlinear mathe-
matical models .]
title
[ Matematicheskoe Modelirovanie ,
journal
2
volume
(1) :
number
112?118 .
pages
( In Russian
) .
language
]
venue
Figure 2: Two examples where imposing soft global constraints improves field extraction errors. Soft-
DD converged in 1 iteration on the first example, and 7 iterations on the second. When a reference is
citing a book and not a section of the book, the correct labeling of the name of the book is title. In
the first example, the baseline CRF incorrectly outputs booktitle, but this is fixed by Soft-DD, which
penalizes outputs based on the constraint that booktitle should co-occur with an address label. In the
second example, the unconstrained CRF output violates the constraint that title and status labels should
not co-occur. The ground truth labeling also violates a constraint that title and language labels should
not co-occur. At convergence of the Soft-DD algorithm, the correct labeling of language is predicted,
which is possible because of the use of soft constraints.
Constraints F1 score Sparsity # of cons
Baseline 94.44
Only-one 94.62 0% 3
Hierarchical 94.55 56.25% 16
Pairwise 95.23 43.19% 609
All 95.39 32.96% 628
All DD 94.60 0% 628
Table 1: Set of constraints learned and F1 scores.
The last row depicts the result of inference using
all constraints as hard constraints.
of some constraint families we consider grows
quadratically with the number of candidate labels,
and there are about 100 in the UMass dataset.
Such a family consists of constraints that the sum
of the counts of two different label types has to
be bounded (a useful example is that there can?t
be more than one out of ?phd thesis? and ?jour-
nal?). Therefore, quickly pruning bad constraints
can save a substantial amount of training time, and
can lead to better generalization.
To do so, we calculate a score that estimates
how useful each constraint is expected to be. Our
score compares how often the constraint is vio-
lated in the ground truth examples versus our pre-
dictions. Here, prediction is done with respect to
the base chain-structured CRF tagger and does not
include global constraints. Note that it may make
sense to consider a constraint that is sometimes vi-
olated in the ground truth, as the penalty learning
algorithm can learn a small penalty for it, which
will allow it to be violated some of the time. Our
importance score is defined as, for each constraint
c on labeled set D,
imp(c) =
?
d?D
[[max
y
w
T
d
y]]
c
?
d?D
[[y
d
]]
c
, (10)
where [[y]]
c
is 1 if the constraint is violated on out-
put y and 0 otherwise. Here, y
d
denotes the ground
truth labeling and w
d
is the vector of scores for the
CRF tagger.
We prune constraints by picking a cutoff value
for imp(c). A value of imp(c) above 1 implies
that the constraint is more violated on the pre-
dicted examples than on the ground truth, and
hence that we might want to keep it.
We also find that the constraints that have the
largest imp values are semantically interesting.
6 Related Work
There are multiple previous examples of augment-
ing chain-structured sequence models with terms
capturing global relationships by expanding the
chain to a more complex graphical model with
non-local dependencies between the outputs. In-
ference in these models can be performed, for
example, with loopy belief propagation (Bunescu
and Mooney, 2004; Sutton and McCallum, 2004)
or Gibbs sampling (Finkel et al, 2005). Be-
lief propagation is prohibitively expensive in our
598
model due to the high cardinalities of the out-
put variables and of the global factors, which in-
volve all output variables simultaneously. There
are various methods for exploiting the combi-
natorial structure of these factors, but perfor-
mance would still have higher complexity than our
method. While Gibbs sampling has been shown
to work well tasks such as named entity recogni-
tion (Finkel et al, 2005), our previous experiments
show that it does not work well for citation extrac-
tion, where it found only low-quality solutions in
practice because the sampling did not mix well,
even on a simple chain-structured CRF.
Recently, dual decomposition has become a
popular method for solving complex structured
prediction problems in NLP (Koo et al, 2010;
Rush et al, 2010; Rush and Collins, 2012; Paul
and Eisner, 2012; Chieu and Teow, 2012). Soft
constraints can be implemented inefficiently using
hard constraints and dual decomposition? by in-
troducing copies of output variables and an aux-
iliary graphical model, as in Rush et al (2012).
However, at every iteration of dual decomposition,
MAP must be run in this auxiliary model. Further-
more the copying of variables doubles the num-
ber of iterations needed for information to flow
between output variables, and thus slows conver-
gence. On the other hand, our approach to soft
constraints has identical per-iteration complexity
as for hard constraints, and is a very easy modifi-
cation to existing hard constraint code.
Initial work in machine learning for citation ex-
traction used Markov models with no global con-
straints. Hidden Markov models (HMMs), were
originally employed for automatically extracting
information from research papers on the CORA
dataset (Seymore et al, 1999; Hetzner, 2008).
Later, CRFs were shown to perform better on
CORA, improving the results from the Hmm?s
token-level F1 of 86.6 to 91.5 with a CRF(Peng
and McCallum, 2004).
Recent work on globally-constrained inference
in citation extraction used an HMM
CCM
, which is
an HMM with the addition of global features that
are restricted to have positive weights (Chang et
al., 2012). Approximate inference is performed
using beam search. This method increased the
HMM token-level accuracy from 86.69 to 93.92
on a test set of 100 citations from the CORA
dataset. The global constraints added into the
model are simply that each label only occurs
once per citation. This approach is limited in its
use of an HMM as an underlying model, as it
has been shown that CRFs perform significantly
better, achieving 95.37 token-level accuracy on
CORA (Peng and McCallum, 2004). In our ex-
periments, we demonstrate that the specific global
constraints used by Chang et al (2012) help on the
UMass dataset as well.
7 Experimental Results
Our baseline is the one used in Anzaroot and
McCallum (2013), with some labeling errors re-
moved. This is a chain-structured CRF trained
to maximize the conditional likelihood using L-
BFGS with L2 regularization.
We use the same features as Anzaroot and Mc-
Callum (2013), which include word type, capital-
ization, binned location in citation, regular expres-
sion matches, and matches into lexicons. In addi-
tion, we use a rule-based segmenter that segments
the citation string based on punctuation as well as
probable start or end segment words (e.g. ?in? and
?volume?). We add a binary feature to tokens that
correspond to the start of a segment in the output
of this simple segmenter. This final feature im-
proves the F1 score on the cleaned test set from
94.0 F1 to 94.44 F1, which we use as a baseline
score.
We then use the development set to learn the
penalties for the soft constraints, using the percep-
tron algorithm described in section 3.1. MAP in-
ference in the model with soft constraints is per-
formed using Soft-DD, shown in Algorithm 2.
We instantiate constraints from each template in
section 5.1, iterating over all possible labels that
contain a B prefix at any level in the hierarchy and
pruning all constraints with imp(c) < 2.75 cal-
culated on the development set. We asses perfor-
mance in terms of field-level F1 score, which is
the harmonic mean of precision and recall for pre-
dicted segments.
Table 1 shows how each type of constraint fam-
ily improved the F1 score on the dataset. Learning
all the constraints jointly provides the largest im-
provement in F1 at 95.39. This improvement in F1
over the baseline CRF as well as the improvement
in F1 over using only-one constraints was shown
to be statistically significant using the Wilcoxon
signed rank test with p-values < 0.05. In the
all-constraints settings, 32.96% of the constraints
have a learned parameter of 0, and therefore only
599
Stop F1 score Convergence Avg Iterations
1 94.44 76.29% 1.0
2 95.07 83.38% 1.24
5 95.12 95.91% 1.61
10 95.39 99.18% 1.73
Table 2: Performance from terminating Soft-DD
early. Column 1 is the number of iterations we
allow each example. Column 3 is the % of test
examples that converged. Column 4 is the aver-
age number of necessary iterations, a surrogate for
the slowdown over performing unconstrained in-
ference.
421 constraints are active. Soft-DD converges,
and thus solves the constrained inference prob-
lem exactly, for all test set examples after at most
41 iterations. Running Soft-DD to convergence
requires 1.83 iterations on average per example.
Since performing inference in the CRF is by far
the most computationally intensive step in the iter-
ative algorithm, this means our procedure requires
approximately twice as much work as running the
baseline CRF on the dataset. On examples where
unconstrained inference does not satisfy the con-
straints, Soft-DD converges after 4.52 iterations
on average. For 11.99% of the examples, the
Soft-DD algorithm satisfies constraints that were
not satisfied during unconstrained inference, while
in the remaining 11.72% Soft-DD converges with
some constraints left unsatisfied, which is possible
since we are imposing them as soft constraints.
We could have enforced these constraints as
hard constraints rather than soft ones. This exper-
iment is shown in the last row of Table 1, where
F1 only improves to 94.6. In addition, running
the DD algorithm with these constraints takes 5.21
iterations on average per example, which is 2.8
times slower than Soft-DD with learned penalties.
In Figure 2, we analyze the performance of
Soft-DD when we don?t necessarily run it to con-
vergence, but stop after a fixed number of itera-
tions on each test set example. We find that a large
portion of our gain in accuracy can be obtained
when we allow ourselves as few as 2 dual decom-
position iterations. However, this only amounts to
1.24 times as much work as running the baseline
CRF on the dataset, since the constraints are satis-
fied immediately for many examples.
In Figure 2 we consider two applications of our
Soft-DD algorithm, and provide analysis in the
caption.
We train and evaluate on the UMass dataset in-
stead of CORA, because it is significantly larger,
has a useful finer-grained labeling schema, and its
annotation is more consistent. We were able to ob-
tain better performance on CORA using our base-
line CRF than the HMM
CCM
results presented
in Chang et al (2012), which include soft con-
straints. Given this high performance of our base
model on CORA, we did not apply our Soft-DD
algorithm to the dataset. Furthermore, since the
dataset is so small, learning the penalties for our
large collection of constraints is difficult, and test
set results are unreliable. Rather than compare our
work to Chang et al (2012) via results on CORA,
we apply their constraints on the UMass data us-
ing Soft-DD and demonstrate accuracy gains, as
discussed above.
7.1 Examples of learned constraints
We now describe a number of the useful con-
straints that receive non-zero learned penalties
and have high importance scores, defined in Sec-
tion 5.6. The importance score of a constraint pro-
vides information about how often it is violated
by the CRF, but holds in the ground truth, and a
non-zero penalty implies we enforce it as a soft
constraint at test time.
The two singleton constraints with highest im-
portance score are that there should only be at
most one title segment in a citation and that there
should be at most one author segment in a cita-
tion. The only one author constraint is particu-
larly useful for correctly labeling editor segments
in cases where unconstrained inference mislabels
them as author segments. As can be seen in Table
3, editor fields are among the most improved with
our new method, largely due to this constraint.
The two hierarchical constraints with the high-
est importance scores with non-zero learned
penalties constrain the output such that number
of person segments does not exceed the number
of first segments and vice-versa. Together, these
constraints penalize outputs in which the number
of person segments do not equal the number of
first segments, i.e., every author should have a first
name.
One important pairwise constraint penalizes
outputs in which thesis segments don?t co-occur
with school segments. School segments label the
name of the university that the thesis was submit-
ted to. The application of this constraint increases
the performance of the model on school segments
600
Label U C +
venue/series 35.29 66.67 31.37
venue/editor/person/first 66.67 94.74 28.07
venue/school 40.00 66.67 26.67
venue/editor/person/last 75.00 94.74 19.74
venue/editor 77.78 90.00 12.22
venue/editor/person/middle 81.82 91.67 9.85
Table 3: Labels with highest improvement in F1.
U is in unconstrained inference. C is the results of
constrained inference. + is the improvement in F1.
dramatically, as can be seen in table 3.
An interesting form of pairwise constraints pe-
nalize outputs in which some labels do not co-
occur with other labels. Some examples of con-
straints in this form enforce that journal segments
should co-occur with pages segments and that
booktitle segments should co-occur with address
segments. An example of the latter constraint be-
ing employed during inference is the first example
in Figure 2. Here, the constrained inference pe-
nalizes output which contains a booktitle segment
but no address segment. This penalization leads
allows the constrained inference to correctly label
the booktitle segment as a title segment.
The above example constraints are almost al-
ways satisfied on the ground truth, and would be
useful to enforce as hard constraints. However,
there are a number of learned constraints that are
often violated on the ground truth but are still use-
ful as soft constraints. Take, for example, the con-
straint that the number of number segments does
not exceed the number of booktitle segments, as
well as the constraint that it does not exceed the
number of journal segments. These constraints
are moderately violated on ground truth examples,
however. For example, when booktitle segments
co-occur with number segments but not with jour-
nal segments, the second constraint is violated. It
is still useful to impose these soft constraints, as
strong evidence from the CRF allows us to violate
them, and they can guide the model to good pre-
dictions when the CRF is unconfident.
8 Conclusion
We introduce a novel modification to the stan-
dard projected subgradient dual decomposition al-
gorithm for performing MAP inference subject to
hard constraints to one for performing MAP in the
presence of soft constraints. In addition, we offer
an easy-to-implement procedure for learning the
penalties on soft constraints. This method drives
many penalties to zero, which allows users to auto-
matically discover discriminative constraints from
large families of candidates.
We show via experiments on a recent substantial
dataset that using soft constraints, and selecting
which constraints to use with our penalty-learning
procedure, can lead to significant gains in accu-
racy. We achieve a 17% gain in accuracy over
a chain-structured CRF model, while only need-
ing to run MAP in the CRF an average of less
than 2 times per example. This minor incremen-
tal cost over Viterbi, plus the fact that we obtain
certificates of optimality on 100% of our test ex-
amples in practice, suggests the usefulness of our
algorithm for large-scale applications. We encour-
age further use of our Soft-DD procedure for other
structured prediction problems.
Acknowledgments
This work was supported in part by the Cen-
ter for Intelligent Information Retrieval, in part
by DARPA under agreement number FA8750-13-
2-0020, in part by NSF grant #CNS-0958392,
and in part by IARPA via DoI/NBC contract
#D11PC20152. The U.S. Government is autho-
rized to reproduce and distribute reprint for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect those of the sponsor.
References
Sam Anzaroot and Andrew McCallum. 2013. A new
dataset for fine-grained citation field extraction. In
ICML Workshop on Peer Reviewing and Publishing
Models.
Stephen Poythress Boyd and Lieven Vandenberghe.
2004. Convex optimization. Cambridge university
press.
Razvan Bunescu and Raymond J Mooney. 2004.
Collective information extraction with relational
markov networks. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 438. Association for Computational
Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2012. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399?431, 6.
Hai Leong Chieu and Loo-Nin Teow. 2012. Com-
bining local and non-local information with dual de-
composition for named entity recognition from text.
601
In Information Fusion (FUSION), 2012 15th Inter-
national Conference on, pages 231?238. IEEE.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Association for Computational Linguistics.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363?370. Association for Computational Lin-
guistics.
Erik Hetzner. 2008. A simple method for citation
metadata extraction using hidden markov models. In
Proceedings of the 8th ACM/IEEE-CS joint confer-
ence on Digital libraries, pages 280?284. ACM.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. The
MIT Press.
Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. Mrf optimization via dual decomposi-
tion: Message-passing revisited. In Computer Vi-
sion, 2007. ICCV 2007. IEEE 11th International
Conference on, pages 1?8. IEEE.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1288?1298. Association for Compu-
tational Linguistics.
Michael J Paul and Jason Eisner. 2012. Implicitly in-
tersecting weighted automata using dual decompo-
sition. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 232?242. Association for Computa-
tional Linguistics.
Fuchun Peng and Andrew McCallum. 2004. Accu-
rate information extraction from research papers us-
ing conditional random fields. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 329?336, Boston,
Massachusetts, USA, May 2 - May 7. Association
for Computational Linguistics.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Defense Technical Information Center.
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and lagrangian relax-
ation for inference in natural language processing.
J. Artif. Intell. Res. (JAIR), 45:305?362.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1?11. Association for Computa-
tional Linguistics.
Alexander M Rush, Roi Reichart, Michael Collins, and
Amir Globerson. 2012. Improved parsing and pos
tagging using inter-sentence consistency constraints.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1434?1444.
Kristie Seymore, Andrew McCallum, Roni Rosenfeld,
et al 1999. Learning hidden markov model struc-
ture for information extraction. In AAAI-99 Work-
shop on Machine Learning for Information Extrac-
tion, pages 37?42.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Ma-
chine Learning. MIT Press.
David Sontag. 2010. Approximate Inference in Graph-
ical Models using LP Relaxations. Ph.D. thesis,
Massachusetts Institute of Technology, Department
of Electrical Engineering and Computer Science.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant entities
in information extraction. Technical report, DTIC
Document.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first in-
ternational conference on Machine learning, page
104. ACM.
602
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 78?86,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Lexicon Infused Phrase Embeddings for Named Entity Resolution
Alexandre Passos, Vineet Kumar, Andrew McCallum
School of Computer Science
University of Massachusetts, Amherst
{apassos,vineet,mccallum}@cs.umass.edu
Abstract
Most state-of-the-art approaches for
named-entity recognition (NER) use semi
supervised information in the form of
word clusters and lexicons. Recently
neural network-based language models
have been explored, as they as a byprod-
uct generate highly informative vector
representations for words, known as word
embeddings. In this paper we present
two contributions: a new form of learn-
ing word embeddings that can leverage
information from relevant lexicons to
improve the representations, and the first
system to use neural word embeddings
to achieve state-of-the-art results on
named-entity recognition in both CoNLL
and Ontonotes NER. Our system achieves
an F1 score of 90.90 on the test set for
CoNLL 2003?significantly better than
any previous system trained on public
data, and matching a system employing
massive private industrial query-log data.
1 Introduction
In many natural language processing tasks, such
as named-entity recognition or coreference reso-
lution, syntax alone is not enough to build a high
performance system; some external source of in-
formation is required. In most state-of-the-art
systems for named-entity recognition (NER) this
knowledge comes in two forms: domain-specific
lexicons (lists of word types related to the de-
sired named entity types) and word representa-
tions (either clusterings or vectorial representa-
tions of word types which capture some of their
syntactic and semantic behavior and allow gener-
alizing to unseen word types).
Current state-of-the-art named entity recogni-
tion systems use Brown clusters as the form of
word representation (Ratinov and Roth, 2009;
Turian et al., 2010; Miller et al., 2004; Brown et
al., 1992), or other cluster-based representations
computed from private data (Lin and Wu, 2009).
While very attractive due to their simplicity, gen-
erality, and hierarchical structure, Brown clusters
are limited because the computational complex-
ity of fitting a model scales quadratically with the
number of words in the corpus, or the number of
?base clusters? in some efficient implementations,
making it infeasible to train it on large corpora or
with millions of word types.
Although some attempts have been made to
train named-entity recognition systems with other
forms of word representations, most notably those
obtained from training neural language models
(Turian et al., 2010; Collobert and Weston, 2008),
these systems have historically underperformed
simple applications of Brown clusters. A disad-
vantage of neural language models is that, while
they are inherently more scalable than Brown clus-
ters, training large neural networks is still often
expensive; for example, Turian et al (2010) re-
port that some models took multiple days or weeks
to produce acceptable representations. Moreover,
language embeddings learned from neural net-
works tend to behave in a ?nonlinear? fashion, as
they are trained to encourage a many-layered neu-
ral network to assign high probability to the data.
These neural networks can detect nonlinear rela-
tionships between the embeddings, which is not
possible in a log-linear model such as a condi-
tional random field, and therefore limiting how
much information from the embeddings can be ac-
tually leveraged.
Recently Mikolov et al (Mikolov et al., 2013a;
78
Mikolov et al., 2013b) proposed two simple log-
linear language models, the CBOW model and the
Skip-Gram model, that are simplifications of neu-
ral language models, and which can be very effi-
ciently trained on large amounts of data. For ex-
ample it is possible to train a Skip-gram model
over more than a billion tokens with a single ma-
chine in less than half a day. These embeddings
can also be trained on phrases instead of individual
word types, allowing for fine granularity of mean-
ing.
In this paper we make the following contribu-
tions. (1) We show how to extend the Skip-Gram
language model by injecting supervisory train-
ing signal from a collection of curated lexicons?
effectively encouraging training to learn similar
embeddings for phrases which occur in the same
lexicons. (2) We demonstrate that this method
outperforms a simple application of the Skip-
Gram model on the semantic similarity task on
which it was originally tested. (3) We show that
a linear-chain CRF is able to successfully use
these log-linearly-trained embeddings better than
the other neural-network-trained embeddings. (4)
We show that lexicon-infused embeddings let us
easily build a new highest-performing named en-
tity recognition system on CoNLL 2003 data
(Tjong Kim Sang and De Meulder, 2003) which
is trained using only publicly available data. (5)
We also present results on the relatively under-
studied Ontonotes NER task (Weischedel et al.,
2011), where we show that our embeddings out-
perform Brown clusters.
2 Background and Related Work
2.1 Language models and word embeddings
A statistical language model is a way to assign
probabilities to all possible documents in a given
language. Most such models can be classified
in one of two categories: they can directly as-
sign probabilities to sequences of word types, such
as is done in n-gram models, or they can oper-
ate in a lower-dimensional latent space, to which
word types are mapped. While most state-of-
the-art language models are n-gram models, the
representations used in models of the latter cate-
gory, henceforth referred to as ?embeddings,? have
been found to be useful in many NLP applications
which don?t actually need a language model. The
underlying intuition is that when language models
compress the information about the word types in
a latent space they capture much of the common-
alities and differences between word types. Hence
features extracted from these models then can gen-
eralize better than features derived from the word
types themselves.
One simple language model that discovers use-
ful embeddings is known as Brown clustering
(Brown et al., 1992). A Brown clustering is a
class-based bigram model in which (1) the prob-
ability of a document is the product of the proba-
bilities of its bigrams, (2) the probability of each
bigram is the product of the probability of a bi-
gram model over latent classes and the probability
of each class generating the actual word types in
the bigram, and (3) each word type has non-zero
probability only on a single class. Given a one-to-
one assignment of word types to classes, then, and
a corpus of text, it is easy to estimate these proba-
bilities with maximum likelihood by counting the
frequencies of the different class bigrams and the
frequencies of word tokens of each type in the cor-
pus. The Brown clustering algorithm works by
starting with an initial assignment of word types
to classes (which is usually either one unique class
per type or a small number of seed classes corre-
sponding to the most frequent types in the corpus),
and then iteratively selecting the pair of classes to
merge that would lead to the highest post-merge
log-likelihood, doing so until all classes have been
merged. This process produces a hierarchical clus-
tering of the word types in the corpus, and these
clusterings have been found useful in many appli-
cations (Ratinov and Roth, 2009; Koo et al., 2008;
Miller et al., 2004). There are other similar models
of distributional clustering of English words which
can be similarly effective (Pereira et al., 1993).
One limitation of Brown clusters is their com-
putational complexity, as training takes O(kV
2
+
N)x time to train, where k is the number of base
clusters, V size of vocabulary, and N number of
tokens. This is infeasible for large corpora with
millions of word types.
Another family of language models that pro-
duces embeddings is the neural language mod-
els. Neural language models generally work by
mapping each word type to a vector in a low-
dimensional vector space and assigning probabil-
ities to n-grams by processing their embeddings
in a neural network. Many different neural lan-
guage models have been proposed (Bengio et al.,
2003; Morin and Bengio, 2005; Bengio, 2008;
79
Mnih and Hinton, 2008; Collobert and Weston,
2008; Mikolov et al., 2010). While they can cap-
ture the semantics of word types, and often gen-
eralize better than n-gram models in terms of per-
plexity, applying them to NLP tasks has generally
been less successful than Brown clusters (Turian
et al., 2010).
Finally, there are algorithms for computing
word embeddings which do not use language mod-
els at all. A popular example is the CCA family of
word embeddings (Dhillon et al., 2012; Dhillon et
al., 2011), which work by choosing embeddings
for a word type that capture the correlations be-
tween the embeddings of word types which occur
before and after this type.
2.2 The Skip-gram Model
A main limitation of neural language models is
that they often have many parameters and slow
training times. To mitigate this, Mikolov et
al. (2013a; 2013b) recently proposed a family
of log-linear language models inspired by neu-
ral language models but designed for efficiency.
These models operate on the assumption that, even
though they are trained as language models, users
will only look at their embeddings, and hence all
they need is to produce good embeddings, and not
high-accuracy language models.
The most successful of these models is
the skip-gram model, which computes the
probability of each n-gram as the product of
the conditional probabilities of each context
word in the n-gram conditioned on its central
word. For example, the probability for the n-
gram ?the cat ate my homework? is represented as
P (the|ate)P (cat|ate)P (my|ate)P (homework|ate).
To compute these conditional probabilities the
model assigns an embedding to each word type
and defines a binary tree of logistic regression
classifiers with each word type as a leaf. Each
classifier takes a word embedding as input and
produces a probability for a binary decision cor-
responding to a branch in the tree. Each leaf in the
tree has a unique path from the root, which can be
interpreted as a set of (classifier,label) pairs. The
skip-gram model then computes a probability of a
context word given a target word as the product of
the probabilities, given the target word?s embed-
dings, of all decisions on a path from the root to
the leaf corresponding to the context word. Figure
1 shows such a tree structured model.
...
...
A An San Diego New York City
...
...
Figure 1: A binary Huffman tree. Circles repre-
sent binary classifiers. Rectangles represent to-
kens, which can be multi-word.
The likelihood of the data, then, given a set N
of n-grams, with m
n
being n-gram n?s middle-
word, c
n
each context word, w
c
n
i
the parameters
of the i-th classifier in the path from the root to
c
n
in the tree, l
c
n
i
its label (either 1 or ?1), e
f
the
embedding of word type f , and ? is the logistic
sigmoid function, is
?
n?N
?
c
n
?n
?
i
?(l
c
n
i
w
c
n
i
T
e
m
n
). (1)
Given a tree, then, choosing embeddings e
m
n
and classifier parameters w
c
n
i
to maximize equa-
tion (1) is a non-convex optimization problem
which can be solved with stochastic gradient de-
scent.
The binary tree used in the model is com-
monly estimated by computing a Huffman coding
tree (Huffman, 1952) of the word types and their
frequencies. We experimented with other tree esti-
mation schemes but found no perceptible improve-
ment in the quality of the embeddings.
It is possible to extend these embeddings to
model phrases as well as tokens. To do so,
Mikolov et al (2013b) use a phrase-building cri-
terion based on the pointwise mutual information
of bigrams. They perform multiple passes over
a corpus to estimate trigrams and higher-order
phrases. We instead consider candidate trigrams
for all pairs of bigrams which have a high PMI
and share a token.
2.3 Named Entity Recognition
Named Entity Recognition (NER) is the task of
finding all instances of explicitly named entities
and their types in a given document. While
80
detecting named entities is superficially simple,
since most sequences of capitalized words are
named entities (excluding headlines, sentence be-
ginnings, and a few other exceptions), finding all
entities is non trivial, and determining the correct
named entity type can sometimes be surprisingly
hard. Performing the task well often requires ex-
ternal knowledge of some form.
In this paper we evaluate our system on two
labeled datasets for NER: CoNLL 2003 (Tjong
Kim Sang and De Meulder, 2003) and Ontonotes
(Weischedel et al., 2011). The CoNLL dataset
has approximately 320k tokens, divided into 220k
tokens for training, 55k tokens for development,
and 50k tokens for testing. While the training and
development sets are quite similar, the test set is
substantially different, and performance on it de-
pends strongly on how much external knowledge
the systems have. The CoNLL dataset has four
entity types: PERSON, LOCATION, ORGANIZA-
TION, AND MISCELLANEOUS. The Ontonotes
dataset is substantially larger: it has 1.6M tokens
total, with 1.4M for training, 100K for develop-
ment, and 130k for testing. It also has eighteen
entity types, a much larger set than the CoNLL
dataset, including works of art, dates, cardinal
numbers, languages, and events.
The performance of NER systems is commonly
measured in terms of precision, recall, and F1 on
the sets of entities in the ground truth and returned
by the system.
2.3.1 Baseline System
In this section we describe in detail the baseline
NER system we use. It is inspired by the system
described in Ratinov and Roth (2009).
Because NER annotations are commonly not
nested (for example, in the text ?the US Army?,
?US Army? is treated as a single entity, instead
of the location ?US? and the organization ?US
Army?) it is possible to treat NER as a sequence
labeling problem, where each token in the sen-
tence receives a label which depends on which en-
tity type it belongs to and its position in the en-
tity. Following Ratinov and Roth (2009) we use
the BILOU encoding, where each token can either
BEGIN an entity, be INSIDE an entity, be the LAST
token in an entity, be OUTSIDE an entity, or be the
single UNIQUE token in an entity.
Our baseline architecture is a stacked linear-
chain CRF (Lafferty et al., 2001) system: we train
two CRFs, where the second CRF can condition
on the predictions made by the first CRF as well as
features of the data. Both CRFs, following Zhang
and Johnson (2003), have roughly similar features.
While local features capture a lot of the clues
used in text to highlight named entities, they can-
not necessarily disambiguate entity types or detect
named entities in special positions, such as the first
tokens in a sentence. To solve these problems most
NER systems incorporate some form of external
knowledge. In our baseline system we use lexi-
cons of months, days, person names, companies,
job titles, places, events, organizations, books,
films, and some minor others. These lexicons were
gathered from US Census data, Wikipedia cate-
gory pages, and Wikipedia redirects (and will be
made publicly available upon publication).
Following Ratinov and Roth (2009), we also
compare the performance of our system with a
system using features based on the Brown clusters
of the word types in a document. Since, as seen
in section 2.1, Brown clusters are hierarchical, we
use features corresponding to prefixes of the path
from the root to the leaf for each word type.
More specifically, the feature templates of the
baseline system are as follows. First for each token
we compute:
? its word type;
? word type, after excluding digits and lower-
casing it;
? its capitalization pattern;
? whether it is punctuation;
? 4-character prefixes and suffixes;
? character n-grams from length 2 to 5;
? whether it is in a wikipedia-extracted lexicon
of person names (first, last, and honorifics),
dates (months, years), place names (country,
US state, city, place suffixes, general location
words), organizations, and man-made things;
? whether it is a demonym.
For each token?s label we have feature templates
considering all token?s features, all neighboring
token?s features (up to distance 2), and bags of
words of features of tokens in a window of size
8 around each token. We also add a feature mark-
ing whether a token is the first occurrence of its
word type in a document.
When using Brown clusters we add as token
features all prefixes of lengths 4, 6, 10, and 20,
of its brown cluster.
For the second-layer model we use all these fea-
tures, as well as the label predicted for each token
81
Figure 2: Chain CRF model for a NER system
with three tokens. Filled rectangles represent fac-
tors. Circles at top represent labels, circles at bot-
tom represent binary token based features. Filled
circles indicate the phrase embeddings for each to-
ken.
by the first-layer model.
As seen in the Experiments Section, our base-
line system is competitive with state-of-the-art
systems which use similar forms of information.
We train this system with stochastic gradient as-
cent, using the AdaGrad RDA algorithm (Duchi et
al., 2011), with both `
1
and `
2
regularization, au-
tomatically tuned for each experimental setting by
measuring performance on the development set.
2.4 NER with Phrase Embeddings
In this section we describe how to extend our base-
line NER system to use word embeddings as fea-
tures.
First we group the tokens into phrases, assign-
ing to each token a single phrase greedily. We
prefer shorter phrases over longer ones, sinceour
embeddings are often more reliable for the shorter
phrases, and since the longer phrases in our dic-
tionary are mostly extracted from Wikipedia page
titles, which are not always semantically meaning-
ful when seen in free text. We then add factors
connecting each token?s label with the embedding
for its phrase.
Figure 2 shows how phrase embeddings are
plugged into a chain-CRF based NER system.
Following Turian (2010), we scale the embed-
ding vector by a real number, which is a hyper-
parameter tuned on the development data. Con-
necting tokens to phrase embeddings of their
neighboring tokens did not improve performance
for phrase embeddings, but it was mildly benefi-
cial for token embeddings.
3 Lexicon-infused Skip-gram Models
The Skip-gram model as defined in Section 2.2 is
fundamentally trained in unsupervised fashion us-
ing simply words and their n-gram contexts. In-
jecting some NER-specific supervision into the
embeddings can make them more relevant to the
NER task.
Lexicons are a simple yet powerful way to pro-
vide task-specific supervisory information to the
model without the burden of labeling additional
data. However, while lexicons have proven use-
ful in various NLP tasks, a small amount of noise
in a lexicon can severely impair the its usefulness
as a feature in log-linear models. For example,
even legitimate data, such as the Chinese last name
?He? occurring in a lexicon of person last names,
can cause the lexicon feature to fire spuriously
for many training tokens that are labeled PERSON,
and then this lexicon feature may be given low or
even negative weight.
We propose to address both these problems by
employing lexicons as part of the word embedding
training. The skip-gram model can be trained to
predict not only neighboring words but also lexi-
con membership of the central word (or phrase).
The resulting embedding training will thus be
somewhat supervised by tending to bring together
the vectors of words sharing a lexicon member-
ship. Furthermore, this type of training can effec-
tively ?clean? the influence of noisy lexicons be-
cause even if ?He? appears in the PERSON lexicon,
it will have a sufficiently different context distribu-
tion than labeled named person entities (e.g. a lack
of preceding honorifics, etc) that the presence of
this noise in the lexicon will not be as problematic
as it was previously.
Furthermore, while Skip-gram models can be
trained on billions of tokens to learn word em-
beddings for over a million word types in a sin-
gle day, this might not be enough data to cap-
ture reliable embeddings of all relevant named en-
tity phrases. Certain sets of word types, such as
names of famous scientists, can occur infrequently
enough that the Skip-gram model will not have
enough contextual examples to learn embeddings
that highlight their relevant similarities.
In this section we describe how to extend the
Skip-gram model to incorporate auxiliary infor-
mation from lexicons, or lists of related words, en-
couraging the model to assign similar embeddings
to word types in similar lexicons.
82
New YorkThe ofstate is often referred
...
...
...
...
stateThe
...
New York
US-STATE
WIKI-LOCATION
BUSINESS
Figure 3: A Semi supervised Skip-gram Model.
?New York? predicts the word ?state?. With
lexicon-infusion, ?New York? also predicts its lex-
icon classes: US-State, Wiki-location
.
In the basic Skip-gram model, as seen in Sec-
tion 2.2, the likelihood is, for each n-gram, a prod-
uct of the probability of the embedding associated
with the middle word conditioned on each context
word. We can inject supervision in this model by
also predicting, given the embedding of the mid-
dle word, whether it is a member of each lexicon.
Figure 3 shows an example, where the word ?New
York? predicts ?state?, and also its lexicon classes:
Business, US-State and Wiki-Location.
Hence, with subscript s iterating over each lex-
icon (or set of related words), and l
m
n
s
being a la-
bel for whether each word is in the set, and w
s
indicating the parameters of its classifier, the full
likelihood of the model is
(2)
?
n ?N
(
?
c
n
?n
?
i
?(l
c
n
i
w
c
n
i
T
e
m
n
)
)
(
?
s
?(l
m
n
s
w
T
s
e
m
n
)
)
.
This is a simple modification to equation (1) that
also predicts the lexicon memberships. Note that
the parameters w
s
of the auxiliary per-lexicon
classifiers are also learned. The lexicons are not
inserted in the binary tree with the words; instead,
each lexicon gets its own binary classifier.
Algorithm 1 Generating the training examples for
lexicon-infused embeddings
1: for all n-gram n with middle word m
n
do
2: for all Context-word c
n
do
3: for all Classifier, label pair (w
c
n
i
,l
c
n
i
)
in the tree do
4: Add training example
e
m
n
, w
c
n
i
, l
c
n
5: end for
6: end for
7: for all Lexicon s, with label l
m
n
s
do
8: Add training example e
m
n
, w
s
, l
m
n
s
9: end for
10: end for
In practice, a very small fraction of words are
present in a lexicon-class and this creates skewed
training data, with overwhelmingly many negative
examples. We address this issue by aggressively
sub-sampling negative training data for each lex-
icon class. We do so by randomly selecting only
1% of the possible negative lexicons for each to-
ken.
A Skip-gram model has V binary classifiers. A
lexicon-infused Skip-gram model predicts an ad-
ditional K classes, and thus has V + K binary
classifiers. If number of classes K is large, we can
induce a tree over the classes, similarly to what is
done over words in the vocabulary. In our trained
models, however, we have one million words in
the vocabulary and twenty-two lexicons, so this is
not necessary.
4 Experiments
Our phrase embeddings are learned on the combi-
nation of English Wikipedia and the RCV1 Cor-
pus (Lewis et al., 2004). Wikipedia contains 8M
articles, and RCV1 contains 946K. To get candi-
date phrases we first select bigrams which have
a pointwise mutual information score larger than
1000. We discard bigrams with stopwords from a
manually selected list. If two bigrams share a to-
ken we add its corresponding trigram to our phrase
list. We further add page titles from the English
Wikipedia to the list of candidate phrases, as well
as all word types. We get a total of about 10M
phrases. We restrict the vocabulary to the most fre-
quent 1M phrases. All our reported experiments
are on 50-dimensional embeddings. Longer em-
beddings, while performing better on the semantic
similarity task, as seen in Mikolov et al (2013a;
83
Model Accuracy
Skip-Gram 29.89
Lex-0.05 30.37
Lex-0.01 30.72
Table 1: Accuracy for Semantic-Syntactic task,
when restricted to Top 30K words. Lex-0.01 refers
to a model trained with lexicons, where 0.01% of
negative examples were used for training.
2013b), did not perform as well on NER.
To train phrase embeddings, we use a con-
text of length 21. We use lexicons derived from
Wikipedia categories and data from the US Cen-
sus, totaling K = 22 lexicon classes. We use a
randomly selected 0.01% of negative training ex-
amples for lexicons.
We perform two sets of experiments. First, we
validate our lexicon-infused phrase embeddings
on a semantic similarity task, similar to Mikolov et
al (Mikolov et al., 2013a). Then we evaluate their
utility on two named-entity recognition tasks.
For the NER Experiments, we use the base-
line system as described in Section 2.3.1. NER
systems marked as ?Skip-gram? consider phrase
embeddings; ?LexEmb? consider lexicon-infused
embeddings; ?Brown? use Brown clusters, and
?Gaz? use our lexicons as features.
4.1 Syntactic and Semantic Similarity
Mikolov et al. (2013a) introduce a test set to mea-
sure syntactic and semantic regularities for words.
This set contains 8869 semantic and 10675 syn-
tactic questions. Each question consists of four
words, such as big, biggest, small, smallest. It
asks questions of the form ?What is the word that
is similar to small in the same sense as biggest is
similar to big?. To test this, we compute the vec-
tor X = vector(?biggest?) ? vector(?big?) +
vector(?small?). Next, we search for the word
closest to X in terms of cosine distance (exclud-
ing ?biggest?, ?small?, and ?big?). This question
is considered correctly answered only if the clos-
est word found is ?smallest?. As in Mikolov et
al (Mikolov et al., 2013a), we only search over
words which are among the 30K most frequent
words in the vocabulary.
Table 1 depicts the accuracy on Semantic Syn-
tactic Task for models trained with 50 dimensions.
We find that lexicon-infused embeddings perform
better than Skip-gram. Further, lex-0.01 performs
System Dev Test
Baseline 92.22 87.93
Baseline + Brown 93.39 90.05
Baseline + Skip-gram 93.68 89.68
Baseline + LexEmb 93.81 89.56
Baseline + Gaz 93.69 89.27
Baseline + Gaz + Brown 93.88 90.67
Baseline + Gaz + Skip-gram 94.23 90.33
Baseline + Gaz + LexEmb 94.46 90.90
Ando and Zhang (2005) 93.15 89.31
Suzuki and Isozaki (2008) 94.48 89.92
Ratinov and Roth (2009) 93.50 90.57
Lin and Wu (2009) - 90.90
Table 2: Final NER F1 scores for the CoNLL 2003
shared task. On the top are the systems presented
in this paper, and on the bottom we have base-
line systems. The best results within each area are
highlighted in bold. Lin and Wu 2009 use massive
private industrial query-log data in training.
the best, and we use this model for further NER
experiments. There was no perceptible difference
in computation cost from learning lexicon-infused
embeddings versus learning standard Skip-gram
embeddings.
4.2 CoNLL 2003 NER
We applied our models on CoNLL 2003 NER data
set. All hyperparameters were tuned by training
on training set, and evaluating on the development
set. Then the best hyperparameter values were
trained on the combination of training and devel-
opment data and applied on the test set, to obtain
the final results.
Table 2 shows the phrase F1 scores of all sys-
tems we implemented, as well as state-of-the-
art results from the literature. Note that us-
ing traditional unsupervised Skip-gram embed-
dings is worse than Brown clusters. In contrast,
our lexicon-infused phrase embeddings Lex-0.01
achieves 90.90?a state-of-the-art F1 score for the
test set. This result matches the highest F1 previ-
ously reported, in Lin and Wu (2009), but is the
first system to do so without using massive private
data. Our result is signficantly better than the pre-
vious best using public data.
4.3 Ontonotes 5.0 NER
Similarly to the CoNLL NER setup, we tuned the
hyperparameters on the development set. We use
84
System Dev Test
Baseline 79.04 79.85
Baseline + Brown 79.95 81.38
Baseline + Skip-gram 80.59 81.91
Baseline + LexEmbd 80.65 81.82
Baseline + Gaz 79.85 81.31
Baseline + Gaz + Brown 80.53 82.05
Baseline + Gaz + Skip-gram 80.70 82.30
Baseline + Gaz + LexEmb 80.81 82.24
Table 3: Final NER F1 scores for Ontonotes 5.0
dataset. The results in bold face are the best on
each evaluation set.
the same list of lexicons as for CoNLL NER.
Table 3 summarize our results. We found that
both Skip-gram and Lexicon infused embeddings
give better results than using Brown Clusters as
features. However, in this case Skip-gram embed-
dings give marginally better results. (So as not to
jeopardize our ability to fairly do further research
on this task, we did not analyze the test set errors
that may explain this.) These are, to the best of our
knowledge, the first published performance num-
bers on the Ontonotes NER task.
5 Conclusions
We have shown how to inject external supervision
to a Skip-gram model to learn better phrase em-
beddings. We demonstrate the quality of phrase
embeddings on three tasks: Syntactic-semantic
similarity, CoNLL 2003 NER, and Ontonotes 5.0
NER. In the process, we provide a new public
state-of-the-art NER system for the widely con-
tested CoNLL 2003 shared task.
We demonstrate how we can plug phrase em-
beddings into an existing log-linear CRF System.
This work demonstrates that it is possible to
learn high-quality phrase embeddings and fine-
tune them with external supervision from billions
of tokens within one day computation time. We
further demonstrate that learning embeddings is
important and key to improve NLP Tasks such as
NER.
In future, we want to explore employing embed-
dings to other NLP tasks such as dependency pars-
ing and coreference resolution. We also want to
explore improving embeddings using error gradi-
ents from NER.
References
Rie Kubota Ando and Tong Zhang. 2005. A high-
performance semi-supervised learning method for
text chunking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 1?9. Association for Computational Lin-
guistics.
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio. 2008. Neural net language models.
Scholarpedia, 3(1):3881.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Processing
Systems, pages 199?207.
Paramveer Dhillon, Jordan Rodu, Dean Foster, and
Lyle Ungar. 2012. Two step cca: A new spec-
tral method for estimating vector models of words.
arXiv preprint arXiv:1206.6403.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 999999:2121?2159.
David A Huffman. 1952. A method for the construc-
tion of minimum-redundancy codes. Proceedings of
the IRE, 40(9):1098?1101.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361?397.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1030?1038. Association for
Computational Linguistics.
85
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. arXiv preprint arXiv:1310.4546.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In HLT-NAACL, volume 4, pages
337?342. Citeseer.
Andriy Mnih and Geoffrey E Hinton. 2008. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of the 31st annual meeting on Associa-
tion for Computational Linguistics, pages 183?190.
Association for Computational Linguistics.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147?
155. Association for Computational Linguistics.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In ACL, pages 665?673.
Citeseer.
Erik F Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 142?147. Association for Computational Lin-
guistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, et al. 2011. OntoNotes Release 4.0. Lin-
guistic Data Consortium.
Tong Zhang and David Johnson. 2003. A robust
risk minimization based named entity recognition
system. In Proceedings of the seventh conference
on Natural language learning at HLT-NAACL 2003-
Volume 4, pages 204?207. Association for Compu-
tational Linguistics.
86
