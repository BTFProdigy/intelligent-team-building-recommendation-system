Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 145?148,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Event Matching Using the Transitive Closure of Dependency Relations
Daniel M. Bikel and Vittorio Castelli
IBM T. J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598
{dbikel,vittorio}@us.ibm.com
Abstract
This paper describes a novel event-matching
strategy using features obtained from the tran-
sitive closure of dependency relations. The
method yields a model capable of matching
events with an F-measure of 66.5%.
1 Introduction
Question answering systems are evolving from their
roots as factoid or definitional answering systems
to systems capable of answering much more open-
ended questions. For example, it is one thing to ask
for the birthplace of a person, but it is quite another
to ask for all locations visited by a person over a
specific period of time.
Queries may contain several types of arguments:
person, organization, country, location, etc. By far,
however, the most challenging of the argument types
are the event or topic arguments, where the argument
text can be a noun phrase, a participial verb phrase
or an entire indicative clause. For example, the fol-
lowing are all possible event arguments:
? the U.S. invasion of Iraq
? Red Cross admitting Israeli and Palestinian
groups
? GM offers buyouts to union employees
In this paper, we describe a method to match
an event query argument to the sentences that
mention that event. That is, we seek to model
p(s contains e | s, e), where e is a textual description
of an event (such as an event argument for a GALE
distillation query) and where s is an arbitrary sen-
tence. In the first example above, ?the U.S. inva-
sion of Iraq?, such a model should produce a very
high score for that event description and the sentence
?The U.S. invaded Iraq in 2003.?
2 Low-level features
As the foregoing implies, we are interested in train-
ing a binary classifier, and so we represent each
training and test instance in a feature space. Con-
ceptually, our features are of three different varieties.
This section describes the first two kinds, which we
call ?low-level? features, in that they attempt to cap-
ture how much of the basic information of an event
e is present in a sentence s.
2.1 Lexical features
We employ several types of simple lexical-matching
features. These are similar to the ?bag-of-
words? features common to many IR and question-
answering systems. Specifically, we compute the
value overlap(s, e) = ws?we|we |1 , where we (resp: ws) is
the {0,1}-valued word-feature vector for the event
(resp: sentence). This value is simply the fraction
of distinct words in e that are present in s. We then
quantize this fraction into the bins [0, 0], (0, 0.33],
(0.33, 0.66], (0.66, 0.99], (0.99, 1], to produce one
of five, binary-valued features to indicate whether
none, few, some, many or all of the words match.1
2.2 Argument analysis and submodels
Since an event or topic most often involves entities
of various kinds, we need a method to recognize
those entity mentions. For example, in the event
?Abdul Halim Khaddam resigns as Vice President
of Syria?, we have a ?????? mention, an ??????-
???? mention and a ??? (geopolitical entity) mention.
We use an information extraction toolkit (Florian
et al, 2004) to analyze each event argument. The
toolkit performs the following steps: tokenization,
part-of-speech tagging, parsing, mention detection,
within-document coreference resolution and cross-
document coreference resolution. We also apply the
toolkit to our entire search corpus.
After determining the entities in an event descrip-
tion, we rely on lower-level binary classifiers, each
of which has been trained to match a specific type
1Other binnings did not significantly alter the performance
of the models we trained, and so we used the above binning
strategy for all experiments reported in this paper.
145
of entity. For example, we use a ??????-matching
model to determine if, say, ?Abdul Halim Khad-
dam? from an event description is mentioned in a
sentence.2 We build binary-valued feature functions
from the output of our four lower-level classifiers.
3 Dependency relation features
Employing syntactic or dependency relations to aid
question answering systems is by no means new (At-
tardi et al, 2001; Cui et al, 2005; Shen and Klakow,
2006). These approaches all involved various de-
grees of loose matching of the relations in a query
relative to sentences. More recently, Wang et al
(2007) explored the use a formalism called quasi-
synchronous grammar (Smith and Eisner, 2006) in
order to find a more explicit model for matching the
set of dependencies, and yet still allow for looseness
in the matching.
3.1 The dependency relation
In contrast to previous work using relations, we do
not seek to model explicitly a process that trans-
forms one dependency tree to another, nor do we
seek to come up with ad hoc correlation measures
or path similarity measures. Rather, we propose to
use features based on the transitive closure of the
dependency relation of the event and that of the de-
pendency relation of the sentence. Our aim was to
achieve a balance between the specificity of depen-
dency paths and the generality of dependency pairs.
In its most basic form, a dependency tree for
a sentence w = ??1, ?w, . . . , ?k? is a rooted tree
? = ?V, E, r?, where V = {1, . . . , k}, E ={
(i, j) : ?i is the child of ? j
}
and r ? {1, . . . , k} :
?r is the root word. Each element ?i of our word
sequence, rather than being a simple lexical item
drawn from a finite vocabulary, will be a complex
structure. With each word wi we associate a part-
of-speech tag ti, a morph (or stem) mi (which is wi
itself if wi has no variant), a set of nonterminal labels
Ni, a set of synonyms S i for that word and a canon-
ical mention cm(i). Formally, we let each sequence
element be a sextuple ?i = ?wi, ti,mi, Ni, S i, cm(i)?.
2This is not as trivial as it might sound: the model must deal
with name variants (parts of names, alternate spellings, nick-
names) and with metonymic uses of titles (?Mr. President? re-
ferring to Bill Clinton or George W. Bush).
S(ate)
NP(Cathy)
Cathy
VP(ate)
ate
Figure 1: Simple lexicalized tree.
We derive dependency trees from head-
lexicalized syntactic parse trees. The set of
nonterminal labels associated with each word is the
set of labels of the nodes for which that word was
the head. For example, in the lexicalized tree in
Figure 1, the head word ?ate? would be associated
with both the nonterminals S and VP. Also, if a
head word is part of an entity mention, then the
?canonical? version of that mention is associated
with the word, where canonical essentially means
the best version of that mention in its coreference
chain (produced by our information extraction
toolkit), denoted cm(i). In Figure 1, the first word
w1 = Cathy would probably be recognized as a
?????? mention, and if the coreference resolver
found it to be coreferent with a mention earlier
in the same document, say, Cathy Smith, then
cm(1) = Cathy Smith.
3.2 Matching on the transitive closure
Since E represents the child-of dependency relation,
let us now consider the transitive closure, E?, which
is then the descendant-of relation.3 Our features are
computed by examining the overlap between E?e and
E?s, the descendant-of relation of the event descrip-
tion e and the sentence s, respectively. We use the
following, two-tiered strategy.
Let de, ds be elements of E?e and E
?
s, with dx.d de-
noting the index of the word that is the descendant
in dx and dx.a denoting the ancestor. We define the
following matching function to match the pair of de-
scendants (or ancestors):
matchd(de, ds) = (1)
(
mde.d = mds.d
)
? (cm(de.d) = cm(ds.d))
where matcha is defined analogously for ancestors.
That is, matchd(de, ds) returns true if the morph of
the descendant of de is the same as the morph of
the descendant of ds, or if both descendants have
canonical mentions with an exact string match; the
3We remove all edges (i, j) from E? where either wi or w j is
a stop word.
146
function returns false otherwise, and matcha is de-
fined analogously for the pair of ancestors. Thus,
the pair of functions matchd,matcha are ?morph or
mention? matchers. We can now define our main
matching function in terms of matchd and matcha:
match(de, ds) = matchd(de, ds) ? matcha(de, ds).
(2)
Informally, match(de, ds) returns true if the pair
of descendants have a ?morph-or-mention? match
and if the pair of ancestors have a ?morph-or-
mention? match. When match(de, ds) = true, we
use ?morph-or-mention? matching features.
If match(de, ds) = false we then attempt to per-
form matching based on synonyms of the words in-
volved in the two dependencies (the ?second tier? of
our two-tiered strategy). Recall that S de.d is the set
of synonyms for the word at index de.d. Since we
do not perform word sense disambiguation, S de.d is
the union of all possible synsets for wde.d. We then
define the following function for determining if two
dependency pairs match at the synonym level:
synmatch(de, ds) = (3)
(
S de.d ? S ds.d , ?
)
?
(
S de.a ? S ds.a , ?
)
.
This function returns true iff the pair of descen-
dants share at least one synonym and the pair of an-
cestors share at least one synonym. If there is a syn-
onym match, we use synonym-matching features.
3.3 Dependency matching features
The same sorts of features are produced whether
there is a ?morph-or-mention? match or a synonym
match; however, we still distinguish the two types
of features, so that the model may learn different
weights according to what type of matching hap-
pened. The two matching situations each produce
four types of features. Figure 2 shows these four
types of features using the event of ?Abdul Halim
Khaddam resigns as Vice President of Syria? and the
sentence ?The resignation of Khaddam was abrupt?
as an example. In particular, the ?depth? features at-
tempt to capture the ?importance? the dependency
match, as measured by the depth of the ancestor in
the event dependency tree.
We have one additional type of feature: we com-
pute the following kernel function on the two sets of
dependencies E?e and E
?
s and create features based on
quantizing the value:
K(E?e, E
?
s) = (4)?
(de,ds)?E?e?E?s : match(de,ds)
(?(de) ? ?(ds))?1,
?((i, j)) being the path distance in ? from node i to j.
4 Data and experiments
We created 159 queries to test this model frame-
work. We adapted a publicly-available search en-
gine (citation omitted) to retrieve documents au-
tomatically from the GALE corpus likely to be
relevant to the event queries, and then used a
set of simple heuristics?a subset of the low-
level features described in ?2?to retrieve sen-
tences that were more likely than not to be rel-
evant. We then had our most experienced an-
notator annotate sentences with five possible tags:
relevant, irrelevant, relevant-in-context,
irrelevant-in-context and garbage (to deal
with sentences that were unintelligible ?word
salad?).4 Crucially, the annotation guidelines for
this task were that an event had to be explicitly men-
tioned in a sentence in order for that sentence to be
tagged relevant.
We separated the data roughly into an 80/10/10
split for training, devtest and test. We then trained
our event-matching model solely on the examples
marked relevant or irrelevant, of which there
were 3546 instances. For all the experiments re-
ported, we tested on our development test set, which
comprised 465 instances that had been marked
relevant or irrelevant.
We trained the kernel version of an averaged per-
ceptron model (Freund and Schapire, 1999), using a
polynomial kernel with degree 4 and additive term 1.
As a baseline, we trained and tested a model using
only the lexical-matching features. We then trained
and tested models using only the low-level features
and all features. Figure 3 shows the performance
statistics of all three models, and Figure 4 shows the
ROC curves of these models. Clearly, the depen-
dency features help; at our normal operating point of
0, F-measure rises from 62.2 to 66.5. Looking solely
4The *-in-context tags were to be able to re-use the data
for an upstream system capable of handling the GALE distilla-
tion query type ?list facts about [event]?.
147
Feature type Example Comment
Morph bigram x-resign-Khaddam Sparse, but helpful.
Tag bigram x-VBZ-NNP
Nonterminal x-VP-NP All pairs from Ni ? N j for (i, j) ? E?e.
Depth x-eventArgHeadDepth=0 Depth is 0 because ?resigns? is root of event.
Figure 2: Types of dependency features. Example features are for e = ?Abdul Halim Khaddam resigns as Vice
President of Syria? and s = ?The resignation of Khaddam was abrupt.? In example features, x ? {m, s}, depending on
whether the dependency match was due to ?morph-or-mention? matching or synonym matching.
Model R P F
lex 36.6 76.3 49.5
low-level 63.9 60.5 62.2
all 69.1 64.1 66.5
Figure 3: Performance of models.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Tru
e p
osi
tiv
e ra
te
False positive rate
all featureslow-level featureslexical features
Figure 4: ROC curves of model with only low-level fea-
tures vs. model with all features.
at pairs of predictions, McNemar?s test reveals dif-
ferences (p  0.05) between the predictions of the
baseline model and the other two models, but not
between those of the low-level model and the model
trained with all features.
5 Discussion
There have been several efforts to incorporate de-
pendency information into a question-answering
system. These have attempted to define either ad
hoc similarity measures or a tree transformation pro-
cess, whose parameters must be learned. By using
the transitive closure of the dependency relation, we
believe that?especially in the face of a small data
set?we have struck a balance between the represen-
tative power of dependencies and the need to remain
agnostic with respect to similarity measures or for-
malisms; we merely let the features speak for them-
selves and have the training procedure of a robust
classifier learn the appropriate weights.
Acknowledgements
This work supported by DARPA grant HR0011-06-
02-0001. Special thanks to Radu Florian and Jeffrey
Sorensen for their helpful comments.
References
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, Alessandro Tommasi, Ellen M.
Voorhees, and D. K. Harman. 2001. Selectively using
relations to improve precision in question answering.
In TREC-10, Gaithersburg, Maryland.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-
Seng Chua. 2005. Question answering passage re-
trieval using dependency relations. In SIGIR 2005,
Salvador, Brazil, August.
Radu Florian, Hani Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicholas Nicolov, and Salim Roukos. 2004. A statis-
tical model for multilingual entity detection and track-
ing. In HLT-NAACL 2004, pages 1?8.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277?296.
Dan Shen and Dietrich Klakow. 2006. Exploring corre-
lation of dependency relation paths for answer extrac-
tion. In COLING-ACL 2006, Sydney, Australia.
David A. Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projection
of syntactic dependencies. In HLT-NAACL Workshop
on Statistical Machine Translation, pages 23?30.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? a quasi-
synchronous grammar for QA. In EMNLP-CoNLL
2007, pages 22?32.
148
Recovering latent information in treebanks
David Chiang and Daniel M. Bikel
University of Pennsylvania
Dept of Computer and Information Science
200 S 33rd Street
Philadelphia PA 19104 USA
{dchiang,dbikel}@cis.upenn.edu
Abstract
Many recent statistical parsers rely on a preprocess-
ing step which uses hand-written, corpus-specific
rules to augment the training data with extra infor-
mation. For example, head-finding rules are used
to augment node labels with lexical heads. In this
paper, we provide machinery to reduce the amount
of human effort needed to adapt existing models to
new corpora: first, we propose a flexible notation for
specifying these rules that would allow them to be
shared by different models; second, we report on an
experiment to see whether we can use Expectation-
Maximization to automatically fine-tune a set of
hand-written rules to a particular corpus.
1 Introduction
Most work in statistical parsing does not operate
in the realm of parse trees as they appear in many
treebanks, but rather on trees transformed via aug-
mentation of their node labels, or some other trans-
formation (Johnson, 1998). This methodology is il-
lustrated in Figure 1. The information included in
the node labels? augmentations may include lexical
items, or a node label suffix to indicate the node is an
argument and not an adjunct; such extra information
may be viewed as latent information, in that it is not
directly present in the treebank parse trees, but may
be recovered by some means. The process of recov-
ering this latent information has largely been limited
to the hand-construction of heuristics. However, as
is often the case, hand-constructed heuristics may
not be optimal or very robust. Also, the effort re-
quired to construct such rules can be considerable.
In both respects, the use of such rules runs counter
to the data-driven approach to statistical parsing.
In this paper, we propose two steps to address
this problem. First, we define a new, fairly simple
syntax for the identification and transformation of
node labels that accommodates a wide variety of
node-label augmentations, including all those that
Model
parsed data +annotated data +
annotated data Training Decoding parsed data
Figure 1: Methodology for the development of a sta-
tistical parser. A + indicates augmentation.
are performed by existing statistical parsers that we
have examined. Second, we explore a novel use of
Expectation-Maximization (Dempster et al, 1977)
that iteratively reestimates a parsing model using
the augmenting heuristics as a starting point. Specif-
ically, the EM algorithm we use is a variant of
the Inside-Outside algorithm (Baker, 1979; Lari and
Young, 1990; Hwa, 1998). The reestimation adjusts
the model?s parameters in the augmented parse-tree
space to maximize the likelihood of the observed
(incomplete) data, in the hopes of finding a better
distribution over augmented parse trees (the com-
plete data). The ultimate goal of this work is to mini-
mize the human effort needed when adapting a pars-
ing model to a new domain.
2 Background
2.1 Head-lexicalization
Many of the recent, successful statistical parsers
have made use of lexical information or an im-
plicit lexicalized grammar, both for English and,
more recently, for other languages. All of these
parsers recover the ?hidden? lexicalizations in a
treebank and find the most probable lexicalized tree
when parsing, only to strip out this hidden infor-
mation prior to evaluation. Also, in all these pars-
ing efforts lexicalization has meant finding heads
of constituents and then propagating those lexical
heads to their respective parents. In fact, nearly
identical head-lexicalizations were used in the dis-
S(caught?VBD)
NP(boy?NN)
DET
The
NN
boy
ADVP(also?RB)
RB
also
VP(caught?VBD)
VBD
caught
NP(ball?NN)
DET
the
NN
ball
Figure 2: A simple lexicalized parse tree.
criminative models described in (Magerman, 1995;
Ratnaparkhi, 1997), the lexicalized PCFG models
in (Collins, 1999), the generative model in (Char-
niak, 2000), the lexicalized TAG extractor in (Xia,
1999) and the stochastic lexicalized TAG models
in (Chiang, 2000; Sarkar, 2001; Chen and Vijay-
Shanker, 2000). Inducing a lexicalized structure
based on heads has a two-pronged effect: it not
only allows statistical parsers to be sensitive to lex-
ical information by including this information in
the probability model?s dependencies, but it also
determines which of all possible dependencies?
both syntactic and lexical?will be included in the
model itself. For example, in Figure 2, the nontermi-
nal NP(boy?NN) is dependent on VP(caught?VBD)
and not the other way around.
2.2 Other tree transformations
Lexicalization via head-finding is but one of many
possible tree transformations that might be use-
ful for parsing. As explored thoroughly by John-
son (1998), even simple, local syntactic trans-
formations on training trees for an unlexicalized
PCFG model can have a significant impact on pars-
ing performance. Having picked up on this idea,
Collins (1999) devises rules to identify arguments,
i.e., constituents that are required to exist on a par-
ticular side of a head child constituent dominated
by a particular parent. The parsing model can then
probabilistically predict sets of requirements on ei-
ther side of a head constituent, thereby incorporat-
ing a type of subcategorization information. While
the model is augmented to include this subcat-
prediction feature, the actual identification of argu-
ments is performed as one of many preprocessing
steps on training trees, using a set of rules sim-
ilar to those used for the identification of heads.
Also, (Collins, 1999) makes use of several other
transformations, such as the identification of sub-
jectless sentences (augmenting S nodes to become
SG) and the augmentation of nonterminals for gap
threading. Xia (1999) combines head-finding with
argument identification to extract elementary trees
for use in the lexicalized TAG formalism. Other re-
searchers investigated this type of extraction to con-
struct stochastic TAG parsers (Chiang, 2000; Chen
and Vijay-Shanker, 2000; Sarkar, 2001).
2.3 Problems with heuristics
While head-lexicalization and other tree transfor-
mations allow the construction of parsing models
with more data-sensitivity and richer representa-
tions, crafting rules for these transformations has
been largely an art, with heuristics handed down
from researcher to researcher. What?s more, on
top of the large undertaking of designing and im-
plementing a statistical parsing model, the use of
heuristics has required a further effort, forcing the
researcher to bring both linguistic intuition and,
more often, engineering savvy to bear whenever
moving to a new treebank. For example, in the rule
sets used by the parsers described in (Magerman,
1995; Ratnaparkhi, 1997; Collins, 1999), the sets of
rules for finding the heads of ADJP, ADVP, NAC,
PP and WHPP include rules for picking either the
rightmost or leftmost FW (foreign word). The ap-
parently haphazard placement of these rules that
pick out FW and the rarity of FW nodes in the data
strongly suggest these rules are the result of engi-
neering effort. Furthermore, it is not at all apparent
that tree-transforming heuristics that are useful for
one parsing model will be useful for another. Fi-
nally, as is often the case with heuristics, those used
in statistical parsers tend not to be data-sensitive,
and ironically do not rely on the words themselves.
3 Rule-based augmentation
In the interest of reducing the effort required to con-
struct augmentation heuristics, we would like a no-
tation for specifying rules for selecting nodes in
bracketed data that is both flexible enough to encode
the kinds of rule sets used by existing parsers, and
intuitive enough that a rule set for a new language
can be written easily without knowledge of com-
puter programming. Such a notation would simplify
the task of writing new rule sets, and facilitate ex-
perimentation with different rules. Moreover, rules
written in this notation would be interchangeable
between different models, so that, ideally, adapta-
tion of a model to a new corpus would be trivial.
We define our notation in two parts: a structure
pattern language, whose basic patterns are speci-
fications of single nodes written in a label pattern
language.
3.1 Structure patterns
Most existing head-finding rules and argument-
finding rules work by specifying parent-child rela-
tions (e.g., NN is the head of NP, or NP is an argu-
ment of VP). A generalization of this scheme that
is familiar to linguists and computer scientists alike
would be a context-free grammar with rules of the
form
A? A1 ? ? ? (Ai)l ? ? ? An,
where the superscript l specifies that if this rule gets
used, the ith child of A should be marked with the
label l.
However, there are two problems with such an ap-
proach. First, writing down such a grammar would
be tedious to say the least, and impossible if we
want to handle trees with arbitrary branching fac-
tors. So we can use an extended CFG (Thatcher,
1967), a CFG whose right-hand sides are regular ex-
pressions. Thus we introduce a union operator (?)
and a Kleene star (?) into the syntax for right-hand
sides.
The second problem that our grammar may be
ambiguous. For example, the grammar
X? YhY ? YYh
could mark with an h either the first or second sym-
bol of YY. So we impose an ordering on the rules of
the grammar: if two rules match, the first one wins.
In addition, we make the ? operator noncommuta-
tive: ?? ? tries to match ? first, and ? only if it does
not match ?, as in Perl. (Thus the above grammar
would mark the first Y.) Similarly, ?? tries to match
as many times as possible, also as in Perl.
But this creates a third and final problem: in the
grammar
X? (YYh ? Yh)(YY ? Y),
it is not defined which symbol of YYY should be
marked, that is, which union operator takes priority
over the other. Perl circumvents this problem by al-
ways giving priority to the left. In algebraic terms,
concatenation left-distributes over union but does
not right-distribute over union in general.
However, our solution is to provide a pair of con-
catenation operators: , which gives priority to the
left, and ?, which gives priority to the right:
X ? (YYh ? Yh)  (YY ? Y) (1)
X ? (YYh ? Yh) ? (YY ? Y) (2)
Rule (1) marks the second Y in YYY, but rule (2)
marks the first Y. More formally,
? ? (? ? ?) = (? ? ?) ? (? ? ?)
(? ? ?)  ? = (?  ?) ? (?  ?)
But if ? contains no unions or Kleene stars, then
?  ? = ? ? ? (? ??)
?  ? = ? ? ? (? ??)
So then, consider the following rules:
VP ? ??  VBh  ??, (3)
VP ? ?? ? VBh ? ??. (4)
where ? is a wildcard pattern which matches any
single label (see below). Rule (3) mark with an h
the rightmost VB child of a VP, whereas rule (4)
marks the leftmost VB. This is because the Kleene
star always prefers to match as many times as possi-
ble, but in rule (3) the first Kleene star?s preference
takes priority over the last?s, whereas in rule (4) the
last Kleene star?s preference takes priority over the
first?s.
Consider the slightly more complicated exam-
ples:
VP ? ?? ? (VBh ?MDh) ? ?? (5)
VP ? ?? ? ((VBh ?MDh)  ??) (6)
Rule (5) marks the leftmost child which is either a
VB or a MD, whereas rule (6) marks the leftmost
VB if any, or else the leftmost MD. To see why this
so, consider the string MD VB X. Rule (5) would
mark the MD as h, whereas rule (6) would mark
the VB. In both rules VB is preferred over MD, and
symbols to the left over symbols to the right, but in
rule (5) the leftmost preference (that is, the prefer-
ence of the last Kleene star to match as many times
as possible) takes priority, whereas in rule (6) the
preference for VB takes priority.
3.2 Label patterns
Since nearly all treebanks have complex nontermi-
nal alphabets, we need a way of concisely specify-
ing classes of labels. Unfortunately, this will neces-
sarily vary somewhat across treebanks: all we can
define that is truly treebank-independent is the ?
pattern, which matches any label. For Penn Tree-
bank II style annotation (Marcus et al, 1993), in
which a nonterminal symbol is a category together
with zero or more functional tags, we adopt the fol-
lowing scheme: the atomic pattern a matches any
label with category a or functional tag a; more-
over, we define Boolean operators ?, ?, and ?. Thus
NP ? ?ADV matches NP?SBJ but not NP?ADV.1
3.3 Summary
Using the structure pattern language and the la-
bel pattern language together, one can fully encode
the head/argument rules used by Xia (which resem-
ble (5) above), and the family of rule sets used by
Black, Magerman, Collins, Ratnaparkhi, and others
(which resemble (6) above). In Collins? version of
the head rules, NP and PP require special treatment,
but these can be encoded in our notation as well.
4 Unsupervised learning of augmentations
In the type of approach we have been discussing
so far, hand-written rules are used to augment the
training data, and this augmented training data is
then used to train a statistical model. However, if we
train the model by maximum-likelihood estimation,
the estimate we get will indeed maximize the likeli-
hood of the training data as augmented by the hand-
written rules, but not necessarily that of the training
data itself. In this section we explore the possibility
of training a model directly on unaugmented data.
A generative model that estimates P(S ,T,T +)
(where T+ is an augmented tree) is normally used
for parsing, by computing the most likely (T,T +)
for a given S . But we may also use it for augment-
ing trees, by computing the most likely T + for a
given sentence-tree pair (S ,T ). From the latter per-
spective, because its trees are unaugmented, a tree-
bank is a corpus of incomplete data, warranting the
use of unsupervised learning methods to reestimate
a model that includes hidden parameters. The ap-
proach we take below is to seed a parsing model
using hand-written rules, and then use the Inside-
Outside algorithm to reestimate its parameters. The
resulting model, which locally maximizes the likeli-
hood of the unaugmented training data, can then be
used in two ways: one might hope that as a parser,
it would parse more accurately than a model which
only maximizes the likelihood of training data aug-
mented by hand-written rules; and that as a tree-
augmenter, it would augment trees in a more data-
sensitive way than hand-written rules.
4.1 Background: tree adjoining grammar
The parsing model we use is based on the stochas-
tic tree-insertion grammar (TIG) model described
1Note that unlike the noncommutative union operator ?, the
disjunction operator ? has no preference for its first argument.
by Chiang (2000). TIG (Schabes and Waters, 1995)
is a weakly-context free restriction of tree adjoin-
ing grammar (Joshi and Schabes, 1997), in which
tree fragments called elementary trees are com-
bined by two composition operations, substitution
and adjunction (see Figure 3). In TIG there are
certain restrictions on the adjunction operation.
Chiang?s model adds a third composition operation
called sister-adjunction (see Figure 3), borrowed
from D-tree substitution grammar (Rambow et al,
1995).2
There is an important distinction between derived
trees and derivation trees (see Figure 3). A deriva-
tion tree records the operations that are used to com-
bine elementary trees into a derived tree. Thus there
is a many-to-one relationship between derivation
trees and derived trees: every derivation tree speci-
fies a derived tree, but a derived tree can be the result
of several different derivations.
The model can be trained directly on TIG deriva-
tions if they are available, but corpora like the
Penn Treebank have only derived trees. Just as
Collins uses rules to identify heads and arguments
and thereby lexicalize trees, Chiang uses nearly the
same rules to reconstruct derivations: each training
example is broken into elementary trees, with each
head child remaining attached to its parent, each ar-
gument broken into a substitution node and an ini-
tial root, and each adjunct broken off as a modifier
auxiliary tree.
However, in this experiment we view the derived
trees in the Treebank as incomplete data, and try to
reconstruct the derivations (the complete data) using
the Inside-Outside algorithm.
4.2 Implementation
The expectation step (E-step) of the Inside-Outside
algorithm is performed by a parser that computes all
possible derivations for each parse tree in the train-
ing data. It then computes inside and outside prob-
abilities as in Hwa?s experiment (1998), and uses
these to compute the expected number of times each
event occurred. For the maximization step (M-step),
we obtain a maximum-likelihood estimate of the pa-
rameters of the model using relative-frequency es-
2The parameters for sister-adjunction in the present model
differ slightly from the original. In the original model, all the
modifier auxiliary trees that sister-adjoined at a particular po-
sition were generated independently, except that each sister-
adjunction was conditioned on whether it was the first at that
position. In the present model, each sister-adjunction is condi-
tioned on the root label of the previous modifier tree.
NP
NNP
John
S
NP? VP
VB
leave
VP
MD
should
VP?
NP
NN
tomorrow
(?1)
(?2)
(?) (?)
?
?2
?1
1
?
2
?
2,1
S
NP
NNP
John
VP
MD
should
VP
VB
leave
NP
NN
tomorrow
Derivation tree Derived tree
Figure 3: Grammar and derivation for ?John should leave tomorrow.? In this derivation, ?1 gets substituted,
? gets adjoined, and ? gets sister-adjoined.
timation, just as in the original experiment, as if
the expected values for the complete data were the
training data.
Smoothing presents a special problem. There are
several several backoff levels for each parameter
class that are combined by deleted interpolation. Let
?1, ?2 and ?3 be functions from full history con-
texts Y to less specific contexts at levels 1, 2 and
3, respectively, for some parameter class with three
backoff levels (with level 1 using the most specific
contexts). Smoothed estimates for parameters in this
class are computed as follows:
e = ?1e1 + (1 ? ?1)(?2e2 + (1 ? ?2)e3)
where ei is the estimate of p(X | ?i(Y)) for some
future context X, and the ?i are computed by the
formula found in (Bikel et al, 1997), modified to
use the multiplicative constant 5 found in the similar
formula of (Collins, 1999):
?i =
(
1 ?
di?1
di
) (
1
1 + 5ui/di
)
(7)
where di is the number of occurrences in training of
the context ?i(Y) (and d0 = 0), and ui is the number
of unique outcomes for that context seen in training.
There are several ways one might incorporate this
smoothing into the reestimation process, and we
chose to depart as little as possible from the orig-
inal smoothing method: in the E-step, we use the
smoothed model, and after the M-step, we use the
original formula (7) to recompute the smoothing
weights based on the new counts computed from
the E-step. While simple, this approach has two im-
portant consequences. First, since the formula for
the smoothing weights intentionally does not maxi-
mize the likelihood of the training data, each itera-
tion of reestimation is not guaranteed to increase the
87.3
87.35
87.4
87.45
87.5
87.55
87.6
0 5 10 15 20
F-
m
ea
su
re
Iteration
Figure 4: English, starting with full rule set
likelihood of the training data. Second, reestimation
tends to increase the size of the model in memory,
since smoothing gives nonzero expected counts to
many events which were unseen in training. There-
fore, since the resulting model is quite large, if an
event at a particular point in the derivation forest
has an expected count below 10?15, we throw it out.
4.3 Experiment
We first trained the initial model on sections 02?21
of the WSJ corpus using the original head rules, and
then ran the Inside-Outside algorithm on the same
data. We tested each successive model on some
held-out data (section 00), using a beam width of
10?4, to determine at which iteration to stop. The
F-measure (harmonic mean of labeled precision and
recall) for sentences of length ? 100 for each itera-
tion is shown in Figure 4. We then selected the ninth
reestimated model and compared it with the initial
model on section 23 (see Figure 7). This model did
only marginally better than the initial model on sec-
tion 00, but it actually performs worse than the ini-
tial model on section 23. One explanation is that the
84.5
84.55
84.6
84.65
84.7
84.75
84.8
84.85
84.9
84.95
85
85.05
0 5 10 15 20 25 30 35 40
F-
m
ea
su
re
Iteration
Figure 5: English, starting with simplified rule set
73
73.05
73.1
73.15
73.2
73.25
73.3
73.35
73.4
73.45
73.5
0 5 10 15 20 25 30 35 40
F-
m
ea
su
re
Iteration
Figure 6: Chinese, starting with full rule set
head rules, since they have been extensively fine-
tuned, do not leave much room for improvement.
To test this, we ran two more experiments.
The second experiment started with a simplified
rule set, which simply chooses either the leftmost or
rightmost child of each node as the head, depend-
ing on the label of the parent: e.g., for VP, the left-
most child is chosen; for NP, the rightmost child
is chosen. The argument rules, however, were not
changed. This rule set is supposed to represent the
kind of rule set that someone with basic familiarity
with English syntax might write down in a few min-
utes. The reestimated models seemed to improve on
this simplified rule set when parsing section 00 (see
Figure 5); however, when we compared the 30th
reestimated model with the initial model on section
23 (see Figure 7), there was no improvement.
The third experiment was on the Chinese Tree-
bank, starting with the same head rules used in
(Bikel and Chiang, 2000). These rules were origi-
nally written by Xia for grammar development, and
although we have modified them for parsing, they
have not received as much fine-tuning as the English
rules have. We trained the model on sections 001?
270 of the Penn Chinese Treebank, and reestimated
it on the same data, testing it at each iteration on
sections 301?325 (Figure 6). We selected the 38th
reestimated model for comparison with the initial
model on sections 271?300 (Figure 7). Here we did
observe a small improvement: an error reduction of
3.4% in the F-measure for sentences of length ? 40.
4.4 Discussion
Our hypothesis that reestimation does not improve
on the original rule set for English because that
rule set is already fine-tuned was partially borne
out by the second and third experiments. The model
trained with a simplified rule set for English showed
improvement on held-out data during reestimation,
but showed no improvement in the final evaluation;
however, the model trained on Chinese did show a
small improvement in both. We are uncertain as to
why the gains observed during the second experi-
ment were not reflected in the final evaluation, but
based on the graph of Figure 5 and the results on
Chinese, we believe that reestimation by EM can
be used to facilitate adaptation of parsing models
to new languages or corpora.
It is possible that our method for choosing
smoothing weights at each iteration (see ?4.2) is
causing some interference. For future work, more
careful methods should be explored. We would
also like to experiment on the parsing model of
Collins (1999), which, because it can recombine
smaller structures and reorder subcategorization
frames, might open up the search space for better
reestimation.
5 Conclusion
Even though researchers designing and implement-
ing statistical parsing models have worked in the
methodology shown in Figure 1 for several years
now, most of the work has focused on finding ef-
fective features for the model component of the
methodology, and on finding effective statistical
techniques for parameter estimation. However, there
has been much behind-the-scenes work on the ac-
tual transformations, such as head finding, and most
of this work has consisted of hand-tweaking exist-
ing heuristics. It is our hope that by introducing this
new syntax, less toil will be needed to write non-
terminal augmentation rules, and that human effort
will be lessened further by the use of unsupervised
methods such as the one presented here to produce
better models for parsing and tree augmentation.
? 100 words ? 40 words
Model Step LR LP CB 0CB ? 2 CB LR LP CB 0CB ? 2 CB
Original initial 86.95 87.02 1.21 62.38 82.33 87.68 87.76 1.02 65.30 84.86
Original 9 86.37 86.71 1.26 61.42 81.79 87.18 87.48 1.06 64.41 84.23
Simple initial 84.50 84.18 1.54 57.57 78.35 85.46 85.17 1.29 60.71 81.11
Simple 30 84.21 84.50 1.53 57.95 77.77 85.12 85.35 1.30 60.94 80.62
Chinese initial 75.30 76.77 2.72 45.95 67.05 78.37 80.03 1.79 52.82 74.75
Chinese 38 75.20 77.99 2.66 47.69 67.63 78.79 81.06 1.69 54.15 75.08
Figure 7: Results on test sets. Original = trained on English with original rule set; Simple = English, sim-
plified rule set. LR = labeled recall, LP = labeled precision; CB = average crossing brackets, 0 CB = no
crossing brackets, ? 2 CB = two or fewer crossing brackets. All figures except CB are percentages.
Acknowledgments
This research was supported in part by NSF grant
SBR-89-20230. We would like to thank Anoop
Sarkar, Dan Gildea, Rebecca Hwa, Aravind Joshi,
and Mitch Marcus for their valuable help.
References
James K. Baker. 1979. Trainable grammars for speech
recognition. In Proceedings of the Spring Conference
of the Acoustical Society of America, pages 547?550.
Daniel M. Bikel and David Chiang. 2000. Two statisti-
cal parsing models applied to the Chinese Treebank.
In Proceedings of the Second Chinese Language Pro-
cessing Workshop, pages 1?6.
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
the Fifth Conference on Applied Natural Language
Processing (ANLP 1997), pages 194?201.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of ANLP-NAACL2000, pages
132?139.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of TAGs from the Penn Treebank. In Pro-
ceedings of the Sixth International Workshop on Pars-
ing Technologies (IWPT 2000), pages 65?76, Trento.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of ACL-2000, pages 456?463.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univ. of
Pennsylvania.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. J. Roy. Stat. Soc. B, 39:1?38.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of COLING-ACL ?98, pages 557?563.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24:613?
632.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rosenberg and Arto
Salomaa, editors, Handbook of Formal Languages
and Automata, volume 3, pages 69?124. Springer-
Verlag, Heidelberg.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35?56.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL ?95, pages
276?283.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D-tree grammars. In Proceedings of ACL ?95,
pages 151?158.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2).
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL-2001,
pages 175?182.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: a cubic-time parsable formalism
that lexicalizes context-free grammar without chang-
ing the trees produced. Computational Linguistics,
21:479?513.
J. W. Thatcher. 1967. Characterizing derivation trees of
context-free grammars through a generalization of fi-
nite automata theory. J. Comp. Sys. Sci., 1:317?322.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proceedings of the 5th Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS-99), pages 398?403.
c? 2004 Association for Computational Linguistics
Intricacies of Collins? Parsing Model
Daniel M. Bikel?
University of Pennsylvania
This article documents a large set of heretofore unpublished details Collins used in his parser, such
that, along with Collins? (1999) thesis, this article contains all information necessary to duplicate
Collins? benchmark results. Indeed, these as-yet-unpublished details account for an 11% relative
increase in error from an implementation including all details to a clean-room implementation of
Collins? model. We also show a cleaner and equally well-performing method for the handling of
punctuation and conjunction and reveal certain other probabilistic oddities about Collins? parser.
We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain
well-known details, revealing that bilexical dependencies are barely used by the model and that
head choice is not nearly as important to overall parsing performance as once thought. Finally,
we perform experiments that show that the true discriminative power of lexicalization appears to
lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword
and its part of speech.
1. Introduction
Michael Collins? (1996, 1997, 1999) parsing models have been quite influential in the
field of natural language processing. Not only did they achieve new performance
benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz
1993), and not only did they serve as the basis of Collins? own future work (Collins
2000; Collins and Duffy 2002), but they also served as the basis of important work
on parser selection (Henderson and Brill 1999), an investigation of corpus variation
and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa
2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the
automatic labeling of semantic roles and predicate-argument extraction (Gildea and
Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts.
Recently, in order to continue our work combining word sense with parsing (Bikel
2000) and the study of language-dependent and -independent parsing features (Bikel
and Chiang 2000), we built a multilingual parsing engine that is capable of instanti-
ating a wide variety of generative statistical parsing models (Bikel 2002).1 As an ap-
propriate baseline model, we chose to instantiate the parameters of Collins? Model 2.
This task proved more difficult than it initially appeared. Starting with Collins? (1999)
thesis, we reproduced all the parameters described but did not achieve nearly the
same high performance on the well-established development test set of Section 00 of
? Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104. E-mail:
dbikel@linc.cis.upenn.edu
1 This engine is publicly available at http://www.cis.upenn.edu/?dbikel/software.html
Submission received: 18 January 2003; Revised submission received: 20 March 2004; Accepted for
publication: 10 June 2004
480
Computational Linguistics Volume 30, Number 4
the Penn Treebank. Together with Collins? thesis, this article contains all the informa-
tion necessary to replicate Collins? parsing results.2 Specifically, this article describes
all the as-yet-unpublished details and features of Collins? model and some analysis
of the effect of these features with respect to parsing performance, as well as some
comparative analysis of the effects of published features.3 In particular, implementing
Collins? model using only the published details causes an 11% increase in relative error
over Collins? own published results. That is, taken together, all the unpublished details
have a significant effect on overall parsing performance. In addition to the effects of
the unpublished details, we also have new evidence to show that the discriminative
power of Collins? model does not lie where once thought: Bilexical dependencies play
an extremely small role in Collins? models (Gildea 2001), and head choice is not nearly
as critical as once thought. This article also discusses the rationale for various param-
eter choices. In general, we will limit our discussion to Collins? Model 2, but we make
occasional reference to Model 3, as well.
2. Motivation
There are three primary motivations for this work. First, Collins? parsing model repre-
sents a widely used and cited parsing model. As such, if it is not desirable to use it as
a black box (it has only recently been made publicly available), then it should be pos-
sible to replicate the model in full, providing a necessary consistency among research
efforts employing it. Careful examination of its intricacies will also allow researchers
to deviate from the original model when they think it is warranted and accurately
document those deviations, as well as understand the implications of doing so.
The second motivation is related to the first: science dictates that experiments be
replicable, for this is the way we may test and validate them. The work described here
comes in the wake of several previous efforts to replicate this particular model, but
this is the first such effort to provide a faithful and equally well-performing emulation
of the original.
The third motivation is that a deep understanding of an existing model?its in-
tricacies, the interplay of its many features?provides the necessary platform for ad-
vancement to newer, ?better? models. This is especially true in an area like statis-
tical parsing that has seen rapid maturation followed by a soft ?plateau? in per-
formance. Rather than simply throwing features into a new model and measuring
their effect in a crude way using standard evaluation metrics, this work aims to
provide a more thorough understanding of the nature of a model?s features. This
understanding not only is useful in its own right but should help point the way
toward newer features to model or better modeling techniques, for we are in the
best position for advancement when we understand existing strengths and limita-
tions.
2 In the course of replicating Collins? results, it was brought to our attention that several other
researchers had also tried to do this and had also gotten performance that fell short of Collins?
published results. For example, Gildea (2001) reimplemented Collins? Model 1 but obtained results
with roughly 16.7% more relative error than Collins? reported results using that model.
3 Discovering these details and features involved a great deal of reverse engineering, and ultimately,
much discussion with Collins himself and perusal of his code. Many thanks to Mike Collins for his
generosity. As a word of caution, this article is exhaustive in its presentation of all such details and
features, and we cannot guarantee that every reader will find every detail interesting.
481
Bikel Intricacies of Collins? Parsing Model
3. Model Overview
The Collins parsing model decomposes the generation of a parse tree into many small
steps, using reasonable independence assumptions to make the parameter estimation
problem tractable. Even though decoding proceeds bottom-up, the model is defined
in a top-down manner. Every nonterminal label in every tree is lexicalized: the label
is augmented to include a unique headword (and that headword?s part of speech) that
the node dominates. The lexicalized PCFG that sits behind Model 2 has rules of the
form
P ? LnLn?1 ? ? ?L1HR1 ? ? ?Rn?1Rn (1)
where P, Li, Ri, and H are all lexicalized nonterminals, and P inherits its lexical head
from its distinguished head-child, H. In this generative model, first P is generated, then
its head-child H, then each of the left- and right-modifying nonterminals are generated
from the head outward. The modifying nonterminals Li and Ri are generated condi-
tioning on P and H, as well as a distance metric (based on what material intervenes
between the currently generated modifying nonterminal and H) and an incremental
subcategorization frame feature (a multiset containing the arguments of H that have
yet to be generated on the side of H in which the currently generated nonterminal
falls). Note that if the modifying nonterminals were generated completely indepen-
dently, the model would be very impoverished, but in actuality, because it includes
the distance and subcategorization frame features, the model captures a crucial bit of
linguistic reality, namely, that words often have well-defined sets of complements and
adjuncts, occurring with some well-defined distribution in the right-hand sides of a
(context-free) rewriting system.
The process proceeds recursively, treating each newly generated modifier as a
parent and then generating its head and modifier children; the process terminates
when (lexicalized) preterminals are generated. As a way to guarantee the consistency
of the model, the model also generates two hidden +STOP+ nonterminals as the leftmost
and rightmost children of every parent (see Figure 7).
4. Preprocessing Training Trees
To the casual reader of Collins? thesis, it may not be immediately apparent that there
are quite a few preprocessing steps for each annotated training tree and that these
steps are crucial to the performance of the parser. We identified 11 preprocessing steps
necessary to prepare training trees when using Collins? parsing model:
1. pruning of unnecessary nodes
2. adding base NP nodes (NPBs)
3. ?repairing? base NPs
4. adding gap information (applicable to Model 3 only)
5. relabeling of sentences with no subjects (subjectless sentences)
6. removing null elements
7. raising punctuation
8. identification of argument nonterminals
9. stripping unused nonterminal augmentations
482
Computational Linguistics Volume 30, Number 4
10. ?repairing? subjectless sentences
11. head-finding
The order of presentation in the foregoing list is not arbitrary, as some of the steps
depend on results produced in previous steps. Also, we have separated the steps into
their functional units; an implementation could combine steps that are independent
of one another (for clarity, our implementation does not, however). Finally, we note
that the final step, head-finding, is actually required by some of the previous steps
in certain cases; in our implementation, we selectively employ a head-finding module
during the first 10 steps where necessary.
4.1 Coordinated Phrases
A few of the preprocessing steps rely on the notion of a coordinated phrase. In this
article, the conditions under which a phrase is considered coordinated are slightly
more detailed than is described in Collins? thesis. A node represents a coordinated
phrase if
? it has a nonhead child that is a coordinating conjunction and
? that conjunction is either
? posthead but nonfinal, or
? immediately prehead but noninitial (where ?immediately?
means ?with nothing intervening except punctuation?).4
In the Penn Treebank, a coordinating conjunction is any preterminal node with the
label CC. This definition essentially picks out all phrases in which the head-child is
truly conjoined to some other phrase, as opposed to a phrase in which, say, there is
an initial CC, such as an S that begins with the conjunction but.
4.2 Pruning of Unnecessary Nodes
As a preprocessing step, pruning of unnecessary nodes simply removes preterminals
that should have little or no bearing on parser performance. In the case of the English
Treebank, the pruned subtrees are all preterminal subtrees whose root label is one of
{??, ??, .}. There are two reasons to remove these types of subtrees when parsing
the English Treebank: First, in the treebanking guidelines (Bies 1995), quotation marks
were given the lowest possible priority and thus cannot be expected to appear within
constituent boundaries in any kind of consistent way, and second, neither of these
types of preterminals?nor any punctuation marks, for that matter?counts towards
the parsing score.
4.3 Adding Base NP Nodes
An NP is basal when it does not itself dominate an NP; such NP nodes are relabeled
NPB. More accurately, an NP is basal when it dominates no other NPs except possessive
NPs, where a possessive NP is an NP that dominates POS, the preterminal possessive
4 Our positional descriptions here, such as ?posthead but nonfinal,? refer to positions within the list of
immediately dominated children of the coordinated phrase node, as opposed to positions within the
entire sentence.
483
Bikel Intricacies of Collins? Parsing Model
NP
?
?
?
?
?
?
NP
NNP
John
CC
and
NP
NNP
Jane
NP
?
?
?
?
?
?
NPB
NNP
John
CC
and
NPB
NNP
Jane
NP
?
?
?
?
?
?
NP
NPB
NNP
John
CC
and
NP
NPB
NNP
Jane
(a) Coordinated phrase (b) Base NPs relabeled (c) Extra NP nodes inserted
Figure 1
An NP that constitutes a coordinated phrase.
NP
?
?
?
?
?
?
?
?
?
?
NPB
?
?
?



the comedian
,
,
NPB
?
?
?



Tom Foolery
NP
?
?
?
?
?
?
?
?
?
?
NPB
?
?
?



the comedian
,
,
NP
NPB
?
?
?



Tom Foolery
(a) Before extra NP addition
(the NPB the comedian is the head
child).
(b) After extra NP insertion.
Figure 2
A nonhead NPB child of NP requires insertion of extra NP.
marker for the Penn Treebank. These possessive NPs are almost always themselves
base NPs and are therefore (almost always) relabeled NPB.
For consistency?s sake, when an NP has been relabeled as NPB, a normal NP node
is often inserted as a parent nonterminal. This insertion ensures that NPB nodes are
always dominated by NP nodes. The conditions for inserting this ?extra? NP level are
slightly more detailed than is described in Collins? thesis, however. The extra NP level
is added if one of the following conditions holds:
? The parent of the NPB is not an NP.
? The parent of the NPB is an NP but constitutes a coordinated phrase (see
Figure 1).
? The parent of the NPB is an NP but
? the parent?s head-child is not the NPB, and
? the parent has not already been relabeled as an NPB (see
Figure 2).5
In postprocessing, when an NPB is an only child of an NP node, the extra NP level
is removed by merging the two nodes into a single NP node, and all remaining NPB
nodes are relabeled NP.
5 Only applicable if relabeling of NPs is performed using a preorder tree traversal.
484
Computational Linguistics Volume 30, Number 4
VP
?
?
?
?
?
?
VB
need
NP
NPB
?
?
?
?
?
?
?
?
DT
the
NN
will
S
?
?
?



to continue
VP
?
?
?
?
?
?
VB
need
NP
?
?
?
?
?
?
NPB
??
DT
the
NN
will
S
?
?
?



to continue
(a) Before repair. (b) After repair.
Figure 3
An NPB is ?repaired.?
4.4 Repairing Base NPs
The insertion of extra NP levels above certain NPB nodes achieves a degree of con-
sistency for NPs, effectively causing the portion of the model that generates children
of NP nodes to have less perplexity. Collins appears to have made a similar effort to
improve the consistency of the NPB model. NPB nodes that have sentential nodes as
their final (rightmost) child are ?repaired?: The sentential child is raised so that it
becomes a new right-sibling of the NPB node (see Figure 3).6 While such a transforma-
tion is reasonable, it is interesting to note that Collins? parser performs no equivalent
detransformation when parsing is complete, meaning that when the parser produces
the ?repaired? structure during testing, there is a spurious NP bracket.7
4.5 Adding Gap Information
The gap feature is discussed extensively in chapter 7 of Collins? thesis and is applicable
only to his Model 3. The preprocessing step in which gap information is added locates
every null element preterminal, finds its co-indexed WHNP antecedent higher up in the
tree, replaces the null element preterminal with a special trace tag, and threads the
gap feature in every nonterminal in the chain between the common ancestor of the
antecedent and the trace. The threaded-gap feature is represented by appending -g to
every node label in the chain. The only detail we would like to highlight here is that an
implementation of this preprocessing step should check for cases in which threading
is impossible, such as when two filler-gap dependencies cross. An implementation
should be able to handle nested filler-gap dependencies, however.
4.6 Relabeling Subjectless Sentences
The node labels of sentences with no subjects are transformed from S to SG. This step
enables the parsing model to be sensitive to the different contexts in which such sub-
jectless sentences occur as compared to normal S nodes, since the subjectless sentences
are functionally acting as noun phrases. Collins? example of
[S [S Flying planes] is dangerous ]
6 Collins defines a sentential node, for the purposes of repairing NPBs, to be any node that begins with
the letter S. For the Penn Treebank, this defines the set {S, SBAR, SBARQ, SINV, SQ}.
7 Since, as mentioned above, the only time an NPB is merged with its parent is when it is the only child
of an NP.
485
Bikel Intricacies of Collins? Parsing Model
NP
?
?
?
?
?
?
?
?
NP
?
?
?
?
NP
? ?
NNP
John
,
,
,
,
CC
and
NP
NNP
Jane
??
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NP
NNP
John
,
,
,
,
CC
and
NP
NNP
Jane
Figure 4
Raising punctuation: Perverse case in which multiple punctuation elements appear along a
frontier of a subtree.
illustrates the utility of this transformation. However, the conditions under which an
S may be relabeled are not spelled out; one might assume that every S whose subject
(identified in the Penn Treebank with the -SBJ function tag) dominates a null element
should be relabeled SG. In actuality, the conditions are much stricter. An S is relabeled
SG when the following conditions hold:
? One of its children dominates a null element child marked with -SBJ.
? Its head-child is a VP.
? No arguments appear prior to the head-child (see Sections 4.9 and 4.11)
The latter two conditions appear to be an effort to capture only those subjectless
sentences that are based around gerunds, as in the flying planes example.8
4.7 Removing Null Elements
Removing null elements simply involves pruning the tree to eliminate any subtree
that dominates only null elements. The special trace tag that is inserted in the step
that adds gap information (Section 4.5) is excluded, as it is specifically chosen to be
something other than the null-element preterminal marker (which is -NONE- in the
Penn Treebank).
4.8 Raising Punctuation
The step in which punctuation is raised is discussed in detail in chapter 7 of Collins?
thesis. The main idea is to raise punctuation?which is any preterminal subtree in
which the part of speech is either a comma or a colon?to the highest possible point
in the tree, so that it always sits between two other nonterminals. Punctuation that
occurs at the very beginning or end of a sentence is ?raised away,? that is, pruned. In
addition, any implementation of this step should handle the case in which multiple
punctuation elements appear as the initial or final children of some node, as well
as the more pathological case in which multiple punctuation elements appear along
the left or right frontier of a subtree (see Figure 4). Finally, it is not clear what to do
with nodes that dominate only punctuation preterminals. Our implementation simply
issues a warning in such cases and leaves the punctuation symbols untouched.
8 We assume the G in the label SG was chosen to stand for the word gerund.
486
Computational Linguistics Volume 30, Number 4
S
?
?
?
?
?
?
?
?
?
?
?
?
NP-A
NNP
Elizabeth
VP-HEAD
?
?
?
?
?
?
?
?
?
?
VBD-HEAD
was
VP-A
?
?
?
?
?
?
?
?
VBN-HEAD
elected
S-A
NP-HEAD-A
NPB
?
?
?
?
DT
a
NN
director
Figure 5
Head-children are not exempt from being relabeled as arguments.
4.9 Identification of Argument Nonterminals
Collins employs a small set of heuristics to mark certain nonterminals as arguments, by
appending -A to the nonterminal label. This section reveals three unpublished details
about Collins? argument finding:
? The published argument-finding rule for PPs is to choose the first
nonterminal after the head-child. In a large majority of cases, this marks
the NP argument of the preposition. The actual rule used is slightly
more complicated: The first nonterminal to the right of the head-child
that is neither PRN nor a part-of-speech tag is marked as an argument.
The nonterminal PRN in the Penn Treebank marks parenthetical
expressions, which can occur fairly often inside a PP, as in the phrase on
(or above) the desk.
? Children that are part of a coordinated phrase (see Section 4.1) are
exempt from being relabeled as argument nonterminals.
? Head-children are distinct from their siblings by virtue of the
head-generation parameter class in the parsing model. In spite of this,
Collins? trainer actually does not exempt head-children from being
relabeled as arguments (see Figure 5).9
4.10 Stripping Unused Nonterminal Augmentations
This step simply involves stripping away all nonterminal augmentations, except those
that have been added from other preprocessing steps (such as the -A augmentation
for argument labels). This includes the stripping away of all function tags and indices
marked by the Treebank annotators.
9 It is not clear why this is done, and so in our parsing engine, we make such behavior optional via a
run-time setting.
487
Bikel Intricacies of Collins? Parsing Model
NP
?
?
?
?
?
?
?
?
?
?
NP
NNP
John
CC
and
NP-HEAD
NNP
Jane
??
NP
?
?
?
?
?
?
?
?
?
?
NP-HEAD
NNP
John
CC
and
NP
NNP
Jane
NPB
?
?
?
?
?
?
?
?
?
?
NNP
John
CC
and
NNP-HEAD
Jane
Figure 6
Head moves from right to left conjunct in a coordinated phrase, except when the parent
nonterminal is NPB.
4.11 Repairing Subjectless Sentences
With arguments identified as described in Section 4.9, if a subjectless sentence is found
to have an argument prior to its head, this step detransforms the SG so that it reverts
to being an S.
4.12 Head-Finding
Head-finding is discussed at length in Collins? thesis, and the head-finding rules used
are included in his Appendix A. There are a few unpublished details worth mention-
ing, however.
There is no head-finding rule for NX nonterminals, so the default rule of picking
the leftmost child is used.10 NX nodes roughly represent the N? level of syntax and in
practice often denote base NPs. As such, the default rule often picks out a less-than-
ideal head-child, such as an adjective that is the leftmost child in a base NP.
Collins? thesis discusses a case in which the initial head is modified when it is
found to denote the right conjunct in a coordinated phrase. That is, if the head rules
pick out a head that is preceded by a CC that is non-initial, the head should be modified
to be the nonterminal immediately to the left of the CC (see Figure 6). An important
detail is that such ?head movement? does not occur inside base NPs. That is, a phrase
headed by NPB may indeed look as though it constitutes a coordinated phrase?it has
a CC that is noninitial but to the left of the currently chosen head?but the currently
chosen head should remain chosen.11 As we shall see, there is exceptional behavior
for base NPs in almost every part of the Collins parser.
10 In our first attempt at replicating Collins? results, we simply employed the same head-finding rule for
NX nodes as for NP nodes. This choice yields different?but not necessarily inferior?results.
11 In Section 4.1, we defined coordinated phrases in terms of heads, but here we are discussing how the
head-finder itself needs to determine whether a phrase is coordinated. It does this by considering the
potential new choice of head: If the head-finding rules pick out a head that is preceded by a noninitial
CC (Jane), will moving the head to be a child to the left of the CC (John) yield a coordinated phrase? If
so, then the head should be moved?except when the parent is NPB.
488
Computational Linguistics Volume 30, Number 4
VP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+STOP+ VB-HEAD
need
ADVP
RB
undoubtedly
NP
?
?
?
?
?
?
NPB
?
?


the will
S
?
?
?



to continue
+STOP+
Figure 7
vi feature is true when generating right-hand +STOP+ nonterminal, because the NP the will to
continue contains a verb.
5. Training
The trainer?s job is to decompose annotated training trees into a series of head- and
modifier-generation steps, recording the counts of each of these steps. Referring to
(1), each H, Li, and Ri are generated conditioning on previously generated items,
and each of these events consisting of a generated item and some maximal history
context is counted. Even with all this decomposition, sparse data are still a problem,
and so each probability estimate for some generated item given a maximal context
is smoothed with coarser distributions using less context, whose counts are derived
from these ?top-level? head- and modifier-generation counts.
5.1 Verb Intervening
As mentioned in Section 3, instead of generating each modifier independently, the
model conditions the generation of modifiers on certain aspects of the history. One
such function of the history is the distance metric. One of the two components of
this distance metric is what we will call the ?verb intervening? feature, which is a
predicate vi that is true if a verb has been generated somewhere in the surface string
of the previously generated modifiers on the current side of the head. For example,
in Figure 7, when generating the right-hand +STOP+ nonterminal child of the VP, the
vi predicate is true, because one of the previously generated modifiers on the right
side of the head dominates a verb, continue.12 More formally, this feature is most easily
defined in terms of a recursively defined cv (?contains verb?) predicate, which is true
if and only if a node dominates a verb:
cv(P) =
?
?
?
?
?
?
?
?
M child of P
cv(M) if M is not a preterminal
true if P is a verb preterminal
false otherwise
(2)
12 Note that any word in the surface strings dominated by the previously generated modifiers will trigger
the vi predicate. This is possible because in a history-based model (cf. Black et al 1992), anything
previously generated?that is, anything in the history?can appear in the conditioning context.
489
Bikel Intricacies of Collins? Parsing Model
Referring to (2), we define the verb-intervening predicate recursively on the first-order
Markov process generating modifying nonterminals:
vi(Li) =
{
false if i ? 1
cv(Li?1) ? vi(Li?2) if i > 1
(3)
and similarly for right modifiers.
What is considered to be a verb? While this is not spelled out, as it happens, a
verb is any word whose part-of-speech tag is one of {VB, VBD, VBG, VBN, VBP, VBZ}. That
is, the cv predicate returns true only for these preterminals and false for all other
preterminals. Crucially, this set omits MD, which is the marker for modal verbs. Another
crucial point about the vi predicate is that it does not include verbs that appear within
base NPs. Put another way, in order to emulate Collins? model, we need to amend the
definition of cv by stipulating that cv(NPB) = false.
5.2 Skip Certain Trees
One oddity of Collins? trainer that we mention here for the sake of completeness is
that it skips certain training trees. For ?odd historical reasons,?13 the trainer skips
all trees with more than 500 tokens, where a token is considered in this context to
be a word, a nonterminal label, or a parenthesis. This oddity entails that even some
relatively short sentences get skipped because they have lots of tree structure. In the
standard Wall Street Journal training corpus, Sections 02?21 of the Penn Treebank,
there are 120 such sentences that are skipped. Unless there is something inherently
wrong with these trees, one would predict that adding them to the training set would
improve a parser?s performance. As it happens, there is actually a minuscule (and
probably statistically insignificant) drop in performance (see Table 5) when these trees
are included.
5.3 Unknown Words
5.3.1 The Threshold Problem. Collins mentions in chapter 7 of his thesis that ?[a]ll
words occurring less than 5 times in training data, and words in test data which have
never been seen in training, are replaced with the ?UNKNOWN? token (page 186).? The
frequency below which words are considered unknown is often called the unknown-
word threshold. Unfortunately, this term can also refer to the frequency above which
words are considered known. As it happens, the unknown-word threshold Collins
uses in his parser for English is six, not five.14 To be absolutely unambiguous, words
that occur fewer than six times, which is to say, words that occur five times or fewer, in
the data are considered ?unknown.?
5.3.2 Not Handled in a Uniform Way. The obvious way to incorporate unknown
words into the parsing model, then, is simply to map all low-frequency words in
the training data to some special +UNKNOWN+ token before counting top-level events
for parameter estimation (where ?low-frequency? means ?below the unknown-word
threshold?). Collins? trainer actually does not do this. Instead, it does not directly
modify any of the words in the original training trees and proceeds to break up these
unmodified trees into the top-level events. After these events have been collected
13 This phrase was taken from a comment in one of Collins? preprocessing Perl scripts.
14 As with many of the discovered discrepancies between the thesis and the implementation, we
determined the different unknown-word threshold through reverse engineering, in this case, through
an analysis of the events output by Collins? trainer.
490
Computational Linguistics Volume 30, Number 4
and counted, the trainer selectively maps low-frequency words when deriving counts
for the various context (back-off) levels of the parameters that make use of bilexical
statistics. If this mapping were performed uniformly, then it would be identical to
mapping low-frequency words prior to top-level event counting; this is not the case,
however. We describe the details of this unknown-word mapping in Section 6.9.2.
While there is a negligible yet detrimental effect on overall parsing performance
when one uses an unknown-word threshold of five instead of six, when this change is
combined with the ?obvious? method for handling unknown words, there is actually
a minuscule improvement in overall parsing performance (see Table 5).
6. Parameter Classes and Their Estimation
All parameters that generate trees in Collins? model are estimates of conditional prob-
abilities. Even though the following overview of parameter classes presents only the
maximal contexts of the conditional probability estimates, it is important to bear in
mind that the model always makes use of smoothed probability estimates that are
the linear interpolation of several raw maximum-likelihood estimates, using various
amounts of context (we explore smoothing in detail in Section 6.8).
6.1 Mapped Versions of the Set of Nonterminals
In Sections 4.5 and 4.9, we saw how the raw Treebank nonterminal set is expanded to
include nonterminals augmented with -A and -g. Although it is not made explicit in
Collins? thesis, Collins? model uses two mapping functions to remove these augmenta-
tions when including nonterminals in the history contexts of conditional probabilities.
Presumably this was done to help alleviate sparse-data problems. We denote the ?ar-
gument removal? mapping function as alpha and the ?gap removal? mapping function
as gamma. For example:
? ?(NP-A-g) = NP-g
? ?(NP-A-g) = NP-A
? ?(?(NP-A-g)) = NP
Since gap augmentations are present only in Model 3, the gamma function effectively
is the identity function in the context of Models 1 and 2.
6.2 The Head Parameter Class
The head nonterminal is generated conditioning on its parent nonterminal label, as
well as the headword and head tag which they share, since parents inherit their lexi-
cal head information from their head-children. More specifically, an unlexicalized head
nonterminal label is generated conditioning on the fully lexicalized parent nontermi-
nal. We denote the parameter class as follows:
PH(H | ?(P), wh, th) (4)
6.3 The Subcategorization Parameter Class
When the model generates a head-child nonterminal for some lexicalized parent non-
terminal, it also generates a kind of subcategorization frame (subcat) on either side of
the head-child, with the following maximal context:
PsubcatL( subcatL|?(?(H)),?(?(P)), wh, th) (5)
491
Bikel Intricacies of Collins? Parsing Model
S(sat?VBD)
?
?
?
?
?
?
?
?
NP-A(John?NNP)
NNP(John?NNP)
John
VP(sat?VBD)
VBD(sat?VBD)
sat
Figure 8
A fully lexicalized tree. The VP node is the head-child of S.
PsubcatR( subcatR|?(?(H)),?(?(P)), wh, th) (6)
Probabilistically, it is as though these subcats are generated with the head-child, via
application of the chain rule, but they are conditionally independent.15 These subcats
may be thought of as lists of requirements on a particular side of a head. For example,
in Figure 8, after the root node of the tree has been generated (see Section 6.10), the
head child VP is generated, conditioning on both the parent label S and the headword
of that parent, sat?VBD. Before any modifiers of the head-child are generated, both a
left- and right-subcat frame are generated. In this case, the left subcat is {NP-A} and
the right subcat is {}, meaning that there are no required elements to be generated on
the right side of the head. Subcats do not specify the order of the required arguments.
They are dynamically updated multisets: When a requirement has been generated, it
is removed from the multiset, and subsequent modifiers are generated conditioning
on the updated multiset.16
The implementation of subcats in Collins? parser is even more specific: Subcats
are multisets containing various numbers of precisely six types of items: NP-A, S-A,
SBAR-A, VP-A, g, and miscellaneous. The g indicates that a gap must be generated and
is applicable only to Model 3. Miscellaneous items include all nonterminals that were
marked as arguments in the training data that were not any of the other named types.
There are rules for determining whether NPs, Ss, SBARs, and VPs are arguments, and
the miscellaneous arguments occur as the result of the argument-finding rule for PPs,
which states that the first non-PRN, non-part-of-speech tag that occurs after the head
of a PP should be marked as an argument, and therefore nodes that are not one of the
four named types can be marked.
6.4 The Modifying Nonterminal Parameter Class
As mentioned above, after a head-child and its left and right subcats are generated,
modifiers are generated from the head outward, as indicated by the modifier nonter-
minal indices in Figure 1. A fully lexicalized nonterminal has three components: the
nonterminal label, the headword, and the headword?s part of speech. Fully lexicalized
modifying nonterminals are generated in two steps to allow for the parameters to be
independently smoothed, which, in turn, is done to avoid sparse-data problems. These
two steps estimate the joint event of all three components using the chain rule. In the
15 Using separate steps to generate subcats on either side of the head allows not only for conditional
independence between the left and right subcats, but also for these parameters to be separately
smoothed from the head-generation parameter.
16 Our parsing engine allows an arbitrary mechanism for storage and discharge of requirements: They
can be multisets, ordered lists, integers (simply to constrain the number of requirements), or any other
mechanism. The mechanism used is determined at runtime.
492
Computational Linguistics Volume 30, Number 4
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NPB
?
?
?
?
JJ
short
NN
grass
NP
NPB
?
?
?
?
JJ
tall
NNS
trees
,
,
CC
and
NP
NPB
?
?
?
?
JJ
bushy
NNS
bushes
Figure 9
A tree containing both punctuation and conjunction.
first step, a partially lexicalized version of the nonterminal is generated, consisting
of the unlexicalized label plus the part of speech of its headword. These partially lex-
icalized modifying nonterminals are generated conditioning on the parent label, the
head label, the headword, the head tag, the current state of the dynamic subcat, and
a distance metric. Symbolically, the parameter classes are
PL(L(t)i |?(P), ?(H), wh, th, subcatL,?L) (7)
PR(R(t)i |?(P), ?(H), wh, th, subcatR,?R) (8)
where ? denotes the distance metric.17 As discussed above, one of the two components
of this distance metric is the vi predicate. The other is a predicate that simply reports
whether the current modifier is the first modifier being generated, that is, whether i =
1. The second step is to generate the headword itself, where, because of the chain rule,
the conditioning context consists of everything in the histories of expressions (7) and (8)
plus the partially lexicalized modifier. As there are some interesting idiosyncrasies with
these headword-generation parameters, we describe them in more detail in Section 6.9.
6.5 The Punctuation and Coordinating Conjunction Parameter Classes
6.5.1 Inconsistent Model. As discussed in Section 4.8, punctuation is raised to the
highest position in the tree. This means that in some sense, punctuation acts very
much like a coordinating conjunction, in that it ?conjoins? the two siblings between
which it sits. Observing that it might be helpful for conjunctions to be generated
conditioning on both of their conjuncts, Collins introduced two new parameter classes
in his thesis parser, Ppunc and PCC.18
As per the definition of a coordinated phrase in Section 4.1, conjunction via a
CC node or a punctuation node always occurs posthead (i.e., as a right-sibling of the
head). Put another way, if a conjunction or punctuation mark occurs prehead, it is
17 Throughout this article we use the notation L(w, t)i to refer to the three items that constitute a fully
lexicalized left-modifying nonterminal, which are the unlexicalized label Li, its headword wLi , and its
part of speech tLi , and similarly for right modifiers. We use L(t)i to refer to the two items Li and tLi of a
partially lexicalized nonterminal. Finally, when we do not wish to distinguish between a left and right
modifier, we use M(w, t)i, M(t)i, and Mi.
18 Collins? thesis does not say what the back-off structure of these new parameter classes is, that is, how
they should be smoothed. We have included this information in the complete smoothing table in the
Appendix.
493
Bikel Intricacies of Collins? Parsing Model
not generated via this mechanism.19 Furthermore, even if there is arbitrary material
between the right conjunct and the head, the parameters effectively assume that the
left conjunct is always the head-child. For example, in Figure 9, the rightmost NP
(bushy bushes) is considered to be conjoined to the leftmost NP (short grass), which is
the head-child, even though there is an intervening NP (tall trees).
The new parameters are incorporated into the model by requiring that all mod-
ifying nonterminals be generated with two boolean flags: coord, indicating that the
nonterminal is conjoined to the head via a CC, and punc, indicating that the nonter-
minal is conjoined to the head via a punctuation mark. When either or both of these
flags is true, the intervening punctuation or conjunction is generated via appropriate
instances of the Ppunc/PCC parameter classes.
For example, the model generates the five children in Figure 9 in the following
order: first, the head-child is generated, which is the leftmost NP (short grass), con-
ditioning on the parent label and the headword and tag. Then, since modifiers are
always generated from the head outward, the right-sibling of the head, which is the
tall trees NP, is generated with both the punc and CC flags false. Then, the rightmost
NP (bushy bushes) is generated with both the punc and CC booleans true, since it is
considered to be conjoined to the head-child and requires the generation of an in-
tervening punctuation mark and conjunction. Finally, the intervening punctuation is
generated conditioning on the parent, the head, and the right conjunct, including the
headwords of the two conjoined phrases, and the intervening CC is similarly generated.
A simplified version of the probability of generating all these children is summarized
as follows:
p?H(NP | NP,grass,NN)?
p?R(NP(trees,NNS),punc=0,coord=0 | NP,NP,grass,NN)?
p?R(NP(bushes,NNS),punc=1,coord=1 | NP,NP,grass,NN)?
p?punc(,(,) | NP,NP,NP, bushes, NNS,grass,NN)?
p?CC(CC(and) | NP,NP,NP,bushes,NNS,grass,NN) (9)
The idea is that using the chain rule, the generation of two conjuncts and that which
conjoins them is estimated as one large joint event.20
This scheme of using flags to trigger the Ppunc and PCC parameters is problematic,
at least from a theoretical standpoint, as it causes the model to be inconsistent. Fig-
ure 10 shows three different trees that would all receive the same probability from
Collins? model. The problem is that coordinating conjunctions and punctuation are
not generated as first-class words, but only as triggered from these punc and coord
flags, meaning that the number of such intervening conjunctive items (and the order
in which they are to be generated) is not specified. So for a given sentence/tree pair
containing a conjunction and/or a punctuation mark, there is an infinite number of
similar sentence/tree pairs with arbitrary amounts of ?conjunctive? material between
the same two nodes. Because all of these trees have the same, nonzero probability, the
sum
?
T P(T), where T is a possible tree generated by the model, diverges, meaning
the model is inconsistent (Booth and Thompson 1973). Another consequence of not
generating posthead conjunctions and punctuation as first-class words is that they
19 In fact, if punctuation occurs before the head, it is not generated at all?a deficiency in the parsing
model that appears to be a holdover from the deficient punctuation handling in the model of Collins
(1997).
20 In (9), for clarity we have left out subcat generation and the use of Collins? distance metric in the
conditioning contexts. We have also glossed over the fact that lexicalized modifying nonterminals are
actually generated in two steps, using two differently smoothed parameters.
494
Computational Linguistics Volume 30, Number 4
(a)
NP
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
NP
NPB
NN
fire
,
,
CC
and
,
,
ADVP...
RB
ultimately
(b)
NP
?
?
?
?
?
?
?


?
?







NP
NPB
NN
fire
,
,
CC
and
ADVP...
RB
ultimately
(c)
NP
?
?
?
?
?
?


?
?






NP
NPB
NN
fire
CC
and
,
,
ADVP...
RB
ultimately
Figure 10
The Collins model assigns equal probability to these three trees.
do not count when calculating the head-adjacency component of Collins? distance
metric.
When emulating Collins? model, instead of reproducing the Ppunc and PCC param-
eter classes directly in our parsing engine, we chose to use a different mechanism that
does not yield an inconsistent model but still estimates the large joint event that was
the motivation behind these parameters in the first place.
6.5.2 History Mechanism. In our emulation of Collins? model, we use the history,
rather than the dedicated parameter classes PCC and Ppunc, to estimate the joint event
of generating a conjunction (or punctuation mark) and its two conjuncts. The first
big change that results is that we treat punctuation preterminals and CCs as first-class
objects, meaning that they are generated in the same way as any other modifying
nonterminal.
The second change is a little more involved. First, we redefine the distance met-
ric to consist solely of the vi predicate. Then, we add to the conditioning context
a mapped version of the previously generated modifier according to the following
495
Bikel Intricacies of Collins? Parsing Model
mapping function:
?(Mi) =
?
?
?
?
?
?
?
+START+ if i = 0
CC if Mi = CC
+PUNC+ if Mi = , or Mi = :
+OTHER+ otherwise
(10)
where Mi is some modifier Li or Ri.21 So, the maximal context for our modifying
nonterminal parameter class is now defined as follows:
PM
(
M(t)i |?(P), ?(H), wh, th, subcatside, vi(Mi), ?(Mi?1), side
)
(11)
where side is a boolean-valued event that indicates whether the modifier is on the left or
right side of the head. By treating CC and punctuation nodes as first-class nonterminals
and by adding the mapped version of the previously generated modifier, we have, in
one fell swoop, incorporated the ?no intervening? component of Collins? distance
metric (the i = 0 case of the delta function) and achieved an estimate of the joint
event of a conjunction and its conjuncts, albeit with different dependencies, that is, a
different application of the chain rule. To put this parameterization change in sharp
relief, consider the abstract tree structure
P
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
. . . . . . H CC R1
To a first approximation, under the old parameterization, the conjunction of some node
R1 with a head H and a parent P looked like this:
p?H(H |P) ? p?R(R1, coord=1 |P, H) ? p?CC(CC |P, H, R1)
whereas under the new parameterization, it looks like this:
p?H(H |P) ? p?R(CC |P, H, +START+) ? p?R(R1 |P, H, CC)
Either way, the probability of the joint conditional event {H, CC, R1 |P} is being esti-
mated, but with the new method, there is no need to add two new specialized pa-
rameter classes, and the new method does not introduce inconsistency into the model.
Using less simplification, the probability of generating the five children of Figure 9 is
now
p?H(NP | NP,grass,NN)?
p?M(NP(trees, NNS) | NP, NP, grass, NN, {}, false, +START+, right)?
p?M(,(,, ,) | NP, NP, grass, NN, {}, false, +OTHER+, right)?
p?M(CC(and, CC) | NP, NP, grass, NN, {}, false, +PUNC+, right)?
p?M(NP(bushes, NNS) | NP, NP, grass, NN, {}, false, CC, right) (12)
21 Originally, we had an additional mechanism that attempted to generate punctuation and conjunctions
with conditional independence. One of our reviewers astutely pointed out that the mechanism led to a
deficient model (the very thing we have been trying to avoid), and so we have subsequently removed
it from our model. The removal leads to a 0.05% absolute reduction in F-measure (which in this case is
also a 0.05% relative increase in error) on sentences of length ? 40 words in Section 00 of the Penn
Treebank. As this difference is not at all statistically significant (according to a randomized stratified
shuffling test [Cohen 1995]), all evaluations reported in this article are with the original model.
496
Computational Linguistics Volume 30, Number 4
As shown in Section 8.1, this new parameterization yields virtually identical perfor-
mance to that of the Collins model.22
6.6 The Base NP Model: A Model unto Itself
As we have already seen, there are several ways in which base NPs are exceptional
in Collins? parsing model. This is partly because the flat structure of base NPs in the
Penn Treebank suggested the use of a completely different model by which to generate
them. Essentially, the model for generating children of NPB nodes is a ?bigrams of
nonterminals? model. That is, it looks a great deal like a bigram language model,
except that the items being generated are not words, but lexicalized nonterminals.
Heads of NPB nodes are generated using the normal head-generation parameter, but
modifiers are always generated conditioning not on the head, but on the previously
generated modifier. That is, we modify expressions (7) and (8) to be
PL,NPB(L(t)i |P, L(w, t)i?1) (13)
PR,NPB(R(t)i |P, R(w, t)i?1) (14)
Though it is not entirely spelled out in his thesis, Collins considers the previously
generated modifier to be the head-child, for all intents and purposes. Thus, the subcat
and distance metrics are always irrelevant, since it is as though the current modifier is
right next to the head.23 Another consequence of this is that NPBs are never considered
to be coordinated phrases (as mentioned in Section 4.12), and thus CCs dominated
by NPB are never generated using a PCC parameter; instead, they are generated us-
ing a normal modifying-nonterminal parameter. Punctuation dominated by NPB, on
the other hand, is still, as always, generated via Ppunc parameters, but crucially, the
modifier is always conjoined (via the punctuation mark) to the ?pseudohead? that
is the previously generated modifier. Consequently, when some right modifier Ri is
generated, the previously generated modifier on the right side of the head, Ri?1, is
never a punctuation preterminal, but always the previous ?real? (i.e., nonpunctuation)
preterminal.24
Base NPs are also exceptional with respect to determining chart item equality, the
comma-pruning rule, and general beam pruning (see Section 7.2 for details).
6.7 Parameter Classes for Priors on Lexicalized Nonterminals
Two parameter classes that make their appearance only in Appendix E of Collins?
thesis are those that compute priors on lexicalized nonterminals. These priors are
used as a crude proxy for the outside probability of a chart item (see Baker [1979] and
Lari and Young [1990] for full descriptions of the Inside?Outside algorithm). Previous
work (Goodman 1997) has shown that the inside probability alone is an insufficient
scoring metric when comparing chart items covering the same span during decoding
and that some estimate of the outside probability of a chart item should be factored
into the score. A prior on the root (lexicalized) nonterminal label of the derivation
forest represented by a particular chart item is used for this purpose in Collins? parser.
22 As described in Bikel (2002), our parsing engine allows easy experimentation with a wide variety of
different generative models, including the ability to construct history contexts from arbitrary numbers
of previously generated modifiers. The mapping function delta and the transition function tau
presented in this section are just two examples of this capability.
23 This is the main reason that the cv (?contains verb?) predicate is always false for NPBs, as that
predicate applies only to material that intervenes between the current modifier and the head.
24 Interestingly, unlike in the regular model, punctuation that occurs to the left of the head is generated
when it occurs within an NPB. Thus, this particular?albeit small?deficiency of Collins? punctuation
handling does not apply to the base NP model.
497
Bikel Intricacies of Collins? Parsing Model
The prior of a lexicalized nonterminal M(w, t) is broken down into two separate
estimates using parameters from two new classes, Ppriorw and PpriorNT :
Pprior(M(w, t)) = Ppriorw(w, t) ? PpriorNT(M |w, t)
where p?(M |w, t) is smoothed with p?(M | t) and estimates using the parameters of the
Ppriorw class are unsmoothed.
6.8 Smoothing Weights
Many of the parameter classes in Collins? model?and indeed, in most statistical pars-
ing models?define conditional probabilities with very large conditioning contexts. In
this case, the conditioning contexts represent some subset of the history of the gen-
erative process. Even if there were orders of magnitude more training data available,
the large size of these contexts would cause horrendous sparse-data problems. The
solution is to smooth these distributions that are made rough primarily by the abun-
dance of zeros. Collins uses the technique of deleted interpolation, which smoothes
the distributions based on full contexts with those from coarser models that use less
of the context, by successively deleting elements from the context at each back-off
level. As a simple example, the head parameter class smoothes PH0(H |P, wh, th) with
PH1(H |P, th) and PH2(H |P). For some conditional probability p(A |B), let us call the
reduced context at the ith back-off level ?i(B), where typically ?0(B) = B. Each esti-
mate in the back-off chain is computed via maximum-likelihood (ML) estimation, and
the overall smoothed estimate with n back-off levels is computed using n? 1 smooth-
ing weights, denoted ?0, . . . ,?n?2. These weights are used in a recursive fashion: The
smoothed version e?i = p?i(A |?i(B)) of an unsmoothed ML estimate ei = p?i(A |?i(B)) at
back-off level i is computed via the formula
e?i = ?iei + (1 ? ?i)e?i+1, 0 ? i < n ? 1, e?n?1 = en?1 (15)
So, for example, with three levels of back-off, the overall smoothed estimate would be
defined as
e?0 = ?0e0 + (1 ? ?0)
[
?1e1 + (1 ? ?1)e2
]
(16)
It is easy to prove by structural induction that if
0 ? ?i ? 1 and
?
A
p?i(A |?i(B)) = 1, 0 ? i < n ? 1
then
?
A
p?0(A |?0(B)) = 1 (17)
Each smoothing weight can be conceptualized as the confidence in the estimate
with which it is being multiplied. These confidence values can be derived in a number
of sensible ways; the technique used by Collins was adapted from that used in Bikel
et al (1997), which makes use of a quantity called the diversity of the history context
(Witten and Bell 1991), which is equal to the number of unique futures observed in
training for that history context.
6.8.1 Deficient Model. As previously mentioned, n back-off levels require n?1 smooth-
ing weights. Collins? parser effectively uses n weights, because the estimator always
498
Computational Linguistics Volume 30, Number 4
adds an extra, constant-valued estimate to the back-off chain. Collins? parser hard-
codes this extra value to be a vanishingly small (but nonzero) ?probability? of 10?19,
resulting in smoothed estimates of the form
e?0 = ?0e0 + (1 ? ?0)
[
?1e1 + (1 ? ?1)
[
?2e2 + (1 ? ?2) ? 10?19
]]
(18)
when there are three levels of back-off. The addition of this constant-valued en =
10?19 causes all estimates in the parser to be deficient, as it ends up throwing away
probability mass. More formally, the proof leading to equation (17) no longer holds:
The ?distribution? sums to less than one (there is no history context in the model for
which there are 1019 possible outcomes).25
6.8.2 Smoothing Factors and Smoothing Terms. The formula given in Collins? thesis
for computing smoothing weights is
?i =
ci
ci + 5ui
where ci is the count of the history context ?i(B) and ui is the diversity of that context.26
The multiplicative constant five is used to give less weight to the back-off levels
with more context and was optimized by looking at overall parsing performance on
the development test set, Section 00 of the Penn Treebank. We call this constant the
smoothing factor and denote it as ff . As it happens, the actual formula for computing
smoothing weights in Collins? implementation is
?i =
{ ci
ci+ft+ff ui
if ci > 0
0 otherwise
(19)
where ft is an unmentioned smoothing term. For every parameter class except the
subcat parameter class and Ppriorw , ft = 0 and ff = 5.0. For the subcat parameter class,
ft = 5.0 and ff = 0. For Ppriorw , ft = 1.0 and ff = 0.0. This curiously means that diversity
is not used at all when smoothing subcat-generation probabilities.27
The second case in (19) handles the situation in which the history context was never
observed in training, that is, where ci = ui = 0, which would yield an undefined value
25 Collins used this technique to ensure that even futures that were never seen with an observed history
context would still have some probability mass, albeit a vanishingly small one (Collins, personal
communication, January 2003). Another commonly used technique would be to back off to the uniform
distribution, which has the desirable property of not producing deficient estimates. As with all of the
treebank- or model-specific aspects of the Collins parser, our engine uses equation (16) or (18)
depending on the value of a particular run-time setting.
26 The smoothing weights can be viewed as confidence values for the probability estimates with which
they are multiplied. The Witten-Bell technique crucially makes use of the quantity ni =
ci
ui
, the average
number of transitions from the history context ?i(B) to a possible future. With a little algebraic
manipulation, we have
?i =
ni
ni + 5
a quantity that is at its maximum when ni = ci and at its minimum when ni = 1, that is, when every
future observed in training was unique. This latter case represents when the model is most
?uncertain,? in that the transition distribution from ?i(B) is uniform and poorly trained (one
observation per possible transition). Because these smoothing weights measure, in some sense, the
closeness of the observed distribution to uniform, they can be viewed as proxies for the entropy of the
distribution p(? |?i(B)).
27 As mentioned above, the Ppriorw parameters are unsmoothed. However, as a result of the deficient
estimation method, they still have an associated lambda value, the computation of which, just like the
subcat-generation probability estimates, does not make use of diversity.
499
Bikel Intricacies of Collins? Parsing Model
Table 1
Back-off levels for PLw /PRw , the modifier headword generation parameter classes. wLi and tLi
are, respectively, the headword and its part of speech of the nonterminal Li. This table is
basically a reproduction of the last column of Table 7.1 in Collins? thesis.
Back-off PLw(wLi | . . .)
level PRw(wRi | . . .)
0 ?(Li), tLi , coord, punc, ?(P), ?(H), wh, th,?L, subcat
1 ?(Li), tLi , coord, punc, ?(P), ?(H), th,?L, subcat
2 tLi
Table 2
Our new parameter class for the generation of headwords of modifying nonterminals.
Back-off level PMw(wMi | . . .)
0 ?(Mi), tMi , ?(P), ?(H), wh, th, subcatside, vi(Mi), ?(Mi?1), side
1 ?(Mi), tMi , ?(P), ?(H), th, subcatside, vi(Mi), ?(Mi?1), side
2 tMi
when ft = 0. In such situations, making ?i = 0 throws all remaining probability mass
to the smoothed back-off estimate, e?i+1. This is a crucial part of the way smoothing
is done: If a particular history context ?i(B) has never been observed in training, the
smoothed estimate using less context, ?i+1(B), is simply substituted as the ?best guess?
for the estimate using more context; that is, e?i = e?i+1.28
6.9 Modifier Head-Word Generation
As mentioned in Section 6.4, fully lexicalized modifying nonterminals are generated in
two steps. First, the label and part-of-speech tag are generated with an instance of PL
or PR. Next, the headword is generated via an instance of one of two parameter classes,
PLw or PRw . The back-off contexts for the smoothed estimates of these parameters are
specified in Table 1. Notice how the last level of back-off is markedly different from
the previous two levels in that it removes nearly all the elements of the history: In
the face of sparse data, the probability of generating the headword of a modifying
nonterminal is conditioned only on its part of speech.
6.9.1 Smoothing and the Last Level of Back-Off. Table 1 is misleading, however. In
order to capture the most data for the crucial last level of back-off, Collins uses words
that occur on either side of the headword, resulting in a general estimate p?(w | t), as
opposed to p?Lw(wLi | tLi). Accordingly, in our emulation of Collins? model, we replace
the left- and right-word parameter classes with a single modifier headword generation
parameter class that, as with (11), includes a boolean side component that is deleted
from the last level of back-off (see Table 2).
Even with this change, there is still a problem. Every headword in a lexicalized
parse tree is the modifier of some other headword?except the word that is the head of
the entire sentence (i.e., the headword of the root nonterminal). In order to properly
duplicate Collins? model, an implementation must take care that the P(w | t) model
includes counts for these important headwords.29
28 This fact is crucial in understanding how little the Collins parsing model relies on bilexical statistics, as
described in Section 8.2 and the supporting experiment shown in Table 6.
29 In our implementation, we add such counts by having our trainer generate a ?fake? modifier event in
500
Computational Linguistics Volume 30, Number 4
S(sat?VBD)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
NP-A(Fido?NNP)
NPB(Fido?NNP)
?
?
?
?
JJ
Faithful
NNP
Fido
ADVP(faithfully?RB)
RB
faithfully
VP(sat?VBD)
VBD
sat
Figure 11
The low-frequency word Fido is mapped to +UNKNOWN+, but only when it is generated, not
when it is conditioned upon. All the nonterminals have been lexicalized (except for
preterminals) to show where the heads are.
6.9.2 Unknown-Word Mapping. As mentioned above, instead of mapping every low-
frequency word in the training data to some special +UNKNOWN+ token, Collins? trainer
instead leaves the training data untouched and selectively maps words that appear in
the back-off levels of the parameters from the PLw and PRw parameter classes. Rather
curiously, the trainer maps only words that appear in the futures of these parameters,
but never in the histories. Put another way, low-frequency words are generated as
+UNKNOWN+ but are left unchanged when they are conditioned upon. For example, in
Figure 11, where we assume Fido is a low-frequency word, the trainer would derive
counts for the smoothed parameter
pLw
(
+UNKNOWN+ | NP-A, NNP, coord = 0, punc = 0, S, VP, sat, VBD, . . .
)
(20)
However, when collecting events that condition on Fido, such as the parameters
pL
(
JJ(JJ) | NPB, NNP, Fido
)
pLw
(
Faithful | JJ, JJ, NPB, NNP, Fido
)
the word would not be mapped.
This strange mapping scheme has some interesting consequences. First, imagine
what happens to words that are truly unknown, that never occurred in the training
data. Such words are mapped to the +UNKNOWN+ token outright before parsing. When-
ever the parser estimates a probability with such a truly unknown word in the history,
it will necessarily throw all probability mass to the backed-off estimate (?e1 in our ear-
lier notation), since +UNKNOWN+ effectively never occurred in a history context during
training.
The second consequence is that the mapping scheme yields a ?superficient?30
model, if all other parts of the model are probabilistically sound (which is actually
which the observed lexicalized root nonterminal is considered a modifier of +TOP+, the hidden
nonterminal that is the parent of the observed root of every tree (see Section 6.10 for details on the
+TOP+ nonterminal).
30 The term deficient is used to denote a model in which one or more estimated distributions sums to less
than 1. We use the term superficient to denote a model in which one or more estimated distributions
sums to greater than 1.
501
Bikel Intricacies of Collins? Parsing Model
Table 3
Back-off structure for PTOPNT and PTOPw , which estimate the probability of generating H(w, t) as
the root nonterminal of a parse tree. PTOPNT is unsmoothed. n/a: not applicable.
Back-off level PTOPNT (H(t) | . . .) PTOPw(w | . . .)
0 +TOP+ t, H, +TOP+
1 n/a t
not the case here). With a parsing model such as Collins? that uses bilexical dependen-
cies, generating words in the course of parsing is done very much as it is in a bigram
language model: Every word is generated conditioning on some previously generated
word, as well as some hidden material. The only difference is that the word being
conditioned upon is often not the immediately preceding word in the sentence. How-
ever, one could plausibly construct a consistent bigram language model that generates
words with the same dependencies as those in a statistical parser that uses bilexical
dependencies derived from head-lexicalization.
Collins (personal communication, January 2003) notes that his parser?s unknown-
word-mapping scheme could be made consistent if one were to add a parameter class
that estimated p?(w | +UNKNOWN+), where w ? VL ? {+UNKNOWN+}. The values of these
estimates for a given sentence would be constant across all parses, meaning that the
?superficiency? of the model would be irrelevant when determining arg max
T
P(T |S).
6.10 The top Parameter Classes
It is assumed that all trees that can be generated by the model have an implicit non-
terminal +TOP+ that is the parent of the observed root. The observed lexicalized root
nonterminal is generated conditioning on +TOP+ (which has a prior probability of 1.0)
using a parameter from the class PTOP. This special parameter class is mentioned in a
footnote in chapter 7 of Collins? thesis. There are actually two parameter classes used
to generated observed roots, one for generating the partially lexicalized root nonter-
minal, which we call PTOPNT , and the other for generating the headword of the entire
sentence, which we call PTOPw . Table 3 gives the unpublished back-off structure of
these two additional parameter classes.
Note that PTOPw backs off to simply estimating p?(w | t). Technically, it should be
estimating p?NT(w | t), which is to say the probability of a word?s occurring with a tag
in the space of lexicalized nonterminals. This is different from the last level of back-off
in the modifier headword parameter classes, which is effectively estimating p?(w | t) in
the space of lexicalized preterminals. The difference is that in the same sentence, the
same headword can occur with the same tag in multiple nodes, such as sat in Figure
8, which occurs with the tag VBD three times (instead of just once) in the tree shown
there. Despite this difference, Collins? parser uses counts from the (shared) last level
of back-off of the PLw and PRw parameters when delivering e1 estimates for the PTOPw
parameters. Our parsing engine emulates this ?count sharing? for PTOPw by default,
by sharing counts from our PMw parameter class.
7. Decoding
Parsing, or decoding, is performed via a probabilistic version of the CKY chart-
parsing algorithm. As with normal CKY, even though the model is defined in a top-
down, generative manner, decoding proceeds bottom-up. Collins? thesis gives a pseu-
502
Computational Linguistics Volume 30, Number 4
docode version of his algorithm in an appendix. This section contains a few practical
details.
7.1 Chart Item Equality
Since the goal of the decoding process is to determine the maximally likely theory,
if during decoding a proposed chart item is equal (or, technically, equivalent) to an
item that is already in the chart, the one with the greater score survives. Chart item
equality is closely tied to the generative parameters used to construct theories: We want
to treat two chart items as unequal if they represent derivation forests that would be
considered unequal according to the output elements and conditioning contexts of the
parameters used to generate them, subject to the independence assumptions of the
model. For example, for two chart items to be considered equal, they must have the
same label (the label of the root of their respective derivation forests? subtrees), the
same headword and tag, and the same left and right subcat. They must also have the
same head label (that is, label of the head-child).
If a chart item?s root label is an NP node, its head label is most often an NPB node,
given the ?extra? NP levels that are added during preprocessing to ensure that NPB
nodes are always dominated by NP nodes. In such cases, the chart item will contain a
back pointer to the chart item that represents the base NP. Curiously, however, Collins?
implementation considers the head label of the NP chart item not to be NPB, but rather
the head label of the NPB chart item. In other words, to get the head label of an NP chart
item, one must ?peek through? the NPB and get at the NPB?s head label. Presumably,
this was done as a consideration for the NPB nodes? being ?extra? nodes, in some sense.
It appears to have little effect on overall parsing accuracy, however.
7.2 Pruning
Ideally, every parse theory could be kept in the chart, and when the root symbol has
been generated for all theories, the top-ranked one would ?win.? In order to speed
things up, Collins employs three different types of pruning. The first form of pruning
is to use a beam: The chart memoizes the highest-scoring theory in each span, and if a
proposed chart item for that span is not within a certain factor of the top-scoring item,
it is not added to the chart. Collins reports in his thesis that he uses a beam width of
105. As it happens, the beam width for his thesis experiments was 104. Interestingly,
there is a negligible difference in overall parsing accuracy when this wider beam is
used (see Table 5). An interesting modification to the standard beam in Collins? parser
is that for chart items representing NP or NP-A derivations with more than one child,
the beam is expanded to be 104 ? e3. We suspect that Collins made this modification
after he added the base NP model, to handle the greater perplexity associated with
NPs.
The second form of pruning employed is a comma constraint. Collins observed
that in the Penn Treebank data, 96% of the time, when a constituent contained a
comma, the word immediately following the end of the constituent?s span was either
a comma or the end of the sentence. So for speed reasons, the decoder rejects all
theories that would generate constituents that violate this comma constraint.31 There
is a subtlety to Collins? implementation of this form of pruning, however. Commas
are quite common within parenthetical phrases. Accordingly, if a comma in an input
31 If one generates commas as first-class words, as we have done, one must take great care in applying
this comma constraint, for otherwise, chart items that represent partially completed constituents (i.e.,
constituents for which not all modifiers have been generated) may be incorrectly rejected. This is
especially important for NPB constituents.
503
Bikel Intricacies of Collins? Parsing Model
Table 4
Overall parsing results using only details found in Collins (1997, 1999). The first two lines
show the results of Collins? parser and those of our parser in its ?complete? emulation mode
(i.e., including unpublished details). All reported scores are for sentences of length ? 40
words. LR (labeled recall) and LP (labeled precision) are the primary scoring metrics. CBs is
the number of crossing brackets. 0 CBs and ? 2 CBs are the percentages of sentences with 0
and ? 2 crossing brackets, respectively. F (the F-measure) is the evenly weighted harmonic
mean of precision and recall, or LP?LR1
2 (LP+LR)
.
Performance on Section 00
Model LR LP CBs 0 CBs ? 2 CBs F
Collins? Model 2 89.75 90.19 0.77 69.10 88.31 89.97
Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01
Clean-room Model 2 88.85 88.97 0.92 65.55 87.06 88.91
sentence occurs after an opening parenthesis and before a closing parenthesis or the
end of the sentence, it is not considered a comma for the purposes of the comma
constraint. Another subtlety is that the comma constraint should effectively not be
employed when pursuing theories of an NPB subtree. As it turns out, using the comma
constraint also affects accuracy, as shown in Section 8.1.
The final form of pruning employed is rather subtle: Within each cell of the chart
that contains items covering some span of the sentence, Collins? parser uses buckets
of items that share the same root nonterminal label for their respective derivations.
Only 100 of the top-scoring items covering the same span with the same nonterminal
label are kept in a particular bucket, meaning that if a new item is proposed and there
are already 100 items covering the same span with the same label in the chart, then it
will be compared to the lowest-scoring item in the bucket. If it has a higher score, it
will be added to the bucket and the lowest-scoring item will be removed; otherwise,
it will not be added. Apparently, this type of pruning has little effect, and so we have
not duplicated it in our engine.32
7.3 Unknown Words and Parts of Speech
When the parser encounters an unknown word, the first-best tag delivered by Ratna-
parkhi?s (1996) tagger is used. As it happens, the tag dictionary built up when training
contains entries for every word observed, even low-frequency words. This means that
during decoding, the output of the tagger is used only for those words that are truly
unknown, that is, that were never observed in training. For all other words, the chart
is seeded with a separate item for each tag observed with that word in training.
8. Evaluation
8.1 Effects of Unpublished Details
In this section we present the results of effectively doing a ?clean-room? implemen-
tation of Collins? parsing model, that is, using only information available in (Collins
1997, 1999), as shown in Table 4.
The clean-room model has a 10.6% increase in F-measure error compared to
Collins? parser and an 11.0% increase in F-measure error compared to our engine
in its complete emulation of Collins? Model 2. This is comparable to the increase in
32 Although we have implemented a version of this type of pruning that limits the number of items that
can be collected in any one cell, that is, the maximum number of items that cover a particular span.
504
Computational Linguistics Volume 30, Number 4
Table 5
Effects of independently removing or changing individual details on overall parsing
performance. All reported scores are for sentences of length ? 40 words. ?With beam width =
105, processing time was 3.36 times longer than with standard beam (104). ?No count sharing
was performed for PTOPw (see Section 6.10), and p(w | t) estimates were side-specific (see
Section 6.9.1). See Table 4 for definitions of column headings.
Performance on Section 00
Model description LR LP CBs 0 CBs ? 2 CBs F
Collins? Model 2 89.75 90.19 0.77 69.10 88.31 89.97
Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01
Unknown word threshold = 5 and unknown
words handled in uniform way (see Sec-
tion 5.3)
89.94 90.22 0.77 68.99 89.27 90.08
No training trees skipped (see Section 5.2) 89.85 90.12 0.78 68.71 89.16 89.98
Beam width = 105? 89.90 90.14 0.78 68.93 89.16 90.02
Nondeficient estimation (see Section 6.8.1) 89.75 90.00 0.80 68.82 88.88 89.87
No comma constraint (see Section 7.2) 89.52 89.80 0.84 68.09 88.20 89.66
No universal p(w | t) model? 89.40 89.17 0.88 66.14 87.92 89.28
Clean-room Model 2 88.85 88.97 0.92 65.55 87.06 88.91
error seen when removing such published features as the verb-intervening component
of the distance metric, which results in an F-measure error increase of 9.86%, or the
subcat feature, which results in a 7.62% increase in F-measure error.33 Therefore, while
the collection of unpublished details presented in Sections 4?7 is disparate, in toto
those details are every bit as important to overall parsing performance as certain of
the published features.
This does not mean that all the details are equally important. Table 5 shows the
effect on overall parsing performance of independently removing or changing certain
of the more than 30 unpublished details.34 Often, the detrimental effect of a particular
change is quite insignificant, even by the standards of the performance-obsessed world
of statistical parsing, and occasionally, the effect of a change is not even detrimental at
all. That is why we do not claim the importance of any single unpublished detail,
but rather that of their totality, given that several of the unpublished details are,
most likely, interacting. However, we note that certain individual details, such as the
universal p(w | t) model, do appear to have a much more marked effect on overall
parsing accuracy than others.
8.2 Bilexical Dependencies
The previous section accounts for the noticeable effects of all the unpublished details
of Collins? model. But what of the details that were published? In chapter 8 of his
thesis, Collins gives an account on the motivation of various features of his model,
including the distance metric, the model?s use of subcats (and their interaction with the
distance metric), and structural versus semantic preferences. In the discussion of this
last issue, Collins points to the fact that structural preferences?which, in his model, are
33 These F-measures and the differences between them were calculated from experiments presented in
Collins (1999, page 201); these experiments, unlike those on which our reported numbers are based,
were on all sentences, not just those of length ? 40 words. As Collins notes, removing both the distance
metric and subcat features results in a gigantic drop in performance, since without both of these
features, the model has no way to encode the fact that flatter structures should be avoided in several
crucial cases, such as for PPs, which tend to prefer one argument to the right of their head-children.
34 As a reviewer pointed out, the use of the comma constraint is a ?published? detail. However, the
specifics of how certain commas do not apply to the constraint is an ?unpublished detail,? as
mentioned in Section 7.2.
505
Bikel Intricacies of Collins? Parsing Model
Table 6
Number of times our parsing engine was able to deliver a probability for the various levels of
back-off of the modifier-word generation model, PMw , when testing on Section 00, having
trained on Sections 02?21. In other words, this table reports how often a context in the
back-off chain of PMw that was needed during decoding was observed in training.
Back-off level Number of accesses Percentage
0 3,257,309 1.5
1 24,294,084 11.0
2 191,527,387 87.4
Total 219,078,780 100.0
modeled primarily by the PL and PR parameters?often provide the right information
for disambiguating competing analyses, but that these structural preferences may be
?overridden? by semantic preferences. Bilexical statistics (Eisner 1996), as represented
by the maximal context of the PLw and PRw parameters, serve as a proxy for such
semantic preferences, where the actual modifier word (as opposed to, say, merely its
part of speech) indicates the particular semantics of its head. Indeed, such bilexical
statistics were widely assumed for some time to be a source of great discriminative
power for several different parsing models, including that of Collins.
However, Gildea (2001) reimplemented Collins? Model 1 (essentially Model 2 but
without subcats) and altered the PLw and PRw parameters so that they no longer had
the top level of context that included the headword (he removed back-off level 0,
as depicted in Table 1). In other words, Gildea removed all bilexical statistics from
the overall model. Surprisingly, this resulted in only a 0.45% absolute reduction in
F-measure (3.3% relative increase in error). Unfortunately, this result was not entirely
conclusive, in that Gildea was able to reimplement Collins? baseline model only par-
tially, and the performance of his partial reimplementation was not quite as good as
that of Collins? parser.35
Training on Sections 02?21, we have duplicated Gildea?s bigram-removal exper-
iment, except that our chosen test set is Section 00 instead of Section 23 and our
chosen model is the more widely used Model 2. Using the mode that most closely
emulates Collins? Model 2, with bigrams, our engine obtains a recall of 89.89% and a
precision of 90.14% on sentences of length ? 40 words (see Table 8, Model Mtw,tw).
Without bigrams, performance drops only to 89.49% on recall, 89.95% on precision?
an exceedingly small drop in performance (see Table 8, Model Mtw,t). In an additional
experiment, we have examined the number of times that the parser is able, while de-
coding Section 00, to deliver a requested probability for the modifier-word generation
model using the increasingly less-specific contexts of the three back-off levels. The
results are presented in Table 6. Back-off level 0 indicates the use of the full history
context, which contains the head-child?s headword. Note that probabilities making use
of this full context, that is, making use of bilexical dependencies, are available only
1.49% of the time. Combined with the results from the previous experiment, this sug-
gests rather convincingly that such statistics are far less significant than once thought
to the overall discriminative power of Collins? models, confirming Gildea?s result for
Model 2.36
35 The reimplementation was necessarily only partial, as Gildea did not have access to all the
unpublished details of Collins? models that are presented in this article.
36 On a separate note, it may come as a surprise that the decoder needs to access more than 219 million
probabilities during the course of parsing the 1,917 sentences of Section 00. Among other things, this
506
Computational Linguistics Volume 30, Number 4
Table 7
Results on Section 00 with simplified head rules. The baseline model is our engine in its
closest possible emulation of Collins? Model 2. See Table 4 for definitions of column headings.
LR LP CBs 0 CBs ? 2 CBs F
Collins? Model 2 89.75 90.19 0.77 69.10 88.31 89.97
Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01
Simplified head rules 88.55 88.80 0.86 67.25 87.42 88.67
8.3 Choice of Heads
If not bilexical statistics, then surely, one might think, head-choice is critical to the
performance of a head-driven lexicalized statistical parsing model. Partly to this end,
in Chiang and Bikel (2002), we explored methods for recovering latent information
in treebanks. The second half of that paper focused on a use of the Inside?Outside
algorithm to reestimate the parameters of a model defined over an augmented tree
space, where the observed data were considered to be the gold-standard labeled brack-
etings found in the treebank, and the hidden data were considered to be the head-
lexicalizations, one of the most notable tree augmentations performed by modern
statistical parsers. These expectation maximization (EM) experiments were motivated
by the desire to overcome the limitations imposed by the heuristics that have been
heretofore used to perform head-lexicalization in treebanks. In particular, it appeared
that the head rules used in Collins? parser had been tweaked specifically for the En-
glish Penn Treebank. Using EM would mean that very little effort would need to be
spent on developing head rules, since EM could take an initial model that used simple
heuristics and optimize it appropriately to maximize the likelihood of the unlexical-
ized (observed) training trees. To test this, we performed experiments with an initial
model trained using an extremely simplified head-rule set in which all rules were of
the form ?if the parent is X, then choose the left/rightmost child.? A surprising side
result was that even with this simplified set of head-rules, overall parsing performance
still remained quite high. Using our simplified head-rule set for English, our engine in
its ?Model 2 emulation mode? achieved a recall of 88.55% and a precision of 88.80%
for sentences of length ?40 words in Section 00 (see Table 7). So contrary to our ex-
pectations, the lack of careful head-choice is not crippling in allowing the parser to
disambiguate competing theories and is a further indication that semantic preferences,
as represented by conditioning on a headword, rarely override structural ones.
8.4 Lexical Dependencies Matter
Given that bilexical dependencies are almost never used and have a surprisingly small
effect on overall parsing performance, and given that the choice of head is not terribly
critical either, one might wonder what power, if any, head-lexicalization is providing.
The answer is that even when one removes bilexical dependencies from the model,
there are still plenty of lexico-structural dependencies, that is, structures being gen-
erated conditioning on headwords and headwords being generated conditioning on
structures.
To test the effect of such lexicostructural dependencies in our lexicalized PCFG-
style formalism, we experimented with the removal of the head tag th and/or the
head word wh from the conditioning contexts of the PMw and PM parameters. The re-
certainly points to the utility of caching probabilities (the 219 million are tokens, not types).
507
Bikel Intricacies of Collins? Parsing Model
Table 8
Parsing performance with various models on Section 00 of the Penn Treebank. PM is the
parameter class for generating partially lexicalized modifying nonterminals (a nonterminal
label and part of speech). PMw is the parameter class that generates the headword of a
modifying nonterminal. Together, PM and PMw generate a fully lexicalized modifying
nonterminal. The check marks indicate the inclusion of the headword wh and its part of
speech th of the lexicalized head nonterminal H(th, wh) in the conditioning contexts of PM and
PMw . See Table 4 for definitions of the remaining column headings.
Parameter class PM PMw Score
Conditioning on th wh th wh LR LP CBs 0 CBs ? 2 CBs F
Model Mtw,tw     89.89 90.14 0.78 68.82 89.21 90.01
Mtw,t    89.49 89.95 0.80 67.98 88.82 89.72
Mt,t   88.20 88.89 0.91 65.00 87.13 88.54
Mtw,?   89.24 89.86 0.81 66.80 88.76 89.55
Mt,?  88.01 88.96 0.91 63.93 86.91 88.48
M?,? 87.01 88.75 0.96 61.08 86.00 87.87
sults are shown in Table 8. Model Mtw,tw shows our baseline, and Model M?,? shows
the effect of removing all dependence on the headword and its part of speech, with the
other models illustrating varying degrees of removing elements from the two parame-
ter classes? conditioning contexts. Notably, including the headword wh in or removing
it from the PM contexts appears to have a significant effect on overall performance, as
shown by moving from Model Mtw,t to Model Mt,t and from Model Mtw,? to Model
Mt,?. This reinforces the notion that particular headwords have structural preferences,
so that making the PM parameters dependent on headwords would capture such pref-
erences. As for effects involving dependence on the head tag th, observe that moving
from Model Mtw,t to Model Mtw,? results in a small drop in both recall and precision,
whereas making an analogous move from Model Mt,t to Model Mt,? results in a drop
in recall, but a slight gain in precision (the two moves are analogous in that in both
cases, th is dropped from the context of PMw ). It is not evident why these two moves
do not produce similar performance losses, but in both cases, the performance drops
are small relative to those observed when eliminating wh from the conditioning con-
texts, indicating that headwords matter far more than parts of speech for determining
structural preferences, as one would expect.
9. Conclusion
We have documented what we believe is the complete set of heretofore unpublished
details Collins used in his parser, such that, along with Collins? (1999) thesis, thi
s article contains all information necessary to duplicate Collins? benchmark results.
Indeed, these as-yet-unpublished details account for an 11% relative increase in er-
ror from an implementation including all details to a clean-room implementation of
Collins? model. We have also shown a cleaner and equally well-performing method
for the handling of punctuation and conjunction, and we have revealed certain other
probabilistic oddities about Collins? parser. We have not only analyzed the effect of
the unpublished details but also reanalyzed the effect of certain well-known details,
revealing that bilexical dependencies are barely used by the model and that head
choice is not nearly as important to overall parsing performance as once thought. Fi-
nally, we have performed experiments that show that the true discriminative power
of lexicalization appears to lie in the fact that unlexicalized syntactic structures are
generated conditioning on the headword and head tag. These results regarding the
508
Computational Linguistics Volume 30, Number 4
lack of reliance on bilexical statistics suggest that generative models still have room
for improvement through the employment of bilexical-class statistics, that is, depen-
dencies among head-modifier word classes, where such classes may be defined by, say,
WordNet synsets. Such dependencies might finally be able to capture the semantic
preferences that were thought to be captured by standard bilexical statistics, as well
as to alleviate the sparse-data problems associated with standard bilexical statistics.
This is the subject of our current research.
Appendix: Complete List of Parameter Classes
This section contains tables for all parameter classes in Collins? Model 3, with appro-
priate modifications and additions from the tables presented in Collins? thesis. The
notation is that used throughout this article. In particular, for notational brevity we
use M(w, t)i to refer to the three items Mi, tMi , and wMi that constitute some fully lexi-
calized modifying nonterminal and similarly M(t)i to refer to the two items Mi and tMi
that constitute some partially lexicalized modifying nonterminal. The (unlexicalized)
nonterminal-mapping functions alpha and gamma are defined in Section 6.1. As a
shorthand, ?(M(t)i) = ?(Mi), tMi .
The head-generation parameter class, PH, gap-generation parameter class, PG, and
subcat-generation parameter classes, PsubcatL and PsubcatR , have back-off structures as
follows:
Back-off level PH(H| . . .) PG(G| . . .)
PsubcatL( subcatL| . . .)
PsubcatR( subcatR| . . .)
0 ?(P), wh, th ?(?(P)), ?(?(H)), wh, th
1 ?(P), th ?(?(P)), ?(?(H)), th
2 ?(P) ?(?(P)), ?(?(H))
The two parameter classes for generating modifying nonterminals that are not
dominated by a base NP, PM and PMw , have the following back-off structures. Recall
that back-off level 2 of the PMw parameters includes words that are the heads of the
observed roots of sentences (that is, the headword of the entire sentence).
Back-off level PM
(
M(t)i, coord, punc | . . .
)
0 ?(P), ?(H), wh, th, ?side, subcatside, side
1 ?(P), ?(H), th, ?side, subcatside, side
2 ?(P), ?(H), ?side, subcatside, side
Back-off level PMw
(
wMi | . . .
)
0 ?(M(t)i), coord, punc, ?(P), ?(H), wh, th, ?side, subcatside, side
1 ?(M(t)i), coord, punc, ?(P), ?(H), th, ?side, subcatside, side
2 tMi
The two parameter classes for generating modifying nonterminals that are children
of base NPs (NPB nodes), PM,NPB and PMw,NPB, have the following back-off structures.
Back-off level 2 of the PMw,NPB parameters includes words that are the heads of the
observed roots of sentences (that is, the headword of the entire sentence). Also, note
that there is no coord flag, as coordinating conjunctions are generated in the same
way as regular modifying nonterminals when they are dominated by NPB. Finally, we
define M0 = H, that is, the head nonterminal label of the base NP that was generated
using a PH parameter.
509
Bikel Intricacies of Collins? Parsing Model
Back-off level PM,NPB
(
M(t)i, punc| . . .
)
PMw,NPB
(
wMi | . . .
)
0 P, M(w, t)i?1, side Mi, tMi , punc, P, M(w, t)i?1, side
1 P, M(t)i?1, side Mi, tMi , punc, P, M(t)i?1, side
2 P, Mi?1, side tMi
The two parameter classes for generating punctuation and coordinating conjunc-
tions, Ppunc and Pcoord, have the following back-off structures (Collins, personal com-
munication, October 2001), where
? type is a flag that obtains the value p in the history contexts of Ppunc
parameters and c in the history contexts of Pcoord parameters;
? M(w, t)i is the modifying preterminal that is being conjoined to the
head-child;
? tp or tc is the particular preterminal (part-of-speech tag) that is conjoining
the modifier to the head-child (such as CC or :);
? wp or wc is the particular word that is conjoining the modifier to the
head-child (such as and or :).
Back-off level Pcoord( tc| . . .) Pcoordw(wc| . . .)
Ppunc( tp
?
? . . .) Ppuncw(wp
?
? . . .)
0 wh, th, P, H, M(w, t)i, type ttype, wh, th, P, H, M(w, t)i, type
1 th, P, H, M(t)i, type ttype, th, P, H, M(t)i, type
2 type ttype
The parameter classes for generating fully lexicalized root nonterminals given the
hidden root +TOP+, PTOP and PTOPw , have the following back-off structures (identical
to Table 3; n/a: not applicable).
Back-off level PTOPNT(H(t) | . . .) PTOPw(w | . . .)
0 +TOP+ t, H, +TOP+
1 n/a t
The parameter classes for generating prior probabilities on lexicalized nontermi-
nals M(w, t), Ppriorw and PpriorNT , have the following back-off structures, where prior is a
dummy variable to indicate that Ppriorw is not smoothed (although the Ppriorw parameters
still have an associated smoothing weight; see note 27).
Back-off level Ppriorw(w, t| . . .) PpriorNT(M| . . .)
0 prior w, t
1 prior t
Acknowledgments
I would especially like to thank Mike
Collins for his invaluable assistance and
great generosity while I was replicating his
thesis results and for his comments on a
prerelease draft of this article. Many thanks
to David Chiang and Dan Gildea for the
many valuable discussions during the
course of this work. Also, thanks to the
anonymous reviewers for their helpful and
astute observations. Finally, thanks to my
Ph.D. advisor Mitch Marcus, who during
the course of this work was, as ever, a
source of keen insight and unbridled
optimism. This work was supported in part
by NSF grant no. SBR-89-20239 and DARPA
grant no. N66001-00-1-8915.
510
Computational Linguistics Volume 30, Number 4
References
Baker, J. K. 1979. Trainable grammars for
speech recognition. In Spring Conference of
the Acoustical Society of America, pages
547?550, Boston.
Bies, A. 1995. Bracketing guidelines for
Treebank II style Penn Treebank Project.
Available at ftp://ftp.cis.upenn.edu/pub/
treebank/doc/manual/root.ps.gz.
Bikel, Daniel M. 2000. A statistical model for
parsing and word-sense disambiguation.
In Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora, Hong Kong, October.
Bikel, Daniel M. 2002. Design of a
multi-lingual, parallel-processing
statistical parsing engine. In Proceedings of
HLT2002, San Diego.
Bikel, Daniel M. and David Chiang. 2000.
Two statistical parsing models applied to
the Chinese Treebank. In Martha Palmer,
Mitch Marcus, Aravind Joshi, and Fei Xia,
editors, Proceedings of the Second Chinese
Language Processing Workshop, pages 1?6,
Hong Kong.
Bikel, Daniel M., Richard Schwartz, Ralph
Weischedel, and Scott Miller. 1997.
Nymble: A high-performance learning
name-finder. In Fifth Conference on Applied
Natural Language Processing, pages
194?201, Washington, DC.
Black, Ezra, Frederick Jelinek, John Lafferty,
David Magerman, Robert Mercer, and
Salim Roukos. 1992. Towards
history-based grammars: Using richer
models for probabilistic parsing. In
Proceedings of the Fifth DARPA Speech and
Natural Language Workshop, Harriman, NY.
Booth, T. L. and R. A. Thompson. 1973.
Applying probability measures to abstract
languages. IEEE Transactions on Computers,
volume C-22: 442?450.
Chiang, David and Daniel M. Bikel. 2002.
Recovering latent information in
treebanks. In Proceedings of COLING?02,
Taipei.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. MIT Press,
Cambridge, MA.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. In Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 184?191,
Santa Cruz, CA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL-EACL ?97, pages
16?23, Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing. In
International Conference on Machine
Learning, Stanford University, Stanford, CA.
Collins, Michael and Nigel Duffy. 2002.
New ranking algorithms for parsing and
tagging: Kernels over discrete structures,
and the voted perceptron. In Proceedings of
ACL-02, pages 263?270, Philadelphia.
Eisner, Jason. 1996. Three new probabilistic
models for dependency parsing: An
exploration. In Proceedings of the 16th
International Conference on Computational
Linguistics (COLING-96), pages 340?345,
Copenhagen, August.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods in
Natural Language Processing, Pittsburgh.
Gildea, Daniel and Daniel Jurafsky. 2000.
Automatic labeling of semantic roles. In
Proceedings of ACL 2000, Hong Kong.
Gildea, Daniel and Martha Palmer. 2002.
The necessity of parsing for predicate
argument recognition. In Proceedings of
ACL 2002, Philadelphia.
Goodman, Joshua. 1997. Global thresholding
and multiple-pass parsing. In Proceedings
of the Second Conference on Empirical
Methods in Natural Language Processing,
Brown University, Providence, RI.
Henderson, John C. and Eric Brill. 1999.
Exploiting diversity in natural language
processing: Combining parsers. In
Proceedings of the Fourth Conference on
Empirical Methods in Natural Language
Processing, College Park, MD.
Hwa, Rebecca. 2001. On minimizing
training corpus for parser acquisition. In
Proceedings of the Fifth Computational
Natural Language Learning Workshop,
Toulouse, France, July.
Hwa, Rebecca, Philip Resnik, and Amy
Weinberg. 2002. Breaking the resource
bottleneck for multilingual parsing. In
Workshop on Linguistic Knowledge
Acquisition and Representation: Bootstrapping
Annotated Language Data, Third
International Conference on Language
Resources and Evaluation (LREC-2002), Las
Palmas, Canary Islands, June.
Lari, K. and S. J. Young. 1990. The
estimation of stochastic context-free
grammars using the Inside-Outside
algorithm. Computer Speech and Language,
4:35?56.
511
Bikel Intricacies of Collins? Parsing Model
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19:313?330.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Conference on Empirical Methods in
Natural Language Processing, University of
Pennsylvania, Philadelphia, May.
Witten, I. T. and T. C. Bell. 1991. The
zero-frequency problem: Estimating the
probabilities of novel events in adaptive
text compression. IEEE Transactions on
Information Theory 37: 1085?1094.
Two Statistical Parsing Models Applied to the Chinese Treebank 
Daniel M. Bikel David Chiang 
Department of Computer & Information Science 
University of Pennsylvania 
200 South 33rd Street 
Philadelphia, PA 19104-6389 
(dbikel, dchiang)?cis, upenn, edu 
Abst ract  
This paper presents the first-ever 
results of applying statistical pars- 
ing models to the newly-available 
Chinese Treebank. We have em- 
ployed two models, one extracted 
and adapted from BBN's SIFT Sys- 
tem (Miller et al, 1998) and a TAG- 
based parsing model, adapted from 
(Chiang, 2000). On sentences with 
<40 words, the former model per- 
forms at 69% precision, 75% recall, 
and the latter at 77% precision and 
78% recall. 
1 In t roduct ion  
Ever since the success of HMMs' applica- 
tion to part-of-speech tagging in (Church, 
1988), machine learning approaches to nat- 
ural language processing have steadily be- 
come more widespread. This increase has of 
course been due to their proven efficacy in 
many tasks, but also to their engineering effi- 
Cacy. Many machine learning approaches let 
the data speak for itself (data ipsa loquun- 
tur), as it were, allowing the modeler to focus 
on what features of the data are important, 
rather than on the complicated interaction 
of such features, as had often been the case 
with hand-crafted NLP systems. The success 
of statistical methods in particular has been 
quite evident in the area of syntactic pars- 
ing, most recently with the outstanding re- 
sults of (Charniak, 2000) and (Colhns, 2000) 
on the now-standard English test set of the 
Penn Treebank (Marcus et al, 1993). A sig- 
nificant trend in parsing models has been the 
incorporation of linguistically-motivated f a- 
tures; however, it is important o note that 
"linguistically-motivated" does not necessarily 
mean "language-dependent"---often, it means 
just the opposite. For example, almost all sta- 
tistical parsers make use of lexicalized non- 
terminals in some way, which allows lexical 
items' indiosyncratic parsing preferences to 
be modeled, but the paring between head 
words and their parent nonterminals i  deter- 
mined almost entirely by the training data, 
thereby making this feature--which models 
preferences of particular words of a par- 
ticular language---almost entirely language- 
independent. In this paper, we will explore the 
use of two parsing models, which were origi- 
nally designed for English parsing, on parsing 
Chinese, using the newly-available Chinese 
Treebank. We will show that the language- 
dependent components of these parsers are 
quite compact, and that with little effort they 
can be adapted to produce promising results 
for Chinese parsing. We also discuss directions 
for future work. 
2 Mode ls  and  Mod i f i ca t ions  
We will briefly describe the two parsing mod- 
els employed (for a full description of the BBN 
model, see (Miller et al, 1998) and also (Bikel, 
2000); for a full description Of the TAG model, 
see (Chiang, 2000)). 
2.1 Model  2 of (Collins, 1997) 
Both parsing models discussed in this paper 
inherit a great deal from this model, so we 
briefly describe its "progenitive" features here, 
describing only how each of the two models of 
this paper differ in the subsequent two sec- 
tions. 
The lexicalized PCFG that sits behind 
Model 2 of (Collins, 1997) has rules of the 
form 
P ~ LnLn-I"'" L IHR I " "  .Rn-IRn (1) 
S(will-MD) 
NP(AppI,~NNP) VP(wilI-MD) 
NNP 
I Apple MD VP (buy-VB) 
VB PRT(out-RP) NP(Microsoft--NNP) 
I \[ I 
buy RP NNP 
I I 
out Microsoft 
Figure 1: A sample sentence with parse tree. 
where P, Li, R/ and H are all lexicalized 
nonterminals, and P inherits its lexical head 
from its distinguished head child, H. In this 
generative model, first P is generated, then 
its head-child H, then each of the left- and 
right-modifying nonterminals are generated 
from the head outward. The modifying non- 
terminals Li and R/are  generated condition- 
ing on P and H, as well as a distance met- 
ric (based on what material intervenes be- 
tween the currently-generated modifying non- 
terminal and H) and an incremental subcat 
frame feature (a multiset containing the com- 
plements of H that have yet to be gener- 
ated on the side of H in which the currently- 
generated nonterminal falls). Note that if the 
modifying nonterminals were generated com- 
pletely independently, the model would be 
very impoverished, but in actuality, by includ- 
ing the distance and subcat frame features, 
the model captures a crucial bit of linguis- 
tic reality, viz., that words often have well- 
defined sets of complements and adjuncts, dis- 
persed with some well-defined istribution in 
the right hand sides of a (context-free) rewrit- 
ing system. 
2.2 BBN Mode l  
2.2.1 Overv iew 
The BBN model is also of the lexicalized 
PCFG variety. In the BB.N model, as with 
Model 2 of (Collins, 1997), modifying non- 
terminals are generated conditioning both on 
the parent P and its head child H. Unlike 
Model 2 of (Collins, 1997), they are also gen- 
erated conditioning on the previously gener- 
ated modifying nonterminal, L/-1 or Pq-1, 
and there is no subcat frame or distance fea- 
ture. While the BBN model does not per- 
form at the level of Model 2 of (Collins, 1997) 
on Wall Street Journal text, it is also less 
language-dependent, eschewing the distance 
metric (which relied on specific features of the 
English Treebank) in favor of the "bigrams on 
nonterminals" model. 
2.2.2 Mode l  Parameters  
This section briefly describes the top-level 
parameters used in the BBN parsing model. 
We use p to denote the unlexicalized nonter- 
minal corresponding to P in (1), and simi- 
larly for li, ri and h. We now present he top- 
level generation probabilities, along with ex- 
amples from Figure 1. For brevity, we omit the 
smoothing details of BBN's model (see (Miller 
et al, 1998) for a complete description); we 
note that all smoothing weights are computed 
via the technique described in (Bikel et al, 
1997). 
The probability of generating p as the 
root label is predicted conditioning on only 
+TOP+,  which is the hidden root of all parse 
trees: 
P (Pl + TOP+) ,  e.g., P(S I + TOP+).  (2) 
The probability of generating a head node h 
with a parent p is 
P(h I P), e.g., P(VP \] S). (3) 
The probability of generating a left-modifier 
li is 
PL(li I Z -i,p, h, wh),, e.g., (4) 
PL(NP \] + BEGIN+, S, VP, will) 
2 
when generating the NP for NP(Apple-NNP), 
and the probability of generating a right mod- 
ifier ri is 
PR(ri i ri-i,p, h, Wh), e.g., (5) 
Pn(NP I PRT, VP, VB, buy) 
when generating the NP for NP(Microsoft- 
NNP). 1 
The probabilities for generating lexical ele- 
ments (part-of-speech tags and words) are as 
follows. The part of speech tag of the head of 
the entire sentence, th, is computed condition- 
ing only on the top-most symbol p:2 
P(th I P)- (6) 
Part of speech tags of modifier constituents, 
tli and tri, are predicted conditioning on the 
modifier constituent li or r/, the tag of the 
head constituent, h, and the word of the head 
constituent, Wh 
P(tl, \[li, th, Wh) and P(tr~ \[ ri, th, Wh). (7) 
The head word of the entire sentence, Wh, is 
predicted conditioning only on the top-most 
symbol p and th: 
P(whlth,p). (8) 
Head words of modifier constituents, w h and 
Wry, are predicted conditioning on all the con- 
text used for predicting parts of speech in (7), 
as well as the parts of speech themsleves 
P(wt, \[ tl,, li, th, Wh) 
and P(wri \[ try, ri, th, Wh). (9) 
The original English model also included a 
word feature to heIp reduce part-of-speech 
ambiguity for unknown words, but this com- 
ponent of the model was removed for Chinese, 
as it was language-dependent. 
The probability of an entire parse tree is 
the product of the probabilities of generat- 
ing all of the elements of that parse tree, 
1The hidden nonterminal +BEGIN+ is used to 
provide a convenient mechanism for determining the 
initial probability of the underlying Markov process 
generating the modifying nonterminals; the hidden 
nonterminal +END+ is used to provide consistency to
the underlying Markov process, i.e., so that he proba- 
bilities of all possible nonterminal sequences sum to 1. 
2This is the one place where we altered the original 
model, as the lexical components of the head of the 
entire sentence were all being estimated incorrectly, 
causing an inconsistency in the model. We corrected 
the estimation of th and Wh in our implementation. 
where an element is either a constituent la- 
bel, a part of speech tag or a word. We obtain 
maximum-likelihood estimates of the param- 
eters of this model using frequencies gathered 
from the training data. 
2.3 TAG Mode l  
The model of (Chiang, 2000) is based 
on stochastic TAG (Resnik, 1992; Schabes, 
1992). In this model a parse tree is built up not 
out of lexicalized phrase-structure rules but by 
tree fragments (called elementary trees) which 
are texicalized in the sense that each fragment 
contains exactly one lexical item (its anchor). 
In the variant of TAG we use, there are 
three kinds of elementary tree: initial, (pred- 
icative) auxiliary, and modifier, and three 
composition operations: substitution, adjunc- 
tion, and sister-adjunction. Figure 2 illus- 
trates all three of these operations, c~i is an 
initial tree which substitutes at the leftmost 
node labeled NP$;/~ is an auxiliary tree which 
adjoins at the node labeled VP. See (Joshi and 
Schabes, 1997) for a more detailed explana- 
tion. 
Sister-adjunction is not a standard TAG op- 
eration, but borrowed from D-Tree Grammar 
(Rainbow et al, 1995). In Figure 2 the modi- 
fier tree V is sister adjoined between the nodes 
labeled VB and NP$. Multiple modifier trees 
can adjoin at the same place, in the spirit of 
(Schabes and Shieber, 1994). 
In stochastic TAG, the probability of gen- 
erating an elementary tree depends on the el- 
ementary tree itself and the elementary tree 
it attaches to. The parameters are as follows: 
= i 
I + P (NONE I = i 
# 
where c~ ranges over initial trees,/~ over aux- 
iliary trees, 3' over modifier trees, and T/over 
nodes. Pi(c~) is the probability of beginning 
a derivation with c~; Ps(o~ I 77) is the prob- 
ability of substituting o~ at 7; Pa(/~ I r/) is 
the probability of adjoining ~ at 7/; finally, 
Pa(NONE I 7) is the probability of nothing 
adjoining at ~/. 
Our variant adds another set of parameters: 
3 
~2 
(O~2) S l ~. 2 
NPJ~ ..... VP S 
, . "  gap 
...... ! ~ 4 
.... ?....... ! i ~.. . ............ NP VP .,._.re d
{ \[ buy \ ": I 
NP  VP  PRT  NP ~ NNP 
I ~ .  I I I MD VP 
NNP MD VP* RP NNP Apple I 
I I I I will 
Apple will out Microsoft VB PitT NP 
I I I 
(oq) (fl) (7) (a3) buy RP NNP 
I I 
out Microsoft 
de.vat .n  tree 
tree 
Figure 2: Grammar and derivation for "Apple will buy out Microsoft." 
~ Psa(T I ~7, i , f )  + Psa(STOP I ~l,i,f) = 1 
This is the probability of sister-adjoining 7 
between the ith and i + lth children of ~ (al- 
lowing for two imaginary children beyond the 
leftmost and rightmost children). Since multi- 
ple modifier trees can adjoin at the same lo- 
cation, Psa(7) is also conditioned on a flag f 
which indicates whether '7 is the first modi- 
fier tree (i.e., the one closest o the head) to 
adjoin at that location. 
For our model we break down these prob- 
abilities further: first the elementary tree is 
generated without its anchor, and then its an- 
chor is generated. See (Chiang, 2000) for more 
details. 
During training each example is broken 
into elementary trees using head rules and 
argument/adjunct rules similar to those of 
(Collins, 1997). The rules are interpreted as 
follows: a head is kept in the same elemen- 
tary tree in its parent, an argument is broken 
off into a separate initial tree, leaving a sub- 
stitution node, and an adjunct is broken off 
into a separate modifier tree. A different rule 
is used for extracting auxiliary trees; see (Chi- 
ang, 2000) for details. Xia (1999) describes a
similar process, and in fact our rules for the 
Xinhua corpus are based on hers. 
2.4 Modif icat ions 
The primary language-dependent component 
that had to be changed in both models was 
the head table, used to determine heads when 
training. We modified the head rules described 
in (Xia, 1999) for the Xinhua corpus and sub- 
stituted these new rules into both models. 
The (Chiang, 2000) model had the following 
additional modifications. 
? The new corpus had to be prepared for 
use with the trainer and parser. Aside 
from technicalities, this involved retrain- 
ing the part-of-speech tagger described in 
(Ratnaparkhi, 1997), which was used for 
tagging unknown words. We also lowered 
the unknown word threshold from 4 to 2 
because the Xinhua corpus was smaller 
than the WSJ corpus. 
? In addition to the change to the head- 
finding rules, we also changed the rules 
for classifying modifiers as arguments or 
adjuncts. In both cases the new rules were 
adapted from (Xia, 1999). 
? For the tests done in this paper, a beam 
width of 10 -4 was used. 
The BBN model had the following additional 
modifications: 
? As with the (Chiang, 2000) model, we 
similarly lowered the unknown word 
threshold of the BBN model from its de- 
fault 5 to 2. 
? The language-dependent word-feature 
was eliminated, causing parts of speech 
for unknown words to be predicted solely 
on the head relations in the model. 
? The default beam size in the probabilis- 
tic CKY parsing algorithm was widened. 
The default beam pruned away chart en- 
tries whose scores were not within a fac- 
tor of e -5 of the top-ranked subtree; this 
4 
Model, test set 
BBN-allt, WSJ-all 
BBN-small-h WSJ-small* 
BBN,  Xinhua:~ 
Chiang-all, WSJ-all 
Chiang-small, WSJ-small 
Ch iang,  X inhua  
BBN-allt, WSJ-all 
BBN-smallI, WSJ-small* 
BBN,  Xinhua:~ 
Chiang-all, WSJ-all 
Chiang-small, WSJ-small 
Ch iang,  X inhua  
<40 words 
LR LP CB 0CB <2CB 
84.7 86.5 1.12 60.6 83.2 
79.0 80.7 1.66 47.0 74.6 
69.0 74.8 2.05 45.0 68.5 
86.9 86.6 1.09 63.2 84.3 
78.9 79.6 1.75 44.8 72.4 
76.8 77.8 1.99 50.8 74.1 
-< 100 words 
LR LP CB 0CB _<2CB 
83.9 85.7 1.31 57.8 80.8 
78.4 80.0 1.92 44.3 71.3 
67.5 73.5 2.87 39.9 61.8 
86.2 85.8 1.29 60.4 81.8 
77.1 78.8 2.00 43.25 70.5 
73.3 74.6 3.03 44.8 66.8 
Table 1: Results for both parsing models on all test sets. Key: LR = labeled recall, LP = labeled 
precision, CB = avg. crossing brackets, 0CB = zero crossing brackets, <2CB = <2 crossing 
brackets. All results are percentages, except for those in the CB column, tUsed larger beam 
settings and lower unknown word threshold than the defaults. *3 of the 400 sentences were not 
parsed due to timeouts and/or pruning problems. :~3 of the 348 sentences did not get parsed due 
to pruning problems, and 2 other sentences had length mismatches ( coring program errors). 
tight limit was changed to e -9. Also, the 
default decoder pruned away all but the 
top 25-ranked chart entries in each cell; 
this limit was expanded to 50. 
3 Exper iments  and  Resu l ts  
The Chinese Treebank consists of 4185 sen- 
tences of Xinhua newswire text. We blindly 
separated this into training, devtest and test 
sets, with a roughly 80/10/10 split, putting 
files 001-270 (3484 sentences, 84,873 words) 
into the training set, 301-325 (353 sentences, 
6776 words) into the development test set and 
reserving 271-300 (348 sentences, 7980 words) 
for testing. See Table 1 for results. 
In order to put the new Chinese Treebank 
results into context with the unmodified (En- 
glish) parsing models, we present results on 
two test sets from the Wall Street Journal: 
WSJ-all, which is the complete Section 23 (the 
de facto standard test set for English pars- 
ing), and WSJ-small, which is the first 400 
sentences of Section 23 and which is roughly 
comparable in size to the Chinese test set. 
Furthermore, when testing on WSJ-small, we 
trained on a subset of our English training 
data roughly equivalent in size to our Chinese 
training set (Sections 02 and 03 of the Penn 
Treebank); we have indicated models trained 
on all English training with "-all", and mod- 
els trained with the reduced English train- 
ing set with "-small". Therefore, by compar- 
ing the WSJ-small results with the Chinese 
results, one can reasonably gauge the perfor- 
mance gap between English parsing on the 
Penn Treebank and Chinese parsing on the 
Chinese Treebank. 
The reader will note that the modified BBN 
model does significantly poorer than (Chiang, 
2000) on Chinese. While more investigation is 
required, we suspect part of the difference may 
be due to the fact that currently, the BBN 
model uses language-specific rules to guess 
part of speech tags for unknown words. 
4 Conc lus ions  and  Future  Work  
There is no question that a great deal of care 
and expertise went into creating the Chinese 
Treebank, and that it is a source of important 
grammatical information that is ufiique to the 
Chinese language. However, there are definite 
similarities between the grammars of English 
and Chinese, especially when viewed through 
the lens of the statistical models we employed 
here. In both languages, the nouns, adjec- 
tives, adverbs, and verbs have preferences for 
certain arguments and adjuncts, and these 
preferences--in spite of the potentially vastly- 
different configurations of these items--are f- 
fectively modeled. As discussed in the intro- 
duction, lexica! items' idiosyncratic parsing 
preferences are modeled by lexicalizing the 
grammar formalism, using a lexicalized PCFG 
in one case and a lexicalized stochastic TAG 
in the other. Linguistically-reasonable inde- 
pendence assumptions are made, such as the 
independence of grammar productions in the 
case of the PCFG model, or the independence 
of the composition operations in the case of 
the LTAG model, and we would argue that 
these assumptions are no less reasonable for 
the Chinese grammar than they are for that 
of English. While results for the two languages 
are far from equal, we believe that further tun- 
ing of the head rules, and analysis of develop- 
ment test set errors will yield significant per- 
formance gains on Chinese to close the gap. 
Finally, we fully expect hat absolute perfor- 
mance will increase greatly as additional high- 
quality Chinese parse data becomes available. 
5 Acknowledgements  
This research Was funded in part by NSF 
grant SBR-89-20230-15. We would greatly 
like to acknowledge the researchers at BBN 
who allowed us to use their model: Ralph 
Weischedel, Scott Miller, Lance Rarnshaw, 
Heidi Fox and Sean Boisen. We would also 
like to thank Mike Collins and our advisors 
Aravind Joshi and Mitch Marcus. 
Re ferences  
Daniel M. Bikel, Richard Schwartz, Ralph 
Weischedel, and Scott Miller. 1997. Nymble: 
A high-performance learning name-finder. In 
Fifth Conference on Applied Natural Language 
Processing, pages 194-201,, Washington, D.C. 
Daniel M. Bikel. 2000. A statistical model for 
parsing and word-sense disarnbiguation. In 
Joint SIGDAT Conference on Empirical Meth- 
ods in Natural Language Processing and Very 
Large Corpora, Hong Kong, October. 
Eugene Charniak. 2000. A maximum entropy- 
inspired parser. In Proceedings of the 1st Meet- 
ing of the North American Chapter of the As- 
sociation for Computational Linguistics, pages 
132-139, Seattle, Washington, April 29 to May 
4. 
David Chiang. 2000. Statistical parsing with an 
automatically-extracted tr eadjoining gram- 
mar. In Proceedings of the 38th Annual Meeting 
of the Association for Computational Linguis- 
tics. 
Kenneth Church. 1988. A stochastic parts pro- 
gram and noun phrase parser for unrestricted 
text. In Second Conference on Applied Natural 
Language Processing, pages 136-143, Austin, 
Texas. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
ceedings of ACL-EACL '97, pages 16-23. 
Michael Collins. 2000. Discriminative r ranking 
for natural language parsing. In International 
Conference on Machine Learning. (to appear). 
Aravind K. Joshi and Yves Schabes. 1997. Tree- 
adjoining grammars. In A. Salomma and 
G. Rosenberg, editors, Handbook of Formal 
Languages and Automata, volume 3, pages 69- 
124. Springer-Verlag, Heidelberg. 
Mitchell P. Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building a 
large annotated corpus of English: The Penn 
Treebank. Computational Linguistics, 19:313- 
330. 
Scott Miller, Heidi Fox, Lance Ramshaw, and 
Ralph Weischedel. 1998. SIFT - Statistically- 
derived Information From Text. In Seventh 
Message Understanding Conference (MUC-7), 
Washington, D.C. 
Owen Rainbow, K. Vijay-Shanker, and David 
Weir. 1995. D-tree grammars. In Proceedings 
of the 33rd Annual Meeting of the Assocation 
for Computational Linguistics, pages 151-158. 
Adwait Ratnaparkhi. 1997. A simple introduc- 
tion to maximum entropy models for natural 
language processing. Technical Report 1RCS 
Report 97-08, Institute for Research in Cogni- 
tive Science, May. 
Philip Resnik. 1992. Probabilistic tree-adjoining 
grammar as a framework for statistical nat- 
ural language processing. In Proceedings of 
COLING-92, pages 418-424. 
Yves Schabes and Stuart M. Shieber. 1994. An 
alternative conception of tree-adjoining deriva- 
tion. Computational Linguistics, 20(1):91-124. 
Yves Schabes. 1992. Stochastic lexicalized 
tree-adjoining grammars. In Proceedings of 
COLING-92, pages 426-432. 
Fei Xia. 1999. Extracting tree adjoining ram- 
mars from bracketed corpora. In Proceedings 
of the 5th Natural Language Processing Pacific 
Rim Symposium (NLPRS-99). 
A Statistical Model for Parsing and Word-Sense Disambiguation 
Danie l  M.  B ike l  
Dept. of Computer  & Information Science 
University of Pennsylvania 
200 South 33rd Street, Philadelphia, PA 19104-6389, U.S.A. 
db ike l@c is ,  upenn,  edu  
Abst rac t  
This paper describes a first attempt at a sta- 
tistical model for simultaneous syntactic pars- 
ing and generalized word-sense disambigna- 
tion. On a new data set we have constructed 
for the task, while we were disappointed not 
to find parsing improvement over a traditional 
parsing model, our model achieves a recall of 
84.0% and a precision of 67.3% of exact synset 
matches on our test corpus, where the gold 
standard has a reported inter-annotator agree- 
ment  of 78.6%. 
1 I n t roduct ion  
In this paper we describe a generative, statis- 
tical model for simultaneously producing syn- 
tactic parses and word senses in sentences. 
We begin by motivating this new approach to 
these two, previously-separate problems, then, 
after reviewing previous work in these areas, 
we describe our model in detail. Finally, we 
will present he promising results of this, our 
first attempt, and the direction of future work. 
2 Mot ivat ion  for  the  Approach  
2.1 Mot ivat ion f rom examples  
Consider the following examples: 
1. IBM bought Lotus for $200 million. 
2. Sony widened its product line with per- 
sonal computers. 
3. The bank issued a check for $100,000. 
4. Apple is expecting \[NP strong results\]. 
5. IBM expected \[SBAa each employee to 
wear a shirt and tie\]. 
With Example 1, the reading \[IBM bought 
\[Lotus for $200 million\]\] is nearly impossi- 
ble, for the simple reason that a monetary 
amount is a likely instrument for buying and 
not for describing a company. Similarly, there 
is a reasonably strong preference in Example 
2 for \[pp with personal computers\] to attach 
to widened, because personal computers are 
products with which a product line could be 
widened. As pointed out by (Stetina and Na- 
gao, 1997), word sense information can be a 
proxy for the semantic- and world-knowledge 
we as humans bring to bear on attachment 
decisions uch as these. This proxy effect is 
due to the "lightweight semantics" that word 
senses--in particular WordNet word senses-- 
convey. 
Conversely, both the syntactic and semantic 
context in Example 3 let us know that bank 
is not a river bank and that check is not a 
restaurant bill. In Examples 4 and 5, knowing 
that the complement of expect is an NP or an 
SBAR provides information as to whether the 
sense is "await" or "require". Thus, Examples 
3-5 illustrate how the syntactic ontext of a 
word can help determine its meaning. 
2.2 Mot ivat ion  f rom prev ious  work  
2.2.1 Pars ing  
In recent years, the success of statistical pars- 
ing techniques can be attributed to several fac- 
tors, such as the increasing size of comput- 
ing machinery to accommodate larger models, 
the availability of resources uch as the Penn 
Treebank (Marcus et al, 1993) and the suc- 
cess of machine learning techniques for lower- 
level NLP problems, such as part-of-speech 
tagging (Church, 1988; Brill, 1995), and PP- 
attachment (Brill and Resnik, 1994; Collins 
and Brooks, 1995). However, perhaps even 
more significant has been the lexicalization 
of the grammar formalisms being probabilis- 
tically modeled: crucially, all the recent, suc- 
cessful statistical parsers have in some way 
made use of bilexical dependencies. This in- 
cludes both the parsers that attach probabili- 
ties to parser moves (Magerman, 1995; Ratna- 
parkhi, 1997), but also those of the lexicalized 
PCFG variety (Collins, 1997; Charniak, 1997). 
155 
Even more crucially, the bilexical dependen- 
cies involve head-modifier relations (hereafter 
referred to simply as "head relations"). The in- 
tuition behind the lexicalization of a grammar 
formalism is to capture lexical items' idiosyn- 
cratic parsing preferences. The intuition be- 
hind using heads as the members of the bilex- 
ical relations is twofold. First, many linguis- 
tic theories tell us that the head of a phrase 
projects the skeleton of that phrase, to be filled 
in by specifiers, complements and adjuncts; 
such a notion is captured quite directly by 
a formalism such as LTAG (Joshi and Sch- 
abes, 1997). Second, the head of a phrase usu- 
ally conveys ome large component of the se- 
mantics of that phraseJ In this way, using 
head-relation statistics encodes a bit of the 
predicate-argument structure in the syntac- 
tic model. While there are cases such as John 
was believed to have been shot by Bill where 
structural preference virtually eliminates one 
of the two semantically plausible analyses, it 
is quite clear that semantics--and, in particu- 
lar, lexical head semantics--play a very im- 
portant role in reducing parsing ambiguity. 
(See (Collins, 1999), pp. 207ff., for an excel- 
lent discussion of structural vs. semantic pars- 
ing preferences, including the above John was 
believed.., example.) 
Another motivation for incorporating word 
senses into a statistical parsing model has 
been to ameliorate the sparse data prob- 
lem. Inspired by the PP-attachment work of 
(Stetina and Nagao, 1997), we use Word- 
Net vl.6 (Miller et al, 1990) as our seman- 
tic dictionary, where the hypernym structure 
provides the basis for semantically-motivated 
soft clusters. 2 We discuss this benefit of word 
senses and the details of our implementation 
further in Section 4. 
2.2.2 Word-sense  d isambiguat ion  
While there has been much work in this area, 
let us examine the features used in recent 
1Heads originated this way, but it has become nec- 
essary to distinguish "semantic" heads, such as nouns 
and verbs, that correspond roughly to predicates and 
arguments, from "functional" heads, such as deter- 
miners, INFL's and complemeutizers, that correspond 
roughly to logical operators or are purely syntactic el- 
ements. In this paper, we almost always intend "head" 
to mean "semantic head". 
2Soft clusters are sets where the elements have 
weights indicating the strength of their membership 
in the set, which in this case allows for a probability 
distribution to be defined over a word's membership 
in all the clusters. 
statistical approaches. (Yarowsky, 1992) uses 
wide "bag-of-words" contexts with a naive 
Bayes classifier. (Yarowsky, 1995) also uses 
wide context, but incorporates the one-sense- 
per-discourse and one-sense-per-collocation 
constraints, using an unsupervised learn- 
ing technique. The supervised technique in 
(Yarowsky, 1994) has a more specific notion 
of context, employing not just words that can 
appear within a window of I k ,  but crucially 
words that abut and fall in the ~2 window 
of the target word. More recently, (Lin, 1997) 
has shown how syntactic ontext, and depen- 
dency structures in particular, can be suc- 
cessfully employed for word sense disambigua- 
tion. (Stetina and Nagao, 1997) have shown 
that by employing a fairly simple and some- 
what ad-hoc unsupervised method of WSD us- 
ing a WordNet-based similarity heuristic, they 
could enhance PP-attachment performance to
a significantly higher level than systems that 
made no use of lexical semantics (88.1% accu- 
racy). Most recently, in (Stetina et al, 1998), 
the authors made use of head-driven bilexi- 
cal dependencies with syntactic relations to 
attack the problem of generalized word-sense 
disambiguation, precisely one of the two prob- 
lems we are dealing with here. 
3 The  Mode l  
3.1 Overv iew 
The parsing model we started with was ex- 
tracted from BBN's SIFT system (Miller et 
al., 1998), which we briefly present again here, 
using examples from Figure 1 to illustrate the 
model's parameters. 3 
The model generates the head of a con- 
stituent first, then each of the left- and right- 
modifiers, generating from the head outward, 
using a bigram model of node labels. Here are 
the first few elements generated by the model 
for the tree of Figure 1: 
1. S and its head word and part of speech, 
caught- VB D. 
2. The head constituent of S, VP. 
3. The head word of the VP, caught-VBD. 
4. The premodifier constituent ADVP. 
3We began with the BBN parser because its authors 
were kind enough to allow us to extent it, and because 
its design allowed easy integration with our existing 
WordNet code. 
156 
S(caught-VBD) 
NP(boy-NN) ADVP(also-RB) VP(caught-VBD) 
DET NN RB VBD NP(ball-NN) I I I 
The boy also \[ caught DET NN 
i I 
the ball 
Figure 1: A sample sentence with parse tree. 
5. The head word of the premodifier ADVP, 
also-RB. 
6. The premodifier constituent NP. 
7. The head word of the premodifier NP, 
boy-NN. 
8. The +END+ (null) postmodifier con- 
stituent of the VP. 
This process recurses on each of the modifier 
constituents (in this case, the subject NP and 
the VP) until all words have been generated. 
(Note that many words effectively get gener- 
ated high up in the tree; in this example sen- 
tence, the last words to get generated are the 
two the's ) 
More formally, the lexicalized PCFG that 
sits behind the parsing model has rules of the 
form 
Figure 1. For brevity, we omit the smooth- 
ing details of BBN's model (see (Miller et al, 
1998) for a complete description); we note that 
all smoothing weights are computed via the 
technique described in (Bikel et al, 1997). 
The probability of generating p as the 
root label is predicted conditioning on only 
+TOP+,  which is the hidden root of all parse 
trees: 
P(P l  +TOP+) ,  e.g., P(S I + TOP+).  (2) 
The probability of generating a head node h 
with a parent p is 
P(hlp), e.g., P(VP  I S). (3) 
The probability of generating a left-modifier l~ 
is 
P ~ LnLn- I " "L1HRI" "Rn- iRn  (1) 
where P,  H, L, and .P~ are all lexicalized non- 
terminals, i.e., of the form X(w, t, f l ,  where X 
is a traditional CFG nonterminal and (w, t, f /  
is the word-part-of.-speech-word-feature tripl  
that is the head of the phrase denoted by X. 4 
The lexicalized nonterminal H is so named be- 
cause it is the head constztuent, where P inher- 
its its head triple from this head constituent. 
The constituents labeled L~ and .R~ are left- 
and right-modifier constituents, respectively. 
3.2 P robab i l i ty  s t ruc ture  o f  the  
original  mode l  
We use p to denote the unlexicalized nontermi- 
nal corresponding to P, and similarly for li, ri 
and h. We now present he top-level genera- 
tion probabilities, along with examples from 
4The inclusion of the word feature in the BBN 
model was due to the work described in (Weischedel 
et al, 1993), where word features helped reduce part 
of speech ambiguity for unknown words. 
PL(Iz Ilz-l,p,h, wh),, e.g., (4) 
PL (NP I ADVP, S, VP, caught) 
when generating the NP for NP(boy-NN), and 
the probability of generating a right modifier 
r; is 
PR(r~ I r i- l ,p, h, Wh), e.g., (5) 
PR(NP I + BEGIN+, VP, VBD, caught) 
when generating the NP for NP(ball-NN). 5 
The probabilities for generating lexical el- 
ements (part-of-speech tags, words and word 
features) are as follows. The part of speech 
tag of the head of the entire sentence, th, is 
5The bidden onterminal +BEGIN+ is used to pro- 
vide a convenient mechanism for determining the ini- 
tial probability of the underlying Markov process gen- 
erating the modifying nonterminals; the hidden non- 
terminal +END+ is used to provide consistency to the 
underlying Markov process, i.e., so that the probabil- 
ities of all possible nonterminal sequences sum to 1. 
157 
computed conditioning only on the top-most 
symbol p:6 
P(th I P). (6) 
Part of speech tags of modifier constituents, 
tt, and tri, are predicted conditioning on the 
modifier constituent lz or ri, the tag of the 
head constituent, h, and the word of the head 
constituent, Wh: 
P(tt, Ill, th, Wh) and P(tr,  \[ri, th, Wh). (7) 
The head word of the entire sentence, Wh, is 
predicted conditioning only on the top-most 
symbol p and th. 
P(Wh\[th,p). (8) 
Head words of modifier constituents, wl, and 
wr,, are predicted conditioning on all the con- 
text used for predicting parts of speech in (7), 
as well as the parts of speech themsleves 
P(wt, \]tt,, li, th, Wh) 
and P(wr, \] try, r~, th, Wh). (9) 
The word feature of the head of the entire sen- 
tence, fh, is predicted conditioning on the top- 
most symbol p, its head word, wh, and its head 
tag, th: 
P(fh \[Wh, th,p). (10) 
Finally, the word features for the head words 
of modifier constituents, fz, and fr,, are pre- 
dicted conditioning on all the context used to 
predict modifier head words in (9), as well as 
the modifier head words themselves: 
P(ft, I known(wt, ) tt~, li, th, Wh) (11) 
and P(fr, I known(w~,), tr,, ri, th, Wh) 
where known(x) is a predicate returning true 
if the word x was observed more than 4 times 
in the training data. 
The probability of an entire parse tree is the 
product of the probabifities of generating all of 
the elements of that parse tree, where an el- 
ement is either a constituent label, a part of 
speech tag, a word or a word feature. We ob- 
tain maximum-likelihood estimates of the pa- 
rameters of this model using frequencies gath- 
ered from the training data. 
6This is the one place where we have altered the 
original model, as the lexical components ofthe head 
of the entire sentence were all being estimated incor- 
rectly, causing an inconsistency in the model. We have 
corrected the estimation of th, Wh and fh in our im- 
plementation. 
4 Word-sense  Extens ions  to  the  
Lex ica l  Mode l  
The desired output structure of our com- 
bined parser/word-sense disambiguator is a 
standard, Treebank-style parse tree, where the 
words not only have parts ef speech, but also 
WordNet synsets. Incorporating synsets into 
the lexical part of the model is fairly straight- 
forward: a synset is yet another element o be 
generated. The question is when to generate it. 
The lexical model has decomposed the genera- 
tion of the (w, t, f )  triple into three steps, each 
conditioning on all the history of the previ- 
ous step. While it is probabilistically identical 
to predict synsets at any of the four possible 
points if we continue to condition on all the 
history at each step, we would like to pick the 
point that is most well-founded both in terms 
of the underlying linguistic structure and in 
terms of what can be well-estimated. In Sec- 
tion 2.2.1 we mentioned the soft-clustering as- 
pect of synsets; in fact, they have a duality. 
On the one hand, they serve to add specificity 
to what might otherwise be an ambiguous lexi- 
cal item; on the other, they are sets, clustering 
lexical items that have similar meanings. Even 
further, noun and verb synsets form a con- 
cept taxonomy, the hypernym relation forming 
a partial ordering on the lemmas contained 
in WordNet. The former aspect corresponds 
roughly to what we as human listeners or read- 
ers do: we hear or see a sequence of words in 
context, and determine incrementally the par- 
ticular meaning of each of those words. The 
latter aspect corresponds more closely to a 
mental model of generation: we have a desire 
or intention to convey, we choose the appropri- 
ate concepts with which to convey it, and we 
realize that desire or intention with the most 
felicitous syntactic structure and lexical real- 
izations of those concepts. As this is a genera- 
tive model, we generate a word's synset after 
generating the part of speech tag but before 
generating the word itself /  
The synset of the head of the entire sen- 
tence, Sh is predicted conditioning only on the 
top-most symbol p and the head tag, th: 
P(Sh\[th,p). (12) 
We accordingly changed the probability of 
7We believe that synsets and parts of speech are 
largely orthogonal with respect to tlieir lexical infor- 
mation, and thus their relative order of prediction was 
not a concern. 
158 
generating the head word of the entire sen- 
tence to be 
P(Wh I Sh, th,p). (13) 
The probability estimates for (12) and (13) are 
not smoothed. 
The probability model for generating 
synsets of modifier constituents mi, complete 
with smoothing components, i  as follows: 
P(Sm, I tin,, m~, Wh, Sh) = (14) 
~0P(Sm, I tm,, m,, w~, sh) 
+ AlP(SIn, I tm,,m,,sh) 
+ A2P(sm, It~,,rn,,@l(Sh)) 
. . .  
+ )~n+llS(Sm, \[tm,,mi,@n(Sh)) 
+ ~+2P(Sm, I tin,, m,) 
+ ~n+3P(Sm, ltm,) 
where @'(Sh) is the i th hypernym of Sh. The 
WordNet hypernym relations, however, do not 
form a tree, but a DAG, so whenever there are 
multiple hypernyms, the uniformly-weighted 
mean is taken of the probabilities condition- 
ing on each of the hypernyms. That is, 
P(Sm, I t~,,mi, @3(Sh) = (15) 
n 
1 Z P(Sm, Itm,,m~, @~(sh)) 
n k=l 
when@~(~h) = (~(~h) ,  . . . ,  @~(sh)}. 
Note that in the first level of back-off, we no 
longer condition on the head word, but strictly 
on its synset, and thereafter on hypernyms of 
that synset; these models, then, get at the 
heart of our approach, which is to abstract 
away from lexical head relations, and move 
to the more general lexico-semantic relations, 
here represented by synset relations. 
Now that we generate synsets for words us- 
ing (14), we can also change the word genera- 
tion model to have synsets in its history: 
P(w~, I sm,, t~,, m,, Wh, Sh) = (16) 
),0P(wm, Isin,, t~,  mi, wh) 
+ ~lP(wm, I sin,, t~,  mi, Sh) 
+ A2P(wm, ISm,,tm,,mi,@l(sh)) 
. .o  
+ A~,+lP(wm, I s~,,tm,,m,,@~(Sh)) 
+ ~,~+2.P(w~, I sin,, tin,, m,) 
+ ,Xn+3P(Wm, ISm,,tm,) 
? ~n+4P(Wm, \]Sin,) 
where once again, @i(Sh) is the zth hypernym 
of Sh. For both the word and synset prediction 
models, by backing off up the hypernym chain, 
there is an appropriate confiation of similar 
head relations. For example, if in training the 
verb phrase \[strike the target\] had been seen, if 
the unseen verb phrase \[attack the target\] ap- 
peared during testing, then the training from 
the semantically-similar training phrase could 
be used, since this sense of attack is the hy- 
pernym of this sense of stroke. 
Finally, we note that both of these synset- 
and word-prediction probability estimates 
contain an enormous number of back-off lev- 
els for nouns and verbs, corresponding to the 
head word's depth in the synset hierarchy. A
valid concern would be that the model might 
be backing off using histories that are fax too 
general, so we experimented with limiting the 
hypernym back-off to only two, three and four 
levels. This change produced a negligible dif- 
ference in parsing performance, s 
5 A New Approach ,  A New Data  
Set  
Ideally, the well-established gold standard for 
syntax, the Penn Treebank, would have a 
parallel word-sense-annotated corpus; unfor- 
tunately, no such word-sense corpus exists. 
However, we do have SemCor (Miller et al, 
1994), where every noun, verb, adjective and 
adverb from a 455k word portion of the Brown 
Corpus has been assigned a WordNet synset. 
While all of the Brown Corpus was anno- 
tated in the style of Treebank I, a great deal 
was also more recently annotated in Tree- 
bank II format, and this corpus has recently 
been released by the Linguistic Data Con- 
sortium. 9 As it happens, the intersection be- 
tween the Treebank-II-annotated Brown and 
SemCor comprises ome 220k words, most of 
which is fiction, with some nonfiction and hu- 
mor writing as well. 
We went through all 220k words of the cor- 
pora, synchronizing them. That is, we made 
sure that the corpora were identical up to 
the spelling of individual tokens, correcting all 
8We aim to investigate the precise effects of our 
back-off strategy in the next version of our combined 
parsing/WSD model. 
9We were given permission to use a pre-release ver- 
sion of this Treebank II-style corpus. 
159 
tokenization and sentence-breaking discrepan- 
cies. This correcton task ranged from the sim- 
ple, such as connecting two sentences in one 
corpus that were erroneously broken, to the 
middling, such as joining two tokens in Sem- 
Cor that comprised a hyphenate in Brown, to 
the difficult, such as correcting egregious parse 
annotation errors, or annotating entire sen- 
tences that were omitted from SemCor. In par- 
ticular, the case of hyphenates was quite fre- 
quent, as it was the default in SemCor to split 
up all such words and assign them their indi- 
vidual word senses (synsets). In general, we at- 
tempted to make SemCor look as much as pos- 
sible like the Treebank II-annotated Brown, 
and we used the following guidelines for as- 
signing word senses to hyphenates: 
1. Assign the word sense of the head of 
the hyphenate. E g., both twelve-foot and 
ten-foot get the word sense of foot_ l  (the 
unit of measure qual to 12 inches). 
2. If there is no clear head, then attempt 
to annotate with the word sense of the 
hypernym of the senses of the hyphenate 
components. E.g., U.S.-Soviet gets the 
word sense of country_2 (a state or na- 
tion). 
3. If options 1 and 2 are not possible, the 
hyphenate is split in the Treebank II file. 
4. If the hyphenate has the prefix non- or 
anti-, annotate with the word sense of 
that which follows, with the understand- 
ing that a post-processing step could re- 
cover the antonymous word sense, if nec- 
essary. 
After three passes through the corpora, they 
were perfectly synchronized. We are seeking 
permission to make this data set available to 
any who already have access to both SemCor 
and the Treebank II version of Brown. 
After this synchronization process, we 
merged the word-sense annotations ofour cor- 
rected SemCor with the tokens of our cor- 
rected version of the Treebank II Brown data. 
Here we were forced to make two decisions. 
First, SemCor allows multiple synsets to be as- 
signed to a particular word; in these cases, we 
simply discard all but the first assigned synset. 
Second, WordNet has collocations, whereas 
Treebank does not. To deal with this dis- 
parity, we re-analyze annotated collocations 
as a sequence of separate words that have 
all been assigned the same synset as was as- 
signed the collocation as a whole. This is not 
as unreasonable as it may sound; for exam- 
ple, v ice_pres ident  is a lemma in WordNet 
and appears in SemCor, so the merged corpus 
has instances where the word president has 
the synset vice pres ident  l, but only when 
preceded by the word vice. The cost of this 
decision is an increase in average polysemy. 
6 T ra in ing  and  Decod ing  
Using this merged corpus, actual training of 
our model proceeds in an identical fashion 
to training the non-WordNet-extended model, 
except that for each lexical relation, the hy- 
pernym chain of the parent head is followed 
to derive counts for the various back-off levels 
described in Section 4. We also developed a
"plug-'n'-play" lexical model system to facili- 
tate experimentation with various word- and 
synset-prediction models and back-off strate- 
gies. 
Even though the model is a top-down, gen- 
erative one, parsing proceeds bottom-up. The 
model is searched via a modified version of 
CKY, where candidate parse trees that cover 
the same span of words are ranked against 
each other. In the unextended parsing model, 
the cells corresponding to spans of length one 
are seeded with (w,t , f )  triples, with every 
possible tag t for a given word zv (the word- 
feature f is computed eterministically forw); 
this step introduces the first degree of ambi- 
guity in the decoding process. Our WordNet- 
extended model adds to this initial ambiguity, 
for each cell is seeded with (w, t, f, s) quadru- 
ples, with every possible synset s for a given 
word-tag pair. 
During decoding, two forms of pruning are 
employed: a beam is applied to each cell in the 
chart, pruning away all parses whose ranking 
score is not within a factor of e -k  of the top- 
ranked parse, and only the top-ranked n sub- 
trees are maintained, and the rest are pruned 
away. The "out-of-the-box" BBN program uses 
values of-5 and 25 for k and n, respectively. 
We changed these to default o -9 and 50, be- 
cause generating additional unseen items (in 
our case, synsets) will necessarily ower inter- 
mediate ranking scores. 
7 Exper iments  and  Resu l t s  
7.1 Pars ing 
Initially, we created a small test set, blindly 
choosing the last 117 sentences, or 1%, of 
160 
our 220k word corpus, sentences which were, 
as it happens, from section "r" of the Brown 
Corpus. After some disappointing parsing 
results using both the regular parser and 
our WordNet-extended version, we peeked in 
(Francis and Ku~era, 1979) and discovered 
this was the humor writing section; our ini- 
tial test corpus was literally a joke. To cre- 
ate a more representative t st set, we sam- 
pled every 100th sentence to create a new liT- 
sentence test set that spanned the entire range 
of styles in the 220k words; we put all other 
sentences in the training set. 1? For the sake of 
comparison, we present results for both test 
sets (from section "r" and the balanced test 
set) and both the standard model (Norm) and 
our WN-extended model (WN-ext) in Table 
1.11 We note that after we switched to the bal- 
anced test set, we did not use the "out-of-the- 
box" version of the BBN parser, as its default 
settings for pruning away low-count items and 
the threshold at which to count a word as "un- 
known" were too high to yield decent results. 
Instead, we used precisely the same settings as 
for our WordNet-extended version, complete 
with the larger beam width discussed in the 
previous ection32 
, The reader will note that our extended 
model performs at roughly the same level 
as the unextended version with respect to 
parsing--a shave better with the "r" test set, 
and slightly worse on the balanced test set. 
Recall, however, that this is in spite of adding 
more intermediate ambiguity during the de- 
coding process, and yet using the same beam 
width. Furthermore, our extensions have oc- 
curred strictly within the framework of the 
original model, but we believe that for the 
true advantages of synsets to become appar- 
ent, we must use trilexical or even tetralex- 
~?We realize these are very small test sets, but we 
presume they are large enough to at least give a good 
indicator of performance onthe tasks evaluated. They 
were kept small to allow for a rapid train-test-analyze 
cycle, z.e., they were actually used as development test 
sets. With the completion of these initial experiments, 
we are going to designate a proper three-way divsion 
of training, devtest and test set of this new merged 
corpus. 
UThe scores in the rows labeled Norm, "r", indicat- 
ing the performance of the standard BBN model on 
the "r" test set, are actually scores based on 116 of the 
117 sentences, asone sentence did not get parsed ue 
to a timeout in the program. 
~2This is partly an unfair comparison, then, since 
ours is a larger model, but we wanted to give the stan- 
dard model every conceivable advantage. 
Model, 
test set 
Norm, '~r"* 
WN-ext, "r" 
Norm, bal 
WN-ext, bal 
Norm, "r"* 
WN-ext, "r" 
Norm, bal 
WN-ext, bal 
<40 words t 
LR LP I ~ 0CB <2CB 
69.7 72.6 2.93 31.9 55.0 
69.7 72.7 2.86 30.8 56.0 
83.1 85.0 0.82 75.9 85.7 
82.9 84.0 1.02 70.5 81.3 
All sentences 
LR LP CB 0CB <2CB 
68.6 71.2 3.83 25.9 44.8 
69.7 71.5 3.77 i25.0 45.7 
82.0 84.4 1.00 73 .5  83.8 
80.5 82.2 1.43 !68.4 78.6 
Table 1: Results for both parsing models on 
both test sets. All results are percentages, ex- 
cept for those in the CB column. *See footnote 
11. 
S(will) 
NP(Jane) VP(will) 
Jane will VP(kill) 
kill NP(Bob) 
I 
Bob 
Figure 2: Head rules are tuned for syntax, not 
semantics. 
ical dependencies. Whereas such long-range 
dependencies might cripple a standard gen- 
erative model, the soft-clustering aspects of 
synsets hould offset the sparse data problem. 
As an example of the lack of such dependen- 
cies, in the current model when predicting the 
attachment of \[bought company \[for million\]\], 
there is no current dependence between the 
verb bought and the object of the preposition 
mill ion--a dependence shown to be useful in 
virtually all the PP attachment work, and par- 
ticularly in (Stetina and Nagao, 1997). Re- 
lated to this issue, we note that the head rules, 
which were nearly identical to those used in 
(Collins, 1997), have not been tuned at all to 
this task. For example, in the sentence in Fig- 
ure 2, the subject Jane is predicted condition- 
ing on the head of the VP, which is the modal 
wdl, as opposed to the more semantically- 
content-rich kill. So, while the head relations 
provide a very useful structure for many syn- 
tactic decisions the parser needs to make, it is 
quite possible that the synset relations of this 
model would require additional or different de- 
161 
Noun 
Verb 
Adj 
Adv 
I Recall Precision 
86.5% 70.9% 
84.0% 59.5% 
80.2% 70.4% 
78.5% ~ 75.8% 
I T?tal  I 84"0% I 67"3% I 
Table 2: Word sense disambiguation results for 
balanced test set. 
pendencies that would help in the prediction 
of correct synsets, and in turn help further e- 
duce certain syntactic ambiguities, uch as PP 
attachment. This is because the "lightweight 
semantics" offered by synset relations can pro- 
vide selectional and world-knowledge r stric- 
tions that simple lexicalized nonterminal rela- 
tions cannot. 
7.2 Word-sense d isambiguat ion 
The WSD results on the balanced test set are 
shown in Table 2. A few important points must 
be made when evaluating these results. First, 
almost all other WSD approaches are aimed at 
distinguishing homonyms, as opposed to the 
type of fine-grained istinctions that can be 
made by WordNet. Second, almost all other 
WSD approaches attempt o disambiguate a 
small set of such homonymous terms, whereas 
here we are attacking the generahzed word- 
sense disambiguation problem. Third, we call 
attention to the fact that SemCor has a re- 
ported inter-annotator agreement of 78.6% 
overall, and as low as 70% for words with pol- 
ysemy of 8 or above (Fellbaum et al, 1998), so 
it is with this upper bound in mind that one 
must consider the precision of any generalized 
WSD system. Finally, we note that the scores 
in Table 2 are for exact synset matches; that 
is, if our program delivers a synset hat is, say, 
the hypernym or sibling of the correct answer, 
no credit is given. 
While it is tempting to compare these re- 
sults to those of (Stetina et al, 1998), who re- 
ported 79.4% overall accuracy on a different, 
larger test set using their non-discourse model, 
we note that that was more of an upper- 
bound study, examining how well a WSD al- 
gorithm could perform if it had access to gold- 
standard-perfect parse trees33 By way of fur- 
ther comparison, that algorithm has a feature 
space similar to the synset-prediction compo- 
1nit is not clear how or why the results of (Stetina et 
al., 1998) exceeded the reported inter-annotator agree- 
ment of the entire corpus. 
nents of our model, but the steps used to rank 
possible answers are based largely on heuris- 
tics; in contrast, our model is based entirely 
on maximum-likelihood probability estimates. 
A final note on the scores of Table 2: given 
the fact that there is not a deterministic 
mapping between the 50-odd Treebank and 
4 WordNet parts of speech, when our pro- 
gram delivers a synset for a WordNet part of 
speech that is different from our gold file, we 
have called this a recall error, as this is con- 
sistent with all other WSD work, where part 
of speech ambiguity is not a component of an 
algorithm's precision. 
8 Future  Work  
This paper represents a first attempt at a 
combined parsing/word sense disambiguation 
model. Although it has been very useful to 
work with the BBN model, we are currently 
implementing and hope to augment a more 
state-of-the-art model, vzz., Models 2 and 3 of 
(Collins, 1997). We would also like to explore 
the use of a more radical model, where nonter- 
minals only have synsets as their heads, and 
words are generated strictly at the leaves. We 
would also like to incorporate long-distance 
context in the model as an aid to WSD, a 
demonstrably effective feature in virtually all 
the recent, statistical WSD work. Also, as 
mentioned earlier, we believe there are several 
features that would allow significant parsing 
improvement. Finally, we would like to inves- 
tigate the incorporation ofunsupervised meth- 
ods for WSD, such as the heuristically-based 
methods of (Stetina and Nagao, 1997) and 
(Stetina et al, 1998), and the theoretically 
purer bootstrapping method of (Yarowsky, 
1995). Bolstered by the success of (Stetina 
and Nagao, 1997), (Lin, 1997) and especially 
(Stetina et al, 1998), we believe there is great 
promise the incorporation of word-sense into 
a probabilistic parsing model. 
9 Acknowledgements  
I would like to greatly acknowledge the re- 
searchers at BBN who allowed me to use and 
abuse their parser and who fostered the begin- 
ning of this research effort: Scott Miller, Lance 
Ramshaw, Heidi Fox, Sean Boisen and Ralph 
Weischedeh Thanks to Michelle Engel, who 
helped enormously with the task of prepar- 
ing the new data set. Finally, I would like to 
thank my advisor Mitch Marcus for his invalu- 
able technical advice and support. 
162 
References  
Daniel M. Bikel, Richard Schwartz, Ralph 
Weischedel, and Scott Miller. 1997. Nymble: 
A high-performance learning name-finder. In 
Fzfth Conference on Applied Natural Language 
Processing, pages 194-201,, Washington, D.C. 
E. Brill and P. Resnik. 1994. A rule- 
based approach to prepositional phrase attach- 
ment disambiguation. In Fifteenth Interna- 
twnal Conference on Computatwnal Linguzstics 
(COLING-1994). 
Eric Brill. 1995. Transformation-based error- 
driven learning and natural anguage process- 
ing: A case study in part-of-speech tagging. 
Computational Linguistics, 21(4):543-565. 
Eugene Charniak. 1997. Statistical parsing with 
a context-free grammar and word statistics. In 
Proceedings of the Fourteenth National Con- 
ference on Artificial Intelligence, Menlo Park. 
AAAI Press/MIT Press. 
Kenneth Church. 1988. A stochastic parts pro- 
gram and noun phrase parser for unrestricted 
text. In Second Conference on Applzed Natu- 
ral Language Processing, pages 136-143, Austin, 
Texas. 
M. Collins and J. Brooks. 1995. Prepositional 
phrase attachment through a backed-off model. 
In Thwd Workshop on Very Large Corpora, 
pages 27-38. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
ceedings of ACL-EACL '97, pages 16-23. 
Michael John Collins. 1999. Head-Driven Stat~stz- 
cal Models for Natural Language Parsing. Ph.D. 
thesis, University of Pennsylvania. 
Christiane Fellbaum, Jaochim Grabowski, and 
Shah Landes. 1998. Performance and confi- 
dence in a semantic annotation task. In Chris? 
tiane Fellbaum, editor, WordNet: An Electronic 
Lexzeal Database, chapter 9. MIT Press, Cam- 
bridge, Massachusetts. 
W. N. Francis and H. Ku~era. 1979. Manual of 
Information to accompany A Standard Corpus 
of Present-Day Edited American English, for 
use with Digital Computers. Department ofLin- 
guistics, Brown University, Providence, Rhode 
Island. 
Aravind K. Joshi and Yves Schabes. 1997. Tree- 
adjoining grammars. In A. Salomma and 
G. Rosenberg, editors, Handbook of Formal Lan- 
guages and Automata, volume 3, pages 69-124. 
Springer-Verlag, Heidelberg. 
Dekang Lin. 1997. Using syntactic dependency as
local context o resolve word sense ambiguity. 
In Proceedings of the 35th Annual Meeting o/ 
the Assoczation for Computational Linguistics, 
Madrid, Spain. 
D. Magerman. 1995. Statistical decision tree 
models for parsing. In 33rd Annual Meeting 
of the Association for Computational Linguis- 
tics, pages 276-283, Cambridge, Massachusetts. 
Morgan Kaufmann Publishers. 
Mitchell P. Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building a 
laxge annotated corpus of English: The Penn 
Treebank. Computatzonal Linguistics, 19:313- 
330. 
George A. Miller, Richard T. Beckwith, Chris- 
tiane D. Fellbaum, Derek Gross, and Kather- 
ine J. Miller. 1990. WordNet: An on-line lexi- 
cal database. Internatwnal Journal of Lexicog- 
raphy, 3(4):235-244. 
George A. Miller, Martin Chodorow, Shari Lan- 
des, Claudia Leacock, and Robert G. Thomas. 
1994. Using a semantic oncordance for sense 
identification. In Proceedings o/the ARPA Hu- 
man Language Technology Workshop. 
Scott Miller, Heidi Fox, Lance Ramshaw, and 
Ralph Weischedel. 1998. SIFT - Statistically- 
derived Information From Text. In Seventh 
Message Understanding Conference (MUC-7), 
Washington, D.C. 
Adwait Ratnaparkhi. 1997. A linear observed 
time statistical parser based on maximum en- 
tropy models. In Proceedzngs of the Second 
Conference on Empzmcal Methods zn Natural 
Language Processing, Brown University, Prov- 
idence, Rhode Island. 
Jiri Stetina and Makoto Nagao. 1997. Corpus 
based PP attachment ambiguity resolution with 
a semantic dictionary. In Fifth Workshop on 
Very Large Corpora, pages 66-80, Beijing. 
Jiri Stetina, Sadao Kurohashi, and Makoto Na- 
gao. 1998. General word sense disambiguation 
method based on a full sentential context. In 
COLING-ACL '98 Workshop: Usage of Word- 
Net m Natural Language Processing Systems, 
Montreal, Canada, August. 
R. Weischedel, M. Meteer, R. Schwartz, 
L. Ramshaw, and J. Palmucci. 1993. Coping 
with ambiguity and unknown words through 
probabilistic methods. Computational Linguis- 
tics, 19(2):359-382. 
David Yarowsky. 1992. Word-sense disambigua- 
tion using statistical models of roget's categories 
trained on large corpora. In Fourteenth Interna- 
tional Conference on Computational Linguistics 
(COLING), pages 454-460. 
David Yarowsky. 1994. Decision lists for lexi- 
cal ambiguity resolution: Application to accent 
restoration i Spanish and French. In Proceed- 
ings of the 32nd Annual Meeting o/the Assoca- 
tion for Computational Linguistics, pages 88- 
95. 
David Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of 
the Association for Computational Linguistics, 
pages 189-196. - - 
163 
A Distributional Analysis of a Lexicalized Statistical Parsing Model
Daniel M. Bikel
Department of Computer and Information Science
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
dbikel@cis.upenn.edu
Abstract
This paper presents some of the first data visualiza-
tions and analysis of distributions for a lexicalized
statistical parsing model, in order to better under-
stand their nature. In the course of this analysis,
we have paid particular attention to parameters that
include bilexical dependencies. The prevailing view
has been that such statistics are very informative but
suffer greatly from sparse data problems. By using a
parser to constrain-parse its own output, and by hy-
pothesizing and testing for distributional similarity
with back-off distributions, we have evidence that
finally explains that (a) bilexical statistics are actu-
ally getting used quite often but that (b) the distri-
butions are so similar to those that do not include
head words as to be nearly indistinguishable inso-
far as making parse decisions. Finally, our analysis
has provided for the first time an effective way to
do parameter selection for a generative lexicalized
statistical parsing model.
1 Introduction
Lexicalized statistical parsing models, such as those
built by Black et al (1992a), Magerman (1994),
Collins (1999) and Charniak (2000), have been
enormously successful, but they also have an enor-
mous complexity. Their success has often been
attributed to their sensitivity to individual lexical
items, and it is precisely this incorporation of lexical
items into features or parameter schemata that gives
rise to their complexity. In order to help determine
which features are helpful, the somewhat crude-but-
effective method has been to compare a model?s
overall parsing performance with and without a fea-
ture. Often, it has seemed that features that are
derived from linguistic principles result in higher-
performing models (cf. (Collins, 1999)). While
this may be true, it is clearly inappropriate to high-
light ex post facto the linguistically-motivated fea-
tures and rationalize their inclusion and state how
effective they are. A rigorous analysis of features or
parameters in relation to the entire model is called
for. Accordingly, this work aims to provide a thor-
ough analysis of the nature of the parameters in a
Collins-style parsing model, with particular focus
on the two parameter classes that generate lexical-
ized modifying nonterminals, for these are where
all a sentence?s words are generated except for the
head word of the entire sentence; also, these two
parameter classes have by far the most parameters
and suffer the most from sparse data problems. In
spite of using a Collins-style model as the basis for
analysis, throughout this paper, we will attempt to
present information that is widely applicable be-
cause it pertains to properties of the widely-used
Treebank (Marcus et al, 1993) and lexicalized pars-
ing models in general.
This work also sheds light on the much-discussed
?bilexical dependencies? of statistical parsing mod-
els. Beginning with the seminal work at IBM (Black
et al, 1991; Black et al, 1992b; Black et al, 1992a),
and continuing with such lexicalist approaches as
(Eisner, 1996), these features have been lauded for
their ability to approximate a word?s semantics as
a means to override syntactic preferences with se-
mantic ones (Collins, 1999; Eisner, 2000). How-
ever, the work of Gildea (2001) showed that, with
an approximate reimplementation of Collins? Model
1, removing all parameters that involved dependen-
cies between a modifier word and its head resulted
in a surprisingly small decrease in overall parse ac-
curacy. The prevailing assumption was then that
such bilexical statistics were not useful for mak-
ing syntactic decisions, although it was not entirely
clear why. Subsequently, we replicated Gildea?s
experiment with a complete emulation of Model
2 and presented additional evidence that bilexical
statistics were barely getting used during decod-
ing (Bikel, 2004), appearing to confirm the origi-
nal result. However, the present work will show
that such statistics do get frequently used for the
highest-probability parses, but that when a Collins-
style model generates modifier words, the bilexical
parameters are so similar to their back-off distribu-
tions as to provide almost no extra predictive infor-
mation.
2 Motivation
A parsing model coupled with a decoder (an al-
gorithm to search the space of possible trees for a
given terminal sequence) is largely an engineering
effort. In the end, the performance of the parser
with respect to its evaluation criteria?typically ac-
curacy, and perhaps also speed?are all that matter.
Consequently, the engineer must understand what
the model is doing only to the point that it helps
make the model perform better. Given the some-
what crude method of determining a feature?s ben-
efit by testing a model with and without the fea-
ture, a researcher can argue for the efficacy of that
feature without truly understanding its effect on the
model. For example, while adding a particular fea-
ture may improve parse accuracy, the reason may
have little to do with the nature of the feature and
everything to do with its canceling other features
that were theretofore hurting performance. In any
case, since this is engineering, the rationalization
for a feature is far less important than the model?s
overall performance increase.
On the other hand, science would demand that,
at some point, we analyze the multitude of features
in a state-of-the-art lexicalized statistical parsing
model. Such analysis is warranted for two reasons:
replicability and progress. The first is a basic tenet
of most sciences: without proper understanding of
what has been done, the relevant experiment(s) can-
not be replicated and therefore verified. The sec-
ond has to do with the idea that, when a discipline
matures, it can be difficult to determine what new
features can provide the most gain (or any gain, for
that matter). A thorough analysis of the various dis-
tributions being estimated in a parsing model allows
researchers to discover what is being learned most
and least well. Understanding what is learned most
well can shed light on the types of features or depen-
dencies that are most efficacious, pointing the way
to new features of that type. Understanding what is
learned least well defines the space in which to look
for those new features.
3 Frequencies
3.1 Definitions and notation
In this paper we will refer to any estimated dis-
tribution as a parameter that has been instantiated
from a parameter class. For example, in an n-
gram language model, p(wi |wi?1) is a parameter
class, whereas the estimated distribution p?( ? | the)
is a particular parameter from this class, consisting
of estimates of every word that can follow the word
?the?.
For this work, we used the model described in
(Bikel, 2002; Bikel, 2004). Our emulation of
Collins? Model 2 (hereafter referred to simply as
?the model?) has eleven parameter classes, each of
which employs up to three back-off levels, where
back-off level 0 is just the ?un-backed-off? maximal
context history.1 In other words, a smoothed prob-
ability estimate is the interpolation of up to three
different unsmoothed estimates. The notation and
description for each of these parameter classes is
shown in Table 1.
3.2 Basic frequencies
Before looking at the number of parameters in the
model, it is important to bear in mind the amount
of data on which the model is trained and on which
actual parameters will be induced from parameter
classes. The standard training set for English con-
sists of Sections 02?21 of the Penn Treebank, which
in turn consist of 39,832 sentences with a total of
950,028 word tokens (not including null elements).
There are 44,113 unique words (again, not includ-
ing null elements), 10,437 of which occur 6 times
or more.2 The trees consist of 904,748 brackets
with 28 basic nonterminal labels, to which func-
tion tags such as -TMP and indices are added in
the data to form 1184 observed nonterminals, not
including preterminals. After tree transformations,
the model maps these 1184 nonterminals down to
just 43. There are 42 unique part of speech tags that
serve as preterminals in the trees; the model prunes
away three of these (?, ? and .).
Induced from these training data, the model con-
tains 727,930 parameters; thus, there are nearly as
many parameters as there are brackets or word to-
kens. From a history-based grammar perspective,
there are 727,930 types of history contexts from
which futures are generated. However, 401,447 of
these are singletons. The average count for a history
context is approximately 35.56, while the average
diversity is approximately 1.72. The model contains
1,252,280 unsmoothed maximum-likelihood proba-
bility estimates (727, 930 ?1.72 ? 1, 252, 280). Even
when a given future was not seen with a particu-
lar history, it is possible that one of its associated
1Collins? model splits out the PM and PMw classes into left-
and right-specific versions, and has two additional classes for
dealing with coordinating conjunctions and inter-phrasal punc-
tuation. Our emulation of Collins? model incorporates the in-
formation of these specialized parameter classes into the exist-
ing PM and PMw parameters.
2We mention this statistic because Collins? thesis experi-
ments were performed with an unknown word threshold of 6.
Notation Description No. of back-off levels
PH Generates unlexicalized head child given lexicalized parent 3
PsubcatL Generates subcat bag on left side of head child 3
PsubcatR Generates subcat bag on right side of head child 3
PM (PM,NPB) Generates partially-lexicalized modifying nonterminal (with NPB parent) 3
PMw (PMw,NPB) Generates head word of modifying nonterminal (with NPB parent) 3
PpriorNT Priors for nonterminal conditioning on its head word and part of speech 2
Ppriorlex Priors for head word/part of speech pairs (unconditional probabilities) 0
PTOPNT Generates partially-lexicalized child of +TOP+? 1
PTOPw Generates the head word for children of +TOP+? 2
Table 1: All eleven parameter classes in our emulation of Collins? Model 2. A partially-lexicalized nonter-
minal is a nonterminal label and its head word?s part of speech (such as NP(NN)). ?The hidden nonterminal
+TOP+ is added during training to be the parent of every observed tree.
PP(IN/with)
IN(IN/with) {NP?A} NP?A(NN/ . . . )
Figure 1: A frequent PMw history context, illustrated
as a tree fragment. The . . . represents the future that
is to be generated given this history.
back-off contexts was seen with that future, leading
to a non-zero smoothed estimate. The total num-
ber of possible non-zero smoothed estimates in the
model is 562,596,053. Table 2 contains count and
diversity statistics for the two parameter classes on
which we will focus much of our attention, PM and
PMw . Note how the maximal-context back-off lev-
els (level 0) for both parameter classes have rela-
tively little training: on average, raw estimates are
obtained with history counts of only 10.3 and 4.4 in
the PM and PMw classes, respectively. Conversely,
observe how drastically the average number of tran-
sitions n increases as we remove dependence on the
head word going from back-off level 0 to 1.
3.3 Exploratory data analysis: a common
distribution
To begin to get a handle on these distributions, par-
ticularly the relatively poorly-trained and/or high-
entropy distributions of the PMw class, it is useful to
perform some exploratory data analysis. Figure 1
illustrates the 25th-most-frequent PMw history con-
text as a tree fragment. In the top-down model, the
following elements have been generated:
? a parent nonterminal PP(IN/with) (a PP
headed by the word with with the part-of-
speech tag IN)
? the parent?s head child IN
? a right subcat bag containing NP-A (a single NP
argument must be generated somewhere on the
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  500  1000  1500  2000  2500  3000  3500
cu
m
m
u
la
tiv
e 
de
ns
ity
rank
Figure 2: Cumulative density function for the PMw
history context illustrated in Figure 1.
right side of the head child)
? a partially-lexicalized right-modifying nonter-
minal
At this point in the process, a PMw parameter condi-
tioning on all of this context will be used to estimate
the probability of the head word of the NP-A(NN),
completing the lexicalization of that nonterminal. If
a candidate head word was seen in training in this
configuration, then it will be generated conditioning
on the full context that crucially includes the head
word with; otherwise, the model will back off to a
history context that does not include the head word.
In Figure 2, we plot the cumulative density func-
tion of this history context. We note that of the
3258 words with non-zero probability in this con-
text, 95% of the probability mass is covered by the
1596 most likely words.
In order to get a better visualization of the proba-
bility distribution, we plotted smoothed probability
estimates versus the training-data frequencies of the
words being generated. Figure 3(a) shows smoothed
estimates that make use of the full context (i.e., in-
clude the head word with) wherever possible, and
Figure 3(b) shows smoothed estimates that do not
use the head word. Note how the plot in Figure 3(b)
appears remarkably similar to the ?true? distribu-
Back-off PM PMw
level c? ?d n c? ?d n
0 10.268 1.437 7.145 4.413 1.949 2.264
1 558.047 3.643 153.2 60.19 8.454 7.120
2 1169.6 5.067 230.8 21132.1 370.6 57.02
Table 2: Average counts and diversities of histories of the PM and PMw parameter classes. c and d are
average history count and diversity, respectively. n = c
d
is the average number of transitions from a history
context to some future.
 1e-06
 1e-05
 0.0001
 0.001
 0.01
 0.1
 1  10  100  1000  10000s
m
o
o
th
ed
 p
ro
ba
bi
lit
y 
es
tim
at
e
word frequency
(a) prob. vs. word freq., back-off level 1
 1e-06
 1e-05
 0.0001
 0.001
 0.01
 0.1
 1  10  100  1000  10000  100000s
m
o
o
th
ed
 p
ro
ba
bi
lit
y 
es
tim
at
e
word frequency
(b) prob. vs. word freq., back-off level 2
Figure 3: Probability versus word frequency for head words of NP-A(NN) in the PP construction.
tion of 3(a). 3(b) looks like a slightly ?compressed?
version of 3(b) (in the vertical dimension), but the
shape of the two distributions appears to be roughly
the same. This observation will be confirmed and
quantified by the experiments of ?5.3
4 Entropies
A good measure of the discriminative efficacy of a
parameter is its entropy. Table 3 shows the aver-
age entropy of all distributions for each parameter
class.4 By far the highest average entropy is for the
PMw parameter class.
Having computed the entropy for every distri-
bution in every parameter class, we can actually
plot a ?meta-distribution? of entropies for a pa-
rameter class, as shown in Figure 4. As an ex-
ample of one of the data points of Figure 4, con-
sider the history context explored in the previous
section. While it may be one of the most fre-
quent, it also has the highest entropy at 9.141
3The astute reader will further note that the plots in Figure
3 both look bizarrely truncated with respect to low-frequency
words. This is simply due to the fact that all words below a
fixed frequency are generated as the +UNKNOWN+ word.
4The decoder makes use of two additional parameter classes
that jointly estimate the prior probability of a lexicalized non-
terminal; however, these two parameter classes are not part of
the generative model.
PH 0.2516 PTOPNT 2.517
PsubcatL 0.02342 PTOPw 2.853
PsubcatR 0.2147
PM 1.121
PMw 3.923
Table 3: Average entropies for each parameter class.
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 0  50000  100000  150000  200000  250000
en
tr
op
y
rank
Figure 4: Entropy distribution for the PMw parame-
ters.
bits, as shown by Table 4. This value not only
confirms but quantifies the long-held intuition that
PP-attachment requires more than just the local
phrasal context; it is, e.g., precisely why the PP-
specific features of (Collins, 2000) were likely to
be very helpful, as cases such as these are among
the most difficult that the model must discrimi-
nate. In fact, of the top 50 of the highest-entropy
Back-off PM PMw
level min max avg median min max avg median
0 3.080E-10 4.351 1.128 0.931 4.655E-8 9.141 3.904 3.806
1 4.905E-7 4.254 0.910 0.667 2.531E-6 9.120 4.179 4.224
2 8.410E-4 3.501 0.754 0.520 0.002 8.517 3.182 2.451
Overall 3.080E-10 4.351 1.121 0.917 4.655E-8 9.141 3.922 3.849
Table 4: Entropy distribution statistics for PM and PMw .
Figure 5: Total modifier word?generation entropy
broken down by parent-head-modifier triple.
distributions from PMw , 25 involve the config-
uration PP --> IN(IN/<prep>) NP-A(NN/. . .),
where <prep> is some preposition whose tag is IN.
Somewhat disturbingly, these are also some of the
most frequent constructions.
To gauge roughly the importance of these
high-frequency, high-entropy distributions, we per-
formed the following analysis. Assume for the mo-
ment that every word-generation decision is roughly
independent from all others (this is clearly not true,
given head-propagation). We can then compute the
total entropy of word-generation decisions for the
entire training corpus via
HPMw =
?
c?PMw
f (c) ? H(c) (1)
where f (c) is the frequency of some history con-
text c and H(c) is that context?s entropy. The to-
tal modifier word-generation entropy for the cor-
pus with the independence assumption is 3,903,224
bits. Of these, the total entropy for contexts of the
form PP ? IN NP-A is 618,640 bits, representing
a sizable 15.9% of the total entropy, and the sin-
gle largest percentage of total entropy of any parent-
head-modifier triple (see Figure 5).
On the opposite end of the entropy spectrum,
there are tens of thousands of PMw parameters
with extremely low entropies, mostly having to do
with extremely low-diversity, low-entropy part-of-
speech tags, such as DT, CC, IN or WRB. Perhaps even
more interesting is the number of distributions with
identical entropies: of the 206,234 distributions,
there are only 92,065 unique entropy values. Dis-
tributions with the same entropy are all candidates
for removal from the model, because most of their
probability mass resides in the back-off distribution.
Many of these distributions are low- or one-count
history contexts, justifying the common practice of
removing transitions whose history count is below a
certain threshold. This practice could be made more
rigorous by relying on distributional similarity. Fi-
nally, we note that the most numerous low-entropy
distributions (that are not trivial) involve generating
right-modifier words of the head child of an SBAR
parent. The model is able to learn these construc-
tions extremely well, as one might expect.
5 Distributional similarity and bilexical
statistics
We now return to the issue of bilexical statis-
tics. As alluded to earlier, Gildea (2001) per-
formed an experiment with his partial reimplemen-
tation of Collins? Model 1 in which he removed the
maximal-context back-off level from PMw , which
effectively removed all bilexical statistics from his
model. Gildea observed that this change resulted
in only a 0.5% drop in parsing performance. There
were two logical possibilities for this behavior: ei-
ther such statistics were not getting used due to
sparse data problems, or they were not informa-
tive for some reason. The prevailing view of the
NLP community had been that bilexical statistics
were sparse, and Gildea (2001) adopted this view
to explain his results. Subsequently, we duplicated
Gildea?s experiment with a complete emulation of
Collins? Model 2, and found that when the decoder
requested a smoothed estimate involving a bigram
when testing on held-out data, it only received an
estimate that made use of bilexical statistics a mere
1.49% of the time (Bikel, 2004). The conclusion
was that the minuscule drop in performance from re-
moving bigrams must have been due to the fact that
they were barely able to be used. In other words, it
appeared that bigram coverage was not nearly good
enough for bigrams to have an impact on parsing
performance, seemingly confirming the prevailing
view.
But the 1.49% figure does not tell the whole story.
The parser pursues many incorrect and ultimately
low-scoring theories in its search (in this case, us-
ing probabilistic CKY). So rather than asking how
many times the decoder makes use of bigram statis-
tics on average, a better question is to ask how
many times the decoder can use bigram statistics
while pursuing the top-ranked theory. To answer
this question, we used our parser to constrain-parse
its own output. That is, having trained it on Sec-
tions 02?21, we used it to parse Section 00 of the
Penn Treebank (the canonical development test set)
and then re-parse that section using its own highest-
scoring trees (without lexicalization) as constraints,
so that it only pursued theories consistent with those
trees. As it happens, the number of times the de-
coder was able to use bigram statistics shot up to
28.8% overall, with a rate of 22.4% for NPB con-
stituents.
So, bigram statistics are getting used; in fact, they
are getting used more than 19 times as often when
pursuing the highest-scoring theory as when pursu-
ing any theory on average. And yet there is no dis-
puting the fact that their use has a surprisingly small
effect on parsing performance. The exploratory data
analysis of ?3.3 suggests an explanation for this per-
plexing behavior: the distributions that include the
head word versus those that do not are so similar
as to make almost no difference in terms of parse
accuracy.
5.1 Distributional similarity
A useful metric for measuring distributional simi-
larity, as explored by (Lee, 1999), is the Jensen-
Shannon divergence (Lin, 1991):
JS (p ? q ) = 1
2
[
D
(
p
?
?
? avgp,q
)
+ D
(
q
?
?
? avgp,q
)]
(2)
where D is the Kullback-Leibler divergence
(Cover and Thomas, 1991) and where avgp,q =
1
2 (p(A) + q(A)) for an event A in the event space
of at least one of the two distributions. One inter-
pretation for the Jensen-Shannon divergence due to
Slonim et al (2002) is that it is related to the log-
likelihood that ?the two sample distributions orig-
inate by the most likely common source,? relating
the quantity to the ?two-sample problem?.
In our case, we have p = p(y | x1, x2) and q =
p(y | x1), where y is a possible future and x1, x2 are
elements of a history context, with q representing
a back-off distribution using less context. There-
fore, whereas the standard JS formulation is agnos-
min max avg. median
JS 0?1 2.729E-7 2.168 0.1148 0.09672
JS 1?2 0.001318 1.962 0.6929 0.6986
JS 0?2 0.001182 1.180 0.3774 0.3863
Table 5: Jensen-Shannon statistics for back-off pa-
rameters in PMw .
tic with respect to its two distributions, and averages
them in part to ensure that the quantity is defined
over the entire space, we have the prior knowledge
that one history context is a superset of the other,
that ?x1? is defined wherever ?x1, x2? is. In this case,
then, we have a simpler, ?one-sided? definition for
the Jensen-Shannon divergence, but generalized to
the multiple distributions that include an extra his-
tory component:
JS (p ? q ) =
?
x2
p(x2) ? D (p(y | x1, x2) ? p(y | x1) )
= Ex2 D (p(y | x1, x2) ? p(y | x1) ) (3)
An interpretation in our case is that this is the ex-
pected number of bits x2 gives you when trying to
predict y.5 If we allow x2 to represent an arbitrary
amount of context, then the Jensen-Shannon diver-
gence JS b?a = JS (pb || pa) can be computed for
any two back-off levels, where a, b are back-off lev-
els s.t. b < a (meaning pb is a distribution using
more context than pa). The actual value in bits of
the Jensen-Shannon divergence between two distri-
butions should be considered in relation to the num-
ber of bits of entropy of the more detailed distribu-
tion; that is, JS b?a should be considered relative to
H(pb). Having explored entropy in ?4, we will now
look at some summary statistics for JS divergence.
5.2 Results
We computed the quantity in Equation 3 for every
parameter in PMw that used maximal context (con-
tained a head word) and its associated parameter
that did not contain the head word. The results are
listed in Table 5. Note that, for this parameter class
with a median entropy of 3.8 bits, we have a median
JS divergence of only 0.097 bits. The distributions
are so similar that the 28.8% of the time that the de-
coder uses an estimate based on a bigram, it might
as well be using one that does not include the head
word.
5Or, following from Slonim et al?s interpretation, this quan-
tity is the (negative of the) log-likelihood that all distributions
that include an x2 component come from a ?common source?
that does not include this component.
? 40 words
?00 ?23
Model LR LP LR LP
m3 n/a n/a 88.6 88.7
m2-emu 89.9 90.0 88.8 88.9
reduced 90.0 90.2 88.7 88.9
all sentences
Model ?00 ?23
m3 n/a n/a 88.0 88.3
m2-emu 88.8 89.0 88.2 88.3
reduced 89.0 89.0 88.0 88.2
Table 6: Parsing results on Sections 00 and 23 with
Collins? Model 3, our emulation of Collins? Model
2 and the reduced version at a threshold of 0.06. LR
= labeled recall, LP = labeled precision.6
6 Distributional Similarity and Parameter
Selection
The analysis of the previous two sections provides
a window onto what types of parameters the pars-
ing model is learning most and least well, and onto
what parameters carry more and less useful infor-
mation. Having such a window holds the promise
of discovering new parameter types or features that
would lead to greater parsing accuracy; such is the
scientific, or at least, the forward-minded research
perspective.
From a much more purely engineering perspec-
tive, one can also use the analysis of the previous
two sections to identify individual parameters that
carry little to no useful information and simply re-
move them from the model. Specifically, if pb is
a particular distribution and pb+1 is its correspond-
ing back-off distribution, then one can remove all
parameters pb such that
JS (pb||pb+1)
H(pb) < t,
where 0 < t < 1 is some threshold. Table 6 shows
the results of this experiment using a threshold of
0.06. To our knowledge, this is the first example
of detailed parameter selection in the context of a
generative lexicalized statistical parsing model. The
consequence is a significantly smaller model that
performs with no loss of accuracy compared to the
full model.6
Further insight is gained by looking at the per-
centage of parameters removed from each parame-
ter class. The results of (Bikel, 2004) suggested that
the power of Collins-style parsing models did not
6None of the differences between the Model 2?emulation
results and the reduced model results is statistically significant.
PH 13.5% PTOPw 0.023%
PsubcatL 0.67% PM 10.1%
PsubcatR 1.8% PMw 29.4%
Table 7: Percentage of parameters removed from
each parameter class for the 0.06-reduced model.
lie primarily with the use of bilexical dependencies
as was once thought, but in lexico-structural depen-
dencies, that is, predicting syntactic structures con-
ditioning on head words. The percentages of Table
7 provide even more concrete evidence of this as-
sertion, for whereas nearly a third of the PMw pa-
rameters were removed, a much smaller fraction of
parameters were removed from the PsubcatL , PsubcatR
and PM classes that generate structure conditioning
on head words.
7 Discussion
Examining the lower-entropy PMw distributions re-
vealed that, in many cases, the model was not so
much learning how to disambiguate a given syn-
tactic/lexical choice, but simply not having much
to learn. For example, once a partially-lexicalized
nonterminal has been generated whose tag is fairly
specialized, such as IN, then the model has ?painted
itself into a lexical corner?, as it were (the extreme
example is TO, a tag that can only be assigned to the
word to). This is an example of the ?label bias?
problem, which has been the subject of recent dis-
cussion (Lafferty et al, 2001; Klein and Manning,
2002). Of course, just because there is ?label bias?
does not necessarily mean there is a problem. If
the decoder pursues a theory to a nonterminal/part-
of-speech tag preterminal that has an extremely low
entropy distribution for possible head words, then
there is certainly a chance that it will get ?stuck? in a
potentially bad theory. This is of particular concern
when a head word?which the top-down model gen-
erates at its highest point in the tree?influences an
attachment decision. However, inspecting the low-
entropy word-generation histories of PMw revealed
that almost all such cases are when the model is
generating a preterminal, and are thus of little to no
consequence vis-a-vis syntactic disambiguation.
8 Conclusion and Future Work
With so many parameters, a lexicalized statistical
parsing model seems like an intractable behemoth.
However, as statisticians have long known, an ex-
cellent angle of attack for a mass of unruly data
is exploratory data analysis. This paper presents
some of the first data visualizations of parameters
in a parsing model, and follows up with a numerical
analysis of properties of those distributions. In the
course of this analysis, we have focused in on the
question of bilexical dependencies. By constrain-
parsing the parser?s own output, and by hypothe-
sizing and testing for distributional similarity, we
have presented evidence that finally explains that
(a) bilexical statistics are actually getting used with
great frequency in the parse theories that will ulti-
mately have the highest score, but (b) the distribu-
tions involving bilexical statistics are so similar to
their back-off counterparts as to make them nearly
indistinguishable insofar as making different parse
decisions. Finally, our analysis has provided for the
first time an effective way to do parameter selec-
tion with a generative lexicalized statistical parsing
model.
Of course, there is still much more analysis, hy-
pothesizing, testing and extrapolation to be done. A
thorough study of the highest-entropy distributions
should reveal new ways in which to use grammar
transforms or develop features to reduce the entropy
and increase parse accuracy. A closer look at the
low-entropy distributions may reveal additional re-
ductions in the size of the model, and, perhaps, a
way to incorporate hard constraints without disturb-
ing the more ambiguous parts of the model more
suited to machine learning than human engineering.
9 Acknowledgements
Thanks to Mitch Marcus, David Chiang and Ju-
lia Hockenmaier for their helpful comments on this
work. I would also like to thank Bob Moore for
asking some insightful questions that helped prompt
this line of research. Thanks also to Fernando
Pereira, with whom I had invaluable discussions
about distributional similarity. This work was sup-
ported in part by DARPA grant N66001-00-1-9815.
References
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of HLT2002, San Diego, CA.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics. To appear.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavens, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Speech and Natural Language
Workshop, pages 306?311, Pacific Grove, California.
Morgan Kaufmann Publishers.
Ezra Black, Frederick Jelinek, John Lafferty, David
Magerman, Robert Mercer, and Salim Roukos.
1992a. Towards history-based grammars: Using
richer models for probabilistic parsing. In Proceed-
ings of the 5th DARPA Speech and Natural Language
Workshop, Harriman, New York.
Ezra Black, John Lafferty, and Salim Roukos. 1992b.
Development and evaluation of a broad-coverage
probabilistic grammar of english-language computer
manuals. In Proceedings of the 30th ACL, pages 185?
192.
Eugene Charniak. 2000. A maximum entropy?inspired
parser. In Proceedings of the 1st NAACL, pages 132?
139, Seattle, Washington, April 29 to May 4.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In International Conference
on Machine Learning.
Thomas Cover and Joy A. Thomas. 1991. Elements of
Information Theory. John Wiley & Sons, Inc., New
York.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?345,
Copenhagen, August.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and An-
ton Nijholt, editors, Advances in Probabilistic and
Other Parsing Technologies, pages 29?62. Kluwer
Academic Publishers, October.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
Pittsburgh, Pennsylvania.
Dan Klein and Christopher D. Manning. 2002. Condi-
tional structure versus conditional estimation in NLP
models. In Proceedings of the 2002 Conference on
Empirical Methods for Natural Language Processing.
John Lafferty, Fernando Pereira, and Andrew McCal-
lum. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
ICML.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th ACL, pages 25?32.
Jianhua Lin. 1991. Divergence measures based on the
Shannon entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
David Magerman. 1994. Natural Language Parsing as
Statistical Pattern Recognition. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, Pennsylvania.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313?330.
Noam Slonim, Nir Friedman, and Naftali Tishby.
2002. Unsupervised document classification using
sequential information maximization. Technical Re-
port 2002?19, Leibniz Center, The School of Com-
puter Science and Engineering, Hebrew University,
Jerusalem, Israel.
Automatic WordNet mapping using word sense disambiguation* 
Changki Lee 
Geunbae Leer 
Natural Language Processing Lab 
Dept. of Computer Science and Engineering 
Pohang University of Science & Technology 
San 31, Hyoja-Dong, Pohang, 790-784, Korea 
{leeck,gblee }@postech.ac.kr 
Seo JungYun 
Natural Language Processing Lab 
Dept. of Computer Science 
Sogang University 
Sinsu-dong 1, Mapo-gu, Seoul, Korea 
seojy@ ccs.sogang.ac.kr 
Abstract 
This paper presents the automatic 
construction of a Korean WordNet from 
pre-existing lexical resources. A set of 
automatic WSD techniques i described for 
linking Korean words collected from a 
bilingual MRD to English WordNet synsets. 
We will show how individual linking 
provided by each WSD method is then 
combined to produce aKorean WordNet for 
nouns. 
1 Introduction 
There is no doubt on the increasing 
importance of using wide coverage ontologies 
for NLP tasks especially for information 
retrieval and cross-language information 
retrieval. While these ontologies exist in English, 
there are very few available wide range 
ontologies for other languages. Manual 
construction of the ontology by experts is the 
most reliable technique but is costly and highly 
time-consuming. This is the reason for many 
researchers having focused on massive 
acquisition of lexical knowledge and semantic 
information from pre-existing lexical resources 
as automatically aspossible. 
This paper presents a novel approach for 
automatic WordNet mapping using word sense 
disambiguafion. The method has been applied to 
link Korean words from a bilingual dictionary to 
English WordNet synsets. 
To clarify the description, an example is given. 
To link the first sense of Korean word 
"gwan-mog" to WordNet synset, we employ a 
bilingual Korean-English dictionary. The first 
sense of 'gwan-mog' has 'bush' as a translation 
in English and 'bush' has five synsets in 
WordNet. Therefore the first sense of 
'gwan-mog" has five candidate synsets. 
Somehow we decide a synset {shrub, bush} 
among five candidate synsets and link the sense 
of 'gwan-mog' to this synset. 
As seen from this example, when we link the 
senses of Korean words to WordNet synsets, 
there are semantic ambiguities. To remove the 
ambiguities we develop new word sense 
disambiguation heuristics and automatic mapping 
method to construct Korean WordNet based on 
the existing English WordNet. 
This paper is organized as follows. In section 2, 
we describe multiple heuristics for word sense 
disambiguation for sense linking. In section 3, we 
explain the method of combination for these 
heuristics. Section 4 presents ome experiment 
results, and section 5 will discuss some related 
researches. Finally we draw some conclusions 
and future researches in section 6. The automatic 
mapping-based Korean WordNet plays a natural 
Korean-English bilingual thesaurus, o it will be 
directly applied to Korean-English cross-lingual 
information retrieval as well as Korean 
monolingual information retrieval. 
2 Multiple heuristics for word sense 
disambiguation 
As the mapping method escribed in this paper 
has been developed for combining multiple 
individual solutions, each single heuristic must be 
seen as a container for some part of the linguistic 
knowledge needed to disarnbiguate the 
* This research was supported by KOSEF special purpose basic research (1997.9- 2000.8 #970-1020-301-3) 
Corresponding author 
142 
ambiguous WordNet synsets. Therefore, not a 
single heuristic is suitable to all Korean words 
collected from a bilingual dictionary. We will 
describe each individual WSD (word sense 
disambiguation) heuristic for Korean word 
mapping into corresponding English senses. 
Korean word English word WordNet synset 
wsl 
= WS 2 
. . .  
W$ n 
Figure 1: Word-to-Concept Association 
Figure 1 shows the Korean word to WordNet 
synset association. The j-th sense of Korean 
word kw has m translations in English and n 
WordNet synsets as candidate senses. Each 
heuristic is applied to the candidate senses (ws,, 
. . . .  ws) and provides cores for them. 
2.1 Heuristic 1: Maximum Similarity 
This heuristic comes from our previous 
Korean WSD research (Lee and Lee, 2000) and 
assumes that all the translations in English for 
the same Korean word sense are semantically 
similar. So this heuristic provides the maximum 
score to the sense that is most similar to the 
senses of the other translations. This heuristic is 
applied when the number of translations for the 
same Korean word sense is more than 1. The 
following formula explains the idea. 
Hi(s,) = max support( s,, ew~) - 1 
~'~, (n-1)+a k,=l 
where EWi = (ewl s, ~ synset(ew)} 
In this formula, Hi(s) is a heuristic score of 
synset s, s is a candidate synset, ew is a 
translation into English, n is the number of 
translations and synset(ew) is the set of synsets 
of the translation ew. So Ew becomes the set of 
translations which have the synset s r. The 
parameter txcontrols the relative contribution of 
candidate synsets in different number of 
translations: as the value of a increases, the 
candidate synsets in smaller number of 
translations get relatively less weight (a=0.5 was 
tuned experimentally), support(s,ew) calculates 
the maximum similarity with the synset s and the 
translation ew, which is defined as : 
support(si, ew) = max S(si, s) 
sEsynset( ew ) 
S2) = ~ S im( st, s2) if sire(s,, s2) _> 0 S(sl, 
l 0 otherwise 
Similarity measures lower than a threshold 0 
are considered to be noise and are ignored. In our 
experiments, 0=0.3 was used. sim(s,s2) computes 
the conceptual similarity between concepts s~ and 
sz as in the following formula : 
sim(sl, s2)= 2 x level(MSCA(sl, s:)) 
level(sO + level(s2) 
where MSCA(sl,s2) represents he most specific 
common ancestor of concepts s~ and s2 and 
level(s) refers to the depth of concept s from the 
root node in the WordNetL 
2.2 Heuristic 2: Prior Probability 
This heuristic provides prior probability to 
each sense of a single translation as score. 
Therefore we will give maximum score to the 
synset of a monosemous translation, that is, the 
translation which has only one corresponding 
synset. The following formula explains the idea. 
H2(s,) = max P(s, l ew) 
?,n, E EW~ 
where EWi = {ew I si ~ synset(ew) } 
1 P ( si I ewj ) -~ - -  
n j  
where si ~ synset(ewj), nj = Isyr et( w,)l 
In this formula, n is the number of synsets of 
the translation e~t~. 
2.3 Heuristic 3: Sense Ordering 
(Gale et al, 1992) reports that word sense 
disambiguation would be at least 75% correct if a 
system assigns the most frequently occurring 
sense. (Miller et al, 1994) found that automatic 
I We use English WordNet version 1.6 
- L 
143 
assignment of polysemous words in Brown 
Corpus to senses in WordNet was 58% correct 
with a heuristic of most frequently occurring 
sense. We adopt these previous results to 
develop sense ordering heuristic. 
The sense ordering heuristic provides the 
maximum score to the most frequently used 
sense of a Ixanslation. The following formula 
explains the heuristic. 
H3(sO = max SO(s,,ew) 
ewEEW, 
where EW, = {ew I si ~ synset(ew) } 
ot 
SO(s,,ew) x~ 
where si ~ synset(ew) 
^ synset(ew) is sorted by frequency 
^ s, is the x-  th synset in synset(ew) 
In this formula, x refers to the sense order of s 
in synset(ew): x is 1 when s, is the most 
frequently used sense of ew. The information 
about the sense order of synsets of an English 
word was extracted from the WordNet. 
0.8 
prob 
0.7 
0.$ 
o.5 
0.4 
0.3 
0.2 
i 
o .1  I 
O 
O. 
#"-  " " t l .  A A = 
! 2 \] 4 5 $ 7 8 9 
ordm" 
Figure 2: Sense distribution in SemCor 
The value a=0.705 and fl=2.2 was acquired 
from a regression of Figure 2 semcor corpus 2
data distribution. 
2.4 Heuristic 4: IS-A relation 
This heuristic is based on the following facts: 
2 semcor is a sense tagged corpus from part of 
Brown corpus. 
I f  two Korean words have an IS-A relation, 
their translations in English should also 
have an IS-A relation. 
Korean word English word WordNet 
Figure 3: IS-A relation 
Figure 3 explains IS-A relation heuristic. In 
figure 3, hkw is a hypemym of a Korean word kw 
and hew is a translation of hkw and ew is a 
translation of kw. 
This heuristic assigns score 1 to the synsets 
which satisfy the above assumption according to 
the following formula: 
H,(s,) = max 1R(si, ew) 
ew~EW,  
where EWi = {ewl si ~ synset(ew) } 
IR (s , ,ew)={;  otherwisefflsa(s"si) 
where si ~ synset(ew) , sj ~ synset(hew) 
In this formula, lsA(s,,s 2) returns true if s, is a 
kind of s 2. 
2.5 Heuristic 5: Word Match 
This heuristic assumes that related concepts 
will be expressed using the same content words. 
Given two definitions - that of the bilingual 
dictionary and that of the WordNet - this 
heuristic computes the total amount of shared 
content words. 
Hs(si) = max WM (si, ew) 
where EW~ = (ewl s~ ~ synset(ew) } 
WM (si, ew) = sim(X,Yi) 
s im(X,Y)  = IX n YI 
Ix rl 
In this formula, X is the set of content words in 
English examples of bilingual dictionary and Y is 
144 
the set of content words of definition and 
example of the synset s, in WordNet. 
2.6 Heuristic 6: Cooccurrence 
This heuristic uses cooccurrence measure 
acquired from the sense tagged Korean 
definition sentences of bilingual dictionary. To 
build sense tagged corpus, we use the definition 
sentences which have monosemous translation 
in bilingual dictionary. And we uses the 25 
semantic tags of WordNet as sense tag : 
n6(s,) = max p(t, I x) 
xeDef 
with p =,  - Z(l_a)/2 ? ~ ' (1~ ~) 
p(ti l x) = Freq(ti, x) 
Freq(x) 
In this formula, Defis the set of content words 
of a Korean definition sentence, t is a semantic 
tag corresponding to the synset s and n refers to 
Freq(x). 
3 Combining heuristics with decision tree 
learning 
Given a candidate synset of a translation and 
6 individual heuristic scores, our goal is to use 
all these 6 scores in combination to classify the 
synset as linking or discarding. 
The combination ofheuristics i  performed by 
decision tree learning for non-linear relationship. 
Each internal node of a decision tree is a choice 
point, dividing an individual method into ranges 
of possible values. Each leaf node is labeled 
with a classification (linking or disc~ding). The 
most popular method of decision tree induction, 
employed here, is C4.5 (Quinlan, 1993). 
Figure 4 shows a training phase in decision 
,tree based combination method. In the training 
phase, the candidate synset ws k of a Korean 
word is manually classified as linking or 
discarding and get assigned scores by each 
heuristic. A training data set is constructed by 
these scores and manual classification. The 
training data set is used to optimize a model for 
combining heuristics. 
/ /~  Heur,st,c 3 ~ ~K 
~ Heuristic 4 ~ ('~'~"~ 
\ 1 x -H Ig2  l I Oeoi  oo 
\[ Classfflcatzon I ~ -~" \[ tree learning 
Figure 4: Training phase 
Heuristic 2 
Heuristic 3 n L=nklng or 
HeunsUc 4 Discarding 
Heunstlc 5 
Heunst=c 6
Figure 5: Mapping phase 
Figure 5 shows a mapping phase. In the 
mapping phase, the new candidate synset ws~ of a 
Korean word is rated using 6 heuristics, and then 
the decision tree, which is learned in the training 
phase, classifies w& as linking or discarding. The 
synset classified as linking is linked to the 
corresponding Korean word. 
4 Evaluation 
In this section, we evaluate the performance of
each six heuristics as well as the combination 
method. To evaluate the performance ofWordNet 
mapping, the candidate synsets of 3260 senses of 
Korean words in bilingual dictionary was 
manually classified as linking or discarding. 
We define 'precision' as the proportion of 
correctly linked senses of Korean words to all the 
linked senses of Korean words in a test set. We 
also define 'coverage' as the proportion of linked 
senses of Korean words to all the senses of 
Korean words in a test set. 
Table 1 contains the results for each heuristic 
evaluated individually against the manually 
classified ata. The test set here consists of the 
3260 manually classified senses. 
In general, the results of each heuristic seem to 
be poor, but are always better than the random 
choice baseline. The best heuristic according to 
145 
the precision is the maximum similarity heuristic. 
But it was applied to only 59.51% of 3260 
senses of Korean words. The results of each 
heuristic are better than the random mapping, 
with a statistically significance at the 99% level. 
Random 
mapping 
Heuristic 1 
Precision(%) 
49.85 
75.21 
Coverage(%) 
100.0 
59.51 
Heuristic 2 74.66 100.0 
Heuristic 3 71.87 100.0 
Heuristic 4 55.49 29.36 
Heuristic 5 : 56.48 63.01 
Heuristic 6 67.24 64.14 
Table 1: Individual heuristics performance 
Summing 
Logistic regression 
Decisioin tree 
Preeisi0n(%) 
84.61 
86.41 
93.59 
Coverage(%) 
100.0 
100.0 
77.12 
Table 2: Performance and comparison of the 
decision tree based combination 
We performed 10-fold cross validation to 
evaluate the performance of the combination of 
all the heuristics using the decision tree - we 
split the data into ten parts, reserved one part as 
a validation set, trained the decision tree on the 
other nine parts and then evaluate the reserved 
part. This process is repeated nine times using 
each of the other nine parts as a validation set. 
Table 2 shows the results of the other trials of 
the combination of all the heuristics. Summing 
is a way to simply sum all the scores of each 
heuristic. Then the candidate synset which has 
the highest summation of the scores is selected. 
Logistic regression, as described in (Hosmer and 
Lemeshow, 1989), is a popular technique for 
binary classification. This technique applies an 
inverse logit function and employs the iterative 
reweighted least squares algorithm. This 
technique determines the weight of each 
heuristic. 
With the combination of the heuristics using 
summing, we obtained an improvement over 
maximum similarity heuristic (heuristic 1) of 9%, 
maintaining a coverage 100%. The decision tree 
is able to correctly map 93.59% of the senses of 
Korean words in bilingual dictionary, 
maintaining a coverage 77.12%. 
Applying the decision tree to combine all the 
heuristics for all Korean words in bilingual 
dictionary, we obtain a preliminary version of the 
Korean WordNet containing 21654 senses of 
17696 Korean nouns with an accuracy of 93.59% 
(-2-0.84% with 99% confidence). 
5 Related works 
Several attempts have been performed to 
automatically produce multilingual ontologies. 
(Knight & Luk 1994) focuses on the construction 
of Sensus, a large knowledge base for supporting 
the Pangloss Machine Translation system, 
merging ontologies (ONTOS and UpperModel) 
and WordNet with monolingual and bilingual 
dictionaries. (Okumura & Hovy 1994) describes a 
semi-automatic method for associating a Japanese 
lexicon to an ontology using a Japanese/English 
bilingual dictionary as a 'bridge'. Several exical 
resources and techniques are combined in 
(Atserias et al, 1997) to map Spanish words from 
a bilingual dictionary to WordNet. In (Farreres et 
al., 1998), use of a taxonomic structure derived 
from a monolingual MRD is proposed as an aid 
to the mapping process. 
This research is contrasted that it utilized 
bilingual dictionary to build monolingual 
thesaurus based on the existing popular lexical 
resources and used the combination of multiple 
unsupervided WSD heuristics. 
6 Conclusion 
This paper has explored the automatic 
construction of a Korean WordNet from 
pre-existing lexical resources - English wordNet 
and Korean/English bilingual dictionary. We 
presented several techniques for word sense 
disambiguation and their application to 
disambiguate the translations in bilingual 
dictionary. We obtained a preliminary version of 
the Korean WordNet containing 21654 senses of 
17696 Korean nouns. In a series of experiments, 
we observed that the accuracy of mapping is over 
90%. 
References 
Atserias J., Climent S., Farreras J., Rigau G. and 
Rodriguez H. (1997) Combining Multiple 
Methods for the Automatic Construction of 
146 
Muldlingual WordNets. In proceeding of the 
Conference on Recent Advances on NLP. 
Farreres X., Rigau G., and Rodnguez H.. (1998) 
Using WordNet for building WordNets. In 
Proceedings of COLING-ACL Workshop on Usage 
of WordNet in Natural Language Processing 
Systems. 
Gale W., Church K., and Yarowsky D. (1992) 
Estimating upper and lower bounds on the 
performance of word-sense disarnbiguation 
programs. In Proceeding of 3Oth Annual Meeting 
of the Association for Computational Linguistics. 
Hosmer Jr. and Lemeshow S. (1989) Applied 
Logistic Regression. Wiley, New York. 
Knight K. and Luk S. (1994) Building a large-scale 
knowledge base for machine translation. In 
Proceeding of the American Association for 
Artificial Intelligence. 
Miller G. (1990) Five papers on WordNet. 
Special Issue of International Journal of 
Lexicography. 
Miller G., Chodorow M., Landes S., Leacock C. and 
Thomas R.. (1994) Using a semantic 
concordance for sense identification. In 
Proceedings of the Human Language Technology 
Workshop. 
Okumura A. and Hovy E. (1994) Building 
Japanese-English Dictionary based on Ontology for 
Machine Translation. In Proceedings of ARPA 
Workshop on Human Language Technology. 
Quinlan R.. (1993) C4.5: Programs For Machine 
Learning. Morgan Kaufmann Publishers. 
Seungwoo Lee and Geunbae Lee. (2000) 
Unsupervised Noun Sense Disambiguafion Using 
Local Context and Co-occurrence. In Journal of 
Korean Information Science Society. (in press) 
147 
