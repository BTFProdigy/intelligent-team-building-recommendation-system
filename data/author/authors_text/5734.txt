Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 531?539,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Linefeed Insertion into Japanese Spoken Monologue for Captioning
Tomohiro Ohno
Graduate School of
International Development,
Nagoya University, Japan
ohno@nagoya-u.jp
Masaki Murata
Graduate School of
Information Science,
Nagoya University, Japan
murata@el.itc.nagoya-u.ac.jp
Shigeki Matsubara
Information Technology Center,
Nagoya University, Japan
matubara@nagoya-u.jp
Abstract
To support the real-time understanding of
spoken monologue such as lectures and
commentaries, the development of a cap-
tioning system is required. In monologues,
since a sentence tends to be long, each
sentence is often displayed in multi lines
on one screen, it is necessary to insert
linefeeds into a text so that the text be-
comes easy to read. This paper proposes
a technique for inserting linefeeds into a
Japanese spoken monologue text as an el-
emental technique to generate the read-
able captions. Our method appropriately
inserts linefeeds into a sentence by ma-
chine learning, based on the information
such as dependencies, clause boundaries,
pauses and line length. An experiment us-
ing Japanese speech data has shown the ef-
fectiveness of our technique.
1 Introduction
Real-time captioning is a technique for support-
ing the speech understanding of deaf persons, el-
derly persons, or foreigners by displaying tran-
scribed texts of monologue speech such as lec-
tures. In recent years, there exist a lot of re-
searches about automatic captioning, and the tech-
niques of automatic speech recognition (ASR)
aimed for captioning have been developed (Bou-
lianne et al, 2006; Holter et al, 2000; Imai et
al., 2006; Munteanu et al, 2007; Saraclar et al,
2002; Xue et al, 2006). However, in order to gen-
erate captions which is easy to read, it is important
not only to recognize speech with high recognition
rate but also to properly display the transcribed
text on a screen (Hoogenboom et al, 2008). Es-
pecially, in spoken monologue, since a sentence
tends to be long, each sentence is often displayed
as a multi-line text on a screen. Therefore, proper
linefeed insertion for the displayed text is desired
so that the text becomes easy to read.
Until now, there existed few researches about
how to display text on a screen in automatic cap-
tioning. As the research about linefeed insertion,
Monma et al proposed a method based on pat-
terns of a sequence of morphemes (Monma et
al., 2003). However, the target of the research is
closed-captions of Japanese TV shows, in which
less than or equal to 2 lines text is displayed on
a screen and the text all switches to other text at
a time. In the work, the highest priority concept
on captioning is that one screen should be filled
with as much text as possible. Therefore, a se-
mantic boundary in a sentence is hardly taken into
account in linefeed insertion, and the readability
of the caption is hardly improved.
This paper proposes a technique for inserting
linefeeds into transcribed texts of Japanese mono-
logue speech as an elemental technique to gener-
ate readable captions. We assume that a screen for
displaying only multi-line caption is placed to pro-
vide the caption information to the audience on the
site of a lecture. In our method, the linefeeds are
inserted into only the boundaries between bunset-
sus1, and the linefeeds are appropriately inserted
into a sentence by machine learning, based on the
information such as morphemes, dependencies2,
clause boundaries, pauses and line length.
We conducted an experiment on inserting line-
feeds by using Japanese spoken monologue data.
As the results of inserting linefeeds for 1,714 sen-
tences, the recall and precision of our method were
82.66% and 80.24%, respectively. Our method
improved the performance dramatically compared
1Bunsetsu is a linguistic unit in Japanese that roughly cor-
responds to a basic phrase in English. A bunsetsu consists of
one independent word and zero or more ancillary words.
2A dependency in Japanese is a modification relation in
which a modifier bunsetsu depends on a modified bunsetsu.
That is, the modifier bunsetsu and the modified bunsetsu work
as modifier and modifyee, respectively.
531
Figure 1: Caption display of spoken monologue
with four baseline methods, which we established
for comparative evaluation. The effectiveness of
our method has been confirmed.
This paper is organized as follows: The next
section describes our assumed caption and the pre-
liminary analysis. Section 3 presents our linefeed
insertion technique. An experiment and discussion
are reported in Sections 4 and 5, respectively. Fi-
nally, Section 6 concludes the paper.
2 Linefeed Insertion for Spoken
Monologue
In our research, in an environment in which cap-
tions are displayed on the site of a lecture, we as-
sume that a screen for displaying only captions is
used. In the screen, multi lines are always dis-
played, being scrolled line by line. Figure 1 shows
our assumed environment in which captions are
displayed.
As shown in Figure 2, if the transcribed text of
speech is displayed in accordance with only the
width of a screen without considering the proper
points of linefeeds, the caption is not easy to read.
Especially, since the audience is forced to read
the caption in synchronization with the speaker?s
utterance speed, it is important that linefeeds are
properly inserted into the displayed text in consid-
eration of the readability as shown in Figure 3.
To investigate whether the line insertion facili-
tates the readability of the displayed texts, we con-
ducted an experiment using the transcribed text of
lecture speeches in the Simultaneous Interpreta-
tion Database (SIDB) (Matsubara et al, 2002). We
randomly selected 50 sentences from the data, and
then created the following two texts for each sen-
tence based on two different concepts about line-
feed insertion.
?1?Text into which linefeeds were forcibly in-
serted once every 20 characters
????????????????????
????????????????????
????????????????????
????????????????????
???????????????????
For example, environmental problem, 
population problem, AIDS problem and 
so on, a lot of global-scale problems 
have occurred, and unfortunately, 
these problems seem to continue 
during 21st century or to become 
worse if we look through blue glasses.
Figure 2: Caption of monologue speech
????????
?????????
??????????
???????????????????
????????????
??????????
??????????????
?????????????????
(For example, environmental problem)
(population problem)
(AIDS problem and so on)
a lot of global-scale problems 
have occurred
(and unfortunately, these problems)
(to continue during also 21st century)
(or if we look through blue glasses)
(seems to become worse)
Figure 3: Caption into which linefeeds are prop-
erly inserted
49
50
49
37
40
36
43
48
34
49
1 2 3 4 5 6 7 8 9 10
?1? Forcible insertion of linefeeds
?2? Proper insertion of linefeeds
subject ID
# of sentences
50
45
40
35
30
25
20
15
10
5
0
Figure 4: Result of investigation of effect of line-
feed insertion into transcription
?2?Text into which linefeeds were properly
inserted in consideration of readability by
hand3
Figure 2 and 3 show examples of the text (1) and
(2), respectively. 10 examinees decided which of
the two texts was more readable. Figure 4 shows
the result of the investigation. The ratio that each
examinee selected text (2) was 87.0% on average.
There was no sentence in the text group (1) which
was selected by more than 5 examinees. These
indicates that a text becomes more readable by
proper insertion of linefeeds.
Here, since a bunsetsu is the smallest seman-
tically meaningful language unit in Japanese, our
method adopts the bunsetsu boundaries as the can-
didates of points into which a linefeed is inserted.
In this paper, hereafter, we call a bunsetsu bound-
ary into which a linefeed is inserted a linefeed
point.
33 persons inserted linefeeds into the 50 sentences by dis-
cussing where to insert the linefeeds.
532
Table 1: Size of analysis data
sentence 221
bunsetsu 2,891
character 13,899
linefeed 883
character per line 13.2
3 Preliminary Analysis about Linefeed
Points
In our research, the points into which linefeeds
should be inserted is detected by using machine
learning. To find the effective features, we investi-
gated the spoken language corpus. In our investi-
gation, we used Japanese monologue speech data
in the SIDB (Matsubara et al, 2002). The data
is annotated by hand with information on mor-
phological analysis, bunsetsu segmentation, de-
pendency analysis, clause boundary detection, and
linefeeds insertion. Table 1 shows the size of the
analysis data. Among 2,670 (= 2, 891?221) bun-
setsu boundaries, which are candidates of linefeed
points, there existed 833 bunsetsu boundaries into
which linefeeds were inserted, that is, the ratio of
linefeed insertion was 31.2%.
The linefeeds were inserted by hand so that the
maximum number of characters per line is 20. We
set the number in consideration of the relation be-
tween readability and font size on the display. In
the analysis, we focused on the clause boundary,
dependency relation, line length, pause and mor-
pheme of line head, and investigated the relations
between them and linefeed points.
3.1 Clause Boundary and Linefeed Point
Since a clause is one of semantically meaningful
language units, the clause boundary is considered
to be a strong candidate of a linefeed point. In the
analysis data, there existed 969 clause boundaries
except sentence breaks. Among them, 490 were
the points into which linefeeds were inserted, that
is, the ratio of linefeed insertion was 51.1%. This
ratio is higher than that of bunsetsu boundaries.
This indicates that linefeeds tend to be inserted
into clause boundaries.
We investigated the ratio of linefeed insertion
about 42 types4 of clause boundaries, which were
seen in the analysis data. Table 2 shows the top 10
4In our research, we used the types of clause boundaries
defined by the Clause Boundary Annotation Program (Kash-
ioka and Maruyama, 2004).
Table 2: Ratio of linefeed insertion for clause
boundary type
type of ratio of linefeed
clause boundary insertion (%)
topicalized element-wa 50.8
discourse marker 12.0
quotational clause 22.1
adnominal clause 23.3
compound clause-te 90.2
supplement clause 68.0
compound clause-ga 100.0
compound clause-keredomo 100.0
condition clause-to 93.5
adnominal clause-toiu 27.3
clause boundary types about the occurrence fre-
quency, and each ratio of linefeed insertion. In
the case of ?compound clause-ga? and ?compound
clause-keredomo,? the ratio of linefeed insertion
was 100%. On the other hand, in the case of ?quo-
tational clause,? ?adnominal clause? and ?adnomi-
nal clause-toiu,? the ratio of linefeed insertion was
less than 30%. This means that the likelihood of
linefeed insertion is different according to the type
of the clause boundary.
3.2 Dependency Structure and Linefeed
Point
When a bunsetsu depends on the next bunsetsu, it
is thought that a linefeed is hard to be inserted into
the bunsetsu boundary between them because the
sequence of such bunsetsus constitutes a semanti-
cally meaningful unit. In the analysis data, there
existed 1,459 bunsetsus which depend on the next
bunsetsu. Among the bunsetsu boundaries right
after them, 192 were linefeed points, that is, the
ratio of linefeed insertions was 13.2%. This ra-
tio is less than half of that for all the bunsetsu
boundaries. On the other hand, when the bunsetsu
boundary right after the bunsetsu which does not
depend on the next bunsetsu, the ratio of linefeed
insertion was 52.7%.
Next, we focused on the type of the dependency
relation, by which the likelihood of linefeed inser-
tion is different. For example, when the bunsetsu
boundary right after a bunsetsu on which the final
bunsetsu of an adnominal clause depends, the ra-
tio of linefeed insertion was 43.1%. This ratio is
higher than that for all the bunsetsu boundaries.
In addition, we investigated the relation be-
533
???????????????????
???????????????????
:  dependency relation? bunsetsu
[Dependency structure]
[Result of linefeed insertion in the analysis data]
A writer of the magazine in which 
only old domestic cars are covered
asks to get a story about my car
?? ???
????
???? ??? ??? ?? ?? ??
????
???
?????
only 
domestic cars
in which 
are covered
old
of the 
magazine
a 
writer
my car
to get a
story about
ask
Figure 5: Relation between dependency structure
and linefeed points
tween a dependency structure and linefeed points,
that is, whether the dependency structure is closed
within a line or not. Here, a line whose depen-
dency structure is closed means that all bunsetsus,
except the final bunsetsu, in the line depend on one
of bunsetsus in the line. Since, in many of seman-
tically meaningful units, the dependency structure
is closed, the dependency structure of a line is con-
sidered to tend to be closed. In the analysis data,
among 883 lines, 599 lines? dependency structures
were closed.
Figure 5 shows the relation between depen-
dency structure and linefeed points. In this exam-
ple, linefeeds are not inserted right after bunset-
sus which depend on the next bunsetsu (e.g. ??
? (my)? and ??? (car)?). Instead, a linefeed is
inserted right after a bunsetsu which does not de-
pend on the next bunsetsu (???? (a writer)?).
In addition, the dependency structure in each line
is closed.
3.3 Line Length and Linefeed Point
An extremely-short line is considered to be hardly
generated because the readability goes down if the
length of each line is very different. In the analysis
data, a line whose length is less than or equal to 6
characters occupied only 7.59% of the total. This
indicates that linefeeds tend to be inserted into the
place where a line can maintain a certain length.
3.4 Pause and Linefeed Point
It is thought that a pause corresponds to a syn-
tactic boundary. Therefore, there are possibility
that a linefeed becomes more easily inserted into
a bunsetsu boundary at which a pause exists. In
our research, a pause is defined as a silent interval
equal to or longer than 200ms. In the analysis data,
among 748 bunsetsu boundaries at which a pause
exists, linefeeds were inserted into 471 bunsetsu
boundaries, that is, the ratio of linefeed insertion
was 62.97%. This ratio is higher than that for all
the bunsetsu boundaries, thus, we confirmed that
linefeeds tend to be inserted into bunsetsu bound-
aries at which a pause exists.
3.5 Morpheme Located in the Start of a Line
There exist some morphemes which are unlikely
to become a line head. We investigated the ratio
that each leftmost morpheme of all the bunsetsus
appears at a line head. Here, we focused on the
basic form and part-of-speech of a morpheme. The
morphemes which appeared 20 times and of which
the ratio of appearance at a line head was less than
10% were as follows:
? Basic form:
??? (think) [2/70]?, ??? (problem)
[0/42]?, ??? (do) [3/33]?, ??? (become)
[2/32]????? (necessary) [1/21]?
? Part-of-speech:
noun-non independent-general [0/40]?
noun-nai adjective stem [0/40]?
noun-non independent-adverbial [(0/27]
If the leftmost morpheme of a bunsetsu is one of
these, it is thought that a linefeed is hardly inserted
right after the bunsetsu.
4 Linefeed Insertion Technique
In our method, a sentence, on which morphologi-
cal analysis, bunsetsu segmentation, clause bound-
ary analysis and dependency analysis are per-
formed, is considered the input. Our method de-
cides whether or not to insert a linefeed into each
bunsetsu boundary in an input sentence. Under
the condition that the number of characters in each
line has to be less than or equal to the maximum
number of characters per line, our method identi-
fies the most appropriate combination among all
combinations of the points into which linefeeds
can be inserted, by using the probabilistic model.
In this paper, we describe an input sentence
which consists of n bunsetsus as B = b1 ? ? ? bn,
and the result of linefeeds insertion as R =
r1 ? ? ? rn. Here, ri is 1 if a linefeed is inserted right
after bunsetsu bi, and is 0 otherwise. We describe
a sequence of bunsetsus in the j-th line among the
m lines created by dividing an input sentence as
Lj = bj1 ? ? ? bjnj (1 ? j ? m), and then, r
j
k = 0 if
k ?= nj , and rjk = 1 otherwise.
534
4.1 Probabilistic Model for Linefeed
Insertion
When an input sentenceB is provided, our method
identifies the result of linefeeds insertionR, which
maximizes the conditional probability P (R|B).
Assuming that whether or not a linefeed is inserted
right after a bunsetsu is independent of other line-
feed points except the linefeed point of the start of
the line which contains the bunsetsu, P (R|B) can
be calculated as follows:
P (R|B) (1)
= P (r11 = 0, ? ? ? , r1n1 = 1, ? ? ? , r
m
1 = 0, ? ? ? , rmnm = 1|B)
?= P (r11 = 0|B) ? P (r12 = 0|r11 = 0, B) ? ? ? ?
?P (r1n1 = 1|r
1
n1?1 = 0, ? ? ? , r
1
1 = 0, B) ? ? ? ?
?P (rm1 = 0|rm?1nm?1 = 1, B) ? ? ? ?
?P (rmm = 1|rmnm?1 = 0, ? ? ? , r
m
1 = 0, rm?1nm?1 = 1, B)
where P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 = 0, rj?1nj?1 =
1, B) is the probability that a linefeed is inserted
right after a bunsetsu bjk when the sequence of
bunsetsus B is provided and the linefeed point of
the start of the j-th line is identified. Similarly,
P (rjk = 0|r
j
k?1 = 0, ? ? ? , r
j
1 = 0, rj?1nj?1 = 1, B)
is the probability that a linefeed is not inserted
right after a bunsetsu bjk. These probabilities are
estimated by the maximum entropy method. The
result R which maximizes the conditional proba-
bility P (R|B) is regarded as the most appropriate
result of linefeed insertion, and calculated by dy-
namic programming.
4.2 Features on Maximum Entropy Method
To estimate P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) and P (r
j
k = 0|r
j
k?1 =
0, ? ? ? , rj1 = 0, rj?1nj?1 = 1, B) by the maximum
entropy method, we used the following features
based on the analysis described in Section 2.2.
Morphological information
? the rightmost independent morpheme (a part-
of-speech, an inflected form) and rightmost
morpheme (a part-of-speech) of a bunsetsu bjk
Clause boundary information
? whether or not a clause boundary exists right
after bjk
? a type of the clause boundary right after bjk (if
there exists a clause boundary)
Dependency information
? whether or not bjk depends on the next bun-
setsu
? whether or not bjk depends on the final bun-
setsu of a clause
? whether or not bjk depends on a bunsetsu to
which the number of characters from the start
of the line is less than or equal to the maxi-
mum number of characters
? whether or not bjk is depended on by the final
bunsetsu of an adnominal clause
? whether or not bjk is depended on by the bun-
setsu located right before it
? whether or not the dependency structure of
a sequence of bunsetsus between bjk and b
j
1,
which is the first bunsetsu of the line, is
closed
? whether or not there exists a bunsetsu which
depends on the modified bunsetsu of bjk,
among bunsetsus which are located after bjk
and to which the number of characters from
the start of the line is less than or equal to the
maximum number of characters
Line length
? any of the following is the class into which
the number of characters from the start of the
line to bjk is classified
? less than or equal to 2
? more than 2 and less than or equal to 6
? more than 6
Pause
? whether or not a pause exists right after bjk
Leftmost morpheme of a bunsetsu
? whether or not the basic form or part-of-
speech of the leftmost morpheme of the next
bunsetsu of bjk is one of the morphemes enu-
merated in Section 3.5.
5 Experiment
To evaluate the effectiveness of our method, we
conducted an experiment on inserting linefeeds by
using discourse speech data.
5.1 Outline of Experiment
As the experimental data, we used the transcribed
data of Japanese discourse speech in the SIDB
(Matsubara et al, 2002). All the data are anno-
tated with information on morphological analysis,
clause boundary detection and dependency anal-
ysis by hand. We performed a cross-validation
experiment by using 16 discourses. That is, we
535
repeated the experiment, in which we used one
discourse from among 16 discourses as the test
data and the others as the learning data, 16 times.
However, since we used 2 discourse among 16
discourses as the preliminary analysis data, we
evaluated the experimental result for the other 14
discourses (1,714 sentences, 20,707 bunsetsus).
Here, we used the maximum entropy method tool
(Zhang, 2008) with the default options except ?-i
2000.?
In the evaluation, we obtained recall, precision
and the ratio of sentences into which all linefeed
points were correctly inserted (hereinafter called
sentence accuracy). The recall and precision are
respectively defined as follows.
recall = # of correctly inserted LFs# of LFs in the correct data
precision = # of correctly inserted LFs# of automatically inserted LFs
For comparison, we established the following
four baseline methods.
1. Linefeeds are inserted into the rightmost bun-
setsu boundaries among the bunsetsu bound-
aries into which linefeeds can be inserted so
that the length of the line does not exceed
the maximum number of characters (Line-
feed insertion based on bunsetsu bound-
aries).
2. Linefeeds are inserted into the all clause
boundaries (Linefeed insertion based on
clause boundaries).
3. Linefeeds are inserted between adjacent bun-
setsus which do not depend on each other
(Linefeed insertion based on dependency
relations).
4. Linefeeds are inserted into the all bunsetsu
boundaries in which a pause exists (Linefeed
insertion based on pauses).
In the baseline 2, 3 and 4, if each condition is not
fulfilled within the maximum number of charac-
ters, a linefeed is inserted into the rightmost bun-
setsu boundary as well as the baseline 1.
In the experiment, we defined the maximum
number of characters per line as 20. The cor-
rect data of linefeed insertion were created by ex-
perts who were familiar with displaying captions.
There existed 5,497 inserted linefeeds in the 14
discourses, which were used in the evaluation.
Table 3: Experimental results
recall (%) precision (%) F-measure
our method 82.66 80.24 81.43
(4,544/5,497) (4,544/5,663)
baseline 1 27.47 34.51 30.59
(1,510/5,497) (1,510/4,376)
baseline 2 69.34 48.65 57.19
(3,812/5,497) (3,812/7,834)
baseline 3 89.48 53.73 67.14
(4,919/5,497) (4,919/9,155)
baseline 4 69.84 55.60 61.91
(3,893/5,497) (3,893/6,905)
5.2 Experimental Result
Table 3 shows the experimental results of the base-
lines and our method. The baseline 1 is very sim-
ple method which inserts linefeeds into the bun-
setsu boundaries so that the length of the line does
not exceed the maximum number of characters per
line. Therefore, the recall and precision were the
lowest.
In the result of baseline 2, the precision was
low. As described in the Section 3.1, the degree
in which linefeeds are inserted varies in differ-
ent types of clause boundaries. In the baseline
2, because linefeeds are also inserted into clause
boundaries which have the tendency that linefeeds
are hardly inserted, the unnecessary linefeeds are
considered to have been inserted.
The recall of baseline 3 was very high. This
is because, in the correct data, linefeeds were
hardly inserted between two neighboring bunset-
sus which are in a dependency relation. However,
the precision was low, because, in the baseline
3, linefeeds are invariably inserted between two
neighboring bunsetsus which are not in a depen-
dency relation.
In the baseline 4, both the recall and precision
were not good. The possible reason is that the bun-
setsu boundaries at which a pause exists do not
necessarily correspond to the linefeed points.
On the other hand, the F-measure and the sen-
tence accuracy of our method were 81.43 and
53.15%, respectively. Both of them were highest
among those of the four baseline, which showed
an effectiveness of our method.
5.3 Causes of Incorrect Linefeed Insertion
In this section, we discuss the causes of the in-
correct linefeed insertion occurred in our method.
Among 1,119 incorrectly inserted linefeeds, the
most frequent cause was that linefeeds were in-
536
??????????????????
????????
That is the period which I call
the first period without apology
Figure 6: Example of incorrect linefeed insertion
in ?adnominal clause.?
??????????????
?????
??????????????
??????????????????
(about how detail I can speak)
(I have a concern)
(from serious story to easy story )
(I want to speak)
Figure 7: Example of extra linefeed insertion
serted into clause boundaries of a ?adnominal
clause? type. The cause occupies 10.19% of the
total number of the incorrectly inserted linefeeds.
In the clause boundaries of the ?adnominal clause?
type, linefeeds should rarely be inserted funda-
mentally. However, in the result of our method,
a lot of linefeeds were inserted into the ?adnomi-
nal clause.? Figure 6 shows an example of those
results. In this example, a linefeed is inserted into
the ?adnominal clause? boundary which is located
right after the bunsetsu ????? (call).? The se-
mantic chunk ????????????? (is the
period which I call)? is divided.
As another cause, there existed 291 linefeeds
which divide otherwise one line according to the
correct data into two lines. Figure 7 shows an ex-
ample of the extra linefeed insertion. Although, in
the example, a linefeed is inserted between ???
???????????? (about how detail I
can speak)? and ?????? (I have a concern),?
the two lines are displayed in one line in the cor-
rect data. It is thought that, in our method, line-
feeds tend to be inserted even if a line has space to
spare.
6 Discussion
In this section, we discuss the experimental results
described in Section 5 to verify the effectiveness
of our method in more detail.
6.1 Subjective Evaluation of Linefeed
Insertion Result
The purpose of our research is to improve the read-
ability of the spoken monologue text by our line-
feed insertion. Therefore, we conducted a subjec-
tive evaluation of the texts which were generated
by the above-mentioned experiment.
In the subjective evaluation, examinees looked
at the two texts placed side-by-side between which
the only difference is linefeed points, and then se-
35
34
40
45
39
48
45
47 47
44
1 2 3 4 5 6 7 8 9 10
Baseline 3
Our method
subject ID
# of sentences
50
45
40
35
30
25
20
15
10
5
0
Figure 8: Result of subjective evaluation
lected the one which was felt more readable. Here,
we compared our method with the baseline 3, of
which F-measure was highest among four base-
lines described in Section 5.1. Ten examinees
evaluated 50 pairs of the results generated from
the same 50 randomly selected sentences.
Figure 8 shows the result of subjective evalua-
tion. This graph shows the number of each method
selected by each examinee. The ratio that our
method was selected was 94% in the highest case,
and 68% even in the lowest case. We confirmed
the effectiveness of our method for improving the
readability of the spoken monologue text.
On the other hand, there existed three sentences
for which more than 5 examinees judged that the
results of baseline 3 were more readable than those
of our method. From the analysis of the three sen-
tences, we found the following phenomena caused
text to be less readable
? Japanese syllabary characters (Hiragana) are
successionally displayed across a bunsetsu
boundary.
? The length of anteroposterior lines is ex-
tremely different each other.
Each example of the two causes is shown in
Figure 9 and 10, respectively. In Figure 9, a
bunsetsu boundary existed between Japanese syl-
labary characters ?????? (I)? and ?????
(if truth be told)? and these characters are succes-
sionally displayed in the same line. In these cases,
it becomes more difficult to identify the bunsetsu
boundary, therefore, the text is thought to become
difficult to read. In Figure 10, since the length of
the second line is extremely shorter than the first
line or third line, the text is thought to become dif-
ficult to read.
537
?????????????
???????????????????
????????
(Actually, I, if truth be told, I)
when I was a college student,  
(I) used to dodge  my train fare and
(be caught )
Actually, I, if truth be told, I used to dodge my train fare and be caught
when I was a college student.
Figure 9: Example of succession of hiragana
??????????????????
???
???????????????????
??????????????
I, the energy resources of which 
the remaining amount became little
in which humans who are in the past 
and future fight
(wrote a science-fiction novel)
(over)
I wrote a science-fiction novel, in which humans who are in the past and future 
fight over the energy resources of which the remaining amount became little.
Figure 10: Lines that have extremely different
length
Table 4: Other annotator?s results
recall (%) precision (%) F-measure
by human 89.82 (459/511) 89.82 (459/511) 89.82
our method 82.19 (420/511) 81.71 (420/514) 81.95
6.2 Comparison with Linefeeds Inserted by
Human
The concept of linefeed insertion for making the
caption be easy to read varies by the individual.
When multiple people insert linefeeds for the same
text, there is possibility that linefeeds are inserted
into different points.
Therefore, for one lecture data (128 sentences,
511 bunsetsus) in the experimental data, we con-
ducted an experiment on linefeed insertion by an
annotator who was not involved in the construc-
tion of the correct data. Table 4 shows the re-
call and the precision. The second line shows
the result of our method for the same lecture
data. In F-measure, our method achieved 91.24%
(81.95/89.82) of the result by the human annotator.
6.3 Performance of Linefeed Insertion Based
on Automatic Natural Language Analysis
In the experiment described in Section 5, we used
the linguistic information provided by human as
the features on the maximum entropy method.
However, compared with baseline 1, our method
uses a lot of linguistic information which should
be provided not by human but by natural language
analyzers under the real situation. Therefore, to
fairly evaluate our method and four baselines, we
conducted an experiment on linefeed insertion by
using the automatically provided information on
clause boundaries and dependency structures5.
5We used CBAP (Kashioka and Maruyama, 2004) as
a clause boundary analyzer and CaboCha (Kudo and Mat-
sumoto, 2002) with default learning data as a dependency
parser.
Table 5: Experimental results when information of
features are automatically provided
recall (%) precision (%) F-measure
our method 77.37 75.04 76.18
(4,253/5,497) (4,253/5,668)
baseline 1 27.47 34.51 30.59
(1,510/5,497) (1,510/4,376)
baseline 2 69.51 48.63 57.23
(3,821/5,497) (3,821/7,857)
baseline 3 84.01 52.03 64.26
(4,618/5,497) (4,618/8,876)
baseline 4 69.84 55.60 61.91
(3,893/5,497) (3.893/6,905)
Table 5 shows the result. Compared with Table
3, it shows the decreasing rate of the performance
of our method was more than those of four base-
lines which use simply only basic linguistic infor-
mation. However, the F-measure of our method
was more than 10% higher than those of four base-
lines.
7 Conclusion
This paper proposed a method for inserting line-
feeds into discourse speech data. Our method can
insert linefeeds so that captions become easy to
read, by using machine learning techniques on fea-
tures such as morphemes, dependencies, clause
boundaries, pauses and line length. An experi-
ment by using transcribed data of Japanese dis-
course speech showed the recall and precision was
82.66% and 80.24%, respectively, and we con-
firmed the effectiveness of our method.
In applying the linefeed insertion technique to
practical real-time captioning, we have to consider
not only the readability but also the simultaneity.
Since the input of our method is a sentence which
tends to be long in spoken monologue, in the fu-
ture, we will develop more simultaneous a tech-
nique in which the input is shorter than a sentence.
In addition, we assumed the speech recognition
system with perfect performance. To demonstrate
practicality of our method for automatic speech
transcription, an experiment using a continuous
speech recognition system will be performed in
the future.
Acknowledgments
This research was partially supported by the
Grant-in-Aid for Scientific Research (B) (No.
20300058) and Young Scientists (B) (No.
21700157) of JSPS, and by The Asahi Glass
Foundation.
538
References
G. Boulianne, J.-F. Beaumont, M. Boisvert,
J. Brousseau, P. Cardinal, C. Chapdelaine,
M. Comeau, P. Ouellet, and F. Osterrath. 2006.
Computer-assisted closed-captioning of live TV
broadcasts in French. In Proceedings of 9th Interna-
tional Conference on Spoken Language Processing,
pages 273?276.
T. Holter, E. Harborg, M. H. Johnsen, and T. Svendsen.
2000. ASR-based subtitling of live TV-programs for
the hearing impaired. In Proceedings of 6th Interna-
tional Conference on Spoken Language Processing,
volume 3, pages 570?573.
R. B. Hoogenboom, K. Uehara, T. Kanazawa,
S. Nakano, H. Kuroki, S. Ino, and T. Ifukube. 2008.
An application of real-time captioning system using
automatic speech recognition technology to college
efl education for deaf and hard-of-hearing students.
Gunma University Annual Research Reports, Cul-
tural Science Series, 57.
T. Imai, S. Sato, A. Kobayashi, K. Onoe, and
S. Homma. 2006. Online speech detection
and dual-gender speech recognition for captioning
broadcast news. In Proceedings of 9th International
Conference on Spoken Language Processing, pages
1602?1605.
H. Kashioka and T. Maruyama. 2004. Segmentation of
semantic units in Japanese monologues. In Proceed-
ings of ICSLT2004 and Oriental-COCOSDA2004,
pages 87?92.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Pro-
ceedings of 6th Conference on Computational Natu-
ral Language Learning, pages 63?69.
S. Matsubara, A. Takagi, N. Kawaguchi, and Y. Ina-
gaki. 2002. Bilingual spoken monologue corpus
for simultaneous machine interpretation research.
In Proceedings of 3rd International Conference on
Language Resources and Evaluation, pages 153?
159.
T. Monma, E. Sawamura, T. Fukushima, I. Maruyama,
T. Ehara, and K. Shirai. 2003. Automatic closed-
caption production system on TV programs for
hearing-impaired people. Systems and Computers
in Japan, 34(13):71?82.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modelling for automatic lecture tran-
scription. In Proceedings of 8th Annual Conference
of the International Speech Communication Associ-
ation, pages 2353?2356.
M. Saraclar, M. Riley, E. Bocchieri, and V. Gof-
fin. 2002. Towards automatic closed captioning:
Low latency real time broadcast news transcription.
In Proceedings of 7th International Conference on
Spoken Language Processing, pages 1741?1744.
J. Xue, R. Hu, and Y. Zhao. 2006. New improvements
in decoding speed and latency for automatic caption-
ing. In Proceedings of 9th International Conference
on Spoken Language Processing, pages 1630?1633.
L. Zhang. 2008. Maximum entropy mod-
eling toolkit for Python and C++. http:
//homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html. [Online; accessed
1-March-2008].
539
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 929?937,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Hypernym Discovery Based on Distributional Similarity                     
and Hierarchical Structures 
Ichiro Yamada?, Kentaro Torisawa?, Jun?ichi Kazama?, Kow Kuroda?,  
Masaki Murata?, Stijn De Saeger?, Francis Bond? and Asuka Sumida? 
 
?National Institute of Information and Communications Technology 
3-5 Hikaridai, Keihannna Science City 619-0289, JAPAN 
{iyamada,torisawa,kazama,kuroda,murata,stijn,bond}@nict.go.jp
?Japan Advanced Institute of Science and Technology 
1-1 Asahidai, Nomi-shi, Ishikawa-ken 923-1211, JAPAN 
a-sumida@jaist.ac.jp 
 
Abstract 
This paper presents a new method of devel-
oping a large-scale hyponymy relation data-
base by combining Wikipedia and other Web 
documents. We attach new words to the hy-
ponymy database extracted from Wikipedia 
by using distributional similarity calculated 
from documents on the Web. For a given tar-
get word, our algorithm first finds k similar 
words from the Wikipedia database. Then, 
the hypernyms of these k similar words are 
assigned scores by considering the distribu-
tional similarities and hierarchical distances 
in the Wikipedia database. Finally, new hy-
ponymy relations are output according to the 
scores. In this paper, we tested two distribu-
tional similarities. One is based on raw verb-
noun dependencies (which we call ?RVD?), 
and the other is based on a large-scale clus-
tering of verb-noun dependencies (called 
?CVD?). Our method achieved an attachment 
accuracy of 91.0% for the top 10,000 rela-
tions, and an attachment accuracy of 74.5% 
for the top 100,000 relations when using 
CVD. This was a far better outcome com-
pared to the other baseline approaches. Ex-
cluding the region that had very high scores, 
CVD was found to be more effective than 
RVD. We also confirmed that most relations 
extracted by our method cannot be extracted 
merely by applying the well-known lexico-
syntactic patterns to Web documents. 
1 Introduction 
Large-scale taxonomies such as WordNet (Fell-
baum 1998) play an important role in informa-
tion extraction and question answering. However, 
extremely high costs are borne to manually en-
large and maintain such taxonomies. Thus, appli-
cations using these taxonomies tend to face the 
drawback of data sparseness. This paper presents 
a new method for discovering a large set of hy-
ponymy relations. Here, a word1 X is regarded as 
a hypernym of a word Y if Y is a kind of X or Y 
is an instance of X. We are able to generate 
large-scale hyponymy relations by attaching new 
words to the hyponymy database extracted from 
Wikipedia (referred to as ?Wikipedia relation 
database?) by using distributional similarity cal-
culated from Web documents. Relations ex-
tracted from Wikipedia are relatively clean. On 
the other hand, reliable distributional similarity 
can be calculated using a large number of docu-
ments on the Web. In this paper, we combine the 
advantages of these two resources.  
Using distributional similarity, our algorithm 
first computes k similar words for a target word. 
Then, each k similar word assigns a score to its 
ancestors in the hierarchical structures of the 
Wikipedia relation database. The hypernym that 
has the highest score for the target word is se-
lected as the hypernym of the target word. Figure 
1 is an overview of the proposed approach. 
In the experiment, we extracted hypernyms for 
approximately 670,000 target words that are not 
included in the Wikipedia relation database but 
are found on the Web. We tested two distribu-
tional similarities: one based on raw verb-noun 
dependencies (RVD) and the other based on a 
large-scale clustering of verb-noun dependencies 
(CVD). The experimental results showed that the 
proposed methods were more effective than the 
other baseline approaches. In addition, we con-
firmed that most of the relations extracted by our 
method could not be extracted using the lexico-
syntactic pattern-based method.  
In the remainder of this paper, we first intro-
                                                 
1 In this paper, we use the term ?word? for both ?a 
single-word word? and ?a multi-word word.? 
929
duce some related works in Section 2. Section 3 
describes the Wikipedia relation database. Sec-
tion 4 describes the distributional similarity cal-
culated by the two methods. In Section 5, we 
describe a method to discover an appropriate 
hypernym for each target word. The experimen-
tal results are presented in Section 6 before con-
cluding the paper in Section 7. 
2 Related Works 
Most previous researchers have relied on lex-
ico-syntactic patterns for hyponymy acquisition. 
Lexico-syntactic patterns were first used by 
Hearst (1992). The patterns used by her included 
?NP0 such as NP1,? in which NP0 is a hypernym 
of NP1. Using these patterns as seeds, Hearst dis-
covered new patterns by which to semi-
automatically extract hyponymy relations. Pantel 
et al (2004a) proposed a method to automatical-
ly discover the patterns using a minimal edit dis-
tance. Ando et al (2003) applied predefined lex-
ico-syntactic patterns to Japanese news articles. 
Snow et al (2005) generalized these lexico-
syntactic pattern-based methods by using depen-
dency path features for machine learning. Then, 
they extended the framework such that this me-
thod was capable of making use of heterogenous 
evidence (Snow et al 2006). These pattern-based 
methods require the co-occurrences of a target 
word and the hypernym in a document. It should 
be noted that the requirement of such co-
occurrences actually poses a problem when we 
extract a large set of hyponymy relations since 
they are not frequently observed (Shinzato et al 
2004, Pantel et al 2004b). 
Clustering-based methods have been proposed 
as another approach. Caraballo (1999), Pantel et 
al. (2004b), and Shinzato et al (2004) proposed a 
method to find a common hypernym for word 
classes, which are automatically constructed us-
ing some measures of word similarities or hierar-
chical structures in HTML documents. Etzioni et 
al. (2005) used both a pattern-based approach 
and a clustering-based approach. The required 
amount of co-occurrences is significantly re-
duced due to class-based generalization 
processes. Note that these clustering-based me-
thods obtain the same hypernym for all the words 
in a particular class. This causes a problem for 
selecting an appropriate hypernym for each word 
in the case when the granularity or the construc-
tion of the classes is incorrect. Figure 2 shows 
the drawbacks of the existing approaches. 
Ponzetto et al (2007) and Sumida et al (2008) 
proposed a method for acquiring hyponymy rela-
tions from Wikipedia. This Wikipedia-based ap-
proach can extract a large volume of hyponymy 
relations with high accuracy. However, it is also 
true that this approach does not account for many 
words that usually appear in Web documents; 
this could be because of the unbalanced topics in 
Wikipedia or merely because of the incomplete 
coverage of articles on Wikipedia. Our method 
can target words that frequently appear on the 
Web but are not included in the Wikipedia rela-
tion database, thus making the results of the Wi-
kipedia-based approach richer and more ba-
lanced. Our approach uses distributional similari-
Figure 1: Overview of the proposed approach. 
hypernym : 
Target word:  Selected from the Web 
: word
k similar words
No direct co-occurrences of 
hypernym and hyponym in 
corpora are needed.
Selected from hypernyms in the 
Wikipedia relation database.
A hypernym is selected for 
each word independently.
Wikipedia relation database
Wikipedia-based approach
(Ponzetto et al 2007 and 
Sumida et al 2008)
Hyponymy relations are 
extracted using the layout 
information of Wikipedia.
Wikipedia
Figure 2: Drawbacks in existing approaches for hypo-
nymy acquisition. 
Pattern-based method
(Hearst 1992, Pantel et al 
2004a, Ando et al 2003, 
Snow et al 2005, Snow et al 
2006, and Etzioni et al 2005)
Clustering-based method
(Caraballo 1999, Pantel et al 
2004b, Shinzato et al 2004, 
and Etzioni et al 2005)
DocumentsCorpus/documents
Co-occurrences 
in a pattern are 
needed 
hypernym such as word hypernym ..?   word
word
word
wordword
Word Class
word
The same hypernym 
is selected for all 
words in a class.
930
ty, which is computed based on the noun-verb 
dependency profiles on the Web. The use of dis-
tributional similarity resembles the clustering-
based approach; however, our method can select 
a hypernym for each word independently, and it 
does not suffer from class granularity mismatch 
or the low quality of classes. In addition, our ap-
proach exploits the hierarchical structures of the 
Wikipedia hypernym relations.  
3 Wikipedia Relation Database 
Our Wikipedia relation database is based on the 
extraction method of Sumida et al (2008). They 
proposed a method of automatically acquiring 
hyponymy relations by focusing on the hierar-
chical layout of articles on Wikipedia. By way of 
an example, Figure 3 shows part of the source 
code clipped from the article titled ?Penguin.? 
An article has hierarchical structures composed 
of titles, sections, itemizations, etc. The entire 
article is divided into sections titled ?Anatomy,? 
?Mating habits,? ?Systematics and evolution,? 
?Penguins in popular culture,? and so on. The 
section ?Systematics and evolution? has a sub-
section ?Systematics,? which is further divided 
into ?Aptenodytes,? ?Eudyptes,? and so on. 
Some of these section-subsection relations can be 
regarded as valid hyponymy relations. In this 
article, relations such as the one between ?Apte-
nodytes? and ?Emperor Penguin? and that be-
tween ?Book? and ?Penguins of the World? are 
valid hyponymy relations.  
First, Sumida et al (2008) extracted hypony-
my relation candidates from hierarchical struc-
tures on Wikipedia. Then, they selected proper 
hyponymy relations using a support vector ma-
chine classifier. They used several kinds of fea-
tures for the hyponymy relation candidate, such 
as a POS tag for each word, the appearance of 
morphemes of each word, the distance between 
two words in the hierarchical structures of Wiki-
pedia, and the last character of each word. As a 
result of their experiments, approximately 2.4 
million hyponymy relations in Japanese were 
extracted, with a precision rate of 90.1%.  
Compared to the traditional taxonomies, these 
extracted hyponymy relations have the following 
characteristics (Fellbaum 1998, Bond et al 2008). 
(a) The database includes a more extensive 
vocabulary. 
(b)  The database includes a large number of 
named entities. 
Popular Japanese taxonomies GoiTaikei (Ike-
hara et al 1997) and Bunrui-Goi-Hyo (1996) 
contain approximately 300,000 words and 
96,000 words, respectively. In contrast, the ex-
tracted hyponymy relations contain approximate-
ly 1.2 million hyponyms and are undoubtedly 
much larger than the existing taxonomies. 
Another difference is that since Wikipedia covers 
a large number of named entities, the extracted 
hyponymy relations also contain a large number 
of named entities.  
Note that the extracted relations have a hierar-
chical structure because one hypernym of a cer-
tain word may also be the hyponym of another 
hypernym. However, we observed that the depth 
of the hierarchy, on an average, is extremely 
shallow. To make the hierarchy appropriate for 
our method, we extended these into a deeper hie-
rarchical structure. The extracted relations in-
clude many compound nouns as hypernyms, and 
we decomposed a compound noun into a se-
quence of nouns using a morphological analyzer. 
Since Japanese is a head-final language, the suf-
fix of a noun sequence becomes the hypernym of 
the original compound noun if the suffix forms 
another valid compound noun. We extracted suf-
fixes of compound nouns and manually checked 
whether they were valid compound nouns; then, 
we constructed a hierarchy of compound nouns. 
The hierarchy can be extended such that it in-
cludes the hyponyms of the original hypernym 
and the resulting hierarchy constitutes a hierar-
chical taxonomy. We use this hierarchical tax-
onomy as a target for expansion.2  
                                                 
2  Note that this modification was performed as part of 
another project of ours aimed at constructing a large-scale 
and clean hypernym knowledge base by human annotation. 
We do not think this cost is directly relevant to the method 
proposed here. 
Figure 3: A part of source code clipped from the 
article ?Penguin? in Wikipedia. 
'''Penguins''' are a group of 
[[Aquatic animal|aquatic]], 
[[flightless bird]]s. 
== Anatomy == 
== Mating habits == 
==Systematics and evolution== 
===Systematics=== 
* Aptenodytes 
**[[Emperor Penguin]] 
** [[King Penguin]] 
* Eudyptes 
== Penguins in popular culture == 
== Book == 
* Penguins 
* Penguins of the World 
== Notes == 
* Penguinone 
* the [[Penguin missile]] 
[[Category:Penguins]] 
[[Category:Birds]]
931
4 Distributional Similarity 
The distributional hypothesis states that words 
that occur in similar contexts tend to be semanti-
cally similar (Harris 1985). In this section, we 
first introduce distributional similarity based on 
raw verb-noun dependencies (RVD). To avoid 
the sparseness problem of the co-occurrence of 
verb-noun dependencies, we also use distribu-
tional similarity based on a large-scale clustering 
of verb-noun dependencies (CVD). 
In the experiment mentioned in the following 
section, we used the TSUBAKI corpus (Shinzato 
et al 2008) to calculate distributional similarity. 
This corpus provides a collection of 100 million 
Japanese Web pages containing 6 ? 109
 
sentences. 
4.1 Distributional Similarity Based on RVD 
When calculating the distributional similarity 
based on RVD, we use the triple <v, rel, n>, 
where v is a verb, n is a noun phrase, and rel 
stands for the relation between v and n. In Japa-
nese, a relation rel is represented by postposi-
tions attached to n and the phrase composed of n 
and rel modifies v. Each triple is divided into two 
parts. The first is <v, rel> and the second is n. 
Then, we consider the conditional probability of 
occurrence of the pair <v, rel>: P(<v, rel>|n).  
P(<v, rel>|n) can be regarded as the distribution 
of the grammatical contexts of the noun phrase n. 
The distributional similarity can be defined as 
the distance between these distributions. There 
are several kinds of functions for evaluating the 
distance between two distributions (Lee 1999). 
Our method uses the Jensen-Shannon divergence. 
The Jensen-Shannon divergence between two 
probability distributions, )|( 1nP ?  and )|( 2nP ? , 
can be calculated as follows: 
 
)),
2
)|()|(
||)|((
)
2
)|()|(
||)|(((
2
1
))|(||)|((
21
2
21
1
21
nPnP
nPD
nPnP
nPD
nPnPD
KL
KL
JS
?+??+
?+??=
??
 
 
where DKL indicates the Kullback-Leibler diver-
gence and is defined as follows: 
 
.
)|(
)|(
log)|())|(||)|((
2
1
121 ? ???=?? nP nPnPnPnPDKL  
 
Finally, the distributional similarity between 
two words, n1 and n2, is defined as follows: 
 
)).|(||)|((1),( 2121 nPnPDnnsim JS ???=  
 
This similarity assumes a value from 0 to 1. If 
two words are similar, the value will be close to 
1; if two words have entirely different meanings, 
the value will be 0.
 
In the experiment, we used 1,000,000 noun 
phrases and 100,000 pairs of verbs and postposi-
tions to calculate the probability P(<v, rel>|n) 
from the dependency relations extracted from the 
above-mentioned Web corpus (Shinzato et al 
2008). The probabilities are computed using the 
following equation by modifying for the fre-
quency using the log function: 
 
?
>?<
+><
+><=><
Drelv
nrelvf
nrelvf
nrelvP
,
1),,(log(
1)),,(log(
)|,(
,0),,(if >>< nrelvf
  
where f(<v, rel, n>) is the frequency of a triple 
<v, rel, n> and D is the set defined as { <v, rel > | 
f(<v, rel, n>) > 0 }. In the case of f(<v, rel, n>) = 
0, P(<v, rel>|n) is set to 0.  
Instead of using the observed frequency di-
rectly as in the usual maximum likelihood esti-
mation, we modified it as above. Although this 
might seems strange, this kind of modification is 
common in information retrieval as a term 
weighing method (Manning et al 1999) and  it is 
also applied in some studies to yield better word 
similarities (Terada et al 2006, Kazama et al 
2009). We also adopted this idea in this study. 
4.2 Distributional Similarity Based on CVD 
Rooth et al (1999) and Torisawa (2001) showed 
that EM-based clustering using verb-noun de-
pendencies can produce semantically clean noun 
clusters. We exploit these EM-based clustering 
results as the smoothed contexts for noun n. In 
Torisawa?s model (2001), the probability of oc-
currence of the triple <v, rel, n> is defined as 
follows: 
 
,)()|()|,(
),,(
? ? ><=
><
Aadef aPanParelvP
nrelvP
 
 
where a denotes a hidden class of <v,rel> and n. 
In this equation, the probabilities P(<v,rel>|a), 
P(n|a), and P(a) cannot be calculated directly 
because class a is not observed in a given corpus. 
The EM-based clustering method estimates these 
probabilities using a given corpus. In the E-step, 
932
the probability P(a|<v,rel>) is calculated. In the 
M-step, the probabilities P(<v,rel>|a), P(n|a), 
and P(a) are updated to arrive at the maximum 
likelihood using the results of the E-step. From 
the results of estimation of this EM-based clus-
tering method, we can obtain the probabilities 
P(<v,rel>|a), P(n|a), and P(a) for each <v, rel>, n, 
and a. Then, P(a|n) is calculated by the following 
equation: 
 
.
)()|(
)()|(
)|( ? ?= Aa aPanP
aPanP
naP  
 
P(a|n) can be used to find the class of n. For 
example, the class that has the maximum P(a|n) 
can be regarded as the class to which n belongs. 
Noun phrases that occur with similar pairs 
<v,rel> tend to be classified in the same class. 
Kazama et al (2008) proposed the paralleliza-
tion of this EM-based clustering with the aim of 
enabling large-scale clustering and using the re-
sulting clusters in named entity recognition. Ka-
zama et al (2009) reported the calculation of 
distributional similarity using the clustering re-
sults. The distributional similarity was calculated 
by the Jensen-Shannon divergence, which was 
used in this paper. Similar to the case in Kazama 
et al, we performed word clustering using 
1,000,000 noun phrases and 2,000 classes. Note 
that the frequencies of dependencies were mod-
ified with the log function, as in RVD, described 
in the previous section. 
5 Discovering an Appropriate Hyper-
nym for a Target word 
In the Wikipedia relation database, there are 
about 95,000 hypernyms and about 1.2 million 
hyponyms. In both RVD and CVD, the words 
used were selected according to the number (the 
number of kinds, not the frequency) of <v, rel >s 
that n has dependencies in the data. As a result, 1 
million words were selected. The number of 
common words that are also included in the Wi-
kipedia relation database are as follows: 
 
Hypernyms     28,015 (common hypernyms) 
Hyponyms   175,022 (common hyponyms) 
 
These common hypernyms become candidates 
for hypernyms for a target word. On the other 
hand, the common hyponyms are used as clues 
for identifying appropriate hypernyms. 
In our task, the potential target words are 
about 810,000 in number and are not included in 
the Wikipedia relation database. These include 
some strange words or word phrases that are ex-
tracted due to the failure of morphological analy-
sis. We exclude these words using simple rules. 
Consequently, the number of target words for our 
process is reduced to about 670,000.  
In the following section, we outline the scor-
ing method that uses k similar words to discover 
an appropriate hypernym for a target word. We 
also explain several baseline approaches that use 
distributional similarity. 
5.1 Scoring with k similar Words 
In this approach, we first calculate the similari-
ties between the common hyponyms and a target 
word and select the k most similar common hy-
ponyms. Here, we use a similarity threshold val-
ue Smin to avoid the effect of words having lower 
similarities. If the similarity is less than the thre-
shold value, the word is excluded from the set of 
k similar words. Next, each k similar word votes 
a score to its ancestors in the hierarchical struc-
tures of the Wikipedia relation database. The 
score used to vote for a hypernym nhyper is as fol-
lows: 
 
,),(
)(
)()(
1),(?
??
? ?=
trghyperhypo
hypohyper
nksimilarnDescn
hypotrg
nnr
hyper
nnsimd
nscore
 
 
where ntrg is the target word, Desc(nhyper) is the 
descendant of the hypernym nhyper, ksimilar(ntrg) 
is the k similar word of ntrg, 
1),( ?hypohyper nnrd is a 
penalty that depends on the differences in the 
depth of hierarchy, d is a parameter for the penal-
ty value and has a value between 0 and 1, and 
r(ntrg, nhypo) is the difference in the depth of hie-
rarchy between ntrg and nhypo. sim(ntrg,nhypo) is a 
distributional similarity between ntrg and nhypo.  
As a result of scoring, each hypernym has a 
score for the target word. The hypernym that has 
the highest score for the target word is selected 
as its hypernym. The hyponymy relations thus 
produced are ranked according to the scores. 
Figure 4 shows an example of the scoring 
process. In this example, we use CitroenAX as the 
target word whose hypernym will be identified. 
First, the k similar words are extracted from the 
common hyponyms in the Wikipedia relation: 
Opel Astra, TVR Tuscan, Mitsubishi Minica, and 
Renault Lutecia are extracted. Next, each k simi-
lar word votes a score to its ancestors. The words 
Opel Astra, TVR Tuscan, and Renault Lutecia 
vote to their parent car and the word Mitsubishi 
933
Minica votes to its parent mini-vehicle and its 
grandparent car with a small penalty. Finally, the 
hypernym car, which has the highest score, is 
selected as the hypernym of the target word Ci-
troenAX. 
5.2 Baseline Approaches 
Using distributional similarity, we can also de-
velop the following baseline approaches to dis-
cover hyponymy relations. 
 
Selecting the hypernym of the most similar hy-
ponym (baseline approach 1) 
We use the heuristics that similar words tend to 
have the same hypernym. In this approach, we 
first calculate the similarities between the com-
mon hyponyms and the target word. The com-
mon hyponym most similar to the target word is 
extracted. Then, the parent of the extracted 
common hyponym is regarded as the hypernym 
of the target word. This approach outputs several 
hypernyms when the most similar hyponym has 
several hypernyms. This approach can be consi-
dered to be the same as the scoring method using 
k similar words when k = 1. We use the distribu-
tional similarity between the target word and the 
most similar hyponym in the Wikipedia relation 
database as the score for the appropriateness of 
the resulting hyponymy. 
 
Selecting the most similar hypernym (baseline 
approach 2) 
The distributional similarity between the com-
mon hypernym and the target word is calculated. 
Then, the hypernym that has the highest distribu-
tional similarity is regarded as the hypernym of 
the target word. The similarity is used as the 
score of the appropriateness of the produced hy-
ponymy. 
 
Scoring based on the average similarity of the 
hypernym?s children (baseline approach 3) 
This approach uses the probabilistic distributions 
of the hypernym?s children. We define the prob-
ability )|( hyperchild nP ? characterized by the children 
of the hypernym nhyper, as follows: 
 
,
)(
)()|(
)|(
)(
)(
?
?
?
?
?
=?
hyperhypo
hyperhypo
nChn
hypo
nChn
hypohypo
hyperchild nP
nPnP
nP  
 
where Ch(nhyper) is a set of all children of nhyper. 
Then, distributional similarities between a com-
mon hypernym nhyper and the target word nhypo are 
calculated. The hypernym that has the highest 
distributional similarity is selected as the hyper-
nym of the word. This distributional similarity is 
used as the score of the appropriateness of the 
produced hyponymy. 
If a hypernym has only a few children, the re-
liability of the probabilistic distribution of 
hypernym defined here will be low because the 
Wikipedia relation database includes some incor-
rect relations. For this reason, we use the hyper-
nym only if the number of children it has is more 
than a threshold value.  
6 Experiments 
We evaluated our proposed methods by using it 
in experiments to discover hypernyms from the 
Wikipedia relation database for the target words 
extracted from about 670,000 noun phrases.  
6.1 Parameter Estimation by Preliminary 
Experiments 
In the proposed methods, there are several para-
meters. We performed parameter optimization by 
randomly selecting 694 words as development 
data in our preliminary experiments. The hyper-
nyms of these words were determined manually. 
We adjusted the parameters so that each method 
achieved the best performance for this develop-
ment data. 
The parameters in the scoring method with k 
similar words were adjusted as follows3:  
 (RVD) 
Number of similar words:         k = 100. 
Similarity threshold:           Smin = 0.05. 
Penalty value for ancestors:    d = 0.6. 
                                                 
3 We tested the parameter values k = {100, 200, 300, 400, 
500, 600, 700, 800, 900, 1000}, Smin={0, 0.05, 0.1, 0.15, 0.2, 
0.25, 0.3, 0.35, 0.4} and d={0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 
0.8, 0.85, 0.9, 0.95, 1.0}. 
Figure 4: Overview of the scoring process.
car
CitroenAX
mini
vehicle
hybrid 
vehicle
Opel 
Astra
Renault 
Lutecia
Mitsubishi
Minica
k similar words
Each k-similar word
votes the score to its 
ancestors in the Wikipedia 
relation database.
Target word selected 
from the Web text (ntrg).
TVR 
Tuscan
: common hypernym(nhyper)
: k similar word &  
common hyponym(nhypo)
x d1
x d0
x d0
934
(CVD) 
Number of similar words:         k = 200. 
Similarity threshold:                Smin = 0.3. 
Penalty value for ancestors:    d = 0.6. 
 
The parameter in baseline approach 3 was ad-
justed as follows: 
Threshold for the number of children: 20. 
6.2 Evaluation of the Experimental Results 
on the Basis of Score Ranking 
Using the adjusted parameters, we conducted 
experiments to extract the hypernym of each tar-
get word with the help of the scoring method 
based on k similar words. In these experiments, 
two kinds of distributional similarity mentioned 
in Section 4 were exploited individually. The 
words that were used in the development data 
were excluded.  
We also conducted a comparative experiment 
in which the parameter value for the penalty of 
the hierarchal difference, d, was set to 0 to clari-
fy the ability of using hierarchal structures in the 
k similar words method. This means each k simi-
lar word votes only to their parent. 
We then judged the quality of each acquired 
hypernym. The evaluation data sets were sam-
pled from the top 1,000, 10,000, 100,000, and 
670,000 results that were ranked according to the 
score of each method. Then, against 200 samples 
that were randomly sampled from each set, one 
of the authors judged whether the hypernym ex-
tracted by each method for the target word was 
correct or not. In this evaluation, if the sentence 
?The target word is a kind of the hypernym? or 
?The target word is an instance of the hypernym? 
was consistent, the extracted hyponymy was 
judged as correct. It should be noted that the out-
puts of the compared methods are combined and 
shuffled to enable fair comparison. In addition, 
baseline approach 1 extracted several hypernyms 
for the target word. In this case, we judged the 
hypernym as correct when the case where one of 
the hypernyms was correct.  
The precision of each result is shown in Table 
1. The results of the k similar words method are 
far better than those of the other baseline me-
thods. In particular, the k similar words method 
with CVD outperformed the methods of the k 
similar words where the parameter value d was 
set to 0 and the method using RVD except for the 
top 1,000 results. This means that the use of hie-
rarchal structures and the clustering process for 
calculating distributional similarity are effective 
for this task. We confirmed the significant differ-
ences of the proposed method (CVD) as com-
pared with all the baseline approaches at the 1% 
significant level by the Fisher?s exact test (Hays 
1988). 
The precision of baseline approach 2 that se-
lected the most similar hypernym was the worst 
among all the methods. There were words that 
were similar to the target word among the hyper-
nyms extracted incorrectly. For example, the 
word semento-kojo (cement factory) was ex-
tracted for the hypernym of the word kuriningu-
kojo (dry cleaning plant). It is difficult to judge 
whether the word is a hypernym or just a similar 
word by using only the similarity measure. 
As for the results of baseline approach 1 using 
the most similar hyponym and baseline approach 
3 using the similarity of the set of hypernym?s 
children, the noise on the Wikipedia relation da-
tabase decreased the precision. Moreover, over-
specified hypernyms were extracted incorrectly 
by these methods. In contrast, the method of 
scoring based on the use of k similar words was 
robust against noise because it uses the voting 
approach for the similarities. Further, this me-
thod can extract hypernyms that are not over-
specific because it uses all descendants for scor-
ing.  
Table 2 shows some examples of relations ex-
tracted by the k similar words method using 
CVD. 
 
Table 1:  Precision of each approach based on the score ranking. CVD represents the method that uses the dis-
tributional similarity based on large-scale of clustering of verb-noun dependencies. RVD represents the 
one based on raw verb-noun dependencies. 
 k-similar words
(CVD) 
k-similar words
(RVD) 
k-similar words
(CVD, d = 0)
Baseline  
approach 1 
(CVD) 
Baseline  
approach 2 
(CVD) 
Baseline  
approach 3 
(CVD) 
1,000 0.940 1.000 0.850 0.730 0.290 0.630 
10,000 0.910 0.875 0.875 0.555 0.300 0.445 
100,000 0.745 0.710 0.730 0.500 0.280 0.435 
670,000 0.520 0.500 0.470 0.345 0.115 0.170 
935
6.3 Investigation of the Extracted Relation 
Overlap with a Conventional Method 
We randomly sampled 300 hyponymy rela-
tions that were extracted correctly using the k 
similar words method exploiting CVD and inves-
tigated whether or not these relations can be ex-
tracted by the conventional method based on the 
lexico-syntactic pattern. The possible hyponymy 
relations were extracted using the pattern-based 
method (Ando et al 2003) from the TSUBAKI 
corpus (Shinzato et al 2008). From a comparison 
of these relations, we found only 57 common 
hyponymy relations. That is, the remaining 243 
hyponymy relations were not included in the 
possible hyponymy relations. This result indi-
cates that our method can acquire the hyponymy 
relations that cannot be extracted by the conven-
tional pattern-based method. 
6.4 Discussions 
We investigated the reason for the errors gener-
ated by the method of scoring using k similar 
words exploiting CVD. We conducted experi-
ments on hypernym extraction targeting 694 
words in the development data mentioned in Sec-
tion 6.1. Among these, 286 relations were ex-
tracted incorrectly. In these relations, there were 
some frequent hypernyms. For example, the 
word sakuhin (work) appeared 28 times and hon 
(book) appeared 20 times. As shown in Table 2, 
hon (book) was also extracted for the target word 
meru-seminah (mail seminar). It is really diffi-
cult even for a human to identify whether the 
title is that of the book or the event. If we can 
identify these difficult hypernyms in advance, we 
can improve precision by excluding them from 
the target hypernyms. This will be one of the top-
ics for future study. 
7 Conclusion 
In this paper, we proposed a method for disco-
vering hyponymy relations between nouns by 
fusing the Wikipedia relation database and words 
from the Web. We demonstrated that the method 
using k similar words has high accuracy. The 
experimental results showed the effectiveness of 
using hierarchal structures and the clustering 
process for calculating distributional similarity 
for this task. The experimental results showed 
that our method could achieve 91.0% attachment 
accuracy for the top 10,000 hyponymy relations 
and 74.5% attachment accuracy for the top 
100,000 relations when using the clustering-
based similarity. We confirmed that most rela-
tions extracted by the proposed method could not 
be handled by the lexico-syntactic pattern-based 
method. Future work will be to filter out difficult 
hypernyms for hyponymy extraction process to 
achieve higher precision. 
References 
M. Ando, S. Sekine and S. Ishizaki. 2003. Automatic 
Extraction of Hyponyms from Newspaper Using 
Lexicosyntactic Patterns. IPSJ SIG Notes, 2003-
NL-157, pp. 77?82 (in Japanese). 
F. Bond, H. Isahara, K. Kanzaki and K. Uchimoto. 
2008. Boot-strapping a WordNet Using Multiple 
Existing WordNets. In the 6th International Confe-
rence on Language Resources and Evaluation 
(LREC), Marrakech.  
Bunruigoihyo. 1996. The National Language Re-
search Institute (in Japanese). 
S. A. Caraballo. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text. In 
Proceedings of the Conference of the Association 
for Computational Linguistics (ACL). 
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. 
Shaked, S. Soderland, D. Weld and A. Yates. 2005. 
Unsupervised Named-Entity Extraction from the 
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91?134. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Table2:  Hypernym discovery results by the k-similar 
words based approach (CVD). The underline indi-
cates the hypernyms which are extracted incorrectly.
Score Target word Extracted hypernym 
58.6 INDIVI burando 
(fashion label)
54.3 kureome (Cleome) hana (flower)
34.4 UOKR  gemu (game)
21.7 Okido (Okido) machi (town)
20.5 Sumatofotsu 
(Smart fortwo) 
kuruma  
(car) 
15.6 Fukagawameshi 
(Fukagawa rice)
ryori (dish) 
8.9 John Barry sakkyokuka 
 (composer)
8.5 JVM sofuto-wea 
(software) 
6.6 metangasu 
(methane gas) 
genso 
(chemical element)
5.4 me-ru semina 
(mail seminar) 
Hon (book) 
3.9 gurometto 
(grommet) 
shohin 
(merchandise)
3.1 supuringubakku  
(spring back) 
gensho 
(phenomenon)
936
Database. Cambridge, MA: MIT Press. 
Z. Harris. 1985. Distributional Structure. In Katz, J. J. 
(ed.) The Philosophy of Linguistics, Oxford Uni-
versity Press, pp. 26?47. 
W. L. Hays. 1988. Statistics: Analyzing Qualitative 
Data, Rinehart and Winston, Inc., Ch. 18, pp. 769?
783. 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In Proceedings of 
the 14th Conference on Computational Linguistics 
(COLING), pp. 539?545.  
S. Ikehara, M. Miyazaki, S. Shirai, A. Yokoo, H. Na-
kaiwa, K. Ogura, Y. Ooyama and Y. Hayashi. 1997. 
Goi-Taikei A Japanese Lexicon, Iwanami Shoten. 
J. Kazama and K. Torisawa. 2008. Inducing Gazet-
teers for Named Entity Recognition by Large-scale 
Clustering of Dependency Relations. In Proceed-
ings of ACL-08: HLT, pp. 407?415. 
J. Kazama, Stijn De Saeger, K. Torisawa and M. Mu-
rata. 2009. Generating a Large-scale Analogy List 
Using a Probabilistic Clustering Based on Noun-
Verb Dependency Profiles. In 15th Annual Meeting 
of the Association for Natural Language 
Processing, C1?3 (in Japanese). 
L. Lee. 1999. Measures of Distributional Similarity. 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, pp. 25?
32. 
C. D. Manning and H. Schutze. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press. 
P. Pantel, D. Ravichandran and E. Hovy. 2004a. To-
wards Terascale Knowledge Acquisition. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics. 
P. Pantel and D. Ravichandran. 2004b. Automatically 
Labeling Semantic Classes. In Proceedings of the 
Human Language Technology and North American 
Chapter of the Association for Computational Lin-
guistics Conference. 
S. P. Ponzetto, and M. Strube. 2007. Deriving a Large 
Scale Taxonomy from Wikipedia. In Proceedings 
of the 22nd National Conference on Artificial Intel-
ligence, pp. 1440?1445. 
M. Rooth, S. Riezler, D. Presher, G. Carroll and F. 
Beil. 1999. Inducing a Semantically Annotated 
Lexicon via EM-based Clustering. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics, pp. 104?111. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypo-
nymy Relations from Web Documents. In Proceed-
ings of HLT-NAACL, pp. 73?80. 
K. Shinzato, D. Kawahara, C. Hashimoto and S. Ku-
rohashi. 2008. A Large-Scale Web Data Collection 
as A Natural Language Processing Infrastructure. 
In the 6th International Conference on Language 
Resources and Evaluation (LREC). 
R. Snow, D. Jurafsky and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Dis-
covery. NIPS 2005. 
R. Snow, D. Jurafsky, A. Y. Ng. 2006. Semantic Tax-
onomy Induction from Heterogenous Evidence. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th annual 
meeting of the Association for Computational Lin-
guistics, pp. 801?808. 
A. Sumida, N. Yoshinaga and K. Torisawa. 2008. 
Boosting Precision and Recall of Hyponymy Rela-
tion Acquisition from Hierarchical Layouts in Wi-
kipedia. In the 6th International Conference on 
Language Resources and Evaluation (LREC). 
A. Terada, M. Yoshida, H. Nakagawa. 2006. A Tool 
for Constructing a Synonym Dictionary using con-
text Information. In proceedings of IPSJ SIG Tech-
nical Reports, vol.2006 No.124, pp. 87-94. (In Jap-
anese). 
K. Torisawa. 2001. An Unsupervised Method for Ca-
nonicalization of Japanese Postpositions. In Pro-
ceedings of the 6th Natural Language Processing 
Pacific Rim Symposium (NLPRS), pp. 211?218. 
K. Torisawa, Stijn De Saeger, Y. Kakizawa, J. Kaza-
ma, M. Murata, D. Noguchi and A. Sumida. 2008. 
TORISHIKI-KAI, An Autogenerated Web Search 
Directory. In Proceedings of the second interna-
tional symposium on universal communication, pp. 
179?186, 2008. 
937
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1172?1181,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Large-Scale Verb Entailment Acquisition from the Web
Chikara Hashimoto? Kentaro Torisawa? Kow Kuroda?
Stijn De Saeger? Masaki Murata? Jun?ichi Kazama?
National Institute of Information and Communications Technology
Sorakugun, Kyoto, 619-0289, JAPAN
{
? ch,? torisawa,? kuroda,? stijn,?murata,? kazama}@nict.go.jp
Abstract
Textual entailment recognition plays a
fundamental role in tasks that require in-
depth natural language understanding. In
order to use entailment recognition tech-
nologies for real-world applications, a
large-scale entailment knowledge base is
indispensable. This paper proposes a con-
ditional probability based directional sim-
ilarity measure to acquire verb entailment
pairs on a large scale. We targeted 52,562
verb types that were derived from 108
Japanese Web documents, without regard
for whether they were used in daily life
or only in specific fields. In an evaluation
of the top 20,000 verb entailment pairs ac-
quired by previous methods and ours, we
found that our similarity measure outper-
formed the previous ones. Our method
also worked well for the top 100,000 re-
sults.
1 Introduction
We all know that if you snored, you must have
been sleeping, that if you are divorced, you must
have been married, and that if you won a lawsuit,
you must have sued somebody. These relation-
ships between events where one is the logical con-
sequence of the other are called entailment. Such
knowledge plays a fundamental role in tasks that
require in-depth natural language understanding,
e.g., answering questions and using natural lan-
guage interfaces.
This paper proposes a novel method for verb
entailment acquisition. Using a Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents, we automat-
ically acquired such verb pairs as snore ? sleep
and divorce ? marry, where entailment holds be-
tween the verbs in the pair.1 Our definition of ?en-
tailment? is the same as that in WordNet3.0; v
1
entails v
2
if v
1
cannot be done unless v
2
is, or has
been, done.2
Our method follows the distributional similar-
ity hypothesis, i.e., words that occur in the same
context tend to have similar meanings. Just as in
the methods of Lin and Pantel (2001) and Szpek-
tor and Dagan (2008), we regard the arguments
of verbs as the context in the hypothesis. How-
ever, unlike the previous methods, ours is based
on conditional probability and is augmented with
a simple trick that improves the accuracy of verb
entailment acquisition. In an evaluation of the top
20,000 verb entailment pairs acquired by the pre-
vious methods and ours, we found that our similar-
ity measure outperformed the previous ones. Our
method also worked well for the top 100,000 re-
sults,
Since the scope of Natural Language Process-
ing (NLP) has advanced from a formal writing
style to a colloquial style and from restricted to
open domains, it is necessary for the language re-
sources for NLP, including verb entailment knowl-
edge bases, to cover a broad range of expressions,
regardless of whether they are used in daily life
or only in specific fields that are highly techni-
cal. As we will discuss later, our method can ac-
quire, with reasonable accuracy, verb entailment
pairs that deal not only with common and familiar
verbs but also with technical and unfamiliar ones
like podcast ? download and jibe ? sail.
Note that previous researches on entailment ac-
quisition focused on templates with variables or
word-lattices (Lin and Pantel, 2001; Szpektor and
Dagan, 2008; Barzilay and Lee, 2003; Shinyama
1Verb entailment pairs are described as v
1
? v
2
(v
1
is
the entailing verb and v
2
is the entailed one) henceforth.
2WordNet3.0 provides entailment relationships between
synsets like divorce, split up?marry, get married, wed, con-
join, hook up with, get hitched with, espouse.
1172
et al, 2002). Certainly these templates or word
lattices are more useful in such NLP applications
as Q&A than simple entailment relations between
verbs. However, our contention is that entailment
certainly holds for some verb pairs (like snore ?
sleep) by themselves, and that such pairs consti-
tute the core of a future entailment rule database.
Although we focused on verb entailment, our
method can also acquire template-level entailment
pairs with a reasonable accuracy.
The rest of this paper is organized as follows.
In ?2, related works are described. ?3 presents our
proposed method. After this, an evaluation of our
method and the existing methods is presented in
Section 4. Finally, we conclude the paper in ?5.
2 Related Work
Previous studies on entailment, inference rules,
and paraphrase acquisition are roughly classi-
fied into those that require comparable corpora
(Shinyama et al, 2002; Barzilay and Lee, 2003;
Ibrahim et al, 2003) and those that do not (Lin
and Pantel, 2001; Weeds and Weir, 2003; Geffet
and Dagan, 2005; Pekar, 2006; Bhagat et al, 2007;
Szpektor and Dagan, 2008).
Shinyama et al (2002) regarded newspaper arti-
cles that describe the same event as a pool of para-
phrases, and acquired them by exploiting named
entity recognition. They assumed that named en-
tities are preserved across paraphrases, and that
text fragments in the articles that share several
comparable named entities should be paraphrases.
Barzilay and Lee (2003) also used newspaper ar-
ticles on the same event as comparable corpora
to acquire paraphrases. They induced paraphras-
ing patterns by sentence clustering. Ibrahim et al
(2003) relied on multiple English translations of
foreign novels and sentence alignment to acquire
paraphrases. We decided not to take this approach
since using comparable corpora limits the scale
of the acquired paraphrases or entailment knowl-
edge bases. Although obtaining comparable cor-
pora has been simplified by the recent explosion
of the Web, the availability of plain texts is incom-
parably better.
Entailment acquisition methods that do not re-
quire comparable corpora are mostly based on the
distributional similarity hypothesis and use plain
texts with a syntactic parser. Basically, they parse
texts to obtain pairs of predicate phrases and their
arguments, which are regarded as features of the
predicates with appropriately assigned weights.
Lin and Pantel (2001) proposed a paraphrase ac-
quisition method (non-directional similarity mea-
sure) called DIRT which acquires pairs of binary-
templates (predicate phrases with two argument
slots) that are paraphrases of each other. DIRT em-
ploys the following similarity measure proposed
by Lin (1998):
Lin(l, r) =
?
f?F
l
?F
r
[w
l
(f) + w
r
(f)]
?
f?F
l
w
l
(f) +
?
f?F
r
w
r
(f)
where l and r are the corresponding slots of two
binary templates, F
s
is s?s feature vector (argu-
ment nouns), and w
s
(f) is the weight of f ? F
s
(PMI between s and f ). The intuition behind this
is that the more nouns two templates share, the
more semantically similar they are. Since we ac-
quire verb entailment pairs based on unary tem-
plates (Szpektor and Dagan, 2008) we used the
Lin formula to acquire unary templates directly
rather than using the DIRT formula, which is the
arithmetic-geometric mean of Lin?s similarities for
two slots in a binary template.
Bhagat et al (2007) developed an algorithm
called LEDIR for learning the directionality of
non-directional inference rules like those pro-
duced by DIRT. LEDIR implements a Direction-
ality Hypothesis: when two binary semantic re-
lations tend to occur in similar contexts and the
first one occurs in significantly more contexts than
the second, then the second most likely implies the
first and not vice versa.
Weeds and Weir (2003) proposed a general
framework for distributional similarity that mainly
consists of the notions of what they call Precision
(defined below) and Recall:
Precision(l, r) =
?
f?F
l
?F
r
w
l
(f)
?
f?F
l
w
l
(f)
where l and r are the targets of a similarity mea-
surement, F
s
is s?s feature vector, and w
s
(f) is the
weight of f ? F
s
. The best performing weight is
PMI. Precision is a directional similarity measure
that examines the coverage of l?s features by those
of r?s, with more coverage indicating more simi-
larity.
Szpektor and Dagan (2008) proposed a direc-
tional similarity measure called BInc (Balanced-
Inclusion) that consists of Lin and Precision, as
BInc(l, r) =
?
Lin(l, r) ? Precision(l, r)
1173
where l and r are the target templates. For weight-
ing features, they used PMI. Szpektor and Dagan
(2008) also proposed a unary template, which is
defined as a template consisting of one argument
slot and one predicate phrase. For example, X take
a nap ? X sleep is an entailment pair consisting
of two unary templates. Note that the slot X must
be shared between templates. Though most of the
previous entailment acquisition studies focused on
binary templates, unary templates have an obvi-
ous advantage over binary ones; they can handle
intransitive predicate phrases and those that have
omitted arguments. The Japanese language, which
we deal with here, often omits arguments, and thus
the advantage of unary templates is obvious.
As shown in ?4, our method outperforms Lin,
Precision, and BInc in accuracy.
Szpector et al (2004) addressed broad coverage
entailment acquisition. But their method requires
an existing lexicon to start, while ours does not.
Apart from the dichotomy of the compara-
ble corpora and the distributional similarity ap-
proaches, Torisawa (2006) exploited the structure
of Japanese coordinated sentences to acquire verb
entailment pairs. Pekar (2006) used the local
structure of coherent text by identifying related
clauses within a local discourse. Zanzotto et al
(2006) exploited agentive nouns. For example,
they acquired win ? play from ?the player wins.?
Geffet and Dagan (2005) proposed the Distribu-
tional Inclusion Hypotheses, which claimed that if
a word v entails another word w, then all the char-
acteristic features of v are expected to appear with
w, and vice versa. They applied this to noun en-
tailment pair acquisition, rather than verb pairs.
3 Proposed Method
This section presents our method of verb entail-
ment acquisition. First, the basics of Japanese are
described. Then, we present the directional sim-
ilarity measure that we developed in ?3.2. ?3.3
describes the structure and acquisition of the web-
based data from which entailment pairs are de-
rived. Finally, we show how we acquire verb en-
tailment pairs using our proposed similarity mea-
sure and the web-based data in ?3.4.
3.1 Basics of Japanese
Japanese explicitly marks arguments including the
subject and object by postpositions, and is a head-
final language. Thus, a verb phrase consisting of
an object hon (book) and a verb yomu (read), for
example, is expressed as hon-wo yomu (book-ACC
read) ?read a book? with the accusative postpo-
sition wo marking the object.3 Accordingly, we
refer to a unary template as ?p, v? hereafter, with
p and v referring to the postposition and a verb.
Also, we abbreviate a template-level entailment
?p
l
, v
l
? ? ?p
r
, v
r
? as l ? r for simplicity. We
define a unary template as a template consisting
of one argument slot and one predicate, following
Szpektor and Dagan (2008).
3.2 Directional Similarity Measure based on
Conditional Probability
The directional similarity measure that we devel-
oped and called Score is defined as follows:
Score(l, r) = Score
base
(l, r) ? Score
trick
(l, r)
where l and r are unary templates, and Score in-
dicates the probability of l ? r. Score
base
, which
is the base of Score, is defined as follows:
Score
base
(l, r) =
?
f?F
l
?F
r
P (r|f)P (f |l)
where F
s
is s?s feature vector (nouns including
compounds). The intention behind the definition
of Score
base
is to emulate the conditional proba-
bility P (v
r
|v
l
)
4 in a distributional similarity style
function. Note that P (v
r
|v
l
) should be 1 when en-
tailment v
l
? v
r
holds (i.e., v
r
is observed when-
ever v
l
is observed) and we have reliable proba-
bility values. Then, if we can directly estimate
P (v
r
|v
l
), it is reasonable to assume v
l
? v
r
if
P (v
r
|v
l
) is large enough. However, we cannot es-
timate P (v
r
|v
l
) directly since it is unlikely that we
will observe the verbs v
r
and v
l
at the same time.
(People do not usually repeat v
r
and v
l
in the same
document to avoid redundancy.) Thus, instead of
a direct estimation, we substitute Score
base
(l, r)
as defined above. In other words, we assume
P (v
r
|v
l
) ? P (r|l) ? ?
f?F
l
?F
r
P (f |l)P (r|f).
Actually, Score
base
originally had another mo-
tivation, inspired by Torisawa (2005), for which no
postposition but the instrumental postposition de
was relevant. In this discussion, all of the nouns
(fs) that are marked by the instrumental postposi-
tion are seen as ?tools,? and P (f |l) is interpreted
3ACC represents an accusative postposition in Japanese.
Likewise, NOM, DAT, INS, and TOP are the symbols for the
nominative, dative, instrumental, and topic postpositions.
4Remember that v
l
and v
r
are the verbs of unary tem-
plates l and r.
1174
as a measure of how typically the tool f is used
to perform the action denoted by (the v
l
of) l; if
P (f |l) is large enough, f is a typical tool used in
l. On the other hand, P (r|f) indicates the proba-
bility of (the v
r
of) r being the purpose for using
the tool f . See (1) for an example.
(1) konro-de chouri-suru
cooking.stove-INS cook
?cook (something) using a cooking stove.?
The purpose of using a cooking stove is to cook.
Torisawa (2005) has pointed out that when r ex-
presses the purpose of using a tool f , P (r|f) tends
to be large. This predicts that P (r|cooking stove)
is large, where r is ?de, cook?.
According to this observation, if f is a single
purpose tool and P (f |l), the probability of f be-
ing the tool by which l is performed, and P (r|f),
the probability of r being the purpose of using the
tool f , are large enough, then the typical perfor-
mance of the action v
l
should contain some ac-
tions that can be described by v
r
, i.e., the pur-
pose of using f . Moreover, if all the typical tools
(fs) used in v
l
are also used for v
r
, most perfor-
mances of the action v
l
should contain a part de-
scribed by the action v
r
. In summary, this means
that when ?
f?F
l
?F
r
P (r|f)P (f |l), Score
base
, has
a large value, we can expect v
l
? v
r
.
For example, let v
l
be deep-fry and v
r
be cook.
Note that v
l
? v
r
holds for this example. There
are many tools that are used for deep-frying,
such as cooking stove, pot, or pan. This means
that P (cooking stove|l), P (pot|l), or P (pan|l) are
large. On the other hand, the purpose of using all
of these tools is cooking, based on common sense.
Thus, probabilities such as P (r|cooking stove)
and P (r|pan) should have large values. Accord-
ingly, ?
f?F
l
?F
r
P (f |l)P (r|f), Score
base
, should
be relatively large for deep-fry ? cook,
Actually, we defined Score
base
based on the
above assumption However, through a series of
preliminary experiments, we found that the same
score could be applied without losing the preci-
sion to the other postpositions. Thus, we gener-
alized the framework so that it could deal with
most postpositions, namely ga (NOM), wo (ACC),
ni (DAT), de (INS), and wa (TOP). Note that this
is a variation of the distributional inclusion hy-
pothesis (Geffet and Dagan, 2005), but that we do
not use mutual information as in previous works,
based on the hypothesis discussed above. Actu-
ally, as shown in ?4, our conditional probability
based method outperformed the mutual informa-
tion based metrics in our experiments.
On the other hand, Score
trick
implements an-
other assumption that if only one feature con-
tributes to Score
base
and the contribution of the
other nouns is negligible, if any, the similarity is
unreliable. Accordingly, for Score
trick
, we uni-
formly ignore the contribution of the most domi-
nant feature from the similarity measurement.
Score
trick
(l, r)
= Score
base
(l, r) ? max
f?F
l
?F
r
P (r|f)P (f |l)
As shown in ?4, this trick actually improved the
entailment acquisition accuracy.
We used maximum likelihood estimation to ob-
tain P (r|f) and P (f |l) in the above discussion.
Bannard and Callison-Burch (2005) and Fujita
and Sato (2008) also proposed directional simi-
larity measures based on conditional probability,
which are very similar to Score
base
, although ei-
ther their method?s prerequisites or the targets of
the similarity measurements were different from
ours. The method of Bannard and Callison-Burch
(2005) requires bilingual parallel corpora, and
uses the translations of expressions as its feature.
Fujita and Sato (2008) dealt with productive pred-
icate phrases, while our target is non-productive
lexical units, i.e., verbs. Thus, this is the first
attempt to apply a conditional probability based
similarity measure to verb entailment acquisition.
In addition, the trick implemented in Score
trick
is
novel.
3.3 Preparing Template-Feature Tuples
Our method starts from a dataset called template-
feature tuples, which was derived from the Web
in the following way: 1) Parse the Japanese Web
corpus (Kawahara and Kurohashi, 2006a) derived
from 108 Japanese Web documents with Japanese
dependency parser KNP (Kawahara and Kuro-
hashi, 2006b). 2) Extract triples ?n, p, v? consist-
ing of nouns (n), postpositions (p), and verbs (v),
where an n marked by a p depends on a v from
the parsed Web text. 3) From the triple database,
construct template-feature tuples ?n, ?p, v?? by re-
garding ?p, v? as a unary template and n as one of
its features. 4) Convert the verbs into their canon-
ical forms as defined by KNP. 5) Filter out tuples
that fall into one of the following categories: 5-
1) Freq(?p, v?) < 20. 5-2) Its verb is passivized,
1175
causativized, or negated. 5-3) Its verb is semanti-
cally vague like be, do, or become. 5-4) Its post-
position is something other than ga (NOM), wo
(ACC), ni (DAT), de (INS), or wa (TOP).
The resulting unary template-feature tuples in-
cluded 127,808 kinds of templates that consisted
of 52,562 verb types and five kinds of postpo-
sitions. The verbs included compound words
like bosi-kansen-suru (mother.to.child-infection-
do) ?infect from mothers to infants.?
3.4 Acquiring Entailment Pairs
We acquired verb entailment pairs using the fol-
lowing procedure: i) From the template-feature
tuples mentioned in ?3.3, acquire unary template
pairs that exhibit an entailment relation between
them using the directional similarity measure in
?3.2. ii) Convert the acquired unary templates
?p, v? into naked verbs v by stripping the postpo-
sitions p. iii) Remove the duplicated verb pairs
resulting from stripping ps. To be precise, when
we removed the duplicated pairs, we left the high-
est ranked one. iv) Retrieve N-best verb pairs as
the final output from the result of iii). That is, we
first acquired unary template pairs and then trans-
formed them into verb pairs.
Although this paper focuses on verb entailment
acquisition, we also evaluated the accuracy of
template-level entailment acquisition, in order to
show that our similarity measure works well, not
only for verb entailment acquisition, but also for
template entailment acquisition (See ?4.4). we
created two kinds of unary templates: the ?Scoring
Slots? template and the ?Nom(inative) Slots? tem-
plate. The first is simply the result of the procedure
i); all of the templates have slots that are used for
similarity scoring. The second one was obtained
in the following way: 1) Only templates whose p
is not a nominative are sampled from the result of
the procedure i). 2) Their ps are all changed to a
nominative. Templates of the second kind are used
to show that the corresponding slots between tem-
plates (nominative, in this case) that are not used
for similarity scoring can be incorporated to re-
sulting template-level entailment pairs if the scor-
ing function really captures the semantic similarity
between templates.
Note that, for unary template entailment pairs
like (2) to be well-formed, the two unary slots (X-
wo) between templates must share the same noun
as the index i indicates. This is relevant in ?4.4.
(2) X
i
-wo musaborikuu ? X
i
-wo taberu
X
i
-ACC gobble X
i
-ACC eat
4 Evaluation
We compare the accuracy of our method with that
of the alternative methods in ?4.1. ?4.2 shows
the effectiveness of the trick. We examine the en-
tailment acquisition accuracy for frequent verbs in
?4.3, and evaluate the performance of our method
when applied to template-level entailment acquisi-
tion in ?4.4. Finally, by showing the accuracy for
verb pairs obtained from the top 100,000 results,
we claim that our method provides a good start-
ing point from which a large-scale verb entailment
resource can be constructed in ?4.5.
For the evaluation, three human annotators (not
the authors) checked whether each acquired entail-
ment pair was correct. The average of the three
Kappa values for each annotator pair was 0.579
for verb entailment pairs and 0.568 for template
entailment pairs, both of which indicate the mid-
dling stability of this evaluation annotation.
4.1 Experiment 1: Verb Pairs
We applied Score, BInc, Lin, and Precision to the
template-feature tuples (?3.3), obtained template
entailment pairs, and finally obtained verb entail-
ment pairs by removing the postpositions from the
templates as described in ?3. As a baseline, we
created pairs from randomly chosen verbs.
Since we targeted all of the verbs that ap-
peared on the Web (under the condition of
Freq(?p, v?) ? 20), the annotators were con-
fronted with technical terms and slang that they
did not know. In such cases, they consulted dic-
tionaries (either printed or machine readable ones)
and the Web. If they still could not find the mean-
ing of a verb, they labeled the pair containing the
unknown verb as incorrect.
We used the accuracy = # of correct pairs# of acquired pairs as
an evaluation measure. We regarded a pair as cor-
rect if it was judged correct by one (Accuracy-1),
two (Accuracy-2), or three (Accuracy-3) annota-
tors.
We evaluated 200 entailment pairs sampled
from the top 20,000 for each method (# of ac-
quired pairs = 200). For fairness, the evaluation
samples for each method were shuffled and placed
in one file from which the annotators worked. In
this way, they were unable to know which entail-
ment pair came from which method.
1176
Note that the verb entailment pairs produced
by Lin do not provide the directionality of en-
tailment. Thus, the annotators decided the direc-
tionality of these entailment pairs as follows: i)
Copy 200 original samples and reverse the order
of v
1
and v
2
. ii) Shuffle the 400 Lin samples
(the original and reversed samples) with the other
ones. iii) Evaluate all of the shuffled pairs. Each
Lin pair was regarded as correct if either direction
was judged correct. In other words, we evaluated
the upper bound performance of the LEDIR algo-
rithm.
Table 1 shows the accuracy of the acquired
verb entailment pairs for each method. Figure 1
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
BInc 0.450 0.255 0.125
Precision 0.725 0.545 0.385
Lin 0.590 0.370 0.160
Random 0.050 0.010 0.005
Table 1: Accuracy of verb entailment pairs.
shows the accuracy figures for the N-best entail-
ment pairs for each method, with N being 1,000,
2,000, . . ., or 20,000. We observed the following
points from the results. First, Score outperformed
all the other methods. Second, Score and Pre-
cision, which are directional similarity measures,
worked well, while Lin, which is a symmetric one,
performed poorly even though the directionality of
its output was determined manually.
Looking at the evaluated samples, Score suc-
cessfully acquired pairs in which the entailed
verbs generalized entailing verbs that were techni-
cal terms. (3) shows examples of Score?s outputs.
(3) a. RSS-haisin-suru ? todokeru
RSS-feed-do deliver
?feed the RSS data?
b. middosippu-maunto-suru ? tumu
midship-mounting-do mount
?have (engine) midship-mounted?
The errors made by DIRT (4) and BInc (5) in-
cluded pairs consisting of technical terms.
(4) kurakkingu-suru
software.cracking-do
?crack a (security) system?
? koutiku-hosyu-suru
building-maintenance-do
?build and maintain a system?
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
BInc
Precision
Lin
Figure 1: Accuracy of verb entailment pairs.
(5) suisou-siiku-suru
tank-raising-do
?raise (fish) in a tank?
? siken-houryuu-suru
test-discharge-do
?stock (with fish) experimentally?
These terms are related in some sense, but they
are not entailment pairs.
4.2 Experiment 2: Effectiveness of the Trick
Next, we investigated the effectiveness of the trick
described in ?3. We evaluated Score, Score
trick
,
and Score
base
. Table 2 shows the accuracy figures
for each method. Figure 2 shows the accuracy fig-
ures for the N-best outputs for each method. The
1177
Method Acc-1 Acc-2 Acc-3
Score 0.770 0.660 0.460
Score
trick
0.725 0.610 0.395
Score
base
0.590 0.465 0.315
Table 2: Effectiveness of the trick.
results illustrate that introducing the trick signif-
icantly improved the performance of Score
base
,
and so did multiplying Score
trick
and Score
base
,
which is our proposal Score.
(6) shows an example of Score
base
?s errors.
(6) gazou-sakusei-suru ? henkou-suru
image-making-do change-do
?make an image? ?change?
This pair has only two shared nouns (f ? F
l
?F
r
),
and more than 99.99% of the pair?s similarity re-
flects only one of the two. Clearly, the trick would
have prevented the pair from being highly ranked.
4.3 Experiment 3: Pairs of Frequent Verbs
We found that the errors made by Lin and BInc
in Experiment 1 were mostly pairs of infrequent
verbs such as technical terms. Thus, we con-
ducted the acquisition of entailment pairs targeting
more frequent verbs to see how their performance
changed. The experimental conditions were the
same as in Experiment 1, except that the templates
(?p, v?) used were all Freq(?p, v?) ? 200.
Table 3 shows the accuracy figures for each
method with the changes in accuracy from those
of the original methods in parentheses. The re-
Method Acc-1 Acc-2 Acc-3
Score
0.690 0.520 0.335
(?0.080) (?0.140) (?0.125)
BInc 0.455 0.295 0.160(+0.005) (+0.040) (+0.035)
Precision 0.450 0.355 0.205(?0.275) (?0.190) (?0.180)
Lin 0.635 0.385 0.205(+0.045) (+0.015) (+0.045)
Table 3: Accuracy of frequent verb pairs.
sults show that the accuracies of Score and Pre-
cision (the two best methods in Experiment 1) de-
graded, while the other two improved a little. We
suspect that the performance difference between
these methods would get smaller if we further re-
stricted the target verbs to more frequent ones.
Accuracy-1
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-2
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Accuracy-3
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  5000  10000  15000  20000
Score
ScoretrickScorebase
Figure 2: Accuracy of verb entailment pairs ac-
quired by Score, Score
trick
, and Score
base
.
However, we believe that dealing with verbs com-
prehensively, including infrequent ones, is impor-
tant, since, in the era of information explosion, the
impact on applications is determined not only by
frequent verbs but also infrequent ones that consti-
tute the long tail of a verb-frequency graph. Thus,
this tendency does not matter for our purpose.
4.4 Experiment 4: Template Pairs
This section presents the entailment acquisition
accuracy for template pairs to show that our
method can also perform the entailment acqui-
sition of unary templates. We presented pairs
of unary templates, obtained by the procedure in
1178
?3.4, to the annotators. In doing so, we restricted
the correct entailment pairs to those for which en-
tailment always held regardless of what argument
filled the two unary slots, and the two slots had to
be filled with the same argument, as exemplified
in (2). We evaluated Score and Precision.
Table 4 shows the accuracy of the acquired pairs
of unary templates. Compared to verb entailment
Method Acc-1 Acc-2 Acc-3
Score
0.655 0.510 0.300
Scoring (?0.115) (?0.150) (?0.160)
Slots Precision 0.565 0.430 0.265(?0.160) (?0.115) (?0.120)
Score
0.665 0.515 0.315
Nom (?0.105) (?0.145) (?0.145)
Slots Precision 0.490 0.325 0.215(?0.235) (?0.220) (?0.170)
Table 4: Accuracy of entailment pairs of templates
whose slots were used for scoring.
acquisition, the accuracy of both methods dropped
by about 10%. This was mainly due to the evalua-
tion restriction exemplified in (2) which was not
introduced in the previous experiments; the an-
notators ignored the argument correspondence be-
tween the verb pairs in Experiment 1. Also note
that Score outperformed Precision in this experi-
ment, too.
(7) and (8) are examples of the Scoring Slots
template entailment pairs and (9) is that of the
Nom Slots acquired by our method.
(7) X-wo tatigui-suru ? X-wo taberu
X-ACC standing.up.eating-do X-ACC eat
?eat X standing up? ?eat X?
(8) X-de marineedo-suru ? X-wo ireru
X-INS marinade-do X-ACC pour
?marinate with X? ?pour X?
(9) X-ga NBA-iri-suru ? ? ? (was X-de (INS))
X-NOM NBA-entering-do
?X joins an NBA team?
? X-ga nyuudan-suru ? ? ? (was X-de)
X-NOM enrollment-do
?X joins a team?
4.5 Experiment 5: Verb Pairs form the Top
100,000
Finally, we examined the accuracy of the top
100,000 verb pairs acquired by Score and Preci-
sion. As Table 5 shows, Score outperformed Pre-
Method Acc-1 Acc-2 Acc-3
Score 0.610 0.480 0.300
Precision 0.470 0.295 0.190
Table 5: Accuracy of the top 100,000 verb pairs.
cision. Note also that Score kept a reasonable ac-
curacy for the top 100,000 results (Acc-2: 48%).
The accuracy is encouraging enough to consider
human annotation for the top 100,000 results to
produce a language resource for verb entailment,
which we actually plan to do.
Below are correct verb entailment examples
from the top 100,000 results of our method.
(10) The 121th pair
kaado-kessai-suru ? siharau
card-payment-do pay
?pay by card? ?pay?
(11) The 6,081th pair
saitei-suru ? sadameru
adjudicate-do settle
?adjudicate? ?settle?
(12) The 15,464th pair
eraa-syuuryou-suru ? jikkou-suru
error-termination-do perform-do
?abend? ?execute?
(13) The 30,044th pair
ribuuto-suru ? kidou-suru
reboot-do start-do
?reboot? ?boot?
(14) The 57,653th pair
rinin-suru ? syuunin-suru
resignation-do accession-do
?resign? ?accede?
(15) The 70,103th pair
sijou-tounyuu-suru ? happyou-suru
market-input-do publication-do
?bring to the market? ?publicize?
Below are examples of erroneous pairs from our
results. (16) is a causal relation but not an entail-
ment. (17) is a contradictory pair.
(16) The 5,475th pair
juken-suru ? goukaku-suru
take.an.exam-do acceptance-do
?take an exam? ?gain admission?
1179
(17) The 40,504th pair
ketujou-suru ? syutujou-suru
not.take.part-do take.part-do
?not take part? ?take part?
5 Conclusion
This paper addressed verb entailment acquisition
from the Web, and proposed a novel directional
similarity measure Score. Through a series of ex-
periments, we showed i) that Score outperforms
the previously proposed measures, Lin, Precision,
and BInc in large scale verb entailment acquisi-
tion, ii) that our proposed trick implemented in
Score
trick
significantly improves the accuracy of
verb entailment acquisition despite its simplicity,
iii) that Score worked better than the others even
when we restricted the target verbs to more fre-
quent ones, iv) that our method is also moder-
ately successful at producing template-level en-
tailment pairs, and v) that our method maintained
reasonable accuracy (in terms of human annota-
tion) for the top 100,000 results. As examples of
the acquired verb entailment pairs illustrated, our
method can acquire from an ocean of information,
namely the Web, a variety of verb entailment pairs
ranging from those that are used in daily life to
those that are used in very specific fields.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL2005),
pages 597?604.
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
HLT-NAACL 2003, pages 16?23.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP2007), pages 161?170.
Atsushi Fujita and Satoshi Sato. 2008. A probabilis-
tic model for measuring grammaticality and similar-
ity of automatically generated paraphrases of pred-
icate phrases. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING2008), pages 225?232.
Maayan Geffet and Ido Dagan. 2005. The dis-
tributional inclusion hypotheses and lexical entail-
ment. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL2005), pages 107?114.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing (IWP2003), pages
57?64.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case Frame Compilation from the Web using High-
Performance Computing. In Proceedings of The 5th
International Conference on Language Resources
and Evaluation (LREC-06), pages 1344?1347.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
Fully-Lexicalized Probabilistic Model for Japanese
Syntactic and Case Structure Analysis. In Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the
Association for Computational Linguistics (HLT-
NAACL2006), pages 176?183.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics (COLING-ACL1998), pages
768?774.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of the main confer-
ence on Human Language Technology Conference
of the North American Chapter of the Association
of Computational Linguistics (HLT-NAACL2006),
pages 49?56.
Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of the 2nd international
Conference on Human Language Technology Re-
search (HLT2002), pages 313?318.
Idan Szpector, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2004), pages 41?48.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary template. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics (COLING2008), pages 849?856.
Kentaro Torisawa. 2005. Automatic acquisition of ex-
pressions representing preparation and utilization of
an object. In Proceedings of the Recent Advances
in Natural Language Processing (RANLP05), pages
556?560.
1180
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese cood-
inated sentences and noun-verb co-occurences. In
Proceedings of the Human Language Technology
Conference of the Norh American Chapter of the
ACL (HLT-NAACL2006), pages 57?64.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP2003), pages 81?88.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of the 44th
Annual Meeting of the Association for Computa-
tional Linguistics and 21th InternationalConference
on Computational Linguistics (COLING-ACL2006),
pages 849?856.
1181
Information Retrieval Capable of Visualization and High Precision
Qing Ma1,2 and Kousuke Enomoto1
1Ryukoku University / 2NICT, Japan
qma@math.ryukoku.ac.jp
Masaki Murata and Hitoshi Isahara
NICT, Japan
{murata,isahara}@nict.go.jp
Abstract
We present a neural-network based self-
organizing approach that enables vi-
sualization of the information retrieval
while at the same time improving its
precision. In computer experiments,
two-dimensional documentary maps in
which queries and documents were
mapped in topological order accord-
ing to their similarities were created.
The ranking of the results retrieved us-
ing the maps was better than that of
the results obtained using a conven-
tional TFIDF method. Furthermore, the
precision of the proposed method was
much higher than that of the conven-
tional TFIDF method when the process
was focused on retrieving highly rel-
evant documents, suggesting that the
proposed method might be especially
suited to information retrieval tasks in
which precision is more critical than re-
call.
1 Introduction
Information retrieval (IR) has been studied since
an earlier stage [e.g., (Menzel, 1966)] and sev-
eral kinds of basic retrieval models have been pro-
posed (Salton and Buckley, 1988) and a number
of improved IR systems based on these models
have been developed by adopting various NLP
techniques [e.g., (Evans and Zhai, 1996; Mitra
et al, 1997; Mandara, et al, 1998; Murata, et
al., 2000)]. However, an epoch-making technique
that surpasses the TFIDF weighted vector space
model, the main approach to IR at present, has not
yet been invented and IR is still relatively impre-
cise. There are also challenges presenting a large
number of retrieval results to users in a visual and
intelligible form.
Our aim is to develop a high-precision, visual
IR system that consists of two phases. The first
phase is carried out using conventional IR tech-
niques in which a large number of related docu-
ments are gathered from newspapers or websites
in response to a query. In the second phase the
visualization of the retrieval results and picking
are performed. The visualization process clas-
sifies the query and retrieval results and places
them on a two-dimensional map in topological
order according to the similarity between them.
To improve the precision of the retrieval process,
the picking process involves further selection of a
small number of highly relevant documents based
on the classification results produced by the visu-
alization process.
This paper presents a new approach by using
the self-organizing map (SOM) proposed by Ko-
honen (Kohonen, 1997) for this second IR phase1.
To enable the second phase to be slotted into a
practical IR system as described above, visual-
1There have been a number of studies of SOM on data
mining and visualization [e.g., (Kohonen, et al, 2000)] since
the WEBSOM was developed in 1996. To our knowledge,
however, these works mainly focused on confirming the ca-
pabilities of SOM in the self-organization and/or in the vi-
sualization. In this study, we slot the SOM-based processing
into a practical IR system that enables visualization of the
IR while at the same time improving its precision. The an-
other feature of our study differing from others is that we
performed comparative studies with TFIDF-based IR meth-
ods, the major approach to IR in NLP field.
138
ization and picking should be carried out for a
single query and set of related documents. In
this paper, however, for the purpose of evaluating
the proposed system, correct answer data, consist-
ing of multiple queries and related documents as
used in the 1999 IR contest, IREX (Murata, et
al., 2000), was used. The procedure of the sec-
ond IR-phase in this paper is therefore as follows.
Given a set of queries and related documents, a
documentary map is first automatically created
through self-organization. This map provides vis-
ible and continuous retrieval results in which all
queries and documents are placed in topological
order according to their similarity2. The docu-
mentary map provides users with an easy method
of finding documents related to their queries and
also enables them to see the relationships between
documents with regard to the same query, or even
the relationships between documents across dif-
ferent queries. In addition, the documents related
to a query can be ranked by simply calculating
the Euclidean distances between the points of the
queries and the points of the documents in the
map and then choosing the N closest documents
in ranked order as the retrieval results for each
query. If a small N is set, then the retrieval results
are limited to the most highly relevant documents,
thus improving the retrieval precision.
Computer experiments showed that meaning-
ful two-dimensional documentary maps could be
created; The ranking of the results retrieved us-
ing the map was better than that of the results ob-
tained using a conventional TFIDF method. Fur-
thermore, the precision of the proposed method
was much higher than that of the conventional
TFIDF method when the retrieval process focused
on retrieving the most highly relevant documents,
which indicates that the proposed method might
be particularly useful for picking the best docu-
ments, thus greatly improving the IR precision.
2 Self-organizing documentary maps
and ranking related documents
A SOM can be visualized as a two-dimensional
array of nodes on which a high-dimensional in-
2For a specific query, other queries and documents in the
map are considered to be irrelevant (i.e., documents unre-
lated to the query). This map is therefore equivalent to a
map consisting of one query and related and unrelated docu-
ments, which will be adopted in the practical IR system that
we aim to develop.
put vector can be mapped in an orderly manner
through a learning process. After the learning, a
meaningful nonlinear coordinate system for dif-
ferent input features is created over the network.
This learning process is competitive and unsuper-
vised and is called a self-organizing process.
Self-organizing documentary maps are ones in
which given queries and all related documents
in the collection are mapped in order of similar-
ity, i.e., queries and documents with similar con-
tent are mapped to (or best-matched by) nodes
that are topographically close to one another, and
those with dissimilar content are mapped to nodes
that are topographically far apart. Ranking is the
procedure of ranking documents related to each
query from the map by calculating the Euclidean
distances between the points of the queries and
the points of the documents in the map and choos-
ing the N closest documents as the retrieval result.
2.1 Data
The queries are those used in a dry run of the
1999 IREX contest and the documents relating to
the queries are original Japanese newspaper arti-
cles used in the contest as the correct answers. In
this study, only nouns (including Japanese verbal
nouns) were selected for use.
2.2 Data coding
Suppose we have a set of queries:
Q = {Q i (i = 1, ? ? ? , q)}, (1)
where q is the total number of queries, and a set
of documents:
A = {Ai j (i = 1, ? ? ? , q, j = 1, ? ? ? , ai)},
(2)
where ai is the total number of documents related
to Q i. For simplicity, where there is no need to
distinguish between queries and documents, we
use the same term ?documents? and the same no-
tation Di to represent either a query Q i or a doc-
ument Ai j. That is, we define a new set
D = {Di (i = 1, ? ? ? , d)} = Q
?
A (3)
which includes all queries and documents. Here,
d is the total number of queries and documents,
i.e.,
d = q +
q?
i=1
ai. (4)
139
Each document, Di, can then be defined by the
set of nouns it contains as
Di = {noun(i)1 , w(i)1 , ? ? ? , noun(i)ni , w(i)ni }, (5)
where noun(i)k (k = 1, ? ? ? , ni) are all different
nouns in the document Di and w(i)k is a weight
representing the importance of noun(i)k (k =
1, ? ? ? , ni) in document Di. The weights are com-
puted by their tf or tfidf values. That is,
w(i)j = tf(i)j or tf(i)j idfj . (6)
In the case of using tf, the weights are normalized
such that
w(i)1 + ? ? ?+ w(i)ni = 1. (7)
Also, when using the Japanese thesaurus, Bun-
rui Goi Hyou (The National Institute for Japanese
Language, 1964) (BGH for short), synonymous
nouns in the queries are added to the sets of
nouns from the queries shown in Eq. (5) and their
weights are set to be the same as those of the orig-
inal nouns.
Suppose we have a correlative matrix whose el-
ement dij is some metric of correlation, or a sim-
ilarity distance, between the documents Di and
Dj ; i.e., the smaller the dij , the more similar the
two documents. We can then code document Di
with the elements in the i-th row of the correlative
matrix as
V (Di) = [di1, di2, ? ? ? , did]T . (8)
The V (Di) ? <d is the input to the SOM. There-
fore, the method to compute the similarity dis-
tance dij is the key to creating the maps. Note
that the individual dij of vector V (Di) only re-
flects the relationships between a pair of docu-
ments when they are considered independently.
To establish the relationships between the doc-
ument Di and all other documents, representa-
tions such as vector V (Di) are required. Even
if we have these high-dimensional vectors for
all the documents, it is still difficult to estab-
lish their global relationships. We therefore need
to use an SOM to reveal the relationships be-
tween these high-dimensional vectors and repre-
sent them two-dimensionally. In other words, the
role of the SOM is merely to self-organize vec-
tors; the quality of the maps created depends on
the vectors provided.
In computing the similarity distance dij be-
tween documents, we take two factors into ac-
count: (1) the larger the number of common
nouns in two documents, the more similar the two
documents should be (i.e., the shorter the simi-
larity distance); (2) the distance between any two
queries should be based on their application to the
IR processing; i.e., by considering the procedure
used to rank the documents relating to each query
from the map. For this reason, the document-
similarity distance between queries should be set
to the largest value. To satisfy these two factors,
dij is calculated as follows:
dij =
?
??????
??????
1 if both Di and Dj
are queries
1? |Cij ||Di|+|Dj |?|Cij | not the case mentioned
above and i 6= j
0, if i=j
(9)
where |Di| and |Dj | are values (the numbers of
elements) of sets of documents Di and Dj de-
fined by Eq. (5) and |Cij | is the value of the in-
tersection Cij of the two sets Di and Dj . |Cij |
is therefore some metric of document similarity
(the inverse of the similarity distance dij) between
documents Di and Dj which is normalized by
|Di|+|Dj |?|Cij |. Before describing the methods
for computing them, we first rewrite the definition
of documents given by Eq. (5) for Di and Dj as
follows.
Di = {(c1, w(i)c1 , ? ? ? , cl, w(i)cl ),
(n(i)1 , w(i)1 , ? ? ? , n(i)mi , w(i)mi)}, (10)
and
Dj = {(c1, w(j)c1 , ? ? ? , cl, w(j)cl ),
(n(j)1 , w(j)1 , ? ? ? , n(j)mj , w(j)mj )}, (11)
where ck (k = 1, ? ? ? , l) are the common nouns of
documents Di and Dj and n(i)k (k = 1, ? ? ? ,mi)
and n(j)k (k = 1, ? ? ? ,mj) are nouns of documents
Di and Dj which differ from each other. By com-
paring Eq. (5) and Eqs. (10) and (11), we know
140
that l+mi +mj = ni + nj . Thus, |Di| (or |Dj |)
of Eq. (9) can be calculated as follows.
|Di| =
l?
k=1
w(i)ck +
mi?
k=1
w(i)k . (12)
For calculating |Cij |, on the other hand, since the
weights (of either common or different nouns)
generally differ between two documents, we de-
vised four methods which are expressed as fol-
lows.
Method A:
|Cij | =
l?
k=1
max(w(i)ck , w(j)ck ). (13)
Method B:
|Cij | =
l?
k=1
w(i)ck + w(j)ck
2 . (14)
Method C:
|Cij | =
?
?????
?????
?l
k=1 max(w(i)ck , w(j)ck ) if one is a query
and the other
is a document
?l
k=1
w(i)ck+w
(j)
ck
2 . if both are
documents
(15)
Method D:
|Cij | =
?
?????
?????
?l
k=1 max(w(i)ck , w(j)ck ) if one is a query
and the other
is a document?l
k=1 min(w(i)ck , w(j)ck ). if both are
documents
(16)
Note that we need not consider the case where
both are queries for calculating |Cij | because this
has been considered independently as shown by
Eq. (9).
3 Experimental Results
3.1 Data
Six queries Q i (i = 1, ? ? ? , q, q = 6) and 433
documents Ai j (i = 1, ? ? ? , q, q = 6, j =
1, ? ? ? , ai and
?q
i=1 ai = 433) used in the dry run
Table 1: Distribution of documents used in the
experiments
a1 a2 a3 a4 a5 a6
?6
i=1 ai
80 89 42 108 49 65 433
of the 1999 IREX contest were used for our ex-
periments. The distribution of these documents,
i.e., the number ai (i = 1, ? ? ? , q, q = 6) of docu-
ments related to each query, is shown in Table 1.
It should be noted that since the proposed IR
approach will be slotted into a practical IR sys-
tem in the second phase in which a small number
(say below 1,000, or even below 500) of the re-
lated documents should have been collected, this
experimental scale is definitely a practical one.
3.2 SOM
We used a SOM of a 40?40 two-dimensional ar-
ray. Since the total number d of queries and doc-
uments to be mapped was 439, i.e., d = q +?6
i=1 ai = 439, the number of dimensions of in-
put n was 439. In the ordering phase, the number
of learning steps T was set at 10,000, the initial
value of the learning rate ?(0) at 0.1, and the ini-
tial radius of the neighborhood ?(0) at 30. In the
fine adjustment phase, T was set at 15,000, ?(0)
at 0.01, and ?(0) at 5. The initial reference vec-
tors mi(0) consisted of random values between 0
and 1.0.
3.3 Results
We first performed a preliminary experiment and
analysis to determine which of the four methods
was the optimal one for calculating |Cij | shown
in Eqs. (13)-(16). Table 2 shows the IR precision,
i.e., the precision of the ranking results obtained
from the self-organized documentary maps cre-
ated using the four methods. The IR precision was
calculated by follows.
P = 1q
q?
i=1
#related to Q i in the retrieved ai documents
ai ,
(17)
where q is the total number of queries, # means
number, and ai is the total number of documents
related to Q i as shown in Table 1.
In the case of using tf values as weights of
nouns, method B obviously did not work. Al-
141
Table 2: IR precision for the four methods for cal-
culating |Cij |
Weight Method
A
Method
B
Method
C
Method
D
tf 0.33 0.20 0.41 0.45
tfidf 0.85 0.76 0.91 0.78
though the similarity between queries was manda-
torily set to the largest value, all six queries were
mapped in almost the same position, thus produc-
ing the poorest result. We consider the reason for
this was as follows. In general, the number of
words in a query is much smaller than the num-
ber of words in the documents, and the number
of queries is much smaller than the number of
documents collected. As described in section 2,
each query was defined by a vector consisting of
all similarities between the query and five other
queries and all documents in the collection. We
think that using the average weights of words ap-
pearing in the queries and documents to calculate
the similarities between queries and documents,
as in method B, tends to produce similar vectors
for the queries. All of these query vectors are then
mapped to almost the same position. With coding
method A, because the larger of the two weights
of a query and a document is used, the same prob-
lem could also arise in practice. There were no es-
sential differences between coding methods C and
D, which were almost equally precise. Neither of
these methods have the shortcomings described
above for methods A and B. However, when tfidf
values were used as the weights of the nouns, even
methods A and B worked quite well. Therefore, if
we use tfidf values as the weights of the nouns, we
may use either of the four methods. Based on this
analysis and the preliminary experimental result
that method C and D had highest precisions in the
cases of using tf and tfidf values as weights of the
nouns, respectively, we used methods C and D for
calculating |Cij | in all the remaining experiments.
Table 3 shows the IR precision obtained using
various methods. From this table we can see that
the proposed method in the case of SOM (w=tfidf,
C), i.e., using method C for calculating |Cij |, us-
ing tfidf values as the weights of nouns, and not
using the Japanese thesaurus (BGH), in the case
of SOM (w=tfidf, D), i.e., using method D, us-
ing tfidf values, and not using the BGH, and in
Table 3: IR precision obtained using various
methods
TFIDF TFIDF
(BGH)
SOM
(w=tf,
D)
SOM
(w=
tfidf,
C)
SOM
(w=
tfidf,
C,
BGH)
SOM
(w=
tfidf,
D)
SOM
(w=
tfidf,
D,
BGH)
0.67 0.75 0.45 0.91 0.77 0.78 0.73
Table 4: IR precision for top N related documents
N TFIDF TFIDF
(BGH)
SOM
(w=tf,
D)
SOM
(w=
tfidf,
C)
SOM
(w=
tfidf,
C,
BGH)
SOM
(w=
tfidf,
D)
SOM
(w=
tfidf,
D,
BGH)
10 0.83 0.88 0.75 1.0 0.97 1.0 0.97
20 0.79 0.86 0.68 0.99 0.95 0.98 0.97
30 0.73 0.84 0.62 0.99 0.94 0.97 0.91
40 0.71 0.82 0.58 0.98 0.90 0.97 0.87
the case of SOM (w=tfidf, C, BGH), i.e., using
method C, using tfidf values, and using the BGH
produced the highest, second highest, and third
highest precision, respectively, of all the methods
including the conventional TFIDF method. When
the BGH was used, however, the IR precision of
the proposed method dropped inversely, whereas
that of the conventional TFIDF improved. The
lower precision of the proposed method when us-
ing BGH might be due to the calculation of the
denominator of Eq. (9); this will be investigated
in future study.
Table 4 shows the IR precision obtained using
various methods when the retrieval process is fo-
cused on the top N related documents. From this
table we can see that the IR precision of the pro-
posed method, no matter whether the BGH was
used or not, or whether method C or D was used
for calculating |Cij |, was much higher than that
of the conventional TFIDF method when the pro-
cess was focused on retrieving the most relevant
documents. This result demonstrated that the pro-
posed method might be especially useful for pick-
ing highly relevant documents, thus greatly im-
proving the precision of IR.
Figure 1 shows the left-top area of a self-
organized documentary map obtained using the
proposed method in the case of SOM (w=tfidf,
D)3. From this map, we can see that query Q 4
3Note that the map obtained using the proposed method
in the case of SOM (w=tfidf, C), which had the highest IR
precision, was better than this.
142
Figure 1: Left-top area of self-organized docu-
mentary map
and its related documents A4 ? (where * denotes
an Arabic numeral), Q 2 and its related docu-
ments A2 ? were mapped in positions near each
other. Similar results were obtained for the other
queries which were not mapped in the area of the
figure. This map provides visible and continu-
ous retrieval results in which all queries and docu-
ments are placed in topological order according to
their similarities. The map provides an easy way
of finding documents related to queries and also
shows the relationships between documents with
regard to the same query and even the relation-
ships between documents across different queries.
Finally, it should be noted that each map that
consists of 400 to 500 documents was obtained in
10 minutes by using a personal computer with a
3GHZ CPU of Pentium 4.
4 Conclusion
This paper described a neural-network based self-
organizing approach that enables information re-
trieval to be visualized while improving its preci-
sion. This approach has a practical use by slot-
ting it into a practical IR system as the second-
phase processor. Computer experiments of practi-
cal scale showed that two-dimensional documen-
tary maps in which queries and documents are
mapped in topological order according to their
similarities can be created and that the ranking
of the results retrieved using the created maps
is better than that produced using a conventional
TFIDF method. Furthermore, the precision of the
proposed method was much higher than that of
the conventional TFIDF method when the pro-
cess was focused on retrieving the most relevant
documents, suggesting that the proposed method
might be especially suited to information retrieval
tasks in which precision is more important than
recall.
In future work, we first plan to re-confirm the
effectiveness of using the BGH and to further im-
prove the IR accuracy of the proposed method.
We will then begin developing a practical IR sys-
tem capable of visualization and high precision
using a two-phase IR procedure. In the first phase,
a large number of related documents are gath-
ered from newspapers or websites in response to
a query presented using conventional IR; the sec-
ond phase involves visualization of the retrieval
results and picking the most relevant results.
References
H. Menzel. 1966. Information needs and uses in science
and technology. Annual Review of Information Science
and Technology, 1, pp. 41-69.
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing & Management, 24(5), pp. 513-523.
D. A. Evans and C. Zhai. 1996. Noun-phrase analysis in
unrestricted text for information retrieval. ACL?96, pp.
17-24.
M. Mitra, C. Buckley, A. Singhal, and C. Cardie, C.
1997. An analysis of statistical and syntactic phrases.
RIAO?97, pp. 200-214.
R. Mandara, T. Tokunana, and H. Tanaka 1998. The use
of WordNet in information retrieval. COLING-ACL?98
Workshop: Usage of WordNet in Natural Language
Processing Systems, pp. 31-37.
M. Murata, Q. Ma, K. Uchimoto, H. Ozaku, M. Uchiyama,
and H. Hitoshi 2000. Japanese probabilistic informa-
tion retrieval using location and category information.
IRAL?2000.
T. Kohonen 1997. Self-organizing maps. Springer, 2nd
Edition.
T. Kohonen, S. Kaski, K. Lagus, J. Salojarrvi, J. Honkela,
V. Paatero, and A. Saarela. 2000. Self Organization of
a Massive Document Collection. IEEE Trans. Neural
Networks, 11, 3, pp. 574-585.
The National Institute for Japanese Language. 1964. Bunrui
Goi Hyou (Japanese Thesaurus). Dainippon-tosho.
143
Trend Survey on Japanese Natural Language Processing Studies
over the Last Decade
Masaki Murata?, Koji Ichii?, Qing Ma?,?, Tamotsu Shirado?,
Toshiyuki Kanamaru?,?, and Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{murata,qma,shirado,kanamaru,isahara}@nict.go.jp
?Port and Airport Research Institute
Nagase 3-1-1, Yokosuka, Kanagawa 239-0826, Japan, ichii@pari.go.jp
?Ryukoku University, Otsu 520-2194, Japan, qma@math.ryukoku.ac.jp
?Kyoto University, Yoshida-Nihonmatsu, Sakyo, Kyoto 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Abstract
Using natural language processing, we
carried out a trend survey on Japanese
natural language processing studies that
have been done over the last ten years.
We determined the changes in the num-
ber of papers published for each re-
search organization and on each re-
search area as well as the relationship
between research organizations and re-
search areas. This paper is useful
for both recognizing trends in Japanese
NLP and constructing a method of sup-
porting trend surveys using NLP.
1 Introduction
We conducted a trend survey on Japanese nat-
ural language processing studies that have been
done over the last ten years. We used biblio-
graphic information from journal papers and an-
nual conference papers of the Association for
Natural Language Processing, Japan (The Asso-
ciation for Natural Language Processing, 1995-
2004; The Association for Natural Language Pro-
cessing, 1994-2003). Just ten years have passed
since the association was established. Therefore,
we can use the bibliographic information from the
past ten years. In this study, we investigated what
kinds of studies have been presented in journal
papers and annual conference papers on the Asso-
ciation for Natural Language Processing, Japan.
We first digitized documents listed in the bibli-
ographic information and then extracted various
pieces of useful information for the trend survey.
Figure 1: Change in the number of papers
We also examined the changes in the number of
papers put up by each Japanese research orga-
nization and the changes in the number of pa-
pers written on specific research areas. More-
over, we examined the relationship between each
Japanese research organization and each research
area. This study is useful for trend surveys of
studies performed by members of in the Associa-
tion for Natural Language Processing, Japan.
2 Trend survey on NLP research studies
We show the changes in the number of journal
papers and conference papers in Figure 1. Jour-
nal papers are reviewed, but conference papers are
not reviewed in the association. In comparing the
journal papers and conference papers, we found
that the number of conference papers was much
larger than that of journal papers. We also found
that although both types of papers decreased in
number at some point, they both demonstrate an
upward trend.
Conference papers have a temporal peak in the
fourth year and a temporal drop in the sixth year,
250
Figure 2: Change in the number of journal papers
by each research organization (The two numbers in
the parentheses indicate the total number of papers
and the average value of published years.)
while journal papers have a peak in the sixth year
and a drop in the eighth year. The temporal peak
and drop of the journal papers occurred just two
years after the peak and drop of the conference
papers. We presume this is because journal papers
need more time for reviewing and publishing, and
because journal papers are presented later than
conference papers for studies performed at the
same time.
3 Trend survey on research
organizations
Next, we investigated the change in the number
of papers put out by each research organization.
The results are represented in contour in Figures
2 and 3. The height in contour (the depth of a
black color) indicates the number of papers. We
calculated the average (we call it average value)
of the average, the mode, and the median of the
published years by using the data of the number
of papers performed by each research organiza-
tion. In the figures, each research organization is
listed in ascending order of the average value. We
added the total number of papers and the average
value to each research organization in the figures.
Therefore, research organizations that had many
papers in the earlier years are displayed higher
on the list, while research organizations that had
Figure 3: Change in the number of conference pa-
pers by each research organization
many papers in the later years are displayed lower.
Here, we displayed only research organizations
that had many total papers. If a research orga-
nization?s name was changed during the ten-year
period, we used the name that had the most usage
on published papers for displaying it.1
From these figures, we can see that ATR and
CRL (NICT) put out many journal papers, and
NTT, ATR, Tokyo Institute of Technology, CRL,
and the University of Tokyo put out many confer-
ence papers. We also found that while NTT and
ATR had many papers in the earlier years, CRL
and the Univ. of Tokyo had many papers in the
later years. We can expect that because CRL and
the Univ. of Tokyo demonstrate an upward ten-
dency, their quantity of papers will continue to in-
crease in the future. Using these figures, we can
see very easily in which reference year each re-
search organization put out many papers.
4 Trend survey on research areas
Next, we investigated the change in the number
of papers in each research area. The results are in
Figures 4, 5, and 6. (Because the volume of data
for conference papers was large, it was divided
into two figures.). For journal papers, the height
1When we counted the frequency of a research organiza-
tion whose name was changed, we used all the names of it
including old and new names.
251
Figure 4: Change in the number of journal papers
in each research area
in contour indicates the number of papers. For
conference papers, the height in contour indicates
the base two logarithm of the number of papers
added by one. Using the same method as that de-
scribed above, we calculated the average of the
average, mode, and median of the years papers
were published using the data of the number of
papers in each research area. In the figures, each
research area is displayed in ascending order of
the average value. We added the total number of
papers and the average value to each research area
in the figures. Here, we divided the title of each
paper into words using ChaSen software (Mat-
sumoto et al, 1999), and we treaded each word as
a research area. A paper with a particular word in
Figure 5: Change in the number of conference pa-
pers in each research area (part I)
its title was categorized in the research area indi-
cated by the word. Wemanually eliminated words
that were not indicative of a research area, for ex-
ample, ?teki? (of) and ?kenkyu? (study).
From these figures, it is clear that the research
areas of ?Japanese? and ?analysis? were studied
in an especially large number of papers. We
also found that for journal papers, because the
research areas of ?verb?, ?noun?, ?disambigua-
tion?, ?probability?, ?corpus?, and ?polysemic?
were displayed higher on the list, these areas were
studied thoroughly in the earlier years. Likewise,
we found that the research areas of ?morphol-
ogy?, ?dependency?, ?dialogue?, and ?speech?
were studied thoroughly in the sixth year and the
252
Figure 6: Change in the number of conference pa-
pers in each research area (part II)
research areas of ?summarization?, ?retrieval?,
?translation? and so on were studied well in the
later years. Special journal issues on ?summariza-
tion? were published in the sixth and ninth years,
so the research area of ?summarization? was rep-
resented in many papers in those years. We can
expect that because the research area of ?transla-
tion? demonstrates an upward tendency, the num-
ber of papers on this topic will continue to in-
crease in the future.
In terms of conference papers, we found that
the research areas of ?bilingual?, ?morphology?,
?probability?, ?dictionary?, ?statistics?, and so on
were studied well in the earlier years. In the lower
part of the figures, such research areas as ?re-
Figure 7: Change in the number of conference pa-
pers at each research organization in the research
area of ?translation?
Figure 8: Change in the number of conference pa-
pers in each research area in the research area of
?translation?
trieval?, ?summarization?, ?question? and ?para-
phrase? are found. Thus, we can see that these
research areas were studied thoroughly in recent
years. We can see very easily in which reference
years each research area was studied using these
figures.
5 Trend survey using part of data
Although we have focused on using all the data
in the trend survey so far, we can narrow down
the survey by looking only at a certain part of
the data. For example, when we want to exam-
253
Figure 9: Relationship between research organizations and research areas in journal papers (The name
of each research organization is given a ??? symbol.)
ine a trend survey on translation in more detail,
all we have to do is to extract papers on transla-
tion and use them for a trend survey. We carried
out a trend survey on machine translation in this
manner. We first extracted papers whose titles in-
cluded the word ?translation? and then performed
the same investigations as in Sections 3 and 4.
The results are in Figures 7 and 8. The height in
contour (the depth of a color) indicates the num-
ber of papers. From Figure 7, we can see that
NTT had many papers in the earlier years, and
ATR had many papers in later years. From Figure
7, we can also see that studies on translation of-
ten dealt with specific topics such as ?semantics?,
?knowledge? and ?dictionary? in earlier years and
?support?, ?example?, and ?retrieval? in more re-
cent years.
6 Relationship between research
organizations and research topics
Finally, we investigated the various research ar-
eas that research organizations studied more fre-
quently during the ten-year period. Here, we
show only the results for journal papers. We used
the same method as in the previous sections for
extracting research organizations and research ar-
eas from the data. We counted the cooccurrent
frequency of each research organization and each
research area. We then constructed a cross table
in this manner and then performed the dual scal-
ing method (Weller and Romney, 1990; Ueda et
al., 2003). The result is depicted in Figure 9. The
dual scaling method displays the relationship be-
tween research organizations and research areas.
In Figure 9, ?translation? appears in the lower
left quadrant, ?learning? appears in the lower
right quadrant, ?statistics? and ?retrieval? appear
in the upper right quadrant, and ?noun? and ?sen-
tence? appear in the upper left quadrant. In the
vicinity around these words, the research areas
and organizations relating to them appear. For ex-
ample, in the upper right quadrant, Hitachi and
University of Tokushima appear near ?statistics?
and ?retrieval?, which were frequent study topics
for them. Similarly, ?summarization? appears in
the near upper right area of the source origin and
is surrounded by JAIST, Toyohashi University of
Technology, and Tokyo Institute of Technology.,
indicating it was a frequent topic of study at those
institutions. We can easily see which research
topics were primarily studied by each organiza-
tion using this figure.
Also in Figure 9, research areas on numeri-
cals such as ?probability? and ?learning? appear
254
on the right side. Therefore, we can interpret the
figure as depicting quantitative research topics on
the right side and qualitative research topics on
the left side. Research areas using complicated
processing such as ?learning? and ?translation?
appear in the lower area and research areas deal-
ing with theory such as ?probability?, ?grammar?,
?sentence?, and ?noun? appear in the upper area.
Therefore, we can interpret the figure as depict-
ing theoretical research topics in the upper area
and research topics using complicated processing
in the lower area.
7 Conclusion
In this paper, we described a trend survey carried
out on Japanese natural language processing stud-
ies done over the last ten years. We were able to
investigate trend surveys on research areas very
easily by treating divided words in titles by a mor-
phological analyzer as the indications of research
areas. We displayed the changes in the number of
papers put out by each research organization and
written on specific research topics. We also dis-
played the relationship between research organi-
zations and research areas using the dual scaling
method. The simple methods we used that are de-
scribed here made it possible to show many useful
results.
This paper has the following two significant ef-
fects:
 This paper explained a trend survey on
Japanese natural language processing. By
reading it, we can understand the trends in
research on Japanese natural language pro-
cessing. For example, we can find out
which research areas were studied more of-
ten and we can see which research organiza-
tions were involved in studying natural lan-
guage processing. We can also see which re-
search organization studied a particular re-
search area most often over the ten-year pe-
riod.
 We used natural language processing to
carry out the trend survey described here.
For example, we automatically detected the
indication of a research area from words
used in titles by using a morphological ana-
lyzer. In addition, we displayed words that
were extracted by the morphological ana-
lyzer in several ways to display the results
of the trend survey effectively. The methods
used in this paper would be useful in other
trend surveys.
In short, this paper is useful for recognizing trends
in Japanese NLP and for constructing methods of
supporting trend surveys using NLP.
In the future, we would like to perform an in-
ternational trend survey on natural language pro-
cessing using international conference and jour-
nal papers such as IJCNLP, ACL, and the Journal
of Computational Linguistics. We would also like
to do trend surveys on other topics such as AI, bi-
ology, politics, and sociology.
The kinds of investigations we did can easily be
altered to do many other kinds of investigations
as well. For example, we can use the dual scal-
ing method by investigating the relationship be-
tween the reference years and the research organi-
zations/areas. We can also use the representation
in contour for the relationship between research
organizations and research areas. Although we
showed the data in ascending order of the aver-
age value of the published years, we could show
the data in different order, for example, the or-
der of the total number of papers or the order of
the location, i.e., showing similar research orga-
nizations/areas that are located near each other by
clustering research organizations/areas using their
cooccurrent words. We would like to continue
to study these kinds of support methods for trend
surveys in the future.
References
The Association for Natural Language Processing. 1994-
2003. Journal of Natural Language Processing.
The Association for Natural Language Processing. 1995-
2004. Proceedings of the Annual Meeting of The Associ-
ation for Natural Language Processing.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, and Masayuki Asahara.
1999. Japanese morphological analysis system ChaSen
version 2.0 manual 2nd edition.
Taichiro Ueda, Masao Karita, and Kazue Honda. 2003. Jis-
sen Workshop Excel Tettei Katsuyou Tahenryou Kaiseki.
Shuuwa System. (in Japanese).
Susan C. Weller and A. Kimball Romney. 1990. Metric
Scaling : Correspondence Analysis (Quantitative Appli-
cations in the Social Sciences). SAGE Publications.
255
Obtaining Japanese Lexical Units for Semantic Frames
from Berkeley FrameNet Using a Bilingual Corpus
Toshiyuki Kanamaru
Kyoto University
Yoshida Nihonmatsu-cho, Sakyo-ku
Kyoto, 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Masaki Murata Kow Kuroda Hitoshi Isahara
National Institute of Information and
Communications Technology (NICT)
3-5 Hikaridai, Seikacho, Sorakugun
Kyoto, 619-0289, Japan
{murata,kuroda,isahara}@nict.go.jp
Abstract
An attempt was made to semi-automatically ob-
tain ?lexical units? (LUs) for Japanese from
the English LUs defined in the semantic frame
database provided by Berkeley FrameNet (BFN)
using an English-Japanese bilingual corpus.
This task was a prerequisite to building a com-
plete database of semantic frames for Japanese.
In the task, a Japanese word is first translated
into an English word or phrase, E. E is one
of the lexical units that evoked a particular se-
mantic frame, F , in the BFN database. When
other lexical units of F are translated back into
Japanese, this defines a candidate set of F for
the lexical units of F in Japanese. The via-
bility of the proposed method was tested on a
Japanese verb (X-ga Y -wo) osou (roughly mean-
ing ?X attack(s) Y ,? ?X hit(s) Y ,? ?X surprise(s)
Y ? in English, showing that it is a relatively pol-
ysemous word). The resulting translation was
compared to semantic descriptions provided by
IPAL and Nihongo Goi-Taikei (A Japanese Lex-
icon), two well-known language resources for
Japanese, and also by the Frame Oriented Con-
cept Analysis of Language (FOCAL). The com-
parison revealed that FOCAL, BFN, Goi Taikei,
and IPAL provided finer-grained descriptions in
this specific order.
1 Introduction
Making use of deep semantics in information pro-
cessing is one of the major problems confronting
today?s NLP community. More and more NLP
researchers are realizing that they need seman-
tic/lexical resources that go beyond such ones as
WordNet (Fellbaum, 1998) that only specify hier-
archical semantic relationships. One of the cru-
cial reasons for this is that raw linguistic data
embodies semantic associations that are difficult
to capture in terms of such hierarchical relation-
ships, one of which is the so-called ?semantic
field? effect, a class of associative relationships
among words (or concepts). To deal with these
issues, deeper semantics are needed with descrip-
tions that incorporate ontological inferences. Let
us assume that X attacked Y is to be interpreted.1
This is a complex situation. In interpreting The
man attacked a bank, it may be necessary to spec-
ify (by inference) that the subject used a weapon
(e.g., a gun) and his purpose was to obtain money
(illegally), whereas in interpreting The wolf at-
tacked a flock of sheep, it may be necessary to
specify that the subject never used a weapon and
its purpose was to eat one or two individual sheep
(rather than the entire flock) after killing them.
Relevant inferences are clearly situation-based, or
?case-based? in the sense of Case-based Reason-
ing (Kolodner, 1993), and difficult to specify in
terms of the lexical semantic descriptions avail-
able in resources such as WordNet (Fellbaum,
1998) which don?t specify associative relation-
ships among concepts, including the relationships
between ROBBER (e.g., a man) and WAREHOUSE
OF VALUABLES (e.g., a bank, museum, jewelry
shop), and the one between a PREDATOR (e.g., a
wolf) and its PREY (e.g., sheep, rabbit). Thus, the
NLP community has a critical need for resources
that encode this kind of information.
Along with PropBank (Kingsbury and Palmer,
2002; Ellsworth et al, 2004), Berkeley FrameNet
1One of the anonymous reviewers told us that it was un-
clear how ontological inferences of this sort are related to
BFN?s frame definitions. The question boils down to the
question of definition, i.e., what kind of information we need
to define semantic frames to encode, and as we will see later,
this is exactly the question addressed by FOCAL claiming
that BFN frames are too coarse-grained to be used as an ef-
fective knowledge-base for ontological inferences.
11
(BFN) (Baker et al, 1998) is an ongoing research
project that is attempting to meet the demand for
resources that encode deeper lexical semantics by
providing a semantic frame lexicon (sometimes
called the ?FrameNet?) and a corpus annotated
for semantic information encoded in terms of se-
mantic frames.
Thus far, BFN has produced ?a lexical database
that currently contains more than 8,900 lexical
units, more than 6,100 of which are fully anno-
tated, in more than 625 semantic frames, exem-
plified in more than 135,000 annotated sentences?
(cited from the FrameNet web page). Other
ongoing projects, i.e., the German FrameNet
or ?SALSA? (Erk et al, 2003), the Spanish
FrameNet (Subirats and Petruck, 2003), and the
Japanese FrameNet (Ohara et al, 2003), are try-
ing to build lexical resources that are compatible
with the BFN, but for Japanese at least, no data
has been released in a usable form, except for a
few annotation examples for verbs of motion.
In sum, no useful resource exists for frame-
based description/analysis of Japanese. This is
one of the reasons that we attempted the task in
this paper, along with our efforts to assess the use-
fulness of the database provided by BFN.
The anonymous reviewers of our paper pointed
out that there have been some similar projects
and other methodologies that have tried to trans-
late BFN into other languages automatically, such
as BiFrameNet (Chen and Fung, 2004) and Ro-
mance FrameNet2, and that it would have been
better to include the comparison against them.
BiFrameNet presented an automatic approach
to constructing a bilingual semantic network us-
ing the Chinese HowNet, which is a Chinese
ontology. While it is an interesting approach,
we have not compared their results with ours,
mainly because they seem to have used differ-
ent resources and had somewhat different goals,
along with the space consideration.
No papers are released, let alne being avail-
able to us, related to the Romance FrameNet
project for the time being. We couldn?t help
putting a comparison with it on hold.3
2http://ic2.epfl.ch/?pallotta/rfn/
3One of the anonymous reviewers criticized us for failing
to mention Romance FrameNet project in our paper; it is just
unreasonable. The project was announced on June 1 on the
2 Proposed Procedure
We used a bilingual corpus (Utiyama and Isahara,
2003) to examine which semantic frames of BFN
contained LUs relevant to the Japanese verb osou.
JFN, for example, used a mono-lingual corpus to
construct the semantic frames. In cases like this,
the construction might be inefficient because they
have to construct all semantic frames by them-
selves. But this affects on the reliability of the
frames identified and described. This risk of arbi-
trary description can be reduced by using a bilin-
gual corpus, if it is of high-quality.
2.1 Identifying English equivalents of ?osou?
We chose Japanese-English alignments from the
bilingual corpus in which the Japanese text con-
tained osou, i.e., the target verb. We obtained 135
alignments from the corpus.
The bilingual corpus is consists of two subcor-
pra. One subcorpus is made of one-to-one align-
ments. Another is of one-to-many alignments. In
the latter, one Japanese sentence is aligned with
several English sentences.
In the first case, it was straightforward to spec-
ify an English word or phrase that translated the
target verb, osou. In the second case, however, it
is not. So, we singled out an English sentence that
corresponds to a Japanese sentence that contained
osou. In this process, the identification of osou?s
English translations was done manually.
After this procedure, the following five verbs
were identified as English translations of osou:
assault, attack, hit, pound, and strike4.
2.2 Identifying relevant semantic frames
Based on these five verbs, we extracted seman-
tic frames using FrameSQL (Sato, 2003). Seman-
tic frames with LUs that included any of the five
verbs were chosen from the BFN semantic frame
database (referred to here as BFN).
Corpora Mailing List, just one week before the submission
deadline. This means that we had little chance to know about
the project unless we were ?insiders.?
4There were a few other verbs or constructions that
served as English translations of osou in the alignments: for
example, besiege, engulf, feel pain, occur, hurt, kill, rob,
shoot, stab, suffer, wreak on were used as its translations.
But we filtered out those less frequent items (whose fre-
quency is less than 3) for purposes of simplicity.
12
Based on Frame Semantics (Fillmore, 1982),
BFN posits that a semantic frame is an organi-
zation of ?semantic roles,? which BFN terms as
?Frame Elements? (FEs). Usually, LUs are in-
stantiations or lexical realizations of FEs. Thus,
an LU in a frame, F , is a word, or phrase, that, ac-
cording to the assumptions of Frame Semantics,
?evokes? frame F . The definition of the ?Attack?
frame in the BFN database is used in Figure 1to
illustrate the procedure. As indicated, assault, at-
tack and strike are listed as LUs of the ?Attack?
frame.
After manually examining all the semantic
frames thus obtained, the five BFN frames were
recognized as relevant to the various senses of the
target word osou: 1. ?Attack?; 2. ?Cause harm?
3. ?Experience bodily harm? 4. ?Cause impact?
5. ?Impact?
Semantic frames in the BFN database are sup-
posedly related to one another. There are vari-
ous relationships, some of which are sometimes
encoded by establishing explicit ?frame-to-frame
relations? (such as ?is used? relation) between
two frames. Using this information, we obtained
the following relationships between the five
frames: 1. ?Attack?; 2. ?Cause harm?, is used:
?Experience bodily harm?; 3. ?Cause impact?,
uses: ?Impact?
2.3 Identifying relevant frame-evoking LUs
in English
Each semantic frame has a number of FEs, each of
which has lexical realizations, which called LUs.
In the work reported here, only verbal LUs were
selected as relevant from the English LUs made
available in the BFN database.5 Admittedly, there
5 On this point, we recognize a certain kind of discrep-
ancy between the theory and the practice in the BFN frame-
work. If a LU is, according to its defintion, a lexical realiza-
tion of a certain FE of a certain frame, more nominals should
be identified and listed as LUs. For example, in Jack or-
dered a hamburger at McDonald?s, hamburger is a noun that
evokes the ?Cooking creation? frame. While the ?Selling?
frame is evoked by order.v, this means that, according the
definition of LU, hamburger.n needs to be identified as an
LU of the ?Cooking creation? frame; more specifically, it is
an LU that instantiates the ?Food? FE of the frame. It is ob-
vious that the QUALIA STRUCTURE (Pustejovsky, 1995) of
hamburger.n contains information of this sort. We suspect
that this aspect of ?frame-evocation by nominals? does not
seem to be properly recognized and coded, and that BFN?s
current practice of mostly identifying predicates as LUs is
somewhat misleading, if we could say so, because it con-
are a few nominal LUs in certain frames in the
BFN, but we ignored them because they found
them to be less relevant to our specific task.
After identifying all the relevant LUs for the
three frames above, we obtained all the English
verbs that translated the senses of the target word
osou identified in terms of Frame Semantics.
For example, the relevant LUs for the ?Attack?
frame are the following verbs: ambush, assault,
attack, charge, invade, jump, lay, set, storm, and
strike
As was the case with the ?Attack? frame, we
extracted the relevant LUs for the ?Cause harm?
and ?Cause impact? frames. We manually
merged the extracted LUs, and obtained 93 ver-
bal LUs relevant to the Japanese verb osou.
2.4 Obtaining LU candidates for Japanese
FEs
Table 1: 15 most frequently occurring nouns
Noun Freq.
jiken (incident) 39
boukou (criminal assault) 32
josei (woman) 28
taiho (arrest) 23
hikoku (accused, defendant) 21
yougi (charge, suspicion) 20
kougeki (attack) 20
shounen (boy) 14
tero (terrorism) 14
shougai (injury) 13
higai (damage, harm) 12
kenkei (prefectural police department) 12
manshon (apartment) 12
butai (military unit) 10
fujo (girl and woman) 10
Using the bilingual corpus again, we gathered
alignments that had English texts containing the
English LUs specified in the way previously de-
scribed. We obtained 262 alignments. This proce-
dure defined a set of Japanese sentences contain-
ing Japanese words or phrases that were natural
translations of the LUs in the BFN.
ceals the fact that there can be, and actually are, many kinds
of frame-evoking effects. BFN has been concentrating on
identifying LUs for ?governors,? not LUs for the entire set of
FEs, for whatever reason. In this respect, it is crucial to note
that not all frame-evokers are frame-governors: hamburger.n
clearly evokes the ?Cooking creation? frame, but there the
noun does not govern the ?Cooking creation? frame. Ar-
guably, it is unreasonable and even gratuitous to posit the
?Hamburger? frame to make hamburger.n a governor.
13
Attack
Definition:
An Assailant physically attacks a Victim (which is usually but not always sentient), causing or intending to cause the Victim
physical injury. The Weapon used by the Assailant may also be mentioned, in addition to the usual Place, Time, Purpose, and
Reason. Sometimes a location is used metonymically to stand for the Assailant or the Victim, and in such cases the Place FE
will be annotated on a second FE layer.
As soon as he stepped out of the bar he was SET upon by four men in ski-masks.
Is he INVADING Iraq just to cover other shortcomings?
Then Jon-O?s forces AMBUSHED them on the left flank from a line of low hills.
FEs:
Core:
Assailant [Asl] The person (or other self-directed entity) that is attempting physical harm to the Victim.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Victim [Vic] This FE is the being or entity that is injured by the Assailant?s attack.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Lexical Units
ambush.n, ambush.v, assail.v, assault.n, assault.v, attack.n, attack.v, charge.n, charge.v, fall.v, incursion.n, invade.v, inva-
sion.n, jump.v, lay ((into)).v, offensive.n, onset.n, onslaught.n, raid.v, set.v, storm. v, strike.n, strike.v
Created by infinity on Fri Nov 22 14:05:22 PST 2002
Figure 1: BFN definition of ?Attack? frame (partial)
It should be noted, however, that there is no es-
tablished method of recognizing these units au-
tomatically; they are part of a text without being
marked as such. To solve this problem, we hy-
pothesized that their statistical properties in the
texts could be used to pick them up; i.e., we as-
sumed that these LUs were relatively specific to
these types of texts and would appear at higher
frequencies than usual in the collected text.
We collected nouns with higher frequencies un-
der this assumption using a KH Coder 6.
The results were sorted according to the parts
of speech. The high-frequency nouns thus ob-
tained are listed in Table 1.
This provided little information about the se-
mantic classification of the nouns because there
was no indication of the LUs that they instan-
tiated. Semantic groupings are latent, how-
ever. This meant that we were able to ?clus-
ter? the nouns based on certain generic proper-
ties to obtain an initial approximation of these
groupings. We used a tool called msort (stand-
ing for ?meaning sort?) (Murata et al, 2001) to
establish generic, domain-independent semantic
6The KH Coder is a free analyzer that uses a combination
of ChaSen (Matsumoto et al, 1999) and MySQL. This is
freely available at http://khc.sourceforge.net/.
groupings.78
Nouns occurring more than three times were
obtained, as shown below:9
human dansei (man), danshi (boy), josei (woman), fujo
(woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), . . .
organization kokka (country), gaikoku (foreign country),
kokusai (international), sekai (world), . . .
product yakubutsu (drug), manshon (apartment), heya
(room), keesu (case), naifu (knife), shoujuu (rifle), . . .
7msort sorts a given set of nouns based on their encod-
ings in a Japanese thesaurus Bunrui Goi-hyou (National Lan-
guage Research Institute, 1964).
8One of the anonymous reviewers commented on this
?domain-independence? with a critical tone, questioning the
validity of the proposed method. This evaluation is clearly
based on a misunderstanding: the semantic association, or
conceptual dependence, between the ?Assailant? and the
?Victim? FEs is already encoded when we collected only
sentences whose main verbs are osou (in Japanese texts) or
its translations (in English texts). What we have done with
msort is to get subgroupings given a larger semantic group-
ing of ?harm-causing? at a more generic level. Based on
our coding experience, we are sure that subclassfication of a
given semantic class is based on ?semantic types? rather than
semantic roles. To give proper subgroupings of the events
that the ?Attack? frame is relevant, it is necessary to know
whether an ?Assailant? is a human ([+human, +animate,
. . . ]) or an animal ([?human, +animate, . . . ]), or whether
a ?Victim? is a human ([+human, +animate, . . . ]) or an
animal ([?human, +animate, . . . ]). If we insist that such
subclassifications in terms of semantic types into messy de-
tails are irrelevant, we are committing what we meant by
?mere generalizations for generalizations,? failing to recog-
nized what is really needed in NLP tasks.
9The listings ending with ?. . . ? are partial.
14
body part itai (body), soshiki (organization)
plant dansei (man), josei (woman), soshiki (tissue)
space genba (field), chiiki (region), mokuteki (purpose),
hokubu (northern area), shinai (city center)
amount gruupu (group)
relation jijou (circumstances), keesu (case), jitai (matter),
jiken (incident), ryakushiki (informality), kankei (rela-
tionship), mokuteki (purpose), genkou (current), . . .
activity jisatsu (suicide), satsugai (slaying), shougai (in-
jury), juushou (serious injuries), ishiki (conscious-
ness), utagai (doubt), yougi (suspicion), sousa (inves-
tigation), sousaku (search), shirabe (investigation), . . .
2.5 Identifying LUs for Japanese FEs
Based on the generic semantic groupings pro-
duced by msort, we classified nouns into sub-
classes by intution, so that they corresponded to
the FEs of the BFN frames in the following way:
Recall that a semantic frame is a collection of
semantic roles, or FEs. In the case of ?Attack?,
the frame has two ?core? FEs, i.e., ?Assailant?
and ?Victim?, and some other ?peripheral? or
?noncore? FEs such as ?Place?, ?Time?, and
?Weapon?. Thus, ?Attack? denotes a situation
in which an agent recognizable as an ?Assailant?
causes (or tries to cause) some ?Harm? or ?Injury?
to someone or a group of people recognizable as a
?Victim? at some ?Place? and ?Time?, sometimes
using an item recognizable as a ?Weapon?.
This means that all we need to do is to clas-
sify the nouns in Table 1 into semantic classes
such as ?Assailant?, ?Victim?, ?Place?, ?Time?, or
?Weapon?, with appropriate subclasses where hu-
man assailants are distinguished from nonhuman
assailants.10 The groupings provided by msort
turned out to be useful for this purpose.11
Using this procedure, the nouns obtained on a
frequency-basis for ?Attack? were classified into
the two core FEs, as follows:
10It is important to note that the target data selection pro-
cedure of BFN is biased. For example, they put aside a num-
ber of problematic cases like metaphorical expressions, and
this is clearly reflected in the current frame definitions. We
repeated noticed that metaphorically extended senses of a
word were systematically dropped in the current release of
BFN. For illustration, the sense of attack.n in heart attack
is not described in BFN. Descriptive ?gaps? of this sort are
clearly undesirable; some specific kinds of mapping prob-
lems between English LUs provided in BFN and Japanese
LUs arise from this.
11We were sometimes unable to identify an FE for a noun
class based solely on the output of msort. In these cases, we
looked at its usage in the corpus to determine its FE.
? ?Assailant?: dansei (man), goutou (burglary/burglar,
robbery/robber), heishi (soldier), hikoku (accused per-
son), butai (military unit), kyoudan (religious group)
? ?Victim?: danshi (boy), josei (woman), fujo (girl and
woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), shoujo (girl),
aite (opponent), nihonjin (Japanese), . . .
2.6 Advantages of proposed method
Using msort turned out to be more beneficial
than anticipated when it came to selecting non-
core FEs. msort helped to determine noncore
FEs correctly to a certain extent. The ?Attack?
frame, for example, includes noncore FEs such as
?Place?, ?Time?, ?Purpose?, and ?Reason? in ad-
dition to its core FEs, ?Assailant? and ?Victim?.
msort automatically groups naifu (knife), raifuru
(rifle), and pisutoru (pistol) into the ?product?
category, which corresponds to the ?Weapon? FE.
Similarly, it automatically groups chiiki (Regional
site), hokubu (northern area), and shinai (Inner
city) into the ?location? category, which corre-
sponds to ?Place?. Thus, part of the FE assign-
ment task can be done automatically using msort.
The procedure also produced some interesting
results. For example, the proposed method auto-
matically specifies a set of lexical items (or lex-
ical units) that clearly have the frame-evocation
effect but that are not properly identified as frame
elements of a semantic frame in BFN, either in
terms of core FEs or peripheral FEs (= noncore
FEs). The semantic groupings that were thus au-
tomatically identified are enumerated below:
1. Names denoting an act(ion) of N (N suru (or sareru))
(?(make) do N?): ranbou (violence), boukou (crimi-
nal assault), bouryoku (violence), jikkou (execution),
shuugeki (assault), kougeki (attack)
2. Names denoting a state of affairs N (V shita + N) (N
that S V ): satsugai (slaying), shougai (injury), goutou
(burglary/burglar, robbery/robber), satsujin (murder),
sasshou (killing and wounding)
3. Result ((Y ni) V shite, N wo owaseta) (?did V , and in-
flicted N to Y ): juushou (serious injuries)
4. Parts of the compound words: kyoushuu (assault
force) (a part of ?assault? force)
5. LUs of crime-related frames resulting from ?Attack?:
utagai (doubt), yougi (charge, suspicion), sousa (in-
vestigation), sousaku (search), shirabe (investigation),
kentou (investigation), hanketsu (judgement), . . .
A second look at the lexical items in 1 above
confirmed that most of these words or phrases can
15
be seen as LUs that realize, in Japanese, some of
the FEs of BFN?s ?Attack? frame.12 As sets of
lexical items were not classified automatically, we
had to determine all classifications manually.
2.7 Overall results
When the procedure was applied to ?Attack?,
?Cause harm? and ?Cause impact?, the following
Japanese LUs for their major FEs were specified:
1. Core FEs of ?Attack?:
?Assailant?: dansei (man), goutou (burglary/burglar, rob-
bery/robber), heishi (soldier), hikoku (accused
person), . . .
?Victim?: danshi (boy), josei (woman), fujo (girls and
women), joshi (girl), danji (young boy), . . .
2. Noncore FEs of ?Attack?:
?Place?: genba (field), chiiki (region), hokubu (northern
part), shinai (city center)
?Weapon?: naifu (knife), shoujuu (rifle), tanjuu (pistol)
3. Core FEs of ?Cause harm?:
?Body part? : senaka (back)
4. Core FEs of ?Cause impact?:
?Impactee?: doru (dollar), shijou (market), ginkou (bank),
shokoku (some countries)
?Impactor?: saigai (disaster), jishin (earthquake), fukyou
(depression), dageki (damage)
3 Comparison with other resources
To evaluate our results, we compared them with
other Japanese resources and methods for anal-
ysis, i.e., IPAL (IPA, 1987) and Nihongo Goi
Taikei (a Japanese lexicon) (hereafter called Goi
Taikei) (Ikehara et al, 1997), which are widely
used lexical resources, and semantic frame anal-
ysis by FOCAL (Nakamoto et al, to appear;
Kuroda et al, 2004), which is a recent frame-
work being developed with the aim of provid-
ing BFN-style semantic annotation and analy-
sis for Japanese independent of the Japanese
FrameNet (Ohara et al, 2003).
3.1 Comparison with Goi Taikei descriptions
Goi Taikei contains detailed information on the
predicate-argument structure classified according
to usage. Its semantic description of osou is given
below:
12For the reason of this argument, see note 5 above.
(1) 20 zokusei henka (property change) (motion)
N1 ga N2 wo osou
N1 strike N2
N1 (1270 shimpai (concern) 1262 kanashimi (sorrow)
2056 sainann (disaster) 2359 kishou (atmospheric
phenomena) 1000 tyuushou (abstract)) N2 (2 gutai
(object))
(2) 23 shintai dousa (physical motion) (motion)
N1 ga N2 wo osou
N1 attack N2
N1 (3 shutai (subject) 535 doubutsu (animal) 2416 by-
ouki (disease)) N2 (2 gutai (object))
(3) 23 shintai dousa (physical motion)
31 kanjou dousa (affective motion) (motion)
N1 ga N2 no fui wo osou
N1 surprise N2
N1 (4 hito (man) 1001 tyuushoubutsu (abstruc-
tion/abstraction?) 1235 koto (event)) N2 (4 hito
(man))
The word meanings were classified from the
properties of osou for nouns related to surface
cases of the verb. When we compared the frames
in BFN and the description provided by Goi
Taikei, and examined how the BFN frames corre-
sponded to the Goi Taikei definitions, we obtained
the following relationships:
Table 2: BFN/Goi-Taikei correspondences
Attack (2) 23 shintai dousa (physical motion)
Cause harm (1) 20 zokusei henka (property change)
Cause impact (1) 20 zokusei henka (property change)
First, we did not obtain the meaning ?An unex-
pected event occurred? like (3) in the Goi Taikei.
It was difficult to extract words whose meanings
described a manner of action, such as fui wo (by
surprise) using this method. It was also insuffi-
cient to extract only co-occurring nouns from sen-
tences related to verbs. As might be expected,
there was a close relationship between (2) and the
?Attack? frame. However, we were unable to find
?Assailant?s such as sickness in the BFN FEs. Fi-
nally, the ?Cause impact? frame and (1) were very
similar, except that assailant in (1) includes feel-
ings such as worry or sadness.
There was a good correlation between the se-
mantic frame constructed from BFN and the one
from Goi Taikei. With this method, however, we
met difficulties in extracting frames that did not
appear on the surface, such as ?manner of action?.
16
3.2 Comparison with IPAL descriptions
We compared the frames we obtained with the
definitions from the IPA Lexicon (IPA, 1987). Be-
low is an excerpt from the description of osou
from IPAL:
? Caption: osou001001 Semantic definition: An unde-
sirable thing unexpectedly occurs to someone.
Sentence valence pattern: N1 -ga N2 -wo
Noun phrase 1: bouto (rioter), goutou (burglary),
kuma (bear), sentouki (fighter plane), boufuu (wind
storm), jishinn (earthquake), ekibyou (plague), keizai
kiki (economic crisis)
Noun phrase 2: tabibito (traveler), fune (ship), nin-
gen (human)/kokudo (national land), kuni (country),
kouban (police box)
Example 1: Boufuu ga fune wo osotta. (A stormy wind
struck a ship.)
? Caption: osou001002
Semantic definition: Undesirable feelings and physio-
logical phenomena happening suddenly.
Sentence pattern: N1 -ga N2 -wo
Noun phrase 1: takamaru fuann (increased anxiety),
shi no kyoufu (fear of death), iyana kimochi (unpleas-
ant feelings)/ hageshii hiroukan (acute tiredness), ne-
muke (drowsiness)
Noun phrase 2: kare (he)
Example 1: Nemuke ga totsuzen kare wo osotta.
(Drowsiness fell upon him suddenly.)
Example 2: Kanojo ha fuann ni osowareta. (She be-
came uneasy suddenly.)
The IPAL description of osou identifies its two
senses13 We compared the BFN frames and the
IPAL descriptions (in terms of predicate frames)
and obtained the following correspondences:
Table 3: BFN/IPAL correspondences
Attack osou001001
Cause harm osou001001
Cause impact osou001001
All of the frames obtained from BFN seemed to
be classified into the first meaning in IPAL, e.g.,
there were no BFN frames in which ?Assailant?
recognized ?sickness.? With IPAL definitions,
it was difficult to distinguish the difference be-
tween The bear attacked the traveler and *An eco-
nomic crisis attacked the traveler, the latter of
which sounds unnatural and quite odd, whereas
we can do it with BFN definitions: the former
13A term, ?predicate frame,? is used in the IPAL to char-
acterize semantic properties of a predicate. While the idea
of predicate frames is somewhat related to semantic frames,
predicate frames are not defined as semantic frames in the
sense of Frame Semantics/BFN.
can be classified as an expression in the ?Attack?
frame, whereas the latter can not. The reason
for this is probably that BFN frames successfully
specify the semantic interdependence between the
?Assailant? and ?Victim? roles, whereas such in-
terdependece is not encoded in the IPAL descrip-
tions. We believe this is one of the strengths of
frame-based semantic description.
BFN definitions are not detailed enough, how-
ever. They face problems when we try to ac-
count for the constrast between The shark at-
tacked the swimmer and ?*The shark attacked the
bank, for example. The latter sentences doesn?t
makes sense unless it is reinterpreted some way,
while it is straightforward to interpret the first sen-
tence against a predatory situation.
In interpreting the second, there is a clear con-
flict or ?competition? between two strong read-
ings: one interpretation (reading 1) is against the
situation of ?Predation?, where the shark is inter-
preted as a ?Predator? and the bank as a ?Prey?.
Another (reading 2) is against the situation of
?Bank Robbery?, where the shark is interpreted
as a ?Bank Robber? and the bank as a ?Warehouse
of Valuables? (or simply as a ?Bank?). If reading
2 wins out, an implicit ?type coercion? (Puste-
jovsky, 1995) takes place to the shark so that the
referent of the shark is switched to a human who
acts as a ?Robber? with a nickname ?shark.? If
reading 1 wins out, by contrast, another kind of
implicit type coercion takes place to the bank so
that the referent of the bank is switched to an ani-
mal (an instance of fish, dolphin, or whale) which
acts as a ?Prey?, being called ?the bank? for some
unclear reasons. The preference of the reinter-
pretation for reading 2 over the other can be ac-
counted for if we are allowed to say that to find
someone being called ?shark? is more likely than
to find some animal being called ?bank.?
What this suggests is this: pieces of semantic
information that would account for ?selectional
restrictions? of this sort are not specified in the
BFN definitions (yet). Therefore, it can be said
that the frames constructed from BFN do not
classify all meanings of osou in the same way
IPAL does not, but these frames specify some
finer-grained, selectional aspects of osou?s lexical
meaning than the IPAL description. As we will
see in the next section, this is one of the strong
17
motivations that a framework called FOCAL has
tried to extend the BFN.
3.3 Comparison with FOCAL descriptions
FOCAL is a theoretical framework for semantic
analysis and annotation. Its development has been
strongly influenced by BFN, but it also tries to
extend BFN?s scope of semantic analysis to the
next stage.
In the case of X-ga Y-wo osou, FOCAL recog-
nizes 15 frames in total, listed in Table 4, specify-
ing their hierarchical organization.14
These frames are identified and classified based
on the semantic co-variations between ?Harm
Cause(r))? X , a special case of ?Cause(r)?,
and ?Harm Experiencer? Y , a special case of
?Experiencer?. This is important to note that FO-
CAL puts more emphasis on the specification of
the semantic co-variation between X and Y in
terms of semantic features because they are cru-
cial characteristics of a semantic frame, which are
not captured in the Goi Taikei and IPAL descrip-
tions, and are not clearly encoded even in the BFN
description.
In FOCAL, frames are defined as idealized
models of situations such as Robbery, Predation,
assuming that human understanding is situation-
based. The descriptive task of FOCAL, then, is
to recognize situations and give adequately de-
tailed descriptions to them. Given R is a set of
situation-specific roles {r1, . . . , rn}, which are
called semantic roles in BFN. Semantic frames
are useful only if they serves as specifications of
the co-variations among such Rs.
For example, F06, as a subclass of the ?Attack?
class event is defined as follows:
Definition of F06: Attack(R) = Attack(Predator(X),
Prey(Y ))
= Hunt(Hunter(X), Target(Y ), Purpose(Z))
where Z = Eat(Eater(X), Food(Y ), Purpose(Z?));
where Z? = Satisfy (r1(Z), Hunger)
There seems to be no English noun that names r1.
These are the frames that account for more or
less all possible readings of X-ga Y -wo osou. The
14 Space limitation disallowed us to show that the 15
frames thus recognized are nearly optimal to exhaustively
specify all the situations against which the senses of osou are
determined. This was confirmed by multivariate analyses on
psychological experiments (Nakamoto et al, to appear). We
regret this because the result would surely have answered the
question from one of the anonymous reviewers.
Table 4: 15 FOCAL frames with groups G1?G5
G1 F01 harm to Y caused by conflict between
groups X and Y
G1 F02 harm to Y caused by X?s invasion
G1 F03 harm to Y caused by X?s robbery
G1 F04 harm to Y caused by X?s violence
G1 F05 harm to Y caused by X?s raping
G2 F06 harm to Y caused by X?s preying attack
G2 F07 harm to Y caused by X?s nonpreying attack
(e.g., X?s defense)
G3 F08 harm to Y due to an unexpected accident X
G3 F09 harm to Y caused by a natural phenomenon
X (on a smaller scale, e.g., gust)
G3 F10 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., earthquake, flood)
G3 F11 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., spread of an
epidemic)
G4 F12 harm to Y caused by a social phenomenon X
G5 F13 harm to Y caused by a disease X
(nontemporary, e.g., cancer)
G5 F14 harm to Y caused by a disease symptom X
(temporary, e.g., heart attack)
G5 F15 harm to Y caused by a bad feeling X
(temporary, e.g., drowsiness)
validity of this claim was confirmed through psy-
chological experiments, and reported in (Kuroda
et al, 2004; Nakamoto et al, to appear). The
BFN identifies 3 frames relevant to the semantics
of osou, while FOCAL uses a total of 15 frames
to determine the range of situations against which
people understand the sentences whose main verb
is osou.
The 3 BFN frames have been compared with
the 15 frames below to assess how well they cor-
respond to one another:
Table 5: BFN/FOCAL correspondences
Attack Part of G1 F01?F05
Cause harm [UNCLEAR] [UNCLEAR]
Cause impact [UNCLEAR] [UNCLEAR]
[UNCLEAR] G5 F13?F15
This comparison revealed several differences.
First, FOCAL specifies situations that the
?Attack? frame applies to in much greater de-
tail, although its descriptions are based on se-
mantic frames like BFN?s descriptions are. This
is mainly because FOCAL identifies frames in
terms of conceivable differences in the ?pur-
poses,? or ?intended effects? of the ?Harm
18
Cause(r)?15, of which BFN?s ?Assailant? is a spe-
cial case. This suggests that BFN frames can be
further elaborated according to the subclassifica-
tion of ?Assailant? in terms of its purpose.16
The same is conversely true of ?Cause harm?
and ?Cause impact? frames. These BFN frames
need to be generalized so that they include nonhu-
man, nonintentional agents, which is not done in
the current BFN. Better matches would be found
if the ?Cause harm? and ?Cause impact? frames
were further classified according to the properties
of the ?Harm causer? and ?Impactor? just as in the
?Attack? frame.
While FOCAL explicitly groups the F01?F05
frames into G1 and combines it with another
group, G2, to yield a more general semantic class
{G1, G2}, it is not clear whether BFN captures
this hybrid class, since the hierarchical relation-
ships among frames are not sufficiently specified.
In fact, the comparison with FOCAL revealed
that BFN does not classify the ?Assailant? types in
as much detail as FOCAL does. According to FO-
CAL?s assumptions, it is ?Assailant??s ?Purpose?
(including the ?null? value) that defines the differ-
ences in otherwise similar situations. To identify
such subtle differences is exactly what humans
are very good at and computers are not. Speci-
fication of information of this kind is one of the
serious demands arising from many of the NLP
tasks.
To conclude, we noted that the granularity
of the semantic descriptions provided by BFN,
IPAL, Goi Taikei, and FOCAL had the following
hierarchy: FOCAL > BFN ? Goi Taikei > IPAL
This suggests that, while BFN is clearly useful
for a variety of purposes, its semantic descrip-
tions are not detailed enough, particularly when
dealing with the polysemy of relatively frequent
words like osou in Japanese or hit in English.
While our result is only suggestive at best, let
15This is not the same as BFN?s ?Harm causer? role,
which is much more specific than ?Harm Cause(r)? in FO-
CAL?s sense.
16The question of ?where to stop,? addressed by one of
the anonymous reviewers, would have been answered if we
had enough space to show that those 15 frames/situations are
nearly optimal to account for all the semantic classifications
reflected in selectional restrictions, as explained in note 14.
Clearly, we do not need to identify all semantically possible
subclassifications; we just need to identify psychologically
real subclassifications.
us make a brief comment on some methodologi-
cal aspects of the BFN framework.
Overall, BFN definitions for semantic frames
are much more oriented or even ?biased? for de-
scriptions of activities intended and caused by
human, volitional agents. In fact, BFN took a
methodological decision not to include metaphor-
ical uses and other ?problematic? uses of words
for ease of lexicon-building, thereby sacrificing
its descriptive range, causing a problem with bi-
ased data coverage, as far as we could see. In
the case of osou, for example, there were clearly
many examples in which harm is not caused by
a human, i.e., cases described by FOCAL frame
clusters G2: F06?F07, G3: F08?F11, G4: F12,
and G5: F13?F15. Therefore, as far as we are
concerned with the viability of the frame-based
description of situations that can be expressed us-
ing osou in Japanese, the current status of the
BFN database is only partially successful in that it
successfully captures the class of situations spec-
ified by G1.
4 Conclusion
We proposed a new translation-like method using
BFN to find Japanese LUs that corresponded to
English LUs in BFN semantic frames. We eval-
uated a technique of identifying Japanese LUs
based on English LUs using a bilingual corpus.
We evaluated the results by comparing them with
other Japanese language resources and analyses,
IPAL, Goi Taikei, and FOCAL. The comparison
revealed that FOCAL, BFN, Goi Taikei, and IPAL
provided finer-grained descriptions in this specific
order.
Our method allowed us to easily find Japanese
LUs that corresponded to LUs in BFN seman-
tics and at the same level of granularity as BFN.
Even if all the relevant sentenceswere not manu-
ally examined when the semantic frame was con-
structed, we were able to collect several members
of FEs. Our method also automatically specified
a set of lexical titems that clearly had the frame-
evocation effect but that were not properly iden-
tified as Frame Elements of a semantic frame in
BFN.
There are several problems still remaining that
need to be addressed. Because the bilingual cor-
pus used was a newspaper corpus, the target se-
19
mantic domains were limited. There is therefore
a possibility that we failed to identify certain se-
mantic frames. We plan to do further experiments
using a greater number of bilingual corpora with
a wider domain coverage.
In the comparison of the analyses by BFN and
by FOCAL, only one target verb osou is used in
this work. Clearly, this is insufficient and our re-
sult is only suggestive at best. To draw a realis-
tic conclusion, we will definitely need to examine
more target words and make the comparison more
reliable.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the COLING-ACL ?98, Montreal;
Canada.
Benfung Chen and Pascale Fung. 2004. Biframenet:
Bilingual frame semantics resource construction by
cross-lingual induction. In Proceedings of the 20th
International Conference on Computational Lin-
guistics (COLING 2004).
Michael Ellsworth, Katrin Erk, Paul Kingsbury, and
Sebastian Pado?. 2004. PropBank, SALSA, and
FrameNet: How design determines product. In
Proceedings of the LREC 2004 Workshop on Build-
ing Lexical Resources from Semantically Annotated
Corpora, Lisbon.
Katrin Erk, Andrea Kowalski, Sebastian Pado?, and
Manfred Pinkal. 2003. Towards a resource for
lexical semantics: A large German corpus with ex-
tensive semantic annotation. In Proceedings of the
ACL-03.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Charles J. Fillmore. 1982. Frame semantics. In Lin-
guistic Society of Korea, editor, Linguistics in the
Morning Calm, pages 111?137, Seoul. Hanshin.
Satoru Ikehara, Mahahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei: A Japanese Lexicon. Iwanami Shoten,
Tokyo. (in Japanese, 5 volumes/CDROM).
IPA, 1987. IPA Lexicon of the Japanese Language for
Computers: Basic Verbs. Information-Technology
Promotion Agency. (in Japanese).
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd In-
ternational Conference on Language Resources and
Evaluation (LREC-2002).
Kolodner, Janet. L. 2004. Case-Based Reasoning.
Morgan Kauffman.
Kow Kuroda, Keiko Nakamoto, Toshiyuki Kanamaru,
Masahiro Tatsuoka, and Hajime Nozawa. 2004.
A scope of concept analysis based on ?seman-
tic frames?: Berkeley FrameNet and Beyond. In
Conference Handbook of the 5th Meeting of The
Japanese Cognitive Linguistics Association, pages
133?153. (in Japanese).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara, 1999. Japanese
Morphological Analysis System ChaSen version
2.2.1. NAIST Technical Report NAIST-IS-TR. (in
Japanese).
Masaki Murata, Kyoko Kanzaki, Kiyotaka Uchimoto,
Qing Ma, and Hitoshi Isahara. 2001. Meaning sort
? three examples: dictionary construction, tagged
corpus construction, and information presentation
system ?. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
Second International Conference, CICLing 2001,
Mexico City, February 2001 Proceedings, pages
305?318. Springer Publisher.
Keiko Nakamoto, Kow Kuroda, and Hajime Nozawa.
to appear. Defining the feature rating task as
a(nother) powerful method to explore sentence
meanings: With a special interest with how they are
mentally represented. In Japanese Journal of Cog-
nitive Psychology. (in Japanese).
National Language Research Institute. 1964. Bunrui
Goihyo (Word List by Semantic Principles). Syuei
Shuppan. (in Japanese).
Pustejovsky, James. 1995. The Generative Lexicon.
MIT Press.
Kyoko Hirose Ohara, Seiko Fujii, Hiroaki Saito, Shun
Ishizaki, Toshio Ohori, and Ryoko Suzuki. 2003.
The Japanese FrameNet project: A preliminary re-
port. In Proceedings of Pacific Association for
Computational Linguistics, pages 249?254.
Hiroaki Sato. 2003. FrameSQL: A software tool for
FrameNet. In ASIALEX ?03 Tokyo Proceedings,
pages 251?258. Asian Association of Lexicogra-
phy.
Carlos Subirats and Miriam R. L. Petruck. 2003. Sur-
prise: Spanish FrameNet. Presentation at Work-
shop on Frame Semantics, International Congress
of Linguists. July 29, 2003, Prague, Czech Repub-
lic.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the Annual
Meeting of the ACL-03, pages 72?79. ACL-2003.
20
Non-Factoid Japanese Question Answering through Passage Retrieval
that Is Weighted Based on Types of Answers
Masaki Murata and Sachiyo Tsukawaki
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{murata,tsuka}@nict.go.jp
Qing Ma
Ryukoku University
Otsu, Shiga, 520-2194, Japan
qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru
Kyoto University
Yoshida-Nihonmatsu-Cho, Sakyo
Kyoto, 606-8501 Japan
kanamaru@hi.h.kyoto-u.ac.jp
Hitoshi Isahara
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We constructed a system for answering non-
factoid Japanese questions. We used var-
ious methods of passage retrieval for the
system. We extracted paragraphs based on
terms from an input question and output
them as the preferred answers. We classified
the non-factoid questions into six categories.
We used a particular method for each cate-
gory. For example, we increased the scores
of paragraphs including the word ?reason?
for questions including the word ?why.? We
participated at NTCIR-6 QAC-4, where our
system obtained the most correct answers
out of all the eight participating teams. The
rate of accuracy was 0.77, which indicates
that our methods were effective.
1 Introduction
A question-answering system is an application de-
signed to produce the correct answer to a question
given as input. For example, when ?What is the
capital of Japan?? is given as input, a question-
answering system may retrieve text containing sen-
tences like ?Tokyo is Japan?s capital and the coun-
try?s largest and most important city?, and ?Tokyo
is also one of Japan?s 47 prefectures?, from Web-
sites, newspaper articles, or encyclopedias. The sys-
tem then outputs ?Tokyo? as the correct answer.
We believe question-answering systems will become
a more convenient alternative to other systems de-
signed for information retrieval and a basic compo-
nent of future artificial intelligence systems. Numer-
ous researchers have recently been attracted to this
important topic. These researchers have produced
many interesting studies on question-answering sys-
tems (Kupiec, 1993; Ittycheriah et al, 2001; Clarke
et al, 2001; Dumis et al, 2002; Magnini et al, 2002;
Moldovan et al, 2003). Evaluation conferences and
contests on question-answering systems have also
been held. In particular, the U.S.A. has held the Text
REtrieval Conferences (TREC) (TREC-10 commit-
tee, 2001), and Japan has hosted the Question-
Answering Challenges (QAC) (National Institute of
Informatics, 2002) at NTCIR (NII Test Collection
for IR Systems ) 3. These conferences and contests
have aimed at improving question-answering sys-
tems. The researchers who participate in these create
question-answering systems that they then use to an-
swer the same questions, and each system?s perfor-
mance is then evaluated to yield possible improve-
ments.
We addressed non-factoid question answering in
NTCIR-6 QAC-4. For example, when the question
was ?Why are people opposed to the Private Infor-
mation Protection Law?? the system retrieved sen-
tences based on terms appearing in the question and
output an answer using the retrieved sentences. Nu-
merous studies have addressed issues that are in-
volved in the answering of non-factoid questions
(Berger et al, 2000; Blair-Goldensohn et al, 2003;
727
Xu et al, 2003; Soricut and Brill, 2004; Han et al,
2005; Morooka and Fukumoto, 2006; Maehara et
al., 2006; Asada, 2006).
We constructed a system for answering non-
factoid Japanese questions for QAC-4. We used
methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions
including the word ?why.? We performed exper-
iments using the NTCIR-6 QAC-4 data collection
and tested the effectiveness of our methods.
2 Categories of Non-Factoid Questions
We used six categories of non-factoid questions in
this study. We constructed the categories by con-
sulting the dry run data in QAC-4.
1. Definition-oriented questions (Questions that
require a definition to be given in response.)
e.g., K-1 to wa nandesuka? (What is K-1?)
2. Reason-oriented questions (Questions that re-
quire a reason to be given in response.)
e.g., kojin jouhou hokogou ni hantai shiteiru
hito wa doushite hantai shiteiru no desuka?
(Why are people opposed to the Private Infor-
mation Protection Law?)
3. Method-oriented questions (Questions that re-
quire an explanation of a method to be given in
response.)
e.g., sekai isan wa donoyouni shite kimeru no
desuka?? (How is a World Heritage Site deter-
mined?)
4. Degree-oriented questions (Questions that re-
quire an explanation of the degree of something
to be given in response.)
5. Change-oriented questions (Questions that re-
quire a description of things that change to be
given in response.)
e.g., shounen hou wa dou kawari mashitaka?
(How was the juvenile law changed?)
6. Detail-oriented questions (Questions that re-
quire a description of the particulars or details
surrounding a sequence of events to be given in
response.)
e.g., donoyouna keii de ryuukyuu oukoku wa ni-
hon no ichibu ni natta no desuka? (How did
Ryukyu come to belong to Japan?)
3 Question-answering Systems in this
Study
The system has three basic components:
1. Prediction of type of answer
The system predicts the answer to be a partic-
ular type of expression based on whether the
input question is indicated by an interrogative
pronoun, an adjective, or an adverb. For exam-
ple, if the input question is ?Why are people
opposed to the Private Information Protection
Law??, the word ?why? suggests that the an-
swer will be an expression that describes a rea-
son.
2. Document retrieval
The system extracts terms from the input ques-
tion and retrieves documents by using these
terms. Documents that are likely to contain
the correct answer are thus gathered during the
retrieval process. For example, for the input
question ?Why are people opposed to the Pri-
vate Information Protection Law??, the system
extracts ?people,? ?opposed,? ?Private,? ?Infor-
mation,? ?Protection,? and ?Law? as terms and
retrieves the appropriate documents based on
these.
3. Answer detection
The system separates the retrieved documents
into paragraphs and retrieves those that contain
terms from the input question and a clue ex-
pression (e.g., ?to wa? (copula sentence) for the
definition sentence). The system outputs the re-
trieved paragraphs as the preferred answer.
3.1 Prediction of type of answer
We used the following rules for predicting the type
of answer. We constructed the rules by consulting
the dry run data in QAC-4.
728
1. Definition-oriented questions Questions in-
cluding expressions such as ?to wa nani,?
?donna,? ?douiu,? ?douitta,? ?nanimono,?
?donoyouna mono,? ?donna mono,? and ?douiu
koto? (which all mean ?what is?) are rec-
ognized by the system as being definition-
oriented questions.
2. Reason-oriented questions Questions including
expressions such as ?naze? (why), ?naniyue?
(why), ?doushite? (why), ?nani ga riyuu de?
(what is the reason), and ?donna riyuu de?
(what reason), are recognized by the system as
being reason-oriented questions.
3. Method-oriented questions Questions includ-
ing expressions such as ?dou,? ?dousureba,?
?douyatte,? ?dono youni shite,? ?ikani shite,?
?ikani,? and ?donnna houhou de? (which all
mean ?how?) are recognized by the system as
being method-oriented questions.
4. Degree-oriented questions Questions including
expressions such as ?dorekurai? (how much),
?dorekurai no? (to what extent), and ?dono
teido? (to what extent), are recognized by the
system as being degree-oriented questions.
5. Change-oriented questions Questions includ-
ing expressions such as ?naniga chigau? (What
is different), ?donoyuni kawaru? (How is ...
changed), and ?dokoga kotonaru? (What is dif-
ferent), are recognized by the system as being
change-oriented questions.
6. Detail-oriented questions Questions including
expressions such as ?dono you na keii,? ?dono
you na ikisatsu,? and ?dono you na nariyuki?
(which all mean ?how was?) are recognized by
the system as being detail-oriented questions.
3.2 Document retrieval
Our system extracts terms from a question by using
the morphological analyzer, ChaSen (Matsumoto et
al., 1999). The analyzer first eliminates preposi-
tions, articles, and similar parts of speech. It then
retrieves documents by using the extracted terms.
The documents are retrieved as follows:
We first retrieve the top k
dr1
documents with the
highest scores calculated using the equation
Score(d)
=
?
term t
?
?
?
tf(d, t)
tf(d, t) + kt
length(d) + k
+
? + k
+
? log
N
df(t)
?
?
?
,
(1)
where d is a document, t is a term extracted from
a question, and tf(d, t) is the frequency of t oc-
curring in d. Here, df(t) is the number of docu-
ments in which t appears, N is the total number
of documents, length(d) is the length of d, and ?
is the average length of all documents. Constants
k
t
and k
+
are defined based on experimental re-
sults. We based this equation on Robertson?s equa-
tion (Robertson and Walker, 1994; Robertson et al,
1994). This approach is very effective, and we have
used it extensively for information retrieval (Murata
et al, 2000; Murata et al, 2001; Murata et al, 2002).
The question-answering system uses a large number
for k
t
.
We extracted the top 300 documents and used
them in the next procedure.
3.3 Answer detection
In detecting answers, our system first generates can-
didate expressions for them from the extracted docu-
ments. We use two methods for extracting candidate
expressions. Method 1 uses a paragraph as a candi-
date expression. Method 2 uses a paragraph, two
continuous paragraphs, or three continuous para-
graphs as candidate expressions.
We award each candidate expression the follow-
ing score.
Score(d)
= ?mint1?T log
?
t2?T3
(2dist(t1, t2)df(t2)
N
)
+ 0.00000001 ? length(d)
= maxt1?T
?
t2?T3
log
N
2dist(t1, t2) ? df(t2)
+ 0.00000001 ? length(d)
(2)
729
T3 = {t|t ? T, 2dist(t1, t)df(t)
N
? 1}, (3)
where d is a candidate expression, T is the set of
terms in the question, dist(t1, t2) is the distance
between t1 and t2 (defined as the number of char-
acters between them with dist(t1, t2) = 0.5 when
t1 = t2), and length(d) is the number of charac-
ters in a candidate expression. The numerical term,
0.00000001 ? length(d), is used for increasing the
scores of long paragraphs.
For reason-oriented questions, our system uses
some reason terms such as ?riyuu? (reason),
?gen?in? (cause), and ?nazenara? (because) as terms
for Eq. 2 in addition to terms from the input ques-
tion. This is because we would like to increase the
score of a document that includes reason terms for
reason-oriented questions.
For method-oriented questions, our system uses
some method terms such as ?houhou? (method),
?tejun? (procedure), and ?kotoniyori? (by doing) as
terms for second document retrieval (re-ranking) in
addition to terms from the input question.
For detail-oriented questions, our system uses
some method terms such as ?keii? (a detail, or a se-
quence of events), ?haikei? (background), and ?rek-
ishi? (history) as terms for second document re-
trieval (re-ranking) in addition to terms from the in-
put question.
For degree-oriented questions, when candidate
paragraphs include numerical expressions, the score
(Score(d)) is multiplied by 1.1.
For definition-oriented questions, the system first
extracts focus expressions. When the question in-
cludes expressions such as ?X-wa?, ?X-towa?, ?X-
toiunowa?, and ?X-tte?, X is extracted as a fo-
cus expression. The system multiplies the score,
(Score(d)), of the candidate paragraph having ?X-
wa?, ?X-towa or something by 1.1. When the can-
didate expression includes focus expressions having
modifiers (including modifier clauses and modifier
phrases), the modifiers are used as candidate expres-
sions, and the scores of the candidate expressions are
multiplied by 1.1.
Below is an example of a candidate expression
that is a modifier clause in a sentence.
Table 1: Results
Method Correct A B C D
Method 1 57 18 42 10 89
Method 2 77 5 67 19 90
(There were a total of 100 questions.)
Question sentence: sekai isan jouyaku to
wa dono youna jouyaku desu ka?
(What is the Convention concerning the
Protection of the World Cultural and Nat-
ural Heritage?)
Sentence including answers:
1972 nen no dai 17 kai yunesuko soukai de
saitaku sareta sekai isan jouyaku ....
(Convention concerning the Pro-
tection of the World Cultural
and Natural Heritage, which
was adopted in 1972 in the 17th gen-
eral assembly meeting of the UN Educational,
Scientific and Cultural Organization.)
Finally, our system extracts candidate expressions
having high scores, (Score(d)s), as the preferred
output. Our system extracts candidate expressions
having scores that are no less than the highest score
multiplied by 0.9 as the preferred output.
We constructed the methods for answer detection
by consulting the dry run data in QAC-4.
4 Experiments
The experimental results are listed in Table 1. One
hundred non-factoid questions were used in the ex-
periment. The questions, which were generated by
the QAC-4 organizers, were natural and not gener-
ated by using target documents. The QAC-4 orga-
nizers checked four or fewer outputs for each ques-
tion. Methods 1 and 2 were used to determine what
we used as answer candidate expressions (Method 1
uses one paragraph as a candidate answer. Method
2 uses one paragraph, two paragraphs, or three para-
graphs as candidate answers.).
?A,? ?B,? ?C,? and ?D? are the evaluation criteria.
?A? indicates output that describes the same content
as that in the answer. Even if there is a supplemen-
tary expression in the output, which does not change
730
the content, the output is judged to be ?A.? ?B? in-
dicates output that contains some content similar to
that in the answer but contains different overall con-
tent. ?C? indicates output that contains part of the
same content as that in the answer. ?D? indicates
output does not contain any of the same content as
that in the answer. The numbers for ?A,? ?B,? ?C,?
and ?D? in Table 1 indicate the number of questions
where an output belongs to ?A,? ?B,? ?C,? and ?D?.
?Correct? indicates the number of questions where
an output belongs to ?A,? ?B,? or ?C?. The evalu-
ation criteria ?Correct? was also used officially at
NTCIR-6 QAC-4.
We found the following.
? Method 1 obtained higher scores in evaluation
A than Method 2. This indicates that Method 1
can extract a completely relevant answer more
accurately than Method 2.
? Method 2 obtained higher scores in evaluation
?Correct? than Method 1. The rate of accuracy
for Method 2 was 0.77 according to evaluation
?Correct?. This indicates that Method 2 can ex-
tract more partly relevant answers than Method
1. When we want to extract completely relevant
answers, we should use Method 1. When we
want to extract more answers, including partly
relevant answers, we should use Method 2.
? Method 2 was the most accurate (0.77) of those
used by all eight participating teams. We could
detect paragraphs as answers including input
terms and the key terms related to answer types
based the methods discussed in Section 3.3.
Our system obtained the best results because
our method of detecting answers was the most
effective.
Below is an example of the output of Method 1,
which was judged to be ?A.?
Question sentence:
jusei ran shindan wa douiu baai ni okon-
awareru noka?
(When is amniocentesis performed on a
pregnant woman?)
System output:
omoi idenbyou no kodono ga umareru no
wo fusegu.
(To prevent the birth of children with seri-
ous genetic disorders )
Examples of answers given by organizers:
omoi idenbyou
(A serious genetic disorder)
omoi idenbyou no kodomo ga umareru
kanousei ga takai baai
(To prevent the birth of children with seri-
ous genetic disorders.)
5 Conclusion
We constructed a system for answering non-factoid
Japanese questions. An example of a non-factoid
question is ?Why are people opposed to the Pri-
vate Information Protection Law?? We used vari-
ous methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions in-
cluding the word ?why.? We participated at NTCIR-
6 QAC-4, where our system obtained the most cor-
rect answers out of all the eight participating teams.
The rate of accuracy was 0.77, which indicates that
our methods were effective.
We would like to apply our method and system to
Web data in the future. We would like to construct a
sophisticated system that can answer many kinds of
complicated queries such as non-factoid questions
based on a large amount of Web data.
Acknowledgements
We are grateful to all the organizers of NTCIR-6
who gave us the chance to participate in their con-
test to evaluate and improve our question-answering
system. We greatly appreciate the kindness of all
those who helped us.
731
References
Yoshiaki Asada. 2006. Processing of definition type
questions in a question answering system. Master?s
thesis, Yokohama National University. (in Japanese).
AdamBerger, Rich Caruana, David Cohn, Dayne Freitag,
and Vibhu Mittal. 2000. Bridging the lexical chasm:
Statistical approaches to answer-finding. In Proceed-
ings of the 23rd annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR-2000), pages 192?199.
Sasha Blair-Goldensohn, Kathleen R. McKeown, and
Andrew Hazen Schlaikjer. 2003. A hybrid approach
for qa track definitional questions. In Proceedings
of the 12th Text Retrieval Conference (TREC-2003),
pages 185?192.
Charles L. A. Clarke, Gordon V. Cormack, and
Thomas R. Lynam. 2001. Exploiting redundancy
in question answering. In Proceedings of the 24th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Susan Dumis, Michele Banko, Eric Brill, Jimmy Lin, and
Andrew Ng. 2002. Web question answering: Is more
always better? In Proceedings of the 25th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Kyoung-Soo Han, Young-In Song, Sang-Bum Kim, and
Hae-Chang Rim. 2005. Phrase-based definitional
question answering using definition terminology. In
Lecture Notes in Computer Science 3689, pages 246?
259.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. IBM?s Statistical Ques-
tion Answering System. In TREC-9 Proceedings.
Julian Kupiec. 1993. MURAX: A robust linguistic ap-
proach for question answering using an on-line ency-
clopedia. In Proceedings of the Sixteenth Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Hideyuki Maehara, Jun?ichi Fukumoto, and Noriko
Kando. 2006. A BE-based automated evaluation
for question-answering system. IEICE-WGNLC2005-
109, pages 19?24. (in Japanese).
Bernardo Magnini, Matto Negri, Roberto Prevete, and
Hristo Tanev. 2002. Is it the right answer? Exploiting
web redundancy for answer validation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis sys-
tem ChaSen version 2.0 manual 2nd edition.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mi-
hai Surdeanu. 2003. Performance issues and er-
ror analysis in an open-domain question answering
system. ACM Transactions on Information Systems,
21(2):133?154.
Kokoro Morooka and Jun?ichi Fukumoto. 2006. Answer
extraction method for why-type question answering
system. IEICE-WGNLC2005-107, pages 7?12. (in
Japanese).
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara. 2000.
Japanese probabilistic information retrieval using lo-
cation and category information. The Fifth Interna-
tional Workshop on Information Retrieval with Asian
Languages, pages 81?88.
Masaki Murata, Masao Utiyama, Qing Ma, Hiromi
Ozaku, and Hitoshi Isahara. 2001. CRL at NTCIR2.
Proceedings of the Second NTCIR Workshop Meeting
on Evaluation of Chinese & Japanese Text Retrieval
and Text Summarization, pages 5?21?5?31.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002.
High performance information retrieval using many
characteristics and many techniques. Proceedings of
the Third NTCIR Workshop (CLIR).
National Institute of Informatics. 2002. Proceedings of
the Third NTCIR Workshop (QAC).
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-Poisson model for
probabilistic weighted retrieval. In Proceedings of the
Seventeenth Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
Radu Soricut and Eric Brill. 2004. Automatic question
answering: Beyond the factoid. In In Proceedings
of the Human Language Technology and Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-2004), pages
57?64.
TREC-10 committee. 2001. The tenth text retrieval con-
ference. http://trec.nist.gov/pubs/trec10/t10 proceed-
ings.html.
Jinxi Xu, Ana Licuanan, and Ralph Weischedel. 2003.
TREC 2003 QA at BBN: answering definitional ques-
tions. In Proceedings of the 12th Text Retrieval Con-
ference (TREC-2003), pages 98?106.
732
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 587?594,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine-Learning-Based Transformation of Passive Japanese Sentences
into Active by Separating Training Data into Each Input Particle
Masaki Murata
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
murata@nict.go.jp
Tamotsu Shirado
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
shirado@nict.go.jp
Toshiyuki Kanamaru
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
kanamaru@nict.go.jp
Hitoshi Isahara
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We developed a new method of transform-
ing Japanese case particles when trans-
forming Japanese passive sentences into
active sentences. It separates training data
into each input particle and uses machine
learning for each particle. We also used
numerous rich features for learning. Our
method obtained a high rate of accuracy
(94.30%). In contrast, a method that did
not separate training data for any input
particles obtained a lower rate of accu-
racy (92.00%). In addition, a method
that did not have many rich features for
learning used in a previous study (Mu-
rata and Isahara, 2003) obtained a much
lower accuracy rate (89.77%). We con-
firmed that these improvements were sig-
nificant through a statistical test. We
also conducted experiments utilizing tra-
ditional methods using verb dictionar-
ies and manually prepared heuristic rules
and confirmed that our method obtained
much higher accuracy rates than tradi-
tional methods.
1 Introduction
This paper describes how passive Japanese sen-
tences can be automatically transformed into ac-
tive. There is an example of a passive Japanese
sentence in Figure 1. The Japanese suffix reta
functions as an auxiliary verb indicating the pas-
sive voice. There is a corresponding active-voice
sentence in Figure 2. When the sentence in Fig-
ure 1 is transformed into an active sentence, (i) ni
(by), which is a case postpositional particle with
the meaning of ?by?, is changed into ga, which is
a case postpositional particle indicating the sub-
jective case, and (ii) ga (subject), which is a
case postpositional particle indicating the subjec-
tive case, is changed into wo (object), which is
a case postpositional particle indicating the objec-
tive case. In this paper, we discuss the transfor-
mation of Japanese case particles (i.e., ni ? ga)
through machine learning.1
The transformation of passive sentences into ac-
tive is useful in many research areas including
generation, knowledge extraction from databases
written in natural languages, information extrac-
tion, and answering questions. For example, when
the answer is in the passive voice and the ques-
tion is in the active voice, a question-answering
system cannot match the answer with the question
because the sentence structures are different and
it is thus difficult to find the answer to the ques-
tion. Methods of transforming passive sentences
into active are important in natural language pro-
cessing.
The transformation of case particles in trans-
forming passive sentences into active is not easy
because particles depend on verbs and their use.
We developed a new method of transforming
Japanese case particles when transforming pas-
sive Japanese sentences into active in this study.
Our method separates training data into each in-
put particle and uses machine learning for each in-
put particle. We also used numerous rich features
for learning. Our experiments confirmed that our
method was effective.
1In this study, we did not handle the transformation of
auxiliary verbs and the inflection change of verbs because
these can be transformed based on Japanese grammar.
587
inu ni watashi ga kama- reta.
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 1: Passive sentence
inu ni watashi ga kama- reta.
ga wo
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 3: Example in corpus
inu ga watashi wo kanda.
(dog) subject (I) object (bite)
(Dog bit me.)
Figure 2: Active sentence
2 Tagged corpus as supervised data
We used the Kyoto University corpus (Kurohashi
and Nagao, 1997) to construct a corpus tagged for
the transformation of case particles. It has ap-
proximately 20,000 sentences (16 editions of the
Mainichi Newspaper, from January 1st to 17th,
1995). We extracted case particles in passive-
voice sentences from the Kyoto University cor-
pus. There were 3,576 particles. We assigned a
corresponding case particle for the active voice to
each case particle. There is an example in Figure
3. The two underlined particles, ?ga? and ?wo?
that are given for ?ni? and ?ga? are tags for case
particles in the active voice. We called the given
case particles for the active voice target case par-
ticles, and the original case particles in passive-
voice sentences source case particles. We created
tags for target case particles in the corpus. If we
can determine the target case particles in a given
sentence, we can transform the case particles in
passive-voice sentences into case particles for the
active voice. Therefore, our goal was to determine
the target case particles.
3 Machine learning method (support
vector machine)
We used a support vector machine as the basis
of our machine-learning method. This is because
support vector machines are comparatively better
than other methods in many research areas (Kudoh
and Matsumoto, 2000; Taira and Haruno, 2001;
Small Margin Large Margin
Figure 4: Maximizing margin
Murata et al, 2002).
Data consisting of two categories were classi-
fied by using a hyperplane to divide a space with
the support vector machine. When these two cat-
egories were, positive and negative, for example,
enlarging the margin between them in the train-
ing data (see Figure 42), reduced the possibility of
incorrectly choosing categories in blind data (test
data). A hyperplane that maximized the margin
was thus determined, and classification was done
using that hyperplane. Although the basics of this
method are as described above, the region between
the margins through the training data can include
a small number of examples in extended versions,
and the linearity of the hyperplane can be changed
to non-linear by using kernel functions. Classi-
fication in these extended versions is equivalent
to classification using the following discernment
function, and the two categories can be classified
on the basis of whether the value output by the
function is positive or negative (Cristianini and
Shawe-Taylor, 2000; Kudoh, 2000):
2The open circles in the figure indicate positive examples
and the black circles indicate negative. The solid line indi-
cates the hyperplane dividing the space, and the broken lines
indicate the planes depicting margins.
588
f(x) = sgn
(
l
?
i=1
?iyiK(xi,x) + b
)
(1)
b =
maxi,y
i
=?1
bi + mini,y
i
=1
bi
2
bi = ?
l
?
j=1
?jyjK(xj,xi),
where x is the context (a set of features) of an in-
put example, xi indicates the context of a training
datum, and yi (i = 1, ..., l, yi ? {1,?1}) indicates
its category. Function sgn is:
sgn(x) = 1 (x ? 0), (2)
?1 (otherwise).
Each ?i (i = 1, 2...) is fixed as a value of ?i that
maximizes the value of L(?) in Eq. (3) under the
conditions set by Eqs. (4) and (5).
L(?) =
l
?
i=1
?
i
?
1
2
l
?
i,j=1
?
i
?
j
y
i
y
j
K(x
i
,x
j
) (3)
0 ? ?
i
? C (i = 1, ..., l) (4)
l
?
i=1
?
i
y
i
= 0 (5)
Although function K is called a kernel function
and various functions are used as kernel functions,
we have exclusively used the following polyno-
mial function:
K(x,y) = (x ? y + 1)d (6)
C and d are constants set by experimentation. For
all experiments reported in this paper, C was fixed
as 1 and d was fixed as 2.
A set of xi that satisfies ?i > 0 is called a sup-
port vector, (SVs)3, and the summation portion of
Eq. (1) is only calculated using examples that are
support vectors. Equation 1 is expressed as fol-
lows by using support vectors.
f(x) = sgn
?
?
?
i:x
i
?SV
s
?iyiK(xi,x) + b
?
?(7)
b =
bi:y
i
=?1,x
i
?SV
s
+ bi:y
i
=1,x
i
?SV
s
2
bi = ?
?
i:x
i
?SV
s
?jyjK(xj ,xi),
3The circles on the broken lines in Figure 4 indicate sup-
port vectors.
Table 1: Features
F1 part of speech (POS) of P
F2 main word of P
F3 word of P
F4 first 1, 2, 3, 4, 5, and 7 digits of category number
of P5
F5 auxiliary verb attached to P
F6 word of N
F7 first 1, 2, 3, 4, 5, and 7 digits of category number
of N
F8 case particles and words of nominals that have de-
pendency relationship with P and are other than
N
F9 first 1, 2, 3, 4, 5, and 7 digits of category num-
ber of nominals that have dependency relationship
with P and are other than N
F10 case particles of nominals that have dependency
relationship with P and are other than N
F11 the words appearing in the same sentence
F12 first 3 and 5 digits of category number of words
appearing in same sentence
F13 case particle taken by N (source case particle)
F14 target case particle output by KNP (Kurohashi,
1998)
F15 target case particle output with Kondo?s method
(Kondo et al, 2001)
F16 case patterns defined in IPAL dictionary (IPAL)
(IPA, 1987)
F17 combination of predicate semantic primitives de-
fined in IPAL
F18 predicate semantic primitives defined in IPAL
F19 combination of semantic primitives of N defined
in IPAL
F20 semantic primitives of N defined in IPAL
F21 whether P is defined in IPAL or not
F22 whether P can be in passive form defined in
VDIC6
F23 case particles of P defined in VDIC
F24 type of P defined in VDIC
F25 transformation rule used for P and N in Kondo?s
method
F26 whether P is defined in VDIC or not
F27 pattern of case particles of nominals that have de-
pendency relationship with P
F28 pair of case particles of nominals that have depen-
dency relationship with P
F29 case particles of nominals that have dependency
relationship with P and appear before N
F30 case particles of nominals that have dependency
relationship with P and appear after N
F31 case particles of nominals that have dependency
relationship with P and appear just before N
F32 case particles of nominals that have dependency
relationship with P and appear just after N
589
Table 2: Frequently occurring target case particles in source case particles
Source case particle Occurrence rate Frequent target case Occurrence rate
particles in in
source case particles source case particles
ni (indirect object) 27.57% (493/1788) ni (indirect object) 70.79% (349/493)
ga (subject) 27.38% (135/493)
ga (subject) 26.96% (482/1788) wo (direct object) 96.47% (465/482)
de (with) 17.17% (307/1788) ga (subject) 79.15% (243/307)
de (with) 13.36% (41/307)
to (with) 16.11% (288/1788) to (with) 99.31% (286/288)
wo (direct object) 6.77% (121/1788) wo (direct object) 99.17% (120/121)
kara (from) 4.53% ( 81/1788) ga (subject) 49.38% ( 40/ 81)
kara (from) 44.44% ( 36/ 81)
made (to) 0.78% ( 14/1788) made (to) 100.00% ( 14/ 14)
he (to) 0.06% ( 1/1788) ga (subject) 100.00% ( 1/ 1)
no (subject) 0.06% ( 1/1788) wo (direct object) 100.00% ( 1/ 1)
Support vector machines are capable of han-
dling data consisting of two categories. Data con-
sisting of more than two categories is generally
handled using the pair-wise method (Kudoh and
Matsumoto, 2000).
Pairs of two different categories (N(N-1)/2
pairs) are constructed for data consisting of N cat-
egories with this method. The best category is de-
termined by using a two-category classifier (in this
paper, a support vector machine4 is used as the
two-category classifier), and the correct category
is finally determined on the basis of ?voting? on
the N(N-1)/2 pairs that result from analysis with
the two-category classifier.
The method discussed in this paper is in fact a
combination of the support vector machine and the
pair-wise method described above.
4 Features (information used in
classification)
The features we used in our study are listed in Ta-
ble 1, where N is a noun phrase connected to the
4We used Kudoh?s TinySVM software (Kudoh, 2000) as
the support vector machine.
5The category number indicates a semantic class of
words. A Japanese thesaurus, the Bunrui Goi Hyou (NLRI,
1964), was used to determine the category number of each
word. This thesaurus is ?is-a? hierarchical, in which each
word has a category number. This is a 10-digit number that
indicates seven levels of ?is-a? hierarchy. The top five lev-
els are expressed by the first five digits, the sixth level is ex-
pressed by the next two digits, and the seventh level is ex-
pressed by the last three digits.
6Kondo et al constructed a rich dictionary for Japanese
verbs (Kondo et al, 2001). It defined types and characteris-
tics of verbs. We will refer to it as VDIC.
case particle being analyzed, and P is the phrase?s
predicate. We used the Japanese syntactic parser,
KNP (Kurohashi, 1998), for identifying N, P, parts
of speech and syntactic relations.
In the experiments conducted in this study, we
selected features. We used the following proce-
dure to select them.
? Feature selection
We first used all the features for learning. We
next deleted only one feature from all the fea-
tures for learning. We did this for every fea-
ture. We decided to delete features that would
make the most improvement. We repeated
this until we could not improve the rate of ac-
curacy.
5 Method of separating training data
into each input particle
We developed a new method of separating train-
ing data into each input (source) particle that uses
machine learning for each particle. For example,
when we identify a target particle where the source
particle is ni, we use only the training data where
the source particle is ni. When we identify a tar-
get particle where the source particle is ga, we use
only the training data where the source particle is
ga.
Frequently occurring target case particles are
very different in source case particles. Frequently
occurring target case particles in all source case
particles are listed in Table 2. For example, when
ni is a source case particle, frequently occurring
590
Table 3: Occurrence rates for target case particles
Target case Occurrence rate
particle Closed Open
wo (direct object) 33.05% 29.92%
ni (indirect object) 19.69% 17.79%
to (with) 16.00% 18.90%
de (with) 13.65% 15.27%
ga (subject) 11.07% 10.01%
ga or de 2.40% 2.46%
kara (from) 2.13% 3.47%
Other 2.01% 1.79%
target case particles are ni or ga. In contrast, when
ga is a source case particle, a frequently occurring
target case particle is wo.
In this case, it is better to separate training data
into each source particle and use machine learn-
ing for each particle. We therefore developed this
method and confirmed that it was effective through
experiments (Section 6).
6 Experiments
6.1 Basic experiments
We used the corpus we constructed described in
Section 2 as supervised data. We divided the su-
pervised data into closed and open data (Both the
closed data and open data had 1788 items each.).
The distribution of target case particles in the data
are listed in Table 3. We used the closed data to
determine features that were deleted in feature se-
lection and used the open data as test data (data
for evaluation). We used 10-fold cross validation
for the experiments on closed data and we used
closed data as the training data for the experiments
on open data. The target case particles were deter-
mined by using the machine-learning method ex-
plained in Section 3. When multiple target parti-
cles could have been answers in the training data,
we used pairs of them as answers for machine
learning.
The experimental results are listed in Tables 4
and 5. Baseline 1 outputs a source case particle
as the target case particle. Baseline 2 outputs the
most frequent target case particle (wo (direct ob-
ject)) in the closed data as the target case particle
in every case. Baseline 3 outputs the most fre-
quent target case particle for each source target
case particle in the closed data as the target case
particle. For example, ni (indirect object) is the
most frequent target case particle when the source
case particle is ni, as listed in Table 2. Baseline 3
outputs ni when the source case particle is ni. KNP
indicates the results that the Japanese syntactic
parser, KNP (Kurohashi, 1998), output. Kondo in-
dicates the results that Kondo?s method, (Kondo et
al., 2001), output. KNP and Kondo can only work
when a target predicate is defined in the IPAL dic-
tionary or the VDIC dictionary. Otherwise, KNP
and Kondo output nothing. ?KNP/Kondo + Base-
line X? indicates the use of outputs by Baseline
X when KNP/Kondo have output nothing. KNP
and Kondo are traditional methods using verb dic-
tionaries and manually prepared heuristic rules.
These traditional methods were used in this study
to compare them with ours. ?Murata 2003? indi-
cates results using a method they developed in a
previous study (Murata and Isahara, 2003). This
method uses F1, F2, F5, F6, F7, F10, and F13 as
features and does not have training data for any
source case particles. ?Division? indicates sepa-
rating training data into each source particle. ?No-
division? indicates not separating training data for
any source particles. ?All features? indicates the
use of all features with no features being selected.
?Feature selection? indicates features are selected.
We did two kinds of evaluations: ?Eval. A? and
?Eval. B?. There are some cases where multiple
target case particles can be answers. For example,
ga and de can be answers. We judged the result to
be correct in ?Eval. A? when ga and de could be
answers and the system output the pair of ga and
de as answers. We judged the result to be correct
in ?Eval. B? when ga and de could be answers and
the system output ga, de, or the pair of ga and de
as answers.
Table 4 lists the results using all data. Table 5
lists the results where a target predicate is defined
in the IPAL and VDIC dictionaries. There were
551 items in the closed data and 539 in the open.
We found the following from the results.
Although selection of features obtained higher
rates of accuracy than use of all features in the
closed data, it did not obtain higher rates of accu-
racy in the open data. This indicates that feature
selection was not effective and we should have
used all features in this study.
Our method using all features in the open data
and separating training data into each source parti-
cle obtained the highest rate of accuracy (94.30%
in Eval. B). This indicates that our method is ef-
591
Table 4: Experimental results
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 58.67% 61.41% 62.02% 64.60%
Baseline 2 33.05% 33.56% 29.92% 30.37%
Baseline 3 84.17% 88.20% 84.17% 88.20%
KNP 27.35% 28.69% 27.91% 29.14%
KNP + Baseline 1 64.32% 67.06% 67.79% 70.36%
KNP + Baseline 2 48.10% 48.99% 45.97% 46.48%
KNP + Baseline 3 81.21% 84.84% 80.82% 84.45%
Kondo 39.21% 40.88% 39.32% 41.00%
Kondo + Baseline 1 65.27% 68.57% 67.34% 70.41%
Kondo + Baseline 2 54.87% 56.54% 53.52% 55.26%
Kondo + Baseline 3 78.08% 81.71% 78.30% 81.88%
Murata 2003 86.86% 89.09% 87.86% 89.77%
Our method, no-division + all features 89.99% 92.39% 90.04% 92.00%
Our method, no-division + feature selection 91.28% 93.40% 90.10% 92.00%
Our method, division + all features 91.22% 93.79% 92.28% 94.30%
Our method, division + feature selection 92.06% 94.41% 91.89% 93.85%
Table 5: Experimental results on data that can use IPAL and VDIC dictionaries
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 57.71% 58.98% 58.63% 58.81%
Baseline 2 37.39% 37.39% 37.29% 37.29%
Baseline 3 84.03% 86.57% 86.83% 88.31%
KNP 74.59% 75.86% 75.88% 76.07%
Kondo 76.04% 77.50% 78.66% 78.85%
Our method, no-division + all features 94.19% 95.46% 94.81% 94.81%
Our method, division + all features 95.83% 96.91% 97.03% 97.03%
fective.
Our method that used all the features and did
not separate training data for any source particles
obtained an accuracy rate of 92.00% in Eval. B.
The technique of separating training data into each
source particles made an improvement of 2.30%.
We confirmed that this improvement has a signifi-
cance level of 0.01 by using a two-sided binomial
test (two-sided sign test). This indicates that the
technique of separating training data for all source
particles is effective.
Murata 2003 who used only seven features and
did not separate training data for any source par-
ticles obtained an accuracy rate of 89.77% with
Eval. B. The method (92.00%) of using all fea-
tures (32) made an improvement of 2.23% against
theirs. We confirmed that this improvement had
a significance level of 0.01 by using a two-sided
binomial test (two-sided sign test). This indicates
that our increased features are effective.
KNP and Kondo obtained low accuracy rates
(29.14% and 41.00% in Eval. B for the open data).
We did the evaluation using data and proved that
these methods could work well. A target predicate
in the data is defined in the IPAL and VDIC dictio-
naries. The results are listed in Table 5. KNP and
Kondo obtained relatively higher accuracy rates
(76.07% and 78.85% in Eval. B for the open data).
However, they were lower than that for Baseline 3.
Baseline 3 obtained a relatively high accuracy
rate (84.17% and 88.20% in Eval. B for the open
data). Baseline 3 is similar to our method in terms
of separating the training data into source parti-
cles. Baseline 3 separates the training data into
592
Table 6: Deletion of features
Deleted Closed data Open data
features Eval. A Eval. B Eval. A Eval. B
Acc. Diff. Acc. Diff. Acc. Diff. Acc. Diff.
Not deleted 91.22% ? 93.79% ? 92.28% ? 94.30% ?
F1 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F2 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F3 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F4 91.50% 0.28% 94.13% 0.34% 91.72% -0.56% 93.68% -0.62%
F5 91.22% 0.00% 93.62% -0.17% 91.95% -0.33% 93.96% -0.34%
F6 91.00% -0.22% 93.51% -0.28% 92.23% -0.05% 94.24% -0.06%
F7 90.66% -0.56% 93.18% -0.61% 91.78% -0.50% 93.90% -0.40%
F8 91.22% 0.00% 93.79% 0.00% 92.39% 0.11% 94.24% -0.06%
F9 91.28% 0.06% 93.62% -0.17% 92.45% 0.17% 94.07% -0.23%
F10 91.33% 0.11% 93.85% 0.06% 92.00% -0.28% 94.07% -0.23%
F11 91.50% 0.28% 93.74% -0.05% 92.06% -0.22% 93.79% -0.51%
F12 91.28% 0.06% 93.62% -0.17% 92.56% 0.28% 94.35% 0.05%
F13 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F14 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.41% 0.11%
F15 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F16 91.39% 0.17% 93.90% 0.11% 92.34% 0.06% 94.30% 0.00%
F17 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F18 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.46% 0.16%
F19 91.33% 0.11% 93.90% 0.11% 92.28% 0.00% 94.30% 0.00%
F20 91.11% -0.11% 93.68% -0.11% 92.34% 0.06% 94.35% 0.05%
F21 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F22 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F23 91.28% 0.06% 93.79% 0.00% 92.28% 0.00% 94.24% -0.06%
F24 91.22% 0.00% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F25 89.54% -1.68% 92.11% -1.68% 90.04% -2.24% 92.39% -1.91%
F26 91.16% -0.06% 93.74% -0.05% 92.28% 0.00% 94.30% 0.00%
F27 91.22% 0.00% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F28 90.94% -0.28% 93.51% -0.28% 92.11% -0.17% 94.13% -0.17%
F29 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.30% 0.00%
F30 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F31 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.24% -0.06%
F32 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
source particles and uses the most frequent tar-
get case particle. Our method involves separating
the training data into source particles and using
machine learning for each particle. The fact that
Baseline 3 obtained a relatively high accuracy rate
supports the effectiveness of our method separat-
ing the training data into source particles.
6.2 Experiments confirming importance of
features
We next conducted experiments where we con-
firmed which features were effective. The results
are listed in Table 6. We can see the accuracy rate
for deleting features and the accuracy rate for us-
ing all features. We can see that not using F25
greatly decreased the accuracy rate (about 2%).
This indicates that F25 is particularly effective.
F25 is the transformation rule Kondo used for P
and N in his method. The transformation rules in
Kondo?s method were made precisely for ni (indi-
rect object), which is particularly difficult to han-
dle. F25 is thus effective. We could also see not
using F7 decreased the accuracy rate (about 0.5%).
F7 has the semantic features for N. We found that
the semantic features for N were also effective.
6.3 Experiments changing number of
training data
We finally did experiments changing the number
of training data. The results are plotted in Figure
5. We used our two methods of all features ?Di-
vision? and ?Non-division?. We only plotted the
593
Figure 5: Changing number of training data
accuracy rates for Eval. B in the open data in the
figure. We plotted accuracy rates when 1, 1/2, 1/4,
1/8, and 1/16 of the training data were used. ?Divi-
sion?, which separates training data for all source
particles, obtained a high accuracy rate (88.36%)
even when the number of training data was small.
In contrast, ?Non-division?, which does not sepa-
rate training data for any source particles, obtained
a low accuracy rate (75.57%), when the number of
training data was small. This indicates that our
method of separating training data for all source
particles is effective.
7 Conclusion
We developed a new method of transform-
ing Japanese case particles when transforming
Japanese passive sentences into active sentences.
Our method separates training data for all input
(source) particles and uses machine learning for
each particle. We also used numerous rich features
for learning. Our method obtained a high rate of
accuracy (94.30%). In contrast, a method that did
not separate training data for all source particles
obtained a lower rate of accuracy (92.00%). In ad-
dition, a method that did not have many rich fea-
tures for learning used in a previous study obtained
a much lower accuracy rate (89.77%). We con-
firmed that these improvements were significant
through a statistical test. We also undertook ex-
periments utilizing traditional methods using verb
dictionaries and manually prepared heuristic rules
and confirmed that our method obtained much
higher accuracy rates than traditional methods.
We also conducted experiments on which fea-
tures were the most effective. We found that
Kondo?s transformation rule used as a feature in
our system was particularly effective. We also
found that semantic features for nominal targets
were effective.
We finally did experiments on changing the
number of training data. We found that our
method of separating training data for all source
particles could obtain high accuracy rates even
when there were few training data. This indicates
that our method of separating training data for all
source particles is effective.
The transformation of passive sentences into ac-
tive sentences is useful in many research areas
including generation, knowledge extraction from
databases written in natural languages, informa-
tion extraction, and answering questions. In the
future, we intend to use the results of our study for
these kinds of research projects.
References
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
IPA. 1987. (Information?Technology Promotion Agency,
Japan). IPA Lexicon of the Japanese Language for Com-
puters IPAL (Basic Verbs). (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. Transactions of Infor-
mation Processing Society of Japan, 42(3):465?477. (in
Japanese).
Taku Kudoh and Yuji Matsumoto. 2000. Use of support vec-
tor learning for chunk identification. CoNLL-2000, pages
142?144.
Taku Kudoh. 2000. TinySVM: Support Vector Machines.
http://cl.aist-nara.ac.jp/?taku-ku//software/TinySVM/
index.html.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto Univer-
sity text corpus project. 3rd Annual Meeting of the Asso-
ciation for Natural Language Processing, pages 115?118.
(in Japanese).
Sadao Kurohashi, 1998. Japanese Dependency/Case Struc-
ture Analyzer KNP version 2.0b6. Department of Infor-
matics, Kyoto University. (in Japanese).
Masaki Murata and Hitoshi Isahara, 2003. Conversion of
Japanese Passive/Causative Sentences into Active Sen-
tences Using Machine Learning, pages 115?125. Springer
Publisher.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002. Com-
parison of three machine-learning methods for Thai part-
of-speech tagging. ACM Transactions on Asian Language
Information Processing, 1(2):145?158.
NLRI. 1964. Bunrui Goi Hyou. Shuuei Publishing.
Hirotoshi Taira and Masahiko Haruno. 2001. Feature se-
lection in svm text categorization. In Proceedings of
AAAI2001, pages 480?486.
594
Multilingual Aligned Parallel Treebank Corpus Reflecting
Contextual Information and Its Applications
Kiyotaka Uchimoto? Yujie Zhang? Kiyoshi Sudo?
Masaki Murata? Satoshi Sekine? Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{uchimoto,yujie,murata,isahara}@nict.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
{sudo,sekine}@cs.nyu.edu
Abstract
This paper describes Japanese-English-Chinese
aligned parallel treebank corpora of newspaper
articles. They have been constructed by trans-
lating each sentence in the Penn Treebank and
the Kyoto University text corpus into a cor-
responding natural sentence in a target lan-
guage. Each sentence is translated so as to
reflect its contextual information and is anno-
tated with morphological and syntactic struc-
tures and phrasal alignment. This paper also
describes the possible applications of the par-
allel corpus and proposes a new framework to
aid in translation. In this framework, paral-
lel translations whose source language sentence
is similar to a given sentence can be semi-
automatically generated. In this paper we show
that the framework can be achieved by using
our aligned parallel treebank corpus.
1 Introduction
Recently, accurate machine translation systems
can be constructed by using parallel corpora
(Och and Ney, 2000; Germann et al, 2001).
However, almost all existing machine transla-
tion systems do not consider the problem of
translating a given sentence into a natural sen-
tence reflecting its contextual information in the
target language. One of the main reasons for
this is that we had many problems that had to
be solved by one-sentence to one-sentence ma-
chine translation before we could solve the con-
textual problem. Another reason is that it was
difficult to simply investigate the influence of
the context on the translation because sentence
correspondences of the existing bilingual doc-
uments are rarely one-to-one, and are usually
one-to-many or many-to-many.
On the other hand, high-quality treebanks
such as the Penn Treebank (Marcus et al, 1993)
and the Kyoto University text corpus (Kuro-
hashi and Nagao, 1997) have contributed to
improving the accuracies of fundamental tech-
niques for natural language processing such as
morphological analysis and syntactic structure
analysis. However, almost all of these high-
quality treebanks are based on monolingual cor-
pora and do not have bilingual or multilin-
gual information. There are few high-quality
bilingual or multilingual treebank corpora be-
cause parallel corpora have mainly been actively
used for machine translation between related
languages such as English and French, there-
fore their syntactic structures are not required
so much for aligning words or phrases. How-
ever, syntactic structures are necessary for ma-
chine translation between languages whose syn-
tactic structures are different from each other,
such as in Japanese-English, Japanese-Chinese,
and Chinese-English machine translations, be-
cause it is more difficult to automatically align
words or phrases between two unrelated lan-
guages than between two related languages. Ac-
tually, it has been reported that syntactic struc-
tures contribute to improving the accuracy of
word alignment between Japanese and English
(Yamada and Knight, 2001). Therefore, if we
had a high-quality parallel treebank corpus, the
accuracies of machine translation between lan-
guages whose syntactic structures are differ-
ent from each other would improve. Further-
more, if the parallel treebank corpus had word
or phrase alignment, the accuracy of automatic
word or phrase alignment would increase by
using the parallel treebank corpus as training
data. However, so far, there is no aligned par-
allel treebank corpus whose domain is not re-
stricted. For example, the Japanese Electronics
Industry Development Association?s (JEIDA?s)
bilingual corpus (Isahara and Haruno, 2000)
has sentence, phrase, and proper noun align-
ment. However, it does not have morphologi-
cal and syntactic information, the alignment is
partial, and the target is restricted to a white
paper. The Advance Telecommunications Re-
search dialogue database (ATR, 1992) is a par-
allel treebank corpus between Japanese and En-
glish. However, it does not have word or phrase
alignment, and the target domain is restricted
to travel conversation.
Therefore, we have been constructing aligned
parallel treebank corpora of newspaper articles
between languages whose syntactic structures
are different from each other since 2001; they
meet the following conditions.
1. It is easy to investigate the influence of the con-
text on the translation, which means the sen-
tences that come before and after a particular
sentence, and that help us to understand the
meaning of a particular word such as a pro-
noun.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. They are open to the public.
To construct parallel corpora that satisfy these
conditions, each sentence in the Penn Tree-
bank (Release 2) and the Kyoto University text
corpus (Version 3.0) has been translated into
a corresponding natural sentence reflecting its
contextual information in a target language by
skilled translators, revised by native speakers,
and each parallel translation has been anno-
tated with morphological and syntactic struc-
tures, and phrasal alignment. Henceforth, we
call the parallel corpus that is constructed by
pursuing the above policy an aligned parallel
treebank corpus reflecting contextual informa-
tion. In this paper, we describe an aligned par-
allel treebank corpus of newspaper articles be-
tween Japanese, English, and Chinese, and its
applications.
2 Construction of Aligned Parallel
Treebank Corpus Reflecting
Contextual Information
2.1 Human Translation of Existing
Monolingual Treebank
The Penn Treebank is a tagged corpus of Wall
Street Journal material, and it is divided into 24
sections. The Kyoto University text corpus is a
tagged corpus of the Mainichi newspaper, which
is divided into 16 sections according to the cat-
egories of articles such as the sports section and
the economy section. To maintain the consis-
tency of expressions in translation, a few partic-
ular translators were assigned to translate arti-
cles in a particular section, and the same trans-
lator was assigned to the same section. The
instructions to translators for Japanese-English
translation is basically as follows.
1. One-sentence to one-sentence translation as a
rule
Translate a source sentence into a target sen-
tence. In case the translated sentence becomes
unnatural by pursuing this policy, leave a com-
ment.
2. Natural translation reflecting contextual infor-
mation
Except in the case that the translated sentence
becomes unnatural by pursuing policy 1, trans-
late a source sentence into a target sentence
naturally.
By deletion, replacement, or supplementation,
let the translated sentence be natural in the
context.
In an entire article, the translated sentences
must maintain the same meaning and informa-
tion as those of the original sentences.
3. Translations of proper nouns
Find out the translations of proper nouns by
looking up the nouns in a dictionary or by using
a web search. In case a translation cannot be
found, use a temporary name and report it.
We started the construction of Japanese-
Chinese parallel corpus in 2002. The Japanese
sentences of the Kyoto University text corpus
were also translated into Chinese by human
translators. Then each translated Chinese sen-
tence was revised by a second Chinese native.
The instruction to the translators is the same
as that given in the Japanese-English human
translations.
The breakdown of the parallel corpora is
shown in Table 1. We are planning to trans-
late the remaining 18,714 sentences of the Kyoto
University text corpus and the remaining 30,890
sentences of the Penn Treebank. As for the nat-
uralness of the translated sentences, there are
207 (1%) unnatural English sentences of the
Kyoto University text corpus, and 462 (2.5%)
unnatural Japanese sentences of the Penn Tree-
bank generated by pursuing policy 1.
2.2 Morphological and Syntactic
Annotation
In the following sections, we describe the anno-
tated information of the parallel treebank cor-
pus based on the Kyoto University text corpus.
2.2.1 Morphological and Syntactic
Information of Japanese-English
corpus
Translated English sentences were analyzed by
using the Charniak Parser (Charniak, 1999).
Then, the parsed sentences were manually re-
vised. The definitions of part-of-speech (POS)
categories and syntactic labels follow those of
the Treebank I style (Marcus et al, 1993).
We have finished revising the 10,328 parsed
sentences that appeared from January 1st to
11th. An example of morphological and syn-
tactic structures is shown in Figure 1. In this
figure, ?S-ID? means the sentence ID in the
Kyoto University text corpus. EOJ means the
boundary between a Japanese parsed sentence
and an English parsed sentence. The definition
of Japanese morphological and syntactic infor-
mation follows that of the Kyoto University text
corpus (Version 3.0). The syntactic structure is
represented by dependencies between Japanese
phrasal units called bunsetsus. The phrasal
Table 1: Breakdown of the parallel corpora
Original corpus Languages # of parallel sentences
Kyoto University text corpus Japanese-English 19,669 (from Jan. 1st to 17th in 1995)
Japanese-Chinese 38,383 (all)
Penn Treebank Japanese-English 18,318 (from section 0 to 9)
Total Japanese-English 37,987 (Approximately 900,000 English words)
Japanese-Chinese 38,383 (Approximately 900,000 Chinese words)
# S-ID:950104141-008
* 0 2D
???? ???? * ?? * * *
* 1 2D
?? ?????? * ?? ?? * *
? ?? * ??? ???????? * *
?? ??? * ??? ???????? * *
? ? * ?? ???? * *
* 2 6D
?? ???? * ?? ???? * *
? ? ? ??? * ??? ????????
? ? * ?? ?? * *
* 3 4D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 4 5D
??? ???? ??? ?? * ???? ???
* 5 6D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 6 -1D
??? ???? ?? ?? * ?????? ??????
? ? ?? ??? ?????? ???? ???
?? ?? ?? ??? ????????? ???????? ???
? ? * ?? ?? * *
EOJ
(S1 (S (NP (PRP They))
(VP (VP (VBD were)
(NP (DT all))
(ADJP (NP (QP (RB about)
(CD nineteen))
(NNS years))
(JJ old)))
(CC and)
(VP (VBD had)
(S (NP (DT no)
(NN strength))
(VP (VBN left)
(SBAR (S (VP (ADVP (RB even))
(TO to)
(VP (VB answer)
(NP (NNS questions))))))))))
(. .)))
EOE
Figure 1: Example of morphological and syn-
tactic information.
units or bunsetsus are minimal linguistic units
obtained by segmenting a sentence naturally in
terms of semantics and phonetics, and each of
them consists of one or more morphemes.
2.2.2 Chinese Morphological
Information of Japanese-Chinese
corpus
Chinese sentences are composed of strings of
Hanzi and there are no spaces between words.
The morphological annotation, therefore, in-
cludes providing tags of word boundaries and
POSs of words. We analyzed the Chinese sen-
tences by using the morphological analyzer de-
veloped by Peking University (Zhou and Duan,
1994). There are 39 categories in this POS set.
Then the automatically tagged sentences were
revised by the third native Chinese. In this
pass the Chinese translations were revised again
while the results of word segmentation and POS
tagging were revised. Therefore the Chinese
translations are obtained with a high quality.
We have finished revising the 12,000 tagged sen-
tences. The revision of the remaining sentences
is ongoing. An example of tagged Chinese sen-
tences is shown in Figure 2. The letters shown
Figure 2: Example of morphological informa-
tion of Chinese corpus.
after ?/? indicate POSs. The Chinese sentence is
the translation of the Japanese sentence in Fig-
ure 1. The Chinese sentences are GB encoded.
The 38,383 translated Chinese sentences have
1,410,892 Hanzi and 926,838 words.
2.3 Phrasal Alignment
This section describes the annotated informa-
tion of 19,669 sentences of the Kyoto University
text corpus.
The minimum alignment unit should be as
small as possible, because bigger units can be
constructed from units of the minimum size.
However, we decided to define a bunsetsu as the
minimum alignment unit. One of the main rea-
sons for this is that the smaller the unit is, the
higher the human annotation cost is. Another
reason is that if we define a word or a morpheme
as a minimum alignment unit, expressions such
as post-positional particles in Japanese and arti-
cles in English often do not have alignments. To
effectively absorb those expressions and to align
as many parts as possible, we found that a big-
ger unit than a word or a morpheme is suitable
as the minimum alignment unit. We call the
minimum alignment based on bunsetsu align-
ment units the bunsetsu unit translation pair.
Bigger pairs than the bunsetsu unit translation
pairs can be automatically extracted based on
the bunsetsu unit translation pairs. We call all
of the pairs, including bunsetsu unit transla-
tion pairs, translation pairs. The bunsetsu unit
translation pairs for idiomatic expressions often
become unnatural. In this case, two or more
bunsetsu units are combined and handled as a
minimum alignment unit. The breakdown of
the bunsetsu unit translation pairs is shown in
Table 2.
Table 2: Breakdown of the bunsetsu unit trans-
lation pairs.
(1) total # of translation pairs 172,255
(2) # of different translation pairs 146,397
(3) # of Japanese expressions 110,284
(4) # of English expressions 111,111
(5) average # of English expressions 1.33
corresponding to a Japanese expression ((2)/(3))
(6) average # of Japanese expressions 1.32
corresponding to a English expression ((2)/(4))
(7) # of ambiguous Japanese expressions 15,699
(8) # of ambiguous English expressions 12,442
(9) # of bunsetsu unit translation pairs 17,719
consisting of two or more bunsetsus
An example of phrasal alignment is shown in
Figure 3. A Japanese sentence is shown from
the line after the S-ID to the EOJ. Each line
indicates a bunsetsu. Each rectangular line in-
dicates a dependency between bunsetsus. The
leftmost number in each line indicates the bun-
setsu ID. The corresponding English sentence is
shown in the next line after that of the EOJ
(End of Japanese) until the EOE (End of En-
glish). The English expressions corresponding
to each bunsetsu are tagged with the corre-
sponding bunsetsu ID such as <P id=?bunsetsu
ID?></P>. When there are two or more fig-
ures in the tag id such as id=?1,2?, it means two
or more bunsetsus are combined and handled as
a minimum alignment unit.
For example, we can extract the following
translation pairs from Figure 3.
 (J) ??? (yunyuu-ga) / ????? (kaikin-sa-reta);
(E)that had been under the ban
 (J) ??????? (beikoku-san-ringo-no); (E)of apples
imported from the U.S.
 (J) ???? (dai-ichi-bin-ga); (E)The first cargo
 (J)???????(uridasa-reta); (E)was brought to the
market.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga); (E)The first cargo / of apples im-
ported from the U.S.
# S-ID:950110003-001
1 ????????????????
2 ????????????????
3 ????????????????
4 ????????????????
5 ????????????????
6 ????????????????
7 ????????????????
8 ????????????????
9 ????????????????
10 ???????????????
11 ????????????????
EOJ
<P id="4">The first cargo</P> <P id="3">of apples
imported from the U.S.</P> <P id="1,2">that had been
under the ban</P> <P id="7">completed</P> <P id="6">
quarantine</P> <P id="7">and</P> <P id="11">was brought
to the market</P> <P id="10">for the first time</P>
<P id="5">on the 9th</P> <P id="9">at major supermarket
chain stores</P> <P id="8">in the Tokyo metropolitan
area</P> <P id="11">.</P>
EOE
Figure 3: Example of phrasal alignment.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga) /???????(uridasa-reta); (E)The
first cargo / of apples imported from the U.S. / was
brought to the market.
Here, Japanese and English expressions are
divided by the symbol ?;?, and ?/? means a
bunsetsu boundary.
An overview of the criteria of the alignment
is as follows. Align as many parts as possible,
except if a certain part is redundant. More de-
tailed criteria will be attached with our corpus
when it is open to the public.
1. Alignment of English grammatical elements
that are not expressed in Japanese
English articles, possessive pronouns, infinitive
to, and auxiliary verbs are joined with nouns
and verbs.
2. Alignment between a noun and its substitute
expression
A noun can be aligned with its substitute ex-
pression such as a pronoun.
3. Alignment of Japanese ellipses
An English expression is joined with its related
elements. For example, the English subject is
joined with its related verb.
4. Alignment of supplementary or explanatory ex-
pression in English
Supplementary or explanatory expressions in
English are joined with their related words.
? Ex.?
# S-ID:950104142-003
1 ???????????
2 ???????????
3 ???????????
4 ???????????
5 ???????????
6 ???????????
EOJ
<P id="1">The Chinese character used for "ka"</P>
has such meanings as "beautiful" and "splendid."
EOE
?"?? (ka)??? (niwa)" corresponds to
"The Chinese character used for "ka""
5. Alignment of date and time
When a Japanese noun representing date and
time is adverbial, the English preposition is
joined with the date and time.
6. Alignment of coordinate structures
When English expressions represented by ?X
(A + B)? correspond to Japanese expressions
represented by ?XA + XB?, the alignment of
X overlaps.
? Ex.?
# S-ID:950106149-005
1 ?????????????
2 ?????????????
3 ?????????????
4 ?????????????
5 ?????????????
6 ?????????????
7 ?????????????
8 ?????????????
EOJ
In the Kinki Region, disposal of wastes started
<P id="2"><P id="4"> at offshore sites of</P>
Amagasaki</P> and <P id="4">Izumiotsu</P> from
1989 and 1991 respectively.
EOE
?"??? (Amagasaki-oki) ? (de)" corresponds to
"at offshore sites of Amagasaki"
?"???? (Izumiotsu-oki) ? (de)" corresponds to
"at offshore sites of ? Izumiotsu"
3 Applications of Aligned Parallel
Treebank Corpus
3.1 Use for Evaluation of Conventional
Methods
The corpus as described in Section 2 can be
used for the evaluation of English-Japanese and
Japanese-English machine translation. We can
directly compare various methods of machine
translation by using this corpus. It can be sum-
marized as follows in terms of the characteristics
of the corpus.
One-sentence to one-sentence translation
can be simply used for the evaluation of
various methods of machine translation.
Morphological and syntactic information
can be used for the evaluation of methods
that actively use morphological and syntactic
information, such as methods for example-
based machine translation (Nagao, 1981;
Watanabe et al, 2003), or transfer-based
machine translation (Imamura, 2002).
Phrasal alignment is used for the evaluation of
automatically acquired translation knowledge
(Yamamoto and Matsumoto, 2003).
An actual comparison and evaluation is our
future work.
3.2 Analysis of Translation
One-sentence to one-sentence translation
reflects contextual information. Therefore, it
is suitable to investigate the influence of the
context on the translation. For example, we
can investigate the difference in the use of
demonstratives and pronouns between English
and Japanese. We can also investigate the
difference in the use of anaphora.
Morphological and syntactic information
and phrasal alignment can be used to investi-
gate the appropriate unit and size of transla-
tion rules and the relationship between syntac-
tic structures and phrasal alignment.
3.3 Use in Conventional Systems
One-sentence to one-sentence translation
can be used for training a statistical translation
model such as GIZA++ (Och and Ney, 2000),
which could be a strong baseline system for
machine translation.
Morphological and syntactic information
and phrasal alignment can be used to acquire
translation knowledge for example-based ma-
chine translation and transfer-based machine
translation.
In order to show what kind of units are help-
ful for example-based machine translation, we
investigated whether the Japanese sentences of
newspaper articles appearing on January 17,
1995, which we call test-set sentences, could be
translated into English sentences by using trans-
lation pairs appearing from January 1st to 16th
as a database. First, we found that only one out
of 1,234 test-set sentences agreed with one out
of 18,435 sentences in the database. Therefore,
a simple sentence search will not work well. On
the other hand, 6,659 bunsetsus out of 12,632
bunsetsus in the test-set sentences agreed with
those in the database. If words in bunsetsus are
expanded into their synonyms, the combination
of the expanded bunsetsus sets in the database
may cover the test-set sentences. Next, there-
fore, we investigated whether the Japanese test-
set sentences could be translated into English
sentences by simply combining translation pairs
appearing in the database. Given a Japanese
sentence, words were extracted from it and
translation pairs that include those words or
their synonyms, which were manually evalu-
ated, were extracted from the database. Then,
the English sentence was manually generated by
just combining English expressions in the ex-
tracted translation pairs. One hundred two rel-
atively short sentences (the average number of
bunsetsus is about 9.8) were selected as inputs.
The number of equivalent translations, which
mean that the translated sentence is grammat-
ical and has the same meaning as the source
sentence, was 9. The number of similar transla-
tions, which mean that the translated sentence
is ungrammatical, or different or wrong mean-
ings of words, tenses, and prepositions are used
in the translated sentence, was 83. The num-
ber of other translations, which mean that some
words are missing, or the meaning of the trans-
lated sentence is completely different from that
of the original sentence, was 10. For example,
the original parallel translation is as follows:
Japanese:????????????????????????
????????????????????????
English: New Party Sakigake proposed that towards the or-
dinary session, both parties found a council to dis-
cuss policy and Diet management.
Given the Japanese sentence, the translated
sentence was:
Translation:Sakigake Party suggested to set up an organiza-
tion between the two parties towards the regular
session of the Diet to discuss under the theme of
policies and the management of the Diet.
This result shows that only 9% of input sen-
tences can be translated into sentences equiv-
alent to the original ones. However, we found
that approximately 90% of input sentences can
be translated into English sentences that are
equivalent or similar to the original ones.
3.4 Similar Parallel Translation
Generation
The original aim of constructing an aligned par-
allel treebank corpus as described in Section 2 is
to achieve a new framework for translation aid
as described below.
It would be very convenient if multilingual
sentences could be generated by just writing
sentences in our mother language. Today, it
can be formally achieved by using commercial
machine translation systems. However, the au-
tomatically translated sentences are often in-
comprehensible. Therefore, we have to revise
the original and translated sentences by find-
ing and referring to parallel translation whose
source language sentence is similar to the orig-
inal one. In many cases, however, we cannot
find such similar parallel translations to the in-
put sentence. Therefore, it is difficult for users
who do not have enough knowledge of the target
languages to generate comprehensible sentences
in several languages by just searching similar
parallel translations in this way. Therefore, we
propose to generate similar parallel translations
whose source language sentence is similar to
the input sentence. We call this framework for
translation aid similar parallel translation gen-
eration.
We investigated whether the framework can
be achieved by using our aligned parallel tree-
bank corpus. As the first step of this study,
we investigated whether an appropriate parallel
translation can be generated by simply combin-
ing translation pairs extracted from our aligned
parallel treebank corpus in the following steps.
1. Extract each content word with its adjacent
function word in each bunsetsu in a given sen-
tence
2. The extracted content words and their adjacent
function words are expanded into their syn-
onyms and class words whose major and minor
POS categories are the same
3. Find translation pairs including the expanded
content words with their expanded adjacent
function words in the given sentence
4. For each bunsetsu, select a translation pair that
has similar dependency relationship to those in
the given sentence
5. Generate a parallel translation by combining
the selected translation pairs
The input sentences were randomly selected
from 102 sentences described in Section 3.3.
The above steps, except the third step, were
basically conducted manually. The Examples
of the input sentences and generated parallel
translations are shown in Figure 4.
The basic unit of translation pairs in our
aligned parallel treebank corpus is a bunsetsu,
and the basic unit in the selection of transla-
tion pairs is also a bunsetsu. One of the ad-
vantages of using a bunsetsu as a basic unit is
that a Japanese expression represented as one
of various expressions in English, or omitted in
English, such as Japanese post-positional par-
ticles, is paired with a content word. There-
fore, the translation of such an expression is ap-
propriately selected together with the transla-
tion of a content word when a certain trans-
lation pair is selected. If the translation of
such an expression was selected independently
of the translation of a content word, the com-
bination of each translation would be ungram-
matical or unnatural. Another advantage of the
basic unit, bunsetsu, is that we can easily refer
to dependency information between bunsetsus
when we select an appropriate translation pair
because the original treebank has the depen-
dency information between bunsetsus. These
advantages are utilized in the above generation
steps. For example, in the first step, a content
word ??? (kokkai, Diet session)? in the sec-
ond example in Figure 4 was extracted from the
bunsetsu ????? (tsuujo-kokkai, the ordinary
Diet session) ? (ni, case marker)?, and it was
expanded into its class word ?? (kai, meeting)?
in the second step. Then, a translation pair
?(J)??????????? (kokuren-kodomo-
no-kenri-iinkai)? (ni, case marker); (E)the UN
Committee on the Rights of the Child /(J)
?? (taishi); (E)towards? was extracted as a
translation pair in the third step. Since the
dependency between ????????????
(kokuren-kodomo-no-kenri-iinkai, the UN Com-
mittee on the Rights of the Child)? and ???
(taishi, towards)? is similar to that between ?
???? (tsuujo-kokkai, the ordinary Diet ses-
sion)? (ni, case marker)? and ??? (muke, to-
wards)? in the input sentence, this translation
pair was selected in the fourth step. Finally,
the bunsetsu ???????????? (kokuren-
kodomo-no-kenri-iinkai, the UN Committee on
the Rights of the Child) ? (ni, case marker)?
and its translation ?the UN Committee on the
Rights of the Child? was used for generation of
a parallel translation in the fifth step.
When we use the generated parallel transla-
tion for the exact translation of the input sen-
tence, we should replace ??????????
?? (kokuren-kodomo-no-kenri-iinkai)? and its
translation ?the UN Committee on the Rights
of the Child? with ????? (tsuujo-kokkai, the
ordinary Diet session)? and its translation ?the
ordinary Diet session? by consulting a bilingual
dictionary. In this example, ??? (sono)? and
?them? should also be replaced with ??? (ry-
oto)? and ?both parties?. It is easy to identify
words in the generated translation that should
be replaced with words in the input sentence
because each bunsetsu in translation pairs is al-
ready aligned. In such cases, templates such as
?[?? (kaigi)]? (ni)?? (muke)? and ?towards
[council]? can be automatically generated by
generalizing content words expanded in the sec-
ond step and their translation in the generated
translation. The average number of English ex-
pressions corresponding to a Japanese expres-
sion is 1.3 as shown in Table 2. Even when there
are two or more possible English expressions, an
appropriate English expression can be chosen
by selecting a Japanese expression by referring
to dependencies in extracted translation pairs.
Therefore, in many cases, English sentences can
be generated just by reordering the selected ex-
pressions. The English word order was esti-
mated manually in this experiment. However,
we can automatically estimate English word or-
der by using a language model or an English
surface sentence generator such as FERGUS
(Bangalore and Rambow, 2000). Unnatural or
ungrammatical parallel translations are some-
times generated in the above steps. However,
comprehensible translations can be generated
as shown in Figure 4. The biggest advantage
of this framework is that comprehensible target
sentences can be generated basically by refer-
ring only to source sentences. Although it is
costly to search and select appropriate transla-
tion pairs, we believe that human labor can be
reduced by developing a human interface. For
example, when we use a Japanese text gener-
ation system from keywords (Uchimoto et al,
2002), users should only select appropriate key-
words.
We are investigating whether or not we can
generate similar parallel translations to all of
the Japanese sentences appearing on January
17, 1995. So far, we found that we can gen-
erate similar parallel translations to 691 out of
840 sentences (the average number of bunsetsus
is about 10.3) including the 102 sentences de-
scribed in Section 3.3. We found that we could
not generate similar parallel translations to 149
out of 840 sentences.
In the proposed framework of similar paral-
lel translation generation, the language appear-
ing in a corpus corresponds to a controlled lan-
guage, and users are allowed to use only the
controlled language to write sentences in the
source language. We believe that high-quality
bilingual or multilingual documents can be gen-
erated by letting us adapt ourselves to the con-
trolled environment in this way.
4 Conclusion
This paper described aligned parallel treebank
corpora of newspaper articles between lan-
guages whose syntactic structures are different
from each other; they meet the following condi-
tions.
1. It is easy to investigate the influence of the con-
text on the translation.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. It is open to the public.
To construct parallel corpora that satisfy
these conditions, each sentence in the existing
monolingual high-quality treebanks has been
translated into a corresponding natural sentence
reflecting its contextual information in a target
language by skilled translators, and each par-
allel translation has been annotated with mor-
phological and syntactic structures and phrasal
alignment.
This paper also described the possible ap-
plications of the parallel corpus and proposed
a similar parallel translation generation frame-
work. In this framework, a parallel translation
whose source language sentence is similar to a
given sentence can be semi-automatically gen-
erated. In this paper we demonstrated that
the framework could be achieved by using our
aligned parallel treebank corpus.
In the near future, the aligned parallel tree-
bank corpora will be open to the public, and
expanded. We are planning to use the corpora
actively for machine translation, as a transla-
tion aid, and for second language learning. We
are also planning to develop automatic or semi-
automatic alignment system and an efficient in-
terface for machine translation aid.
Input sentence
(Japanese only)
???????????????????????????????????????????(Prime Minister
Murayama and Finance Minister Takemura met in the presidential office and they exchanged their
opinions, mainly on the issue of the new faction being formed by the New Democratic Union.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????
(E) Finance Minister Takemura held the meeting at the official residence to exchange views about the
formation of the new party of the New Democratic Union.
Input sentence
(Japanese only)
????????????????????????????????????????????????(New
Party Sakigake proposed that towards the ordinary session, both parties found a council to discuss policy
and Diet management.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
???????
(E) Sakigake proposed to set up an organization between them towards the UN Committee on the Rights
of the Child to discuss under the theme of policies and the management of the Diet.
Input sentence
(Japanese only)
?????????????????????????????????????????????(The meeting
was also intended to slow the movement towards the new party by the New Democratic Union, which is
trying to deepen the relationship with the New Frontier Party.)
Generated paral-
lel translation
(J) ?????????????????????????????????????????????
(E) The meeting had meanings to restrict the movement that the new party of New Democratic Union
is progressing to strengthen the coalition with The New Frontier Party.
Input sentence
(Japanese only)
?????????????????????????????????????????????????
???????(Lower House Diet Member Tatsuo Kawabata of the New Frontier Party decided on the
16th that he would hand in notification of his secession to the party on the 17th, in order to form a new
faction with Sadao Yamahana?s group.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
?????????
(E) On 16th Tatsuo Kawabata, a member of the House of Representatives of the New Frontier Party
decided to submit The notice to leave the party to the Shinsei Party on the 17th in order to establish a
new faction with Yuukichi Amano and others.
Input sentence
(Japanese only)
???????????????????????????(As for the faction name in the Upper House,
they will decide after they consider how to form a relationship with Democratic Reform Union.)
Generated paral-
lel translation
(J) ?????????????????????
(E) The name of the faction will be decided after discussing the relationship with the JTUC.
Figure 4: Example of generated similar parallel translations.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data.
References
ATR. 1992. Dialogue Database. http://www.red.atr.co.jp/
database page/taiwa.html.
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
E. Charniak. 1999. A Maximum-Entropy-Inspired Parser.
Technical Report CS-99-12.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada
2001. Fast Decoding and Optimal Decoding for Machine
Translation. In Proceedings of the ACL-EACL, pages 228?
235.
K. Imamura. 2002. Application of translation knowledge ac-
quired by hierarchical phrase alignment for pattern-based
MT. In Proceedings of the TMI, pages 74?84.
H. Isahara and M. Haruno. 2000. Japanese-English aligned
bilingual corpora. In Jean Veronis, editor, Parallel Text
Processing - Alignment and Use of Translation Corpora,
pages 313?334. Kluwer Academic Publishers.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Nagao. 1981. A Framework of a Mechanical Translation
between Japanese and English by Analogy Principle. In
Proceedings of the International NATO Symposium on Ar-
tificial and Human Intelligence.
F. J. Och and H. Ney. 2000. Improved Statistical Alignment
Models. In Proceedings of the ACL, pages 440?447.
K. Uchimoto, S. Sekine, and H. Isahara. 2002. Text Gen-
eration from Keywords. In Proceedings of the COLING,
pages 1037?1043.
H. Watanabe, S. Kurohashi, and E. Aramaki. 2003. Finding
Translation Patterns from Paired Source and Target De-
pendency Structures. In Michael Carl and Andy Way, ed-
itors, Recent Advances in Example-Based Machine Trans-
lation, pages 397?420. Kluwer Academic Publishers.
K. Yamada and K. Knight. 2001. A Syntax-based Statistical
Translation Model. In Proceedings of the ACL, pages 523?
530.
K. Yamamoto and Y. Matsumoto. 2003. Extracting Transla-
tion Knowledge from Parallel Corpora. In Michael Carl
and Andy Way, editors, Recent Advances in Example-
Based Machine Translation, pages 365?395. Kluwer Aca-
demic Publishers.
Q. Zhou and H. Duan. 1994. Segmentation and POS Tag-
ging in the Construction of Contemporary Chinese Cor-
pus. Journal of Computer Science of China, Vol.85. (in
Chinese)
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 197?200, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Support Vector Machines
Tomohiro Mitsumori
 
, Masaki Murata

, Yasushi Fukuda

Kouichi Doi
 
, and Hirohumi Doi
 

Graduate School of Information Science, Nara Institute of Science and Technology
8916-5, Takayama-cho, Ikoma-shi, Nara, 630-0101, Japan

mitsumor,doy  @is.naist.jp, doi@cl-sciences.co.jp

National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
murata@nict.go.jp

Sony-Kihara Research Center Inc.
1-14-10 Higashigotanda, Shinagawa-ku, Tokyo, 141-0022, Japan
yasu@krc.sony.co.jp
Abstract
In this paper, we describe our systems for
the CoNLL-2005 shared task. The aim of
the task is semantic role labeling using a
machine-learning algorithm. We apply the
Support Vector Machines to the task. We
added new features based on full parses
and manually categorized words. We also
report on system performance and what
effect the newly added features had.
1 Introduction
The CoNLL-2005 shared task (Carreras and
Ma`rquez, 2005) concerns the recognition of au-
tomatic semantic roles for the English language.
Given a sentence, the task consists of analyzing the
propositions expressed by various target verbs of the
sentence. The semantic roles of constituents of the
sentence are extracted for each target verb. There
are semantic arguments such as Agent, Patient, and
Instrument and also adjuncts such as Locative and
Temporal. We performed the semantic role labeling
using Support Vector Machines (SVMs). Systems
that used SVMs achieved good performance in the
CoNLL-2004 shared task, and we added data on full
parses to it. We prepared a feature used by the full
parses, and we also categorized words that appeared
in the training set and added them as features. Here,
we report on systems for automatically labeling se-
mantic roles in a closed challenge in the CoNLL-
2005 shared task.
This paper is arranged as follows. Section 2 de-
scribes the SVMs. Our system is described Sec-
tion 3, where we also describe methods of data rep-
resentation, feature coding, and the parameters of
SVMs. The experimental results and conclusion are
presented in Sections 4 and 5.
2 Support Vector Machines
SVMs are one of the binary classifiers based on
the maximum margin strategy introduced by Vap-
nik (Vapnik, 1995). This algorithm has achieved
good performance in many classification tasks, e.g.
named entity recognition and document classifica-
tion. There are some advantages to SVMs in that
(i) they have high generalization performance inde-
pendent of the dimensions of the feature vectors and
(ii) learning with a combination of multiple features
is possible by using the polynomial kernel func-
tion (Yamada and Matsumoto, 2003). SVMs were
used in the CoNLL-2004 shred task and achieved
good performance (Hacioglu et al, 2004) (Kyung-
Mi Park and Rim, 2004). We used YamCha (Yet
Another Multipurpose Chunk Annotator) 1 (Kudo
and Matsumoto, 2001), which is a general purpose
SVM-based chunker. We also used TinySVM2 as a
package for SVMs.
3 System Description
3.1 Data Representation
We changed the representation of original data ac-
cording to Hacioglu et al (Hacioglu et al, 2004) in
our system.
1http://chasen.org/? taku/software/yamcha/
2http://chasen.org/? taku/software/TinySVM/
197
  Bracketed representation of roles was con-
verted into IOB2 representation (Ramhsaw and
Marcus, 1995) (Sang and Veenstra, 1999).
  Word-by-word was changed to the phrase-by-
phrase method (Hacioglu et al, 2004).
Word tokens were collapsed into base phrase (BP)
tokens. The BP headwords were rightmost words.
Verb phrases were not collapsed because some in-
cluded more the one predicate.
3.2 Feature Coding
We prepared the training and development set by us-
ing files corresponding to: words, predicated partial
parsing (part-of-speech, base chunks), predicate full
parsing trees (Charniak models), and named entities.
We will describe feature extraction according to Fig.
1. Figure 1 shows an example of an annotated sen-
tence.
1st Words (Bag of Words): All words appearing in
the training data.
2nd Part of Speech (POS) Tags
3rd Base Phrase Tags: Partial parses (chunks +
clauses) predicted with UPC processors.
4th Named Entities
5th Token Depth : This means the degree of depth
from a predicate (see Fig. 2). We used full
parses predicted by the Charniak parser. In this
figure, the depth of paid , which is a predicate,
is zero and the depth of April is -2.
6th Words of Predicate
7th Position of Tokens: The position of the current
word from the predicate. This has three value
of ?before?, ?after?, and ?-? (for the predicate).
8th Phrase Distance on Flat Path: This means the
distance from the current token to the predi-
cate as a number of the phrase on flat path.
For example, the phrase distance of ?April? is
4, because two ?NP? and one ?PP? exist from
?paid?(predicate) to ?April? (see 3rd column in
Fig.1).
Table 1: Five most frequently categorized BP head-
words appearing in training set.
Class Examples
Person he, I, people, investors, we
Organization company, Corp., Inc., companies, group
Time year, years, time, yesterday, months
Location Francisco, York, California, city, America
Number %, million, billion, number, quarter
Money price, prices, cents, money, dollars
9th Flat Path: This means the path from the current
word to the predicate as a chain of the phrases.
The chain begins from the BP of the current
word to the BP of the predicate.
10th Semantic Class : We collected the most fre-
quently occurring 1,000 BP headwords appear-
ing in the training set and tried to manually
classified. The five classes (person, organiza-
tion, time, location, number and money) were
relatively easy to classify. In the 1,000 words,
the 343 words could be classified into the five
classes. Remainder could not be classified. The
details are listed in Table 1.
Preceding class: The class (e.g. B-A0 or I-A1) of
the token(s) preceding the current token. The
number of preceding tokens is dependent on the
window size. In this paper, the left context con-
sidered is two.


	



	






	

	
 


Proceedings of the Workshop on Information Extraction Beyond The Document, pages 1?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Development of an automatic trend exploration system
using the MuST data collection
Masaki Murata1
murata@nict.go.jp
Qing Ma3,1
3qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru1,4
1kanamaru@nict.go.jp
Hitoshi Isahara1
isahara@nict.go.jp
1National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
3Ryukoku University
Otsu, Shiga, 520-2194, Japan
Koji Ichii2
ichiikoji@hiroshima-u.ac.jp
Tamotsu Shirado1
shirado@nict.go.jp
Sachiyo Tsukawaki1
tsuka@nict.go.jp
2Hiroshima University
1-4-1 Kagamiyama, Higashi-hiroshima,
Hiroshima 739-8527, Japan
4Kyoto University
Yoshida-nihonmatsu-cho, Sakyo-ku,
Kyoto, 606-8501, Japan
Abstract
The automatic extraction of trend informa-
tion from text documents such as news-
paper articles would be useful for explor-
ing and examining trends. To enable this,
we used data sets provided by a workshop
on multimodal summarization for trend in-
formation (the MuST Workshop) to con-
struct an automatic trend exploration sys-
tem. This system first extracts units, tem-
porals, and item expressions from news-
paper articles, then it extracts sets of ex-
pressions as trend information, and finally
it arranges the sets and displays them in
graphs. For example, when documents
concerning the politics are given, the sys-
tem extracts ?%? and ?Cabinet approval
rating? as a unit and an item expression in-
cluding temporal expressions. It next ex-
tracts values related to ?%?. Finally, it
makes a graph where temporal expressions
are used for the horizontal axis and the
value of percentage is shown on the ver-
tical axis. This graph indicates the trend
of Cabinet approval rating and is useful
for investigating Cabinet approval rating.
Graphs are obviously easy to recognize
and useful for understanding information
described in documents. In experiments,
when we judged the extraction of a correct
graph as the top output to be correct, the
system accuracy was 0.2500 in evaluation
A and 0.3334 in evaluation B. (In evalua-
tion A, a graph where 75% or more of the
points were correct was judged to be cor-
rect; in evaluation B, a graph where 50%
or more of the points were correct was
judged to be correct.) When we judged
the extraction of a correct graph in the top
five outputs to be correct, accuracy rose to
0.4167 in evaluation A and 0.6250 in eval-
uation B. Our system is convenient and ef-
fective because it can output a graph that
includes trend information at these levels
of accuracy when given only a set of doc-
uments as input.
1 Introduction
We have studied ways to automatically extract
trend information from text documents, such as
newspaper articles, because such a capability will
be useful for exploring and examining trends. In
this work, we used data sets provided by a work-
shop on multimodal summarization for trend in-
formation (the MuST Workshop) to construct an
automatic trend exploration system. This system
firsts extract units, temporals, and item expres-
sions from newspaper articles, then it extract sets
of expressions as trend information, and finally it
arranges the sets and displays them in graphs. For
example, when documents concerning the politics
1
are given, the system extracts ?%? and ?Cabinet
approval rating? as a unit and an item expression
including temporal expressions. It next extracts
values related to ?%?. Finally, it makes a graph
where temporal expressions are used for the hor-
izontal axis and the value of percentage is shown
on the vertical axis. This graph indicates the trend
of Cabinet approval rating and is useful for inves-
tigating Cabinet approval rating. Graphs are obvi-
ously easy to recognize and useful for understand-
ing information described in documents.
2 The MuST Workshop
Kato et al organized the workshop on multimodal
summarization for trend information (the MuST
Workshop) (Kato et al, 2005). In this work-
shop, participants were given data sets consisting
of newspaper documents (editions of the Mainichi
newspaper from 1998 and 1999 (Japanese docu-
ments)) that included trend information for vari-
ous domains. In the data, tags for important ex-
pressions (e.g. temporals, numerical expressions,
and item expressions) were tagged manually.1 The
20 topics of the data sets (e.g., the 1998 home-run
race to break the all-time Major League record,
the approval rating for the Japanese Cabinet, and
news on typhoons) were provided. Trend infor-
mation was defined as information regarding the
change in a value for a certain item. A change in
the number of home runs hit by a certain player or
a change in the approval rating for the Cabinet are
examples of trend information. In the workshop,
participants could freely use the data sets for any
study they chose to do.
3 System
3.1 Structure of the system
Our automatic trend exploration system consists
of the following components.
1. Component to extract important expressions
First, documents related to a certain topic are
given to the system, which then extracts im-
portant expressions that will be used to ex-
tract and merge trend information. The sys-
tem extracts item units, temporal units, and
item expressions as important expressions.
1We do not use manually provided tags for important ex-
pressions because our system automatically extracts impor-
tant expressions.
Here, important expressions are defined as
expressions that play important roles in a
given document set. Item expressions are de-
fined as expressions that are strongly related
to the content of a given document set.
1a. Component to extract important item
units
The system extracts item units that will
be used to extract and merge trend infor-
mation.
For example, when documents concern-
ing the home-run race are given, ?hon?
or ?gou? (the Japanese item units for the
number of home runs) such as in ?54
hon? (54th home run) are extracted.
1b. Component to extract important tempo-
ral units
The system extracts temporal units that
will also be used to extract and merge
trend information.
For example, the system extracts tempo-
ral units such as ?nichi? (day), ?gatsu?
(month), and ?nen? (year). In Japanese,
temporal units are used to express dates,
such as in ?2006 nen, 3 gatsu, 27 nichi?
for March 27th, 2006.
1c. Component to extract important item
expressions
The system extracts item expressions
that will also be used to extract and
merge trend information.
For example, the system extracts expres-
sions that are objects for trend explo-
ration, such as ?McGwire? and ?Sosa?
as item expressions in the case of docu-
ments concerning the home-run race.
2. Component to extract trend information sets
The system identifies the locations in sen-
tences where a temporal unit, an item unit,
and an item expression that was extracted by
the component to extract important expres-
sions appear in similar sentences and extracts
sets of important expressions described by
the sentences as a trend information set. The
system also extracts numerical values appear-
ing with item units or temporal units, and
uses the connection of the numerical values
and the item units or temporal units as nu-
merical expressions or temporal expressions.
2
For example, in the case of documents con-
cerning the home-run race, the system ex-
tracts a set consisting of ?item expression:
McGwire?, ?temporal expression: 11 day?
(the 11th), and ?numerical expression: 47
gou? (47th home run) as a trend information
set.
3. Component to extract and display important
trend information sets
The system gathers the extracted trend infor-
mation sets and displays them as graphs or by
highlighting text displays.
For example, for documents concerning
the home-run race, the system displays as
graphs the extracted trend information sets
for ?McGwire? . In these graphs, temporal
expressions are used for the horizontal axis
and the number of home runs is shown on the
vertical axis.
3.2 Component to extract important
expressions
The system extracts important expressions that
will be used to extract trend information sets. Im-
portant expressions belong to one of the following
categories.
? item units
? temporal units
? item expressions
We use ChaSen (Matsumoto et al, 1999), a
Japanese morphological analyzer, to extract ex-
pressions. Specifically, we use the parts of
speeches in the ChaSen outputs to extract the ex-
pressions.
The system extracts item units, temporal units,
and item expressions by using manually con-
structed rules using the parts of speeches. The
system extracts a sequence of nouns adjacent to
numerical values as item units. It then extracts
expressions from among the item units which in-
clude an expression regarding time or date (e.g.,
?year?, ?month?, ?day?, ?hour?, or ?second?) as
temporal units. The system extracts a sequence of
nouns as item expressions.
The system next extracts important item units,
temporal units, and item expressions that play im-
portant roles in the target documents.
The following three methods can be used to ex-
tract important expressions. The system uses one
of them. The system judges that an expression
producing a high value from the following equa-
tions is an important expression.
? Equation for the TF numerical term in Okapi
(Robertson et al, 1994)
Score =
?
i?Docs
TF
i
TF
i
+
l
i
?
(1)
? Use of total word frequency
Score =
?
i?Docs
TF
i
(2)
? Use of total frequency of documents where a
word appears
Score =
?
i?Docs
1 (3)
In these equations, i is the ID (identification
number) of a document, Docs is a set of document
IDs, TF
i
is the occurrence number of an expres-
sion in document i, l is the length of document i,
and ? is the average length of documents inDocs.
To extract item expressions, we also applied a
method that uses the product of the occurrence
number of an expression in document i and the
length of the expression as TF
i
, so that we could
extract longer expressions.
3.3 Component to extract trend information
sets
The system identifies the locations in sentences
where a temporal unit, an item unit, and an item
expression extracted by the component to extract
important expressions appears in similar sentences
and extracts sets of important expressions de-
scribed by the sentences as a trend information
set. When more than one trend information set
appears in a document, the system extracts the one
that appears first. This is because important and
new things are often described in the beginning of
a document in the case of newspaper articles.
3.4 Component to extract and display
important trend information sets
The system gathers the extracted trend informa-
tion sets and displays them in graphs or as high-
lighted text. In the graphs, temporal expressions
3
are used for the horizontal axis and numerical ex-
pressions are used for the vertical axis. The system
also displays sentences used to extract trend infor-
mation sets and highlights important expressions
in the sentences.
The system extracts multiple item units, tempo-
ral units, and item expressions (through the com-
ponent to extract important expressions) and uses
these to make all possible combinations of the
three kinds of expression. The system extracts
trend information sets for each combination and
calculates the value of one of the following equa-
tions for each combination. The system judges
that the combination producing a higher value rep-
resents more useful trend information. The fol-
lowing four equations can be used for this purpose,
and the system uses one of them.
? Method 1 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? S
1
? S
2
? S
3
(4)
? Method 2 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? (S
1
? S
2
? S
3
)
1
3 (5)
? Method 3 ? Use the frequency of trend in-
formation sets
M = Freq (6)
? Method 4 ? Use the scores of important ex-
pressions
M = S
1
? S
2
? S
3
(7)
In these equations, Freq is the number of trend
information sets extracted as described in Section
3.3, and S1, S2, and S3 are the values of Score as
calculated by the corresponding equation in Sec-
tion 3.2.
The system extracts the top five item units, the
top five item expressions, and the top three tem-
poral units through the component to extract im-
portant expressions and forms all possible combi-
nations of these (75 combinations). The system
then calculates the value of the above equations for
these 75 combinations and judges that a combina-
tion having a larger value represents more useful
trend information.
4 Experiments and Discussion
We describe some examples of the output of our
system in Sections 4.1, 4.2, and 4.3, and the re-
sults from our system evaluation in Section 4.4.
We made experiments using Japanese newspaper
articles.
4.1 Extracting important expressions
To extract important expressions we applied the
equation for the TF numerical term in Okapi and
the method using the product of the occurrence
number for an expression and the length of the
expression as TF
i
for item expressions. We did
experiments using the three document sets for ty-
phoons, the Major Leagues, and political trends.
The results are shown in Table 1.
We found that appropriate important expres-
sions were extracted for each domain. For ex-
ample, in the data set for typhoons, ?typhoon?
was extracted as an important item expression and
an item unit ?gou? (No.), indicating the ID num-
ber of each typhoon, was extracted as an im-
portant item unit. In the data set for the Major
Leagues, the MuST data included documents de-
scribing the home-run race between Mark McG-
wire and Sammy Sosa in 1998. ?McGwire? and
?Sosa? were properly extracted among the higher
ranks. ?gou? (No.) and ?hon? (home run(s)), im-
portant item units for the home-run race, were
properly extracted. In the data set for political
trends, ?naikaku shiji ritsu? (cabinet approval rat-
ing) was properly extracted as an item expression
and ?%? was extracted as an item unit.
4.2 Graphs representing trend information
We next tested how well our system graphed the
trend information obtained from the MuST data
sets. We used the same three document sets as in
the previous section. As important expressions in
the experiments, we used the item unit, the tempo-
ral unit, and the item expression with the highest
scores (the top ranked ones) which were extracted
by the component to extract important expressions
using the method described in the previous sec-
tion. The system made the graphs using the com-
ponent to extract trend information sets and the
component to extract and display important trend
information sets. The graphs thus produced are
shown in Figs. 1, 2, and 3. (We used Excel to draw
these graphs.) Here, we made a temporal axis for
each temporal expression. However, we can also
4
Table 1: Examples of extracting important expressions
Typhoon
item units temporal units item expressions
gou nichi taihuu
(No.) (day) (typhoon)
me-toru ji gogo
(meter(s)) (o?clock) (afternoon)
nin jigoro higai
(people) (around x o?clock) (damage)
kiro fun shashin setsumei
(kilometer(s)) (minute(s)) (photo caption)
miri jisoku chuushin
(millimeter(s)) (per hour) (center)
Major League
item units temporal units item expressions
gou nichi Maguwaia
(No.) (day) (McGwire)
hon nen honruida
(home run(s)) (year) (home run)
kai gatsu Ka-jinarusu
(inning(s)) (month) (Cardinals)
honruida nen buri Ma-ku Maguwaia ichiruishu
(home run(s)) (after x year(s) interval) (Mark McGwire, the first baseman)
shiai fun So-sa
(game(s)) (minute(s)) (Sosa)
Political Trend
item units temporal units item expressions
% gatsu naikaku shiji ritsu
(%) (month) (cabinet approval rating)
pointo gen nichi Obuchi naikaku
(decrease of x point(s)) (day) (Obuchi Cabinet)
pointo zou nen Obuchi shushou
(increase of x point(s)) (year) (Prime Minister Obuchi)
dai kagetu shijiritsu
(generation) (month(s)) (approval rating)
pointo bun no kitai
(point(s)) (divided) (expectation)
5
Figure 1: Trend graph for the typhoon data set
Figure 2: Trend graph for the Major Leagues data
set
display a graph where regular temporal intervals
are used in the temporal axis.
For the typhoon data set, gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as the top ranked item unit, temporal unit, and
item expression. The system extracted trend in-
formation sets using these, and then made a graph
where the temporal expression (day) was used for
the horizontal axis and the ID numbers of the ty-
phoons were shown on the vertical axis. The
MuST data included data for September and Octo-
ber of 1998 and 1999. Figure 1 is useful for seeing
when each typhoon hit Japan during the typhoon
season each year. Comparing the 1998 data with
that of 1999 reveals that the number of typhoons
increased in 1999.
For the Major Leagues data set, gou (No.), nichi
(day), and Maguwaia (McGwire) were extracted
with the top rank. The system used these to make
a graph where the temporal expression (day) was
used for the horizontal axis and the cumulative
number of home runs hit by McGwire was shown
on the vertical axis (Fig. 2). The MuST data
included data beginning in August, 1998. The
graph shows some points where the cumulative
number of home runs decreased (e.g., September
Figure 3: Trend graph for the political trends data
set
4th), which was obviously incorrect. This was be-
cause our system wrongly extracted the number of
home runs hit by Sosa when this was given close
to McGwire?s total.
In the political trends data set, %, gatsu
(month), and naikaku shiji ritsu (cabinet approval
rating) were extracted with the top rankings. The
system used these to make a graph where the
temporal expression (month) was used for the
horizontal axis and the Cabinet approval rating
(Japanese Cabinet) was shown as a percentage on
the vertical axis. The MuST data covered 1998
and 1999. Figure 2 shows the cabinet approval
rating of the Obuchi Cabinet. We found that the
overall approval rating trend was upwards. Again,
there were some errors in the extracted trend infor-
mation sets. For example, although June was han-
dled correctly, the system wrongly extracted May
as a temporal expression from the sentence ?in
comparison to the previous investigation in May?.
4.3 Sentence extraction and highlighting
display
We then tested the sentence extraction and high-
lighting display with respect to trend information
using the MuST data set; in this case, we used
the typhoon data set. As important expressions,
we used the item unit, the temporal unit, and the
item expression extracted with the highest scores
(the top ranked ones) by the component to extract
important expressions using the method described
in the previous section. Gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as an item unit, a temporal unit, and an item ex-
pression. The system extracted sentences includ-
ing the three expressions and highlighted these ex-
pressions in the sentences. The results are shown
in Figure 4. The first trend information sets to ap-
6
Sept. 16, 1998 No. 5
Large-scale and medium-strength Typhoon No. 5 made landfall near Omaezaki in Shizuoka Pre-
fecture before dawn on the 16th, and then moved to the northeast involving the Koshin, Kantou,
and Touhoku areas in the storm.
Sept. 21, 1998 No. 8
Small-scale Typhoon No. 8 made landfall near Tanabe City in Wakayama Prefecture around 4:00
p.m. on the 21st, and weakened while tracking to the northward across Kinki district.
Sept. 22, 1998 No. 7
Typhoon No. 7 made landfall near Wakayama City in the afternoon on the 22nd, and will hit the
Kinki district.
Sept. 21, 1998 No. 8
The two-day consecutive landfall of Typhoon No. 8 on the 21st and Typhoon No. 7 on the 22nd
caused nine deaths and many injuries in a total of six prefectures including Nara, Fukui, Shiga,
and so on.
Oct. 17, 1998 No. 10
Medium-scale and medium-strength Typhoon No. 10 made landfall on Makurazaki City in
Kagoshima Prefecture around 4:30 p.m. on the 17th, and then moved across the West Japan area
after making another landfall near Sukumo City in Kochi Prefecture in the evening.
Aug. 20, 1999 No. 11
The Meteorological Office announced on the 20th that Typhoon No. 11 developed 120 kilometers
off the south-southwest coast of Midway.
Sept. 14, 1999 No. 16
Typhoon No. 16, which developed off the south coast in Miyazaki Prefecture, made landfall near
Kushima City in the prefecture around 5:00 p.m. on the 14th.
Sept. 15, 1999 No. 16
Small-scale and weak Typhoon No. 16 became extratropical in Nagano Prefecture and moved out
to sea off Ibaraki Prefecture on the 15th.
Sept. 24, 1999 No. 18
Medium-scale and strong Typhoon No. 18 made landfall in the north of Kumamoto Prefecture
around 6:00 a.m. on the 24th, and after moving to Suo-Nada made another landfall at Ube City
in Yamaguchi Prefecture before 9:00 p.m., tracked through the Chugoku district, and then moved
into the Japan Sea after 10:00 p.m.
Sept. 25, 1999 No. 18
Typhoon No. 18, which caused significant damage in the Kyushu and Chugoku districts, weakened
and made another landfall before moving into the Sea of Okhotsk around 10:00 a.m. on the 25th.
Figure 4: Sentence extraction and highlighting display for the typhoon data set
7
pear are underlined twice and the other sets are
underlined once. (In the actual system, color is
used to make this distinction.) The extracted tem-
poral expressions and numerical expressions are
presented in the upper part of the extracted sen-
tence. The graphs shown in the previous section
were made by using these temporal expressions
and numerical expressions.
The extracted sentences plainly described the
state of affairs regarding the typhoons and were
important sentences. For the research being done
on summarization techniques, this can be consid-
ered a useful means of extracting important sen-
tences. The extracted sentences typically describe
the places affected by each typhoon and whether
there was any damage. They contain important
descriptions about each typhoon. This confirmed
that a simple method of extracting sentences con-
taining an item unit, a temporal unit, and an item
expression can be used to extract important sen-
tences.
The fourth sentence in the figure includes infor-
mation on both typhoon no.7 and typhoon no.8.
We can see that there is a trend information set
other than the extracted trend information set (un-
derlined twice) from the expressions that are un-
derlined once. Since the system sometimes ex-
tracts incorrect trend information sets, the high-
lighting is useful for identifying such sets.
4.4 Evaluation
We used a closed data set and an open data set
to evaluate our system. The closed data set was
the data set provided by the MuST workshop or-
ganizer and contained 20 domain document sets.
The data sets were separated for each domain.
We made the open data set based on the MuST
data set using newspaper articles (editions of the
Mainichi newspaper from 2000 and 2001). We
made 24 document sets using information retrieval
by term query. We used documents retrieved by
term query as the document set of the domain for
each query term.
We used the closed data set to adjust our system
and used the open data set to calculate the evalua-
tion scores of our system for evaluation.
We judged whether a document set included the
information needed to make trend graphs by con-
sulting the top 30 combinations of three kinds of
important expression having the 30 highest values
as in the method of Section 3.4. There were 19
documents including such information in the open
data. We used these 19 documents for the follow-
ing evaluation.
In the evaluation, we examined how accurately
trend graphs could be output when using the top
ranked expressions. The results are shown in Table
2. The best scores are described using bold fonts
for each evaluation score.
We used five evaluation scores. MRR is the av-
erage of the score where 1/r is given as the score
when the rank of the first correct output is r (Mu-
rata et al, 2005b). TP1 is the average of the pre-
cision in the first output. TP5 is the average of
the precision where the system includes a correct
output in the first five outputs. RP is the average
of the r-precision and AP is the average of the av-
erage precision. (Here, the average means that the
evaluation score is calculated for each domain data
set and the summation of these scores divided by
the number of the domain data sets is the average.)
R-precision is the precision of the r outputs where
r is the number of correct answers. Average pre-
cision is the average of the precision when each
correct answer is output (Murata et al, 2000). The
r-precision indicates the precision where the recall
and the precision have the same value. The preci-
sion is the ratio of correct answers in the system
output. The recall is the ratio of correct answers
in the system output to the total number of correct
answers.
Methods 1 to 4 in Table 2 are the methods used
to extract useful trend information described in
Section 3.4. Use of the expression length means
the product of the occurrence number for an ex-
pression and the length of the expression was used
to calculate the score for an important item ex-
pression. No use of the expression length means
this product was not used and only the occurrence
number was used.
To calculate the r-precision and average preci-
sion, we needed correct answer sets. We made the
correct answer sets by manually examining the top
30 outputs for the 24 (= 4? 6) methods (the com-
binations of methods 1 to 4 and the use of Equa-
tions 1 to 3 with or without the expression length)
and defining the useful trend information among
them as the correct answer sets.
In evaluation A, a graph where 75% or more of
the points were correct was judged to be correct.
In evaluation B, a graph where 50% or more of the
points were correct was judged to be correct.
8
Table 2: Experimental results for the open data
Evaluation A Evaluation B
MRR TP1 TP5 RP AP MRR TP1 TP5 RP AP
Use of Equation 1 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1360 0.1162 0.5522 0.4211 0.7368 0.1968 0.1565
Method 2 0.3847 0.3158 0.4211 0.1360 0.1150 0.5343 0.4211 0.6316 0.1880 0.1559
Method 3 0.3557 0.2632 0.4211 0.1360 0.1131 0.5053 0.3684 0.6316 0.1805 0.1541
Method 4 0.3189 0.2632 0.4211 0.1125 0.0973 0.4492 0.3158 0.6316 0.1645 0.1247
Use of Equation 2 and the expression length
Method 1 0.3904 0.3158 0.4737 0.1422 0.1154 0.5746 0.4211 0.7368 0.2127 0.1674
Method 2 0.3877 0.3158 0.4737 0.1422 0.1196 0.5544 0.4211 0.7368 0.2127 0.1723
Method 3 0.3895 0.3158 0.5263 0.1422 0.1202 0.5491 0.4211 0.7895 0.2127 0.1705
Method 4 0.2216 0.1053 0.3684 0.0846 0.0738 0.3765 0.2105 0.5789 0.1328 0.1043
Use of Equation 3 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
Use of Equation 1 and no use of the expression length
Method 1 0.3789 0.3158 0.4737 0.1294 0.1152 0.5456 0.4211 0.7368 0.2002 0.1627
Method 2 0.3750 0.3158 0.4211 0.1294 0.1137 0.5215 0.4211 0.6842 0.2002 0.1621
Method 3 0.3333 0.2632 0.4211 0.1119 0.1072 0.4798 0.3684 0.6842 0.1763 0.1552
Method 4 0.2588 0.1053 0.4737 0.1269 0.0872 0.3882 0.1579 0.6842 0.1833 0.1189
Use of Equation 2 and no use of the expression length
Method 1 0.3277 0.2105 0.4737 0.1134 0.0952 0.4900 0.2632 0.7895 0.1779 0.1410
Method 2 0.3662 0.2632 0.4737 0.1187 0.1104 0.5417 0.3684 0.7368 0.1831 0.1594
Method 3 0.3504 0.2632 0.4737 0.1187 0.1116 0.5167 0.3684 0.7368 0.1884 0.1647
Method 4 0.1877 0.0526 0.3684 0.0775 0.0510 0.3131 0.1053 0.5263 0.1300 0.0879
Use of Equation 3 and no use of the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
9
From the experimental results, we found that
the method using the total frequency for a word
(Equation 2) and the length of an expression was
best for calculating the scores of important expres-
sions.
Using the length of an expression was impor-
tant. (The way of using the length of an expres-
sion was described in the last part of Section 3.2.)
For example, when ?Cabinet approval rating? ap-
pears in documents, a method without expression
lengths extracts ?rating?. When the system ex-
tracts trend information sets using ?rating?, it ex-
tracts wrong information related to types of ?rat-
ing? other than ?Cabinet approval rating?. This
hinders the extraction of coherent trend informa-
tion. Thus, it is beneficial to use the length of an
expression when extracting important item expres-
sions.
We also found that method 1 (using both the fre-
quency of the trend information sets and the scores
of important expressions) was generally the best.
When we judged the extraction of a correct
graph as the top output in the experiments to be
correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
In terms of the evaluation scores for the 24 original
data sets (these evaluation scores were multiplied
by 19/24), when we judged the extraction of a cor-
rect graph as the top output in the experiments to
be correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
Our system is convenient and effective because it
can output a graph that includes trend information
at these levels of accuracy when given only a set
of documents as input.
As shown in Table 2, the best values for RP
(which indicates the precision where the recall and
the precision have the same value) and AP were
0.2127 and 0.1705, respectively, in evaluation B.
This RP value indicates that our system could
extract about one out of five graphs among the cor-
rect answers when the recall and the precision had
the same value.
5 Related studies
Fujihata et al (Fujihata et al, 2001) developed a
system to extract numerical expressions and their
related item expressions by using syntactic infor-
mation and patterns. However, they did not deal
with the extraction of important expressions or
gather trend information sets. In addition, they did
not make a graph from the extracted expressions.
Nanba et al (Nanba et al, 2005) took an
approach of judging whether the sentence rela-
tionship indicates transition (trend information)
or renovation (revision of information) and used
the judgment results to extract trend information.
They also constructed a system to extract nu-
merical information from input numerical units
and make a graph that includes trend information.
However, they did not consider ways to extract
item numerical units and item expressions auto-
matically.
In contrast to these systems, our system auto-
matically extracts item numerical units and item
expressions that each play an important role in a
given document set. When a document set for
a certain domain is given, our system automati-
cally extracts item numerical units and item ex-
pressions, then extracts numerical expressions re-
lated to these, and finally makes a graph based
on the extracted numerical expressions. When a
document set is given, the system automatically
makes a graph that includes trend information.
Our system also uses an original method of pro-
ducing more than one graphs and selecting an ap-
propriate graph among them using Methods 1 to 4,
which Fujihata et al and Namba et al did not use.
6 Conclusion
We have studied the automatic extraction of trend
information from text documents such as newspa-
per articles. Such extraction will be useful for ex-
ploring and examining trends. We used data sets
provided by a workshop on multimodal summa-
rization for trend information (the MuST Work-
shop) to construct our automatic trend exploration
system. This system first extracts units, tempo-
rals, and item expressions from newspaper arti-
cles, then it extracts sets of expressions as trend
information, and finally it arranges the sets and
displays them in graphs.
In our experiments, when we judged the extrac-
tion of a correct graph as the top output to be cor-
rect, the system accuracy was 0.2500 in evaluation
10
A and 0.3334 in evaluation B. (In evaluation A, a
graph where 75% or more of the points were cor-
rect was judged to be correct; in evaluation B, a
graph where 50% or more of the points were cor-
rect was judged to be correct.) When we judged
the extraction of a correct graph in the top five out-
puts to be correct, we obtained accuracy of 0.4167
in evaluation A and 0.6250 in evaluation B. Our
system is convenient and effective because it can
output a graph that includes trend information at
these levels of accuracy when only a set of docu-
ments is provided as input.
In the future, we plan to continue this line of
study and improve our system. We also hope to
apply the method of using term frequency in doc-
uments to extract trend information as reported by
Murata et al (Murata et al, 2005a).
References
Katsuyuki Fujihata, Masahiro Shiga, and Tatsunori
Mori. 2001. Extracting of numerical expressions
by constraints and default rules of dependency struc-
ture. Information Processing Society of Japan,
WGNL 145.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. MuST: A workshop on multimodal
summarization for trend information. Proceedings
of the Fifth NTCIR WorkshopMeeting on Evaluation
of Information Access Technologies: Information
Retrieval, Question Answering and Cross-Lingual
Information Access.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis
system ChaSen version 2.0 manual 2nd edition.
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara.
2000. Japanese probabilistic information retrieval
using location and category information. The Fifth
International Workshop on Information Retrieval
with Asian Languages, pages 81?88.
Masaki Murata, Koji Ichii, Qing Ma, Tamotsu Shirado,
Toshiyuki Kanamaru, and Hitoshi Isahara. 2005a.
Trend survey on Japanese natural language process-
ing studies over the last decade. In The Second In-
ternational Joint Conference on Natural Language
Processing, Companion Volume to the Proceedings
of Conference including Posters/Demos and Tutorial
Abstracts.
Masaki Murata, Masao Utiyama, and Hitoshi Isahara.
2005b. Use of multiple documents as evidence with
decreased adding in a Japanese question-answering
system. Journal of Natural Language Processing,
12(2).
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
Information Processing Society of Japan, WGNL
168, pages 67?74.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
11
Hybrid Neuro and Rule-Based Part of Speech Taggers 
Qing Ma, Masaki  Murata,  Kiyotaka Uch imoto ,  H i tosh i  Isahara 
Communic~t ion s l{esea.rch Labora.tory 
Ministry of Posts a.nd Telecommm~ications 
588-2, lwa,oka,, Nishi-ku, Kobe 6511-2/192, 3a, pa,n 
{qma, murata., uchimoto,  isa.hara}{))crl.go.jp 
Abstract  
A hybrid system R)r tagging part of speech is 
descril)ed that consists of a neuro tagger and 
a rule-based correcter. The neuro tagger is 
an initia.1--state a.nnotator tha.t uses difl'ertnt 
h_,,ngths of contexts based on longe, st context l)ri- 
ority. Its inputs a.re weighted 1)y information 
gains tha.t are obtained by information ma.xi- 
mization. The rule-1)ased correcter is construct- 
ed by a. sol; of trm~sfc)rma.tion rules to xna.ke Ul) 
for the shortcomings o\[' the nou17o tagger. Corn- 
puter experiments show that ahnost 20% of the 
errors ma.de by the neuro tagger a.re correct- 
ed by the, st trans\[orma.tion rules, so tha.t the 
hybrid system ca.n reach a.n a,tcura.cy of 95.5% 
counting only the ambiguous words and 99.1% 
counting all words when a. small Thai corpus 
with 22,311 a mbig;uous words is used t))v tra.in- 
ing. This a(;cu racy is far higher than that using 
an IIMM and is also higher tha.n that using a. 
rule-1)ased model. 
1 Introduct ion 
Many pa.rt of speech (POS) tatters  proposed 
so far (e.g., Brill, 1994; Meria.ldo, 1994; l)aele- 
marls, el. al., 1996; and Schmid, 1994) ha.re 
achieved a. high accura.ey partly because a. very 
large amount of dal,~ was used to 1;rain them 
(e.g., on the order of 1,000,000 words for \]'hl- 
glish). For ma.ny other la.nguages (e.g., Thai, 
which we treat in this paper)~ however, it is not 
as easy to cremate \]a.rge corpora from which lm:ge 
amounts of tr~fining data can be extra.cted. It is 
therefore desirable to construct a practic;d tag- 
ger tha.t needs as little training d a.t;a~ as possible. 
A multi-neuro tagger (Ma a.nd ls~hara, 11998) 
and its slimmed-down version called the ela.s- 
tic neuro tagger (Ma, el; al., 1999), which have 
high genera.lizing ability and therefore are good 
at dealing with the problems of data sp~u:se- 
hess, were proposed to satist~y this requh:ement. 
These taggers perform POS tagging using difl'er- 
ent lengths of corltexts I)~.~sed on longest context 
prk)rity, and each element of tile input is weight- 
ed with information gains (Quinla.n, 1993) for 
retlecting that tile elements of the input h~ve 
different rtlevances in t~Gging. They ha.d a tag- 
ging accuracy of 94.4% (counting only the am- 
biguous words in part of speech) in computer ex- 
periments when a. small 'l'ha.i corpus with 22,311 
am biguous words was u se(l for tr~fi n ing. This ~(:- 
curacy is bu" higher thml t\]lat; USillg tile hidden 
Marker model (IIMM), the main approach to 
\])art o\[ speech tagging, ~nd is ~dso higher t,\]lan 
tha.t using a. rule-based mode\]. 
Neuro taggers, however, htwe several crucial 
shortcomings. First, even in the case where the 
POS of a word is uniquely determined by the 
word on its left, for example, a neural net will 
also try to perlbrnl tagging based on tile com- 
plete context. As a result, even for" when the 
word on tile left; is the same, the tagging result~ 
s will be difl'erent if the complete contexts are 
different, rl'ha, t is~ the neuro tagger carl hard- 
ly acquire the rules with single inputs. Fur- 
thermore, although lexica.l in\[brma.tion is very 
ilnport~ult in t~gging, it is difficult for: neural 
nets to use it becmme doing so would make the 
network enorlnous. That is, the neuro tagger 
ca.nnot acquire (;lit rules with lexical informs> 
tion. Additionally, Imca.use of convergence and 
509 
over-training l)roblems, it is impossible and also 
not advisM)le to train neural nets to an a.ccura,- 
cy of 100%. The training should be stopped at 
an appropriate level of a.ceuracy. Consequently, 
neural nets may not acquire some usefnl rules. 
To make up for these shortcomings of the 
neuro tagger, we introduce in this pa.per a rule- 
based corrector as tile post-processor and con- 
struct a hyl)rid system. The rule-based cot- 
rector is constructed by a set of transforma- 
tion rules, which is acqnired by transforma?ion- 
based error-driven learning (Brill, 1.994:) from 
training corpus using a set of templates. The 
templates are designed to SUl)l)ly the rules that 
the neuro tagger can hardly acquire. Actual- 
ly, by examining the transformation rules ac- 
quired in the computer experiments, the 99.9% 
of them are exactly those that the neuro tagger 
can hardly acquire~ even when using a template 
set including those for generating the'rules that 
the neuro tagger can easily acquire. This rein- 
forces onr expectation that the rule-based ap- 
proach is a well-suited method to cope with the 
shortcomings of the neuro t~gger. Computer ex- 
periments hows thai; about  200/0 of errors made 
by the neuro tagger can be corrected by using 
these rules and that the hybrid system ca.n reach 
an accuracy of 95.5% counting only the aml)ign- 
ous words and 99.1% counting all words in l, he 
testing corpus, when tile same corpus described 
above is used for training. 
2 POS Tagg ing  Prob lems 
In this paper, suppose there is a lexicon V, 
where the POSs that can be served by each word 
are list.ed, and tiler(; is a set of POSs, l?. That is, 
unknown words that do not exist in the lexicon 
are not dealt with. The POS tagging problem 
is thus to find a string of POSs T = T172..-% 
(ri C F, i = 1 , - . . , s )  by following procedure 
~o when sentence W = wlw2...w.~ (wi C V, 
i = 1 , . - . , s )  is given. 
#:W ~ -+ rt, (1) 
where t is the index of the target word (the word 
to be tagged), and W t is a word sequence with 
length l + 1 + r (:entered on the target word: 
l i e  t : wt_  1 . . . .  i o  t ? . . Wt+r~ (2) 
where t - 1 > 1, t + r _< s. 'l'agging ca.n thus be 
regarded as a classification problem 1) 3, replacing 
the POS with (;lass and can therefore be handled 
by using neural nets. 
3 Hybr id  System 
Our hybrid system (Fig. J) consists of a neuro 
tagger, which is used as an initial-state an nota- 
i;or, ~nd a rule-based corrector, which corrects 
the outputs of the neuro tagger. When a word 
seque,~ce W t \[see l~q. (2)\] is given, the neuro 
tagger outl)ut a tagging result rN(Wt) for tile 
target word wt at first. The rule-based correc- 
tor then corrects the output of the neuro tagger 
as a fine tuner and gives tile final tagging result 
Ncuro Tagger Rule-Based 
Corrcctor 
Figure 1: Hybrid neuro and rule-based tagger. 
3.1 Neuro  tagger 
As shown in Fig. 2, the neuro tagger consists 
of a three-layer I)erceptron with elastic input. 
This section mainly descril)es the construction 
of inl)ut and output of the nenro tagger, and 
the elasticity by which it; becomes possible to 
use variable length of context for tagging. For 
details of the architecture of l)erceptron see e.g., 
Haykin, 1994 and for details of the features of 
the neuro tagger see Ma and isahara, 1998 and 
Ma, et aJ., 1999. 
lnl)ut I PT  is constructed fi'om word se- 
quence W t \[Eq. (2)\], which is centered on target 
word wt and has length l + 1 + r: 
I PT  = (iptt_ l , . . ' ,  i ph , . . . ,  iph+.,.), (3) 
provided that input length l+ J+r  has elasticity, 
as described a,t tile end of this section. When 
woM wis given in position x (x = t - l , . . . , t+r ) ,  
510 
OPT 
ip t t_  I ...... i l ) \[t_ I ipl t 
11"1" 
il)lt+l ...... i\]Ht+r 
Figure 2: Neuro tagger. 
element ipt ,  of input I PT  is a. weighted pattern, 
defined as 
ipt.,: = :/.,,. (e,,,,('-,, 'e,." ,q,,~), (4) 
where g;,; is the inIbrmation gain which can be 
obtained using information theory (for details 
see Ma and lsahara., 11998) and 7 is the number 
of tyl)es of POSs. l\[' w is a word that apl)ears 
in the tra.ining data, then ea.ch bit e,,,i can be 
obtained: 
= P,,ot,(/ I , , ,) ,  (s) 
where l>'rob(ril'w) is a prior l>robal)ility of r i 
tha.t (;he woM 'w can take,. It is estitnated from 
the t raining (la,ta: 
C _ (<,  "') 
c' ( ,4  ' 
where C'(r ( ,w)  is the number of lfimes both r: 
at++d w al)pea, r , a,nd C(w)  is the number oi' times 
w appears in the training data. 1\[ w is a word 
that does not at)pear ill (,he training data~ then 
each t)it c,,,,i is obtained: 
~ i\['r i is a. candidate 
(7) e.,,,," = (} otherwise, 
where 7,, is the number of P()Ss that the word 
'w Call ta.ke. Output OPT is defined as 
OFT= ,o+), (s) 
provi<led that the output OI)T is decoded as 
S r/ i fO i= l  &O/=0for j? i  
YN(Wt) Unknown otherwise, 
(.9) 
where rN(W,) is the ?a.gging result obtained by 
the neuro tagger. 
There is more inforlnation available for con- 
structing the input for words on the left, be- 
cause they have already been tagged. In the 
tagging phase, instead of using (4:)-(6), the in- 
put can be constructed simply as 
i / , ,_4 = . oPT( - i ) ,  (1()) 
where i = 1, . . . ,1,  and O I )T ( - i )  means the out- 
put of the tagger for the ith word before the 
target word. ltowever, in the training process, 
the out;put of the tagger is not alway.a correct 
a.nd cannot be ted back to the inputs directly. 
Instead, a weighted awerage of the actual output 
a.nd tlm desired output is used: 
iptt_i = 9t- i  ' (wol ,T " 0 PT  ( -  i) + WlOJ,:s " I) l iS) ,  
(1.1) 
where 1)l':,q' is the desired output, 
o: , : ,5 '  : (& , / )2 , . . . ,  
whose bits are defined as 
\] iI' r i is a desired answer 
I)i = 0 otherwise, (la) 
and WOl,'r and w/)l,\],q' are respecLh:ely de\[(ned as 
1'\]013J 
~,:o1 '~ . . . .  (.14) 
1JACT '  
a,nd 
'w>l,; ,s,  = :1 - wopT ,  (15) 
where \]@,uo and \]'JAC'T are  the objective and 
actual errors. Thus, at the beginning of train- 
ing, the weighting of the desired output is largo. 
It decreases to zero during training. 
Elastic inputs are used in the neuro tagger 
so that the length of COlltext is variable in tag- 
ging based on longest context priority. In (te~ 
tail, (l, r) is initially set as large as possible for 
tagging. If rN('Wi) = Unknown,  then (1, r) is 
reduced by some constant interval. This l)ro- 
cess is repeated until rN(W~) 7 k Unknown or 
(1, r) = (0,0). On the other hand, to nmke the 
same set of connection weights of the neu ro tag- 
ger with the largest (1,'r) ava.ilable a.s lnuch as 
511 
possible when using short inputs for tagging, in 
training phase the neuro tagger is regarded as a 
neural network that has gradually grown fi'om 
small one. The training is therefore performed 
step by step from small networks to large ones 
(for details see Ma, et al 1999). 
3.2 Rule-based eorreetor 
Even when the POS of a word can be deter- 
mined with certainty by only the word on the 
left, for example, tile neuro tagger still tries to 
tag based on the complete context. That is, 
in general, what tile neuro tagger can easily 
acquire by learning is the rules whose condi- 
tional parts are constructed by all inpttts ip tx  
(x = t - l , . . . , t  + r) that are .joined with all 
AND logical operator, i.e., ( ip t t - t  & "'" iptt  & 
? .. iptt+~, -+ OPT) .  In other words, it is (lit: 
ticult for tile neuro tagger to learn rules whose 
conditional parts are constructed by only a sin- 
gle input like ( ipt,.  --+ OPT)  ~). Also, although 
lexical information is very important in tagging, 
it is difficult for the neuro tagger to use it, be- 
cause doing so would make the network euof  
mous. Tha.t is, the neuro tagger cannot acquire 
rules whose conditional parts consist of lexical 
information like (w -4 OPT) ,  (w&r  -4  OPT) ,  
and (w~w2 --+ OPT) ,  where w, Wl, and w2 are 
words and 7- is tile POS. Furthermore, because 
of convergence and over-training 1)rol)lems, it is 
iml)ossible and also not advisable to train net> 
ral nets to all accuracy of 100%. The training 
should be stopped at an apl)rol)riate level of a.c- 
curacy. Thus, neural net may not acquire some 
useful rules. 
The transfbrmation rule-based corrector 
makes up for these crucial shortcomings. 
The rules are acquired Dora a training co l  
pus using a set of transformation templates 
by transformation-based rror-driven learning 
(Brill, 1994). Tile templates are constructed 
using only those that supply the rules that tile 
nenro tagger can hardly acquire, i.e., are those 
1)The neuro tagger can also learn this kind of rules 
because it can tag tile word using only ipt, (the input 
of tile target word), ill the case of reducing tile (I, r) to 
(0,0), as described in Sec. a.l. The rules with single 
input described here, however, are a more general case, 
ill which the input call be ipt,~ (~: = t - 1, . . . ,  t + r). 
for acquiring the rules with single input, with 
lexical information, and with AND logica.1 in- 
put of POSs and lexical information. The set of 
templa,tes i  shown in Table 112). 
According to the learning procedure shown 
in Table 2, an ordered list of transformation 
rules are acquired by applying the template set 
to a training corpus, which had ah'eady been 
tagged by the neuro tagger. After tile trans- 
formation rules are acquired, a corl)us is tagged 
as tbllows. It is first tagged by the neuro tag- 
ger. The tagged corpus is then corrected by 
using the ordered list of transformation rules. 
The correction is a repetitive process applying 
the rules ill order to the corptlS, which is then 
updated, until all rules have been applied. 
4 Exper imenta l  Results  
Data :  For our computer experiments, we used 
tile same Thai corpus used by Ma et al (1999). 
Its 10,d52 sentences were randomly divided in- 
to two sets: one with 8,322 sentences for trail> 
ing and the other with 2,1.30 sentences for test- 
int. The training set contained 12d,331 word- 
s, of which 22,311 were ambiguous; the testing 
set contained a4,5~14 words, of which 6,717 were 
ambiguous. For training tile n euro tagger, only 
the ambiguous wor(ls in the. training set were 
used. For training the HMM, all tilt words in 
the training set were used. In both cases, all the 
words in tile training set were used to estimate 
Prob( r i lw) ,  tim probability of "c i that wor(I w 
can be (for details on the HMM, see Ma, et al, 
1999). In the corpus, 4:7 types of POSs are de- 
fined (Charoenporn et al, 1997); i.e., 7 = 47. 
Neuro tagger: The neuro tagger was con- 
structed by a three-layer perceptron whose 
input-middle-outI)ut layers had p z, 2 7 units, 
respectively, where p = 7 ? (1 + I + r). The 
(l + 1 + r) had tile following elasticity. In train- 
ing, tile (I, r) was increased step by step as (71,1) 
-+ (u,2) (a,2) (a,a) a,d gra,dual 
training fl'om a small to a large network was 
pertbrmed. Ill tagging, on the other hand, the 
2)To see whether this set is suitable, a immloer of ad- 
ditional experiments were conducted using various sets 
of templates. The details are described in Sec. 4. 
512 
r ~ I a,1)lc 1: Set o\[' templa.tes for tra.ns\[orln;~tion rules 
Change t;ag v a to t;ag v ? when: 
(single inlm|;) 
( input ('onsists of a POS) 
1. left (right) word is tagged v. 
2. second left (right) word is tagged r. 
3. third left (right) word is ta.gged r. 
(inI)n|; consist;s of a word) 
4. ta.rget word is ~. 
5. left (right) word is w. 
6. second left, (right) word is w. 
(AND logical inpu? ot" words) 
7. l, arget word is 'UO 1 ~tlld left (right) word is wu. 
8. left; (right) word is u,1 and second lcfl, (,'ight) word is w2. 
9. left, word is w~ a.nd right, word is 'wu. 
(AND logical in.lint; of POS and words) 
10. ta.rget word is uq and left (right) word is llaggod r. 
:11. left (righl;) word is .w~ and left. (right) word is tagged r. 
12. ta.rget word is w~, left (right) word is ,w.,, and left (right) word is tagged r. 
Ta,1)le 2: l)roetdure for learning transi'orma,tion rules 
1. Apply neuro taggtr to training corpus, which is then updated. 
2. Compare tagged results with desired ones and find errors.  
3. Ma.teh templates l'or all errors and obtain set of tra.nsformation rules. 
d. Select rule in corpus with the maximum value of' (cn l , _qood-h .  cnl,_bad), where 
cnZ_qood: number that transforms incorrect ags to correct elliS: 
c'nl._bad: number that transforms correct tags to incorrect ones, 
h: weight to control the strict, hess of generating 1;he rule. 
5. Apply selecttd rule to training corpus, which is then updated. 
6. Append selected rule to ordered list o1" trausl'orma.tion rules. 
7. Ih'4)eal; steps :2 through (j until no such rule can I)e selected, i.e., c 'n , t _good-  
h,. cnl,_bad < O. 
(l, 'r) was inversely reduce(l ste l) by step as (3,3) 
-+ (3,2) (2,2) (2,:1) O,:l) (:l,o) 
(0,0) a.s needed, provided tJlat the number of 
units in the middle layer was kept a.t the ma.xi- 
I l l  l l l l l  vahle. 
Ru le -based  cor re t to r :  The parameter h in 
the tw~Juat;ion function (cnl ,_9ood - h, . c'M._bad) 
used in 1;he learning procedure (Table 2) is a 
weight to control the strictness of generating a. 
rule. IF It is large, the weight of cnt_bad is la.rge 
and the possibility of generating incorrect rules 
is reduced. By regarding the neuro tagger as ~d- 
ready having high accuracy and using tile rule-- 
based correcter as a fine tuner, weight h. was set 
to a. large vahm, 100. Applying |;lit templates 
Co the training corptm, which had already been 
tagged 1) 5, the neuro ta.gger, we obta.ined a.n or- 
dered list; of 520 transfbrmation rules. '.l'~d)le 3 
shows the first 15 transfbrmation rules. 
Results :  Table 4 shows the resull;s of I)()S tag- 
ging for the testing data.. In addition to the 
accuracy o\[" the neuro tagger and hybrid sys- 
tem, the ta.ble also shows tile accuracy of a, bast- 
line model, the IIMM, and a rule-based model 
\['or comparison. The baseline model is one that 
performs tagging without using the contextual 
inlorma.tion; instead, it performs ta.gging using 
only f'requency informa.tion: the proba.bility of 
P()S that; tach word can be. The rule-based 
model, to be exact, is also a hybrid system con- 
513 
'l'a.1)le 3: First 15 transfbrmation rules 
No. F rom To  Cond i t i on  
1 PREL 
2 PREL 
3 Unknown 
4 XVHI4 
5 VATT 
6 Unknown 
7 NCI4N 
8 VATT 
O PREL 
i0 VST~ 
ii VfiTT 
12 NCMN 
13 NCHN 
14 Unknown 
15 NCNN 
RPRE le f t  word is punctuation and r ight  tuord is 5~gu 
RPRE le f t  yard is ~ 
RDVN le f t  ~ord Ls tagged XVfiE 
XVBH le f t  word is II~D 
flDVN le f t  word is  ~ 
VRTT le f t  word is  tagged PREL 
RPRE le f t  word is ua 
VSTfi l e f t  word is ~q~ 
RPRE r ight  word is ~gu and second r ight  word is a~q;J 
RDVN target word is ~t~4 
ADVN target  word is  ~4~ 
RPRE target word is n14 and le f t  word is eentluu 
RPRE le f t  word is ~tt and le f t  ward is tagged NCHN 
fiDVN th i rd  le f t  word is tagged WCT 
CNIT taPget ~ord is nn~ 
where PREL: Relat ive Pronoun, RPRE: Preposit ion,  fiDVN: fidverb with normal form . . . .  
Table d: Results of POS ta,gging for testing data* 
model baseline IIMM rule-based lleuro hybrid 
accuracy 0.836 0.891 0.935 0.944 0.955 
*Accurac9 was determiued only for am lfiguous words. 
sisting of an initial-state annotator and a set of 
transformation rules. As the initial-state anno- 
b~tor, however, the baseline model is used in- 
stea.d of' the neuro tagger. And, its rule set. has 
1,177 transformation rules acquired h'om a more 
general teml)late set, which is described at the 
end of this section. The reason for using a gener- 
al template set is that the sol; of tra.nsibrma.tion 
rules in the rule-based model should be the main 
annotator, not a fine post-processing tuner. For 
the same reason, the parameter to control the 
strictness of generating a rule, h, was set to  a 
small value, \], so that a larger number of rules 
were generated. 
As shown in the table, the accuracy of the 
nenro tagger was far higher than that of the 
HMM and higher than that of the rule-based 
model. The accuracy of the rule-based mod- 
el, on the other hand, was also far higher than 
that of the IIMM, ~lthough it was inferior to 
that of the neuro tagger. The accuracy of the 
hybrid system was 1.1% higher than that of the 
neuro tagger. Actually, the rule-based corrector 
corrected 88.4% and 19.7% of the errors made 
by the neuro tagger for the training and testing 
data, respectively. 
Because the template set shown in Table 1 
was designed only to make up for the short- 
comings of the neuro tagger, tile set is smal- 
l compared to that used by Brill (1994). To 
see whether this set is la.rge enough for our sys- 
tem, we perlbrmed two additional experiments 
in which (\]) a sol; constructed 193' adding the 
templates with OR logical input of words to the 
original set and (2) a, set constructed 1)5' fnrther 
adding the templates with AND and OR logi- 
cal inputs of POSs to the set of case (1) were 
used. The set used in case (2) inclnded the set 
used by Brill (\]994) and all the nets nsed in our 
experiments. It was also used for acquiring the 
transformation rules in the rule-based model. 
The experimental results show that compared 
to the original case, the accuracy in case (1) 
was improved very little and the accuracy in 
case (2) was also improved only 0.03%. These 
results show that the original set is nearly la.rge 
enough for our system. 
To see whether tile set is snitable tbr our 
system, we performed ~tn additional experimen- 
t using the original set in which the templa.tes 
with OR logical inputs were used instead of the 
templates with AND logical inputs. The accu- 
racy dropped by 0.1%. Therefore, tile templates 
with AND logical inputs are more suitable than 
514 
those with O11 logical inputs. 
We also performed an experiment using a 
template set without lexical intbrmation. In this 
case, l;he accuracy dropl)ed by 0.9%, indicating 
that lexical informatioll is important in tagging. 
To determine the effect o1' using a. large h, 
for generating rules, we per\['ormed an experi- 
ment with h = 1. In this case, the accuracy 
dropped by only 0.045%, an insignifica.nt differ- 
ence compared to the case of h, = 100. 
By examining the acquired rules that were 
obtained by al)plying the most COml)lete tem- 
plate set, i.e., the set used in case (2) described 
above, we found that 99.9% of them were those 
that can be obtained by a.pl)lying the original 
set of templates, rl'ha.t is, the acquired rules 
were almost those that are dif\[icult \['or the neu- 
re tagger to acquire. '.l'his rein forced our expec- 
tat;ion that the rule-based al)l)roach is a well- 
suited method to cope with the shortcoming of 
the neuro tagger. 
Finally, il, should 1)e noted that ill the liter- 
atures, tile tagging a.ccuracy is usua.lly delined 
by counting a.ll tile words regardless of whether 
they are a.nlbiguous or not. If we used this dell- 
nil:ion, t\]le accura.cy of our hybrid system would 
be 99.1%. 
5 Conc lus ion  
To collstruct a 1)tactical tagger that needs as 
little training data. a.s possible, neuro taggers, 
which have high generalizing al)ility and there- 
fore a.re good at dealing with the problems ofda~ 
ta. sl)a,rseness, have been proposed so fa.r. Neu- 
re tatters,  however, have crucial shortcomings: 
they ca.nnot utilize lexical information; they 
have trouble learning rules with single inputs; 
and they cannot learn training data to an ac~ 
curacy of 100%. To make up for these short- 
comings, we introduced a rule-based correcter, 
which is constructed by a. set of trans\[brma.tion 
rules obtained by error-driven learning, for post 
1)recessing and constructed a hybrid tagging 
system, l{y examining the transtbrma.tion rules 
acquired in the computer experiments, we found 
that 1;he 99.9% of them were those that; the neu- 
re tagger can hardly acquire, even when using a. 
template set including t;hose for generating the 
rules that the neuro tagger can easily acquire. 
This reinlbrced our expecta.tion that the rule- 
based approach is a well-suited method to cope 
with the shortcoming of the neuro tagger. Com- 
puter experiments showed that 19.7% of the er- 
rors made by the neuro tagger were corrected 
by the tra.nslbrmation rules, so the hybrid sys- 
tem rea.ched an accuracy of 95.5% counting only 
the ambiguous words and 99.\]% counting all the 
words in the testing data, when a small corpus 
with only 22,311 ambiguous words was used tbr 
train int. ~l'h is ind icates thai; ou r tagging ,qystem 
can nearly reach a pra.ctica.l level in terms of tag- 
ging accuracy even when a small Thai corpus is 
used tbr tra.ining. This kind of tagging system 
can be used to constructs multilingua.1 corpora 
that include languages in which large corpora 
have not yet been constructed. 
References  
l~rill, E.: Transfornmtion-based rror-driven lca.rn- 
ing and natural language processing: ~ case s- 
tudy ill 1)art-of-sl)eech tagging, Computational 
Li~g'uistics, Vol. 21, No. 4, pp. 543-565, 199~1. 
Cha.roenporll, T., Sornlertlanlva.nich, V., ~md Isa- 
hara, 11.: Building a la.rge Thai text corpus 
parl; of speech tagged corpus: OI{CIlll), Pro< 
Natural Language Processi~fl Pacific lNm ,5'gn~- 
po.du'm \[997, Phuket, Thailand, pp. 509-5\]2, 
1997. 
I)aelemans, W., Z~wrel, a., Berck, P., and C,i/lis, S.: 
MI3'I': A m<mlory-based pm-t of speech tagger- 
genera.tot, P'roc. /tl.h Workshop on Very Large 
Co,'po~zl, Copenhagen, l)em na.rk, pp. 1-1+1, 99(5. 
l\]aykin, S.: Neural Nchvorlcs, Macmillan College 
Publishing Coral)any, Inc., 199/t. 
Ma, Q. and lsahm'a., H.: A multi-neuro tagger us- 
ing variable lengths of contexts, Prec. COLING- 
ACL'g8, Montreal, pp. 802-806, 1998. 
Ma, Q., Uchimoto, K., Mura.ta, M., and 1sahara H.: 
F, lastic neural networks tbr part of speech tag: 
ging, Prec. IJCNN'99, Washington, \])C., pp. 
2991-2996, 1999. 
Meriaklo, B.: Tagging English text with a proba- 
bilistic model, Computational Linguistics, Vo\]. 
20, No. 2, pp. 1.55-171, 19(.)4. 
Quinla.n, 3.: G'~.5: Programs Jot Machine Learning, 
San Mateo, CA: Morgan Kaufinann, 1993. 
Schmid, 1t.: l'art-of-speech tagging with neural net- 
works, Prec. COLING'94, Kyoto, Japan, pp. 
172-176, 1994. 
515 
Bunsetsu  Ident i f i ca t ion  Us ing  Category -Exc lus ive  Ru les  
Masaki Murata Kiyotaka Uchimoto Qing Ma Hitoshi Isahara 
C()mmunications Research Laboratory, Ministry of Posts and ~I~lecommunications 
588-2, \]waoka, Nishi-ku, Kobe, 651-2d92, Japan 
? - j  ( ' . tel:-k81- 78-969-2 \]81 tax: +81- 78-369-2189 http://www-karc.crl, go.j p/ips/murata 
{ murata,u(:himoto,qma,isahara}(@crl.go.ji) 
Abstract 
This pal>or describes two new bunsetsu identificatkm 
methods using supervised learning. Sin(:e ,Jat)anese 
syntactic analysis ix usnally done after bunsetsu 
identification, lmnsetsu identiiieation is iml)orl;ant 
for analyzing Japanese sentences. In experiments 
comparing the four previously available machine- 
learning methods (decision tree, maximmn-entropy 
method, example-based apI)roaeh and deeiskm list,) 
an(l two new methods llSing categot'y-exclusive rul s~ 
the new method using l;he category-exclusive rules 
with the highest similarity t)erformetl best. 
1 Introduction 
This paper is about machine learning methods for 
identifying bwnsr'ts'~zs, which correspond to English 
phrasal units such as noun phrases and t)rel)ositional 
phrases. Since .Japanes(.' syntactic analysis ix usu- 
ally done after lmnselisu identitication (Uchimot;o el; 
a\].. 1999), i(lentitlying lmnsetsu is important l'or an- 
alyzing ,J;~p~tnese ltt(}tl(:es. The  conventional stud- 
ies on lmnsetsu  identitieation ~ have used hand-made 
rules (Kameda, \]995; Kurohashi, 3998), })ill; bun- 
sel;su identification is not an easy task. Conventional 
studies used many hand-nmde rules develot)ed at the 
cost of many man-hours. Kurohashi, tbr examl)le, 
made 146 rules for lmnsetsu identification (Kuro- 
hashi, 1998). 
Itl }l.tl a . t te t l lp t  to reduce the mnnber of man- 
hours, we used machine-learning methods for bun- 
setsu identitication. Because it; was not clear which 
machine-learning method would 1)e the one most al)- 
propriate for bunsctsu identification, so we tried a 
variety of them. In this paper we rei)orl; ext)er- 
inlel lts comparing tbur inachine-learning me, thods 
(decision tree, maximmn entropy, example-based, 
and decision list; methods) and our new methods us- 
ing category-exclusive rules. 
l lhmsetsu ideni,illcation is a ln'oblem similar to ohm,king 
(lLamshaw and Marcus, 1995; Sang and \h;ellsl;ra, 1999) in 
other l;mguages. 
2 Bunsetsu identification problem 
We conducted experiments (m the following super- 
vised learning methods tbr idel~tiflying }mnsetsu: 
? \])eeision {;l'ee method 
? Max i lnun l  ent ropy  method  
? Examt)le-based method (use of sinfilarity) 
? Decision list (use of probability and frequency) 
? Method 1 (use of exclusive rules) 
? Method 2 (use of exclusive rules with the high- 
est similarity). 
In general, t)misetsu identification is (tone afl;er 
morl)hological and l)efore syntactic analysis. Mor- 
1)hological analysis correst)onds to part-of-st)ee(:h 
tagging ill English. Japanese syntactic structures are 
usually ref)resented by the. relations between lmn- 
setsus, which correspond to l)hrasal units such as a 
noml l)hrase or a t)repositional 1)hrase in \]r, nglish. 
St), 1)unsetsu identification is imi)ortant in .lnpanese 
sentence mmlysis. 
In this paper, we identit\[y a bunsetsu by using 
intbrmation Dora a morl~hological nalysis. Bun- 
setsu identitication is treated as the task of deciding 
whether to insert a "\[" mark to indicate the partition 
between two hunsetsus as in Figure 1. There, fore, 
bunsetsu identilical;ion is done by judging whether a
partition mark should be inserted between two adja- 
cent nlorphemes or not. (We. do not use l;he inserted 
partition mark in the tbllowing analysis ill this paper 
for the sake of simplicity.) 
Our lmnsetsu identification method uses i;t1(} lilOr- 
phok)gk:al intbrmation of the two preceding and two 
succeeding morphemes ofan analyzed space bel;ween 
two adjacent morphemes. We use the following mor- 
phological information: 
(i) Major part-of  speech (POS) category, 2 
(ii) Minor P()S category or intlection tYl)e, 
(iii) Semantic information (the first three-digit nun> 
bet of a category nmnlmr as used ill "BGIt" 
(NLI{,I, 1964:)), 
2Part-of-spec.ch ~ttegories follow those of 3 \[/MAN (Kuro- 
hashi and N~tgao, 1998). 
565 
bohu .qa 
(I) nominative-case particle 
(I identify bunsetsu.) 
\[ bunsetsu wo 
(bunsetsu) objective-case particle 
I matomeagcru 
(identify) 
Figure 1: Example of identified bunsetsus 
Major POS 
Minor POS 
Semantics 
Word 
bun wo ~ugiru 
(sentence) (obj) (divide) 
((I) divide sentences) 
Noun Particle Verb 
Normal Noun Case-Particle Normal Form 
x None 217 
x wo ku.qiru 
Symbol 
Punctuation 
X 
X 
Figure 2: hfformation used in bunsetsu identification 
(iv) Word (lexical iifformation). 
For simplicity we do not use the "Semmltic infor- 
matioif' and "Word" in either of the two outside 
morphemes. 
Figure 2 shows the information used to judge 
whether or not to insert a partition mark in the space 
between two adjacent morphemes, "wo (obj)" and 
"kugiru (divide)," in the sentence "bun wo kugiru. 
((I) divide sentences)." 
3 Bunsetsu  ident i f i ca t ion  process  fo r  
each  mach ine- learn ing  method 
a.1 Deeision-tree method 
In this work we used the program C4.5 (Quinlan, 
1.995) for the decision-tree l arning method. The 
four types of information, (i) major POS, (ii) mi- 
nor POS, (iii) semmltic information, and (iv) word, 
mentioned in the previous section were also used 
as features with the decision-tree l arning method. 
As shown in Figure 3, the number of features is 12 
(2 + 4 + 4 + 2) because we do not use (iii) semantic 
information and (iv) word information from the two 
outside morphemes. 
In Figure 2, for example, the value of the feature 
'the major POS of the far left morpheme' is 'Noun.' 
a.2 Maximum-entropy method 
The maximum-entropy method is useful with sparse 
data conditions and has been used by many re- 
searchers (Berger et al, 1996; Ratnaparkhi, 1996; 
Ratnaparkhi, 1997; Borthwick el; al., 1998; Uchi- 
moto et al, 1999). In our maximuln-entropy exper- 
iment we used Ristad's system (Ristad, 1998). The 
analysis is performed by calculating the probability 
of inserting or not inserting a partition mark, from 
the output of the system. Whichever probability is 
higher is selected as the desired answer. 
In the maximum-entropy method, we use the same 
four types of morI)hological information, (i) major 
POS, (ii) minor POS, (iii) semantic information, and 
(iv) word, as in the decision-tree method. However, 
it, does not consider a combination of features. Un- 
like the decision-tree method, as a result, we had to 
combine features mmmally. 
First we considered a combination of the bits of 
each morphological information. Because there were 
four types of information, the total number of com- 
binations was 2 ~-  1. Since this number is large 
and intractable, we considered that (i) major POS, 
(ii) minor POS, (iii) semantic information, aim (iv) 
word information gradually becolne inore specific in 
this order, and we coml)ined the four types of infor- 
mation in the following way: 
Information A: (i) major POS 
Intbrmation B: (i) major POS and (ii) minor POS 
hfformat, ion C: (i) major POS, (ii) minor POS and 
(iii) semantic information 
Information D: (i) major POS, (ii) minor POS, 
(iii) semantic informa~aion a d (iv) word 
(~) 
We used only Information A and B for the two out- 
side morphemes because we (lid not use semantic 
and word information in the same way it is used in 
the decision-tree inethod. 
Next, we considered the combinations ofeach type 
of information. As shown in Figure 4, the number 
of combinations was 64 (2 x 4 x 4 x 2). 
For data sparseness, in addition to the above com- 
binations, we considered the cases in which frst, one 
of the two outside morphemes was not used, sec- 
ondly, neither of the two outside ones were used, m~d 
thirdly, only one of the two middle ones is used. The 
nmnber of features used in the maximum-entropy 
method is 152, which is obtained as follows: a
3When we extr~,cted features  f rom all of  the  ar t ic les  on  
566 
Far M't mort)henm Left morl)heme 
( Ma.ior POS '} 
~'Major POS'\[ ,J Minor POS 
I, Minor POgJ +/S,mmntic Information + 
( \?ord , 
2 4 
Right morl)hemc Far right mort)heine 
Minor POS f Ma.ior POS ~ 
Semantic Infi)rmation ' + \[ Minor POS j 
Word 
4 2 
Figure 3: Features used in the decision-tree method 
Far left morl)heme 
Information 
hffbrnmtion A } 
2 
Left morpheme Right morl)hcme 
( In format ion!} (Infbrmation A
J hfformation J Intbrmation B
/hfformation & ~ hfformation C
I, Infbrmation 1, hffbrmation D
4 4 
Far right morpheme 
J" Information 
& \[hlformation B A } 
2 
Figure 4: lJ~,atm'es used in the maximunt-entrol)y met.hod. 
No. of t>atures= 2 x 4 x 4 x 2 
+2 x 4 x 4 
+ 4 x 4 x 2 
+ 4 x 4 
+ 4 
+ 4 
= 152 
In Figure 2, (;lie feature that uses Infornultion 
B in the far left morl)heme, Infbrnmtion D in the 
left mort)heine, Information C in the right mor- 
pheme, and Information A in the fa.r right mop 
l/heme is "Noun: Nornml Noun; Particle: Case- 
Particle: none: wo; Verl): Nornml Form: 217; Sym- 
bol". In tim maximmn-entrol)y method we used for 
each space 152 ligatures uch as this ()tie. 
3.3  Example -based  method (use of  
s imi lar i ty)  
An example-based method was t)rollosed t) 3, Nagao 
(Nagao, 1984) in an attempt to solve I)roblenls in 
machine translation. To resolve a. l)rol)h'm, it; uses 
the most similar (;xami)le. In the i)resent work, the 
examt)le-1)ased method imt)artially used the same 
four types of information (see Eli. (1)) as used in 
the maxinmm-entrotly method, 
To use tills method, we must define the similarity 
of an ini)ut to an example. We use the 152 1)atterns 
fl'om the maximum-entropy method to establish the 
level of similarity. We define the similarity S be- 
tween all input and an exmnl)le according to which 
one of these 152 levels is the lnatching level, as fol- 
lows. (The equation reflects the importance of the 
two middle morphemes.) 
January 1, 1995 of a Kyoto University corpus (l;hc mnnber of 
spaces between mrl)henms was 25,81d) by using this method, 
the nunfl)e,r of types of features was 1,534,701. 
S = s(m_t) x s(m-H) x 10,000 
+ s(m_2) x s(.q.~) (2) 
Here m- l ,  m4-\], m-2, and m+2 refer respectively to 
the left;, rigid;, far M't, ;rod far righl; mortflmnms, and 
s(x) is the mort)hological similarity of a ll lOl't)hell le 
x, which is defined as follows: 
s(x) =1 (when no information of x is matched) 
2 (when Information A of x is matdmd) 
3 (when hfl~)rmal:ion B of x is mate, heal) 
4 (when Information C of x is mat;cited) 
5 (when Information D of x is matched) 
(a) 
Figure 5 shows an exmnple of the levels of sim- 
ilarity. When a pattern matches Information A of 
all four lnort)henies , uch as "Noun; Particle; Verb; 
Symbol", its similarity is 40,004 (2 x 2 x 10,000 + 
2 x 2). When a pattern matches a pattern, such as 
" ; Particle: Case-Particle: none: wo; ; ", its 
similarity is 50,001 (5 x 1 x 10,000 + 1 x 1). 
The exmnl)le-1)ased method extracts the exam- 
ple with the highest level of similm'ity and checks 
whether or not that exami)le is marked. A partition 
marl{ is inserted in tile input data only when the ex~ 
amt)le iv marked. When multit)le exalnl)les have the 
same highest level of similarity, the selection of tile 
best example is ambiguous, hi this case, we count 
tile number of nlarked and mlinarked sl)aces in all 
of the examples and choose the larger. 
a .4  Decis ion- l ist  method (use of p robab i l i ty  
and f l ' equeney)  
T i le  decision-list method was proposed by Rivest 
(Rivest, 1987), in which tile rules are not expressed 
as a tree structure like in the decision-tree method, 
567 
No information 
hffornmtion A
Information B
Information C
Information D
bun wo kugiru 
(senWnce) (obj) (divide) 
S(X) ?~\]'- 2 7D,--1 ?/Zq-1 '11~'+2 
1 . . . .  
2 Noun Particle Verb Symbol 
3 Normal Noun Case-Particle Normal Form Punctuation 
4 x None 217 x 
5 x wo kugiru x 
Figure 5: Exmnple of levels of similarity 
but are expanded by combining all the features, and 
are stored in a one-dimensional list. A priority or- 
der is defined in a certain way and all of the rules 
are arranged in this order. The decision-list method 
searches for rules Dora tile top of the list and an- 
alyzes a particular problem by using only the first 
applicable rule. 
In this study we used ill the decision-list method 
the same 152 types of patterns that were used in/;lie 
ma.ximuln-entropy method. 
To determine the priority order of the rules, we re- 
ferred to Yarowsky's method (Yarowsky, 1994) and 
Nishiokwama's method (Nishiokaymna et al, 1998) 
and used the probability a.nd frequency of each rule 
as measures of this priority order. When nnlltiple 
rifles had the same probability, the rules were ar- 
ranged in order of their frequency. 
Suppose, for example, that Pattern A "Noun: 
Normal Noun; Particle: Case-Particle: none: wo; 
Verb: Normal Form: 217; Symhol: Punctuatioif' 
occurs 13 times in a learlfing set and that tell of 
the occurrences include the inserted partition Inal:k. 
Suppose also thai; Pattern B "Noun; Particle; Verb; 
Symbol" occurs 12a times in a learning set and that 
90 of the occurrences include the mark. 
This exmnple is recognized by the following rules: 
Pattern A ~ Partition 76.9% (10/ 13), Freq. 23 
Pattern B => Partition 73.2% (90/123), Freq. 123 
Many similar rules were made and were then listed 
in order of their probabilities and, for any one prob- 
ability, in order of their frequencies. This list was 
searched from tile top ml(:l the answer was obtained 
by using the first, ai)plicable rule. 
3.5 Method  I (use of eategory-exe lus ive  
rules) 
So far, we have described the four existing machine 
learning methods. In the next two sections we de- 
scribe our methods. 
It is reasoimble to consider tile 152 patterns used 
in three of the previous methods. Now, let us sup- 
pose that the 152 patterns fl'om the learning set yield 
the statistics of Figure 6. 
"Partition" means that the rule determines that a 
partition mark should be inserted in the input data 
and "non-t)arl:ition" ineans that tile rule determines 
that a partition mark should not be inserted. 
Suppose that when we solve a hypothetical prob- 
lem Patterns A to G are apt)licable. If we use the 
decision-list inethod, only Rule A is used, which is 
applied first, and this determines that a partition 
mark should not be inserted. For Rules B, C, and 
D, although the fl'equency of each rule is lower thml 
that of Rule A, tile suln of their frequencies of the 
rules is higher, so we think that it is better to use 
Rules B, C, ml(t D than Rule A. Method 1 follows 
this idea, but we do not simply sum up tile frequen- 
cies. Instead, we count the munber of exalnples used 
ill Rules B~ C, and D and judge the category having 
tile largest number of exmnplcs that satisfy the pat- 
tern with the highest probability to be the desired 
ai1swer. 
For exmnple, suppose that in the above examt)le 
the number of examples atis(ying Rules B, C, and 
D is 65. (Because some exmnples overlq) in multi- 
pie rules, the total nunfl)er of exalnples is actually 
smaller than the total number of tile frequencies of 
the three rules.) In this case, among the examples 
used by the rules having 100% probability, tile nmn- 
ber of examples of partition is 65, m~d the number 
of examt)les of non-t)artitioi~ s 34. So, we deternline 
that tile desired answer is to partition. 
A rule having 100% probability is called a 
category-exclusive rule because all the data satist~y- 
ing it belong to one category, which is either parti- 
tion or noi>partition. Because for any given space 
the number of rules used call be as large as 152, 
category-exclusive rules are applie(t often ~. Method 
1 uses all of these category-exclusive rules, so we call 
it tile method using category-exclusive rules. 
Solving problems by using rules whose prol)abili- 
l;ies are nol; 100% may result ill the wrong solutions. 
Almost all of the traditional machine learning meth- 
ods solve problelns by usiug rules whose i)robabilities 
4'l'he ratio of the spaces analyzed by using category- 
exclusive rules is 99.30% (16864/16983) in Experiinent 1 of 
Section d. This indicates that ahnost all of the spaces are 
analyzed by category-exclusive rules. 
568 
\]{,uIe A: 
l{,uh.' B: 
Rule C: 
Rule D: 
Rule E: 
Rule F: 
Rule G: 
l 'attern A 
Pattern \] 
Pattern C 
Patl;crn \]) 
Patt;ern 1'3 
Pal;l;erll F 
Pat tern  G 
-?- I)rol)ability of non-i)al|;ition 
=> probability of partition 
=> i)rolml)ility of partition 
=> probal)ility of 1)artition 
:~ i)robability of partition 
:~ probability of parl:ition 
=> probability of non-partition 
m0% (a4/34)  
100% (,33/a3) 
\]oo% (25/2s)  
J()0(X, ( 1!)/ 19) 
8J.3% (1?)?}/123) 
76.9% ( 10/ ~a) 
57.4% (31{)/540) 
Figm'e 6: ;Ill (',Xaml)h; of rules used in Method :l 
1,?equency 34 
Frequency 33 
Frequency 25 
Frequency 19 
Frequcncy 123 
Fl'o, qucncy \] 3 
lq'equc, ncy 540 
are not J(}0%. By using such methods, we cannot 
hol)e to improve, a,:curacy. If we want to improve ac- 
(;llra(;y~ we nlllst use catt;gory-excltl,qive l'lllCS. The l 'e  
are some eases, however, tbr which, even if we take 
this at)l)r()ach, eategory-exchlsive rules aye rarely al> 
plied. In such cases, we lilllSl; add new feai;ures t() 
I;he mlalysis to create a situation in which many 
c i~tegory -exeh ls ive  ru les  Call \])(; appli(~d. 
I\]owever, il; is not suflieient to use  t ;~/tt~ory- 
exclusive rules. There arc, many nmaningless rules 
which \]la,1)l)ell to  1)e c~tl;egor3~-ex(;lll,qive o l l ly  ill ~t 
learning set. We lllllSt consider how to (~Iimim/te 
such meaningh;ss rule,q. 
3.6  Method  2 (r ising category -exc lus ive  
ru les  w i th  the  h ighest  sint i lar it ;y) 
Method 2 combines the, exami)h>based method and 
Method 1. That  is, it; combines the. method using 
similarity m~d the method usint'~ category-exchlsive 
rules in order to eliminate the meaningless (:art;gory- 
exclusive rules ment;ion(;(l i l lhe 1)revious t;el;ion. 
Mel;ho(1 2 also uses 152 patl(!rus for i(tentillving 
\])llllS(',|;,qll. q~h(!s(, ~ t)~li;l,(!l'll~q ~/l'(? llF,(RI ;IS rules i~ the 
,q;lllle ~,\;;ty }is ill Met\ ] lo( \ ]  \]. l)esir('d HIISWtH',q ill'(; (h;t(; l-  
mined by using the rule. having the high(>t probabil- 
ity. When mull;iple rules have the same 1)rolmbility, 
M(;thod 2 uses the wdue of the similarity described 
in the section of the examl)h>based m(;thod and }lll- 
alyzes the 1)robk:m with the rule having the highest 
simihu'ity. When multiple rules have th(; stone prob- 
nbilil;y and similm'ity, the method takes the exam- 
pies used by the rules having the highest probabil ity 
and the higlmst si,nilarity, and chooses the (:ategory 
with the larger llllllI))CF Of exami)les as t:hc desired 
answer~ in the same way as in Method 1. 
Itowever, when (:ategory-c.xchlsive rules having 
l l lt iro tha l l  Olte fre(l l lel lCy exist, the a})ove t ) roccdl l r ( ;  
is performed after el iminating all of the category- 
exclusive rules having one frequency, in el;her words, 
category-exclusive rules having more than one fl'e- 
quency are giwm a higher priority than category- 
exclusive rules having only one. flTo.qll(;nc.y })lit hav- 
ing ~ high(w similarity. This is 1)c(:ause eategory- 
(;xclusivc rules having only one fl'equen(:y are not so 
reliabh',. 
4 Experiments and discussion 
In our experiments we used a Kyoto  University text 
eorlms (Knrohashi and Nagao, 1997), which is a 
tagged corpus made Ul) of articles fi'om the Mainichi 
newspaper. All exl)eriments reported in this paper 
we.re performed using art, ielcs dated fi'om ,\]mmary 
\] to 5, 1995. We obtained the correct infi)rnmtion 
()n morphoh)gy and }mnse.t;su identiticathm from the 
tagged corpus. 
The following experiments were conducted to de- 
termine which supervised \]earnillg~ lnethod achieves 
the high<'.st a(:Cllra(:y l~tl;e. 
? Exlmriment \] 
\[,(;arninl,; set: ,Janllary 1, 1.995 
~J?t;Sl; st?t: ,\]atlll~/l'y 3, 1.995 
? \]';xt)eriment 2 
Learning set: 3ammry 4, 1995 
Test set: .\]a.nuary 5, 1995 
ltecm>e we used F, xlmriment \] in maki,lg Method 
I and Method 2, \]i;Xl)erinieut 71 is a ch)sc'd data..~et 
for Mel:l~od \] and Method 2. So, we l)crformed Ex- 
lmriment 2. 
The ,'(;suits arc. listed in '12fl)lt;,q I to d. \Ve used 
KNP2.0b4 (Kurohashi, 11997) mM KNP2.0t/6 (Kuro- 
hashi: 1998), which are bmlsetsu identitication and 
syntael;i(" analysis systems using tmmy hand-made 
rules in addition 1;o the six methods des(:ribed in 
Section 3. Be('mtse KNP is not based on a machine 
learning inethod but :many hand-made rules, in the 
KNP results "Learning selY and '~'.Test et" in the ta- 
llies have nt) meanings. In the eXll(wiment of KNP, 
we also uses morphological information in a corpus. 
~\].~hc ';F': ill l;\]le tables indicates the F-measure~ which 
is the. harmonic mean of a recall and a precision. A 
recall is l;he fl'action of correctly identilied partit ions 
out of all the partitions. A t)reeision is the ffae- 
th)n of correctly identitied partit ions out of all the 
SlmCeS which were judged to have a partit ion mark  
inserted. 
Tables I to -/I show the. following results: 
? In the test set I;he dc.cision-tree method was 
a little better thmt the maximmn-entropy 
569 
Table 1: Results of learning set of Exper iment  1 
Method D ~ 
Decision %-ee 99.58% 
Maximum Entropy 99.20% 
Example-Based 99.98% 
Decision List 99.98% 
Method 1 99.98% 
Method 2 99.98% 
KNP 2.0114 99.23% 
KNP 2.0116 99.73% 
Recall Precision 
99.66% 99.51% 
99.35% 99.06% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
99.78% 98.69% 
99.77% 99.69% 
The number of spaces between two )nor flmmes is 
25,814. The number of lmrtitions is 9,523. 
Table 3: Results of learning set of Exi)er iment 2 
Method 
Decision Tree 
Maximum Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.01)4 
KNP 2.066 
99.07% I 
99.99% I 
99.99% I 
99.99% I 
99.99% I 
98.94% I 
99.47% I 
RecaU Precision 
99.71% 99.69% 
99.23% 98.92% 
100,00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
99.50% 98.39% 
99.47% 99.48% 
The mlmber of spaces between two mor )heines is 
27,665. The number of partitions is 10 143. 
Table 2: Results of test set of Exper iment  1 
Method 
~ ion  Tree 
Maximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.066 
- F Recall 
98.87% 98.67% 99.08% 
98.90% 98.75% 99.06% 
99.02% 198.69% 99.36% 
98.95% i 98.43% ! 99.48% 
98.98% 198.54%! 99.43% 
99.16% I 98.88% ' 99.45% 
99.13% 99.72% ! 98.54% 
99.66% ~_99.68% ! 99.64% 
:)heroes is The lmmber of spaces between two mor 
Precision 
16,983. The mmiber of partitions is 6,166. 
T~ble 4: Results of test set of Exper iment  2 
Method 
Decision Tree 
M aximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.01)6 
P RTcad I Precision 
98.50% 98.51% I 98.49%- 
98.57% 98.55%1 
i 98.82% 98.71%1 
198.75% 98.27%1 
i 98.79% 98.54% I 
i 98.9\[1% 98.65% I 
199.(/7% 99.43%1 
L 99.51% 99.40% ~_ 
98.59% 
98.93% 
99.23% 
99.43% 
99.15% 
98.71% 
99.61% 
The nmnber of" spaces between two morphemes i
32,3o4. The number of partitions is 11,756. 
method.  Al though the maximuln-entropy 
method has a weak point  in that  it, does not 
learn the combinat ions of features, we could 
overcome this weakness by malting almost all of 
the combilmtions of features to produce a higher 
accuracy rate. 
? Tile decision-list n lethod was bet ter  t itan the 
maximum-entropy method in this experinmnt.  
? Tile example-based nlethod obtained the high- 
est accuracy rate among the four exist ing meth- 
ods. 
? Altt lough Method 1, which uses tim category- 
exclusive rule, was worse than the exmnple- 
based method,  it was better  than ti le decision- 
list method.  One reason for this was that  
ti le decision-l ist metl lod chooses rules rmldomly 
when mult iple rules have identical probabi l i t ies 
mid fl'equeneies. 
? Method 2, which uses the category-exchlsive 
rule with the highest similarity, achieved the 
highest accuracy rate among ti le supervised 
learning methods.  
? Tim example-based method,  tim decision-l ist 
inethod, Method 1 and Method 2 obta ined ac- 
curacy rates of about  100% for the leanfing set. 
This indicates that  these methods m:e especial ly 
strong for learning sets. 
? Tile two methods using similarity example-  
based method mid Method 2) were always bet-  
ter than the other methods, indicat ing that  the 
use of s imilar ity is eflective if we can define it 
approl)r iately. 
? We carried out experinmnts by using KNP,  a 
system that  uses ninny ha.nd-made rules. The 
F-measure of KNP was highest in the test set. 
? We used two versions of KNP, KNP 2.0b4 and 
KNP 2.0b6. The lat ter  was mud l  better  t lmn 
tlm former, iudicat ing tha.t the improvements 
made by hand are  effective. But, the mainte-  
nance of rules by hand has a l imit, so the im- 
provements made by hand are not always effec- 
tive. 
Tlle above exper iments indicate that  Method 2 is 
best among the machine learning methods '5. 
In Table 5 we show some cases which were par- 
t i t ioned incorrectly with KNP but correctly with 
51n these experiments, the. differences were very small. 
But, we think that the differences are significant to some ex- 
tent because we performed Experiment 1 and Experiment 2, 
the data we used are a large corplls containing about a few 
ten thousand morphemes and tagged objectively in advance, 
and the difference of about 0.1% is large in the precisions of 
99%. 
570 
Table 5: Cases when KNP was incorrect and Method 
2 wan correct  
ko tsukotsu \[ N H1,,'Tr 9aman.sh i 
(steadily) (lm prurient wit;h) 
L_{"" 1)e patient with ... steadily) 
lyoyuu wo \] motte \]~xrT;~l~ shirizoke 
i(enough Stl'engi;h) obj (have) (1)eat off) 
(... beat off ... having enough sl;rength) 
~aisha wo I gurupu-wake \ [ ~  
,.o.,v,,,,y obj (~r,,,,pi,,~) (do) 
(... 11o grouping companies) 
Method 2. A partition with "NEED" indicates that 
KNP missed inserting the i)artition mark, and a par- 
tition with "WRONG" indicates that KNP inserted 
the partitiol~ mark incorrectly. In the test set of Ex- 
periment 1, the F-measure of KNP2.0b6 was 99.66%. 
The F-measur(. ~ increases to 99.83%, ml(ler the as- 
sumption that when KNP2.0t)6 or Method 2 in cor- 
rect, the answer is correct. Although the accuracy 
rate for KNP2.0b6 was high, there were some cases 
in which KNP t)artitioned incorrectly and Method 
2 partitioned correctly, A combination of Method 
2 with KNP2.0b6 may be able to iml)rove the F- 
lile~lsllrO. 
The only 1)revious research resolving Imnnetsu 
identification by machine learning methods, in the 
work by Zhang (Zhang and Ozeki, 1998). The 
decision-tree, ine, thod was used in this work. But 
this work used only a small mmther of intor- 
l l l;ttion for t)llllsetsll identification" and (lid not 
achieve ll igh accuracy rat;es. (The  recall  rate 
was 97 .6%(=2502/ (2502+62) ) ,  the 1)recision rate 
was 92 .4%(=2502/ (2502+205) ) ,  and F -measure  was 
94.2%.) 
5 Conc lus ion  
To solve tile t)roblem of aecm'ate btmsetsu iden- 
tification, we carried out ext)eriments comt)aring 
tbur existing machine-learning methods (decision- 
tree method, maxilnum-entrol)y method, examI)le- 
based method and decision-list method). We ob- 
tained the following order of acem'acy in bunsetsu 
identification. 
Example-Based > Decision List > 
Maximum Entropy > Decision Tree 
We also described a new method which uses 
category-exclusive rules with the highest similarity. 
This method performed better than the other learn- 
ing methods in ore" exi)eriments. 
(>l'his work used oifly the POS information of the two roof 
phemes of an analyzed space. 
References  
Adam L. Berger, Stephen A. Della Pietra, and Vincent 
J. \])ella Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Linguistics, 
22(l):ag-rl. 
Andrew Borthwicl% John Sterling, Eugene Agichtein, aim 
Ralph Grishlnan. 1998. Exploiting Diverse Knowledge 
Sources via Maximum I';ntropy in Named Entity ll.ecoglfi- 
tion. In Proceedings of the Sixth Workshop on Very LaT~le 
Corpora, pages 152 -160. 
Masayuki Kameda. 1995. Simple Jalmnese analysis tool q jp.  
The Association for Natural Lan, guage Processing, the Isl 
National Convention, pages 349-352. (ill .)~ti)~tlleSe ). 
Sadao Kurohashi and Makot<) Nagao. 1997. Kyoto University 
text corpus 1)ro./ect. pages 115-118. (in .lapanese). 
Sadao lgurohashi and MM~oto Nagao, 1998. Japanese Mof  
phological Analysis System JUMAN version 3.5. \])ei)art- 
mcnt of Informatics, Kyoto University. (in Japanese). 
Sadao Kurohashi, 1997. Japanese Dependency/Case Struc- 
ture Anahdzer KNI ) version 2.Obj. Department of lnfof  
matics, Kyoto Ulfiversity. (in Japmmse). 
Sadao Kurohashi, 1!)!18. Japanese Dependency/Uase Struc- 
ture Analyzer KNI' version 2.0b6. Del)artment of Infor- 
m~tics, l(yoto University. (in .lapmmse.). 
Makoto Nagao. 1984. A I,'ralneworl( of a Mechanical Transh> 
ti(m between Jai)anese alld English 1)y Analogy lh'incit)le. 
Artificial a\]l(l Iluman hitelligence~ pages 173 q80. 
Shigeyuld Nishiokayama, Takehito Utsuro, and Yu.ii Mat- 
sumoto. 119!18. Extracting preference of dependency be- 
twee/t ,Japanese subordinate clauses from corlms. IE ICE-  
WGNL098- ll, pages 31-38. (in .lapnese). 
NI,I{,\]. 1964. (National Language Resemvh Institute). 
Word List b?l Semantic Principles. Syuei SyuI)pan. (in 
Japanese). 
.\]. 1{.. Quinl;m. 1995. Pro qrams for machine learning. 
Lmme A. I{;mlshaw ;uld Mitchell 1'. Marcus. 1(.)(.15. Text 
clmnking using transformationq)ased l arning. \]11 Proceed- 
ira.IS of th, e Th, ird Workshop on Very La*#e Corpora, l)ages 
82 (.)4. 
Adwait l{.atnat)arklfi. 1996. A Maximum l,;nl;ropy Model tin" 
l'art-OlCSt)eech Tagging. 1)rocecdin9 s of P)mpirieal Method 
for Natural Language l'roeessings, pages 133 1,12. 
Adwait ll,atnaparkhi, 19!17. A l,inear Observed Time Statis- 
ticaI l'm'ser Based ou Maximum \]~ntropy Models. ht t~l'o - 
ceedings of Empirical Method for Natural l, an.quage l'ro- 
cessings. 
Eric Sven Ristad. 1998. Maximum Elltropy Modeling 
Toolkit, Release 1.6 beta. http://www,mnemonic.com/ 
software/metal. 
Ronald L. Rivest. 1987. lmarning l)ecision lasts. Machine 
Learning~ 2:229 -246. 
Erik P. Tjong Kim Sang and .lorn \reenstra. 1999. ll.el)re- 
senting text chunks, in EA CL'99. 
Kiyotaka Uehimoto, Satoshi Sekiim, and IIitoshi Isalmra. 
1999.. Japanese dependency structure analysis based on 
maximmn entrol)y models. In Proceedings of the Ninth 
CoTtference of the 15"ulwpeau Chapter of the Association 
for Computational Linguistics (P)A CL), pages 196-203. 
\])avid Yarowsky. 1!)94. Decision lists fo," lexieal ambiguity 
resolution: Application to accent restoration in Spanish 
and l,Yench. In 22th Alt, Tt~tal Meeting of the Assoeitation 
of the Computational Linguistics, pages 88-95. 
Yujie Zlmng and Kazuhiko Ozeki. 1998. The applica- 
tion of classitieation trees to bunsetsu segmentation of 
Jat)anese sentences. Journal of Natural Language Process- 
ing, 5(4):17-33. 
571 
Word Order  Acqu is i t ion  f rom Corpora  
Kiyotaka Uchimoto I, Masaki Murata t, Qing Ma*, 
Satoshi Sekine*, and Hitoshi Isahara t 
tCommunications Research Laboratory 
Ministry of Posts and Telecommunications 
588-2, Iwaoka, Iwaoka-cho, Nishi-ku 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto ,murata, qma, isahara\] @crl. go. jp 
*New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekYne~cs, nyu. edu 
Abstract 
In this paper we describe a method of acquiring word 
order fl'om corpora. Word order is defined as the or- 
der of modifiers, or the order of phrasal milts called 
'bunsetsu' which depend on the stone modifiee. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to de- 
ciding the word order mid which word order tends to 
be selected when several kinds of information con- 
flict. The contribution rate of each piece of informa- 
tion in deciding word order is eiIiciently learned by a 
model within a maximum entropy framework. The 
performance of this traiimd model can be ewfluated 
by checking how many instances of word order st- 
letted by the model agree with those in the original 
text. In this paper, we show t, hat even a raw cor- 
pits that has not been tagged can be used to train 
the model, if it is first analyzed by a parser. This 
is possible because the word order of the text in the 
corpus is correct. 
1 Introduction 
Although it is said tha~ word order is free in 
Japanese, linguistic research shows that there art 
certain word order tendencies - -  adverbs of time, for 
example, tend to t)recede subjects, mM bunsetsus in 
a sentence that are modified by a long modifier tend 
to precede other bunsetsus in the sentence. Knowl- 
edge of these word order tendencies would be useful 
in analyzing and generating sentences. 
Ii1 this paper we define word order as the order of 
nrodifiers, or the order of bunsetsns wlfich depend on 
the same modifiee. There arc several elements which 
contribute to deciding the word order, and they are 
summarized by Saeki (Saeki, 1.998) as basic condi- 
tions that govern word order. When interpreting 
these conditions according to our definition, we era: 
summarize them ,~ tbllows. 
Component la l  eondit lons 
? A bunsetsu having a deep dependency tends 
to precede a bunsetsu having a shallow depen- 
dency. 
When there is a long distance between amodifier 
and its modifiee, the modifier is defined as a bun- 
setsu having a deep dependency. For example, 
the usual word order of modifiers in Japanese 
is tlm following: a bunsetsu which contains an 
interjection, a bunsetsu which contains an ad- 
verb of time, a bunsetsu which contains a sub- 
ject, and a bunsetsu which contains an object. 
Here, the bunsetsu containing an adverb of time 
is defined as a bunsetsu having deeper depen- 
dency than the one containing a subject. We 
call the concept representing the distance be- 
tween a modifier and its modifiee the depth of 
dependency. 
A bunsetsu having wide dependency tends to 
precede a bunsetsu having narrow dependency. 
A bunsetsu having wide dependency is defined 
as a bunsetsu which does not rigidly restrict its 
modifiee. For example, the bunsetsu "~btqlo_c 
(to Tokyo)" often depends on a bunsetsu whicll 
contains a verb of motion such as "ihu (go)" 
while the bunsetsu "watashi_.qa (I)" can depend 
on a bunsetsu which contains any kind of verb. 
Here, the bunsetsu "watashi_ga (I)" is defined as 
a bunsetsu having wider dependency than 1;11o 
tmnsetsu ':Tok~./o_c (to Tokyo)." We call the 
concept of how rigidly a modifier restricts its 
modifiee the width of dependency. 
Syntact i c  condit ions 
? A bunsetsu modified by a long inodifier ton(Is to 
precede a bunsetsu modified by a short lnodifier. 
A long modifier is a long clause, or a clause that 
contains many bunsetsus. 
? A bunsel, su containing a reference pronoun tends 
to precede other bunsetsus in the sentence. 
? A bunsetsu containing a repetition word tends 
to precede other bunsetsus in the sentence. 
A repetition word is a word referring to a word 
in a preceding sentence. For example, Taro 
mid Hanako in the following text are repetition 
words. "Taro and Hanako love each other. Taro 
is a civil servant and Hanako is a doctor." 
? A bunsetsu containing the case marker "wa" 
tends to precede other bunsetsus in the sentence. 
A mnnber of studies have tried to discover the rela- 
tionship between these conditions and word order in 
871 
Japanese. Tokunaga and Tanalca proposed a model 
for estimating JaI)anese word order based on a dic- 
tionary. They focused on the width of dependency 
(Tokunaga and Tanal~a, 1991). Under their model, 
however, word order is restricted to the order of case 
elements of verbs, and it is pointed out that the 
model can deal with only the obligatory case and 
it cmmot deal with contextual information (Saeki, 
1998). An N-gram model fbr detecting word order 
has also been proposed by Maruyama (Maruyama, 
1994), but under this model word order is defined as 
the order of morpheines in a sentence. The problem 
setting of Maruyama's study thus differed fl'om ours, 
and the conditions listed above were not taken into 
account in that study. As for estimating word or- 
der in English, a statistical model has been proposed 
by Shaw and Hatzivassiloglou (Shaw and Hatzivas- 
siloglou, 1999). Under their model, however, word 
order is restricted to the order of premodifiers or 
modifiers depending on nouns, and the model does 
not simultaneously take into account many elements 
that contribute to determining word order. It would 
be difficult to apply the model to estimating word 
order in Japanese when considering the many condi- 
tions as listed above. 
In this paper, we propose a method for acquiring 
from corpora the relationship between the conditions 
itemized above and word order in Japanese. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to decid- 
ing the word order and which word order tends to be 
selected when several kinds of information conflict. 
The contribution rate of each piece of information in 
deciding word order is efficiently learned by a model 
within a maximum entrot)y (M.E.) framework. The 
performance of the trained model can be evaluated 
according to how many instances of word order se- 
lected by the model agree with those in the original 
text. Because the word order of the text in the corpus 
is correct, the model can be trained using a raw co> 
pus instead of a tagged corpus, if it is first analyzed 
by a parser. In this paper, we show experimental re- 
sults demonstrating that this is indeed possible even 
when the parser is only 90% accurate. 
This work is a part of the corpus based text gen- 
eration. A whole sentence can be generated in the 
natural order by using the trained model, given de- 
pendencies between bunsetsus. It could be helpful 
for several applications uch as refinement support 
and text generation in machine translation. 
2 Word  Order  Acqu is i t ion  and 
Es t imat ion  
2.1 Word  Order  Mode l  
This section describes a model which estimates the 
likelihood of the appropriate word order. We call 
this model a word order model, and we implemented 
it within an M.E. framework. 
Given tokenization of a test corpus, the problem 
of word order estimation in Japanese can be reduced 
to the problem of assigning one of two tags to each 
relationship between two modifiers. A relationship 
could be tagged with "1" to indicate that the order 
of the two modifiers is appropriate, or with "0" to in- 
dicate that it is not. Ordering all modifiers so as to 
assign the tag "1" to all relationshit)s indicates that 
all modifiers art  in the appropriate word order. The 
two tags form the space of "futures" in the M.E. 
formulation of our estimation problem of word or- 
der between two modifiers. The M.E. model, as well 
as other similar models allows the computation of 
P(flh) for any f in the space of possible futures, F, 
and for every h in the space of possible histories, H. 
A "history" in maximum entropy is all of the condi- 
tioning data that enable us to make a decision in the 
space of futures. In the estimation problem of word 
order, we could reformulate this in terms of finding 
the probability of f associated with the relationship 
at index t in the test cortms as: 
P(flht) = P( f l  hfformation derivable 
from the test corpus 
related to relationship t) 
The computation of P(flh) in any M.E. models is 
dependent on a set of "features" which should be 
hdpful in making a prediction about the flmlre. Like 
most current M.E. models in computational linguis- 
tics, our model is restricted to features which are 
binary functions of the history and future. For in- 
stance, one of our features is 
1. : if has(h,x) = true, 
x = "Mdfl'l - Head-  
g(h,f) = POS(Major) : verb" (1) 
&f= l  
0 : otherwise. 
Here "has(h,x)" is a binary flmction which returns 
true if the history h has feature z. We focus on the 
attributes of a bunsetsu itself and on the features 
occurring between bunsetsus. 
Given a set of features and some training data, 
the maximum entropy estimation process produces a
model ill which every feature .qi has associated with it 
a parameter ai. This allows us to compute the con- 
ditional probability as follows (Berger et al, 1996): 
ag~ (h .f) 
P( / Ih ) -  1L ' (2) 
Z (h) 
ct i . (3) 
Y i 
The maximum entropy estimation technique guaran- 
tees that for every feature gi, the expected value of 
gi according to the M.E. model will equal the empir- 
ical expectation of gi in the training corpus. In other 
words: 
P(h,/). Mh, f) 
h,f 
= (4) 
h / 
Here /5 is an empirical probability and l~ Ie  is the 
872 
Table l: Example of estimating the probabilities of word orders. 
? ) I"I~{H (yesterday) / ?=x~ (tennis) / ~(f~l~l$ (Taro) / bLo ( l ) I t~yed. ) "  /I~,,'~<~,~ x llail.-)'nxt, x 1.7:-.x,~j~m ::: 0.6 x 0.8 x 0.3 0.144 ) \["NI!Iii:k (Taro) )  I~1!11 (yesterday) / ?:LX ~ (tennis) / bt:? (played.)" IP:mi~ ~H x I~*H,)-: x,,< x t?r,l~.?::x,: := 0.4 x 0.8 ? 0.7 0.224 
\[")kl'!l~l:~ (Taro) / -Y : :x '~ (tennis) / 'a~H (yesterday) / b?:o (plowed.)" I/)~:,,~ll x tg.=.x'~.,~H x P,k~a*~L-)::?~ =: 0.4 x 0.2 x 0 .7  0.05(i 
1"9:-':;? ~ (tennis) / II~{H (yesl.erd;ty) / >kfl\[~l~: ( l'&l'O) / \[,~:.? (played.)" \[13*lll,2<I,u:l. x l: ':.xt, jall x l!).:.x.~,.j<r~m :: 0.6 x 0.2 x 0.3 0.036 
\["5:=x ~ (tennis) / )kf~lll:t (Taro) / I~I~Ft (yesterday) / blz0 (played.)" ~ ? H  x l~;.:.xr, ~,~ZI? x l?y:.x~l~,~ =: 0.4 x 0.2 x 0.3 0.024 
prol)ability assigned by the M.E. model. 
We detine a word order model as a model which 
learns the at)l)ropriate order of each pair of nlodifiers 
which depend on the same modifiee. 'l'his model is 
derived from Eq. (2) as follows? Assmne that there 
are two bunsetsus 231 and 23~ which depend on the 
buusetsu B and that  It is the information derivable 
from the test corpus. \]?lie probability that "B\] B2" is 
the at)propriate order is given by the following equa- 
tion: 
\]iik: :1 ffi( l  ,b) (~'i ,i 
where .qi(1 < i < k) is a fl,atm'e and "1" indicates 
that the order is at)propriate. The terms cq,i and 
(~0,i are estimated fl'oln a eorl)us which is nlorpho- 
logically and syntactically analyzed. When there are 
three or more b\]msetsus that det)end on tit('. S}tlne 
? moditiee, the probability is estimated as follows: I or 
~'t bunsetsus 231, 232, . . . ,  23n which depend on the 
bmtsetsu B and for the information h derivaMe from 
the test corpus, the prot)ability t;hat "23\] 23~ . . .  23," 
is the at)propriate order, or P( l lh) ,  is represented ass 
the probability that every two bunsetsus "Bi ~i-Fj 
(1 _< i < n - 1,1 < j < 'n - i)" are the appropri~ 
ate. order, or  P({14~i, i_ l . j  = l \ [ l<  i <. n - -  1, l :< j < 
n - - i} lh ) ,  ilere "I4Li+j -- l" represents that ".l~i 
23i-Fj" is the appropriate order. Let us assume that 
every 14Q,~:+j is independent each other. Then 1)(1 Ih,) 
is derived as follows: 
2 ' ( l ib )  = \]1\]. i , . -  \ ] ,  
1 < .i _< , , , -  i}lh) 
n- - |  n - i  
i -- I  j --1 
= l l  1I j), 
i= l  j= l  
where \[Si,i+ j is the information derivable when fb- 
cusing on the bunsetsu 13 m~d its modifiers 13i and 
Bi+j. 
For example, in the sentence "I~ U (kinou, yester- 
day) / :kflll ~ (Taro_wa, Taro) / -P ~ x ~ (tcnnis_wo, 
tennis) / b t:o (sita., l)layed.)," where a "/" repre- 
sents a bunsetsu boundary, there are three bunset- 
sus that depend on the verb "b  ~: (sita)." We train 
a word order inodel under the assmnl)tion that the 
orders of three t)airs of modifiers -"I~l U" and "~ 
f$1.~," "Net\] " and "?  7-:7, ~ ," and ":kl~l*l~" and "5: 
m :7, ~"  .... are al)ttropriate. We use various ldnds of 
intormation in and around the target bunsetsus as 
features. For example, the information or the feature 
that a noun of time i)recedes a t)rot)er noun is deriv- 
able fl'om the order "IP} H (yesterday) / Y;fll~ I~ (Taro) 
/ b 1=o (pl~\yed.)," and the feature that a case fol- 
lowed by a case marker "w?' precedes a case followed 
by a caqe marker "wo" is derivable from the order ":~ 
fll~ It. ( Taro_wa, Taro) / ? ~ 7. ?k (tennis_wo, tennis) / 
b 2C_o (sita., t)layed.)." 
2.2 Word Order  Es t imat ion  
This section describes the algorithm of estimating 
the word order by using a trained word order model. 
The word order estimation is defined as deciding 
the order of ntoditiers or bunsetsus which depend 
on the same modifiee. The input of this task con- 
sists of modifiers and informat, ion necessary to know 
whether or not features are found. The output is 
the order of the inodifiers. We assume that lexical 
selection in each bunsetsu is already done and all 
del)endencies in a sentence are found. The informa- 
tion necessary to know whether or not features are 
found is morphological, syntactic, semmltic, and ('on- 
textual information, and the locations of bunsetsu 
bonndaries. The features used in our ext)eriments 
are described in Section 3. 
Word order is estimated in the following steps. 
Procedures 
1. All possible orders of modifiers are found. 
2. For each, the probability that it is apt)ropriate 
is estimated by a word order model, or Eq. (6). 
3. The order with the highest probability of 1)eing 
approl)riate is selected. 
l)br example, given the sentence "1~ U (kinou, 
yesterday) /:kfilIl~ (Taro_wa, Taro) /? : :x  ~ (tcn- 
nis_wo, temfis) / b t:o (sita., played.)," tim modi- 
tiers of a verb "b  ?:_ (played)" are three tmnsetsus, 
"l~ U (yesterday)," :k~/i ~ (Taro)," "?  = x ~ (ten- 
nis)." Their apt)ropriate order is estimated in the 
following steps. 
1. The probabilities that the orders of the three 
pairs of modifiers "N- LI " and ":is: BII l~ ," "I~ 
U" and "?~:7 ,~,"  and "~fllIl?" and "?  
c .x  ~" are appropriate are estimated. As- 
sume, for example, ~-H ,;k~l~ta, PrI~ It ,? :-~. ~, and 
P;kfzlIla,~ ca  ~ are respectively 0.6, 0.8, and 0.7. 
2. As shown in Table 1, probabilities are estimated 
for all six possible orders. The order "I~ U / :k 
fill IS / -7- ~- y. ~ / b \]Co ," which has the highest 
probability, is selected as the most apt)ropriate 
order. 
2.3 Per fo rmance  Eva luat ion  
The pcrformancc of a word order model can be eval- 
uated in the following way. First, extract from a 
test corpus bunsetsus having two or more modifiers. 
Then, using those 1)unsetsus and their modifiers as 
873 
Data 
~Jtlnsetsll lJtlnsetstl nHIYiber Of babel 
nlllllber modifier 
0 1 P 
1 5 
2 3 
3 4 
4 5 P 
5 
Table 2: Example of modifiers extracted fl'om a corpus. 
Modifiers (Bunsetsu number) 
Strings in a bunsetsu 
>kflli ~ ( Taro_to, Taro and) 
~Y'{a ( lIanako_to, llanako) 
-Y- = x q~ (tennis_no, tennis) 
~lc .  (sial_hi, tournament) 
lll'C, (dete,, participate,) 
~@ b t=, (yusyo_sita., won.) 
Moditiers whose modiliee is the bunsetsu 
in the left column. 
~,~ a (0) 
?=x0~ (2) 
~a  (0) ~-r-~a 0) '~ :  (a) 
~e  (o) ~?-~* (1) If'~ (4) 
input, estimate the orders of the modifiers as de- 
scribed in Section 2.2. The percentage of the modi- 
flees whose modifiers' word order agrees with that in 
the original text then gives what we call the agree- 
ment rate. It is a measure of how close the word 
order estimated by the model is to the actual word 
order in the training corpus. 
We use the following two measurements to calcu- 
late the agreement rate. 
Pa i r  of  modi f ie rs  The first measurement is the 
percentage of the pairs of modifiers whose word 
order agrees with that in the test corpus. For 
exmnple, given the sentence in a test corpus "N 
kl (kinou, yesterday) / ~t I la  (Taro_wa, Taro) 
/ -7- = 2` ~2 (tennis_wo, tennis) / t. ~:o (sita., 
played.)," if the word order estimated by the 
model is "~ H (yesterday) / -7" -- 2. ~ (tennis) / 
~1~ ~:~ (Taro) / b too (played.)," then the or- 
ders of the pairs of modifiers in the original sen- 
tence are "N H / ;k~l~  ," "15 H / -7- =- :7, ~ ," and 
"~lit:~ / ~--2` ~ ," and those in the estimated 
word order are "~H / -~---2`~," "~H / 
1~1~ la~ ," and "Y- = 2` ~ / %:t~ll lak ." The agreement 
rate is 67% (2/3) because two of the three orders 
are the same as those in the original sentence. 
Complete  agreement  The second measurement is 
the percentage of the modifiees whose modifiers' 
word order agrees with that in the test corpus. 
3 Exper iments  and  D iscuss ion  
In our experiment, we used the Kyoto University text 
corpus (Version 2) (Kurohashi mid Nagao, 1997), a 
tagged corpus of the Mainichi newspaper. For train- 
ing, we used 17,562 sentences from newspaper arti- 
cles appearing in 1995, from January 1st to Jmmary 
8th and from Jmmary 10th to June 9th. For testing, 
we used 2,394 sentences fl'om articles appearing on 
January 9th and from June 10th to June 30th. 
3.1 Def in i t ion of  Word  Order  in  a Corpus  
In the Kyoto University corpus, each bunsetsu has 
only one modifiee. When a bunsetsu Bm depends on 
a bunsetsu Bd and there is a bunsetsu /3p that de- 
pends on and is coordinate with \])d, Bp has not only 
the information that its modifiee is \]~d but also a la- 
bel indicating a coordination or the information that 
it is coordinate with B d. This information indirectly 
shows that the bunsetsu Bm can depend on both \]3p 
and Bd. In this case, we consider Bm a modifier of 
both Bv and B d. 
Under this condition, modifiers of a bunsetsu B 
are identified in the following steps. 
1. Bunsetsus that depend on a bunsetsu B are clas- 
sifted as modifiers of B. 
2. When B has a label indicating a coordination, 
bunsetsus that are to tile left of 13 and depend on 
the same modifiee as B are classified as modifiers 
of B. 
3. Bunsetsus that depend on a modifier of B and 
have a label indicating a coordination are clas- 
sifted as modifiers of B. The third step is re- 
peated. 
When the above procedure is completed, all bunset- 
sus that coordinate with each other are identified as 
modifiers which depend oi1 the same nmdifiee. For 
example, from the data listed on the left side of To- 
ble 2, the modifiers listed in the right-hand column 
are identified for each bunsetsu. "Nt~I; ~ (Taro_to, 
Taro and)," "?~g-~ IS (Hanako_to, Hanako)," "ql "(, 
(dete,, participate,)" are all identified as modifiers 
which depend on the same modifiee "~ b 7=? 
(yusyo_sita., won.)." 
3.2 Exper imenta l  Resu l ts  
The features used in our experiment are listed in Ta- 
bles 3 and 4. Each feature consists of a type and 
a value. The features consist basically of some at- 
tributes of the bunsetsu itself, and syntactic and con- 
textual information. We call the features listed in 
Tables 3 'basic features.' We selected them man- 
ually so that they reflect the basic conditions gov- 
erning word order that were sunmmrized by Saeki 
(Saeki, 1998). The features in Table 4 are combina- 
tions of basic features ('combined features') and were 
also selected manually. They are represented by the 
nmne of the target bunsetsu plus the feature type of 
the basic features. The total number of features was 
about 190,000, and 51,590 of them were observed in 
the training cortms three or more times. These were 
the ones we used in our experiment. 
The following terms are used in these tables: 
Mdf r l ,  Mdf r2 ,  Mdfe:  The word order model de- 
scribed in Section 2.1 estimates the probability 
that modifiers are in the appropriate order as 
the product of the probabilities of all pairs of 
modifiers. When estimating the probability tbr 
each pair of modifiers, the model assmnes that 
the two modifiers are in the appropriate order. 
Here we call the left modifier Mdfrl, the right 
modifier Mdfr2, and their modifiee Mdfe. 
Head:  the rightmost word in a bunsetsu other than 
those whose major pro't-of-speech I category is 
1Part-of-speech categories follow those of JUMAN (Kuro- 
hashi and Nagao, 1998). 
874 
Table 3: Basic features. 
Bas ic  features  
Feature values (Number of type) Feature type tegors\] Target 
1)tmsetsus 
1 Mdfrl, Mdfr2, 
Mdfe 
2 Mdfrl, Mdfr2, 
Mdfe 
3 Mdfrl, Mdfr2, 
Mdfe 
4 Mdfrl, Mdh'2, 
Mdfe 
m m  
\[\]ead-POS(Major) 
\[tead-POS(Minor) 
\[lead-hff(Major) 
\[Iead-Inf(Minor) 
\[Iead-SemFeat(110) 
\[1ead-SemFeat(111 ) 
\[Iead-SemFeat(433) 
5 Mdfrl, Mdff2, rype(String) 
Mdfe rype(Major) 
type(Minor) 
6 Mdfrl, Mdfr2, lOSIll l(String) 
Mdfe lOSIIIl(Minor) 
lOSII12(String) 
lOSUI2(Minor) 
7 Mdfrl, Mdfr2, Period 
Mdfe 
8 Mdfrl, Mdfr2 Numl)erOfMdfrs 
Mdfe NumberOfMdfrs 
9 Mdfrl, Mdfr2, 
Mdfe 
10 Mdfl'l, Mdfr2 
Coordination 
(Total : 90) 
Mdfrl-MdfrType-ll )to-Mdfr2-Typc 
Mdfl'2-MdfrTypeql )to-Mdfl-I -q'ype 
Mdfi'l -Md frType-lDto- Md fr2-MdfrType 
11 Mdfrl, Mdfr2, Rel)etition-llead-l,ex 
Mdfe Repetition-Md fr-lleadq,ex 
12 Mdfrl, Mdfr2 ReferencePronoun 
i~efereneePronou  (String) 
',.%066) 
~u (verb), s~u (adjective), ~,'~ (noun) . . . .  (11) 
~'~:~ (common oun), m'~ (quantifier) . . . .  (24) 
~1~ (vowel verb) . . . .  (30) 
'.'~ (stem), t~*~ (fandamental form) . . . .  (60) 
rrue (1) 
true (1) 
true (1) 
:~, :a ,  <-u<, t:u, ~, ~=, t . . . .  (7:3) 
uJ:~ (post-positional particle), . . .  (43) 
?,~JJ'~ (e~use marker), ~*$ (imperative form) . . .  (102) 
'~',5, ~<', a~, ,., l,~. . . . .  (63) 
)ill\], ~$m (ease marker), . . .  (5) 
~, ~, *, ~,*,, ... ((~3) 
~,~;~1 (ease marker) . . . .  (41 
\[nil\], \[exist\] (2) 
A(0), B(1), C(2), 1)(3 or more) (4) 
A(2), B(:3), C(4 or more) (3) 
P(Coordim~te), A(Apposition), I)(otherwise) (3) 
\]'rue, False (2) 
rrue, False (2) 
true, False (2) 
86.65% 73.87% 
\[--0.79%) (--1.54%) 
87.07% 75.03% 
',--O.37%) (--0.38%) 
87.39% 75.20% 
',-0.05%) (-0.m%) 
87.21% 75.20% 
\[--0.23%) (--0.21%) 
84.78% 70.03% 
(-2.66%) (-5.38%) 
87.32% 75.14% 
(-0.12% (--0.27%) 
87.39% 7 ~  
(-0.05%) (+0.13%) 
87.14% 74.86% 
(-0.30%) (-0.55%) 
87.40% 70.30?./o 
(-0.04%) (-0.0~%) 
8~.2~% 73.61% 
(--1.18% (--1.80%) 
87.34% 75.09% 
(--0.10%) (--0.'32%) 
\[nil\],\[exist\] (2) 87.31% 75.id% 
\[nil\], \[exist\] (2) (--0.13%) (--0.27%) 
\[nil\], \[exist\] (2) 87.27% 75.12% 
:~, :a,  :~ .~,~=.~, ,  ~. . . . .  (42) (--0.17%) (--0.29%) 
'%} @ (special marks)," "112 N (1)ost-posi~ioual 
particles)," or "}~N~? (suffixes)." 
Head-Lex :  the fllndalnental forth (unintlected 
forln) of the head word. Only words with a fre- 
quency of tlve or more are used. 
Head- In f :  the inflection type of a head. 
SemFeat :  We use the upper third layers of bunrui 
.qoihyou (NLl/I(National Language Research In- 
stitute), 19641 as semantic features. Bunrui goi- 
hyou is a Japanese thesaurus that has a tree 
structure and consists of seven layers. The tree 
has words in its leaves, and each word has a fig- 
ure indicating its category number. For exam- 
ple, the figure in parenthesis of a feature "Head- 
SemFeat( l l0)" in Table 3 shows the upper three 
digits of the category number of the head word 
or the ancestor node of the head word in the 
third layer in the tree. 
Type:  the rightmost word other than those whose 
major part-of-speech category is "~@ (special 
marks)." If the major category of the word 
is neither "NJN (post-positional particles)" nor 
"}~/~'~ (suffixes)," and the word is inflectable, 2 
then the type is represented by the inflection 
type. 
JOSHI1 ,  JOSHI2 : JOSHI1  is the rightmost post- 
positional particle in the bunsetsu. And if there 
are two or more post-positional particles in the 
bunsetsu, JOSHI2 is the second-rightmost post- 
positiolml particle. 
NmnberOfMdf rs :  number of modifiers. 
2The inflection types follow those of J UMAN. 
Mdfr l -Mdf rType ,  Mdf r2 -Mdf rType:  Types of 
tile modifiers of Mdfi'l and Mdfr2. 
X- IDto -Y :  X is identical to Y. 
Repet i t ion -Head-Lex :  a ret)etition word allpear- 
ing ill a preceding senteuce. 
Re ferencePronour l :  a reference pronoun appear- 
ing in the target bunsetsu or ill its modifiers. 
Categories 1 to 6 ill Table 3 reI)resent attributes 
in a bunsetsu, categories 7 to 10 represent syntac- 
tic information, and categories 11 and 12 represent 
contextual information. 
The results of our experiment are listed in Table 5. 
The first line shows tlle agreement rate when we esti- 
mated word order for 5,278 bunsetsus that have two 
or more modifiers and were extracted from 2,394 sen- 
tences al)pearing on Jmmary 9th and from June 10th 
to June 301tl. \Ve used bunsetsu boundary informa- 
tion and syntactic and contextual information which 
were derivable froln the test corpus and related to 
the input bunsetsus. As syntactic ilffOrlnation we 
used dependency inforlnation, coordinate structure, 
and information on whether the target bunsetsu is at 
the eM of a sentence. As contextual information we 
used the preceding sentence. The values in the row 
labeled Baseline1 in Table 5 are the agreement rates 
obtained when every order of all pairs of modifiers 
was selected randolnly. And values in the B&seline2 
row are the agreement rates obtained when we used 
the following equation instead of Eq. (5): 
freq(w12) 
PMu'(llh) = freq(w12) + frcq(w21)" (7) 
875 
Table 4: Combined features. 
Accuracy without 
the feature 
Pair of Complete 
modifiers :tgreement 
87.23% 74.65% 
(-0.21%) (-0.76%) 
Combined  features  
- -  Twin tbatures 
(Mdfr 1-Type, Mdfr2-Type) ,  
(Mdfr 1-Type,  Mdfe- I Iead-Lex) ,  
(Mdfr  1-Type,  Md fe- t tead-POS) ,  
(Mdfr  1-Type,  Mdfr  1-Coordlnrtt ion),  
(Mdfr  1-Type, Mdf r2 -Mdf rType- IDto -Md fr1-2"ypc), 
(MdfrE-Type,  Mdfe-Head-Lex) ,  
(Mdfrg-Type,  Mdfe-Head-POS) ,  
(Mdfr2-Type,  Mdfr2-Ooord inat ion) ,  
(Mdfr2-Type,  Md fr 1-MdfrType- lDto-Mdfr2-Type) ,  
Mdfr  1-Head-Lex,  Mdfe-Per iod) ,  
Mdfr  1 -nead-POS,  Mdfe-Per lod) ,  
Mdfr  1-1tead-POS, Mdfr  1-Repet l t lon-Head- I ,ex) ,  
Mdfr2- I tead-Lex,  Mdfe-Pet lod) ,  
Mdfr2-Ite,~d-POS, Mdfe-Per lod) ,  
Mdf r2 - I Iead-POS,  Mdfr2- I tepet l t lon- I lead-Lex)  
' t~iplet  tca tures  87.22% i 74.86% 
Mdfr l -Wype,  Mdfr2-Type,  Mdfe- l lead-Lex) ,  (--0.220./0) (--0.55%) 
Mdfr l -Type ,  Mdf r2 -Type,  Mdfe-Head-POS) ,  
Mdf r l -Type ,  Mdf r l -Coord inat lon ,  Mdfe-Type) ,  
MdfrE-Type,  Mdfr2-Coord lnat lon ,  Mdfe-Type) ,  
Mdf r l - JOSHI1 ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-Lex),  
Mdf r l -aOSHl l ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-POS) ,  
Mdf r2 - JOSHI1 ,  Mdfr2- JOSI I I2 ,  Mdfe-Head-Lex),  
Mdf r2-aOSH\[1 ,  Mdfr2-JOSl~12, Mdfo- l lead-POS)  
All of  above  combined  features  85.79% 77.67% 
~--1.65%) (--3.74%) 
Table 5: Results of agreement rates. 
Agreement  ra te  
Pair of modifiers Coml)lete agreement 
Our method 87.44%(72,367/14,137) 75.41% (3,980/5,278) 
Baseline1 48.96% (6,921/14,137) 35.10% (7,747/5,278) 
Baseline2 49.20% (6,956/14,137) 53.84% (1,786/5,278) 
IIere we assume that B1 and \]32 are modifiers, their 
modifiee is B, the word types of B1 and \]32 are re- 
spectively Wl and we. The values frcq(wr2) and 
frcq(w.27 ) then respectively represent the fl'equencies 
with which w7 and w,2 appeared in the order "WT, we, 
mid w" and "w2, WT, and w" in Malnichi newspaper 
articles fl'om 1991 to 1997. a Equation (7) means 
that given the sentence "~t~lt I:t (Taro_wa) / ~- -- ~, 
(tennis_wo) / b ~-:o (sita.)," one of two possibili- 
ties, "1$ (wa) / ~ (wo) / t, ~:o (sita.)" and "#c (wo) 
/ tS (wa) / b ~:o (sita.)," which has the higher fre- 
quency, is selected. 
3.3 Features  and Agreement  Rate  
This section describes how much each feature set con- 
tributes to improving the agreement rate. 
The values listed in the rightmost columns in Ta- 
bles 3 and 4 shows the performance of the word or- 
der estimation without each feature set. The values 
in parentheses are the percentage of improvement or
degradation to the formal experiment. In the exper- 
iments, when a basic feature was deleted, the com- 
bined features that included the basic feature were 
also deleted. The most useful feature is the type of 
3When wl and w2 were the same word, we used the head 
words in Bt  and 132 as Wl and w2. When one offreq(wt2) and 
freq(w21) was zero and the other was five or more, we used 
the f lequencies when they appeared in the order "Wl ws" and 
"w2 wt,"  respectively~ instead of frcq(wi2) al,d freq(wsl). 
When both freq(wl.2) and freq(w27) were zero, we instead 
used random figures between 0 and t. 
bunsetsu, which basically signifies the case marker or 
inflection type. This result is close to our expecta- 
tions. 
We selected features that, according to linguistic 
studies, as mudl  as possible reflect the basic condi- 
tions governing word order. The rightmost column 
in Tables 3 and 4 shows the extent o which each con- 
dition contributes to improving the agreement rate. 
However, each category of features might be rougher 
than that which is linguistically interesting. For ex- 
ample, all case markers uch as "wa" and "wo" were 
classified into the same category, and were deleted 
together in the experiment when single categories 
were removed. An experiment that considers each 
of these markers eparately would help us verify the 
importance of these markers separately. If we find 
new features in future linguistic research on word or- 
der, the experiments lacking each feature separately 
would help us verify their importance in the same 
manner .  
3.4 Tra in ing  Corpus  and Agreement  Rate  
The agreement rates for the training corpus and the 
test corpus are shown in Figure 1 as a function of 
the amount of training data (ntunber of sentences). 
The agreement rates in the "pair of modifiers" and 
'?!19~ . . . . .  %:I:: 'i, ~ . . . . . . . . .  ~':z~-,~',:: : : i 
= 
?I 90 90 
!' 
S5 ~5 i ~ - 
E 
75 ~ 75 
7O 7O 
65 . . . . . . . .  (,5 0 . . . .  
0 2000 400{) 6000 80O0 Io(mO 120{}0 14O00 16000 18000 2000 4000 6,300 ~000 IODO0 12(;00 14000 16O00 la00{1 
\] 11o NtllObor o~ Snnloncos \]o 111o Traif l in9 Data l lm Number ol Sentences in l lm T f 4dlli11U \[)ala 
Figure 1: Relationship between tile amount of training 
data and the agreement rate. 
"Complete agreement" measurements were respec- 
tiw~ly 82.54% and 68.40%. These values were ob- 
tained with very small training sets (250 sentences). 
These rates m'e considerably higher than those of 
the baselines, indicating that word order in Japanese 
can be acquired fl'om newspaper articles even with a 
small training set. 
With 17,562 training sentences, the agreemenl, 
rate in the "Complete agreement" measurement was 
75.41%. We randomly selected and analyzed 100 
modifiees from 1,298 modifiees whose modifiers' word 
order did not agree with those in the original text. 
We found that 48 of them were in a natural order 
and 52 of them were in an unnatural order. The 
former result shows that the word order was rela- 
tively fl'ee and several orders were acceptable. The 
latter result shows that the word order acquisition 
was not sufficient. To complete the acquisition we 
need more training corpora and features which take 
into account different information than that m Ta- 
bles 3 mid 4. We found many idiomatic expres- 
876 
sions in the uimatural word order results, such as "~ 
ffl\[~il~:5~ (houchi-kokka_ga,  country under the rule 
of law) / \ [ l} l~  (kiitc, to listen) /~#t~ (alcireru, 
to disgust), ~rj ~ b ?= a ~ (souan-s~,ta-no_ga, orlgl- 
,ration) / ~ *o ~- *o co (somosomo-no, at all) / ~t~  U 
(hg~'ima~'4 the beginning)," and ""~ l~ (g#-~4 taste) / 
~'~B (seikon, one's heart and soul) /g~ 6 (homcru, 
to trot somethil,g into soinething)." We think that 
the apt)ropriate word order for these idiomatic ex- 
pressions could be acquired if we had more training 
data. We also found several coordinate structures in 
the Ulnlatural word order results, suggesting that we 
should survey linguistic studies on coordinate struc- 
tures and try to find efllcient features for acquiring 
word order from coordinate structures. 
We (lid not use the results of semantic and con- 
textual analyses as input because corpora with se- 
mantic and contextuM tags were not available. If 
such corpora were available, we could more et\[iciently 
use features dealing with seinantic features, reference 
pronouns, and repetition words. We plan to make 
corpora with semantic and contextual tags and use 
these tags as input. 
3.5 Acqu is i t ion  f rom a Raw Corpus  
In this section, we show that a raw cortms instead of 
a tagged corpus can be used to train the lnodel, if it 
is first analyzed by a parser. We used the lnorl)holog- 
ical analyzer JUMAN and a tmrser KNP (Kurohashi, 
11198) which is based on a det)endency grainlnar, 
it, order to extract iuforumtion from a raw corpus 
for detecting whether or not each feature is found. 
'l?tm accuracy of JUMAN for detecting inorphologi- 
cal boundaries and part-of-speech tags is about 98%, 
and the parsecs dependency accuracy is about 90%. 
These results were obtained from analyzing Mainichi 
newspaper articles. 
We used 217,562 sentences for training. When 
these sel~t, ences were all extracted from a raw corlms , 
the agreement rate was 87.64% for "pair of modifiers" 
and was 75.77% for "Colnplete agreement." When 
the 217,562 training sentences were sentences fl'oln 
the tagged cortms (17,562 sentences) used in our for- 
real exl)eriment aInl froln a raw cortms, the agree- 
" e S :~ ment rate for "pair of lno(hfi.r, was 87.66% and 
for "Complete agreement" was 75.88%. These rates 
were about 0.5% higher than those obtained when we 
used only sentences from a tagged corlms. Thus, we 
can acquire word order by adding inforlnation froln 
a rmv corpus even if we do not have a large tagged 
corpus. The results also indicate that the parser ac- 
curacy is not so significant for word order acquisition 
and that an accuracy of about 90% is sufficient. 
4 Conc lus ion 
This paper described a method of acquiring word or- 
der froln corpora. We defined word order as the order 
of lnodifiers which depend on tile same lnodifiee. The 
lnethod uses a model which estimates the likelihood 
of the apt)ropriate word order. The lnodel automat- 
ically discovers what the tendency of the word order 
in Japanese is by nsing various ldnds of information 
in and arouud the target bunsetsus plus syntactic 
and contextual inforlnation. The contribution rate 
of each piece of inforination in deciding word order 
is efficiently learned by a model implemented within 
an ),,I.E. framework. Comparing results of experi- 
ments controlling for each piece of information, we 
found that the type of inforinatiou having the great~ 
est influence was the case marker or inflection type in 
a bunsetsu. Analyzing the relationship between the 
amount of training data and the agreement rate, we 
fimnd that word order could be acquired even with 
a small set of training data. We also folmd that a 
raw cortms as well as a tagged cortms can be used to 
train the model, if it is first, analyzed by a parser. The 
agreement rate was 75.41% for the Kyoto University 
corpus. We analyzed the lnodifiees whose modifiers' 
word order did not agree with that in the original 
text, and folmd that 48% of theln were in a natural 
order. This shows that, in umny cases, word order 
in Japanese is relatively free and several orders are 
acceptable. 
The text we used were lmwspaper articles, which 
tend to have a standard word order, but we think 
that word orders tend to differ between ditferent 
styles of writing. We would therefore like to carry 
out experiments with other types of texts, such as 
novels, having styles different froln that of newspa- 
pers. 
it has been (lift\]cult o evaluate tile reslflts of text 
generation objectively becmlse there have been no 
good stmldards for ewlllmtion. By using the stan- 
(lard we describe in this paper, however, we can evN- 
uate results objectively, at least for word order esti- 
mation in text, generation. 
We expect hat our lnodel can be used for several 
applications as well as linguistic veritication, such as 
text; refinement silt)port and text generation in nla- 
chine translation. 
References  
Adam L. Better, Stephen A. I)ella Pietra, and Vincent J. Della 
Pietra. 199(L A Maximum t'\]ntropy Approach to N~ttural ,~tn- 
gmtge Processing. Computational Linguistics, 22(11:39-71. 
Sadao Kurohashi and Makol.o Nagao. 1997. Kyoto University 
Text Corl)uS Project. In Proceedings of The Third Annual 
Mectin9 of The Association for Natural Language Process- 
ing, pages 115-118. (in Japanese). 
Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morpho- 
logical Analysis System JUMAN Version 3.6. l)epartment of 
Informatics, Kyoto University. 
Sadao Kurohashi, 11198. Japanese Dependency/Case Structure 
Analyzer KNP Version 2.0b6. Department of Inforln~tties, Ky- 
ore University. 
IIiroshl Maruyama. 1994. Experhnents on V~rord-Order Recovery 
Using N-Cram Models. In YTte \]~9th Annual (;onvention IPS 
Japan. (in Japanese). 
NLRl(National Language Research Institute). 1964. Word List 
by Semantic Pri~ciples. Syuei Syuppan. (in Japmlese). 
Tetsuo Saekl. 1998. Yousetsu nihongo no 9ojun (Survey: Word 
Order in Japanese). Kuroshio Syupl)an. (in Japanese). 
James Shaw and Vasileios Ilatzivassiloglou. 1999. Ordering 
Among l'remodifiers. In Proceedings of the 37th Annual Meet- 
ing of the Association for Computational Linguistics (,4 CL), 
pages 135-143. 
Takenobu Tokunaga and IIozumi Tanaka. 1991. On Estimat- 
ing Japanese Word Order B~used on Valency Information. 
Keiryo Kokugogaku (Mathematical Linguistics), 18(21:53-(;5. 
(in Japanese). 
877 
A Statistical Approach to the Processing of Metonymy 
Masao Ut iyama,  Masak i  Murata ,  and H i tosh i  I sahara  
Communicat ions  Research L~boratory, MPT ,  
588-2, Iwaoka, Nishi-ku, Kobe, Hyogo 651-2492 Japa l  
{mut iyam~,murat~, isahara} ~crl.go.j  p
Abst ract  
This paper describes a statistical approach to 
tile interpretation of metonymy. A metonymy 
is received as an input, then its possible inter- 
p retations are ranked by al)t)lying ~ statistical 
measure. The method has been tested experi- 
mentally. It; correctly interpreted 53 out of 75 
metonymies in Jat)anese. 
1 I n t roduct ion  
Metonymy is a figure of st)eech in which tile 
name of one thing is substituted for that of 
something to which it is related. The czplicit 
tc.~m is 'the name of one thing' and the implicit 
t;c~"m is 'the name of something to which it; is 
related'. A typical examt)le of m(;tonymy is
He read Shal(esl)eare. (1) 
'Slmkesl)(~are' is substitut(~d for 'the works of 
Shakespeare'. 'Shakest)eare' is the explicit term 
and 'works' is the implicit term. 
Metonymy is pervasive in natural language. 
The correc~ treatment of lnetonylny is vital tbr 
natural language l)rocessing api)lications , es- 
1)ecially for machine translation (Kamei and 
Wakao, 19!)2; Fass, 1997). A metonymy may be 
aecel)table in a source language but unaccet)t- 
able in a target language. For example, a direct 
translation of 'he read Mao', which is acceptable 
in English an(1 Japanese, is comt)letely unac- 
ceptal)le in Chinese (Kamei and Wakao, 1992). 
In such cases, the machine trmlslation system 
has to interl)ret metonynfies to generate accept- 
able translations. 
Previous approaches to processing lnetonymy 
have used hand-constructed ontologies or se- 
mantic networks (.\]?ass, 1988; Iverson and Hehn- 
reich, 1992; B(maud et al, 1996; Fass, 1997). 1 
1As for metal)her l)rocessing, I 'errari (1996) used t;ex- 
Such al)t)roaches are restricted by the knowl- 
edge bases they use, and may only be applicable 
to domain-specific tasks because the construc- 
tion of large knowledge bases could be very d i f  
ficult. 
The method outlined in this I)apcr, on the 
other hand, uses cortms statistics to interpret 
metonymy, so that ~ variety of metonynfies 
can be handled without using hand-constructed 
knowledge bases. The method is quite t)romis- 
ing as shown by the exl)erimental results given 
in section 5. 
2 Recogn i t ion  and  In terpretat ion  
Two main steps, recognition and i'ntc.'q~vc- 
ration, are involved in the processing of 
metonyn~y (Fass, 1.!)97). in tile recognition st;el), 
metonylnic exl)ressions are labeled. 1111 the in- 
tel'l)r(:tation st;el) , the meanings of those ext)res- 
sions me int, eri)reted. 
Sentence (1), for examl)le, is first recognized 
as a metonymy an(t ~Shakespeare' is identified 
as the explicit term. 't'he interpretation 'works' 
is selected as an implicit term and 'Shakespeare' 
is replaced 1)y 'the works of Shakespeare'. 
A conq)rehensive survey by Fass (\]997) shows 
that the most COllllllOll metho(1 of recogniz- 
ing metonymies i by selection-restriction vio- 
lations. Whether or not statistical approaches 
can recognize metonymy as well as the selection- 
restriction violation method is an interesting 
question. Our concern here, however, is the 
interpretation of metonymy, so we leave that 
question for a future work. 
In interpretation, an implicit term (or terms) 
that is (are) related to the explicit term is (are) 
selected. The method described in this paper 
uses corpus st~tistics for interpretation. 
tual clues obtained through corl)us mmlysis tor detecting 
metal)lmrs. 
885 
This method, as applied to Japanese 
metonymies, receives a metonymy in a phrase 
of the tbnn 'Noun A Case-Marker R Predicate 
V' and returns a list of nouns ranked in or- 
der of the system's estimate of their suitability 
as interpretations of the metonylny, aSSulning 
that noun A is the explicit tenn. For exam- 
ple, given For'a  wo (accusative-case) kau (buy) 
(buy a Ford),  Vay .sya (ear), V .st .sdl  , 
r'uma (vehicle), etc. are returned, in that order. 
Tile method fbllows tile procedure outlined 
below to interpret a inetonymy. 
1. Given a metonymy in the form 'Noun A 
Case-Marker R Predicate V', nouns that 
can 1)e syntactically related to the explicit 
term A are extracted from a corpus. 
2. The extracted nouns are rmlked according 
to their appropriateness a interpretations 
of the metonymy by applying a statistical 
measure. 
The first step is discussed in section 3 and the 
second in section 4. 
3 In fo rmat ion  Source  
\?e use a large corpus to extract nouns which 
can be syntactically related to the exl)licit term 
of a metonylny. A large corpus is vahmble as a 
source of such nouns (Church and Hanks, 1990; 
Brown et al, 1992). 
We used Japanese noun phrases of the fornl 
A no B to extract nouns that were syntactically 
related to A. Nouns in such a syntactic relation 
are usually close semantic relatives of each other 
(Murata et al, 1999), and occur relatively infre- 
quently. We thus also used an A near B rela- 
tion, i.e. identifying tile other nouns within the 
target sentence, to extract nouns that may be 
more loosely related to A, trot occur more fre- 
quently. These two types of syntactic relation 
are treated differently by the statistical nleasure 
which we will discuss in section 4. 
The Japanese noun phrase A no B roughly 
corresponds to the English noun phrase B of A, 
lint it has a nmch broader ange of usage (Kuro- 
hashi and Sakai, 1999). In fact, d no B can ex- 
press most of the possible types of semmltic re- 
lation between two nouns including metonymic 
2~Ford' is spelled qtSdo' ill Japanese. We have used 
English when we spell Japanese loan-words from English 
for the sake of readability. 
concepts uch as that the name of a container 
can represent its contents and the name of an 
artist can imply an art~brnl (conta iner  for 
contents and artist for a r t fo rm below).a Ex- 
amples of these and similar types of metonymic 
concepts (Lakoff and Johnson, 1980; Fass, 1997) 
are given below. 
Container for contents  
? glass no mizu (water) 
? naV  (pot) , y6 i (food) 
Art ist  for artform 
? Beethoven o kyoku (music) 
? Picas.so no e (painting) 
Object  for user 
? ham .sandwich no kyaku (customer) 
? sax no .sO.sya (t)erformer) 
Whole  tbr part 
? kuruma (car) no tirc 
? door" no knob 
These exalnt)les uggest hat we can extract 
semantically related nouns by using tile A no B 
relation. 
4 Stat is t ica l  Measure  
A nletonymy 'Noun A Case-Marker R, Predi- 
cate V' can be regarded as a contraction of 
'Noun A Syntactic-Relation (2 Noun B Case- 
Marker R Predicate V', where A has relation 
Q to B (Yamamoto et al, 1998). For exam- 
ple, Shakc.spcare wo yomu (read) (read Shake- 
speare) is regarded as a contraction of Shake- 
speare no .sakuhin (works) 'wo yomu (read the 
works of Shakespeare), where A=Shake.spcare, 
Q=no, B=.sakuhin, R=wo,  and V=yomu. 
Given a metonymy in the fbrln A R 17, the 
appropriateness of noun B as an interpretation 
of the metonymy under the syntactic relation Q 
is defined by 
LQ(BIA,/~, V) - Pr(BIA, (2, 1~, V), (2) 
ayamamoto et al (\]998) also used A no /3 relation 
to interpret metonymy. 
886 
where Pr( . - . )  represents l)robal/ility and Q is 
either an A no B relation or an A near \]3 re- 
lation. Next;, the appropriateness of noun \]3 is 
defined by 
M(BIA, Ie, V) -nlaxLc~(BIA, l~,V ). (3) 
O 
We rank nouns 1)y at)plying the measure 214. 
Equation (2) can be decomposed as follows: 
LQ(!31A, R,, V) 
= Pr (B IA  , Q, R,, V)  
Pr(A, Q, B, R,, V) 
Pr( A, Q, R, v) 
Pr(A, Q, 13)lh'(R, VIA, Q, Ix) 
Pr(A, Q) Pr(R, VIA, Q) 
Pr(BIA , Q)Pr(R, VIB) 
-~ er(R, v) ' (4) 
where (A, O) and {\]~,, V} are assumed to l)e in- 
del)endent of each other. 
Let f(event)1)e the frequen(:y of an cve'nt and 
Classc.s(\])) be the set of semantic (:lasses to 
which B belongs. 'l'he expressions in Equation 
(4) are then detined t)y 4 
I'r(~lA, Q) - .t'(A, Q, ~x) _ f (A,  Q, ~) 
f (A ,  Q) ~1~ f (A ,  Q, 13)' 
(5) 
Pr(~., riB) 
IU~,I~,v) i' ' *: .1 (U, ~, V) > 0, 
.~- ~,c~cl .......... (10 Pr(l)'l(/)f(C/'R'V) 
J'US) 
otherwise, 
((0 
Pr (B IC  ) - .f(13)/ICI-s.w-.XB)l j ( c )  (r) 
We onfitted Pr(H,, 17) fi'om Equat ion (4) whell 
we calculated Equation (3) in the experiment 
de, scribed in section 5 for the sake of simplicit> 
4Strictly speaking, Equation (6) does not satist\]y 
X',,e,vpr(R, vl/x) -- 1. We h~wc adopted this det- 
inition for the sake of simplicity. This simplifi- 
cation has little effect on the tilml results because 
~--;c'cc~ ........ (m Pr(l~lC)f(C,I~', V) << I will usually 
hohl. More Sol)histieated methods (M;mning ml(t 
Schiitze, 1999) of smoothing f)robability distribution 
m~y I)e I)eneticial. itowever, al)l)lying such methods 
and comparing their effects on the interpretation of
metonymy is beyond the scope of this l)aper. 
This t reatment  does not alter the order of the 
nouns ranked by the syst;em because l?r(H., V) 
is a constant for a given metonymy of the form 
AR V. 
Equations (5) and (6) difl'er in their t reatment  
of zero frequency nouns. In Equat ion (5), a 
noun B such that  f (A ,  Q, B) = 0 will l)e ignored 
(assigned a zero probal)ility) because it is un- 
likely that  such a noml will have a close relation- 
shii / with noun A. In Equation (6), on the other 
hand, a noun B such that  f (B ,  R, V) = 0 is as- 
signed a non-zero probability. These treatments 
reflect the asymmetrical  proper~y of inetonymy, 
i.e. ill a nletonylny of the form A 1{ 1~ an 
implicit term 13 will have a much t ighter rela- 
tionship with the explicit term A than with the 
predicate V. Consequently, a nouil \]3 such that 
f (A ,Q ,  B) >> 0 A f (B ,  JR, V) = 0 may be ap- 
propri~te as an interpretation of the metonymy. 
Therefore, a non-zero t)robat)ility should be as- 
sign(;d to Pr(l~., VI1X ) ev~,n it' I (B ,  2e, V) ; (). ~ 
Equation (7) is the probabil ity that  noun J3 
occurs as a member of (::lass C. This is reduced to 
fU~) if13 is not ambiguous, i.e. IC/a,~,sc.,s,(/3)\[ = f(c) 
1. If it is ambiguous, then f (B )  is distr ibuted 
equally to all classes in Classes(B).  
The frequency of class C is ol)tained simi- 
larly: 
.f(B) (8) 
. f (c )  = ~ ICl(-~c..~(13)1' 11C-.(7 
where 13 is a noun which belongs to the class C. 
Finally we derive 
f(13, ~, v) 
BqC 
(.0) 
In summary,  we use the measure M as de- 
fined in Equat ion (3), and cah:ulated by apply- 
ing Equat ion (4) to Equation (9), to rank nouns 
according to their apl)ropriateness as possible 
interpretat ions of a metonymy. 
Example  Given the statistics below, bottle we 
akeru (open) (open a bottle) will be interpreted 
5The use of Equation (6) takes into account a noun/3 
such that J'(l:~, l{, V) = 0. But, Stlch & llOtlll is usually ig- 
nored if there is another noun B' such that f(13', H., V) > 
0 be~,~,,se. Eo'~ct ....... U~)P, USIO)J'(C,~e.,V) << a < 
J'(lY, H,, V) will usually hokl. This means thai the co- 
occurrence 1)rol)al)iliW between implicit terms and verbs 
are also important in eliminating inapl)rol)riate nomls. 
887 
as described in the fbllowing t)aragraphs, assum- 
ing that cap and rcizSko (refl'igerator) are the 
candidate implicit terms. 
Statistics: 
f(bottlc, no, cap) = 1, 
f(bottlc, no, reizgko) = O, 
f(bottlc, no) = 2, 
f ( bottlc, ncar, cap) = 1, 
f (bottle, near, rciz6ko) = 2, 
f(bottlc, ncar) = 503, 
f(cap) = 478, 
f(rcizSko) = 1521, 
f(cap, wo, akcru) = 8, and 
f(rciz6ko, wo, akcru) = 23. 
f(bottlc, no, rciz6ko) = 0 indicates that bottle 
and rcizSko are not close semantic relatives of 
each other. This shows the effectiveness of us- 
ing A no B relation to filter out loosely related 
words. 
Measure: 
L,o(cap) 
Lncar(Cap) = 
Lno(reizSko) = 
Lncar ( reizS ko ) -~ 
f ( bott:le, no, cap) 
.f ( bottlc, no) 
\](ca,p, wo, a\]~c'ru) 
X 
1 8 
-8 .37?10 -3 , 
2 478 
f (bottle, near, cap) 
f(bottlc, near) 
f ( caI), "wo, a\]~cru) 
X 
.f ( ) 
1 8 
50--3 47-8 = 3.33 ? 10 -5, 
.f ( bottlc, no, rcizSko )
.f ( bottlc, no) 
f ( rcizako, wo, ahcru ) 
? 
.f ( rcizdko 
0 23 
2 1521 
.f ( bottlc, near, rcizSko) 
f (bottlc, near) 
f(rcizSko, wo, akcru) 
X 
f ( rciz~ko ) 
2 23 
503 1521 
- 6.01 x 1() -~,  
M(c p) 
= max{Lno(cap),Lnea.,.(cap)} 
= 8.37 x lO-3, and 
~r ( reizSko )
= 6.01? 10 -5 , 
where L,,o(Cap) = L,~o(Caplbo~tle, wo, akeru), 
M(c p) = M(c pl ot tz , and so o51. 
Since M > M we conclude 
that cap is a more appropriate imt)licit term 
than rcizSho. This conclusion agrees with our 
intuition. 
5 Exper iment  
5 .1  Mater ia l  
Metonymies  Seventy-five lnetonymies were 
used in an ext)erilnent to test tile prol)osed 
lnethod. Sixty-two of them were collected from 
literature oll cognitive linguistics (Yamanashi, 
1988; Yamam~shi, 1995) and psycholinguistics 
(Kusumi, 1995) in Japanese, paying attention 
so that the types of metonymy were sufficiently 
diverse. The remaining 13 metonymies were 
direct translations of the English metonymies 
listed in (Kalnei and Wakao, 1992). These 13 
metonylnies are shown in Table 2, along with 
the results of the experiment. 
Corpus  A corpus which consists of seven 
years of issues of the Mainichi Newspaper (Dora 
1991 to 1997) was used in the experiment. The 
sentences in tlle cortms were mort)hologically 
analyzed by ChaSen version 2.0b6 (Matsumoto 
et al, 1999). The corpus consists of about 153 
million words. 
Semant ic  Class A Japanese thesaurus, Bun- 
rui Goi-tty6 (The N~tional Language Research 
Institute, 1996), was used in the experiment. It 
has a six-layered hierarchy of abstractions and 
contains more than 55,000 nouns. A class was 
defined as a set of nouns which are classified in 
the same abstractions in the top three layers. 
The total nmnber of classes thus obtained was 
43. If a noun was not listed in the thesaurus, it 
was regarded as being in a class of its own. 
888 
5.2 Method 
'.1.11(; method we have dcseril)e,d was applied I;O 
the metonynfie, s (lescril)e,(t ill section 5.1. Tile 
1)r()eedure described 1)clew was followed in in- 
tert)rel;ing a metonynly. 
1. Given a mel,onymy of the, form :Noun A 
Case-Marker R Predicate, V', nouns re- 
\]al;e(l to A 1)y A 'n,o .1:1 relation an(l/or A 
near H relation were extra(:ix'~(l from 1;he, 
corl)us described in Se(:tion 5.\]. 
2. The exl;racted llOllllS @an(lidatcs) were 
ranked acc()rding t() the nw, asure M d(;tined 
in \]{quation (3). 
5.3 Resu l ts  
The r(;sult of at)l)lying the proi)osexl me, thod to 
our sol; of metol~ymies i  summarized in 'l'alfle 
1. A reasonably good result (:an 1)e s(;cn for 
q)oi;h r(,\]ai;ions', i.e. l;he result ot)i;aincd \])y us- 
ing both A no 11 an(t d ncm" 1\] l'elal;ion~; wllen 
extracting nouus fl'onl th(' cOllmS, \[1'1~(', a(:(:u- 
ra(:y of q)ol;h re, l~tions', the ratio ()f lhe nllnil)er 
of (:orrc(:l;ly intcrl)r(;te,(1 ; t()l)-rank(;(l (:an(li(lates 
to l;he, total mmfl)er of m(',l;()nymies in ()it\]' set, 
w,,s 0.7:, (=5' ,V isa+22))  alld ('ol,ti(t(' l,ce 
inWwva.1 estimal;e was t)(;l;ween ().6\] an(t 0.8\].. 
\?e regard this result as quite t)ronfising. 
Since the mc, i;onymies we used wcr(; g(m(u'a\]: 
(lomain-in(lel)(',ndca~t, on(s, l;h(~ (legr(', ~, ()f a(:cu- 
racy achi(;ve, l in this (~xp(;rim(;nt i~; likely t() t)(; 
r(',t)(',al;e(l when our me?hod is ~q)l)lie(l t() oth(;r 
genural sets ()f mel;onymies. 
'.\['~l)l(; l : tt3xl)erimental r('sults. 
I{,elal;ions used Corre(;t \?'rong 
Both relations 53 22 
Only A 'no B 50 25 
Only A near  13 d3 32 
Tal)le 1 also shows that  'both relations' is 
more ae(:ural;e than (',il;her the result obtained 
1)y solely using the A no \]3 relation or the A 
near  B relation. The use of multit)le relations 
in mel, onyn~y int(;rl)retation is I;hus seen to l)e 
1)enefieial. 
aThe correct;hess was judged by the authors. A candi- 
dat(; was judged correct when it; made sense in .Ial)anese. 
For examl)le, we rcgard(;d bet:r, cola, all(l mizu (W;d;el') 
as all (:orr(!c\[; intcrl)r(~l;ations R)r glas.s we nom, u (drink) 
(drink a glass) because lhey llla(le ,q(~llSC in some (:ontcxt. 
Table 2 shows the, results of applying the 
method to the, thirteen directly translated 
metonymies dcscril)ed in sect;ion 5.1.. Aster- 
isks (*) in the tirst (;ohlillll indicate that  direct 
translation of the sentences result in unaccel)t- 
able Japanes(;. The, C's and W's in t;he sec- 
ond eohmm respectively indicate that  the top- 
ranked ('andi(latcs were correct and wrong. The 
s(;nten(:es in the l;hir(t column are the original 
English metonymi(;s adol)tc, d fl'om (Kamci and 
\?akao, t992). The Japanese llletollylllies in 
th(: form h loun  ease-lnarker predi(:ate 7', in the 
fourth column, are the illputs I;o the method. 
In this ('ohunn, we and  9 a mainly r(;present 
I;he ac(:usal;ive-casc and nominative-ease, re- 
Sl)ectively. The nouns listed in the last eolmnn 
m'e the tot) three candidates, in order, according 
to the. measure M that was defined ill Equation 
(3). 
Th(,,se, l'csull;s ( lemonstrate the et\[~(:tiveness of 
lhe m(',thod. '.l>n out of t;11(: 13 m(;tonynfies 
w(u'c intc, rt)rete,(l (:orre, ctly. Moreover, if we 
rcsl;ri(:t our al;l;(',nti()n to the ten nietonylHics 
i}mt m'e a(:(:Cl)tal)le, ill ,/al)anese, all l)ut one 
w(;rc, inl;('rl)r(;te(t (:orrectly. The a(:curacy was 
0.9 ---- (/)/\]0), higher than that  for q)oth rela- 
tions' in Tal)le i. The reason fi)r the higher de- 
gl'ee of ac(:tlra(;y is l;\]lal; the lll(;|;Ollyllli(;s in Tal)le 
2 arc semi,what yi)ical and relativ(;ly easy to 
int(~rl)rel; , while, the lnel;(nlynlics (:olle(:l;c(t fl'()m 
,lal)anese sour(:es included a (liversity of l;yl)es 
and wcr(~ more difficult to intext)let. 
Finally, 1;11(', efl'ecl;iv(umss of using scnlanl;i(: 
classes is discussed. The, l;op candidates ot! six 
out of the 75 metonynfies were assigned their 
al)prot)riatenc, ss by using their semantic lasses, 
i.e. the wducs of 1;11o measure 114 was calculated 
with f (H , /~ ,  V) = 0 in lgquat;ion (6). Of the, se, 
l;hrce were corrccl,. 011 l;hc, other hand, if sc- 
manl;ic class is not use(l, then three of the six 
are still COITeC|;. Here there was no lint)rove- 
merit. However, when we surveyed the results 
of the whole experiment, wc found that  nouns 
for wlfich .f iB, R,, V) -- 0 often lind (:lose re- 
lationship with exl)licit terms ill m(;tonynfics 
and were al)propriate as interpretat ions of the 
metonynfics. We need more research betbre we 
(:an ju(lgc the etl'ectivc, ness of utilizing semantic 
classes. 
rPl'edicatcs are lemmatized. 
889 
Table 2: Results of applying the proposed lnethod to direct translat ions of the metonymies in
(Kanmi and Wakao, 1992). 
Sentences Noun Case-Mm'l~er Pred. Candidates 
C Dave drank the glasses. 
C The .kettle is boiling. 
C Ile bought a Ford. 
C lie has got a Pieasso in his room. 
C Atom read Stcinbeck. 
C 
C 
W 
C 
W 
C 
Ted played J3ach. 
Ite read Mao. 
We need a couple of strong bodies 
tbr our team. 
There a r___q a lot of good heads in the 
university. 
Exxon has raised its price again. 
glass we nomu 
yakan ga waku 
Ford we kau 
Picasso we motu 
Stcinbcck we yomu 
Bach we hiku 
Mao we yomu 
karada ga hituy5 
atama ga iru 
Exxon 9 a agcru 
Washington is insensitive to the 
needs of the people. 
Washington ga musinkci 
C The T.V. said it was very crowded 
at; the festival. 
W The sign said fishing was prohibited 
here .  
T. V. 9a in 
hy&siki ga iu 
beer, cola, mizu (water) 
yu (hot water), 
oyu (hot water), 
nett5 (boiling water) 
zy@Ssya (car), best seller, 
kuruma (vehicle) 
c (painting), image, aizin (love,') 
gensaku (original work), 
mcisaku (fmnous tory), 
daihySsaku (important work) 
mcnuetto (minuet), kyoku (music), 
piano 
si (poem), tyosyo (writings), 
tyosaku (writings) 
carc, ky~tsoku (rest;), 
kaigo (nursing) 
hire (person),tomodati (friend), 
bySnin (sick person) 
Nihon ( Japan) ,ziko (accident), 
kigy5 (company) 
zikanho (assistant vice-minister), 
scikai (political world), 
9ikai (Congress) 
cotn l l lentgto l '~ anl lOl l l lcer  I (:~stel" 
mawari (surrmmding), 
zugara (design) 
.seibi (lnaintclmnce) 
6 Discuss ion  
Semant ic  Re la t ion  The method proposed in 
this pnper identifies implicit terms fbr tile ex- 
plicit term in a metonymy. However, it is not 
concerned with the semantic relation between 
an explicit; term and implicit term, because such 
semantic relations are not directly expressed ill 
corpora, i.e. noun phrases of the form A no 
B can be found in corpora bul; their senmntic 
relations are not. If we need such semantic re- 
lations, we must semantical ly analyze the noun 
phrases (Kurohashi and Sakai, 1999). 
App l i cab i l i ty  to  o ther  languages  Japan- 
ese noun phrases of the form A no B are specitie 
to Japanese. The proposed method, however, 
could easily be extended to other languages. For 
exmnple, in English, noun phrases B of d could 
be used to extract semantical ly related nouns. 
Nouns related by is-a relations or par t -o f  re- 
lations could also be extracted from corpora 
(Hearst, 1992; Berland and Charniak, 1999). If 
such semantical ly related nouns are extracted, 
then they can be ranked according to the mea- 
sure M defined in Equat ion (3). 
Lex ica l ly  based  approaches  Generative 
Lexicon theory (Pustejovsky, 1995) proposed 
the qualia structure which encodes emantic re- 
lations among words explicitly. It is useflfl to 
infer an implicit term of the explicit term in 
a metonymy. The proposed approach, on the 
other hand, uses corpora to infer implicit terms 
and thus sidesteps the construction of qualia 
structure. 8 
7 Conc lus ion  
This paper discussed a statistical approach to 
the interpretat ion of metonymy. The method 
tbllows the procedure described below to inter- 
pret a metonymy in Japanese: 
1. Given a metonymy of the tbrm 'Noun A 
SBriscoe t al. (1990) discusses the use o1" machine- 
readable dictionaries and corpora for acquMng lexical 
semantic information. 
890 
Case-Marker 1{ Predicate V', nouns that 
are syntactically related to the explicit 
terlll A are extracted front a corpus. 
'.2. The extracted nouns are ranked according 
to their degree of appropriateness as inter- 
pretations of the metonymy by applying a 
statistical measure. 
The method has been tested experimentally. 
Fifty-three out of seventy-five metonymies were 
correctly interpreted. This is quite a prolnis- 
ing first; step towm'd the statistical processing 
of metonymy. 
References  
Matthew Berland and Eugene Charniak. 1999. 
Finding parts in very large corpora. In A (7L- 
99, pages 57- 64. 
Jacques Bouaud, Bruno Bachimont, and Pierre 
Zwcigenbaum. 1996. Processing nletonyllly: 
a domain-model heuristic graph travcrsal 3t> 
preach. In COLINC-95, pages 137-142. 
Ted Briscoc, Ann Copestake, and Bran Bogu- 
racy. 1990. Enjoy the paper: L(;xi(:al seman- 
tics via lexicology. In COLING-90, pages 4:2-- 
4:7. 
I)(fi;cr F. l~rown, gincenl; ,l. Delia Pietra, Pe- 
ter V. deSouza, ,\]enifer C. \]~ai, m~d l/.ol)(',rl; I,. 
Mercer. 1992. Class-1)ased n-gram models of 
m~l;ur~l lmlguage. C~o'm,p'u, tat  ioruzl Li'n, guistics, 
1.8(4) :467 479. 
Kelmeth Ward Church and Patrick Hanks. 
1990. Word association orms, mutual in- 
formation, and lexicography. Uomputatio'n, al 
Lin.quistics, 16(1):22 29. 
Dan Fass. 1988. Metonymy and lnel;al)hor: 
What's the difference? In COLING-88, 
pages \]77-181. 
Dan Fass. 1997. Processin9 Mctonymy and 
Me.taph, or, volume 1 of Cont, cm.porar'y Studies 
in Cognitive Science and '\]'cch, nology. Ablcx 
Publishing Corporation. 
Steplmne Fcrrari. 1996. Using textual clues 
to improve metaphor processing. In ACL-95, 
pages 351-354. 
Marl;i A. Hearst. 1992. Automatic acquisition 
of hyponyms fi:om large text corpora. In 
COLING-92, pages 539 545. 
Eric iverson mid Stephen Helmreich. 1992. 
Metallel: An integrated approach to non- 
literal phrase interpretation. Computational 
Intelligence, 8(3):477 493. 
Shin-ichiro I(amei and Takahiro Wakao. 1992. 
Metonymy: Itcassessment, survey of accept- 
ability, and its treatment in a machine trans- 
lation system. In ACL-92, pages 309-311. 
Sadao Kurohashi and Yasuyuki Sakai. 1999. 
Semantic mmlysis of ,Japmmse noun phrases: 
A new approach to dictionary-lmsed under- 
standing. In ACL-99, pages 481 488. 
Takashi Kusumi. 1995. ttiyu-no S'yori-Katci- 
t;o lmi-Kdzfi (Pr'occssin 9 and Semantic Struc- 
ture of "\]'ropes). Kazama Pul)lisher. (in 
Jalmnese). 
George Lakoff and Mm'k Johnson. 1980. 
Meta, phors lye Live By. Chicago University 
Press. 
Christopher D. Mmming and Hinrich Schiitze, 
1999. Fou'ndations of Statistical Nat.ur(d Lan- 
guage \])recessing, chapter 6. The MIT Press. 
Yuji Matsmnoto, Akira Kitauchi, Tatsuo 
Yamashita, and Yoshitalm Hirano. 1999. 
Japanese morphological anMysis system 
ChaScn mmmal. Nara Institute of Science 
and Technology. 
Masaki Murata, Hitoshi Isalmra, and Makoto 
Nagao. 1999. IX.csolut, ion of indirect anal)hera 
in Jal)anese s(;ntcn('es using examples "X no 
Y (X of Y)". In A 6%'99 Work.shop orl, Core/" 
e.'l'(:,ncc and It.s AppIica, tio'ns, 1)ages 31 38. 
,lames l'ustejovsky. 1995. 2Yt, c Generative Lex- 
icon. 'J?he MI'I' Press. 
Tim National Language I/.ese~rch lalstitute. 
1996. Bv, nr',ui Goi-hyO Z~h,o-bav,(Th:l;o'nom, y 
of ,lapo, nc.s'e., e'nla'ulcd cditio@. (in ,Japancse). 
Atsmnu Yammnoto. Masaki Murata, and 
Makoto Nagao. 1998. Example-based 
metonymy interpretation. In \])'roe. of the 
~t,h. Annual \]lgcel;in 9 of th, c Association for 
Natural Language Prwccssing, pages 606 609. 
(in Japanese). 
Masa-aki Yamanashi. 1988. Hiyu-to \]~ikai 
('1;ropes and Understanding). Tokyo Univer- 
sity Publisher. (in Jalmnese ).
Masa-aki Yamalmshi. 1995. Ninti Bunpa-ron 
(Cognitive Linguistics). Hitsuji Publisher. 
(ill Japanese). 
891 
 
	ff 
		ff 
	ff 	

 
	 ffProceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 892?901,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Comma Insertion for Japanese Text Generation
Masaki Murata
Graduate School of
Information Science,
Nagoya University, Japan
murata@el.itc.nagoya-u.ac.jp
Tomohiro Ohno
Graduate School of
International Development,
Nagoya University, Japan
ohno@nagoya-u.jp
Shigeki Matsubara
Graduate School of
Information Science,
Nagoya University, Japan
matubara@nagoya-u.jp
Abstract
This paper proposes a method for automat-
ically inserting commas into Japanese texts.
In Japanese sentences, commas play an im-
portant role in explicitly separating the con-
stituents, such as words and phrases, of a sen-
tence. The method can be used as an ele-
mental technology for natural language gen-
eration such as speech recognition and ma-
chine translation, or in writing-support tools
for non-native speakers. We categorized the
usages of commas and investigated the ap-
pearance tendency of each category. In this
method, the positions where commas should
be inserted are decided based on a machine
learning approach. We conducted a comma
insertion experiment using a text corpus and
confirmed the effectiveness of our method.
1 Introduction
In Japanese sentences, commas are inserted to mark
word boundaries that might be otherwise unclear be-
cause Japanese is a non-segmented language. They
are also inserted at sharp semantic boundaries to im-
prove the readability of a sentence. While there is a
tendency about the positions where commas should
be inserted in a Japanese sentence, there is no clear
standard for these positions. Therefore, it is hard
for non-natives of Japanese such as foreign students
to insert commas properly, and the method for au-
tomatic comma insertion is required to support sen-
tence generation by such people. In addition, this
method is expected to be useful for improving read-
ability of texts generated by automatic speech recog-
nition or machine translation.
This paper proposes a method for automatically
inserting commas into Japanese texts. There are
several usages of commas, and the positions to in-
sert commas depend on these usages. Therefore,
we grouped the usages of commas into nine cate-
gories, and investigated the appearance tendency for
each category to find the effective features of ma-
chine learning by using Japanese newspaper arti-
cles. Based on the analysis of comma positions, our
method decides whether or not to insert a comma
at each bunsetsu1 boundary in an input sentence by
machine learning.
We conducted an experiment on comma insertion
using the Kyoto Text Corpus (Kurohashi and Nagao,
1998), and obtained higher recall and precision than
those of the baseline, leading us to confirm the ef-
fectiveness of our method.
This paper is organized as follows: The next sec-
tion presents related works. Section 3 gives prelim-
inary analyses. Section 4 explains how our comma
insertion method works. An experiment and discus-
sions are presented in Sections 5 and 6, respectively.
2 Related Works
There have been many investigations on comma in-
sertion into output texts of speech recognition sys-
tems to improve the readability (Christensen et al,
2001; Kim and Woodland, 2001; Liu et al, 2006;
Shimizu et al, 2008). Their methods insert commas
using pause information of speakers, based on the
idea that a point at which a speaker takes a breath
partly corresponds to a point where a comma is in-
serted. However, since pause information cannot be
obtained from texts, we cannot use this approach be-
cause our targets are written texts.
In addition, there have been some investigations
1Bunsetsu is a linguistic unit in Japanese that roughly corre-
sponds to a basic phrase in English. A bunsetsu consists of one
independent word and zero or more ancillary words.
892
on comma insertion into non-Japanese written texts
(White and Rajkumar, 2008; Guo et al, 2010). In
Japanese, there are several usages of commas, and
some usages are specific to Japanese due to its lin-
guistic nature. Therefore, just adopting the above
mentioned methods, which have been developed
to process non-Japanese texts, is not sufficient to
enable high-quality comma insertion into Japanese
sentences. Development of a method based on the
detailed analysis of Japanese commas is required.
Furthermore, there have been some investiga-
tions on comma insertion into Japanese written texts
(Hayashi, 1992; Suzuki et al, 1995). These investi-
gations have adopted rule-based methods. However,
the number of their rules is not necessarily sufficient,
and no quantitative evaluation has been performed.
3 Analyses on Comma Usages
There have been several discussions on commas,
including the draft of ?Kutou-hou (punctuation)?
made by Archives Division, Minister?s Secretariat,
Japanese Ministry of Education, Science and Cul-
ture in 1906. There are several usages of commas,
and depending on the usage, the types of positions
where commas are inserted are different. First, we
examined some previous publications on commas
(Honda, 1982; Inukai, 2002; Shogakukan?s editior-
ial department, 2007). Based on the results of the ex-
amination, we classified the usages of commas into
nine categories shown in Table 1. Here, commas
in Japanese sentences and commas in English sen-
tences have some common roles. In Japanese sen-
tences, some commas have the same roles as com-
mas in English sentences, but some commas have
roles specific to Japanese due to its linguistic nature
such as ?Japanese is a non-segmented language? or
?Japanese has kanji characters and katakana charac-
ters.?
In our study, positions where a comma should
be inserted are detected by using machine learning.
We investigated the Kyoto Text Corpus version 4.0
(Kurohashi and Nagao, 1998) to find the effective
features. The Kyoto Text Corpus is a collection of
Japanese articles of Mainichi newspaper. We used
the articles on January 1st and from January 3rd to
11th in 1995 as the analysis data. Table 2 shows
the size of the data. The data had been manually
Table 1: Categorization of usages of commas
# usage of comma
1 commas between clauses
2 commas indicating clear dependency relations
3 commas for avoiding reading mistakes and
reading difficulty
4 commas indicating the subject
5 commas inserted after a conjunction or
adverb at the beginning of a sentence
6 commas inserted between parallel words or
phrases
7 commas inserted after an adverbial phrase to
indicate time
8 commas emphasizing the adjacent word
9 other
Table 2: Size of the analysis data
sentences 11,821
bunsetsus 117,501
characters 503,970
commas 16,595
characters per sentence 42.63
annotated with information on morphological anal-
ysis, bunsetsu segmentation and dependency2 anal-
ysis. Clause boundaries were detected by the clause
boundary detection program CBAP (Kashioka and
Maruyama, 2004).
Out of all the inserted commas, only 1.43%
were inserted at positions which were not bunsetsu
boundaries. Therefore, we analyzed only commas
inserted at bunsetsu boundaries. Of 105,680 bun-
setsu boundaries, commas were inserted at 16,357
bunsetsu boundaries, that is, the rate of comma
insertion was 15.48%. In the following sections,
we focus on morphemes, clause boundaries, depen-
dency relation and the number of characters between
commas, and investigate their relations with com-
mas.
3.1 Commas between Clauses
If a sentence consists of several clauses, inserting
a comma between clauses makes clear the sentence
2A dependency in a Japanese sentence is a modification re-
lation in which a modifier bunsetsu depends on a modified bun-
setsu. That is, the modifier bunsetsu and the modified bunsetsu
work as a modifier and a modifyee, respectively.
893
Table 3: Rates of comma insertion according to the clause
boundary type
type of clause boundary ratio of comma
insertion (%)
topicalized element-wa 16.94 (1,446/8,536)
adnominal clause 0.72 (43/5,960)
continuous clause 84.57 (2,685/3,175)
compound clause-te 23.31 (394/1,690)
quotational clause 4.40 (74/1,680)
supplement clause 17.53 (245/1,398)
discourse marker 60.13 (650/1,081)
compound clause-ga 93.85 (946/1,008)
compound clause-de 84.52 (606/717)
condition clause-to 81.66 (423/518)
structure. Therefore, a clause boundary is consid-
ered to be a strong candidate of a position where a
comma is inserted. For example, in the following
sentence3:
? ??????????????????????
?????????????????????
(Toward lifting the sanctions imposed on Iraq by
United Nations, the aim seems to be to request fur-
ther cooperation from France, which has close ties
to Iraq.)
a comma is inserted at the clause boundary right af-
ter the continuous clause ???????????
??? (Toward lifting the sanctions imposed
on Iraq by United Nations).? Like this example, the
same usage of commas is seen in English as well.
In the analysis data, there existed 29,278 clause
boundaries excluding sentence breaks. Among
them, commas were inserted at 8,805 positions
(30.01%). The rate is higher than that of bunsetsu
boundaries. This indicates that commas tend to be
inserted at clause boundaries.
We investigated the rate of comma insertion about
114 types4 of clause boundaries. Table 3 shows the
top 10 clause boundary types according to the oc-
currence frequency, and the rates of comma inser-
3We underlined commas which we mentioned in the exam-
ple and the corresponding positions in the translation of the ex-
ample.
4In our research, we used the types of clause boundaries de-
fined by the Clause Boundary Annotation Program (Kashioka
and Maruyama, 2004).
!"#$%&'()*+,-./.012345678#9:;1<=>?@ABC6*DE>1FG+HIJKLMBC6N!"#$%&'()*+',+$-.,/0.&$($,.#1.2$/3$1+.$42/*1+$/3$1+.$*/256)$1+.$&12/#4$72.&.#,.$/3$8+'#($#/1$/#59$-2'#4&$+/7.$-:1$(5&/$,(:&.&$6'33',:51$72/-5.0&;<!!"#$%&$" !'%(%#'%#)*+,%-.&/0#
!"#
!"#$%&#'!()*
$%&'()*
+#,&-$&(#!"#$%&#.(!'$%
+,-
/&,!0&1
./.0
2-#312+
12345
1$(!-.
67#
!"#4%2-+
89:
5(&1&-,&
;<=
%!5&
>?@AB5*CD=
-!$#!-)6#/(2-.1
EF+
*2""2,7)$
GHI
5(!/)&01
JKLAB5
,+71&1
Figure 1: Commas making clear dependency relations
tion. In cases of ?continuous clause? and ?com-
pound clause-de,? the rates were higher than 84%.
On the other hand, in cases of ?adnominal clause?
and ?quotational clause,? the rates were lower than
5%. This means that the likelihoods of comma inser-
tion are different according to the clause boundary
type.
3.2 Commas and Dependency Structure
Commas have a role to make dependency relations
clearer. Commas tend to be inserted right after a
bunsetsu that depends on a distant bunsetsu. In Fig-
ure 1, although the bunsetsu ?? ? (in Asia)?
depends on the bunsetsu ?? ? ? (causes),?
if the comma right after the bunsetsu ????? (in
Asia)? is not inserted, the readers might mistakenly
understand that the bunsetsu ????? (in Asia)?
depends on the next bunsetsu ?????? (strong).?
To avoid the mistake, the comma is inserted.
In the analysis data, there existed 66,984 bunset-
sus which depend on the next bunsetsu. Among the
bunsetsu boundaries right after them, 2,302 (3.44%)
were the positions where a comma was inserted. On
the other hand, in the case of a bunsetsu bound-
ary right after a bunsetsu which does not depend on
the next bunsetsu, the rate of comma insertion was
36.32% (14,055/38,696).
In addition, when the modifyee of a bunsetsu is
located outside the clause containing the bunsetsu,
i.e. to the right of the clause end, commas are con-
sidered to be more frequently inserted right after the
bunsetsu because such bunsetsu causes more com-
plex dependency structure. The rate of comma in-
sertion right after such bunsetsu is 54.24%.
894
3.3 Commas for Avoiding Reading Mistakes
and Reading Difficulty
Although, unlike English, Japanese is a non-
segmented language, word boundaries are easy to
detect because Japanese has three types of charac-
ters; hiragana characters, katakana characters, and
kanji characters. However, if the same types of char-
acters appear sequentially, readers may make a read-
ing mistake or feel difficulty in reading them. To
avoid such mistakes and difficulty, there is a usage
of commas specific to Japanese.
In the following example, a comma is inserted
between two sequentially appearing words ??
(burned)? and ?? (ashes)? both of which consist of
only kanji characters.
? ?????????????????????
??????????????????????
??????????(He seemed to acknowledge
that he had carried the corpse of Mr. Kawasaki to an
acquaintance in Hanasaki, Katashina-mura, Tone-
gun, Gunma Prefecture, burned it and abandoned
its ashes in the mountain forest in Katashina-mura.)
The comma was inserted because if there was no
comma, the word boundary would become unclear
and reading difficulty would be caused. Among
2,409 bunsetsu boundaries over which kanji charac-
ters appeared sequentially, commas were inserted at
2,188 (90.83%) bunsetsu boundaries. In the case of
katakana characters, the rate was 97.69% (211/216).
Commas tend to be inserted at most bunsetsu bound-
aries if kanji characters or katakana characters se-
quentially appear over a boundary.
3.4 Commas Indicating the Subject
Commas are considered to be inserted right after a
bunsetsu that represents the subject of a sentence.
For example, in Figure 2, a comma is inserted right
after the bunsetsu ??? (war)? to indicate that the
bunsetsu is the subject of the sentence. Here, we pay
attention to the clause boundary of the type ?topi-
calized element-wa.? The rate that commas were in-
serted at the clause boundaries ?topicalized element-
wa? was 16.94% (1,446/8,536). This rate is almost
the same as that of bunsetsu boundaries. On the
other hand, the commas inserted at the clause bound-
aries ?topicalized element-wa? accounted for 8.84%
(1,446/16,357) of all the inserted commas.
!"#$%&'()*+,-./"01234567-809:;<34=>?@ABC!"#$%&'#()*#+&#,)-.%/+) 0)1#2345#)()6#732%&'#8*%-#)#/393$%4-3&5:#)&/#0)1#+1%$)53/#503#*+93*#8*%-#4%$$;5+%&#35<=>!"#$%&'!"#$%&'()!% ()*("+ *+,,%- -."/./0("+ 01231-(&#%)/2/*(3&/"4 4565%6#0/34#%,%7 7/45/#-!2/- 89:;231-(&#3(**84!("#/49: <=>?@A5%6#!6(*%4/)!!"#$%&$" !'%(%#'%#)*+,%-.&/0#
Figure 2: Comma insertion at the clause boundary ?topi-
calized element-wa?
In the case of the clause boundary ?topicalized
element-wa? right after a bunsetsu which does not
depend on the next bunsetsu (e.g., the bunsetsu ??
? (war)? in Figure 2), the rate of comma inser-
tion was 20.71% (1,426/6,886). The rate is higher
than that of all the clause boundaries ?topicalized
element-wa.? This shows that commas tend to be
especially inserted at the ?topicalized element-wa?
right after bunsetsus which do not depend on the
next bunsetsu.
3.5 Commas after Conjunction or Adverb
Commas tend to be inserted right after a conjunc-
tion or an adverb located at the beginning of a sen-
tence. These commas correspond to English com-
mas which are inserted right after a word such as
?however? and ?furthermore? located at the begin-
ning of a sentence.
? ??????????????????????
(However, I do not feel like agreeing on it.)
In the analysis data, there existed 695 bunset-
sus whose rightmost morpheme is a conjunction
and which are located at the beginning of a sen-
tence. Among them, commas were inserted right
after 498 (71.65%) bunsetsus. In the case of bun-
setsus whose rightmost morpheme is an adverb, the
rate was 30.97% (140/452).
3.6 Commas Inserted between Parallel Words
or Phrases
Commas have a function which makes clear sepa-
ration between parallel words or phrases. The fol-
lowing example shows commas separating parallel
nouns.
895
? ??????????????????????
????????????????????(The
United Nations should play a lot of roles in a broad
range of fields, such as the global environment,
population, and food.)
In this example, commas are inserted to separate
parallel nouns ? ? (environment),? ?? (popula-
tion)? and ? ? (food)?. In English, there are com-
mas which perform the same role. In fact, commas
were inserted between ?environment? and ?popula-
tion? and between ?population? and ?food? in the
translation of the above example. When bunsetsus
whose rightmost morpheme is a noun appear se-
quentially, the rate of comma insertion between such
bunsetsus is 59.39% (3,330/5,607).
Also, commas are inserted to separate parallel
phrases. In the following example,
? ??????????????????????
??????????????????????
????????(The menu is decided by avoid-
ing the menu the Prime Minister ate on the previous
night, and by considering the balance between the
Japanese food and the European food.)
a comma is inserted right after the bunsetsu ???
? (avoiding)? to make clear separation between the
parallel phrases ?????????? (by avoid-
ing the menu)? and ??????????????
?? (by considering the balance between the
Japanese food and the European food).? The rate
of comma insertion between two parallel phrases is
79.89% (751/940). This is much higher than that of
bunsetsu boundaries, indicating that commas tend to
be inserted when phrases are paralleled.
3.7 Number of Characters between Commas
If there are too many commas at a short distance,
the sentence becomes hard to read. Therefore, the
number of characters between commas is expected
to be not too small. Also, because a long sequence
of characters without a comma is generated if the
distance between commas is very long, the occur-
rence frequency of such sequences of characters is
considered to be low.
We investigated the number of characters between
commas and its occurrence frequency. Figure 3
!
"!!
#!!
$!!
%!!
&!!
'!!
(!!
)!!
" $ & ( * "" "$ "& "( "* #" #$ #& #( #* $" $$ $& $( $* %" %$
!"
"#
$%
&"
%'
($
%)
#%
&"
*
!"#$%&'()'*+,&,*-%&.
Figure 3: Number of characters between commas and its
occurrence frequency
shows the results of investigation. When the num-
ber of characters between commas is either large or
small, the occurrence frequency is low.
4 Comma Insertion Method
In our method, a sentence, on which morphologi-
cal analysis, bunsetsu segmentation, clause bound-
ary analysis and dependency analysis have been per-
formed, is considered the input. Our method de-
cides whether or not to insert a comma at each bun-
setsu boundary in an input sentence. Based on the
analysis results in Section 3, our method adopts the
bunsetsu boundaries as candidate positions where a
comma is inserted. Our method identifies the most
appropriate combination among all combinations of
positions where a comma can be inserted, by using
the probabilistic model. In this paper, input sen-
tences which consist of n bunsetsus are represented
by B = b1 ? ? ? bn, and the results of comma inser-
tion by R = r1 ? ? ? rn. Here, ri is 1 if a comma
is inserted right after bunsetsu bi, and 0 otherwise.
We indicate the j-th sequence of bunsetsus created
by dividing an input sentence into m sequences as
Lj = b
j
1 ? ? ? b
j
nj (1 ? j ? m), and then, r
j
k = 0 if
1 ? k < nj , and rjk = 1 if k = nj .
4.1 Probabilistic Model for Comma Insertion
When an input sentence B is provided, our method
identifies the comma insertion R that maximizes
the conditional probability P (R|B). Assuming that
whether or not to insert a comma right after a bun-
setsu is independent of other commas except the
896
Table 4: Features used for the maximum entropy method
morphological the rightmost independent morpheme, i.e. head word, (part-of-speech and inflected form) and
information rightmost morpheme (part-of-speech) of a bunsetsu bjk
the rightmost morpheme (a surface form) of bjk if the rightmost morpheme is a particle
the first morpheme (part-of-speech) of bjk+1
commas inserted whether or not a clause boundary exists right after bjk
between clauses type of a clause boundary right after bjk if there exists a clause boundary
commas indicating whether or not bjk depends on the next bunsetsu
clear dependency whether or not bjk depends on a bunsetsu located after the final bunsetsu of the clause including
relations the next bunsetsu of bjk
whether or not bjk is depended on by the bunsetsu located right before it
whether or not the dependency structure of a sequence of bunsetsus between bjk and b
j
1 is closed
commas avoiding whether or not both the rightmost morpheme of bjk and first morpheme of b
j
k+1 are kanji
reading mistakes and characters
reading difficulty whether or not both the rightmost morpheme of bjk and first morpheme of b
j
k+1 are katakana
characters
commas indicating whether or not there exists a clause boundary ?topicalized element-wa? right after bjk and b
j
k
the subject depends on the next bunsetsu
whether or not there exists a clause boundary ?topicalized element-wa? right after bjk and the
string of characters right before bjk is ? ? (dewa)?
the number of characters in a phrase indicating the subject5 if there exists a clause boundary
?topicalized element-wa? right after bjk
whether or not a clause boundary ?topicalized element-wa? exists right after bjk and a bunsetsu
whose rightmost morpheme is a verb depends on the modified bunsetsu of bjk
commas inserted whether or not bjk appears at the beginning of a sentence and its rightmost morpheme is a
after a conjunction conjunction
or adverb at the be-
ginning of a sentence
whether or not bjk appears at the beginning of a sentence and its rightmost morpheme is an
adverb
commas inserted whether or not both the rightmost morphemes of bjk and b
j
k+1 are nouns
between parallel whether or not a predicate at the sentence end is depended on by bjk whose rightmost
words or phrases independent morpheme is a verb and by any of the bunsetsus which are located after bjk and of
which the rightmost independent morpheme is a verb
number of characters one of the following 4 categories if the number of characters from bj1 to bjk is found there
from bj1 to bjk ([num = 1], [2 ? num ? 3], [4 ? num ? 21], [22 ? num])
one appearing immediately before that bunsetsu,
P (R|B) can be calculated as follows:
P (R|B) (1)
=P (r11 = 0, ? ? ? , r
1
n1?1 = 0, r
1
n1 = 1, ? ? ? ,
rm1 = 0, ? ? ? , r
m
nm?1 = 0, r
m
nm = 1|B)
?=P (r11 = 0|B)? ? ? ?
?P (r1n1?1 = 0|r
1
n1?2 = 0, ? ? ? , r
1
1 = 0, B)
?P (r1n1 = 1|r
1
n1?1 = 0, ? ? ? , r
1
1 = 0, B)? ? ? ?
?P (rm1 = 0|r
m?1
nm?1 = 1, B)? ? ? ?
?P (rmnm?1 = 0|r
m
nm?2= 0,? ? ?, r
m
1 = 0, r
m?1
nm?1= 1, B)
?P (rmnm = 1|r
m
nm?1 = 0, ? ? ? , r
m
1 = 0, r
m?1
nm?1 = 1, B)
where P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 = 0, r
j?1
nj?1 =
1, B) is the probability that a comma is inserted right
after a bunsetsu bjk when the sequence of bunset-
sus B is provided and the position of j-th comma is
identified. Similarly, P (rjk = 0|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) is the probability that a comma
is not inserted right after a bunsetsu bjk. These
probabilities are estimated by the maximum entropy
method. The result R which maximizes the condi-
tional probability P (R|B) is regarded as the most
appropriate result of comma insertion, and calcu-
lated by dynamic programming.
897
4.2 Features on Maximum Entropy Method
To estimate P (rjk = 1|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) and P (r
j
k = 0|r
j
k?1 = 0, ? ? ? , r
j
1 =
0, rj?1nj?1 = 1, B) by the maximum entropy method,
we used the features in Table 4 based on the analysis
described in Section 3.
5 Experiment
To evaluate the effectiveness of our method, we con-
ducted an experiment using a Japanese text corpus.
5.1 Outline of Experiment
As the experimental data, we used the newspaper ar-
ticles in the Kyoto Text Corpus version 4.0 (Kuro-
hashi and Nagao, 1998). We used the articles from
January 14th to 17th as the test data. The training
data is same as the analysis data. Table 5 shows the
size of the test data. Here, we used the maximum
entropy method tool (Le, 2008) with the default op-
tions except ?-i 2000.?
In the evaluation, we obtained the recall, the pre-
cision and their harmonic mean, i.e., F-measure.
The recall and precision are respectively defined as
follows.
recall=
# of correctly inserted commas
# of commas in the correct data
precision=
# of correctly inserted commas
# of inserted commas
In our research, to realize automatic comma in-
sertion with high quality, we analyzed each usage of
commas and decided the features for the ME method
based on the analysis. To confirm the effectiveness
of our features, we established the baseline method
as a comparative method whereby commas are in-
serted by the ME method in which only simple mor-
phological information is used. The baseline method
uses the morphological information in Table 4 and
the information of the rightmost morpheme (a sur-
face form) of a bunsetsu as features.
5.2 Experimental Results
Table 6 shows the experimental results of the base-
line and our method. The recall and precision
were 69.13% and 84.13% respectively, and we con-
firmed that our method had higher performance than
Table 5: Size of test data
sentences 4,659
bunsetsus 46,511
characters 198,899
commas 6,549
characters per sentence 42.69
Table 6: Experimental results
recall precision F-measure
our 69.13% 84.13% 75.90
method (4,527/6,549) (4,527/5,381)
baseline 51.38% 70.90% 59.58
(3,365/6,549) (3,365/4,746)
the baseline method. The percentage of sentences
wherein all commas were correctly inserted was
55.81%.
Figure 4 shows the comparison between the re-
sults of our method and the baseline method. The
baseline method was not able to insert commas right
after the bunsetsu ???????? (are floated)? or
?? ? ?? (not decided)? but inserted com-
mas at unnatural positions such as between ? ??
(calling himself)? and ?????? (the vice com-
mander).? On the other hand, our method was able
to insert commas properly at such bunsetsu bound-
aries.
6 Discussion
6.1 Error Analysis
Among positions where commas existed in the test
data, there existed 2,022 positions where our method
did not insert commas. Among them, 862 were
clause boundaries, and the clause boundary ?topical-
ized element-wa? accounted for 53.36% (460/862)
of them. There were a lot of clause boundaries of
the type ?topicalized element-wa,? and the number
of commas inserted at such boundaries was large.
But, the rate of comma insertion itself was not very
high. We can say that the four features about ?topi-
calized element-wa? did not always work well. Ta-
5Phrases indicating the subject is a sequence of bunsetsus
consisting of bjk and all bunsetsus that are connected to b
j
k when
we trace their dependency relationship in modifier-to-modifyee
direction.
898
!"#$%&'()*+,-./0-123456789:;<0=>?@ABCD=EFGHIJKLMGNBOPQRSTUO=BV$WUXYGZ[\O]^MWT_`=abcdef
!"#$%#&'()*)+,+-&.)/&0/1%.2&3)*/4&3+$*5/.&(/3,/.+,2-&6/.(%50#
'7+8%5*-&.)/&9+2#,&#4&':%9# 3*.2&*5&;)*9+5/- <%5*# =+.#2+9+-&
.)/&4#,9/,&>+$#,&9*5*(./,&+50&.)/&7,*./,&6+*3)* ;+8+*2+ +,/&
4>#+./0&+(&.)/&3+50*0+./-)#7/?/,-&(*53/&.)/&4,+9/7#,8&#4&
.)/&1,/0*3+./0&1#>*.*3+>&1+,.2&*(&5#.&0/3*0/0- .)/&3##,0*5+.*#5&
9+8/(&>*..>/&)/+07+2@&A
!"#$%&'(!)*
!"#$%&'()*+,-./0-123456789:;<0>?@ABCD=EFGHIJKLMGNBOPQRSTUOBV$WUXYGZ[\O]^MWT_`abcdef
!"#$%#&'()*)+,+-&.)/&0/1%.2&3)*/4&3+$*5/.&(/3,/.+,2-&6/.(%50#
'7+8%5*-&.)/&9+2#,&#4&':%9# 3*.2&*5&;)*9+5/ <%5*# =+.#2+9+-&
.)/&4#,9/,&>+$#,&9*5*(./,&+50&.)/&7,*./,&6+*3)* ;+8+*2+ +,/&
4>#+./0&+(&.)/&3+50*0+./ )#7/?/,-&(*53/&.)/&4,+9/7#,8&#4&
.)/&1,/0*3+./0&1#>*.*3+>&1+,.2&*(&5#.&0/3*0/0 .)/&3##,0*5+.*#5&
9+8/(&>*..>/&)/+07+2@&A
+,-&./0&*
ghij$NkU/lm-Ono:&pUO=qrGlm-cstf
!B)*>/&.)/&?*3/&3#99+50/, 3+>>*5C&)*9(/>4&D+,3#(&+11/+,(&
*5&1%$>*3-&+5&+3.%+>&3#99+50/,&*(&%53/,.+*5@A
!"#$%&'(!)*
ghij$NkU=/lm-Ono:&pUO=qrGlm-cstf
!B)*>/&.)/&?*3/&3#99+50/,- 3+>>*5C&)*9(/>4&D+,3#(&+11/+,(&*5&
1%$>*3-&+5&+3.%+>&3#99+50/,&*(&%53/,.+*5@A
+,-&./0&*
Figure 4: Comparison of the results of our method and
baseline method
ble 7 shows the results of comma insertion at the
clause boundaries ?topicalized element-wa.? While
there existed 601 commas at such boundaries in the
test data, only 141 commas were inserted correctly.
We need to consider more effective features about
?topicalized element-wa.?
As for other cases, there existed 130 bunsetsu
boundaries between parallel words where commas
were not inserted. One example of such case is
shown below.
? correct data:
? ???? ?????????
????????????????????
? (Put pork backfat, garlic, ginger and shredded
green onion in a bowl, and add red bell peppers for
color.)
Table 7: Result of comma insertion at the clause bound-
aries ?topicalized element-wa.?
recall precision F-measure
23.46% 59.49% 33.65
(141/601) (141/237)
? our method:
??????????????????????
??????????????????????
(Put pork backfat garlic, ginger and shredded green
onion in a bowl, and add red bell peppers for color.)
In the correct data, a comma was inserted between
the bunsetsu ?? (backfat)? and ????? (gar-
lic).?
If a comma should be inserted right after the bun-
setsu ?? ? (backfat),? the number of characters be-
tween commas would become too small to be judged
as appropriate by the proposed method. So, the fea-
ture about the number of characters between com-
mas may have had harmful effects there. On the
other hand, a comma was inserted properly between
the bunsetsu ????? (garlic)? and ? ???
(ginger).? This is because katakana characters ap-
peared sequentially in addition to appearing as par-
allel nouns.
6.2 Unnatural Comma Insertion
When commas are inserted at obviously unnatural
positions, they have a major impact on the under-
standing of a sentence by readers. Here, we inves-
tigated how many commas had been inserted at ob-
viously unnatural positions by our method. For the
article on January 14th (217 sentences, 2,349 bun-
setsus) in the test data, we examined 47 commas in-
serted incorrectly. Three persons decided whether
or not the inserted commas were obviously unnat-
ural through consultations. Concretely, when all of
the three persons felt that an inserted comma would
make readers understand wrongly the meaning of
the sentence, the comma was judged to be obviously
unnatural.
Among 47 commas, 4 commas were judged obvi-
ously unnatural. This result shows that our method
is capable of inserting commas at natural positions
on some level.
899
Table 8: Comparison with human judgement
recall precision F-measure
by human 78.30% 80.58% 79.42
(249/318) (249/309)
our method 71.07% 82.78% 76.48
(226/318) (226/273)
6.3 Comparison with Human Judgement
In our experiment, we evaluated the results of
comma insertion of our method by comparing them
with the correct data. However, the sufficient level
to be reached by automatic comma insertion is un-
certain. Here, we evaluated our method by com-
paring them with the results of comma insertion by
another person. By using the same data as used in
the subsection 6.2, we conducted an experiment on
comma insertion by an annotator who was familiar
with writing Japanese documents. Table 8 shows the
recall, the precision and the F-measure. The second
row shows the results of our method for the same
data. As the F-measure of the annotator was 79.42,
it turned out that comma insertion task was diffi-
cult even for humans. For F-measure, our method
achieved 96.30% (76.48/79.42) of the annotator?s re-
sult. Also, the precision of our method was 82.78%.
Although the comma insertion task is difficult, our
method was able to properly insert commas.
7 Conclusion
This paper proposed a method for inserting commas
into Japanese texts. Our method appropriately in-
serts commas based on the machine learning method
using such features as morphemes, dependencies
and clause boundaries. An experiment by using the
Kyoto Text Corpus (Kurohashi and Nagao, 1998)
showed an F-measure of 75.90, and we confirmed
the effectiveness of our method.
The analysis of the experimental results showed
that our method cannot insert commas of the par-
ticular usage. As a future work, it is necessary to
find more useful features for commas of this usage
and improve the recall of our method. Also, we will
examine ?commas emphasizing the adjacent word?
which were not included in our targets.
Acknowledgments
This research was partially supported by the Grant-
in-Aids for Scientific Research (B) (No. 22300051)
and Young Scientists (B) (No. 21700157), and by
the Continuation Grants for Young Researchers of
The Asahi Glass Foundation.
References
Heidi Christensen, Yoshihiko Gotoh, and Steve Renals.
2001. Punctuation annotation using statistical prosody
models. In Proceedings of ISCA Workshop on Prosody
in Speech Recognition and Understanding, pages 35?
40.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2010. A linguistically inspired statistical model for
Chinese punctuation generation. ACM Transactions
on Asian Language Information Processing, 9(2):6:1?
6:27.
Yoshihiko Hayashi. 1992. A three-level revision model
for improving Japanese bad-styled expressions. In
Proceeding of 14th International Conference on Com-
putational Linguistics, pages 665?671.
Katsuichi Honda. 1982. Nihongo no sakubun gijutsu
(Japanese writing skill). Asahi Shimbun Publications
Inc. (In Japanese).
Takashi Inukai. 2002. Moji hyouki tankyuhou (Method of
questioning characters and notations). Asakura Pul-
ishing Co., Ltd. (In Japanese).
Hideki Kashioka and Takehiko Maruyama. 2004. Seg-
mentation of semantic unit in Japanese monologue.
In Proceedings of International Conference on Speech
Language Technology and Oriental COCOSDA, pages
87?92.
Ji-hwan Kim and P. C. Woodland. 2001. The use of
prosody in a combined system for punctuation gener-
ation and speech recognition. In Proceedings of 7th
European Conference on Speech Communication and
Technology, pages 2757?2760.
Sadao Kurohashi and Makoto Nagao. 1998. Building
a Japanese parsed corpus while improving the parsing
system. In Proceedings of 1st International Confer-
ence on Language Resources and Evaluation, pages
719?724.
Zhang Le. 2008. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.
ac.uk/s0450736/maxent toolkit.html.
[Online; accessed 1-March-2008].
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006. En-
riching speech recognition with automatic detection of
900
sentence boundaries and disfluencies. IEEE Trans-
actions on Audio, Speech, and Language Processing,
14(5):1526?1540.
Tohru Shimizu, Satoshi Nakamura, and Tatsuya Kawa-
hara. 2008. Effect of punctuation marks for speech
translation unit boundary detection. IEICE technical
report. Speech, 108(338):127?131. (In Japanese).
Shogakukan?s editiorial department. 2007. kutoten,
kigou, hugou katuyoujiten (dictionary of punctuations
and symbols ). Shogakukan. (In Japanese).
Eiji Suzuki, Shizuo Shimada, Kunio Kondo, and Hisashi
Sato. 1995. Automatic recognition of optimal punc-
tuation in Japanese documents. In Proceedings of
50th National Convention of IPSJ, 50(3):185?186. (In
Japanese).
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Proceed-
ings of Workshop on Grammar Engineering Across
Frameworks, pages 17?24.
901
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247?256,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Bayesian Method for Robust Estimation of Distributional Similarities
Jun?ichi Kazama Stijn De Saeger Kow Kuroda
Masaki Murata? Kentaro Torisawa
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan
{kazama, stijn, kuroda, torisawa}@nict.go.jp
?Department of Information and Knowledge Engineering
Faculty/Graduate School of Engineering, Tottori University
4-101 Koyama-Minami, Tottori, 680-8550 Japan?
murata@ike.tottori-u.ac.jp
Abstract
Existing word similarity measures are not
robust to data sparseness since they rely
only on the point estimation of words?
context profiles obtained from a limited
amount of data. This paper proposes a
Bayesian method for robust distributional
word similarities. The method uses a dis-
tribution of context profiles obtained by
Bayesian estimation and takes the expec-
tation of a base similarity measure under
that distribution. When the context pro-
files are multinomial distributions, the pri-
ors are Dirichlet, and the base measure is
the Bhattacharyya coefficient, we can de-
rive an analytical form that allows efficient
calculation. For the task of word similar-
ity estimation using a large amount of Web
data in Japanese, we show that the pro-
posed measure gives better accuracies than
other well-known similarity measures.
1 Introduction
The semantic similarity of words is a long-
standing topic in computational linguistics be-
cause it is theoretically intriguing and has many
applications in the field. Many researchers have
conducted studies based on the distributional hy-
pothesis (Harris, 1954), which states that words
that occur in the same contexts tend to have similar
meanings. A number of semantic similarity mea-
sures have been proposed based on this hypothesis
(Hindle, 1990; Grefenstette, 1994; Dagan et al,
1994; Dagan et al, 1995; Lin, 1998; Dagan et al,
1999).
?The work was done while the author was at NICT.
In general, most semantic similarity measures
have the following form:
sim(w1, w2) = g(v(w1), v(w2)), (1)
where v(wi) is a vector that represents the con-
texts in which wi appears, which we call a context
profile of wi. The function g is a function on these
context profiles that is expected to produce good
similarities. Each dimension of the vector corre-
sponds to a context, fk, which is typically a neigh-
boring word or a word having dependency rela-
tions with wi in a corpus. Its value, vk(wi), is typ-
ically a co-occurrence frequency c(wi, fk), a con-
ditional probability p(fk|wi), or point-wise mu-
tual information (PMI) between wi and fk, which
are all calculated from a corpus. For g, various
works have used the cosine, the Jaccard coeffi-
cient, or the Jensen-Shannon divergence is uti-
lized, to name only a few measures.
Previous studies have focused on how to de-
vise good contexts and a good function g for se-
mantic similarities. On the other hand, our ap-
proach in this paper is to estimate context profiles
(v(wi)) robustly and thus to estimate the similarity
robustly. The problem here is that v(wi) is com-
puted from a corpus of limited size, and thus in-
evitably contains uncertainty and sparseness. The
guiding intuition behind our method is as follows.
All other things being equal, the similarity with
a more frequent word should be larger, since it
would be more reliable. For example, if p(fk|w1)
and p(fk|w2) for two given words w1 and w2 are
equal, but w1 is more frequent, we would expect
that sim(w0, w1) > sim(w0, w2).
In the NLP field, data sparseness has been rec-
ognized as a serious problem and tackled in the
context of language modeling and supervised ma-
chine learning. However, to our knowledge, there
247
has been no study that seriously dealt with data
sparseness in the context of semantic similarity
calculation. The data sparseness problem is usu-
ally solved by smoothing, regularization, margin
maximization and so on (Chen and Goodman,
1998; Chen and Rosenfeld, 2000; Cortes and Vap-
nik, 1995). Recently, the Bayesian approach has
emerged and achieved promising results with a
clearer formulation (Teh, 2006; Mochihashi et al,
2009).
In this paper, we apply the Bayesian framework
to the calculation of distributional similarity. The
method is straightforward: Instead of using the
point estimation of v(wi), we first estimate the
distribution of the context profile, p(v(wi)), by
Bayesian estimation and then take the expectation
of the original similarity under this distribution as
follows:
simb(w1, w2) (2)
= E[sim(w1, w2)]{p(v(w1)),p(v(w2))}
= E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}.
The uncertainty due to data sparseness is repre-
sented by p(v(wi)), and taking the expectation en-
ables us to take this into account. The Bayesian
estimation usually gives diverging distributions for
infrequent observations and thus decreases the ex-
pectation value as expected.
The Bayesian estimation and the expectation
calculation in Eq. 2 are generally difficult and
usually require computationally expensive proce-
dures. Since our motivation for this research is to
calculate good semantic similarities for a large set
of words (e.g., one million nouns) and apply them
to a wide range of NLP tasks, such costs must be
minimized.
Our technical contribution in this paper is to
show that in the case where the context profiles are
multinomial distributions, the priors are Dirich-
let, and the base similarity measure is the Bhat-
tacharyya coefficient (Bhattacharyya, 1943), we
can derive an analytical form for Eq. 2, that en-
ables efficient calculation (with some implemen-
tation tricks).
In experiments, we estimate semantic similari-
ties using a large amount of Web data in Japanese
and show that the proposed measure gives bet-
ter word similarities than a non-Bayesian Bhat-
tacharyya coefficient or other well-known similar-
ity measures such as Jensen-Shannon divergence
and the cosine with PMI weights.
The rest of the paper is organized as follows. In
Section 2, we briefly introduce the Bayesian esti-
mation and the Bhattacharyya coefficient. Section
3 proposes our new Bayesian Bhattacharyya coef-
ficient for robust similarity calculation. Section 4
mentions some implementation issues and the so-
lutions. Then, Section 5 reports the experimental
results.
2 Background
2.1 Bayesian estimation with Dirichlet prior
Assume that we estimate a probabilistic model for
the observed data D, p(D|?), which is parame-
terized with parameters ?. In the maximum like-
lihood estimation (MLE), we find the point esti-
mation ?? = argmax?p(D|?). For example, we
estimate p(fk|wi) as follows with MLE:
p(fk|wi) = c(wi, fk)/
X
k
c(wi, fk). (3)
On the other hand, the objective of the Bayesian
estimation is to find the distribution of ? given
the observed data D, i.e., p(?|D), and use it in
later processes. Using Bayes? rule, this can also
be viewed as:
p(?|D) = p(D|?)pprior(?)p(D) . (4)
pprior(?) is a prior distribution that represents the
plausibility of each ? based on the prior knowl-
edge. In this paper, we consider the case where
? is a multinomial distribution, i.e.,
?
k ?k = 1,
that models the process of choosing one out of K
choices. Estimating a conditional probability dis-
tribution ?k = p(fk|wi) as a context profile for
each wi falls into this case. In this paper, we also
assume that the prior is the Dirichlet distribution,
Dir(?). The Dirichlet distribution is defined as
follows.
Dir(?|?) =
?(
PK
k=1 ?k)
QK
k=1 ?(?k)
K
Y
k=1
??k?1k . (5)
?(.) is the Gamma function. The Dirichlet distri-
bution is parametrized by hyperparameters ?k(>
0).
It is known that p(?|D) is also a Dirichlet dis-
tribution for this simplest case, and it can be ana-
lytically calculated as follows.
p(?|D) = Dir(?|{?k + c(k)}), (6)
where c(k) is the frequency of choice k in data D.
For example, c(k) = c(wi, fk) in the estimation
of p(fk|wi). This is very simple: we just need to
add the observed counts to the hyperparameters.
248
2.2 Bhattacharyya coefficient
When the context profiles are probability distribu-
tions, we usually utilize the measures on probabil-
ity distributions such as the Jensen-Shannon (JS)
divergence to calculate similarities (Dagan et al,
1994; Dagan et al, 1997). The JS divergence is
defined as follows.
JS(p1||p2) =
1
2(KL(p1||pavg) + KL(p2||pavg)),
where pavg = p1+p22 is a point-wise average of p1
and p2 and KL(.) is the Kullback-Leibler diver-
gence. Although we found that the JS divergence
is a good measure, it is difficult to derive an ef-
ficient calculation of Eq. 2, even in the Dirichlet
prior case.1
In this study, we employ the Bhattacharyya co-
efficient (Bhattacharyya, 1943) (BC for short),
which is defined as follows:
BC(p1, p2) =
K
X
k=1
?
p1k ? p2k.
The BC is also a similarity measure on probabil-
ity distributions and is suitable for our purposes as
we describe in the next section. Although BC has
not been explored well in the literature on distribu-
tional word similarities, it is also a good similarity
measure as the experiments show.
3 Method
In this section, we show that if our base similarity
measure is BC and the distributions under which
we take the expectation are Dirichlet distributions,
then Eq. 2 also has an analytical form, allowing
efficient calculation.
Here, we calculate the following value given
two Dirichlet distributions:
BCb(p1, p2) = E[BC(p1, p2)]{Dir(p1|?? ),Dir(p2|?? )}
=
ZZ
???
Dir(p1|?
?
)Dir(p2|?
?
)BC(p1, p2)dp1dp2.
After several derivation steps (see Appendix A),
we obtain the following analytical solution for the
above:
1A naive but general way might be to draw samples of
v(wi) from p(v(wi)) and approximate the expectation using
these samples. However, such a method will be slow.
= ?(?
?
0)?(?
?
0)
?(??0 + 12 )?(?
?
0 + 12 )
K
X
k=1
?(?
?
k + 12 )?(?
?
k + 12 )
?(??k)?(?
?
k)
, (7)
where ??0 =
?
k ?
?
k and ?
?
0 =
?
k ?
?
k. Note that
with the Dirichlet prior, ??k = ?k + c(w1, fk) and
??k = ?k + c(w2, fk), where ?k and ?k are the
hyperparameters of the priors of w1 and w2, re-
spectively.
To put it all together, we can obtain a new
Bayesian similarity measure on words, which can
be calculated only from the hyperparameters for
the Dirichlet prior, ? and ?, and the observed
counts c(wi, fk). It is written as follows.
BCb(w1, w2) = (8)
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + 12 )?(?0 + b0 +
1
2 )
?
K
X
k=1
?(?k + c(w1, fk) + 12 )?(?k + c(w2, fk) +
1
2 )
?(?k + c(w1, fk))?(?k + c(w2, fk))
,
where a0 =
?
k c(w1, fk) and b0 =
?
k c(w2, fk). We call this new measure the
Bayesian Bhattacharyya coefficient (BCb for
short). For simplicity, we assume ?k = ?k = ? in
this paper.
We can see that BCb actually encodes our guid-
ing intuition. Consider four words, w0, w1, w2,
and w4, for which we have c(w0, f1) = 10,
c(w1, f1) = 2, c(w2, f1) = 10, and c(w3, f1) =
20. They have counts only for the first dimen-
sion, i.e., they have the same context profile:
p(f1|wi) = 1.0, when we employ MLE. When
K = 10, 000 and ?k = 1.0, the Bayesian similar-
ity between these words is calculated as
BCb(w0, w1) = 0.785368
BCb(w0, w2) = 0.785421
BCb(w0, w3) = 0.785463
We can see that similarities are different ac-
cording to the number of observations, as ex-
pected. Note that the non-Bayesian BC will re-
turn the same value, 1.0, for all cases. Note
also that BCb(w0, w0) = 0.78542 if we use Eq.
8, meaning that the self-similarity might not be
the maximum. This is conceptually strange, al-
though not a serious problem since we hardly use
sim(wi, wi) in practice. If we want to fix this,
we can use the special definition: BCb(wi, wi) ?
1. This is equivalent to using simb(wi, wi) =
E[sim(wi, wi)]{p(v(wi))} = 1 only for this case.
249
4 Implementation Issues
Although we have derived the analytical form
(Eq. 8), there are several problems in implement-
ing robust and efficient calculations.
First, the Gamma function in Eq. 8 overflows
when the argument is larger than 170. In such
cases, a commonly used way is to work in the log-
arithmic space. In this study, we utilize the ?log
Gamma? function: ln?(x), which returns the log-
arithm of the Gamma function directly without the
overflow problem.2
Second, the calculation of the log Gamma func-
tion is heavier than operations such as simple mul-
tiplication, which is used in existing measures.
In fact, the log Gamma function is implemented
using an iterative algorithm such as the Lanczos
method. In addition, according to Eq. 8, it seems
that we have to sum up the values for all k, be-
cause even if c(wi, fk) is zero the value inside the
summation will not be zero. In the existing mea-
sures, it is often the case that we only need to sum
up for k where c(wi, fk) > 0. Because c(wi, fk)
is usually sparse, that technique speeds up the cal-
culation of the existing measures drastically and
makes it practical.
In this study, the above problem is solved by
pre-computing the required log Gamma values, as-
suming that we calculate similarities for a large
set of words, and pre-computing default values for
cases where c(wi, fk) = 0. The following values
are pre-computed once at the start-up time.
For each word:
(A) ln?(?0 + a0) ? ln?(?0 + a0 + 12)
(B) ln?(?k+c(wi, fk))?ln?(?k+c(wi, fk)+ 12)
for all k where c(wi, fk) > 0
(C) ? exp(2(ln?(?k + 12) ? ln?(?k)))) +
exp(ln?(?k + c(wi, fk)) ? ln?(?k +
c(wi, fk) + 12) + ln?(?k +
1
2) ? ln?(?k))
for all k where c(wi, fk) > 0;
For each k:
(D): exp(2(ln?(?k + 12)).
In the calculation of BCb(w1, w2), we first as-
sume that all c(wi, fk) = 0 and set the output
variable to the default value. Then, we iterate
over the sparse vectors c(w1, fk) and c(w2, fk). If
2We used the GNU Scientific Library (GSL)
(www.gnu.org/software/gsl/), which implements this
function.
c(w1, fk) > 0 and c(w2, fk) = 0 (and vice versa),
we update the output variable just by adding (C).
If c(w1, fk) > 0 and c(w2, fk) > 0, we update
the output value using (B), (D) and one additional
exp(.) operation. With this implementation, we
can make the computation of BCb practically as
fast as using other measures.
5 Experiments
5.1 Evaluation setting
We evaluated our method in the calculation of sim-
ilarities between nouns in Japanese.
Because human evaluation of word similari-
ties is very difficult and costly, we conducted au-
tomatic evaluation in the set expansion setting,
following previous studies such as Pantel et al
(2009).
Given a word set, which is expected to con-
tain similar words, we assume that a good simi-
larity measure should output, for each word in the
set, the other words in the set as similar words.
For given word sets, we can construct input-and-
answers pairs, where the answers for each word
are the other words in the set the word appears in.
We output a ranked list of 500 similar words
for each word using a given similarity measure
and checked whether they are included in the an-
swers. This setting could be seen as document re-
trieval, and we can use an evaluation measure such
as the mean of the precision at top T (MP@T ) or
the mean average precision (MAP). For each input
word, P@T (precision at top T ) and AP (average
precision) are defined as follows.
P@T = 1T
T
X
i=1
?(wi ? ans),
AP = 1R
N
X
i=1
?(wi ? ans)P@i.
?(wi ? ans) returns 1 if the output word wi is
in the answers, and 0 otherwise. N is the number
of outputs and R is the number of the answers.
MP@T and MAP are the averages of these values
over all input words.
5.2 Collecting context profiles
Dependency relations are used as context profiles
as in Kazama and Torisawa (2008) and Kazama et
al. (2009). From a large corpus of Japanese Web
documents (Shinzato et al, 2008) (100 million
250
documents), where each sentence has a depen-
dency parse, we extracted noun-verb and noun-
noun dependencies with relation types and then
calculated their frequencies in the corpus. If a
noun, n, depends on a word, w, with a relation,
r, we collect a dependency pair, (n, ?w, r?). That
is, a context fk, is ?w, r? here.
For noun-verb dependencies, postpositions
in Japanese represent relation types. For
example, we extract a dependency relation
(???, ? ??,? ?) from the sentence below,
where a postposition ?? (wo)? is used to mark
the verb object.
??? (wine)? (wo)?? (buy) (? buy a wine)
Note that we leave various auxiliary verb suf-
fixes, such as ??? (reru),? which is for passiviza-
tion, as a part of w, since these greatly change the
type of n in the dependent position.
As for noun-noun dependencies, we considered
expressions of type ?n1 ? n2? (? ?n2 of n1?) as
dependencies (n1, ?n2,? ?).
We extracted about 470 million unique depen-
dencies from the corpus, containing 31 million
unique nouns (including compound nouns as de-
termined by our filters) and 22 million unique con-
texts, fk. We sorted the nouns according to the
number of unique co-occurring contexts and the
contexts according to the number of unique co-
occurring nouns, and then we selected the top one
million nouns and 100,000 contexts. We used only
260 million dependency pairs that contained both
the selected nouns and the selected contexts.
5.3 Test sets
We prepared three test sets as follows.
Set ?A? and ?B?: Thesaurus siblings We
considered that words having a common
hypernym (i.e., siblings) in a manually
constructed thesaurus could constitute a
similar word set. We extracted such sets
from a Japanese dictionary, EDR (V3.0)
(CRL, 2002), which contains concept hier-
archies and the mapping between words and
concepts. The dictionary contains 304,884
nouns. In all, 6,703 noun sibling sets were
extracted with the average set size of 45.96.
We randomly chose 200 sets each for sets
?A? and ?B.? Set ?A? is a development set to
tune the value of the hyperparameters and
?B? is for the validation of the parameter
tuning.
Set ?C?: Closed sets Murata et al (2004) con-
structed a dataset that contains several closed
word sets such as the names of countries,
rivers, sumo wrestlers, etc. We used all of
the 45 sets that are marked as ?complete? in
the data, containing 12,827 unique words in
total.
Note that we do not deal with ambiguities in the
construction of these sets as well as in the calcu-
lation of similarities. That is, a word can be con-
tained in several sets, and the answers for such a
word is the union of the words in the sets it belongs
to (excluding the word itself).
In addition, note that the words in these test sets
are different from those of our one-million-word
vocabulary. We filtered out the words that are not
included in our vocabulary and removed the sets
with size less than 2 after the filtering.
Set ?A? contained 3,740 words that are actually
evaluated, with about 115 answers on average, and
?B? contained 3,657 words with about 65 answers
on average. Set ?C? contained 8,853 words with
about 1,700 answers on average.
5.4 Compared similarity measures
We compared our Bayesian Bhattacharyya simi-
larity measure, BCb, with the following similarity
measures.
JS Jensen-Shannon divergence between p(fk|w1)
and p(fk|w2) (Dagan et al, 1994; Dagan et
al., 1999).
PMI-cos The cosine of the context profile vec-
tors, where the k-th dimension is the point-
wise mutual information (PMI) between
wi and fk defined as: PMI(wi, fk) =
log p(wi,fk)p(wi)p(fk) (Pantel and Lin, 2002; Pantel
et al, 2009).3
Cls-JS Kazama et al (2009) proposed using
the Jensen-Shannon divergence between hid-
den class distributions, p(c|w1) and p(c|w2),
which are obtained by using an EM-based
clustering of dependency relations with a
model p(wi, fk) =
?
c p(wi|c)p(fk|c)p(c)
(Kazama and Torisawa, 2008). In order to
3We did not use the discounting of the PMI values de-
scribed in Pantel and Lin (2002).
251
alleviate the effect of local minima of the EM
clustering, they proposed averaging the simi-
larities by several different clustering results,
which can be obtained by using different ini-
tial parameters. In this study, we combined
two clustering results (denoted as ?s1+s2? in
the results), each of which (?s1? and ?s2?)
has 2,000 hidden classes.4 We included this
method since clustering can be regarded as
another way of treating data sparseness.
BC The Bhattacharyya coefficient (Bhat-
tacharyya, 1943) between p(fk|w1) and
p(fk|w2). This is the baseline for BCb.
BCa The Bhattacharyya coefficient with absolute
discounting. In calculating p(fk|wi), we sub-
tract the discounting value, ?, from c(wi, fk)
and equally distribute the residual probabil-
ity mass to the contexts whose frequency is
zero. This is included as an example of naive
smoothing methods.
Since it is very costly to calculate the sim-
ilarities with all of the other words (one mil-
lion in our case), we used the following approx-
imation method that exploits the sparseness of
c(wi, fk). Similar methods were used in Pantel
and Lin (2002), Kazama et al (2009), and Pan-
tel et al (2009) as well. For a given word, wi,
we sort the contexts in descending order accord-
ing to c(wi, fk) and retrieve the top-L contexts.5
For each selected context, we sort the words in de-
scending order according to c(wi, fk) and retrieve
the top-M words (L = M = 1600).6 We merge
all of the words above as candidate words and cal-
culate the similarity only for the candidate words.
Finally, the top 500 similar words are output.
Note also that we used modified counts,
log(c(wi, fk)) + 1, instead of raw counts,
c(wi, fk), with the intention of alleviating the ef-
fect of strangely frequent dependencies, which can
be found in the Web data. In preliminary ex-
periments, we observed that this modification im-
proves the quality of the top 500 similar words as
reported in Terada et al (2004) and Kazama et al
(2009).
4In the case of EM clustering, the number of unique con-
texts, fk, was also set to one million instead of 100,000, fol-
lowing Kazama et al (2009).
5It is possible that the number of contexts with non-zero
counts is less than L. In that case, all of the contexts with
non-zero counts are used.
6Sorting is performed only once in the initialization step.
Table 1: Performance on siblings (Set A).
Measure MAP MP
@1 @5 @10 @20
JS 0.0299 0.197 0.122 0.0990 0.0792
PMI-cos 0.0332 0.195 0.124 0.0993 0.0798
Cls-JS (s1) 0.0319 0.195 0.122 0.0988 0.0796
Cls-JS (s2) 0.0295 0.198 0.122 0.0981 0.0786
Cls-JS (s1+s2) 0.0333 0.206 0.129 0.103 0.0841
BC 0.0334 0.211 0.131 0.106 0.0854
BCb (0.0002) 0.0345 0.223 0.138 0.109 0.0873
BCb (0.0016) 0.0356 0.242 0.148 0.119 0.0955
BCb (0.0032) 0.0325 0.223 0.137 0.111 0.0895
BCa (0.0016) 0.0337 0.212 0.133 0.107 0.0863
BCa (0.0362) 0.0345 0.221 0.136 0.110 0.0890
BCa (0.1) 0.0324 0.214 0.128 0.101 0.0825
without log(c(wi, fk)) + 1 modification
JS 0.0294 0.197 0.116 0.0912 0.0712
PMI-cos 0.0342 0.197 0.125 0.0987 0.0793
BC 0.0296 0.201 0.118 0.0915 0.0721
As for BCb, we assumed that all of the hyper-
parameters had the same value, i.e., ?k = ?. It
is apparent that an excessively large ? is not ap-
propriate because it means ignoring observations.
Therefore, ?must be tuned. The discounting value
of BCa is also tuned.
5.5 Results
Table 1 shows the results for Set A. The MAP and
the MPs at the top 1, 5, 10, and 20 are shown for
each similarity measure. As for BCb and BCa, the
results for the tuned and several other values for ?
are shown. Figure 1 shows the parameter tuning
for BCb with MAP as the y-axis (results for BCa
are shown as well). Figure 2 shows the same re-
sults with MPs as the y-axis. The MAP and MPs
showed a correlation here. From these results, we
can see that BCb surely improves upon BC, with
6.6% improvement in MAP and 14.7% improve-
ment in MP@1 when ? = 0.0016. BCb achieved
the best performance among the compared mea-
sures with this setting. The absolute discounting,
BCa, improved upon BC as well, but the improve-
ment was smaller than with BCb. Table 1 also
shows the results for the case where we did not
use the log-modified counts. We can see that this
modification gives improvements (though slight or
unclear for PMI-cos).
Because tuning hyperparameters involves the
possibility of overfitting, its robustness should be
assessed. We checked whether the tuned ? with
Set A works well for Set B. The results are shown
in Table 2. We can see that the best ? (= 0.0016)
found for Set A works well for Set B as well. That
is, the tuning of ? as above is not unrealistic in
252
 0.02
 0.022
 0.024
 0.026
 0.028
 0.03
 0.032
 0.034
 0.036
 1e-06  1e-05  0.0001  0.001  0.01  0.1  1
MA
P
? (log-scale)
BayesAbsolute Discounting
Figure 1: Tuning of ? (MAP). The dashed hori-
zontal line indicates the score of BC.
 0.04 0.06
 0.08 0.1
 0.12 0.14
 0.16 0.18
 0.2 0.22
 0.24 0.26
 1e-06  1e-05  0.0001  0.001  0.01
MP
? (log-scale)
 MP@1
 MP@5
 MP@10
 MP@20
 MP@30
 MP@40
Figure 2: Tuning of ? (MP).
practice because it seems that we can tune it ro-
bustly using a small subset of the vocabulary as
shown by this experiment.
Next, we evaluated the measures on Set C, i.e.,
the closed set data. The results are shown in Ta-
ble 3. For this set, we observed a tendency that
is different from Sets A and B. Cls-JS showed a
particularly good performance. BCb surely im-
proves upon BC. For example, the improvement
was 7.5% for MP@1. However, the improvement
in MAP was slight, and MAP did not correlate
well with MPs, unlike in the case of Sets A and
B.
We thought one possible reason is that the num-
ber of outputs, 500, for each word was not large
enough to assess MAP values correctly because
the average number of answers is 1,700 for this
dataset. In fact, we could output more than 500
words if we ignored the cost of storage. Therefore,
we also included the results for the case where
L = M = 3600 and N = 2, 000. Even with
this setting, however, MAP did not correlate well
with MPs.
Although Cls-JS showed very good perfor-
mance for Set C, note that the EM clustering
is very time-consuming (Kazama and Torisawa,
2008), and it took about one week with 24 CPU
cores to get one clustering result in our computing
environment. On the other hand, the preparation
Table 2: Performance on siblings (Set B).
Measure MAP MP
@1 @5 @10 @20
JS 0.0265 0.208 0.116 0.0855 0.0627
PMI-cos 0.0283 0.203 0.116 0.0871 0.0660
Cls-JS (s1+s2) 0.0274 0.194 0.115 0.0859 0.0643
BC 0.0295 0.223 0.124 0.0922 0.0693
BCb (0.0002) 0.0301 0.225 0.128 0.0958 0.0718
BCb (0.0016) 0.0313 0.246 0.135 0.103 0.0758
BCb (0.0032) 0.0279 0.228 0.127 0.0938 0.0698
BCa (0.0016) 0.0297 0.223 0.125 0.0934 0.0700
BCa (0.0362) 0.0298 0.223 0.125 0.0934 0.0705
BCa (0.01) 0.0300 0.224 0.126 0.0949 0.0710
Table 3: Performance on closed-sets (Set C).
Measure MAP MP
@1 @5 @10 @20
JS 0.127 0.607 0.582 0.566 0.544
PMI-cos 0.124 0.531 0.519 0.508 0.493
Cls-JS (s1) 0.125 0.589 0.566 0.548 0.525
Cls-JS (s2) 0.137 0.608 0.592 0.576 0.554
Cls-JS (s1+s2) 0.152 0.638 0.617 0.603 0.583
BC 0.131 0.602 0.579 0.565 0.545
BCb (0.0004) 0.133 0.636 0.605 0.587 0.563
BCb (0.0008) 0.131 0.647 0.615 0.594 0.568
BCb (0.0016) 0.126 0.644 0.615 0.593 0.564
BCb (0.0032) 0.107 0.573 0.556 0.529 0.496
L = M = 3200 and N = 2000
JS 0.165 0.605 0.580 0.564 0.543
PMI-cos 0.165 0.530 0.517 0.507 0.492
Cls-JS (s1+s2) 0.209 0.639 0.618 0.603 0.584
BC 0.168 0.600 0.577 0.562 0.542
BCb (0.0004) 0.170 0.635 0.604 0.586 0.562
BCb (0.0008) 0.168 0.647 0.615 0.594 0.568
BCb (0.0016) 0.161 0.644 0.615 0.593 0.564
BCb (0.0032) 0.140 0.573 0.556 0.529 0.496
for our method requires just an hour with a single
core.
6 Discussion
We should note that the improvement by using our
method is just ?on average,? as in many other NLP
tasks, and observing clear qualitative change is rel-
atively difficult, for example, by just showing ex-
amples of similar word lists here. Comparing the
results of BCb and BC, Table 4 lists the numbers
of improved, unchanged, and degraded words in
terms of MP@20 for each evaluation set. As can
be seen, there are a number of degraded words, al-
though they are fewer than the improved words.
Next, Figure 3 shows the averaged differences of
MP@20 in each 40,000 word-ID range.7 We can
observe that the advantage of BCb is lessened es-
7Word IDs are assigned in ascending order when we chose
the top one million words as described in Section 5.2, and
they roughly correlate with frequencies. So, frequent words
tend to have low-IDs.
253
Table 4: The numbers of improved, unchanged,
and degraded words in terms of MP@20 for each
evaluation set.
# improved # unchanged # degraded
Set A 755 2,585 400
Set B 643 2,610 404
Set C 3,153 3,962 1,738
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0
 0.01 0.02
 0.03 0.04
 0.05 0.06
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
-0.01 0 0.01
 0.02 0.03 0.04
 0.05 0.06 0.07
 0.08
 0  500000  1e+06Avg
. Diff. 
of MP
@20
ID range
Figure 3: Averaged Differences of MP@20 be-
tween BCb (0.0016) and BC within each 40,000
ID range (Left: Set A. Right: Set B. Bottom: Set
C).
pecially for low-ID words (as expected) with on-
average degradation.8 The improvement is ?on av-
erage? in this sense as well.
One might suspect that the answer words tended
to be low-ID words, and the proposed method is
simply biased towards low-ID words because of
its nature. Then, the observed improvement is a
trivial consequence. Table 5 lists some interest-
ing statistics about the IDs. We can see that BCb
surely outputs more low-ID words than BC, and
BC more than Cls-JS and JS.9 However, the av-
erage ID of the outputs of BC is already lower
than the average ID of the answer words. There-
fore, even if BCb preferred lower-ID words than
BC, it should not have the effect of improving
the accuracy. That is, the improvement by BCb
is not superficial. From BC/BCb, we can also see
that the IDs of the correct outputs did not become
smaller compared to the IDs of the system outputs.
Clearly, we need more analysis on what caused
the improvement by the proposed method and how
that affects the efficacy in real applications of sim-
ilarity measures.
The proposed Bayesian similarity measure out-
performed the baseline Bhattacharyya coefficient
8This suggests the use of different ?s depending on ID
ranges (e.g., smaller ? for low-ID words) in practice.
9The outputs of Cls-JS are well-balanced in the ID space.
Table 5: Statistics on IDs. (A): Avg. ID of an-
swers. (B): Avg. ID of system outputs. (C): Avg.
ID of correct system outputs.
Set A Set C
(A) 238,483 255,248
(B) (C) (B) (C)
Cls-JS (s1+s2) 282,098 176,706 273,768 232,796
JS 183,054 11,3442 211,671 201,214
BC 162,758 98,433 193,508 189,345
BCb(0.0016) 55,915 54,786 90,472 127,877
BC/BCb 2.91 1.80 2.14 1.48
and other well-known similarity measures. As
a smoothing method, it also outperformed a
naive absolute discounting. Of course, we can-
not say that the proposed method is better than
any other sophisticated smoothing method at this
point. However, as noted above, there has
been no serious attempt to assess the effect of
smoothing in the context of word similarity cal-
culation. Recent studies have pointed out that
the Bayesian framework derives state-of-the-art
smoothing methods such as Kneser-Ney smooth-
ing as a special case (Teh, 2006; Mochihashi et
al., 2009). Consequently, it is reasonable to re-
sort to the Bayesian framework. Conceptually,
our method is equivalent to modifying p(fk|wi)
as p(fk|wi) =
{
?(?0+a0)?(?k+c(wi,fk)+ 12 )
?(?0+a0+ 12 )?(?k+c(wi,fk))
}2
and
taking the Bhattacharyya coefficient. However,
the implication of this form has not yet been in-
vestigated, and so we leave it for future research.
Our method is the simplest one as a Bayesian
method. We did not employ any numerical opti-
mization or sampling iterations, as in a more com-
plete use of the Bayesian framework (Teh, 2006;
Mochihashi et al, 2009). Instead, we used the ob-
tained analytical form directly with the assump-
tion that ?k = ? and ? can be tuned directly by
using a simple grid search with a small subset of
the vocabulary as the development set. If substan-
tial additional costs are allowed, we can fine-tune
each ?k using more complete Bayesian methods.
We also leave this for future research.
In terms of calculation procedure, BCb has the
same form as other similarity measures, which is
basically the same as the inner product of sparse
vectors. Thus, it can be as fast as other similar-
ity measures with some effort as we described in
Section 4 when our aim is to calculate similarities
between words in a fixed large vocabulary. For ex-
ample, BCb took about 100 hours to calculate the
254
top 500 similar nouns for all of the one million
nouns (using 16 CPU cores), while JS took about
57 hours. We think this is an acceptable additional
cost.
The limitation of our method is that it can-
not be used efficiently with similarity measures
other than the Bhattacharyya coefficient, although
that choice seems good as shown in the experi-
ments. For example, it seems difficult to use the
Jensen-Shannon divergence as the base similar-
ity because the analytical form cannot be derived.
One way we are considering to give more flexi-
bility to our method is to adjust ?k depending on
external knowledge such as the importance of a
context (e.g., PMIs). In another direction, we will
be able to use a ?weighted? Bhattacharyya coeffi-
cient:
?
k ?(w1, fk)?(w2, fk)
?
p1k ? p2k, where
the weights, ?(wi, fk), do not depend on pik, as
the base similarity measure. The analytical form
for it will be a weighted version of BCb.
BCb can also be generalized to the case where
the base similarity is BCd(p1, p2) =
?K
k=1 pd1k ?
pd2k, where d > 0. The Bayesian analytical form
becomes as follows.
BCdb (w1, w2) =
?(?0 + a0)?(?0 + b0)
?(?0 + a0 + d)?(?0 + b0 + d)
?
K
X
k=1
?(?k + c(w1, fk) + d)?(?k + c(w2, fk) + d)
?(?k + c(w1, fk))?(?k + c(w2, fk))
.
See Appendix A for the derivation. However, we
restricted ourselves to the case of d = 12 in this
study.
Finally, note that our BCb is different from
the Bhattacharyya distance measure on Dirichlet
distributions of the following form described in
Rauber et al (2008) in its motivation and analyti-
cal form:
p
?(??0)?(?
?
0)
q
Q
k ?(?
?
k)
q
Q
k ?(?
?
k)
?
Q
k ?((?
?
k + ?
?
k)/2)
?( 12
PK
k (?
?
k + ?
?
k))
. (9)
Empirical and theoretical comparisons with this
measure also form one of the future directions.10
7 Conclusion
We proposed a Bayesian method for robust distri-
butional word similarities. Our method uses a dis-
tribution of context profiles obtained by Bayesian
10Our preliminary experiments show that calculating sim-
ilarity using Eq. 9 for the Dirichlet distributions obtained by
Eq. 6 does not produce meaningful similarity (i.e., the accu-
racy is very low).
estimation and takes the expectation of a base sim-
ilarity measure under that distribution. We showed
that, in the case where the context profiles are
multinomial distributions, the priors are Dirichlet,
and the base measure is the Bhattacharyya coeffi-
cient, we can derive an analytical form, permitting
efficient calculation. Experimental results show
that the proposed measure gives better word simi-
larities than a non-Bayesian Bhattacharyya coeffi-
cient, other well-known similarity measures such
as Jensen-Shannon divergence and the cosine with
PMI weights, and the Bhattacharyya coefficient
with absolute discounting.
Appendix A
Here, we give the analytical form for the general-
ized case (BCdb ) in Section 6. Recall the following
relation, which is used to derive the normalization
factor of the Dirichlet distribution:
Z
?
Y
k
??
?
k?1
k d? =
Q
k ?(?
?
k)
?(??0)
= Z(?
?
)?1. (10)
Then, BCdb (w1, w2)
=
ZZ
???
Dir(?1|?
?
)Dir(?2|?
?
)
X
k
?d1k?d2k d?1 d?2
= Z(?
?
)Z(?
?
)?
ZZ
???
Y
l
??
?
l?1
1l
Y
m
??
?
m?1
2m
X
k
?d1k?d2k d?1 d?2
| {z }
A
.
Using Eq. 10, A in the above can be calculated as
follows:
=
Z
?
Y
m
??
?
m?1
2m
2
4
X
k
?d2k
Z
?
??
?
k+d?1
1k
Y
l?=k
??
?
l?1
1l d?1
3
5 d?2
=
Z
?
Y
m
??
?
m?1
2m
"
X
k
?d2k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
#
d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
Z
?
??
?
k+d?1
2k
Y
m ?=k
??
?
m?1
2m d?2
=
X
k
?(?
?
k + d)
Q
l?=k ?(?
?
l)
?(??0 + d)
?(?
?
k + d)
Q
m?=k ?(?
?
m)
?(??0 + d)
=
Q
?(?
?
l)
Q
?(?
?
m)
?(??0 + d)?(?
?
0 + d)
X
k
?(?
?
k + d)
?(??k)
?(?
?
k + d)
?(??k)
.
This will give:
BCdb (w1, w2) =
?(?
?
0)?(?
?
0)
?(??0 + d)?(?
?
0 + d)
K
X
k=1
?(?
?
k + d)?(?
?
k + d)
?(??k)?(?
?
k)
.
255
References
A. Bhattacharyya. 1943. On a measure of divergence
between two statistical populations defined by their
probability distributions. Bull. Calcutta Math. Soc.,
49:214?224.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. TR-10-98, Computer Science Group,
Harvard University.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
survey of smoothing techniques for ME models.
IEEE Transactions on Speech and Audio Process-
ing, 8(1):37?50.
Corinna Cortes and Vladimir Vapnik. 1995. Support
vector networks. Machine Learning, 20:273?297.
CRL. 2002. EDR electronic dictionary version 2.0
technical guide. Communications Research Labo-
ratory (CRL).
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of ACL 94.
Ido Dagan, Shaul Marcus, and Shaul Markovitch.
1995. Contextual word similarity and estimation
from sparse data. Computer, Speech and Language,
9:123?152.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of ACL 97.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning, 34(1-3):43?69.
Gregory Grefenstette. 1994. Explorations In Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers.
Zellig Harris. 1954. Distributional structure. Word,
pages 146?142.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of
ACL-90, pages 268?275.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT.
Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,
and Masaki Murata. 2009. Generating a large-scale
analogy list using a probabilistic clustering based on
noun-verb dependency profiles. In Proceedings of
15th Annual Meeting of The Association for Natural
Language Processing (in Japanese).
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING/ACL-
98, pages 768?774.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of ACL-IJCNLP 2009, pages 100?
108.
Masaki Murata, Qing Ma, Tamotsu Shirado, and Hi-
toshi Isahara. 2004. Database for evaluating ex-
tracted terms and tool for visualizing the terms. In
Proceedings of LREC 2004 Workshop: Computa-
tional and Computer-Assisted Terminology, pages
6?9.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of EMNLP 2009, pages 938?947.
T. W. Rauber, T. Braun, and K. Berns. 2008. Proba-
bilistic distance measures of the Dirichlet and Beta
distributions. Pattern Recognition, 41:637?645.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
Chikara Hashimoto, and Sadao Kurohashi. 2008.
Tsubaki: An open search engine infrastructure for
developing new information access. In Proceedings
of IJCNLP 2008.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of COLING-ACL 2006, pages 985?992.
Akira Terada, Minoru Yoshida, and Hiroshi Nakagawa.
2004. A tool for constructing a synonym dictionary
using context information. In IPSJ SIG Technical
Report (in Japanese), pages 87?94.
256
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 31?36,
Dublin, Ireland, August 23 2014.
Automatic Detection and Analysis of Impressive Japanese Sentences
Using Supervised Machine Learning
Daiki Hazure, Masaki Murata, Masato Tokuhisa
Department of Information and Electronics
Tottori University
4-101 Koyama-Minami, Tottori 680-8552, Japan
{s082042,murata,tokuhisa}@ike.tottori-u.ac.jp
Abstract
It is important to write sentences that impress the listener or reader (?impressive sentences?) in
many cases, such as when drafting political speeches. The study reported here provides useful in-
formation for writing such sentences in Japanese. Impressive sentences in Japanese are collected
and examined for characteristic words. A number of such words are identified that often appear
in impressive sentences, including jinsei (human life), hitobito (people), koufuku (happiness),
yujou (friendliness), seishun (youth), and ren?ai (love). Sentences using these words are likely
to impress the listener or reader. Machine learning (SVM) is also used to automatically extract
impressive sentences. It is found that the use of machine learning enables impressive sentences
to be extracted from a large amount of Web documents with higher precision than that obtained
with a baseline method, which extracts all sentences as impressive sentences.
1 Introduction
People are always willing to be impressed, and the things that most impress them are liable to be things
they need to live, such as food. On the other hand, the wisdom of human beings is recorded in writing,
saved in the form of sentences, and inherited by future generations. In this study, we therefore focused on
?impressions? and ?sentences? and studied sentences that tend to impress the listener or reader. Hereafter
for brevity we will refer to these as ?impressive sentences?. There were two main topics in this study:
collecting impressive sentences and analyzing them.
1. Collecting impressive sentences
We manually collect impressive sentences as well as sentences that are not particularly impressive.
By using these sentences and supervised machine learning, we collect more impressive sentences
from the Web.
2. Analyzing impressive sentences
We examine and analyze the impressive sentences. By identifying and collecting words that were
often used in them, we clarify the linguistic characteristics of the sentences.
The focus of our study is Japanese sentences.
The study we report in this paper provides useful information for constructing a system that supports
the writing of impressive sentences. Such a system would be useful for writing drafts of politicians?
speeches or for writing project plan documents where the use of impressive sentences would make the
documents more likely to be accepted. In this study, we use natural language processing in an attempt to
support persons in their efforts to write impressive sentences.
The main points of the study are as follows:
? This study is the first attempt to use natural language processing for automatic collection and anal-
ysis of impressive sentences.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
31
? By collecting sentences automatically and examining the collected data, we identified jinsei (human
life), hitobito (people), koufuku (happiness), yujou (friendliness), seishun (youth), ren?ai (love), etc.
as words that often appear in impressive sentences. Sentences containing one or more of these
words are likely to be impressive sentences. These results should prove to be useful for generating
impressive sentences.
? We used machine learning to obtain impressive sentences from a large amount of Web documents
at a 0.4 precision rate. This is much higher than the 0.07 rate obtained with a baseline method.
2 Collecting impressive sentences
We first use the Google search engine to collect impressive sentences and sentences that are not particu-
larly impressive. We then use these sentences as supervised data with machine learning to collect more
impressive and non-impressive sentences from Web documents.1
Hereafter, we will refer to impressive sentences as positive examples and non-impressive sentences as
negative examples.
2.1 Manual collection of impressive sentences
We extract sentences that are obtained by using retrieval words like ?... toiu kotoba ni kando shita? (I
was impressed by the words...) as positive example candidates. We extract sentences that are obtained
by using retrieval words like ?... toiu bun? (the sentences...) as negative example candidates.
Example sentences containing ?... toiu kotoba ni kando shita? and ?... toiu bun? are shown below.
Example sentences containing ?... toiu kotoba ni kando shita?:
?mainichi ga mirai? toiu kotoba ni kando-shita.
(every day) (future) (of) (word) (was impressed)
(I was impressed by the words ?Every day is the future.?)
Example sentences containing ?... toiu bun?:
kanojo wa supoutsu wo suru noga suki desu toiu bun
(she) (sport) (play) (like) (of) (sentence)
(The sentence ?She likes playing sports?)
In the above examples, the sentences mainichi ga mirai (Every day is the future) and michi wa hito
to hito no kakehashi desu (A road is a bridge connecting people with other people) are used as positive
example candidates. The sentences yomitori senyou (Read only) and kanojo wa supoutsu wo suru noga
suki desu (She likes playing sports) are used as negative example candidates.
We also use the Google search engine to retrieve famous sentences and use them as positive example
candidates.2 We collect sentences from sources such as Yahoo! News and use them as negative example
candidates.
We manually judge whether candidates are positive and negative, and in so doing obtain accurate
positive and negative examples.
Our judgment criterion is that sentences that received the comment ?kando shita? (was impressed by)
and famous sentences are judged to be positive. Sentences that do not have emphatic punctuation such
as exclamation marks or that describe objective facts only are judged to be negative.
We performed the above procedure and obtained 1,018 positive examples and 406 negative examples.
2.2 Using supervised machine learning to collect impressive sentences
We conduct machine learning using the positive and negative examples obtained as described in Section
2.1 as supervised data. We use sentences in Web documents as inputs for machine learning. Machine
learning is used to judge whether the sentences are impressive. In this way we collect impressive sen-
tences from Web documents.
The specific procedure is as follows:
1We used the Web documents that Kawahara et al. collected (Kawahara and Kurohashi, 2006).
2Some famous sentences are obtained from http://www.meigensyu.com/.
32
Table 1: Words with high appearance probabilities in positive examples
Word Ratio of Freq. of Freq. of
positive positive negative
koufuku (happiness) 1.00 83 0
yujou (friendliness) 1.00 29 0
seishun (youth) 1.00 18 0
kanashimi (sadness) 1.00 12 0
sonzai (existence) 1.00 10 0
... ... ... ...
wareware (we) 0.97 37 1
fukou (unhappiness) 0.97 32 1
aisa (love) 0.96 23 1
ren?ai (love) 0.96 44 2
koi (love) 0.95 122 7
kodoku (loneliness) 0.94 32 2
konoyo (this world) 0.94 16 1
aishi (love) 0.94 31 2
Word Ratio of Freq. of Freq. of
positive positive negative
aisuru (love) 0.94 30 2
arayuru (every) 0.93 14 1
omae (you) 0.93 13 1
shunkan (moment) 0.92 11 1
jinsei (life) 0.91 145 14
mirai (future) 0.91 20 2
shiawase (happiness) 0.91 20 2
yorokobi (delight) 0.91 10 1
onna (woman) 0.91 115 12
unmei (destiny) 0.90 19 2
shinu (die) 0.90 37 4
... ... ... ...
hitobito (people) 0.81 17 4
kandou (impression) 0.80 8 2
1. The 1,018 positive and 406 negative examples obtained as described in Section 2.1 are used as
supervised data.
2. We use the supervised data to conduct machine learning. The machine learning is used to judge
whether 10,000 sentences newly obtained from Web documents are positive or negative. We man-
ually check sentences judged to be positive and construct new positive and negative examples. We
add the new examples to the supervised data.
3. We repeat the above step 2 procedure ten times.
We use a support vector machine (SVM) for machine learning (Cristianini and Shawe-Taylor, 2000;
Kudoh and Matsumoto, 2000; Isozaki and Kazawa, 2002; Murata et al., 2002; Takeuchi and Collier,
2003; Mitsumori et al., 2005; Chen and Wen, 2006; Murata et al., 2011).3 We use unigram words whose
parts of speech (POSs) are nouns, verbs, adjectives, adjectival verbs, adnominals, and interjections as
features used in machine learning.
The judgment criteria for positive and negative examples in this section are as follows: Sentences
for which a judge can spontaneously produce certain comments are judged to be positive examples.
Sentences that describe objective facts only are judged to be negative examples.
We repeated the procedure ten times. In total, 275 positive and 3,006 negative examples were obtained.
When we add these examples to the original ones, the totals become 1,293 positive and 3,412 negative
examples. In this case we repeated the learning procedure ten times, but more positive and negative
examples could be obtained by repeating it more than ten times.
A subject (Subject A) judged whether the examples were positive or negative. Three other subjects
evaluated 20 examples that were judged positive and 20 that were judged negative by Subject A. We
compared Subject A?s judgments and the majority voting results of the other three subjects? judgments
and obtained 0.58 (moderate agreement) as a kappa value.
3 Analysis of collected impressive sentences
In our analysis, we used the abovementioned 1,293 positive and 3,412 negative examples. We used
certain words to examine the impressive sentences.
We extracted a number of words from the positive and negative examples. For each word, we calcu-
lated its appearance frequency in positive and negative examples and the ratio of its frequency in positive
examples to its frequency in negative ones. We extracted words for which the ratio was higher than 0.8
and words that were at least four times likelier to appear in positive examples than in negative ones. Some
of the extracted words are shown in Table 1. In the table, ?Ratio appearing in positive? indicates the ratio
3In this study, we use a quadratic polynomial kernel as a kernel function of SVM. We confirmed that the kernel produced
good performance in preliminary experiments.
33
Table 2: Impressive sentence extraction performance of various methods
Method Precision Recall F measure
ML method (0th) 0.06 0.25 0.10
ML method (first) 0.26 0.08 0.12
ML method (second) 0.29 0.07 0.11
ML method (fifth) 0.31 0.05 0.09
ML method (10th) 0.40 0.05 0.09
Baseline method 0.07 1.00 0.12
Pattern method 1 0.11 0.08 0.09
Pattern method 2 1.00 0.002 0.003
of the word?s frequency in positive examples to its frequency in the data. ?Frequency in positive? and
?Frequency in negative? respectively show the number of times the word appears in positive and negative
examples.
As the table shows, the words obtaining the highest ratios included jinsei (human life), hitobito (peo-
ple), koufuku (happiness), yujou (friendliness), seishun (youth), and ren?ai (love). Sentences in which
one or more of these words are used are likely to be impressive sentences.
These results are the most important and interesting points in this paper. We found that using the
words shown in the table is a good approach to use if we would like to generate impressive sentences.
Shown below are example sentences containing jinsei (human life), hitobito (people), and koufuku
(happiness).
Example sentences containing jinsei (life):
jinsei wa douro no youna mono da. ichibanno chikamichi wa taitei ichiban warui. michi da.
(life) (road) (be like) (first) (shortcut) (usually) (worst) (road)
(Life is like a road. The first shortcut is usually the worst road.)
Example sentences containing hitobito (people):
hitobito wa kanashimi wo wakachi attekureru tomodachi sae ireba kanashimi wo yawaragerareru.
(people) (sadness) (share) (friend) (if only they have) (sadness) (can soften)
(People can soften their sadness, if only they have a friend with whom they can share it.)
Example sentences containing koufuku (happiness):
fukouna hito wa kibou wo mote. koufukuna hito wa youjin seyo.
(unhappy) (person) (hope) (should have) (happy) (person) (should be on one?s guard)
(Unhappy people should have hope. Happy people should be on their guard.)
4 Automatic impressive sentence extraction performance
The method we describe in this paper is a useful one for automatically extracting impressive sentences.
In this section, we evaluate the extraction performance of this and other methods.
The evaluation results are shown in Table 2. The data set for evaluation consists of 10,000 new
sentences from Web documents. We use each method to extract positive sentences from the set for eval-
uation. We then randomly extract 100 data items (200 for the baseline method only) from the sentences
extracted by each method and manually evaluate them. From the evaluation results we approximately
calculate the precision rates, the recall rates, and the F-measures.
We estimate the denominator of the recall rate from the number of positive examples detected by the
baseline method. The baseline method judges that all the inputs are positive.
In the ?ML method (xth)? we use supervised data for machine learning after adding the xth positive
and negative examples to the supervised data (by the method in Section 2.2). In ?Pattern method 1? we
extract sentences that contain words whose positive appearance ratio is at least 0.8 and that appear at
least four times as positive examples. In ?Pattern method 2? we extract sentences that contain the word
?kando? (impression) as positive examples.
With machine learning we obtain a precision rate of 0.40 after we add the 10th positive and negative
examples to the supervised data. This precision rate is much higher than the 0.07 rate we obtain with the
34
baseline method.
Some may think that the 0.40 precision rate obtained with machine learning is low. However, since
the task of extracting impressive sentences is a very difficult one, and since the rate is much higher than
the baseline method rate, we can say that the machine learning results are at least adequate.
5 Related studies
Many methods have been reported that estimated the orientation (positive or negative contents) or the
emotion of a sentence (Turney and Littman, 2003; Pang and Lee, 2008; Kim and Hovy, 2004; Alm et al.,
2005; Aman and Szpakowicz, 2007; Strapparava andMihalcea, 2008; Inkpen et al., 2009; Neviarouskaya
et al., 2009). However, the studies did not address the task of collecting and analyzing impressive sen-
tences to support the generation of such sentences.
There have been studies that addressed the task of automatically evaluating sentences to support sen-
tence generation (Bangalore and Whittaker, 2000; Mutton and Dale, 2007). However, the studies did not
address the task of generating impressive sentences.
In our study, we used machine learning to extract impressive sentences. There have been other studies
as well in which machine learning was used to extract information (Murata et al., 2011; Stijn De Saeger
and Hashimoto, 2009). Murata et al. extracted articles describing problems, their solutions, and their
causes (Murata et al., 2011). Saeger et al. extracted several types of words from a large scale of Web
documents by using machine learning (Stijn De Saeger and Hashimoto, 2009). In their method, they
manually make supervised data sets for extracted words and extract more words from Web documents
using supervised methods. Their study is similar to ours in that both use the same framework of manually
making a small scale supervised data set and then extracting more data items from Web documents.
6 Conclusion
We collected sentences in Japanese that impressed readers (?impressive sentences?) and examined them
through the use of characteristic words in order to support the generation of impressive sentences. In
our examination, we obtained jinsei (human life), hitobito (people), koufuku (happiness), yujou (friend-
liness), seishun (youth), ren?ai (love), etc. as words that often appear in impressive sentences. Sentences
in which one or more of these words are used would be likely to impress the listener or reader. The
results we obtained should provide useful information for generating impressive sentences.
In this study, we used machine learning to extract impressive sentences and found that with this method
we could extract them from a large amount of Web documents with a precision rate of 0.40.
In future work, we intend to use this method to collect more impressive sentences. We also plan to
analyze the sentences by using not only words but also parameters such as syntax patterns and rhetorical
expressions.
Acknowledgment
This work was supported by JSPS KAKENHI Grant Number 23500178.
References
Cecilia O. Alm, Dan Roth, and Richard Sproat. 2005. Emotions from text: Machine learning for text-based
emotion prediction. In Proceedings of Human Language Technology Conference and Conference on Empirical
Methods in Natural Language (HLT/EMNLP 2005), pages 579?586.
Saima Aman and Stan Szpakowicz. 2007. Identifying expressions of emotion in text. In Proceedings of the 10th
International Conference on Text, Speech and Dialogue (TSD?07), pages 196?205.
Srinivas Bangalore and Owen Rambow and Steve Whittaker. 2000. Evaluation metrics for generation. In Pro-
ceedings of the first international conference on Natural language generation (INLG ?00), pages 1?8.
Peng Chen and Tao Wen. 2006. Margin maximization model of text classification based on support vector ma-
chines. In Machine Learning and Cybernetics, pages 3514?3518.
35
Nello Cristianini and John Shawe-Taylor. 2000. An introduction to support vector machines and other kernel-
based learning methods. Cambridge University Press.
Diana Inkpen, Fazel Keshtkar, and Diman Ghazi. 2009. Analysis and generation of emotion in texts. InKnowledge
Engineering: Principle and Technique, KEPT 2009, pages 3?14.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. Pro-
ceedings of the 19th International Conference on Computational Linguistics (COLING-2002), pages 1?7.
Daisuke Kawahara and Sadao Kurohashi. 2006. Case frame compilation from the web using high-performance
computing. Proceedings of the 5th International Conference on Language Resources and Evaluation,, pages
1?4.
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of Coling 2004,
pages 1367?1373.
Taku Kudoh and Yuji Matsumoto. 2000. Use of support vector learning for chunk identification. CoNLL-2000,
pages 142?144.
Tomohiro Mitsumori, Sevrani Fation, Masaki Murata, Kouichi Doi, and Hirohumi Doi. 2005. Gene/protein
name recognition based on support vector machine using dictionary as features. BMC Bioinformatics, 6(Suppl
1)(S8):1?10.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002. Comparison of three machine-learning methods for Thai
part-of-speech tagging. ACM Transactions on Asian Language Information Processing, 1(2):145?158.
Masaki Murata, Hiroki Tanji, Kazuhide Yamamoto, Stijn De Saeger, Yasunori Kakizawa, and Kentaro Torisawa.
2011. Extraction from the web of articles describing problems, their solutions, and their causes. IEICE Trans-
actions on Information and Systems, E94?D(3):734?737.
Andrew Mutton and Mark Dras and Stephen Wan and Robert Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL
2007), pages 344?351.
Alena Neviarouskaya, Helmut Prendinger, and Mitsuru Ishizuka. 2009. Compositionality principle in recognition
of fine-grained emotions from text. In Proceedings of 4th International AAAI Conference on Weblogs and Social
Media (ICWSM 2009), pages 278?281.
Bo Pang and Lillian Lee. 2008. Opinion minding and sentiment analysis. Foundation and Trend in Information
Retrieval, 2(1-2):1?135.
Kentaro Torisawa Masaki Murata Ichiro Yamada Kow Kuroda Stijn De Saeger, Jun?ichi Kazama and Chikara
Hashimoto. 2009. A web service for automatic word class acquisition. In Proceedings of 3rd International
Universal Communication Symposium (IUCS 2009), pages 132?137.
Carlo Strapparava and Rada Mihalcea. 2008. Learning to identify emotions in text. In Proceedings of the 2008
ACM Symposium on Applied Computing (SAC ?08), page 1556?1560.
Koichi Takeuchi and Nigel Collier. 2003. Bio-medical entity extraction using support vector machine. Proceed-
ings of the ACL 2003 Workshop on NLP in Biomedicine, pages 57?64.
Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information Systems (TOIS), 21(4):315?346.
36
