Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 151?160, Prague, June 2007. c?2007 Association for Computational Linguistics
Using Foreign Inclusion Detection to Improve Parsing Performance
Beatrice Alex, Amit Dubey and Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
{balex,adubey,keller}@inf.ed.ac.uk
Abstract
Inclusions from other languages can be a
significant source of errors for monolin-
gual parsers. We show this for English in-
clusions, which are sufficiently frequent to
present a problem when parsing German.
We describe an annotation-free approach for
accurately detecting such inclusions, and de-
velop two methods for interfacing this ap-
proach with a state-of-the-art parser for Ger-
man. An evaluation on the TIGER cor-
pus shows that our inclusion entity model
achieves a performance gain of 4.3 points in
F-score over a baseline of no inclusion de-
tection, and even outperforms a parser with
access to gold standard part-of-speech tags.
1 Introduction
The status of English as a global language means
that English words and phrases are frequently bor-
rowed by other languages, especially in domains
such as science and technology, commerce, adver-
tising, and current affairs. This is an instance of lan-
guage mixing, whereby inclusions from other lan-
guages appear in an otherwise monolingual text.
While the processing of foreign inclusions has re-
ceived some attention in the text-to-speech (TTS) lit-
erature (see Section 2), the natural language process-
ing (NLP) community has paid little attention both
to the problem of inclusion detection, and to poten-
tial applications thereof. Also the extent to which
inclusions pose a problem to existing NLP methods
has not been investigated.
In this paper, we address this challenge. We focus
on English inclusions in German text. Anglicisms
and other borrowings from English form by far the
most frequent foreign inclusions in German. In spe-
cific domains, up to 6.4% of the tokens of a Ger-
man text can be English inclusions. Even in regular
newspaper text as used for many NLP applications,
English inclusions can be found in up to 7.4% of all
sentences (see Section 3 for both figures).
Virtually all existing NLP algorithms assume that
the input is monolingual, and does not contain for-
eign inclusions. It is possible that this is a safe
assumption, and inclusions can be dealt with ac-
curately by existing methods, without resorting to
specialized mechanisms. The alternative hypothe-
sis, however, seems more plausible: foreign inclu-
sions pose a problem for existing approaches, and
sentences containing them are processed less ac-
curately. A parser, for example, is likely to have
problems with inclusions ? most of the time, they
are unknown words, and as they originate from
another language, standard methods for unknown
words guessing (suffix stripping, etc.) are unlikely to
be successful. Furthermore, the fact that inclusions
are often multiword expressions (e.g., named enti-
ties) means that simply part-of-speech (POS) tag-
ging them accurately is not sufficient: if the parser
posits a phrase boundary within an inclusion this is
likely to severely decrease parsing accuracy.
In this paper, we focus on the impact of En-
glish inclusions on the parsing of German text. We
describe an annotation-free method that accurately
recognizes English inclusions, and demonstrate that
inclusion detection improves the performance of a
state-of-the-art parser for German. We show that the
way of interfacing the inclusion detection and the
parser is crucial, and propose a method for modify-
ing the underlying probabilistic grammar in order to
151
enable the parser to process inclusions accurately.
This paper is organized as follows. We review re-
lated work in Section 2, and present the English in-
clusion classifier in Section 3. Section 4 describes
our results on interfacing inclusion detection with
parsing, and Section 5 presents an error analysis.
Discussion and conclusion follow in Section 6.
2 Related Work
Previous work on inclusion detection exists in the
TTS literature. Here, the aim is to design a sys-
tem that recognizes foreign inclusions on the word
and sentence level and functions at the front-end to
a polyglot TTS synthesizer. Pfister and Romsdor-
fer (2003) propose morpho-syntactic analysis com-
bined with lexicon lookup to identify foreign words
in mixed-lingual text. While they state that their sys-
tem is precise at detecting the language of tokens
and determining the sentence structure, it is not eval-
uated on real mixed-lingual text. A further approach
to inclusion detection is that of Marcadet et. al
(2005). They present experiments with a dictionary-
driven transformation-based learning method and a
corpus-based n-gram approach and show that a com-
bination of both methods yields the best results.
Evaluated on three mixed-lingual test sets in differ-
ent languages, the combined approach yields word-
based language identification error rates (i.e. the per-
centage of tokens for which the language is identi-
fied incorrectly) of 0.78% on the French data, 1.33%
on the German data and 0.84% on the Spanish data.
Consisting of 50 sentences or less for each language,
their test sets are very small and appear to be se-
lected specifically for evaluation purposes. It would
therefore be interesting to determine the system?s
performance on random and unseen data and exam-
ine how it scales up to larger data sets.
Andersen (2005), noting the importance of rec-
ognizing anglicisms to lexicographers, tests algo-
rithms based on lexicon lookup, character n-grams
and regular expressions and a combination thereof to
automatically extract anglicisms in Norwegian text.
On a 10,000 word subset of the neologism archive
(Wangensteen, 2002), the best method of combin-
ing character n-grams and regular expression match-
ing yields an accuracy of 96.32% and an F-score of
59.4 (P = 75.8%, R = 48.8%). This result is unsur-
prisingly low as no differentiation is made between
full-word anglicisms and tokens with mixed-lingual
morphemes in the gold standard.
In the context of parsing, Forst and Kaplan (2006)
have observed that the failure to properly deal with
foreign inclusions is detrimental to a parser?s accu-
racy. However, they do not substantiate this claim
using numeric results.
3 English Inclusion Detection
Previous work reported by Alex (2006; 2005) has
focused on devising a classifier that detects angli-
cisms and other English inclusions in text written in
other languages, namely German and French. This
inclusion classifier is based on a lexicon and search
engine lookup as well as a post-processing step.
The lexicon lookup is performed for tokens
tagged as noun (NN ), named entity (NE ), foreign
material (FM ) or adjective (ADJA/ADJD ) using the
German and English CELEX lexicons. Tokens only
found in the English lexicon are classified as En-
glish. Tokens found in neither lexicon are passed
to the search engine module. Tokens found in
both databases are classified by the post-processing
module. The search engine module performs lan-
guage classification based on the maximum nor-
malised score of the number of hits returned for two
searches per token, one for each language (Alex,
2005). This score is determined by weighting the
number of hits, i.e. the ?absolute frequency? by the
estimated size of the accessible Web corpus for that
language (Alex, 2006). Finally, the rule-based post-
processing module classifies single-character tokens
and resolves language classification ambiguities for
interlingual homographs, English function words,
names of currencies and units of measurement. A
further post-processing step relates language infor-
mation between abbreviations or acronyms and their
definitions in combination with an abbreviation ex-
traction algorithm (Schwartz and Hearst, 2003). Fi-
nally, a set of rules disambiguates English inclusions
from person names (Alex, 2006).
For German, the classifier has been evaluated
on test sets in three different domains: newspaper
articles, selected from the Frankfurter Allgemeine
Zeitung, on internet and telecoms, space travel and
European Union related topics. Table 1 presents an
152
Domain EI tokens EI types EI TTR Accuracy Precision Recall F
Internet 6.4% 5.9% 0.25 98.13% 91.58% 78.92% 84.78
Space 2.8% 3.5% 0.33 98.97% 84.02% 85.31% 84.66
EU 1.1% 2.1% 0.50 99.65% 82.16% 87.36% 84.68
Table 1: English inclusion (EI) token and type statistics, EI type-token-ratios (TTR) as well as accuracy,
precision, recall and F-scores for the unseen German test sets.
overview of the percentages of English inclusion to-
kens and types within the gold standard annotation
of each test set, and illustrates how well the English
inclusion classifier is able to detect them in terms
of F-score. The figures show that the frequency of
English inclusions varies considerably depending on
the domain but that the classifier is able to detect
them equally well with an F-score approaching 85
for each domain.
The recognition of English inclusions bears sim-
ilarity to classification tasks such as named en-
tity recognition, for which various machine learning
(ML) techniques have proved successful. In order to
compare the performance of the English inclusion
classifier against a trained ML classifier, we pooled
the annotated English inclusion evaluation data for
all three domains. As the English inclusion classifier
does not rely on annotated data, it can be tested and
evaluated once for the entire corpus. The ML classi-
fier used for this experiment is a conditional Markov
model tagger which is designed for, and proved suc-
cessful in, named entity recognition in newspaper
and biomedical text (Klein et al, 2003; Finkel et al,
2005). It can be trained to perform similar informa-
tion extraction tasks such as English inclusion detec-
tion. To determine the tagger?s performance over the
entire set and to investigate the effect of the amount
of annotated training data available, a 10-fold cross-
validation test was conducted whereby increasing
sub-parts of the training data are provided when test-
ing on each fold. The resulting learning curves in
Figure 1 show that the English inclusion classifier
has an advantage over the supervised ML approach,
despite the fact the latter requires expensive hand-
annotated data. A large training set of 80,000 tokens
is required to yield a performance that approximates
that of our annotation-free inclusion classifier. This
system has been shown to perform similarly well on
unseen texts in different domains, plus it is easily
 20
 30
 40
 50
 60
 70
 80
 90
 10000  20000  30000  40000  50000  60000  70000  80000
F-
sc
or
e
Amount of training data (in tokens)
Statistical Tagger
English Inclusion Classifier
Figure 1: Learning curve of a ML classifier versus
the English inclusion classifier?s performance.
extendable to a new language (Alex, 2006).
4 Experiments
The primary focus of this paper is to apply the En-
glish inclusion classifier to the German TIGER tree-
bank (Brants et al, 2002) and to evaluate the clas-
sifier on a standard NLP task, namely parsing. The
aim is to investigate the occurrence of English in-
clusions in more general newspaper text, and to ex-
amine if the detection of English inclusions can im-
prove parsing performance.
The TIGER treebank is a bracketed corpus con-
sisting of 40,020 sentences of newspaper text. The
English inclusion classifier was run once over the
entire TIGER corpus. In total, the system detected
English inclusions in 2,948 of 40,020 sentences
(7.4%), 596 of which contained at least one multi-
word inclusion. This subset of 596 sentences is the
focus of the work reported in the remainder of this
paper, and will be referred to as the inclusion set.
A gold standard parse tree for a sentence contain-
ing a typical multi-word English inclusion is illus-
trated in Figure 2. The tree is relatively flat, which
153
is a trait trait of TIGER treebank annotation (Brants
et al, 2002). The non-terminal nodes of the tree rep-
resent the phrase categories, and the edge labels the
grammatical functions. In the example sentence, the
English inclusion is contained in a proper noun (PN )
phrase with a grammatical function of type noun
kernel element (NK ). Each terminal node is POS-
tagged as a named entity (NE ) with the grammatical
function ot type proper noun component (PNC ).
4.1 Data
Two different data sets are used in the experiments:
(1) the inclusion set, i.e., the sentences containing
multi-word English inclusions recognized by the in-
clusion classifier, and (2) a stratified sample of sen-
tences randomly extracted from the TIGER corpus,
with strata for different sentence lengths. The strata
were chosen so that the sentence length distribution
of the random set matches that of the inclusion set.
The average sentence length of this random set and
the inclusion set is therefore the same at 28.4 tokens.
This type of sampling is necessary as the inclusion
set has a higher average sentence length than a ran-
dom sample of sentences from TIGER, and because
parsing accuracy is correlated with sentence length.
Both the inclusion set and the random set consist of
596 sentences and do not overlap.
4.2 Parser
The parsing experiments were performed with a
state-of-the-art parser trained on the TIGER corpus
which returns both phrase categories and grammati-
cal functions (Dubey, 2005b). Following Klein and
Manning (2003), the parser uses an unlexicalized
probabilistic context-free grammar (PCFG) and re-
lies on treebank transformations to increase parsing
accuracy. Crucially, these transformations make use
of TIGER?s grammatical functions to relay pertinent
lexical information from lexical elements up into the
tree.
The parser also makes use of suffix analysis.
However, beam search or smoothing are not em-
ployed. Based upon an evaluation on the NEGRA
treebank (Skut et al, 1998), using a 90%-5%-5%
training-development-test split, the parser performs
with an accuracy of 73.1 F-score on labelled brack-
ets with a coverage of 99.1% (Dubey, 2005b). These
figures were derived on a test set limited to sentences
containing 40 tokens or less. In the data set used
in this paper, however, sentence length is not lim-
ited. Moreover, the average sentence length of our
test sets is considerably higher than that of the NE-
GRA test set. Consequently, a slightly lower perfor-
mance and/or coverage is anticipated, albeit the type
and domain as well as the annotation of both the NE-
GRA and the TIGER treebanks are very similar. The
minor annotation differences that do exist between
NEGRA and TIGER are explained in Brants et. al
(2002).
4.3 Parser Modifications
We test several variations of the parser. The baseline
parser does not treat foreign inclusions in any spe-
cial way: the parser attempts to guess the POS tag
and grammatical function labels of the word using
the same suffix analysis as for rare or unseen Ger-
man words. The additional versions of the parser
are inspired by the hypothesis that inclusions make
parsing difficult, and this difficulty arises primarily
because the parser cannot detect inclusions prop-
erly. Therefore, a suitable upper bound is to give
the parser perfect tagging information. Two further
versions interface with our inclusion classifier and
treat words marked as inclusions differently from
native words. The first version does so on a word-
by-word basis. In contrast, the inclusion entity ap-
proach attempts to group inclusions, even if a group-
ing is not posited by phrase structure rules. We now
describe each version in more detail.
In the TIGER annotation, preterminals include
both POS tags and grammatical function labels.
For example, rather than a preterminal node hav-
ing the category PRELS (personal pronoun), it is
given the category PRELS-OA (accusative personal
pronoun). Due to these grammatical function tags,
the perfect tagging parser may disambiguate more
syntactic information than provided with POS tags
alone. Therefore, to make this model more realistic,
the parser is required to guess grammatical functions
(allowing it to, for example, mistakenly tag an ac-
cusative pronoun as nominative, dative or genitive).
This gives the parser information about the POS tags
of English inclusions (along with other words), but
does not give any additional hints about the syntax
of the sentence.
The two remaining models both take advantage
154
SNP-SB
ART-NK
Das
ADJA-NK
scho?nste
PN-NK
NE-PNC
Road
NE-PNC
Movie
VVFIN-HD
kam
PP-MO
APPR-AC
aus
ART-NK
der
NE-NK
Schweiz
Figure 2: Example parse tree of a German TIGER sentence containing an English inclusion. Translation:
The nicest road movie came from Switzerland.
NE FM NN KON CARD ADJD APPR
1185 512 44 8 8 1 1
Table 2: POS tags of foreign inclusions.
PN
FOM
. . .
FOM
. . .
(a) Whenever a FOM is encoun-
tered...
PN
FP
FOM
. . .
FOM
. . .
(b) ...a new FP category is cre-
ated
Figure 3: Tree transformation employed in the in-
clusion entity parser.
of information from the inclusion detector. To inter-
face the detector with the parser, we simply mark
any inclusion with a special FOM (foreign mate-
rial) tag. The word-by-word parser attempts to guess
POS tags itself, much like the baseline. However,
whenever it encounters a FOM tag, it restricts itself
to the set of POS tags observed in inclusions during
training (the tags listed in Table 2). When a FOM is
detected, these and only these POS tags are guessed;
all other aspects of the parser remain the same.
The word-by-word parser fails to take advantage
of one important trend in the data: that foreign in-
clusion tokens tend to be adjacent, and these adja-
cent words usually refer to the same entity. There
is nothing stopping the word-by-word parser from
positing a constituent boundary between two adja-
cent foreign inclusions. The inclusion entity model
was developed to restrict such spurious bracketing.
It does so by way of another tree transformation.
The new category FP (foreign phrase) is added be-
low any node dominating at least one token marked
FOM during training. For example, when encoun-
tering a FOM sequence dominated by PN as in Fig-
ure 3(a), the tree is modified so that it is the FP rule
which generates the FOM tokens. Figure 3(b) shows
the modified tree. In all cases, a unary rule PN?FP
is introduced. As this extra rule decreases the proba-
bility of the entire tree, the parser has a bias to intro-
duce as few of these rules as possible ? thus limiting
the number of categories which expand to FOMs.
Once a candidate parse is created during testing, the
inverse operation is applied, removing the FP node.
4.4 Method
For all experiments reported in this paper, the parser
is trained on the TIGER treebank. As the inclusion
and random sets are drawn from the whole TIGER
treebank, it is necessary to ensure that the data used
to train the parser does not overlap with these test
sentences. The experiments are therefore designed
as multifold cross-validation tests. Using 5 folds,
each model is trained on 80% of the data while the
remaining 20% are held out. The held out set is then
155
Data P R F Dep. Cov. AvgCB 0CB ?2CB
Baseline model
Inclusion set 56.1 62.6 59.2 74.9 99.2 2.1 34.0 69.0
Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1
Perfect tagging model
Inclusion set 61.3 63.0 62.2 75.1 92.7 1.7 41.5 72.6
Random set 65.8 68.9 67.3 82.4 97.7 1.4 45.9 77.1
Word-by-word model
Inclusion set 55.6 62.8 59.0 73.1 99.2 2.1 34.2 70.2
Random set 63.3 67.3 65.2 81.1 99.2 1.6 40.4 75.1
Inclusion entity model
Inclusion set 61.3 65.9 63.5 78.3 99.0 1.7 42.4 77.1
Random set 63.4 67.5 65.4 80.8 99.2 1.6 40.1 75.7
Table 3: Baseline and perfect tagging for inclusion and random sets and results for the word-by-word and
the inclusion entity models.
intersected with the inclusion set (or, respectively,
the random set). The evaluation metrics are calcu-
lated on this subset of the inclusion set (or random
set), using the parser trained on the corresponding
training data. This process ensures that the test sen-
tences are not contained in the training data.
The overall performance metrics of the parser are
calculated on the aggregated totals of the five held
out test sets. For each experiment, we report pars-
ing performance in terms of the standard PARSE-
VAL scores (Abney et al, 1991), including cov-
erage (Cov), labeled precision (P) and recall (R),
F-score, the average number of crossing brackets
(AvgCB), and the percentage of sentences parsed
with zero and with two or fewer crossing brack-
ets (0CB and ?2CB). In addition, we also report
dependency accuracy (Dep), calculated using the
approach described in Lin (1995), using the head-
picking method used by Dubey (2005a). The la-
beled bracketing figures (P, R and F), and the de-
pendency score are calculated on all sentences, with
those which are out-of-coverage getting zero nodes.
The crossing bracket scores are calculated only on
those sentences which are successfully parsed.
4.5 Baseline and Perfect Tagging
The baseline, for which the unmodified parser is
used, achieves a high coverage at over 99% for both
the inclusion and the random sets (see Table 3).
However, scores differ for the bracketing measures.
Using stratified shuffling1, we performed a t-test on
precision and recall, and found both to be signif-
icantly worse in the inclusion condition. Overall,
the harmonic mean (F) of precision and recall was
65.2 on the random set, 6 points better than 59.2
F observed on the inclusion set. Similarly, depen-
dency and cross-bracketing scores are higher on the
random test set. This result strongly indicates that
sentences containing English inclusions present dif-
ficulty for the parser, compared to length-matched
sentences without inclusions.
When providing the parser with perfect tagging
information, scores improve both for the inclusion
and the random TIGER samples, resulting in F-
scores of 62.2 and 67.3, respectively. However, the
coverage for the inclusion set decreases to 92.7%
whereas the coverage for the random set is 97.7%.
In both cases, the lower coverage is caused by the
parser being forced to use infrequent tag sequences,
with the much lower coverage of the inclusion set
likely due to infrequent tags (notable FM ), solely
associated with inclusions. While perfect tagging
increases overall accuracy, a difference of 5.1 in F-
score remains between the random and inclusion test
sets. Although smaller than that of the baseline runs,
this difference shows that even with perfect tagging,
1This approach to statistical testing is described in: http:
//www.cis.upenn.edu/?dbikel/software.html
156
parsing English inclusions is harder than parsing
monolingual data.
So far, we have shown that the English inclusion
classifier is able to detect sentences that are difficult
to parse. We have also shown that perfect tagging
helps to improve parsing performance but is insuffi-
cient when it comes to parsing sentences containing
English inclusions. In the next section, we will ex-
amine how the knowledge provided by the English
inclusion classifier can be exploited to improve pars-
ing performance for such sentences.
4.6 Word-by-word Model
The word-by-word model achieves the same cover-
age on the inclusion set as the baseline but with a
slightly lower F of 59.0. All other scores, includ-
ing dependency accuracy and cross bracketing re-
sults are similar to those of the baseline (see Ta-
ble 3). This shows that limiting the parser?s choice
of POS tags to those encountered for English inclu-
sions is not sufficient to deal with such constructions
correctly. In the error analysis presented in Sec-
tion 5, we report that the difficulty in parsing multi-
word English inclusions is recognizing them as con-
stituents, rather than recognizing their POS tags. We
attempt to overcome this problem with the inclusion
entity model.
4.7 Inclusion Entity Model
The inclusion entity parser attains a coverage of
99.0% on the inclusion set, similiar to the cover-
age of 99.2% obtained by the baseline model on
the same data. On all other measures, the inclu-
sion entity model exceeds the performance of the
baseline, with a precision of 61.3% (5.2% higher
than the baseline), a recall of 65.9% (3.3% higher),
an F of 63.5 (4.3 higher) and a dependency accu-
racy of 78.3% (3.4% higher). The average number
of crossing brackets is 1.7 (0.4 lower), with 42.4%
of the parsed sentences having no crossing brack-
ets (8.2% higher), and 77.1% having two or fewer
crossing brackets (8.1% higher). When testing the
inclusion entity model on the random set, the per-
formance is very similar to the baseline model on
this data. While coverage is the same, F and cross-
brackting scores are marginally improved, and the
dependency score is marginally deteriorated. This
shows that the inclusion entity model does not harm
 0
 0.002
 0.004
 0.006
 0.008
 0.01
 0.012
 0.014
 10  20  30  40  50  60  70  80
A
ve
ra
ge
 T
ok
en
 F
re
qu
en
cy
Sentence Length in Tokens
Inclusion sample
Stratified random sample
Figure 4: Average relative token frequencies for sen-
tences of equal length.
the parsing accuracy of sentences that do not actu-
ally contain foreign inclusions.
Not only did the inclusion entity parser perform
above the baseline on every metric for the inclusion
set, its performance also exceeds that of the perfect
tagging model on all measures except precision and
average crossing brackets, where both models are
tied. These results clearly indicate that the inclusion
entity model is able to leverage the additional infor-
mation about English inclusions provided by our in-
clusion classifier. However, it is also important to
note that the performance of this model on the in-
clusion set is still consistently lower than that of all
models on the random set. This demonstrates that
sentences with inclusions are more difficult to parse
than monolingual sentences, even in the presence of
information about the inclusions that the parser can
exploit.
Comparing the inclusion set to the length-
matched random set is arguably not entirely fair as
the latter may not contain as many infrequent tokens
as the inclusion set. Figure 4 shows the average rel-
ative token frequencies for sentences of equal length
for both sets. The frequency profiles of the two data
sets are broadly similar (the difference in means of
both groups is only 0.000676), albeit significantly
different according to a paired t-test (p? 0.05). This
is one reason why the inclusion entity model?s per-
formance on the inclusion set does not reach the up-
per limit set by the random sample.
157
Phrase cat. Frequency Example
PN 91 The Independent
CH 10 Made in Germany
NP 4 Peace Enforcement
CNP 2 Botts and Company
? 2 Chief Executives
Table 4: Gold phrase categories of inclusions.
5 Error Analysis
The error analysis is limited to 100 sentences se-
lected from the inclusion set parsed with both the
baseline and the inclusion entity model. This sam-
ple contains 109 English inclusions, five of which
are false positives, i.e., the output of the English in-
clusion classifier is incorrect. The precision of the
classifier in recognizing multi-word English inclu-
sions is therefore 95.4% for this TIGER sample.
Table 4 illustrates that the majority of multi-word
English inclusions are contained in a proper noun
(PN ) phrase, including names of companies, politi-
cal parties, organizations, films, newspapers, etc. A
less frequent phrasal category is chunk (CH ) which
tends to be used for slogans, quotes or expressions
like Made in Germany. Even in this small sam-
ple, annotations of inclusions as either PN or CH ,
and not the other, can be misleading. For example,
the organization Friends of the Earth is annotated
as a PN , whereas another organization International
Union for the Conservation of Nature is marked as
a CH in the gold standard. This suggests that the
annotation guidelines on foreign inclusions could be
improved when differentiating between phrase cate-
gories containing foreign material.
For the majority of sentences (62%), the baseline
model predicts more brackets than are present in the
gold standard parse tree (see Table 5). This number
decreases by 11% to 51% when parsing with the in-
clusion entity model. This suggests that the baseline
parser does not recognize English inclusions as con-
stituents, and instead parses their individual tokens
as separate phrases. Provided with additional infor-
mation of multi-word English inclusions in the train-
ing data, the parser is able to overcome this problem.
We now turn our attention to how accurately the
various parsers are at predicting both phrase brack-
eting and phrase categories (see Table 6). For 46
Phrase bracket (PB) frequency BL IE
PBPRED > PBGOLD 62% 51%
PBPRED < PBGOLD 11% 13%
PBPRED = PBGOLD 27% 36%
Table 5: Bracket frequency of the predicted baseline
(BL) and inclusion entity (IE) model output com-
pared to the gold standard.
(42.2%) of inclusions, the baseline model makes an
error with a negative effect on performance. In 39
cases (35.8%), the phrase bracketing and phrase cat-
egory are incorrect, and constituent boundaries oc-
cur within the inclusion, as illustrated in Figure 5(a).
Such errors also have a detrimental effect on the
parsing of the remainder of the sentence. Overall,
the baseline model predicts the correct phrase brack-
eting and phrase category for 63 inclusions (57.8%).
Conversely, the inclusion entity model, which is
given information on tag consistency within inclu-
sions via the FOM tags, is able to determine the
correct phrase bracketing and phrase category for
67.9% inclusions (10.1% more), e.g. see Figure 5(b).
Both the phrase bracketing and phrase category are
predicted incorrectly in only 6 cases (5.5%). The
inclusion entity model?s improved phrase boundary
prediction for 31 inclusions (28.4% more correct) is
likely to have an overall positive effect on the pars-
ing decisions made for the context which they ap-
pear in. Nevertheless, the inclusion entity parser still
has difficulty determining the correct phrase cate-
gory in 25 cases (22.9%). The main confusion lies
between assigning the categories PN , CH and NP ,
the most frequent phrase categories of multi-word
English inclusions. This is also partially due to the
ambiguity between these phrases in the gold stan-
dard. Finally, few parsing errors (4) are caused by
the inclusion entity parser due to the markup of false
positive inclusions (mainly boundary errors).
6 Discussion and Conclusion
This paper has argued that English inclusions in
German text is an increasingly pervasive instance
of language mixing. Starting with the hypothesis
that such inclusions can be a significant source of
errors for monolingual parsers, we found evidence
that an unmodified state-of-the-art parser for Ger-
158
...
PN-NK
NP-PNC
NE-NK
Made
PP-MNR
APPR-AD
In
NE-NK
Heaven
(a) Partial parsing output of the baseline model with a con-
stiuent boundary in the English inclusion.
...
PN-NK
FOM
Made
FOM
In
FOM
Heaven
(b) Partial parsing output of the inclusion en-
tity model with the English inclusion parsed cor-
rectly.
Figure 5: Comparing baseline model output to inclusion entity model output.
Errors No. of inclusions (in %)
Parser: baseline model, data: inclusion set
Incorrect PB and PC 39 (35.8%)
Incorrect PC 5 (4.6%)
Incorrect PB 2 (1.8%)
Correct PB and PC 63 (57.8%)
Parser: inclusion entity model, data: inclusion set
Incorrect PB and PC 6 (5.5%)
Incorrect PC 25 (22.9%)
Incorrect PB 4 (3.7%)
Correct PB and PC 74 (67.9%)
Table 6: Baseline and inclusion entity model errors
for inclusions with respect to their phrase bracketing
(PB) and phrase category (PC).
man performs substantially worse on a set of sen-
tences with English inclusions compared to a set of
length-matched sentences randomly sampled from
the same corpus. The lower performance on the
inclusion set persisted even when the parser when
given gold standard POS tags in the input.
To overcome the poor accuracy of parsing inclu-
sions, we developed two methods for interfacing the
parser with an existing annotation-free inclusion de-
tection system. The first method restricts the POS
tags for inclusions that the parser can assign to those
found in the data. The second method applies tree
transformations to ensure that inclusions are treated
as phrases. An evaluation on the TIGER corpus
shows that the second method yields a performance
gain of 4.3 in F-score over a baseline of no inclusion
detection, and even outperforms a model involving
perfect POS tagging of inclusions.
To summarize, we have shown that foreign inclu-
sions present a problem for a monolingual parser.
We also demonstrated that it is insufficient to know
where inclusions are or even what their parts of
speech are. Parsing performance only improves if
the parser also has knowledge about the structure of
the inclusions. It is particularly important to know
when adjacent foreign words are likely to be part of
the same phrase. As our error analysis showed, this
prevents cascading errors further up in the parse tree.
Finally, our results indicate that future work could
improve parsing performance for inclusions further:
we found that parsing the inclusion set is still harder
than parsing a randomly sampled test set, even for
our best-performing model. This provides an up-
per bound on the performance we can expect from
a parser that uses inclusion detection. Future work
will also involve determining the English inclusion
classifier?s merit when applied to rule-based parsing.
Acknowledgements
This research is supported by grants from the Scot-
tish Enterprise Edinburgh-Stanford Link (R36759),
ESRC, and the University of Edinburgh. We would
also like to thank Claire Grover for her comments
and feedback.
159
References
Steven Abney, Dan Flickenger, Claudia Gdaniec, Ralph
Grishman, Philip Harrison, Donald Hindle, Robert In-
gria, Frederick Jelinek, Judith Klavans, Mark Liber-
man, Mitchell P. Marcus, Salim Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. Procedure for
quantitatively comparing the syntactic coverage of En-
glish grammars. In Ezra Black, editor, HLT?91: Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 306?311, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Beatrice Alex. 2005. An unsupervised system for identi-
fying English inclusions in German text. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL 2005), Student Re-
search Workshop, pages 133?138, Ann Arbor, Michi-
gan, USA.
Beatrice Alex. 2006. Integrating language knowledge
resources to extend the English inclusion classifier to
a new language. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2006), Genoa, Italy.
Gisle Andersen. 2005. Assessing algorithms for auto-
matic extraction of Anglicisms in Norwegian texts. In
Corpus Linguistics 2005, Birmingham, UK.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories (TLT02), pages 24?41, So-
zopol, Bulgaria.
Amit Dubey. 2005a. Statistical Parsing for German:
Modeling syntactic properties and annotation differ-
ences. Ph.D. thesis, Saarland University, Germany.
Amit Dubey. 2005b. What to do when lexicalization
fails: parsing German with suffix analysis and smooth-
ing. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
2005), pages 314?321, Ann Arbor, Michigan, USA.
Jenny Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics, 6(Suppl 1):S5.
Martin Forst and Ronald M. Kaplan. 2006. The impor-
tance of precise tokenizing for deep grammars. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006), pages
369?372, Genoa, Italy.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), pages 423?430,
Saporo, Japan.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the
Seventh Conference on Natural Language Learning
(CoNLL-03), pages 180?183, Edmonton, Canada.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings
of the International Joint Conference on Artificial In-
telligence (IJCAI-95), pages 1420?1425, Montreal,
Canada.
Jean-Christophe Marcadet, Volker Fischer, and Claire
Waast-Richard. 2005. A transformation-based learn-
ing approach to language identification for mixed-
lingual text-to-speech synthesis. In Proceedings of
Interspeech 2005 - ICSLP, pages 2249?2252, Lisbon,
Portugal.
Beat Pfister and Harald Romsdorfer. 2003. Mixed-
lingual analysis for polyglot TTS synthesis. In
Proceedings of Eurospeech 2003, pages 2037?2040,
Geneva, Switzerland.
Ariel Schwartz and Marti Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Proceedings of the Pacific Sym-
posium on Biocomputing (PSB 2003), pages 451?462,
Kauai, Hawaii.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper text. In Proceedings of
the Conference on Language Resources and Evalua-
tion (LREC 1998), pages 705?712, Granada, Spain.
Boye Wangensteen. 2002. Nettbasert nyordsinnsamling.
Spra?knytt, 2:17?19.
160
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of the ACL Student Research Workshop, pages 133?138,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
An Unsupervised System for Identifying English Inclusions in German Text
Beatrice Alex
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
v1balex@inf.ed.ac.uk
Abstract
We present an unsupervised system that
exploits linguistic knowledge resources,
namely English and German lexical
databases and the World Wide Web, to
identify English inclusions in German
text. We describe experiments with this
system and the corpus which was devel-
oped for this task. We report the classifi-
cation results of our system and compare
them to the performance of a trained ma-
chine learner in a series of in- and cross-
domain experiments.
1 Introduction
The recognition of foreign words and foreign named
entities (NEs) in otherwise mono-lingual text is be-
yond the capability of many existing approaches and
is only starting to be addressed. This language mix-
ing phenomenon is prevalent in German where the
number of anglicisms has increased considerably.
We have developed an unsupervised and highly
efficient system that identifies English inclusions
in German text by means of a computationally in-
expensive lookup procedure. By unsupervised we
mean that the system does not require any anno-
tated training data and only relies on lexicons and
the Web. Our system allows linguists and lexicogra-
phers to observe language changes over time, and to
investigate the use and frequency of foreign words
in a given language and domain. The output also
represents valuable information for a number of ap-
plications, including polyglot text-to-speech (TTS)
synthesis and machine translation (MT).
We will first explain the issue of foreign inclu-
sions in German text in greater detail with exam-
ples in Section 2. Sections 3 and 4 describe the data
we used and the architecture of our system. In Sec-
tion 5, we provide an evaluation of the system out-
put and compare the results with those of a series of
in- and cross-domain machine learning experiments
outlined in Section 6. We conclude and outline fu-
ture work in Section 7.
2 Motivation
In natural language, new inclusions typically fall
into two major categories, foreign words and proper
nouns. They cause substantial problems for NLP ap-
plications because they are hard to process and infi-
nite in number. It is difficult to predict which for-
eign words will enter a language, let alne create an
exhaustive gazetteer of them. In German, there is
frequent exposure to documents containing English
expressions in business, science and technology, ad-
vertising and other sectors. A look at current head-
lines confirms the existence of this phenomenon:
(1) ?Security-Tool verhindert, dass Hacker u?ber
Google Sicherheitslu?cken finden?1
Security tool prevents hackers from finding
security holes via Google.
An automatic classifier of foreign inclusions would
prove valuable for linguists and lexicographers who
1Published in Computerwelt on 10/01/2005:
http://www.computerwelt.at
133
study this language-mixing phenomenon because
lexical resources need to be updated and reflect this
trend. As foreign inclusions carry critical content in
terms of pronunciation and semantics, their correct
recognition will also provide vital knowledge in ap-
plications such as polyglot TTS synthesis or MT.
3 Data
Our corpus is made up of a random selection of
online German newspaper articles published in the
Frankfurter Allgemeine Zeitung between 2001 and
2004 in the domains of (1) internet & telecomms,
(2) space travel and (3) European Union. These do-
mains were chosen to examine the different use and
frequency of English inclusions in German texts of
a more technological, scientific and political nature.
With approximately 16,000 tokens per domain, the
overall corpus comprises of 48,000 tokens (Table 1).
We created a manually annotated gold standard
using an annotation tool based on NITE XML (Car-
letta et al, 2003). We annotated two classes whereby
English words and abbreviations that expand to En-
glish terms were classed as ?English? (EN) and all
other tokens as ?Outside? (O).2 Table 1 presents the
number of English inclusions annotated in each gold
standard set and illustrates that English inclusions
are very sparse in the EU domain (49 tokens) but
considerably frequent in the documents in the inter-
net and space travel domains (963 and 485 tokens,
respectively). The type-token ratio (TTR) signals
that the English inclusions in the space travel data
are less diverse than those in the internet data.
Domain Tokens Types TTR
Internet Total 15919 4152 0.26
English 963 283 0.29
Space Total 16066 3938 0.25
English 485 73 0.15
EU Total 16028 4048 0.25
English 49 30 0.61
Table 1: English token and type statistics and type-
token-ratios (TTR) in the gold standard
2We did not annotate English inclusions if part of URLs
(www.stepstone.de), mixed-lingual unhyphenated compounds
(Shuttleflug) or with German inflections (Receivern) as further
morphological analysis is required to recognise them. Our aim
is to address these issues in future work.
4 System Description
Our system is a UNIX pipeline which converts
HTML documents to XML and applies a set of mod-
ules to add linguistic markup and to classify nouns
as German or English. The pipeline is composed of
a pre-processing module for tokenisation and POS-
tagging as well as a lexicon lookup and Google
lookup module for identifying English inclusions.
4.1 Pre-processing Module
In the pre-processing module, the downloaded Web
documents are firstly cleaned up using Tidy3 to
remove HTML markup and any non-textual in-
formation and then converted into XML. Subse-
quently, two rule-based grammars which we devel-
oped specifically for German are used to tokenise the
XML documents. The grammar rules are applied
with lxtransduce4, a transducer which adds or
rewrites XML markup on the basis of the rules pro-
vided. Lxtransduce is an updated version of
fsgmatch, the core program of LT TTT (Grover
et al, 2000). The tokenised text is then POS-tagged
using TnT trained on the German newspaper corpus
Negra (Brants, 2000).
4.2 Lexicon Lookup Module
For the initial lookup, we used CELEX, a lexical
database of English, German and Dutch containing
full and inflected word forms as well as correspond-
ing lemmas. CELEX lookup was only performed
for tokens which TnT tagged as nouns (NN), for-
eign material (FM) or named entities (NE) since
anglicisms representing other parts of speech are
relatively infrequent in German (Yeandle, 2001).
Tokens were looked up twice, in the German and
the English database and parts of hyphenated com-
pounds were checked individually. To identify cap-
italised English tokens, the lookup in the English
database was made case-insensitive. We also made
the lexicon lookup sensitive to POS tags to reduce
classification errors. Tokens were found either only
in the German lexicon (1), only in the English lexi-
con (2) in both (3) or in neither lexicon (4).
(1) The majority of tokens found exclusively in
3http://tidy.sourceforge.net
4http://www.ltg.ed.ac.uk/?richard/
lxtransduce.html
134
the German lexicon are actual German words. The
remaining are English words with German case in-
flection such as Computern. The word Computer
is used so frequently in German that it already ap-
pears in lexicons and dictionaries. To detect the base
language of the latter, a second lookup can be per-
formed checking whether the lemma of the token
also occurs in the English lexicon.
(2) Tokens found exclusively in the English lexi-
con such as Software or News are generally English
words and do not overlap with German lexicon en-
tries. These tokens are clear instances of foreign in-
clusions and consequently tagged as English.
(3) Tokens which are found in both lexicons are
words with the same orthographic characteristics in
both languages. These are words without inflec-
tional endings or words ending in s signalling ei-
ther the German genitive singular or the German and
English plural forms of that token, e.g. Computers.
The majority of these lexical items have the same
or similar semantics in both languages and represent
assimilated loans and cognates where the language
origin is not always immediately apparent. Only
a small subgroup of them are clearly English loan
words (e.g. Monster). Some tokens found in both
lexicons are interlingual homographs with different
semantics in the two languages, e.g. Rat (council vs.
rat). Deeper semantic analysis is required to classify
the language of such homographs which we tagged
as German by default.
(4) All tokens found in neither lexicon are submit-
ted to the Google lookup module.
4.3 Google Lookup Module
The Google lookup module exploits the World Wide
Web, a continuously expanding resource with docu-
ments in a multiplicity of languages. Although the
bulk of information available on the Web is in En-
glish, the number of texts written in languages other
than English has increased rapidly in recent years
(Crystal, 2001; Grefenstette and Nioche, 2000).
The exploitation of the Web as a linguistic cor-
pus is developing into a growing trend in compu-
tational linguistics. The sheer size of the Web and
the continuous addition of new material in different
languages make it a valuable pool of information in
terms of language in use. The Web has already been
used successfully for a series of NLP tasks such as
MT (Grefenstette, 1999), word sense disambigua-
tion (Agirre and Martinez, 2000), synonym recogni-
tion (Turney, 2001), anaphora resolution (Modjeska
et al, 2003) and determining frequencies for unseen
bi-grams (Keller and Lapata, 2003).
The Google lookup module obtains the number
of hits for two searches per token, one on German
Web pages and one on English ones, an advanced
language preference offered by Google. Each token
is classified as either German or English based on
the search that returns the higher normalised score
of the number of hits. This score is determined by
weighting the number of raw hits by the size of the
Web corpus for that language. We determine the lat-
ter following a method proposed by Grefenstette and
Niochi (2000) by using the frequencies of a series of
representative tokens within a standard corpus in a
language to determine the size of the Web corpus
for that language. We assume that a German word is
more frequently used in German text than in English
and vice versa. As illustrated in Table 2, the Ger-
man word Anbieter (provider) has a considerably
higher weighted frequency in German Web docu-
ments (DE). Conversely, the English word provider
occurs more often in English Web documents (EN).
If both searches return zero hits, the token is classi-
fied as German by default. Word queries that return
zero or a low number of hits can also be indicative
of new expressions that have entered a language.
Google lookup was only performed for the tokens
found in neither lexicon in order to keep computa-
tional cost to a minimum. Moreover, a preliminary
experiment showed that the lexicon lookup is al-
ready sufficiently accurate for tokens contained ex-
clusively in the German or English databases. Cur-
rent Google search options are also limited in that
queries cannot be treated case- or POS-sensitively.
Consequently, interlingual homographs would often
mistakenly be classified as English.
Language DE EN
Hits Raw Normalised Raw Normalised
Anbieter 3.05 0.002398 0.04 0.000014
Provider 0.98 0.000760 6.42 0.002284
Table 2: Raw counts (in million) and normalised
counts of two Google lookup examples
135
5 Evaluation of the Lookup System
We evaluated the system?s performance for all to-
kens against the gold standard. While the accuracies
in Table 3 represent the percentage of all correctly
tagged tokens, the F-scores refer to the English to-
kens and are calculated giving equal weight to preci-
sion (P) and recall (R) as  
	
	 .
The system yields relatively high F-scores of 72.4
and 73.1 for the internet and space travel data but
only a low F-score of 38.6 for the EU data. The lat-
ter is due to the sparseness of English inclusions in
that domain (Table 1). Although recall for this data
is comparable to that of the other two domains, the
number of false positives is high, causing low pre-
cision and F-score. As the system does not look up
one-character tokens, we implemented further post-
processing to classify individual characters as En-
glish if followed by a hyphen and an English inclu-
sion. This improves the F-score by 4.8 for the inter-
net data to 77.2 and by 0.6 for the space travel data to
73.7 as both data sets contain words like E-Mail or
E-Business. Post-processing does not decrease the
EU score. This indicates that domain-specific post-
processing can improve performance.
Baseline accuracies when assuming that all to-
kens are German are also listed in Table 3. As F-
scores are calculated based on the English tokens
in the gold standard, we cannot report comparable
baseline F-scores. Unsurprisingly, the baseline ac-
curacies are relatively high as most tokens in a Ger-
man text are German and the amount of foreign ma-
terial is relatively small. The added classification of
English inclusions yielded highly statistical signif-
icant improvements (p  0.001) over the baseline of
3.5% for the internet data and 1.5% for the space
travel data. When classifying English inclusions in
the EU data, accuracy decreased slightly by 0.3%.
Table 3 also shows the performance of TextCat,
an n-gram-based text categorisation algorithm of
Cavnar and Trenkle (1994). While this language
idenfication tool requires no lexicons, its F-scores
are low for all 3 domains and very poor for the EU
data. This confirms that the identification of English
inclusions is more difficult for this domain, coincid-
ing with the result of the lookup system. The low
scores also prove that such language identification is
unsuitable for token-based language classification.
Domain Method Accuracy F-score
Internet Baseline 94.0% -
Lookup 97.1% 72.4
Lookup + post 97.5% 77.2
TextCat 92.2% 31.0
Space Baseline 97.0% -
Lookup 98.5% 73.1
Lookup + post 98.5% 73.7
TextCat 93.8% 26.7
EU Baseline 99.7% -
Lookup 99.4% 38.6
Lookup + post 99.4% 38.6
TextCat 96.4% 4.7
Table 3: Lookup results (with and without post-
processing) compared to TextCat and baseline
6 Machine Learning Experiments
The recognition of foreign inclusions bears great
similarity to classification tasks such as named en-
tity recognition (NER), for which various machine
learning techniques have proved successful. We
were therefore interested in determining the perfor-
mance of a trained classifier for our task. We ex-
perimented with a conditional Markov model tagger
that performed well on language-independent NER
(Klein et al, 2003) and the identification of gene and
protein names (Finkel et al, 2005).
6.1 In-domain Experiments
We performed several 10-fold cross-validation ex-
periments with different feature sets. They are re-
ferred to as in-domain (ID) experiments as the tagger
is trained and tested on data from the same domain
(Table 4). In the first experiment (ID1), we use the
tagger?s standard feature set including words, char-
acter sub-strings, word shapes, POS-tags, abbrevi-
ations and NE tags (Finkel et al, 2005). The re-
sulting F-scores are high for the internet and space
travel data (84.3 and 91.4) but are extremely low for
the EU data (13.3) due to the sparseness of English
inclusions in that data set. ID2 involves the same
setup as ID1 but eliminating all features relying on
the POS-tags. The tagger performs similarly well
for the internet and space travel data but improves
by 8 points to an F-score of 21.3 for the EU data.
This can be attributed to the fact that the POS-tagger
136
does not perform with perfect accuracy particularly
on data containing foreign inclusions. Providing the
tagger with this information is therefore not neces-
sarily useful for this task, especially when the data
is sparse. Nevertheless, there is a big discrepancy
between the F-score for the EU data and those of the
other two data sets. ID3 and ID4 are set up as ID1
and ID2 but incorporating the output of the lookup
system as a gazetteer feature. The tagger benefits
considerably from this lookup feature and yields bet-
ter F-scores for all three domains in ID3 (internet:
90.6, space travel: 93.7, EU: 44.4).
Table 4 also compares the best F-scores produced
with the tagger?s own feature set (ID2) to the best
results of the lookup system and the baseline. While
the tagger performs much better for the internet
and the space travel data, it requires hand-annotated
training data. The lookup system, on the other hand,
is essentially unsupervised and therefore much more
portable to new domains. Given the necessary lexi-
cons, it can easily be run over new text and text in a
different language or domain without further cost.
6.2 Cross-domain Experiments
The tagger achieved surprisingly high F-scores for
the internet and space travel data, considering the
small training data set of around 700 sentences used
for each ID experiment described above. Although
both domains contain a large number of English in-
clusions, their type-token ratio amounts to 0.29 in
the internet data and 0.15 in the space travel data
(Table 1), signalling that English inclusions are fre-
quently repeated in both domains. As a result, the
likelihood of the tagger encountering an unknown
inclusion in the test data is relatively small.
To examine the tagger?s performance on a new do-
main containing more unknown inclusions, we ran
two cross-domain (CD) experiments: CD1, train-
ing on the internet and testing on the space travel
data, and CD2, training on the space travel and test-
ing on the internet data. We chose these two do-
main pairs to ensure that both the training and test
data contain a relatively large number of English in-
clusions. Table 5 shows that the F-scores for both
CD experiments are much lower than those obtained
when training and testing the tagger on documents
from the same domain. In experiment CD1, the F-
score only amounts to 54.2 while the percentage of
Domain Accuracy F-score
Internet ID1 98.4% 84.3
ID2 98.3% 84.3
ID3 98.9% 90.6
ID4 98.9% 90.8
Best Lookup 97.5% 77.2
Baseline 94.0% -
Space ID1 99.5% 91.4
ID2 99.5% 91.3
ID3 99.6% 93.7
ID4 99.6% 92.8
Best Lookup 98.5% 73.7
Baseline 97.0% -
EU ID1 99.7% 13.3
ID2 99.7% 21.3
ID3 99.8% 44.4
ID4 99.8% 44.4
Best Lookup 99.4% 38.6
Baseline 99.7% -
Table 4: Accuracies and F-scores for ID experiments
Accuracy F-score UTT
CD1 97.9% 54.2 81.9%
Best Lookup 98.5% 73.7 -
Baseline 97.0% - -
CD2 94.6% 22.2 93.9%
Best Lookup 97.5% 77.2 -
Baseline 94.0% - -
Table 5: Accuracies, F-scores and percentages of
unknown target types (UTT) for cross-domain ex-
periments compared to best lookup and baseline
unknown target types in the space travel test data is
81.9%. The F-score is even lower in the second ex-
periment at 22.2 which can be attributed to the fact
that the percentage of unknown target types in the
internet test data is higher still at 93.9%.
These results indicate that the tagger?s high per-
formance in the ID experiments is largely due to the
fact that the English inclusions in the test data are
known, i.e. the tagger learns a lexicon. It is there-
fore more complex to train a machine learning clas-
sifier to perform well on new data with more and
more new anglicisms entering German over time.
The amount of unknown tokens will increase con-
stantly unless new annotated training data is added.
137
7 Conclusions and Future Work
We have presented an unsupervised system that ex-
ploits linguistic knowledge resources including lex-
icons and the Web to classify English inclusions in
German text on different domains. Our system can
be applied to new texts and domains with little com-
putational cost and extended to new languages as
long as lexical resources are available. Its main ad-
vantage is that no annotated training data is required.
The evaluation showed that our system performs
well on non-sparse data sets. While being out-
performed by a machine learner which requires
a trained model and therefore manually annotated
data, the output of our system increases the per-
formance of the learner when incorporating this in-
formation as an additional feature. Combining sta-
tistical approaches with methods that use linguistic
knowledge resources can therefore be advantageous.
The low results obtained in the CD experiments
indicate however that the machine learner merely
learns a lexicon of the English inclusions encoun-
tered in the training data and is unable to classify
many unknown inclusions in the test data. The
Google lookup module implemented in our system
represents a first attempt to overcome this problem
as the information on the Web never remains static
and at least to some extent reflects language in use.
The current system tracks full English word
forms. In future work, we aim to extend it to iden-
tify English inclusions within mixed-lingual tokens.
These are words containing morphemes from dif-
ferent languages, e.g. English words with German
inflection (Receivern) or mixed-lingual compounds
(Shuttleflug). We will also test the hypothesis that
automatic classification of English inclusions can
improve text-to-speech synthesis quality.
Acknowledgements
Thanks go to Claire Grover and Frank Keller for
their input. This research is supported by grants
from the University of Edinburgh, Scottish Enter-
prise Edinburgh-Stanford Link (R36759) and ESRC.
References
Eneko Agirre and David Martinez. 2000. Exploring au-
tomatic word sense disambiguation with decision lists
and the Web. In Proceedings of the Semantic Annota-
tion and Intelligent Annotation workshop, COLING.
Thorsten Brants. 2000. TnT ? a statistical part-of-speech
tagger. In Proceedings of the 6th Applied Natural Lan-
guage Processing Conference.
Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan Kil-
gour, Judy Robertson, and Holgar Voormann. 2003.
The NITE XML toolkit: flexible annotation for multi-
modal language data. Behavior Research Methods, In-
struments, and Computers, 35(3):353?363.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of the 3rd
Annual Symposium on Document Analysis and Infor-
mation Retrieval.
David Crystal. 2001. Language and the Internet. Cam-
bridge University Press.
Jenny Finkel, Shipra Dingare, Christopher Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics. In press.
Gregory Grefenstette and Julien Nioche. 2000. Estima-
tion of English and non-English language use on the
WWW. In Proceedings of RIAO 2000.
Gregory Grefenstette. 1999. The WWW as a resource
for example-based machine translation tasks. In Pro-
ceedings of ASLIB?99 Translating and the Computer.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Moens Marc. 2000. LT TTT - a flexible tokenisation
tool. In Proceedings of the 2nd International Confer-
ence on Language Resources and Evaluation.
Frank Keller and Mirella Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):458?484.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings of the 7th
Conference on Natural Language Learning.
Natalia Modjeska, Katja Markert, and Malvina Nissim.
2003. Using the Web in machine learning for other-
anaphora resolution. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of the
12th European Conference on Machine Learning.
David Yeandle. 2001. Types of borrowing of Anglo-
American computing terminology in German. In
Marie C. Davies, John L. Flood, and David N. Yean-
dle, editors, Proper Words in Proper Places: Studies
in Lexicology and Lexicography in Honour of William
Jervis Jones, pages 334?360. Stuttgarter Arbeiten zur
Germanistik 400, Stuttgart, Germany.
138
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 144?151, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Investigating the Effects of Selective Sampling on the Annotation Task
Ben Hachey, Beatrice Alex and Markus Becker
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
{bhachey,v1balex,s0235256}@inf.ed.ac.uk
Abstract
We report on an active learning experi-
ment for named entity recognition in the
astronomy domain. Active learning has
been shown to reduce the amount of la-
belled data required to train a supervised
learner by selectively sampling more in-
formative data points for human annota-
tion. We inspect double annotation data
from the same domain and quantify poten-
tial problems concerning annotators? per-
formance. For data selectively sampled
according to different selection metrics,
we find lower inter-annotator agreement
and higher per token annotation times.
However, overall results confirm the util-
ity of active learning.
1 Introduction
Supervised training of named entity recognition
(NER) systems requires large amounts of manually
annotated data. However, human annotation is typ-
ically costly and time-consuming. Active learn-
ing promises to reduce this cost by requesting only
those data points for human annotation which are
highly informative. Example informativity can be
estimated by the degree of uncertainty of a single
learner as to the correct label of a data point (Cohn
et al, 1995) or in terms of the disagreement of a
committee of learners (Seung et al, 1992). Ac-
tive learning has been successfully applied to a va-
riety of tasks such as document classification (Mc-
Callum and Nigam, 1998), part-of-speech tagging
(Argamon-Engelson and Dagan, 1999), and parsing
(Thompson et al, 1999).
We employ a committee-based method where the
degree of deviation of different classifiers with re-
spect to their analysis can tell us if an example is
potentially useful. In a companion paper (Becker et
al., 2005), we present active learning experiments
for NER in radio-astronomical texts following this
approach.1 These experiments prove the utility of
selective sampling and suggest that parameters for a
new domain can be optimised in another domain for
which annotated data is already available.
However there are some provisos for active learn-
ing. An important point to consider is what effect
informative examples have on the annotators. Are
these examples more difficult? Will they affect the
annotators? performance in terms of accuracy? Will
they affect the annotators performance in terms of
time? In this paper, we explore these questions us-
ing doubly annotated data. We find that selective
sampling does have an adverse effect on annotator
accuracy and efficiency.
In section 2, we present standard active learn-
ing results showing that good performance can be
achieved using fewer examples than random sam-
pling. Then, in section 3, we address the questions
above, looking at the relationship between inter-
annotator agreement and annotation time and the ex-
amples that are selected by active learning. Finally,
section 4 presents conclusions and future work.
1Please refer to the companion paper for details of the
selective sampling approach with experimental adaptation re-
sults as well as more information about the corpus of radio-
astronomical abstracts.
144
2 Bootstrapping NER
The work reported here was carried out in order to
assess methods of porting a statistical NER system to
a new domain. We started with a NER system trained
on biomedical literature and built a new system to
identify four novel entities in abstracts from astron-
omy articles. This section introduces the Astronomy
Bootstrapping Corpus (ABC) which was developed
for the task, describes our active learning approach
to bootstrapping, and gives a brief overview of the
experiments.
2.1 The Astronomy Bootstrapping Corpus
The ABC corpus consists of abstracts of radio astro-
nomical papers from the NASA Astrophysics Data
System archive2, a digital library for physics, as-
trophysics, and instrumentation. Abstracts were ex-
tracted from the years 1997-2003 that matched the
query ?quasar AND line?. A set of 50 abstracts
from the year 2002 were annotated as seed mate-
rial and 159 abstracts from 2003 were annotated as
testing material. A further 778 abstracts from the
years 1997-2001 were provided as an unannotated
pool for bootstrapping. On average, these abstracts
contain 10 sentences with a length of 30 tokens. The
annotation marks up four entity types:
Instrument-name (IN) Names of telescopes and
other measurement instruments, e.g. Superconduct-
ing Tunnel Junction (STJ) camera, Plateau de Bure
Interferometer, Chandra, XMM-Newton Reflection
Grating Spectrometer (RGS), Hubble Space Tele-
scope.
Source-name (SN) Names of celestial objects,
e.g. NGC 7603, 3C 273, BRI 1335-0417, SDSSp
J104433.04-012502.2, PC0953+ 4749.
Source-type (ST) Types of objects, e.g. Type II Su-
pernovae (SNe II), radio-loud quasar, type 2 QSO,
starburst galaxies, low-luminosity AGNs.
Spectral-feature (SF) Features that can be
pointed to on a spectrum, e.g. Mg II emission, broad
emission lines, radio continuum emission at 1.47
GHz, CO ladder from (2-1) up to (7-6), non-LTE
line.
2http://adsabs.harvard.edu/preprint_
service.html
The seed and test data sets were annotated by two
astrophysics PhD students. In addition, they anno-
tated 1000 randomly sampled sentences from the
pool to provide a random baseline for active learn-
ing. These sentences were doubly annotated and ad-
judicated and form the basis for our calculations in
section 3.
2.2 Inter-Annotator Agreement
In order to ensure consistency in annotation projects,
corpora are often annotated by more than one an-
notator, e.g. in the annotation of the Penn Treebank
(Marcus et al, 1994). In these cases, inter-annotator
agreement is frequently reported between different
annotated versions of a corpus as an indicator for
the difficulty of the annotation task. For example,
Brants (2000) reports inter-annotator agreement in
terms of accuracy and f-score for the annotation of
the German NEGRA treebank.
Evaluation metrics for named entity recognition
are standardly reported as accuracy on the token
level, and as f-score on the phrasal level, e.g.
Sang (2002), where token level annotation refers to
the B-I-O coding scheme.3 Likewise, we will use
accuracy to report inter-annotator agreement on the
token level, and f-score for the phrase level. We
may arbitrarily assign one annotator?s data as the
gold standard, since both accuracy and f-score are
symmetric with respect to the test and gold set. To
see why this is the case, note that accuracy can sim-
ply be defined as the ratio of the number of tokens
on which the annotators agree over the total number
of tokens. Also the f-score is symmetric, since re-
call(A,B) = precision(B,A) and (balanced) f-score is
the harmonic mean of recall and precision (Brants,
2000). The pairwise f-score for the ABC corpus is
85.52 (accuracy of 97.15) with class information and
86.15 (accuracy of 97.28) without class information.
The results in later sections will be reported using
this pairwise f-score for measuring agreement.
For NER, it is also common to compare an anno-
tator?s tagged document to the final, reconciled ver-
sion of the document, e.g. Robinson et al (1999)
and Strassel et al (2003). The inter-annotator f-
score agreement calculated this way for MUC-7 and
Hub 4 was measured at 97 and 98 respectively. The
3B-X marks the beginning of a phrase of type X, I-X denotes
the continuation of an X phrase, and O a non-phrasal token.
145
doubly annotated data for the ABC corpus was re-
solved by the original annotators in the presence
of an astronomy adjudicator (senior academic staff)
and a computational linguist. This approach gives
an f-score of 91.89 (accuracy of 98.43) with class
information for the ABC corpus. Without class in-
formation, we get an f-score of 92.22 (accuracy of
98.49), indicating that most of our errors are due to
boundary problems. These numbers suggest that our
task is more difficult than the generic NER tasks from
the MUC and HUB evaluations.
Another common agreement metric is the kappa
coefficient which normalises token level accuracy
by chance, e.g. Carletta et al (1997). This met-
ric showed that the human annotators distinguish
the four categories with a reproducibility of K=.925
(N=44775, k=2; where K is the kappa coefficient,
N is the number of tokens and k is the number of
annotators).
2.3 Active Learning
We have already mentioned that there are two main
approaches in the literature to assessing the informa-
tivity of an example: the degree of uncertainty of a
single learner and the disagreement between a com-
mittee of learners. For the current work, we employ
query-by-committee (QBC). We use a conditional
Markov model (CMM) tagger (Klein et al, 2003;
Finkel et al, 2005) to train two different models on
the same data by splitting the feature set. In this sec-
tion we discuss several parameters of this approach
for the current task.
Level of annotation For the manual annotation of
named entity examples, we needed to decide on the
level of granularity. The question arises of what con-
stitutes an example that will be submitted to the an-
notators. Possible levels include the document level,
the sentence level and the token level. The most fine-
grained annotation would certainly be on the token
level. However, it seems unnatural for the annota-
tor to label individual tokens. Furthermore, our ma-
chine learning tool models sequences at the sentence
level and does not allow to mix unannotated tokens
with annotated ones. At the other extreme, one may
submit an entire document for annotation. A possi-
ble disadvantage is that a document with some inter-
esting parts may well contain large portions with re-
dundant, already known structures for which know-
ing the manual annotation may not be very useful.
In the given setting, we decided that the best granu-
larity is the sentence.
Sample Selection Metric There are a variety of
metrics that could be used to quantify the degree
of deviation between classifiers in a committee (e.g.
KL-divergence, information radius, f-measure). The
work reported here uses two sentence-level met-
rics based on KL-divergence and one based on f-
measure.
KL-divergence has been used for active learning
to quantify the disagreement of classifiers over the
probability distribution of output labels (McCallum
and Nigam, 1998; Jones et al, 2003). It measures
the divergence between two probability distributions
p and q over the same event space ?:
D(p||q) =
?
x??
p(x) log
p(x)
q(x)
(1)
KL-divergence is a non-negative metric. It is zero
for identical distributions; the more different the two
distributions, the higher the KL-divergence. Intu-
itively, a high KL-divergence score indicates an in-
formative data point. However, in the current formu-
lation, KL-divergence only relates to individual to-
kens. In order to turn this into a sentence score, we
need to combine the individual KL-divergences for
the tokens within a sentence into one single score.
We employed mean and max.
The f-complement has been suggested for active
learning in the context of NP chunking as a struc-
tural comparison between the different analyses of
a committee (Ngai and Yarowsky, 2000). It is the
pairwise f-measure comparison between the multi-
ple analyses for a given sentence:
fMcomp =
1
2
?
M,M ??M
(1? F1(M(t),M
?(t))) (2)
where F1 is the balanced f-measure of M(t) and
M ?(t), the preferred analyses of data point t accord-
ing to different members M,M ? of ensemble M.
We take the complement so that it is oriented the
same as KL-divergence with high values indicating
high disagreement. This is equivalent to taking the
inter-annotator agreement between |M| classifiers.
146
 69
 70
 71
 72
 73
 74
 75
 76
 77
 78
 79
 80
 10000  15000  20000  25000  30000  35000  40000  45000
F-
sc
or
e
Number of Tokens in Training Data
Ave KL-divergence
Random sampling
Figure 1: Learning curve of the real AL experiment.
2.4 Experiments
To tune the active learning parameters discussed
in section 2.3, we ran detailed simulated experi-
ments on the named entity data from the BioNLP
shared task of the COLING 2004 International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications (Kim et al, 2004).
These results are treated in detail in the companion
paper (Becker et al, 2005).
We used the CMM tagger to train two different
models by splitting the feature set to give multiple
views of the same data. The feature set was hand-
crafted such that it comprises different views while
empirically ensuring that performance is sufficiently
similar. On the basis of the findings of the simulation
experiments we set up the real active learning anno-
tation experiment using: average KL-divergence as
the selection metric and a feature split that divides
the full feature set roughly into features of words
and features derived from external resources. As
smaller batch sizes require more retraining iterations
and larger batch sizes increase the amount of anno-
tation necessary at each round and could lead to un-
necessary strain for the annotators, we settled on a
batch size of 50 sentences for the real AL experi-
ment as a compromise between computational cost
and work load for the annotator.
We developed an active annotation tool and ran
real annotation experiments on the astronomy ab-
stracts described in section 2.1. The tool was given
to the same astronomy PhD students for annotation
who were responsible for the seed and test data. The
learning curve for selective sampling is plotted in
figure 1.4 The randomly sampled data was dou-
bly annotated and the learning curve is averaged be-
tween the two annotators.
Comparing the selective sampling performance to
the baseline, we confirm that active learning pro-
vides a significant reduction in the number of exam-
ples that need annotating. In fact, the random curve
reaches an f-score of 76 after approximately 39000
tokens have been annotated while the selective sam-
pling curve reaches this level of performance after
only ? 24000 tokens. This represents a substantial
reduction in tokens annotated of 38.5%. In addition,
at 39000 tokens, selectively sampling offers an error
reduction of 21.4% with a 3 point improvement in
f-score.
3 Evaluating Selective Sampling
Standardly, the evaluation of active learning meth-
ods and the comparison of sample selection metrics
draws on experiments over gold-standard annotated
corpora, where a set of annotated data is at our dis-
posal, e.g. McCallum and Nigam (1998), Osborne
and Baldridge (2004). This assumes implicitly that
annotators will always produce gold-standard qual-
ity annotations, which is typically not the case, as we
discussed in Section 2.2. What is more, we speculate
that annotators might have an even higher error rate
on the supposedly more informative, but possibly
also more difficult examples. However, this would
not be reflected in the carefully annotated and veri-
fied examples of a gold standard corpus. In the fol-
lowing analysis, we leverage information from dou-
bly annotated data to explore the effects on annota-
tion of selectively sampled examples.
To evaluate the practicality and usefulness of ac-
tive learning as a generally applicable methodology,
it is desirable to be able to observe the behaviour
of the annotators. In this section, we will report on
the evaluation of various subsets of the doubly an-
notated portion of the ABC corpus comprising 1000
sentences, which we sample according to a sample
selection metric. That is, examples are added to the
subsets according to the sample selection metric, se-
lecting those with higher disagreement first. This
allows us to trace changes in inter-annotator agree-
4Learning curves reflect the performance on the test set us-
ing the full feature set.
147
ment between the full corpus and selected subsets
thereof. Also, we will inspect timing information.
This novel methodology allows us to experiment
with different sample selection metrics without hav-
ing to repeat the actual time and resource intensive
annotation.
3.1 Error Analysis
To investigate the types of classification errors, it is
common to set up a confusion matrix. One approach
is to do this at the token level. However, we are deal-
ing with phrases and our analysis should reflect that.
Thus we devised a method for constructing a confu-
sion matrix based on phrasal alignment. These con-
fusion matrices are constructed by giving a double
count for each phrase that has matching boundaries
and a single count for each phrase that does not have
matching boundaries. To illustrate, consider the fol-
lowing sentences?annotated with phrases A, B, and
C for annotator 1 on top and annotator 2 on bottom?
as sentence 1 and sentence 2 respectively:A A
BA CA
BA C
A
Sentence 1 will get a count of 2 for A/A and for
A/B and a count of 1 for O/C, while sentence 2
will get 2 counts of A/O, and 1 count each of O/A,
O/B, and O/C. Table 1 contains confusion matrices
for the first 100 sentences sorted by averaged KL-
divergence and for the full set of 1000 random sen-
tences from the pool data. (Note that these confusion
matrices contain percentages instead of raw counts
so they can be directly compared.)
We can make some interesting observations look-
ing at these phrasal confusion matrices. The main
effect we observed is the same as was suggested by
the f-score inter-annotator agreement errors in sec-
tion 2.1. Specifically, looking at the full random set
of 1000 sentences, almost all errors (Where ? is any
entity phrase type, ?/O + O/? errorsall errors = 95.43%) are
due to problems with phrase boundaries. Compar-
ing the full random set to the 100 sentences with
the highest averaged KL-divergence, we can see that
this is even more the case for the sub-set of 100 sen-
tences (97.43%). Therefore, we can observe that
100: A2
IN SN ST SF O
IN 12.0 0.0 0.0 0.0 0.4
SN 0.0 10.4 0.0 0.0 0.4
A1 ST 0.0 0.4 30.3 0.0 1.0
SF 0.0 0.0 0.0 31.1 3.9
O 0.2 0.4 2.9 6.4 ?
1000: A2
IN SN ST SF O
IN 9.4 0.0 0.0 0.0 0.3
SN 0.0 10.1 0.2 0.1 0.3
A1 ST 0.0 0.1 41.9 0.1 1.6
SF 0.0 0.0 0.1 25.1 3.0
O 0.3 0.2 2.4 4.8 ?
Table 1: Phrasal confusion matrices for document
sub-set of 100 sentences sorted by average KL-
divergence and for full random document sub-set of
1000 sentences (A1: Annotator 1, A2: Annotator 2).
Entity 100 1000
Instrument-name 12.4% 9.7%
Source-name 10.8% 10.7%
Source-type 31.7% 43.7%
Spectral-feature 35.0% 28.2%
O 9.9% 7.7%
Table 2: Normalised distributions of agreed entity
annotations.
there is a tendency for the averaged KL-divergence
selection metric to choose sentences where phrase
boundary identification is difficult.
Furthermore, comparing the confusion matrices
for 100 sentences and for the full set of 1000 shows
that sentences containing less common entity types
tend to be selected first while sentences containing
the most common entity types are dispreferred. Ta-
ble 2 contains the marginal distribution for annotator
1 (A1) from the confusion matrices for the ordered
sub-set of 100 and for the full random set of 1000
sentences. So, for example, the sorted sub-set con-
tains 12.4% Instrument-name annotations (the
least common entity type) while the full set con-
tains 9.7%. And, 31.7% of agreed entity annota-
tions in the first sub-set of 100 are Source-type
(the most common entity type), whereas the propor-
148
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0.97
 0.98
 0  5000  10000  15000  20000  25000  30000
In
te
r-a
nn
ot
at
or
 A
gr
ee
m
en
t (A
cc
)
Size (Tokens) of KL-sorted Document Subset
KL-divergence
Figure 2: Raw agreement plotted against KL-sorted
document subsets.
tion of agreed Source-type annotations in the
full random set is 43.7%. Looking at the O row, we
also observe that sentences with difficult phrases are
preferred. A similar effect can be observed in the
marginals for annotator 2.
3.2 Annotator Performance
So far, the behaviour we have observed is what you
would expect from selective sampling; there is a
marked improvement in terms of cost and error rate
reduction over random sampling. However, selec-
tive sampling raises questions of cognitive load and
the quality of annotation. In the following section
we investigate the relationship between informativ-
ity, inter-annotator agreement, and annotation time.
While reusability of selective samples for other
learning algorithms has been explored (Baldridge
and Osborne, 2004), no effort has been made to
quantify the effect of selective sampling on anno-
tator performance. We concentrate first on the ques-
tion: Are informative examples more difficult to an-
notate? One way to quantify this effect is to look
at the correlation between human agreement and the
token-level KL-divergence. The Pearson correlation
coefficient indicates the degree to which two vari-
ables are related. It ranges between ?1 and 1, where
1 means perfectly positive correlation, and ?1 per-
fectly negative correlation. A value of 0 indicates no
correlation. The Pearson correlation coefficient on
all tokens gives a very weak correlation coefficient
of ?0.009.5 However, this includes many trivial to-
5In order to make this calculation, we give token-level agree-
 0.76
 0.77
 0.78
 0.79
 0.8
 0.81
 0.82
 0.83
 0.84
 0.85
 0.86
 100  200  300  400  500  600  700  800  900  1000
In
te
r-a
nn
ot
at
or
 A
gr
ee
m
en
t (F
)
Size (Sents) of Selection Metric-sorted Subset
Ave KL-divergence
Max KL-divergence
F-complement
Figure 3: Human disagreement plotted against se-
lection metric-sorted document subsets.
kens which are easily identified as being outside an
entity phrase. If we look just at tokens that at least
one of the annotators posits as being part of an en-
tity phrase, we observe a larger effect with a Pear-
son correlation coefficient of ?0.120, indicating that
agreement tends to be low when KL-divergence is
high. Figure 2 illustrates this effect even more dra-
matically. Here we plot accuracy against token sub-
sets of size 1000, 2000, .., N where tokens are added
to the subsets according to their KL-divergence, se-
lecting those with the highest values first. This
demonstrates clearly that tokens with higher KL-
divergence have lower inter-annotator agreement.
However, as discussed in sections 2.3 and 2.4,
we decided on sentences as the preferred annota-
tion level. Therefore, it is important to explore these
relationships at the sentence level as well. Again,
we start by looking at the Pearson correlation coeffi-
cient between f-score inter-annotator agreement (as
described in section 2.1) and our active learning se-
lection metrics:
Ave KL Max KL 1-F
All Tokens ?0.090 ?0.145 ?0.143
O Removed ?0.042 ?0.092 ?0.101
Here O Removed means that sentences are removed
for which the annotators agree that there are no en-
tity phrases (i.e. all tokens are labelled as being
outside an entity phrase). This shows a relation-
ment a numeric representation by assigning 1 to tokens on
which the annotators agree and 0 to tokens on which they dis-
agree.
149
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 100  200  300  400  500  600  700  800  900  1000
Av
er
ag
e 
tim
e 
pe
r t
ok
en
Size (Sents) of Selection Metric-sorted Subset
Ave KL-divergence
Max KL-divergence
F-complement
Figure 4: Annotation time plotted against selection
metric-sorted document subsets.
ship very similar to what we observed at the token
level: a negative correlation indicating that agree-
ment is low when KL-divergence is high. Again,
the effect of selecting informative examples is better
illustrated with a plot. Figure 3 plots f-score agree-
ment against sentence subsets sorted by our sentence
level selection metrics. Lower agreement at the left
of these plots indicates that the more informative ex-
amples according to our selection metrics are more
difficult to annotate.
So, active learning makes the annotation more dif-
ficult. But, this raises a further question: What effect
do more difficult examples have on annotation time?
To investigate this, we once again start by looking
at the Pearson correlation coefficient, this time be-
tween the annotation time and our selection metrics.
However, as our sentence-level selection metrics af-
fect the length of sentences selected, we normalise
sentence-level annotation times by sentence length:
Ave KL Max KL 1-F
All Tokens 0.157 ?0.009 0.082
O Removed 0.216 ?0.007 0.106
Here we see a small positive correlations for av-
eraged KL-divergence and f-complement indicating
that sentences that score higher according to our se-
lection metrics do generally take longer to annotate.
Again, we can visualise this effect better by plotting
average time against KL-sorted subsets (Figure 4).
This demonstrates that sentences preferred by our
selection metrics generally take longer to annotate.
4 Conclusions and Future Work
We have presented active learning experiments in
a novel NER domain and investigated negative side
effects. We investigated the relationship between
informativity of an example, as determined by se-
lective sampling metrics, and inter-annotator agree-
ment. This effect has been quantified using the Pear-
son correlation coefficient and visualised using plots
that illustrate the difficulty and time-intensiveness of
examples chosen first by selective sampling. These
measurements clearly demonstrate that selectively
sampled examples are in fact more difficult to anno-
tate. And, while sentence length and entities per sen-
tence are somewhat confounding factors, we have
also shown that selective sampling of informative
examples appears to increase the time spent on in-
dividual examples.
High quality annotation is important for building
accurate models and for reusability. While anno-
tation quality suffers for selectively sampled exam-
ples, selective sampling nevertheless provided a dra-
matic cost reduction of 38.5% in a real annotation
experiment, demonstrating the utility of active learn-
ing for bootstrapping NER in a new domain.
In future work, we will perform further investi-
gations of the cost of resolving annotations for se-
lectively sampled examples. And, in related work,
we will use timing information to assess token, en-
tity and sentence cost metrics for annotation. This
should also lead to a better understanding of the re-
lationship between timing information and sentence
length for different selection metrics.
Acknowledgements
The work reported here, including the related de-
velopment of the astronomy bootstrapping corpus
and the active learning tools, were supported by
Edinburgh-Stanford Link Grant (R36759) as part of
the SEER project. We are very grateful for the time
and resources invested in corpus preparation by our
collaborators in the Institute for Astronomy, Univer-
sity of Edinburgh: Rachel Dowsett, Olivia Johnson
and Bob Mann. We are also grateful to Melissa Kro-
nenthal and Jean Carletta for help collecting data.
150
References
Shlomo Argamon-Engelson and Ido Dagan. 1999.
Committee-based sample selection for probabilistic
classifiers. Journal of Artificial Intelligence Research,
11:335?360.
Jason Baldridge and Miles Osborne. 2004. Ensemble-
based active learning for parse selection. In Pro-
ceedings of the 5th Conference of the North American
Chapter of the Association for Computational Linguis-
tics.
Markus Becker, Ben Hachey, Beatrice Alex, and Claire
Grover. 2005. Optimising selective sampling for boot-
strapping named entity recognition. In ICML-2005
Workshop on Learning with Multiple Views.
Thorsten Brants. 2000. Inter-annotator agreement for a
German newspaper corpus. In Proceedings of the 2nd
International Conference on Language Resources and
Evaluation (LREC-2000).
Jean Carletta, Amy Isard, Stephen Isard, Jacqueline C.
Kowtko, Gwyneth Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguistics,
23(1):13?31.
David. A. Cohn, Zoubin. Ghahramani, and Michael. I.
Jordan. 1995. Active learning with statistical mod-
els. In G. Tesauro, D. Touretzky, and T. Leen, editors,
Advances in Neural Information Processing Systems,
volume 7, pages 705?712. The MIT Press.
Jenny Finkel, Shipra Dingare, Christopher Manning,
Beatrice Alex Malvina Nissim, and Claire Grover.
2005. Exploring the boundaries: Gene and protein
identification in biomedical text. BMC Bioinformat-
ics. In press.
Rosie Jones, Rayid Ghani, Tom Mitchell, and Ellen
Riloff. 2003. Active learning with multiple view fea-
ture sets. In ECML 2003 Workshop on Adaptive Text
Extraction and Mining.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduc-
tion to the bio-entity recognition task at JNLPBA.
In Proceedings of the COLING 2004 International
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named entity recognition
with character-level models. In Proceedings the Sev-
enth Conference on Natural Language Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
Andrew McCallum and Kamal Nigam. 1998. Employing
EM and pool-based active learning for text classifica-
tion. In Proceedings of the 15th International Confer-
ence on Machine Learning.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics.
Patricia Robinson, Erica Brown, John Burger, Nancy
Chinchor, Aaron Douthat, Lisa Ferro, and Lynette
Hirschman. 1999. Overview: Information extraction
from broadcast news. In Proceedings DARPA Broad-
cast News Workshop.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of the
2002 Conference on Computational Natural Language
Learning.
H. Sebastian Seung, Manfred Opper, and Haim Som-
polinsky. 1992. Query by committee. In Computa-
tional Learning Theory.
Stephanie Strassel, Alexis Mitchell, and Shudong Huang.
2003. Multilingual resources for entity extraction. In
Proceedings of the ACL 2003 Workshop on Multilin-
gual and Mixed-language Named Entity Recognition.
Cynthia A. Thompson, Mary Elaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of the 16th International Conference on Ma-
chine Learning.
151
BioNLP 2007: Biological, translational, and clinical language processing, pages 65?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Recognising Nested Named Entities in Biomedical Text
Beatrice Alex, Barry Haddow and Claire Grover
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
{balex,bhaddow,grover}@inf.ed.ac.uk
Abstract
Although recent named entity (NE) annotation ef-
forts involve the markup of nested entities, there has
been limited focus on recognising such nested struc-
tures. This paper introduces and compares three
techniques for modelling and recognising nested
entities by means of a conventional sequence tag-
ger. The methods are tested and evaluated on two
biomedical data sets that contain entity nesting. All
methods yield an improvement over the baseline tag-
ger that is only trained on flat annotation.
1 Introduction
Traditionally, named entity recognition (NER) has
focussed on entities which are continuous, non-
nested and non-overlapping. In other words, each
token in the text belongs to at most one entity, and
NEs consist of a continuous sequence of tokens.
However, in some situations, it may make sense to
relax these restrictions, for example by allowing en-
tities to be nested inside other entities, or allowing
discontinuous entities. GENIA (Ohta et al, 2002)
and BioInfer (Pyysalo et al, 2007) are examples of
recently produced NE-annotated biomedical corpora
where entities nest. Corpora in other domains, for
example the ACE1 data, also contain nested entities.
This paper compares techniques for recognising
nested entities in biomedical text. The difficulty of
this task is that the standard method for convert-
ing NER to a sequence tagging problem with BIO-
encoding (Ramshaw and Marcus, 1995), where each
1http://www.nist.gov/speech/tests/ace/
index.htm
token is assigned a tag to indicate whether it is at the
beginning (B), inside (I), or outside (O) of an en-
tity, is not directly applicable when tokens belong to
more than one entity. Here we explore methods of
reducing the nested NER problem to one or more BIO
problems so that existing NER tools can be used.
This paper is organised as follows. In Section 2,
the problem of nested entities is introduced and mo-
tivated with examples from GENIA and our EPPI
(enriched protein-protein interaction) data. Related
work is reviewed in Section 3. The proposed tech-
niques enabling NER for nested NEs are explained in
Section 4. Section 5 details the experimental setup,
including descriptive statistics of the corpora and
specifics of the classifier. The results of comparing
different tagging methods are analysed in Section 6,
with a discussion and conclusion in Section 7.
2 Nested Entities
The majority of previous work on NER is conducted
using data sets annotated either with continuous,
non-nested and non-overlapping NEs or an annota-
tion scheme reduced to a flat annotation of a similar
kind in order to simplify the recognition task. How-
ever, annotated corpora often contain entities that are
nested or discontinuous. For example, the GENIA
corpus contains nested entities such as:
<RNA><DNA>CIITA</DNA>mRNA</RNA>
where the string ?CIITA? denotes a DNA and the en-
tire string ?CIITA mRNA? refers to an RNA. Such
nesting complicates the task of traditional NER sys-
tems, which generally rely on data represented with
the BIO encoding or other flat annotation variations
thereof. The majority of NER studies on corpora
65
GENIA EPPI
Count Nesting Count Nesting
3,614 ( other name ( protein t ) t ) 1,698 ( fusion ( protein t ) t ( protein t ) )
907 ( DNA ( protein t ) t ) 1,269 ( drug/compound ( protein t ) )
856 ( protein ( protein t ) t ) 455 ( fusion ( fragment t ) t ( protein t ) )
661 ( protein t ( protein t ) ) 412 ( protein ( protein t ) t )
546 ( other name ( DNA t ) t ) 361 ( complex ( protein t ) t ( protein t ) )
541 ( other name t ( other name t ) ) 298 ( fusion ( protein t ) t ( fragment t ) )
470 ( cell type t ( cell type t ) ) 246 ( fragment t ( fragment t ) )
351 ( DNA t ( DNA t ) ) 241 ( cell line t ( cell line t ) )
326 ( other name ( virus t ) t ) 207 ( fragment ( protein t ) )
262 ( other name ( lipid t ) t ) 201 ( fusion ( protein t ) t ( mutant t ) )
Table 1: 10 most frequent types of nesting in the GENIA corpus and the combined TRAIN and DE-
VTEST sections of the EPPI data (see Section 5.1), where t represents the text.
containing nested structures focus on recognising
the outermost (non-embedded) entities (e.g. Kim et
al. 2004) , as they contain the most information,
including that of embedded entities (Zhang et al,
2004). This enables a simplification of the recog-
nition task to a sequential analysis problem.
Our aim is to recognise all levels of NE nesting
occurring in two biomedical corpora: the GENIA
corpus (Version 3.02) and the EPPI corpus (see Sec-
tion 5.1). The latter data set was collected and an-
notated as part of the TXM project. Its annotation
contains 9 different biomedical entities. While the
GENIA corpus contains nested entities up to a level
of four layers of embedding, the nested entities in
the EPPI corpus only have three layers. Table 1 lists
the ten most frequent types of entity nesting occur-
ring in both corpora. In the remainder of the paper,
we differentiate between:
embedded NEs: contained in other NEs
non-embedded NEs: not contained in other NEs
containing NEs: containing other NEs
non-containing NEs: not containing other NEs
The GENIA corpus is made up of a larger per-
centage of both embedded entity (18.61%) and con-
taining entity (16.95%) mentions than the EPPI data
(12.02% and 8.27%, respectively). In both corpora,
nesting can occur in three different ways:
1. Entities containing one or more shorter embed-
ded entities. Such nesting is very frequent in both
data sets. For example, the DNA ?IL-2 promoter? in
the GENIA corpus contains the protein ?IL-2?. In
the EPPI corpus, fusions and complexes often con-
tain nested proteins, e.g. the complex ?CBP/p300?,
where ?CBP? and ?p300? are marked as proteins.
2. Entities with more than one entity type. Al-
though they occur in both data sets, they are very
rare in the GENIA corpus. For example, the string
?p21ras? is annotated both as DNA and protein. In
the EPPI data, proteins can also be annotated as
drug/compound, where it can be clearly established
that the protein is used as a drug to affect the func-
tion of an organism, cell, or biological process.
3. Coordinated entities. Coordinated NEs account
for approximately 2% of all NEs in the GENIA and
EPPI data. In the original corpora they are anno-
tated differently, but for this work they are all con-
verted to a common format.2 The outermost anno-
tation of coordinated structures and any continuous
entity mark-up within them is retained. For exam-
ple, in ?human interleukin-2 and -4? both the con-
tinuous embedded entity ?human interleukin-2? and
the entire string are marked as proteins. The markup
for discontinuous embedded entities, like ?human
interleukin-4? in the previous example, is not re-
tained, as they could be derived in a post-processing
step once nested entities are recognised.
3 Related Work
In previous work addressing nested entities, Shen et
al. (2003), Zhang et al (2004), Zhou et al (2004),
Zhou (2006), and Gu (2006) considered the GENIA
2Both corpora are represented in XML with standoff anno-
tation, potentionally allowing overlapping NEs.
66
corpus, where nested entities are relatively frequent.
All these studies ignore embedded entities occur-
ring in coordinated structures and only retain their
outermost annotation. Shen et al (2003), Zhang et
al. (2004), and Zhou et al (2004) all report on a rule-
based approach to dealing with nested NEs in the
GENIA corpus (Version 3.0) in combination with a
Hidden Markov Model (HMM) that first recognises
innermost NEs. They use four basic hand-crafted
patterns and a combination thereof to generate nest-
ing rules from the training data and thereby derive
NEs containing the innermost NEs. The experimen-
tal setup of these studies differs slightly. While Shen
et al (2003) and Zhang et al (2004) report results
testing on 4% of the abstracts in the GENIA corpus,
Zhou et al (2004) report 10-fold cross-validation
scores. Zhou (2006) applies the same rule-based
method for dealing with nested entities to the out-
put of a mutual information independence model
(MIIM) combined with a support vector machine
(SVM) plus sigmoid. His results are based on 5-fold
cross-validation on the GENIA corpus (Version 3.0).
In each of the studies, the rule-based approach to
nested entities results in an improvement of between
3.0 and 3.5 points in F1 over the baseline model.
However, as explicitly stated by Shen et al (2003)
and Zhang et al (2004), this evaluation is limited to
non-embedded (i.e. top-level and non-nested) enti-
ties. The highest overall F1-score reported for all
entities in the GENIA corpus is 71.2 (Zhou, 2006),
which again only appears to reflect the performance
on non-embedded entities.
Zhang et al (2004) also compare the rule-based
method with HMM-based cascaded recognition that
extends iteratively from the shortest to the longest
entities. Their basic HMM model is combined with
HMM models trained on transformed cascaded an-
notations. During training, embedded entity terms
are replaced by their entity type as a way of unnest-
ing the data. During testing, subsequent iterations
rely on the tagging of the first recognition pass and
are repeated until no more entities are recognised.
However, this method only results in an improve-
ment of 1.2 points in F1 over their basic classifier.
Gu (2006) reports results on recognising nested
entities in the GENIA corpus (Version 3.02) when
training an SVM-light binary classifier to recognise
either proteins or DNA. Training with the outermost
labelling yields better performance on recognising
outermost entities and, conversely, using the inner
labelling results in highest scores for recognising in-
ner entities. The best exact match F1-scores of 73.0
and 47.5 for proteins and DNA, respectively, are ob-
tained when training on data with inner labelling and
evaluating on the inner entities.
McDonald et al (2005) propose structured multil-
abel classification as opposed to sequential labelling
for dealing with nested, discontinuous, and overlap-
ping NEs. This approach uses a novel text segment
representation in preference to the BIO-encoding.
Their corpus contains MEDLINE abstracts on the
inhibition of the enzyme CYP450 (Kulick et al,
2004), specifically those abstracts that contain at
least one overlapping and one discontinuous anno-
tation. While this data does not contain nested NEs,
discontinuous and overlapping NEs make up 6% of
all NEs. The classifier performs competitively with
sequential tagging models on continuous and non-
overlapping entities for NER and noun phrase chunk-
ing. On discontinuous and overlapping NEs in the
biomedical data alone, its best performance is 56.25
F1. As the corpus does not contain nested NEs, it
would be of interest to investigate the algorithm?s
performance on the GENIA corpus.
4 Modelling Techniques
As large amounts of time and effort have been de-
voted to work on non-nested NER using the BIO-
encoding approach, it would be useful if this work
could be easily applied to nested NER. In this paper,
three different ways of addressing nested NER will
be compared: layering, cascading, and joined la-
bel tagging. All techniques aim to reduce the nested
NER problem to one or more BIO problems, so that
existing NER tools can be used. Table 2 shows an ex-
ample representation for each modelling technique
of the following two non-nested and nested entity
annotations found in a GENIA abstract:
<multi cell>mice</multi cell> . . .
<other name><RNA><protein>tumor
necrosis factor-alpha</protein>
(<protein>TNF- alpha</protein>)
messenger RNA</RNA> levels</other name>
In layering, each level of nesting is modelled as a
separate BIO problem. The output of models trained
on individual layers is combined subsequent to tag-
ging by taking the union. Layers can be created
67
Token Inside-out layering Outside-in layering
Model Layer 1 Layer 2 Layer 3 Layer 3 Layer 2 Layer 1
mice B-multi cell O O B-multi cell O O
. . . . . . . . . . . . . . . . . . . . .
tumor B-protein B-RNA B-other name B-other name B-RNA B-protein
necrosis I-protein I-RNA I-other name I-other name I-RNA I-protein
factor-alpha I-protein I-RNA I-other name I-other name I-RNA I-protein
( O I-RNA I-other name I-other name I-RNA O
TNF-alpha B-protein I-RNA I-other name I-other name I-RNA B-protein
) O I-RNA I-other name I-other name I-RNA O
messenger O I-RNA I-other name I-other name I-RNA O
RNA O I-RNA I-other name I-other name I-RNA O
levels O O I-other name I-other name O O
Cascading Joined label tagging
Model All entity types other RNA Joined labels
mice B-multi cell O O B-multi cell+O+O
. . . . . . . . . . . . . . .
tumor B-protein B-other name B-RNA B-protein+B-RNA+B-other name
necrosis I-protein I-other name I-RNA I-protein+I-RNA+I-other name
factor-alpha I-protein I-other name I-RNA I-protein+I-RNA+I-other name
( O I-other name I-RNA O+I-RNA+I-other name
TNF-alpha B-protein I-other name I-RNA B-protein+I-RNA+I-other name
) O I-other name I-RNA O+I-RNA+I-other name
messenger O I-other name I-RNA O+I-RNA+I-other name
RNA O I-other name I-RNA O+I-RNA+I-other name
levels O I-other name O O+O+I-other name
Table 2: Example representation of nested entities for various modelling techniques.
inside-out or outside-in. For inside-out layering, the
first layer is made up of all non-containing entities,
the second layer is composed of all those entities
which only contain one layer of nesting, etc. Con-
versely, outside-in layering means that the first layer
contains all non-embedded entities, the second layer
contains all entities which are only contained within
one outer entity, etc. Both directions of layering can
be modelled using a conventional NE tagger.
Cascading reduces the nested NER task to sev-
eral BIO problems by grouping one or more entity
types and training a separate model for each group.
Again, the output from individual models is com-
bined during tagging. Subsequent models in the cas-
cade may have access to the guesses of previous
ones by means of a GUESS feature. The cascaded
method is unable to recognise entities containing en-
tities of the same type, which may be a drawback for
some data sets. Cascading also raises the issue of
how to group entity types. This is dependent on the
types of entities that nest within a given data set and
would potentially require large amounts of experi-
mentation to determine the best combination. More-
over, training a model for each entity type lengthens
training time considerably, and may degrade perfor-
mance due to the dominance of the O tags for infre-
quent categories. It is possible, however, to create a
cascaded tagger combining one model trained on all
entity types with models trained on entity types that
frequently contain other entities.
Finally, joined label tagging entails creating one
tagging problem for all entities by concatenating the
BIO tags of all levels of nesting. A conventional
named entity recogniser is then trained on the data
containing the joined labels. Once the classifier has
assigned the joined labels during tagging, they are
decoded into their original BIO format for each in-
dividual entity type. Compared to the other tech-
niques, joined label tagging involves a much larger
tag set, which can increase dramatically with the
number of entity types occurring in a data set. This
can result in data sparsity which may have a detri-
mental effect on performance.
5 Experimental Setup
5.1 Corpora
GENIA (V3.02), a large publicly available biomedi-
cal corpus annotated with biomedical NEs, is widely
used in the text mining community (Cohen et al,
2005). This data set consists of 2,000 MEDLINE ab-
stracts in the domain of molecular biology (?0.5m
tokens). The annotations used for the experiments
68
reported here are based on the GENIA ontology,
published in Ohta et al (2002). It contains the fol-
lowing classes: amino acid monomer, atom, body
part, carbohydrate, cell component, cell line, cell
type, DNA, inorganic, lipid, mono-cell, multi-cell,
nucleotide, other name, other artificial source, other
organic compound, peptide, polynucleotide, protein,
RNA, tissue, and virus. In this work, protein, DNA
and RNA sub-types are collapsed to their super-type,
as done in previous studies (e.g. Zhou 2006). To the
best of our knowledge, no inter-annotator agreement
(IAA) figures on the NE-annotation in the GENIA
corpus are reported in the literature.
The EPPI corpus consists of 217 full-text papers
selected from PubMed and PubMedCentral as con-
taining protein-protein interactions (PPIs). The pa-
pers were either retrieved in XML or HTML, depend-
ing on availability, and converted to an internal XML
format. Domain experts annotated all documents
for NEs and PPIs, as well as extra (enriched) in-
formation associated with PPIs and normalisations
of entities to publicly available ontologies. The en-
tity annotations are the focus of the current work.
The types of entities annotated in this data set are:
complex, cell line, drug/compound, experimental
method, fusion, fragment, modification, mutant, and
protein. Out of the 217 papers, 125 were singly
annotated, 65 were doubly annotated, and 27 were
triply annotated. The IAA, measured by taking the
F1 score of one annotator with respect to another
when the same paper is annotated by two different
annotators, ranges from 60.40 for the entity type
mutant to 91.59 for protein, with an overall micro-
averaged F1-score of 84.87. The EPPI corpus (?2m
tokens) is divided into three sections, TRAIN (66%),
DEVTEST (17%), and TEST (17%), with TEST only
to be used for final evaluation, and not to be con-
sulted by the researchers in the development and fea-
ture optimisation phrase. The experiments described
here involve the EPPI TRAIN and DEVTEST sets.
5.2 Pre-processing
All documents are passed through a sequence of pre-
processing steps implemented using the LT-XML2
and LT-TTT2 tools (Grover et al, 2006) with the out-
put of each step encoded in XML mark-up. Tokeni-
sation and sentence splitting is followed by part-of-
speech tagging with the Maximum Entropy Markov
Model (MEMM) tagger developed by Curran and
Clark (2003) (hereafter referred to as C&C ) for
the CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003), trained on the MedPost data
(Smith et al, 2004). Information on lemmatisa-
tion, as well as abbreviations and their long forms, is
added using the morpha lemmatiser (Minnen et al,
2000) and the ExtractAbbrev script of Schwartz and
Hearst (2003), respectively. A lookup step uses on-
tological information to identify scientific and com-
mon English names of species. Finally, a rule-based
chunker marks up noun and verb groups and their
heads (Grover and Tobin, 2006).
5.3 Named Entity Tagging
The C&C tagger, referred to earlier, forms the basis
of the NER component of the TXM natural language
processing (NLP) pipeline designed to detect entity
relations and normalisations (Grover et al, 2007).
The tagger, in common with many ML approaches
to NER, reduces the entity recognition problem to
a sequence tagging problem by using the BIO en-
coding of entities. As well as performing well on
the CoNLL-2003 task, Maximum Entropy Markov
Models have also been successful on biomedical
NER tasks (Finkel et al, 2005). As the vanilla C&C
tagger (Curran and Clark, 2003) is optimised for
performance on newswire text, various modifica-
tions were applied to improve its performance for
biomedical NER. Table 3 lists the extra features
specifically designed for biomedical text. The C&C
tagger was also extended using several gazetteers,
including a protein, complex, experimental method
and modification gazetteer, targeted at recognising
entities occurring in the EPPI data. Further post-
processing specific to the EPPI data involves correct-
ing boundaries of some hyphenated proteins and fil-
tering out entities ending in punctuation.
All experiments with the C&C tagger involve 5-
fold cross-validation on all 2,000 GENIA abstracts
and the combined EPPI TRAIN and DEVTEST sets.
Cross-validation is carried out at the document level.
For simple tagging, the C&C tagger is trained on
the non-containing entities (innermost) or on the
non-embedded entities (outermost). For inside-out
and outside-in layering, a separate C&C model is
trained for each layer of entities in the data, i.e. four
models for the GENIA data and three models for
the EPPI data. Cascading is performed on individual
entities with different orderings, either ordering en-
69
Feature Description
CHARACTER Regular expressions match-
ing typical protein names
WORDSHAPE Extended version of the C&C
WORDTYPE feature
HEADWORD Head word of the current
noun phrase
ABBREVIATION Term identified as an abbre-
viation of a gazetteer term
within a document
TITLE Term seen in a noun phrase in
the document title
WORDCOUNTER Non-stop word that is among
the 10 most frequent ones in
a document
VERB Verb lemma information
added to each noun phrase
token in the sentence
FONT Text in italic and subscript
contained in the original doc-
ument format
Table 3: Extra features added to C&C .
tity models according to performance or entity fre-
quency in the training data, ranging from highest to
lowest. Cascading is also carried out on groups of
entities (e.g. one model for all entities, one for a
specific entity type, and combinations). Subsequent
models in the cascade have access to the guesses of
previous ones via a GUESS feature. Finally, joined
label tagging is done by concatenating individual
BIO tags from the innermost to the outermost layer.
As in the GENIA corpus, the most frequently an-
notated entity type in the EPPI data is protein with al-
most 55% of all annotations in the combined TRAIN
and DEVTEST data (see Table 5). Given that the
scores reported in this paper are calculated as F1
micro-averages over all categories, they are strongly
influenced by the classifier?s performance on pro-
teins. However, scoring is not limited to a particular
layer of entities (e.g. only outermost layer), but in-
cludes all levels of nesting. During scoring, a correct
match is achieved when exactly the same sequence
of text (encoded in start/end offsets) is marked with
the same entity type in the gold standard and the sys-
tem output. Precision, recall and F1 are calculated
in standard fashion from the number of true positive,
false positive and false negative NEs recognised.
6 Results
Table 4 lists overall cross-validation F1-scores cal-
culated for all NEs at all levels of nesting when ap-
plying the various modelling techniques. For GE-
NIA, cascading on individual entities when order-
ing entity models by performance yields the high-
est F1-score of 67.88. Using this method yields
an increase of 3.26 F1 over the best simple tag-
ging method, which scores 64.62 F1. Joined label
tagging results in the second best overall F1-score
of 67.82. Both layering (inside-out) and cascading
(combining a model trained on all NEs with 4 mod-
els trained on other name, DNA, protein, or RNA)
also perform competitively, reaching F1-scores of
67.62 and 67.56, respectively. In the experiments
with the EPPI corpus, cascading is also the winner
with an F1-score of 70.50 when combining a model
trained on all NEswith one trained on fusions. This
method only results in a small, yet statistically sig-
nificant (?2, p ? 0.05), increase in F1 of 0.43 over
the best simple tagging algorithm. This could be due
to the smaller number of nested NEs in the EPPI data
and the fact that this data contains many NEs with
more than one category. Layering (inside-out) per-
forms almost as well as cascading (F1=70.44).
The difference in the overall performance be-
tween the GENIA and the EPPI corpus is partially
due to the difference in the number of NEs which
C&C is required to recognise, but also due to the
fact that all features used are optimised for the EPPI
data and simply applied to the GENIA corpus. The
only feature not used for the experiments with the
GENIA corpus is FONT, as this information is not
preserved in the original XML of that corpus.
7 Discussion and Conclusion
According to the results for the modelling tech-
niques, each proposed method outperforms simple
tagging. Cascading yields the best result on the GE-
NIA (F1=67.88) and EPPI data (F1=70.50), see Ta-
ble 5 for individual entity scores. However, it in-
volves extensive amounts of experimentation to de-
termine the best model combination. The best setup
for cascading is clearly data set dependent. With
larger numbers of entity types annotated in a given
corpus, it becomes increasingly impractical to ex-
haustively test all possible orders and combinations.
Moreover, training and tagging times are lengthened
as more models are combined in the cascade.
70
GENIA V3.02 EPPI
Technique F1 Technique F1
Simple Tagging
Training on innermost entities 64.62 Training on innermost entities 70.07
Training on outermost entities 62.72 Training on outermost entities 69.18
Layering
Inside-out 67.62 Inside-out 70.44
Outside-in 67.02 Outside-in 70.21
Cascading
Individual NE models (by performance) 67.88 Individual NE models (by performance) 70.42
Individual NE models (by frequency) 67.72 Individual NE models (by frequency) 70.43
All-cell type 64.55 All-complex 70.03
All-DNA 65.02 All-drug/compound 70.08
All-other name 66.99 All-fusion 70.50
All-protein 64.77 All-protein 70.02
All-RNA 64.80 All-complex-fusion 70.46
All-other name-DNA-protein-RNA 67.56 All-drug/compound-fusion 70.50
Joined label tagging
Inside-out 67.82 Inside-out 70.37
Table 4: Cross-validation F1-scores for different modelling techniques on the GENIA and EPPI data. Scores
in italics mark statistically significant improvements (?2, p ? 0.05) over the best simple tagging score.
Despite the large number of tags involved in us-
ing joined label tagging, this method outperforms
simple tagging for both data sets and even results in
the second-best overall F1-score of 67.72 obtained
for the GENIA corpus. The fact that joined label
tagging only requires training and tagging with one
model makes this approach a viable alternative to
cascading which is far more time-consuming to run.
Inside-out layering performs competitively both
for the GENIA corpus (F1=67.62) and the EPPI cor-
pus (F1=70.37), considering how little time is in-
volved in setting up such experiments. As with
joined label tagging, minimal optimisation is re-
quired when using this method. One disadvantage
(as compared to simple, and to some extent joined
label tagging) is that training and tagging times in-
crease with the number of layers that are modelled.
In conclusion, this paper introduced and tested
three different modelling techniques for recognising
nested NEs, namely layering, cascading, and joined
label tagging. As each of them reduces nested NER
to one or more BIO-encoding problems, a conven-
tional sequence tagger can be used. It was shown
that each modelling technique outperfoms the sim-
ple tagging method for both biomedical data sets.
Future work will involve testing the proposed
techniques on other data sets containing entity nest-
ing, including the ACE data. We will also determine
their merit when applying a different learning algo-
rithm. Furthermore, possible solutions for recognis-
ing discontinuous entities will be investigated.
8 Acknowledgements
The authors are very grateful to the annotation
team, and to Cognia (http://www.cognia.
com) for their collaboration on the TXM project.
This work is supported by the Text Mining Pro-
gramme of ITI Life Sciences Scotland (http://
www.itilifesciences.com).
References
K. Bretonnel Cohen, Lynne Fox, Philip V. Ogren, and Lawrence
Hunter. 2005. Corpus design for biomedical natural
language processing. In Proceedings of the ACL-ISMB
workshop on linking biological literature, ontologies and
databases: mining biological semantics, pages 38?45.
James R. Curran and Stephen Clark. 2003. Language indepen-
dent NER using a maximum entropy tagger. In Proceedings
of CoNLL-2003, pages 164?167.
71
GENIA V3.02 EPPI
Entity type Count P R F1 Entity type Count P R F1
All 94,014 69.3 66.5 67.9 All 134,059 73.1 68.1 70.5
protein 34,813 75.1 74.9 75.0 protein 73,117 76.2 82.1 79.0
other name 20,914 60.0 67.2 63.4 expt. method 12,550 74.3 72.4 73.3
DNA 10,589 64.2 57.5 60.6 fragment 11,571 54.5 41.7 47.3
cell type 7,408 71.2 69.2 70.2 drug/compound 10,236 64.9 37.7 47.7
other org. compound 4,109 76.6 57.8 65.9 cell line 6,505 68.3 53.4 59.9
cell line 4,081 66.3 53.8 59.4 complex 6,454 62.5 32.2 42.5
lipid 2,359 76.9 65.6 70.8 modification 5,727 95.4 94.2 94.8
virus 2,133 76.0 73.4 74.7 mutant 4,025 40.7 23.2 29.6
multi-cell 1,784 72.5 60.1 65.7 fusion 3,874 56.6 36.0 44.0
Table 5: Individual counts and scores of the most frequent GENIA and all EPPI entity types for the best-
performing method: cascading.
Jenny Rose Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover. 2005.
Exploring the boundaries: Gene and protein identification in
biomedical text. BMC Bioinformatics, 6(Suppl1):S5.
Claire Grover and Richard Tobin. 2006. Rule-based chunking
and reusability. In Proceedings of LREC 2006, pages 873?
878.
Claire Grover, Michael Matthews, and Richard Tobin. 2006.
Tools to address the interdependence between tokenisation
and standoff annotation. In Proceedings of NLPXML 2006,
pages 19?26.
Claire Grover, Barry Haddow, Ewan Klein, Michael Matthews,
Leif Arda Nielsen, Richard Tobin, and Xinglong Wang.
2007. Adapting a relation extraction pipeline for the BioCre-
AtIvE II task. In Proceedings of the BioCreAtIvE Workshop
2007, Madrid, Spain.
Baohua Gu. 2006. Recognizing nested named entities in GE-
NIA corpus. In Proceedings of the BioNLP Worshop, HLT-
NAACL 2006, pages 112?113.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka
Tateisi, and Nigel Collier. 2004. Introduction to the bio-
entity recognition task at JNLPBA. In Proceedings of
JNLPBA 2004, pages 70?75.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel, Ryan
McDonald, Martha Palmer, Andrew Schein, and Lyle Un-
gar. 2004. Integrated annotation for biomedical information
extraction. In Proceedings of BioLINK 2004, pages 61?68.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In Proceedings of HLT/EMNLP 2005, pages 987?
994.
Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust,
applied morphological generation. In Proceedings of INLG
2000, pages 201?208.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi Tsujii.
2002. GENIA corpus: an annotated research abstract corpus
in molecular biology domain. In Proceedings of HLT 2002,
pages 73?77.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bjo?rne,
Jorma Boberg, Jouni Ja?rvinen, and Tapio Salakoski. 2007.
BioInfer: a corpus for information extraction in the biomed-
ical domain. BMC Bioinformatics, 8(50).
Lance Ramshaw and Mitch Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings of the 3rd
Workshop on Very Large Corpora (ACL 1995), pages 82?94.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algo-
rithm for identifying abbreviation definitions in biomedical
text. In Pacific Symposium on Biocomputing, pages 451?
462.
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-Lim
Tan. 2003. Effective adaptation of a hidden markov model-
based named entity recognizer for biomedical domain. In
Proceedings of the BioNLP Workshop, ACL 2003, pages 49?
56.
Larry Smith, Tom Rindflesch, and W. John Wilbur. 2004. Med-
Post: a part-of-speech tagger for biomedical text. Bioinfor-
matics, 20(14):2320?2321.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL-2003,
pages 142?147.
Jie Zhang, Dan Shen, Guodong Zhou, Jian Su, and Chew-Lim
Tan. 2004. Enhancing HMM-based biomedical named en-
tity recognition by studying special phenomena. Journal of
Biomedical Informatics, 37(6):411?422.
Guodong Zhou, Jie Zhang, Jian Su, Dan Shen, and ChewLim
Tan. 2004. Recognizing names in biomedical texts: a ma-
chine learning approach. Bioinformatics, 20(7):1178?1190.
Guodong Zhou. 2006. Recognizing names in biomedical
texts using mutual information independence model and svm
plus sigmoid. International Journal of Medical Informatics,
75:456?467.
72
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 333?336,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
Edinburgh-LTG: TempEval-2 System Description
Claire Grover, Richard Tobin, Beatrice Alex and Kate Byrne
University of Edinburgh
Edinburgh, United Kingdom
{grover, richard, balex, kbyrne3}@inf.ed.ac.uk
Abstract
We describe the Edinburgh information
extraction system which we are currently
adapting for analysis of newspaper text
as part of the SYNC3 project. Our most
recent focus is geospatial and temporal
grounding of entities and it has been use-
ful to participate in TempEval-2 to mea-
sure the performance of our system and to
guide further development. We took part
in Tasks A and B for English.
1 Background
The Language Technology Group (LTG) at Edin-
burgh has been active in the field of information
extraction (IE) for a number of years. Up until re-
cently our main focus has been in biomedical IE
(Alex et al, 2008) but we have also been pursuing
projects in other domains, e.g. digitised histori-
cal documents (Grover et al, 2010) and we are
currently participants in the EU-funded SYNC3
project where our role is to analyse news arti-
cles and establish spatio-temporal and other re-
lations between news events. As a step towards
this goal, we have been extending and adapting
our IE pipeline to ground spatial and temporal en-
tities. We have developed the Edinburgh Geop-
arser for georeferencing documents and have eval-
uated our system against the SpatialML corpus,
as reported in Tobin et al (2010). We are cur-
rently in the process of developing a rule-based
date and time grounding component and it is this
component that we used for Task A, which re-
quires systems to identify the extents of tempo-
ral named entities and provide their interpreta-
tion. The TempEval-2 data also contains event en-
tities and we have adapted the output of our in-
house chunker (Grover and Tobin, 2006) to iden-
tify events for Task B, which requires systems to
identify event denoting words and to compute a
range of attributes for them. In future work we will
adapt our machine-learning-based relation extrac-
tion component (Haddow, 2008) to recognise re-
lations between spatial and temporal entities and
event entities along the lines of the linking tasks.
2 The Edinburgh IE System
Our IE system is a modular pipeline system built
around the LT-XML2
1
and LT-TTT2
2
toolsets.
Documents are converted into our internal doc-
ument format and are then passed through a se-
quence of linguistic components which each add
XML mark-up. Early stages identify paragraphs,
sentences and tokens. Part-of-speech (POS) tag-
ging is done using the C&C tagger (Curran and
Clark, 2003a) and lemmatisation is done using
morpha (Minnen et al, 2000).
We use both rule-based and machine-learning
named entity recognition (NER) components, the
former implemented using LT-TTT2 and the lat-
ter using the C&C maximum entropy NER tagger
(Curran and Clark, 2003b). We are experiment-
ing to find the best combination of the two dif-
ferent NER views but this is not an issue in the
case of date and time entities since we have taken
the decision to use the rule-based output for these.
The main motivation for this decision arises from
the need to ground (provide temporal values for)
these entities and the rules for the grounding are
most naturally implemented as an elaboration of
the rules for recognition.
Our IE pipeline also uses the LT-TTT2 chun-
ker to provide a very shallow syntactic analysis.
Figure 1 shows an example of the results of pro-
cessing at the point where the rule-based NER
and chunker have both applied. As can be seen
from Figure 1, a positive feature for TempEval-
2 is that the verb group analysis provides in-
formation about tense, aspect, voice, modality
and polarity which translate relatively straightfor-
wardly into the Task B attributes. The noun group
analysis provides verbal stem information (e.g.
1
www.ltg.ed.ac.uk/software/ltxml2
2
www.ltg.ed.ac.uk/software/lt-ttt2
333
<s id="s1">
<ng>
<w p="DT" id="w13">The</w>
<w p="NN" id="w17" l="announcement" vstem="announce" headn="yes">announcement</w>
</ng>
<vg tense="pres" voice="pass" asp="simple" modal="yes" neg="yes">
<w p="MD" id="w30" pws="yes" l="must" neg="yes">must</w>
<w p="RB" id="w35" pws="yes" neg="yes">not</w>
<w p="VB" id="w39" pws="yes" l="be">be</w>
<w p="VBN" id="w42" pws="yes" l="make" headv="yes">made</w>
</vg>
<ng>
<timex unit="day" trel="same" type="date" id="rb1">
<w unit="day" trel="same" p="NN" id="w47" l="today">today</w>
</timex>
</ng>
<w p="." id="w52" sb="true">.</w>
</s>
Figure 1: Example of NER tagger and chunker output for the sentence ?The announcement must not be
made today.?
vstem="announce") about nominalisations.
Various attributes are computed for <timex>
elements and these are used by a temporal resolu-
tion component to provide a grounding for them.
The final output of the IE pipeline contains entity
mark-up in ?standoff? format where the entities
point at the word elements using ids. The date
and event entities for ?made? and ?today? are as
follows:
<ent tense="pres" voice="pass" neg="yes"
modal="yes" asp="simple" id="ev1"
subtype="make" type="event">
<parts>
<part ew="w39" sw="w39">made</part>
</parts>
</ent>
<ent wdaynum="5" day="Friday" date="16"
month="4" year="2010" unit="day"
day-number="733877" trel="same"
type="date" id="rb1">
<parts>
<part ew="w47" sw="w47">today</part>
</parts>
</ent>
The date entity has been grounded with respect
to the date of writing (16th April 2010). To do the
grounding we calculate a day-number value for
each date where the day number count starts from
1st January 1 AD. Using this unique day number
we are able to calculate the date for any given day
number as well as the day of the week. We use
the day number to perform simple arithmetic to
ground date expressions such as ?last Monday?,
?the day after tomorrow? etc. Grounding informa-
tion is spread across the attributes for day, date,
month and year. A fully grounded date has a
value for all of these while an underspecified date,
e.g. ?2009?, ?March 13th?, ?next year?, etc., only
has values for some of these attributes.
3 Adaptations for TempEval-2
Our system has been developed independently of
TimeML or TempEval-2 and there is therefore a
gap between what our system outputs and what is
contained in the TempEval-2 data. In order to run
our system over the data we needed to convert it
into our XML input format while preserving the
tokenisation decisions from the original. Certain
tokenisation mismatches required that we extend
various rules to allow for alternative token bound-
aries, for example, we tokenise ?wasn?t? as was +
n?t whereas the TempEval-2 data contains was
+ n + ?t or occasionally wasn + ?t.
Other adaptations fall broadly into two classes:
extension of our system to cover entities in
TempEval-2 that we didn?t previously recognise,
and mapping of our output to fit TempEval-2 re-
quirements.
3.1 Extensions
The date and time entities that our system recog-
nises are more like the MUC7 TIMEX entities
(Chinchor, 1998) than TIMEX3 ones. In partic-
ular, we have focused on dates which can either
be fully grounded or which, though underspeci-
fied, can be grounded to a precise range, e.g. ?last
month? can be grounded to a particular month and
year given a document creation date and it can be
precisely specified if we take it to express a range
from the first to last days of the month. TIMEX3
entities can be vaguer than this, for example, en-
tities of type DURATION such as ?twenty years?,
?some time?, etc. can be recognised as denoting
a temporal period but cannot easily be grounded.
To align our output more closely to TempEval-
2, we added NER rules to recognise examples
334
such as ?a long time?, ?recent years?, ?the past?,
?years?, ?some weeks?, ?10 minutes?. In addition
we needed to compute appropriate information to
allow us to create TempEval-2 values such as P1W
(period of 1 week).
For event recognition, our initial system created
an event entity for every head verb and for ev-
ery head noun which was a nominalisation. This
simple approach goes a long way towards captur-
ing the TempEval-2 events but results in too many
false positives and false negatives for nouns. In
addition our system did not calculate the informa-
tion needed to compute the TempEval-2 class at-
tribute. To help improve performance we added
attributes to potential event entities based on look-
up in lexicons compiled from the training data and
from WordNet (Fellbaum, 1998). These attributes
contribute to the decision as to whether a noun
or verb chunk head should be an event entity or
not
3
. The lexicons derived from the training data
contain the stems of all the nouns which acted
more than once as events as well as information
about those predicates which occurred more than
once as class ASPECTUAL, I STATE, REPORT-
ING or STATE in the training data. Where look-
up succeeds for event, if class look-up also suc-
ceeds then the class attribute is set accordingly. If
class look-up fails, the default, OCCURRENCE,
is used. The WordNet derived lexicon contains in-
formation about whether the first sense of a noun
has event or state as a hypernym. As a result of the
lexical look-up stage, the noun ?work?, for exam-
ple, is marked as having occurred in the training
data as an event and as having event as a hyper-
nym for its first sense. The conjunction of these
cause it to be considered to be an event entity. For
verbs, the only substantive change in our system
was to not consider as events all main verb uses
of ?be? (be happy), ?have? (have a meal) and ?do?
(do the dishes).
3.2 Mapping
For both timex and event entities the creation
of the extents files was a straightforward map-
ping. For the creation of the attributes files,
on the other hand, we used stylesheets to con-
struct appropriate values for the TempEval-2 at-
tributes based on the attributes in our output
XML. The construction of event attributes is not
overly complex: for example, where an event
entity is specified as tense="nonfin" and
3
Our system does not recognise adjective events. How-
ever, passive participles, which are sometimes treated as ad-
jectives in TempEval-2, are frequently treated as verbs in our
system and are therefore recognised.
voice="pass" the TempEval-2 tense attribute
is given the value PASTPART. For modality our
attribute only records whether a modal verb is
present or not, so it was necessary to set the
TempEval-2 modality attribute to the actual modal
verb inside the verb group.
For timex entities, a single value for the value
attribute had to be constructed from the values
of a set of attributes on our entity. For example,
the information in date="16", month="4"
year="2010" has to be converted to 2010-04-
16. For durations other attributes provide the rel-
evant information, for example for ?two days? the
attributes unit="day", quty="2" are used
to create the value P2D (period of 2 days).
4 Evaluation and Error Analysis
The recognition results for both timex and event
extents are shown in Table 1. For Task A (timex)
we achieved a close balance between precision and
recall, while for Task B (events) we erred towards
recall at some cost to precision.
Task Precision Recall F1
Task A 0.85 0.82 0.84
Task B 0.75 0.85 0.80
Table 1: Extent Results
For timex entities our false negatives were all
entities of the vaguest kind, for example, ?10-
hour?, ?currently?, ?third-quarter?, ?overnight?,
?the week?: these are ones which the original sys-
tem did not recognise and for which we added ex-
tra rules, though evidently we were not thorough
enough. The false positives were mostly of the
kind that would usually be a date entity but which
were not considered to be so in the key, for exam-
ple, ?1969?, ?Oct 25?, ?now?, ?the past?, ?a few
days?. In two cases the system mistakenly identi-
fied numbers as times (?1.02?, ?2.41?).
For event entities we had 73 false negatives.
Some of these were caused by verbs being
mistagged as nouns (?complies?, ?stretch?, ?suit?)
while others were nouns which didn?t occur in
the WordNet derived lexicon as events. There
were 143 event false positives. Some of these
are clearly wrong, for example, ?destruction? in
?weapons of mass destruction? while others are
a consequence of the subtle distinctions that the
TempEval-2 guidelines make and which our shal-
low approach cannot easily mimic.
Table 2 shows the results for attribute detec-
tion for both tasks. In the case of timex attributes
335
Task Attribute Score
Task A type 0.84
value 0.63
Task B polarity 0.99
pos 0.97
modality 0.99
tense 0.92
aspect 0.98
class 0.76
Table 2: Attribute Results
there was a set of entities which had systematically
wrong values for both type and value: these were
dates such as ?this week? and ?last week?. These
should have had DATE as their type and a value
such as 1998-W19 to indicate exactly which week
in which year they denote. Our date grounding
does not currently cover the numbering of weeks
in a year and so it would not have been possible
to create appropriate values. Instead we incor-
rectly treated these entities as being of type DU-
RATION with value P1W. Many of the remaining
errors were value errors where the system resolved
relative dates as past references when they should
have been future or vice versa. For example, the
value for ?Monday? in ?He and Palestinian leader
Yasser Arafat meet separately Monday with ...?
should have been 1998-05-04 but our system in-
terpreted it as the past Monday, 1998-04-27. There
were a few cases where the value was correct but
insufficient, for example for ?a year ago? the sys-
tem returned 1988 when it should have produced
1988-Q3.
Our scores for event attributes were high for all
attributes except for class. The high scoring at-
tributes were derived from the output of our chun-
ker and demonstrate the quality of this component.
There does not appear to be a particular pattern
behind the small number of errors for these at-
tributes except that errors for the pos attribute re-
flect POS tagger errors and there were some com-
bined tense and modality errors where ?will? and
?would? should have been interpreted as future
tense but were instead treated as modals. The class
attribute represents information that our system
had not previously been designed to determine.
We computed the class attribute in a relatively
minimal way. Since the class value is OCCUR-
RENCE in nearly 60% of events in the training
data, we use this as the default but, as described in
Section 3, we override this for events which are in
our training data-derived lexicon as REPORTING,
ASPECTUAL, I STATE or STATE. We do not at-
tempt to assign the I ACTION class value and
nearly half of our class errors result from this. An-
other set of errors comes from missing REPORT-
ING events such as ?alleging?, ?telegraphed? and
?acknowledged?.
Acknowledgements
The current phase of development of the Ed-
inburgh IE system is supported by the SYNC3
project (FP7-231854)
4
.
References
Beatrice Alex, Claire Grover, Barry Haddow, Mijail
Kabadjov, Ewan Klein, Michael Matthews, Richard
Tobin, and Xinglong Wang. 2008. Automating cu-
ration using a natural language processing pipeline.
Genome Biology, 9(Suppl 2).
Nancy A. Chinchor. 1998. Proceedings of the Sev-
enth Message Understanding Conference (MUC-7).
Fairfax, Virginia.
James R. Curran and Stephen Clark. 2003a. Inves-
tigating GIS and smoothing for maximum entropy
taggers. In Proceedings of the 11th Meeting of the
European Chapter of the Association for Compu-
tational Linguistics (EACL-03), pages 91?98. Bu-
dapest, Hungary.
James R. Curran and Stephen Clark. 2003b. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, Edmonton, Alberta, Canada.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Claire Grover and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proceedings of the Fifth
International Conference on Language Resources
and Evaluation (LREC 2006).
Claire Grover, Richard Tobin, Kate Byrne, Matthew
Woollard, James Reid, Stuart Dunn, and Julian Ball.
2010. Use of the Edinburgh geoparser for georefer-
encing digitised historical collections. Phil. Trans.
R. Soc. A.
Barry Haddow. 2008. Using automated feature op-
timisation to create an adaptable relation extraction
system. In Proc. of BioNLP 2008, Columbus, Ohio.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Pro-
ceedings of the 1st International Natural Language
Generation Conference, Mitzpe Ramon, Israel.
Richard Tobin, Claire Grover, Kate Byrne, James Reid,
and Jo Walsh. 2010. Evaluation of georeferencing.
In Proceedings of Workshop on Geographic Infor-
mation Retrieval (GIR?10).
4
http://www.sync3.eu/
336
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 13?21,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Bootstrapping a historical commodities lexicon with SKOS and DBpedia
Ewan Klein
ILCC, School of Informatics
University of Edinburgh
EH8 9AB, Edinburgh, UK
ewan@inf.ed.ac.uk
Beatrice Alex
ILCC, School of Informatics
University of Edinburgh
EH8 9AB, Edinburgh, UK
balex@inf.ed.ac.uk
Jim Clifford
Department of History
University of Saskatchewan
Saskatoon, SK S7N 5A5, Canada
jim.clifford@usask.ca
Abstract
Named entity recognition for novel domains
can be challenging in the absence of suitable
training materials for machine-learning or lex-
icons and gazetteers for term look-up. We de-
scribe an approach that starts from a small,
manually created word list of commodities
traded in the nineteenth century, and then uses
semantic web techniques to augment the list
by an order of magnitude, drawing on data
stored in DBpedia. This work was conducted
during the Trading Consequences project on
text mining and visualisation of historical doc-
uments for the study of global trading in the
British empire.
1 Introduction
The Trading Consequences project
1
aims to assist en-
vironmental historians in understanding the economic
and environmental consequences of commodity trad-
ing during the nineteenth century. We are applying text
mining to large quantities of historical text in order to
convert unstructured textual information into structured
data that can be queried and visualised. While prior his-
torical research into commodity flows (Cronon, 1991;
Cushman, 2013; Innis and Drache, 1995; McCook,
2006; Tully, 2009) has focused on a small number
of widely traded natural resources, the large corpora
of digitised documents processed by Trading Conse-
quences is giving historians data about a much broader
range of commodities. A detailed appraisal of trade in
these resources will yield a significantly more accurate
picture of globalisation and its environmental conse-
quences.
In this paper we focus on our approach to building a
lexicon to support the recognition of commodity terms
in text. We provide some background to this work in
Section 2. In Section 3, we describe the process of cre-
ating the lexicon; this starts from a manually collected
seed set of commodity terms which is then expanded
semi-automatically using DBpedia.
2
An evaluation of
the quality of the commodity lexicon is provided in
Section 4.
1
http://tradingconsequences.blogs.
edina.ac.uk/
2
http://www.dbpedia.org
2 Background
Figure 1 shows an overview of the architecture of the
Trading Consequences system. Input documents are
processed by the text mining pipeline, which is based
on the LT-XML2
3
and LT-TTT2
4
toolkits (Grover et al.,
2008). After initial format conversion, the text under-
Documents
Text Mining
Annotated
Documents
XML 2 RDB
Commodities 
RDB
Lexicons
& Gazetteers
Query Interface
Visualisation
Commodities 
OntologySKOS
Figure 1: Architecture of the Trading Consequences
prototype.
goes language identification and OCR post-correction
and normalisation.
5
It is then processed further by shal-
low linguistic analysis, lexicon and gazetteer lookup,
named entity recognition and grounding, and relation
extraction (see Figure 2).
In Trading Consequences, we determine which com-
modities were mentioned when and in relation to which
3
LT-XML2 includes APIs for parsing XML documents
(both as event streams and as trees), creating them, seri-
alising them and navigating them with XPath queries; see
http://www.ltg.ed.ac.uk/software/ltxml2.
4
LT-TTT2 is built around the LT-XML2 programs and pro-
vides NLP components for a variety of text processing tasks
such as tokenisation and sentence-splitting, chunking and
rule-based named entity recognition. It includes a third party
part-of-speech tagger and lemmatiser; see http://www.
ltg.ed.ac.uk/software/lt-ttt2.
5
For more details on dealing with OCR errors, see (Lo-
presti, 2008; Alex et al., 2012).
13
Figure 2: Architecture of the text mining component
locations. We also determine whether locations are
mentioned as points of origin, transit or destination and
whether vocabulary relating to diseases and disasters
appears in the text. All mined information is added
back into the XML documents as different layers of
stand-off annotation.
The annotations are subsequently used to populate a
relational database. This stores not just metadata about
the individual document, but also detailed information
that results from the text mining, such as named enti-
ties, relations, and how these are expressed in the rel-
evant document in context. Visualisations and a query
interface access the database so that users can either
search the mined information directly through textual
queries or browse the data in a more exploratory man-
ner. A temporal dimension for the visualisation is
provided by correlating commodity mentions in doc-
uments with the publication date of those documents.
All information mined from the collections is linked
back to the original documents of the data providers.
We analyse textual data from a variety of sources,
including the House of Commons Parliamentary Pa-
pers (HCPP)
6
from ProQuest;
7
the Early Canadiana On-
line data archive (ECO) from Canadian.org;
8
the Di-
rectors? Correspondence Collection from the Archives
at Kew Gardens available at Jstor Global Plants
(LETTERS);
9
Adam Matthew?s Confidential Print col-
lections (CPRINT);
10
and a subpart of the Foreign and
Commonwealth Office Collection (FCOC) from Jstor.
11
Together these sources amount to over 10 million pages
of text and over 7 billion word tokens. Table 1 provides
an overview of the number of documents and OCR scan
images per collection or sub-collection available to the
Trading Consequences consortium.
We used a variety of techniques for carrying out
named entity recognition, covering not only commodi-
ties, but also places, dates and amounts. Figure 3 shows
some of the entities which we extract from the text,
6
http://parlipapers.chadwyck.co.uk/
home.do
7
http://www.proquest.co.uk
8
http://eco.canadiana.ca
9
http://plants.jstor.org/
10
http://www.amdigital.co.uk
11
http://www.jstor.org/
Collection # of docs # of images
HCPP 118,526 6,448,739
ECO 83,016 3,938,758
LETTERS 14,340 n/a
CPRINT 1,315 140,010
FCOC 1,000 41,611
Table 1: Number of documents and images per collec-
tion. One image usually corresponds to one document
page, except in the case of CPRINT, where it mostly
corresponds to two document pages. The LETTERS col-
lection does not contain OCRed text but summaries of
hand-written letters.
e.g. the places Padang and America, the year 1871,
the commodity cassia bark and the quantity and unit
6,127 piculs. We are also able to identify that Padang
is an origin location and America is a destination loca-
tion and to ground both locations to geographical co-
ordinates. The commodity-place relations LOC(cassia
bark, Padang) and LOC(cassia bark, America), visu-
alised by the red arrows in Figure 3, are also identified.
In this paper, our focus is on commodity mentions, and
we will discuss these in more detail in the next section.
Figure 3: Excerpt from Spices (Ridley, 1912). Ex-
tracted entities are highlighted in colour and relations
are visualised using arrows.
3 Lexicon Construction
In recent years, the dominant paradigm for NER has
been supervised machine learning (Tjong Kim Sang
and De Meulder, 2003). However, to be effective, this
requires a considerable investment of effort in manu-
ally preparing suitable training data. Since we lacked
the resources to create such data, we decided instead
to provide the system with a look-up list of commodity
terms. While there is substantial continuity over time in
the materials that are globally traded as commodities,
it is difficult to work with a modern list of commod-
ity terms as they include many things that did not exist,
or were not widely traded, in the nineteenth century.
There are also a relatively large number of commodi-
ties traded in the nineteenth century that are no longer
used, including a range of materials for dyes and some
nineteenth century drugs. As a result, we set out to de-
velop a new lexicon of commodities traded in the nine-
teenth century.
Before discussing in detail the methods that we used,
it is useful to consider some of our requirements. First
14
we wanted to be able to capture the fact that there can
be multiple names for the same commodity; for exam-
ple, rubber might be referred to in several ways, includ-
ing not just rubber but also India rubber, caoutchouc
and caouchouc. Second, we wanted to include a lim-
ited amount of hierarchical structure in order to sup-
port querying, both in the database interface and also
in the visualisation process. For example, it ought
be possible to group together limes, apples and or-
anges within a common category (or hypernym) such
as Fruit. Third, we wanted the freedom to add arbi-
trary attributes to terms, such as noting that both nuts
and whales are a source of oil.
These considerations argued in favour of a frame-
work that had more structure than a simple list of
terms, but was more like a thesaurus than a dictionary
or linguistically-organised lexicon.
12
This made SKOS
(Simple Knowledge Organization System?Miles and
Bechhofer (2009)) an obvious choice for organising the
thesaurus. SKOS assumes that the ?hierarchical back-
bone? of the thesaurus is organised around concepts.
These are semantic rather than linguistic entities, and
serve as the hooks to which lexical labels are attached.
SKOS employs the Resource Description Framework
(RDF)
13
as a representation language; in particular,
SKOS concepts are identified by URIs. Every concept
has a unique ?preferred? (or canonical) lexical label (ex-
pressed by the property skos:prefLabel), plus any
number of alternative lexical labels (expressed by the
property skos:altLabel). Both of these RDF prop-
erties take string literals (with an optional language tag)
as values.
The graph in Figure 4 illustrates how SKOS al-
lows preferred and alternative lexical labels to be at-
tached to a concept such as dbp:Natural_Rubber.
Figure 4 illustrates a standard shortening for URIs,
dbp:Natural_Rubber
skos:Concept
"rubber"@en
"India rubber"@en
rdf:type
skos:prefLabel
skos:altLabel
skos:altLabel
"caoutchouc"@fr
Figure 4: Preferred and alternative lexical labels in
SKOS.
where a prefix such as dbp: is an alias for the names-
pace http://dbpedia.org/resource/. Con-
sequently dbp:Natural\_Rubber is an abbrevia-
tion that expands to the full URI http://dbpedia.
12
The Lemon lexicon model (McCrae et al., 2010) is based
on SKOS, but its richer structure, while linguistically well mo-
tivated, is more complex than we require for our application.
13
http://www.w3.org/RDF/
org/resource/Natural\_Rubber. In an anal-
ogous way, skos: and rdf: are prefixes that rep-
resent namespaces for the SKOS and RDF vocabularies
respectively.
While a SKOS thesaurus provides a rich organisa-
tional structure for representing knowledge about our
domain, it is not in itself directly usable by our text
mining tools; a further step is required to place the
prefLabel and altLabel values from the the-
saurus into the XML-based lexicon structure required
by the LT-XML2 toolkit during named entity recogni-
tion. We will discuss this in more detail in Section 3.2.
In the remainder of this section, we first describe
how we created a seed set of commodity terms man-
ually and then explain how we used it to bootstrap a
much larger commodity lexicon.
3.1 Manual Curation from Archival Sources
We took as our starting point the records of the Boards
of Customs, Excise, and Customs and Excise, and HM
Revenue and Customs held at the National Archives.
14
They include a collation of annual ledger books list-
ing all of the major goods, ranging from live animals
to works of art, imported into Great Britain during any
given year during the nineteenth century. These con-
tain a wealth of material, including a list of the quantity
and value of the commodities broken down by country.
For the purpose of developing a list of commodities,
we focused on the headings at the top of each page,
drawing on the four books of the 1866 ledgers, which
were the most detailed year available.
15
All together,
the 1866 ledgers listed 760 different import categories.
This data was manually transferred to a spreadsheet in a
manner which closely reflected the original, and a por-
tion is illustrated in Figure 5. In Trading Consequences
we restricted our analysis to raw materials or lightly
processed commodities and thereby discarded all com-
modities which did not fit this definition.
The two major steps in converting the Customs
Ledger records into a SKOS format were (i) selecting a
string to serve as the SKOS prefLabel, and (ii) asso-
ciating the prefLabel with an appropriate semantic
concept. Both these steps were carried out manually.
16
For obvious reasons, we wanted as far as possible to
use an existing ontology as a source of concepts. We
initially experimented with UMBEL,
17
an extensive up-
per ontology in SKOS format based on OpenCyc (Ma-
tuszek et al., 2006). However UMBEL?s coverage of rel-
evant plants and botanical substances was poor, lacking
14
http://discovery.nationalarchives.
gov.uk/SearchUI/details?Uri=C67
15
The customs ledgers used for creation of the seed set of
commodities is stored at The National Archives (collection
CUST 5).
16
Assem et al. (2006) present a methodology for convert-
ing thesauri to SKOS format, but the resources that their case
studies take as a starting point are considerably more exten-
sive and richly structured than the data we discuss here.
17
http://umbel.org
15
Animals Living - AssesAnimals Living - GoatsAnimals Living - KidsAnimals Living - Oxen and BullsAnimals Living - CowsAnimals Living - CalvesAnimals Living - Horses, Mares, Geldings, Colts and FoalsAnimals Living - MulesAnimals Living - SheepAnimals Living - LambsAnimals Living - Swine and HogsAnimals Living - Pigs (sucking)Animals Living - UnenmumeratedAnnatto - RollAnnatto - FlagAntimony - Ore ofAntimony - CrudeAntimony - RegulusApples - RawApples - DriedAqua Fortis - Nitric Acid
Figure 5: Sample spreadsheet entries derived from
1866 Customs Ledger.
for instance entries for alizarin, bergamot andDammar
gum, amongst many others. We eventually decided in-
stead to base the ontology component of the lexicon
on DBpedia (Bizer et al., 2009; Mendes et al., 2012),
a structured knowledge base whose core concepts cor-
respond to Wikipedia pages, augmented by Wikipedia
categories, page links and infobox fields, all of which
are extracted as RDF triples.
Figure 6 illustrates a portion of the converted spread-
sheet, with columns corresponding to the DBpedia con-
cept (using dbp: as the URI prefix), the prefLabel,
and a list of altLabels. Note that asses has been
normalised to a singular form and that it occurs as an
altLabel for the concept dbp:Donkey. This dataConcept prefLabel altLabeldbp:Cork_(material) corkdbp:Cornmeal cornmeal indian44corn4meal,4corn4mealdbp:Cotton cotton cotton4fiberdbp:Cotton_seed cotton4seeddbp:Cowry cowry cowriedbp:Coypu coypu nutria,4river4ratdbp:Cranberry cranberrydbp:Croton_cascarilla croton4cascarilla cascarilladbp:Croton_oil croton4oildbp:Cubeb cubeb cubib,4Java4pepperdbp:Culm culmdbp:Dammar_gum dammar4gum gum4dammardbp:Deer deerdbp:Dipsacus dipsacus 4teaseldbp:Domestic_sheep domestic4sheepdbp:Donkey donkey assdbp:Dracaena_cinnabari dracaena4cinnabari sanguis4draconis,4gum4dragon's4blood
Figure 6: Customs Ledger data converted to SKOS data
types.
(in the form of a CSV file)
18
provides enough informa-
18
Together with other resources from Trading Conse-
quences, the word list is available as base_lexicon.csv
from the Github repository https://github.com/
digtrade/digtrade.
tion to build a rudimentary SKOS thesaurus whose root
concept is tc:Commodity.
19
The following listing
illustrates a portion of the thesaurus for donkey.
20
dbp:Donkey
a skos:Concept ;
skos:prefLabel "donkey"@en ;
skos:altLabel "ass"@en ;
skos:broader tc:Commodity ;
prov:hadPrimarySource
"customs records 1866" .
Translated into plain English, this says: dbp:Donkey
is a skos:Concept, its preferred label is
"donkey", its alternative label is "ass", it has
a broader concept tc:Commodity, and the primary
source of this information (i.e., its provenance) are the
customs records of 1866. Once we have an RDF model
of the thesaurus, it becomes straightforward to carry
out most subsequent processing via query, construct
and update operations in SPARQL (Prud?Hommeaux
and Seaborne, 2008; Seaborne and Harris, 2013), the
standard language for querying RDF data.
3.2 Bootstrapping the Lexicon
The process just described allows us to construct a
small ?base? SKOS thesaurus containing 319 concepts.
However it is obviously a very incomplete list of com-
modities, and by itself would give us poor recall in
identifying commodity mentions. Many kinds of prod-
uct in the Customs Ledgers included open ended sub-
categories (i.e., Oil - Seed Unenumerated or Fruit - Un-
enumerated Dried). Similarly, while the ledgers pro-
vided a comprehensive list of various gums, they only
specified anchovies, cod, eels, herrings, salmon and
turtle as types of fish, grouping all other species under
the ?unenumerated? subcategory.
One approach to augmenting the thesaurus would be
to integrate it with a more general purpose SKOS upper
ontology. In principle, this should be feasible, since
merging two RDF graphs is a standard operation. How-
ever, trying this approach with UMBEL threw up several
practical problems. First, UMBEL includes features that
go beyond the standard framework of SKOS and which
made graph merging harder to control. Second, this
technique made it extremely difficult to avoid adding a
large amount of information that was irrelevant to the
domain of nineteenth century commodities.
Our second approach also involved graph merging,
but tried to minimise manual intervention in determin-
ing which subparts of the general ontology to merge
into. We have already mentioned that one of our orig-
inal motivations for adopting SKOS was the presence
of a concept hierarchy; nevertheless, we had little need
for a multi-layered hierarchy of the kind found in many
19
The conversion from CSV to RDF was carried out with the
help of the Python rdflib library (https://rdflib.
readthedocs.org).
20
The prefixes tc: and prov: are aliases for http://
vocab.inf.ed.ac.uk/tc/ and http://www.w3.
org/ns/prov\# respectively.
16
upper ontologies. In addition to a class hierarchy of the
usual kind, DBpedia contains a level of category, de-
rived from the categories that are used to tag Wikipedia
pages. Figure 7 illustrates categories, such as Domes-
ticated animals, that occur on the page for donkey. We
believe that such Wikipedia categories provide a useful
and (for our purposes) sufficient level of abstraction for
grouping together the ?leaf? concepts that correspond
to lexical items in the SKOS thesaurus (e.g., a concept
like dbp:Donkey). Within DBpedia, these categories
are contained in the namespace http://dbpedia.
org/resource/Category: (for which we use the
alias dbc:) and are related to concepts via the prop-
erty dcterms:subject. Given that the concepts in
Figure 7: Wikipedia categories at the bottom of the
page for Donkey.
our base SKOS thesaurus are drawn from DBpedia, it is
simple to augment the initial SKOS thesaurus G in the
following way: for each leaf concept L in G, augment
G with a new triple of the form ?L skos:broader
C? (i.e., L has broader concept C) whenever L be-
longs to category C in DBpedia. To illustrate, given
our Donkey example above, we would supplement it
with the following triple:
dbp:Donkey
skos:broader dbc:Domesticated_animal
We can retrieve all of the categories associated with
each leaf concept by sending a federated query that ac-
cesses both the DBpedia SPARQL endpoint and a local
instance of the Jena Fuseki
21
server which hosts our
SKOS thesaurus. Since some of the categories recov-
ered in this way were clearly too broad or out of scope,
we manually filtered the list down to a set of 355 cate-
gories before merging the new triples into the base the-
saurus.
Our next step also involved querying DBpedia, this
time to retrieve all new concepts C which belonged to
the categories recovered in the first step; we call this
sibling acquisition, since it allows us to find siblings of
leaf concepts that are children of the Wikipedia cate-
gories already present in the thesaurus. The key steps
in the procedure are illustrated in Figure 8 (where the
top node is the root concept in the SKOS thesaurus,
viz. tc:Commodity). To continue our earlier exam-
ple, the presence of dbc:Domesticated_animal
in the hierarchy triggers the addition of concepts for
animals such as camel, llama and water buffalo. Given
a base thesaurus with 319 concepts, sibling acquisition
21
http://jena.apache.org/documentation/
serving_data/
base thesaurus category acquisition sibling acquisition
Figure 8: Sibling acquisition. A base thesaurus is aug-
mented with new categories (indicated as black ovals),
and these in turn lead to the addition of new leaf
concepts (indicated as black circles) which they are
broader than.
expands the thesaurus to a size of 17,387 concepts.
22
This query-based methodology contrasts with, though
is potentially complementary to, a machine learning
approach to bootstrapping named entity systems as de-
scribed, for example, by Kozareva (2006).
We mentioned earlier that in order for LT-TTT2 to
identify commodity mentions in text, it is necessary to
convert our SKOS thesaurus into an XML-based lexi-
con structure. A fragment of such a lexicon is illus-
trated in Figure 9. The preferred and alternative lexical
labels are represented via separate entries in the lex-
icon, with their value contained in the word attribute
for each entry. The concept and category information is
stored in corresponding attribute values; the pipe sym-
bol (|) is used to separate multiple categories. We have
already seen that alternative lexical labels will include
synonyms and spelling variants (e.g., chinchona ver-
sus cinchona). The set of alternative labels associated
with each concept was further augmented by a series of
postprocessing steps such as pluralisation; hyphenation
and dehyphenation (cocoa nuts versus cocoa-nuts ver-
sus cocoanuts; and the addition of selected head nouns
to form compounds (apple > apple tree, groundnut >
groundnut oil). Such variants are also stored in the lexi-
con as separate entries. The resulting lexicon contained
20,476 commodity terms.
During the recognition step, we perform case-
insensitive matching against the lexicon in combination
with context-dependent rules to decide whether or not
a given string is a commodity; the longest match is pre-
ferred during lookup. Linguistic pre-processing is im-
portant in this step ? for example, we exclude word
tokens tagged as verb, preposition, particle or adverb
in the part-of-speech tagging. As each lexicon entry
is associated with a DBpedia concept and at least one
category, both types of information are added to the
extracted entity mentions for each successful match,
thereby linking the text-mined commodities to the hier-
archy present in the Trading Consequences commodity
thesaurus.
22
We accessed DBpedia via the SPARQL endpoint on 16
Dec 2013, which corresponds to DBpedia version 3.9.
17
<lex>
...
<lex category="Rubber|Nonwoven_fabrics" concept="Natural_rubber" word="caoutchouc"/>
<lex category="Rubber|Nonwoven_fabrics" concept="Natural_rubber" word="indian rubber"/>
<lex category="Rubber|Nonwoven_fabrics" concept="Natural_rubber" word="rubber"/>
...
</lex>
Figure 9: Lexicon entries for the example presented in Figure 4.
4 Evaluation
4.1 Methodology
The quality of text mining software is often evaluated
intrinsically in terms of the precision, recall and bal-
anced F-score of its output compared to a human anno-
tated gold standard. We also use this methodology to
gain a better understanding of the quality of the com-
modity lexicon. We therefore prepared a gold stan-
dard by randomly selecting 25 documents extracts from
each of the five collections listed in Table 1. Since
many of the documents were too long to annotate in
their entirety, we split each file into sub-sections of
equal size (5000 bytes) and randomly selected one sub-
section per document containing one or more com-
modities and commodity-location relations. This re-
sulted in a set of 125 files which we divided into a pilot
set of 25 documents (5 per collection) and a main an-
notation set of 100 documents (20 per collection).
Annotator 1 was provided with guidelines on mark-
ing up entities and relations, and was asked to annotate
the 25 pilot documents using the BRAT annotation tool
(Stenetorp et al., 2012).
23
After an opportunity to clar-
ify any issues, Annotator 1 carried out the main anno-
tation by correcting the system output and adding any
information that was missed by the text mining compo-
nent. We refer to the resulting human-annotated dataset
as the gold standard and compare our system output
against it. Table 2 shows that relative to our gold stan-
dard annotations, the text mining prototype, which uses
the expanded commodity lexicon described in Section
3.2), identified commodity mentions with a precision
(P) of 0.59, a recall (R) of 0.56 and an F-score of 0.57.
These scores are determined with a strict evaluation
where each commodity mention identified by the sys-
tem has to match the manually annotated mention ex-
actly in terms of its boundaries and type to count as
a true positive. As soon as one boundary differs ?
for example, if the annotator identified palm and the
system identified palm trees ?- the mis-match counts
as both a false positive and a false negative. In order
to understand how often the commodity extraction re-
sults in a boundary error, we also applied a lax evalua-
tion where a true positive is counted if both boundaries
match exactly; or if the left boundary differs and the
right matches; or if the left boundary matches and the
23
The pilot data is not included in the gold standard that is
used for the evaluation.
right differs. The improved scores for the lax evalua-
tion listed in Table 2 show that boundary errors signif-
icantly impact on system performance, with an equally
negative effect on recall and precision.
Table 2 also gives inter-annotator agreement (IAA)
scores for 25% of the gold standard. IAA was calcu-
lated by comparing the markup of Annotator 1 with
a second annotator (Annotator 2) for the same data.
The strict and lax scores show that IAA is not par-
ticularly high (F=0.72 and F=0.80) for a task that we
expected to be fairly easy and that boundary errors
are also one of the reasons for the disagreement, al-
beit not to such a large extent as in the system evalu-
ation. After having carried out some error analysis of
the double-annotation, we realised that Annotator 2 had
not completely understood our definition of commodity
and had mistakenly included machinery and tools (e.g.,
scissors) as well as general terms related to commodi-
ties (e.g., produce). Annotator 2 also missed several
relevant commodity mentions which Annotator 1 had
correctly identified. For these reasons, Annotator 2?s
markup was ignored when evaluating the text mining
output.
4.2 Analysis and Lexicon Modification
When examining the output of the text mining pro-
totype, we found that it had identified a total of
31,169,104 commodity mentions (tokens) across all
five collections. However, these corresponded to only
5,841 different commodity terms (types). Since the
Trading Consequences thesaurus contains 20,476 com-
modity terms, only 28.5% of the content in the lexicon
corresponds to identifiable commodity mentions in the
text. The top 1,757 most frequent commodity terms
occur at least 100 times in our data; they make up a to-
tal of 31,113,978 commodity mentions in the text and
therefore amount to 99.8% of all commodity mentions
found. Figure 10 presents the average frequency dis-
tribution of different commodity terms (separated into
bins) across all text collections.
The difference between the strict and lax bound-
ary evaluations described above provide evidence that
some of the commodity mentions in text were sub-
strings of commodity terms in the lexicon (e.g., seal
vs. sealskins) and vice versa. A detailed error analysis
showed that incorrect and missing entries in the lexicon
further decrease precision and recall, respectively, and
OCR errors occurring in the commodity terms in the
18
Evaluation TP FP FN P R F-score
Text Mining Strict 616 431 491 0.59 0.56 0.57
Prototype Lax boundaries 791 256 316 0.76 0.71 0.73
IAA Strict 283 112 109 0.72 0.72 0.72
Lax boundaries 314 81 80 0.78 0.80 0.80
Table 2: Precision (P), recall (R) and F-score figures for evaluating the performance of the commodity recog-
nition prototype, as well as numbers of true positive (TP), false positive (FP) and false negative (FN) mentions.
These figures are compared against equivalent inter-annotator agreement (IAA) scores in 25% of the gold standard
documents. We provide evaluation scores for strict and lax boundary matching of entity mentions.
Evaluation TP FP FN P R F-score
Text Mining Prototype Strict 616 431 491 0.59 0.56 0.57
Lax 791 256 316 0.76 0.71 0.73
(i) Removal of lexicon errors Strict 603 331 504 0.65 0.54 0.59
Lax 765 169 342 0.82 0.69 0.75
(ii) Context Rules Strict 664 483 443 0.58 0.60 0.59
Lax 777 370 330 0.68 0.70 0.69
(iii) Bigram-based additions Strict 673 441 434 0.60 0.61 0.61
Lax 855 259 252 0.77 0.77 0.77
Modified Lexicon: Strict 652 353 455 0.65 0.59 0.62
combination of (i)?(iii) Lax 792 213 315 0.79 0.72 0.75
Table 3: Precision (P), recall (R) and F-score figures for evaluating the performance of the commodity recognition
prototype compared to the same scores for two optimisation steps. We provide evaluation scores for strict and lax
boundary matching of entity mentions.
text also considerably reduce recall (Alex and Burns,
to appear). In our gold standard, 9.1% (101 of 1,107)
of all manually annotated commodity mentions contain
one or more OCR errors. In order to improve the accu-
racy of the lexicon, we carried out three modifications,
which are described below.
Step (i): Removal of errors from lexicon All com-
modity terms below that of rank 1,757 (in bin 1,701?
1,800 and subsequent bins) have a frequency of less
than 100. In Trading Consequences we are particularly
interested in frequently occurring commodities as we
aim to identify trends in trade. Consequently one of
the authors of this paper (an environmental historian)
manually checked the correctness of the top 1,757 com-
modity terms. 84 of them (4.8%) were considered to be
errors (either real errors, OCR errors, commodities out-
side our scope, or overly-ambiguous terms) and were
therefore deleted from the lexicon.
We then tested the effect this change had on the per-
formance for against the gold standard. The scores in
Table 3 show that step (i), deleting incorrect entries
from the lexicon, has an expected positive effect on
precision, which increased by 0.06 (to P=0.65). It also
resulted in a small decrease in recall since Annotator
1 had marked several instances of the word bread as
commodity mentions, which is arguably at the bound-
ary of our definition of ?natural resources or lightly
processed commodities?. He had also annotated pa-
per and linen as commodity mentions, which are not
within our definition. Eliminating incorrect terms from
lexicon does not reduce the number of boundary er-
rors made by the prototype, and consequently the lax
boundary evaluation still results in an increase of 0.16
in F-score compared to the strict evaluation (F=0.59
versus F=0.75), the same as is the case for the proto-
type.
Step (ii): Context rules Having examined the
boundary errors made by the prototype, we also applied
rules to extend commodity mentions to the left or right
in certain contexts. We shift a boundary to the left if a
recognised commodity mention is preceded by a noun
or proper noun starting with an uppercase letter or if
it is preceded by another commodity mention. This
boundary shift is carried out to capture noun phrases
in which the recognised commodity mention is a head
noun which is then specified further by its immediate
left context (e.g., coffee is extended to Liberica coffee
or oil is combined with coconut to yield coconut oil).
We shift a boundary to the right in the case where a
recognised commodity is followed by the word tree or
trees (e.g., palm trees). We tested the effect of apply-
ing these context rules to the prototype (see step (ii)
in Table 3). While this post-processing step decreases
precision very slightly, recall increases by 0.4.
19
Figure 10: Average frequency distribution of different
commodity terms split into bins of size 100. The Trad-
ing Consequences data contains a total of 5,841 differ-
ent commodity terms. The graph is capped at the most
frequent 2,000 terms as it would otherwise show a long
invisible tail of very low average frequencies.
Step (iii): Bigram-based additions Finally, we con-
ducted a frequency-based bigram analysis for a set of
trade-related terms like import, export, farm, planta-
tion of the text-mined collections (see an example in
Figure 11). We manually examined frequently occur-
ring left and right contexts of such words with the aim
of identifying a list of terms for commodities of im-
portance in the nineteenth century but which were not
already contained in the lexicon and were therefore
missed by the text mining. We identified a list of 294
commodity terms (including plural forms and spelling
variants) which we added to the lexicon. Step (iii) in
Table 3 shows that this change increases recall by 0.05
and precision by 0.01. When combining steps (i)?(iii),
we obtain the highest overall F-score of 0.62 with the
strict evaluation.
5 Conclusion
In many named entity recognition tasks, there is rea-
sonable agreement in advance about the ontological
scope of a given class. For example, when identify-
ing mentions of people, locations, companies or dates
in a corpus, we are not in doubt as to what consti-
tutes these classes. By contrast, in the Trading Conse-
quences project, our goal was precisely to gain a better
understanding of what counted as a traded commod-
ity during the nineteenth century. In other words, we
were not only bootstrapping a lexicon, but were also
trying to bootstrap the ontological class ?commodity?
that was true for a specific time period. Given a small
number of clear cases extracted from customs records,
we used the categorial similarity of other entities to our
Figure 11: Most frequent tokens followed by the word
export or exports found in the text-mined output of the
HCPP data. This list excludes all occurrences where the
left context is already recognised as a commodity. The
commodities grain and wine have been marked by an
expert historian as commodities that are missing from
the lexicon.
seed set as means of extrapolating to a much larger set
of candidate commodities. However, it is only when
these candidates can be found as mentions in our cor-
pus that we gain confidence in the belief that we really
have identified new commodities. From the perspective
of historical inquiry, progressing from around a dozen
or so well-studied commodities in nineteenth century
trade to around 2,000 is a significant step forward.
The process of sibling acquisition via SPARQL query
to DBpedia is a novel contribution, as far as we are
aware, and we have argued that it can help to gener-
ate a lexicon which can be used as part of standard
techniques in natural language processing. Although
computational linguists are still relatively unfamiliar
with RDF as a data model, we believe that its flexibility
make it well suited to capturing the combination of lex-
ical and encyclopaedic knowledge that is central to the
digital history research described here. In addition, by
basing our concepts on DBpedia, the ?linking? aspect of
Linked Data (Heath and Bizer, 2011) gives us the po-
tential to connect our commodity thesaurus to a wealth
of other sources of knowledge about commodities.
Acknowledgments
The Trading Consequences project is a Digging Into
Data II project (CIINN01) funded by Jisc, AHRC and
SSHRC. It is a collaboration between the School of
Informatics at the University of Edinburgh, EDINA,
SACHI at the University of St. Andrews, together with
York University and the University of Saskatchewan in
Canada. We are grateful to Kate Byrne and to the three
reviewers for their insightful comments on the paper.
20
References
Beatrice Alex and John Burns. to appear. Estimating
and rating the quality of optically character recog-
nised text. In Proceedings of DATeCH 2014.
Bea Alex, Claire Grover, Ewan Klein, and Richard To-
bin. 2012. Digitised historical text: Does it have to
be mediOCRe? In Proceedings of the LThist 2012
workshop at KONVENS 2012, pages 401?409.
Mark Assem, V?ronique Malais?, Alistair Miles, and
Guus Schreiber. 2006. A method to convert thesauri
to SKOS. In York Sure and John Domingue, edi-
tors, The Semantic Web: Research and Applications,
volume 4011 of Lecture Notes in Computer Science,
pages 95?109. Springer Berlin Heidelberg.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?ren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. Dbpedia ? a crys-
tallization point for the web of data. Web Semantics,
7(3):154?165, September.
William Cronon. 1991. Natures Metropolis: Chicago
and the Great West. W. W. Norton, New York.
Gregory T Cushman. 2013. Guano and the Opening
of the Pacific World: A Global Ecological History.
Cambridge University Press, Cambridge.
Claire Grover, Sharon Givon, Richard Tobin, and Ju-
lian Ball. 2008. Named entity recognition for digi-
tised historical texts. In Proceedings of the Sixth
International Conference on Language Resources
and Evaluation (LREC?08), pages 1343?1346, Mar-
rakech, Morocco.
Tom Heath and Christian Bizer. 2011. Linked Data:
Evolving the Web into a Global Data Space. Syn-
thesis Lectures on the Semantic Web: Theory and
Technology. Morgan & Claypool.
Harold Innis and Daniel Drache. 1995. Staples, Mar-
kets, and Cultural Change Selected Essay. McGill-
Queens University Press, Montreal.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In Proceedings of the Eleventh Conference of
the European Chapter of the Association for Com-
putational Linguistics: Student Research Workshop,
EACL ?06, pages 15?21, Stroudsburg, PA, USA.
Daniel Lopresti. 2008. Optical character recognition
errors and their effects on natural language process-
ing. In Proceedings of the second workshop on Ana-
lytics for Noisy Unstructured Text Data, pages 9?16.
Cynthia Matuszek, John Cabral, Michael J Witbrock,
and John DeOliveira. 2006. An introduction to the
syntax and content of Cyc. In AAAI Spring Sym-
posium: Formalizing and Compiling Background
Knowledge and Its Applications to Knowledge Rep-
resentation and Question Answering, pages 44?49.
Stuart McCook. 2006. Global rust belt: Hemileia Vas-
tatrix and the ecological integration of world coffee
production since 1850. Journal of Global History,
1(2):177?195.
John McCrae, Guadalupe Aguado de Cea, Paul Buite-
laar, Philipp Cimiano, Thierry Declerck, Asun-
ci?n G?mez P?rez, Jorge Gracia, Laura Hollink,
Elena Montiel-Ponsoda, Dennis Spohr, and To-
bias Wunner, 2010. The Lemon Cookbook. The
Monnet Project. http://lemon-model.net/
lemon-cookbook.pdf.
P.N. Mendes, M. Jakob, and C. Bizer. 2012. DBpe-
dia: A multilingual cross-domain knowledge base.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC 2012).
Alistair Miles and Sean Bechhofer. 2009. SKOS
simple knowledge organization system ref-
erence. W3C recommendation, W3C, Au-
gust. http://www.w3.org/TR/2009/
REC-skos-reference-20090818/.
E. Prud?Hommeaux and A. Seaborne. 2008. Sparql
query language for rdf. W3C working draft, 4(Jan-
uary).
Henry Nicholas Ridley. 1912. Spices. London,
Macmillan and co. Ltd.
Andy Seaborne and Steven Harris. 2013. SPARQL
1.1 query language. W3C recommendation,
W3C, March. http://www.w3.org/TR/
2013/REC-sparql11-query-20130321/.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi
?
c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. BRAT: A web-based tool for NLP-
assisted text annotation. In Proceedings of the
Demonstrations at the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, EACL ?12, pages 102?107, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the Seventh Conference on Natural
Language Learning, CONLL ?03, pages 142?147,
Stroudsburg, PA, USA.
John Tully. 2009. A victorian ecological disaster: Im-
perialism, the telegraph, and gutta-percha. Journal
of World History, 20(4):559?579.
21
LAW VIII - The 8th Linguistic Annotation Workshop, pages 59?63,
Dublin, Ireland, August 23-24 2014.
A Web-based Geo-resolution Annotation and Evaluation Tool
Beatrice Alex, Kate Byrne, Claire Grover and Richard Tobin
School of Informatics
University of Edinburgh
{balex,kbyrne3,grover,richard}@inf.ed.ac.uk
Abstract
In this paper we present the Edinburgh Geo-annotator, a web-based annotation tool for the manual
geo-resolution of location mentions in text using a gazetteer. The annotation tool has an inter-
linked text and map interface which lets annotators pick correct candidates within the gazetteer
more easily. The geo-annotator can be used to correct the output of a geoparser or to create
gold standard geo-resolution data. We include accompanying scoring software for geo-resolution
evaluation.
1 Introduction
Many kinds of digitised content have an important geospatial dimension. However not all geospatial
information is immediately accessible, particularly in the case where it is implicit in place names in text.
The process of geo-resolution (also often referred to as geo-referencing, geoparsing or geotagging) links
instances of textual geographic information to location coordinates, enabling searching and linking of
digital content using its geospatial properties.
Geo-resolution tools can never be completely accurate and their performance can vary significantly
depending on the type and quality of the input texts as well as on the gazetteer resources they consult.
For this reason, users of text collections are frequently disappointed in the results of geo-resolution and,
depending on their application and dataset size, they may decide to take remedial action to improve
the quality. The tool we describe here is a web-based, manual annotation tool which can be used to
correct the output of geo-resolution. It has been developed in step with our geo-resolution system, the
Edinburgh Geoparser (Grover et al., 2010), but it could also be used to correct the output of other tools.
In our work, we use the geo-annotator to create gold-standard material for geo-resolution evaluation and
have produced accompanying scoring software.
1
2 Related Work
Within the field of NLP, SpatialML is probably the best known work in the area of geo-referencing.
SpatialML is an annotation scheme for marking up natural language references to places and grounding
them to coordinates. The SpatialML corpus (Mani et al., 2008) instantiates this annotation scheme and
can be used as an evaluation corpus for geo-resolution (Tobin et al., 2010). Other researchers develop
their own geo-annotated corpora and evaluate against these, e.g. Clough (2005), Leidner (2007).
Within the field of Information Retrieval, there is an ACM special interest group on spatially-related
information, SIGSPATIAL
2
, with regular geographic IR conferences (GIR conferences) where geo-
referencing research is presented, see for example Purves et al. (2007).
There are currently several geoparsing tools available, such as GeoLocate
3
, and CLAVIN
4
, as well as
our own tool, the Edinburgh Geoparser. All of these enable users to geo-reference text collections but do
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
The Edinburgh Geo-annotator will be available at http://www.ltg.ed.ac.uk.
2
http://www.sigspatial.org/
3
http://www.museum.tulane.edu/geolocate/
4
http://clavin.bericotechnologies.com/
59
not address the question of how to interact with the geo-annotations in order to correct them, nor do they
assist in creating evaluation materials for particular text collections.
The Edinburgh Geo-annotator has been developed in tandem with the Edinburgh Geoparser and ear-
lier versions have been used in the GeoDigRef project (Grover et al., 2010) to create evaluation data
for historical text collections as well as in the botanical domain (Llewellyn et al., 2012; Llewellyn et
al., 2011) where we adapted it to allow curators to geo-reference the textual metadata associated with
herbarium specimens. The current version has also been used to create gold standard data for Trading
Consequences, a historical text mining project on mining location-centric trading information relevant to
the nineteenth century (Klein et al., 2014). The Pelagios project, which deals with texts about the ancient
world, has recently developed Recogito
5
, a geo-resolution correction tool similar to our own.
3 Annotation Tool
The Edinburgh Geo-annotator is a geo-resolution annotation tool which can be used to correct geo-
resolution output or to create manually annotated gold standard data for evaluating geo-resolution al-
gorithms and tools. The geo-annotator has a web-based interface allowing easy off-site annotation in
inter-disciplinary projects by domain experts (who are not always necessarily the developers of the geo-
referencing software).
6
The interface allows users to select documents from a collection of prepared files
containing annotated location entity mentions. By selecting and loading a document, the user can see its
textual content and the location mentions highlighted within it.
The current tool is set up to select locations from a set of location candidates retrieved from GeoNames
and visualised by pins on a Google Maps (v3) window. However, it can be configured to use candidates
from a different location gazetteer. There are two files associated with each document: (1) an HTML
file which contains the text of the document and (2) an XML file which contains the candidates for each
location mention in the text and in which the annotations are stored. Candidates are linked to location
mentions via identifiers.
All location mentions displayed in the text interface are highlighted in colour (see Figure 1). Those in
red (e.g. Dublin) have one or more potential candidates in the gazetteer, while those in blue (e.g. British
Empire) do not have candidate entries in the gazetteer. There are a number of reasons why a mention
does not have a gazetteer entry. For example, the mention might be an old name of a location which is
not stored in the gazetteer, or the mention contains a misspelling. During the annotation phase, the user
is instructed to go through the red location mentions in the text and select the appropriate candidate.
In some cases there is only one candidate that can be selected (see Figure 2). The user can zoom to
the correct location pin which when selected shows a popup with the relevant gazetteer information for
that entry. The user can choose this candidate by pressing either ?Select for this mention? if the choice
is specific to the selected mention or ?Select for all mentions? if the selection can be propagated for all
mentions with the same string in the document. Once a pin is selected, it and the location mention in the
text turn green. To undo a selection, the user can click on a green pin and press either ?Deselect for this
mention? or ?Deselect for all mentions?.
In other cases, there are many candidates to choose from. For example, when clicking on the first
location mention (Dublin) shown in Figure 1, the map adjusts to the central point of all 42 candidate
locations. When reading a piece of text, human beings can often easily understand which location a
place name refers to based on the context it appears in, which means that choosing between multiple
candidates manually is not expected to be a difficult task. However, the number of location candidates
that are suggested by GeoNames and consequently displayed in the interface can be limited in the data
files, if for example the user only wants to choose between a small number of candidates.
In the case of Dublin (see Figure 1), the user would then zoom into the correct Dublin to select a
candidate and discover that there are two pins which are relevant, Dublin ? the capital, and Baile
?
Atha
Cliath ? the Gaelic name for Dublin and its gazetteer entry referring to the administrative division (see
Figure 3). The gazetteer information in the popup can assist the user to make a choice. In this case, it
is clear from the context that the text refers to the capital. It might not always be as clearcut to choose
5
http://pelagios-project.blogspot.co.at/2014/01/from-bordeaux-to-jerusalem-and-back.
html
6
The geo-annotator is run via a javascript programme which calls an update.cgi script on the server side to write the saved
data to file. We have tested it in Safari, Firefox and Chrome.
60
Figure 1: When an example location mention (e.g. Dublin) is clicked the map adjusts to show all potential
location candidates that exist in the gazetteer for this place name.
between multiple candidates. In such cases, it is important that the annotation guidelines provide detailed
instruction as to which type of gazetteer entry to prefer.
If none of the candidates displayed on the map are correct, then the user must mark this by pressing
?This mention? (or ?All mentions?) in the box located at the top of right corner of the map (see Figure 1).
Once there are only green or blue location mentions left in the text, the annotation for the selected docu-
ment is complete and the user should press ?Save Current Document? and move to the next document in
the collection.
4 Geo-resolution Evaluation
It is important to be able to report the quality of a geo-resolver?s performance in concrete and quantifi-
able terms. Along with the annotation tool, we are therefore also releasing an evaluation script which
compares the manually geo-resolved locations to those predicted by an automatic geoparser.
7
We follow
standard practice in comparing system output to hand-annotated gold standard evaluation data. The script
evaluates the performance of the geo-resolution independently from geo-tagging, meaning that it only
considers named entities which were tagged in the input to the manual geo-resolution annotation but not
those that were missed. It is therefore preferable to use input data which contains manually annotated or
corrected location mentions.
The evaluation script computes the number of correctly geo-resolved locations and accuracy in percent.
Both figures are presented for a strict evaluation of exact match against gazetteer identifier and for a lax
evaluation where the grid references of the gold and the system choice have to occur within a small
distance of one another to count as a match. For a pair of location candidates (gold vs. system), we
compute the Great-circle distance using a special case of the Vincenty formula which is most accurate
for all distances.
8
The lax evaluation is provided as even with clear annotation guidelines, annotators
7
We provide Mac and Linux binaries of the evaluation scripts.
8
For the exact formula, see: http://en.wikipedia.org/wiki/Great-circle_distance
61
Figure 2: Example candidate for the location mention River Liffey and its gazetteer entry information
shown in a popup.
Figure 3: Choosing between multiple candidates for the same location mention.
can find it difficult to chose between different location types for essentially the same place (e.g. see the
example for Dublin in Figure 3).
During the manual annotation, three special cases can arise. Some location mentions do not have a
candidate in the gazetteer (those appearing in blue), while others do have candidates in the gazetteer but
the annotator does not consider any of them correct. Occasionally there are location mentions with one
or more candidates in the gazetteer but an annotator neither chooses one of them nor selects ?none?. The
latter cases are considered to be annotation errors, usually because the annotator has forgotten to resolve
them. The evaluation excludes all three cases when computing accuracy scores but notes them in the
evaluation report in order to facilitate error analysis (see sample output in Figure 4).
total: 11 exact: 10 (90.9\%) within 6.0km 11 (100.0\%)
note: no gold choice for British Empire
note: annotator selected "none" for Irish Free State
Figure 4: Sample output of the geo-resolution evaluation script. When setting the lax evaluation to 6km,
one candidate selected by the system was close enough to the gold candidate to count as a match.
5 Summary
We have presented a web-based manual geo-resolution annotation and evaluation tool which we are
releasing to the research community to facilitate correction of automatic geo-resolution output and eval-
uation of geo-resolution algorithms and techniques. In this paper we introduce the annotation tool and its
main functionalities and describe two geo-resolution evaluation metrics with an example, namely strict
and lax accuracy scoring. The release will contain more detailed documentation of the configuration and
installation process and the document formats for the textual input and candidate gazetteer entries.
62
References
Paul Clough. 2005. Extracting metadata for spatially-aware information retrieval on the internet. In Proceedings
of Workshop on Geographic Information Retrieval (GIR?05).
Claire Grover, Richard Tobin, Kate Byrne, Matthew Woollard, James Reid, Stuart Dunn, and Julian Ball. 2010.
Use of the Edinburgh geoparser for georeferencing digitised historical collections. Phil. Trans. R. Soc. A.
Ewan Klein, Beatrice Alex, Claire Grover, Richard Tobin, Colin Coates, Jim Clifford, Aaron Quigley, Uta Hin-
richs, James Reid, Nicola Osborne, and Ian Fieldhouse. 2014. Digging Into Data White Paper: Trading Conse-
quences.
Jochen L. Leidner. 2007. Toponym Resolution in Text: Annotation, Evaluation and Applications of Spatial
Grounding of Place Names. Ph.D. thesis, School of Informatics, University of Edinburgh.
Clare Llewellyn, Elspeth Haston, and Claire Grover. 2011. Georeferencing botanical data using text analysis tools.
In Proceedings of the Biodiversity Information Standards Annual Conference (TDWG 2011).
Clare Llewellyn, Claire Grover, Jon Oberlander, and Elspeth Haston. 2012. Enhancing the curation of botan-
ical data using text analysis tools. In Panayiotis Zaphiris, George Buchanan, Edie Rasmussen, and Fernando
Loizides, editors, Theory and Practice of Digital Libraries, volume 7489 of Lecture Notes in Computer Science,
pages 480?485. Springer Berlin Heidelberg.
Inderjeet Mani, Janet Hitzeman, Justin Richer, Dave Harris, Rob Quimby, and Ben Wellner. 2008. SpatialML:
Annotation scheme, corpora, and tools. In Proceedings of the Sixth International Language Resources and
Evaluation (LREC?08).
Ross S. Purves, Paul Clough, Christopher B. Jones, Avi Arampatzis, Benedicte Bucher, David Finch, Gaihua Fu,
Hideo Joho, Awase Khirni Syed, Subodh Vaid, and Bisheng Yang. 2007. The design and implementation
of SPIRIT: a spatially-aware search engine for information retrieval on the internet. International Journal of
Geographic Information Systems (IJGIS), 21(7).
Richard Tobin, Claire Grover, Kate Byrne, James Reid, and Jo Walsh. 2010. Evaluation of georeferencing. In
Proceedings of Workshop on Geographic Information Retrieval (GIR?10).
63
