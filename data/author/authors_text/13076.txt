Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 127?137,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improving Translation via Targeted Paraphrasing
Philip Resnik
Linguistics and UMIACS
University of Maryland
resnik@umd.edu
Olivia Buzek
Linguistics and Computer Science
University of Maryland
olivia.buzek@gmail.com
Chang Hu
Computer Science
University of Maryland
changhu@cs.umd.edu
Yakov Kronrod
Linguistics and UMIACS
University of Maryland
yakov@umd.edu
Alex Quinn
Computer Science
University of Maryland
aq@cs.umd.edu
Benjamin B. Bederson
Computer Science and UMIACS
University of Maryland
bederson@cs.umd.edu
Abstract
Targeted paraphrasing is a new approach to the
problem of obtaining cost-effective, reasonable
quality translation that makes use of simple and
inexpensive human computations by monolin-
gual speakers in combination with machine
translation. The key insight behind the process
is that it is possible to spot likely translation
errors with only monolingual knowledge of the
target language, and it is possible to generate al-
ternative ways to say the same thing (i.e. para-
phrases) with only monolingual knowledge
of the source language. Evaluations demon-
strate that this approach can yield substantial
improvements in translation quality.
1 Introduction
For most of the world?s languages, the availability of
translation is limited to two possibilities: high qual-
ity at high cost, via professional translators, and low
quality at low cost, via machine translation (MT). The
spectrum between these two extremes is very poorly
populated, and at any point on the spectrum the ready
availability of translation is limited to only a small
fraction of the world?s languages. There is, of course,
a long history of technological assistance to transla-
tors, improving cost effectiveness using translation
memory (Laurian, 1984; Bowker and Barlow, 2004)
or other interactive tools to assist translators (Esteban
et al, 2004; Khadivi et al, 2006). And there is a
recent and rapidly growing interest in crowdsourc-
ing with non-professional translators, which can be
remarkably effective (Munro, 2010). However, all
these alternatives face a central availability bottle-
neck: they require the participation of humans with
bilingual expertise.
In this paper, we report on a new exploration of
the middle ground, taking advantage of a virtually
unutilized resource: speakers of the source and tar-
get language who are effectively monolingual, i.e.
who each only know one of the two languages rel-
evant for the translation task. The solution we are
proposing has the potential to provide a more cost
effective approach to translation in scenarios where
machine translation would be considered acceptable
to use, if only it were generally of high enough qual-
ity. This would clearly exclude tasks like transla-
tion of medical reports, business contracts, or literary
works, where the validation of a qualified bilingual
translator is absolutely necessary. However, it does
include a great many real-world scenarios, such as
following news reports in another country, reading in-
ternational comments about a product, or generating
a decent first draft translation of a Wikipedia page
for Wikipedia editors to improve.
The use of monolingual participants in a human-
machine translation process is not entirely new.
Callison-Burch et al (2004) pioneered the explo-
ration of monolingual post-editing within the MT
community, an approach extended more recently to
provide richer information to the user by Albrecht et
al. (2009) and Koehn (2009). There have also been at
least two independently developed human-machine
translation frameworks that employ an iterative pro-
tocol involving monolinguals on both the source and
target side. Morita and Ishida (2009) describe a sys-
tem in which target and source language speakers
perform editing of MT output to improve fluency
and adequacy, respectively; they utilize source-side
paraphrasing at a course grain level, although their ap-
proach is limited to requests to paraphrase the entire
sentence when the translation cannot be understood.
127
Bederson et al (2010) describe a similar protocol in
which cross-language communication is enhanced by
metalinguistic communication in the user interface.
Shahaf and Horvitz (2010) use machine translation
as a specific instance of a general game-based frame-
work for combining a range of machine and human
capabilities.
We call the technique used here targeted para-
phrasing. In a nutshell, target-language monolin-
guals identify parts of an initial machine translation
that don?t appear to be right, and source-language
monolinguals provide the MT system with alterna-
tive phrasings that might lead to better translations;
these are then passed through MT again and the best
scoring hypothesis is selected as the final translation.
This technique can be viewed as compatible with
the richer protocol- and game-based approaches, but
it is considerably simpler; in Sections 2 through 4
we describe the method and present evaluation re-
sults on Chinese-English translation. Unlike other
approaches, the technique also offers clear opportu-
nities to replace human participation with machine
components if the latter are up to the task; we discuss
this in Section 5 before wrapping up in Section 6
with conclusions and directions for future work.
2 Targeted Paraphrasing
The starting point for our approach is an observa-
tion: the source sentence provided as input to an MT
system is just one of many ways in which the mean-
ing could have been expressed, and for any given
MT system, some forms of expression are easier to
translate than others. The same basic observation
has been applied quite fruitfully over the past several
years to deal with statistical MT challenges involv-
ing segmentation, morphological analysis, and more
recently, source language word order (Dyer, 2007;
Dyer et al, 2008; Dyer and Resnik, 2010). Here we
apply it to the surface expression of meaning.
For example, consider the following real example
of translation from English to French by an automatic
MT system:
? Source: Polls indicate Brown, a state senator,
and Coakley, Massachusetts? Attorney General,
are locked in a virtual tie to fill the late Sen. Ted
Kennedy?s Senate seat.
? System: Les sondages indiquent Brown,
un se?nateur d?e?tat, et Coakley,
Massachusetts? Procureur ge?ne?ral, sont en-
ferme?s dans une cravate virtuel a` remplir le
regrette? se?nateur Ted Kennedy?s sie`ge au Se?nat.
A French speaker can look at this automatic transla-
tion and see immediately that the underlined parts
are wrong, even without knowing the intended source
meaning. We can identify the spans in the source En-
glish sentence that are responsible for these badly
translated French spans, and change them to alterna-
tive expressions with the same meaning (e.g. chang-
ing Massachusetts? Attorney General to the Attorney
General of Massachusetts); if we do so and then use
the same MT system again, we obtain a translation
that is still imperfect (e.g. cravate means necktie),
but is more acceptable:
? System: Les sondages indiquent que Brown, un
se?nateur d?e?tat, et Coakley, le procureur ge?ne?ral
du Massachusetts, sont enferme?s dans une cra-
vate virtuel pourvoir le sie?ge au Se?nat de Sen.
Ted Kennedy, qui est de?ce?de? re?cemment.
Operationally, then, translation with targeted para-
phrasing includes the following steps.
Initial machine translation. For this paper, we
use the Google Translate Research API, which,
among other advantages, provides word-level align-
ments between the source text and its output. In
principle, however, any automatic translation system
can be used in this role, potentially at some cost
to quality, by performing post hoc target-to-source
alignment.
Identification of mistranslated spans. This step
identifies parts of the source sentence that lead to
ungrammatical, nonsensical, or apparently incorrect
translations on the target side. In the experiments
of Sections 3 and 4, this step is performed by hav-
ing monolingual target speakers identify likely error
spans on the target side, as in the French example
above, and projecting those spans back to the source
spans that generated them using word alignments
as the bridge (Hwa et al, 2005; Yarowsky et al,
2001). In Section 5, we describe a heuristic but effec-
tive method for performing this fully automatically.
Du et al (2010), in this proceedings, explore the
128
use of source paraphrases without targeting appar-
ent mistranslations, using lattice translation (Dyer
et al, 2008) to efficiently represent and decode the
resulting very large space of paraphrase alternatives.
Source paraphrase generation. This step gener-
ates alternative expressions for the source spans iden-
tified in the previous step. In this paper, it is per-
formed by monolingual source speakers who perform
the paraphrase task: the speaker is given a sentence
with a phrase span marked, and is asked to replace the
marked text with a different way of saying the same
thing, so that the resulting sentence still makes sense
and means the same thing as the original sentence.
To illustrate in English, someone seeing John and
Mary took a European vacation this summer might
supply the paraphrase Mary went on a European, ver-
ifying that the resulting John and Mary went on a
European vacation this summer preserves the origi-
nal meaning. This step can also be fully automated
(Max, 2009) by taking advantage of bilingual phrase-
table pivoting (Bannard and Callison-Burch, 2005);
see Max (2010), in these proceedings, for a related
approach in which the paraphrases of a source phrase
are used to refine the estimated probability distribu-
tion over its possible target phrases.
Generating sentential source paraphrases. For
each sentence, there may be multiple paraphrased
spans. These are multiplied out to provide full-
sentence paraphrases. For example, if two non-
overlapping source spans are each paraphrased in
three ways, we generate 9 sentential source para-
phrases, each of which represents an alternative way
of expressing the original sentence.
Machine translation of alternative sentences.
The alternative source sentences, produced via para-
phrase, are sent through the same MT system, and
a single-best translation hypothesis is selected, e.g.
on the basis of the translation system?s model score.
In principle, one could also combine the alternatives
into a lattice representation and decode to find the
best path using lattice translation (Dyer et al, 2008);
cf. Du et al (2010). One could also present trans-
lation alternatives to a target speaker for selection,
similarly to Callison-Burch et al (2004).
Notice that with the exception of the initial trans-
lation, each remaining step in this pipeline can in-
volve either human participation or fully automatic
processing. The targeted paraphrasing framework
therefore defines a rich set of intermediate points on
the spectrum between fully automatic and fully hu-
man translation, of which we explore only a few in
this paper.
3 Pilot Study
In order to assess the potential of our approach,
we conducted a small pilot study, using eleven
sentences in simplified Chinese selected from
the article on ?Water? in Chinese Wikipedia
(http://zh.wikipedia.org/zh-cn/%E6%B0%B4). This
article was chosen because its topic is well known
in both English-speaking and Chinese-speaking pop-
ulations. The first five sentences were taken from
the first paragraph of the article. The other six sen-
tences were taken from a randomly-chosen paragraph
in the article. As a preprocessing step, we removed
any parenthetical items from the input sentences, e.g.
?(H20)?. The shortest sentence in this set has 12 Chi-
nese characters, the longest has 54.1
Human participation in this task was accomplished
using Amazon Mechanical Turk, an online market-
place that enables human performance of small ?hu-
man intelligence tasks? (HITs) in return for micropay-
ments. For each sentence, after we translated it au-
tomatically (using Google Translate), three English-
speaking Mechanical Turk workers (?Turkers?) on
the target side performed identification of mistrans-
lated spans. Each span identified was projected back
to its corresponding source span, and three Chinese-
speaking Turkers were asked to provide paraphrases
of each source span. These tasks were easy to per-
form (no more than around 30 seconds to complete
on average) and inexpensive (less than $1 for the
entire pilot study).2 The Chinese source span para-
phrases were then used to construct full-sentence
paraphrases, which were retranslated, once again by
Google Translate, to produce the output of the tar-
geted paraphrasing translation process.
1Note that this page is not a translation of the corresponding
English Wikipedia page or vice versa.
2The four English-speaking Turkers were recruited through
the normal Mechanical Turk mechanism. The three Chinese-
speaking Turkers were recruited offline by the authors in order to
quickly obtain results, although they participated as full-fledged
Turkers.
129
The initial translation outputs from Google Trans-
late (GT) and the results of the targeted paraphrasing
translation process (TP) were evaluated according
to widely used critera of fluency and adequacy. Flu-
ency ratings were obtained on a 5-point scale from
three native English speakers without knowledge of
Chinese. Translation adequacy ratings were obtained
from three native Chinese speakers who are also flu-
ent in English; they assessed adequacy of English
sentences by comparing the communicated meaning
to the Chinese source sentences.
Fluency was rated on the following scale:
1. Unintelligible: nothing or almost nothing of the sen-
tence is comprehensible.
2. Barely intelligible: only a part of the sentence (less
than 50%) is understandable.
3. Fairly intelligible: the major part of the sentence
passes.
4. Intelligible: all the content of the sentence is com-
prehensible, but there are errors of style and/or of
spelling, or certain words are missing.
5. Very intelligible: all the content of the sentence is
comprehensible. There are no mistakes.
Adequacy was rated on the following scale:
1. None of the meaning expressed in the reference sen-
tence is expressed in the sentence.
2. Little of the reference sentence meaning is expressed
in the sentence.
3. Much of the reference sentence meaning is expressed
in the sentence.
4. Most of the reference sentence meaning is expressed
in the sentence.
5. All meaning expressed in the reference sentence ap-
pears in the sentence.
For each GT output, we averaged across the ratings
of the alternative TP to produce average TP fluency
and adequacy scores. The average GT output rat-
ings, measuring the pure machine translation base-
line, were 2.36 for fluency and 2.91 for adequacy.
Averaging across the TP outputs, these rose to 3.32
and 3.49, respectively.
One could argue that a more sensible evaluation
is not to average across alternative TP outputs, but
rather to simulate the behavior of a target-language
speaker who simply chooses the one translation
among the alternatives that seems most fluent. If
we select the most fluent TP output for each source
sentence according to the English-speakers? average
fluency ratings, we obtain average test set ratings of
3.58 for fluency and 3.73 for adequacy. Those are
respective gains of 0.82 and 1.21 over the baseline
initial MT output, each on a 5-point scale.
Figure 1 shows a selection of outputs: we present
the two cases where the most fluent TP alternative
shows the greatest gain in average fluency rating (best
gain +2.67); two cases near the median gain in av-
erage fluency (median +1); and the worst two cases
with respect to effect on average fluency rating (worst
-0.33). The table accurately conveys a qualitative im-
pression corresponding to the quantitative results: the
overall quality of translations appears to be improved
by our process consistently, despite the absence of
any bilingual input in the improvements.
4 Chinese-English Evaluation
As a followup to our pilot study, we conducted an
evaluation using Chinese-English test data taken from
the NIST MT?08 machine translation evaluation, in
order to obtain fully automatic translation evaluation
scores. We report on results for 49 sentences of the
1,357 in this data set. These underwent the same
targeted paraphrasing process as in the pilot study,
with the addition of a basic step to filter out cheaters:
we disregarded as invalid any responses consisting
purely of ASCII characters (signifying a non-Chinese
response) or responses that were identical to the orig-
inal source text.
Target English speakers identified 115 potential
mistranslation spans, or 2.3 spans per sentence, that
yielded at least one source paraphrase on the source
Chinese side. Chinese speakers provided 138 valid
paraphrases. The entire cost for the human tasks in
this experiment was $5.06, or a bit under $0.11 per
sentence on average.3
Table 1 reports on the results, evaluating in stan-
dard fashion using BLEU with the four English
MT?08 references for each Chinese sentence. Since
the targeted paraphrasing translation process (TP)
produces multiple hypotheses ? one automatic trans-
lation output per sentential paraphrases ? we se-
lected the single best output for each sentence by
3Invalid paraphrase responses were rejected, i.e. zero-cost.
130
Condition Fluency Adequacy Sentence
GT 1.33 2.33 Water play life evolve into important to use.
TP 4.00 4.33 Water in the evolution of life played an important role.
GT 1.33 2.67 Human civilization from the source of the majority of large rivers
in the domain.
TP 3.33 4.67 Most of the origin of human civilization in river basin.
GT 2.33 3.00 In human daily life, the water in drinking, cleaning, washing and
other side to make use of an indispensable.
TP 3.67 3.33 In human daily life, water for drinking, cleaning, washing and other
essential role.
GT 2.00 2.33 Eastern and Western ancient Pak prime material view of both the
water regarded as a kind of basic groups into the elements, water is
the Chinese ancient five rows of a; the West ancient four elements
that also have water.
TP 3.00 3.33 East and West in ancient concept of simple substances regarded wa-
ter as a basic component elements. Among them, the five elements
of water is one of ancient China; Western ancient four elements
that also have water.
GT 4.00 4.00 Early cities will generally be in the water side of the establishment,
in order to solve irrigation, drinking and sewage problems.
TP 4.67 4.33 Early cities are generally built near the water to solve the irrigation,
drinking and sewage problems.
GT 3.0 3.33 Human very early on began to produce a water awareness.
TP 2.67 3.00 Man long ago began to understand the water produced.
Figure 1: Original Google Translate output (GT) for the pilot study in Section 3, together with translations produced by
the targeted paraphrase translation process (TP), selected to show a range from strong to weak improvements in fluency.
131
Condition BLEU
GT (baseline) 28.33
GT n-best oracle 28.47
TP one-best 30.01
TP oracle 30.79
Human upper bound 49.41
Table 1: Results on a 49-sentence subset of the NIST
MT?08 Chinese-English test set
selecting the highest scoring English translation, ac-
cording to the translation score delivered with each
output by the Google Translate Research API. (The
original translation was, of course, included among
the candidates for selection.) This yielded an im-
provement of 1.68 BLEU points on the 49-sentence
test set (TP one-best).
One could argue that this result is simply a result of
having more hypotheses to choose from, not a result
of the targeted paraphrasing process itself. In order
to rule out this possibility, we generated (n+ 1)-best
Google translations, setting n for each sentence to
match the number of alternative translations gener-
ated via targeted paraphrasing. We then chose the
best translation for each sentence, among the (n+1)-
best Google hypotheses, via oracle selection, using
the TERp metric (Snover et al, 2009) to evaluate
each hypothesis against the reference translations.4
The resulting BLEU score for the full set showed
negligible improvement (GT n-best oracle).
We did a similar oracle-best calculation using
TERp for targeted paraphrasing (TP oracle). The
result shows a potential gain of 2.46 BLEU points
over the baseline, if the best scoring alternative from
the targeted paraphrasing process were always cho-
sen.
In addition to aggregate scoring using BLEU, we
also looked at oracle results on a per-sentence ba-
sis using TERp (since BLEU more appropriate to
use at the document level, not the sentence level).
Identifying the best sentential paraphrase alternative
using TERp as an oracle, we find that the TERp
score would improve for 32 of the 49 test sentences,
4An ?oracle? telling us which variant is best is not available
in the real world, of course, but in situations like this one, oracle
studies are often used to establish the magnitude of the potential
gain (Och et al, 2004).
65.3%. For those 32 sentences, the average gain is
8.36 TERp points.5 A fairer measure is the average
obtained when scoring zero gain for the 17 sentences
where no improvement was obtained; taking these
into account, i.e. assuming an oracle who chooses the
original translation if none of the paraphrase-based
alternatives are better, the average improvement over
the entire set of 49 sentences is 5.46 TERp points.
Although we have obtained results on only a small
subset of the full NIST MT?08 test set, our automatic
evaluation confirms the qualitative impressions in
Figure 1 and the subjective ratings results obtained
in our pilot study in Section 3. The TP oracle results
establish that by taking advantage of monolingual
human speakers, it is possible to obtain quite sub-
stantial gains in translation quality. The TP one-best
results demonstrate that the majority of that oracle
gain is obtained in automatic hypothesis selection,
simply by selecting the paraphrase-based alternative
translation with the highest translation score.
The last line in Table 1 shows a human upper
bound computed using the reference translations via
cross validation; that is, for each of the four reference
translations, we evaluate it as a hypothesized transla-
tion using the other three references as ground truth;
these four scores were then averaged. The value of
this upper bound is quite consistent with the bound
computed similarly by Callison-Burch (2009).
5 English-Chinese Evaluation
As we noted in Section 2, the targeted paraphrasing
translation process defines a set of human-machine
combinations that do not require bilingual expertise.
The previous section described human identification
of mistranslated spans on the target side, human gen-
eration of paraphrases for problematic sub-sentential
spans on the source side, and both automatic hypothe-
sis selection and human selection (via fluency ratings,
in Section 3).
In this section, we take a step toward more au-
tomated processing, replacing human identification
of mistranslated spans with an a fully automatic
method.6 The idea behind our automatic error iden-
tification is straightforward: if the source sentence
5?Gains? refer to a lower score: since TERp is an error
measure, lower is better.
6This section contains material we originally reported in
Buzek et al (2010).
132
GT: WTO chief negotiator on behalf of the United States to propose substantial reduction of
agricultural subsidies, Kai Fa countries substantially reduce industrial products import tariffs to Dapo
?? Doha Round of negotiations deadlock.
TP: World Trade Organization negotiator suggested the United States today, a substantial reduction
of agricultural subsidies, developing countries substantially reduce industrial products?? Import
tariffs, in order to break the deadlock in the Doha Round of trade negotiations.
REF: the main delegates at the world trade organization talks today suggested that the us make major
cuts in its agricultural subsidies and that developing countries significantly reduce import duties on
industrial products in order to break the deadlock in the doha round of trade talks .
GT: Emergency session of the Palestinian prime minister Salam Fayyad state will set a new Govern-
ment
TP: Emergency session of the Palestinian Prime Minister Salam Fayyad will set the new government
REF: state of emergency period ends ; palestinian prime minister fayyad to form new government
GT: Indian territory from south to north, one week before the start after another wet season, the
provincial residents hold long drought every rain in the mood to meet the heavy rain, but did not
expect rain came unexpectedly fierce, a rain disaster, roads become rivers, low-lying areas housing to
make Mo in the water, transport almost paralyzed, Zhi Jin statistics about You nearly 500 people due
to floods were killed.
TP: Indian territory from south to north, one week before the start have entered into the rainy season,
provincial residents hold long drought to hope rain in the mood to meet the heavy rain, but did not
feed rain came unexpectedly fierce, a rain disaster, roads change the river, low-lying areas housing
do not water, traffic almost to a standstill, since statistics are nearly 500 people due to floods killed.
REF: the whole of india , from south to north , started to progressively enter the monsoon season a
week ago . the residents of each state all greeted the heavy rains as relief at the end of a long drought
, but didn?t expect that the rain would come with unexpected violence , a real deluge . highways have
become rivers ; houses in low-lying areas have been surbmerged in the water ; the transport system is
nearly paralyzed . to date , figures show that nearly 500 people have unfortunately lost their lives to
the floods .
GT: But the Taliban said in the meantime, the other a German hostages kidnapped in very poor
health, began to fall into a coma and lost consciousness.
TP: But the Taliban said in the meantime, another German hostages kidnapped a very weak body
fell into a coma and began to lose consciousness.
REF: but at the same time the taliban said that another german hostage who had been kidnapped
was in extremely poor health , and had started to become comatose and to lose consciousness .
GT: Taliban spokesman Ahmadi told AFP in an unknown location telephone interview, said: We,
through tribal elders, representatives of direct contact with South Korea.
TP: Taliban spokesman Ahmadi told AFP in an unknown location telephone interview, said: We are
through tribal elders, directly with the South Korean leadership, business
REF: taliban spokesperson ahmadi said in a telephone interview by afp at an undisclosed location :
we have established direct contact with the south korean delegation through tribal elders .
Figure 2: Random sample of 5 items from study in Section 4: original Google translation (GT), results of targeted
paraphrasing translation process (TP), and a human reference translation.
133
is translated to the target and then back-translated, a
comparison of the result with the original is likely to
identify places where the translation process encoun-
tered difficulty.7 Briefly, we automatically translate
source F to target E, then back-translate to produce F?
in the source language. We compare F and F? using
TERp ? which, in addition to its use as an evaluation
metric, is a form of string-edit distance that identifies
various categories of differences between two sen-
tences. When at least two consecutive edits are found,
we flag their smallest containing syntactic constituent
as a potential source of translation difficulty.8
In more detail, we posit that if an area of backtrans-
lation F? has many edits relative to original sentence
F, then that area probably comes from parts of the
target translation that did not represent the desired
meaning in F very well. We only consider consec-
utive edits in certain of the TERp edit categories,
specifically, deletions (D), insertions (I), and shifts
(S); the two remaining categories, matches (M) and
paraphrases (P), indicate that the words are identical
or that the original meaning was preserved. Further-
more, we assume that while a single D, S, or I edit
might be fairly meaningless, a string of at least two of
those types of edits is likely to represent a substantive
problem in the translation.
In order to identify reasonably meaningful para-
phrase units based on potential errors, we rely on a
source language constituency parser. Using the parse,
we find the smallest constituent of the sentence con-
taining all of the tokens in a particular error string. At
times, these constituents can be quite large, even the
entire sentence. To weed out these cases, we restrict
constituent length to no more than 7 tokens.
For example, given
F The most recent probe to visit Jupiter was the
Pluto-bound New Horizons spacecraft in late Febru-
ary 2007.
E La investigacio?n ma?s reciente fue la visita de Ju?piter
a Pluto?n de la envolvente sonda New Horizons a
fines de febrero de 2007.
7Exactly the same insight is behind the ?source-side pseudo-
referencebased feature? employed by Soricut and Echihabi
(2010) in their system for predicting the trustworthiness of trans-
lations.
8It is possible that the difficulty so identified involves back-
translation only, not translation in the original direction. If that
is the case, then more paraphrasing will be done than necessary,
but the quality of the TP process?s output should not suffer.
F? The latest research visit Jupiter was the Pluto-bound
New Horizons spacecraft in late February 2007.
spans in the the bolded phrase in F would be iden-
tified, based on the TERp alignment and smallest
containing constituent as shown in Figure 3.
In order to evaluate this approach, we again use
NIST MT08 data, this time going in the English-
to-Chinese direction since we are assuming source
language resources not currently available for Chi-
nese.9 We used English reference 0 as the source
sentence, and the original Chinese sentence as the
target.10
The data set comprises 1,357 sentence pairs. Us-
ing the above described algorithm to automatically
identify possible problem areas in the translation,
with the Google Translate API providing both the
translation and back-translation, we generated 1,780
potential error spans in 1,006 of the sentences, and,
continuing the targeted paraphrasing process, we ob-
tained up to three source paraphrases per span, for
the problemantic spans in 1,000 of those sentences.
(For six sentences, no paraphrases weres suggested
for any of the problematic spans.) These yielded
full-sentence paraphrase alternatives for the 1,000
sentences, which we again evaluated via an oracle
study.
For this study we used the TER metric (Snover
et al, 2006) rather than TERp. Comparing with the
GT output, we find that TP yields a better-translated
paraphrase sentence is available in 313 of the 1000
cases, or 31.3%, and for those 313 cases, TER for the
oracle-best paraphrase alternative improves on the
TER for the original sentence by 12.16 TER points.
Also taking into account the cases where there is
no improvement over the baseline, the average TER
score improves by 3.8 points. The cost for human
tasks in this study ? just paraphrases, since identi-
fying problematic spans was done automatically ?
was $117.48, or a bit under $0.12 per sentence.
9The Stanford parser (Klein and Manning, 2002), which
we use to identify source syntactic constituents, exists for both
English and Chinese, but TERp uses English resources such as
WordNet in order to capture acceptable variants of expression
for the same meaning. Matt Snover (personal communication) is
working on extension of TERp to other languages.
10We chose reference 0 because on inspection these references
seemed most reflective of native English grammar and usage.
134
NP PP 
NP 
Figure 3: TERp alignment of a source sentence and its back-translation in order to identify a problematic source span.
6 Conclusions and Future Work
In this paper we have focused on a relatively less-
explored space on the spectrum between high quality
and low cost translation: sharing the burden of the
translation task among a fully automatic system and
monolingual human participants, without requiring
human bilingual expertise. The monolingual par-
ticipants in this framework perform straightforward
tasks: they identify parts of sentences in their lan-
guage that seem to have errors, they provide sub-
sentential paraphrases in context, and they judge the
fluency of sentences they are presented with (or, in a
variant still to be explored, they simply select which
target sentence they like the best). Unlike other pro-
posals for exploiting monolingual speakers in human-
machine collaborative translation, the human steps
here are amenable to automation, and in addition
to evaluating a mostly-human variant of our targeted
paraphrasing translation framework, we also assessed
a version in which the identification of mistranslated
spans (to be paraphrased) is done automatically.
Our experimentation yielded a consistent pattern
of results, supporting the conclusion that targeted
paraphrasing can lead to significant improvements
in translation, via several different measures. First,
a very small pilot study for Chinese-English trans-
lation in Wikipedia provided preliminary validation
that translation fluency and accuracy can be improved
quite significantly for a set of fairly chosen test sen-
tences, according to human ratings. Second, a small
experiment in Chinese-English translation using stan-
dard NIST test sentences suggested the potential for
dramatic gains using the BLEU and TERp scores,
with oracle improvements of 2.46 points and 5.46
points, respectively. In addition, a non-oracle experi-
ment, selecting the best hypothesis according to the
MT system?s model score, yielded a gain of nearly 1.7
BLEU points. And third, in a large scale evaluation
of the approach using English-Chinese translation
of 1,000 sentences, this time automating the step of
identifying potentially mistranslated parts of source
sentences, the oracle results demonstrated that a gain
of nearly 4 TER points is available.
These initial studies leave considerable room for
future work. One important step will be to better char-
acterize the relationship between cost and quality in
quantitative terms: how much does it cost to obtain
135
how much quality improvement, and how does that
compare with typical professional translation costs of
$0.25 per word? This question is closely connected
with the dynamics of crowdsourcing platforms such
as Mechanical Turk ? the cost per sentence in these
experiments works out to be around $0.12, but trans-
lation on a large scale will involve a complicated
ecosystem of workers and cheaters, tasks and motiva-
tions and incentives (Quinn and Bederson, 2009). A
related crowdsourcing issue requiring further study
is the availability of monolingual human participants
for a range of language pairs, in order to validate
the argument that drawing on monolingual human
participation will significantly reduce the severity of
the availability bottleneck. And, of course, in the
upper bound in Table 1 makes quite clear the cru-
cial value added by bilingual translators, when they
are available; we hope to explore whether the tar-
geted paraphrasing translation pipeline can improve
the productivity of post-editing by bilinguals, mak-
ing it easier to move toward the upper bound in a
cost-effective way.
Another set of issues concerns the underlying trans-
lation technology. A reviewer correctly notes that the
value of the approach taken here is likely to vary
depending upon the quality of the underlying trans-
lation system, and the approach may break down at
the extrema, when the baseline translation is either
already very good or completely awful. We chose
to use Google Translate for its wide availability and
the fact that it represents a state of the art baseline to
beat; however, in future work we plan to substitute
our own statistical MT systems, which will permit us
to experiment across a range of translation model and
language model LM training set sizes, and therefore
to vary quality while keeping other system details
constant. More directly connected to research in ma-
chine translation, this framework provides a variety
of opportunities for improving fully automatic sta-
tistical MT systems. We plan to implement a fully
automatic targeted paraphrasing translation pipeline,
using the automated methods discussed when intro-
ducing the pipeline in Section 2, including transla-
tion of targeted paraphrase lattices (cf. (Max, 2010;
Du et al, 2010)). Finally, we intend to explore the
application of our approach in scenarios involving
less-common languages, by using a more common
language as a pivot or bridge (Habash and Hu, 2009).
Acknowledgments
This work has been supported in part by the National
Science Foundation under awards BCS0941455 and
IIS0838801. The authors would like to thank three
anonymous reviewers for their helpful comments,
and Chris Callison-Burch and Chris Dyer for their
helpful comments and discussion.
References
Joshua S. Albrecht, Rebecca Hwa, and G. Elisabeta Marai.
2009. Correcting automatic translations through collab-
orations between mt and monolingual target-language
users. In EACL ?09: Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 60?68, Morristown,
NJ, USA. Association for Computational Linguistics.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Benjamin B. Bederson, Chang Hu, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Graphics Interface (GI) confer-
ence.
Lynne Bowker and Michael Barlow. 2004. Bilingual
concordancers and translation memories: a comparative
evaluation. In LRTWRT ?04: Proceedings of the Second
International Workshop on Language Resources for
Translation Work, Research and Training, pages 70?79,
Morristown, NJ, USA. Association for Computational
Linguistics.
Olivia Buzek, Philip Resnik, and Ben Bederson. 2010. Er-
ror driven paraphrase annotation using mechanical turk.
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 217?221, Los Angeles, June.
Association for Computational Linguistics.
Chris Callison-Burch, Colin Bannard, , and Josh
Schroeder. 2004. Improving statistical translation
through editing. In Workshop of the European Associa-
tion for Machine Translation.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Mechan-
ical Turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 286?295, Singapore, August. Association for
Computational Linguistics.
Jinhua Du, Jie Jiang, and Andy Way. 2010. Facilitating
translation using source language paraphrase lattices.
136
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Chris Dyer and Philip Resnik. 2010. Forest translation.
In NAACL?10.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proceedings of HLT-ACL,
Columbus, OH.
C. Dyer. 2007. Noisier channel translation: translation
from morphologically complex languages. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, Prague, June.
Jose? Esteban, Jose? Lorenzo, Antonio S. Valderra?banos,
and Guy Lapalme. 2004. Transtype2 - an innovative
computer-assisted translation system. In The Compan-
ion Volume to the Proceedings of 42st Annual Meeting
of the Association for Computational Linguistics, pages
94?97, Barcelona, Spain, jul. Association for Computa-
tional Linguistics. TT2.
Nizar Habash and Jun Hu. 2009. Improving arabic-
chinese statistical machine translation using english
as pivot language. In StatMT ?09: Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 173?181, Morristown, NJ, USA. Association for
Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325.
Shahram Khadivi, Richard Zens, and Hermann Ney. 2006.
Integration of speech to computer-assisted translation
using finite-state automata. In Proceedings of the COL-
ING/ACL on Main conference poster sessions, pages
467?474, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural lan-
guage parsing. In Suzanna Becker, Sebastian Thrun,
and Klaus Obermayer, editors, Advances in Neural
Information Processing Systems 15 - Neural Informa-
tion Processing Systems, NIPS 2002, pages 3?10. MIT
Press.
Philipp Koehn. 2009. A web-based interactive computer
aided translation tool. In Proceedings of the ACL-
IJCNLP 2009 Software Demonstrations, pages 17?20,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Anne-Marie Laurian. 1984. Machine translation : What
type of post-editing on what type of documents for
what type of users. In 10th International Conference on
Computational Linguistics and 22nd Annual Meeting
of the Association for Computational Linguistics.
Aure?lien Max. 2009. Sub-sentencial paraphrasing by con-
textual pivot translation. In Proceedings of the 2009
Workshop on Applied Textual Inference, pages 18?26,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Aure?lien Max. 2010. Example-based paraphrasing for
improved phrase-based statistical machine translation.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Daisuke Morita and Toru Ishida. 2009. Designing pro-
tocols for collaborative translation. In PRIMA ?09:
Proceedings of the 12th International Conference on
Principles of Practice in Multi-Agent Systems, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Robert Munro. 2010. Haiti emergency response: the
power of crowdsourcing and SMS. Relief 2.0 in Haiti,
Stanford, CA.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alexander Fraser,
Shankar Kumar, Libin Shen, David Smith, Katherine
Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev.
2004. A smorgasbord of features for statistical ma-
chine translation. In HLT-NAACL, pages 161?168.
Alex Quinn and Benjamin B. Bederson. 2009. A tax-
onomy of distributed human computation. Technical
Report HCIL-2009-23, University of Maryland, Octo-
ber.
D. Shahaf and E. Horvitz. 2010. Generalized task markets
for human and machine computation. In AAAI 2010,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2009. TER-Plus: Paraphrases, Semantic,
and Alignment Enhancements to Translation Edit Rate.
Machine Translation.
Radu Soricut and Abdessamad Echihabi. 2010. Trustrank:
Inducing trust in automatic translations via ranking. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 612?621,
Uppsala, Sweden, July. Association for Computational
Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In HLT ?01:
Proceedings of the first international conference on
Human language technology research, pages 1?8, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
137
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 345?348,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Crowdsourcing the evaluation of a domain-adapted named entity
recognition system
Asad B. Sayeed, Timothy J. Meyer,
Hieu C. Nguyen, Olivia Buzek
Department of Computer Science
University of Maryland
College Park, MD 20742
asayeed@cs.umd.edu,
tmeyer1@umd.edu,
{hcnguyen88,olivia.buzek}
@gmail.com
Amy Weinberg
Department of Linguistics
University of Maryland
College Park, MD 20742
weinberg@umiacs.umd.edu
Abstract
Named entity recognition systems sometimes
have difficulty when applied to data from do-
mains that do not closely match the training
data. We first use a simple rule-based tech-
nique for domain adaptation. Data for robust
validation of the technique is then generated,
and we use crowdsourcing techniques to show
that this strategy produces reliable results even
on data not seen by the rule designers. We
show that it is possible to extract large im-
provements on the target data rapidly at low
cost using these techniques.
1 Introduction
1.1 Named entities and errors
In this work, we use crowdsourcing to generate eval-
uation data to validate simple techniques designed to
adapt a widely-used high-performing named entity
recognition system to new domains. Specifically, we
achieve a roughly 10% improvement in precision on
text from the information technology (IT) business
press via post hoc rule-based error reduction. We
first tested the system on a small set of data that we
annotated ourselves. Then we collected data from
Amazon Mechanical Turk in order to demonstrate
that the gain is stable. To our knowledge, there is no
previous work on crowdsourcing as a rapid means
of evaluating error mitigation in named entity rec-
ognizer development.
Named entity recognition (NER) is a well-known
problem in NLP which feeds into many other re-
lated tasks such as information retrieval (IR) and
machine translation (MT) and more recently social
network discovery and opinion mining. Generally,
errors in the underlying NER technology correlate
with a steep price in performance in the NLP sys-
tems further along a processing pipeline, as incor-
rect entities propagate into incorrect translations or
erroneous graphs of social networks.
Not all errors carry the same price. In some ap-
plications, omitting a named entity has the conse-
quence of reducing the availability of training data,
but including an incorrectly identified piece of text
as as a named entity has the consequence of pro-
ducing misleading results. Our application would
be opinion mining; an omitted entity may prevent
the system from attributing an opinion to a source,
but an incorrect entity reveals non-existent opinion
sources.
Machine learning is currently used extensively in
building NER systems. One such system is BBN?s
Identifinder (Bikel et al, 1999). The IdentiFinder al-
gorithm, based on Hidden Markov Models, has been
shown to achieve F-measure scores above 90% when
the training and testing data happen to be derived
from Wall Street Journal text produced in the 1990s.
We use IdentiFinder 3.3 as a starting point for per-
formance improvement in this paper.
The use of machine learning in existing systems
requires us to produce new and costly training data
if we want to adapt these systems directly to other
domains. Our post hoc error reduction strategy is
therefore profoundly different: it relieves us of the
burden of generating complete training examples.
The data we generate are strictly corrections of the
existing system?s output. Our thus cheaper evalua-
tion is therefore primarily on improvements to pre-
345
cision, while minimizing damage to recall, unlike
an evaluation based on retraining with new, fully-
annotated text.
1.2 Crowdsourcing
Crowdsourcing is the use of the mass collabora-
tion of Internet passers-by for large enterprises on
the World Wide Web such as Wikipedia and survey
companies. However, a generalized way to mon-
etize the many small tasks that make up a larger
task is relatively new. Crowdsourcing platforms
like Amazon Mechanical Turk have allowed some
NLP researchers to acquire data for small amounts
of money from large, unspecified groups of Internet
users (Snow et al, 2008; Callison-Burch, 2009).
The use of crowdsourcing for an NLP annotation
task required careful definition of the specifics of
the task. The individuals who perform these tasks
have no specific training, and they are trying to get
through as many tasks as they can, so each task must
be specified very simply and clearly.
Part of our work was to define a named entity
error detection task simply enough that the results
would be consistent across anonymous annotators.
2 Methodology
2.1 Process overview
The overall process for running this experiment was
as follows (figure 1).
Figure 1: Diagram of data pipeline.
First, we performed an initial performance assess-
ment of IdentiFinder on our domain. We selected
200 articles from an IT trade journal. IdentiFinder
was used to tag persons and organizations in these
documents. Domain experts (in this case, the au-
thors of this paper) analyzed the entity tags pro-
duced by the NER system and annotated the erro-
neous tags. We built an error reduction system based
on our error analysis. We then ran the IdentiFinder
output through the error reduction system and eval-
uated its performance against our annotations.
Next, we constructed an Amazon Mechanical
Turk-based interface for na??ve web users or ?Turk-
ers? to annotate the IdentiFinder entities for errors.
We measured the interannotator agreement between
the Turkers and the domain experts, and we evalu-
ated the IdentiFinder output and the repaired output
against the expert-generated and Turker gold stan-
dards.
We selected a new batch of 800 articles and ran
IdentiFinder and the filters on them, and we again
ran our Mechanical Turk application on the Iden-
tiFinder output. We measured the performance of
IdentiFinder and filtered output against the Turker
annotations.
2.2 Performance evaluation
Performance is evaluated in terms of standard pre-
cision and recall of entities. If the system output
contains a person or organization labelled correctly
as such, it considers this to be a hit. If it contains a
person or organization that is mislabelled or other-
wise incorrect in the gold standard annotation, it is
a miss. We compute the F-measure as the harmonic
mean of precision and recall.
As the IdentiFinder output is the baseline, and we
ignore missed entities, by definition the baseline re-
call is 100%.
3 Experiments and results
Here we delve into further detail about the tech-
niques we used and the results that they yielded. The
results are summarized in table 1.
3.1 Baseline performance assessment
We randomly selected 200 documents from Infor-
mationWeek, a major weekly magazine in the IT
business press. Running them through IdentiFinder
produces NIST ACE-standard XML entity markup.
We focused on the ENAMEX tags of person and or-
ganization type that IdentiFinder produces.
After we annotated the ENAMEX tags for errors,
we found that closer inspection of the errors in the
IdentiFinder output allowed us to classify the major-
ity of them into three major categories:
346
Annotator Collection System Precision Recall F-measure
Authors 200 document IdentiFinder only 0.74 1 0.85
Authors 200 document Filtered 0.86 0.98 0.92
MTurk 200 document IdentiFinder only 0.69 1 0.82
MTurk 200 document Filtered 0.79 0.97 0.87
MTurk 800 document IdentiFinder only 0.67 1 0.80
MTurk 800 document Filtered 0.77 0.95 0.85
Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.
? IdentiFinder tags words that are simply not
named entities.
? IdentiFinder assigns the wrong category (per-
son or organization) to an entity.
? IdentiFinder includes extraneous words in an
otherwise correct entity.
The second and third types of error are particu-
larly challenging. An example of the second type is
the following:
Yahoo is a reasonably strong competitor
to Google. It gets about half as much on-
line revenue and search traffic as Google,
. . .
Google is marked twice incorrectly as being a person
rather than an organization.
Finally, here is an example of the third error type:
A San Diego bartender reported that Bill
Gates danced the night away in his bar on
Nov. 11.
IdentiFinder incorrectly marks ?danced? as part of a
person tag.
We were able to find the precision of IdentiFinder
against our annotations: 0.74. This is poorer than the
reported performance of IdentiFinder on Wall Street
Journal text (Bikel et al, 1999).
3.2 Domain-specific error reduction
We wrote a series of rule-based filters to remove
instances of the error types?of which there were
many subtypes?described in the previous sec-
tion. For instance, the third example above was
eliminated via the use of a part-of-speech tagger;
?danced? was labelled as a verb, and entities with
tagged verbs were removed. In the second case,
the mislabelling of Google as a person rather than
an organization is identified by looking at Identi-
Finder?s majority labelling of Google throughout the
corpus?as an organization. Simple rules about cap-
italization allow instances like the first example to
be identified as errors.
This step increases the precision of the system
output to 86%, while only sacrificing a tiny amount
of recall. We see that this 10% increase is main-
tained even on the Mechanical Turk-generated an-
notations.
3.3 Mechanical Turk tasks
The basic unit of Mechanical Turk is the Human In-
telligence Task (HIT). Turkers select HITs presented
as web pages and perform the described task. Data-
collectors create HITs and pay Amazon to disburse
small amounts of money to Turkers who complete
them.
We designed our Mechanical Turk process so that
every HIT we create corresponds to an IdentiFinder-
marked document. Within its corresponding HIT,
each document is broken up into paragraphs. Fol-
lowing every paragraph is a table whose rows con-
sist of every person/organization ENAMEX discov-
ered by IdentiFinder and whose columns consist of
one of the four categories: ?Person,? ?Organization,?
?Neither,? and ?Don?t Know.? Then for each entity,
the user selects exactly one of the four options.
Each HIT is assigned to three different Turkers.
Every entity in that HIT is assigned a person or or-
ganization ENAMEX tag if two of the three Turkers
agreed it was one of those (majority vote); other-
wise, it is marked as an invalid entity.
We calculated the agreement between our annota-
tions and those developed from the Turker majority
347
vote scheme. This yields a Cohen?s ? of 0.68. We
considered this to be substantial agreement.
After processing the same 200 document set from
our own annotation, we found that the precision
of IdentiFinder was 69%, but after error reduction,
it increased to 79% with only a miniscule loss of
known valid entities (recall).
We then took another 800 documents from Infor-
mationWeek and ran them through IdentiFinder. We
did not annotate these documents ourselves, but in-
stead turned them over to Turkers. IdentiFinder out-
put alone has a 67% precision, but after error reduc-
tion, it rises to 77%, and recall is still minimally af-
fected.
4 Discussion
4.1 Benefits
It appears that high-performing NER systems ex-
hibit rather severe domain adaption problems. The
performance of IdentiFinder is quite low on the IT
business press. However, a simple rule-based sys-
tem was able to gain 10% improvement in precision
with little recall sacrificed. This is a particularly im-
portant improvement in applications with low toler-
ance for erroneous entities.
However, rule-based systems built by experts are
known to be vulnerable to new data unseen by the
experts. In order to apply this domain-specific error
reduction reliably, it has to be tested on data gathered
elsewhere. We used crowdsourced data to show that
the rule-based system was robust when confronted
with data that the designers did not see.
One danger in crowdsourcing is a potential lack
of commitment on the part of the annotators, as they
attempt to get through tasks as quickly as possible.
It turns out that in an NER context, we can design a
crowdsourced task that yields relatively reliable re-
sults across data sets by ensuring that for every data
point, there were multiple annotators making only
simple decisions about entity classification.
This method also provides us with a source of eas-
ily acquired supervised training data for testing more
advanced techniques, if required.
4.2 Costs
It took not more than an estimated two person weeks
to complete this work. This includes doing the
expert annotations, designing the Mechanical Turk
tasks, and building the domain-specific error reduc-
tion rules.
For each HIT, each annotator was paid 0.05 USD.
For three annotators for 1000 documents, that is
150.00 USD (plus additional small Amazon sur-
charges and any taxes that apply).
5 Conclusions and Future Work
This work was done on a single publication in a sin-
gle domain. One future experiment would be to see
whether these results are reliable across other pub-
lications in the domain. Another set of experiments
would be to determine the optimum number of an-
notators; we assumed three, but cross-domain results
may be more stable with more annotators.
Retraining an NER system for a particular domain
can be expensive if new annotations must be gen-
erated from scratch. While there is work on using
advanced machine learning techniques for domain
transfer (Guo et al, 2009), simply repairing the the
errors post hoc via a rule-based system can have a
low cost for high gains. This work shows a case
where the results are reliable and the verification
simple, in a context where reducing false positives
is a high priority.
Acknowledgements
This paper is based upon work supported by the Na-
tional Science Foundation under Grant IIS-0729459.
This research was also supported in part by NSF
award IIS-0838801.
References
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP 2009, Singapore, August.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL 2009, Morristown, NJ, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008, Morristown, NJ, USA.
348
Transactions of the Association for Computational Linguistics, 1 (2013) 165?178. Action Editor: David Chiang.
Submitted 11/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Learning to translate with products of novices: a suite of open-ended
challenge problems for teaching MT
Adam Lopez1, Matt Post1, Chris Callison-Burch1,2, Jonathan Weese, Juri Ganitkevitch,
Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,
Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,
Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao?
Department of Computer Science, Johns Hopkins University
1Human Language Technology Center of Excellence, Johns Hopkins University
2Computer and Information Science Department, University of Pennsylvania
Abstract
Machine translation (MT) draws from several
different disciplines, making it a complex sub-
ject to teach. There are excellent pedagogical
texts, but problems in MT and current algo-
rithms for solving them are best learned by
doing. As a centerpiece of our MT course,
we devised a series of open-ended challenges
for students in which the goal was to im-
prove performance on carefully constrained
instances of four key MT tasks: alignment,
decoding, evaluation, and reranking. Students
brought a diverse set of techniques to the prob-
lems, including some novel solutions which
performed remarkably well. A surprising and
exciting outcome was that student solutions
or their combinations fared competitively on
some tasks, demonstrating that even newcom-
ers to the field can help improve the state-of-
the-art on hard NLP problems while simulta-
neously learning a great deal. The problems,
baseline code, and results are freely available.
1 Introduction
A decade ago, students interested in natural lan-
guage processing arrived at universities having been
exposed to the idea of machine translation (MT)
primarily through science fiction. Today, incoming
students have been exposed to services like Google
Translate since they were in secondary school or ear-
lier. For them, MT is science fact. So it makes sense
to teach statistical MT, either on its own or as a unit
? The first five authors were instructors and the remaining au-
thors were students in the worked described here. This research
was conducted while Chris Callison-Burch was at Johns Hop-
kins University.
in a class on natural language processing (NLP), ma-
chine learning (ML), or artificial intelligence (AI). A
course that promises to show students how Google
Translate works and teach them how to build some-
thing like it is especially appealing, and several uni-
versities and summer schools now offer such classes.
There are excellent introductory texts?depending
on the level of detail required, instructors can choose
from a comprehensive MT textbook (Koehn, 2010),
a chapter of a popular NLP textbook (Jurafsky and
Martin, 2009), a tutorial survey (Lopez, 2008), or
an intuitive tutorial on the IBM Models (Knight,
1999b), among many others.
But MT is not just an object of academic study.
It?s a real application that isn?t fully perfected, and
the best way to learn about it is to build an MT sys-
tem. This can be done with open-source toolkits
such as Moses (Koehn et al, 2007), cdec (Dyer et
al., 2010), or Joshua (Ganitkevitch et al, 2012), but
these systems are not designed for pedagogy. They
are mature codebases featuring tens of thousands of
source code lines, making it difficult to focus on
their core algorithms. Most tutorials present them
as black boxes. But our goal is for students to learn
the key techniques in MT, and ideally to learn by
doing. Black boxes are incompatible with this goal.
We solve this dilemma by presenting students
with concise, fully-functioning, self-contained com-
ponents of a statistical MT system: word alignment,
decoding, evaluation, and reranking. Each imple-
mentation consists of a na??ve baseline algorithm in
less than 150 lines of Python code. We assign them
to students as open-ended challenges in which the
goal is to improve performance on objective eval-
uation metrics as much as possible. This setting
mirrors evaluations conducted by the NLP research
165
community and by the engineering teams behind
high-profile NLP projects such as Google Translate
and IBM?s Watson. While we designate specific al-
gorithms as benchmarks for each task, we encour-
age creativity by awarding more points for the best
systems. As additional incentive, we provide a web-
based leaderboard to display standings in real time.
In our graduate class on MT, students took a va-
riety of different approaches to the tasks, in some
cases devising novel algorithms. A more exciting re-
sult is that some student systems or combinations of
systems rivaled the state of the art on some datasets.
2 Designing MT Challenge Problems
Our goal was for students to freely experiment with
different ways of solving MT problems on real data,
and our approach consisted of two separable com-
ponents. First, we provided a framework that strips
key MT problems down to their essence so students
could focus on understanding classic algorithms or
invent new ones. Second, we designed incentives
that motivated them to improve their solutions as
much as possible, encouraging experimentation with
approaches beyond what we taught in class.
2.1 Decoding, Reranking, Evaluation, and
Alignment for MT (DREAMT)
We designed four assignments, each corresponding
to a real subproblem in MT: alignment, decoding,
evaluation, and reranking.1 From the more general
perspective of AI, they emphasize the key problems
of unsupervised learning, search, evaluation design,
and supervised learning, respectively. In real MT
systems, these problems are highly interdependent,
a point we emphasized in class and at the end of each
assignment?for example, that alignment is an exer-
cise in parameter estimation for translation models,
that model choice is a tradeoff between expressivity
and efficient inference, and that optimal search does
not guarantee optimal accuracy. However, present-
ing each problem independently and holding all else
constant enables more focused exploration.
For each problem we provided data, a na??ve solu-
tion, and an evaluation program. Following Bird et
al. (2008) and Madnani and Dorr (2008), we imple-
mented the challenges in Python, a high-level pro-
1http://alopez.github.io/dreamt
gramming language that can be used to write very
concise programs resembling pseudocode.2,3 By de-
fault, each baseline system reads the test data and
generates output in the evaluation format, so setup
required zero configuration, and students could be-
gin experimenting immediately. For example, on re-
ceipt of the alignment code, aligning data and eval-
uating results required only typing:
> align | grade
Students could then run experiments within minutes
of beginning the assignment.
Three of the four challenges also included unla-
beled test data (except the decoding assignment, as
explained in ?4). We evaluated test results against a
hidden key when assignments were submitted.
2.2 Incentive Design
We wanted to balance several pedagogical goals: un-
derstanding of classic algorithms, free exploration
of alternatives, experience with typical experimental
design, and unhindered collaboration.
Machine translation is far from solved, so we ex-
pected more than reimplementation of prescribed al-
gorithms; we wanted students to really explore the
problems. To motivate exploration, we made the as-
signments competitive. Competition is a powerful
force, but must be applied with care in an educa-
tional setting.4 We did not want the consequences
of ambitious but failed experiments to be too dire,
and we did not want to discourage collaboration.
For each assignment, we guaranteed a passing
grade for matching the performance of a specific tar-
get alorithm. Typically, the target was important
but not state-of-the-art: we left substantial room for
improvement, and thus competition. We told stu-
dents the exact algorithm that produced the target ac-
curacy (though we expected them to derive it them-
selves based on lectures, notes, or literature). We
did not specifically require them to implement it, but
the guarantee of a passing grade provided a power-
ful incentive for this to be the first step of each as-
signment. Submissions that beat this target received
additional credit. The top five submissions received
full credit, while the top three received extra credit.
2http://python.org
3Some well-known MT systems have been implemented in
Python (Chiang, 2007; Huang and Chiang, 2007).
4Thanks to an anonymous reviewer for this turn of phrase.
166
This scheme provided strong incentive to continue
experimentation beyond the target alorithm.5
For each assignment, students could form teams
of any size, under three rules: each team had to pub-
licize its formation to the class, all team members
agreed to receive the same grade, and teams could
not drop members. Our hope was that these require-
ments would balance the perceived competitive ad-
vantage of collaboration against a reluctance to take
(and thus support) teammates who did not contribute
to the competitive effort.6 This strategy worked: out
of sixteen students, ten opted to work collaboratively
on at least one assignment, always in pairs.
We provided a web-based leaderboard that dis-
played standings on the test data in real time, iden-
tifying each submission by a pseudonymous han-
dle known only to the team and instructors. Teams
could upload solutions as often as they liked before
the assignment deadline. The leaderboard displayed
scores of the default and target alorithms. This in-
centivized an early start, since teams could verify
for themselves when they met the threshold for a
passing grade. Though effective, it also detracted
from realism in one important way: it enabled hill-
climbing on the evaluation metric. In early assign-
ments, we observed a few cases of this behavior,
so for the remaining assignments, we modified the
leaderboard so that changes in score would only be
reflected once every twelve hours. This strategy
trades some amount of scientific realism for some
measure of incentive, a strategy that has proven
effective in other pedagogical tools with real-time
feedback (Spacco et al, 2006).
To obtain a grade, teams were required to sub-
mit their results, share their code privately with the
instructors, and publicly describe their experimen-
tal process to the class so that everyone could learn
from their collective effort. Teams were free (but not
required) to share their code publicly at any time.
5Grades depend on institutional norms. In our case, high grades
in the rest of class combined with matching all assignment tar-
get alorithms would earn a B+; beating two target alorithms
would earn an A-; top five placement on any assignment would
earn an A; and top three placement compensated for weaker
grades in other course criteria. Everyone who completed all
four assignments placed in the top five at least once.
6The equilibrium point is a single team, though this team would
still need to decide on a division of labor. One student contem-
plated organizing this team, but decided against it.
Some did so after the assignment deadline.
3 The Alignment Challenge
The first challenge was word alignment: given a par-
allel text, students were challenged to produce word-
to-word alignments with low alignment error rate
(AER; Och and Ney, 2000). This is a variant of a
classic assignment not just in MT, but in NLP gen-
erally. Klein (2005) describes a version of it, and we
know several other instructors who use it.7 In most
of these, the object is to implement IBM Model 1
or 2, or a hidden Markov model. Our version makes
it open-ended by asking students to match or beat an
IBM Model 1 baseline.
3.1 Data
We provided 100,000 sentences of parallel data from
the Canadian Hansards, totaling around two million
words.8 This dataset is small enough to align in
a few minutes with our implementation?enabling
rapid experimentation?yet large enough to obtain
reasonable results. In fact, Liang et al (2006) report
alignment accuracy on data of this size that is within
a fraction of a point of their accuracy on the com-
plete Hansards data. To evaluate, we used manual
alignments of a small fraction of sentences, devel-
oped by Och and Ney (2000), which we obtained
from the shared task resources organized by Mihal-
cea and Pedersen (2003). The first 37 sentences
of the corpus were development data, with manual
alignments provided in a separate file. Test data con-
sisted of an additional 447 sentences, for which we
did not provide alignments.9
3.2 Implementation
We distributed three Python programs with the
data. The first, align, computes Dice?s coefficient
(1945) for every pair of French and English words,
then aligns every pair for which its value is above an
adjustable threshold. Our implementation (most of
7Among them, Jordan Boyd-Graber, John DeNero, Philipp
Koehn, and Slav Petrov (personal communication).
8http://www.isi.edu/natural-language/download/hansard/
9This invited the possibility of cheating, since alignments of the
test data are publicly available on the web. We did not adver-
tise this, but as an added safeguard we obfuscated the data by
distributing the test sentences randomly throughout the file.
167
Listing 1 The default aligner in DREAMT: thresh-
olding Dice?s coefficient.
for (f, e) in bitext:
for f_i in set(f):
f_count[f_i] += 1
for e_j in set(e):
fe_count[(f_i,e_j)] += 1
for e_j in set(e):
e_count[e_j] += 1
for (f_i, e_j) in fe_count.keys():
dice[(f_i,e_j)] = \
2.0 * fe_count[(f_i, e_j)] / \
(f_count[f_i] + e_count[e_j])
for (f, e) in bitext:
for (i, f_i) in enumerate(f):
for (j, e_j) in enumerate(e):
if dice[(f_i,e_j)] >= cutoff:
print "%i-%i " % (i,j)
which is shown in Listing 1) is quite close to pseu-
docode, making it easy to focus on the algorithm,
one of our pedagogical goals. The grade program
computes AER and optionally prints an alignment
grid for sentences in the development data, showing
both human and automatic alignments. Finally the
check program verifies that the results represent
a valid solution, reporting an error if not?enabling
students to diagnose bugs in their submissions.
The default implementation enabled immediate
experimentation. On receipt of the code, students
were instructed to align the first 1,000 sentences and
compute AER using a simple command.
> align -n 1000 | grade By varying the
number of input sentences and the threshold for an
alignment, students could immediately see the effect
of various parameters on alignment quality.
We privately implemented IBM Model 1 (Brown
et al, 1993) as the target alorithm for a passing
grade. We ran it for five iterations with English
as the target language and French as the source.
Our implementation did not use null alignment
or symmetrization?leaving out these common im-
provements offered students the possibility of dis-
covering them independently, and thereby rewarded.
A
E
R
?
10
0
20
30
40
50
60
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 1: Submission history for the alignment challenge.
Dashed lines represent the default and baseline system
performance. Each colored line represents a student, and
each dot represents a submission. For clarity, we show
only submissions that improved the student?s AER.
3.3 Challenge Results
We received 209 submissions from 11 teams over a
period of two weeks (Figure 1). Everyone eventually
matched or exceeded IBM Model 1 AER of 31.26.
Most students implemented IBM Model 1, but we
saw many other solutions, indicating that many truly
experimented with the problem:
? Implementing heuristic constraints to require
alignment of proper names and punctuation.
? Running the algorithm on stems rather than sur-
face words.
? Initializing the first iteration of Model 1 with
parameters estimated on the observed align-
ments in the development data.
? Running Model 1 for many iterations. Most re-
searchers typically run Model 1 for five itera-
tions or fewer, and there are few experiments
in the literature on its behavior over many iter-
ations, as there are for hidden Markov model
taggers (Johnson, 2007). Our students carried
out these experiments, reporting runs of 5, 20,
100, and even 2000 iterations. No improve-
ment was observed after 20 iterations.
168
? Implementing various alternative approaches
from the literature, including IBM Model 2
(Brown et al, 1993), competitive linking
(Melamed, 2000), and smoothing (Moore,
2004).
One of the best solutions was competitive linking
with Dice?s coefficient, modified to incorporate the
observation that alignments tend to be monotonic by
restricting possible alignment points to a window of
eight words around the diagonal. Although simple,
it acheived an AER of 18.41, an error reduction over
Model 1 of more than 40%.
The best score compares unfavorably against a
state-of-the-art AER of 3.6 (Liu et al, 2010). But
under a different view, it still represents a significant
amount of progress for an effort taking just over two
weeks: on the original challenge from which we ob-
tained the data (Mihalcea and Pedersen, 2003) the
best student system would have placed fifth out of
fifteen systems. Consider also the combined effort of
all the students: when we trained a perceptron clas-
sifier on the development data, taking each student?s
prediction as a feature, we obtained an AER of 15.4,
which would have placed fourth on the original chal-
lenge. This is notable since none of the systems
incorporated first-order dependencies on the align-
ments of adjacent words, long noted as an impor-
tant feature of the best alignment models (Och and
Ney, 2003). Yet a simple system combination of stu-
dent assignments is as effective as a hidden Markov
Model trained on a comparable amount of data (Och
and Ney, 2003).
It is important to note that AER does not neces-
sarily correlate with downstream performance, par-
ticularly on the Hansards dataset (Fraser and Marcu,
2007). We used the conclusion of the assignment as
an opportunity to emphasize this point.
4 The Decoding Challenge
The second challenge was decoding: given a fixed
translation model and a set of input sentences, stu-
dents were challenged to produce translations with
the highest model score. This challenge introduced
the difficulties of combinatorial optimization under
a deceptively simple setup: the model we provided
was a simple phrase-based translation model (Koehn
et al, 2003) consisting only of a phrase table and tri-
gram language model. Under this simple model, for
a French sentence f of length I , English sentence
e of length J , and alignment a where each element
consists of a span in both e and f such that every
word in both e and f is aligned exactly once, the
conditional probability of e and a given f is as fol-
lows.10
p(e, a|f) =
?
?i,i?,j,j???a
p(f i?i |ej
?
j )
J+1?
j=1
p(ej |ej?1, ej?2)
(1)
To evaluate output, we compute the conditional
probability of e as follows.
p(e|f) =
?
a
p(e, a|f) (2)
Note that this formulation is different from the typ-
ical Viterbi objective of standard beam search de-
coders, which do not sum over all alignments, but
approximate p(e|f) by maxa p(e, a|f). Though the
computation in Equation 2 is intractable (DeNero
and Klein, 2008), it can be computed in a few min-
utes via dynamic programming on reasonably short
sentences. We ensured that our data met this crite-
rion. The corpus-level probability is then the prod-
uct of all sentence-level probabilities in the data.
The model includes no distortion limit or distor-
tion model, for two reasons. First, leaving out the
distortion model slightly simplifies the implementa-
tion, since it is not necessary to keep track of the last
word translated in a beam decoder; we felt that this
detail was secondary to understanding the difficulty
of search over phrase permutations. Second, it actu-
ally makes the problem more difficult, since a simple
distance-based distortion model prefers translations
with fewer permutations; without it, the model may
easily prefer any permutation of the target phrases,
making even the Viterbi search problem exhibit its
true NP-hardness (Knight, 1999a; Zaslavskiy et al,
2009).
Since the goal was to find the translation with the
highest probability, we did not provide a held-out
test set; with access to both the input sentences and
10For simplicity, this formula assumes that e is padded with two
sentence-initial symbols and one sentence-final symbol, and
ignores the probability of sentence segmentation, which we
take to be uniform.
169
the model, students had enough information to com-
pute the evaluation score on any dataset themselves.
The difficulty of the challenge lies simply in finding
the translation that maximizes the evaluation. In-
deed, since the problem is intractable, even the in-
structors did not know the true solution.11
4.1 Data
We chose 48 French sentences totaling 716 words
from the Canadian Hansards to serve as test data.
To create a simple translation model, we used the
Berkeley aligner to align the parallel text from the
first assignment, and extracted a phrase table using
the method of Lopez (2007), as implemented in cdec
(Dyer et al, 2010). To create a simple language
model, we used SRILM (Stolcke, 2002).
4.2 Implementation
We distributed two Python programs. The first,
decode, decodes the test data monotonically?
using both the language model and translation
model, but without permuting phrases. The imple-
mentation is completely self-contained with no ex-
ternal dependencies: it implements both models and
a simple stack decoding algorithm for monotonic
translation. It contains only 122 lines of Python?
orders of magnitude fewer than most full-featured
decoders. To see its similarity to pseudocode, com-
pare the decoding algorithm (Listing 2) with the
pseudocode in Koehn?s (2010) popular textbook (re-
produced here as Algorithm 1). The second pro-
gram, grade, computes the log-probability of a set
of translations, as outline above.
We privately implemented a simple stack decoder
that searched over permutations of phrases, similar
to Koehn (2004). Our implementation increased the
codebase by 44 lines of code and included param-
eters for beam size, distortion limit, and the maxi-
mum number of translations considered for each in-
put phrase. We posted a baseline to the leaderboard
using values of 50, 3, and 20 for these, respectively.
11We implemented a version of the Lagrangian relaxation algo-
rithm of Chang and Collins (2011), but found it difficult to
obtain tight (optimal) solutions without iteratively reintroduc-
ing all of the original constraints. We suspect this is due to
the lack of a distortion penalty, which enforces a strong pref-
erence towards translations with little reordering. However,
the solution found by this algorithm is only approximates the
objective implied by Equation 2, which sums over alignments.
We also posted an oracle containing the most prob-
able output for each sentence, selected from among
all submissions received so far. The intent of this
oracle was to provide a lower bound on the best pos-
sible output, giving students additional incentive to
continue improving their systems.
4.3 Challenge Results
We received 71 submissions from 10 teams (Fig-
ure 2), again exhibiting variety of solutions.
? Implementation of greedy decoder which at
each step chooses the most probable translation
from among those reachable by a single swap
or retranslation (Germann et al, 2001; Langlais
et al, 2007).
? Inclusion of heuristic estimates of future cost.
? Implementation of a private oracle. Some stu-
dents observed that the ideal beam setting was
not uniform across the corpus. They ran their
decoder under different settings, and then se-
lected the most probable translation of each
sentence.
Many teams who implemented the standard stack
decoding algorithm experimented heavily with its
pruning parameters. The best submission used ex-
tremely wide beam settings in conjunction with a
reimplementation of the future cost estimate used in
Moses (Koehn et al, 2007). Five of the submissions
beat Moses using its standard beam settings after it
had been configured to decode with our model.
We used this assignment to emphasize the im-
portance of good models: the model score of the
submissions was generally inversely correlated with
BLEU, possibly because our simple model had no
distortion limits. We used this to illustrate the differ-
ence between model error and search error, includ-
ing fortuitous search error (Germann et al, 2001)
made by decoders with less accurate search.
5 The Evaluation Challenge
The third challenge was evaluation: given a test cor-
pus with reference translations and the output of sev-
eral MT systems, students were challenged to pro-
duce a ranking of the systems that closely correlated
with a human ranking.
170
Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.
stacks = [{} for _ in f] + [{}]
stacks[0][lm.begin()] = initial_hypothesis
for i, stack in enumerate(stacks[:-1]):
for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:
for j in xrange(i+1,len(f)+1):
if f[i:j] in tm:
for phrase in tm[f[i:j]]:
logprob = h.logprob + phrase.logprob
lm_state = h.lm_state
for word in phrase.english.split():
(lm_state, word_logprob) = lm.score(lm_state, word)
logprob += word_logprob
logprob += lm.end(lm_state) if j == len(f) else 0.0
new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
if lm_state not in stacks[j] or \
stacks[j][lm_state].logprob < logprob:
stacks[j][lm_state] = new_hypothesis
winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
def extract_english(h):
return "" if h.predecessor is None else "%s%s " %
(extract_english(h.predecessor), h.phrase.english)
print extract_english(winner)
Algorithm 1 Basic stack decoding algorithm,
adapted from Koehn (2010), p. 165.
place empty hypothesis into stack 0
for all stacks 0...n? 1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hypothesis
place in stack
recombine with existing hypothesis
prune stack if too big
5.1 Data
We chose the English-to-German translation sys-
tems from the 2009 and 2011 shared task at the an-
nual Workshop for Machine Translation (Callison-
Burch et al, 2009; Callison-Burch et al, 2011), pro-
viding the first as development data and the second
as test data. We chose these sets because BLEU
(Papineni et al, 2002), our baseline metric, per-
formed particularly poorly on them; this left room
for improvement in addition to highlighting some
lo
g 1
0
p(
e|f
)?
C
-1200
-1250
-1300
-1350
-1400
-20
days
-18
days
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 2: Submission history for the decoding challenge.
The dotted green line represents the oracle over submis-
sions.
deficiencies of BLEU. For each dataset we pro-
vided the source and reference sentences along with
anonymized system outputs. For the development
data we also provided the human ranking of the sys-
171
tems, computed from pairwise human judgements
according to a formula recommended by Bojar et al
(2011).12
5.2 Implementation
We provided three simple Python programs:
evaluate implements a simple ranking of the sys-
tems based on position-independent word error rate
(PER; Tillmann et al, 1997), which computes a bag-
of-words overlap between the system translations
and the reference. The grade program computes
Spearman?s ? between the human ranking and an
output ranking. The check program simply ensures
that a submission contains a valid ranking.
We were concerned about hill-climbing on the test
data, so we modified the leaderboard to report new
results only twice a day. This encouraged students to
experiment on the development data before posting
new submissions, while still providing intermittent
feedback.
We privately implemented a version of BLEU,
which obtained a correlation of 38.6 with the human
rankings, a modest improvement over the baseline
of 34.0. Our implementation underperforms the one
reported in Callison-Burch et al (2011) since it per-
forms no tokenization or normalization of the data.
This also left room for improvement.
5.3 Evaluation Challenge Results
We received 212 submissions from 12 teams (Fig-
ure 3), again demonstrating a wide range of tech-
niques.
? Experimentation with the maximum n-gram
length and weights in BLEU.
? Implementation of smoothed versions of BLEU
(Lin and Och, 2004).
? Implementation of weighted F-measure to bal-
ance both precision and recall.
? Careful normalization of the reference and ma-
chine translations, including lowercasing and
punctuation-stripping.
12This ranking has been disputed over a series of papers (Lopez,
2012; Callison-Burch et al, 2012; Koehn, 2012). The paper
which initiated the dispute, written by the first author, was di-
rectly inspired by the experience of designing this assignment.
Sp
ea
rm
an
?s
?
0.8
0.6
0.4
-7
days
-6
days
-5
days
-4
days
-3
days
-2
days
-1
days
due
Figure 3: Submission history for the evaluation chal-
lenge.
? Implementation of several techniques used in
AMBER (Chen and Kuhn, 2005).
The best submission, obtaining a correlation of
83.5, relied on the idea that the reference and ma-
chine translation should be good paraphrases of each
other (Owczarzak et al, 2006; Kauchak and Barzi-
lay, 2006). It employed a simple paraphrase sys-
tem trained on the alignment challenge data, us-
ing the pivot technique of Bannard and Callison-
Burch (2005), and computing the optimal alignment
between machine translation and reference under a
simple model in which words could align if they
were paraphrases. When compared with the 20
systems submitted to the original task from which
the data was obtained (Callison-Burch et al, 2011),
this system would have ranked fifth, quite near the
top-scoring competitors, whose correlations ranged
from 88 to 94.
6 The Reranking Challenge
The fourth challenge was reranking: given a test cor-
pus and a large N -best list of candidate translations
for each sentence, students were challenged to select
a candidate translation for each sentence to produce
a high corpus-level BLEU score. Due to an error
our data preparation, this assignment had a simple
solution that was very difficult to improve on. Nev-
ertheless, it featured several elements that may be
useful for future courses.
172
6.1 Data
We obtained 300-best lists from a Spanish-English
translation system built with the Joshua toolkit
(Ganitkevitch et al, 2012) using data and resources
from the 2011 Workshop on Machine Translation
(Callison-Burch et al, 2011). We provided 1989
training sentences, consisting of source and refer-
ence sentences along with the candidate translations.
We also included a test set of 250 sentences, for
which we provided only the source and candidate
translations. Each candidate translation included six
features from the underlying translation system, out
of an original 21; our hope was that students might
rediscover some features through experimentation.
6.2 Implementation
We conceived of the assignment as one in which stu-
dents could apply machine learning or feature engi-
neering to the task of reranking the systems, so we
provided several tools. The first of these, learn,
was a simple program that produced a vector of
feature weights using pairwise ranking optimization
(PRO; Hopkins and May, 2011), with a perceptron
as the underlying learning algorithm. A second,
rerank, takes a weight vector as input and reranks
the sentences; both programs were designed to work
with arbitrary numbers of features. The grade pro-
gram computed the BLEU score on development
data, while check ensured that a test submission
is valid. Finally, we provided an oracle program,
which computed a lower bound on the achievable
BLEU score on the development data using a greedy
approximation (Och et al, 2004). The leaderboard
likewise displayed an oracle on test data. We did
not assign a target alorithm, but left the assignment
fully open-ended.
6.3 Reranking Challenge Outcome
For each assignment, we made an effort to create
room for competition above the target alorithm.
However, we did not accomplish this in the rerank-
ing challenge: we had removed most of the features
from the candidate translations, in hopes that stu-
dents might reinvent some of them, but we left one
highly predictive implicit feature in the data: the
rank order of the underlying translation system. Stu-
dents discovered that simply returning the first can-
didate earned a very high score, and most of them
quickly converged to this solution. Unfortunately,
the high accuracy of this baseline left little room for
additional competition. Nevertheless, we were en-
couraged that most students discovered this by acci-
dent while attempting other strategies to rerank the
translations.
? Experimentation with parameters of the PRO
algorithm.
? Substitution of alternative learning algorithms.
? Implementation of a simplified minimum
Bayes risk reranker (Kumar and Byrne, 2004).
Over a baseline of 24.02, the latter approach ob-
tained a BLEU of 27.08, nearly matching the score
of 27.39 from the underlying system despite an im-
poverished feature set.
7 Pedagogical Outcomes
Could our students have obtained similar results by
running standard toolkits? Undoubtedly. However,
our goal was for students to learn by doing: they
obtained these results by implementing key MT al-
gorithms, observing their behavior on real data, and
improving them. This left them with much more in-
sight into how MT systems actually work, and in
this sense, DREAMT was a success. At the end of
class, we requested written feedback on the design
of the assignments. Many commented positively on
the motivation provided by the challenge problems:
? The immediate feedback of the automatic grad-
ing was really nice.
? Fast feedback on my submissions and my rela-
tive position on the leaderboard kept me both
motivated to start the assignments early and to
constantly improve them. Also knowing how
well others were doing was a good way to
gauge whether I was completely off track or not
when I got bad results.
? The homework assignments were very engag-
ing thanks to the clear yet open-ended setup
and their competitive aspects.
Students also commented that they learned a lot
about MT and even research in general:
173
Question 1 2 3 4 5 N/A
Feedback on my work for this course is useful - - - 4 9 3
This course enhanced my ability to work effectively in a team 1 - 5 8 2 -
Compared to other courses at this level, the workload for this course is high - 1 7 6 1 1
Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
? I learned the most from the assignments.
? The assignments always pushed me one step
more towards thinking out loud how the par-
ticular task can be completed.
? I appreciated the setup of the homework prob-
lems. I think it has helped me learn how to
set up and attack research questions in an or-
ganized way. I have a much better sense for
what goes into an MT system and what prob-
lems aren?t solved.
We also received feedback through an anonymous
survey conducted at the end of the course before
posting final grades. Each student rated aspects
of the course on a five point Likert scale, from 1
(strongly disagree) to 5 (strongly agree). Several
questions pertained to assignments (Table 1), and al-
lay two possible concerns about competition: most
students felt that the assignments enhanced their col-
laborative skills, and that their open-endedness did
not result in an overload of work. For all survey
questions, student satisfaction was higher than av-
erage for courses in our department.
8 Discussion
DREAMT is inspired by several different ap-
proaches to teaching NLP, AI, and computer sci-
ence. Eisner and Smith (2008) teach NLP using
a competitive game in which students aim to write
fragments of English grammar. Charniak et al
(2000) improve the state-of-the-art in a reading com-
prehension task as part of a group project. Christo-
pher et al (1993) use NACHOS, a classic tool for
teaching operating systems by providing a rudimen-
tary system that students then augment. DeNero and
Klein (2010) devise a series of assignments based
on Pac-Man, for which students implement several
classic AI techniques. A crucial element in such ap-
proaches is a highly functional but simple scaffold-
ing. The DREAMT codebase, including grading and
validation scripts, consists of only 656 lines of code
(LOC) over four assignments: 141 LOC for align-
ment, 237 LOC for decoding, 86 LOC for evalua-
tion, and 192 LOC for reranking. To simplify imple-
mentation further, the optional leaderboard could be
delegated to Kaggle.com, a company that organizes
machine learning competitions using a model sim-
ilar to the Netflix Challenge (Bennet and Lanning,
2007), and offers pro bono use of its services for
educational challenge problems. A recent machine
learning class at Oxford hosted its assignments on
Kaggle (Phil Blunsom, personal communication).
We imagine other uses of DREAMT. It could be
used in an inverted classroom, where students view
lecture material outside of class and work on prac-
tical problems in class. It might also be useful in
massive open online courses (MOOCs). In this for-
mat, course material (primarily lectures and quizzes)
is distributed over the internet to an arbitrarily large
number of interested students through sites such as
coursera.org, udacity.com, and khanacademy.org. In
many cases, material and problem sets focus on spe-
cific techniques. Although this is important, there is
also a place for open-ended problems on which stu-
dents apply a full range of problem-solving skills.
Automatic grading enables them to scale easily to
large numbers of students.
On the scientific side, the scale of MOOCs might
make it possible to empirically measure the effec-
tiveness of hands-on or competitive assignments,
by comparing course performance of students who
work on them against that of those who do not.
Though there is some empirical work on competi-
tive assignments in the computer science education
literature (Lawrence, 2004; Garlick and Akl, 2006;
Regueras et al, 2008; Ribeiro et al, 2009), they
generally measure student satisfaction and retention
rather than the more difficult question of whether
such assignments actually improve student learning.
However, it might be feasible to answer such ques-
174
tions in large, data-rich virtual classrooms offered
by MOOCs. This is an interesting potential avenue
for future work.
Because our class came within reach of state-of-
the-art on each problem within a matter of weeks,
we wonder what might happen with a very large
body of competitors. Could real innovation oc-
cur? Could we solve large-scale problems? It may
be interesting to adopt a different incentive struc-
ture, such as one posed by Abernethy and Frongillo
(2011) for crowdsourcing machine learning prob-
lems: rather than competing, everyone works to-
gether to solve a shared task, with credit awarded
proportional to the contribution that each individual
makes. In this setting, everyone stands to gain: stu-
dents learn to solve problems as they are found in
the real world, instructors learn new insights into the
problems they pose, and, in the long run, users of
AI technology benefit from overall improvements.
Hence it is possible that posing open-ended, real-
world problems to students might be a small piece
of the puzzle of providing high-quality NLP tech-
nologies.
Acknowledgments
We are grateful to Colin Cherry and Chris Dyer
for testing the assignments in different settings and
providing valuable feedback, and to Jessie Young
for implementing a dual decomposition solution to
the decoding assignment. We thank Jason Eis-
ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,
Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-
ney Napoles, Michael Rushanan, Joanne Selinski,
Svitlana Volkova, and the anonymous reviewers for
lively discussion and helpful comments on previous
drafts of this paper. Any errors are our own.
References
J. Abernethy and R. M. Frongillo. 2011. A collaborative
mechanism for crowdsourcing prediction problems. In
Proc. of NIPS.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
J. Bennet and S. Lanning. 2007. The netflix prize. In
Proc. of the KDD Cup and Workshop.
S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008.
Multidisciplinary instruction with the natural language
toolkit. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
O. Bojar, M. Ercegovc?evic?, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2).
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 work-
shop on statistical machine translation. In Proc. of
WMT.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. In Proc. of EMNLP.
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,
M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,
W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Read-
ing comprehension programs in a statistical-language-
processing class. In Proc. of Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
B. Chen and R. Kuhn. 2005. AMBER: A modified
BLEU, enhanced ranking metric. In Proc. of WMT.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
W. A. Christopher, S. J. Procter, and T. E. Anderson.
1993. The nachos instructional operating system. In
Proc. of USENIX.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL.
J. DeNero and D. Klein. 2010. Teaching introductory
articial intelligence with Pac-Man. In Proc. of Sym-
posium on Educational Advances in Artificial Intelli-
gence.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
J. Eisner and N. A. Smith. 2008. Competitive grammar
writing. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
175
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3).
J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and
C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and paraphrases. In Proc. of WMT.
R. Garlick and R. Akl. 2006. Intra-class competitive
assignments in CS2: A one-year study. In Proc. of
International Conference on Engineering Education.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. of ACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing. Prentice Hall, 2nd edition.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
D. Klein. 2005. A core-tools statistical NLP course. In
Proc. of Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL.
K. Knight. 1999a. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4).
K. Knight. 1999b. A statistical MT tutorial workbook.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. of AMTA.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn. 2012. Simulating human judgment in machine
translation evaluation campaigns. In Proc. of IWSLT.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
P. Langlais, A. Patry, and F. Gotti. 2007. A greedy de-
coder for phrase-based statistical machine translation.
In Proc. of TMI.
R. Lawrence. 2004. Teaching data structures using
competitive games. IEEE Transactions on Education,
47(4).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3).
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3).
A. Lopez. 2012. Putting human assessments of machine
translation systems in order. In Proc. of WMT.
N. Madnani and B. Dorr. 2008. Combining open-source
with research to re-engineer a hands-on introductory
NLP course. In Proc. of Workshop on Issues in Teach-
ing Computational Linguistics.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2).
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proc. on Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2004. Improving IBM word alignment
model 1. In Proc. of ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
NAACL.
K. Owczarzak, D. Groves, J. V. Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
L. Regueras, E. Verdu?, M. Verdu?, M. Pe?rez, J. de Castro,
and M. Mun?oz. 2008. Motivating students through
on-line competition: An analysis of satisfaction and
learning styles.
P. Ribeiro, M. Ferreira, and H. Simo?es. 2009. Teach-
ing artificial intelligence and logic programming in a
competitive environment. Informatics in Education,
(Vol 8 1):85.
J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,
N. Padua-Perez, and F. Emad. 2006. Experiences with
marmoset: Designing and using an advanced submis-
sion and testing system for programming courses. In
Proc. of Innovation and technology in computer sci-
ence education.
176
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In Proc. of European Conf. on Speech
Communication and Technology.
M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009.
Phrase-based statistical machine translation as a trav-
eling salesman problem. In Proc. of ACL.
177
178
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 217?221,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Error Driven Paraphrase Annotation using Mechanical Turk
Olivia Buzek
Computer Science and Linguistics
University of Maryland
College Park, MD 20742, USA
olivia.buzek@gmail.com
Philip Resnik
Linguistics and UMIACS
University of Maryland
College Park, MD 20742, USA
resnik@umd.edu
Benjamin B. Bederson
Computer Science and HCIL
University of Maryland
College Park, MD 20742, USA
bederson@cs.umd.edu
Abstract
The source text provided to a machine translation
system is typically only one of many ways the input
sentence could have been expressed, and alternative
forms of expression can often produce a better trans-
lation. We introduce here error driven paraphras-
ing of source sentences: instead of paraphrasing a
source sentence exhaustively, we obtain paraphrases
for only the parts that are predicted to be problematic
for the translation system. We report on an Amazon
Mechanical Turk study that explores this idea, and
establishes via an oracle evaluation that it holds the
potential to substantially improve translation quality.
1 Introduction
The source text provided to a translation system is typ-
ically only one of many ways the input sentence could
have been expressed, and alternative forms of expression
can often produce better translation. This observation is
familiar to most statistical MT researchers in the form of
preprocessing choices ? for example, one segmentation
of a Chinese sentence might yield better translations than
another.1 Over the past several years, MT frameworks
have been developed that permit all the alternatives to be
used as input, represented efficiently as a confusion net-
work, lattice, or forest, rather than forcing selection of
a single input representation. This has improved perfor-
mance when applied to phenomena including segmenta-
tion, morphological analysis, and more recently source
langage word order (Dyer, 2007; Dyer et al, 2008; Dyer
and Resnik, to appear).
We have begun to explore the application of the same
key idea beyond low-level processing phenomena such
as segmentation, instead looking at alternative expres-
sions of meaning. For example, consider translating The
1Chinese is written without spaces, so most MT systems need to
segment the input into words as a preprocessing step.
Democratic candidates stepped up their attacks during
the debate. The same basic meaning could have been ex-
pressed in many different ways, e.g.:
? During the debate the Democratic candidates
stepped up their attacks.
? The Democratic contenders ratcheted up their at-
tacks during the debate.
? The Democratic candidates attacked more aggres-
sively during the debate.
? The candidates in the Democratic debate attacked
more vigorously.
These examples illustrate lexical variation, as well as syn-
tactic differences, e.g. whether the attacking or the in-
creasing serves as the main verb. We hypothesize that
variation of this kind holds a potential advantage for
translation systems, namely that some variations may be
more easily translated than others depending on the train-
ing data that was given to the system, and we can im-
prove translation quality by allowing a system to take best
advantage of the variations it knows about, at the sub-
sentential level, just as the systems described above can
take advantage of alternative segmentations.
Paraphrase lattices provide a way to make this hypoth-
esis operational. This idea is a variation on the uses of
paraphrase in translation introduced by Callison-Burch
and explored by others, as well (Callison-Burch et al,
2006; Madnani et al, 2007; Callison-Burch, 2008; Mar-
ton et al, 2009). These authors have shown that perfor-
mance improvements can be gained by exploiting para-
phrases using phrase pivoting. We have investigated us-
ing pivoting to create exhaustive paraphrase lattices, and
we have also investigated defining upper bounds by elic-
iting human sub-sentential paraphrases using Mechani-
cal Turk. Unfortunately, in both cases, we have found
the size of the paraphrase lattice prohibitive: there are
217
too many spans to paraphrase to make using Turk cost-
effective, and automatically generated paraphrase lattices
turn out to be too noisy to produce improved translations.
A potential solution to this problem comes from a dif-
ferent line of work we are pursuing, in which translation
is viewed as a collaborative process involving people and
machines (Bederson et al, 2010). Here, the idea is that
in translating from a source to a target language, source-
and target-language speakers who are not bilingual can
collaborate to improve the quality of automatic transla-
tion, via an iterative protocol involving translation, back
translation, and the use of a very rich user interface. For
example, consider the following translation from English
to French by an automatic MT system:
? Source: Polls indicate Brown, a state senator,
and Coakley, Massachusetts? Attorney General, are
locked in a virtual tie to fill the late Sen. Ted
Kennedy?s Senate seat.
? System: Les sondages indiquent Brown,
un se?nateur d?e?tat, et Coakley,
Massachusetts? Procureur ge?ne?ral, sont enferme?s
dans une cravate virtuel a` remplir le regrette?
se?nateur Ted Kennedy?s sie`ge au Se?nat.
Someone with only a semester of college French (one of
the authors) can look at this automatic translation, and
see that the underlined parts are probably wrong. Chang-
ing the source sentence to rephrase the underlined pieces
(e.g. changing Massachusetts? Attorney General to the
Attorney General of Massachusetts), we obtain a transla-
tion that is still imperfect but is more acceptable:
? System: Les sondages indiquent que Brown, un
se?nateur d?e?tat, et Coakley, le procureur ge?ne?ral
du Massachusetts, sont enferme?s dans une cravate
virtuel pourvoir le sige au Se?nat de Sen. Ted
Kennedy, qui est de?ce?de? re?cemment.
One could imagine (and, indeed, we are building) a vi-
sual interface that allows a human participant on the tar-
get side to communicate back to a source-side collabora-
tor, in effect saying, ?These underlined pieces look like
they were translated poorly; can you rephrase the rele-
vant parts of your sentence, and perhaps that will lead to
a better translation??2
Putting these ideas together ? source paraphrase and
identification of difficult regions of input for translation
? we arrive at the idea of error driven paraphrasing of
source sentences: instead of paraphrasing to introduce as
much variation as possible everywhere in the sentence,
we suggest that instead it makes sense to paraphrase only
2Communicating which parts of the sentence are relevant across lan-
guages is being done via projection across languages using word align-
ments; cf. (Hwa et al, 2001).
the parts of a source sentence that are problematic for the
translation system. In Section 2 we give a first-pass algo-
rithm for error driven paraphrasing, in Section 3 we de-
scribe how this was realized using MTurk, and Sections 4
and 5 provide an oracle evaluation, discussion, and con-
clusions.
2 Identifying source spans with errors
In error driven paraphrasing, the key idea is to focus on
source spans that are likely to be problematic for trans-
lation. Although in principle one could use human feed-
back from the target side to identify relevant spans, in
this paper we begin with an automatic approach, auto-
matically identifying that are likely to be incorrect via
a novel algorithm. Briefly, we automatically translate
source F to target E, then back-translate to produce F? in
the source language. We compare F and F? using TERp
(Snover et al, 2009), a form of string-edit distance that
identifies various categories of differences between two
sentences, and when at least two consecutive non-P (non-
paraphrase) edits are found, we flag their smallest con-
taining syntactic constituent.
In more detail, we posit that areas of F? where there
were many edits from F will correspond to areas in where
the target translation did not match the English very well.
Specifically, deletions (D), insertions (I), and shifts (S)
are likely to represent errors, while matches (M) and
paraphrases (P) probably represent a fairly accurate trans-
lation. Furthermore, we assume that while a single D, S,
or I edit might be fairly meaningless, a string of at least 2
of those types of edits is likely to represent a substantive
problem in the translation.
In order to identify reasonably meaningful paraphrase
units based on potential errors, we rely on a source lan-
guage constituency parser. Using the parse, we find the
smallest constituent of the sentence containing all of the
tokens in a particular error string. At times, these con-
stituents can be quite large, even the entire sentence. To
weed out these cases, we restrict constituent length to no
more than 7 tokens.
For example, given
F The most recent probe to visit Jupiter was the Pluto-
bound New Horizons spacecraft in late February 2007.
E La investigacio?n ma?s reciente fue la visita de Ju?piter a
Pluto?n de la envolvente sonda New Horizons a fines de
febrero de 2007.
F? The latest research visit Jupiter was the Pluto-bound New
Horizons spacecraft in late February 2007.
spans in the the bolded phrase in F would be identified,
based on the TERp alignment and smallest containing
constituent as shown in Figure 1.
218
NP PP 
NP 
Figure 1: TERp alignment of a source sentence and its back-translation in order to identify a problematic source span.
3 Error driven paraphrasing on MTurk
We chose to use translation from English to Chinese in
this first foray into Mechanical Turk for error driven para-
phrase. This made sense for a number of reasons: first,
because we expected to have a much easier time finding
Turkers; second, because we could make use of a high
quality English parser (in this case the Stanford parser);
and, third, because it meant that we as researchers could
easily read and judge the quality of Turkers? paraphrases.
To create an English-to-Chinese data set, we used the
Chinese-to-English data from the MT08 NIST machine
translation evaluation. We used English reference 0 as
the source sentence, and the original Chinese sentence as
the target. We chose reference 0 because on inspection
these references seemed most reflective of native English
grammar and usage. The data set comprises 1357 sen-
tence pairs. Using the the above described algorithm to
identify possible problem areas in the translation, with
the Google Translate API providing both the translation
and back-translation, we generated 1780 potential error
regions in 1006 of the sentences. Then we created HITs
both to obtain paraphrases, and to validate the quality of
paraphrase responses. Costs were $117.48 for obtaining
multiple paraphrases, and $44.06 for verification.
3.1 Obtaining paraphrases
Based on the phrases marked as problematic by our algo-
rithm, we created HITs asking for paraphrases within 5
sentences, as illustrated in Figure 2. Workers were given
60 minutes to come up with a single paraphrase for each
of the five indicated problematic regions, for a reward of
$0.10. If a worker felt they could not come up with an
alternate phrasing for the marked phrase, they had the
option of marking an ?Unable to paraphrase? checkbox.
We assigned each task to 3 workers, resulting in 3 para-
phrases for every marked phrase. From the 1780 errors,
we got 5340 responses. Of these, 4821 contained actual
paraphrase data, while the rest of the responses indicated
an inability to paraphrase, via the checkbox response. All
paraphrases were passed on to the verification phase.
3.2 Paraphrase Verification
In the verification phase, we generated alternative full
sentences based on the 4821 paraphrases. Workers were
shown an original sentence F and asked to compare it to at
most 5 alternatives, with a maximum of 20 comparisons
made in a HIT. (Recall that although F is the conven-
tional notation for source sentences in machine transla-
tion, in this study the F sentences are in English.) Re-
sponses were given in the form of radio buttons, mark-
ing ?Yes? for an alternate sentence if workers felt it was
grammatical and accurately reflected the content of the
219
original sentence, or ?No? if it did not meet both of those
criteria. Workers were given 30 minutes to make their
decisions, for a reward of $0.05. This task was also as-
signed to 3 workers, resulting in 3 judgments for every
paraphrase.
4 Evaluating Results
Using the paraphrase results from Mechanical Turk, we
constructed rephrased full sentences for every combina-
tion of paraphrase alternatives. For example, if a sentence
had 2 sub-spans paraphrased, and the two sub-spans had 2
and 3 unique paraphrasings, respectively, we would con-
struct 2 ? 3 = 6 alternative full sentences. From the
1780 predicted problematic phrases (within the 1006 au-
tomatically identified sentences with possible translation
errors), we generated 14,934 rephrased sentences. Each
rephrased English sentence was translated into a Chinese
sentence, again via the Google Translate API. We then
evaluated results for translation of the original sentences,
and of all their paraphrase alternatives, via the TER met-
ric, using the MT08 original Chinese sentence as the
target-language reference translation. The evaluation set
includes the 1000 sentence where at least one paraphrase
was provided.3
Our evaluation takes the form of an oracle study: if
we knew with perfect accuracy which variant of a sen-
tence to translate, i.e. among the original and all its para-
phrases, based on knowledge of the reference translation,
how well could we do? An ?oracle? telling us which vari-
ant is best is not available in the real world, of course, but
in situations like this one, oracle studies are often used
to establish the magnitude of the potential gain (Och et
al., 2004). In this case, the baseline is the average TER
score for the 1000 original sentences, 84.4. If an ora-
cle were permitted to choose which variant was the best
to translate, the average TER score would drop to 80.6.4
Drilling down a bit further, we find that a better-translated
paraphrase sentence is available in 313 of the 1000 cases,
or31.3%, and for those 313 cases, TER for the best para-
phrase alternative improves on the TER for the original
sentence by 12.16 TER points.
5 Conclusions
This annotation effort has produced gold standard sub-
sentential paraphrases and paraphrase quality ratings for
spans in a large number of sentences, where the choice
of spans to paraphrase is specifically focused on regions
of the sentence that are difficult to translate. In addi-
3For the other 6 sentences, all problematic spans were marked ?Un-
able to paraphrase? by all 3 MTurkers.
4TER measures errors, so lower is better. A reduction in TER of 3.8
for an MT evaluation dataset would be considered quite substantial; a
reduction of 1 point would typically be a publishable result.
tion, we have performed an initial analysis, using human-
generated paraphrases to provide an oracle evaluation of
how much could be gained in translation by translating
paraphrases of problematic regions in the source sen-
tence. The results suggest if paraphrasing is automati-
cally targeted to problematic source spans using a back-
translation comparison, good paraphrases of the problem-
atic spans could improve translation performance quite
substantially.
In future work, we will use a translation system sup-
porting lattice input (Dyer et al, 2008), rather than the
Google Translation API, in order to take advantage of
fully automatic error-driven paraphrasing, using pivot-
based approaches (e.g. (Callison-Burch et al, 2006)) to
complete the automation of the error-driven paraphrase
process. We will also investigate the use of human rather
than machine identification of likely translation prob-
lems, in the context of collaborative translation (Beder-
son et al, 2010).
References
Benjamin B. Bederson, Chang Hu, and Philip Resnik. 2010. Trans-
lation by iterative collaboration between monolingual users. In
Graphics Interface (GI) conference.
Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006.
Improved statistical machine translation using paraphrases. In
Robert C. Moore, Jeff A. Bilmes, Jennifer Chu-Carroll, and Mark
Sanderson, editors, HLT-NAACL. The Association for Computa-
tional Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on paraphrases ex-
tracted from parallel corpora. In EMNLP, pages 196?205. ACL.
Chris Dyer and Philip Resnik. to appear. Forest translation. In
NAACL?10.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing word lattice
translation. In Proceedings of HLT-ACL, Columbus, OH.
C. Dyer. 2007. Noisier channel translation: translation from morpho-
logically complex languages. In Proceedings of the Second Work-
shop on Statistical Machine Translation, Prague, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2001.
Evaluating translational correspondence using annotation projection.
In ACL ?02: Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 392?399, Morristown, NJ,
USA. Association for Computational Linguistics.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie Dorr.
2007. Using paraphrases for parameter tuning in statistical ma-
chine translation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 120?127, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Im-
proved statistical machine translation using monolingually-derived
paraphrases. In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 381?390, Singa-
pore, August. Association for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar,
Kenji Yamada, Alexander Fraser, Shankar Kumar, Libin Shen, David
Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir R. Radev.
2004. A smorgasbord of features for statistical machine translation.
In HLT-NAACL, pages 161?168.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz.
2009. TER-Plus: Paraphrases, Semantic, and Alignment Enhance-
ments to Translation Edit Rate. Machine Translation.
220
Figure 2: HIT format 1: Obtaining sub-sentential paraphrases. Note that as the MTurker types a paraphrase into the box,what
is typed appears immediately (character by character) in the full-sentence context under ?New sentence?, so that they can see
immediately how the entire sentence looks with their paraphrase.
the press trust of india quoted
the government minister for relief and rehabilitation kadam
kadam, the governments relief and rehabilitation minister (2/3)
the government minister concerned with relief and rehabiliation kadam (1/3)
as revealing today that in the last week, the monsoon has started in
all of indias states one
every one of indias state, one (3/3)
each of Indias states one (2/3)
all states of india one (1/3)
after another, and that the financial losses and casualties have been serious in all areas. just in maharashtra, the state which
includes
mumbai, indias largest city,
india?s largest city, mumbai (3/3)
the largest city in India, Mumbai, (3/3)
mumbai, the largest city of india, (3/3)
the number of people
known to have died
who died (3/3)
identified to have died (2/3)
known to have passed away (2/3)
has now reached 358.
Figure 3: Example of error-driven paraphrases produced via HIT format 1, above, for a single sentence. The paraphrase spans
(indented) are shown with the number of MTurkers, out of 3, who labeled that paraphrase in context as acceptable using a ?vali-
dation? HIT.
221
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 399?404,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Value of Monolingual Crowdsourcing in a Real-World Translation
Scenario: Simulation using Haitian Creole Emergency SMS Messages
Chang Hu?, Philip Resnik??, Yakov Kronrod?
Vladimir Eidelman?, Olivia Buzek??, Benjamin B. Bederson?
?UMIACS and Department of Linguistics
?UMIACS and Department of Computer Science
University of Maryland, College Park
{changhu,bederson}@cs.umd.edu
{resnik,vlad,buzek}@umiacs.umd.edu
yakov@umd.edu
Abstract
MonoTrans2 is a translation system that com-
bines machine translation (MT) with human
computation using two crowds of monolin-
gual source (Haitian Creole) and target (En-
glish) speakers. We report on its use in the
WMT 2011 Haitian Creole to English trans-
lation task, showing that MonoTrans2 trans-
lated 38% of the sentences well compared to
Google Translate?s 25%.
1 Introduction
One of the most remarkable success stories to come
out of the January 2010 earthquake in Haiti in-
volved translation (Munro, 2010). While other
forms of emergency response and communication
channels were failing, text messages were still get-
ting through, so a number of people came together to
create a free phone number for emergency text mes-
sages, which allowed earthquake victims to report
those who were trapped or in need of medical atten-
tion. The problem, of course, was that most people
were texting in Haitian Creole (Kreyol), a language
not many of the emergency responders understood,
and few, if any, professional translators were avail-
able. The availability of usable translations literally
became a matter of life and death.
In response to this need, Stanford University grad-
uate student Rob Munro coordinated the rapid cre-
ation of a crowdsourcing framework, which allowed
volunteers ? including, for example, Haitian expa-
triates and French speakers ? to translate messages,
providing responders with usable information in as
little as ten minutes. Translations may not have been
perfect, but to a woman in labor, it had to have made
a big difference for English-speaking responders to
see Undergoing children delivery Delmas 31 instead
of Fanm gen tranche pou fe` yon pitit nan Delmas 31.
What about a scenario, though, in which even am-
ateur bilingual volunteers are hard to find, or too
few in number? What about a scenario, e.g. the
March 2011 earthquake and tsunami in Japan, in
which there are many people worldwide who wish
to help but are not fluent in both the source and tar-
get languages?
For the last few years, we have been exploring the
idea of monolingual crowdsourcing for translation
? that is, technology-assisted collaborative transla-
tion involving crowds of participants who know only
the source or target language (Buzek et al, 2010;
Hu, 2009; Hu et al, 2010; Hu et al, 2011; Resnik
et al, 2010). Our MonoTrans2 framework has pre-
viously shown very promising results on children?s
books: on a test set where Google Translate pro-
duced correct translations for only 10% of the input
sentences, monolingual German and Spanish speak-
ers using our framework produced translations that
were fully correct (as judged by two independent
bilinguals) nearly 70% of the time (Hu et al, 2011).
We used the same framework in the WMT 2011
Haitian-English translation task. For this experi-
ment, we hired Haitian Creole speakers located in
Haiti, and recruited English speakers located in the
U.S., to serve as the monolingual crowds.
2 System
MonoTrans2 is a translation system that combines
machine translation (MT) with human computation
(Quinn et al, 2011) using two ?crowds? of mono-
lingual source (Haitian Creole) and target (English)
399
speakers.1 We summarize its operation here; see Hu
et al (2011) for details.
The Haitian Creole sentence is first automatically
translated into English and presented to the English
speakers. The English speakers then can take any of
the following actions for candidate translations:
? Mark a phrase in the candidate as an error
? Suggest a new translation candidate
? Vote candidates up or down
Identifying likely errors and voting for candidates
are things monolinguals can do reasonably well:
even without knowing the intended interpretation,
you can often identify when some part of a sentence
doesn?t make sense, or when one sentence seems
more fluent or plausible than another. Sometimes
rather than identifying errors, it is easier to suggest
an entirely new translation candidate based on the
information available on the target side, a variant
of monolingual post-editing (Callison-Burch et al,
2004).
Any new translation candidates are then back-
translated into Haitian Creole, and any spans marked
as translation errors are projected back to identify
the corresponding spans in the source sentence, us-
ing word alignments as the bridge (cf. Hwa et al
(2002), Yarowsky et al (2001)).2 The Haitian Cre-
ole speakers can then:
? Rephrase the entire source sentence (cf.
(Morita and Ishida, 2009))
? ?Explain? spans marked as errors
? Vote candidates up or down (based on the back-
translation)
Source speakers can ?explain? error spans by of-
fering a different way of phrasing that piece of the
source sentence (Resnik et al, 2010), in order to
produce a new source sentence, or by annotating the
spans with images (e.g. via Google image search)
or Web links (e.g. to Wikipedia). The protocol then
continues: new source sentences created via partial-
1For the work reported here, we used Google Translate as
the MT component via the Google Translate Research API.
2The Google Translate Research API provides alignments
with its hypotheses.
or full-sentence paraphrase pass back through MT
to the English side, and any explanatory annota-
tions are projected back to the corresponding spans
in the English candidate translations (where the er-
ror spans had been identified). The process is asyn-
chronous: participants on the Haitian Creole and
English sides can work independently on whatever
is available to them at any time. At any point, the
voting-based scores can be used to extract a 1-best
translation.
In summary, the MonoTrans2 framework uses
noisy MT to cross the language barrier, and supports
monolingual participants in doing small tasks that
gain leverage from redundant information, the hu-
man capacity for linguistic and real-world inference,
and the wisdom of the crowd.
3 Experiment
We recruited 26 English speakers and 4 Haitian Cre-
ole speakers. The Haitian Creole speakers were re-
cruited from Haiti and do not speak English. Five of
the 26 English speakers were paid UMD undergrad-
uates; the other 21 were volunteer researchers, grad-
uate students, and staff unrelated to this research. 3
Over a 13 day period, Haitian Creole and English
speaker efforts totaled 15 and 29 hours, respectively.
4 Data Sets
Our original goal of fully processing the entire SMS
clean test and devtest sets could not be realized in the
available time, owing to unanticipated reshuffling of
the data by the shared task organizers and logistical
challenges working with participants in Haiti. Ta-
ble 1 summarizes the data set sizes before and after
reshuffling. We put 1,224 sentences from the pre-
before after
test 1,224 1,274
devtest 925 900
Table 1: SMS clean data sets before and after reshuffling
reshuffling test set, interspersed with 123 of the 925
sentences from the pre-reshuffling devtest set, into
the system ? 1,347 sentences in total. We report
3These, obviously, did not include any of the authors.
400
results on the union of pre- and post-reshuffling de-
vtest sentences (Set A, |A| = 1516), and the post-
reshuffling test set (Set B, |B| = 1274 ).
5 Evaluation
Of the 1,347 sentences available for processing in
MonoTrans2, we define three subsets:
? Touched: Sentences that were processed by at
least one person (657 sentences)
? Each-side: Sentences that were processed by at
least one English speaker followed by at least
one Haitian Creole speaker (431 sentences)
? Full: Sentences that have at least three trans-
lation candidates, of which the most voted-for
one received at least three votes (207 sentences)
We intersect these three sets with sets A and B in or-
der to evaluate MonoTrans2 output against the pro-
vided references (Table 2).4
Set S |S| |S ?A| |S ?B|
Touched 657 162 168
Each-side 431 127 97
Full 207 76 60
Table 2: Data sets for evaluation and their sizes
Tables 3 and 4 report two automatic scoring met-
rics, uncased BLEU and TER, comparing Mono-
Trans2 (M2) against Google Translate (GT) as a
baseline.
Set Condition BLEU TER
Touched ?A
GT 21.75 56.99
M2 23.25 57.27
Each-side ?A
GT 21.44 57.51
M2 21.47 58.98
Full ?A
GT 25.05 54.15
M2 27.59 52.78
Table 3: BLEU and TER results for different levels of com-
pletion on the devtest set A
Since the number of sentences in each evaluated
set is different (Table 2), we cannot directly compare
4Note that according to these definitions, Touched contains
both Each-side and Full, but Each-side does not contain Full.
Set Condition BLEU TER
Touched ?B
GT 19.78 59.88
M2 24.09 58.15
Each-side ?B
GT 21.15 56.88
M2 23.80 57.19
Full ?B
GT 22.51 54.51
M2 28.90 52.22
Table 4: BLEU and TER results for different levels of com-
pletion on the test set B
scores between the sets. However, Table 4 shows
that when the MonoTrans2 process is run on test
items ?to completion?, in the sense defined by ?Full?
(i.e. Full?B), we see a dramatic BLEU gain of 6.39,
and a drop in TER of 2.29 points. Moreover, even
when only target-side or only source-side monolin-
gual participation is available we see a gain of 4.31
BLEU and a drop of 1.73 TER points (Touched?B).
By contrast, the results on the devtest data are en-
couraging, but arguably mixed (Table 3). In order to
step away from the vagaries of single-reference au-
tomatic evaluations, therefore, we also conducted an
evaluation based on human judgments. Two native
English speakers unfamiliar with the project were
recruited and paid for fluency and adequacy judg-
ments: for each target translation paired with its cor-
responding reference, each evaluator rated the tar-
get sentence?s fluency and adequacy on a 5-point
scale, where fluency of 5 indicates complete fluency
and adequacy of 5 indicates complete preservation
of meaning (Dabbadie et al, 2002).5
Sentences N Google MonoTrans2
Full ?A 76 18 (24%) 30 (39%)
Full ?B 60 15 (25%) 23 (38%)
Table 5: Number of sentences with maximum possible
adequacy (5) in Full ?A and Full ?B, respectively.
Similar to Hu et al (2011), we adopt the very con-
servative criterion that a translation output is consid-
ered correct only if both evaluators independently
give it a rating of 5. Unlike Hu et al (2011), for
whom children?s book translation requires both flu-
ency and adequacy, we make this a requirement only
5Presentation order was randomized.
401
for adequacy, since in this scenario what matters to
aid organizations is not whether a translation is fully
fluent, but whether it is correct. On this criterion,
the Google Translate baseline of around 25% cor-
rect improves to around 40% for Monotrans, con-
sistently for both the devtest and test data (Table 5).
Nonetheless, Figures 1 and 2 make it clear that the
improvements in fluency are if anything more strik-
ing.
5.1 Statistical Analysis
Variable Adequacy Fluency
Positive
mostSingleCandidateVote ** ***
candidateCount ** **
numOfAnswers * NS
Negative
roundTrips *** ***
voteCount * .
Table 6: Effects of independent variables in linear regres-
sion for 330 touched sentences
(Signif. codes: ?***? 0.001, ?**? 0.01, ?*? 0.05, ?.? 0.1)
In addition to the main evaluation, we investi-
gated the relationship between tasks performed in
the MonoTrans2 system and human judgments us-
ing linear regression and an analysis of variance.
We evaluate the set of all 330 touched sentences in
Touched?A and Touched?B in order to under-
stand which properties of the MonoTrans2 process
correlate with better translation outcomes.
Our analysis focused on improvement over the
Google Translate baseline, looking specifically at
the improvement based on the human evaluators? av-
eraged fluency and adequacy scores.
Table 6 summarizes the positive and negative
effects for five of six variables we considered that
came out significant for at least one of the measures.
6
The positive results were as expected. Having
more votes for the winning candidate (mostSingle-
CandidateVote) made it more successful, since this
means that more people felt it was a good represen-
tative translation. Having more candidates to choose
6A sixth, numOfVoters, was not significant in the linear re-
gression for either adequacy or fluency.
from (candidateCount) meant that more people had
taken the time to generate alternatives, reflecting at-
tention paid to the sentence. Also, the amount of
attention paid to target speakers? requests for clarifi-
cation (numOfAnswers) is as expected related to the
adequacy of the final translation, and perhaps as ex-
pected does not correlate with fluency of the output
since it helps with meaning and not actual target-side
wording.
We were, however, confused at first by the neg-
ative influence of the roundTrips measure and vote-
Count measures. We conjecture that the first effect
arises due to a correlation between roundTrips and
translation difficulty; much harder sentences would
have led to many more paraphrase requests, and
hence to more round trips. We attempted to inves-
tigate this hypothesis by testing correlation with a
naive measure of sentence difficulty, length, but this
was not fruitful. We suspect that inspecting use of
abbreviations, proper nouns, source-side mistakes,
and syntactic complexity would give us more insight
into this issue.
As for voteCount, the negative correlation is un-
derstandable when considered side by side with
the other vote-based measure, mostSingleCandidat-
eVote. Having a higher number of votes for the win-
ning candidate leads to improvement (strongly sig-
nificant for both adequacy and fluency), so a higher
general vote count means that people were also vot-
ing more times for other candidates. Hence, once the
positive winning vote count is taken into account,
the remaining votes actually represent disagreement
on the candidates, hence correlating negatively with
overall improvement over baseline.
It is important to note that when these measures
are all considered together, they show that there is a
clear correlation between the MonoTrans2 system?s
human processing and the eventual increase in both
quality and fluency of the sentences. As people give
more attention to sentences, these sentences show
better performance, as judged by increase over base-
line.
6 Discussion
Our experiment did not address acquisition of, and
incentives for, monolingual participants. In fact, get-
ting time from Haitian Creole speakers, even for pay,
402
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 1: Human judgments for fluency and adequacy in fully processed devtest items (Full ?A)
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?s
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(a) Fluency Distribution
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 2	 ? 3	 ? 4	 ? 5	 ?
#	 ?of	 ?S
enten
ces	 ?
Google	 ?MonoTrans2	 ?
(b) Adequacy Distribution
Figure 2: Human judgments for fluency and adequacy in fully processed test items (Full ?B)
created a large number of logistical challenges, and
was a contributing factor as to why we did not obtain
translations for the entire test set. However, avail-
ability of monolingual participants is not the issue
being addressed in this experiment: we are confi-
dent that in a real-world scenario like the Haitian
or Japanese earthquakes, large numbers of monolin-
gual volunteers would be eager to help, certainly in
larger total numbers than bilingual volunteers. What
matters here, therefore, is not how much of the test
set was translated in total, but how much the trans-
lations improved for the sentences where monolin-
gual crowdsourcing was involved, compared to the
MT baseline, and what throughput might be like in
a real-world scenario.
We also were interested in throughput, particu-
larly in comparison to bilingual translators. In previ-
ous experimentation (Hu et al, 2011), throughput in
MonoTrans2 extrapolated to roughly 800 words per
day, a factor of 2.5 slower than professional trans-
lators? typical speed of 2000 words per day. In
this experiment, overall translation speed averaged
about 300 words per day, a factor of more than 6
times slower. However, this is an extremely pes-
simistic estimate, for several reasons. First, our pre-
vious experiment had more than 20 users per side,
while here our Haitian crowd consisted of only four
people. Second, we discovered after beginning the
experiment that the translation of our instructions
into Haitian Creole had been done somewhat slop-
pily. And, third, we encountered a range of tech-
nical and logistical problems with our Haitian par-
ticipants, ranging from finding a location with In-
ternet access to do the work (ultimately an Internet
Cafe? turned out to be the best option), to slow and
sporadic connections (even in an Internet Cafe?), to
relative lack of motivation for part-time rather than
full-time work. It is fair to assume that in a real-
world scenario, some unanticipated problems like
these might crop up, but it also seems fair to assume
that many would not; for example, most people from
the Haitian Creole and French-speaking communi-
ties who volunteered using Munro et al?s system
in January 2010 were not themselves located in the
403
third world.
Finally, regarding quality, the results here are
promising, albeit not as striking as those Hu et al
(2011) obtained for Spanish-German translation of
children?s books. The nature of SMS messages
themselves may have been a contributing factor to
the lower translation adequacy: even in clean form,
these are sometimes written using shorthand (e.g.
?SVP?), and are sometimes not syntactically correct.
The text messages are seldom related to each other,
unlike sentences in larger bodies of text where even
partially translated sentences can be related to each
other to provide context, as is the case for children?s
books. One should also keep in mind that the under-
lying machine translation engine, Google Translate
between Haitian Creole and English, is still in an al-
pha phase.
Those considerations notwithstanding, it is en-
couraging to see a set of machine translations get
better without the use of any human bilingual exper-
tise. We are optimistic that with further refinements
and research, monolingual translation crowdsourc-
ing will make it possible to harness the vast num-
ber of technologically connected people who want
to help in some way when disaster strikes.
7 Acknowledgments
This research is supported by NSF contract
#BCS0941455 and by a Google Research Award.
References
Olivia Buzek, Philip Resnik, and Benjamin B. Bederson.
2010. Error driven paraphrase annotation using me-
chanical turk. In NAACL 2010 Workshop on Creating
Speech and Text Language Data With Amazon?s Me-
chanical Turk.
Chris Callison-Burch, Colin Bannard, , and Josh
Schroeder. 2004. Improving statistical translation
through editing. In Workshop of the European Asso-
ciation for Machine Translation.
Marianne Dabbadie, Anthony Hartley, Margaret King,
Keith J. Miller, Widad Mustafa El Hadi, Andrei
Popescu-Belis, Florence Reeder, and Michelle Vanni.
2002. A hands-on study of the reliability and coher-
ence of evaluation metrics. In Workshop at the LREC
2002 Conference, page 8. Citeseer.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of Graphics Inter-
face 2010 on Proceedings of Graphics Interface 2010,
pages 39?46, Ottawa, Ontario, Canada. Canadian In-
formation Processing Society.
Chang Hu, Ben Bederson, Philip Resnik, and Yakov Kro-
nrod. 2011. Monotrans2: A new human computation
system to support monolingual translation. In Human
Factors in Computing Systems (CHI 2011), Vancou-
ver, Canada, May. ACM, ACM.
Chang Hu. 2009. Collaborative translation by monolin-
gual users. In Proceedings of the 27th international
conference extended abstracts on Human factors in
computing systems, pages 3105?3108, Boston, MA,
USA. ACM.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan
Kolak. 2002. Evaluating translational correspon-
dence using annotation projection. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 392?399, Philadelphia, Penn-
sylvania. Association for Computational Linguistics.
Daisuke Morita and Toru Ishida. 2009. Designing pro-
tocols for collaborative translation. In PRIMA ?09:
Proceedings of the 12th International Conference on
Principles of Practice in Multi-Agent Systems, pages
17?32, Berlin, Heidelberg. Springer-Verlag.
Robert Munro. 2010. Crowdsourced translation for
emergency response in haiti: the global collaboration
of local knowledge. In AMTA Workshop on Collabo-
rative Crowdsourcing for Translation. Keynote.
Alexander J. Quinn, Bederson, and Benjamin B. Beder-
son. 2011. Human computation: A survey and tax-
onomy of a growing field. In Human Factors in Com-
puting Systems (CHI 2011), Vancouver, Canada, May.
ACM, ACM.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alexander J. Quinn, and Benjamin B. Bederson. 2010.
Improving translation via targeted paraphrasing. In
EMNLP.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via
robust projection across aligned corpora. In HLT
?01: Proceedings of the first international conference
on Human language technology research, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
404
