Efficient Unsupervised Recursive Word Segmentation Using Minimum
Description Length
Shlomo ARGAMONa Navot AKIVAb Amihood AMIRb Oren KAPAHb
aIllinois Institute of Technology, Dept. of Computer Science, Chicago, IL 60616, USA
argamon@iit.edu
bBar-Ilan University, Dept. of Computer Science, Ramat Gan 52900, ISRAEL
{navot,amir,kapaho}@cs.biu.ac.il
Abstract
Automatic word segmentation is a basic re-
quirement for unsupervised learning in morpho-
logical analysis. In this paper, we formulate a
novel recursive method for minimum descrip-
tion length (MDL) word segmentation, whose
basic operation is resegmenting the corpus on
a prefix (equivalently, a suffix). We derive a
local expression for the change in description
length under resegmentation, i.e., one which de-
pends only on properties of the specific prefix
(not on the rest of the corpus). Such a formula-
tion permits use of a new and efficient algorithm
for greedy morphological segmentation of the
corpus in a recursive manner. In particular, our
method does not restrict words to be segmented
only once, into a stem+affix form, as do many
extant techniques. Early results for English and
Turkish corpora are promising.
1 Introduction
Although computational morphological analyzers
have existed for many years for a number of lan-
guages, there are still many languages for which no
such analyzer exists, but for which there is an abun-
dance of electronically-available text. Developing
a morphological analyzer for a new language by
hand can be costly and time-consuming, requiring
a great deal of effort by highly-specialized experts.
Supervised learning methods, on the other hand, re-
quire annotated data, which is often scarce or non-
existent, and is also costly to develop. For this
reason, there is increasing interest in unsupervised
learning of morphology, in which unannotated text
is analysed to find morphological structures. Even
approximate unsupervised morphological analysis
can be useful, as an aid to human annotators.
This paper addresses a key task for unsuper-
vised morphological analysis: word segmentation,
segmenting words into their most basic meaning-
ful constituents (substrings), called morphs (ortho-
graphic realizations of morphemes). We adopt the
minimum description length (MDL) approach to
word segmentation, which has been shown to be ef-
fective in recent work (notably (Goldsmith, 2001)
and (Brent et al, 1995)). The minimum descrip-
tion length principle (Barron et al, 1998) is an
information-theoretic criterion to prefer that model
for observed data which gives a minimal length cod-
ing of the observed data set (given the model) to-
gether with the model itself.
1.1 Our approach
Our approach in this paper is to better clarify the
use of MDL for morphological segmentation by en-
abling direct use of a variety of MDL coding criteria
in a general and efficient search algorithm. Issues of
computational efficiency have been a bottleneck in
work on unsupervised morphological analysis, lead-
ing to various approximations and heuristics being
used. Our key contribution is to show how a lo-
cal formulation of description length (DL) for word
segmentation enables an efficient algorithm (based
on pattern-matching methods) for greedy morpho-
logical segmentation of the corpus. We thus provide
a search framework method which avoids some re-
strictions needed in previous work for efficiency. In
particular, our method segments words in the cor-
pus recursively, enabling multiple morphs to be ex-
tracted from a single word, rather than just allow-
ing a single stem+affix pair for a given word, as in
many previous approaches. For example, we might
find the segmentation inter+nation+al+ist,
whereas a single-boundary method would segment
the word on just one of those boundaries.
This paper describes the first step in a larger re-
search program; it?s purpose is to show how to most
efficiently recursively segment the words in a cor-
pus based on an MDL criterion, rather than exhibit a
full morphological analysis system. The procedure
developed here is a component of a larger planned
system, which will use semantic and structural in-
formation to correct word segmentation errors and
will cluster morphological relations into productive
paradigms.
1.2 Related work
Several systems for unsupervised learning of mor-
phology have been developed over the last decade or
so. De?jean (1998), extending ideas in Harris (1955),
describes a system for finding the most frequent af-
fixes in a language and identifying possible mor-
pheme boundaries by frequency bounds on the num-
ber of possible characters following a given char-
acter sequence. Brent et al (1995) give an in-
formation theoretic method for discovering mean-
ingful affixes, which was later extended to enable
a novel search algorithm based on a probabilistic
word-generation model (Snover et al, 2002). Gold-
smith (2001) gives a comprehensive heuristic al-
gorithm for unsupervised morphological analysis,
which uses an MDL criterion to segment words
and find morphological paradigms (called signa-
tures). Similarly, Creutz and Lagus (2002) use an
MDL formulation for word segmentation. All of
these approaches assume a stem+affix morpholog-
ical paradigm.
Further, the above approaches only consider in-
formation in words? character sequences for im-
prove morphological segmentation, and do not con-
sider syntactic or semantic context. Schone and Ju-
rafsky (2000) extend this by using latent semantic
analysis (Dumais et al, 1988) to require that a pro-
posed stem+affix split is sufficiently semantically
similar to the stem before the split is accepted. A
conceptually similar approach is taken by Baroni et
al. (2002) who combine use of edit distance to mea-
sure orthographic similarity and mutual information
to measure semantic similarity, to determine mor-
phologically related word pairs.
2 Overview of the Approach
In this section we provide an overview of our ap-
proach to greedy construction of a set of morphs
(a dictionary), using a minimal description length
(MDL) criterion (Barron et al, 1998) (we present
three alternative MDL-type criteria below, of vary-
ing levels of sophistication). The idea is to initialize
a dictionary of morphs to the set of all word types in
the corpus, and incrementally refine it by resegment-
ing affixes (either prefixes or suffixes) from the cor-
pus. Resegmenting on a prefix p (depicted in Fig-
ure 1) means adding the prefix as a new morph, and
removing it from all words where it occurs as a pre-
fix. Some of the morphs thus created may already
exist in the corpus (e.g., ?cognition? in Fig. 1). We
denote the set of morphs starting with p as Vp, and
the set of continuations that follow p by Sp (i.e.,
Vp = pSp). The number of occurrences of a morph
m in the corpus (as currently segmented) is denoted
Dictionary before Dictionary after
relic re
retire lic
recognition tire
relive cognition
tire live
cognition farm
farm
Figure 1: Illustration of resegmenting on the prefix re-.
Note that Vre ={relic, retire, recognition, relive}, and
Sre ={lic, tire, cognition, live}.
by C(m), and the number of tokens in the corpus
with prefix p is denoted B(p) = ?vk?Vp C(vk).
The algorithm examines all prefixes of current
morphs in the dictionary as resegmentation candi-
dates. The candidate p? that would give the greatest
decrease in description length upon resegmentation
is chosen, and the corpus is then resegmented on p?.
This is repeated until no candidate can decrease de-
scription length.
Key to this process is efficient resegmentation of
the corpus, which entails incremental update of the
description length change that each prefix p will
give upon resegmentation, denoted ?CODEp (the
change in the coding cost CODE(M,Data) for the
corpus plus the model M ). This is achieved in two
ways. First, we develop (Sec. 3) expressions for
?CODEp which depend only on simple properties
of p, Vp, and Sp, and their occurrences in the corpus.
This locality property obviates the need to exam-
ine most of the corpus to determine ?CODEp. Sec-
ond, we use a novel word/suffix indexing data struc-
ture which permits efficient resegmentation and up-
date of the statistics on which ?CODEp depends
(Sec. 4). Initial experimental results for the different
models using our algorithm are given in Section 5.
3 Local Description Length Models
As we show below, the key to efficiency is deriving
local expressions for the change in coding length
that will be caused by resegmentation on a partic-
ular prefix p. That is, this coding length change,
?CODEp, should depend only on direct properties
of p, those morphs Vp = {vk = psk} for which it is
a prefix, and those strings Sp = {sk|psk ? Vp} (p?s
continuations). This enables us to efficiently main-
tain the necessary data about the corpus and to up-
date it on resegmentation, avoiding costly scanning
of the entire corpus on each iteration.
We now describe three description length mod-
els for word segmentation. First, we introduce local
description length via two simple models, and then
give a derivation of a local expression for descrip-
tion length change for a more realistic description
length measure.
3.1 Model 1: Dictionary count
Perhaps the simplest possible model is to find a seg-
mentation which minimizes the number of morphs
in the dictionary CODE1(M,Data) = |M |. Al-
though the global minimum will almost always be
the trivial solution where each morph is an individ-
ual letter, this trivial solution may be avoided by
enforcing a minimal morph length (of 2, say). Fur-
thermore, when implemented via a greedy prefix (or
suffix) resegmenting algorithm, this measure gives
surprisingly good results, as we show below.
Locality in this model is easily shown, as
?CODE1p(M) = 1 + |Sp ?M | ? |Vp|
= 1 ? |Sp ?M |
since p is added to M as are all its continuations
not currently in M , while each morph vk ? Vp is re-
moved (being resegmented as the 2-morph sequence
psk).
3.2 Model 1a: Adjusted count
We also found a heuristic modification of Model
1 to work well, based on the intuition that an af-
fix with more continuations that are current morphs
will be better, while to a lesser extent more contin-
uations that are not current morphs indicates lower
quality. This gives the local heuristic formula:
?CODE1ap (M) = 1 + |Sp ?M | ? ?|Sp ?M |
where ? is a tunable parameter determining the rel-
ative weights of the two factors.
3.3 Model 2: MDL
A more theoretically motivated model seeks to min-
imize the combined coding cost of the corpus and
the dictionary (Barron et al, 1998):
CODE2(Data|M) + CODE2(M)
where we assume a minimal length code for the cor-
pus based on the morphs in the dictionary1 .
The coding cost of the dictionary M is:
CODE2(M) = CODE2(M)
= b?m?M len(m)
1As is well known, MDL model estimation is equivalent to
MAP estimation for appropriately chosen prior and conditional
data distribution (Barron et al, 1998).
where b is the number of bits needed to represent a
character and len(m) is the length of m in charac-
ters.
The coding cost CODE(Data|M) of the corpus
given the dictionary is simply the total number of
bits to encode the data using M ?s code:
CODE2(Data|M)
= CODE(M(Data) = M1...N )
= ??Ni=1 log P (mi)
= ??|M |j=1C(mj) log P (mj)
= ??|M |j=1C(mj)(logC(mj) ? logN)
where M(Data) is the corpus segmented according
to M , N is the number of morph tokens in the seg-
mented corpus, mi is the ith morph token in that
segmentation, P (m) is the probability of morph
m in the corpus estimated as P (m) = C(m)/N ,
C(m) is the number of times morph m appears in
the corpus, |M | is the total number of morph types
in M , and mj is the jth morph type in the M .
Now suppose we wish to add a new morph to M
by resegmenting on a prefix p from all morphs shar-
ing that prefix, as above. First, consider the total
change in cost for the dictionary. Note that the ad-
dition of the new morph p will cause an increase of
blen(p) bits to the total dictionary size. At the same
time, each new morph s ? Sp ? M will add its
coding cost blen(s), while each preexisting morph
s? ? Sp?M will not change the dictionary length at
all. Finally, each vk is removed from the dictionary,
giving a change of ?blen(vk). The total change in
coding cost for the dictionary by resegmenting on p
is thus:
?CODE2p(M) = b (len(p)
+?sk?(Sp?M) len(sk)
??k len(vk))
Now consider the change in coding cost for the
corpus after resegmentation. First, consider each
preexisting morph type m 6? Vp, with the same
count after resegmentation (since it does not con-
tain p). The coding cost of each occurrence of m,
however, will change, since the total number of to-
kens in the corpus will change. Thus the total cost
change for such an m is:
?CODE2p(Data|m 6? Vp)
= C(m)(log P (m) ? log P? (m))
= C(m)(logC(m) ? logN ? logC(m) + log N?)
= C(m)(log N? ? logN)
= C(m)(log(N + B(p)) ? logN)
The total corpus cost change for unchanged morphs
depends only on N and B(p):
?CODE2p(Data|M ? Vp)
=?m?M?Vp C(m)(log(N + B(p)) ? logN)
= (?m?M?Vp C(m))(log(N + B(p)) ? logN)
= (N ??vk C(vk))(log(N + B(p)) ? logN)
= (N ?B(p))(log(N + B(p)) ? logN)
Now, consider explicitly each morph vk ? Vp
which will be split after resegmentation. First,
remove the code for each occurrence of vk from
the corpus coding: C(vk) log P (vk). Next, add a
code for each occurrence of the new morph cre-
ated by the prefix: ?C(vk) log P? (p), where P? (p) =
B(p)/(N + B(p)) is the probability of morph p
in the resegmented corpus. Finally, code the con-
tinuations sk: ?C(vk) log P? (sk) (where P? (sk) =
C?(sk)
N? =
C(vk)+C(sk)
N? is the probability of the ?new?
morph sk). Putting this together, we have the cor-
pus coding cost change for Vp (noting that B(p) =
?
vk C(vk)):
?CODE2p(Data|Vp)
=?vk C(vk)[ logP (vk) ? log P? (p) ? log P? (sk) ]
=
?
vk C(vk) (logC(vk) ? logN
+ log N? ? logB(p)
+ log N? ? log C?(sk))
= ?vk C(vk)(logC(vk) ? log C?(sk))
+B(p)(2 log N? ? logN)
?B(p) logB(p)
Thus the cost change for resegmenting on p is:
?CODE2p(M,Data)
= ?CODE2p(M) + ?CODE2p(Data|M)
= ?CODE2p(M) + ?CODE2p(Data|M ? Vp)
+?CODE2p(Data|Vp)
= b
[
len(p) +
?
sk?(Sp?M) len(sk) ?
?
vk len(vk)
]
+ (N ?B(p)) (log(N + B(p)) ? logN)
+ ?vk C(vk)(logC(vk) ? log C?(sk))
+B(p)(2 log N? ? logN)
?B(p) logB(p)
Note that all terms are local to the prefix p, its in-
cluding morphs Vp and its continuations Sp. This
will enable an efficient incremental algorithm for
greedy segmentation of all words in the corpus, as
described in the next section.
4 Efficient Greedy Prefix Search
The straightforward greedy algorithm schema for
finding an approximately minimal cost dictio-
nary is to repeatedly find the best prefix p? =
argminp ?CODEp(M,Data) and resegment the
corpus on p?, until no p? exists with negative
?CODE. However, the expense of passing over the
entire corpus repeatedly would be prohibitive. Due
to lack of space, we sketch here our method for
caching corpus statistics in a pair of tries, in such
a way that ?CODEp can be easily computed for any
prefix p, and such that the data structures can be ef-
ficiently updated when resegmenting on a prefix p.
(A heap is also used for efficiently finding the best
prefix.)
The main data structures consist of two tries. The
first, which we term the main suffix trie (MST), is a
suffix trie (Gusfield, 1997) for all the words in the
corpus. Each node in the MST represents either the
prefix of a current morph (initially, a word in the
corpus), or the prefix of a potential morph (in case
its preceding prefix gets segmented). Each such
node is labeled with various statistics of its prefix p
(denoted by the path to it from the root) and its suf-
fixes, such as its prefix length len(p), its count B(p),
the number of its continuations |Sp|, and the col-
lective length of its continuations
?
sk?Sp len(sk),
as well as the current value of ?CODEp(M,Data)
(computed from these statistics). Also, each node
representing the end of an actual word in the corpus
is marked as such.
The second trie, the reversed prefix trie (RPT),
contains all the words in the corpus in reverse.
Hence each node in the RPT corresponds to the suf-
fix of a word in the corpus. We maintain a list of
pointers at each node in the RPT to each node in the
MST which has an identical suffix. This allows ef-
ficient access to all prefixes of a given string. Also,
those nodes corresponding to a complete word in
the corpus are marked.
Initial construction of the data structures can be
done in time linear in the size of the corpus, us-
ing straightforward extensions of known suffix trie
construction techniques (Gusfield, 1997). Finding
the best prefix p? can be done efficiently by stor-
ing pointers to all the prefixes in a heap, keyed by
?CODEp. To then remove all words prefixed by p?
and add all its continuations as new morphs (as well
as p? itself), proceed as follows, for each continua-
tion sk:
1. If sk is marked in RPT, then it is a complete
word, and only its count needs to be updated.
2. Otherwise
(a) Mark sk?s node in MST as a complete
word, and update its statistics
(b) Add sRk to RPT and mark the correspond-
ing nodes in MST as accepting stems.
3. Update the heap for the changed prefixes.
Prefixes
re- *ter-
un- im-
in- com-
de- trans-
con- sub-
dis- *se-
pre- en-
ex- *pa-
pro- *pe-
over- *mi-
Suffixes
-?s *-at
-ing -ate
-ed -ive
-es -able
-ly -ment
-er -or
?-ers -en
-ion ?-ors
?-ions ?-ings
-al *-is
Figure 2: The first 20 English prefix and suffix morphs
extracted from Reuters-21578 corpus using Model 1.
Meaningless morphs are marked by ?*?; nonminimal
meaningful morphs by ???.
Prefixes Suffixes
? = 1 ? = 2 ? = 1 ? = 2
over-
non-
under-
mis-
food-
stock-
feed-
view-
work-
export-
book-
warn-
borrow-
depres-
market-
high-
narrow-
turn-
trail-
steel-
un-
over-
non-
*der-
dis-
mis-
out-
inter-
trans-
re-
super-
fore-
up-
down-
tele-
stock-
im-
air-
euro-
mid-
-?s
-ly
-ness
-ship
?-ships
?-ization
-ize
?-ized
?-isation
?-izing
?-izes
?-holders
?-izations
?-isations
-water
?-ised
-ise
?-ising
?-ises
?-iser
-?s
-ly
-ness
-ment
?-ments
?-ized
-ize
?-ization
?-izing
?-isation
?-ised
-ise
?-ising
?-ises
-ship
-men
?-ened
?-ening
?-izes
*-mental
Figure 3: The first 20 English prefix and suffix morphs
extracted using Model 1a, as above.
The complexity for resegmenting on p is
O(len(p) +
?
sk?Sp
len(sk) + NSUF(Sp) log(|M |))
where NSUF(Sp) is the number of different morphs
in the previous dictionary that have a suffix in Sp
(which need to be updated in the heap).
5 Experimental Results
In this section we give initial results for the above
algorithm in English and Turkish, showing how
meaningful morphs are extracted using different
greedy MDL criteria. Recall that the models and
algorithm described in this paper are intended as
parts of a more comprehensive morphological anal-
ysis system, as we describe below in future work.
5.1 English
For evaluation in English, we used the standard
Reuters-21578 corpus of news articles (comprising
1.7M word tokens and 32,811 unique words). For
each of the 3 models described above, we extracted
morphs either by resegmenting on prefixes or on
suffixes (looking at the words reversed). When seg-
menting according to Models 1 and 2, a minimum
prefix length of 2 was enforced, to improve morph
quality (though not for suffixes, since in English
there are some one-letter suffixes such as -s).
First, consider morphs found by Model 1 (Fig. 2).
The prefix morphs found are surprisingly good for
this simple model, with only one wrong in the first
15 extracted. That erroneous morph is ter-, which
is part of inter-, however in- was extracted
first; this kind of error could be ameliorated by
a merging postprocessing step. The suffixes are
similarly good, although oddly the system did not
find -s, which caused it to find several compos-
ite morphs, such as -ers and -ions, which can
get resegmented into their components (-er+s and
-ion+s) later.
Model 1a also performs extremely well, for dif-
ferent values of ? (we show just ? = 1 and ? = 2
in Fig. 3, for lack of space). Note that the morphs
found by this model differ qualitatively from those
found by Model 1, in that we get longer morphs
more related to agglutination than to regular inflec-
tion patterns. This suggests that multiple statistical
models should be used together to extract different
facets of a language?s morphological composition.
Finally, morphs from the more complex Model
2 are given in Fig. 4. As in Model 1a, Model
2 gives more agglutinative morphs than inflective
morphs, and has a greater tendency to segment com-
plex morphs (such as -ification-), which pre-
sumably will later be resegmented into their com-
ponent parts (e.g., -if+ic+at+ion). This may
enable construction of hierarchical models of mor-
phological composition in the future.
5.2 Turkish
In addition to English, we tested the method?s abil-
ity to extract meaningful morphs on a small corpus
of Turkish texts from the Turkish Natural Language
Processing Initiative (Oflazer, 2001), which consists
of one foreign ministry press release, texts of two
treaties, and three journal articles on translation.
The corpus comprises 20,284 individual words, of
which 5961 are unique. Turkish is a highly aggluti-
native language, hence a prime candidate for recur-
sive morphological segmentation. Results for Mod-
els 1 and 2 are shown in Tables 5?8. Meaningful
Prefixes
non- rein-
bio- over-
?disi- *ine-
diss- ?interc-
video- fluor-
financier- wood-
quadr- key-
*kl- *kar-
weather- vin-
*jas- ?kings-
Suffixes
-?s -ville
-town -field
?-ification ?-ians
?-alize ?-alising
?-ically ?-ological
-tech -wood
?-ioning ?-etic
?-sively -point
?-nating -tally
?-tational *-uting
Figure 4: The first 20 English prefix and suffix morphs
extracted using Model 2, as above, with b = 8.
p Meaning
bahs- talk (about)
terk- leaving
redd- refuse, rejected
zikr- mention (someone)
bey- Mr., sir
akt- agree
haps- (im)prison
birbirlerin- one to another
s?efin- your chief
tedbirler- precautions
birin- somebody
hu?ku?mlerin- your opinions
u?lkesin- his country
elimiz- our hand
du?zenlemelerin- your arrangements
yerin- your place
kendin- yourself
devletler- governments
bic?imin- your style
istedig?im- (thing) that I want
Figure 5: Turkish morphs segmented as prefixes using
Model 1.
morphs were found using all models, with Model 2
finding longer morphs, as in English. We do note
some issues with boundary letters for Model 2 pre-
fixes, however.
6 Conclusions
We have given a firmer foundation for the use of
minimal description length (MDL) criteria for mor-
phological analysis by giving a novel local formula-
tion of the change in description length (DL) upon
resegmentation of the corpus on a prefix (or suffix),
p Meaning
-nin of
-n?n of
-n? your
-na to your
-ler plural form
-leri plural form
-nda at your
-ni your
-lerin your (things)
-ki that
p Meaning
-si of
-ndan from
-lar? plural form
-lar plural form
-s? of
-lar?n your (pl.) (things)
-lerine to your (pl.) (things)
-ya to
-lara to (pl.)
-dir is
Figure 6: Turkish morphs segmented as suffixes using
Model 1.
p Meaning
hizmet(l)- service
neden(l)- reason
madd- material
birbir- one another
belg- document
izlenim- observation
nitelik- specification
en- width
dil- language
bilg(i)- knowledge
p Meaning
bahs- mention
zih(in)- memory
verg(i)- tax
person(el)- employee
biri- one of
verme- giving
vere(n)- giver
belirsi(z)- unknown
bildirim- announcement
zikr- mention
Figure 7: Turkish morphs segmented as prefixes using
Model 2. Turkish letters in parentheses are not in the
segmented morphs, though a better segmentation would
have included them.
which enables an efficient algorithm for greedy con-
struction of a morph dictionary using an MDL cri-
terion. The algorithm we have devised is generic, in
that it may easily be applied to any local description
length model. Early results of our method, as evalu-
ated by examination of the morphs it extracts, show
high accuracy in finding meaningful morphs based
solely on orthographic considerations; in fact, we
find that Model 1, which depends only on the num-
ber of morphs in the dictionary (and not on frequen-
cies in the corpus at all) gives surprisingly good re-
sults, though Model 2 may generally be preferable
(more experiments on varied and larger corpora still
remain to be run).
We see two immediate directions for future work.
The first comprises direct improvements to the tech-
niques presented here. Rather than segmenting pre-
fixes and suffixes separately, the data structures and
algorithms should be extended to segment both pre-
fixes and suffixes in the current morph list, depend-
ing on which gives the best overall DL improve-
ment. Related is the need to enable approximate
matching of ?boundary? characters due to ortho-
graphic shifts such as -y to -i-, as well as incorpo-
rating other orthographic filters on possible morphs
(such as requiring prefixes to contain a vowel). An-
other algorithmic extension will be to develop an
p Meaning
-isine toward (someone)
-nlerinin of their (things)
-taki which at
-isini from, towards
-yeti to
-iyorsa if (pres.)
-ili with
-likte at (the place of)
-?in of
-imizden from our
p Meaning
-ilerine to (pl.)
-lemektedir it does
*-tik
-ilemez cannot do
-lerimizi our things
-mun from my
-mlar (plural)
-tmak to
-unca while
-lu with
Figure 8: Turkish morphs segmented as suffixes using
Model 2; tables as in Figure 5.
efficient beam-search algorithm (avoiding copying
the entire data structure), which may improve accu-
racy over the current greedy search method. In ad-
dition, we will investigate the use of more sophisti-
cated DL models, including, for example, semantic
similarity between candidate affixes and stems, us-
ing the probability of occurrence of individual char-
acters for coding, or using n-gram probabilities for
coding the corpus as a sequence of morphs (instead
of the unigram coding model used here and previ-
ously).
The second direction involves integrating the cur-
rent algorithm into a larger system for more compre-
hensive morphological analysis. As noted above,
due to the greedy nature of the search, a recombi-
nation step may be needed to ?glue? morphs that
got incorrectly separated (such as un- and -der-).
More fundamentally, we intend to use the algorithm
presented here (with the above extensions) as a sub-
routine in a paradigm construction system along the
lines of Goldsmith (2001). It seems likely that effi-
cient and accurate MDL segmentation as we present
here will enable more effective search through the
space of possible morphological signatures.
Acknowledgements
Thanks to Moshe Fresko and Kagan Agun for help
with the Turkish translations, as well as the anony-
mous reviewers for their comments.
References
M. Baroni, J. Matiasek, and H. Trost. 2002. Unsu-
pervised discovery of morphologically related words
based on orthographic and semantic similarity. In
Proceedings of the Workshop on Morphological
and Phonological Learning of ACL/SIGPHON-2002,
pages 48?57.
Andrew Barron, Jorma Rissanen, and Bin Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information Theory,
44(6):2743?2760, October.
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in minimum description length induction.
In Proceedings of the Fifth International Workshop on
Artificial Intelligence and Statistics, Ft. Lauderdale,
FL.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proceedings of the Work-
shop on Morphological and Phonological Learning of
ACL-02, pages 21?30, Philadelphia.
Herve? De?jean. 1998. Morphemes as necessary concept
for structures discovery from untagged corpora. In
Workshop on Paradigms and Grounding in Natural
Language Learning, pages 295?299, Adelaide.
S. T. Dumais, G. W. Furnas, T. K. Landauer, S. Deer-
wester, and R. Harshman. 1988. Using latent seman-
tic analysis to improve access to textual information.
In Proceedings of the SIGCHI conference on Human
factors in computing systems, pages 281?285. ACM
Press.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27:153?198.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences - Computer Science and Computational Bi-
ology. Cambridge University Press.
Zellig Harris. 1955. From phoneme to morpheme. Lan-
guage, 31:190?222.
Kemal Oflazer. 2001. English Turkish aligned
parallel corpora. Turkish Natural Language
Processing Initiative, Bilkent University.
http://www.nlp.cs.bilkent.edu.tr/Turklang/
corpus/par-corpus/.
Patrick Schone and Daniel Jurafsky. 2000. Knowledge
free induction of morphology using latent semantic
analysis. In Proceedings of CoNLL-2000 and LLL-
2000, pages 67?72, Lisbon.
Matthew Snover, Gaja Jarosz, and Michael Brent. 2002.
Unsupervised learning of morphology using a novel
directed search algorithm: Taking the first step. In
ACL-2002 Workshop on Morphological and Phono-
logical Learning.
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1356?1364,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Decomposition of a Document into Authorial Components 
 
 
Moshe Koppel      Navot Akiva Idan Dershowitz Nachum Dershowitz 
Dept. of Computer Science  Dept. of Bible School of Computer Science 
Bar-Ilan University Hebrew University Tel Aviv University 
Ramat Gan, Israel Jerusalem, Israel Ramat Aviv, Israel 
{moishk,navot.akiva}@gmail.com dershowitz@gmail.com nachumd@tau.ac.il 
 
 
 
 
Abstract 
We propose a novel unsupervised method 
for separating out distinct authorial compo-
nents of a document. In particular, we show 
that, given a book artificially ?munged? 
from two thematically similar biblical 
books, we can separate out the two consti-
tuent books almost perfectly. This allows 
us to automatically recapitulate many con-
clusions reached by Bible scholars over 
centuries of research. One of the key ele-
ments of our method is exploitation of dif-
ferences in synonym choice by different 
authors. 
1 Introduction  
We propose a novel unsupervised method for 
separating out distinct authorial components of a 
document.  
There are many instances in which one is faced 
with a multi-author document and wishes to deli-
neate the contributions of each author. Perhaps the 
most salient example is that of documents of his-
torical significance that appear to be composites of 
multiple earlier texts. The challenge for literary 
scholars is to tease apart the document?s various 
components. More contemporary examples include 
analysis of collaborative online works in which 
one might wish to identify the contribution of a 
particular author for commercial or forensic pur-
poses.  
We treat two versions of the problem. In the 
first, easier, version, the document to be decom-
posed is given to us segmented into units, each of 
which is the work of a single author. The challenge 
is only to cluster the units according to author. In 
the second version, we are given an unsegmented 
document and the challenge includes segmenting 
the document as well as clustering the resulting 
units. 
We assume here that no information about the 
authors of the document is available and that in 
particular we are not supplied with any identified 
samples of any author?s writing. Thus, our me-
thods must be entirely unsupervised.  
There is surprisingly little literature on this 
problem, despite its importance. Some work in this 
direction has been done on intrinsic plagiarism de-
tection (e.g., Meyer zu Eisen 2006) and document 
outlier detection (e.g., Guthrie et al 2008), but this 
work makes the simplifying assumption that there 
is a single dominant author, so that outlier units 
can be identified as those that deviate from the 
document as a whole. We don?t make this simpli-
fying assumption. Some work on a problem that is 
more similar to ours was done by Graham et al 
(2005). However, they assume that examples of 
pairs of paragraphs labeled as same-
author/different-author are available for use as the 
basis of supervised learning. We make no such 
assumption. 
The obvious approach to our unsupervised ver-
sion of the problem would be to segment the text 
(if necessary), represent each of the resulting units 
of text as a bag-of-words, and then use clustering 
algorithms to find natural clusters. We will see, 
however, that this na?ve method is quite inade-
quate. Instead, we exploit a method favored by the 
literary scholar, namely, the use of synonym 
choice. Synonym choice proves to be far more use-
ful for authorial decomposition than ordinary lexi-
cal features. However, synonyms are relatively 
1356
sparse and hence, though reliable, they are not 
comprehensive; that is, they are useful for separat-
ing out some units but not all. Thus, we use a two-
stage process: first find a reliable partial clustering 
based on synonym usage and then use these as the 
basis for supervised learning using a different fea-
ture set, such as bag-of-words. 
We use biblical books as our testbed. We do 
this for two reasons. First, this testbed is well mo-
tivated, since scholars have been doing authorial 
analysis of biblical literature for centuries. Second, 
precisely because it is of great interest, the Bible 
has been manually tagged in a variety of ways that 
are extremely useful for our method. 
Our main result is that given artificial books 
constructed by randomly ?munging? together ac-
tual biblical books, we are able to separate out au-
thorial components with extremely high accuracy, 
even when the components are thematically simi-
lar. Moreover, our automated methods recapitulate 
many of the results of extensive manual research in 
authorial analysis of biblical literature. 
The structure of the paper is as follows. In the 
next section, we briefly review essential informa-
tion regarding our biblical testbed. In Section 3, we 
introduce a na?ve method for separating compo-
nents and demonstrate its inadequacy. In Section 4, 
we introduce the synonym method, in Section 5 we 
extend it to the two-stage method, and in Section 6, 
we offer systematic empirical results to validate 
the method. In Section 7, we extend our method to 
handle documents that have not been pre-
segmented and present more empirical results. In 
Section 8, we suggest conclusions, including some 
implications for Bible scholarship. 
2 The Bible as Testbed 
While the biblical canon differs across religions 
and denominations, the common denominator con-
sists of twenty-odd books and several shorter 
works, ranging in length from tens to thousands of 
verses. These works vary significantly in genre, 
and include historical narrative, law, prophecy, and 
wisdom literature. Some of these books are re-
garded by scholars as largely the product of a sin-
gle author?s work, while others are thought to be 
composites in which multiple authors are well-
represented ? authors who in some cases lived in 
widely disparate periods. In this paper, we will 
focus exclusively on the Hebrew books of the Bi-
ble, and we will work with the original untran-
slated texts. 
The first five books of the Bible, collectively 
known as the Pentateuch, are the subject of much 
controversy. According to the predominant Jewish 
and Christian traditions, the five books were writ-
ten by a single author ? Moses. Nevertheless, scho-
lars have found in the Pentateuch what they believe 
are distinct narrative and stylistic threads corres-
ponding to multiple authors.  
Until now, the work of analyzing composite 
texts has been done in mostly impressionistic fa-
shion, whereby each scholar attempts to detect the 
telltale signs of multiple authorship and compila-
tion. Some work on biblical authorship problems 
within a computational framework has been at-
tempted, but does not handle our problem. Much 
earlier work (for example, Radday 1970; Bee 
1971; Holmes 1994) uses multivariate analysis to 
test whether the clusters in a given clustering of 
some biblical text are sufficiently distinct to be 
regarded as probably a composite text. By contrast, 
our aim is to find the optimal clustering of a docu-
ment, given that it is composite. Crucially, unlike 
that earlier work, we empirically prove the efficacy 
of our methods by testing it against known ground 
truth. Other computational work on biblical au-
thorship problems (Mealand 1995; Berryman et al 
2003) involves supervised learning problems 
where some disputed text is to be attributed to one 
of a set of known authors. The supervised author-
ship attribution problem has been well-researched 
(for surveys, see Juola (2008), Koppel et al (2009) 
and Stamatatos (2009)), but it is quite distinct from 
the unsupervised problem we consider here.  
Since our problem has been dealt with almost 
exclusively using heuristic methods, the subjective 
nature of such research has left much room for de-
bate. We propose to set this work on a firm algo-
rithmic basis by identifying an optimal stylistic 
subdivision of the text. We do not concern our-
selves with how or why such distinct threads exist. 
Those for whom it is a matter of faith that the Pen-
tateuch is not a composition of multiple writers can 
view the distinction investigated here as that of 
multiple styles. 
3 A Na?ve Algorithm 
For expository purposes, we will use a canoni-
cal example to motivate and illustrate each of a 
1357
sequence of increasingly sophisticated algorithms 
for solving the decomposition problem. Jeremiah 
and Ezekiel are two roughly contemporaneous 
books belonging to the same biblical sub-genre 
(prophetic works), and each is widely thought to 
consist primarily of the work of a single distinct 
author. Jeremiah consists of 52 chapters and Eze-
kiel consists of 48 chapters. For our first challenge, 
we are given all 100 unlabeled chapters and our 
task is to separate them out into the two constituent 
books. (For simplicity, let?s assume that it is 
known that there are exactly two natural clusters.) 
Note that this is a pre-segmented version of the 
problem since we know that each chapter belongs 
to only one of the books. 
As a first try, the basics of which will serve as a 
foundation for more sophisticated attempts, we do 
the following: 
1. Represent each chapter as a bag-of-words (us-
ing all words that appear at least k times in the 
corpus). 
2. Compute the similarity of every pair of chapters 
in the corpus. 
3. Use a clustering algorithm to cluster the chap-
ters into two clusters. 
We use k=2, cosine similarity and ncut cluster-
ing (Dhillon et al 2004). Comparing the Jeremiah-
Ezekiel split to the clusters thus obtained, we have 
the following matrix: 
Book Cluster I Cluster II 
Jer 
Eze 
29 
28 
23 
20 
 
As can be seen, the clusters are essentially or-
thogonal to the Jeremiah-Ezekiel split. Ideally, 
100% of the chapters would lie on the majority 
diagonal, but in fact only 51% do. Formally, our 
measure of correspondence between the desired 
clustering and the actual one is computed by first 
normalizing rows and then computing the weight 
of the majority diagonal relative to the whole. This 
measure, which we call normalized majority di-
agonal (NMD), runs from 50% (when the clusters 
are completely orthogonal to the desired split) to 
100% (where the clusters are identical with the 
desired split). NMD is equivalent to maximal ma-
cro-averaged recall where the maximum is taken 
over the (two) possible assignments of books to 
clusters. In this case, we obtain an NMD of 51.5%, 
barely above the theoretical minimum. 
This negative result is not especially surprising 
since there are many ways for the chapters to split 
(e.g., according to thematic elements, sub-genre, 
etc.) and we can?t expect an unsupervised method 
to read our minds. Thus, to guide the method in the 
direction of stylistic elements that might distin-
guish between Jeremiah and Ezekiel, we define a 
class of generic biblical words consisting of all 223 
words that appear at least five times in each of ten 
different books of the Bible. 
Repeating our experiment of above, though li-
miting our feature set to generic biblical words, we 
obtain the following matrix: 
Book Cluster I Cluster II 
Jer 
Eze 
32 
28 
20 
20 
 
As can be seen, using generic words yields 
NMD of 51.3%, which does not improve matters at 
all. Thus, we need to try a different approach. 
4 Exploiting Synonym Usage 
One of the key features used by Bible scholars 
to classify different components of biblical litera-
ture is synonym choice. The underlying hypothesis 
is that different authorial components are likely to 
differ in the proportions with which alternative 
words from a set of synonyms (synset) are used. 
This hypothesis played a part in the pioneering 
work of Astruc (1753) on the book of Genesis ?
using a single synset: divine names ? and has been 
refined by many others using broader feature sets, 
such as that of Carpenter and Hartford-Battersby 
(1900). More recently, the synonym hypothesis has 
been used in computational work on authorship 
attribution of English texts in the work of Clark 
and Hannon (2007) and Koppel et al (2006). 
This approach presents several technical chal-
lenges. First, ideally ? in the absence of a suffi-
ciently comprehensive thesaurus ? we would wish 
to identify synonyms in an automated fashion. 
Second, we need to adapt our similarity measure 
for reasons that will be made clear below. 
4.1 (Almost) Automatic Synset Identification 
One of the advantages of using biblical litera-
ture is the availability of a great deal of manual 
annotation. In particular, we are able to identify 
synsets by exploiting the availability of the stan-
dard King James translation of the Bible into Eng-
1358
lish (KJV). Conveniently, and unlike most modern 
translations, KJV almost invariably translates syn-
onyms identically. Thus, we can generally identify 
synonyms by considering the translated version of 
the text. There are two points we need to be precise 
about. First, it is not actually words that we regard 
as synonymous, but rather word roots. Second, to 
be even more precise, it is not quite roots that are 
synonymous, but rather senses of roots. Conve-
niently, Strong?s (1890 [2010]) Concordance lists 
every occurrence of each sense of each root that 
appears in the Bible separately (where senses are 
distinguished in accordance with the KJV transla-
tion). Thus, we can exploit KJV and the concor-
dance to automatically identify synsets as well as 
occurrences of the respective synonyms in a syn-
set.1 (The above notwithstanding, there is still a 
need for a bit of manual intervention: due to poly-
semy in English, false synsets are occasionally 
created when two non-synonymous Hebrew words 
are translated into two senses of the same English 
word. Although this could probably be handled 
automatically, we found it more convenient to do a 
manual pass over the raw synsets and eliminate the 
problems.)  
The above procedure yields a set of 529 synsets 
including a total of 1595 individual synonyms. 
Most synsets consist of only two synonyms, but 
some include many more. For example, there are 7 
Hebrew synonyms corresponding to ?fear?. 
4.2 Adapting the Similarity Measure 
Let?s now represent a unit of text as a vector in 
the following way. Each entry represents a syn-
onym in one of the synsets. If none of the syn-
onyms in a synset appear in the unit, all their cor-
responding entries are 0. If j different synonyms in 
a synset appear in the unit, then each correspond-
ing entry is 1/j and the rest are 0. Thus, in the typi-
cal case where exactly one of the synonyms in a 
synset appears, its corresponding entry in the vec-
tor is 1 and the rest are 0. 
Now we wish to measure the similarity of two 
such vectors. The usual cosine measure doesn?t 
capture what we want for the following reason. If 
the two units use different members of a synset, 
cosine is diminished; if they use the same members 
of a synset, cosine is increased. So far, so good. 
But suppose one unit uses a particular synonym 
                                                           
1
 Thanks to Avi Shmidman for his assistance with this. 
and the other doesn?t use any member of that syn-
set. This should teach us nothing about the similar-
ity of the two units, since it reflects only on the 
relevance of the synset to the content of that unit; it 
says nothing about which synonym is chosen when 
the synset is relevant. Nevertheless, in this case, 
cosine would be diminished. 
The required adaptation is as follows: we first 
eliminate from the representation any synsets that 
do not appear in both units (where a synset is said 
to appear in a unit if any of its constituent syn-
onyms appear in the unit). We then compute cosine 
of the truncated vectors. Formally, for a unit x 
represented in terms of synonyms, our new similar-
ity measure is cos'(x,y) = cos(x|S(x ?y),y|S(x ?y)), 
where x|S(x ?y) is the projection of x onto the syn-
sets that appear in both x and y.  
4.3  Clustering Jeremiah-Ezekiel Using Syn-
onyms 
We now apply ncut clustering to the similarity 
matrix computed as described above. We obtain 
the following split: 
Book Cluster I Cluster II 
Jer 
Eze 
48 
5 
4 
43 
 
Clearly, this is quite a bit better than results ob-
tained using simple lexical features as described 
above. Intuition for why this works can be pur-
chased by considering concrete examples. There 
are two Hebrew synonyms ? p???h and miq??a? 
corresponding to the word ?corner?, two (min??h 
and t?r?m?h) corresponding to the word ?obla-
tion?, and two (n??a? and ???al) corresponding to 
the word ?planted?. We find that p???h, min??h 
and n??a? tend to be located in the same units and, 
concomitantly, miq??a?, t?r?m?h and ???al are lo-
cated in the same units. Conveniently, the former 
are all Jeremiah and the latter are all Ezekiel.  
While the above result is far better than those 
obtained using more na?ve feature sets, it is, never-
theless, far from perfect. We have, however, one 
more trick at our disposal that will improve these 
results further. 
5 Combining Partial Clustering and Su-
pervised Learning 
Analysis of the above clustering results leads to 
two observations. First, some of the units belong 
1359
firmly to one cluster or the other. The rest have to 
be assigned to one cluster or the other because 
that?s the nature of the clustering algorithm, but in 
fact are not part of what we might think of as the 
core of either cluster. Informally, we say that a unit 
is in the core of its cluster if it is sufficiently simi-
lar to the centroid of its cluster and it is sufficiently 
more similar to the centroid of its cluster than to 
any other centroid. Formally, let S be a set of syn-
sets, let B be a set of units, and let C be a cluster-
ing of B where the units in B are represented in 
terms of the synsets in S. For a unit x in cluster 
C(x) with centroid c(x), we say that x is in the core 
of C(x) if cos'(x,c(x))>?1 and cos'(x,c(x))-cos'(x,c)>?2 
for every centroid c?c(x). In our experiments be-
low, we use ?1=1/?2 (corresponding to an angle of 
less than 45 degrees between x and the centroid of 
its cluster) and ?2=0.1. 
Second, the clusters that we obtain are based on 
a subset of the full collection of synsets that does 
the heavy lifting. Formally, we say that a synonym 
n in synset s is over-represented in cluster C if 
p(x?C|n?x) > p(x?C|s?x) and p(x?C|n?x) > p(x?C). 
That is, n is over-represented in C if knowing that 
n appears in a unit increases the likelihood that the 
unit is in C, relative to knowing only that some 
member of n?s synset appears in the unit and rela-
tive to knowing nothing. We say that a synset s is a 
separating synset for a clustering {C1,C2} if some 
synonym in s is over-represented in C1 and a dif-
ferent synonym in s is over-represented in C2. 
5.1 Defining the Core of a Cluster 
We leverage these two observations to formally 
define the cores of the respective clusters using the 
following iterative algorithm. 
1. Initially, let S be the collection of all synsets, let 
B be the set of all units in the corpus 
represented in terms of S, and let {C1,C2} be 
an initial clustering of the units in B. 
2. Reduce B to the cores of C1 and C2. 
3. Reduce S to the separating synsets for {C1,C2}. 
4. Redefine C1 and C2 to be the clusters obtained 
from clustering the units in the reduced B 
represented in terms of the synsets in reduced S. 
5. Repeat Steps 2-4 until convergence (no further 
changes to the retained units and synsets). 
At the end of this process, we are left with two 
well-separated cluster cores and a set of separating 
synsets. When we compute cores of clusters in our 
Jeremiah-Ezekiel experiment, 26 of the initial 100 
units are eliminated. Of the 154 synsets that appear 
in the Jeremiah-Ezekiel corpus, 118 are separating 
synsets for the resulting clustering. The resulting 
cluster cores split with Jeremiah and Ezekiel as 
follows:  
Book Cluster I Cluster II 
Jer 
Eze 
36 
2 
0 
36 
 
We find that all but two of the misplaced units 
are not part of the core. Thus, we have a better 
clustering but it is only a partial one. 
5.2 Using Cores for Supervised Learning 
Now that we have what we believe are strong 
representatives of each cluster, we can use them in 
a supervised way to classify the remaining unclus-
tered units. The interesting question is which fea-
ture set we should use. Using synonyms would just 
get us back to where we began. Instead we use the 
set of generic Bible words introduced earlier. The 
point to recall is that while this feature set proved 
inadequate in an unsupervised setting, this does not 
mean that it is inadequate for separating Jeremiah 
and Ezekiel, given a few good training examples. 
Thus, we use a bag-of-words representation re-
stricted to generic Bible words for the 74 units in 
our cluster cores and label them according to the 
cluster to which they were assigned. We now apply 
SVM to learn a classifier for the two clusters. We 
assign each unit, including those in the training set, 
to the class assigned to it by the SVM classifier. 
The resulting split is as follows: 
Book Cluster I Cluster II 
Jer 
Eze 
51 
0 
1 
48 
 
Remarkably, even the two Ezekiel chapters that 
were in the Jeremiah cluster (and hence were es-
sentially misleading training examples) end up on 
the Ezekiel side of the SVM boundary.  
It should be noted that our two-stage approach 
to clustering is a generic method not specific to our 
particular application. The point is that there are 
some feature sets that are very well suited to a par-
ticular unsupervised problem but are sparse, so 
they give only a partial clustering. At the same 
time, there are other feature sets that are denser 
and, possibly for that reason, adequate for super-
1360
vised separation of the intended classes but inade-
quate for unsupervised separation of the intended 
classes. This suggests an obvious two-stage me-
thod for clustering, which we use here to good ad-
vantage. 
This method is somewhat reminiscent of semi-
supervised methods sometimes used in text catego-
rization where few training examples are available 
(Nigam et al 2000). However, those methods typi-
cally begin with some information, either in the 
form of a small number of labeled documents or in 
the form of keywords, while we are not supplied 
with these. Furthermore, the semi-supervised work 
bootstraps iteratively, at each stage using features 
drawn from within the same feature set, while we 
use exactly two stages, the second of which uses a 
different type of feature set than the first.  
For the reader?s convenience, we summarize the 
entire two-stage method: 
1. Represent units in terms of synonyms. 
2. Compute similarities of pairs of units using 
cos'. 
3. Use ncut to obtain an initial clustering. 
4. Use the iterative method to find cluster cores. 
5. Represent units in cluster cores in terms of ge-
neric words. 
6. Use units in cluster cores as training for learn-
ing an SVM classifier. 
7. Classify all units according to the learned SVM 
classifier. 
6 Empirical Results 
We now test our method on other pairs of bibli-
cal books to see if we obtain comparable results to 
those seen above. We need, therefore, to identify a 
set of biblical books such that (i) each book is suf-
ficiently long (say, at least 20 chapters), (ii) each is 
written by one primary author, and (iii) the authors 
are distinct. Since we wish to use these books as a 
gold standard, it is important that there be a broad 
consensus regarding the latter two, potentially con-
troversial, criteria. Our choice is thus limited to the 
following five books that belong to two biblical 
sub-genres: Isaiah, Jeremiah, Ezekiel (prophetic 
literature), Job and Proverbs (wisdom literature). 
(Due to controversies regarding authorship (Pope 
1952, 1965), we include only Chapters 1-33 of 
Isaiah and only Chapters 3-41 of Job.) 
Recall that our experiment is as follows: For 
each pair of books, we are given all the chapters in 
the union of the two books and are given no infor-
mation regarding labels. The object is to sort out 
the chapters belonging to the respective two books. 
(The fact that there are precisely two constituent 
books is given.) 
We will use the three algorithms seen above: 
1. generic biblical words representation and ncut 
clustering; 
2. synonym representation and ncut clustering; 
3. our two-stage algorithm. 
We display the results in two separate figures. 
In Figure 1, we see results for the six pairs of 
books that belong to different sub-genres. In Figure 
2, we see results for the four pairs of books that are 
in the same genre. (For completeness, we include 
Jeremiah-Ezekiel, although it served above as a 
development corpus.) All results are normalized 
majority diagonal. 
 
 
Figure 1. Results of three clustering methods for differ-
ent-genre pairs 
 
 
Figure 2. Results of three clustering methods for same-
genre pairs 
    
As is evident, for different-genre pairs, even the 
simplest method works quite well, though not as 
well as the two-stage method, which is perfect for 
five of six such pairs. The real advantage of the 
two-stage method is for same-genre pairs. For 
1361
these the simple method is quite erratic, while the 
two-stage method is near perfect. We note that the 
synonym method without the second stage is 
slightly worse than generic words for different-
genre pairs (probably because these pairs share 
relatively few synsets) but is much more consistent 
for same-genre pairs, giving results in the area of 
90% for each such pair. The second stage reduces 
the errors considerably over the synonym method 
for both same-genre and different-genre pairs. 
7  Decomposing Unsegmented Documents 
Up to now, we have considered the case where 
we are given text that has been pre-segmented into 
pure authorial units. This does not capture the kind 
of decomposition problems we face in real life. For 
example, in the Pentateuch problem, the text is 
divided up according to chapter, but there is no 
indication that the chapter breaks are correlated 
with crossovers between authorial units. Thus, we 
wish now to generalize our two-stage method to 
handle unsegmented text. 
7.1 Generating Composite Documents 
To make the problem precise, let?s consider 
how we might create the kind of document that we 
wish to decompose. For concreteness, let?s think 
about Jeremiah and Ezekiel. We create a composite 
document, called Jer-iel, as follows: 
1. Choose the first k1 available verses of Jeremiah, 
where k1 is a random integer drawn from the 
uniform distribution over the integers 1 to m. 
2. Choose the first k2 available verses of Ezekiel, 
where k2 is a new random integer drawn from 
the above distribution. 
3. Repeat until one of the books is exhausted; then 
choose the remaining verses of the other book. 
For the experiments discussed below, we use 
m=100 (though further experiments, omitted for 
lack of space, show that results shown are essen-
tially unchanged for any m?60). Furthermore, to 
simulate the Pentateuch problem, we break Jer-iel 
into initial units by beginning a new unit whenever 
we reach the first verse of one of the original chap-
ters of Jeremiah or Ezekiel. (This does not leak any 
information since there is no inherent connection 
between these verses and actual crossover points.) 
7.2 Applying the Two-Stage Method 
Our method works as follows. First, we refine 
the initial units (each of which might be a mix of 
verses from Jeremiah and Ezekiel) by splitting 
them into smaller units that we hope will be pure 
(wholly from Jeremiah or from Ezekiel). We say 
that a synset is doubly-represented in a unit if the 
unit includes two different synonyms of that syn-
set. Doubly-represented synsets are an indication 
that the unit might include verses from two differ-
ent books. Our object is thus to split the unit in a 
way that minimizes doubly-represented synonyms. 
Formally, let M(x) represent the number of synsets 
for which more than one synonym appear in x. Call 
?x1,x2? a split of x if x=x1x2. A split ?x1',x2'? is optim-
al if ?x1',x2'?= argmax M(x)-max(M(x1),M(x2)) where 
the maximum is taken over all splits of x. If for an 
initial unit, there is some split for which M(x)-
max(M(x1),M(x2)) is greater than 0, we split the unit 
optimally; if there is more than one optimal split, 
we choose the one closest to the middle verse of 
the unit. (In principle, we could apply this proce-
dure iteratively; in the experiments reported here, 
we split only the initial units but not split units.) 
Next, we run the first six steps of the two-stage 
method on the units of Jer-iel obtained from the 
splitting process, as described above, until the 
point where the SVM classifier has been learned. 
Now, instead of classifying chapters as in Step 7 of 
the algorithm, we classify individual verses. 
The problem with classifying individual verses 
is that verses are short and may contain few or no 
relevant features. In order to remedy this, and also 
to take advantage of the stickiness of classes across 
consecutive verses (if a given verse is from a cer-
tain book, there is a good chance that the next 
verse is from the same book), we use two smooth-
ing tactics. 
Initially, each verse is assigned a raw score by 
the SVM classifier, representing its signed distance 
from the SVM boundary. We smooth these scores 
by computing for each verse a refined score that is 
a weighted average of the verse?s raw score and 
the raw scores of the two verses preceding and 
succeeding it. (In our scheme, the verse itself is 
given 1.5 times as much weight as its immediate 
neighbors and three times as much weight as sec-
ondary neighbors.) 
Moreover, if the refined score is less than 1.0 
(the width of the SVM margin), we do not initially 
1362
assign the verse to either class. Rather, we check 
the class of the last assigned verse before it and the 
first assigned verse after it. If these are the same, 
the verse is assigned to that class (an operation we 
call ?filling the gaps?). If they are not, the verse 
remains unassigned.  
To illustrate on the case of Jer-iel, our original 
?munged? book has 96 units. After pre-splitting, 
we have 143 units. Of these, 105 are pure units. 
Our two cluster cores, include 33 and 39 units, re-
spectively; 27 of the former are pure Jeremiah and 
30 of the latter are pure Ezekiel; no pure units are 
in the ?wrong? cluster core. Applying the SVM 
classifier learned on the cluster cores to individual 
verses, 992 of the 2637 verses in Jer-iel lie outside 
the SVM margin and are assigned to some class. 
All but four of these are assigned correctly. Filling 
the gaps assigns a class to 1186 more verses, all 
but ten of them correctly. Of the remaining 459 
unassigned verses, most lie along transition points 
(where smoothing tends to flatten scores and where 
preceding and succeeding assigned verses tend to 
belong to opposite classes). 
7.3 Empirical Results 
We randomly generated composite books for 
each of the book pairs considered above. In Fig-
ures 3 and 4, we show for each book pair the per-
centage of all verses in the munged document that 
are ?correctly? classed (that is, in the majority di-
agonal), the percentage incorrectly classed (minori-
ty diagonal) and the percentage not assigned to 
either class. As is evident, in each case the vast 
majority of verses are correctly assigned and only a 
small fraction are incorrectly assigned. That is, we 
can tease apart the components almost perfectly.  
 
 
Figure 3. Percentage of verses in each munged differ-
ent-genre pair of books that are correctly and incorrectly 
assigned or remain unassigned. 
 
Figure 4. Percentage of verses in each munged same-
genre pair of books that are correctly and incorrectly 
assigned or remain unassigned. 
8 Conclusions and Future Work 
We have shown that documents can be decom-
posed into authorial components with very high 
accuracy by using a two-stage process. First, we 
establish a reliable partial clustering of units by 
using synonym choice and then we use these par-
tial clusters as training texts for supervised learn-
ing using generic words as features. 
We have considered only decompositions into 
two components, although our method generalizes 
trivially to more than two components, for example 
by applying it iteratively. The real challenge is to 
determine the correct number of components, 
where this information is not given. We leave this 
for future work. 
Despite this limitation, our success on munged 
biblical books suggests that our method can be 
fruitfully applied to the Pentateuch, since the broad 
consensus in the field is that the Pentateuch can be 
divided into two main authorial categories: Priestly 
(P) and non-Priestly (Driver 1909). (Both catego-
ries are often divided further, but these subdivi-
sions are more controversial.) We find that our 
split corresponds to the expert consensus regarding 
P and non-P for over 90% of the verses in the Pen-
tateuch for which such consensus exists. We have 
thus been able to largely recapitulate several centu-
ries of painstaking manual labor with our auto-
mated method. We offer those instances in which 
we disagree with the consensus for the considera-
tion of scholars in the field. 
In this work, we have exploited the availability 
of tools for identifying synonyms in biblical litera-
ture. In future work, we intend to extend our me-
thods to texts for which such tools are unavailable. 
1363
References  
J. Astruc. 1753. Conjectures sur les m?moires originaux 
dont il paroit que Moyse s?est servi pour composer le 
livre de la Gen?se. Brussels. 
R. E. Bee. 1971. Statistical methods in the study of the 
Masoretic text of the Old Testament. J. of the Royal 
Statistical Society, 134(1):611-622. 
M. J. Berryman, A. Allison, and D. Abbott. 2003. Sta-
tistical techniques for text classification based on 
word recurrence intervals. Fluctuation and Noise Let-
ters, 3(1):L1-L10. 
J. E. Carpenter, G. Hartford-Battersby. 1900. The Hex-
ateuch: According to the Revised Version. London. 
J. Clark and C. Hannon. 2007. A classifier system for 
author recognition using synonym-based features. 
Proc. Sixth Mexican International Conference on Ar-
tificial Intelligence, Lecture Notes in Artificial Intel-
ligence, vol. 4827, pp. 839-849. 
I. S. Dhillon, Y. Guan, and B. Kulis. 2004. Kernel k-
means: spectral clustering and normalized cuts. Proc. 
ACM International Conference on Knowledge Dis-
covery and Data Mining (KDD), pp. 551-556. 
S. R. Driver. 1909. An Introduction to the Literature of 
the Old Testament (8th ed.). Clark, Edinburgh. 
N. Graham, G. Hirst, and B. Marthi. 2005. Segmenting 
documents by stylistic character. Natural Language 
Engineering, 11(4):397-415. 
D. Guthrie, L. Guthrie, and Y. Wilks. 2008. An unsu-
pervised probabilistic approach for the detection of 
outliers in corpora. Proc. Sixth International Lan-
guage Resources and Evaluation (LREC'08), pp. 28-
30. 
D. Holmes. 1994. Authorship attribution, Computers 
and the Humanities, 28(2):87-106.  
P. Juola. 2008. Author Attribution. Series title: 
Foundations and Trends in Information Retriev-
al. Now Publishing, Delft. 
M. Koppel, N. Akiva, and I. Dagan. 2006. Feature in-
stability as a criterion for selecting potential style 
markers. J. of the American Society for Information 
Science and Technology, 57(11):1519-1525. 
M. Koppel, J.  Schler, and S. Argamon. 2009.  Compu-
tational methods in authorship attribution. J. of the 
American Society for Information Science and Tech-
nology, 60(1):9-26. 
D. L. Mealand. 1995. Correspondence analysis of Luke. 
Lit. Linguist Computing, 10(3):171-182. 
S. Meyer zu Eisen and B. Stein. 2006. Intrinsic plagiar-
ism detection. Proc. European Conference on Infor-
mation Retrieval (ECIR 2006), Lecture Notes in 
Computer Science, vol. 3936, pp. 565?569. 
K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mit-
chell. 2000. Text classification from labeled and un-
labeled documents using EM, Machine Learning, 
39(2/3):103-134. 
M. H. Pope. 1965. Job (The Anchor Bible, Vol. XV). 
Doubleday, New York, NY. 
M. H. Pope. 1952. Isaiah 34 in relation to Isaiah 35, 40-
66. Journal of Biblical Literature, 71(4):235-243. 
Y. Radday. 1970. Isaiah and the computer: A prelimi-
nary report, Computers and the Humanities, 5(2):65-
73. 
E. Stamatatos. 2009. A survey of modern authorship 
attribution methods. J. of the American Society for 
Information Science and Technology, 60(3):538-556. 
J. Strong. 1890. The Exhaustive Concordance of the 
Bible. Nashville, TN. (Online edition: 
http://www.htmlbible.com/sacrednamebiblecom/kjvs
trongs/STRINDEX.htm; accessed 14 November 
2010.) 
 
 
 
 
 
 
 
1364
