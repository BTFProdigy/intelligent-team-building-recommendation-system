Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 468?478,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Web Search Ranking by Incorporating Structured 
Annotation of Queries* 
 
 
Xiao Ding1, Zhicheng Dou2, Bing Qin1, Ting Liu1, Ji-Rong Wen3 
 
1Research Center for Social Computing and Information Retrieval 
Harbin Institute of Technology, China 
 
2Microsoft Research Asia, Beijing 100190, China 
 
3Renmin University of China, Beijing, China 
1{xding, qinb, tliu}@ir.hit.edu.cn; 
2zhichdou@microsoft.com; 3jirong.wen@gmail.com 
 
 
 
 
 
 
Abstract? 
Web users are increasingly looking for 
structured data, such as lyrics, job, or recipes, 
using unstructured queries on the web. 
However, retrieving relevant results from such 
data is a challenging problem due to the 
unstructured language of the web queries. In 
this paper, we propose a method to improve 
web search ranking by detecting Structured 
Annotation of queries based on top search 
results. In a structured annotation, the original 
query is split into different units that are 
associated with semantic attributes in the 
corresponding domain. We evaluate our 
techniques using real world queries and achieve 
significant improvement. 
1 Introduction 
Search engines are getting more sophisticated by 
utilizing information from multiple diverse sources. 
One such valuable source of information is 
structured and semi-structured data, which is not 
very difficult to access, owing to information 
extraction (Wong et al, 2009; Etzioni et al, 2008; 
Zhai and Liu 2006) and semantic web efforts. 
                                                          
? *Work was done when the first author was visiting Microsoft 
Research Asia 
Driving the web search evolution are the user 
needs. Users usually have a template in mind when 
formulating queries to search for information. 
Agarwal et al, (2010) surveyed a search log of 15 
million queries from a commercial search engine. 
They found that 90% of queries follow certain 
templates. For example, by issuing the query 
?taylor swift lyrics falling in love?, the users are 
actually seeking for the lyrics of the song ?Mary's 
Song (oh my my my)? by artist Taylor Swift. The 
words ?falling in love? are actually part of the 
lyrics they are searching for. However, some top 
search results are irrelevant to the query, although 
they contain all the query terms. For example, the 
first top search result shown in Figure 1(a) does 
not contain the required lyrics. It just contains the 
lyrics of another song of Taylor Swift, rather than 
the song that users are seeking. 
A possible way to solve the above ranking 
problem is to understand the underlying query 
structure. For example, after recognizing that 
?taylor swift? is an artist name and ?falling in love? 
are part of the lyrics, we can improve the ranking 
by comparing the structured query with the 
corresponding structured data in documents 
(shown in Figure 1(b)). Some previous studies 
investigated how to extract structured information 
from user queries, such as query segmentation 
(Bergsma and Wang, 2007). The task of query 
segmentation is to separate the query words into 
468
disjointed segments so that each segment maps to a 
semantic unit (Li et al, 2011). For example, the 
segmentation of the query ?taylor swift lyrics 
falling in love? can be ?taylor swift | lyrics | falling 
in love?. Since query segmentation cannot tell 
?talylor swift? is an artist name and ?falling in love? 
are part of lyrics, it is still difficult for us to judge 
whether each part of the query segmentations 
matches the right field of the documents or not 
(such as judge whether ?talylor swift? matches the 
artist name in the document). Recently, a lot of 
work (Sarkas et al, 2010; Li et al, 2009) proposed 
the task of structured annotation of queries which 
aims to detect the structure of the query and assign 
a specific label to it. However, to our knowledge, 
the previous methods do not exploit an effective 
approach for improving web search ranking by 
incorporating structured annotation of queries. 
In this paper, we investigate the possibility of 
using structured annotation of queries to improve 
web search ranking. Specifically, we propose a 
greedy algorithm which uses the structured data 
(named annotated tokens in Figure 1(b)) extracted 
from the top search results to annotate the latent 
structured semantics in web queries. We then 
compute matching scores between the annotated 
query and the corresponding structured 
information contained in documents. The top 
search results can be re-ranked according to the 
matching scores. However, it is very difficult to 
extract structured data from all of the search results. 
Hence, we propose a relevance feedback based re-
ranking model. We use these structured documents 
whose matching scores are greater than a threshold 
as feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Experiments on a large web search dataset from 
a major commercial search engine show that the F-
Measure of structured annotation generated by our 
approach is as high as 91%. On this dataset, our re-
ranking model using the structured annotations 
significantly outperforms two baselines. 
The main contributions of our work include: 
1. We propose a novel approach to generate 
structured annotation of queries based on top 
search results. 
2. Although structured annotation of queries has 
been studied previously, to the best of our 
knowledge this is the first paper that attempts 
to improve web search ranking by 
incorporating structured annotation of queries. 
The rest of this paper is organized as follows. 
We briefly introduce related work in Section 2. 
Section 3 presents our method for generating 
structured annotation of queries. We then propose 
two novel re-ranking models based on structured 
annotation in Section 4. Section 5 introduces the 
data used in this paper. We report experimental 
results in Section 6. Finally we conclude the work 
in Section 7. 
 
Figure 1. Search results of query ?taylor swift lyrics falling in love? and processing pipeline 
[Taylor Swift, #artist_name, 0.34]
...
[Mary?s Song (oh my my my), #song_name, 0.16]
[Crazier, #song_name, 0.1]
[Jump Then Fall, #song_name, 0.08]
...
[Growing up and falling in love?, #lyrics, 0.16]
[Feel like I?m falling and ?, #lyrics, 0.1]
[I realize your love is the best ?, #lyrics, 0.08]
d1 [Taylor Swift, #artist_name]
[Crazier, #song_name]
[Feel like I?m falling and ?, #lyrics]
d2 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
d3 [Taylor Swift, #artist_name]
[Jump Then Fall, #song_name]
[I realize your love is the best ?, #lyrics]
d4 [Taylor Swift, #artist_name]
[Mary?s Song (oh my my my), #song_name]
[Growing up and falling in love?, #lyrics]
Search Results (a)
Weighted Annotated Tokens (c)Query Structured Annotation Generation (d)Top Results Re-ranking (e)
Annotated Tokens (b)
1.
2.
3.
4.
Query: taylor swift lyrics falling in love
<[taylor swift, #artist_name] lyrics 
[falling in love, #lyrics]>
 
1. 
  
2. 
  
3. 
  
4. 
1.
2.
3.
4.
469
2 Related Work 
There is a great deal of prior research that 
identifies query structured information. We 
summarize this research according to their 
different approaches. 
2.1 Structured Annotation of Queries 
Recently, a lot of work has been done on 
understanding query structure (Sarkas et al, 2010; 
Li et al, 2009; Bendersky et al, 2010). One 
important method is structured annotation of 
queries which aims to detect the structure of the 
query and assign a specific label to it. Li et al, 
(2009) proposed web query tagging and its goal is 
to assign to each query term a specified category, 
roughly corresponding to a list of attributes. A 
semi-supervised Conditional Random Field (CRF) 
is used to capture dependencies between query 
words and to identify the most likely joint 
assignment of words to ?categories.? Comparing 
with previous work, the advantages of our 
approach are on the following aspects. First, we 
generate structured annotation of queries based on 
top search results, not some global knowledge base 
or query logs. Second, they mainly focus on the 
method of generating structured annotation of 
queries, rather than leverage the generated query 
structures to improve web search rankings. In this 
paper, we not only offer a novel solution for 
generating structured annotation of queries, but 
also propose a re-ranking approach to improve 
Web search based on structured annotation of 
queries. Bendersky et al, (2011) also used top 
search results to generate structured annotation of 
queries. However, the annotations in their 
definition are capitalization, POS tags, and 
segmentation indicators, which are different from 
ours. 
2.2 Query Template Generation 
The concept of query template has been discussed 
in a few recent papers (Agarwal et al, 2010; Pasca 
2011; Liu et al, 2011; Szpektor et al, 2011). A 
query template is a sequence of terms, where each 
term could be a word or an attribute. For example, 
<#artist_name lyrics #lyrics> is a query template, 
?#artist_name? and ?#lyrics? are attributes, and 
?lyrics? is a word. Structured annotation of queries 
is different from query template, as a query 
template can instantiate multiple queries while a 
structured annotation only serves for a specific 
query. Unlike query template, our work is ranking-
oriented. We aim to automatically annotate query 
structure based on top search results, and further 
use these structured annotations to re-rank top 
search results for improving search performance. 
2.3 Query Segmentation 
The task of query segmentation is to separate the 
query words into disjointed segments so that each 
segment maps to a semantic unit (Li et al, 2011). 
Query segmentation techniques have been well 
studied in recent literature (Tan and Peng, 2008; 
Yu and Shi, 2009). However, structured annotation 
of queries cannot only separate the query words 
into disjoint segments but can also assign each 
segment a semantic label which can help the search 
engine to judge whether each part of query 
segmentation matches the right field of the 
documents or not. 
2.4 Entity Search 
The problem of entity search has received a great 
deal of attention in recent years (Guo et al, 2009; 
Bron et al, 2010; Cheng et al, 2007). Its goal is to 
answer information needs that focus on entities. 
The problem of structured annotation of queries is 
related to entity search because for some queries, 
structured annotation items are entities or attributes. 
Some existing entity search approaches also 
exploit knowledge from the structure of webpages 
(Zhao et al, 2005). Annotating query structured 
information differs from entity search in the 
following aspects. First, structured annotation 
based ranking is applicable for all queries, rather 
than just entity related queries. Second, the result 
of an entity search is usually a list of entities, their 
attributes, and associated homepages, whereas our 
work uses the structured information from 
webpages to annotate query structured information 
and further leverage structured annotation of 
queries to re-rank top search results. 
Table 1. Example domain schemas 
Domain Schema Example structured annotations 
lyrics #artist_name 
#song_name 
#lyrics 
<lyrics of [hey jude, #song_name] [beatles, 
#artist_name]> 
job #category 
#location 
<[teacher, #category] job in [America, 
#location]> 
recipe  #directions 
#ingredients 
<[baking, # directions] [bread, # 
ingredients] recipe> 
 
470
3 Structured Annotation of Queries  
3.1 Problem Definition 
We start our discussion by defining some basic 
concepts. A token is defined as a sequence of 
words including space, i.e., one or more words. For 
example, the bigram ?taylor swift? can be a single 
token. As our objective is to find structured 
annotation of queries in a specific domain, we 
begin with a definition of domain schema. 
Definition 1 (Domain Schema): For a given 
domain of interest, the domain schema is the set of 
attributes. We denote the domain schema as ? =
{?1, ?2,? , ??}, where each ??  is the name of an 
attribute of the domain. Sample domain schemas 
are shown in Table 1. In contrast to previous 
methods (Agarwal et al, 2010), our definition of 
domain schema does not need attribute values. For 
the sake of simplicity, this paper assumes that 
attributes in domain schema are available. 
However, it is not difficult to pre-specify attributes 
in a specific domain. 
Definition 2 (Annotated Token): An annotated 
token in a specific domain is a pair [?, ?], where v 
is a token and a is a corresponding attribute for v 
in this domain. [hey jude, #song_name] is an 
example of an annotated token for the ?lyrics? 
domain shown in Table 1. The words ?hey jude? 
comprise a token, and its corresponding attribute 
name is #song_name. If a token does not have any 
corresponding attributes, we denote it as free token. 
Definition 3 (Structured Annotation): A 
structured annotation p is a sequence of terms <
?1,?2,?,?? >, where each ?? could be a free token or 
an annotated token, and at least one of the terms is 
an annotated token, i.e., ?? ? [1, ?] for which ?? is 
an annotated token. 
Given the schema for the domain ?lyrics?, 
<[taylor swift, #artist_name] lyrics [falling in love, 
#lyrics]> is a possible structured annotation for the 
query ?taylor swift lyrics falling in love?. In this 
annotation, [taylor swift, #artist_name] and 
[falling in love, #lyrics] are two annotated tokens. 
The word ?lyrics? is a free token. 
Intuitively, a structured annotation corresponds 
to an interpretation of the query as a request for 
some structured information from documents. The 
set of annotated tokens expresses the information 
need of the documents that have been requested. 
The free tokens may provide more diverse 
information. Annotated tokens and free tokens 
together cover all query terms, reflecting the 
complete user intent of the query. 
3.2 Generating Structured Annotation 
In this paper, given a domain schema A, we 
generate structured annotation for a query q based 
on the top search results of q. We propose using 
top search results, rather than some global 
knowledge base or query logs, because: 
(1) Top search results have been proven to be 
a successful technique for query explanation 
(Bendersky et al, 2010). 
(2) We have observed that in most cases, a 
reasonable percentage of the top search results are 
relevant to the query. By aggregating structured 
information from the top search results, we can get 
more query-dependent annotated tokens than using 
global data sources which may contain more noise 
and outdated. 
(3) Our goal for generating structured 
annotation is to improve the ranking quality of 
queries. Using top search results enables 
simultaneous and consistent detection of structured 
information from documents and queries. 
As mentioned in Section 3.1, we generate 
structured annotation of queries based on annotated 
tokens, which are actually structured data (shown 
in Figure 1(b)) embedded in web documents. In 
this paper, we assume that the annotated tokens are 
Algorithm 1: Query Structured Annotation Generation 
Input: a list of weighted annotated tokens T = {t1, ? , tm} ; 
          a query q = ?w1, ? , wn?  where wi ? W; 
a pre-defined threshold score ?. 
Output: a query structured annotation p = <s1, ? , sk>. 
  1: Set p = q = {s1, ?, sn}, where si = wi 
  2: for u = 1 to T.size do 
  3:       compute ?????(?, ??) 
            = ?????(?, ??. ?)  
            = ??. ? ? ???0??<??????(??? , ??. ?), 
            where pij = si,?,sj, s.t. sl ? W for l ? [i, j]. //pij is just 
in the remaining query words 
  4: end for 
  5: find the maximum matching tu with  
            ???? = ??????1?????????(?, ??) 
  6: if ?????(?, ????) > ? then 
  7:      replace si,?,sj in p with [si,?,sj, tmax.a ] 
  8:      remove tmax from T 
9:      n ? n ? (j - i) 
10:      go to step 2 
11: else  
12:      return p 
13: end if 
 
471
available and we mainly focus on how to use these 
annotated tokens from top search results to 
generate structured annotation of queries. The 
approach is comprised of two parts, one for 
weighting annotated tokens and the other for 
generating structured annotation of queries based 
on the weighted annotated tokens. 
Weighting: As shown in Figure 1, annotated 
tokens extracted from top results may be 
inconsistent, and hence some of the extracted 
annotated tokens are less useful or even useless for 
generating structured annotation. 
We assume that a better annotated token should 
be supported by more top results; while a worse 
annotated token may appear in fewer results. 
Hence we aggregate all the annotated tokens 
extracted from top search results, and evaluate the 
importance of each unique one by a ranking-aware 
voting model as follows. For an annotated token [v, 
a], its weight w is defined as: 
                      ? =
1
?
? ??1????                           (1) 
where wj is a voting from document dj, and 
?? = {
? ? ? + 1
?
,             if [?, ?] ? ??
0,                      else        
 
Here, N is the number of top search results and j 
is the ranking position of document dj. We then 
generate a weighted annotated token [v, a, w] for 
each original unique token [v, a]. 
Generating: The process by which we map a 
query q to Structured Annotation is shown in 
Algorithm 1. The algorithm takes as input a list of 
weighted annotated tokens and the query q, and 
outputs the structured annotation of the query q. 
The algorithm first partitions the query q by 
comparing each sub-sequence of the query with all 
the weighted annotated tokens, and find the 
maximum matching annotated token (line 1 to line 
5). Then, if the degree of match is greater than the 
threshold ? which is a pre-defined threshold score 
for fuzzy string matching, the query substring will 
be assigned the attribute label of the maximum 
matching annotated token (line 6 to line 8). The 
algorithm stops when all the weighted annotated 
tokens have been scanned, and outputs the 
structured annotation of the query.  
Note that in some cases, the query may fail to 
exactly match with the annotated tokens, due to 
spelling errors, acronyms or abbreviations in users? 
queries. For example, in the query ?broken and 
beatuful lyrics?, ?broken and beatuful? is a 
misspelling of ?broken and beautiful.? We adopt a 
fuzzy string matching function for comparing a 
sub-sequence string s with a token v: 
          ???(?, ?) = 1 ?
????????????(?,?)
max (|?|,|?|)
                (2) 
where EditDistance(s, v) measures the edit 
distances of two strings, |s| is the length of string s 
and |v| is the length of string v. 
4 Ranking with Structured Annotation 
Given a domain schema ? = {?1, ?2, ? , ??}, and a 
query q, suppose that ? = < ?1,?2,?,?? >  is the 
structured annotation for query q obtained using 
the method introduced in the above sections. p can 
better reflects the user?s real search intent than the 
original q, as it presents the structured semantic 
information needed instead of a simple word string. 
Therefore, a document di can better satisfy a user?s 
information need if it contains corresponding 
structured semantic information in p. Suppose that 
Ti is the set of annotated tokens extracted from 
document di, we compute a re-ranking score, 
denoted by RScore, for document di as follows: 
RScore(q, di) = ?????(?, ??) 
                      = ?????(?, ??) 
                      = ? ? ?????(?? , ?)????1????  
where 
  ?????(?? , ?)= {
???(?? . ?? , ?. ?),        if ?? . ?? = ?. ?
0,                                else
      (3) 
where ??  is an annotated token in p and t is an 
annotated token in di. We use Equation (2) to 
compute the similarity between values in query 
annotated tokens and values in document annotated 
tokens. We propose two re-ranking models, 
namely the conservative re-ranking model, to re-
rank top results based on RScore and relevance 
feedback based re-ranking model. 
4.1 Conservative Re-ranking Model 
A nature way to re-rank top search results is 
according to their RScore. However, we fail to 
obtain annotated tokens from some retrieved 
documents, and hence the RScore of these 
documents are not available. In the conservative 
re-ranking model, we only re-rank search results 
that have an RScore. For example, suppose there 
are five retrieved documents {d1, d2, d3, d4, d5} for 
query q, we can extract structured information 
from document d3 and d4 and RScore(q, d4) > 
RScore(q, d3). Note that we cannot obtain 
472
structured information from d1, d2, and d5.  In the 
conservative re-ranking method, d1, d2, and d5 
retain their original positions; while d3 and d4 will 
be re-ranked according to their RScore. Therefore, 
the final ranking generated by our conservative re-
ranking model should be {d1, d2, d4, d3, d5}, in 
which the documents are re-ranked among the 
affected positions. 
There is also useful information in the 
documents without structured data, such as 
community question answering websites. However, 
in the conservative re-ranking model they will not 
be re-ranked. This may hurt the performance of our 
re-ranking model. One reasonable solution is 
relevance feedback model. 
4.2 Relevance Feedback based Re-ranking 
Model 
The disadvantage of the conservative re-ranking 
model is that it only can re-rank those top search 
results with structured data. To make up its 
limitation, we propose a relevance feedback based 
re-ranking model. The key idea of this model is 
based on the observation that the search results 
with the corrected annotated tokens could give 
implicit feedback information. Hence, we use these 
structured documents whose RScore are greater 
than a threshold ? (empirically set it as 0.6) as 
feedback documents, to effectively re-rank other 
search results to bring more relevant and novel 
information to the user. 
Formally, given a query Q and a document 
collection C, a retrieval system returns a ranked list 
of documents D. Let di denote the i-th ranked 
document in the ranked list. Our goal is to study 
how to use these feedback documents, J ? {d1,?, 
dk}, to effectively re-rank the other r search results: 
U ? {dk+1,?, dk+r}. A general formula of relevance 
feedback model (Salton et al 1990) R is as follows: 
?(??) = (1 ? ?)??(Q) + ???(J)             (4) 
where ? ? [0, 1] is the feedback coefficient, and ?? 
and ?? are two models that map a query and a set 
of relevant documents, respectively, into some 
comparable representations. For example, they can 
be represented as vectors of weighted terms or 
language models. 
In this paper, we explore the problem in the 
language model framework, particularly the KL-
divergence retrieval model and mixture-model 
feedback method (Zhai and Lafferty, 2001), mainly 
because language models deliver state-of-the-art 
retrieval performance and the mixture-model based 
feedback is one of the most effective feedback 
techniques which outperforms Rocchio feedback. 
4.2.1 The KL-Divergence Retrieval Model 
The KL-divergence retrieval model was introduced 
in Lafferty and Zhai, (2001) as a special case of the 
risk minimization retrieval framework and can 
support feedback more naturally. In this model, 
queries and documents are represented by unigram 
language models. Assuming that these language 
models can be appropriately estimated, KL-  
divergence retrieval model measures the relevance 
value of a document D with respect to a query Q 
by computing the negative Kullback-Leibler 
divergence between the query language model ?? 
and the document language model ?? as follows: 
?(?, ?) = ??(??||??) = ?? ?(?|??)???
?(?|??)
?(?|??)
???       (5) 
where V is the set of words in our vocabulary. 
Intuitively, the retrieval performance of the KL-
divergence relies on the estimation of the 
document model ?? and the query model ??.  
For the set of k relevant documents, the 
document model ??  is estimated as ?(w|??) =
1
?
?
?(?,??)
|??|
?
?=1 , where ?(?, ??) is the count of word 
w in the i-th relevant document, and |??| is the total 
number of words in that document. The document 
model ??  needs to be smoothed and an effective 
method is Dirichlet smoothing (Zhai et al, 2001). 
The query model intuitively captures what the 
user is interested in, and thus would affect retrieval 
performance. With feedback documents, ??  is 
estimated by the mixture-model feedback method. 
4.2.2 The Mixture Model Feedback Method 
As the problem definition in Equation (4), the 
query model can be estimated by the original query 
model ?(?|??) =
?(?,?)
|?|
 (where c(w,Q) is the count 
of word w in the query Q, and |Q| is the total 
number of words in the query) and the feedback 
document model. Zhai and Lafferty, (2001) 
proposed a mixture model feedback method to 
estimate the feedback document model. More 
specifically, the model assumes that the feedback 
documents can be generated by a background 
language model ?(?|?) estimated using the whole 
collection and an unknown topic language model 
473
?? to be estimated. Formally, let F ? C be a set of 
feedback documents. In this paper, F is comprised 
of documents that RScore are greater than?. The 
log-likelihood function of the mixture model is: 
???(?|??) = 
      ? ? ?(?, ?)??? log [(1 ? ?)?(?|??) + ??(?|?)]???     (6) 
where ? ? [0,1)  is a mixture noise parameter 
which controls the weight of the background 
model. Given a fixed ?, a standard EM algorithm 
can then be used to estimate ?(?|??), which is 
then interpolated with the original query model 
?(?|Q) to obtain an improved estimation of the  
query model: 
?(?|??) = (1 ? ?)?(?|?) + ??(?|??)         (7) 
 where ? is the feedback coefficient. 
5 Data 
We used a dataset composed of 12,396 queries 
randomly sampled from query logs of a search 
engine. For each query, we retrieved its top 100 
results from a commercial search engine. The 
documents were judged by human editors. A five-
grade (from 0 to 4 meaning from bad to perfect) 
relevance rating was assigned for each document. 
We used a proprietary query domain classifier to 
identify queries in three domains, namely ?lyrics,?  
?recipe,? and ?job,? from the dataset. The statistics 
about these domains are shown in Table 2. To 
investigate how many queries may potentially have 
structured annotations, we manually created 
structured annotations for these queries. The last 
column of Table 2 shows the percentage of queries 
that have structured annotations created by 
annotators. We found that for each domain, there 
was on average more than 90% of queries 
identified by us that had a certain structured 
annotation. This indicates that a large percentage 
of these queries contain structured information, as 
we expected. 
6 Experimental Results 
In this section, we present the structured annotation 
of queries and further re-rank the top search results 
for the three domains introduced in Section 5. We 
used the ranking returned by a commercial search 
engine as our one of the Baselines. Note that as the 
baseline already uses a large number of ranking 
signals, it is very difficult to improve it any further. 
We evaluate the ranking quality using the widely 
used Normalized Discounted Cumulative Gain 
measure (NDCG) (Javelin and Kekalainen., 2000). 
We use the same configuration for NDCG as 
(Burges et al 2005). More specifically, for a given 
query q, the NDCG@K is computed as: 
                        ?? = 
1
??
? (2?(?)?1)??=1
log (1 + ?)
                            (4) 
Mq is a normalization constant (the ideal NDCG) 
so that a perfect ordering would obtain an NDCG 
of 1; and r(j) is the rating score of the j-th  
document in the ranking list.  
6.1 Overall Results 
6.1.1 Quality of Structured Annotation of 
Queries 
We generated the structured annotation of queries 
based on the top 10 search results and used ? =
0.04  for Algorithm 1. We used several existing 
metrics, P (Precision), R (Recall), and F-Measure 
to evaluate the quality of the structured annotation. 
As a query structured annotation may contain more 
than one annotated token, we concluded that the 
 
Figure 2. Ranking Quality (* indicates significant 
improvement) 
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0.62
NDCG@1 NDCG@3 NDCG@5
V
a
lu
e
 o
f 
m
e
a
s
u
r
e
m
e
n
t
Measurement
Seg-Ranker Ori-Ranker Con-Ranker FB-Ranker
*
*
*
*
*
*
Table 3. Quality of Structured Annotation. All the 
improvements are significant (p < 0.05) 
Domain Method Precision Recall F-Measure 
lyrics Baseline 
Our 
90.06% 
95.45% 
84.92% 
89.83% 
87.41% 
92.55% 
job Baseline 
Our 
89.62% 
95.31% 
80.14% 
84.93% 
84.62% 
89.82% 
recipe Baseline 
Our 
83.96% 
89.68% 
84.23% 
88.44% 
84.09% 
89.06% 
All Baseline 
Our 
87.88% 
93.61% 
83.10% 
88.45% 
85.42% 
90.96% 
 
Table 2. Domain queries used in our experiment 
Domain Containing 
Keyword 
Queries 
 
Structured  
Annotation% 
lyrics ?lyrics? 196 95% 
job ?job? 124 92% 
recipe ?recipe? 76   93% 
 
474
annotation was correct only if the entire annotation 
was completely the same as the annotation labeled 
by annotators. Otherwise we treated the structured 
annotation as incorrect. Experimental results for 
the three domains are shown in Table 3. We 
compare our approach with Xiao Li, (2010) 
(denoted as baseline), on the dataset described in 
Section 5. They labeled the semantic structure of 
noun phrase queries based on semi-Markov CRFs. 
Our approach achieves better performance than the 
baseline (about 5.5% significant improvement on 
F-Measure). This indicates that the approach of 
generating structured annotation based on the top 
search results is more effective. With the high-
quality structured annotation of queries in hand, it 
may be possible to obtain better ranking results 
using our proposed re-ranking models. 
6.1.2 Re-ranking Result 
We used the models introduced in Section 4 to re-
rank the top 10 search results, based on structured 
annotation of queries and annotated tokens.  
Recall that our goal is to quantify the 
effectiveness of structured annotation of queries 
for real web search. One dimension is to compare 
with the original search results of a commercial 
search engine (denoted as Ori-Ranker). The other 
is to compare with the query segmentation based 
re-ranking model (denoted as Seg-Ranker; Li et 
al., 2011) which tries to improve web search 
ranking by incorporating query segmentation. Li et 
al., (2011) incorporated query segmentation in the 
BM25, unigram language model and bigram 
language model retrieval framework, and bigram 
language model achieved the best performance. In 
this paper, Seg-Ranker integrates bigram language 
model with query segmentation. 
The ranking results of these models are shown 
in Figure 2. This figure shows that all our two 
rankers significantly outperform the Ori-Ranker? 
the original search results of a commercial search 
engine. This means that using high-quality 
structured annotation does help better 
understanding of user intent. By comparing these 
structured annotations and the annotated tokens in 
documents, we can re-rank the more relevant 
results higher and yield better ranking quality. 
Figure 2 also suggests that structured annotation 
based re-ranking models outperform query 
segmentation based re-ranking model. This is 
mainly because structured annotation can not only 
separate the query words into disjoint segments but 
can also assign each segment a semantic label. 
Taking full advantage of the semantic label can 
lead to better ranking performance. 
Furthermore, Figure 2 shows that FB-Ranker 
outperforms Con-Ranker. The main reason is that 
in Con-Ranker, we can only reasonably re-rank the 
search results with structured data. However, in 
FB-Ranker we can not only re-rank the structured 
search results but also can re-rank other documents 
by incorporating implicit information from those 
structured documents.  
On average, FB-Ranker achieves the best 
ranking performance. Table 4 shows more detailed 
Table 4. Detailed ranking results on three domains. 
All the improvements are significant (p < 0.05) 
Domain Ranking Method NDCG@1 NDCG@3 NDCG@5 
lyrics Seg-Ranker 0.572 0.574 0.575 
Ori-Ranker 
FB-Ranker 
0.621 
0.637 
0.628 
0.639 
0.636 
0.647 
recipe Seg-Ranker 0.629 0.631 0.634 
Ori-Ranker 
FB-Ranker 
0.678 
0.707 
0.687 
0.704 
0.696 
0.709 
job Seg-Ranker 0.438 0.413 0.408 
Ori-Ranker 
FB-Ranker 
0.470 
0.504 
0.453 
0.474 
0.442 
0.459 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.02 0.04 0.06 0.08 0.1 0.3 0.5 0.7 0.9
V
a
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Query structured annotation generation threshold ?
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0 0.02 0. 4 .06 0.08 .1 0.3 0.5 0.7 0.9 perfect
N
D
C
G
@
3
Que tr ctured annotation generation threshold ?
Seg-Ranker Ori-Ranker FB-Ranker
 
                          (a) Quality of re-ranking                                    (b) Quality of query structured annotation 
Figure 3. Quality of re-ranking and quality of query structured annotation with different number of search results 
475
results for the three selected domains. This table 
shows that FB-Ranker consistently outperforms the 
two baseline rankers on these domains. In the 
remaining part of this paper, we will only report 
the results for this ranker, due to space limitations. 
Table 4 also indicates that we can get robust 
ranking improvement in different domains, and we 
will consider applying it to more domains. 
6.2 Experiment with Different Thresholds of 
Query Structured Annotation Algorithm 
As introduced in Algorithm 1, we pre-defined a 
threshold ? for fuzzy string matching. We 
evaluated the quality of re-ranking and query 
structured annotation with different settings for ?. 
The results are shown in Figure 3. We found that: 
(1) When we use ? = 0, which means that the 
structured annotations can be generated no matter 
how small the similarity between the query string 
and a weighted annotated token is, we can get a 
significant NDCG@3 gain of 2.15%. Figure 3(b) 
shows that the precision of the structured 
annotation is lowest when ? = 0 . However, the 
precision is still as high as 0.7375, and the highest 
recall is obtained in this case. This means that the 
quality of the generated structured annotations is 
still reasonable, and hence we can get a ranking 
improvement when ? = 0, as shown in Figure 3(a). 
(2) Figure 3(a) suggests that the quality of re-
ranking increases when the threshold ? increases 
from 0 to 0.05. It then decreases when ? increases 
from 0.06 to 0.5. Comparing these two figures 
shows that the trend of re-ranking performance 
adheres to the quality of the structured annotation. 
The settings for ? dramatically affect the recall and 
precision of the structured annotation; and hence 
the ranking quality is impacted. The larger ? is, the 
lower the recall of the structured annotation is. 
(3) Since the re-ranking performance 
dramatically changes along with the quality of the 
structured annotation, we conducted a re-ranking 
experiment with perfect structured annotations (F-
Measure equal to 1.0). Perfect structured 
annotations mean the annotations created by 
annotators as introduced in Section 5. The results 
are shown in the last bar of Figure 3(a). We did not 
find a large space for ranking improvement. The 
NDCG@3 when using perfect structured 
annotations was 0.606, which is just slightly better 
than our best result (yield when ?=0.05). It 
indicates that our structured annotation generation 
algorithm is already quite effective. 
(4) Figure 3(a) shows that our approach 
outperforms the two baseline approaches with most 
settings for ?. This indicates that our approach is 
relatively stable with different settings for ?. 
6.3 Experiment with Number of Top Search 
Results 
The above experiments are conducted based on the 
top 10 search results. In this section, by adjusting 
the number of top search results, ranging from 2 to 
100, we investigate whether the quality of 
structured annotation of queries and the 
performance of re-ranking are affected by the 
quantity of search results. The results shown in 
Figure 4 indicate that the number of search results 
does affect the quality of structured annotation of 
queries and the performance of re-ranking. 
Structured annotations of queries become better 
when more search results are used from 2 to 20. 
This is because more search results cover more 
websites in our domain list, and hence can generate 
more annotated tokens. More results also provide 
more evidence for voting the importance of 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100
V
a
u
l
u
e
 
o
f
 
m
e
a
s
u
r
e
m
e
n
t
Number of search results
Precision
Recall
F-Measure
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
2 3 4 5 6 7 8 9 10 0 30 4 50 60 70 80 90 100
N
D
C
G
@
3
Number of search result
Seg-Ranker Ori-Ranker FB-Ranker
 
                              (a) Quality of re-ranking                                   (b) Quality of query structured annotation 
Figure 4. Quality of re-ranking and quality of query structured annotation with different number of search results 
476
annotated tokens, and hence can improve the 
quality of structured annotation of queries. 
In addition, we also found that structured 
annotation of queries become worse when too 
many lower ranked results are used (e.g, using 
results ranked lower than 20). This is because the 
lower ranked results are less relevant than the 
higher ranked results. They may contain more 
irrelevant or noisy annotated tokens than higher 
ranked documents; and hence using them may 
harm the precision of the structured annotations. 
Figure 4 also indicates that the quality of ranking 
and the accuracy of structured annotations are 
correlated. 
7 Conclusions 
In this paper, we studied the problem of improving 
web search ranking by incorporating structured 
annotation of queries. We proposed a systematic 
solution, first to generate structured annotation of 
queries based on top search results, and then 
launching two structured annotation based re-
ranking models. We performed a large-scale 
evaluation over 12,396 queries from a major search 
engine. The experiment results show that the F-
Measure of query structured annotation generated 
by our approach is as high as 91%. In the same 
dataset, our structured annotation based re-ranking 
model significantly outperforms the original ranker 
? the ranking of a major search engine, with 
improvements 5.2%. 
 
Acknowledgments 
This work was supported by National Natural Science 
Foundation of China (NSFC) via grant 61273321, 
61133012 and the Nation-al 863 Leading Technology 
Research Project via grant 2012AA011102. 
References  
G. Agarwal, G. Kabra, and K. C.-C. Chang. Towards 
rich query interpretation: walking back and forth for 
mining query templates. In Proc. of WWW '10. 
M. Bendersky, W. Bruce Croft and D. A. Smith. Joint 
Annotation of Search Queries, In Proc. of ACL-HLT 
2011. 
M. Bendersky, W. Bruce Croft and D. A. Smith. 
Structural Annotation of Search Queries Using 
Pseudo-Relevance Feedback, In Proc. Of CIKM 2010. 
S. Bergsma and Q. I. Wang. Learning noun phrase 
query segmentation. In Proceedings of EMNLP-
CoNLL'07. 
M. Bron, K. Balog, and M. de Rijke. Ranking related 
entities: components and analyses. In Proc. of 
CIKM ?10. 
C. Buckley. Automatic query expansion using SMART. 
InProc. of TREC-3, pages 69?80, 1995. 
C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, 
N. Hamilton, and G. Hullender. Learning to rank 
usinggradient descent. In Proceedings of ICML '05. 
T. Cheng, X. Yan, and K. C.-C. Chang. Supporting 
entity search: a large-scale prototype search engine. 
In Proc. of SIGMOD ?07. 
O. Etzioni, M. Banko, S. Soderland, and D.S. Weld, 
(2008). Open Information Extraction from the Web, 
Communications of the ACM, 51(12): 68-74. 
J. Guo, G. Xu, X. Cheng, and H. Li. Named entity 
recognition in query. In Proc. Of SIGIR? 2009. 
K. Jarvelin and J. Kekalainen. Ir evaluation methods for 
retrieving highly relevant documents. In SIGIR '00. 
J. Lafferty and C. Zhai, Document language models, 
query models, and risk minimization for information 
retrieval, In Proceedings of SIGIR'01, pages 111-119, 
2001. 
V. Lavrenko and W. B. Croft. Relevance based 
language models. In Proc. of SIGIR, pages 120?127, 
2001. 
Y. Li, BJP. Hsu, CX. Zhai and K. Wang. Unsupervised 
Query Segmentation Using Clickthrough for 
Information Retrieval. In Proc. of SIGIR'11. 
X. Li, Y.-Y. Wang, and A. Acero. Extracting structured 
information from user queries with semi-supervised 
conditional random fields. In Proc. of SIGIR'09. 
Y. Liu, X. Ni, J-T. Sun, Z. Chen. Unsupervised 
Transactional Query Classification Based on 
Webpage Form Understanding. In Proc. of CIKM '11. 
Y. Liu, M. Zhang, L. Ru, and S. Ma. Automatic query 
type identification based on click-through 
information. In LNCS, 2006. 
M. Pasca. Asking What No One Has Asked Before: 
Using Phrase Similarities To Generate Synthetic 
Web Search Queries. In Proc. of CIKM '11. 
G. Salton and C. Buckley. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 
41(4):288-297, 1990. 
477
N. Sarkas, S. Paparizos, and P. Tsaparas. Structured 
annotations of web queries. In Proc. of SIGMOD'10. 
I. Szpektor, A. Gionis, and Y. Maarek. Improving 
recommendation for long-tail queries via templates. 
In Proc. of WWW '11 
B. Tan and F. Peng. Unsupervised query segmentation 
using generative language models and wikipedia. In 
WWW?08. 
T.-L. Wong, W. Lam, and B. Chen. Mining 
employment market via text block detection and 
adaptive cross-domain information extraction. In 
Proc. SIGIR, pages 283?290, 2009. 
X. Yu and H. Shi. Query segmentation using 
conditional random fields. In Proceedings of KEYS 
'09. 
C. Zhai and J. Lafferty, Model-based feedback in the 
language modeling approach to information 
retrieval , In Proceedings of CIKM'01, pages 403-410, 
2001. 
C. Zhai and J. Lafferty, A study of smoothing methods 
for language models applied to ad hoc information 
retrieval, In Proceedings of SIGIR'01, pages 334-342, 
2001. 
Y. Zhai and B. Liu. Structured data extraction from the 
Web based on partial tree alignment. IEEE Trans. 
Knowl. Data Eng., 18(12):1614?1628, Dec. 2006. 
H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu. 
Fully automatic wrapper generation for search 
engines. In Proceedings of WWW ?05. 
S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint 
optimization of wrapper generation and template 
detection. In Proc. of SIGKDD'07. 
478
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415?1425,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Using Structured Events to Predict Stock Price Movement:
An Empirical Investigation
Xiao Ding
??
, Yue Zhang
?
, Ting Liu
?
, Junwen Duan
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{xding, tliu, jwduan}@ir.hit.edu.cn
?
Singapore University of Technology and Design
yue zhang@sutd.edu.sg
Abstract
It has been shown that news events influ-
ence the trends of stock price movements.
However, previous work on news-driven
stock market prediction rely on shallow
features (such as bags-of-words, named
entities and noun phrases), which do not
capture structured entity-relation informa-
tion, and hence cannot represent complete
and exact events. Recent advances in
Open Information Extraction (Open IE)
techniques enable the extraction of struc-
tured events from web-scale data. We
propose to adapt Open IE technology for
event-based stock price movement pre-
diction, extracting structured events from
large-scale public news without manual
efforts. Both linear and nonlinear mod-
els are employed to empirically investigate
the hidden and complex relationships be-
tween events and the stock market. Large-
scale experiments show that the accuracy
of S&P 500 index prediction is 60%, and
that of individual stock prediction can be
over 70%. Our event-based system out-
performs bags-of-words-based baselines,
and previously reported systems trained on
S&P 500 stock historical data.
1 Introduction
Predicting stock price movements is of clear in-
terest to investors, public companies and govern-
ments. There has been a debate on whether the
market can be predicted. The Random Walk The-
ory (Malkiel, 1973) hypothesizes that prices are
determined randomly and hence it is impossible to
outperform the market. However, with advances
of AI, it has been shown empirically that stock
?
This work was done while the first author was visiting
Singapore University of Technology and Design
Figure 1: Example news for Apple Inc. and
Google Inc.
price movement is predictable (Bondt and Thaler,
1985; Jegadeesh, 1990; Lo and MacKinlay, 1990;
Jegadeesh and Titman, 1993). Recent work (Das
and Chen, 2007; Tetlock, 2007; Tetlock et al.,
2008; Si et al., 2013; Xie et al., 2013; Wang and
Hua, 2014) has applied Natural Language Process-
ing (NLP) techniques to help analyze the effect of
web texts on stock market prediction, finding that
events reported in financial news are important ev-
idence to stock price movement prediction.
As news events affect human decisions and the
volatility of stock prices is influenced by human
trading, it is reasonable to say that events can influ-
ence the stock market. Figure 1 shows two pieces
of financial news about Apple Inc. and Google
Inc., respectively. Shares of Apple Inc. fell as trad-
ing began in New York on Thursday morning, the
day after its former CEO Steve Jobs passed away.
Google?s stock fell after grim earnings came out.
Accurate extraction of events from financial news
may play an important role in stock market pre-
diction. However, previous work represents news
documents mainly using simple features, such as
bags-of-words, noun phrases, and named entities
(Lavrenko et al., 2000; Kogan et al., 2009; Luss
and d?Aspremont, 2012; Schumaker and Chen,
2009). With these unstructured features, it is dif-
ficult to capture key events embedded in financial
news, and even more difficult to model the impact
of events on stock market prediction. For exam-
ple, representing the event ?Apple has sued Sam-
sung Electronics for copying ?the look and feel?
1415
of its iPad tablet and iPhone smartphone.? using
term-level features {?Apple?, ?sued?, ?Samsung?,
?Electronics?, ?copying?, ...} alone, it can be dif-
ficult to accurately predict the stock price move-
ments of Apple Inc. and Samsung Inc., respec-
tively, as the unstructured terms cannot indicate
the actor and object of the event.
In this paper, we propose using structured in-
formation to represent events, and develop a pre-
diction model to analyze the relationship between
events and the stock market. The problem is im-
portant because it provides insights into under-
standing the underlying mechanisms of the influ-
ence of events on the stock market. There are two
main challenges to this method. On the one hand,
how to obtain structured event information from
large-scale news streams is a challenging problem.
We propose to apply Open Information Extraction
techniques (Open IE; Banko et al. (2007); Et-
zioni et al. (2011); Fader et al. (2011)), which
do not require predefined event types or manu-
ally labeled corpora. Subsequently, two ontolo-
gies (i.e. VerbNet and WordNet) are used to gen-
eralize structured event features in order to reduce
their sparseness. On the other hand, the problem
of accurately predicting stock price movement us-
ing structured events is challenging, since events
and the stock market can have complex relations,
which can be influenced by hidden factors. In ad-
dition to the commonly used linear models, we
build a deep neural network model, which takes
structured events as input and learn the potential
relationships between events and the stock market.
Experiments on large-scale financial news
datasets from Reuters
1
(106,521 documents)
and Bloomberg
2
(447,145 documents) show that
events are better features for stock market predic-
tion than bags-of-words. In addition, deep neu-
ral networks achieve better performance than lin-
ear models. The accuracy of S&P 500 index pre-
diction by our approach outperforms previous sys-
tems, and the accuracy of individual stock predic-
tion can be over 70% on the large-scale data.
Our system can be regarded as one step towards
building an expert system that exploits rich knowl-
edge for stock market prediction. Our results are
helpful for automatically mining stock price re-
lated news events, and for improving the accuracy
of algorithm trading systems.
1
http://www.reuters.com/
2
http://www.bloomberg.com/
2 Method
2.1 Event Representation
We follow the work of Kim (1993) and design a
structured representation scheme that allows us to
extract events and generalize them. Kim defines
an event as a tuple (O
i
, P , T ), where O
i
? O is
a set of objects, P is a relation over the objects
and T is a time interval. We propose a representa-
tion that further structures the event to have roles
in addition to relations. Each event is composed
of an action P , an actor O
1
that conducted the
action, and an object O
2
on which the action was
performed. Formally, an event is represented as
E = (O
1
, P, O
2
, T ), where P is the action, O
1
is the actor,O
2
is the object and T is the timestamp
(T is mainly used for aligning stock data with
news data). For example, the event ?Sep 3, 2013
- Microsoft agrees to buy Nokia?s mobile phone
business for $7.2 billion.? is modeled as: (Actor =
Microsoft, Action = buy, Object = Nokia?s mobile
phone business, Time = Sep 3, 2013).
Previous work on stock market prediction rep-
resents events as a set of individual terms (Fung
et al., 2002; Fung et al., 2003; Hayo and Ku-
tan, 2004; Feldman et al., 2011). For example,
?Microsoft agrees to buy Nokia?s mobile phone
business for $7.2 billion.? can be represented by
{?Microsoft?, ?agrees?, ?buy?, ?Nokia?s?, ?mo-
bile?, ...} and ?Oracle has filed suit against Google
over its ever-more-popular mobile operating sys-
tem, Android.? can be represented by {?Oracle?,
?has?, ?filed?, ?suit?, ?against?, ?Google?, ...}.
However, terms alone might fail to accurately pre-
dict the stock price movement ofMicrosoft, Nokia,
Oracle and Google, because they cannot indicate
the actor and object of the event. To our knowl-
edge, no effort has been reported in the literature
to empirically investigate structured event repre-
sentations for stock market prediction.
2.2 Event Extraction
A main contribution of our work is to extract and
use structured events instead of bags-of-words in
prediction models. However, structured event ex-
traction can be a costly task, requiring predefined
event types and manual event templates (Ji and Gr-
ishman, 2008; Li et al., 2013). Partly due to this,
the bags-of-words-based document representation
has been the mainstream method for a long time.
To tackle this issue, we resort to Open IE, extract-
ing event tuples from wide-coverage data with-
1416
out requiring any human input (e.g. templates).
Our system is based on the system of Fader et al.
(2011) and the work of Ding et al. (2013); it does
not require predefined target event types and la-
beled training examples. Given a natural language
sentence obtained from news texts, the following
procedure is used to extract structured events:
1. Event Phrase Extraction. We extract the
predicate verb P of a sentence based on
the dependency parser of Zhang and Clark
(2011), and then find the longest sequence of
words P
v
, such that P
v
starts at P and satis-
fies the syntactic and lexical constraints pro-
posed by Fader et al. (2011). The content of
these two constraints are as follows:
? Syntactic constraint: every multi-word
event phrase must begin with a verb, end
with a preposition, and be a contiguous
sequence of words in the sentence.
? Lexical constraint: an event phrase
should appear with at least a minimal
number of distinct argument pairs in a
large corpus.
2. Argument Extraction. For each event
phrase P
v
identified in the step above, we find
the nearest noun phrase O
1
to the left of P
v
in the sentence, and O
1
should contain the
subject of the sentence (if it does not contain
the subject of P
v
, we find the second near-
est noun phrase). Analogously, we find the
nearest noun phrase O
2
to the right of P
v
in
the sentence, and O
2
should contain the ob-
ject of the sentence (if it does not contain the
object of P
v
, we find the second nearest noun
phrase).
An example of the extraction algorithm is as fol-
lows. Consider the sentence,
Instant view: Private sector adds 114,000 jobs
in July: ADP.
The predicate verb is identified as ?adds?, and
its subject and object ?sector? and ?jobs?, respec-
tively. The structured event is extracted as (Private
sector, adds, 114,000 jobs).
2.3 Event Generalization
Our goal is to train a model that is able to make
predictions based on various expressions of the
same event. For example, ?Microsoft swallows
Nokia?s phone business for $7.2 billion? and ?Mi-
crosoft purchases Nokia?s phone business? report
the same event. To improve the accuracy of our
prediction model, we should endow the event ex-
traction algorithm with generalization capacity.
To this end, we leverage knowledge from two
well-known ontologies, WordNet (Miller, 1995)
and VerbNet (Kipper et al., 2006). The pro-
cess of event generalization consists of two steps.
First, we construct a morphological analysis tool
based on the WordNet stemmer to extract lemma
forms of inflected words. For example, in ?In-
stant view: Private sector adds 114,000 jobs in
July.?, the words ?adds? and ?jobs? are trans-
formed to ?add? and ?job?, respectively. Second,
we generalize each verb to its class name in Verb-
Net. For example, ?add? belongs to the multi-
ply class. After generalization, the event (Private
sector, adds, 114,000 jobs) becomes (private sec-
tor, multiply class, 114,000 job). Similar methods
on event generalization have been investigated in
Open IE based event causal prediction (Radinsky
and Horvitz, 2013).
2.4 Prediction Models
1. Linear model. Most previous work uses linear
models to predict the stock market (Fung et al.,
2002; Luss and d?Aspremont, 2012; Schumaker
and Chen, 2009; Kogan et al., 2009; Das and
Chen, 2007; Xie et al., 2013). To make direct com-
parisons, this paper constructs a linear prediction
model by using Support Vector Machines (SVMs),
a state-of-the-art classification model. Given a
training set (d
1
, y
1
), (d
2
, y
2
), ..., (d
N
, y
N
),
where n ? [1, N ], d
n
is a news document and
y
i
? {+1, ?1} is the output class. d
n
can be
news titles, news contents or both. The output
Class +1 represents that the stock price will in-
crease the next day/week/month, and the output
Class -1 represents that the stock price will de-
crease the next day/week/month. The features
can be bag-of-words features or structured event
features. By SVMs, y = argmax{Class +
1, Class ? 1} is determined by the linear func-
tion w ??(d
n
, y
n
), where w is the feature weight
vector, and ?(d
n
, y
n
) is a function that maps d
n
into a M-dimensional feature space. Feature tem-
plates will be discussed in the next subsection.
2. Nonlinear model. Intuitively, the relationship
between events and the stock market may be more
complex than linear, due to hidden and indirect
1417
? 
News documents 
?1 
Class +1 The polarity of the stock price movement is positive 
Class -1 The polarity of the stock price movement is negative 
Input Layer 
Output Layer 
Hidden Layers ? 
? 
?2 ?3 ?M 
Figure 2: Structure of the deep neural network
model
relationships. We exploit a deep neural network
model, the hidden layers of which is useful for
learning such hidden relationships. The structure
of the model with two hidden layers is illustrated
in Figure 2. In all layers, the sigmoid activation
function ? is used.
Let the values of the neurons of the output layer
be y
cls
(cls ? {+1,?1}), its input be net
cls
, and
y
2
be the value vector of the neurons of the last
hidden layer; then:
y
cls
= f(net
cls
) = ?(wcls ? y2) (1)
where wcls is the weight vector between the neu-
ron cls of the output layer and the neurons of the
last hidden layer. In addition,
y
2k
= ?(w
2k ? y1) (k ? [1, |y2|])
y
1j
= ?(w
1j ??(dn)) (j ? [1, |y1|])
(2)
Here y
1
is the value vector of the neu-
rons of the first hidden layer, w
2k =
(w
2k1
, w
2k2
, ..., w
2k|y
1
|
), k ? [1, |y
2
|] and
w
1j = (w1j1, w1j2, ..., w1jM ), j ? [1, |y1|].
w
2kj
is the weight between the kth neuron of
the last hidden layer and the jth neuron of the
first hidden layer; w
1jm
is the weight between
the jth neuron of the first hidden layer and the
mth neuron of the input layer m ? [1, M ]; d
n
is a news document and ?(d
n
) maps d
n
into a
M-dimensional features space. News documents
and features used in the nonlinear model are the
same as those in the linear model, which will be
introduced in details in the next subsection. The
standard back-propagation algorithm (Rumelhart
et al., 1985) is used for supervised training of the
neural network.
train dev test
number of
instances
1425 178 179
number of
events
54776 6457 6593
time inter-
val
02/10/2006
-
18/16/2012
19/06/2012
-
21/02/2013
22/02/2013
-
21/11/2013
Table 1: Dataset splitting
2.5 Feature Representation
In this paper, we use the same features (i.e. docu-
ment representations) in the linear and nonlinear
prediction models, including bags-of-words and
structured events.
(1) Bag-of-words features. We use the clas-
sic ?TFIDF? score for bag-of-words features. Let
L be the vocabulary size derived from the train-
ing data (introduced in the next section), and
freq(t
l
) denote the number of occurrences of
the lth word in the vocabulary in document d.
TF
l
=
1
|d|
freq(t
l
), ?l ? [1 , L], where |d| is
the number of words in the document d (stop
words are removed). TFIDF
l
=
1
|d|
freq(t
l
) ?
log(
N
|{d :freq(t
l
)>0}|
), where N is the number of
documents in the training set. The feature vector
? can be represented as? = (?
1
, ?
2
, ..., ?
M
) =
(TFIDF
1
, TFIDF
2
, ..., TFIDF
M
). The TFIDF
feature representation has been used by most pre-
vious studies on stock market prediction (Kogan et
al., 2009; Luss and d?Aspremont, 2012).
(2) Event features. We represent an event
tuple (O
1
, P, O
2
, T ) by the combination of
elements (except for T) (O
1
, P , O
2
, O
1
+ P ,
P + O
2
, O
1
+ P + O
2
). For example, the
event tuple (Microsoft, buy, Nokia?s mobile phone
business) can be represented as (#arg1=Microsoft,
#action=get class, #arg2=Nokia?s mobile phone
business, #arg1 action=Microsoft get class,
#action arg2=get class Nokia?s mobile phone
business, #arg1 action arg2=Microsoft get class
Nokia?s mobile phone business). Structured
events are more sparse than words, and we reduce
sparseness by two means. First, verb classes
(Section 2.3) are used instead of verbs for P. For
example, ?get class? is used instead of the verb
?buy?. Second, we use back-off features, such
as O
1
+ P (?Microsoft get class?) and P + O
2
(?get class Nokia?s mobile phone business?), to
address the sparseness ofO
1
andO
2
. Note that the
order of O
1
and O
2
is important for our task since
they indicate the actor and object, respectively.
1418
 0.52
 0.53
 0.54
 0.55
 0.56
 0.57
 0.58
 0.59
 0.6
1day 1week 1month
Ac
cu
rac
y
Time span
bow+svmbow+deep neural network
event+svm
event+deep neural network
(a) Accuarcy
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
1day 1week 1month
MC
C
Time span
bow+svmbow+deep neural network
event+svm
event+deep neural network
(b) MCC
Figure 3: Overall development experiment results
3 Experiments
Our experiments are carried out on three differ-
ent time intervals: short term (1 day), medium
term (1 week) and long term (1 month). We test
the influence of events on predicting the polarity
of stock change for each time interval, comparing
the event-based news representation with bag-of-
words-based news representations, and the deep
neural network model with the SVM model.
3.1 Data Description
We use publicly available financial news from
Reuters and Bloomberg over the period from Oc-
tober 2006 to November 2013. This time span
witnesses a severe economic downturn in 2007-
2010, followed by a modest recovery in 2011-
2013. There are 106,521 documents in total
from Reuters News and 447,145 from Bloomberg
News. News titles and contents are extracted from
HTML. The timestamps of the news are also ex-
tracted, for alignment with stock price informa-
tion. The data size is larger than most previous
work in the literature.
We mainly focus on predicting the change of the
Standard & Poor?s 500 stock (S&P 500) index
3
,
obtaining indices and stock price data from Yahoo
Finance. To justify the effectiveness of our predic-
tion model, we also predict price movements of
fifteen individual shares from different sectors in
S&P 500. We automatically align 1,782 instances
of daily trading data with news titles and contents
from the previous day/the day a week before the
stock price data/the day a month before the stock
price data, 4/5 of which are used as the training
3
Standard & Poor?s 500 is a stock market index based
on the market capitalizations of 500 large companies having
common stock listed on the NYSE or NASDAQ.
data, 1/10 for development testing and 1/10 for
testing. As shown in Table 1, the training, devel-
opment and test set are split temporally, with the
data from 02/10/2006 to 18/16/2012 for training,
the data from 19/06/2012 to 21/02/2013 for de-
velopment testing, and the data from 22/02/2013
to 21/11/2013 for testing. There are about 54,776
events in the training set, 6,457 events in the de-
velopment set and 6,593 events in the test set.
3.2 Evaluation Metrics
We use two assessment metrics. First, a standard
and intuitive approach to measuring the perfor-
mance of classifiers is accuracy. However, this
measure is very sensitive to data skew: when a
class has an overwhelmingly high frequency, the
accuracy can be high using a classifier that makes
prediction on the majority class. Previous work
(Xie et al., 2013) uses an additional evaluation
metric, which relies on the Matthews Correlation
Cofficient (MCC) to avoid bias due to data skew
(our data are rather large and not severely skewed,
but we also use MCC for comparison with previ-
ous work). MCC is a single summary value that
incorporates all 4 cells of a 2*2 confusion matrix
(True Positive, False Positive, True Negative and
False Negative, respectively). GivenTP ,TN , FP
and FN :
MCC =
TP ?TN?FP ?FN
?
(TP+FP)(TP+FN )(TN +FP)(TN +FN )
(3)
3.3 Overall Development Results
We evaluate our four prediction methods (i.e.
SVM with bag-of-word features (bow), deep neu-
ral network with bag-of-word features (bow),
1419
1 day 1 week 1 month
1 layer
Accuracy 58.94% 57.73% 55.76%
MCC 0.1249 0.0916 0.0731
2 layers
Accuracy 59.60% 57.73% 56.19%
MCC 0.1683 0.1215 0.0875
Table 2: Different numbers of hidden layers
title content content +
title
bloomberg
title + title
Acc 59.60% 54.65% 56.83% 59.64%
MCC 0.1683 0.0627 0.0852 0.1758
Table 3: Different amounts of data
SVM with event features and deep neural network
with event features) on three time intervals (i.e.
1 day, 1 week and 1 month, respectively) on the
development dataset, and show the results in Fig-
ure 3. We find that:
(1) Structured event is a better choice for rep-
resenting news documents. Given the same pre-
diction model (SVM or deep neural network), the
event-based method achieves consistently better
performance than the bag-of-words-based method
over all three time intervals. This is likely due
to the following two reasons. First, being an ex-
traction of predicate-argument structures, events
carry the most essential information of the docu-
ment. In contrast, bag-of-words can contain more
irrelevant information. Second, structured events
can directly give the actor and object of the action,
which is important for predicting stock market.
(2) The deep neural network model achieves
better performance than the SVM model, partly by
learning hidden relationships between structured
events and stock prices. We give analysis to these
relationships in the next section.
(3) Event information is a good indicator for
short-term volatility of stock prices. As shown in
Figure 3, the performance of daily prediction is
better than weekly and monthly prediction. Our
experimental results confirm the conclusion of
Tetlock, Saar-Tsechansky, and Macskassy (2008)
that there is a one-day delay between the price
response and the information embedded in the
news. In addition, we find that some events may
cause immediate changes of stock prices. For ex-
ample, former Microsoft CEO Steve Ballmer an-
nounced he would step down within 12 months
on 23/08/2013. Within an hour, Microsoft shares
jumped as much as 9 percent. This fact indicates
that it may be possible to predict stock price move-
ment on a shorter time interval than one day. How-
Google Inc.
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
67.86% 0.4642 61.17% 0.2301 55.70% 0.1135
Boeing Company
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
68.75% 0.4339 57.14% 0.1585 56.04% 0.1605
Wal-Mart Stores
Company News Sector News All News
Acc MCC Acc MCC Acc MCC
70.45% 0.4679 62.03% 0.2703 56.04% 0.1605
Table 4: Individual stock prediction results
ever, we cannot access fine-grained stock price
historical data, and this investigation will be left
as future work.
3.4 Experiments with Different Numbers of
Hidden Layers of the Deep Neural
Network Model
Cybenko (1989) states that when every processing
element utilizes the sigmoid activation function,
one hidden layer is enough to solve any discrim-
inant classification problem, and two hidden lay-
ers are capable to parse arbitrary output functions
of input pattern. Here we conduct a development
experiment by different number of hidden layers
for the deep neural network model. As shown in
Table 2, the performance of two hidden layers is
better than one hidden layer, which is consistent
with the experimental results of Sharda and De-
len (2006) on the task of movie box-office predic-
tion. It indicates that more hidden layers can ex-
plain more complex relations (Bengio, 2009). In-
tuitively, three or more hidden layers may achieve
better performance. However, three hidden lay-
ers mean that we construct a five-layer deep neu-
ral network, which is difficult to train (Bengio et
al., 1994). We did not obtain improved accuracies
using three hidden layers, due to diminishing gra-
dients. A deep investigation of this problem is out
of the scope of this paper.
3.5 Experiments with Different Amounts of
Data
We conduct a development experiment by extract-
ing news titles and contents from Reuters and
Bloomberg, respectively. While titles can give the
central information about the news, contents may
provide some background knowledge or details.
Radinsky et al. (2012) argued that news titles are
more helpful for prediction compared to news con-
1420
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  100  200  300  400  500
Ac
cu
rac
y
Company Ranking
Wal-Mart
GoogleBoeing
Nike
Qualcomm
Apache
Starbucks
Avon
Visa
Symantec
Hershey
Mattel
Actavis GannettSanDisk
individual stock
(a) Accuarcy
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  100  200  300  400  500
MC
C
Company Ranking
Wal-MartGoogle
Boeing
Nike Qualcomm
Apache
Starbucks
Avon
Visa
Symantec
Hershey
Mattel
Actavis Gannett
SanDisk
individual stock
(b) MCC
Figure 4: Individual stock prediction experiment results
tents, and this paper mainly uses titles. Here we
design a comparative experiment to analyze the ef-
fectiveness of news titles and contents. First, we
use Reuters news to compare the effectiveness of
news titles and contents, and then add Bloomberg
news titles to investigate whether the amounts of
data matters. Table 3 shows that using only news
titles achieves the best performance. A likely rea-
son is that we may extract some irrelevant events
from news contents.
With the additional Bloomberg data, the results
are not dramatically improved. This is intuitively
because most events are reported by both Reuters
news and Bloomberg news. We randomly se-
lect about 9,000 pieces of news documents from
Reuters and Bloomberg and check the daily over-
lap manually, finding that about 60% of the news
are reported by both Reuters and Bloomberg. The
overlap of important news (news related to S&P
500 companies) is 80% and the overlap of unim-
portant news is 40%.
3.6 Individual Stock Prediction
In addition to predicting the S&P 500 index, we
also investigate the effectiveness of our approach
on the problem of individual stock prediction us-
ing the development dataset. We select three well-
known companies, Google Inc., Boeing Company
and Wal-Mart Stores from three different sec-
tors (i.e. Information Technology, Industrials and
Consumer Staples, respectively) classified by the
Global Industry Classification Standard (GICS).
We use company news, sector news and all news to
predict individual stock price movement, respec-
tively. The experimental results are listed in Ta-
ble 4.
The result of individual stock prediction by us-
ing only company news dramatically outperforms
the result of S&P 500 index prediction. The main
reason is that company-related events can directly
affect the volatility of company shares. There is
a strong correlation between company events and
company shares. Table 4 also shows that the result
of individual stock prediction by using sector news
or all news does not achieve a good performance,
probably because there are many irrelevant events
in all news, which would reduce the performance
of our prediction model.
The fact that the accuracy of these well-known
stocks are higher than the index may be because
there is relatively more news events dedicated to
the relevant companies. To gain a better under-
standing of the behavior of the model on more
individual stocks, we randomly select 15 compa-
nies (i.e. Google Inc., Boeing Company, Wal-Mart
Stores, Nike Inc., QUALCOMM Inc., Apache Cor-
poration, Starbucks Corp., Avon Products, Visa
Inc., Symantec Corp., The Hershey Company,
Mattel Inc., Actavis plc, Gannett Co. and SanDisk
Corporation) from S&P 500 companies. More
specifically, according to the Fortune ranking of
S&P 500 companies
4
, we divide the ranked list
into five parts, and randomly select three compa-
nies from each part. The experimental results are
shown in Figure 4. We find that:
(1) All 15 individual stocks can be predicted
with accuracies above 50%, while 60% of the
stocks can be predicted with accuracies above
60%. It shows that the amount of company-related
events has strong relationship with the volatility of
4
http://money.cnn.com/magazines/fortune/fortune500/.
The amount of company-related news is correlated to the
fortune ranking of companies. However, we find that the
trade volume does not have such a correlation with the
ranking.
1421
S&P 500 Index Prediction
Individual Stock Prediction
Google Inc. Boeing Company Wal-Mart Stores
Accuracy MCC Accuracy MCC Accuracy MCC Accuracy MCC
dev 59.60% 0.1683 67.86% 0.4642 68.75% 0.4339 70.45% 0.4679
test 58.94% 0.1649 66.97% 0.4435 68.03% 0.4018 69.87% 0.4456
Table 5: Final experimental results on the test dataset
company shares.
(2) With decreasing company fortune rankings,
the accuracy and MCC decrease. This is mainly
because there is not as much daily news about low-
ranking companies, and hence one cannot extract
enough structured events to predict the volatility
of these individual stocks.
3.7 Final Results
The final experimental results on the test dataset
are shown in Table 5 (as space is limited, we show
the results on the time interval of one day only).
The experimental results on the development and
test datasets are consistent, which indicate that our
approach has good robustness. The following con-
clusions obtained from development experiments
also hold on the test dataset:
(1) Structured events are more useful represen-
tations compared to bags-of-words for the task of
stock market prediction.
(2) A deep neural network model can be more
accurate on predicting the stock market compared
to the linear model.
(3) Our approach can achieve stable experiment
results on S&P 500 index prediction and individ-
ual stock prediction over a large amount of data
(eight years of stock prices and more than 550,000
pieces of news).
(4) The quality of information is more impor-
tant than the quantity of information on the task
of stock market prediction. That is to say that the
most relevant information (i.e. news title vs news
content, individual company news vs all news) is
better than more, but less relevant information.
3.8 Analysis and Discussion
We use Figure 5 to demonstrate our analysis to
the development experimental result of Google
Inc. stock prediction, which directly shows the
relationship between structured events and the
stock market. The links between each layer show
the magnitudes of feature weights in the model
learned using the training set.
Three events, (Google, says bought stake in,
China?s XunLei), (Google, reveals stake in, Chi-
?1 ?2 ?3 ?4 ?5 ?6 ?7 ?8 ?M ?1: (Google, says bought stake in, China?s XunLei) ?4: (Google, reveals stake in, Chinese social website) ?6: (Capgemini, partners, Google apps software)  ?2: (Oracle, sues, Google) ?5: (Google map, break, privacy law) ?8: (Google, may pull out of, China) 
? 
? 
Figure 5: Prediction of Google Inc. (we only show
structured event features since backoff features are
less informative)
nese social website) and (Capgemini, partners,
Google apps software), have the highest link
weights to the first hidden node (from the left).
These three events indicate that Google constantly
makes new partners and expands its business area.
The first hidden node has high-weight links to
Class +1, showing that Google?s positive coopera-
tion can lead to the rise of its stock price.
Three other events, (Oracle, sues, Google),
(Google map, break, privacy law) and (Google,
may pull out of, China), have high-weight links
to the second hidden node. These three events
show that Google was suffering questions and
challenges, which could affect its reputation and
further pull down its earnings. Correspondingly,
the second hidden node has high-weight links to
Class -1. These suggest that our method can au-
tomatically and directly reveal complex relation-
ships between structured events and the stock mar-
ket, which is very useful for investors, and can fa-
cilitate the research of stock market prediction.
Note that the event features used in our predic-
tion model are generalized based on the algorithm
introduced in Section 2.5. Therefore, though a
specific event in the development test set might
have never happened, its generalized form can be
found in the training set. For example, ?Google
acquired social marketing company Wildfire In-
1422
teractive? is not in the training data, but ?Google
get class? (?get? is the class name of ?acquire?
and ?buy? in VerbNet) can indeed be found in the
training set, such as ?Google bought stake in Xun-
Lei? on 04/01/2007. Hence although the full spe-
cific event feature does not fire, its back-offs fire
for a correct prediction. For simplicity of showing
the event, we did not include back-off features in
Figure 5.
4 Related Work
Stock market prediction has attracted a great deal
of attention across the fields of finance, computer
science and other research communities in the
past. The literature of stock market prediction
was initiated by economists (Keynes, 1937). Sub-
sequently, the influential theory of Efficient Mar-
ket Hypothesis (EMH) (Fama, 1965) was estab-
lished, which states that the price of a security re-
flects all of the information available and that ev-
eryone has a certain degree of access to the infor-
mation. EMH had a significant impact on security
investment, and can serve as the theoretical basis
of event-based stock price movement prediction.
Various studies have found that financial news
can dramatically affect the share price of a se-
curity (Chan, 2003; Tetlock et al., 2008). Cul-
ter et al. (1998) was one of the first to investi-
gate the relationship between news coverage and
stock prices, since which empirical text analysis
technology has been widely used across numerous
disciplines (Lavrenko et al., 2000; Kogan et al.,
2009; Luss and d?Aspremont, 2012). These stud-
ies primarily use bags-of-words to represent finan-
cial news documents. However, as Schumaker and
Chen (2009) and Xie et al. (2013) point out, bag-
of-words features are not the best choice for pre-
dicting stock prices. Schumaker and Chen (2009)
extract noun phrases and named entities to aug-
ment bags-of-words. Xie et al. (2013) explore a
rich feature space that relies on frame semantic
parsing. Wang et al. (2014) use the same fea-
tures as Xie et al. (2013), but they perform non-
parametric kernel density estimation to smooth out
the distribution of features. These can be regarded
as extensions to the bag-of-word method. The
drawback of these approaches, as discussed in the
introduction, is that they do not directly model
events, which have structured information.
There has been efforts to model events more di-
rectly (Fung et al., 2002; Hayo and Kutan, 2005;
Feldman et al., 2011). Fung, Yu, and Lam (2002)
use a normalized word vector-space to model
event. Feldman et al. (2011) extract 9 prede-
fined categories of events based on heuristic rules.
There are two main problems with these efforts.
First, they cannot extract structured event (e.g. the
actor of the event and the object of the event). Sec-
ond, Feldman et al. (2011) can obtain only lim-
ited categories of events, and hence the scalabil-
ity of their work is not strong. In contrast, we
extract structured events by leveraging Open In-
formation Extraction technology (Open IE; Yates
et al. (2007); Etzioni et al. (2011); Faber et al.
(2011)) without predefined event types, which can
effectively solve the two problems above.
Apart from events, sentiment analysis is another
perspective to the problem of stock prediction
(Das and Chen, 2007; Tetlock, 2007; Tetlock et
al., 2008; Bollen et al., 2011; Si et al., 2013). Tet-
lock (2007) examines how qualitative information
(i.e. the fraction of negative words in a particular
news column) is incorporated in aggregate market
valuations. Tetlock, Saar-Tsechansky, and Mac-
skassy (2008) extend that analysis to address the
impact of negative words in all Wall Street Joural
(WSJ) and Dow Jones News Services (DJNS) sto-
ries about individual S&P500 firms from 1980 to
2004. Bollen and Zeng (2011) study whether the
large-scale collective emotion on Twitter is cor-
related with the volatility of Dow Jones Indus-
trial Average (DJIA). From the experimental re-
sults, they find that changes of the public mood
match shifts in the DJIA values that occur 3 to 4
days later. Sentiment-analysis-based stock mar-
ket prediction focuses on investigating the influ-
ence of subjective emotion. However, this paper
puts emphasis on the relationship between objec-
tive events and the stock price movement, and is
orthogonal to the study of subjectivity. As a result,
our model can be combined with the sentiment-
analysis-based method.
5 Conclusion
In this paper, we have presented a framework for
event-based stock price movement prediction. We
extracted structured events from large-scale news
based on Open IE technology and employed both
linear and nonlinear models to empirically investi-
gate the complex relationships between events and
the stock market. Experimental results showed
that events-based document representations are
1423
better than bags-of-words-based methods, and
deep neural networks can model the hidden and in-
directed relationship between events and the stock
market. For further comparisons, we freely release
our data at http://ir.hit.edu.cn/?xding/data.
Acknowledgments
We thank the anonymous reviewers for their
constructive comments, and gratefully acknowl-
edge the support of the National Basic Re-
search Program (973 Program) of China via Grant
2014CB340503, the National Natural Science
Foundation of China (NSFC) via Grant 61133012
and 61202277, the Singapore Ministry of Educa-
tion (MOE) AcRF Tier 2 grant T2MOE201301
and SRG ISTD 2012 038 from Singapore Univer-
sity of Technology and Design. We are very grate-
ful to Ji Ma for providing an implementation of the
neural network algorithm.
References
Michele Banko, Michael J Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction for the web. In IJCAI,
volume 7, pages 2670?2676.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and trends
R
? in Machine Learning,
2(1):1?127.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8.
Werner FM Bondt and Richard Thaler. 1985. Does
the stock market overreact? The Journal of finance,
40(3):793?805.
Wesley S Chan. 2003. Stock price reaction to news and
no-news: Drift and reversal after headlines. Journal
of Financial Economics, 70(2):223?260.
David M Cutler, James M Poterba, and Lawrence H
Summers. 1998. What moves stock prices? Bern-
stein, Peter L. and Frank L. Fabozzi, pages 56?63.
George Cybenko. 1989. Approximation by superposi-
tions of a sigmoidal function. Mathematics of con-
trol, signals and systems, 2(4):303?314.
Sanjiv R Das and Mike Y Chen. 2007. Yahoo! for
amazon: Sentiment extraction from small talk on the
web. Management Science, 53(9):1375?1388.
Xiao Ding, Bing Qin, and Ting Liu. 2013. Building
chinese event type paradigm based on trigger clus-
tering. In Proc. of IJCNLP, pages 311?319, Octo-
ber.
Oren Etzioni, Anthony Fader, Janara Christensen,
Stephen Soderland, and Mausam Mausam. 2011.
Open information extraction: The second genera-
tion. In Proceedings of the Twenty-Second inter-
national joint conference on Artificial Intelligence-
Volume Volume One, pages 3?10. AAAI Press.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1535?1545. Association for Computational
Linguistics.
Eugene F Fama. 1965. The behavior of stock-market
prices. The journal of Business, 38(1):34?105.
Ronen Feldman, Benjamin Rosenfeld, Roy Bar-Haim,
and Moshe Fresko. 2011. The stock sonarsentiment
analysis of stocks based on a hybrid approach. In
Twenty-Third IAAI Conference.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, and Wai
Lam. 2002. News sensitive stock trend prediction.
In Advances in Knowledge Discovery and Data Min-
ing, pages 481?493. Springer.
Bernd Hayo and Ali M Kutan. 2005. The impact of
news, oil prices, and global market developments
on russian financial markets1. Economics of Tran-
sition, 13(2):373?393.
Narasimhan Jegadeesh and Sheridan Titman. 1993.
Returns to buying winners and selling losers: Im-
plications for stock market efficiency. The Journal
of Finance, 48(1):65?91.
Narasimhan Jegadeesh. 1990. Evidence of predictable
behavior of security returns. The Journal of Fi-
nance, 45(3):881?898.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In ACL,
pages 254?262.
John Maynard Keynes. 1937. The general theory of
employment. The Quarterly Journal of Economics,
51(2):209?223.
Jaegwon Kim. 1993. Supervenience and mind: Se-
lected philosophical essays. Cambridge University
Press.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending verbnet with novel
verb classes. In Proceedings of LREC, volume 2006,
page 1.
Shimon Kogan, Dimitry Levin, Bryan R. Routledge,
Jacob S. Sagi, and Noah A. Smith. 2009. Predicting
risk from financial reports with regression. In Proc.
NAACL, pages 272?280, Boulder, Colorado, June.
Association for Computational Linguistics.
1424
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000.
Mining of concurrent text and time series. In KDD-
2000 Workshop on Text Mining, pages 37?44.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. of ACL (Volume 1: Long Papers),
pages 73?82, August.
Andrew W Lo and Archie Craig MacKinlay. 1990.
When are contrarian profits due to stock mar-
ket overreaction? Review of Financial studies,
3(2):175?205.
Ronny Luss and Alexandre d?Aspremont. 2012.
Predicting abnormal returns from news using text
classification. Quantitative Finance, pp.1?14,
doi:10.1080/14697688.2012.672762.
Burton G. Malkiel. 1973. A Random Walk Down Wall
Street. W. W. Norton, New York.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings of the
sixth ACM international conference on Web search
and data mining, pages 255?264. ACM.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proc. of WWW, pages
909?918. ACM.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations
by error propagation. Technical report, DTIC Doc-
ument.
Robert P Schumaker and Hsinchun Chen. 2009.
Textual analysis of stock market prediction using
breaking financial news: The azfin text system.
ACM Transactions on Information Systems (TOIS),
27(2):12.
Ramesh Sharda and Dursun Delen. 2006. Predict-
ing box-office success of motion pictures with neu-
ral networks. Expert Systems with Applications,
30(2):243?254.
Jianfeng Si, Arjun Mukherjee, Bing Liu, Qing Li,
Huayi Li, and Xiaotie Deng. 2013. Exploiting
topic based twitter sentiment for stock prediction. In
Proc. of ACL (Volume 2: Short Papers), pages 24?
29, Sofia, Bulgaria, August. Association for Compu-
tational Linguistics.
Paul C Tetlock, Maytal Saar-Tsechansky, and Sofus
Macskassy. 2008. More than words: Quantifying
language to measure firms? fundamentals. The Jour-
nal of Finance, 63(3):1437?1467.
Paul C Tetlock. 2007. Giving content to investor sen-
timent: The role of media in the stock market. The
Journal of Finance, 62(3):1139?1168.
William Yang Wang and Zhenhao Hua. 2014. A
semiparametric gaussian copula regression model
for predicting financial risks from earnings calls. In
Proc. of ACL, June.
Boyi Xie, Rebecca J. Passonneau, Leon Wu, and
Germ?an G. Creamer. 2013. Semantic frames to pre-
dict stock price movement. In Proc. of ACL (Volume
1: Long Papers), pages 873?883, August.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information ex-
traction on the web. In Proc. of NAACL: Demonstra-
tions, pages 25?26. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
1425
