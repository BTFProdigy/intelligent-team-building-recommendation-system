2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 523?527,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Co-reference via Pointing and Haptics in Multi-Modal Dialogues
Lin Chen, Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
{lchen43,bdieugen}@uic.edu
Abstract
This paper describes our ongoing work on
resolving third person pronouns and deictic
words in a multi-modal corpus. We show that
about two thirds of these referring expressions
have antecedents that are introduced by point-
ing gestures or by haptic-ostensive actions
(actions that involve manipulating an object).
After describing our annotation scheme, we
discuss the co-reference models we learn from
multi-modal features. The usage of haptic-
ostensive actions in a co-reference model is a
novel contribution of our work.
1 Introduction
Co-reference resolution has received a lot of atten-
tion. However, as Eisenstein and Davis (2006)
noted, most research on co-reference resolution has
focused on written text. This task is much more
difficult in dialogue, especially in multi-modal di-
alogue contexts. First, utterances are informal, un-
grammatical and disfluent. Second, people sponta-
neously use gestures and other body language. As
noticed by Kehler (2000), Goldin-Meadow (2003),
and Chen et al (2011), in a multi-modal corpus,
the antecedents of referring expressions are often in-
troduced via gestures. Whereas the role played by
pointing gestures in referring has been studied, the
same is not true for other types of gestures. In this
paper, alongside pointing gestures, we will discuss
the role played by Haptic-Ostensive (H-O) actions,
i.e., referring to an object by manipulating it in the
world (Landragin et al, 2002; Foster et al, 2008).
As far as we know, no computational models of co-
reference have been developed that include H-O ac-
tions: (Landragin et al, 2002) focused on percep-
tual salience and (Foster et al, 2008) on generation
rather than interpretation. We should point out that
at the time of writing we only focus on resolving
third person pronouns and deictics.
The rest of this paper is organized as follows. In
Section 2 we describe our multi-modal annotation
scheme. In Section 3 we present the pronoun/deictic
resolution system. In Section 4, we discuss experi-
ments and results.
2 The Data Set
The dataset we use in this paper is a subset of the
ELDERLY-AT-HOME corpus (Di Eugenio et al,
2010), a multi-modal corpus in the domain of elderly
care. It contains 20 human-human dialogues. In
each dialogue, a helper (HEL) and an elderly person
(ELD) performed Activities of Daily Living (Krapp,
2002), such as getting up from chairs, finding pots,
cooking pastas, in a realistic setting, a studio apart-
ment used for teaching and research. The corpus
contains videos and voice data in avi format, haptics
data collected via instrumented gloves in csv format,
and the transcribed utterances in xml format.
We focused on specific subdialogues in this cor-
pus, that we call Find tasks: a Find task is a con-
tinuous time span during which the two subjects
were collaborating on finding objects. Find tasks
arise naturally while helping perform ADLs such as
preparing dinner. An excerpt from a Find task
is shown below, including annotations for pointing
gestures and for H-O actions (annotations are per-
523
formed via the Anvil tool (Kipp, 2001)).
ELD : Can you get me a pot?
HEL: (opens cabinet, takes out pot, without saying a word)
[Open(HEL,Cabinet1),Take-Out(HEL,Pot1)]
ELD: Not that one, try over there.
[Point(ELD,Cabinet5)]
Because the targets of pointing gestures and H-
O actions are real life objects, we designed a refer-
ring index system to annotate them. The referring
index system consists of compile time indices and
run time indices. We give pre-defined indices to tar-
gets which cannot be moved, like cabinets, draw-
ers, fridge. We assign run time indices to targets
which can be moved, and exist in multiple copies,
like cups, glasses. A referring index consists of a
type and an index; the index increases according to
the order of appearance in the dialogue. For exam-
ple, ?Pot#1? means the first pot referred to in the
dialogue. If a pointing gesture or H-O action in-
volved multiple objects, we used JSON (JavaScript
Object Notation)1 Array to mark it. For example,
[C#1, C#2] means Cabinet#1 and Cabinet#2.
We define a pointing gesture as a hand gesture
without physical contact with the target, whereas
gestures that involve physical contact with an ob-
ject are haptic-obstensive (H-O).2 We use four tracks
in Anvil to mark these gestures, two for pointing
gestures, and two for H-O actions. In each pair of
tracks, one track is used for HEL, one for ELD. For
both types of gestures, we mark the start time, end
time and the target(s) of the gesture using the re-
ferring index system we introduced above. Addi-
tionally we mark the type of an H-O action: Touch,
Hold, Take-Out (as in taking out an object from a
cabinet or the fridge), Close, Open.3
Our co-reference annotation follows an approach
similar to (Eisenstein and Davis, 2006). We mark
the pronouns and deictics which need to be resolved,
their antecedents, and the co-reference links be-
tween them. To mark pronouns, deictics and tex-
tual antecedents, we use the shallow parser from
1http://www.json.org/
2Whereas not all haptic actions are ostensive, in our dia-
logues they all potentially perform an ostensive function.
3Our subjects occasionally hold objects together, e.g. to fill
a pot with water: these actions are not included among the H-O
actions, and are annotated separately.
Find Subtasks 142
Length (Seconds) 5009
Speech Turns 1746
Words 8213
Pointing Gestures 362
H-O Actions 629
Pronouns and Deictics 827
Resolved Ref. Expr. 757
Textual Antecedent 218
Pointing Gesture Antecedent 266
H-O Antecedent 273
Table 1: Annotation Statistics
Apache OpenNLP Tools 4 to chunk the utterances in
each turn. We use heuristics rules to automatically
mark potential textual antecedents and the phrases
we need to resolve. Afterwards we use Anvil to edit
the results of automatic processing. To annotate co-
reference links, we first assign each of the textual
antecedents, the pointing gestures and H-O actions
a unique markable index. Finally, we link referring
expressions to their closest antecedent (if applicable)
using the markable indices.
Table 1 shows corpus and annotation statistics.
We annotated 142 Find subtasks, whose total length
is about 1 hour and 24 minutes. This sub-corpus
comprises 1746 spoken turns, which include 8213
words. 10% of the 8213 words (827 words) are pro-
nouns or deictics. Note that for only 757/827 (92%)
were the annotators able to determine an antecedent.
Interestingly, 71% of those 757 pronouns or deictics
refer to specific antecedents that are introduced ex-
clusively by gestures, either pointing or H-O actions.
In the earlier example, only the type for the referent
of that in No, not that one had been introduced textu-
ally, but not its specific antecedent pot1. Clearly, to
be effective on such data any model of co-reference
must include the targets of pointing gestures and H-
O actions. Our current model does not take into ac-
count the type provided by the de dicto interpretation
of indefinites such as a pot above, but we intend to
address this issue in future work.
In order to verify the reliability of our annotations,
we double coded 15% of the data for pointing ges-
tures and H-O actions, namely the dialogues from
3 pairs of subjects, or 22 Find subtasks. We ob-
4http://incubator.apache.org/opennlp/
524
tained reasonable ? values: for pointing gestures,
?=0.751, for H-O actions, ?=0.703, and for co-
reference, ?=0.70.
3 The Co-reference Model
In this paper we focus on how to use gesture infor-
mation (pointing or H-O) to solve the referring ex-
pressions of interest. Given a pronoun or deictic, we
build co-reference pairs by pairing it with the targets
of pointing gestures and H-O actions in a given time
window. We mark the correct pairs as ?True? and
then we train a classification model to judge if a co-
reference pair is a true pair. The main component
of the resolution system is the co-reference classifi-
cation model. Since our antecedents are not textual,
most of the traditional features for co-reference res-
olution do not apply. Rather, we use the following
multi-modal features - U is the utterance containing
the pronoun / deictic to be solved:
? Time distance between the spans of U and of
the pointing/H-O action. If the two spans over-
lap, the distance is 0.
? Speaker agreement: If the speaker of U and the
actor of the pointing/H-O action are the same.
? Markable type agreement: If the markable type
of the pronoun/deictic and of the targets of
pointing gesture/H-O action are compatible.
? Number agreement: If the number of the pro-
noun/deictic is the same as that of the targets of
the pointing gesture/H-O action.
? Object agreement: If the deictic is contained
in a phrase, such as ?this big blue bowl?,
we will check if the additional object descrip-
tion ?bowl? matches the targets of pointing
gesture/H-O action.
? H-O Action type: for co-reference pairs with
antecedents from H-O actions.
For markable type agreement, we defined two
types of markables: PLC (place) and OBJ (object).
PLC includes all the targets which cannot easily
be moved, OBJ includes all the targets like cups,
pots. We use heuristics rules to assign markable
types to pronouns/deictics and the targets of point-
ing gestures/H-O actions. To determine the number
of the targets, we extract information from the an-
notations; if the target is a JSON array, it means it
is plural. To extract additional object description for
the object agreement feature, we use the Stanford
Typed Dependency parser (De Marneffe and Man-
ning., 2008). We check if the pronoun/deictic is in-
volved in ?det? and ?nsubj? relations, if so, we ex-
tract the ?gov? element of that relation as the object
to compare with the target of gestures/H-O actions.
4 Experiments and Discussions
We have experimented with 3 types of classification
models: Maximum Entropy (MaxEnt), Decision
Tree and Support Vector Machine (SVM), respec-
tively implemented via the following three pack-
ages: MaxEnt, J48 from Weka (Hall et al, 2009),
and LibSVM (Chang and Lin, 2011). All of the
results reported below are calculated using 10 fold
cross validation.
We have run a series of experiments changing the
history length from 0 to 10 seconds for generating
co-reference pairs (history changes in increments of
1 second, hence, there are 11 sets of experiments).
For each history length, we build the 3 models men-
tioned above. An additional baseline model treats a
co-reference pair as ?True? if speaker agreement is
true for the pair, and the time distance is 0. Beside
the specified baseline, J48 can be seen as a more so-
phisticated baseline as well. When we ran the 10
fold experiment with J48 algorithm, 5 out of 10 gen-
erated decision trees only used 3 attributes.
We use two metrics to measure the performance
of the models. One are the standard precision, re-
call and F-Score with respect to the generated co-
reference pairs; the other is the number of pro-
nouns and deictics that are correctly resolved. Given
a pronoun/deictic pi, if the classifier returns more
than one positive co-reference pair for pi, we use a
heuristic resolver to choose the target. We divide
those positive pairs into two subsets, those where
the speaker of pi is the same as the performer of the
gesture (SAME), and those with the other speaker
(OTHER). If SOME is not empty, we will choose
SOME, otherwise OTHER. If the chosen set con-
tains more than one pair, we will choose the target
525
Model Hist. Prec. Rec. F.
Number
Resolved
Baseline 2 .707 .526 .603 359
J48 1 .801 .534 .641 371
SVM 2 .683 .598 .637 369
MaxEnt 0 .738 .756 .747 374
MaxEnt 2 .723 .671 .696 384
Table 2: Gesture&Haptics Co-reference Model Results
of the gesture/H-O action in the most recent pair.
Given the space limit, Table 2 only shows the
results for each model which resolved most pro-
nouns/deictics, and the model which produced the
best F-score. In Table 2, with the change of History
window setting, the gold standard of co-reference
pairs change. When the history window is larger,
there are more co-reference candidate pairs, which
help resolve more pronouns and deictics.
Given we work on a new corpus, it is hard to
compare our results to previous work, additionally
our models currently do not deal with textual an-
tecedents. For example Strube and Mu?ller (2003)
reports their best F-Measure as .4742, while ours
is .747. As concerns accuracy, whereas 384/827
(46%) may appear low, note the task we are per-
forming is harder since we are trying to solve all pro-
nouns/deictics via gestures, not only the ones which
have an antecedent introduced by a pointing or H-O
action (see Table 1). Even if our feature set is lim-
ited, all the classification models perform better than
baseline in all the experiments; the biggest improve-
ment is 14.4% in F-score, and solving 25 more pro-
nouns and deictics. There are no significant differ-
ences in the performances of the 3 different classifi-
cation models. Table 2 shows that the history length
of the best models is less than or equal to 2 seconds,
which is within the standard error range of annota-
tions when we marked the time spans for events.
5 Conclusions
This paper introduced our multi-modal co-reference
annotation scheme that includes pointing gestures
and H-O actions in the corpus ELDERLY-AT-
HOME. Our data shows that 2/3 of antecedents of
pronouns/deictics are introduced by pointing ges-
tures or H-O actions, and not in speech. A co-
reference resolution system has been built to resolve
pronouns and deictics to the antecedents introduced
by pointing gestures and H-O actions. The classi-
fication models show better performance than the
baseline model. In the near future, we will integrate
a module which can resolve pronouns and deictics to
textual antecedents, including type information pro-
vided by indefinite descriptions. This will make the
system fully multi-modal. Additionally we intend
to study issues of timing. Preliminary studies of our
corpus show that the average distance between a pro-
noun/deictic and its antecedent is 8.26? for textual
antecedents, but only 0.66? for gesture antecedents,
consistent with our results that show the best models
include very short histories, at most 2? long.
Acknowledgments
This work is supported by award IIS 0905593 from
the National Science Foundation. Thanks to the
other members of the RoboHelper project, espe-
cially to Anruo Wang, for their many contributions,
especially to the data collection effort. Additionally,
we thank the anonymous reviewers for their valuable
comments.
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Lin Chen, Anruo Wang, and Barbara Di Eugenio. 2011.
Improving pronominal and deictic co-reference resolu-
tion with multi-modal features. In Proceedings of the
SIGDIAL 2011 Conference, pages 307?311, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Marie-Catherine De Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1?8. Association for Computational
Linguistics.
Barbara Di Eugenio, Milos? Z?efran, Jezekiel Ben-
Arie, Mark Foreman, Lin Chen, Simone Franzini,
Shankaranand Jagadeesan, Maria Javaid, and Kai
Ma. 2010. Towards Effective Communication with
Robotic Assistants for the Elderly: Integrating Speech,
Vision and Haptics. In Dialog with Robots, AAAI 2010
Fall Symposium, Arlington, VA, USA, November.
526
Jacob Eisenstein and Randall Davis. 2006. Gesture
Improves Coreference Resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages 37?
40.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll. 2008.
The roles of haptic-ostensive referring expressions in
cooperative, task-based human-robot dialogue. In
Proceedings of the 3rd ACM/IEEE international con-
ference on Human Robot Interaction, HRI ?08, pages
295?302. ACM.
S. Goldin-Meadow. 2003. Hearing gesture: How our
hands help us think. Harvard University Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Andrew Kehler. 2000. Cognitive Status and Form of
Reference in Multimodal Human-Computer Interac-
tion. In AAAI 00, The 15th Annual Conference of the
American Association for Artificial Intelligence, pages
685?689.
Michael Kipp. 2001. Anvil-a generic annotation tool
for multimodal dialogue. In Proceedings of the 7th
European Conference on Speech Communication and
Technology, pages 1367?1370.
Kristine M. Krapp. 2002. The Gale Encyclopedia of
Nursing & Allied Health. Gale Group, Inc. Chapter
Activities of Daily Living Evaluation.
F. Landragin, N. Bellalem, and L. Romary. 2002. Refer-
ring to objects with spoken and haptic modalities. In
Proceedings of the Fourth IEEE International Confer-
ence on Multimodal Interfaces (ICMI?02), pages 99?
104, Pittsburgh, PA.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1.
527
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 114?119,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Lucene and Maximum Entropy Model Based Hedge Detection System
Lin Chen
University of Illinois at Chicago
Chicago, IL, USA
lin@chenlin.net
Barbara Di Eugenio
University of Illinois at Chicago
Chicago, IL, USA
bdieugen@uic.edu
Abstract
This paper describes the approach to
hedge detection we developed, in order to
participate in the shared task at CoNLL-
2010. A supervised learning approach is
employed in our implementation. Hedge
cue annotations in the training data are
used as the seed to build a reliable hedge
cue set. Maximum Entropy (MaxEnt)
model is used as the learning technique to
determine uncertainty. By making use of
Apache Lucene, we are able to do fuzzy
string match to extract hedge cues, and
to incorporate part-of-speech (POS) tags
in hedge cues. Not only can our system
determine the certainty of the sentence,
but is also able to find all the contained
hedges. Our system was ranked third on
the Wikipedia dataset. In later experi-
ments with different parameters, we fur-
ther improved our results, with a 0.612
F-score on the Wikipedia dataset, and a
0.802 F-score on the biological dataset.
1 Introduction
A hedge is a mitigating device used to lessen the
impact of an utterance1. As a very important way
to precisely express the degree of accuracy and
truth assessment in human communication, hedg-
ing is widely used in both spoken and written lan-
guages. Detecting hedges in natural language text
can be very useful for areas like text mining and
information extraction. For example, in opinion
mining, hedges can be used to assess the degree
of sentiment, and refine sentiment classes from
{positive, negative, objective} to {positive, some-
how positive, objective, somehow objective, nega-
tive, somehow negative}.
1http://en.wikipedia.org/wiki/
Hedge(linguistics)
Hedge detection related work has been con-
ducted by several people. Light et al (2004)
started to do annotations on biomedicine article
abstracts, and conducted the preliminary work of
automatic classification for uncertainty. Medlock
and Briscoe (2007) devised detailed guidelines
for hedge annotations, and used a probabilistic
weakly supervised learning approach to classify
hedges. Ganter and Strube (2009) took Wikipedia
articles as training corpus, used weasel words? fre-
quency and syntactic patterns as features to clas-
sify uncertainty.
The rest of the paper is organized as follows.
Section 2 shows the architecture of our system.
Section 3 explains how we make use of Apache
Lucene to do fuzzy string match and incorporate
POS tag in hedge cues and our method to gener-
ate hedge cue candidates. Section 4 describes the
details of using MaxEnt model to classify uncer-
tainty. We present and discuss experiments and
results in section 5, and conclude in section 6.
2 System Architecture
Our system is divided into training and testing
modules. The architecture of our system is shown
in Figure 1.
In the training module, we use the training cor-
pus to learn a reliable hedge cue set with bal-
anced support and confidence, then train a Max-
Ent model for each hedge cue to classify the un-
certainty for sentences matched by that hedge cue.
In the testing module, the learned hedge cues
are used to match the sentences to classify, then
each matched sentence is classified using the cor-
responding MaxEnt model. A sentence will be
classified as uncertain if the MaxEnt model deter-
mines it is. Because of this design, our system is
not only able to check if a sentence is uncertain,
but also can detect the contained hedges.
114
Testing Data
Lucene Indexer
Index with POS
Hedge Cues
More Cue?
Get A Hedge Cue
Get Matched Sentences
More Sentence?
Marked
Uncertain?
Get the MaxEnt Model
Uncertain?
Mark Sentece Uncertainty
yes
no
no
yes
Get A Sentence
yes
yes
Output
Marked
Uncertainty 
no
Lucene Indexer
Training Data
Index with POS
Hedge Cues
Annotation Extender
Token Pruner
POS Tag Replacer
Confidence?
Support?
MaxEnt Models
MaxEnt Trainer
Hedge Cue Candidate Generator
Hedge Cue 
Candidates
yes
Trainging Testing
Figure 1: System Architecture
3 Learn Hedge Cues
The training data provided by CoNLL-2010
shared task contain ?<ccue></ccue>? annota-
tions for uncertain sentences. Most of the annota-
tions are either too strict, which makes them hard
to use to match other sentences, or too general,
which means that most of the matched sentences
are not uncertain.
Similar to how Liu (2007) measures the useful-
ness of association rules, we use support and con-
fidence to measure the usefulness of a hedge cue.
Support is the ratio of sentences containing a
hedge cue to all sentences. Because in a train-
ing dataset, the number of all the sentences is a
fixed constant, we only use the number of sen-
tences containing the hedge cue as support, see
formula 1. In the other part of this paper, sentences
matched by hedge cues means sentences contains
hedge cues. We use support to measure the degree
of generality of a hedge cue.
sup = count of matched sentences (1)
Confidence is the ratio of sentences which con-
tain a hedge cue and are uncertain to all the sen-
tences containing the hedge cue, as formula 2.
We use confidence to measure the reliability for
a word or phrase to be a hedge cue.
conf = count of matched and uncertaincount of matched sentences (2)
3.1 Usage of Apache Lucene
Apache Lucene2 is a full text indexing Java library
provided as an open source project of Apache
Foundation. It provides flexible indexing and
search capability for text documents, and it has
very high performance. To explain the integra-
tion of Lucene into our implementation, we need
to introduce several terms, some of which come
from McCandless et al (2010).
? Analyzer: Raw texts are preprocessed before
being added to the index: text preprocessing
components such as tokenization, stop words
removal, and stemming are parts of an ana-
lyzer.
? Document: A document represents a collec-
tion of fields, it could be a web page, a text
file, or only a paragraph of an article.
? Field: A field represents a document or the
meta-data associated with that document, like
the author, type, URL. A field has a name and
a value, and a bunch of options to control how
Lucene will index its value.
? Term: The very basic unit of a search. It con-
tains a field name and a value to search.
2http://lucene.apache.org
115
? Query: The root class used to do search upon
an index.
In our implementation, Lucene is used for the
following 3 purposes:
? Enable quick counting for combinations of
words and POS tags.
? Store the training and testing corpus for fast
counting and retrieval.
? Allow gap between words or POS tags in
hedge cues to match sentences.
Lucene provides the capability to build cus-
tomized analyzers for complex linguistics analy-
sis. Our customized Lucene analyzer employs to-
kenizer and POS tagger from OpenNLP tools3 to
do tokenization and POS tagging. For every word
in the sentence, we put two Lucene tokens in the
same position, by setting up the second token?s Po-
sitionIncremental attribute to be 0.
For example, for sentence it is believed to be
very good, our analyzer will make Lucene store it
as Figure 2 in its index.
It to beis believed very good
PRP TO VBVBZ VBN RB VBN
60 1 2 3 4 5
Figure 2: Customized Tokenizer Example
Indexing text in that way, we are able to match
sentences cross words and POS tags. For example,
the phrase it is believed will be matched by it is be-
lieved, it is VBN, it VBZ believed. This technique
enables us to generalize a hedge cue.
In our implementation, all the data for training
and testing are indexed. The indexing schema is: a
sentence is treated as a Lucene document; the con-
tent of the sentence is analyzed by our customized
analyzer; other information like sentence id, sen-
tence position, uncertainty is stored as fields of the
document. In this way, we can query all those
fields, and when we find a match, we can easily
get al the information out just from the index.
Lucene provides various types of queries to
search the indexed content. We use SpanNear-
Query and BooleanQuery to search the matched
sentences for hedge cues. We rely on SpanNear-
Query?s feature of allowing positional restriction
3http://opennlp.sourceforge.net
when matching sentences. When building a Span-
NearQuery, we can specify the position gap al-
lowed among the terms in the query. We build a
SpanNearQuery from a hedge cue, put each token
as a term of the query, and set the position gap to
be 2. Take Figure 3 as an example, because the
gap between token is and said is 1, is less than the
specified gap setting 2, so It is widely said to be
good will count as a match with hedge cue is said.
Figure 3: SpanNearQuery Matching Example
We use BooleanQuery with nested SpanNear-
Query and TermQuery to count uncertain sen-
tences, then to calculate the confidence of a hedge
cue.
3.2 Hedge Cue Candidate Generation
We firstly tried to use the token as the basic unit for
hedge cues. However, several pieces of evidence
suggest it is not appropriate.
? Low Coverage. We only get 42 tokens in
Wikipedia training data, using 20, 0.4 as the
thresholds for support and confidence.
? Irrelevant words or stop words with lower
thresholds. When we use 5, 0.3 as the thresh-
olds for coverage and confidence, we get 279
tokens, however, words like is, his, musical,
voters, makers appear in the list.
We noticed that many phrases with similar
structures or fixed collocations appear very often
in the annotations, like it is believed, it is thought,
many of them, many of these and etc. Based on this
observation, we calculated the support and confi-
dence for some examples, see table 1.
Hedge Cue Sup. Conf.
it is believed 14 .93
by some 30 .87
many of 135 .65
Table 1: Hedge Cue Examples
We decided to use the phrase or collocation as
the basic unit for hedge cues. There are two prob-
lems in using the original annotations as hedge
cues:
116
? High confidence but low coverage: annota-
tions that contain proper nouns always have
very high confidence, usually 100%, how-
ever, they have very low support.
? High coverage but low confidence: annota-
tions with only one token are very frequent,
but only a few of them result in enough con-
fidence.
To balance confidence and support, we built our
hedge cue candidate generator. Its architecture is
presented in Figure 4.
Cue Annotations
Cue Candidates
Annotation Extender
Tokens > 1
NO
Token Pruner
POS Tag Replacer
YES
Figure 4: Hedge Cue Candidate Generator
The three main components of the hedge cue
candidate generator are described below.
Annotation Extender: When the input hedge
cue annotation contains only 1 token, this compo-
nent will be used. It will generate 3 more hedge
cue candidates by adding the surrounding tokens.
We expect to discover candidates with higher con-
fidence.
Token Pruner: According to our observations,
proper nouns rarely contribute to the uncertainty
of a sentence, and our Lucene based string match-
ing method ensures that the matched sentences re-
main matched after we remove tokens from the
original cue annotation. So we remove proper
nouns in the original cue annotation to generate
hedge cue candidates. By using this component,
we expect to extract hedge cues with higher sup-
port.
POS Tag Replacer: This component is used to
generalize similar phrases, by using POS tags to
replace the concrete words. For example, we use
the POS tag VBN to replace believed in it is be-
lieved to generate it is VBN. Hence, when a sen-
tence contains it is thought in the testing dataset,
even if it is thought never appeared in the train-
ing data set, we will still be able to match it and
classify it against the trained MaxEnt model. We
expect that this component will be able to increase
support. Due to the O(2n) time complexity, we did
not try the brute force approach to replace every
word, only the words with the POS tags in Table 2
are replaced in the process.
POS Description Example
VBN past participle verb it is believed
NNS plural common noun some countries
DT determiner some of those
CD numeral, cardinal one of the best
Table 2: POS Tag Replacer Examples
After hedge cue candidates are generated, we
convert them to Lucene queries to calculate their
confidence and support. We prune those that fall
below the predefined confidence and support set-
tings.
4 Learn Uncertainty
Not all the learned hedge cues have 100% uncer-
tainty confidence, given a hedge cue, we need to
learn how to classify whether a matched sentence
is uncertain or not. The classification model is,
given a tuple of (Sentence, Hedge Cue), in which
the sentence contains the hedge cue, we classify it
to the outcome set {Certain, Uncertain}.
MaxEnt is a general purpose machine learn-
ing technique, it makes no assumptions in addi-
tion to what we know from the data. MaxEnt has
been widely used in Natural Language Processing
(NLP) tasks like POS tagging, word sense disam-
biguation, and proved its efficiency. Due to Max-
Ent?s capability to combine multiple and depen-
dent knowledge sources, we employed MaxEnt as
our machine learning model. Features we used to
train the model include meta information features
and collocation features.
Meta Information Features include three fea-
tures:
? Sentence Location: The location of the sen-
tence in the article, whether in the title or in
the content. We observed sentences in the ti-
tle are rarely uncertain.
? Number of Tokens: The number of tokens
in the sentence. Title of article is usually
shorter, and more likely to be certain.
117
? Hedge Cue Location: The location of
matched tokens in a sentence. We consider
them to be in the beginning, if the first token
of the matched part is the first token in the
sentence; to be at the end, if the last token of
the matched part is the last token of the sen-
tence; otherwise, they are in the middle. We
were trying to use this feature as a simplified
version to model the syntactic role of hedge
cues in sentences.
Collocation Features include the word and POS
tag collocation features:
? Word Collocation: Using a window size of 5,
extract all the word within that window, ex-
cluding punctuation.
? POS Tag Collocation: Using a window size
of 5, extract all the POS tags of tokens within
that window, excluding punctuation.
We use the OpenNLP MaxEnt4 Java library as
the MaxEnt trainer and classifier. For each hedge
cue, the training is iterated 100 times, with no cut
off threshold for events.
5 Experiments and Discussion
We first ran experiments to evaluate the perfor-
mance of the entire system. We used official
dataset as training and testing, with different con-
fidence and support thresholds. The result on offi-
cial Wikipedia dataset is presented in Table 3. Re-
sult on the biological dataset is listed in Table 4.
In the result tables, the first 2 columns are the con-
fidence and support threshold; ?Cues? is the num-
ber of generated hedge cues; the last 3 columns are
standard classifier evaluation measures.
Our submitted result used 0.35, 5 as the thresh-
olds for confidence and support. We officially
placed third on the Wikipedia dataset, with a
0.5741 F-score, and third from last on the biolog-
ical dataset, with a 0.7692 F-score. In later ex-
periments, we used different parameters, which re-
sulted in a 0.03 F-score improvement. We believe
the big difference of ranking on different datasets
comes from the incomplete training. Due to incor-
rect estimation of running time, we only used the
smaller training file in our submitted biological re-
sult.
From Table 3 and 4, we can see that a higher
confidence threshold gives higher precision, and
4http://maxent.sourceforge.net
Conf. Sup. Cues Prec. Recall F
0.4
10 360 0.658 0.561 0.606
15 254 0.672 0.534 0.595
20 186 0.682 0.508 0.582
0.45
10 293 0.7 0.534 0.606
15 190 0.717 0.503 0.591
20 137 0.732 0.476 0.577
0.5
5 480 0.712 0.536 0.612
10 222 0.736 0.492 0.590
15 149 0.746 0.468 0.575
20 112 0.758 0.443 0.559
Table 3: Evaluation Result on Wikipedia Dataset
Conf. Sup. Cues Prec. Recall F
0.4
10 330 0.68 0.884 0.769
15 229 0.681 0.861 0.76
20 187 0.679 0.842 0.752
0.45
10 317 0.689 0.878 0.772
15 220 0.69 0.857 0.764
20 179 0.688 0.838 0.756
0.5
5 586 0.724 0.899 0.802
10 297 0.742 0.841 0.788
15 206 0.742 0.819 0.779
20 169 0.74 0.8 0.769
Table 4: Evaluation Result on Biological Dataset
a lower support threshold leads to higher recall.
Since the lower support threshold could generate
more hedge cues, it will generate less training in-
stances for hedge cues with both low confidence
and support, which affects the performance of the
MaxEnt classifier. In both datasets, it appears that
0.5 and 5 are the best thresholds for confidence
and support, respectively.
Beyond the performance of the entire system,
our hedge cue generator yields very promising re-
sults. Using the best parameters we just noted
above, our hedge cue generator generates 52 hedge
cues with confidence 100% on the Wikipedia
dataset, and 332 hedge cues in the biological
dataset. Some hedge cue examples are shown in
Table 5.
We also ran experiments to verify the perfor-
mance of our MaxEnt classifier. We used the same
setting of datasets as for the system performance
evaluation. Given a hedge cue, we extracted all
the matched sentences from the training set to train
a MaxEnt classifier, and used it to classify the
matched sentences by the hedge cue in testing set.
118
Hedge Cue Sup. Conf. TestSize Prec. Recall F
indicated that 63 0.984 6 1.0 1.0 1.0
by some 30 0.867 29 0.966 1.000 0.983
are considered 29 0.724 10 0.750 0.857 0.800
some of NNS 62 0.613 27 1.000 0.778 0.875
the most JJ 213 0.432 129 0.873 0.475 0.615
Table 6: MaxEnt Classifier Performance
Hedge Cue Conf. Sup.
probably VBN 1.0 21
DT probably 1.0 15
many NNS believe 1.0 10
NNS suggested DT 1.0 248
results suggest 1.0 122
has VBN widely VBN 1.0 10
Table 5: Generated Hedge Cue Examples
Table 6 shows the results, the hedge cues were
manually chosen with relative higher support.
We can see that the performance of the MaxEnt
classifier correlates tightly with confidence and
support. Higher confidence means a more accu-
rate detection for a phrase to be hedge cue, while
higher support means more training instances for
the classifier: the best strategy would be to find
hedge cues with both high confidence and support.
While experimenting with the system, we found
several potential improvements.
? Normalize words. Take the word suggest as
an example. In the generated hedge cues,
we found that its other forms are everywhere,
like it suggested, NNS suggests a, and DT
suggesting that. As we put POS tags into
Lucene index, we can normalize words to
their base forms using a morphology parser,
and put base forms into index. After that, the
query with suggest will match all the forms.
? Use more sophisticated features to train the
MaxEnt classifier. Currently we only use
shallow linguistics information as features,
however we noticed that the role of the phrase
could be very important to decide whether it
indicates uncertainty. We can deep parse sen-
tences, extract the role information, and add
it to the feature list of classifier.
6 Conclusion
In this paper, we described the hedge detection
system we developed to participate in the shared
task of CoNLL-2010. Our system uses a heuristic
learner to learn hedge cues, and uses MaxEnt as
its machine learning model to classify uncertainty
for sentences matched by hedge cues. Hedge cues
in our system include both words and POS tags,
which make themmore general. Apache Lucene is
integrated into our system to efficiently run com-
plex linguistic queries on the corpus.
Acknowledgments
This work is supported by award IIS-0905593
from the National Science Foundation.
References
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
Wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore,
August. Association for Computational Linguistics.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings of
BioLink 2004 Workshop on Linking Biological Lit-
erature, Ontologies and Databases: Tools for Users,
pages 17?24, Boston, Mass, May.
Bing Liu. 2007. Web data mining: exploring hyper-
links, contents, and usage data. Springer.
Michael McCandless, Erik Hatcher, and Otis Gospod-
neti. 2010. Lucene in action. Manning Publications
Co, 2nd edition.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999. Association for Computational Lin-
guistics, June.
119
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 65?75,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exploring Effective Dialogue Act Sequences
in One-on-one Computer Science Tutoring Dialogues
Lin Chen, Barbara Di Eugenio
Computer Science
U. of Illinois at Chicago
lchen43,bdieugen@uic.edu
Davide Fossati
Computer Science
Carnegie Mellon U. in Qatar
davide@fossati.us
Stellan Ohlsson, David Cosejo
Psychology
U. of Illinois at Chicago
stellan,dcosej1@uic.edu
Abstract
We present an empirical study of one-on-
one human tutoring dialogues in the domain
of Computer Science data structures. We
are interested in discovering effective tutor-
ing strategies, that we frame as discovering
which Dialogue Act (DA) sequences corre-
late with learning. We employ multiple lin-
ear regression, to discover the strongest mod-
els that explain why students learn during
one-on-one tutoring. Importantly, we define
?flexible? DA sequence, in which extraneous
DAs can easily be discounted. Our experi-
ments reveal several cognitively plausible DA
sequences which significantly correlate with
learning outcomes.
1 Introduction
One-on-one tutoring has been shown to be a very ef-
fective form of instruction compared to other educa-
tional settings. Much research on discovering why
this is the case has focused on the analysis of the
interaction between tutor and students (Fox, 1993;
Graesser et al, 1995; Lepper et al, 1997; Chi et al,
2001). In the last fifteen years, many such analyses
have been approached from a Natural Language Pro-
cessing (NLP) perspective, with the goal of build-
ing interfaces that allow students to naturally inter-
act with Intelligent Tutoring Systems (ITSs) (Moore
et al, 2004; Cade et al, 2008; Chi et al, 2010).
There have been two main types of approaches to
the analysis of tutoring dialogues. The first kind
of approach compares groups of subjects interact-
ing with different tutors (Graesser et al, 2004; Van-
Lehn et al, 2007), in some instances contrasting the
number of occurrences of relevant features between
the groups (Evens and Michael, 2006; Chi et al,
2010). However, as we already argued in (Ohlsson
et al, 2007), this code-and-count methodology only
focuses on what a certain type of tutor (assumed to
be better according to certain criteria) does differ-
ently from another tutor, rather than on strategies
that may be effective independently from their fre-
quencies of usage by different types of tutor. Indeed
we had followed this same methodology in previous
work (Di Eugenio et al, 2006), but a key turning
point for our work was to discover that our expert
and novice tutors were equally effective (please see
below).
The other kind of approach uses linear regression
analysis to find correlations between dialogue fea-
tures and learning gains (Litman and Forbes-Riley,
2006; Di Eugenio et al, 2009). Whereas linear
regression is broadly used to analyze experimental
data, only few analyses of tutorial data or tutoring
experiments use it. In this paper, we follow
Litman and Forbes-Riley (2006) in correlating se-
quences of Dialogue Acts (DAs) with learning gains.
We extend that work in that our bigram and trigram
DAs are not limited to tutor-student DA bigrams ?
Litman and Forbes-Riley (2006) only considers bi-
grams where one DA comes from the tutor?s turn
and one from the student?s turn, in either order. Im-
portantly, we further relax constraints on how these
sequences are built, in particular, we are able to
model DA sequences that include gaps. This allows
us to discount the noise resulting from intervening
DAs that do not contribute to the effectiveness of
the specific sequence. For example, if we want to
65
explore sequences in which the tutor first provides
some knowledge to solve the problem (DPI) and
then knowledge about the problem (DDI) (DPI and
DDI will be explained later), an exchange such as
the one in Figure 1 should be taken into account
(JAC and later LOW are the tutors, students are indi-
cated with a numeric code, such as 113 in Figure 1).
However, if we just use adjacent utterances, the ok
from the student (113) interrupts the sequence, and
we could not take this example into account. By al-
lowing gaps in our sequences, we test a large number
of linear regression models, some of which result in
significant models that can be used as guidelines to
design an ITS. Specifically, these guidelines will be
used for further improvement of iList, an ITS that
provides feedback on linked list problems and that
we have developed over the last few years. Five
different versions of iList have been evaluated with
220 users (Fossati et al, 2009; Fossati et al, 2010).
iList is available at http://www.digitaltutor.net, and
has been used by more than 550 additional users at
15 different institutions.
JAC: so we would set k equal to e and then delete. [DPI]
113: ok.
JAC: so we?ve inserted this whole list in here.[DDI]
113: yeah.
Figure 1: {DPI, DDI} Sequence Excerpt
The rest of the paper is organized as follows.
In Section 2, we describe the CS-Tutoring corpus,
including data collection, transcription, and anno-
tation. In Section 3, we introduce our methodol-
ogy that combines multiple linear regression with n-
grams of DAs that allow for gaps. We discuss our
experiments and results in Section 4.
2 The CS Tutoring Corpus
2.1 Data Collection
During the time span of 3 semesters, we collected a
corpus of 54 one-on-one tutoring sessions on Com-
puter Science data structures: linked list, stack and
binary search tree. (In the following context, we
will refer them as Lists, Stacks and Trees). Each stu-
dent only participated in one session, and was ran-
domly assigned to one of two tutors: LOW, an expe-
rienced Computer Science professor, with more than
30 years of teaching experience; or JAC, a senior un-
dergraduate student in Computer Science, with only
one semester of previous tutoring experience. In the
end 30 students interacted with LOW and 24 with
JAC.
Students took a pre-test right before the tutoring
session, and an identical post-test immediately after.
The test had two problems on Lists, two problems on
Stacks, and four problems on Trees. Each problem
was graded out of 5 points, for a possible maximum
score of 10 points each for Lists and Stacks, and 20
points for Trees. Pre and post-test scores for each
topic were later normalized to the [0..1] interval, and
learning gains were computed.
Table 1 includes information on session length.
Note that for each topic, the number of sessions is
lower than 54. The tutor was free to tutor on what
he felt was more appropriate, after he was given an
informal assessment of the student?s performance on
the pre-test (tutors were not shown pre-tests to avoid
that they?d tutor to the pre-test only). Hence, not
every student was tutored on every topic.
Topic N Session length (minutes)Min Max Total ? ?
Lists 52 3.4 41.4 750.4 14.4 5.8
Stacks 46 0.3 9.4 264.5 5.8 1.8
Trees 53 9.1 40.0 1017.6 19.2 6.6
Sessions 54 12.8 61.1 2032.5 37.6 6.1
Table 1: CS Tutoring Corpus - Descriptives
Each tutoring session was videotaped. The cam-
era was pointing at the sheets of paper on which tu-
tors and students were writing during the session.
The videos were all transcribed. The transcripts
were produced according to the rules and conven-
tions described in the transcription manual of the
CHILDES project (MacWhinney, 2000). Dialogue
excerpts included in this paper show some of the
transcription conventions. For example, ?+...?
denotes trailing, ?xxx? unintelligible speech and
?#? a short pause (see Figure 2). The CHILDES
transcription manual also provides directions on ut-
terance segmentation.
An additional group of 53 students (control
group) took the pre- and post-tests, but instead of
participating in a tutoring session they attended a
40 minute lecture about an unrelated CS topic. The
rationale for such a control condition was to assess
66
LOW: what?s the if? [Prompt]
LOW: well of course, don?t do this if t two is null so if t
two isn?t null we can do that and xxx properly # thinking
I put it in here. [DPI]
LOW: or else if t two is null that?s telling us that this is
the +. . . [Prompt,FB]
Figure 2: {Prompt,DPI,FB} sequence excerpt
whether by simply taking the pre-test students would
learn about data-structures, and hence, to tease out
whether any learning we would see in the tutored
conditions would be indeed due to tutoring.
The learning gain, expressed as the difference
between post-score and pre-score, of students that
received tutoring was significantly higher than the
learning gain of the students in the control group, for
all the topics. This was showed by ANOVA between
the aggregated group of tutored students and the
control group, and was significant at the p < 0.01
for each topic. There was no significant difference
between the two tutored conditions in terms of learn-
ing gain. The fact that students did not learn more
with the experienced tutor was an important finding
that led us to question the approach of comparing
and contrasting more and less experienced tutors.
Please refer to (Di Eugenio et al, 2009) for further
descriptive measurements of the corpus.
2.2 Dialogue Act Annotation
Many theories have been proposed as concerns DAs,
and there are many plausible inventories of DAs, in-
cluding for tutorial dialogue (Evens and Michael,
2006; Litman and Forbes-Riley, 2006; Boyer et al,
2010). We start from a minimalist point of view,
postulating that, according to current theories of
skill acquisition (Anderson, 1986; Sun et al, 2005;
Ohlsson, 2008), at least the following types of tuto-
rial intervention can be explained in terms of why
and how they might support learning:
1. A tutor can tell the student how to perform the
task.
2. A tutor can state declarative information about
the domain.
3. A tutor can provide feedback:
(a) positive, to confirm that a correct but tentative
step is in fact correct;
(b) negative, to help a student detect and correct an
error.
We first read through the entire corpus and exam-
ined it for impressions and trends, as suggested by
(Chi, 1997). Our informal assessment convinced us
that our minimalist set of tutoring moves was an ap-
propriate starting point. For example, contrary to
much that has been written about an idealized so-
cratic type of tutoring where students build knowl-
edge by themselves (Chi et al, 1994), our tutors
are rather directive in style, namely, they do a lot
of telling and stating. Indeed our tutors talk a lot,
to the tune of producing 93.5% of the total words!
We translated the four types above into the follow-
ing DAs: Direct Procedural Instruction (DPI), Di-
rect Declarative Instruction (DDI), Positive Feed-
back (+FB), and Negative Feedback (-FB). Besides
those 4 categories, we additionally annotated the
corpus for Prompt (PT), since our tutors did explic-
itly invite students to be active in the interaction.
We also annotated for Student Initiative (SI), to cap-
ture active participation on the part of the student?s.
SI occurs when the student proactively produces a
meaningful utterance, by providing unsolicited ex-
planation (see Figures 6 and 4), or by asking ques-
tions. As we had expected, SIs are not as frequent as
other moves (see below). However, this is precisely
the kind of move that a regression analysis would
tease out from others, if it correlates with learning,
even if it occurs relatively infrequently. This indeed
happens in two models, see Table 8.
Direct Procedural Instruction(DPI) occurs when
the tutor directly tells the student what task to per-
form. More specifically:
? Utterances containing correct steps that lead to
the solution of a problem, e.g. see Figure 1.
? Utterances containing high-level steps or sub-
goals (it wants us to put the new node that con-
tains G in it, after the node that contains B).
? Utterances containing tactics and strategies (so
with these kinds of problems, the first thing I
have to say is always draw pictures).
? Utterances where the tutor talked in the first-
person but in reality the tutor instructed the stu-
dent on what to do (So I?m pushing this value
onto a stack. So I?m pushing G back on).
Direct Declarative Instruction (DDI) occurred
when the tutor provided facts about the domain or
67
a specific problem. The key to determine if an ut-
terance is DDI is that the tutor is telling the student
something that he or she ostensibly does not already
know. Common sense knowledge is not DDI ( ten
is less than eleven ). Utterances annotated as DDI
include:
? Providing general knowledge about data struc-
tures (the standard format is right child is al-
ways greater than the parent, left child is al-
ways less than the parent).
? Telling the student information about a specific
problem (this is not a binary search tree).
? Conveying the results of a given action (so now
since we?ve eliminated nine, it?s gone).
? Describing pictures of data structures (and then
there is a link to the next node).
Prompts (PT) occur when the tutor attempts to
elicit a meaningful contribution from the student.
We code for six types of tutor prompts, including:
? Specific prompt: An attempt to get a specific
response from the student (that?s not b so what
do we want to do?).
? Diagnosing: The tutor attempts to determine
the student?s knowledge state (why did you put
a D there?).
? Confirm-OK: The tutor attempts to determine if
the student understood or if the student is pay-
ing attention (okay, got that idea?).
? Fill-in-the-blank: The tutor does not complete
an utterance thereby inviting the student to
complete the utterance, e.g. see Figure 2.
Up to now we have discussed annotations for ut-
terances that do not explicitly address what the stu-
dent has said or done. However, many tutoring
moves concern providing feedback to the student.
Indeed as already known but not often acted upon in
ITS interfaces, tutors do not just point out mistakes,
but also confirm that the student is making correct
steps. While the DAs discussed so far label single
utterances, our positive and negative feedback (+FB
and -FB) annotations comprise a sequence of con-
secutive utterances, that starts where the tutor starts
providing feedback. We opted for a sequence of ut-
terances rather than for labeling one single utterance
because we found it very difficult to pick one single
utterance as the one providing feedback, when the
tutor may include e.g. an explanation that we con-
sider to be part of feedback. Positive feedback oc-
curs when the student says or does something cor-
rect, either spontaneously or after being prompted
by the tutor. The tutor acknowledges the correctness
of the student?s utterance, and possibly elaborates on
it with further explanation. Negative feedback oc-
curs when the student says or does something incor-
rect, either spontaneously or after being prompted
by the tutor. The tutor reacts to the mistake and pos-
sibly provides some form of explanation.
After developing a first version of the coding
manual, we refined it iteratively. During each itera-
tion, two human annotators independently annotated
several dialogues for one DA at a time, compared
outcomes, discussed disagreements, and fine-tuned
the scheme accordingly. This process was repeated
until a sufficiently high inter-coder agreement was
reached. The Kappa values we obtained in the fi-
nal iteration of this process are listed in Table 2
(Di Eugenio and Glass, 2004; Artstein and Poesio,
2008). In Table 2, the ?Double Coded*? column
refers to the sessions that we double coded to cal-
culate the inter-coder agreement. This number does
not include the sessions which were double coded
when coders were developing the coding manual.
The numbers of double-coded sessions differ by DA
since it depends on the frequency on the particular
DA (recall that we coded for one DA at a time).
For example, since Student Initiatives (SI) are not as
frequent, we needed to double code more sessions
to find a number of SI?s high enough to compute a
meaningful Kappa (in our whole corpus, there are
1157 SIs but e.g. 4957 Prompts).
Category Double Coded* Kappa
DPI 10 .7133
Feedback 5 .6747
DDI 10 .8018
SI 14 .8686
Prompt 8 .9490
Table 2: Inter-Coder Agreement in Corpus
The remainder of the corpus was then indepen-
dently annotated by the two annotators. For our
final corpus, for the double coded sessions we did
not come to a consensus label when disagreements
arose; rather, we set up a priority order based on
68
topic and coder (e.g., during development of the
coding scheme, when coders came to consensus
coding, which coder?s interpretation was chosen
more often), and we chose the annotation by a cer-
tain coder based on that order.
As a final important note, given our coding
scheme some utterances have more than one label
(see Figures 2 and 4), whereas others are not la-
belled at all. Specifically, most student utterances,
and some tutor utterances, are not labelled (see Fig-
ures 1 and 4).
3 Method
3.1 Linear Regression Models
In this work, we adopt a multiple regression model,
because it can tell us how much variation in learning
outcomes is explained by the variation of individual
features in the data. The features we use include pre-
test score, the length of the tutoring sessions, and
DAs, both the single DAs we annotated for and DA
n-grams, i.e. DA sequences of length n. Pre-test
score is always included since the effect of previ-
ous knowledge on learning is well established, and
confirmed in our data (see all Models 1 in Table 4);
indeed multiple linear regression allows us to factor
out the effect of previous knowledge on learning, by
quantifying the predictive power of features that are
added beyond pre-test score.
3.2 n-gram Dialogue Act Model
n-grams (sequences of n units, such as words, POS
tags, dialogue acts) have been used to derive lan-
guage models in computational linguistics for a long
time, and have proven effective in tasks like part-of-
speech tagging, spell checking.
Our innovation with regard to using DA n-grams
is to allow gaps in the sequence. This allows us
to extract the sequences that are really effective,
and to eliminate noise. Note that from the point
of view of an effective sequence, noise is anything
that does not contribute to the sequence. For ex-
ample, a tutor?s turn may be interrupted by a stu-
dent?s acknowledgments, such as ?OK? or ?Uh-hah?
(see Figure 1). Whereas these acknowledgments
perform fundamental functions in conversation such
as grounding (Clark, 1992), they may not directly
correlate with learning (a hypothesis to test). If we
counted them in the sequence, they would contribute
two utterances, transforming a 3 DA sequence into a
5 DA sequence. As well known, the higher the n, the
sparser the data becomes, i.e., the fewer sequences
of length n we find, making the task of discover-
ing significant correlations all the harder. Note that
some of the bigrams in (Litman and Forbes-Riley,
2006) could be considered to have gaps, since they
pair one student move (say SI) with each tutor move
contained in the next tutor turn (eg, in our Figure 6
they would derive two bigrams [SI, FB], and [SI,
Prompt]). However, this does not result in a system-
atic exploration of all possible sequences of a certain
length n, with all possible gaps of length up to m, as
we do here.
The tool that allows us to leave gaps in sequences
is part of Apache Lucene,1 an open source full text
search library. It provides strong capabilities to
match and count efficiently. Our counting method
is based on two important features provided by
Lucene, that we already used in other work (Chen
and Di Eugenio, 2010) to detect uncertainty in dif-
ferent types of corpora.
? Synonym matching: We can specify several
different tokens at the same position in a field
of a document, so that each of them can be used
to match the query.
? Precise gaps: With Lucene, we can precisely
specify the gap between the matched query and
the indexed documents (sequences of DAs in
our case) using a special type of query called
SpanNearQuery.
To take advantage of Lucene as described above,
we use the following algorithm to index our corpus.
1. For each Tutor-Topic session, we generate n-
gram utterance sequences ? note that these are
sequences of utterances at this point, not of
DAs.
2. We prune utterance sequences where either 0
or only 1 utterance is annotated with a DA, be-
cause we are mining sequences with at least 2
DAs. Recall that given our annotation, some ut-
terances are not annotated (see e.g. Figure 1).
3. After pruning, for each utterance sequence, we
generate a Lucene document: each DA label on
an utterance will be treated as a token, multiple
1http://lucene.apache.org/
69
labels on the same utterance will be treated as
?synonyms?.
By indexing annotations as just described, we
avoid the problem of generating too many combina-
tions of labels. After indexing, we can use SpanN-
earQuery to query the index. SpanNearQuery allows
us to specify the position distance allowed between
each term in the query.
Figure 3 is the field of the generated Lucene doc-
ument corresponding to the utterance sequences in
Figure 4. We can see that each utterance of the tu-
tor is tagged with 2 DAs. Those 2 DAs produce 2
tokens, which are put into the same position. The
tokens in the same position act as synonyms to each
other during the query.
Figure 3: Lucene Document Example for DAs
258: okay.
JAC: its right child is eight. [DDI, FB]
258: uh no it has to be greater than ten. [SI]
JAC: right so it?s not a binary search tree # it?s not a b s t,
right? [DDI,Prompt]
Figure 4: {FB, SI, DDI} is most effective in Trees
4 Experiments and Results
Here we build on our previous results reported
in (Di Eugenio et al, 2009). There we had shown
that, for lists and stacks, models that include positive
and negative feedback are significant and explain
more of the variance with respect to models that only
include pre-test score, or include pre-test score and
session length. Table 4 still follows the same ap-
proach, but adds to the regression models the addi-
tional DAs, DPI, DDI, Prompt and SI that had not
been included in that earlier work. The column M
refers to three types of models, Model 1 only in-
cludes Pre-test, Model 2 adds session length to Pre-
test, and Model 3 adds to Pre-test all the DAs. As ev-
idenced by the table, only DPI provides a marginally
significant contribution, and only for lists. Note that
length is not included in Model 3?s. We did run all
the equivalent models to Model 3?s including length.
The R2?s stay the same (literally, to the second dec-
imal digit), or minimally decrease. However, in all
these Model 3+?s that include length no DA is sig-
nificant, hence we consider them as less explana-
tory than the Model 3?s in Table 4: finding that a
longer dialogue positively affects learning does not
tell us what happens during that dialogue which is
conducive to learning.
Note that the ? weights on the pre-test are al-
ways negative in every model, namely, students with
higher pre-test scores learn less than students with
lower pre-test scores. This is an example of the well-
known ceiling effect: students with more previous
knowledge have less learning opportunity. Also no-
ticeable is that the R2 for the Trees models are much
higher than for Lists and Stacks, and that for Trees
no DA is significant (although there will be signifi-
cant trigram models that involve DAs for Trees). We
have observed that Lists are in general more diffi-
cult than Stacks and Trees (well, at least than binary
search trees) for students.
Topic Pre-Test ? Gain ?
Lists .40 .27 .14 .25
Stacks .29 .30 .31 .24
Trees .50 .26 .30 .24
Table 3: Learning gains and t-test statistics
Indeed Table 3 shows that in the CS-tutoring cor-
pus the average learning gain is only .14 for Lists,
but .31 for Stacks and .30 for Trees; whereas stu-
dents have the lowest pre-test score on Stacks, and
hence they have more opportunities for learning,
they learn as much for Trees, but not for Lists.
We now examine whether DA sequences help us
explain why student learn. We have run 24 sets of
linear regression experiments, which are grouped as
the following 6 types of models.
? With DA bigrams (DA sequences of length 2):
? Gain ? DA Bigram
? Gain ? DA Bigram + Pre-test Score
? Gain ? DA Bigram + Pre-test Score +
Session Length
? With DA trigrams (DA sequences of length 3):
? Gain ? DA Trigram
? Gain ? DA Trigram + Pre-test Score
? Gain ? DA Trigram + Pre-test Score +
Session Length
For each type of model:
70
Topic M Predictor ? R2 P
Lists
1 Pre-test ?.47 .20 < .001
2
Pre-test ?.43 .29 < .001Length .01 < .001
3
Pre-test ?.500
.377
< .001
+FB .020 < .01
-FB .039 ns
DPI .004 < .1
DDI .001 ns
SI .005 ns
Prompt .001 ns
Stacks
1 Pre-test ?.46 .296 < .001
2 Pre-test ?.46 .280 < .001Length ?.002 ns
3
Pre-test ?.465
.275
< .001
+FB ?.017 < .01
-FB ?.045 ns
DPI .007 ns
DDI .001 ns
SI .008 ns
Prompt ?.006 ns
Trees
1 Pre-test ?.739 .676 < .001
2 Pre-test ?.733 .670 < .001Length .001 ns
3
Pre-test ?.712
.667
< .001
+FB ?.002 ns
-FB ?.018 ns
DPI ?.001 ns
DDI ?.001 ns
SI ?.001 ns
Prompt ?.001 ns
All
1 Pre-test ?.505 .305 < .001
2 Pre-test ?.528 .338 < .001Length .06 < .001
3
Pre-test ?.573
.382
< .001
+FB .009 < .001
-FB ?.024 ns
DPI .001 ns
DDI .001 ns
SI .001 ns
Prompt .001 ns
Table 4: Linear Regression ? Human Tutoring
1. We index the corpus according to the length of
the sequence (2 or 3) using the method we in-
troduced in section 3.2.
2. We generate all the permutations of all the DAs
we annotated for within the specified length;
count the number of occurrences of each per-
mutation using Lucene?s SpanNearQuery al-
lowing for gaps of specified length. Gaps can
span from 0 to 3 utterances; for example, the
excerpt in Figure 1 will be counted as a {DPI,
DDI} bigram with a gap of length 1. Gaps can
be discontinuous.
3. We run linear regressions2 on the six types of
models listed above, generating actual models
by replacing a generic DA bi- or tri-gram with
each possible DA sequence we generated in
step 2.
4. We output those regression results, in which the
whole model and every predictor are at least
marginally significant (p < 0.1).
The number of generated significant models is
shown in Figure 5. In the legend of the Figure,
B stands for Bigram DA sequence, T stands for
Trigram DA sequence, L stands for session Length,
P stands for Pre-test score. Not surprisingly, Fig-
ure 5 shows that, as the allowed gap increases in
length, the number of significant models increases
too, which give us more models to analyze.
0
10
20
30
40
50
60
Gap Allowed
N
um
be
r o
f S
ig
ni
fic
an
t M
od
el
s
0 1 2 3
?
?
?
??
?
?
?
?
?
Predictors
T
T+P
T+P+L
B
B+P
B+P+L
Figure 5: Gaps Allowed vs. Significant Models
Figure 5 shows that there are a high number of
significant models. In what follows we will present
first of all those that improve on the models that
do not use sequences of DAs, as presented in Ta-
ble 4. Improvement here means not only that the
R2 is higher, but that the model is more appropriate
as an approximation of a tutor strategy, and hence,
constitutes a better guideline for an ITS. For exam-
ple, take model 3 for Lists in Table 4. It tells us
that positive feedback (+FB) and direct procedural
instruction (DPI) positively correlate with learning
2We used rJava, http://www.rforge.net/rJava/
71
gains. However, this obviously cannot mean that our
ITS should only produce +FB and DPI. The ITS is
interacting with the student, and it needs to tune its
strategies according to what happens in the interac-
tion; model 3 doesn?t even tell us if +FB and DPI
should be used together or independently. Models
that include sequences of DAs will be more useful
for the design of an ITS, since they point out what
sequences of DAs the ITS may use, even if they still
don?t answer the question, when should the ITS en-
gage in a particular sequence ? we have addressed
related issues in our work on iList (Fossati et al,
2009; Fossati et al, 2010).
4.1 Bigram Models
{DPI, Feedback} Model Indeed the first signifi-
cant models that include a DA bigram include the
{DPI, Feedback} DA sequence. Note that we distin-
guish between models that employ Feedback (FB)
without distinguishing between positive and nega-
tive feedback; and models where the type of feed-
back is taken into account (+FB, -FB). Table 5 shows
that for Lists, a sequence that includes DPI followed
by any type of feedback (Feedback, +FB, -FB) pro-
duces significant models when the model includes
pre-test. Table 5 and all tables that follow include
the column Gap that indicates the length of the gap
within the DA sequence with which that model was
obtained. When, as in Table 5, multiple numbers
appear in the Gap column, this indicates that the
model is significant with all those gap settings. We
only show the ?, R2 and P values for the gap length
which generates the highest R2 for a model, and the
corresponding gap length is in bold font: for exam-
ple, the first model for Lists in Table 5 is obtained
with a gap length = 2. For Lists, these models are not
as predictive as Model 3 in Table 4, however we be-
lieve they are more useful from an ITS design point
of view: they tell us that when the tutor gives direct
instruction on how to solve the problem, within a
short span of dialogue the tutor produces feedback,
since (presumably) the student will have tried to ap-
ply that DPI. For Stacks, a {DPI, -FB} model (with-
out taking pre-test into account) significantly corre-
lates (p < 0.05) with learning gain, and marginally
significantly correlates with learning gain when the
model also includes pre-test score. This latter model
is actually more predictive than Model 3 for Stacks
in Table 4 that includes +FB but not DPI. We can
see the ? weight is negative for the sequence {DPI,
-FB} in the Stacks model. No models including the
bigram {DPI, -FB} are significant for Trees.
Topic Predictor ? R2 P Gap
Lists
DPI, -FB .039 .235 <.001 2, 3Pre-test ?.513 < .001
DPI, +FB .019
.339
<.001
0, 1, 2, 3Pre-test ?.492 < .001
Length .011 < 0.05
DPI, FB .016
.333
<.05
0, 1, 2, 3Pre-test ?.489 < .001
Length .011 < 0.05
Stacks
DPI, -FB ?.290 .136 <.05 0, 1, 2, 3
DPI, -FB ?.187 .342 <.1 0, 1, 2, 3Pre-test ?.401 < .001
Table 5: DPI, Feedback Model
{FB, DDI} Model A natural question arises:
since Feedback following DPI results in significant
models, are there any significant models which in-
clude sequences whose first component is a Feed-
back move? We found only two that are signif-
icant, when Feedback is followed by DDI (Direct
Declarative Instruction). Note that here we are not
distinguishing between negative and positive feed-
back. Those models are shown in Table 6. The
Lists model is not more effective than the original
Model 3 for Lists in Table 4, but the model for Trees
is slightly more explanatory than the best model
for Trees in that same table, and includes a bigram
model, whereas in Table 4, only pre-test is signifi-
cant for Trees.
Topic Predictor ? R2 P Gap
Lists
FB, DDI .1478
.321
<.1
1Pre-test ?.470 < .001
Length .011 < .05
Trees FB, DDI .0709 .6953 <.05 0Pre-test ?.7409 < .001
Table 6: {FB, DDI} Model
4.2 Trigram Models
{DPI, FB, DDI} Model Given our significant bi-
gram models for DPI followed by FB, and FB fol-
lowed by DDI, it is natural to ask whether the com-
bined trigram model {DPI, FB, DDI} results in a
significant model. It does for the topic List, as
shown in table 7, however again the R2 is lower than
72
that of Model 3 in Table 4. This suggests that an ef-
fective tutoring sequence is to provide instruction on
how to solve the problem (DPI), then Feedback on
what the student does, and finally some declarative
instruction (DDI).
Topic Predictor ? R2 P Gap
Lists
DPI, FB, DDI .156
.371
<.01
1Pre-test ?.528 < .001
Length .012 < .05
Table 7: {DPI, FB, DDI} Model
More effective trigram models include Prompt
and SI. Up to now, only one model including se-
quences of DAs was superior to the simpler models
in Table 4. Interestingly, different trigrams that still
include some form of Feedback, DPI or DDI, and
then either Prompt or SI (Student Initiative) result in
models that exhibit slightly higher R2; additionally
in all these models the trigram predictor is highly
significant. These models are listed in table 8 (note
that the two Trees models differ because in one FB is
generic Feedback, irregardless of orientation, in the
other it?s +FB, i.e., positive feedback). In detail, im-
provements in R2 are 0.0382 in topic Lists, 0.12 in
topic Stacks and 0.0563 in topic Trees. The highest
improvement is in Stacks.
Topic Predictor ? R2 P Gap
Lists
PT,DPI,FB .266
.415
<.01
0Pre-test ?.463 < .001
Length .011 < .05
Stacks DDI,FB,PT ?.06 .416 <.01 1Pre-test ?.52 < .001
Trees +FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Trees FB,SI,DDI .049 .732 <.01 1Pre-test ?.746 < .001
Table 8: Highest R2 Models
It is interesting to note that the model for Lists add
Prompt at the beginning to a bigram that had already
been found to contribute to a significant model. For
Trees, likewise, we add another DA to the bigram
{FB,DDI} that had been found to be significant; this
time, it is Student Initiative (SI) and it occurs in
the middle. This indicates that, after the tutor pro-
vides feedback, the student takes the initiative, and
the tutor responds with one piece of information the
student didn?t know (DDI). Of course, the role of
Prompts and SI is not surprising, although interest-
ingly they are significant only in association with
certain tutor moves. It is well known that students
learn more when they build knowledge by them-
selves, either by taking the initiative (SI), or after
the tutor prompts them to do so (Chi et al, 1994;
Chi et al, 2001).
LOW: it?s backwards # it?s got four elements, but they
are backwards. [DDI]
234: so we have do it again. [SI]
LOW: so do it again. [FB]
LOW: do what again? [Prompt]
Figure 6: {DDI, FB, PT} is most effective in Stacks
4.3 Other models
We found other significant models, specifically,
{DDI,DPI} for all three topics, and {-FB,SI} for
Lists. However, their R2 are very low, and much
lower than any of the other models presented so
far. Besides models that include only one DA se-
quence and pre-test score to predict learning gain,
we also ran experiments to see if adding multiple
DA sequences to pre-test score will lead to signifi-
cant models ? namely, we experimented with mod-
els which include two sequences as predictors, say,
the two bigrams {-FB,SI} and {FB,DDI}. However,
no significant models were found.
5 Conclusions
In this paper, we explored effective tutoring strate-
gies expressed as sequence of DAs. We first pre-
sented the CS-Tutoring corpus. By relaxing the DA
n-gram definition via the fuzzy matching provided
by Apache Lucene, we managed to discover several
DA sequences that significantly correlate with learn-
ing gain. Further, we discovered models with higher
R2 than models which include only one single DA,
which are also more informative from the point of
view of the design of interfaces to ITSs.
6 Acknowledgments
This work was mainly supported by ONR (N00014-
00-1-0640), and by the UIC Graduate College
(2008/2009 Dean?s Scholar Award). Partial sup-
port is also provided by NSF (ALT-0536968, IIS-
0905593).
73
References
John R. Anderson. 1986. Knowledge compilation: The
general learning mechanism. Machine learning: An
artificial intelligence approach, 2:289?310.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596. Survey Article.
Kristy Elizabeth Boyer, Robert Phillips, Amy Ingram,
Eun Young Ha, Michael Wallis, Mladen Vouk, and
James Lester. 2010. Characterizing the effectiveness
of tutorial dialogue with Hidden Markov Models. In
Intelligent Tutoring Systems, pages 55?64. Springer.
Whitney L. Cade, Jessica L. Copeland, Natalie K. Per-
son, and Sidney K. D?Mello. 2008. Dialogue modes
in expert tutoring. In Intelligent Tutoring Systems,
volume 5091 of Lecture Notes in Computer Science,
pages 470?479. Springer Berlin / Heidelberg.
Lin Chen and Barbara Di Eugenio. 2010. A lucene
and maximum entropy model based hedge detection
system. In Proceedings of the Fourteenth Confer-
ence on Computational Natural Language Learning,
pages 114?119, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439?477.
Michelene T. H. Chi, Stephanie A. Siler, Takashi Ya-
mauchi, and Robert G. Hausmann. 2001. Learning
from human tutoring. Cognitive Science, 25:471?533.
Min Chi, Kurt VanLehn, and Diane Litman. 2010. The
more the merrier? Examining three interaction hy-
potheses. In Proceedings of the 32nd Annual Confer-
ence of the Cognitive Science Society (CogSci2010),
Portland,OR.
Michelene T.H. Chi. 1997. Quantifying qualitative anal-
yses of verbal data: A practical guide. Journal of the
Learning Sciences, 6(3):271?315.
Herbert H. Clark. 1992. Arenas of Language Use. The
University of Chicago Press, Chicago, IL.
Barbara Di Eugenio and Michael Glass. 2004. The
Kappa statistic: a second look. Computational Lin-
guistics, 30(1):95?101. Squib.
Barbara Di Eugenio, Trina C. Kershaw, Xin Lu, Andrew
Corrigan-Halpern, and Stellan Ohlsson. 2006. To-
ward a computational model of expert tutoring: a first
report. In FLAIRS06, the 19th International Florida
AI Research Symposium, Melbourne Beach, FL.
Barbara Di Eugenio, Davide Fossati, Stellan Ohlsson,
and David Cosejo. 2009. Towards explaining effec-
tive tutorial dialogues. In Annual Meeting of the Cog-
nitive Science Society, pages 1430?1435, Amsterdam,
July.
Martha W. Evens and Joel A. Michael. 2006. One-on-
one Tutoring by Humans and Machines. Mahwah, NJ:
Lawrence Erlbaum Associates.
Davide Fossati, Barbara Di Eugenio, Christopher Brown,
Stellan Ohlsson, David Cosejo, and Lin Chen. 2009.
Supporting Computer Science curriculum: Exploring
and learning linked lists with iList. IEEE Transac-
tions on Learning Technologies, Special Issue on Real-
World Applications of Intelligent Tutoring Systems,
2(2):107?120, April-June.
Davide Fossati, Barbara Di Eugenio, Stellan Ohlsson,
Christopher Brown, and Lin Chen. 2010. Generat-
ing proactive feedback to help students stay on track.
In ITS 2010, 10th International Conference on Intelli-
gent Tutoring Systems. Poster.
Barbara A. Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the design of instructional systems.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Arthur C. Graesser, Natalie K. Person, and Joseph P.
Magliano. 1995. Collaborative dialogue patterns in
naturalistic one-to-one tutoring. Applied Cognitive
Psychology, 9:495?522.
Arthur C. Graesser, Shulan Lu, George Tanner Jack-
son, Heather Hite Mitchell, Mathew Ventura, Andrew
Olney, and Max M. Louwerse. 2004. AutoTutor:
A tutor with dialogue in natural language. Behav-
ioral Research Methods, Instruments, and Computers,
36:180?193.
Mark R. Lepper, Michael F. Drake, and Teresa
O?Donnell-Johnson. 1997. Scaffolding techniques of
expert human tutors. In K. Hogan and M. Pressley, ed-
itors, Scaffolding student learning: Instructional ap-
proaches and issues. Cambridge, MA: Brookline.
Diane Litman and Kate Forbes-Riley. 2006. Correla-
tions between dialogue acts and learning in spoken
tutoring dialogues. Natural Language Engineering,
12(02):161?176.
Brian MacWhinney. 2000. The Childes Project: Tools
for Analyzing Talk: Transcription format and pro-
grams, volume 1. Psychology Press, 3 edition.
Johanna D. Moore, Kaska Porayska-Pomsta, Sebastian
Varges, and Claus Zinn. 2004. Generating Tutorial
Feedback with Affect. In FLAIRS04, Proceedings of
the Seventeenth International Florida Artificial Intel-
ligence Research Society Conference.
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, Da-
vide Fossati, Xin Lu, and Trina C. Kershaw. 2007.
Beyond the code-and-count analysis of tutoring dia-
logues. In Proceedings of the 13th International Con-
ference on Artificial Intelligence in Education, pages
349?356, Los Angeles, CA, July. IOS Press.
Stellan Ohlsson. 2008. Computational models of skill
acquisition. The Cambridge handbook of computa-
tional psychology, pages 359?395.
74
Ron Sun, Paul Slusarz, and Chris Terry. 2005. The Inter-
action of the Explicit and the Implicit in Skill Learn-
ing: A Dual-Process Approach. Psychological Re-
view, 112:159?192.
Kurt VanLehn, Arthur C. Graesser, G. Tanner Jackson,
Pamela W. Jordan, Andrew Olney, and Carolyn P.
Rose?. 2007. When are tutorial dialogues more effec-
tive than reading? Cognitive Science, 31(1):3?62.
75
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 307?311,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Improving Pronominal and Deictic Co-Reference Resolution with
Multi-Modal Features
Lin Chen, Anruo Wang, Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
{lchen43,awang28,bdieugen}@uic.edu
Abstract
Within our ongoing effort to develop a com-
putational model to understand multi-modal
human dialogue in the field of elderly care,
this paper focuses on pronominal and deictic
co-reference resolution. After describing our
data collection effort, we discuss our anno-
tation scheme. We developed a co-reference
model that employs both a simple notion of
markable type, and multiple statistical mod-
els. Our results show that knowing the type
of the markable, and the presence of simulta-
neous pointing gestures improve co-reference
resolution for personal and deictic pronouns.
1 Introduction
Our ongoing research project, called RoboHelper,
focuses on developing an interface for older people
to effectively communicate with a robotic assistant
that can help them perform Activities of Daily Liv-
ing (ADLs) (Krapp, 2002), so that they can safely re-
main living in their home (Di Eugenio et al, 2010).
We are devising a multi-modal interface since peo-
ple communicate with one another using a variety of
verbal and non-verbal signals, including haptics, i.e.,
force exchange (as when one person hands a bowl to
another person, and lets go only when s/he senses
that the other is holding it). We have collected a
mid size multi-modal human-human dialogue cor-
pus, that we are currently processing and analyz-
ing. Meanwhile, we have started developing one
core component of our multi-modal interface, a co-
reference resolution system. In this paper, we will
present the component of the system that resolves
pronouns, both personal (I, you, it, they), and deictic
(this, that, these, those, here, there). Hence, this pa-
per presents our first steps toward a full co-reference
resolution module, and ultimately, the multi-modal
interface.
Co-reference resolution is likely the discourse
and dialogue processing task that has received the
most attention. However, as Eisenstein and Davis
(2006) notes, research on co-reference resolution
has mostly been applied to written text; this task
is more difficult in dialogue. First, utterances may
be informal, ungrammatical or disfluent; second,
people spontaneously use hand gestures, body ges-
tures and gaze. Pointing gestures are the eas-
iest gestures to identify, and vision researchers in
our project are working on recognizing pointing and
other hand gestures (Di Eugenio et al, 2010). In this
paper, we replicate the results from (Eisenstein and
Davis, 2006), that pointing gestures help improve
co-reference, in a very different domain. Other work
has shown that gestures can help detect sentence
boundaries (Chen and Harper, 2010) or user inten-
tions (Qu and Chai, 2008).
The rest of the paper is organized as follows. In
Section 2 we describe the data collection and the on-
going annotation. In Section 3 we discuss our co-
reference resolution system, and we present experi-
ments and results in Section 4.
2 The ELDERLY-AT-HOME corpus
Due to the absence of multi-modal collaborative
human-human dialogue corpora that include haptic
data beyond what can be acquired via point-and-
touch interfaces, and in the population of interest,
307
Figure 1: Experiment Excerpts
we undertook a new data collection effort. Our ex-
periments were conducted in a fully functional stu-
dio apartment at Rush University in Chicago ? Fig-
ure 1 shows two screen-shots from our recorded ex-
periments. We equipped the room with 7 web cam-
eras to ensure multiple points of view. Each of the
two participants in the experiments wears a micro-
phone, and a data glove on their dominant hand to
collect haptics data. The ADLs we focused on in-
clude ambulating, getting up from a bed or a chair,
finding pots, opening cans and containers, putting
pots on a stove, setting the table etc. Two students
in gerontological nursing play the role of the helper
(HEL), both in pilot studies and with real subjects.
In 5 pilot dialogues, two faculty members played the
role of the elderly person (ELD). In the 15 real ex-
periments, ELD resides in an assisted living facil-
ity and was transported to the apartment mentioned
above. All elderly subjects are highly functioning at
a cognitive level and do not have any major physical
impairment.
The size of our collected video data is shown
in Table 1. The number of subjects refers to the
number of different ELD?s and does not include the
helpers; we do include our 5 pilot dialogues though,
since those pilot interactions do not measurably dif-
fer from those with the real subjects. Usually one
experiment lasts about 50? (recording starts after in-
formed consent and after the microphones and data
gloves have been put on). Further, we eliminated
irrelevant content such as interruptions, e.g. by the
person who accompanied the elderly subjects, and
further explanations of the tasks. This resulted in
about 15 minutes of what we call effective data for
each subject; the effective data comprises 4782 turns
(see Table 1).
Subjects Raw(Mins) Effective(Mins) Turns
20 482 301 4782
Table 1: ELDERLY-AT-HOME Corpus Size
The effective portion of the data was transcribed
by the first two authors using the Anvil video anno-
tation tool (Kipp, 2001). A subset of the transcribed
data was annotated for co-reference, yielding 114
sub-dialogues corresponding to the tasks subjects
perform, such as finding bowls, filling a pot with wa-
ter, etc. (see Table 2).
An annotation excerpt is shown in Figure 2.
Markable tokens are classified into PLC(Place),
PERS(Person), OBJ(Object) types, and numbered
by type, e.g., PLC#5. Accordingly, we mark pro-
nouns with types as well, RPLC, RPERS, ROBJ, e.g.
RPLC#5. If a subject produced a pointing gesture,
we generate a markable token to mark what is being
pointed to at the end of the utterance (see Utt. 4 and 5
in Figure 2). Within the same task, if two markables
have the same type and the same markable index,
they are taken to co-refer (hence, longer chains of
reference across tasks are cut into shorter spans).
Haptics annotation is at the beginning. We have
identified grab, hold, give and receive as high-level
haptics phonemes that may be useful from the lan-
guage point of view. We have recently started anno-
tating our corpus with those labels.
Subjects Tasks Utterances Gestures Pronouns
12 114 1920 896 1635
Table 2: Annotated Corpus Size
In order to test the reliability of our annotation,
we double coded about 18% of the data, namely 21
sub-dialogues comprising 213 pronouns, on which
we computed the Kappa coefficient (Carletta, 1996).
Similar to (Rodr?guez et al, 2010), we measured the
reliability of markable annotations, and of link to
the antecedent annotations. As concerns the mark-
able level, we obtained ?=0.945, which is high but
no surprisingly for such a simple task. At the link to
the antecedent level, we compared the links from
pronouns to antecedents in a specified context of 4
utterances, obtaining a reasonable ?=0.723.
308
3: PERS#1(HEL/NNP) : RPERS#1(I/PRP) do/VBP n?t/RB see/VB any/DT OBJ#3(pasta/NN) ./.
4: PERS#2(ELD/NNP) : Try/VB over/IN RPLC#5(there/RB) ./. {PLC#5(cabinet/NN)}
5: PERS#1(HEL/NNP) : This/DT RPLC#5(one/NN) ?/. {PLC#5(cabinet/NN)}
6: PERS#2(ELD/NNP) : Oh/UH ,/, yes/RB ./.
Figure 2: Annotation Excerpt
3 Our approach
Utterances and Gestures
Find Markables Generate Candidates
Coreference Pairs
Preprocessing
Markable Model Coreference Model
Figure 3: Co-reference System Architecture
The architecture of our co-reference resolution
system is shown in Figure 3.
We first pre-process a dialogue by splitting turns
into sentences, tokenizing sentences into tokens,
POS tagging tokens. The Markable model is used
to classify whether a token can be referred to and
what type of markable it is. The Markable model?s
feature set includes the POS tag of the token, the
word, the surrounding tokens? POS tags in a win-
dow size of 3. The model outputs markable classes:
Place/Object/Person, or None, which means the to-
ken is not markable. A pointed-to entity serves as a
markable by default.
To perform resolution, each pronoun to be re-
solved ( I, you, it, they; this, that, these, those, here,
there) is paired with markables in the context of the
previous 2 utterances, the current utterance and the
utterance that follows, by using {pronoun, markable
type} compatibility rules. For example, let?s con-
sider the excerpt in Figure 2. To resolve one in
utterance 5, the system will generate 3 candidate
token pairs: <one(5,2), pasta(3,6)>, <one(5,2),
cabinet(4,-1)>, <one(5,2), cabinet(5,-1)> (includ-
ing the pointed-to markable is a way of roughly ap-
proximating information that will be returned by the
vision component). The elements in those pairs
are tokens with their coordinates in the format (Sen-
tenceIndex, TokenIndex); markables pointed to are
given negative token indices.
The Co-reference model will filter out the pairs
<pronoun, markable> that it judges to be incor-
rect. For the Co-reference model, we adopted a
subset of features which are commonly used in co-
reference resolution in written text. These features
apply to each <pronoun, markable> pair and in-
clude: Lexical features, i.e. words and POS tags for
both anaphora and antecedent; Syntactic features,
i.e. syntactic constraints such as number and per-
son agreement; Distance features, i.e. sentence dis-
tance, token distance and markable distance. Addi-
tionally, the Co-reference model uses pointing ges-
ture information. If the antecedent in the <pronoun,
markable> was pointed to, the pair is tagged as Is-
Pointed. In our data, people often use pronouns
and hand gestures instead of nouns when introduc-
ing new entities. It is not possible to map these
pronouns to a textual antecedent since none exists.
This confirms the findings from (Kehler, 2000): in
a multi-modal corpus, he found that no pronoun is
used without a gesture when it refers to a referent
which is not in focus.
4 Experiments and Discussion
The classification models described above were im-
plemented using the Weka package (Hall et al,
2009). Specifically, for each model, we experi-
mented with J48 (a decision tree implementation)
and LibSVM (a Support Vector Machine implemen-
tation). All the results reported below are calculated
using 10 fold cross-validation.
We evaluated the performances of individual
models separately (Tables 3 and 4), and of the sys-
tem as a whole (Table 5).
Algorithm Precision Recall F-Measure
J48 0.984 0.984 0.984
LibSVM 0.979 0.936 0.954
Baseline 0.971 0.971 0.971
Table 3: Markable Model Performance
The results in Table 3 are not surprising, since de-
tecting the type of markables is a simple task. In-
deed the results of the baseline model are extremely
309
Method J48 LibSVMPrecision Recall F-Measure Precision Recall F-Measure
Text + Gesture 0.700 0.684 0.686 0.672 0.669 0.670
Text Only 0.655 0.656 0.656 0.624 0.624 0.624
Table 4: Co-reference Model Performance
Words Method Features Precision Recall F-Measure
All Pronouns
J48 Text Only 0.544 0.332 0.412Text + Gesture 0.482 0.783 0.596
LibSVM Text Only 0.56 0.27 0.364Text + Gesture 0.522 0.6 0.559
Baseline Text Only 0.367 0.254 0.300Text + Gesture 0.376 0.392 0.384
3rd Person + Deictic
J48 Text Only 0.264 0.028 0.05Text + Gesture 0.438 0.902 0.589
LibSVM Text Only 0.6 0.009 0.017Text + Gesture 0.525 0.695 0.598
Baseline Text Only 0.172 0.114 0.137Text + Gesture 0.301 0.431 0.354
Table 5: Co-reference System Performance (Markable + Co-reference Models)
high as well. We compute the baseline by assigning
to the potential markable (i.e., each word) its most
frequent class in the training set (recall that the four
classes include None as well).
For the Co-reference model, we conducted 2 sets
of experiments to ascertain the effect of including
Gesture in the model. As shown in Table 4, both J48
and LibSVM obtain better results when we include
gestures in the model. ?2 shows that differences in
precision and recall 1 are significant at the p ? 0.01
level, though the absolute improvement is not high.
As concerns the evaluation of the whole system,
we ran a 4-way experiment, where we examine the
performance of the system on all pronouns, and on
those pronouns left after eliminating first and second
person pronouns, without and with Gesture informa-
tion. We also ran two sets of baseline experiments.
In the baseline experiments, we link each pronoun
we want to resolve, to the most recent utterance-
markable token and to a pointed-to markable token
(if applicable). Markables are filtered by the same
compatibility rules mentioned above.
Regarding the metrics we used for evaluation, we
used the same method as Strube and Mu?ller (2003),
which is also similar to MUC standard (Hirschman,
1?2 does not apply to the F-Measure.
1997). As the golden set, we used the human an-
notated links from the pronouns to markables in the
same context of four utterances used by the system.
Then, we compared the co-reference links found by
the system against the golden set, and we finally cal-
culated precision, recall and F-Measure.
Table 5 shows that the F-measure is higher when
including gestures, no matter the type of pronouns.
When we include gestures, there is no difference be-
tween ?All Pronouns? and ?3rd Person + Deictic?.
In the ?3rd Person + Deictic? experiments, we ob-
served huge drops in recall, from 0.902 to 0.028 for
J48, and from 0.695 to 0.009 for LibSVM algorithm.
This confirms the point we made earlier, that 3rd
person pronouns/deictic words (Kehler, 2000) often
do not have textual antecedents, since when accom-
panied by simultaneous pointing they introduce new
entities in a dialogue.
Comparison to previous work is feasible only at a
high level, because of the usage of different corpora
and/or measurement metrics. This said, our model
with gestures outperforms Strube andMu?ller (2003),
who did not use gesture information to resolve pro-
nouns in spoken dialogue. Strube and Mu?ller (2003)
used the 20 Switchboard dialogues as their experi-
ment dataset, and used the MUC metrics. Our re-
310
sults are similar to Eisenstein and Davis (2006), but
there are two main differences. First, the corpus
they used is smaller than what we used in this pa-
per. Their corpus was collected by themselves and
consisted of 16 videos, each video was 2-3 minutes
in length. Second, they used a difference measure-
ment metrics called CEAF (Luo, 2005).
5 Conclusions
In this paper, we presented the new ELDERLY-AT-
HOME multi-modal corpus we collected. A co-
reference resolution system for personal and deic-
tic pronouns has been developed on the basis of the
annotated corpus. Our results confirm that gestures
improve co-reference resolution; a simple notion of
type also helps. The Markable and Co-reference
modules we presented are a first start in developing
a full multi-modal co-reference resolution module.
Apart from completing the annotation of our cor-
pus, we will develop an annotation scheme for hap-
tics, and investigate how haptics information affects
co-reference and other dialogue phenomena. Ulti-
mately, both pointing gestures and haptic informa-
tion will automatically be recognized by the collab-
orators in the project we are members of.
Acknowledgments
This work is supported by award IIS 0905593 from
the National Science Foundation. Thanks to the
other members of the RoboHelper project, for their
many contributions, especially to the data collection
effort.
References
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22:249?254.
Lei Chen and Mary P. Harper. 2010. Utilizing gestures
to improve sentence boundary detection. Multimedia
Tools and Applications, pages 1?33.
Barbara Di Eugenio, Milos? Z?efran, Jezekiel Ben-
Arie, Mark Foreman, Lin Chen, Simone Franzini,
Shankaranand Jagadeesan, Maria Javaid, and Kai
Ma. 2010. Towards Effective Communication with
Robotic Assistants for the Elderly: Integrating Speech,
Vision and Haptics. InDialog with Robots, AAAI 2010
Fall Symposium, Arlington, VA, USA, November.
Jacob Eisenstein and Randall Davis. 2006. Gesture
Improves Coreference Resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages 37?
40.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Lynette Hirschman. 1997. Muc-7 coreference task defi-
nition.
Andrew Kehler. 2000. Cognitive Status and Form of
Reference in Multimodal Human-Computer Interac-
tion. In AAAI 00, The 15th Annual Conference of the
American Association for Artificial Intelligence, pages
685?689.
Michael Kipp. 2001. Anvil-a generic annotation tool
for multimodal dialogue. In Proceedings of the 7th
European Conference on Speech Communication and
Technology, pages 1367?1370.
Kristine M. Krapp. 2002. The Gale Encyclopedia of
Nursing & Allied Health. Gale Group, Inc. Chapter
Activities of Daily Living Evaluation.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, HLT ?05, pages 25?
32, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Shaolin Qu and Joyce Y. Chai. 2008. Beyond attention:
the role of deictic gesture in intention recognition in
multimodal conversational interfaces. In Proceedings
of the 13th international conference on Intelligent user
interfaces, pages 237?246.
Kepa Joseba Rodr?guez, Francesca Delogu, Yannick Ver-
sley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of wikipedia and blogs in the
live memories corpus. In Proceedings of the 7th In-
ternational Conference on Language Ressources and
Evaluation (LREC 2010), pages 157?163.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1.
311
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 290?294,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Improving Sentence Completion in Dialogues with Multi-Modal Features
Anruo Wang, Barbara Di Eugenio, Lin Chen
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607, USA
awang28, bdieugen, lchen43@uic.edu
Abstract
With the aim of investigating how humans un-
derstand each other through language and ges-
tures, this paper focuses on how people un-
derstand incomplete sentences. We trained a
system based on interrupted but resumed sen-
tences, in order to find plausible completions
for incomplete sentences. Our promising re-
sults are based on multi-modal features.
1 Introduction
Our project, called RoboHelper, focuses on devel-
oping an interface for elderly people to effectively
communicate with robotic assistants that can help
them perform Activities of Daily Living (ADLs)
(Krapp, 2002), so that they can safely remain living
in their home (Di Eugenio et al, 2010; Chen et al,
2011). We are developing a multi-modal interface
since people communicate with each other using a
variety of verbal and non-verbal signals, including
haptics, i.e., force exchange (as when one person
hands a bowl to another person, and lets go only
when s/he senses that the other is holding it). We
collected a medium size multi-modal human-human
dialogue corpus, then processed and analyzed it. We
observed that a fair number of sentences are incom-
plete, namely, the speaker does not finish the utter-
ance. Because of that, we developed a core compo-
nent of our multi-modal interface, a sentence com-
pletion system, trained on the set of interrupted but
eventually completed sentences from our corpus. In
this paper, we will present the component of the sys-
tem that predicts reasonable completion structures
for an incomplete sentence.
Sentence completion has been addressed within
information retrieval, to satisfy user?s information
needs (Grabski and Scheffer, 2004). Completing
sentences in human-human dialogue is more diffi-
cult than in written text. First, utterances may be in-
formal, ungrammatical or dis-fluent; second, people
interrupt each other during conversations (DeVault
et al, 2010; Yang et al, 2011). Additionally, the
interaction is complex, as people spontaneously use
hand gestures, body language and gaze besides spo-
ken language. As noticed by (Bolden, 2003), during
face-to-face interaction, the completion problem is
not only an exclusively verbal phenomenon but ?an
action embedded within a complex web of differ-
ent meaning-making fields?. Accordingly, among
our features, we will include pointing gestures, and
haptic-ostensive (H-O) actions, e.g., referring to an
object by manipulating it in the real world (Landra-
gin et al, 2002; Foster et al, 2008).
The paper is organized as follows. In Section 2 we
describe our data collection and multi-modal anno-
tation. In Section 3 we discuss how we generate our
training data, and in Section 4 the model we train
for sentence completion, and the results we obtain.
2 Dataset
In contrast with other sentence completion systems
that focus on text input, the dataset we use in this
paper is a subset of the ELDERLY-AT-HOME cor-
pus, a multi-modal corpus in the domain of elderly
care, which includes collaborative human-human di-
alogues, pointing gestures and haptic-ostensive (H-
O) actions. Our experiments were conducted in
a fully functional apartment and included a helper
290
(HEL) and an elderly person (ELD). HEL helps
ELD to complete several realistic tasks, such as
putting on shoes, finding a pot, cooking pasta and
setting the table for dinner. We used 7 web cameras
to videotape the whole experiment, one microphone
each to record the audio and one data glove each to
collect haptics data. We ran 20 realistic experiments
in total, and then imported the videos and audios (in
avi format), haptics data (in csv format) and tran-
scribed utterances (in xml format) into Anvil (Kipp,
2001) to build the multi-modal corpus.
Among other annotations (for example Dialogue
Acts) we have annotated these dialogues for Point-
ing gestures and H-O actions. Due to the setting
of our experiments, the targets of pointing gestures
and H-O actions are real life objects, thus we de-
signed a reference index system to annotate them.
We give pre-defined indices to targets which can-
not be moved, such as cabinets, draws, and fridge.
We also assign runtime indices to targets which can
be moved, like pots, glasses, and plates. For exam-
ple, ?Glass1? refers to the first glass that appears in
one experiment. In our annotation, a ?Pointing? ges-
ture is defined as a hand gesture without any phys-
ical contact between human and objects. Hand
gestures with physical contact to objects are anno-
tated as H-O actions. H-O actions are further subdi-
vided into 7 subtypes, including ?Holding?, ?Touch-
ing?,?Open? and ?Close?. In order to verify the reli-
ability of our annotations, we double coded 15% of
the pointing gestures and H-O actions. Kappa val-
ues of 0.751 for pointing gestures, and of 0.703 for
H-O actions, are considered acceptable, especially
considering the complexity of these real life tasks
(Chen and Di Eugenio, 2012).
In this paper, we focus on specific sub-dialogues
in the corpus, which we call interruptions. An inter-
ruption can occur at any point in human-human dia-
logues: it happens when presumably the interrupter
(ITR) thinks s/he has already understood what the
speaker (SPK) means before listening to the entire
sentence. By observing the data from our corpus,
we conclude that there are generally three cases of
interruptions. First, the speaker (SPK) stops speak-
ing and does not complete the sentence ? these are
the incomplete sentences whose completion a robot
would need to infer. In the second type of inter-
ruption, after being interrupted SPK continues with
(a) few words, and then stops without finishing the
whole sentence: hence, there is a short time over-
lap between two sentences (7 cases). The third case
occurs when the SPK ignores the ITR and finishes
the entire sentence. In this case, the SPK and the
ITR speak simultaneously (198 cases). The number
of interruptions ranges from 1 to 37 in each experi-
ment. An excerpt from an interruption with a subse-
quent completion (an example of case 3) is shown
below. The interruption occurs at the start of the
overlap between the two speakers, marked by < and
>. This example also includes annotations for point-
ing gestures and for H-O actions.
Elder: I need some glasses from < that cabinet >.
[Point (Elder, Cabinet1)]
Helper: < From this > cabinet?
[Point (Helper, Cabinet2)]
Helper: Is this the glass you < ?re looking for? >
[Touching (Helper, Glass1)]
Elder: < No, that one.>
[Point (Elder, Cabinet1, Glass2)]
As concerns annotation for interruptions, it proceeds
from identifying interrupted sentences to finding
<interrupted sentences, candidate structure> pairs
which will be used for generating grammatical com-
pletion for an incomplete sentence. Each in-
terrupted sentence is marked with two categories:
incomplete form, from the start of the sentence
to where it is interrupted, such as ?I need some
glasses?; complete form, from the start of a sentence
to where the speaker stops, ?I need some glasses
from that cabinet.?
Table 2 shows distribution statistics for our
ELDERLY-AT-HOME corpus. It contains a total of
4839 sentences, which in turn contain 7219 clauses.
320 sentences are incomplete in the sense of case 1
(after interruption SPK never completes his/her sen-
tence); whereas 205 sentences are completed after
interruption (cases 2 and 3).
Sentences 4,839
Clauses 7,219
Pointing Gestures 362
H-O Actions 629
Incomplete sentences 320
Interrupted sentences 205
Table 1: Corpus Distributions
291
3 Candidate Pairs Generation
The question is now, how to generate plausible train-
ing instances to predict completions for incomplete
sentences. We use the 205 sentences that have
been interrupted but for which we have comple-
tions; however, we cannot only use those pairs for
training, since we would run the risk of overfit-
ting, and not being able to infer appropriate com-
pletions for other sentences. To generate addi-
tional<Interrupted sentences, candidate structure>
pairs, we need to match an interrupted sentence IntS
with its potential completions ? basically, to check
whether IntS can match the prefix of other sentences
in the corpus. We do so by comparing the POS se-
quence and parse tree of IntS with the POS sequence
and parse tree of the prefix of another sentence. Both
IntS and other sentences in the corpus are parsed via
the Stanford Parser (Klein and Manning, 2003).
Before discussing the details though, we need
to deal with one potential problem: the POS se-
quence for the incomplete portion of IntS may not
be correctly assigned. For example, when the sen-
tence ?The/DT, top/JJ, cabinet/NN.? is interrupted as
?The/DT, top/NN?, the POS tag of NN is assigned
to ?top?; this is incorrect, and engenders noise for
finding correct completions.
We first pre-process a dialogue by splitting turns
into sentences, tokenizing sentences into tokens, and
POS tagging tokens. Although for the interrupted
sentences, we could obtain a correct POS tag se-
quence by parsing the incomplete and resumed por-
tions together, this would not work for a truly incom-
plete sentence (whose completion is our goal). Thus,
to treat both interrupted sentences and incomplete
sentences in the same way, we train a POS tag Cor-
rection Model to correct fallaciously assigned POS
tags. The POS tag Correction Model?s feature set
includes the POS tag of the token, the word, and the
previous tokens? POS tags in a window size of 3.
The model outputs the corrected POS tags.
The POS tag Correction model described above
was implemented using the Weka package (Hall et
al., 2009). Specifically, we experimented with J48
(a decision tree implementation), Naive Bayes (NB),
and LibSVM (a Support Vector Machine implemen-
tation). All the results reported below are calculated
using 10 fold cross-validation.
J48 NB LibSVM
Accuracy 0.829 0.680 0.532
Table 2: POS tag Correction Model Performance
The results in Table 2 are not surprising, since de-
tecting the POS tag of a known word is a simple
task. Additionally, it is not surprising that J48 is
more accurate than NB, since NB is known to of-
ten behave as a baseline method. What is surprising
though is the poor performance of SVMs, which are
generally among the top performers for a broad va-
riety of tasks. We are investigating why this may be
the case. At any rate, by applying the J48 model, we
obtain more accurate POS tag assignments for inter-
rupted sentences (and in our future application, for
the incomplete sentence we need to complete).
Once we have corrected the POS assignments for
each interrupted sentence IntS, we retrieve poten-
tial grammatical structures for IntS, by comparing
IntS with the prefixes of all complete sentences in
the corpus via POS tags and parse trees. Note that
due to the complexity of building a parse tree cor-
rection model in our corpus, we only build a model
to correct the POS tags, but ignore the possible in-
correct parse trees of the incomplete portion of an
interrupted sentence. The matching starts from the
last word in IntS back to the first word, with weights
assigned to each position in decreasing order. Due to
the size of our corpus, it is not possible to find ex-
actly matched POS tag sequences for every incom-
plete sentence; thus, we also consider the parsed tree
structures and mismatched POS tags between IntS?s
and complete sentences by reducing weights accord-
ing to the size of the matched phrases and distances
of mismatched POS tags. After this, a matching
score is calculated for each incomplete and candi-
date structure pair.
Due to the large number of candidate structures,
only the top 150 candidate structures for each IntS
are selected and manually annotated with three
classifications: ?R?, when the candidate structure
provides a grammatically ?reasonable? structure,
which can be used as a template for completion;
?U?, which means the candidate structure gives
an ?ungrammatical? structure, thus this candidate
structure cannot be used as template for completion;
292
?T?, the candidate structure is exactly the same as
what the speaker was originally saying, as judged
based on the video and audio records. An example
of an incomplete sentence with candidate structures
in each of the three categories is shown below.
It/PRP, feels/VBZ | It/PRP, feels/VBZ, good/JJR
[R] It/PRP, ?s/VBZ, fine/JJ, like/IN, this/DT]
[U] We/PRP, did/VBD, n?t/RB
[T] It/PRP, is/VBZ, better/JJR
10543 interrupted sentences and candidate pairs
are generated. 5268 of those 10543 pairs
(49.97%) were annotated as ?Reasonable?, 4727
pairs (44.85%) were annotated as ?Unreasonable?,
and 545 pairs (5.17%) were annotated as ?Same with
original sentence?.
Incomplete Sentence and Structure pairs 10,543
Reasonable structures (R) 5,268
Unreasonable structures (U) 4,729
Exactly same structures (T) 545
Table 3: Distribution of completion classifications
4 Results and Discussion
On the basis of the annotation, we trained a ?Rea-
sonable Structure Selection (RSS)? model via su-
pervised learning methods. For each pair <IntS,
Candidate>, the feature set includes word and POS
tag of the tokens of IntS and its candidate structure
sentence. Co-occurring pointing gestures and H-O
actions for both IntS and Candidate are also included
in the model. Co-occurrence is defined as tempo-
ral overlap between the gesture (pointing or H-O ac-
tion) and the duration of the utterance. For each
training instance, we include the following features:
IntS: <words, POS tags>, <Pointing (Person / Ob-
ject / Location)>, <H-O action (Person / Object /
Location / Type)>;
Candidate: <words/POS tags)>, <Pointing (Per-
son / Object / Location)>, <H-O action (Person /
Object / Location / Type)>;
<Matching Score>;
<Classification: R, U, or T>.
We trained the RSS model also using the Weka
package. The same methods mentioned earlier
(J48, NB and SVM) are used, with 10-fold cross-
validations. Results are shown in Table 4. We
J48 NB LibSVM
Precision R, U, T 0.822 0.724 0.567
R, U 0.843 0.761 0.600
Recall R, U, T 0.820 0.725 0.512
R, U 0.842 0.762 0.563
F-Measure R, U, T 0.818 0.711 0.390
R, U 0.841 0.761 0.440
Table 4: Reasonable Structure Selection models
ran two different sets of experiments using two ver-
sions of training instances: Classification with three
classes, R, U and T, and classification with two
classes, R and U. When training with only two
classes, the T instances are marked as R. We exper-
imented with collapsing R and T candidates since T
candidates may lead to overfitting, and some R can-
didates might even provide better structures for an
incomplete sentence than what exactly one speaker
had originally said. Not surprisingly, results im-
prove for two-way classification. Based on the J48
model, we observed that the POS tag features play
a significant part in classification, whereas the word
features are redundant. Further, pointing gestures
and H-O actions do appear in some subtrees of the
larger decision tree, but not on every branch. We
speculate that this is due to the fact that pointing ges-
tures or H-O actions do not accompany every utter-
ance.
5 Conclusions and Future Work
In this paper, we introduced our multi-modal sen-
tence completion schema which includes pointing
gestures and H-O actions in the corpus ELDERLY-
AT-HOME. Our data shows that it is possible to pre-
dict what people will say, even if the utterance is
not complete. Our promising results include multi-
modal features, which as we have shown elsewhere
(Chen and Di Eugenio, 2012) improve traditional
co-reference resolution models. In the near future,
we will implement the last module of our sentence
completion system, the one that fills the chosen can-
didate structure with actual words.
293
References
G.B. Bolden. 2003. Multiple modalities in collaborative
turn sequences. Gesture, 3(2):187?212.
L. Chen and B. Di Eugenio. 2012. Co-reference via
pointing and haptics in multi-modal dialogues. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics. short paper, to appear.
L. Chen, A. Wang, and B. Di Eugenio. 2011. Im-
proving pronominal and deictic co-reference resolu-
tion with multi-modal features. In Proceedings of the
SIGDIAL 2011 Conference, pages 307?311. Associa-
tion for Computational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2010.
Incremental interpretation and prediction of utterance
meaning for interactive dialogue. Dialogue and Dis-
course, 2(1):143170.
B. Di Eugenio, M. Zefran, J. Ben-Arie, M. Foreman,
L. Chen, S. Franzini, S. Jagadeesan, M. Javaid, and
K. Ma. 2010. Towards effective communication with
robotic assistants for the elderly: Integrating speech,
vision and haptics. In 2010 AAAI Fall Symposium Se-
ries.
M.E. Foster, E.G. Bard, M. Guhe, R.L. Hill, J. Ober-
lander, and A. Knoll. 2008. The roles of haptic-
ostensive referring expressions in cooperative, task-
based human-robot dialogue. In Proceedings of the
3rd ACM/IEEE international conference on Human
robot interaction, pages 295?302. ACM.
K. Grabski and T. Scheffer. 2004. Sentence completion.
In Proceedings of the 27th annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 433?439. ACM.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H. Wit-
ten. 2009. The WEKA data mining soft-
ware: An update. SIGKDD Explorations, 11(1).
http://www.cs.waikato.ac.nz/ml/weka/.
M. Kipp. 2001. Anvil-a generic annotation tool for mul-
timodal dialogue. In Seventh European Conference on
Speech Communication and Technology.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
K.M. Krapp. 2002. The Gale Encyclopedia of Nursing
& Allied Health: DH, volume 2. Gale Cengage.
F. Landragin, N. Bellalem, L. Romary, et al 2002. Re-
ferring to objects with spoken and haptic modalities.
F. Yang, P.A. Heeman, and A.L. Kun. 2011. An
investigation of interruptions and resumptions in
multi-tasking dialogues. Computational Linguistics,
37(1):75?104.
294
Proceedings of the SIGDIAL 2013 Conference, pages 183?192,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Multimodality and Dialogue Act Classification in the RoboHelper Project
Lin Chen
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607
lchen43@uic.edu
Barbara Di Eugenio
Department of Computer Science
University of Illinois at Chicago
851 S Morgan ST, Chicago, IL 60607
bdieugen@uic.edu
Abstract
We describe the annotation of a multi-
modal corpus that includes pointing ges-
tures and haptic actions (force exchanges).
Haptic actions are rarely analyzed as full-
fledged components of dialogue, but our
data shows haptic actions are used to ad-
vance the state of the interaction. We re-
port our experiments on recognizing Di-
alogue Acts in both offline and online
modes. Our results show that multimodal
features and the dialogue game aid in DA
classification.
1 Introduction
When people collaborate on physical or virtual
tasks that involve manipulation of objects, dia-
logues become rich in gestures of different kinds;
the actions themselves that collaborators engage
in also perform a communicative function. Col-
laborators gesture while speaking, e.g. saying
?Try there?? while pointing to a faraway location;
they perform actions to reply to their partner?s ut-
terances, e.g. opening a cabinet to comply with
?please check cabinet number two?. Conversely,
they use utterances to reply to their partner?s ges-
tures and actions, e.g. saying ?not there, try the
other one? after their partner opens a cabinet. Ges-
tures and actions are an important part of such di-
alogues; while the role of pointing gestures has
been explored, the role that haptic actions (force
exchanges) play in an interaction has not.
In this paper, we present our corpus of multi-
modal dialogues in a home care setting: a helper
is helping an elderly person perform activities of
daily living (ADLs) such as preparing dinner. We
investigate how to apply Dialogue Act (DA) clas-
sification to these multimodal dialogues. Many
challenges arise. First, an utterance may not di-
rectly follow a spoken utterance, but a gesture or a
haptic action. Likewise, the next move is not nec-
essarily an utterance, it can be a gesture (pointing
or haptics) only, or a multimodal utterance. Third,
when people use gestures and actions together
with utterances, the utterances become shorter,
hence the textual context that has been used to ad-
vantage in many previous models is impoverished.
Our contributions concern: exploring the dialogue
functions of what we call Haptic-Ostensive (H-O)
actions (Foster et al, 2008), namely haptics ac-
tions that often perform a referential function; ex-
perimenting with both offline and online DA clas-
sification, whereas most previous work only fo-
cuses on offline classification (Stolcke et al, 2000;
Hastie et al, 2002; Di Eugenio et al, 2010a); high-
lighting the role played by multimodal features
and dialogue structure (in the form of dialogue
games) as concerns DA classification.
Our work is part of the RoboHelper project (Di
Eugenio et al, 2010b) whose ultimate goal is to
deploy robotic assistants for the elderly so that
they can safely remain living in their home. The
models we derive from our experiments are the
building blocks of a multimodal information-state
based dialogue manager, whose architecture is
shown in Figure 1. The dialogue manager per-
forms reference resolution, specifically resolving
third person pronouns and deictics in utterances;
classifies utterances to DAs; infers the dialogue
games for utterances; updates the dialogue state,
and finally decides what the next step is in the in-
teraction. We have discussed our approach to mul-
timodal reference resolution in (Chen et al, 2011;
Chen and Di Eugenio, 2012). In this paper, we fo-
cus on the Dialogue Act classification component.
We will also touch on Dialogue Game inference.
Our collaborators are developing the speech pro-
cessing, vision and haptic recognition components
(Franzini and Ben-Arie, 2012; Ma and Ben-Arie,
2012; Javaid and Z?efran, 2012), that, when inte-
grated with the dialogue manager we are building,
183
Figure 1: System Architecture
will make the interface situated in and able to deal
with a real environment.
After discussing related work in Section 2, we
present our multimodal corpus and the multidi-
mensional annotation scheme we devised in Sec-
tion 3. In Section 4 we discuss all the features we
used to build machine learning models to classify
DAs. Sections 5 is devoted to our experiments and
the results we obtained. We conclude and discuss
future work in Section 6.
2 Related Work
Due to its importance in dialogue research, DA
classification has been the focus of a large body
of research (Stolcke et al, 2000; Sridhar et
al., 2009; Di Eugenio et al, 2010a; Boyer
et al, 2011). Some of this work has been
made possible by several available corpora tagged
with DAs, including HCRC Map Task (Ander-
son et al, 1991), CallHome (Levin et al, 1998),
Switchboard (Graff et al, 1998), ICSI Meeting
Recorder (MRDA) (Shriberg et al, 2004), and the
AMI multimodal corpus (Carletta, 2007).
Researchers have applied various approaches
to this task. Initially only simple textual fea-
tures were used, e.g. n-grams were used to
model the constraints for DA sequences in an
HMM model (Stolcke et al, 2000). Zimmermann
et al (2006) investigated the joint segmentation
and classification of DAs using prosodic features.
Sridhar et al (2009) showed that prosodic cues
can improve DA classification for a Maximum En-
tropy based model. Di Eugenio et al (2010a)
extended Latent Semantic Analysis with linguis-
tic features, including dialogue game information.
Boyer et al (2011) integrates facial expressions
to significantly improve the recognition of several
DAs, whereas Ha et al (2012) shows that auto-
matically recognized postural features may help to
disambiguate DAs.
It should be pointed out that most of this work
focuses on offline DA classification ? namely, DA
classification is performed on the corpus using
the gold-standard classification for the previous
DA(s). Since some sort of history of previous
DAs is used by all systems, using online classi-
fication for the previous DAs will unavoidably im-
pact performance (Sridhar et al, 2009; Kim et al,
2012). Additionally, for models such as HMMs
and CRF that approach the problem as sequence
labeling, online processing means that only a par-
tial sequence is available.
3 The ELDERLY-AT-HOME Corpus
This work is based on the ELDERLY-AT-HOME
corpus, a multimodal corpus in the domain of el-
derly care (Chen and Di Eugenio, 2012). The
corpus contains 20 human-human dialogues. In
each dialogue, a helper (HEL) and an elderly
person (ELD) perform Activities of Daily Liv-
ing (ADL) (Krapp, 2002), such as getting up from
chairs, finding pots, cooking pasta. The setting
is a fully equipped studio apartment used for
teaching and research in a partner university (see
Figure 2). The corpus contains 482 minutes of
recorded videos, which comprise 301 minutes of
what we call effective video, obtained by eliminat-
ing irrelevant content such as explanations of the
tasks and interruptions by the person who accom-
panied the elderly subject (who is not playing the
part of the helper). This 301 minutes contain 4782
spoken turns. The corpus includes video and au-
dio data in .avi and .wav format, haptics data col-
lected via instrumented gloves in .csv format, and
the transcribed utterances in xml format.
The Find subcorpus of our corpus comprises
only Find tasks, where subjects look for and re-
trieve various kitchen objects such as pots, silver-
ware, pasta, etc. from various locations in the
apartment. We define a Find task as a continuous
time span during which the two subjects are col-
laborating on finding objects. Find tasks naturally
arise while performing an ADL such as preparing
dinner. Figure 3 shows a Find task example.
184
Figure 2: Data Collection Experiment
Figure 3: Find Task Example
3.1 Annotation
We devised a multidimensional annotation scheme
since we are interested in investigating the role
played in the interaction by modalities different
from speech. Our annotation scheme comprises
three main components: the multimodal event an-
notation, which includes annotating for pointing
gestures, haptic-ostensive actions, their features,
and their relationships to utterances; the dialogue
act annotation; and the referential expression an-
notations already described in (Chen et al, 2011;
Chen and Di Eugenio, 2012).
3.1.1 Multimodal Event Annotation
To study the roles played by different sorts of mul-
timodal actions, and how they contribute to the
flow of the dialogue, pointing gestures, Haptic-
Ostensive (H-O) actions, and the relations among
them have been annotated on the Find subcorpus.
The Find subcorpus contains 137 Find tasks, col-
lected from the dialogues of 19 pairs of subjects
from the larger corpus. 1 The multimodal annota-
1One pair of subjects was excluded, because ELD ap-
peared confused. Our goal was to recruit elderly subjects with
tion tool Anvil (Kipp, 2001) was used to transcribe
all the utterances, and to annotate for all categories
described in this paper. Each annotation category
is an annotation group in Anvil. For each subject,
one track is defined for each annotation group, for
a total of 4 tracks per subject in Anvil.
Pointing gestures are used naturally when peo-
ple refer to a far away object. We define a pointing
gesture as a hand gesture without physical contact
with the target. Our definition of pointing gesture
does not include head or other body part move-
ments used to indicate targets. Our corpus in-
cludes very few occurrences of those; additionally,
our collaborators in the RoboHelper project focus
on recognizing hand gestures. We have identified
two types of pointing gestures. The first is, point-
ing gestures with an identifiable target, which is
usually indicated by a short time stable hand point-
ing. The other type is without a fixed target. It
usually happens when the subject points to several
targets in a short time, or the subject just points to
a large space area.
For a pointing gesture, we mark two attributes:
the time span and the target. The time span of
a pointing gesture starts when the subject initi-
ates the hand movement, ends when the subject
starts to draw the hand back. We have devised a
Referring Index System (Chen and Di Eugenio,
2012) to mark the different types of targets: sin-
gle identifiable target, multiple identifiable targets
and unidentifiable target.
During Find tasks, subjects need to physically
interact with the objects, e.g. they need to open
cabinets to get plates, to put a pot on the stove etc.
Those physical contact actions often perform a re-
ferring function as well, either adding new enti-
ties to the discourse model, or referring to an al-
ready established referent. For example, in Fig-
ure 3, the action [Touch(Hel,Drawer1)] that ac-
companies Utt4 disambiguates This by referring to
Drawer1, tantamount to a pointing gesture; con-
versely, the action [Takeout(HEL,spoon1)] associ-
ated with Utt8 establishes a referent for spoon1.
Following (Foster et al, 2008), we label Haptic-
Ostensive (H-O) those actions that involve physi-
cal contact with an object, and that can at the same
time perform a referring function. Note that target
objects here exclude the partner?s body parts, as
when HEL helps ELD get up from a chair.
No existing work that we know of identifies
intact cognitive functions, but this subject was an exception.
185
types of H-O actions. Hence, we had to define our
own categories, based on the following two princi-
ples: (1) The H-O types must be grounded in our
data, namely, the definitions are empirically based:
these H-O actions are frequently observed in the
corpus. (2) They are within the scope of what our
collaborators can recognize from the haptic sig-
nals. The five H-O action types we defined are:
? Touch: when the subject only touches the
targets, no immediate further actions are per-
formed
? MANIP-HOLD: when the subject takes out
or picks up an object and holds it stably for a
short period of time
? MANIP-NO-HOLD: when the subject takes
out or picks up an object, but without explic-
itly showing it to the other subject
? Open: starts when the subject has physical
contact with the handle of the fridge, a cabi-
net or a drawer, and starts to pull; ends when
the physical contact is off
? Close: when the subject has physical con-
tact with the handle of the fridge, a cabinet
or a drawer, and starts to push; ends when the
physical contact is off
For H-O action annotation, three attributes are
marked: time span, target and action type. The
?Target? attribute is similar to the ?Target? at-
tribute in pointing gesture annotation. Since H-
O actions are more accurate than pointing ges-
tures (Foster et al, 2008), the targets are all iden-
tifiable.
Table 1 provides distributions of the length in
seconds for different types of events in the Find
corpus. Table 2 shows the counts of different
events divided by type of participant. From these
two tables, it is apparent that:
? Pointing gestures and H-O actions were fre-
quently used: their total corresponds to 61%
of the number of utterances
? Utterances are short: only 1.7?, and 4.2
words on average
? ELD performed 66% of pointing gestures,
and HEL 97.5% of H-O actions
Multimodal Event Relation Annotation.
Pointing gestures and H-O actions can accompany
an utterance, e.g. see move 2 in Figure 3: HEL
Utterances Pointing H-O Actions Total
2555? 571? 1088? 4377?
Table 1: Find Subcorpus: Length in seconds
ELD HEL Total
Utterances 756 760 1516
Words 3612 2981 6593
Pointing 219 113 332
H-O Actions 15 582 597
Table 2: Find Subcorpus: Counts
asks ?Down there? while pointing to a drawer;
or can be used independently, e.g. see move 6
in Figure 3: HEL does not utter any words, but
opens the drawer after ELD confirms that is
the right drawer with ?Uh-huh?. In the latter
case, HEL used an action to respond to ELD.
Pointing gestures and H-O actions are followed
by utterances as well, e.g. move 11 in Figure 3:
after HEL opens a drawer, ELD says ?Yes, there
it is?.
To understand how pointing gestures and H-O
actions participate in the dialogues and how they
interact with utterances, we further annotated the
relationship between utterances, pointing gestures
and H-O actions. Just using timespans is not
sufficient. It is not necessarily the case that utter-
ance U is associated with gesture / H-O action G
if their timespans overlap. This type of annotation
is purely local: the fact that turns 2-5 in Figure 3
confirm which drawer to open, would be captured
at the dialogue game level.
First, we assign to each utterance, pointing ges-
ture and H-O action a unique event index, so that
we can refer to these events with their indices. For
pointing gestures and H-O actions, we define two
more attributes: ?associates? and ?follows?. If a
pointing gesture or H-O action is associated with
an utterance, the ?associates? value will be the in-
dex of that utterance; by default, the ?associates?
value is empty. If a pointing gesture or H-O ac-
tion independently follows an utterance, the ?fol-
lows? value will be that utterance?s index. E.g.,
for move 6 in Figure 3, we mark the H-O action
?Open? with ?follows [5]?.
For utterances, we only mark the ?follows? at-
tribute. If an utterance directly follows a point-
ing gesture or H-O action, we use the index of the
pointing gesture or H-O action as the ?follows?
value. By default, the ?follows? attribute of an ut-
terance is empty. It means that an utterance fol-
186
lows its immediate previous utterance.
We define a move as any combination of related
utterances, pointing gestures and H-O actions, per-
formed by the same subject. On the basis of the
event relation annotations, we can compute the di-
alogue?s move flow using the following algorithm.
1. Order all the utterances in a Find task session
by the utterance start time
2. Until all the utterances are processed, for
each unprocessed utterance ui:
(a) If ui follows a pointing gesture or H-O
action, that pointing gesture or H-O ac-
tion forms a new move mk; add mk to
the sequence before ui
(b) Find all the pointing gestures and H-
O actions labelled as associates of ui.
These events form the movemi together
with ui
(c) Recursively find the events which fol-
low the last generated move, together
with all their associated events to form
another move
This algorithm computes 1791 moves, as shown in
Table 3. More than 90% of pointing gestures are
used with utterances. Only 377 out of 596 H-O ac-
tions are included in themoves, mostly because the
H-O action ?Close? frequently follows an ?Open?
action (these cases are not detected by the algo-
rithm, because they don?t advance the dialogue).
ELD HEL Total
Utterances 545 507 1052
Pointing 9 11 20
H-O 5 213 218
Utterance&Pointing 209 100 309
Utterance&H-O 2 153 155
Total 770 984 1754
Table 3: Moves Statistics in Find Corpus
3.1.2 Dialogue Act Annotation
Since the Find corpus is task-oriented in nature,
we built on the dialogue act inventory of HCRC
MapTask, a well-known task oriented corpus (An-
derson et al, 1991). The MapTask tag set con-
tains 11 moves:2 instruct, explain, check, align,
query-w, query-yn; acknowledge, reply-y, reply-n,
reply-w, clarify. However, this inventory of DAs
does not cover utterances that are used to respond
2A twelfth move, Ready, does not appear in our corpus.
to gestures and actions, such as Utt.11 in Figure 3.
The semantics of the reply-{y/n/w} tags does not
cover these situations. Hence, we devised three
more tags, which apply only to statements that fol-
low a move composed exclusively of a gesture or
an action (in the sense of ?follow? just discussed):
? state-y: a statement which conveys ?yes?,
such as Utt.11 in Figure 3.
? state-n: a statement which conveys ?no?, e.g.
if Utt.11 had been Wait, try the third drawer.
? state: still a statement , but not conveying ac-
ceptance or rejection, e.g. So we got the soup.
Hence, the DAs in {state-y, state-n, state} are
used to tag responses to actions, and the DAs
in {reply-y, reply-n, reply-w} are used to tag re-
sponses to utterances. Table 4 shows the distribu-
tion of DAs by subject.
Dialogue Act ELD HEL Total Ratio
Instruct 295 19 314 20.7%
Acknowledge 22 186 208 13.7%
Reply-y 179 3 182 12.0%
Check 1 155 156 10.3%
Query-yn 23 133 156 10.3%
Query-w 3 144 147 9.7%
Reply-w 132 4 136 9.0%
State-y 40 36 76 5.0%
State-n 16 50 66 4.4%
Reply-n 27 9 36 2.4%
State 7 15 22 1.5%
Explain 10 4 14 0.9%
Align 1 2 3 0.3%
Total 756 760 1516 100%
Table 4: Dialogue Act Counts in Find Corpus
Intercoder Agreement. In order to verify the
reliability of our annotations, we double coded
15% of the data for pointing gestures, H-O actions
and DAs. These are the dialogues from 3 pairs of
subjects, and contain 22 Find tasks. Because the
pointing gestures and H-O actions are time span
based, when we calculate agreement, we use an
overlap based approach. If the two annotations
from the two coders overlap by more than 50% of
the event length, and the other attributes are the
same, we count this as a match. We used ? to
measure the reliability of the annotation (Cohen,
1960). We obtained reasonable values: for point-
ing gestures, ?=0.751, for H-O actions, ?=0.703,
and for DAs, ?=0.789.
187
4 Experimental Setup
We ran experiments classifying the DA tag for the
current utterance. We employ supervised learn-
ing approaches, specifically: Conditional Random
Field (CRF) (Lafferty et al, 2001), Maximum En-
tropy (MaxEnt), Naive Bayes (NB), and Decision
Tree (DT). These algorithms are widely used for
DA classification (Sridhar et al, 2009; Ivanovic,
2008; Ha et al, 2012; Kim et al, 2012). We
used Mallet (McCallum, 2002) to build CRF mod-
els. MaxEnt models were built using the Max-
Ent 3 package from the Apache OpenNLP pack-
age. Naive Bayes and Decision Tree models were
built with theWeka (Hall et al, 2009) package (for
decision trees, we used the J48 implementation).
All the results we will show below were obtained
using 10 fold cross validation.
4.1 Features
Among our goals were not only to obtain effec-
tive classifiers, but also to investigate which kind
of features are most effective for our tasks. As
a consequence, beyond textual features and dia-
logue history features, we experimented with mul-
timodal features extracted from other modalities,
utterance features, and automatically inferred dia-
logue game features.
Textual features (TX) are the most widely used
features for DA classification (Stolcke et al, 2000;
Bangalore et al, 2008; Sridhar et al, 2009; Di Eu-
genio et al, 2010a; Kim et al, 2010; Boyer et al,
2011; Ha et al, 2012; Kim et al, 2012). The tex-
tual features we use include lexical, syntactic, and
heuristic features.
? Lexical features: Unigrams of the words and
part-of-speech tags in the current utterance.
The words used in the features are processed
using the morphology tool from the Stanford
parser (De Marneffe and Manning, 2008).
? Syntactic features: The top node and its
first two child nodes from the sentence parse
tree. If an utterance contains multiple sen-
tences, we use the last sentence. Sentences
are parsed using the Stanford parser.
? Number of sentences and number of words in
the utterance. We use Apache OpenNLP li-
brary 4 to detect sentences and tokenize them.
3http://maxent.sourceforge.net
4http://opennlp.apache.org/
? Heuristic features: whether an utterance con-
tains WH words (e.g. what, where), whether
an utterance contains yes/no words (e.g. yes,
no, yeah, nope).
Utterance features (UT) are extracted from
the current utterance?s meta information. Previ-
ous research showed that utterance meta informa-
tion such as the utterance speaker can help classify
DAs (Ivanovic, 2008; Kim et al, 2010).
? The actor of the utterance
? The time length of the utterance
? The distance of the current utterance from the
beginning of the dialogue
The pointing gesture feature (PT) indicates
whether the actor of the current utterance ui is
making a pointing gesture G, i.e., whether G is
associated with ui, and hence, part of move mi.
Haptic-Ostensive features (H-O) indicate
whether the actor of the current utterance ui is per-
forming any H-O action G i.e., whether G is asso-
ciated with ui, and hence, part of move mi; and
the type of that action, if yes.
Location features (LO) include the locations
of the two actors, whether they are in the same
location, whether the actor of the current utter-
ance changes the location during the utterance.
Since we do not have precise measurement of sub-
jects? locations, we annotate approximate loca-
tions by dividing the apartment into four large ar-
eas: kitchen, table, lounge and bed.
The dialogue game feature (DG) models hi-
erarchical dialogue structure. Some previous re-
search on DA classification has shown that hier-
archical dialogue structure encoded via the no-
tion of conversational games (Carlson, 1983) sig-
nificantly improves DA classification (Hastie et
al., 2002; Sridhar et al, 2009; Di Eugenio et al,
2010a). In MapTask, a game is defined as a se-
quence of moves starting with an initiation (in-
struct, explain, check, align, query-yn, query-w)
and encompassing all utterances up until the pur-
pose of the game has been fulfilled, or abandoned.
In the Find corpus, dialogue games have not been
annotated. In order to use the DG feature, we use
a just-in-time approach to infer dialogue games.
For each dialogue, we maintain a stack for dia-
logue games. When an utterance is classified as
an initiating DA tag, we assume the dialogue has
188
entered a new dialogue game, and push the DA la-
bel as the dialog game to the top of the stack. The
DG feature value is the top element of the stack.
The dialogue game feature is always inferred at
run time during classification process, just before
an utterance is being processed. Hence, when we
classify the DA for the current utterance ui, the
DG value that we use is the closest preceding ini-
tiating DA.
Dialogue history features (DH) model what
happened before the current utterance (Sridhar et
al., 2009; Di Eugenio et al, 2010a). We encode:
? The previous move?s actor
? Whether the previous move has the same ac-
tor as the current move
? The type of the previous move; if it is an ut-
terance, its DA tag; if it is an H-O action, the
type of H-O action
5 DA Classification Experiments
We ran the DA classification experiments with
three goals. First, we wanted to assess the ef-
fectiveness of different types of features, espe-
cially, the effectiveness of gesture, H-O action, lo-
cation and dialogue game features. Second, we
wanted to compare the performances of different
machine learning algorithms on such a multimodal
dialogue dataset. Third, we wanted to investigate
the performances of different algorithms in the on-
line and offline experiment settings. The DA clas-
sification task could be treated as a sequence label-
ing problem (Stolcke et al, 2000). However, dif-
ferent from other sequence labeling problems such
as part-of-speech tagging, a dialogue system can-
not wait until the whole dialogue ends to classify
the current DA. A dialogue system needs online
DA classification models to classify the DAs when
a new utterance is processed by the system. There
are two differences between online and offline DA
classification modes. First, when we generate the
dialogue history and dialogue game features, we
use the previously classified DA tag results for on-
line mode, while we use the gold-standard DA tags
for offline mode. Second, MaxEnt (using beam
search) and CRF evaluate and classify all the ut-
terances in a dialogue at the same time in offline
mode; however in online mode, MaxEnt and CRF
can only work on the partial sequence up to the
utterance to classify. Whereas this may sound ob-
vious, it explains why the performance of these
classifiers may be even more negatively affected
in online mode with respect to their offline perfor-
mance, as compared to other classifiers. We will
see that indeed this will happen for CRF, but not
for MaxEnt.
To evaluate feature effectiveness, we group the
features into seven groups: textual features (TX),
utterance features (UT), pointing gesture fea-
ture (PT), H-O action features (H-O), location
features (LO), dialogue game feature (DG), dia-
logue history features (DH). Then we generate all
the combinations of feature groups to run exper-
iments. For each classification algorithm, we ran
10-fold cross-validation experiments, for each fea-
ture group combination, in both online and offline
mode. It would be impossible to report all our re-
sults. Similarly to (Ha et al, 2012), we report our
results with single feature groups and incremen-
tal feature group combinations, as shown in Ta-
ble 5. Whereas all combinations were tried, the
omitted results do not shed any additional light on
the problem. The majority baseline, which al-
ways assigns the most frequent tag to every utter-
ance, has an accuracy of 20.3%.
The CRF offline model performs best, which
confirms the results of (Kim et al, 2010; Kim
et al, 2012). This is due to the strong correla-
tion between dialogue history features (DH) and
the states of the CRF. In online mode, when there
is noise in the previous DA tags, the CRF?s per-
formance drops significantly (p?.005, using ?2).
A significant drop in performance from offline to
online mode also happens to NB (p?.005) and
DT (p<.025). MaxEnt performs very stably, the
best online model performs only .015 worse than
the best offline model. The best MaxEnt offline
model beats the other algorithms? best models ex-
cept CRF, while the MaxEnt online model outper-
forms all the other algorithms? online models. Our
results thus demonstrate that MaxEnt works best
for online DA classification on our data.
As concerns features, for online models, textual
features (TX) are the most predictive as a feature
type used by itself. When we add pointing ges-
ture (PT), H-O features (H-O) and location fea-
tures (LO) together to textual features, we notice
a significant performance improvement for most
models (except CRF models). For MaxEnt, which
gives the best results for online models, none of
the gesture, H-O action and location features alone
significantly improve the results, but all three to-
189
Features CRF MaxEnt NB DT
Offline Online Offline Online Offline Online Offline Online
1. TX (Textual) .654 .641 .630 .630 .449 .453 .450 .450
2. UT (Utterance) .506 .376 .353 .353 .417 .417 .392 .392
3. PT (Pointing) .225 .155 .210 .210 .212 .212 .212 .212
4. H-O (Haptic-Ostensive) .187 .147 .237 .237 .243 .243 .212 .212
5. LO (Location) .259 .176 .264 .264 .259 .259 .265 .265
6. DG (Dialogue Game) .737 .136 .305 .189 .212 .212 .212 .212
7. DH (Dialogue History) .895 .119 .480 .302 .478 .284 .471 .294
8. TX+PT .654 .651 .639 .639 .453 .453 .450 .450
9. TX+PT+H-O .670 .649 .637 .637 .456 .456 .449 .449
10. TX+PT+H-O+LO .648 .645 .657? .657? .523? .523? .536? .536?
11. TX+PT+H-O+LO+UT .668 .612 .685 .685 .563 .563 .568 .568
12. TX+PT+H-O+LO+UT+DG .770?? .528 .722?? .709?? .566 .591?? .576 .607??
13. TX+PT+H-O+LO+UT+DG+DH .847? .475 .757? .742? .635? .606 .671? .627
Table 5: Dialogue Act Classification Accuracy: * indicates significant improvement after adding PT+H-
O+LO to TX (cf. lines 1 and 10); ** indicates significant improvement after adding DG to TX+PT+H-
O+LO+UT (cf. lines 11 and 12); ?indicates significant improvement after adding DH to TX+PT+H-
O+LO+UT+DG (cf. lines 12 and 13); bold font indicates the feature group set giving best performance
for each column.
gether do. This confirms the finding of (Ha et al,
2012) that non-verbal features help DA classifica-
tion. To assess which feature is the most important
among those three non-verbal features, we exam-
ined the experiment results with a leave-one-out
strategy, that is for each classifier in offline and
online modes, we leave one of the gesture, H-O
and location features out from the full experiment
feature set (TX+PT+H-O+LO+UT+DG+DH). No
significant difference was discovered.
When the dialogue game features (DG) are
added to the models, performance increases sig-
nificantly for CRF offline model (p<.005), Max-
Ent offline (p<.005) and online (p<.05) mod-
els, NB online model (p<.05) and DT online
model (p<.005). It confirms previous findings, in-
cluding by our group (Di Eugenio et al, 2010a),
that dialogue game features (DG) play a very im-
portant role in DA classification, even via the sim-
ple approximation we used. When the dialogue
history features (DH) are added to the models,
performance increased significantly for all the of-
fline models and the MaxEnt online model, with
p<.005. This confirms previous findings that dia-
logue history helps with DA classification.
6 Conclusions and Future Work
In this paper we described our multimodal cor-
pus which is annotated with multimodal informa-
tion (pointing gestures and H-O actions) and dia-
logue acts. Our corpus analysis shows that peo-
ple actively use pointing gestures and H-O actions
alongside utterances in dialogues. The function of
H-O actions in dialogue had hardly been studied
before. Our experiments show that MaxEnt per-
forms best for the online DA classification task.
Multimodal and dialogue game features both im-
prove DA classification.
Short-term future work includes manual anno-
tation for dialogue games, in the hope that more
accurate dialogue game features may further im-
prove DA classification. Longer term future work
includes prediction of the specific next move ? the
specific DA and/or the specific gesture, pointing or
H-O action. We have now developed some of the
building blocks of an information-state based mul-
timodal dialogue manager. The major aspects we
still need to address are defining the information-
state for the Find task, and developing rules to up-
date the information-state with multimodal infor-
mation, the classified DAs, and the co-reference
resolution models we already built (Chen et al,
2011; Chen and Di Eugenio, 2012). Once the
information-state component is in place, we can
expect better and more detailed predictions.
Acknowledgments
This work is supported by award IIS 0905593
from the National Science Foundation. Thanks
to the other members of the RoboHelper project,
for their many contributions, especially to the data
collection effort.
190
References
Anne H. Anderson, Miles Bader, Ellen Gurman Bard,
Elizabeth Boyle, Gwyneth Doherty, Simon Gar-
rod, Stephen Isard, Jacqueline Kowtko, Jan McAl-
lister, Jim Miller, Catherine Sotillo, and Henry S.
1991. The HCRC Map Task corpus. Language and
Speech, 34(4):351.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2008. Learning the structure of
task-driven human?human dialogs. IEEE Transac-
tions on Audio, Speech, and Language Processing,
16(7):1249?1259.
K.E. Boyer, J.F. Grafsgaard, E.Y. Ha, R. Phillips, and
J.C. Lester. 2011. An affect-enriched dialogue act
classification model for task-oriented dialogue. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 1190?1199.
Association for Computational Linguistics.
Jean Carletta. 2007. Unleashing the killer corpus:
experiences in creating the multi-everything AMI
meeting corpus. Language Resources and Evalua-
tion, 41(2):181?190.
Lauri Carlson. 1983. Dialogue games: An approach to
discourse analysis. D. Reidel Publishing Company.
Lin Chen and Barbara Di Eugenio. 2012. Co-reference
via pointing and haptics in multi-modal dialogues.
In The 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies. The As-
sociation for Computational Linguistics.
Lin Chen, Anruo Wang, and Barbara Di Eugenio.
2011. Improving pronominal and deictic co-
reference resolution with multi-modal features. In
Proceedings of SIGdial 2011, the 12th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 307?311, Portland, Oregon, June.
Association for Computational Linguistics.
J. Cohen. 1960. A coefficient of agreement for nomi-
nal scales. Educational and psychological measure-
ment, 20(1):37?46.
Marie-Catherine De Marneffe and Christopher D.
Manning. 2008. The Stanford typed dependencies
representation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1?8. Association for Com-
putational Linguistics.
Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin.
2010a. Dialogue act classification, higher order di-
alogue structure, and instance-based learning. Dia-
logue & Discourse, 1(2):1?24.
Barbara Di Eugenio, Milos? Z?efran, Jezekiel Ben-
Arie, Mark Foreman, Lin Chen, Simone Franzini,
Shankaranand Jagadeesan, Maria Javaid, and Kai
Ma. 2010b. Towards Effective Communication
with Robotic Assistants for the Elderly: Integrating
Speech, Vision and Haptics. In Dialog with Robots,
AAAI 2010 Fall Symposium, Arlington, VA, USA,
November.
M.E. Foster, E.G. Bard, M. Guhe, R.L. Hill, J. Ober-
lander, and A. Knoll. 2008. The roles of haptic-
ostensive referring expressions in cooperative, task-
based human-robot dialogue. In Proceedings of the
3rd ACM/IEEE International Conference on Human
Robot Interaction, pages 295?302. ACM.
Simone Franzini and Jezekiel Ben-Arie. 2012. Speech
recognition by indexing and sequencing. Interna-
tional Journal of Computer Information Systems and
Industrial Management Applications, 4:358?365.
David Graff, Alexandra Canavan, and George Zip-
perlen. 1998. Switchboard-2 Phase I.
Eun Young Ha, Joseph F. Grafsgaard, Christopher
Mitchell, Kristy Elizabeth Boyer, and James C.
Lester. 2012. Combining verbal and nonverbal
features to overcome the ?information gap? in task-
oriented dialogue. In Proceedings of SIGdial 2012,
the 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 247?256,
Seoul, South Korea, July. Association for Computa-
tional Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Helen Wright Hastie, Massimo Poesio, and Stephen Is-
ard. 2002. Automatically predicting dialogue struc-
ture using prosodic features. Speech Communica-
tion, 36(1?2):63?79.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Maria Javaid and Milos? Z?efran. 2012. Interpreting
communication through physical interaction during
collaborative manipulation. Draft, October.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of EMNLP 2010, the Con-
ference on Empirical Methods in Natural Language
Processing, pages 862?871. Association for Com-
putational Linguistics.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats. In Proceedings of the 26th Pacific Asia
Conference on Language, Information, and Compu-
tation, pages 463?472, Bali, Indonesia, November.
Faculty of Computer Science, Universitas Indone-
sia.
Michael Kipp. 2001. Anvil-a generic annotation tool
for multimodal dialogue. In Proceedings of the
7th European Conference on Speech Communica-
tion and Technology, pages 1367?1370.
191
Kristine M. Krapp. 2002. The Gale Encyclopedia of
Nursing & Allied Health. Gale Group, Inc. Chapter
Activities of Daily Living Evaluation.
John D. Lafferty, Andrew McCallum, and Fer-
nando C.N. Pereira. 2001. Conditional Random
Fields: Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282?289. Morgan Kaufmann Publishers
Inc.
L. Levin, A. Thyme?-Gobbel, A. Lavie, K. Ries, and
K. Zechner. 1998. A discourse coding scheme for
conversational Spanish. In Fifth International Con-
ference on Spoken Language Processing.
K. Ma and J. Ben-Arie. 2012. Multi-view multi-
class object detection via exemplar compounding.
In IEEE-IAPR 21st International Conference on
Pattern Recognition (ICPR 2012), Tsukuba, Japan,
November.
Andrew Kachites McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit.
E. Shriberg, R. Dhillon, S.V. Bhagat, J. Ang, and
H. Carvey. 2004. The ICSI Meeting Recorder
Dialog Act (MRDA) Corpus. In Proceedings of
5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, MA, April 30-May 1.
V.K.R. Sridhar, S. Bangalore, and S. Narayanan. 2009.
Combining lexical, syntactic and prosodic cues for
improved online dialog act tagging. Computer
Speech & Language, 23(4):407?422.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
Matthias Zimmermann, Andreas Stolcke, and Eliza-
beth Shriberg. 2006. Joint segmentation and clas-
sification of dialog acts in multiparty meetings. In
ICASSP 2006, the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume 1. IEEE.
192
