Proceedings of NAACL HLT 2007, Companion Volume, pages 205?208,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Comparing Wikipedia and German Wordnet
by Evaluating Semantic Relatedness on Multiple Datasets
Torsten Zesch and Iryna Gurevych and Max M?hlh?user
Ubiquitous Knowledge Processing Group, Telecooperation Division
Darmstadt University of Technology, D-64289 Darmstadt, Germany
{zesch,gurevych,max} (at) tk.informatik.tu-darmstadt.de
Abstract
We evaluate semantic relatedness mea-
sures on different German datasets show-
ing that their performance depends on: (i)
the definition of relatedness that was un-
derlying the construction of the evalua-
tion dataset, and (ii) the knowledge source
used for computing semantic relatedness.
We analyze how the underlying knowl-
edge source influences the performance
of a measure. Finally, we investigate the
combination of wordnets andWikipedia to
improve the performance of semantic re-
latedness measures.
1 Introduction
Semantic similarity (SS) is typically defined via the
lexical relations of synonymy (automobile ? car)
and hypernymy (vehicle ? car), while semantic re-
latedness (SR) is defined to cover any kind of lexi-
cal or functional association that may exist between
two words. Many NLP applications, like sense tag-
ging or spelling correction, require knowledge about
semantic relatedness rather than just similarity (Bu-
danitsky and Hirst, 2006). For these tasks, it is not
necessary to know the exact type of semantic rela-
tion between two words, but rather if they are closely
semantically related or not. This is also true for the
work presented herein, which is part of a project
on electronic career guidance. In this domain, it
is important to conclude that the words ?baker? and
?bagel? are closely related, while the exact type of a
semantic relation does not need to be determined.
As we work on German documents, we evalu-
ate a number of SR measures on different German
datasets. We show that the performance of mea-
sures strongly depends on the underlying knowledge
source. While WordNet (Fellbaum, 1998) mod-
els SR, wordnets for other languages, such as the
German wordnet GermaNet (Kunze, 2004), contain
only few links expressing SR. Thus, they are not
well suited for estimating SR.
Therefore, we apply theWikipedia category graph
as a knowledge source for SR measures. We show
that Wikipedia based SR measures yield better cor-
relation with human judgments on SR datasets than
GermaNet measures. However, using Wikipedia
also leads to a performance drop on SS datasets,
as knowledge about classical taxonomic relations
is not explicitly modeled. Therefore, we combine
GermaNet with Wikipedia, and yield substantial im-
provements over measures operating on a single
knowledge source.
2 Datasets
Several German datasets for evaluation of SS or SR
have been created so far (see Table 1). Gurevych
(2005) conducted experiments with a German trans-
lation of an English dataset (Rubenstein and Goode-
nough, 1965), but argued that the dataset (Gur65)
is too small (it contains only 65 noun pairs), and
does not model SR. Thus, she created a German
dataset containing 350 word pairs (Gur350) con-
taining nouns, verbs and adjectives that are con-
nected by classical and non-classical relations (Mor-
ris and Hirst, 2004). However, the dataset is bi-
ased towards strong classical relations, as word
pairs were manually selected. Thus, Zesch and
Gurevych (2006) semi-automatically created word
pairs from domain-specific corpora. The resulting
ZG222 dataset contains 222 word pairs that are con-
nected by all kinds of lexical semantic relations.
Hence, it is particularly suited for analyzing the ca-
pability of a measure to estimate SR.
205
CORRELATION r
DATASET YEAR LANGUAGE # PAIRS POS TYPE SCORES # SUBJECTS INTER INTRA
Gur65 2005 German 65 N SS discrete {0,1,2,3,4} 24 .810 -
Gur350 2006 German 350 N, V, A SR discrete {0,1,2,3,4} 8 .690 -
ZG222 2006 German 222 N, V, A SR discrete {0,1,2,3,4} 21 .490 .647
Table 1: Comparison of datasets used for evaluating semantic relatedness.
3 Semantic Relatedness Measures
Semantic wordnet based measures Lesk (1986)
introduced a measure (Les) based on the number of
word overlaps in the textual definitions (or glosses)
of two terms, where higher overlap means higher
similarity. As GermaNet does not contain glosses,
this measure cannot be employed. Gurevych (2005)
proposed an alternative algorithm (PG) generating
surrogate glosses by using a concept?s relations
within the hierarchy. Following the description in
Budanitsky and Hirst (2006), we further define sev-
eral measures using the taxonomy structure.
simPL = l(c1, c2)
simLC = ? log
l(c1, c2)
2? depth
simRes = IC(ci) = ? log(p(lcs(c1, c2)))
distJC = IC(c1) + IC(c2)? 2IC(lcs(c1, c2))
simLin = 2?
IC(lcs(c1, c2))
IC(c1) + IC(c2)
PL is the taxonomic path length l(c1, c2) between
two concepts c1 and c2. LC normalizes the path
length with the depth of the taxonomy. Res com-
putes SS as the information content (IC) of the low-
est common subsumer (lcs) of two concepts, while
JC combines path based and IC features.1 Lin is
derived from information theory.
Wikipedia based measures For computing the
SR of two words w1 and w2 using Wikipedia, we
first retrieve the articles or disambiguation pages
with titles that equal w1 and w2 (see Figure 1). If
we hit a redirect page, we retrieve the correspond-
ing article or disambiguation page instead. In case
of an article, we insert it into the candidate article
set (A1 for w1, A2 for w2). In case of a disam-
biguation page, the page contains links to all en-
coded word senses, but it may also contain other
1Note that JC returns a distance value instead of a similarity
value resulting in negative correlation with human judgments.
Wo
rd w
2
Wo
rd w
1
1
Can
dida
te 
arti
cle sets
Can
dida
te 
arti
cle pair
s
3
a
Sem
ant
ic R
elat
edn
ess
Sem
ant
ic re
late
dne
ss o
f (w
1, w
2)
Ma
x
a
A 2
A 1
2
a
1
aArt
icle
Dis
am
big.
 pa
ge
Red
irec
ts
Figure 1: Steps for computing SR using Wikipedia.
links. Therefore, we only consider links conforming
to the pattern ?Title_(DisambiguationText)?2 (e.g.
?Train_(roller coaster)?). Following all such links
gives the candidate article set. If no disambiguation
links are found, we take the first link on the page, as
most important links tend to come first. We add the
corresponding articles to the candidate set. We form
pairs from each candidate article ai ? A1 and each
article aj ? A2. We then compute SR(ai, aj) for
each pair. The output of the algorithm is the maxi-
mum SR value maxai?A1,aj?A2(SR(ai, aj)).
3
As most SR measures have been developed for
taxonomic wordnets, porting them to Wikipedia re-
quires some modifications (see Figure 2). Text over-
lap measures can be computed based on the article
text, while path based measures operate on the cate-
gory graph. We compute the overlap between article
2?_(DisambiguationText)? is optional.
3Different from our approach, Strube and Ponzetto (2006)
use a disambiguation strategy that returns only a single candi-
date article pair. This unnecessarily limits a measure?s potential
to consider SR between all candidate article pairs. They also
limit the search for a lcs to a manually specified threshold of 4.
206
Wikip
edia
 
cate
gory 
grap
h
A B
A D
B B
B D
C B
C D
Cat
ego
ry pair
sA
C
B
B
D
Cat
ego
ries
 
of a
rticl
e 1
A
H
G
C
E
F
B
D
Artic
le 1 Text
1
AB
C
Text
1
Text
2
Text
2
First 
para
grap
h
Full te
xt
Artic
le 2 Text
2
BD
Text
1
LCS
(B,D
)
Cat
ego
ry gra
ph ba
sed
Text 
bas
ed
Com
bina
tion
 
stra
tegy
Avg
Best
Candidate article pair
Cat
ego
ries
 
of a
rticl
e 2
C 2
C 1
a) b)
Figure 2: SR measures adapted on Wikipedia.
texts based on (i) the first paragraph, as it usually
contains a short gloss, and (ii) the full article text.
As Wikipedia articles do not form a taxonomy, path
based measures have to be adapted to the Wikipedia
category graph (see the right part of Figure 2). We
define C1 and C2 as the set of categories assigned to
article ai and aj , respectively. We compute the SR
value for each category pair (ck, cl) with ck ? C1
and cl ? C2. We use two different strategies to com-
bine the resulting SR values: First, we choose the
best value among all pairs (ck, cl), i.e., the minimum
for path based, and the maximum for information
content based measures. As a second strategy, we
average over all category pairs.
4 Experiments & Results
Table 2 gives an overview of our experimental re-
sults on three German datasets. Best values for each
dataset and knowledge source are in bold. We use
thePGmeasure in optimal configuration as reported
by Gurevych (2005). For the Les measure, we give
the results for considering: (i) only the first para-
graph (+First) and (ii) the full text (+Full). For the
path length based measures, we give the values for
averaging over all category pairs (+Avg), or tak-
ing the best SR value computed among the pairs
(+Best). For each dataset, we report Pearson?s cor-
relation r with human judgments on pairs that are
found in both resources (BOTH). Otherwise, the re-
sults would not be comparable. We additionally use
a subset containing only noun-noun pairs (BOTH
NN). This comparison is fairer, because article titles
in Wikipedia are usually nouns. Table 2 also gives
the inter annotator agreement for each subset. It con-
stitutes an upper bound of a measure?s performance.
Our results on Gur65 using GermaNet are very
close to those published by Gurevych (2005), rang-
ing from 0.69?0.75. For Gur350, the performance
drops to 0.38?0.50, due to the lower upper bound,
and because GermaNet does not model SR well.
These findings are endorsed by an even more sig-
nificant performance drop on ZG222. The measures
based on Wikipedia behave less uniformly. Les
yields acceptable results on Gur350, but is generally
not among the best performing measures. LC +Avg
yields the best performance on Gur65, but is outper-
formed on the other datasets by PL +Best, which
performs equally good for all datasets.
If we compare GermaNet based and Wikipedia
based measures, we find that the knowledge source
has a major influence on performance. When evalu-
ated on Gur65, that contains pairs connected by SS,
GermaNet based measures perform near the upper
bound and outperformWikipedia based measures by
a wide margin. On Gur350 containing a mix of SS
and SR pairs, most measures perform comparably.
Finally, on ZG222, that contains pairs connected by
SR, the best Wikipedia based measure outperforms
all GermaNet based measures.
The impressive performance of PL on the
SR datasets cannot be explained with the struc-
tural properties of the category graph (Zesch and
Gurevych, 2007). Semantically related terms, that
would not be closely related in a taxonomic word-
net structure, are very likely to be categorized under
the same Wikipedia category, resulting in short path
lengths leading to high SR. These findings are con-
trary to that of (Strube and Ponzetto, 2006), where
LC outperformed path length. They limited the
search depth using a manually defined threshold,
and did not compute SR between all candidate ar-
ticle pairs.
Our results show that judgments on the perfor-
mance of a measure must always be made with re-
spect to the task at hand: computing SS or SR. De-
pending on this decision, we can choose the best un-
derlying knowledge source. GermaNet is better for
207
GUR65 GUR350 ZG222
BOTH NN BOTH BOTH NN BOTH BOTH NN
# Word Pairs 53 116 91 55 45
Inter Annotator Agreement 0.80 0.64 0.63 0.44 0.43
GermaNet
PG 0.69 0.38 0.42 0.23 0.21
JC -0.75 -0.52 -0.48 -0.19 -0.25
Lin 0.73 0.50 0.50 0.08 -0.12
Res 0.71 0.42 0.42 0.10 0.13
Wikipedia
Les +First 0.16 0.36 0.32 0.01 0.11
Les +Full 0.19 0.34 0.37 0.13 0.17
PL +Avg -0.32 -0.34 -0.46 -0.36 -0.43
PL +Best -0.35 -0.42 -0.53 -0.43 -0.49
LC +Avg 0.37 0.25 0.34 0.30 0.30
LC +Best 0.21 0.12 0.21 0.15 0.12
Combination
Linear 0.77 0.59 0.60 0.38 0.43
POS - 0.55 - 0.48 -
Table 2: Correlation r of human judgments with SR measures on different datasets.
estimating SS, while Wikipedia should be used to
estimate SR. Therefore, a measure based on a single
knowledge source is unlikely to perform well in all
settings. We computed a linear combination of the
best measure from GermaNet and from Wikipedia.
Results for this experiment are labeled Linear in Ta-
ble 2. POS is an alternative combination strategy,
where Wikipedia is only used for noun-noun pairs.
GermaNet is used for all other part-of-speech (POS)
combinations. For most datasets, we find a combi-
nation strategy that outperforms all single measures.
5 Conclusion
We have shown that in deciding for a specific mea-
sure and knowledge source it is important to con-
sider (i) whether the task at hand requires SS or
SR, and (ii) which POS are involved. We pointed
out that the underlying knowledge source has a ma-
jor influence on these points. GermaNet is better
used for SS, and contains nouns, verbs, and adjec-
tives, while Wikipedia is better used for SR between
nouns. Thus, GermaNet and Wikipedia can be re-
garded as complementary. We have shown that com-
bining them significantly improves the performance
of SR measures up to the level of human perfor-
mance.
Future research should focus on improving the
strategies for combining complementary knowledge
sources. We also need to evaluate a wider range of
measures to validate our findings. As the simple PL
measure performs remarkably well, we should also
consider computing SR based on the Wikipedia arti-
cle graph instead of the category graph.
Acknowledgments
This work was supported by the German Research Foundation
under grant "Semantic Information Retrieval from Texts in the
Example Domain Electronic Career Guidance", GU 798/1-2.
References
Alexander Budanitsky and Graeme Hirst. 2006. Evaluating
WordNet-based Measures of Semantic Distance. Computa-
tional Linguistics, 32(1).
Christiane Fellbaum. 1998. WordNet An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Iryna Gurevych. 2005. Using the Structure of a Conceptual
Network in Computing Semantic Relatedness. In Proc. of
IJCNLP, pages 767?778.
Claudia Kunze, 2004. Lexikalisch-semantische Wortnetze,
chapter Computerlinguistik und Sprachtechnologie, pages
423?431. Spektrum Akademischer Verlag.
Michael Lesk. 1986. Automatic Sense Disambiguation Us-
ing Machine Readable Dictionaries: How to tell a pine cone
from an ice cream cone. In Proc. of the 5th Annual Interna-
tional Conference on Systems Documentation, pages 24?26.
Jane Morris and Graeme Hirst. 2004. Non-Classical Lexical
Semantic Relations. In Proc. of the Workshop on Computa-
tional Lexical Semantics, NAACL-HTL.
Herbert Rubenstein and John B. Goodenough. 1965. Contex-
tual Correlates of Synonymy. Communications of the ACM,
8(10):627?633.
Michael Strube and Simone Paolo Ponzetto. 2006. WikiRelate!
Computing Semantic Relatedness UsingWikipedia. In Proc.
of AAAI, pages 1219?1224.
Torsten Zesch and Iryna Gurevych. 2006. Automatically Creat-
ing Datasets for Measures of Semantic Relatedness. In Proc.
of the Workshop on Linguistic Distances, ACL, pages 16?24.
T. Zesch and I. Gurevych. 2007. Analysis of the Wikipedia
Category Graph for NLP Applications. In Proc. of the
TextGraphs-2 Workshop, NAACL-HLT, (to appear).
208
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125?128,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatically Assessing the Post Quality in Online Discussions on Software
Markus Weimer and Iryna Gurevych and Max Mu?hlha?user
Ubiquitous Knowledge Processing Group, Division of Telecooperation
Darmstadt University of Technology, Germany
http://www.ukp.informatik.tu-darmstadt.de
[mweimer,gurevych,max]@tk.informatik.tu-darmstadt.de
Abstract
Assessing the quality of user generated con-
tent is an important problem for many web
forums. While quality is currently assessed
manually, we propose an algorithm to as-
sess the quality of forum posts automati-
cally and test it on data provided by Nab-
ble.com. We use state-of-the-art classifi-
cation techniques and experiment with five
feature classes: Surface, Lexical, Syntactic,
Forum specific and Similarity features. We
achieve an accuracy of 89% on the task of
automatically assessing post quality in the
software domain using forum specific fea-
tures. Without forum specific features, we
achieve an accuracy of 82%.
1 Introduction
Web 2.0 leads to the proliferation of user generated
content, such as blogs, wikis and forums. Key prop-
erties of user generated content are: low publication
threshold and a lack of editorial control. Therefore,
the quality of this content may vary. The end user
has problems to navigate through large repositories
of information and find information of high qual-
ity quickly. In order to address this problem, many
forum hosting companies like Google Groups1 and
Nabble2 introduce rating mechanisms, where users
can rate the information manually on a scale from 1
(low quality) to 5 (high quality). The ratings have
been shown to be consistent with the user commu-
nity by Lampe and Resnick (2004). However, the
1http://groups.google.com
2http://www.nabble.com
percentage of manually rated posts is very low (0.1%
in Nabble).
Departing from this, the main idea explored in the
present paper is to investigate the feasibility of au-
tomatically assessing the perceived quality of user
generated content. We test this idea for online fo-
rum discussions in the domain of software. The per-
ceived quality is not an objective measure. Rather, it
models how the community at large perceives post
quality. We choose a machine learning approach to
automatically assess it.
Our main contributions are: (1) An algorithm for
automatic quality assessment of forum posts that
learns from human ratings. We evaluate the system
on online discussions in the software domain. (2)
An analysis of the usefulness of different classes of
features for the prediction of post quality.
2 Related work
To the best of our knowledge, this is the first work
which attempts to assess the quality of forum posts
automatically. However, on the one hand work has
been done on automatic assessment of other types of
user generated content, such as essays and product
reviews. On the other hand, student online discus-
sions have been analyzed.
Automatic text quality assessment has been stud-
ied in the area of automatic essay scoring (Valenti
et al, 2003; Chodorow and Burstein, 2004; Attali
and Burstein, 2006). While there exist guidelines
for writing and assessing essays, this is not the case
for forum posts, as different users cast their rating
with possibly different quality criteria in mind. The
same argument applies to the automatic assessment
of product review usefulness (Kim et al, 2006c):
125
Stars Label on the website Number
? Poor Post 1251
?? Below Average Post 44
? ? ? Average Post 69
? ? ?? Above Average Post 183
? ? ? ? ? Excellent Post 421
Table 1: Categories and their usage frequency.
Readers of a review are asked ?Was this review help-
ful to you?? with the answer choices Yes/No. This
is very well defined compared to forum posts, which
are typically rated on a five star scale that does not
advertise a specific semantics.
Forums have been in the focus of another track
of research. Kim et al (2006b) found that the re-
lation between a student?s posting behavior and the
grade obtained by that student can be assessed au-
tomatically. The main features used are the num-
ber of posts, the average post length and the aver-
age number of replies to posts of the student. Feng
et al (2006) and Kim et al (2006a) describe a sys-
tem to find the most authoritative answer in a fo-
rum thread. The latter add speech act analysis as a
feature for this classification. Another feature is the
author?s trustworthiness, which could be computed
based on the automatic quality classification scheme
proposed in the present paper. Finding the most au-
thoritative post could also be defined as a special
case of the quality assessment. However, it is def-
initely different from the task studied in the present
paper. We assess the perceived quality of a given
post, based solely on its intrinsic features. Any dis-
cussion thread may contain an indefinite number of
good posts, rather than a single authoritative one.
3 Experiments
We seek to develop a system that adapts to the qual-
ity standards existing in a certain user community
by learning the relation between a set of features
and the perceived quality of posts. We experimented
with features from five classes described in table 2:
Surface, Lexical, Syntactic, Forum specific and Sim-
ilarity features.
We use forum discussions from the Software cat-
egory of Nabble.com.5 The data consists of 1968
rated posts in 1788 threads from 497 forums. Posts
can be rated by multiple users, but that happens
5http://www.nabble.com/Software-f94.html
rarely. 1927 posts were rated by one, 40 by two and
1 post by three users. Table 1 shows the distribu-
tion of average ratings on a five star scale. From
this statistics, it becomes evident that users at Nab-
ble prefer extreme ratings. Therefore, we decided
to treat the posts as being binary rated.: Posts with
less than three stars are rated ?bad?. Posts with more
than three stars are ?good?.
We removed 61 posts where all ratings are ex-
actly three stars. We removed additional 14 posts
because they had contradictory ratings on the binary
scale. Those posts were mostly spam, which was
voted high for commercial interests and voted down
for being spam. Additionally, we removed 30 posts
that did not contain any text but only attachments
like pictures. Finally, we removed 331 non English
posts using a simple heuristics: Posts that contained
a certain percentage of words above a pre-defined
threshold, which are non-English according to a dic-
tionary, were considered to be non-English.
This way, we obtained 1532 binary classified
posts: 947 good posts and 585 bad posts. For each
post, we compiled a feature vector, and feature val-
ues were normalized to the range [0.0, . . . , 1.0].
We use support vector machines as a state-of-the-
art-algorithm for binary classification. For all exper-
iments, we used a C-SVM with a gaussian RBF ker-
nel as implemented by LibSVM in the YALE toolkit
(Chang and Lin, 2001; Mierswa et al, 2006). Pa-
rameters were set to C = 10 and ? = 0.1. We per-
formed stratified ten-fold cross validation6 to esti-
mate the performance of our algorithm. We repeated
several experiments according to the leave-one-out
evaluation scheme and found comparable results to
the ones reported in this paper.
4 Results and Analysis
We compared our algorithm to a majority class clas-
sifier as a baseline, which achieves an accuracy of
62%. As it is evident from table 3, most system con-
figurations outperform the baseline system. The best
performing single feature category are the Forum
specific features. As we seek to build an adaptable
system, analyzing the performance without these
features is worthwhile: Using all other features, we
6See (Witten and Frank, 2005), chapter 5.3 for an in-depth
description.
126
Feature category Feature name Description
Surface Features
Length The number of tokens in a post.
Question Frequency The percentage of sentences ending with ???.
Exclamation Frequency The percentage of sentences ending with ?!?.
Capital Word Frequency The percentage of words in CAPITAL, which is often associated with shouting.
Lexical Features
Information about
the wording of the
posts
Spelling Error Frequency The percentage of words that are not spelled correctly.3
Swear Word Frequency The percentage of words that are on a list of swear words we compiled from
resources like WordNet and Wikipedia4, which contains more than eighty words
like ?asshole?, but also common transcriptions like ?f*ckin?.
Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set
(Marcus et al, 1994). We used TreeTagger (Schmid, 1995) based on the english
parameter files supplied with it.
Forum specific
features
Properties of a post
that are only
present in forum
postings
IsHTML Whether or not a post contains HTML. In our data, this is encoded explicitly,
but it can also be determined by regular expressions matching HTML tags.
IsMail Whether or not a post has been copied from a mailing list. This is encoded
explicitly in our data.
Quote Fraction The fraction of characters that are inside quotes of other posts. These quotes are
marked explicitly in our data.
URL and Path Count The number of URLs and filesystem paths. Post quality in the software do-
main may be influenced by the amount of tangible information, which is partly
captured by these features.
Similarity features Forums are focussed on a topic. The relatedness of a post to the topic of the
forum may influence post quality. We capture this relatedness by the cosine
between the posts unigram vector and the unigram vector of the forum.
Table 2: Features used for the automatic quality assessment of posts.
achieve an only slightly worse classification accu-
racy. Thus, the combination of all other features
captures the quality of a post fairly well.
SUF LEX SYN FOR SIM Avg. accuracy
Baseline 61.82%? ? ? ? ? 89.10%? ? ? ? ? 61.82%
? ? ? ? ? 71.82%
? ? ? ? ? 82.64%
? ? ? ? ? 85.05%
? ? ? ? ? 62.01%
? ? ? ? ? 89.10%? ? ? ? ? 89.36%? ? ? ? ? 85.03%? ? ? ? ? 82.90%? ? ? ? ? 88.97%
? ? ? ? ? 88.56%? ? ? ? ? 85.12%
? ? ? ? ? 88.74%
Table 3: Accuracy with different feature sets. SUF: Surface,
LEX: Lexical, SYN: Syntax, FOR: Forum specific, SIM: simi-
larity. The baseline results from a majority class classifier.
We performed additional experiments to identify
the most important features from the Forum specific
ones. Table 4 shows that IsMail and Quote Frac-
tion are the dominant features. This is noteworthy,
as those features are not based on the domain of dis-
cussion. Thus, we believe that these features will
perform well in future experiments on other data.
ISM ISH QFR URL PAC Avg. accuracy? ? ? ? ? 85.05%? ? ? ? ? 73.30%
? ? ? ? ? 61.82%
? ? ? ? ? 73.76%
? ? ? ? ? 61.29%
? ? ? ? ? 61.82%
? ? ? ? ? 74.41%? ? ? ? ? 85.05%? ? ? ? ? 73.30%? ? ? ? ? 85.05%? ? ? ? ? 85.05%? ? ? ? ? 84.99%? ? ? ? ? 85.05%
Table 4: Accuracy with different forum specific features.
ISM: IsMail, ISH: IsHTML, QFR: QuoteFraction, URL: URL-
Count, PAC: PathCount.
Error Analysis Table 5 shows the confusion ma-
trix of the system using all features. Many posts
that were misclassified as good ones show no ap-
parent reason to be classified as bad posts to us. The
understanding of their rating seems to require deep
knowledge about the specific subject of discussion.
The few remaining posts are either spam or rated
negatively to signalize dissent with the opinion ex-
pressed in the post. Posts that were misclassified as
bad ones often contain program code, digital signa-
tures or other non-textual parts in the body. We plan
to address these issues with better preprocessing in
127
true good true bad sum
pred. good 490 72 562
pred. bad 95 875 970
sum 585 947 1532
Table 5: Confusion matrix for the system using all features.
the future. However, the relatively high accuracy al-
ready achieved shows that these issues are rare.
5 Conclusion and Future Work
Assessing post quality is an important problem for
many forums on the web. Currently, most forums
need their users to rate the posts manually, which is
error prone, labour intensive and last but not least
may lead to the problem of premature negative con-
sent (Lampe and Resnick, 2004).
We proposed an algorithm that has shown to be
able to assess the quality of forum posts. The al-
gorithm applies state-of-the-art classification tech-
niques using features such as Surface, Lexical, Syn-
tactic, Forum specific and Similarity features to
do so. Our best performing system configuration
achieves an accuracy of 89.1%, which is signifi-
cantly higher than the baseline of 61.82%. Our ex-
periments show that forum specific features perform
best. However, slightly worse but still satisfactory
performance can be obtained even without those.
So far, we have not made use of the structural in-
formation in forum threads yet. We plan to perform
experiments investigating speech act recognition in
forums to improve the automatic quality assessment.
We also plan to apply our system to further domains
of forum discussion, such as the discussions among
active Wikipedia users.
We believe that the proposed algorithm will sup-
port important applications beyond content filtering
like automatic summarization systems and forum
specific search.
Acknowledgments
This work was supported by the German Research Foundation
as part of the Research Training Group ?Feedback-Based Qual-
ity Management in eLearning? under the grant 1223. We are
thankful to Nabble for providing their data.
References
Yigal Attali and Jill Burstein. 2006. Automated essay scoring
with e-rater v.2. The Journal of Technology, Learning, and
Assessment, 4(3), February.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library
for support vector machines. Software available at http:
//www.csie.ntu.edu.tw/?cjlin/libsvm.
Martin Chodorow and Jill Burstein. 2004. Beyond essay
length: Evaluating e-raters performance on toefl essays.
Technical report, ETS.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy. 2006.
Learning to detect conversation focus of threaded discus-
sions. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associa-
tion of Computational Linguistics (HLT-NNACL).
Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and Eduard
Hovya. 2006a. Mining and assessing discussions on the web
through speech act analysis. In Proceedings of the Workshop
on Web Content Mining with Human Language Technologies
at the 5th International Semantic Web Conference.
Jihie Kim, Erin Shaw, Donghui Feng, Carole Beal, and Eduard
Hovy. 2006b. Modeling and assessing student activities in
on-line discussions. In Proceedings of the Workshop on Ed-
ucational Data Mining at the conference of the American As-
sociation of Artificial Intelligence (AAAI-06), Boston, MA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pen-
neacchiotti. 2006c. Automatically assessing review helpful-
ness. In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages 423 ?
430, Sydney, Australia, July.
Cliff Lampe and Paul Resnick. 2004. Slash(dot) and burn:
Distributed moderation in a large online conversation space.
In Proceedings of ACM CHI 2004 Conference on Human
Factors in Computing Systems, Vienna Austria, pages 543?
550.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated Corpus
of English: The Penn Treebank. Computational Linguistics,
19(2):313?330.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin
Scholz, and Timm Euler. 2006. YALE: Rapid prototyping
for complex data mining tasks. In KDD ?06: Proceedings of
the 12th ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 935?940, New York,
NY, USA. ACM Press.
Helmut Schmid. 1995. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference on New
Methods in Language Processing, Manchester, UK.
Salvatore Valenti, Francesca Neri, and Alessandro Cucchiarelli.
2003. An overview of current research on automated es-
say grading. Journal of Information Technology Education,
2:319?329.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann,
San Francisco, 2 edition.
128
