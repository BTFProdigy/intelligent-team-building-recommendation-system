Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1238?1247, Dublin, Ireland, August 23-29 2014.
Online Gaming for Crowd-sourcing Phrase-equivalents 
A. Kumaran 
Microsoft Research 
Bangalore, India 
a.kumaran@microsoft.com 
Melissa Densmore 
University of Cape Town 
Cape Town, South Africa 
mdensmore@acm.org 
Shaishav Kumar 
Microsoft Research 
Bangalore, India 
v-shaisk@microsoft.com 
  
Abstract 
We propose the use of a game with a purpose (GWAP) to facilitate crowd-sourcing of phrase-
equivalents, as an alternative to expert or paid crowd-sourcing. Doodling is an online multi-
player game, in which one player (drawer), draws pictures on a shared board to get the other 
players (guessers) to guess the meaning behind an assigned phrase.  In this paper we describe 
the system and results from several experiments intended to improve the quality of information 
generated by the play. In addition, we describe the mechanism by which we take candidate 
phrases generated during the games and filter out true phrase equivalents. We expect that, at 
scale, this game will be more cost-efficient than paid mechanisms for a similar task, and demon-
strate this by comparing the productivity of an hour of game play to an equivalent crowd-sourced 
Amazon Mechanical Turk task to produce phrase-equivalents over one week.  
1  Introduction 
While it is fairly well known when individual words have the same meaning, it is far more difficult to 
determine when phrases or even sentences carry the same basic idea.  While it might be possible to 
address this task with machine learning techniques, building a corpus of sentences from which to seed 
a database requires human intelligence.  We suggest a game with a purpose (GWAP) that will serve to 
generate phrases with similar meanings, while simultaneously providing meta-information about the 
quality of the match.  In this drawing game, called Doodling, individuals compete in groups to guess the 
meaning behind a given drawing that is being drawn by one designated drawer trying to convey a given 
phrase or a short sentence.  The designated drawer decides when a guessed phrase matches the source 
phrase. For example ?How far is the airport?? might match semantically ?What is the distance to the 
airport??  In addition, the drawer can indicate for each partial guess how close it is on a scale of 1-3 to 
help the guessers converge on phrases that will match the given phrase or sentence.  We then pass all of 
the guesses and annotations through an SVM classifier to automatically identify potential phrase-equiv-
alents. In this study we examine several techniques for using this system to generate high quality data 
while also making the game more enjoyable.  We measure the efficacy of each technique by comparing 
our results to a gold standard: using human evaluators to rate the phrase matches generated through the 
game manually.  We also compare Doodling to a paid crowdsourcing paradigm ? Amazon?s Mechanical 
Turk ? to source phrase equivalents for the same set of phrases, and we show that our approach might 
be cost effective for large scale sourcing of paraphrases of equivalent quality. 
 
2 Background 
In this section we define the problem we are trying to address, and discuss the various ways it has been 
approached in the past. 
2.1 Phrase-equivalents & Evaluation of Quality 
In this paper, we define phrase equivalents (PEs) as text elements ? phrases or short sentences ? that 
have same or similar semantic content, but with surface structure different from each other. PEs are 
similar to paraphrases, but broader in scope, inclusive of partial matches in meaning as well as complete 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
1238
paraphrases. PEs are useful for many NLP systems from simple language modelling and smoothing, to 
complex Machine Translation technology for generation of a surface form in the target language.  Most 
existing corpora are hand-created, and hence they tend to be small in size, and available only in limited 
languages and domains.  Other data driven approaches ? such as, creation of paraphrases using mono-
lingual machine translation (Quirk et al., 2004), mining inference rules from text corpora (Lin & Pantel, 
2001), or paraphrase extraction from parallel corpora (Dolan et al., 2004) (Barzilay & McKeown, 2001) 
? were shown to be effective, but such approaches require significant seed corpora which are available 
only in limited domains and languages.  In addition, the (Lin & Pantel, 2001) approach can generate 
equivalents using user defined patterns, and may not be appropriate for generating loosely related con-
ceptual paraphrases like the human generated ones that Doodling may generate.   
The criteria used for evaluating phrase equivalents differ vastly in research literature, ranging from 
conceptual equivalence (Barzilay & McKeown, 2001), to interchangeability (Ibrahim et al, 2003), to 
preservation of grammatical correctness and semantic equivalence (Callison-Burch, 2005), and the 
standard metric of BLEU score (Callison-Burch, 2005; Papineni et al., 2002).  In general, there is no 
accepted standard model for measuring quality, hence we adopted manual annotation by experts.   
2.2 Crowdsourcing & Games with a Purpose (GWAP) for Computational Linguistics  
Many flavors of crowdsourcing paradigms exist for the creation of language data.  From the for-pay 
model where the contribution is for monetary rewards (Callison-Burch, 2009; Irvine & Klementiev, 
2010; Chen & Dolan, 2011), to the for-recognition model, where the contribution is made for individu-
als? visibility in a community (e.g., SourceForge), and the common-good model, value is produced for 
the benefit of some community (Kumaran et al., 2009).  In this paper, we explore the for-fun model 
(Cooper et al., 2010; Law et al., 2007; Von Ahn & Dabbish, 2004; Von Ahn et al., 2006), in which data 
is a by-product of some gameplay, often referred to as ?Games with a Purpose? (Von Ahn & Dabbish, 
2008), which have been shown to be very successful in many domains.  
Specifically with respect to generation of paraphrases or phrase equivalents, (Chen & Dolan, 2011) 
present their paraphrase collection using video annotations, focusing primarily on viability of establish-
ing Mechanical Turk for providing paraphrases in a productive way. (Barzilay & McKeown, 2003) pos-
ited that multiple translations of a foreign text may be a naturally occurring source for paraphrases as 
each is authored by a different translator; our approach is analogous to this approach, though our source 
phrases/sentences are not from a foreign language. (Chklovski, 2005) presents an online paraphrase 
collection tool and studies the incentive model for responsible contributions by volunteers. Paraphrases 
generated by Doodling would be similar to paraphrases labelled under class ?Phrasal? and to a lesser 
extent class ?Elaboration? in (Chen & Dolan, 2011).     In our earlier work (Kumaran et al., 2012) we 
focused on a proof-of-concept methodology using a Pictionary-based approach for generation of para-
phrases.  In this paper, we expand our concept for generating phrase equivalents in scale inexpensively, 
using several game and UI/UX features, and also compare it with a realistic for-pay baseline using Me-
chanical Turk.  The power of our methodology is its self-verification mechanisms (by drawer annotating 
the response for convergence, and the final acceptance) that validates the generated paraphrases. 
 
3 Doodling as a Game 
In this section, we present the design elements and the game flow of the Doodling game. 
3.1 Game Design 
In the Doodling game, the games are played in rooms with one player (designated as the Drawer) 
sketches an assigned concept - as phrase or sentence - while other players in the game room (Guessers) 
attempt to guess the assigned concept from the drawing that is being replicated to all screens. The 
Guessers typically start guessing the words first (based on the concept that the Drawer starts sketching 
on the screen); while the game will automatically indicate exact partial matches (for example, ?Taxi? as 
a guess for the given phrase, ?Taxi Driver?), the drawer also has the ability to provide feedback using 
annotations.  The Drawer may annotate partial guesses as incorrect (red), on the right track (orange), or 
partially correct (green), to guide the convergence. All the guessers? guesses and the drawer?s annotation 
are broadcast to all the players in the room.  Such broadcasting provides a mechanism in which players 
1239
can build on the top of other?s guesses, gradually building up the phrase or the sentence.  At some point, 
if one of the guessers guess the right phrase exactly, the game is closed automatically.  In addition, if 
the drawer judges the guess as having the same meaning as the assigned concept (for example, ?Cabbie? 
for ?Taxi Driver?), he/she can end the round by marking the guess as correct, rewarding the guesser 
with game points.  If the timer runs out before a correct guess happens, then the game times out.  Figure 
1 shows the UI during the progress of a game (the given text element being ?taxi driver?). 
 
Figure 1: Doodling Game 
 
Our primary intuition is that the sketches provide a language-independent means of communication 
of concepts that is effectively employed for the generation of phrase equivalents. Thus, we leverage a 
fun drawing-guessing game to fulfill the linguistic purpose of generating phrase-equivalents.  An im-
portant aspect of making Doodling effective was to make it engaging to play. We underwent multiple 
user studies followed by changes to the game?s UI/UX. Earlier trials had revealed the need for additional 
feedback from the drawer, leading to the introduction of 3-stage annotations of guessed phrases.  From 
a usability standpoint, the UI and gestures were optimized for use with touchscreen capable devices, 
including of the use of swipe gestures for annotating incoming guesses. 
The Doodling game subscribes to the Inversion Problem (Von Ahn & Dabbish, 2008), where one of 
the players produces an output in the form of a sketch for a given input phrase. The other players attempt 
to guess the given input. The game may produce multiple surface forms of a single semantic intent that 
have a relationship similar to that of the input-output pair in the ?noisy-channel? model. 
3.2 Game Elements  
While the game dynamics promote the resolution of the underlying computational problem (i.e., the 
generation of phrase equivalents), we made certain modifications to the basic sketch-and-convey meta-
phor ? in the formation and constitution of the game rooms, in the assignment of roles to players in a 
round-robin fashion, and the drawer?s feedback using annotation, in exposing every player?s guess to 
the entire game room, and the winning strategy that encourages building on each other?s guesses ? in 
order to help the rounds finish successfully, converge faster, and be more competitive.  Above all, the 
game dynamics and the UI were designed to make Doodling enjoyable as a game.  
Roles: Users may join existing game rooms, or can create a new private game room after logging in 
to the Doodling portal.  In a game room, one of the users is assigned ? randomly ? the role of Drawer 
(D), and the others the role of Guessers (G).  At the end of a given game round, the role of drawer cycles 
among the game room participants.  All G?s both compete (the first guesser to guess right ? either fully 
or partially ? is rewarded), as well as collaborate (each builds on other?s guesses to build longer phrases 
for bigger rewards) in guessing the text element being conveyed by the D. 
1240
Game Round: Like the sketches, the individual guesses of a given G are broadcast to the entire room, 
along with any annotation from D on each of the guesses (red/orange/green). While the right guesses 
(either lexicographic match, or as judged by D) gives the game point to the specific G, the broadcast of 
guesses and feedbacks from D to the entire game room provides a transparent mechanism to help each 
player build on the guesses of the others.  The game round closes with exact reproduction of the source 
phrase by one of the G?s, or by D accepting a full semantic equivalent by double tapping a tile.  As an 
incentive for the role of the drawer, the D is also rewarded with some game points. 
Data: In our current experiments, we used standard phrases from a generic WikiTravel (http://wik-
itravel.org/en/wikitravel:phrasebook_template) tourism phrase book as input elements. The authors 
subjectively classified each text element as Easy or Hard, depending on the potential difficulty to express 
it as a sketch; though such annotation implies additional preparatory work, it may be well worth the 
investment as such tagged corpora forms the seed for many variations. We plan to add text elements in 
many domains (Celebrities, Movies and Idioms), to provide diversity to the players.   
 
Text Element Diff. Granularity 
Cheese Omelette Easy Phrase 
Museum of Modern Art Hard Phrase 
I would like a bowl of soup. Easy Sentence 
I am not feeling well! Hard Sentence 
Table 1: Sample of text elements used in the initial seed corpus 
In order to understand the dynamics of the game, and to improve the quality and quantity of the phrase 
equivalents generated in Doodling, we incorporated many features.   
Number of Players: The application supports 2-4 players per game room, to measure the effect of 
room size on convergence rate and the player enjoyment.  We hypothesize that those game rooms with 
more players will lead to better completion primarily due to higher productivity in phrase generation.   
Hints & Reminders: we provided hints to all guessers at the beginning of the game to prime then on 
what to expect about the guess phrase.  Hints are simple text elements, such as ?Short Phrase? or ?Hard 
Sentence?, etc. In addition, we also provided some reminders periodically for improving the game dy-
namics, especially for the new players, including a reminder to the drawer that they can accept non-
exact phrases with the same meaning by double-tapping on the guess tile.  Reminders appear on the 
screen, and fade away unobtrusively.  Some game rooms were provided the hints, while others are not, 
in order to measure how helpful the hints are for game completion.   
Soft Matches:  Exact lexicographic guesses (full or partial) are automatically rewarded by the game 
engine. However, as the primary mechanism for gathering paraphrases, soft matches were allowed and 
rewarded at the discretion of the drawer (either by the double-tap action that accepts a guess as a correct 
phrase equivalent, or by the swipe-right action that which indicates a potential partial match).  Yet, to 
discourage collusion or cheating, a reporting mechanism is provided: The final accepted guess along 
with the input text element are shown to all participants, to report any unsatisfactory acceptance.  
Metrics: For measuring the effectiveness of the Doodling game, we define many metrics ranging 
from completion statistics (completion rate and completion time), to quality by comparison with gold 
data (true positives as compared with user-annotated data, precision and accuracy of automatically clas-
sified data), to qualitative user feedback (fun factor).   
4 Doodling: Experimental Evaluation 
Doodling is an HTML5 app that is accessible from most devices - touchscreen laptop or tablets - and 
deployed in the cloud (http://doodle1.cloudapp.net/).  After deployment, we recruited volunteers (pri-
marily graduate students) to log in and play the game for one hour.  As the volunteers entered the game 
server, they were assigned to different game rooms; each room was instrumented for a specific config-
uration (game room size - between 2 and 4 players, and availability of hints and reminders). Each room 
was given the same set of 38 phrases in the same sequence, to keep the variability to a minimum.  After 
an hour, the games were closed and the players asked to fill in an online questionnaire.   
In these trials, the 14 volunteers played a total of 112 games, in different game rooms. Most players 
had previously been exposed to the sketch-and-convey metaphor through Pictionary-type games.   
1241
4.1 Quality of the Generated Data 
Basis for evaluation: We first extracted all of the text elements annotated as a potential match (green, 
orange, or winning) by the Drawer. Each of the three authors then independently classified according to 
the relevance of the match. The following five classes were used for annotating every annotated text 
element: EF (Exact Full Match), EP (Exact Partial Match), TF (True Full Match), TP (True Partial 
Match) or NM (Not a Match). Partial matches entailed guesses which captured some sub-element of the 
seed text, but not the entire meaning.  We then measured inter-annotator agreement of author?s annota-
tions using a Fleiss Kappa measure (Fleiss, 1971), which stood at 0.7424, indicating substantial agree-
ment among our annotations.  Hence, we used our annotation (using majority voting for resolving any 
conflicts) as the gold data set for validating automatically the user generated paraphrases, in subsequent 
sections.  
Quality of the generated data:  Of the 112 games played, 98 of them completed successfully. Games 
were considered incomplete if the timer expired before successful completion. Of the 98 completed 
games, 15 of the final guesses were false positives (i.e. NM, wrong answers accepted erroneously), 42 
games closed with guessers reproducing the exact text element given to the drawer (i.e. EF), and 19 
games closed with Drawer correctly accepting a guess that is semantically equivalent to the given text 
element (i.e. TF, a true phrase-equivalent), and the remaining producing various degrees partial semantic 
matches (i.e. TP, true partial phrase-equivalents).  The average time of completion for successfully com-
pleted games was 160 seconds.  
In addition, most of the games, irrespective of whether closed correctly or not, produced partial equiv-
alents to the given text element as intermediate guesses, thus providing valuable data for research. These 
include all the potential matches which were not accepted as the final answer for a game, but were 
marked as green or orange via the drawers? swipe-based annotation. Table 3 shows the breakdown of 
the gold classification of all of the potential matches. 
4.2 From Game to Corpus  
Once assured the quality of the generated data, we devised a methodology for automatically detecting 
phrase equivalents (full or partial) from the user generated data, so that the game would be able to scale 
without the need for human annotators to verify individual guesses.  We designed a classifier for auto-
matically validating phrase equivalents (partial or full), based only on the game meta data, and very 
shallow text level features, and not based on any linguistic (such as, dictionaries, thesauri, etc.) or other 
specialized corpora (such as, parallel or paraphrase corpora). Our basic premise is that if such a classifier 
can identify good paraphrases with simple features, then we will be able to identify the phrase equiva-
lents automatically, in new domains or languages. 
Our classifier uses only simple game and text-level features: hardness of the input text element 
(easy/medium/hard), status of completion flag and cheating flag at completion, order and time of the 
guess, drawer?s annotation (green/orange/red), cross-game evidence, substring similarities to the input 
text element and orthographic overlap with the input text.  First, we extracted any exact matches (EF or 
EP) by removing any text elements that were a substring of the original guess, leaving us with a training 
corpus of 122 potential phrase-equivalents. We trained the classifier using a 5-fold cross-validation this 
corpus.  Some paraphrases thus extracted are shown in Table 2. 
 
Source phrase Paraphrases extracted 
Police Officer Policeman, Police Inspector, Police Superintend 
I lost my luggage. I need to find my bag at the lost-and-found counter, Lost-and-found luggage counter. 
School Teacher Class Teacher, Teacher teaching in school. 
Railway Station  Railroad Station, Railway Platform 
Table 2: Automatically Extracted Paraphrases 
 Doodling Doodling  + SVM MTurk 
 Raw Corpus Training Corpus SVM = NM SVM = TF|TP Corpus 
Size 234 122 73 49 92 
Exact Full (EF) 42 EF and EP Data automatically removed from  
Corpus using String and Substring Match 
0 
Exact Partial (EP) 71 21 
1242
True Full (TF) 30 30 2 28 53 
True Partial (TP) 11 11 5 6 13 
Not a Match (NM) 81 81 66 15 5 
Precision (TF+TP/Size) 17% 34% 10% 69% 72% 
Table 3: Comparison of corpora produced by Doodling and MTurk to gold data. SVM numbers are an average of the results 
generated during the 5-fold cross-validation. 
The classifier reduces the burden on expert hand-annotators, by automatically filtering out text ele-
ments that are likely to not be a match. As can be seen in Table 3, only 17% of the raw corpus constitutes 
useful data. Removing exact and substring matches (EF and EP) increases the precision to 34%. The 
usable corpus produced by the classifier (SVM=TF|TP) has a precision of 69%, with only 10% of the 
remaining corpus (SVM=NM) constituting false negative, or ?lost? data. The overall accuracy of the 
classifier (% of true positives + true negatives) is 82%. 
This methodology provides a viable means of generating paraphrase corpora, with a small amount of 
hand-crafted corpus in a new domain.  The classifier can be fine-tuned either for accuracy of prediction 
(precision) or productivity (recall); in our experiments we fine-tuned it for precision.  Also, we believe 
that given that these features used are devoid of linguistic or domain information, our results may pro-
vide a lower bound on the quality of automatic identification of phrase equivalents; this may be im-
proved substantially by use of appropriate linguistic resources or specialized corpora.  
In addition to phrase-equivalent data, many of the guesses relate semantically to the input text ele-
ment, in varying degrees.  Using similar features as used in the classifier, the annotation data can be 
used for identifying sets of related words for given input text elements, creating valuable resources for 
search query expansion.   
5 Mechanical Turk Experiments 
To understand the quantitative difference between Doodling and a paid crowdsourcing model for gen-
erating paraphrases we designed a ?Data Collection? Mechanical Turk task using the same phrases that 
were used in our user experiments.  Based on previous work relating to designing of Turk experiments 
and accepted best practices, we kept the task description simple: Each task asked a respondent to gen-
erate five unique and semantically equivalent phrases for a given source phrase. The respondents were 
chosen based on their familiarity with English as their first language, and each phrase was to be anno-
tated by 20 respondents over one week duration; this duration was chosen to keep the respondent popu-
lation size roughly equal to that of our user experiment. Reward for completing the generation of five 
phrase-equivalents for a single given phrase was fixed at $0.10USD, in line with the rewards given out 
for tasks with similar levels of difficulty as cited in published literature (Callison-Burch et al. 2009; 
Dolan et al, 2011).  Though the time frame was a larger than the duration of our experiments (one hour) 
significantly, the overall time taken for task is comparable to the time spent in gameplay.  
At the end of the one-week duration of the experiment, 14 out of 38 phrases got at least one set of 
valid paraphrases, leading to a completion percentage of 37%.  Most of the submitted phrases were 
annotated only by one respondent; the average number of respondents per phrase was 1.23. The anno-
tation data was judged by the authors in the same scale as outlined in Section 5.1, and the Fleiss Kappa 
measure for the annotation was 0.74, signifying significant agreement between their judgments.  Overall, 
72% of the MTurker generated paraphrases were accepted as full or partial alternatives (See Table 3).  
While the quality of data is very good, any misunderstanding of the task generated results that are sig-
nificantly off the mark:  For example, ?How do I get to the nearest international airport?? was generated 
for ?International Airport? as the source phrase.  Since the participation and completion was low, we 
extended the duration of the task by another week, but the second week yielded only 2 additional com-
pleted tasks indicating that the duration of the experiment was not the sole factor in the relative low rate 
of task completion; perhaps it is the nature of the task that did not attract significant participation.   
1243
6 Discussions  
6.1 Viability of Doodling as a Game 
The 85% successful completion (98 out of 112) of the games is encouraging, and indicates the viability 
of the game to complete successfully.  At the end of the experimental session (wherein 30 rounds of the 
game had been completed by each player on an average), the players were asked to fill in an online 
survey to measure various qualitative metrics on effectiveness of Doodling as a game.  A wide variety 
of questions were asked, ranging from specific input (How did [a specific feature] affect your ability to 
guess the right phrase?) to generic qualitative measures (Would you play this game again?).  Among 
the questions were three specific questions on how much the players enjoyed the game as a drawer, as 
a guesser and overall, in a scale of 1 (Hated it.) to 5 (Loved it!).  From the 10 respondents, the enjoyment 
factor averaged at 4.7 overall.  Such high score validates the game design and UX as a viable mechanism 
for an enjoyable game.  Further, 9 out of 10 respondents said that they would definitely play the game 
again, with comments such as ?It was very interesting and fun? and ?This game is kind of addictive?, 
indicating attraction of the game for subsequent engagement 
6.2 Use of Hints & Reminders 
We find no evidence for the hints or reminders to be valuable either in improving the quality of the 
result, or helping the time for convergence/completion.  We note that several gamers resorted to other 
means of indicating the structure of the guess phrases, such as drawing out a number of dashes to indicate 
the size of the guess phrase, with some of them requesting us to do the same.   
6.3 Scaling Up: Comparison with Mechanical Turk for crowd-sourcing phrase equivalents 
GWAPs have been criticized for their complexity, long time-to-market, and hidden running costs (Wang 
et al., 2012).  Paid crowd-sourcing methods, by comparison, are simpler to set up, and have lower initial 
costs.  While a concrete, direct comparison is not possible, Table 3 lays out some of the differences 
between the two methods, especially with reference to our metrics. 
 
 
 Mechanical Turk (MT) Doodling 
Experimental Operating Costs US$82 US$90 
Ongoing Costs US$0.10/source phrase US$90/month 
Setup Costs Minimal 3 man-months 
Players/Workers 9 14 
Time 2 weeks  1 hour 
Completion (Games with ? 1 TF generated) 14/38 (37%) 38/38 (100%) 
Quantity (# of Unique TFs) 53 28 
Precision (% of usable data) 72% 69% (with Classifier) 
Table 4: Comparison of MTurk and Doodling experiments for generation of phrase-equivalents 
 
In the case of the Doodling game, the development of the game took 3 man-months, while Mechanical 
Turk?s (MT) setup time was minimal. Both the Doodling and MT experiments had similar operational 
costs, at US$90 and US$82 respectively. This cost of $90 for Doodling consists of hosting and band-
width charges incurred for two virtual servers running on a commercial cloud platform.  However, once 
we scale Doodling up to permit more users and higher productivity, we expect the costs to remain fixed, 
whereas MT costs will scale proportionally to the productivity at US$0.10 per source. In addition, even 
with approximately equal investment, one hour of Doodling game play is more productive than the two 
weeks of MT task. As discussed in Section 5, we encountered a significant limitation of paid crowd-
sourcing: workers may not choose to do tasks they consider uninteresting.  While it is possible to in-
crease the pay rate to increase the completion rate, this entails additional costs, with deteriorating com-
pletion rates.  While we expect the productivity of Doodling to scale with the number of users, MT?s 
productivity is low even for our limited experiment, and may not scale at all.   
1244
To put this in perspective, the time taken to generate useful data using Mechanical Turk varies highly 
depending on the task: (Chen and Dolan, 2011) reported a duration of 2 months, whereas (Callison-
Burch et al., 2009) reported 2 days for their experiments.  In our Doodling experiment, the task comple-
tion rate for the game (one hour, 14 players) is faster than the equivalent Mechanical Turk task (two 
weeks, 9 workers). We argue that for scalable data collection, a fixed recurring cost for a reliable com-
pletion rate may be preferable over a variable recurring cost. Furthermore, the Doodling game setup is 
easily scalable to large user base with little marginal cost, and hence we hypothesize that the economy 
of scale will make Doodling cheaper than MT for diverse domains. Finally, while MT workers tend to 
be transient, gamers tend to be loyal, particularly if the game is perceived to be interesting.  Such a user 
base may be likely to participate and be productive in other (perhaps related) GWAPs for the generation 
of useful language data. 
6.4 Cheating 
Doodling depends on fair gameplay in order to generate reliable phrase-equivalent. Although we did not 
have many cases of cheating during the trials, cases of cheating will be unavoidable as the game scales 
to more users. The drawer scribbling answers to the canvas is a most obvious form of cheating, which 
may require sophisticated image recognition algorithms to weed out automatically. However, we opted 
for a low-cost approach of allowing any guesser to mark a certain game round as cheating, if they find 
the drawer scribbling on the canvas. Any guesser can also mark a game round as cheating, if he/she 
finds the drawer concluding a game round with guesses that are not equivalent phrases. All guessers in 
a room other than the guesser who provided the accepted guess, are given three seconds to report cheat-
ing in case the guess was not found as a suitable equivalent phrase. While this methodology may not 
work in a two player room, we expect that in larger rooms the competitive nature of the players will 
keep a game honest.  Frequent offenders may be penalized. Proposed penalties would be banning from 
game rooms, disabling certain roles or introducing harder authentication protocol to prune out offending 
players.   
Along the same lines, we intend to introduce an ?inappropriate or offending? flag, to be flagged for a 
drawing or a guess, by any of the players in the room.  Such flags, once set, may need to be investigated 
offline, and the players penalized in order to discourage misuse or abuse of the game environment. 
7 Conclusions and Future Work 
In this paper, we explored gaming as a methodology for generating paraphrase data that is useful for 
NLP or IR research and development of practical systems.  Specifically, we outlined a game-with-a-
purpose ? Doodling ? that is based on sketch-and-convey metaphor, where a sketch by a Drawer was 
used as a mechanism for abstracting a concept (the source phrase) which was then surfaced by different 
guessers in the game room, potentially producing paraphrases.   We showed that our online multiplayer 
game was effective in generating paraphrase data, by mining user guesses in the familiar sketch-and-
convey paradigm, and rewarding phrase-equivalents in addition to exact phrase guesses. Our experi-
ments for just one hour with volunteers have shown that this game can generate high quality data in 
scale. Most importantly, our volunteers rated the game ?very enjoyable?, even after an hour of continual 
play.  In addition, we presented a classification mechanism to automatically identify good partial or full 
phrase-equivalents from the user guesses, using only the meta-level features of the game and shallow 
text features, opening an avenue for data generation in diverse domains, with a small seed corpora.  We 
believe the quality of such identification may be improved significantly with addition of linguistic re-
sources, such as, dictionaries or thesauri.  Finally, our experiments with Amazon?s Mechanical Turk 
indicated that our game is comparable to and potentially more scalable than paid crowd-sourcing.  We 
believe such a game may be a viable mechanism for generating paraphrase data in diverse domains and 
languages, cheaply. 
7.1 Future Work 
Currently, we are in the process of developing and releasing Doodling as a multiplayer game app, 
providing a potential opportunity to study its uptake in the Internet, and the quality of data generated.  
In our experiments we measured, through a post-game survey, the potential for Doodling being a fun 
game, and we obtained a score of 4.7 out of 5 for ?fun-factor?, in addition to many verbal comments on 
1245
how enjoyable the game was.  Such user feedback amply indicate Doodling?s potential for scaling well 
as a game in diverse domains, such as sports, entertainment and idioms.  Also, while the current imple-
mentation of Doodling game works well for phrases, we have ample evidence that it works for short 
sentences (such as, ?My luggage is lost?, ?Where is the nearest post office?? etc.).  We hope to extend 
it to complex sentences as future work. 
One of our goals long term is to explore the game?s potential for generating parallel data ? perhaps 
through a game being played between two players conversant in two different languages.  While this 
multi- and cross-lingual game poses significant challenges, it provides for an interesting exploration into 
generation of parallel data through games.  Significantly, it may also provide opportunities for language 
learning and/or cross-cultural awareness, as many of the idioms and culture-specific phrases are not 
readily conveyed by the surface forms in one language or another.  If successful, this may pave way for 
cost-effective generation of parallel data between many languages of the world.  
1246
Reference 
Barzilay, R. 2003. Information Fusion for Mutli-document summarization: Paraphrasing and Generation. Ph.D. 
thesis @ Columbia University. 
Barzilay, R., and McKeown, K. 2001. Extracting paraphrases from a parallel corpus. 39th Annual Meeting of the 
Association for Computational Linguistics. 
Callison-Burch, C. 2009. Fast, cheap, and creative: evaluating translation quality using Amazon?s Mechanical 
Turk. EMNLP?09. 
Callison-Burch, C., Cohn, T., and Lapata, M.  2008. ParaMetric: An Automatic Evaluation Metric for Paraphras-
ing, International Conference on Computational Linguistics, 2008. 
Chen, D.L. and Dolan, W. 2011. Collecting highly parallel data for paraphrase evaluation. 49th Annual Meeting 
of the Association for Computational Linguistics. 
Lin, D., and Pantel, P. DIRT - Discovery of inference rules from text. Proceedings of the seventh ACM SIGKDD 
International conference on Knowledge discovery and data mining. ACM, 2001. 
Chklovski, T. Collecting paraphrase corpora from volunteer contributors. Proceedings of the 3rd K-CAP. Interna-
tional conference on Knowledge Capture, ACM, 2005. 
Cooper, S., Khatib, F., Treuille, A., Barbero, J., Lee, J., Beenen, M., Leaver-Fey, A., Baker, D., Popovic, Z. and 
Foldit Players. 2010. Predicting protein structures with a multiplayer online game. Nature (466), Aug 2010. 
Dolan, W., Quirk, C., and Brockett, C. 2004. Unsupervised construction of large paraphrase corpora: Exploiting 
massively parallel news sources. 20th International Conference on Computational Linguistics. 
Fleiss, J. L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76, 378?382. 
Ibrahim, A., Katz, B., and Lin, J. 2003. Extracting structural paraphrases from aligned monolingual corpora. Sec-
ond International Workshop on Paraphrasing (collocated with ACL 2003). 
Irvine, A. and Klementiev, A. 2010. Using Mechanical Turk to annotate lexicons for less commonly used lan-
guages. NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk. 
Kumaran, A., Jauhar, S. K., and Basu, S. 2012. Doodling: A Gaming Paradigm for Generating Language Data. 
Human Computation Workshop 2012. 
Kumaran, A., Saravanan, K., Datha, N., Ashok, B. and Dendi, V. 2009. WikiBABEL: a wiki-style platform for 
creation of parallel data. ACL-IJCNLP 2009. 
Law, E.L.M., Von Ahn, L., Dannenberg, R. B. and Crawford, M. 2007. Tagatune: A game for music and sound 
annotation. ISMIR?07. 
Papineni, K., Roukos, S., Ward, T., and Zhu, W. J. 2002. Bleu: A method for automatic evaluation of machine 
translation. 40th Annual Meeting of the Association for Computational Linguistics. 
Quirk, C., Brockett, C., and Dolan, W. 2004. Monolingual machine translation for paraphrase generation. Empir-
ical Methods in Natural Language Processing (EMNLP-2004). 
Von Ahn, L. and Dabbish, L. 2004. Labeling images with a computer game. CHI?04. 
Von Ahn, L., Kedia, M. and Blum, M. 2006. Verbosity: a game for collecting common-sense facts. CHI?06.  
Von Ahn, L. and Dabbish, L. 2008. Designing games with a purpose. Communications of the ACM, Vol 51.  
Wang, A., Hoang, C. D. V. and Kan, M.  2012. Perspectives on crowdsourcing annotations for natural language 
processing.  Language Resources & Evaluation Conference, 2012. 
1247
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1256?1265,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Hashing-based Approaches to Spelling Correction of Personal Names
Raghavendra Udupa
Microsoft Research India
Bangalore, India
raghavu@microsoft.com
Shaishav Kumar
Microsoft Research India
Bangalore, India
v-shaisk@microsoft.com
Abstract
We propose two hashing-based solutions to
the problem of fast and effective personal
names spelling correction in People Search
applications. The key idea behind our meth-
ods is to learn hash functions that map similar
names to similar (and compact) binary code-
words. The two methods differ in the data
they use for learning the hash functions - the
first method uses a set of names in a given lan-
guage/script whereas the second uses a set of
bilingual names. We show that both methods
give excellent retrieval performance in com-
parison to several baselines on two lists of
misspelled personal names. More over, the
method that uses bilingual data for learning
hash functions gives the best performance.
1 Introduction
Over the last few years, People Search has emerged
as an important search service. Unlike general Web
Search and Enterprise Search where users search for
information on a wide range of topics including peo-
ple, products, news, events, etc., People Search is
about people. Hence, personal names are used pre-
dominantly as queries in People Search. As in gen-
eral Web Search, a good percentage of queries in
People Search is misspelled. Naturally, spelling cor-
rection of misspelled personal names plays a very
important role in not only reducing the time and ef-
fort needed by users to find people they are search-
ing for but also in ensuring good user experience.
Spelling errors in personal names are of a differ-
ent nature compared to those in general text. Long
before People Search became widely popular, re-
searchers working on the problem of personal name
matching had recognized the human tendency to be
inexact in recollecting names from the memory and
specifying them. A study of personal names in
hospital databases found that only 39% of the er-
rors in the names were single typographical errors
(Friedman and Sideli, 1992)1. Further, multiple and
long distance typographical errors (Gregzorz Kon-
drak for Grzegorz Kondrak), phonetic errors (as in
Erik Bryl for Eric Brill), cognitive errors (as in Sil-
via Cucerzan for Silviu Cucerzan) and word substi-
tutions (as in Rob Moore for Bob Moore) are ob-
served relatively more frequently in personal names
compared to general text.
In addition to within-the-word errors, People
Search queries are plagued by errors that are not
usually seen in general text. The study by Fried-
man and Sideli discovered that 36% of the errors
were due to addition or deletion of a word (as in
Ricardo Baeza for Ricardo Baeza-Yates) (Friedman
and Sideli, 1992). Although word addition and dele-
tion generally do not come under the purview of
spelling correction, in People Search they are im-
portant and need to be addressed.
Standard approaches to general purpose spelling
correction are not well-suited for correcting mis-
spelled personal names. As pointed out by
(Cucerzan and Brill, 2004), these approaches ei-
ther try to correct individual words (and will fail to
correct Him Clijsters to Kim Clijsters) or employ
features based on relatively wide context windows
1In contrast, 80% of misspelled words in general text are due
to single typographical errors as found by (Damerau, 1964).
1256
which are not available for queries in Web Search
and People Search. Spelling correction techniques
meant for general purpose web-queries require large
volumes of training data in the form of query logs
for learning the error models (Cucerzan and Brill,
2004), (Ahmad and Kondrak, 2005). However,
query logs are not available in some applications
(e.g. Email address book search). Further, un-
like general purpose web-queries where word order
often matters, in People Search word order is lax
(e.g. I might search for either Kristina Toutanova or
Toutanova Kristina). Therefore, spelling correction
techniques that rely crucially on bigram and higher
order language models will fail on queries with a dif-
ferent word order than what is observed in the query
log.
Unlike general purpose Web Search where it is
not reasonable to assume the availability of a high-
coverage trusted lexicon, People Search typically
employs large authoritative name directories. For
instance, if one is searching for a friend on Face-
book, the correct spelling of the friend?s name exists
in the Facebook people directory2 (assuming that the
friend is a registered user of Facebook at the time of
the search). Similarly, if one is searching for a con-
tact in Enterprise address book, the correct spelling
of the contact is part of the address book. In fact,
even in Web Search, broad-coverage name directo-
ries are available in the form of Wikipedia, IMDB,
etc. The availability of large authoritative name di-
rectories that serve as the source of trusted spellings
of names throws open the possibility of correcting
misspelled personal names with the help of name
matching techniques (Pfeifer et al, 1996), (Chris-
ten, 2006), (Navarro et al, 2003). However, the best
of the name matching techniques can at best work
with a few thousand names to give acceptable re-
sponse time and accuracy. They do not scale up to
the needs of People Search applications where the
directories can have millions of names.
In this work, we develop hashing-based name
similarity search techniques and employ them for
spelling correction of personal names. The motiva-
tion for using hashing as a building block of spelling
correction is the following: given a query, we want
to return the global best match in the name directory
2http://www.facebook.com/directory/people/
that exceeds a similarity threshold. As matching the
query with the names in the directory is a time con-
suming task especially for large name directories,
we solve the search problem in two stages:
? NAME BUCKETING: For each token of the
query, we do an approximate nearest neighbor
search of the name tokens of the directory and
produce a list of candidates, i.e., tokens that are
approximate matches of the query token. Using
the list of candidate tokens, we extract the list
of candidate names which contain at least one
approximately matching token.
? NAME MATCHING: We do a rigorous match-
ing of the query with candidate names.
Clearly, our success in finding the right name sug-
gestion for the query in the NAME MATCHING
stage depends crucially on our success in getting
the right name suggestion in the list of candidates
produced by the NAME BUCKETING stage search.
Therefore, we need a name similarity search tech-
nique that can ensure very high recall without pro-
ducing too many candidates. Hashing is best suited
for this task of fast and approximate name match-
ing. We hash the query tokens as well as directory
tokens into d bit binary codes. With binary codes,
finding approximate matches for a query token is as
easy as finding all the database tokens that are at a
Hamming distance of r or less from the query token
in the binary code representation (Shakhnarovich et
al., 2008), (Weiss et al, 2008). When the binary
codes are compact, this search can be done in a frac-
tion of a second on directories containing millions
of names on a simple processor.
Our contributions are:
? We develop a novel data-driven technique for
learning hash functions for mapping similar
names to similar binary codes using a set of
names in a given language/script (i.e. monolin-
gual data). We formulate the problem of learn-
ing hash functions as an optmization problem
whose relaxation can be solved as a generalized
Eigenvalue problem. (Section 2.1).
? We show that hash functions can also be learnt
using bilingual data in the form of name equiv-
alents in two languages. We formulate the
1257
problem of learning hash functions as an opt-
mization problem whose relaxation can be
solved using Canonical Correlation Analysis.
(Section 2.2)
? We develop new similarity measures for match-
ing names (Section 3.1).
? We evaluate the two methods systematically
and compare our performance against multiple
baselines. (Section 5).
2 Learning Hash Functions
In this section, we develop two techniques for learn-
ing hash functions using names as training data. In
the first approach, we use monolingual data consist-
ing of names in a language whereas in the second we
use bilingual name pairs. In both techniques, the key
idea is the same: we learn hash functions that map
similar names in the training data to similar code-
words.
2.1 M-HASH: Learning with Monolingual
Names Data
Let (s, s?) be a pair of names and w (s, s?) be their
similarity3. We are given a set of name pairs T =
{(s, s?)} as the training data. Let ? (s) ? Rd1 be the
feature representation of s. We want to learn a hash
function f that maps each name to a d bit codeword:
f : s 7? {?1, 1}d. We also want the Hamming dis-
tance of the codeword of s to the codeword of s? be
small when w (s, s?) is large. Further, we want each
bit of the codewords to be either 1 or ?1 with equal
probablity and the successive bits of the codewords
to be uncorrelated. Thus we arrive at the following
optimization problem4:
minimize :
?
(s,s?)?T
w
(
s, s?
) ?
?f (s)? f
(
s?
)?
?2
s.t. :
?
s:(s,s?)?T
f (s) = 0
?
s:(s,s?)?T
f (s) f (s)T = ?2Id
f (s) , f
(
s?
)
? {?1, 1}d
3We used 1? length normalized Edit Distance between s
and s? as w (s, s?).
4Note that the Hamming distance of a codeword y to another
codeword y? is 14 ?y ? y
??2.
where Id is an identity matrix of size d? d.
Note that the second constraint helps us avoid the
trap of mapping all names to the same codeword and
thereby making the Hamming error zero while satis-
fying the first and last constraints.
It can be shown that the above minimization prob-
lem is NP-hard even for 1-bit codewords (Weiss et
al., 2008). Further, the optimal solution gives code-
words only for the names in the training data. As we
want f to be defined for all s, we address the out-of-
sample extension problem by relaxing f as follows5:
fR (s) = A
T? (s) =
(
aT1 ? (s) , . . . , a
T
d ? (s)
)T
(1)
where A = [a1, . . . , ad] ? Rd1?d is a rank d matrix
(d ? d1).
After the linear relaxation (Equation 1), the first
constraint simply means that the data be centered,
i.e., have zero mean. We center ? by subtracting the
mean of ? from every ? (s) ? ? to get ??.
Subsequent to the above relaxation, we get the
following optimization problem:
minimize : Tr AT ??L??TA (2)
s.t. : (3)
AT ????TA = ?2Id (4)
whereL is the graph Laplacian for the similarity ma-
trix W defined by the pairwise similarities w (s, s?).
The minimization problem can be transformed
into a generalized Eigenvalue problem and solved
efficiently using either Cholesky factorization or QZ
algorithm (Golub and Van Loan, 1996):
??L??TA = ????TA? (5)
where ? is a d? d diagonal matrix.
OnceA has been estimated from the training data,
the codeword of a name s can be produced by bina-
rizing each coordinate of fR (s):
f (s) =
(
sgn
(
aT1 ? (s)
)
, . . . , sgn
(
aTd ? (s)
))T
(6)
where sgn(u) = 1 if u > 0 and?1 otherwise for all
u ? R.
5In contrast to our approach, Spectral Hashing, a well-
known hashing technique, makes the unrealistic assumption
that the training data is sampled from a multidimensional uni-
form distribution to address the out-of-sample extension prob-
lem (Weiss et al, 2008).
1258
In the reminder of this work, we call the system
that uses the hash function learnt from monolingual
data as M-HASH.
2.2 B-HASH: Learning with Bilingual Names
Data
Let (s, t) be a pair of name s and its transliteration
equivalent t in a different language/script. We are
given the set T = {(s, t)} as the training data. Let
? (s) ? Rd1 (and resp. ? (t) ? Rd2) be the feature
representation of s (and resp. t). We want to learn
a pair of hash functions f, g that map names to d bit
codewords: f : s 7? {?1, 1}d, g : t 7? {?1, 1}d.
We also want the Hamming distance of the code-
word of a name to the codeword of its transliteration
be small. As in Section 2.1, we want each bit of the
codewords to be either 1 or?1 with equal probablity
and the successive bits of the codewords to be uncor-
related. Thus we arrive at the following optimization
problem:
minimize :
?
(s,t)?T
?f (s)? g (t)?2
s.t. :
?
s:(s,t)?T
f (s) = 0
?
t:(s,t)?T
g (t) = 0
?
s:(s,t)?T
f (s) f (s)T = ?2Id
?
t:(s,t)?S
g (t) g (t)T = ?2Id
f (s) , g (t) ? {?1, 1}d
where Id is an identity matrix of size d? d.
As we want f (and resp. g) to be defined for all s
(and resp. t), we relax f (and resp. g) as follows:
fR (s) = A
T? (s) (7)
gR (t) = B
T? (s) (8)
where A = [a1, . . . , ad] ? Rd1?d and B =
[b1, . . . , bd] ? Rd2?d are rank d matrices.
As before, we center ? and ? to get ?? and ?? re-
spectively. Thus, we get the following optimization
problem:
minimize : Tr H
(
A,B; ??, ??
)
(9)
s.t. : (10)
AT ????TA = ?2Id (11)
BT ????TB = ?2Id (12)
where H
(
A,B; ??, ??
)
=
(
AT ???BT ??
)(
AT ???BT ??
)T
.
The minimization problem can be solved as a gen-
eralized Eigenvalue problem:
????TB = ????TA? (13)
????TA = ????TB? (14)
where ? is a d ? d diagonal matrix. Further, Equa-
tions 13 and 14 find the canonical coefficients of ??
and ?? (Hardoon et al, 2004).
As with monolingual learning, we get the code-
word of s by binarizing the coordinates of fR (s)6:
f (s) =
(
sgn
(
aT1 ? (s)
)
, . . . , sgn
(
aTd ? (s)
))T
(15)
In the reminder of this work, we call the system
that uses the hash function learnt from bilingual data
as B-HASH.
3 Similarity Score
In this section, we develop new techniques for com-
puting the similarity of names at token level as well
as a whole. We will use these techniques in the
NAME MATCHING stage of our algorithm (Sec-
tion 4.2.1).
3.1 Token-level Similarity
We use a logistic function over multiple distance
measures to compute the similarity between name
tokens s and s?:
K
(
s, s?
)
=
1
1 + e?
?
i ?idi(s,s
?)
. (16)
While a variety of distance measures can be
employed in Equation 16, two obvious choices
6As a biproduct of bilingual learning, we can hash names in
the second language using g:
g (t) =
(
sgn
(
bT1 ? (t)
)
, . . . , sgn
(
bTd ? (t)
))T
1259
are the normalized Damerau-Levenshtein edit dis-
tance between s and s? and the Hamming dis-
tance between the codewords of s and s? (=
?f (s)? f (s?)?). In our experiments, we found that
the continuous relaxation ?fR (s)? fR (s?)? was
better than ?f (s)? f (s?)? and hence we used it
with Damerau-Levenshtein edit distance. We esti-
mated ?1 and ?2 using a small held out set.
3.2 Multi-token Name Similarity
Let Q = s1s2 . . . sI and D = s?1s
?
2 . . . s
?
J be two
multi-token names. To compute the similarity be-
tween Q and D, we first form a weighted bipartite
graph with a node for each si and a node for each s?j
and set edge weight toK
(
si, s?j
)
. We then compute
the weight (?max) of the maximum weighted match-
ing7 in this graph. The similarity between Q and D
is then computed as
K (Q,D) =
?max
|I ? J + 1|
. (17)
4 Spelling Correction using Hashing
In this section, we describe our algorithm for
spelling correction using hashing as a building
block.
4.1 Indexing the Name Directory
Given a name directory, we break each name into its
constituent tokens and form a set of distinct name to-
kens. Using the name tokens and the original names,
we build an inverted index which, for each name to-
ken, lists all the names that have the token as a con-
stituent. Further, we hash each name token into a d
bit codeword as described in Equation 6 (and resp.
Equation 15) when using the hash function learnt on
monolingual data (and resp. bilingual data) and store
in a hash table.
4.2 Querying the Name Directory
Querying is done in two stages:
NAME BUCKETING and NAME MATCHING.
7In practice, a maximal matching computed using a greedy
approach suffices since many of the edges in the bipartite graph
have low weight.
4.2.1 Name Bucketing
Given a query Q = s1s2 . . . sI , we hash each si
into a codeword yi and retrieve all codewords in the
hash table that are at a Hamming distance of r or
less from yi. We rank the name tokens thus retrieved
using the token level similarity score of Section 3.1
and retain only the top 100. Using the top tokens, we
get al names which contain any of the name tokens
as a constituent to form the pool of candidates C for
the NAME MATCHING stage.
4.2.2 Name Matching
First we find the best match for a query Q in the
set of candidates C as follows:
D? = argmax
D?C
K (Q,D) . (18)
Next we suggest D? as the correction for Q if
K (Q,D?) exceeds a certain empirically determined
threshold.
5 Experiments and Results
We now discuss the experiments we conducted to
study the retrieval performance of the two hashing-
based approaches developed in the previous sec-
tions. Apart from evaluating the systems on test sets
using different name directories, we were interested
in comparing our systems with several baselines, un-
derstanding the effect of some of the choices we
made (e.g. training data size, conjugate language)
and comparative analysis of retrieval performance
on queries of different complexity.
5.1 Experimental Setup
We tested the proposed hashing-based spelling cor-
rection algorithms on two test sets:
? DUMBTIONARY: 1231 misspellings of var-
ious names from Dumbtionary8 and a name
directory consisting of about 550, 000 names
gleaned from the English Wikipedia. Each of
the misspellings had a correct spelling in the
name directory.
? INTRANET: 200 misspellings of employees
taken from the search logs of the intranet
of a large organization and a name directory
8http://www.dumbtionary.com
1260
consisting of about 150, 000 employee names.
Each of the misspellings had a correct spelling
in the name directory.
Table 1 shows the average edit distance of a mis-
spelling from the correct name. Compared to
DUMBTIONARY, the misspellings in INTRANET
are more severe as the relatively high edit distance
indicates. Thus, INTRANET represents very hard
cases for spelling correction.
Test Set Average Std. Dev.
DUMBTIONARY 1.39 0.76
INTRANET 2.33 1.60
Table 1: Edit distance of a misspelling from the correct
name.
5.1.1 Training
For M-HASH, we used 30,000 single token
names in English (sampled from the list of names
in the Internet Movie Database9) as training data
and for B-HASH we used 14,941 parallel single to-
ken names in English-Hindi 10. Each name was
represented as a feature vector over character bi-
grams. Thus, the name token Klein has the bigrams
{?k, kl, le, ei, in, n?} as the features.
We learnt the hash functions from the training
data by solving the generalized Eigenvalue problems
of Sections 2.1 and 2.2. For both M-HASH and B-
HASH we used the top 32 Eigenvectors to form the
hash function resulting in a 32 bit representation for
every name token11.
5.1.2 Performance Metric
We measured the performance of all the systems
using Precision@1, the fraction of names for which
a correct spelling was suggested at Rank 1.
5.1.3 Baselines
The baselines are two popular search engines
(S1 and S2), Double Metaphone (DM), a widely
9http://www.imdb.com
10We obtained the names from the organizers
of NEWS2009 workshop (http://www.acl-ijcnlp-
2009.org/workshops/NEWS2009/pages/sharedtask.html).
11We experimented with codewords of various lengths and
found that the 32 bit representation gave the best tradeoff be-
tween retrieval accuracy and speed.
used phonetic search algorithm (Philips, 2000) and
BM25, a very popular Information Retrieval algo-
rithm (Manning et al, 2008). To use BM25 algo-
rithm for spelling correction, we represented each
name as a bag of bigrams and set the parameters K
and b to 2 and 0.75 respectively.
5.2 Results
5.2.1 DUMBTIONARY
Table 2 compares the results of the hashing-based
systems with the baselines on DUMBTIONARY. As
the misspellings in DUMBTIONARY are relatively
easier to correct, all the systems give reasonably
good retrieval results. Nevertheless, the results of
M-HASH and B-HASH are substantially better than
the baselines. M-HASH reduced the error over the
best baseline (S1) by 13.04% whereas B-HASH re-
duced by 46.17% (Table 6).
M-HASH B-HASH S1 S2 DM BM25
87.93 92.53 86.12 79.33 78.95 84.70
Table 2: Precision@1 of the various systems on DUMB-
TIONARY.
To get a deeper understanding of the retrieval per-
formance of the various systems, we studied queries
of varying complexity of misspelling. Table 3 com-
pares the results of our systems with S1 for queries
that are at various edit distances from the correct
names. We observe that M-HASH and B-HASH are
better than S1 in dealing with relatively less severe
misspellings. More interestingly, B-HASH is con-
sistently and significantly better than S1 even when
the misspellings are severe.
Distance M-HASH B-HASH S1
1 96.18 96.55 89.59
2 81.79 87.42 75.76
3 44.07 67.80 59.65
4 21.05 31.58 29.42
5 0.00 37.50 0.00
Table 3: Precision@1 for queries at various edit distances
on DUMBTIONARY.
5.2.2 INTRANET
For INTRANET, search engines could not be used
as baselines and therefore we compare our systems
1261
with Double Metaphone and BM25 in Table 4. We
observe that both M-HASH and B-HASH give sign-
ficantly better retrieval results than the baselines. M-
HASH reduced the error by 36.20% over Double
Metaphone whereas B-HASH reduced it by 51.73%.
Relative to BM25, M-HASH reduced the error by
31.87% whereas B-HASH reduced it by 48.44%.
M-HASH B-HASH DM BM25
70.65 77.79 54.00 56.92
Table 4: Precision@1 of the various systems on IN-
TRANET.
Table 5 shows the results of our systems for
queries that are at various edit distances from the
correct names. We observe that the retrieval results
for each category of queries are consistent with the
results on DUMBTIONARY. As before, B-HASH
gives signficantly better results than M-HASH.
Distance M-HASH B-HASH
1 82.76 87.93
2 57.14 72.86
3 34.29 65.71
4 38.46 53.85
5 6.67 26.67
Table 5: Precision@1 for queries at various edit distances
on INTRANET.
Test Set M-HASH B-HASH
DUMBTIONARY 13.04 46.17
INTRANET 36.20 51.73
Table 6: Percentage error reduction over the best base-
line.
5.2.3 Effect of Training Data Size
As both M-HASH and B-HASH are data driven
systems, the effect of training data size on retrieval
performance is important to study. Table 7 com-
pares the results for systems trained with various
amounts of training data on DUMBTIONARY. B-
HASH trained with just 1000 name pairs gives
95.5% of the performance of B-HASH trained with
15000 name pairs. Similarly, M-HASH trained with
1000 names gives 98.5% of the performance of
M-HASH trained with 30000 name pairs. This is
probably because the spelling mistakes in DUMB-
TIONARY are relatively easy to correct.
Table 8 shows the results on INTRANET. We see
that increase in the size of training data brings sub-
stantial returns for B-HASH. In contrast, M-HASH
gives the best results at 5000 and does not seem to
benefit from additional training data.
Size M-HASH B-HASH
1000 86.60 88.34
5000 87.36 91.13
10000 86.96 92.53
15000 87.19 92.20
30000 87.93 -
Table 7: Precision@1 on DUMBTIONARY as a function
of training data size.
Size M-HASH B-HASH
1000 66.04 66.03
5000 70.65 72.67
10000 68.09 75.26
15000 68.60 77.79
30000 65.40 -
Table 8: Precision@1 on INTRANET as a function of
training data size.
5.2.4 Effect of Conjugate Language
In Sections 5.2.1 and 5.2.2, we saw that bilingual
data gives substantially better results than monolin-
gual data. In the experiments with bilingual data,
we used English-Hindi data for training B-HASH.
A natural question to ask is what happens when we
use someother language, say Hebrew or Russian or
Tamil, instead of Hindi. In other words, does the
retrieval performance, on an average, vary substan-
tially with the conjugate language?
Table 9 compares the results on DUMB-
TIONARY when B-HASH was trained using
English-Hindi, English-Hebrew, English-Russian,
and English-Tamil data. We see that the retrieval
results are good despite the differences in the script
and language. Clearly, the source language (English
in our experiments) benefits from being paired with
any target language. However, some languages seem
1262
to give substantially better results than others when
used as the conjugate language. For instance, Hindi
as a conjugate for English seems to be better than
Tamil. At the time of writing this paper, we do not
know the reason for this behavior. We believe that a
combination of factors including feature representa-
tion, training data, and language-specific confusion
matrix need to be studied in greater depth to say any-
thing conclusively about conjugate languages.
Conjugate DUMBTIONARY INTRANET
Hindi 92.53 77.79
Hebrew 91.30 71.68
Russian 89.42 64.94
Tamil 90.48 69.12
Table 9: Precision@1 of B-HASH for various conjugate
languages.
5.2.5 Error Analysis
We looked at cases where either M-HASH or
B-HASH (or both) failed to suggest the correct
spelling. It turns out that in the DUMBTIONARY
test set, for 81 misspelled names, both M-HASH and
B-HASH failed to suggest the correct name at rank
1. Similarly, in the case of INTRANET test set, both
M-HASH and B-HASH failed to suggest the correct
name at rank 1 for 47 queries. This suggests that
queries that are difficult for one system are also in
general difficult for the other system. However, B-
HASH was able to suggest correct names for some
of the queries where M-HASH failed. In fact, in the
INTRANET test set, whenever B-HASH failed, M-
HASH also failed. And interestingly, in the DUMB-
TIONARY test set, the average edit distance of the
query and the correct name for the cases where M-
HASH failed to get the correct name in top 10 while
B-HASH got it at rank 1 was 2.96. This could be be-
cause M-HASH attempts to map names with smaller
edit distances to similar codewords.
Table 10 shows some interesting cases we found
during error analysis. For the first query, M-HASH
suggested the correct name whereas B-HASH did
not. For the second query, both M-HASH and B-
HASH suggested the correct name. And for the third
query, B-HASH suggested the correct name whereas
M-HASH did not.
Query M-HASH B-HASH
John Tiler John Tyler John Tilley
Ddear Dragba Didear Drogba Didear Drogba
James Pol James Poe James Polk
Table 10: Error Analysis.
5.3 Query Response Time
The average query response time is a measure of
the speed of a system and is an important factor
in real deployments of a Spelling Correction sys-
tem. Ideally, one would like the average query re-
sponse time to be as small as possible. However, in
practice, average query response time is not only a
function of the algorithm?s computational complex-
ity but also the computational infrastructure support-
ing the system. In our expriments, we used a sin-
gle threaded implementation of M-HASH and B-
HASH on an Intel Xeon processor (2.86 GHz). Ta-
ble 11 shows the average query response time. We
note that M-HASH is substantially slower than B-
HASH. This is because the number of collisions
in the NAME BUCKETING stage is higher for M-
HASH.
We would like to point out that both
NAME BUCKETING and NAME MATCHING
stages can be multi-threaded on a multi-core ma-
chine and the query response time can be decreased
by an order easily. Further, the memory footprint
of the system is very small and the codewords
require 4.1 MB for the employees name directory
(150,000 names) and 13.8 MB for the Wikipedia
name directory (550,000 names).
Test Set MHASH BHASH
DUMBTIONARY 190 87
INTRANET 148 75
Table 11: Average response time in milliseconds (single
threaded system running on 2.86 GHz Intel Xeon Proces-
sor).
6 Related Work
Spelling correction of written text is a well stud-
ied problem (Kukich, 1996), (Jurafsky and Mar-
tin, 2008). The first approach to spelling correc-
1263
tion made use of a lexicon to correct out-of-lexicon
terms by finding the closest in-lexicon word (Dam-
erau, 1964). The similarity between a misspelled
word and an in-lexicon word was measured using
Edit Distance (Jurafsky and Martin, 2008). The next
class of approaches applied the noisy channel model
to correct single word spelling errors (Kernighan et
al., 1990), (Brill and Moore, 2000). A major flaw of
single word spelling correction algorithms is they do
not make use of the context of the word in correcting
the errors. The next stream of approaches explored
ways of exploiting the word?s context (Golding and
Roth, 1996), (Cucerzan and Brill, 2004). Recently,
several works have leveraged the Web for improved
spelling correction (Chen et al, 2007),(Islam and
Inkpen, 2009), (Whitelaw et al, 2009). Spelling cor-
rection algorithms targeted for web-search queries
have been developed making use of query logs and
click-thru data (Cucerzan and Brill, 2004), (Ah-
mad and Kondrak, 2005), (Sun et al, 2010). None
of these approaches focus exclusively on correcting
name misspellings.
Name matching techniques have been studied in
the context of database record deduplication, text
mining, and information retrieval (Christen, 2006),
(Pfeifer et al, 1996). Most techniques use one or
more measures of phonetic similarity and/or string
similarity. The popular phonetic similarity-based
techniques are Soundex, Phonix, and Metaphone
(Pfeifer et al, 1996). Some of the string similarity-
based techniques employ Damerau-Levenshtein edit
distance, Jaro distance or Winkler distance (Chris-
ten, 2006). Data driven approaches for learning edit
distance have also been proposed (Ristad and Yiani-
los, 1996). Most of these techniques either give poor
retrieval performance on large name directories or
do not scale.
Hashing techniques for similarity search is also a
well studied problem (Shakhnarovich et al, 2008).
Locality Sensitive Hashing (LSH) is a theoretically
grounded data-oblivious approach for using random
projections to define the hash functions for data ob-
jects with a single view (Charikar, 2002), (Andoni
and Indyk, 2006). Although LSH guarantees that
asymptotically the Hamming distance between the
codewords approaches the Euclidean distance be-
tween the data objects, it is known to produce long
codewords making it practically inefficient. Re-
cently data-aware approaches that employ Machine
Learning techniques to learn hash functions have
been proposed and shown to be a lot more effective
than LSH on both synthetic and real data. Semantic
Hashing employs Restricted Boltzmann Machine to
produce more compact codes than LSH (Salakhutdi-
nov and Hinton, 2009). Spectral Hashing formalizes
the requirements for a good code and relates them to
the problem of balanced graph partitioning which is
known to be NP hard (Weiss et al, 2008). To give
a practical algorithm for hashing, Spectral Hashing
assumes that the data are sampled from a multidi-
mensional uniform distribution and solves a relaxed
partitioning problem.
7 Conclusions
We developed two hashing-based techniques for
spelling correction of person names in People
Search applications.To the best of our knowledge,
these are the first techniques that focus exclusively
on correcting spelling mistakes in person names.
Our approach has several advantages over other
spelling correction techniques. Firstly, we do not
suggest incorrect suggestions for valid queries un-
like (Cucerzan and Brill, 2004). Further, as we sug-
gest spellings from only authoritative name direc-
tories, the suggestions are always well formed and
coherent. Secondly, we do not require query logs
and other resources that are not easily available un-
like (Cucerzan and Brill, 2004), (Ahmad and Kon-
drak, 2005). Neither do we require pairs of mis-
spelled names and their correct spellings for learn-
ing the error model unlike (Brill and Moore, 2000)
or large-coverage general purpose lexicon for unlike
(Cucerzan and Brill, 2004) or pronunciation dictio-
naries unlike (Toutanova and Moore, 2002). Thirdly,
we correct the query as a whole unlike (Ahmad and
Kondrak, 2005) and can handle word order changes
unlike (Cucerzan and Brill, 2004). Fourthly, we
do not iteratively process misspelled name unlike
(Cucerzan and Brill, 2004). Fifthly, we handle large
name directories efficiently unlike the spectrum of
name matching techniques discussed in (Pfeifer et
al., 1996). Finally, our training data requirement is
relatively small.
As future work, we would like to explore the pos-
sibility of learning hash functions using 1) bilingual
1264
and monolingual data together and 2) multiple con-
jugate languages.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. In
HLT ?05: Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, pages 955?962, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Alexandr Andoni and Piotr Indyk. 2006. Near-optimal
hashing algorithms for approximate nearest neighbor
in high dimensions. In FOCS, pages 459?468.
Rahul Bhagat and Eduard H. Hovy. 2007. Phonetic mod-
els for generating spelling variants. In IJCAI, pages
1570?1575.
Mikhail Bilenko, Raymond J. Mooney, William W. Co-
hen, Pradeep D. Ravikumar, and Stephen E. Fienberg.
2003. Adaptive name matching in information inte-
gration. IEEE Intelligent Systems, 18(5):16?23.
E. Brill and R. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of ACL ?00, pages 286?293.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In STOC, pages 380?388.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improving
query spelling correction using web search results. In
EMNLP-CoNLL, pages 181?189.
P. Christen. 2006. A comparison of personal name
matching: techniques and practical issues. Techni-
cal Report TR-CS-06-02, Dept. of Computer Science,
ANU, Canberra.
William W. Cohen, Pradeep D. Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of string
distance metrics for name-matching tasks. In IIWeb,
pages 73?78.
S Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of EMNLP ?04, pages
293?300.
F.J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. Communications of
ACM, 7(3):171?176.
C. Friedman and R. Sideli. 1992. Tolerating spelling
errors during patient validation. Computers and
Biomedical Research, 25:486?509.
Andrew R. Golding and Dan Roth. 1996. Applying win-
now to context-sensitive spelling correction. CoRR,
cmp-lg/9607024.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
Computations. Johns Hopkins University Press, Balti-
more, MD, 3rd edition.
David R. Hardoon, Sa?ndor Szedma?k, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods. Neu-
ral Computation, 16(12):2639?2664.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using google web 1tn-gram data
set. In CIKM, pages 1689?1692.
D. Jurafsky and J.H. Martin. 2008. Speech and Lan-
guage Processing. Prentice-Hall.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on a
noisy channel model. In COLING, pages 205?210.
K. Kukich. 1996. Techniques for automatically correct-
ing words in a text. Computing Surveys, 24(4):377?
439.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
G. Navarro, R. Baeza-Yates, and J. Azevedo-Arcoverde.
2003. Matchsimile: a flexible approximate matching
tool for searching proper names. Journal of the Amer-
ican Society for Information Science and Technology,
54(1):3?15.
U. Pfeifer, T. Poersch, and N. Fuhr. 1996. Retrieval ef-
fectiveness of proper name search methods. Informa-
tion Processing and Management, 32(6):667?679.
L. Philips. 2000. The double metaphone search algo-
rithm. C/C++ Users Journal.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learning
string edit distance. CoRR, cmp-lg/9610005.
Ruslan Salakhutdinov and Geoffrey E. Hinton. 2009. Se-
mantic hashing. Int. J. Approx. Reasoning, 50(7):969?
978.
Gregory Shakhnarovich, Trevor Darrell, and Piotr In-
dyk. 2008. Nearest-neighbor methods in learning
and vision. IEEE Transactions on Neural Networks,
19(2):377?377.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proceedings of ACL 2010.
K. Toutanova and R. Moore. 2002. Pronounciation mod-
eling for improved spelling correction. In Proceedings
of ACL ?02, pages 141?151.
Yair Weiss, Antonio B. Torralba, and Robert Fergus.
2008. Spectral hashing. In NIPS, pages 1753?1760.
Casey Whitelaw, Ben Hutchinson, Grace Chung, and Ged
Ellis. 2009. Using the web for language independent
spellchecking and autocorrection. In EMNLP, pages
890?899.
1265
