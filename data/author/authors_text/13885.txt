Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125?129,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
English-Czech MT in 2008 ?
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin Popel,
Jan Pta?c?ek, Jan Rous?, Zdene?k ?Zabokrtsky?
Charles University, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz
{popel,jan.rous}@matfyz.cz
Abstract
We describe two systems for English-to-
Czech machine translation that took part
in the WMT09 translation task. One of
the systems is a tuned phrase-based system
and the other one is based on a linguisti-
cally motivated analysis-transfer-synthesis
approach.
1 Introduction
We participated in WMT09 with two very dif-
ferent systems: (1) a phrase-based MT based
on Moses (Koehn et al, 2007) and tuned for
English?Czech translation, and (2) a complex
system in the TectoMT platform ( ?Zabokrtsky? et
al., 2008).
2 Data
2.1 Monolingual Data
Our Czech monolingual data consist of (1)
the Czech National Corpus (CNC, versions
SYN200[056], 72.6%, Kocek et al (2000)), (2)
a collection of web pages downloaded by Pavel
Pecina (Web, 17.1%), and (3) the Czech mono-
lingual data provided by WMT09 organizers
(10.3%). Table 1 lists sentence and token counts
(see Section 2.3 for the explanation of a- and t-
layer).
Sentences 52 M
with nonempty t-layer 51 M
a-nodes (i.e. tokens) 0.9 G
t-nodes 0.6 G
Table 1: Czech monolingual training data.
? The work on this project was supported by the grants
MSM0021620838, 1ET201120505, 1ET101120503, GAUK
52408/2008, M?SMT ?CR LC536 and FP6-IST-5-034291-STP
(EuroMatrix).
2.2 Parallel Data
As the source of parallel data we use an internal
release of Czech-English parallel corpus CzEng
(Bojar et al, 2008) extended with some additional
texts. One of the added sections was gathered
from two major websites containing Czech sub-
titles to movies and TV series1. The matching of
the Czech and English movies is rather straight-
forward thanks to the naming conventions. How-
ever, we were unable to reliably determine the se-
ries number and the episode number from the file
names. We employed a two-step procedure to au-
tomatically pair the TV series subtitle files. For
every TV series:
1. We clustered the files on both sides to remove
duplicates
2. We found the best matching using a provi-
sional translation dictionary. This proved to
be a successful technique on a small sample
of manually paired test data. The process was
facilitated by the fact that the correct pairs of
episodes usually share some named entities
which the human translator chose to keep in
the original English form.
Table 2 lists parallel corpus sizes and the distri-
bution of text domains.
English Czech
Sentences 6.91 M
with nonempty t-layer 6.89 M
a-nodes (i.e. tokens) 61 M 50 M
t-nodes 41 M 33 M
Distribution: [%] [%]
Subtitles 68.2 Novels 3.3
Software Docs 17.0 Commentaries/News 1.5
EU (Legal) Texts 9.5 Volunteer-supplied 0.4
Table 2: Czech-English data sizes and sources.
1www.opensubtitles.org and titulky.com
125
2.3 Data Preprocessing using TectoMT
platform: Analysis and Alignment
As we believe that various kinds of linguistically
relevant information might be helpful in MT, we
performed automatic analysis of the data. The
data were analyzed using the layered annotation
scheme of the Prague Dependency Treebank 2.0
(PDT 2.0, Hajic? and others (2006)), i.e. we used
three layers of sentence representation: morpho-
logical layer, surface-syntax layer (called analyti-
cal (a-) layer), and deep-syntax layer (called tec-
togrammatical (t-) layer).
The analysis was implemented using TectoMT,
( ?Zabokrtsky? et al, 2008). TectoMT is a highly
modular software framework aimed at creating
MT systems (focused, but by far not limited to
translation using tectogrammatical transfer) and
other NLP applications. Numerous existing NLP
tools such as taggers, parsers, and named entity
recognizers are already integrated in TectoMT, es-
pecially for (but again, not limited to) English and
Czech.
During the analysis of the large Czech mono-
lingual data, we used Jan Hajic??s Czech tagger
shipped with PDT 2.0, Maximum Spanning Tree
parser (McDonald et al, 2005) with optimized set
of features as described in Nova?k and ?Zabokrtsky?
(2007), and a tool for assigning functors (seman-
tic roles) from Klimes? (2006), and numerous other
components of our own (e.g. for conversion of an-
alytical trees into tectogrammatical ones).
In the parallel data, we analyzed the Czech side
using more or less the same scenario as used for
the monolingual data. English sentences were an-
alyzed using (among other tools) Morce tagger
Spoustova? et al (2007) and Maximum Spanning
Tree parser.2
The resulting deep syntactic (tectogrammatical)
Czech and English trees are then aligned using T-
aligner?a feature based greedy algorithm imple-
mented for this purpose (Marec?ek et al, 2008). T-
aligner finds corresponding nodes between the two
given trees and links them. For deciding whether
to link two nodes or not, T-aligner makes use of
a bilingual lexicon of tectogrammatical lemmas,
morphosyntactic similarities between the two can-
didate nodes, their positions in the trees and other
similarities between their parent/child nodes. It
2In some previous experiments (e.g. ?Zabokrtsky? et al
(2008)), we used phrase-structure parser Collins (1999) with
subsequent constituency-dependency conversion.
also uses word alignment generated from surface
shapes of sentences by GIZA++ tool, Och and Ney
(2003). We use acquired aligned tectogrammatical
trees for training some models for the transfer.
As analysis of such amounts of data is obvi-
ously computationally very demanding, we run it
in parallel using Sun Grid Engine3 cluster of 40
4-CPU computers. For this purpose, we imple-
mented a rather generic tool that submits any Tec-
toMT pipeline to the cluster.
3 Factored Phrase-Based MT
We essentially repeat our experiments from last
year (Bojar and Hajic?, 2008): GIZA++ align-
ments4 on a-layer lemmas (a-layer nodes corre-
spond 1-1 to surface tokens), symmetrized using
grow-diag-final (no -and) heuristic5 .
Probably due to the domain difference (the test
set is news), including Subtitles in the parallel data
and Web in the monolingual data did not bring any
improvement that would justify the additional per-
formance costs. For most of the phrase-based ex-
periments, we thus used only 2.2M parallel sen-
tences (27M Czech and 32M English tokens) and
43M Czech sentences (694 M tokens).
In Table 3 below, we report the scores for the
following setups selected from about 50 experi-
ments we ran in total:
Moses T is a simple phrase-based translation (T)
with no additional factors. The translation is
performed on truecased word forms (i.e. sen-
tence capitalization removed unless the first
word seems to be a name). The 4-gram lan-
guage model is based on the 43M sentences.
Moses T+C is a factored setup with form-to-form
translation (T) and target-side morphological
coherence check following Bojar and Hajic?
(2008). The setup uses two language mod-
els: 4-grams of word forms and 7-grams of
morphological tags.
Moses T+C+C&T+T+G 84k is a setup desirable
from the linguistic point of view. Two in-
dependent translation paths are used: (1)
form?form translation with two target-side
checks (lemma and tag generated from the
target-side form) as a fine-grained baseline
3http://gridengine.sunsource.net/
4Default settings, IBM models and iterations: 153343.
5Later, we found out that the grow-diag-final-and heuris-
tic provides insignificantly superior results.
126
with the option to resort to (2) an independent
translation of lemma?lemma and tag?tag
finished by a generation step that combines
target-side lemma and tag to produce the fi-
nal target-side form.
We use three language models in this setup
(3-grams of forms, 3-grams of lemmas, and
10-grams of tags).
Due to the increased complexity of the setup,
we were able to train this model on 84k par-
allel sentences only (the Commentaries sec-
tion) and we use the target-side of this small
training data for language models, too.
For all the setups we perform standard MERT
training on the provided development set.6
4 Translation Setup Based on
Tectogrammatical Transfer
In this translation experiment, we follow the tradi-
tional analysis-transfer-synthesis approach, using
the set of PDT 2.0 layers: we analyze the input
English sentence up to the tectogrammatical layer
(through the morphological and analytical ones),
then perform the tectogrammatical transfer, and
then synthesize the target Czech sentence from its
tectogrammatical representation. The whole pro-
cedure consists of about 80 steps, so the following
description is necessarily very high level.
4.1 Analysis
Each sentence is tokenized (roughly according to
the Penn Treebank conventions), tagged by the En-
glish version of the Morce tagger Spoustova? et al
(2007), and lemmatized by our lemmatizer. Then
the dependency parser (McDonald et al, 2005) is
applied. Then the analytical trees resulting from
the parser are converted to the tectogrammatical
ones (i.e. functional words are removed, only
morphologically indispensable categories are left
with the nodes using a sequence of heuristic proce-
dures). Unlike in PDT 2.0, the information about
the original syntactic form is stored with each t-
node (values such as v:inf for an infinitive verb
form, v:since+fin for the head of a subor-
dinate clause of a certain type, adj:attr for
an adjective in attribute position, n:for+X for a
given prepositional group are distinguished).
6We used the full development set of 2k sentences for
?Moses T? and a subset of 1k sentences for the other two
setups due to time constraints.
One of the steps in the analysis of English is
named entity recognition using Stanford Named
Entity Recognizer (Finkel et al, 2005). The nodes
in the English t-layer are grouped according to the
detected named entities and they are assigned the
type of entity (location, person, or organization).
This information is preserved in the transfer of the
deep English trees to the deep Czech trees to al-
low for the appropriate capitalization of the Czech
translation.
4.2 Transfer
The transfer phase consists of the following steps:
? Initiate the target-side (Czech) t-trees sim-
ply by ?cloning? the source-side (English) t-
trees. Subsequent steps usually iterate over
all t-nodes. In the following, we denote a
source-side t-node as S and the correspond-
ing target-side node as T.
? Translate formemes using
two probabilistic dictionaries
(p(T.formeme|S.formeme, S.parent.lemma)
and p(T.formeme|S.formeme)) and a few
manual rules. The formeme translation
probability estimates were extracted from a
part of the parallel data mentioned above.
? Translate lemmas using a probabilistic dictio-
nary (p(T.lemma|S.lemma)) and a few rules
that ensure compatibility with the previously
chosen formeme. Again, this probabilistic
dictionary was obtained using the aligned
tectogrammatical trees from the parallel cor-
pus.
? Fill the grammatemes (deep-syntactic equiv-
alent of morphological categories) gender
(for denotative nouns) and aspect (for verbs)
according to the chosen lemma. We also
fix grammateme values where the English-
Czech grammateme correspondence is non-
trivial (e.g. if an English gerund expression is
translated to Czech as a subordinating clause,
the tense grammateme has to be filled). How-
ever, the transfer of grammatemes is defi-
nitely much easier task than the transfer of
formemes and lemmas.
4.3 Synthesis
The transfer step yields an abstract deep
syntactico-semantical tree structure. Firstly,
127
we derive surface morphological categories
from their deep counterparts taking care of their
agreement where appropriate and we also remove
personal pronouns in subject positions (because
Czech is a pro-drop language).
To arrive at the surface tree structure, auxil-
iary nodes of several types are added, including
(1) reflexive particles, (2) prepositions, (3) subor-
dinating conjunctions, (4) modal verbs, (5) ver-
bal auxiliaries, and (6) punctuation nodes. Also,
grammar-based node ordering changes (imple-
mented by rules) are performed: e.g. if an English
possessive attribute is translated using Czech gen-
itive, it is shifted into post-modification position.
After finishing the inflection of nouns, verbs,
adjectives and adverbs (according to the values of
morphological categories derived from agreement
etc.), prepositions may need to be vocalized: the
vowel -e or -u is attached to the preposition if the
pronunciation of prepositional group would be dif-
ficult otherwise.
After the capitalization of the beginning of each
sentence (and each named entity instance), we ob-
tain the final translation by flattening the surface
tree.
4.4 Preliminary Error Analysis
According to our observations most errors happen
during the transfer of lemmas and formemes.
Usually, there are acceptable translations of
lemma and formeme in respective n-best lists
but we fail to choose the best one. The sce-
nario described in Section 4.2 uses quite a
primitive transfer algorithm where formemes
and lemmas are translated separately in two
steps. We hope that big improvements could
be achieved with more sophisticated algo-
rithms (optimizing the probability of the whole
tree) and smoothed probabilistic models (such
as p(T.lemma|S.lemma, T.parent.lemma) and
p(T.formeme|S.formeme, T.lemma, T.parent.lemma)).
Other common errors include:
? Analysis: parsing (especially coordinations
are problematic with McDonald?s parser).
? Transfer: the translation of idioms and col-
locations, including named entities. In these
cases, the classical transfer at the t-layer
is not appropriate and utilization of some
phrase-based MT would help.
? Synthesis: reflexive particles, word order.
5 Experimental Results and Discussion
Table 3 reports lowercase BLEU and NIST scores
and preliminary manual ranks of our submissions
in contrast with other systems participating in
English?Czech translation, as evaluated on the
official WMT09 unseen test set. Note that auto-
matic metrics are known to correlate quite poorly
with human judgements, see the best ranking but
?lower scoring? PC Translator this year and also
in Callison-Burch et al (2008).
System BLEU NIST Rank
Moses T 14.24 5.175 -3.02 (4)
Moses T+C 13.86 5.110 ?
Google 13.59 4.964 -2.82 (3)
U. of Edinburgh 13.55 5.039 -3.24 (5)
Moses T+C+C&T+T+G 84k 10.01 4.360 -
Eurotran XP 09.51 4.381 -2.81 (2)
PC Translator 09.42 4.335 -2.77 (1)
TectoMT 07.29 4.173 -3.35 (6)
Table 3: Automatic scores and preliminary human
rank for English?Czech translation. Systems in
italics are provided for comparison only. Best re-
sults in bold.
Unfortunately, this preliminary evaluation sug-
gests that simpler models perform better, partly
because it is easier to tune them properly both
from computational point of view (e.g. MERT
not stable and prone to overfitting with more fea-
tures7), as well as from software engineering point
of view (debugging of complex pipelines of tools
is demanding). Moreover, simpler models run
faster: ?Moses T? with 12 sents/minute is 4.6
times faster than ?Moses T+C?. (Note that we have
not tuned either of the models for speed.)
While ?Moses T? is probably nearly identical
setup as Google and Univ. of Edinburgh use,
the knowledge of correct language-dependent to-
kenization and the use of relatively high quality
large language model data seems to bring moder-
ate improvements.
6 Conclusion
We described our experiments with a complex lin-
guistically motivated translation system and vari-
ous (again linguistically-motivated) setups of fac-
tored phrase-based translation. An automatic eval-
uation seems to suggest that simpler is better, but
we are well aware that a reliable judgement comes
only from human annotators.
7For ?Moses T+C+C&T+T+G?, we observed BLEU
scores on the test set varying by up to five points absolute
for various weight settings yielding nearly identical dev set
scores.
128
References
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143?146, Columbus, Ohio, June. Association for
Computational Linguistics.
Ondr?ej Bojar, Miroslav Jan??c?ek, Zdene?k ?Zabokrtsky?,
Pavel ?Ces?ka, and Peter Ben?a. 2008. CzEng 0.7:
Parallel Corpus with Community-Supplied Transla-
tions. In Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, May. ELRA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T0 1, Philadelphia.
Va?clav Klimes?. 2006. Analytical and Tectogrammat-
ical Analysis of a Natural Language. Ph.D. thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity, Prague, Czech Rep.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of European Machine Translation Confer-
ence (EAMT 08), pages 102?111, Hamburg, Ger-
many.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT
?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancou-
ver, British Columbia, Canada.
Va?clav Nova?k and Zdene?k ?Zabokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, ed-
itors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th I nternational Conference on
Text, Speech and Dialogue, Lecture Notes in Com-
puter Science, pages 92?98, Pilsen, Czech Repub-
lic. Springer Science+Business Media Deutschland
GmbH.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular Hybrid MT System
with Tectogrammatics Used as Transfer Layer. In
Proc. of the ACL Workshop on Statistical Machine
Translation, pages 167?170, Columbus, Ohio, USA.
129
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 297?307, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Exploiting Reducibility in Unsupervised Dependency Parsing
David Marec?ek and Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, 11800 Prague, Czech Republic
{marecek,zabokrtsky}@ufal.mff.cuni.cz
Abstract
The possibility of deleting a word from a sen-
tence without violating its syntactic correct-
ness belongs to traditionally known manifes-
tations of syntactic dependency. We introduce
a novel unsupervised parsing approach that is
based on a new n-gram reducibility measure.
We perform experiments across 18 languages
available in CoNLL data and we show that
our approach achieves better accuracy for the
majority of the languages then previously re-
ported results.
1 Introduction
The true nature of the notion of dependency (af-
ter removing sedimentary deposits of rules imposed
only by more or less arbitrary conventions) remains
still somewhat vague and elusive. This holds in spite
of a seemingly strong background intuition and even
after a decade of formalized large-scale dependency-
based resources being available to the research com-
munity. It is undeniable that a huge progress has
been reached in the field of supervised dependency
parsing, especially due to the CoNLL shared task se-
ries. However, when it comes to unsupervised pars-
ing, there are surprisingly few clues we could rely
on.
As mentioned e.g. by Ku?bler et al2009), one of
the traditional linguistic criteria for recognizing de-
pendency relations (including their head-dependent
orientation) is that a head H of a construction C de-
termines the syntactic category of C and can often
replace C. Or, in words of Dependency Analysis by
Reduction (Lopatkova? et al2005), stepwise dele-
tion of dependent elements within a sentence pre-
serves its syntactic correctness. A similar idea of
dependency analysis by splitting a sentence into all
possible acceptable fragments is used by Gerdes and
Kahane (2011).
Of course, all the above works had to respond to
the notorious fact that there are many language phe-
nomena precluding the ideal (word by word) sen-
tence reducibility (e.g. in the case of prepositional
groups, or in the case of subjects in English finite
clauses). However, we disregard their solutions ten-
tatively and borrow only the very core of the re-
ducibility idea: if a word can be removed from a
sentence without damaging it, then it is likely to be
dependent on some other (still present) word.
As it is usual with dichotomies in natural lan-
guages, it seems more adequate to use a continuous
scale instead of the reducible-irreducible opposition.
That is why we introduce a simple reducibility mea-
sure based on n-gram corpus statistics. We employ
this reducibility measure as the main feature in our
unsupervised parsing procedure.
The procedure is based on a commonly used
Bayesian inference technique called Gibbs sampling
(Gilks et al1996). In our sampler, the more re-
ducible a given token is, the more likely it is to
be sampled as a dependant and not as a head. Af-
ter certain number of sampling iterations, for each
sentence a final dependency tree is created (one to-
ken per node, including punctuation) that maximizes
the product of edge probabilities gathered along the
sampling history.
Our approach allows to utilize information from
297
very large corpora. While the computationally de-
manding sampling procedure can be applied only
on limited data, the unrepeated precomputation of
statistics for reducibility estimates can easily exploit
much larger data.
We are not aware of any other published work
on unsupervised parsing employing reducibility or
a similar idea. Dominating approaches in unsuper-
vised parsing are typically based on repeated pat-
terns, and not on the possibility of a deletion inside
a pattern. It seems that the two views of depen-
dency (frequent co-occurrence of head-dependant
pair, versus reducibility of the dependant) are rather
complementary, so fruitful combinations can be
hopefully expected in future.
The remainder of this paper is structured as fol-
lows. Section 2 briefly outlines the state of the art in
unsupervised dependency parsing. Our measure of
reducibility based on a large monolingual corpus is
presented in Section 3. Section 4 shows our models
which serve for generating probability estimates for
edge sampling described in Section 5. Experimen-
tal parsing results for languages included in CoNLL
shared task treebanks are summarized in Section 6.
Section 7 concludes this article.
2 Related Work
The most popular approach in unsupervised de-
pendency parsing of the recent years is to employ
Dependency Model with Valence (DMV), which
was introduced by Klein and Manning (2004).
The inference algorithm was further improved by
Smith (2007) and Cohen et al2008). Headden,
Johnson and McClosky (2009) introduced the Ex-
tended Valence Grammar (EVG) and added lexical-
ization and smoothing. Blunsom and Cohn (2010)
use tree substitution grammars, which allow learn-
ing larger dependency fragments.
Unfortunately, many of these works show results
only for English.1 However, the main feature of
unsupervised methods should be their applicabil-
ity across a wide range of languages. Such exper-
iments were done by Spitkovsky (2011b; 2011c),
where the parsing algorithm was evaluated on all 19
languages included in CoNLL 2006 (Buchholz and
1The state-of-the-art unsupervised parsers achieve more
than 50% of attachment score measured on the Penn Treebank.
Marsi, 2006) and 2007 (Nivre et al2007) shared
tasks.
The fully unsupervised linguistic analysis
(Spitkovsky et al2011a) shows that the unsuper-
vised part-of-speech tags may be more useful for
this task than the supervised ones.
Another possibility for obtaining dependency
structures for languages without any linguistically
annotated resources can be the projection using a
parallel treebank with a resource-rich language (typ-
ically English). McDonald et al2011) showed that
such projection produce better structures than the
current unsupervised parsers do. However, our task
is different. We would like to produce structures that
are not burdened by any linguistic conventions.
In this paper, we describe a novel approach to un-
supervised dependency parsing. Our model differs
from DMV, since we employ the reducibility feature
and use fertility of nodes instead of generating STOP
signs.
We use Gibbs sampling procedure for inference
instead of Variational Bayes, which has been more
common for induction of linguistic strucures. Gibbs
sampling algorithm for grammar induction was used
also by Marec?ek and Z?abokrtsky? (2011). However,
their sampling algorithm produces generally non-
projective trees. Our sampler, which is described in
Section 5, introduces a completely different small-
change operator that guarantees projective edges.
3 Computing Reducibility scores
We call a word (or a sequence of words) in a sen-
tence reducible, if the sentence after removing the
word remains grammatically correct. Although we
cannot automatically recognize grammaticality of
such newly created sentence, we can search for it
in a large corpus. If we find it, we assume the word
was reducible in the original sentence.
Since the number of such reducible word se-
quences found in any corpus will be low, we de-
termine the reducibility scores from their individual
types (part-of-speech tags). This then implicitly al-
lows some sharing of the scores between different
word sequences.
The necessity to search for the whole sentences
in the corpus and not only for some smaller context
(considering, for example, just left and right neigh-
298
bor), which would lead to lower sparsity, is rational-
ized by the following example:
Their children went to school.
I took their children to school.
The verb ?went? would be reducible in the context
?their children went to school?, because the sequence
?their children to school? occurs in the second sen-
tence. One could find such examples frequently even
for large contexts. For instance, verbs in free word-
order languages can be placed almost at any posi-
tion in a sentence; therefore, without the full sen-
tence context, they would have to be considered as
reducible. To prevent this, we decided to work ex-
clusively with the full sentence context instead of
shorter contexts.
Other way that would lead to lower sparsity would
be searching for sequences of part-of-speech tags in-
stead of sequences of word forms. However, this
also does not bring desired results. For instance, the
two following sentence patterns
DT NNS VBD IN DT NN .
DT NNS VBD DT NN .
are quite frequent in English and we can deduce
from them that the preposition IN is reducible. But
this is of course a wrong deduction, since the prepo-
sition cannot be removed from the prepositional
phrase. Using part-of-speech tags instead of word
forms is thus not suitable for computing reducibility
scores.
Although we search for reducible sequences of
word forms in the corpus, we compute reducibil-
ity scores for sequences of part-of-speech tags. This
requires to have the corpus morphologically disam-
biguated. A sequences of part-of-speech tags will be
denoted as ?PoS n-gram? in the following text.
Assume a PoS n-gram g = [t1, . . . , tn]. We go
through the corpus and search for all its occurrences.
For each such occurrence, we remove the respec-
tive words from the current sentence and check in
the corpus whether the rest of the sentence occurs at
least once elsewhere in the corpus.2 If so, then such
occurrence of PoS n-gram is reducible, otherwise it
is not. We denote the number of such reducible oc-
2We do not take into account sentences with less then 10
words, because they could be nominal (without any verb) and
might influence the reducibility scores of verbs.
unigrams R bigrams R trigrams R
VB 0.04 VBN IN 0.00 IN DT JJ 0.00
TO 0.07 IN DT 0.02 JJ NN IN 0.00
IN 0.11 NN IN 0.04 NN IN NNP 0.00
VBD 0.12 NNS IN 0.05 VBN IN DT 0.00
CC 0.13 JJ NNS 0.07 JJ NN . 0.00
VBZ 0.16 NN . 0.08 DT JJ NN 0.04
NN 0.22 DT NNP 0.09 DT NNP NNP 0.05
VBN 0.24 DT NN 0.09 NNS IN DT 0.14
. 0.32 NN , 0.11 NNP NNP . 0.15
NNS 0.38 DT JJ 0.13 NN IN DT 0.23
DT 0.43 JJ NN 0.14 NNP NNP , 0.46
NNP 0.78 NNP . 0.15 IN DT NNP 0.55
JJ 0.84 NN NN 0.22 DT NN IN 0.59
RB 2.07 IN NN 0.67 NNP NNP NNP 0.64
, 3.77 NNP NNP 0.76 IN DT NN 0.80
CD 55.6 IN NNP 1.81 IN NNP NNP 4.27
Table 1: Reducibility scores of the most frequent
English n-grams. (V* are verbs, N* are nouns, DET
are determiners, IN are prepositions, JJ are adjec-
tives, RB are adverbs, CD are numerals, and CC are
coordinating conjunctions)
currences of PoS n-gram g by r(g). The number of
all its occurrences is c(g).
The relative reducibility R(g) of a PoS n-gram g
is then computed as
R(g) =
1
N
r(g) + ?1
c(g) + ?2
, (1)
where the normalization constant N , which ex-
presses relative reducibility over all the PoS n-grams
(denoted by G), causes the scores are concentrated
around the value 1.
N =
?
g?G(r(g) + ?1)
?
g?G(c(g) + ?2)
(2)
Smoothing constants ?1 and ?2, which prevent re-
ducibility scores from being equal to zero, are set
to
?1 =
?
g?G r(g)
?
g?G c(g)
, ?2 = 1 (3)
This setting causes that even if a given PoS n-gram is
not reducible anywhere in the corpus, its reducibility
score is 1/(c(g) + 1).
Tables 1, 2, and 3 show reducibility scores of the
most frequent PoS n-grams of three selected lan-
guages: English, German, and Czech. If we consider
only unigrams, we can see that the scores for verbs
are often among the lowest. Verbs are followed by
prepositions and nouns, and the scores for adjectives
299
unigrams R bigrams R trigrams R
VVPP 0.00 NN APPR 0.00 NN APPR NN 0.01
APPR 0.27 APPR ART 0.00 ADJA NN APPR 0.01
VVFIN 0.28 ART ADJA 0.00 APPR ART ADJA 0.01
APPRART 0.32 NN VVPP 0.00 NN KON NN 0.01
VAFIN 0.37 NN $( 0.01 ADJA NN $. 0.01
KON 0.37 NN NN 0.01 NN ART NN 0.32
NN 0.43 NN ART 0.21 ART NN ART 0.49
ART 0.49 ADJA NN 0.28 NN ART ADJA 0.90
$( 0.57 NN $, 0.67 ADJA NN ART 0.95
$. 1.01 NN VAFIN 0.85 NN APPR ART 0.95
NE 1.14 NN VVFIN 0.89 NN VVPP $. 1.01
CARD 1.38 NN $. 0.95 ART NN APPR 1.35
ADJA 2.38 ART NN 1.07 ART ADJA NN 1.58
$, 2.94 NN KON 2.41 APPR ART NN 2.60
ADJD 3.54 APPR NN 2.65 APPR ADJA NN 2.65
ADV 7.69 APPRART NN 3.06 ART NN VVFIN 9.51
Table 2: Reducibility scores of the most frequent
German n-grams. (V* are verbs, N* are nouns, ART
are articles, APPR* are prepositions, ADJ* are ad-
jectives, ADV are adverbs, CARD are numerals, and
KON are conjunctions)
and adverbs are very high for all three examined lan-
guages. That is desired, because the reducible uni-
grams will more likely become leaves in dependency
trees. Considering bigrams, the couples [determiner
? noun], [adjective ? noun], and [preposition ? noun]
obtained reasonably high scores. However, there
are also n-grams such as the German trigram [de-
terminer ? noun ? preposition] (ART-NN-APPR)
whose reducibility score is undesirably high.3
4 Models
We introduce a new generative model that is dif-
ferent from the widely used Dependency Model
with Valence (DMV). In DMV (Klein and Manning,
2004) and in the extended model EVG (Headden III
et al2009), there is a STOP sign indicating that no
more dependents in a given direction will be gener-
ated. Given a certain head, all its dependents in left
direction are generated first, then the STOP sign in
that direction, then all its right dependents and then
STOP in the other direction. This process continues
recursively for all generated dependents.
Our model introduces fertility of a node, which
substitutes the STOP sign. For a given head, we first
generate the number of its left and right children
3The high reducibility score of ART-NN-APPR was proba-
bly caused by German particles, which have the same PoS tag
as prepositions.
unigrams R bigrams R trigrams R
P4 0.00 RR AA 0.00 RR NN Z: 0.00
RV 0.00 Z: J, 0.00 NN RR AA 0.00
Vp 0.06 Vp NN 0.00 NN AA NN 0.16
Vf 0.06 VB NN 0.12 AA NN RR 0.23
P7 0.16 NN Vp 0.13 NN RR NN 0.46
J, 0.24 NN VB 0.18 NN J? NN 0.46
RR 0.28 NN RR 0.22 AA NN NN 0.47
VB 0.33 NN AA 0.23 NN Z: Z: 0.48
NN 0.72 NN J? 0.62 NN Z: NN 0.52
J? 1.72 AA NN 0.62 NN NN NN 0.70
C= 1.85 NN NN 0.70 AA AA NN 0.72
PD 2.06 NN Z: 0.97 AA NN Z: 0.86
AA 2.22 Z: NN 1.72 NN NN Z: 1.38
Dg 3.21 Z: Z: 1.97 RR NN NN 2.26
Z: 4.01 J? NN 2.05 RR AA NN 2.65
Db 4.62 RR NN 2.20 Z: NN Z: 8.32
Table 3: Reducibility scores of the most frequent
Czech n-grams. (V* are verbs, N* are nouns, P* are
pronouns, R* are prepositions, A* are adjectives, D*
are adverbs, C* are numerals, J* are conjunctions,
and Z* is punctuation)
(fertility model) and then we fill these positions by
generating its individual dependents (edge model).
If a zero fertility is generated in both the directions,
the head becomes a leaf.
Besides the fertility model and the edge model,
we use two more models (subtree model and dis-
tance model), which force the generated trees to
have more desired shape.4
4.1 Fertility Model
We express a fertility of a node by a pair of num-
bers: the number of its left dependents and the num-
ber of its right dependents. For example, fertility
?1-3? means that the node has one left and three
right dependents, fertility ?0-0? indicates that it is
a leaf. Fertility is conditioned by part-of-speech tag
of the node and it is computed following the Chi-
nese restaurant process. This means that if a specific
fertility has been frequent for a given PoS tag in the
past, it is more likely to be generated again. The
formula for computing probability of fertility fi of a
word on the position i in the corpus is as follows:
Pf (fi|ti) =
c?i(?ti, fi?) + ?P0(fi)
c?i(?ti?) + ?
, (4)
4In fact, the subtree model and the distance model disrupt a
bit the generative story, because the probabilites do not sum up
to one when they are used. However, they proved to help with
inducing better linguistic structures.
300
where ti is part-of-speech tag of the word on the po-
sition i, c?i(?ti, fi?) stands for the count of words
with PoS tag ti and fertility fi in the history, and
P0 is a prior probability for the given fertility which
depends on the total number of node dependents de-
noted by |fi| (the sum of numbers of left and right
dependents):
P0(fi) =
1
2|fi|+1
(5)
This prior probability has a nice property: for a
given number of nodes, the product of fertility prob-
abilities over all the nodes is equal for all possible
dependency trees. This ensures the stability of this
model during the inference.
Besides the basic fertility model, we introduce
also an extended fertility model, which uses fre-
quency of a given word form for generating number
of children. We assume that the most frequent words
are mostly function words (e.g. determiners, prepo-
sitions, auxiliary verbs, conjunctions). Such words
tend to have a stable number of children, for exam-
ple (i) some function words are exclusively leaves,
(ii) prepositions have just one child, and (iii) attach-
ment of auxiliary verbs depends on the annotation
style, but number of their children is also not very
variable. The higher the frequency of a word form,
the higher probability mass is concentrated on one
specific number of children and the lower Dirichlet
hyperparameter ? in Equation 4 is needed. The ex-
tended fertility is described by equation
P ?f (fi|ti, wi) =
c?i(?ti, fi?) + ?eF (wi)P0(fi)
c?i(?ti?) + ?eF (wi)
, (6)
where F (wi) is a frequency of the word wi, which
is computed as a number of words wi in our corpus
divided by number of all words.
4.2 Edge Model
After the fertility (number of left and right depen-
dents) is generated, the individual slots are filled us-
ing the edge model. A part-of-speech tag of each de-
pendent is conditioned by part-of-speech tag of the
head and the edge direction (position of the depen-
dent related to the head).5
5For the edge model purposes, the PoS tag of the technical
root is set to ?<root>? and it is in the zero-th position in the
Similarly as for the fertility model, we employ
Chinese restaurant process to assign probabilities of
individual dependent.
Pe(tj |ti, dj) =
c?i(?ti, tj , dj?) + ?
c?i(?ti, dj?) + ?|T |
, (7)
where ti and tj are the part-of-speech tags of the
head and the generated dependent respectively; dj is
a direction of edge between the words i and j, which
can have two values: left and right. c?i(?ti, tj , dj?)
stands for the count of edges ti ? tj with the direc-
tion dj in the history, |T | is a number of unique tags
in the corpus and ? is a Dirichlet hyperparameter.
4.3 Distance Model
Distance model is an auxiliary model that prevents
the resulting trees from being too flat. Ideally, it
would not be needed, but experiments showed that
it helps to infer better trees. This simple model says
that shorter edges are more probable than longer
ones. We define probability of a distance between
a word and its parent as its inverse value,6 which is
then normalized by the normalization constant d.
Pd(i, j) =
1
d
(
1
|i? j|
)?
(8)
The hyperparameter ? determines the weight of this
model.
4.4 Subtree Model
The subtree model uses the reducibility measure. It
plays an important role since it forces the reducible
words to be leaves and reducible n-grams to be sub-
trees. Words with low reducibility are forced to-
wards the root of the tree. We define desc(i) as a
sequence of tags [tl, . . . , tr] that corresponds to all
the descendants of the word wi including wi, i.e. the
whole subtree of wi. The probability of such sub-
tree is proportional to its reducibility R(desc(i)).
The hyperparameter ? determines the weight of the
model; s is a normalization constant.
Ps(i) =
1
s
R(desc(i))? (9)
sentence, so the head word of the sentence is always its right
dependent.
6Distance between any word and the technical root of the
dependency tree was set to 10. Since each technical root has
only one dependent, this value does not affect the model.
301
4.5 Probability of the Whole Treebank
We want to maximize the probability of the whole
generated treebank, which is computed as follows:
Ptreebank =
n?
i=1
(P ?f (fi|ti, wi) (10)
Pe(ti|tpi(i), di) (11)
Pd(i, pi(i)) (12)
Ps(i)), (13)
where pi(i) denotes the parent of the word on the
position i. We multiply the probabilities of fertil-
ity, edge, distance from parent, and subtree over all
words (nodes) in the corpus. The extended fertility
model P ?f can be substituted by its basic variant Pf .
5 Sampling Algorithm
For stochastic searching for the most probable de-
pendency trees, we employ Gibbs sampling, a stan-
dard Markov Chain Monte Carlo technique (Gilks et
al., 1996). In each iteration, we loop over all words
in the corpus in a random order and change the de-
pendencies in their neighborhood (a small change
described in Section 5.2). In the end, ?average? trees
based on the whole sampling are built.
5.1 Initialization
Before the sampling starts, we initialize the projec-
tive trees randomly. For doing so, we tried the fol-
lowing two initializers:
? For each sentence, we choose randomly one
word as the head and attach all other words to
it.
? We are picking one word after another in a ran-
dom order and we attach it to the nearest left (or
right) neighbor that has not been attached yet.
The left-right choice is made by a coin flip. If it
is not possible to attach a word to one side, we
attach it to the other side. The last unattached
word becomes the head of the sentence.
While the first method generates only flat trees,
the second one can generate all possible projective
trees. However, the sampler converges to similar re-
sults for both the initializations. Therefore we con-
clude that the choice of the initialization mechanism
The   dog   was   in   the   park  .
(((The) dog) was (in ((the) park)) (.))
Figure 1: Arrow and bracketing notation of a projec-
tive dependency tree.TTThe dogwadosinootpoTTre doki.(dooT)dd
TTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddTTd d
TTThe dogwadosinootpoTTre doki.(dooT)ddTTThe dogwadosinootpoTTre doki.(dooT)ddT Td d
Figure 2: An example of small change in a projec-
tive tree. The bracket (in the park) is removed and
there are five possibilities how to replace it.
is not so important here and we choose the first one
due to its simplicity.
5.2 Small Change Operator
We use the bracketing notation for illustrating the
small change operator. Each projective dependency
tree consisting of n words can be expressed by n
pairs of brackets. Each bracket pair belongs to one
node and delimits its descendants from the rest of
the sentence. Furthermore, each bracketed segment
contains just one word that is not embedded deeper;
this node is the segment head. An example of this
notation is in Figure 1.
The small change is then very simple. We remove
one pair of brackets and add another, so that the con-
ditions defined above are not violated. An example
of such change is in Figure 2.
From the perspective of dependency structures,
the small change can be described as follows:
1. Pick a random non-root word w (the word in
in our example) and find its parent p (the word
was).
2. Find all other children of w and p (the words
dog, park, and .) and denote this set by C.
3. Choose the new head out of w and p. Mark the
new head as g and the second candidate as d.
Attach d to g.
302
4. Select a neighborhoodD adjacent to the word d
as a continuous subset ofC and attach all words
from D to d. D may be also empty.
5. Attach the remaining words from C that were
not in D to the new head g.
5.3 Building ?Average? Trees
The ?burn-in? period is set to 10 iterations. After
this period, we begin to count how many times an
edge occurs at a particular location in the corpus.
These counts are collected over the whole corpus
with the collection-rate 0.01.7
When the samling is finished, we build final de-
pendency trees based on the edge counts obtained
during the sampling. We employ the maximum
spanning tree (MST) algorithm (Chu and Liu, 1965)
to find them; the weights of edges for computing
MST correspond to the number of times they were
present during the sampling. This averaging method
was used also by Marec?ek and Z?abokrtsky? (2011).
Other possibilities for obtaining final depen-
dency trees would be using Eisner?s projective al-
gorithm (Eisner, 1996) or using annealing method
(favoring more likely changes) at the end of the sam-
pling. However, the general non-projective MST al-
gorithm enable non-projective edges, which are by
no means negligible in treebanks (Havelka, 2007).
6 Experiments and Evaluation
We evaluate our parser on 20 treebanks (18
languages) included in CoNLL shared tasks
2006 (Buchholz and Marsi, 2006) and 2007 (Nivre
et al2007).
Similarly to some previous papers on unsuper-
vised parsing (Gillenwater et al2011; Spitkovsky
et al2011b), the tuning experiments were per-
formed on English only. We used English for check-
ing functionality of the individual models and for
optimizing hyperparameter values. The best config-
uration of the parser achieved on English develop-
ment data was then used for parsing all other lan-
guages. This simulates the situation in which we
have only one treebank (English) on which we can
tune our parser and we want to parse other languages
for which we have no manually annotated treebanks.
7After each small change is made, the edges from the whole
corpus are collected with a probability 0.01.
language tokens (mil.) language tokens (mil.)
Arabic 19.7 Greek 20.9
Basque 14.1 Hungarian 26.3
Bulgarian 18.8 Italian 39.7
Catalan 27.0 Japanese 2.6
Czech 20.3 Portuguese 31.7
Danish 15.9 Slovenian 13.7
Dutch 27.1 Spanish 53.4
English 85.0 Swedish 19.2
German 56.9 Turkish 16.5
Table 4: Wikipedia texts statistics
6.1 Data
We need two kinds of data for our experiments: a
smaller treebank, which is used for sampling and for
evaluation, and a large corpus, from which we com-
pute n-gram reducibility scores.
The treebanks are taken from the CoNLL shared
task 2006 and 2007. The experiments are per-
formed for all languages except for Chinese.8 We
use only the testing parts of the treebanks (the files
test.conll) for the dependency tree induction.
As a source of the part-of-speech tags, we use the
fine-grained gold PoS tags, which are in the fifth col-
umn in the CoNLL format.
For obtaining reducibility scores, we used the
W2C corpus9 of Wikipedia articles, which was
downloaded by Majlis? and Z?abokrtsky? (2012). Their
statistics across languages are shown in Table 4. To
make them useful, the necessary preprocessing steps
must have been done. The texts were first automati-
cally segmented and tokenized10 and then they were
part-of-speech tagged by TnT tagger (Brants, 2000),
which was trained on the respective CoNLL train-
ing data (the files train.conll). The quality of
such tagging is not very high, since we do not use
any lexicons11 or pretrained models. However, it is
sufficient for obtaining good reducibility scores.
8We do not have appropriate Chinese segmenter that would
segment Chinese texts in the same way as in CoNLL.
9http://ufal.mff.cuni.cz/?majlis/w2c/
10The segmentation to sentences and tokenization was per-
formed using the TectoMT framework (Popel and Z?abokrtsky?,
2010).
11Using lexicons or another pretrained models for tagging
means using other sources of human annotated data, which is
not allowed if we want to compare our results with others.
303
6.2 Setting the Hyperparameters
The applicability of individual models and their pa-
rameters were tested on development data set of
English (the file en/dtest.conll in CoNLL
shared task 2007).
After several experiments, we have observed that
the extended fertility model provides better results
than the basic fertility model; the parser using the
basic fertility model achieved 44.1% attachment
score for English, whereas the extended fertility
model increased the score to 46.8%. The four hy-
perparameters ?e (extended fertility model), ? (edge
model), ? (distance model), and ? (subtree model),
were set by a grid search algorithm,12 which found
the following optimal values:
?e = 0.01, ? = 1, ? = 1.5, ? = 1
In informal experiments, parameters were tuned
also for other treebanks and we found out that they
vary across languages. Therefore, adjusting the hy-
perparameters on another language would probably
change the scores significantly.
6.3 Evaluation
The best setting from the experiments on English is
now used for evaluating our parser on all CoNLL
languages. To be able to compare our parser attach-
ment score to previously published results, the fol-
lowing steps must be done:
? We take the testing part of each treebank (the
file test.conll) and remove all the punctu-
ation marks. If the punctuation node is not a
leaf, its children are attached to the parent of
the removed node.
? Some previous papers report results on up-to-
10-words sentences only. Therefore we extract
such sentences from the test data and evaluate
on this subsets as well.
12Here we make use of manually annotated trees. However,
we use only English treebank an we are setting only four num-
bers out of several previously given values (e.g ?e out of 0.01,
0.1, 1, 10). These numbers could be tuned also by inspecting
the outputs. So we believe this method can be treated as unsu-
pervised.
CoNLL ? 10 tokens all sentences
language year gil11 our spi11 our
Arabic 06 ? 40.5 16.6 26.5
Arabic 07 ? 48.0 49.5 27.9
Basque 07 ? 30.8 24.0 26.8
Bulgarian 06 58.3 53.2 43.9 46.0
Catalan 07 ? 63.5 59.8 47.0
Czech 06 53.2 58.9 27.7 49.5
Czech 07 ? 63.7 28.4 48.0
Danish 06 45.9 49.5 38.3 38.6
Dutch 06 33.5 48.8 27.8 44.2
English 07 ? 64.1 45.2 49.2
German 06 46.7 60.8 30.4 44.8
Greek 07 ? 30.2 13.2 20.2
Hungarian 07 ? 61.8 34.7 51.8
Italian 07 ? 50.5 52.3 43.3
Japanese 06 57.7 65.4 50.2 50.8
Portuguese 06 54.0 62.3 36.7 50.6
Slovenian 06 50.9 21.0 32.2 18.1
Spanish 06 57.9 67.3 50.6 51.9
Swedish 06 45.0 60.5 50.0 48.2
Turkish 07 ? 13.0 35.9 15.7
Average: 50.3? 54.7? 37.4 40.0
Table 5: Comparison of directed attachment scores
with previously reported results on CoNLL tree-
banks. The column ?gil11? contains results reported
by Gillenwater et al011) (see the best configura-
tion in Table 7 in their paper). They provided only
results on sentences of up to 10 tokens from CoNLL
2006 treebanks. Results in the column ?spi11? are
taken from Spitkovsky et al011b), best configu-
ration in Table 6 in their paper. The average score
in the last line is computed across all comparable
results, i.e. for comparison with ?gil11? only the
CoNLL?06 results are averaged (?). Our parser was
not evaluated on Turkish CoNLL?06 data and Chi-
nese data, because we have not them available.
The resulting scores are given in Table 5. We
compare our results with results previously reported
by Gillenwater (2011) and Spitkovsky (2011b), who
used the CoNLL data for evaluation too. Since they
provide results for several configurations of their
parsers, we choose only the best one from each the
paper. We define the best configuration as the one
304
with the highest average attachment score across all
the tested languages.
We can see that our parser outperforms the pre-
viously published ones. In one case, it is better for
8 out of 10 data sets, in the other case, it is better
for 14 out of 20 data sets. The average attachment
scores, which are computed only from the results
present for both compared parsers, also confirm the
improvement.
However, it is important to note that we used an
additional source of information, namely large unan-
notated corpora for computing reducibility scores,
while the others used only the CoNLL data.
6.4 Error Analysis
Our main motivation for developing an unsupervised
dependency parser was that we wanted to be able
to parse any language. However, the experiments
show that our parser fails for some languages. In
this section, we try to analyze and explain some of
the most substantial types of errors.
Auxiliary verbs in Slovenian ? In the Slovenian
treebank, many verbs are composed of two words:
main verb (marked as Verb-main) and auxiliary
verb (Verb-copula). Our parser choose the aux-
iliary verb as the head and the main verb and all its
dependants become its children. That is why the at-
tachment score is so poor (only 18.1%). In fact, the
induced structure is not so bad. The main verb is
switched with the auxiliary one which causes also
the wrong attachment of all its dependants.
Articles in German ? Attachment of about one
half of German articles is wrong. Instead of the ar-
ticle being attached below the appropriate noun, the
noun is attached below the article. It is a similar
problem as the aforementioned Slovenian auxiliary
verbs. The dependency between content and func-
tion word is switched and the dependants of the con-
tent word are attached to the function word. Klein
and Manning (2004) observed a similar behavior in
their experiments with DMV.
Noun phrases in English ? The structure of
phrases that consist of more nouns are often induced
badly. This is caused probably by ignoring word
forms. For example, the structure of the sequence
?NN NN NN? can be hardly recognized by our parser.
fert. edge dist. subtr. en de cs
(random baseline) 19.8 18.4 26.7
X 8.71 13.7 14.9
X 18.9 20.2 26.5
X 23.6 19.5 25.3
X 28.2 23.7 33.5
X X 21.2 22.9 23.5
X X 19.9 19.7 25.5
X X 7.8 17.5 22.7
X X 24.1 19.5 27.1
X X 25.5 27.5 40.7
X X 31.2 25.2 33.1
X X X 30.7 26.2 22.0
X X X 14.1 18.1 34.6
X X X 36.1 32.2 38.9
X X X 34.8 26.7 42.4
X X X X 46.8 36.5 47.2
Table 6: Ablation analysis. Unlabeled attachment
scores for different combinations of model compo-
nents (fertility model, edge model, distance model
and subtree model). The scores are computed on all
sentences of the development data. Punctuation is
included into the evaluation.
6.5 Ablation Analysis
To investigate the impact of individual components
of the model, we run the parser for all possible com-
ponent combinations. We choose three languages
along the scale of word order freedom: English
(very rigid word order), Czech (relatively free word
order), and German (somewhere in the middle). The
attachment scores are shown in Table 6. If no model
is used for the inference and the sampling algorithm
samples completely random trees, we get the ran-
dom baseline score, which is 19.8% for English13.
From the perspective of the subtree model, which
implements the reducibility feature, we can see that
it is the most useful model here. Alone, it improves
the score for English to 28.2%. If we do not use
it, the score decreases from 46.8% (when all mod-
els are used) to 30.7%. Very important is also the
distance model which eliminates the possibility of
attaching all words to one head word. If we omit
13This relatively high baseline scores are caused by the MST
algorithm, which chooses the most frequent edges from random
trees i.e. the shortest ones.
305
it, the score for English falls drastically to 14.1%.
Some combinations of models have their scores far
below the baseline. This is caused by the fact that
some regularities have been found but the structures
are induced differently and thus all attachments are
wrong.
6.6 Induction without Wikipedia Corpus
We have performed also experiments using exclu-
sively the CoNLL data. However, the numbers of
reducible words in CoNLL training set were very
low (50 words at maximum in CoNLL 2006 train-
ing data and 10 words at maximum in CoNLL 2007
training data). This led to completely unreliable re-
ducibility scores and the consequent poor results.
7 Conclusions and Future Work
We have shown that employing the reducibility fea-
ture is useful in unsupervised dependency parsing
task. We extracted the n-gram reducibility scores
from a large corpus, and then made the computation-
ally demanding inference on smaller data using only
these scores. We evaluated our parser on 18 lan-
guages included in CoNLL and for 14 of them, we
achieved higher attachment scores than previously
published results.
The most errors were caused by function words,
which sometimes take over the dependents of adja-
cent content words. This can be caused by the fact
that the reducibility cannot handle function words
correctly, because they must be reduced together
with a content word, not one after another.
In future work, we would like to estimate the
hyperparameters automatically. Furthermore, we
would like to get rid of manually designed PoS tags
and use some kind of unsupervised clusters in order
to have all the annotation process completely unsu-
pervised. We would also like to employ lexicalized
models that should help in situations in which the
PoS tags are too coarse.
Finally, we would like to move towards deeper
syntactic structures, where the tree would be formed
only by content words and the function words would
be treated in a different way.
Software
The source code of our unsupervised dependency
parser including the script for computing reducibil-
ity scores from large corpora is available at
http://ufal.mff.cuni.cz/?marecek/udp.
Acknowledgement
This research was supported by the grants
GAUK 116310, GA201/09/H057 (Res Infor-
matica), LM2010013, MSM0021620838, and
by the European Commission?s 7th Framework
Program (FP7) under grant agreement n? 247762
(FAUST). We thank anonymous reviewers for their
valuable comments and suggestions.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 1204?1213, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. Proceedings of the sixth conference
on Applied natural language processing, page 8.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning, CoNLL-X ?06,
pages 149?164, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Y. J. Chu and T. H. Liu. 1965. On the Shortest Arbores-
cence of a Directed Graph. Science Sinica, 14:1396?
1400.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Neural Information
Processing Systems, pages 321?328.
Jason Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?345,
Copenhagen, August.
Kim Gerdes and Sylvain Kahane. 2011. Defining depen-
dencies (and constituents). In Proceedings of Depen-
dency Linguistics 2011, Barcelona.
Walter R. Gilks, S. Richardson, and David J. Spiegelhal-
ter. 1996. Markov chain Monte Carlo in practice. In-
terdisciplinary statistics. Chapman & Hall.
306
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a, Fer-
nando Pereira, and Ben Taskar. 2011. Posterior
Sparsity in Unsupervised Dependency Parsing. The
Journal of Machine Learning Research, 12:455?490,
February.
Jir??? Havelka. 2007. Beyond Projectivity: Multilin-
gual Evaluation of Constraints and Measures on Non-
Projective Structures. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, pages 608?615.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 101?109, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?04, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre.
2009. Dependency Parsing. Synthesis Lectures on
Human Language Technologies. Morgan & Claypool
Publishers.
Marke?ta Lopatkova?, Martin Pla?tek, and Vladislav Kubon?.
2005. Modeling syntax of free word-order languages:
Dependency analysis by reduction. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors,
Lecture Notes in Artificial Intelligence, Proceedings of
the 8th International Conference, TSD 2005, volume
3658 of Lecture Notes in Computer Science, pages
140?147, Berlin / Heidelberg. Springer.
Martin Majlis? and Zdene?k Z?abokrtsky?. 2012. Language
richness of the web. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), Istanbul, Turkey, May. Eu-
ropean Language Resources Association (ELRA).
David Marec?ek and Zdene?k Z?abokrtsky?. 2011. Gibbs
Sampling with Treeness constraint in Unsupervised
Dependency Parsing. In Proceedings of RANLP Work-
shop on Robust Unsupervised and Semisupervised
Methods in Natural Language Processing, pages 1?8,
Hissar, Bulgaria.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 62?72, Edinburgh, Scotland, UK., July. Associ-
ation for Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Depen-
dency Parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Noah Ashton Smith. 2007. Novel estimation methods
for unsupervised discovery of latent structure in natu-
ral language text. Ph.D. thesis, Baltimore, MD, USA.
AAI3240799.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang,
and Daniel Jurafsky. 2011a. Unsupervised depen-
dency parsing without gold part-of-speech tags. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011b. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011c. Punctuation: Making a point in unsu-
pervised dependency parsing. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL-2011).
307
Proceedings of the ACL 2010 Conference Short Papers, pages 86?91,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Tackling Sparse Data Issue in Machine Translation Evaluation ?
Ondr?ej Bojar, Kamil Kos, and David Marec?ek
Charles University in Prague, Institute of Formal and Applied Linguistics
{bojar,marecek}@ufal.mff.cuni.cz, kamilkos@email.cz
Abstract
We illustrate and explain problems of
n-grams-based machine translation (MT)
metrics (e.g. BLEU) when applied to
morphologically rich languages such as
Czech. A novel metric SemPOS based
on the deep-syntactic representation of the
sentence tackles the issue and retains the
performance for translation to English as
well.
1 Introduction
Automatic metrics of machine translation (MT)
quality are vital for research progress at a fast
pace. Many automatic metrics of MT quality have
been proposed and evaluated in terms of correla-
tion with human judgments while various tech-
niques of manual judging are being examined as
well, see e.g. MetricsMATR08 (Przybocki et al,
2008)1, WMT08 and WMT09 (Callison-Burch et
al., 2008; Callison-Burch et al, 2009)2.
The contribution of this paper is twofold. Sec-
tion 2 illustrates and explains severe problems of a
widely used BLEU metric (Papineni et al, 2002)
when applied to Czech as a representative of lan-
guages with rich morphology. We see this as an
instance of the sparse data problem well known
for MT itself: too much detail in the formal repre-
sentation leading to low coverage of e.g. a transla-
tion dictionary. In MT evaluation, too much detail
leads to the lack of comparable parts of the hy-
pothesis and the reference.
? This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003
of the Czech Republic), FP7-ICT-2009-4-247762 (Faust),
GA201/09/H057, GAUK 1163/2010, and MSM 0021620838.
We are grateful to the anonymous reviewers for further re-
search suggestions.
1http://nist.gov/speech/tests
/metricsmatr/2008/results/
2http://www.statmt.org/wmt08 and wmt09
0.06 0.08 0.10 0.12 0.14
0.4
0.6 b
cu-bojar
b
google
b
uedin
b
eurotranxp
b
pctrans
b
cu-tectomt
BLEU
Rank
Figure 1: BLEU and human ranks of systems par-
ticipating in the English-to-Czech WMT09 shared
task.
Section 3 introduces and evaluates some new
variations of SemPOS (Kos and Bojar, 2009), a
metric based on the deep syntactic representation
of the sentence performing very well for Czech as
the target language. Aside from including depen-
dency and n-gram relations in the scoring, we also
apply and evaluate SemPOS for English.
2 Problems of BLEU
BLEU (Papineni et al, 2002) is an established
language-independent MT metric. Its correlation
to human judgments was originally deemed high
(for English) but better correlating metrics (esp.
for other languages) were found later, usually em-
ploying language-specific tools, see e.g. Przy-
bocki et al (2008) or Callison-Burch et al (2009).
The unbeaten advantage of BLEU is its simplicity.
Figure 1 illustrates a very low correlation to hu-
man judgments when translating to Czech. We
plot the official BLEU score against the rank es-
tablished as the percentage of sentences where a
system ranked no worse than all its competitors
(Callison-Burch et al, 2009). The systems devel-
oped at Charles University (cu-) are described in
Bojar et al (2009), uedin is a vanilla configuration
of Moses (Koehn et al, 2007) and the remaining
ones are commercial MT systems.
In a manual analysis, we identified the reasons
for the low correlation: BLEU is overly sensitive
to sequences and forms in the hypothesis matching
86
Con- Error
firmed Flags 1-grams 2-grams 3-grams 4-grams
Yes Yes 6.34% 1.58% 0.55% 0.29%
Yes No 36.93% 13.68% 5.87% 2.69%
No Yes 22.33% 41.83% 54.64% 63.88%
No No 34.40% 42.91% 38.94% 33.14%
Total n-grams 35,531 33,891 32,251 30,611
Table 1: n-grams confirmed by the reference and
containing error flags.
the reference translation. This focus goes directly
against the properties of Czech: relatively free
word order allows many permutations of words
and rich morphology renders many valid word
forms not confirmed by the reference.3 These
problems are to some extent mitigated if several
reference translations are available, but this is of-
ten not the case.
Figure 2 illustrates the problem of ?sparse data?
in the reference. Due to the lexical and morpho-
logical variance of Czech, only a single word in
each hypothesis matches a word in the reference.
In the case of pctrans, the match is even a false
positive, ?do? (to) is a preposition that should be
used for the ?minus? phrase and not for the ?end
of the day? phrase. In terms of BLEU, both hy-
potheses are equally poor but 90% of their tokens
were not evaluated.
Table 1 estimates the overall magnitude of this
issue: For 1-grams to 4-grams in 1640 instances
(different MT outputs and different annotators) of
200 sentences with manually flagged errors4, we
count how often the n-gram is confirmed by the
reference and how often it contains an error flag.
The suspicious cases are n-grams confirmed by
the reference but still containing a flag (false posi-
tives) and n-grams not confirmed despite contain-
ing no error flag (false negatives).
Fortunately, there are relatively few false posi-
tives in n-gram based metrics: 6.3% of unigrams
and far fewer higher n-grams.
The issue of false negatives is more serious and
confirms the problem of sparse data if only one
reference is available. 30 to 40% of n-grams do
not contain any error and yet they are not con-
3Condon et al (2009) identify similar issues when eval-
uating translation to Arabic and employ rule-based normal-
ization of MT output to improve the correlation. It is beyond
the scope of this paper to describe the rather different nature
of morphological richness in Czech, Arabic and also other
languages, e.g. German or Finnish.
4The dataset with manually flagged errors is available at
http://ufal.mff.cuni.cz/euromatrixplus/
firmed by the reference. This amounts to 34% of
running unigrams, giving enough space to differ in
human judgments and still remain unscored.
Figure 3 documents the issue across languages:
the lower the BLEU score itself (i.e. fewer con-
firmed n-grams), the lower the correlation to hu-
man judgments regardless of the target language
(WMT09 shared task, 2025 sentences per lan-
guage).
Figure 4 illustrates the overestimation of scores
caused by too much attention to sequences of to-
kens. A phrase-based system like Moses (cu-
bojar) can sometimes produce a long sequence of
tokens exactly as required by the reference, lead-
ing to a high BLEU score. The framed words
in the illustration are not confirmed by the refer-
ence, but the actual error in these words is very
severe for comprehension: nouns were used twice
instead of finite verbs, and a misleading transla-
tion of a preposition was chosen. The output by
pctrans preserves the meaning much better despite
not scoring in either of the finite verbs and produc-
ing far shorter confirmed sequences.
3 Extensions of SemPOS
SemPOS (Kos and Bojar, 2009) is inspired by met-
rics based on overlapping of linguistic features in
the reference and in the translation (Gime?nez and
Ma?rquez, 2007). It operates on so-called ?tec-
togrammatical? (deep syntactic) representation of
the sentence (Sgall et al, 1986; Hajic? et al, 2006),
formally a dependency tree that includes only au-
tosemantic (content-bearing) words.5 SemPOS as
defined in Kos and Bojar (2009) disregards the
syntactic structure and uses the semantic part of
speech of the words (noun, verb, etc.). There are
19 fine-grained parts of speech. For each semantic
part of speech t, the overlapping O(t) is set to zero
if the part of speech does not occur in the reference
or the candidate set and otherwise it is computed
as given in Equation 1 below.
5We use TectoMT (Z?abokrtsky? and Bojar, 2008),
http://ufal.mff.cuni.cz/tectomt/, for the lin-
guistic pre-processing. While both our implementation of
SemPOS as well as TectoMT are in principle freely avail-
able, a stable public version has yet to be released. Our plans
include experiments with approximating the deep syntactic
analysis with a simple tagger, which would also decrease the
installation burden and computation costs, at the expense of
accuracy.
87
SRC Prague Stock Market falls to minus by the end of the trading day
REF praz?ska? burza se ke konci obchodova?n?? propadla do minusu
cu-bojar praha stock market klesne k minus na konci obchodn??ho dne
pctrans praha trh cenny?ch pap??ru? pada? minus do konce obchodn??ho dne
Figure 2: Sparse data in BLEU evaluation: Large chunks of hypotheses are not compared at all. Only a
single unigram in each hypothesis is confirmed in the reference.
-
0.2 0
 
0.2
 
0.4
 
0.6
 
0.8 1  0
.05
 
0.1
 
0.1
5
 
0.2
 
0.2
5
 
0.3
Correlation
BL
EU
 sc
ore
cs
-en
de-
en
es
-en
fr-e
n
hu-
en
en
-cs
en
-de
en
-es
en
-fr
Figure 3: BLEU correlates with its correlation to human judgments. BLEU scores around 0.1 predict
little about translation quality.
O(t) =
?
i?I
?
w?ri?ci
min(cnt(w, t, ri), cnt(w, t, ci))
?
i?I
?
w?ri?ci
max(cnt(w, t, ri), cnt(w, t, ci))
(1)
The semantic part of speech is denoted t; ci
and ri are the candidate and reference translations
of sentence i, and cnt(w, t, rc) is the number of
wordsw with type t in rc (the reference or the can-
didate). The matching is performed on the level of
lemmas, i.e. no morphological information is pre-
served in ws. See Figure 5 for an example; the
sentence is the same as in Figure 4.
The final SemPOS score is obtained by macro-
averaging over all parts of speech:
SemPOS =
1
|T |
?
t?T
O(t) (2)
where T is the set of all possible semantic parts
of speech types. (The degenerate case of blank
candidate and reference has SemPOS zero.)
3.1 Variations of SemPOS
This section describes our modifications of Sem-
POS. All methods are evaluated in Section 3.2.
Different Classification of Autosemantic
Words. SemPOS uses semantic parts of speech
to classify autosemantic words. The tectogram-
matical layer offers also a feature called Functor
describing the relation of a word to its governor
similarly as semantic roles do. There are 67
functor types in total.
Using Functor instead of SemPOS increases the
number of word classes that independently require
a high overlap. For a contrast we also completely
remove the classification and use only one global
class (Void).
Deep Syntactic Relations in SemPOS. In
SemPOS, an autosemantic word of a class is con-
firmed if its lemma matches the reference. We uti-
lize the dependency relations at the tectogrammat-
ical layer to validate valence by refining the over-
lap and requiring also the lemma of 1) the parent
(denoted ?par?), or 2) all the children regardless of
their order (denoted ?sons?) to match.
Combining BLEU and SemPOS. One of the
major drawbacks of SemPOS is that it completely
ignores word order. This is too coarse even for
languages with relatively free word order like
Czech. Another issue is that it operates on lemmas
and it completely disregards correct word forms.
Thus, a weighted linear combination of SemPOS
and BLEU (computed on the surface representa-
tion of the sentence) should compensate for this.
For the purposes of the combination, we compute
BLEU only on unigrams up to fourgrams (denoted
BLEU1, . . . , BLEU4) but including the brevity
penalty as usual. Here we try only a few weight
settings in the linear combination but given a held-
out dataset, one could optimize the weights for the
best performance.
88
SRC Congress yields: US government can pump 700 billion dollars into banks
REF kongres ustoupil : vla?da usa mu?z?e do bank napumpovat 700 miliard dolaru?
cu-bojar kongres vy?nosy : vla?da usa mu?z?e c?erpadlo 700 miliard dolaru? v banka?ch
pctrans kongres vyna?s??? : us vla?da mu?z?e c?erpat 700 miliardu dolaru? do bank
Figure 4: Too much focus on sequences in BLEU: pctrans? output is better but does not score well.
BLEU gave credit to cu-bojar for 1, 3, 5 and 8 fourgrams, trigrams, bigrams and unigrams, resp., but
only for 0, 0, 1 and 8 n-grams produced by pctrans. Confirmed sequences of tokens are underlined and
important errors (not considered by BLEU) are framed.
REF kongres/n ustoupit/v :/n vla?da/n usa/n banka/n napumpovat/v 700/n miliarda/n dolar/n
cu-bojar kongres/n vy?nos/n :/n vla?da/n usa/n moci/v c?erpadlo/n 700/n miliarda/n dolar/n banka/n
pctrans kongres/n vyna?s?et/v :/n us/n vla?da/n c?erpat/v 700/n miliarda/n dolar/n banka/n
Figure 5: SemPOS evaluates the overlap of lemmas of autosemantic words given their semantic part of
speech (n, v, . . . ). Underlined words are confirmed by the reference.
SemPOS for English. The tectogrammatical
layer is being adapted for English (Cinkova? et al,
2004; Hajic? et al, 2009) and we are able to use the
available tools to obtain all SemPOS features for
English sentences as well.
3.2 Evaluation of SemPOS and Friends
We measured the metric performance on data used
in MetricsMATR08, WMT09 and WMT08. For
the evaluation of metric correlation with human
judgments at the system level, we used the Pearson
correlation coefficient ? applied to ranks. In case
of a tie, the systems were assigned the average po-
sition. For example if three systems achieved the
same highest score (thus occupying the positions
1, 2 and 3 when sorted by score), each of them
would obtain the average rank of 2 = 1+2+33 .
When correlating ranks (instead of exact scores)
and with this handling of ties, the Pearson coeffi-
cient is equivalent to Spearman?s rank correlation
coefficient.
The MetricsMATR08 human judgments include
preferences for pairs of MT systems saying which
one of the two systems is better, while the WMT08
and WMT09 data contain system scores (for up to
5 systems) on the scale 1 to 5 for a given sentence.
We assigned a human ranking to the systems based
on the percent of time that their translations were
judged to be better than or equal to the translations
of any other system in the manual evaluation. We
converted automatic metric scores to ranks.
Metrics? performance for translation to English
and Czech was measured on the following test-
sets (the number of human judgments for a given
source language in brackets):
To English: MetricsMATR08 (cn+ar: 1652),
WMT08 News Articles (de: 199, fr: 251),
WMT08 Europarl (es: 190, fr: 183), WMT09
(cz: 320, de: 749, es: 484, fr: 786, hu: 287)
To Czech: WMT08 News Articles (en: 267),
WMT08 Commentary (en: 243), WMT09
(en: 1425)
The MetricsMATR08 testset contained 4 refer-
ence translations for each sentence whereas the re-
maining testsets only one reference.
Correlation coefficients for English are shown
in Table 2. The best metric is Voidpar closely fol-
lowed by Voidsons. The explanation is that Void
compared to SemPOS or Functor does not lose
points by an erroneous assignment of the POS or
the functor, and that Voidpar profits from check-
ing the dependency relations between autoseman-
tic words. The combination of BLEU and Sem-
POS6 outperforms both individual metrics, but in
case of SemPOS only by a minimal difference.
Additionally, we confirm that 4-grams alone have
little discriminative power both when used as a
metric of their own (BLEU4) as well as in a lin-
ear combination with SemPOS.
The best metric for Czech (see Table 3) is a lin-
ear combination of SemPOS and 4-gram BLEU
closely followed by other SemPOS and BLEUn
combinations. We assume this is because BLEU4
can capture correctly translated fixed phrases,
which is positively reflected in human judgments.
Including BLEU1 in the combination favors trans-
lations with word forms as expected by the refer-
6For each n ? {1, 2, 3, 4}, we show only the best weight
setting for SemPOS and BLEUn.
89
Metric Avg Best Worst
Voidpar 0.75 0.89 0.60
Voidsons 0.75 0.90 0.54
Void 0.72 0.91 0.59
Functorsons 0.72 1.00 0.43
GTM 0.71 0.90 0.54
4?SemPOS+1?BLEU2 0.70 0.93 0.43
SemPOSpar 0.70 0.93 0.30
1?SemPOS+4?BLEU3 0.70 0.91 0.26
4?SemPOS+1?BLEU1 0.69 0.93 0.43
NIST 0.69 0.90 0.53
SemPOSsons 0.69 0.94 0.40
SemPOS 0.69 0.95 0.30
2?SemPOS+1?BLEU4 0.68 0.91 0.09
BLEU1 0.68 0.87 0.43
BLEU2 0.68 0.90 0.26
BLEU3 0.66 0.90 0.14
BLEU 0.66 0.91 0.20
TER 0.63 0.87 0.29
PER 0.63 0.88 0.32
BLEU4 0.61 0.90 -0.31
Functorpar 0.57 0.83 -0.03
Functor 0.55 0.82 -0.09
Table 2: Average, best and worst system-level cor-
relation coefficients for translation to English from
various source languages evaluated on 10 different
testsets.
ence, thus allowing to spot bad word forms. In
all cases, the linear combination puts more weight
on SemPOS. Given the negligible difference be-
tween SemPOS alone and the linear combinations,
we see that word forms are not the major issue for
humans interpreting the translation?most likely
because the systems so far often make more im-
portant errors. This is also confirmed by the obser-
vation that using BLEU alone is rather unreliable
for Czech and BLEU-1 (which judges unigrams
only) is even worse. Surprisingly BLEU-2 per-
formed better than any other n-grams for reasons
that have yet to be examined. The error metrics
PER and TER showed the lowest correlation with
human judgments for translation to Czech.
4 Conclusion
This paper documented problems of single-
reference BLEU when applied to morphologically
rich languages such as Czech. BLEU suffers from
a sparse data problem, unable to judge the quality
of tokens not confirmed by the reference. This is
confirmed for other languages as well: the lower
the BLEU score the lower the correlation to hu-
man judgments.
We introduced a refinement of SemPOS, an
automatic metric of MT quality based on deep-
syntactic representation of the sentence tackling
Metric Avg Best Worst
3?SemPOS+1?BLEU4 0.55 0.83 0.14
2?SemPOS+1?BLEU2 0.55 0.83 0.14
2?SemPOS+1?BLEU1 0.53 0.83 0.09
4?SemPOS+1?BLEU3 0.53 0.83 0.09
SemPOS 0.53 0.83 0.09
BLEU2 0.43 0.83 0.09
SemPOSpar 0.37 0.53 0.14
Functorsons 0.36 0.53 0.14
GTM 0.35 0.53 0.14
BLEU4 0.33 0.53 0.09
Void 0.33 0.53 0.09
NIST 0.33 0.53 0.09
Voidsons 0.33 0.53 0.09
BLEU 0.33 0.53 0.09
BLEU3 0.33 0.53 0.09
BLEU1 0.29 0.53 -0.03
SemPOSsons 0.28 0.42 0.03
Functorpar 0.23 0.40 0.14
Functor 0.21 0.40 0.09
Voidpar 0.16 0.53 -0.08
PER 0.12 0.53 -0.09
TER 0.07 0.53 -0.23
Table 3: System-level correlation coefficients for
English-to-Czech translation evaluated on 3 differ-
ent testsets.
the sparse data issue. SemPOS was evaluated on
translation to Czech and to English, scoring better
than or comparable to many established metrics.
References
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-
tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?k
Z?abokrtsky?. 2009. English-Czech MT in 2008. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, Athens, Greece, March. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
workshop on statistical machine translation. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, Athens, Greece. Association for
Computational Linguistics.
Silvie Cinkova?, Jan Hajic?, Marie Mikulova?, Lu-
cie Mladova?, Anja Nedoluz?ko, Petr Pajas, Jarmila
Panevova?, Jir??? Semecky?, Jana S?indlerova?, Josef
Toman, Zden?ka Ures?ova?, and Zdene?k Z?abokrtsky?.
2004. Annotation of English on the tectogram-
matical level. Technical Report TR-2006-35,
U?FAL/CKL, Prague, Czech Republic, December.
90
Sherri Condon, Gregory A. Sanders, Dan Parvaz, Alan
Rubenstein, Christy Doran, John Aberdeen, and
Beatrice Oshika. 2009. Normalization for Auto-
mated Metrics: English and Arabic Speech Transla-
tion. In MT Summit XII.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
nous MT Systems. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
256?264, Prague, June. Association for Computa-
tional Linguistics.
Jan Hajic?, Silvie Cinkova?, Kristy?na C?erma?kova?, Lu-
cie Mladova?, Anja Nedoluz?ko, Petr Pajas, Jir??? Se-
mecky?, Jana S?indlerova?, Josef Toman, Kristy?na
Toms?u?, Mate?j Korvas, Magdale?na Rysova?, Kater?ina
Veselovska?, and Zdene?k Z?abokrtsky?. 2009. Prague
English Dependency Treebank 1.0. Institute of For-
mal and Applied Linguistics, Charles University in
Prague, ISBN 978-80-904175-0-2, January.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova? Raz??mova?. 2006. Prague Dependency
Treebank 2.0. LDC2006T01, ISBN: 1-58563-370-
4.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of Ma-
chine Translation Metrics for Czech as the Target
Language. Prague Bulletin of Mathematical Lin-
guistics, 92.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In ACL 2002,
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania.
M. Przybocki, K. Peterson, and S. Bronsart. 2008. Of-
ficial results of the NIST 2008 ?Metrics for MA-
chine TRanslation? Challenge (MetricsMATR08).
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence and Its Semantic
and Pragmatic Aspects. Academia/Reidel Publish-
ing Company, Prague, Czech Republic/Dordrecht,
Netherlands.
Zdene?k Z?abokrtsky? and Ondr?ej Bojar. 2008. TectoMT,
Developer?s Guide. Technical Report TR-2008-39,
Institute of Formal and Applied Linguistics, Faculty
of Mathematics and Physics, Charles University in
Prague, December.
91
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 281?290,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Stop-probability estimates computed on a large corpus
improve Unsupervised Dependency Parsing
David Marec?ek and Milan Straka
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, 11800 Prague, Czech Republic
{marecek,straka}@ufal.mff.cuni.cz
Abstract
Even though the quality of unsupervised
dependency parsers grows, they often fail
in recognition of very basic dependencies.
In this paper, we exploit a prior knowledge
of STOP-probabilities (whether a given
word has any children in a given direc-
tion), which is obtained from a large raw
corpus using the reducibility principle. By
incorporating this knowledge into Depen-
dency Model with Valence, we managed to
considerably outperform the state-of-the-
art results in terms of average attachment
score over 20 treebanks from CoNLL 2006
and 2007 shared tasks.
1 Introduction
The task of unsupervised dependency parsing
(which strongly relates to the grammar induction
task) has become popular in the last decade, and
its quality has been greatly increasing during this
period.
The first implementation of Dependency Model
with Valence (DMV) (Klein and Manning, 2004)
with a simple inside-outside inference algo-
rithm (Baker, 1979) achieved 36% attachment
score on English and was the first system outper-
forming the adjacent-word baseline.1
Current attachment scores of state-of-the-art un-
supervised parsers are higher than 50% for many
languages (Spitkovsky et al, 2012; Blunsom and
Cohn, 2010). This is still far below the super-
vised approaches, but their indisputable advan-
tage is the fact that no annotated treebanks are
needed and the induced structures are not bur-
dened by any linguistic conventions. Moreover,
1The adjacent-word baseline is a dependency tree in
which each word is attached to the previous (or the follow-
ing) word. The attachment score of 35.9% on all the WSJ
test sentences was taken from (Blunsom and Cohn, 2010).
supervised parsers always only simulate the tree-
banks they were trained on, whereas unsupervised
parsers have an ability to be fitted to different par-
ticular applications.
Some of the current approaches are based on
the DMV, a generative model where the gram-
mar is expressed by two probability distributions:
Pchoose(cd|ch, dir), which generates a new child
cd attached to the head ch in the direction dir (left
or right), and Pstop(STOP |ch, dir , ? ? ? ), which
makes a decision whether to generate another
child of ch in the direction dir or not.2 Such a
grammar is then inferred using sampling or varia-
tional methods.
Unfortunately, there are still cases where the in-
ferred grammar is very different from the gram-
mar we would expect, e.g. verbs become leaves
instead of governing the sentences. Rasooli and
Faili (2012) and Bisk and Hockenmaier (2012)
made some efforts to boost the verbocentricity of
the inferred structures; however, both of the ap-
proaches require manual identification of the POS
tags marking the verbs, which renders them use-
less when unsupervised POS tags are employed.
The main contribution of this paper is a consid-
erable improvement of unsupervised parsing qual-
ity by estimating the Pstop probabilities externally
using a very large corpus, and employing this prior
knowledge in the standard inference of DMV. The
estimation is done using the reducibility principle
introduced in (Marec?ek and Z?abokrtsky?, 2012).
The reducibility principle postulates that if a word
(or a sequence of words) can be removed from
a sentence without violating its grammatical cor-
rectness, it is a leaf (or a subtree) in its dependency
structure. For the purposes of this paper, we as-
sume the following hypothesis:
If a sequence of words can be removed from
2The Pstop probability may be conditioned by additional
parameters, such as adjacency adj or fringe word cf , which
will be described in Section 4.
281
Figure 1: Example of a dependency tree. Se-
quences of words that can be reduced are under-
lined.
a sentence without violating its grammatical cor-
rectness, no word outside the sequence depends on
any word in the sequence.
Our hypothesis is a generalization of the origi-
nal hypothesis since it allows a reducible sequence
to form several adjacent subtrees.
Let?s outline the connection between the Pstop
probabilities and the property of reducibility. Fig-
ure 1 shows an example of a dependency tree. Se-
quences of reducible words are marked by thick
lines below the sentence. Consider for example
the word ?further?. It can be removed and thus,
according to our hypothesis, no other word de-
pends on it. Therefore, we can deduce that the
Pstop probability for such word is high both for
the left and for the right direction. The phrase
?for further discussions? is reducible as well and
we can deduce that the Pstop of its first word
(?for?) in the left direction is high since it cannot
have any left children. We do not know anything
about its right children, because they can be lo-
cated within the sequence (and there is really one
in Figure 1). Similarly, the word ?discussions?,
which is the last word in this sequence, cannot
have any right children and we can estimate that its
right Pstop probability is high. On the other hand,
non-reducible words such, as the verb ?asked? in
our example, can have children, and therefore their
Pstop can be estimated as low for both directions.
The most difficult task in this approach is to au-
tomatically recognize reducible sequences. This
problem, together with the estimation of the stop-
probabilities, is described in Section 3. Our
model, not much different from the classic DMV,
is introduced in Section 4. Section 5 describes the
inference algorithm based on Gibbs sampling. Ex-
periments and results are discussed in Section 6.
Section 7 concludes the paper.
2 Related Work
Reducibility: The notion of reducibility belongs
to the traditional linguistic criteria for recogniz-
ing dependency relations. As mentioned e.g. by
Ku?bler et al (2009), the head h of a construction c
determines the syntactic category of c and can of-
ten replace c. In other words, the descendants of h
can be often removed without making the sentence
incorrect. Similarly, in the Dependency Analysis
by Reduction (Lopatkova? et al, 2005), the authors
assume that stepwise deletions of dependent ele-
ments within a sentence preserve its syntactic cor-
rectness. A similar idea of dependency analysis
by splitting a sentence into all possible acceptable
fragments is used by Gerdes and Kahane (2011).
We have directly utilized the aforementioned
criteria for dependency relations in unsuper-
vised dependency parsing in our previous pa-
per (Marec?ek and Z?abokrtsky?, 2012). Our depen-
dency model contained a submodel which directly
prioritized subtrees that form reducible sequences
of POS tags. Reducibility scores of given POS tag
sequences were estimated using a large corpus of
Wikipedia articles. The weakness of this approach
was the fact that longer sequences of POS tags
are very sparse and no reducibility scores could
be estimated for them. In this paper, we avoid this
shortcoming by estimating the STOP probabilities
for individual POS tags only.
Another task related to reducibility is sentence
compression (Knight and Marcu, 2002; Cohn and
Lapata, 2008), which was used for text summa-
rization. The task is to shorten the sentences while
retaining the most important pieces of informa-
tion, using the knowledge of the grammar. Con-
versely, our task is to induce the grammar using
the sentences and their shortened versions.
Dependency Model with Valence (DMV) has
been the most popular approach to unsupervised
dependency parsing in the recent years. It was in-
troduced by Klein and Manning (2004) and fur-
ther improved by Smith (2007) and Cohen et al
(2008). Headden III et al (2009) introduce the
Extended Valence Grammar and add lexicaliza-
tion and smoothing. Blunsom and Cohn (2010)
use tree substitution grammars, which allow learn-
ing of larger dependency fragments by employ-
ing the Pitman-Yor process. Spitkovsky et al
(2010) improve the inference using iterated learn-
ing of increasingly longer sentences. Further im-
provements were achieved by better dealing with
punctuation (Spitkovsky et al, 2011b) and new
?boundary? models (Spitkovsky et al, 2012).
282
Other approaches to unsupervised dependency
parsing were described e.g. in (S?gaard, 2011),
(Cohen et al, 2011), and (Bisk and Hockenmaier,
2012). There also exist ?less unsupervised? ap-
proaches that utilize an external knowledge of the
POS tagset. For example, Rasooli and Faili (2012)
identify the last verb in the sentence, minimize
its probability of reduction and thus push it to
the root position. Naseem et al (2010) make use
of manually-specified universal dependency rules
such as Verb?Noun, Noun?Adjective. McDon-
ald et al (2011) identify the POS tags by a cross-
lingual transfer. Such approaches achieve better
results; however, they are useless for grammar in-
duction from plain text.
3 STOP-probability estimation
3.1 Recognition of reducible sequences
We introduced a simple procedure for recog-
nition of reducible sequences in (Marec?ek and
Z?abokrtsky?, 2012): The particular sequence of
words is removed from the sentence and if the
remainder of the sentence exists elsewhere in the
corpus, the sequence is considered reducible. We
provide an example in Figure 2. The bigram ?this
weekend? in the sentence ?The next competition
is this weekend at Lillehammer in Norway.? is re-
ducible since the same sentence without this bi-
gram, i.e., ?The next competition is at Lilleham-
mer in Norway.?, is in the corpus as well. Simi-
larly, the prepositional phrase ?of Switzerland? is
also reducible.
It is apparent that only very few reducible se-
quences can be found by this procedure. If we
use a corpus containing about 10,000 sentences, it
is possible that we found no reducible sequences
at all. However, we managed to find a sufficient
amount of reducible sequences in corpora contain-
ing millions of sentences, see Section 6.1 and Ta-
ble 1.
3.2 Computing the STOP-probability
estimations
Recall our hypothesis from Section 1: If a se-
quence of words is reducible, no word outside the
sequence can depend on any word in the sequence.
Or, in terms of dependency structure: A reducible
sequence consists of one or more adjacent sub-
trees. This means that the first word of a reducible
sequence does not have any left children and, sim-
ilarly, the last word in a reducible sequence does
Martin Fourcade was sixth , maintaining his lead at the top of 
the overall World Cup standings , although Svendsen is now 
only 59 points away from the Frenchman in second . The next 
competition is this weekend at Lillehammer in Norway .
Larinto saw off allcomers at Kuopio with jumps of 129.5 and 
124m for a total 240.9 points , just 0.1 points ahead of 
compatriot Matti Hautamaeki , who landed efforts of 127 and 
129.5m . Third place went to Simon Ammann . Andreas 
Kofler , who won at the weekend at Kuusamo , was fourth but 
stays top of the season standings with 150 points .
Third place went to Simon Ammann of Switzerland . Ammann 
is currently just fifth , overall with 120 points . The next 
competition is at Lillehammer in Norway .
Figure 2: Example of reducible sequences of
words found in a large corpus.
not have any right children. We make use of this
property directly for estimating Pstop probabili-
ties.
Hereinafter, P eststop(ch, dir) denotes the STOP-
probability we want to estimate from a large cor-
pus; ch is the head?s POS tag and dir is the direc-
tion in which the STOP probability is estimated.
If ch is very often in the first position of reducible
sequences, P eststop(ch, left) will be high. Similarly,
if ch is often in the last position of reducible se-
quences, P eststop(ch, right) will be high.
For each POS tag ch in the given corpus,
we first compute its left and right ?raw? score
Sstop(ch, left) and Sstop(ch, right) as the relative
number of times a word with POS tag ch was in
the first (or last) position in a reducible sequence
found in the corpus. We do not deal with se-
quences longer than a trigram since they are highly
biased.
Sstop(ch, left) =
# red.seq. [ch, . . . ] + ?
# ch in the corpus
Sstop(ch, right) =
# red.seq. [. . . , ch] + ?
# ch in the corpus
Note that the Sstop scores are not probabilities.
Their main purpose is to sort the POS tags accord-
ing to their ?reducibility?.
It may happen that for many POS tags there
are no reducible sequences found. To avoid zero
scores, we use a simple smoothing by adding ? to
each count:
? = # all reducible sequencesW ,
283
where W denotes the number of words in the
given corpus. Such smoothing ensures that more
frequent irreducible POS tags get a lower Sstop
score than the less frequent ones.
Since reducible sequences found are very
sparse, the values of Sstop(ch, dir) scores are very
small. To convert them to estimated probabilities
P eststop(ch, dir), we need a smoothing that fulfills
the following properties:
(1) P eststop is a probability and therefore its value
must be between 0 and 1.
(2) The number of no-stop decisions (no matter
in which direction) equals to W (number of
words) since such decision is made before
each word is generated. The number of stop
decisions is 2W since they come after gener-
ating the last children in both the directions.
Therefore, the average P eststop(h, dir) over all
words in the treebank should be 2/3.
After some experimenting, we chose the follow-
ing normalization formula
P eststop(ch, dir) =
Sstop(ch, dir)
Sstop(ch, dir) + ?
with a normalization constant ?. The condition
(1) is fulfilled for any positive value of ?. Its exact
value is set in accordance with the requirement (2)
so that the average value of P eststop is 2/3.
?
dir?{l,r}
?
c?C
count(c)P eststop(c, dir) =
2
3 ? 2W,
where count(c) is the number of words with POS
tag c in the corpus. We find the unique value of ?
that fulfills the previous equation numerically us-
ing a binary search algorithm.
4 Model
We use the standard generative Dependency
Model with Valence (Klein and Manning, 2004).
The generative story is the following: First, the
head of the sentence is generated. Then, for each
head, all its left children are generated, then the
left STOP, then all its right children, and then the
right STOP. When a child is generated, the al-
gorithm immediately recurses to generate its sub-
tree. When deciding whether to generate another
child in the direction dir or the STOP symbol,
we use the P dmvstop (STOP |ch, dir , adj , cf ) model.
The new child cd in the direction dir is generated
according to the Pchoose(cd|ch, dir) model. The
probability of the whole dependency tree T is the
following:
Ptree(T ) = Pchoose(head(T )|ROOT , right)
? Ptree(D(head(T )))
Ptree(D(ch)) =?
dir?{l,r}
?
cd?
deps(dir,h)
P dmvstop (?STOP |ch, dir , adj , cf )
Pchoose(cd|ch, dir)Ptree(D(cd))
P dmvstop (STOP |ch, dir , adj , cf ),
where Ptree(D(ch)) is probability of the subtree
governed by h in the tree T .
The set of features on which the P dmvstop and
Pchoose probabilities are conditioned varies among
the previous works. Our P dmvstop depends on the
head POS tag ch, direction dir , adjacency adj ,
and fringe POS tag cf (described below). The
use of adjacency is standard in DMV and enables
us to have different P dmvstop for situations when no
child was generated so far (adj = 1). That is,
P dmvstop (ch, dir , adj = 1, cf ) decides whether the
word ch has any children in the direction dir at
all, whereas P dmvstop (h, dir , adj = 0, cf ) decides
whether another child will be generated next to
the already generated one. This distinction is of
crucial importance for us: although we know how
to estimate the STOP probabilities for adj = 1
from large data, we do not know anything about
the STOP probabilities for adj = 0.
The last factor cf , called fringe, is the POS tag
of the previously generated sibling in the current
direction dir . If there is no such sibling (in case
adj = 1), the head ch is used as the fringe cf .
This is a relatively novel idea in DMV, introduced
by Spitkovsky et al (2012). We decided to use
the fringe word in our model since it gives slightly
better results.
We assume that the distributions of Pchoose and
P dmvstop are good if the majority of the probabil-
ity mass is concentrated on few factors; therefore,
we apply a Chinese Restaurant process (CRP) on
them.
The probability of generating a new child node
cd attached to ch in the direction dir given the his-
tory (all the nodes we have generated so far) is
284
computed using the following formula:
Pchoose(cd|ch, dir) =
=
?c 1|C| + count?(cd, ch, dir)
?c + count?(ch, dir)
,
where count?(cd, ch, dir) denotes the number of
times a child node cd has been attached to ch
in the direction dir in the history. Similarly,
count?(ch, dir) is the number of times something
has been attached to ch in the direction dir . The
?c is a hyperparameter and |C| is the number of
distinct POS tags in the corpus.3
The STOP probability is computed in a similar
way:
P dmvstop (STOP |ch, dir , adj , cf ) =
=?s
2
3 + count?(STOP , ch, dir , adj , cf )
?s + count?(ch, dir , adj , cf )
where count?(STOP , ch, dir , adj , cf ) is the
number of times a head ch had the last child cf
in the direction dir in the history.
The contribution of this paper is the inclusion
of the stop-probability estimates into the DMV.
Therefore, we introduce a new model P dmv+eststop ,
in which the probability based on the previously
generated data is linearly combined with the prob-
ability estimates based on large corpora (Sec-
tion 3).
P dmv+eststop (STOP |ch, dir , 1, cf ) =
= (1? ?) ? ?s
2
3 + count?(STOP , ch, dir , 1, cf )
?s + count?(ch, dir , 1, cf )
+? ? P eststop(ch, dir)
P dmv+eststop (STOP |ch, dir , 0, cf ) =
= P dmvstop (STOP |ch, dir , 0, cf )
The hyperparameter ? defines the ratio between
the CRP-based and estimation-based probability.
The definition of the P dmv+eststop for adj = 0 equals
the basic P dmvstop since we are able to estimate only
the probability whether a particular head POS tag
ch can or cannot have children in a particular di-
rection, i.e if adj = 1.
3The number of classes |C| is often used in the denomi-
nator. We decided to put its reverse value into the numerator
since we observed such model to perform better for a constant
value of ?c over different languages and tagsets.
Finally, we obtain the probability of the whole
generated treebank as a product over the trees:
Ptreebank =
?
T?treebank
Ptree(T ).
An important property of the CRP is the fact that
the factors are exchangeable ? i.e. no matter how
the trees are ordered in the treebank, the Ptreebank
is always the same.
5 Inference
We employ the Gibbs sampling algorithm (Gilks
et al, 1996). Unlike in (Marec?ek and Z?abokrtsky?,
2012), where edges were sampled individually,
we sample whole trees from all possibilities on a
given sentence using dynamic programming. The
algorithm works as follows:
1. A random projective dependency tree is as-
signed to each sentence in the corpus.
2. Sampling: We go through the sentences in a
random order. For each sentence, we sam-
ple a new dependency tree based on all other
trees that are currently in the corpus.
3. Step 2 is repeated in many iterations. In
this work, the number of iterations was set
to 1000.
4. After the burn-in period (which was set to the
first 500 iterations), we start collecting counts
of edges between particular words that ap-
peared during the sampling.
5. Parsing: Based on the collected counts, we
compute the final dependency trees using
the Chu-Liu/Edmonds? algorithm (1965) for
finding maximum directed spanning trees.
5.1 Sampling
Our goal is to sample a new projective dependency
tree T with probability proportional to Ptree(T ).
Since the factors are exchangeable, we can deal
with any tree as if it was the last one in the corpus.
We use dynamic programming to sample a
tree with N nodes in O(N4) time. Neverthe-
less, we sample trees using a modified probabil-
ity P ?tree(T ). In Ptree(T ), the probability of an
edge depends on counts of all other edges, includ-
ing the edges in the same tree. We instead use
P ?tree(T ), where the counts are computed using
only the other trees in the corpus, i.e., probabilities
285
of edges of T are independent. There is a stan-
dard way to sample using the real Ptree(T ) ? we
can use P ?tree(T ) as a proposal distribution in the
Metropolis-Hastings algorithm (Hastings, 1970),
which then produces trees with probabilities pro-
portional to Ptree(T ) using acceptance-rejection
scheme. We do not take this approach and we
sample proportionally to P ?tree(T ) only, because
we believe that for large enough corpora, the two
distributions are nearly identical.
To sample a tree containing words w1, . . . , wN
with probability proportional to P ?tree(T ), we first
compute three tables:
? ti(g, i, j) for g < i or g > j is the sum of
probabilities of any tree on words wi, . . . , wj
whose root is a child of wg, but not an outer-
most child in its direction;
? to(g, i, j) is the same, but the tree is the out-
ermost child of wg;
? fo(g, i, j) for g < i or g > j is the
sum of probabilities of any forest on words
wi, . . . , wj , such that all the trees are children
of wg and are the outermost children of wg in
their direction.
All the probabilities are computed using the P ?tree .
If we compute the tables inductively from the
smallest trees to the largest trees, we can precom-
pute all the O(N3) values in O(N4) time.
Using these tables, we sample the tree recur-
sively, starting from the root. At first, we sam-
ple the root r proportionally to the probability of
a tree with the root r, which is a product of the
probability of left children of r and right chil-
dren of r. The probability of left children of r
is either P ?stop(STOP |r, left) if r has no children,
or P ?stop(?STOP |r, left)fo(r, 1, r? 1) otherwise;
the probability of right children is analogous.
After sampling the root, we sample the ranges
of its left children, if any. We sample the first left
child range l1 proportionally either to to(r, 1, r?1)
if l1 = 1, or to ti(r, l1, r ? 1)fo(r, 1, l1 ? 1)
if l1 > 1. Then we sample the second left child
range l2 proportionally either to to(r, 1, l1 ? 1)
if l2 = 1, or to ti(r, l2, l1 ? 1)fo(r, 1, l2 ? 1)
if l2 > 1, and so on, while there are any left
children. The right children ranges are sampled
similarly. Finally, we recursively sample the chil-
dren, i.e., their roots, their children and so on. It
is simple to verify using the definition of Ptree that
the described method indeed samples trees propor-
tionally to P ?tree .
5.2 Parsing
Beginning the 500th iteration, we start collecting
counts of individual dependency edges during the
remaining iterations. After each iteration is fin-
ished (all the trees in the corpus are re-sampled),
we increment the counter of all directed pairs of
nodes which are connected by a dependency edge
in the current trees.
After the last iteration, we use these collected
counts as weights and compute maximum directed
spanning trees using the Chu-Liu/Edmonds? algo-
rithm (Chu and Liu, 1965). Therefore, the result-
ing trees consist of edges maximizing the sum of
individual counts:
TMST = argmax
T
?
e?T
count(e)
It is important to note that the MST algorithm
may produce non-projective trees. Even if we
average the strictly projective dependency trees,
some non-projective edges may appear in the re-
sult. This might be an advantage since correct
non-projective edges can be predicted; however,
this relaxation may introduce mistakes as well.
6 Experiments
6.1 Data
We use two types of resources in our experiments.
The first type are CoNLL treebanks from the year
2006 (Buchholz and Marsi, 2006) and 2007 (Nivre
et al, 2007), which we use for inference and for
evaluation. As is the standard practice in unsuper-
vised parsing evaluation, we removed all punctu-
ation marks from the trees. In case a punctuation
node was not a leaf, its children are attached to the
parent of the removed node.
For estimating the STOP probabilities (Sec-
tion 3), we use the Wikipedia articles from W2C
corpus (Majlis? and Z?abokrtsky?, 2012), which pro-
vide sufficient amount of data for our purposes.
Statistics across languages are shown in Table 1.
The Wikipedia texts were automatically tok-
enized and segmented to sentences so that their
tokenization was similar to the one in the CoNLL
evaluation treebanks. Unfortunately, we were not
able to find any segmenter for Chinese that would
produce a desired segmentation; therefore, we re-
moved Chinese from evaluation.
The next step was to provide the Wikipedia
texts with POS tags. We employed the TnT tag-
ger (Brants, 2000) which was trained on the re-
286
language tokens red. language tokens red.
(mil.) seq. (mil.) seq.
Arabic 19.7 546 Greek 20.9 1037
Basque 14.1 645 Hungarian 26.3 2237
Bulgarian 18.8 1808 Italian 39.7 723
Catalan 27.0 712 Japanese 2.6 31
Czech 20.3 930 Portuguese 31.7 4765
Danish 15.9 576 Slovenian 13.7 513
Dutch 27.1 880 Spanish 53.4 1156
English 85.0 7603 Swedish 19.2 481
German 56.9 1488 Turkish 16.5 5706
Table 1: Wikipedia texts statistics: total number of
tokens and number of reducible sequences found
in them.
spective CoNLL training data. The quality of such
tagging is not very high since we do not use any
lexicons or pretrained models. However, it is suf-
ficient for obtaining usable stop probability esti-
mates.
6.2 Estimated STOP probabilities
We applied the algorithm described in Section 3 on
the prepared Wikipedia corpora and obtained the
stop-probabilities P eststop in both directions for all
the languages and their POS tags. To evaluate the
quality of our estimations, we compare them with
P tbstop , the stop probabilities computed directly on
the evaluation treebanks. The comparisons on five
selected languages are shown in Figure 3. The in-
dividual points represent the individual POS tags,
their size (area) shows their frequency in the par-
ticular treebank. The y-axis shows the stop prob-
abilities estimated on Wikipedia by our algorithm,
while the x-axis shows the stop probabilities com-
puted on the evaluation CoNLL data. Ideally, the
computed and estimated stop probabilities should
be the same, i.e. all the points should be on the
diagonal.
Let?s focus on the graphs for English. Our
method correctly recognizes that adverbs RB and
adjectives JJ are often leaves (their stop proba-
bilities in both directions are very high). More-
over, the estimates for RB are even higher than
JJ, which will contribute to attaching adverbs to
adjectives and not reversely. Nouns (NN, NNS)
are somewhere in the middle, the stop probabili-
ties for proper nouns (NNP) are estimated higher,
which is correct since they have much less modi-
fiers then the common nouns NN. The determin-
ers are more problematic. Their estimated stop
probability is not very high (about 0.65), while in
the real treebank they are almost always leaves.
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
estimation from W
iki
estimation computed on the treebank
English left-stop English right-stop
NN
NNP
IN
DTNNS
JJ
VBD
RB
NN
NNP
IN
DT
NNS
JJ
VBD
RB
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
German left-stop German right-stop
Spanish left-stop Spanish right-stop
Czech left-stop Czech right-stop
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
Hungarian left-stop Hungarian right-stop
estimation from W
iki
estimation from W
iki
estimation from W
iki
estimation from W
iki
NN
Db
Z:
J^
RR
VB
AA
Vf
Vp
C=
NN
VVFIN
APPRART
ADV
VAFIN
ADJA
NN
Db
Z:
J^
RR
VB
AA
Vf
Vp
C=
NN
ART
APPR
ADJAADV
VAFIN
VVFIN
NE
nc
danp sp
aq
vm
Fc
cc
nc da
np
sp
aq
vm
Fc
cc
NcAf
Tf
Np
Vm
Cc Wpunc
Nc
Af
Wpunc
Tf
Np
Vm
Cc
estimation computed on the treebank
estimation computed on the treebank
estimation computed on the treebank
estimation computed on the treebank
Figure 3: Comparison of P eststop probabilities esti-
mated from raw Wikipedia corpora (y-axis) and
of P tbstop probabilities computed from CoNLL tree-
banks (x-axis). The area of each point shows the
relative frequency of an individual tag.
287
This is caused by the fact that determiners are of-
ten obligatory in English and cannot be simply
removed as, e.g., adjectives. The stop probabil-
ities of prepositions (IN) are also very well rec-
ognized. While their left-stop is very probable
(prepositions always start prepositional phrases),
their right-stop probability is very low. The verbs
(the most frequent verbal tag is VBD) have very
low both right and left-stop probabilities. Our es-
timation assigns them the stop probability about
0.3 in both directions. This is quite high, but still,
it is one of the lowest among other more frequent
tags, and thus verbs tend to be the roots of the de-
pendency trees. We could make similar analyses
for other languages, but due to space reasons we
only provide graphs for Czech, German, Spanish,
and Hungarian in Figure 3.
6.3 Settings
After a manual tuning, we have set our hyperpa-
rameters to the following values:
?c = 50, ?s = 1, ? = 1/3
We have also found that the Gibbs sampler does
not always converge to a similar grammar. For a
couple of languages, the individual runs end up
with very different trees. To prevent such differ-
ences, we run each inference 50 times and take the
run with the highest final Ptreebank (see Section 4)
for the evaluation.
6.4 Results
Table 2 shows the results of our unsupervised
parser and compares them with results previously
reported in other works. In order to see the im-
pact of using the estimated stop probabilities (us-
ing model P dmv+eststop ), we provide results for clas-
sical DMV (using model P dmvstop ) as well. We do
not provide results for Chinese since we do not
have any appropriate tokenizer at our disposal (see
Section 3), and also for Turkish from CoNLL 2006
since the data is not available to us.
We now focus on the third and fourth column of
Table 2. The addition of estimated stop probabil-
ities based on large corpora improves the parsing
accuracy on 15 out of 20 treebanks. In many cases,
the improvement is substantial, which means that
the estimated stop probabilities forced the model
to completely rebuild the structures. For exam-
ple, in Bulgarian, if the P dmvstop model is used,
all the prepositions are leaves and the verbs sel-
dom govern sentences. If the P dmv+eststop model
is used, prepositions correctly govern nouns and
verbs move to roots. We observe similar changes
on Swedish as well. Unfortunately, there are also
negative examples, such as Hungarian, where the
addition of the estimated stop probabilities de-
creases the attachment score from 60.1% to 34%.
This is probably caused by not very good estimates
of the right-stop probability (see the last graph in
Figure 3). Nevertheless, the estimated stop proba-
bilities increase the average score over all the tree-
banks by more than 12% and therefore prove its
usefulness.
In the last two columns of Table 2, we provide
results of two other works reported in the last year.
The first one (spi12) is the DMV-based grammar
inducer by Spitkovsky et al (2012),4 the second
one (mar12) is our previous work (Marec?ek and
Z?abokrtsky?, 2012). Comparing with (Spitkovsky
et al, 2012), our parser reached better accuracy on
12 out of 20 treebanks. Although this might not
seem as a big improvement, if we compare the av-
erage scores over the treebanks, our system signif-
icantly wins by more than 6%. The second system
(mar12) outperforms our parser only on one tree-
bank (on Italian by less than 3%) and its average
score over all the treebanks is only 40%, i.e., more
than 8% lower than the average score of our parser.
To see the theoretical upper bound of our model
performance, we replaced the P eststop estimates by
the P tbstop estimates computed from the evaluation
treebanks and run the same inference algorithm
with the same setting. The average attachment
score of such reference DMV is almost 65%. This
shows a huge space in which the estimation of
STOP probabilities could be further improved.
7 Conclusions and Future Work
In this work, we studied the possibility of esti-
mating the DMV stop-probabilities from a large
raw corpus. We proved that such prior knowledge
about stop-probabilities incorporated into the stan-
dard DMV model significantly improves the unsu-
pervised dependency parsing and, since we are not
aware of any other fully unsupervised dependency
parser with higher average attachment score over
CoNLL data, we state that we reached a new state-
of-the-art result.5
4Possibly the current state-of-the-art results. They were
compared with many previous works.
5A possible competitive work may be the work by Blun-
som and Cohn (2010), who reached 55% accuracy on English
as well. However, they do not provide scores measured on
other CoNLL treebanks.
288
CoNLL this work other systems
language year P dmvstop P dmv+eststop reference P dmv+tbstop spi12 mar12
Arabic 06 10.6 (?8.7) 38.2 (?0.5) 61.2 10.9 26.5
Arabic 07 22.0 (?0.1) 35.3 (?0.2) 65.3 44.9 27.9
Basque 07 41.1 (?0.2) 35.5 (?0.2) 52.3 33.3 26.8
Bulgarian 06 25.9 (?1.4) 54.9 (?0.2) 73.2 65.2 46.0
Catalan 07 34.9 (?3.4) 67.0 (?1.7) 72.0 62.1 47.0
Czech 06 32.3 (?3.8) 52.4 (?5.2) 64.0 55.1 49.5
Czech 07 32.9 (?0.8) 51.9 (?5.2) 62.1 54.2 48.0
Danish 06 30.8 (?4.3) 41.6 (?1.1) 60.0 22.2 38.6
Dutch 06 25.7 (?5.7) 47.5 (?0.4) 58.9 46.6 44.2
English 07 36.5 (?5.9) 55.4 (?0.2) 63.7 29.6 49.2
German 06 29.9 (?4.6) 52.4 (?0.7) 65.5 39.1 44.8
Greek 07 42.5 (?6.0) 26.3 (?0.1) 64.7 26.9 20.2
Hungarian 07 60.8 (?0.2) 34.0 (?0.3) 68.3 58.2 51.8
Italian 07 34.5 (?0.3) 39.4 (?0.5) 64.5 40.7 43.3
Japanese 06 64.8 (?3.4) 61.2 (?1.7) 76.4 22.7 50.8
Portuguese 06 35.7 (?4.3) 69.6 (?0.1) 77.3 72.4 50.6
Slovenian 06 50.1 (?0.2) 35.7 (?0.2) 50.2 35.2 18.1
Spanish 06 38.1 (?5.9) 61.1 (?0.1) 65.6 28.2 51.9
Swedish 06 28.0 (?2.3) 54.5 (?0.4) 61.6 50.7 48.2
Turkish 07 51.6 (?5.5) 56.9 (?0.2) 67.0 44.8 15.7
Average: 36.4 48.7 64.7 42.2 40.0
Table 2: Attachment scores on CoNLL 2006 and 2007 data. Standard deviations are provided in brack-
ets. DMV model using standard P dmvstop probability is compared with DMV with P dmv+eststop , which in-
corporates STOP estimations based on reducibility principle. The reference DMV uses P tbstop , which are
computed directly on the treebanks. The results reported in previous works by Spitkovsky et al (2012),
and Marec?ek and Z?abokrtsky? (2012) follows.
In future work, we would like to focus
on unsupervised parsing without gold POS
tags (see e.g. Spitkovsky et al (2011a) and
Christodoulopoulos et al (2012)). We suppose
that many of the current works on unsupervised
dependency parsers use gold POS tags only as a
simplification of this task, and that the ultimate
purpose of this effort is to develop a fully unsu-
pervised induction of linguistic structure from raw
texts that would be useful across many languages,
domains, and applications.
The software which implements the algorithms
described in this paper, together with P eststop estima-
tions computed on Wikipedia texts, can be down-
loaded at
http://ufal.mff.cuni.cz/?marecek/udp/.
Acknowledgments
This work has been supported by the AMALACH grant
(DF12P01OVV02) of the Ministry of Culture of the Czech
Republic.
Data and some tools used as a prerequisite for
the research described herein have been provided by
the LINDAT/CLARIN Large Infrastructural project, No.
LM2010013 of the Ministry of Education, Youth and Sports
of the Czech Republic.
We would like to thank Martin Popel, Zdene?k Z?abokrtsky?,
Rudolf Rosa, and three anonymous reviewers for many useful
comments on the manuscript of this paper.
References
James K. Baker. 1979. Trainable grammars for speech
recognition. In Speech communication papers presented
at the 97th Meeting of the Acoustical Society, pages 547?
550.
Yonatan Bisk and Julia Hockenmaier. 2012. Induction of lin-
guistic structure with combinatory categorial grammars.
The NAACL-HLT Workshop on the Induction of Linguistic
Structure, page 90.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised induc-
tion of tree substitution grammars for dependency pars-
ing. In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP ?10,
pages 1204?1213, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Thorsten Brants. 2000. TnT - A Statistical Part-of-Speech
Tagger. Proceedings of the sixth conference on Applied
natural language processing, page 8.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 149?164, Strouds-
burg, PA, USA. Association for Computational Linguis-
tics.
Christos Christodoulopoulos, Sharon Goldwater, and Mark
Steedman. 2012. Turning the pipeline into a loop: Iter-
ated unsupervised dependency parsing and PoS induction.
In Proceedings of the NAACL-HLT Workshop on the In-
duction of Linguistic Structure, pages 96?99, June.
Y. J. Chu and T. H. Liu. 1965. On the Shortest Arborescence
of a Directed Graph. Science Sinica, 14:1396?1400.
289
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008.
Logistic normal priors for unsupervised probabilistic
grammar induction. In Neural Information Processing
Systems, pages 321?328.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel mul-
tilingual guidance. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 50?61, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence compres-
sion beyond word deletion. In Proceedings of the 22nd
International Conference on Computational Linguistics -
Volume 1, COLING ?08, pages 137?144, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kim Gerdes and Sylvain Kahane. 2011. Defining depen-
dencies (and constituents). In Proceedings of Dependency
Linguistics 2011, Barcelona.
Walter R. Gilks, S. Richardson, and David J. Spiegelhalter.
1996. Markov chain Monte Carlo in practice. Interdisci-
plinary statistics. Chapman & Hall.
W. Keith Hastings. 1970. Monte carlo sampling methods
using markov chains and their applications. Biometrika,
57(1):pp. 97?109.
William P. Headden III, Mark Johnson, and David McClosky.
2009. Improving unsupervised dependency parsing with
richer contexts and smoothing. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages 101?
109, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach to sen-
tence compression. Artif. Intell., 139(1):91?107, July.
Sandra Ku?bler, Ryan T. McDonald, and Joakim Nivre. 2009.
Dependency Parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
Marke?ta Lopatkova?, Martin Pla?tek, and Vladislav Kubon?.
2005. Modeling syntax of free word-order languages:
Dependency analysis by reduction. In Va?clav Matous?ek,
Pavel Mautner, and Toma?s? Pavelka, editors, Lecture Notes
in Artificial Intelligence, Proceedings of the 8th Interna-
tional Conference, TSD 2005, volume 3658 of Lecture
Notes in Computer Science, pages 140?147, Berlin / Hei-
delberg. Springer.
Martin Majlis? and Zdene?k Z?abokrtsky?. 2012. Language
richness of the web. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Evaluation
(LREC 2012), Istanbul, Turkey, May. European Language
Resources Association (ELRA).
David Marec?ek and Zdene?k Z?abokrtsky?. 2012. Exploiting
reducibility in unsupervised dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12, pages
297?307, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-
source transfer of delexicalized dependency parsers. In
Proceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 62?72, Edin-
burgh, Scotland, UK., July. Association for Computational
Linguistics.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark John-
son. 2010. Using universal linguistic knowledge to guide
grammar induction. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 1234?1244, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Re-
public, June. Association for Computational Linguistics.
Mohammad Sadegh Rasooli and Heshaam Faili. 2012. Fast
unsupervised dependency parsing with arc-standard tran-
sitions. In Proceedings of ROBUS-UNSUP, pages 1?9.
Noah Ashton Smith. 2007. Novel estimation methods
for unsupervised discovery of latent structure in natu-
ral language text. Ph.D. thesis, Baltimore, MD, USA.
AAI3240799.
Anders S?gaard. 2011. From ranked words to dependency
trees: two-stage unsupervised non-projective dependency
parsing. In Proceedings of TextGraphs-6: Graph-based
Methods for Natural Language Processing, TextGraphs-
6, pages 60?68, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.
2010. From baby steps to leapfrog: how ?less is more? in
unsupervised dependency parsing. In Human Language
Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics, HLT ?10, pages 751?759, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, and
Daniel Jurafsky. 2011a. Unsupervised dependency pars-
ing without gold part-of-speech tags. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011b. Punctuation: Making a point in unsuper-
vised dependency parsing. In Proceedings of the Fifteenth
Conference on Computational Natural Language Learn-
ing (CoNLL-2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2012. Three Dependency-and-Boundary Models for
Grammar Induction. In Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL 2012).
290
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 517?527,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Coordination Structures in Dependency Treebanks
Martin Popel, David Marec?ek, Jan S?te?pa?nek, Daniel Zeman, Zdene?k Z?abokrtsky?
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics (U?FAL)
Malostranske? na?me?st?? 25, CZ-11800 Praha, Czechia
{popel|marecek|stepanek|zeman|zabokrtsky}@ufal.mff.cuni.cz
Abstract
Paratactic syntactic structures are noto-
riously difficult to represent in depen-
dency formalisms. This has painful con-
sequences such as high frequency of pars-
ing errors related to coordination. In other
words, coordination is a pending prob-
lem in dependency analysis of natural lan-
guages. This paper tries to shed some
light on this area by bringing a system-
atizing view of various formal means de-
veloped for encoding coordination struc-
tures. We introduce a novel taxonomy of
such approaches and apply it to treebanks
across a typologically diverse range of 26
languages. In addition, empirical obser-
vations on convertibility between selected
styles of representations are shown too.
1 Introduction
In the last decade, dependency parsing has grad-
ually been receiving visible attention. One of
the reasons is the increased availability of depen-
dency treebanks, be they results of genuine depen-
dency annotation projects or converted automat-
ically from previously existing phrase-structure
treebanks.
In both cases, a number of decisions have to be
made during the construction or conversion of a
dependency treebank. The traditional notion of
dependency does not always provide unambiguous
solutions, e.g. when it comes to attaching func-
tional words. Worse, dependency representation is
at a loss when it comes to representing paratactic
linguistic phenomena such as coordination, whose
nature is symmetric (two or more conjuncts play
the same role), as opposed to the head-modifier
asymmetry of dependencies.1
1We use the term modifier (or child) for all types of de-
pendent nodes including arguments.
The dominating solution in treebank design is to
introduce artificial rules for the encoding of coor-
dination structures within dependency trees using
the same means that express dependencies, i.e., by
using edges and by labeling of nodes or edges. Ob-
viously, any tree-shaped representation of a coor-
dination structure (CS) must be perceived only as
a ?shortcut? since relations present in coordination
structures form an undirected cycle, as illustrated
already by Tesnie`re (1959). For example, if a noun
is modified by two coordinated adjectives, there
is a (symmetric) coordination relation between the
two conjuncts and two (asymmetric) dependency
relations between the conjuncts and the noun.
However, as there is no obvious linguistic in-
tuition telling us which tree-shaped CS encoding
is better and since the degree of freedom has sev-
eral dimensions, one can find a number of distinct
conventions introduced in particular dependency
treebanks. Variations exist both in topology (tree
shape) and labeling. The main goal of this pa-
per is to give a systematic survey of the solutions
adopted in these treebanks.
Naturally, the interplay of dependency and co-
ordination links in a single tree leads to serious
parsing issues.2 The present study does not try to
decide which coordination style is the best from
the parsing point of view.3 However, we believe
that our survey will substantially facilitate experi-
ments in this direction in the future, at least by ex-
ploring and describing the space of possible can-
didates.
2CSs have been reported to be one of the most frequent
sources of parsing errors (Green and Z?abokrtsky?, 2012; Mc-
Donald and Nivre, 2007; Ku?bler et al, 2009; Collins, 2003).
Their impact on quality of dependency-based machine trans-
lation can also be substantial; as documented on an English-
to-Czech dependency-based translation system (Popel and
Z?abokrtsky?, 2009), 39% of serious translation errors which
are caused by wrong parsing have to do with coordination.
3There might be no such answer, as different CS conven-
tions might serve best for different applications or for differ-
ent parser architectures.
517
The rest of the paper is structured as follows.
Section 2 describes some known problems related
to CS. Section 3 shows possible ?styles? for rep-
resenting CS. Section 4 lists treebanks whose CS
conventions we studied. Section 5 presents empir-
ical observations on CS convertibility. Section 6
concludes the paper.
2 Related work
Let us first recall the basic well-known character-
istics of CSs.
In the simplest case of a CS, a coordinating
conjunction joins two (usually syntactically and
semantically compatible) words or phrases called
conjuncts. Even this simplest case is difficult to
represent within a dependency tree because, in the
words of Lombardo and Lesmo (1998): Depen-
dency paradigms exhibit obvious difficulties with
coordination because, differently from most lin-
guistic structures, it is not possible to characterize
the coordination construct with a general schema
involving a head and some modifiers of it.
Proper formal representation of CSs is further
complicated by the following facts:
? CSs with more than two conjuncts (multi-
conjunct CSs) exist and are frequent.
? Besides ?private? modifiers of individual
conjuncts, there are modifiers shared by
all conjuncts, such as in ?Mary came and
cried?. Shared modifiers may appear along-
side with private modifiers of particular con-
juncts.
? Shared modifiers can be coordinated, too:
?big and cheap apples and oranges?.
? Nested (embedded) coordinations are possi-
ble: ?John and Mary or Sam and Lisa?.
? Punctuation (commas, semicolons, three
dots) is frequently used in CSs, mostly with
multi-conjunct coordinations or juxtaposi-
tions which can be interpreted as CSs with-
out conjunctions (e.g. ?Don?t worry, be
happy!?).
? In many languages, comma or other punctu-
ation mark may play the role of the main co-
ordinating conjunction.
? The coordinating conjunction may be a mul-
tiword expression (?as well as?).
? Deficient CSs with a single conjunct exist.
? Abbreviations like ?etc.? comprise both the
conjunction and the last conjunct.
? Coordination may form very intricate struc-
tures when combined with ellipsis. For ex-
ample, a conjunct can be elided while its ar-
guments remain in the sentence, such as in
the following traditional example: ?I gave
the books to Mary and the records to Sue.?
? The border between paratactic and hypotactic
surface means of expressing coordination re-
lations is fuzzy. Some languages can use en-
clitics instead of conjunctions/prepositions,
e.g. Latin ?Senatus Populusque Romanus?.
Purely hypotactic surface means such as the
preposition in ?John with Mary? occur too.4
? Careful semantic analysis of CSs discloses
additional complications: if a node is mod-
ified by a CS, it might happen that it is
the node itself (and not its modifiers) what
should be semantically considered as a con-
junct. Note the difference between ?red and
white wine? (which is synonymous to ?red
wine and white wine?) and ?red and white
flag of Poland?. Similarly, ?five dogs and
cats? has a different meaning than ?five dogs
and five cats?.
Some of these issues were recognized already
by Tesnie`re (1959). In his solution, conjuncts are
connected by vertical edges directly to the head
and by horizontal edges to the conjunction (which
constitutes a cycle in every CS). Many different
models have been proposed since, out of which the
following are the most frequently used ones:
? MS = Mel?c?uk style used in the Meaning-
Text Theory (MTT): the first conjunct is the
head of the CS, with the second conjunct at-
tached as a dependent of the first one, third
conjunct under the second one, etc. Coor-
dinating conjunction is attached under the
penultimate conjunct, and the last conjunct
is attached under the conjunction (Mel?c?uk,
1988),
? PS = Prague Dependency Treebank (PDT)
style: all conjuncts are attached under the
coordinating conjunction (along with shared
modifiers, which are distinguished by a spe-
cial attribute) (Hajic? et al, 2006),
4As discussed by Stassen (2000), all languages seem to
have some strategy for expressing coordination. Some of
them lack the paratactic surface means (the so called WITH-
languages), but the hypotactic surface means are present al-
most always.
518
? SS = Stanford parser style:5 the first conjunct
is the head and the remaining conjuncts (as
well as conjunctions) are attached under it.
One can find various arguments supporting the
particular choices. MTT possesses a complex
set of linguistic criteria for identifying the gov-
ernor of a relation (see Mazziotta (2011) for an
overview), which lead to MS. MS is preferred in
a rule-based dependency parsing system of Lom-
bardo and Lesmo (1998). PS is advocated by
S?te?pa?nek (2006) who claims that it can represent
shared modifiers using a single additional binary
attribute, while MS would require a more complex
co-indexing attribute. An argumentation of Tratz
and Hovy (2011) follows a similar direction: We
would like to change our [MS] handling of coordi-
nating conjunctions to treat the coordinating con-
junction as the head [PS] because this has fewer
ambiguities than [MS]. . .
We conclude that the influence of the choice of
coordination style is a well-known problem in de-
pendency syntax. Nevertheless, published works
usually focus only on a narrow ad-hoc selection of
few coordination styles, without giving any sys-
tematic perspective.
Choosing a file format presents a different prob-
lem. Despite various efforts to standardize lin-
guistic annotation,6 no commonly accepted stan-
dard exists. The primitive format used for CoNLL
shared tasks is widely used in dependency parsing,
but its weaknesses have already been pointed out
(cf. Stran?a?k and S?te?pa?nek (2010)). Moreover, par-
ticular treebanks vary in their contents even more
than in their format, i.e. each treebank has its own
way of representing prepositions or different gran-
ularity of syntactic labels.
3 Variations in representing
coordination structures
Our analysis of variations in representing coordi-
nation structures is based on observations from a
set of dependency treebanks for 26 languages.7
5We use the already established MS-PS-SS distinction to
facilitate literature overview; as shown in Section 3, the space
of possible coordination styles is much richer.
6For example, TEI (TEI Consortium, 2013), PML (Hana
and S?te?pa?nek, 2012), SynAF (ISO 24615, 2010).
7The primary data sources are the following: Ancient
Greek: Ancient Greek Dependency Treebank (Bamman and
Crane, 2011), Arabic: Prague Arabic Dependency Tree-
bank 1.0 (Smrz? et al, 2008), Basque: Basque Dependency
Treebank (larger version than CoNLL 2007 generously pro-
In accordance with the usual conventions, we as-
sume that each sentence is represented by one de-
pendency tree, in which each node corresponds
to one token (word or punctuation mark). Apart
from that, we deliberately limit ourselves to CS
representations that have shapes of connected sub-
graphs of dependency trees.
We limit our inventory of means of expressing
CSs within dependency trees to (i) tree topology
(presence or absence of a directed edge between
two nodes, Section 3.1), and (ii) node labeling
(additional attributes stored insided nodes, Sec-
tion 3.2).8 Further, we expect that the set of pos-
sible variations can be structured along several di-
mensions, each of which corresponds to a certain
simple characteristic (such as choosing the left-
most conjunct as the CS head, or attaching shared
modifiers below the nearest conjunct). Even if it
does not make sense to create the full Cartesian
product of all dimensions because some values
cannot be combined, it allows to explore the space
of possible CS styles systematically.9
3.1 Topological variations
We distinguish the following dimensions of topo-
logical variations of CS styles (see Figure 1):
Family ? configuration of conjuncts. We di-
vide the topological variations into three main
groups, labeled as Prague (fP), Moscow (fM), and
vided by IXA Group) (Aduriz and others, 2003), Bulgarian:
BulTreeBank (Simov and Osenova, 2005), Czech: Prague
Dependency Treebank 2.0 (Hajic? et al, 2006), Danish: Dan-
ish Dependency Treebank (Kromann et al, 2004), Dutch:
Alpino Treebank (van der Beek and others, 2002), English:
Penn TreeBank 3 (Marcus et al, 1993), Finnish: Turku De-
pendency Treebank (Haverinen et al, 2010), German: Tiger
Treebank (Brants et al, 2002), Greek (modern): Greek De-
pendency Treebank (Prokopidis et al, 2005), Hindi, Ben-
gali and Telugu: Hyderabad Dependency Treebank (Husain
et al, 2010), Hungarian: Szeged Treebank (Csendes et al,
2005), Italian: Italian Syntactic-Semantic Treebank (Mon-
temagni and others, 2003), Latin: Latin Dependency Tree-
bank (Bamman and Crane, 2011), Persian: Persian Depen-
dency Treebank (Rasooli et al, 2011), Portuguese: Floresta
sinta?(c)tica (Afonso et al, 2002), Romanian: Romanian De-
pendency Treebank (Ca?la?cean, 2008), Russian: Syntagrus
(Boguslavsky et al, 2000), Slovene: Slovene Dependency
Treebank (Dz?eroski et al, 2006), Spanish: AnCora (Taule?
et al, 2008), Swedish: Talbanken05 (Nilsson et al, 2005),
Tamil: TamilTB (Ramasamy and Z?abokrtsky?, 2012), Turk-
ish: METU-Sabanci Turkish Treebank (Atalay et al, 2003).
8Edge labeling can be trivially converted to node labeling
in tree structures.
9The full Cartesian product of variants in Figure 1 would
result in topological 216 variants, but only 126 are applicable
(the inapplicable combinations are marked with ??? in Fig-
ure 1). Those 126 topological variants can be further com-
bined with labeling variants defined in Section 3.2.
519
Main family Prague family (code fP)[14 treebanks]
Moscow family (code fM)
[5 treebanks]
Stanford family (code fS)
[6 treebanks]
Choice of head
Head on left (code hL)
[10 treebanks]
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Head on right (code hR)
[14 treebanks]
Mixed head (code hM) [1 treebank] A mixture of hL and hR
Attachment of shared modifiers
Shared modifier
below the nearest conjunct
(code sN)
[15 treebanks]
Shared modifier below head
(code sH)
[11 treebanks]
dogs
dogsaaan, caaaaataaaaaarocaaaaaoc
on
dogs an, cct do
dogs
an, cct
orao 
o 
dogsaaan, caaaataaaarocaaaon
oc
Attachment of coordinating conjunction
Coordinating conjunction
below previous conjunct (code cP)
[2 treebanks]
?
dogs
and,, acst,,racs dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
below following conjunct (code cF)
[1 treebank]
?
dogssadn,
g c,
adn,tssrdn,dog dooooogsanoooooooooo,san
 ctn
sr 
Coordinating conjunction
between two conjuncts (code cB)
[8 treebanks]
?
dogs
and,  cats rats dooooogsanoos, oocsan
 trn
Coordinating conjunction as the head (code cH)
is the only applicable style for the Prague family [14 treebanks] ? ?
Placement of punctuation
values pP [7 treebanks], pF [1 treebank] and pB [15 treebanks] are analogous to cP, cF and cB
(but applicable also to the Prague family)
Figure 1: Different coordination styles, variations in tree topology. Example phrase: ?(lazy) dogs, cats
and rats?. Style codes are described in Section 3.1.
Stanford (fS) families.10 This first dimension dis-
tinguishes the configuration of conjuncts: in the
Prague family, all the conjuncts are siblings gov-
erned by one of the conjunctions (or a punctuation
fulfilling its role); in the Moscow family, the con-
juncts form a chain where each node in the chain
depends on the previous (or following) node; in
the Stanford family, the conjuncts are siblings ex-
cept for the first (or last) conjunct, which is the
10Names are chosen purely as a mnemonic device, so that
Prague Dependency Treebank belongs to the Prague family,
Mel?c?uk style belongs to the Moscow family, and Stanford
parser style belongs to the Stanford family.
head.11
Choice of head ? leftmost or rightmost. In
the Prague family, the head can be either the left-
most12 (hL) or the rightmost (hR) conjunction or
punctuation. Similarly, in the Moscow and Stan-
ford families, the head can be either the leftmost
(hL) or the rightmost (hR) conjunct. A third op-
11Note that for CSs with just two conjuncts, fM and fS
may look exactly the same (depending on the attachment of
conjunctions and punctuation as described below).
12For simplicity, we use the terms left and right even if
their meaning is reversed for languages with right-to-left
writing systems such as Arabic or Persian.
520
tion (hM) is to mix hL and hR based on some cri-
terion, e.g. the Persian treebank uses hR for coor-
dination of verbs and hL otherwise. For the exper-
iments in Section 5, we choose the head which is
closer to the parent of the whole CS, with the mo-
tivation to make the edge between CS head and its
parent shorter, which may improve parser training.
Attachment of shared modifiers. Shared mod-
ifiers may appear before the first conjunct or after
the last one. Therefore, it seems reasonable to at-
tach shared modifiers either to the CS head (sH),
or to the nearest (i.e. first or last) conjunct (sN).
Attachment of coordinating conjunctions. In
the Moscow family, conjunctions may be either
part of the chain of conjuncts (cB), or they may be
put outside of the chain and attached to the previ-
ous (cP) or following (cF) conjunct. In the Stan-
ford family, conjunctions may be either attached
to the CS head (and therefore between conjuncts)
(cB), or they may be attached to the previous (cP)
or the following (cF) conjunct. The cB option in
both Moscow and Stanford families, treats con-
junctions in the same way as conjuncts (with re-
spect to topology only). In the Prague family, there
is just one option available (cH) ? one of the con-
junctions is the CS head while the others are at-
tached to it.
Attachment of punctuation. Punctuation to-
kens separating conjuncts (commas, semicolons
etc.) could be treated the same way as conjunc-
tions. However, in most treebanks it is treated
differently, so we consider it as well. The val-
ues pP, pF and pB are analogous to cP, cF and
cB except that punctuation may be also attached
to the conjunction in case of pP and pF (other-
wise, a comma before the conjunction would be
non-projectively attached to the member follow-
ing the conjunction).
The three established styles mentioned in Sec-
tion 2 can be defined in terms of the newly intro-
duced abbreviations: PS = fPhRsHcHpB, MS =
fMhLsNcBp?, and SS = fShLsNcBp?.13
3.2 Labeling variations
Most state-of-the-art dependency parsers can pro-
duce labeled edges. However, the parsers produce
only one label per edge. To fully capture CSs,
we need more than one label, because there are
several aspects involved (see the initial assump-
13The question marks indicate that the original Mel?c?uk
and Stanford parser styles ignore punctuation.
tions in Section 3): We need to identify the co-
ordinating conjunction (its POS tag might not be
enough), conjuncts, shared modifiers, and punctu-
ation that separates conjuncts. Besides that, there
should be a label classifying the dependency rela-
tion between the CS and its parent.
Some of the information can be retrieved from
the topology of the tree and the ?main label? of
each node, but not everything. The additional in-
formation can be attached to the main label, but
such approach obscures the logical structure.
In the Prague family, there are two possible
ways to label a conjunction and conjuncts:
Code dU (?dependency labeled at the upper
level of the CS?). The dependency relation of the
whole CS to its parent is represented by the label
of the conjunction, while the conjuncts are marked
with a special label for conjuncts (e.g. ccof in the
Hyderabad Dependency Treebank).
Code dL (?lower level?). The CS is represented
by a coordinating conjunction (or punctuation if
there is no conjunction) with a special label (e.g.
Coord in PDT). Subsequently, each conjunct has
its own label that reflects the dependency relation
towards the parent of the whole CS, therefore, con-
juncts of the same CS can have different labels,
e.g. ?Who[SUBJ] and why[ADV] did it??
Most Prague family treebanks use sH, i.e.
shared modifiers are attached to the head (coor-
dinating conjunction). Each child of the head has
to belong to one of three sets: conjuncts, shared
modifiers, and punctuation or additional conjunc-
tions. In PDT, conjuncts, punctuation and addi-
tional conjunctions are recognized by specific la-
bels. Any other children of the head are shared
modifiers.
In the Stanford and Moscow families, one of
the conjuncts is the head. In practice, it is never la-
beled as a conjunct explicitly, because the fact that
it is a conjunct can be deduced from the presence
of conjuncts among its children. Usually, the other
conjuncts are labeled as conjuncts; conjunctions
and punctuation also have a special label. This
type of labeling corresponds to the dU type.
Alternatively (as found in the Turkish treebank,
dL), all conjuncts in the Moscow chain have their
own dependency labels and the fact that they are
conjuncts follows from the COORDINATION la-
bels of the conjunction and punctuation nodes be-
tween them.
To represent shared modifiers in the Stan-
521
ford and Moscow families, an additional label
is needed again to distinguish between private
and shared modifiers since they cannot be distin-
guished topologically. Moreover, if nested CSs
are allowed, a binary label is not sufficient (i.e.
?shared? versus ?private?) because it also has to
indicate which conjuncts the shared modifier be-
longs to.14
We use the following binary flag codes for cap-
turing which CS participants are distinguished in
the annotation: m01 = shared modifiers anno-
tated; m10 = conjuncts annotated; m11 = both
annotated; m00 = neither annotated.
4 Coordination Structures in Treebanks
In this section, we identify the CS styles defined
in the previous section as used in the primary tree-
bank data sources; statistical observations (such
as the amount of annotated shared modifiers) pre-
sented here, as well as experiments on CS-style
convertibility presented in Section 5.2, are based
on the normalized shapes of the treebanks as con-
tained in the HamleDT 1.0 treebank collection
(Zeman et al, 2012).15
Some of the treebanks were downloaded indi-
vidually from the web, but most of them came
from previously published collections for depen-
dency parsing campaigns: six languages from
CoNLL-2006 (Buchholz and Marsi, 2006), seven
languages from CoNLL-2007 (Nivre et al, 2007),
two languages from CoNLL-2009 (Hajic? and oth-
ers, 2009), three languages from ICON-2010 (Hu-
sain et al, 2010). Obviously, there is a certain
risk that the CS-related information contained in
the source treebanks was slightly biased by the
properties of the CoNLL format upon conversion.
In addition, many of the treebanks were natively
dependency-based (cf. the 2nd column of Table 1),
but some were originally based on constituents
and thus specific converters to the CoNLL for-
mat had to be created (for instance, the Span-
ish phrase-structure trees were converted to de-
pendencies using a procedure described by Civit
et al (2006); similarly, treebank-specific convert-
ers have been used for other languages). Again,
14This is not needed in Prague family where shared modi-
fiers are attached to the conjunction provided that each shared
modifier is shared by conjuncts that form a full subtree to-
gether with their coordinating conjunctions; no exceptions
were found during the annotation process of the PDT.
15A subset of the treebanks whose license
terms permit redistribution is available directly at
http://ufal.mff.cuni.cz/hamledt/.
Danish Romanian
dogsa
n,  anctttr  attt,

ddog
san,n           cntnrnsn     c,n
Hungarian
dogsadnnnn,nnnn ctrdadnnnnrncgdasd
Figure 2: Annotation styles of a few treebanks do
not fit well into the multidimensional space de-
fined in Section 3.1.
there is some risk that the CS-related information
contained in treebanks resulting from such conver-
sions is slightly different from what was intended
in the very primary annotation.
There are several other languages (e.g. Esto-
nian or Chinese) which are not included in our
study, despite of the fact that constituency tree-
banks do exist for them. The reason is that the
choice of their CS style would be biased, because
no independent converters exist ? we would have
to convert them to dependencies ourselves. We
also know about several more dependency tree-
banks that we have not processed yet.
Table 1 shows 26 languages whose treebanks
we have studied from the viewpoint of their CS
styles. It gives the basic quantitative properties of
the treebanks, their CS style in terms of the tax-
onomy introduced in Section 3, as well as statis-
tics related to CSs: the average number of CSs per
100 tokens, the average number of conjuncts per
one CS, the average number of shared modifiers
per one CS,16 and the percentage of nested CSs
among all CSs. The reader can return to Figure
1 to see the basic statistics on the ?popularity? of
individual design decisions among the developers
of dependency treebanks or constituency treebank
converters.
CS styles of most treebanks are easily classifi-
able using the codes introduced in Section 3, plus
a few additional codes:
? p0 = punctuation was removed from the tree-
bank.
16All non-Prague family treebanks are marked sN and
m00 or m10, (i.e. shared modifiers not marked in the origi-
nal annotation, but attached to the head conjunct) because we
found no counterexamples (modifiers attached to a conjunct,
but not the nearest one). The HamleDT normalization proce-
dure contains a few heuristics to detect shared modifiers, but
it cannot recover the missing distinction reliably, so the num-
bers in the ?SMs/CJ? column are mostly underestimated.
522
Language Orig. Data Sents. Tokens Original CS CSs / CJs / SMs / Nested RT
type set style code 100 tok. CS CS CS[%] UAS
Ancient
Greek dep prim. 31 316 461 782 fP hR sH cH pB dL m11 6.54 2.17 0.16 10.3 97.86
Arabic dep C07 3 043 116 793 fP hL sH cH pB dL m00 3.76 2.42 0.13 10.6 96.69
Basque dep prim. 11 225 151 593 fP hR sN cH pP dU m00 3.37 2.09 0.03 5.1 99.32
Bengali dep I10 1 129 7 252 fP hR sH cH pP dU m11 4.87 1.71 0.05 24.1 99.97
Bulgarian phr C06 13 221 196 151 fS hL sN cB pB dU m10 2.99 2.19 0.00 0.0 99.74
Czech dep C07 25 650 437 020 fP hR sH cH pB dL m11 4.09 2.16 0.20 14.6 99.42
Danish dep C06 5 512 100 238 fS* hL sN cP pB dU m10 3.68 1.93 0.13 7.5 99.76
Dutch phr C06 13 735 200 654 fP hR sN cH pP dU m10 2.06 2.17 0.05 3.3 99.47
English phr C07 40 613 991 535 fP hR sH cH pB dU m10 2.07 2.33 0.05 6.3 99.84
Finnish dep prim. 4 307 58 576 fS hL sN cB pB dU m10 4.06 2.41 0.00 6.4 99.70
German phr C09 38 020 680 710 fM hL sN cP pP dU m10 2.79 2.09 0.01 0.0 99.73
Greek dep C07 2 902 70 223 fP hR sH cH pB dL m11 3.25 2.48 0.18 7.2 99.43
Hindi dep I10 3 515 77 068 fP hR sH cH pP dU m11 2.45 1.97 0.04 10.3 98.35
Hungarian phr C07 6 424 139 143 fT hX sN cX pX dL m00 2.37 1.90 0.01 2.2 99.84
Italian dep C07 3 359 76 295 fS hL sN cB pB dU m10 3.32 2.02 0.03 3.8 99.51
Latin dep prim. 3 473 53 143 fP hR sH cH pB dL m11 6.74 2.24 0.41 12.3 97.45
Persian dep prim. 12 455 189 572 fM*hM sN cB pP dU m00 4.18 2.10 0.18 3.7 99.82
Portuguese phr C06 9 359 212 545 fS hL sN cB pB dU m10 2.51 1.95 0.26 11.1 99.16
Romanian dep prim. 4 042 36 150 fP* hR sN cH p0 dU m10 1.80 2.00 0.00 0.0 100.00
Russian dep prim. 34 895 497 465 fM hL sN cB p0 dU m10 4.02 2.02 0.07 3.9 99.86
Slovene dep C06 1 936 35 140 fP hR sH cH pB dL m00 4.31 2.49 0.00 10.8 98.87
Spanish phr C09 15 984 477 810 fS hL sN cB pB dU m10 2.79 1.98 0.14 12.7 99.24
Swedish phr C06 11 431 197 123 fM hL sN cF pF dU m10 3.94 2.19 0.13 0.7 99.66
Tamil dep prim. 600 9 581 fP hR sH cH pB dL m11 1.66 2.46 0.22 3.8 99.67
Telugu dep I10 1 450 5 722 fP hR sH cH pP dU m11 3.48 1.59 0.06 5.0 100.00
Turkish dep C07 5 935 69 695 fM hR sN cB pB dL m10 3.81 2.04 0.00 34.3 99.23
Table 1: Overview of analyzed treebanks. prim. = primary source; C06?C09 = CoNLL 2006?2009;
I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in
nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip
experiment described in Section 5. Style codes are defined in Sections 3 and 4.
? fM* = Persian treebank uses a mix of fM and
fS: fS for coordination of verbs and fM oth-
erwise.
Figure 2 shows three other anomalies:
? fS* = Danish treebank employs a mixture of
fS and fM, where the last conjunct is attached
indirectly via the conjunction.
? fP* = Romanian treebank omits punctuation
tokens and multi-conjunct coordinations get
split.
? fT = Hungarian Szeged treebank uses
?Tesnie`re family? ? disconnected graphs for
CSs where conjuncts (and conjunction and
punctuation) are attached directly to the par-
ent of CS, and so the other style dimensions
are not applicable (hX, cX, pX).
5 Empirical Observations on
Convertibility of Coordination Styles
The various styles cannot represent the CS-related
information to the same extent. For example,
it is not possible to represent nested CSs in the
Moscow and Stanford families without signifi-
cantly changing the number of possible labels.17
The dL style (which is most easily applicable to
the Prague family) can represent coordination of
different dependency relations. This is again not
possible in the other styles without adding e.g. a
special ?prefix? denoting the relations.
We can see that the Prague family has a greater
expressive power than the other two families: it
can represent complex CSs using just one addi-
tional binary label, distinguishing between shared
modifiers and conjuncts. A similar additional label
is needed in the other styles to distinguish between
shared and private modifiers.
Because of the different expressive power, con-
verting a CS from one style to another may
lead to a loss of information. For example, as
17Mel?c?uk uses ?grouping? to nest CSs ? cf. related so-
lutions involving coindexing or bubble trees (Kahane, 1997).
However, these approaches were not used in any of the re-
searched treebanks. To combine grouping with shared modi-
fiers, each group in a tree should have a different identifier.
523
there is no way of representing shared modifiers
in the Moscow family without an additional at-
tribute, converting a CS with shared modifiers
from Prague to Moscow family makes the modi-
fiers private. When converting back, one can use
certain heuristics to handle the most obvious cases,
but sometimes the modifiers will stay private (very
often, the nature of a modifier depends on context
or is debatable even for humans, e.g. ?Young boys
and girls?).
5.1 Transformation algorithm
We developed an algorithm to transform one CS
style to another. Two subtasks must be solved by
the algorithm: identification of individual CSs and
their participants, and transforming of the individ-
ual CSs.
Obviously, the individual CSs cannot be trans-
formed independently because of coordination
nesting. For instance, when transforming a nested
coordination from the Prague style to the Moscow
style (e.g. to fMhL), the leftmost conjunct in the
inner (lower) coordination must climb up to be-
come the head of the inner CS, but then it must
climb up once again to become the head of the
outer (upper) CS too. This shows that inner CSs
must be transformed first.
We tackle this problem by a depth-first recur-
sion. When going down the tree, we only recog-
nize all the participants of the CSs, classify them
and gather them in a separate data structure (one
for each visited CS). The following four types
of CS participants are distinguished: coordinat-
ing conjunctions, conjuncts, shared modifiers, and
punctuations that separate conjuncts.18 No change
of the tree is performed during these descent steps.
When returning back from the recursion (i.e.,
when climbing from a node back up to its par-
ent), we test whether the abandoned node is the
topmost node of some CS. If so, then this CS is
transformed, which means that its participants are
rehanged and relabelled according the the target
CS style.
This procedure naturally guarantees that the in-
18Conjuncts are explicitly marked in most styles. Coordi-
nating conjunctions can be usually identified with the help of
dependency labels and POS tags. Punctuation separating con-
juncts can be detected with high accuracy using simple rules.
If shared modifiers are not annotated (code m00 or m10),
one can imagine rule-based heuristics or special classifiers
trained to distinguish shared modifiers. For the experiments
in this section, we use the HamleDT gold annotation attribute
is shared modifier.
ner CSs are transformed first and that all CSs are
transformed when the recursions returns to the
root.
5.2 Roundtrip experiment
The number of possible conversion directions ob-
viously grows quadratically with the number of
styles. So far, we limited ourselves only to con-
versions from/to the style of the HamleDT tree-
bank collection, which contains all the treebanks
under our study already converted into a com-
mon scheme. The common scheme is based
on the conventions of PDT, whose CS style is
fPhRsHcHpB.19
We selected nine styles (3 families times 3 head
choices) and transformed all the HamleDT scheme
treebanks to these nine styles and back, which we
call a roundtrip. Resulting averaged unlabeled at-
tachment scores (UAS, evaluated against the Ham-
leDT scheme) in the last column of Table 1 indi-
cate that the percentage of transformation errors
(i.e. tokens attached to a different parent after the
roundtrip) is lower than 1% for 20 out of the 26
languages.20 A manual inspection revealed two
main error sources. First, as noted above, the Stan-
ford and Moscow families have lower expressive
power than the Prague family, so naturally, the in-
verse transformation was ambiguous and the trans-
formation heuristics were not capable of identify-
ing the correct variant every time. Second, we also
encountered inconsistencies in the original tree-
banks (which we were not trying to fix in Ham-
leDT for now).
6 Conclusions and Future Work
We described a (theoretically very large) space of
possible representations of CSs within the depen-
dency framework. We pointed out a range of de-
tails that make CSs a really complex phenomenon;
anyone dealing with CSs in treebanking should
take these observations into account.
We proposed a taxonomy of those approaches
19As documented in Zeman et al (2012), the normalization
procedures used in HamleDT embrace many other phenom-
ena as well (not only those related to coordination), and in-
volve both structural transformation and dependency relation
relabeling.
20Table 1 shows that Latin and Ancient Greek treebanks
have on average more than 6 CSs per 100 tokens, more than
2 conjuncts per CS, and Latin has also the highest number of
shared modifiers per CS. Therefore the percentage of nodes
affected by the roundtrip is the highest for these languages
and the lower roundtrip UAS is not surprising.
524
that have been argued for in literature or employed
in real treebanks.
We studied 26 existing treebanks of different
languages. For each value of each dimension in
Figure 1, we found at least one treebank where the
value is used; even so, several treebanks take their
own unique path that cannot be clearly classified
under the taxonomy (the taxonomy could indeed
be extended, for the price of being less clearly ar-
ranged).
We discussed the convertibility between the var-
ious styles and implemented a universal tool that
transforms between any two styles of the taxon-
omy. The tool achieves a roundtrip accuracy close
to 100%. This is important because it opens the
door to easily switching coordination styles for
parsing experiments, phrase-to-dependency con-
version etc.
While the focus of this paper is to explore and
describe the expressive power of various annota-
tion styles, we did not address the learnability of
the styles by parsers. That will be a complemen-
tary point of view, and thus a natural direction of
future work for us.
Acknowledgments
We thank the providers of the primary data re-
sources. The work on this project was sup-
ported by the Czech Science Foundation grants
no. P406/11/1499 and P406/2010/0875, and by
research resources of the Charles University in
Prague (PRVOUK). This work has been using lan-
guage resources developed and/or stored and/or
distributed by the LINDAT-Clarin project of the
Ministry of Education of the Czech Republic
(project LM2010013). Further, we would like to
thank Jan Hajic?, Ondr?ej Dus?ek and four anony-
mous reviewers for many useful comments on the
manuscript of this paper.
References
Itzair Aduriz et al 2003. Construction of a Basque de-
pendency treebank. In Proceedings of the 2nd Work-
shop on Treebanks and Linguistic Theories.
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In LREC, pages 1968?1703.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2003.
The annotation process in the Turkish treebank. In
Proceedings of the 4th Intern. Workshop on Linguis-
tically Interpreteted Corpora (LINC).
David Bamman and Gregory Crane. 2011. The An-
cient Greek and Latin dependency treebanks. In
Language Technology for Cultural Heritage, Theory
and Applications of Natural Language Processing,
pages 79?98. Springer Berlin Heidelberg.
Igor Boguslavsky, Svetlana Grigorieva, Nikolai Grig-
oriev, Leonid Kreidlin, and Nadezhda Frid. 2000.
Dependency treebank for Russian: Concept, tools,
types of information. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 987?991. Association for Computational Lin-
guistics Morristown, NJ, USA.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
Montserrat Civit, Maria Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: From constituents to
dependencies. In FinTAL, volume 4139 of Lec-
ture Notes in Computer Science, pages 141?152.
Springer.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Do?ra Csendes, Ja?nos Csirik, Tibor Gyimo?thy, and
Andra?s Kocsor. 2005. The Szeged treebank. In
TSD, volume 3658 of Lecture Notes in Computer
Science, pages 123?131. Springer.
Mihaela Ca?la?cean. 2008. Data-driven dependency
parsing for Romanian. Master?s thesis, Uppsala
University, August.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k Z?abokrtsky?, and Andreja Z?ele. 2006.
Towards a Slovene dependency treebank. In LREC
2006, pages 1388?1391, Genova, Italy. European
Language Resources Association (ELRA).
Nathan Green and Zdene?k Z?abokrtsky?. 2012. Hy-
brid combination of constituency and dependency
trees into an ensemble dependency parser. In Pro-
ceedings of the Workshop on Innovative Hybrid Ap-
proaches to the Processing of Textual Data, pages
19?26, Avignon, France. Association for Computa-
tional Linguistics.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova?-Raz??mova?. 2006. Prague Dependency
Treebank 2.0. CD-ROM, Linguistic Data Consor-
tium, LDC Catalog No.: LDC2006T01, Philadel-
phia.
525
Jan Hajic? et al 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in mul-
tiple languages. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
Jirka Hana and Jan S?te?pa?nek. 2012. Prague
markup language framework. In Proceedings of the
Sixth Linguistic Annotation Workshop, pages 12?
21, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics, Association for Computational
Linguistics.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking Finnish. In Proceedings of
the Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9), pages 79?90.
Samar Husain, Prashanth Mannem, Bharat Ambati,
and Phani Gadde. 2010. The ICON-2010 tools
contest on Indian language dependency parsing. In
Proceedings of ICON-2010 Tools Contest on Indian
Language Dependency Parsing, Kharagpur, India.
ISO 24615. 2010. Language resource management ?
Syntactic annotation framework (SynAF).
Sylvain Kahane. 1997. Bubble trees and syntactic
representations. In Proceedings of the 5th Meeting
of the Mathematics of the Language, DFKI, Saar-
brucken.
Matthias T. Kromann, Line Mikkelsen, and Stine Kern
Lynge. 2004. Danish dependency treebank.
Sandra Ku?bler, Erhard Hinrichs, Wolfgang Maier, and
Eva Klett. 2009. Parsing coordinations. In Pro-
ceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 406?414,
Athens, Greece, March. Association for Computa-
tional Linguistics.
Vincenzo Lombardo and Leonardo Lesmo. 1998. Unit
coordination and gapping in dependency theory. In
Processing of Dependency-Based Grammars; pro-
ceedings of the workshop. COLING-ACL, Montreal.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19:313?330.
Nicolar Mazziotta. 2011. Coordination of verbal de-
pendents in Old French: Coordination as a specified
juxtaposition or apposition. In Proceedings of In-
ternational Conference on Dependency Linguistics
(DepLing 2011).
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 122?131.
Igor A. Mel?c?uk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Simonetta Montemagni et al 2003. Building the Ital-
ian syntactic-semantic treebank. In Building and us-
ing Parsed Corpora, Language and Speech series,
pages 189?210, Dordrecht. Kluwer.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proceedings of the
NODALIDA Special Session on Treebanks.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
2007 Shared Task. EMNLP-CoNLL, June.
Martin Popel and Zdene?k Z?abokrtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Prokopis Prokopidis, Elina Desipri, Maria Koutsom-
bogera, Harris Papageorgiou, and Stelios Piperidis.
2005. Theoretical and practical issues in the con-
struction of a Greek dependency treebank. In Pro-
ceedings of the 4th Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 149?160.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2012.
Prague dependency style treebank for Tamil. In
Proceedings of LREC 2012, pages 23?25, I?stanbul,
Turkey. European Language Resources Association.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
Persian verbs: The first steps towards Persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231, Poznan?, Poland.
Kiril Simov and Petya Osenova. 2005. Extending
the annotation of BulTreeBank: Phase 2. In The
Fourth Workshop on Treebanks and Linguistic Theo-
ries (TLT 2005), pages 173?184, Barcelona, Decem-
ber.
Otakar Smrz?, Viktor Bielicky?, Iveta Kour?ilova?, Jakub
Kra?c?mar, Jan Hajic?, and Petr Zema?nek. 2008.
Prague Arabic dependency treebank: A word on the
million words. In Proceedings of the Workshop on
Arabic and Local Languages (LREC) 2008, pages
16?23, Marrakech, Morocco. European Language
Resources Association.
Leon Stassen. 2000. And-languages and with-
languages. Linguistic Typology, 4(1):1?54.
Jan S?te?pa?nek. 2006. Capturing a Sentence Struc-
ture by a Dependency Relation in an Annotated Syn-
tactical Corpus (Tools Guaranteeing Data Consis-
tence) (in Czech). Ph.D. thesis, Charles Univer-
526
sity in Prague, Faculty of Mathematics and Physics,
Prague, Czech Republic.
Pavel Stran?a?k and Jan S?te?pa?nek. 2010. Represent-
ing layered and structured data in the CoNLL-ST
format. In Alex Fang, Nancy Ide, and Jonathan
Webster, editors, Proceedings of the Second Inter-
national Conference on Global Interoperability for
Language Resources, pages 143?152, Hong Kong,
China. City University of Hong Kong, City Univer-
sity of Hong Kong.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel annotated cor-
pora for Catalan and Spanish. In LREC. European
Language Resources Association.
TEI Consortium. 2013. TEI P5: Guidelines for Elec-
tronic Text Encoding and Interchange.
Lucien Tesnie`re. 1959. Ele?ments de syntaxe struc-
turale. Paris.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMNLP, pages 1257?1268, Edin-
burgh, Scotland, UK, July. Association for Compu-
tational Linguistics.
Leonoor van der Beek et al 2002. Chapter 5. The
Alpino dependency treebank. In Algorithms for Lin-
guistic Processing NWO PIONIER Progress Report,
Groningen, The Netherlands.
Daniel Zeman, David Marec?ek, Martin Popel,
Loganathan Ramasamy, Jan S?te?pa?nek, Zdene?k
Z?abokrtsky?, and Jan Hajic?. 2012. HamleDT: To
parse or not to parse? In Proceedings of LREC 2012,
pages 2735?2741, I?stanbul, Turkey. European Lan-
guage Resources Association.
527
Proceedings of the ACL Student Research Workshop, pages 172?179,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Deepfix: Statistical Post-editing of Statistical Machine Translation Using
Deep Syntactic Analysis
Rudolf Rosa and David Marec?ek and Ales? Tamchyna
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{rosa,marecek,tamchyna}@ufal.mff.cuni.cz
Abstract
Deepfix is a statistical post-editing sys-
tem for improving the quality of statis-
tical machine translation outputs. It at-
tempts to correct errors in verb-noun va-
lency using deep syntactic analysis and a
simple probabilistic model of valency. On
the English-to-Czech translation pair, we
show that statistical post-editing of statis-
tical machine translation leads to an im-
provement of the translation quality when
helped by deep linguistic knowledge.
1 Introduction
Statistical machine translation (SMT) is the cur-
rent state-of-the-art approach to machine transla-
tion ? see e.g. Callison-Burch et al (2011). How-
ever, its outputs are still typically significantly
worse than human translations, containing vari-
ous types of errors (Bojar, 2011b), both in lexical
choices and in grammar.
As shown by many researchers, e.g. Bojar
(2011a), incorporating deep linguistic knowledge
directly into a translation system is often hard to
do, and seldom leads to an improvement of trans-
lation output quality. It has been shown that it is
often easier to correct the machine translation out-
puts in a second-stage post-processing, which is
usually referred to as automatic post-editing.
Several types of errors can be fixed by employ-
ing rule-based post-editing (Rosa et al, 2012b),
which can be seen as being orthogonal to the sta-
tistical methods employed in SMT and thus can
capture different linguistic phenomena easily.
But there are still other errors that cannot be cor-
rected with hand-written rules, as there exist many
linguistic phenomena that can never be fully de-
scribed manually ? they need to be handled statis-
tically by automatically analyzing large-scale text
corpora. However, to the best of our knowledge,
English Czech
go to the doctor j??t k doktorovi dative case
go to the centre j??t do centra genitive case
go to a concert j??t na koncert accusative case
go for a drink j??t na drink accusative case
go up the hill j??t na kopec accusative case
Table 1: Examples of valency of the verb ?to go?
and ?j??t?. For Czech, the morphological cases of
the nouns are also indicated.
Source: The government spends on the middleschools.
Moses: Vla?da utra?c?? str?edn?? s?koly.
Meaning: The government destroys the middleschools.
Reference: Vla?da utra?c?? za str?edn?? s?koly.
Meaning: The government spends on the middleschools.
Table 2: Example of a valency error in output of
Moses SMT system.
there is very little successful research in statistical
post-editing (SPE) of SMT (see Section 2).
In our paper, we describe a statistical approach
to correcting one particular type of English-to-
Czech SMT errors ? errors in the verb-noun va-
lency. The term valency stands for the way in
which verbs and their arguments are used together,
usually together with prepositions and morpholog-
ical cases, and is described in Section 4. Several
examples of the valency of the English verb ?to go?
and the corresponding Czech verb ?j??t? are shown
in Table 1.
We conducted our experiments using a state-of-
the-art SMT system Moses (Koehn et al, 2007).
An example of Moses making a valency error is
translating the sentence ?The government spends
on the middle schools.?, adapted from our devel-
opment data set. As shown in Table 2, Moses
translates the sentence incorrectly, making an er-
ror in the valency of the ?utra?cet ? s?kola? (?spend ?
school?) pair. The missing preposition changes the
meaning dramatically, as the verb ?utra?cet? is pol-
172
ysemous and can mean ?to spend (esp. money)? as
well as ?to kill, to destroy (esp. animals)?.
Our approach is to use deep linguistic analysis
to automatically determine the structure of each
sentence, and to detect and correct valency errors
using a simple statistical valency model. We de-
scribe our approach in detail in Section 5.
We evaluate and discuss our experiments in
Section 6. We then conclude the paper and pro-
pose areas to be researched in future in Section 7.
2 Related Work
The first reported results of automatic post-editing
of machine translation outputs are (Simard et al,
2007) where the authors successfully performed
statistical post-editing (SPE) of rule-based ma-
chine translation outputs. To perform the post-
editing, they used a phrase-based SMT system in a
monolingual setting, trained on the outputs of the
rule-based system as the source and the human-
provided reference translations as the target, to
achieve massive translation quality improvements.
The authors also compared the performance of the
post-edited rule-based system to directly using the
SMT system in a bilingual setting, and reported
that the SMT system alone performed worse than
the post-edited rule-based system. They then tried
to post-edit the bilingual SMT system with another
monolingual instance of the same SMT system,
but concluded that no improvement in quality was
observed.
The first known positive results in SPE of SMT
are reported by Oflazer and El-Kahlout (2007)
on English to Turkish machine translation. The
authors followed a similar approach to Simard
et al (2007), training an SMT system to post-
edit its own output. They use two iterations of
post-editing to get an improvement of 0.47 BLEU
points (Papineni et al, 2002). The authors used
a rather small training set and do not discuss the
scalability of their approach.
To the best of our knowledge, the best results re-
ported so far for SPE of SMT are by Be?chara et al
(2011) on French-to-English translation. The au-
thors start by using a similar approach to Oflazer
and El-Kahlout (2007), getting a statistically sig-
nificant improvement of 0.65 BLEU points. They
then further improve the performance of their
system by adding information from the source
side into the post-editing system by concatenat-
ing some of the translated words with their source
Direction Baseline SPE Context SPE
en?cs 10.85?0.47 10.70?0.44 10.73?0.49
cs?en 17.20?0.53 17.11?0.52 17.18?0.54
Table 3: Results of SPE approach of Be?chara et al
(2011) evaluated on English-Czech SMT.
words, eventually reaching an improvement of
2.29 BLEU points. However, similarly to Oflazer
and El-Kahlout (2007), the training data used are
very small, and it is not clear how their method
scales on larger training data.
In our previous work (Rosa et al, 2012b), we
explored a related but substantially different area
of rule-based post-editing of SMT. The resulting
system, Depfix, manages to significantly improve
the quality of several SMT systems outputs, using
a set of hand-written rules that detect and correct
grammatical errors, such as agreement violations.
Depfix can be easily combined with Deepfix,1 as
it is able to correct different types of errors.
3 Evaluation of Existing SPE
Approaches
First, we evaluated the utility of the approach of
Be?chara et al (2011) for the English-Czech lan-
guage pair. We used 1 million sentence pairs from
CzEng 1.0 (Bojar et al, 2012b), a large English-
Czech parallel corpus. Identically to the paper, we
split the training data into 10 parts, trained 10 sys-
tems (each on nine tenths of the data) and used
them to translate the remaining part. The second
step was then trained on the concatenation of these
translations and the target side of CzEng. We also
implemented the contextual variant of SPE where
words in the intermediate language are annotated
with corresponding source words if the alignment
strength is greater than a given threshold. We lim-
ited ourselves to the threshold value 0.8, for which
the best results are reported in the paper. We tuned
all systems on the dataset of WMT11 (Callison-
Burch et al, 2011) and evaluated on the WMT12
dataset (Callison-Burch et al, 2012).
Table 3 summarizes our results. The reported
confidence intervals were estimated using boot-
strap resampling (Koehn, 2004). SPE did not lead
to any improvements of BLEU in our experiments.
In fact, SPE even slightly decreased the score (but
1Depfix (Rosa et al, 2012b) performs rule-based post-
editing on shallow-syntax dependency trees, while Deepfix
(described in this paper) is a statistical post-editing system
operating on deep-syntax dependency trees.
173
the difference is statistically insignificant in all
cases).
We conclude that this method does not improve
English-Czech translation, possibly because our
training data is too large for this method to bring
any benefit. We therefore proceed with a more
complex approach which relies on deep linguistic
knowledge.
4 Deep Dependency Syntax, Formemes,
and Valency
4.1 Tectogrammatical dependency trees
Tectogrammatical trees are deep syntactic depen-
dency trees based on the Functional Generative
Description (Sgall et al, 1986). Each node in
a tectogrammatical tree corresponds to a content
word, such as a noun, a full verb or an adjec-
tive; the node consists of the lemma of the con-
tent word and several other attributes. Functional
words, such as prepositions or auxiliary verbs, are
not directly present in the tectogrammatical tree,
but are represented by attributes of the respective
content nodes. See Figure 1 for an example of two
tectogrammatical trees (for simplicity, most of the
attributes are not shown).
In our work, we only use one of the
many attributes of tectogrammatical nodes, called
formeme (Dus?ek et al, 2012). A formeme is a
string representation of selected morpho-syntactic
features of the content word and selected auxiliary
words that belong to the content word, devised to
be used as a simple and efficient representation of
the node.
A noun formeme, which we are most interested
in, consists of three parts (examples taken from
Figure 1):
1. The syntactic part-of-speech ? n for nouns.
2. The preposition if the noun has one (empty
otherwise), as in n:on+X or n:za+4.
3. A form specifier.
? In English, it typically marks the subject
or object, as in n:subj. In case of a
noun accompanied by a preposition, the
third part is always X, as in n:on+X.
? In Czech, it denotes the morphologi-
cal case of the noun, represented by
its number (from 1 to 7 as there are
seven cases in Czech), as in n:1 and
n:za+4.
t-treezone=en
government n:subj
spend v:fin
middle adj:attr
school n:on+X
t-treezone=cs
vl?da n:1
utr?cet v:fin
st?edn? adj:attr
?kola n:za+4
Figure 1: Tectogrammatical trees for the sentence
?The government spends on the middle schools.? ?
?Vla?da utra?c?? za str?edn?? s?koly.?; only lemmas and
formemes of the nodes are shown.
Adjectives and nouns can also have the
adj:attr and n:attr formemes, respectively,
meaning that the node is in morphological agree-
ment with its parent. This is especially important
in Czech, where this means that the word bears the
same morphological case as its parent node.
4.2 Valency
The notion of valency (Tesnie`re and Fourquet,
1959) is semantic, but it is closely linked to syn-
tax. In the theory of valency, each verb has one
or more valency frames. Each valency frame de-
scribes a meaning of the verb, together with argu-
ments (usually nouns) that the verb must or can
have, and each of the arguments has one or several
fixed forms in which it must appear. These forms
can typically be specified by prepositions and mor-
phological cases to be used with the noun, and thus
can be easily expressed by formemes.
For example, the verb ?to go?, shown in Ta-
ble 1, has a valency frame that can be expressed
as n:subj go n:to+X, meaning that the sub-
ject goes to some place.
The valency frames of the verbs ?spend?
and ?utra?cet? in Figure 1 can be written as
n:subj spend n:on+X and n:1 utra?cet
n:za+4; the subject (in Czech this is a noun in
nominative case) spends on an object (in Czech,
the preposition ?za? plus a noun in accusative
case).
In our work, we have extended our scope also
to noun-noun valency, i.e. the parent node can be
either a verb or a noun, while the arguments are al-
ways nouns. Practice has proven this extension to
be useful, although the majority of the corrections
174
performed are still of the verb-noun valency type.
Still, we keep the traditional notion of verb-noun
valency throughout the text, especially to be able
to always refer to the parent as ?the verb? and to
the child as ?the noun?.
5 Our Approach
5.1 Valency models
To be able to detect and correct valency errors, we
created statistical models of verb-noun valency.
We model the conditional probability of the noun
argument formeme based on several features of the
verb-noun pair. We decided to use the following
two models:
P (fn|lv, fEN ) (1)
P (fn|lv, ln, fEN ) (2)
where:
? fn is the formeme of the Czech noun argu-
ment, which is being modelled
? lv is the lemma of the Czech parent verb
? ln is the lemma of the Czech noun argument
? fEN is the formeme of the English noun
aligned to the Czech noun argument
The input is first processed by the model (1),
which performs more general fixes, in situations
where the (lv, fEN ) pair rather unambiguously de-
fines the valency frame required.
Then model (2) is applied, correcting some er-
rors of the model (1), in cases where the noun
argument requires a different valency frame than
is usual for the (lv, fEN ) pair, and making some
more fixes in cases where the correct valency
frame required for the (lv, fEN ) pair was too am-
biguous to make a correction according to model
(1), but the decision can be made once information
about ln is added.
We computed the models on the full training set
of CzEng 1.0 (Bojar et al, 2012b) (roughly 15 mil-
lion sentences), and smoothed the estimated prob-
abilities with add-one smoothing.
5.2 Deepfix
We introduce a new statistical post-editing system,
Deepfix, whose input is a pair of an English sen-
tence and its Czech machine translation, and the
output is the Czech sentence with verb-noun va-
lency errors corrected.
The Deepfix pipeline consists of several steps:
1. the sentences are tokenized, tagged and lem-
matized (a lemma and a morphological tag is
assigned to each word)
2. corresponding English and Czech words are
aligned based on their lemmas
3. deep-syntax dependency parse trees of the
sentences are built, the nodes in the trees are
labelled with formemes
4. improbable noun formemes are replaced with
correct formemes according to the valency
model
5. the words are regenerated according to the
new formemes
6. the regenerating continues recursively to chil-
dren of regenerated nodes if they are in
morphological agreement with their parents
(which is typical for adjectives)
To decide whether the formeme of the noun is
incorrect, we query the valency model for all pos-
sible formemes and their probabilities. If an alter-
native formeme probability exceeds a fixed thresh-
old, we assume that the original formeme is incor-
rect, and we use the alternative formeme instead.
For our example sentence, ?The government
spends on the middle schools.? ? ?Vla?da utra?c?? za
str?edn?? s?koly.?, we query the model (2) and get the
following probabilities:
? P(n:4 | utra?cet, s?kola, n:on+X) = 0.07
(the original formeme)
? P(n:za+4 | utra?cet, s?kola, n:on+X) = 0.89
(the most probable formeme)
The threshold for this change type is 0.86, is
exceeded by the n:za+4 formeme and thus the
change is performed: ?s?koly? is replaced by ?za
s?koly?.
5.3 Tuning the Thresholds
We set the thresholds differently for different types
of changes. The values of the thresholds that we
used are listed in Table 4 and were estimated man-
ually. We distinguish changes where only the
morphological case of the noun is changed from
changes to the preposition. There are three possi-
ble types of a change to a preposition: switching
one preposition to another, adding a new preposi-
tion, and removing an existing preposition. The
175
Correction type Thresholds for models(1) (2)
Changing the noun case only 0.55 0.78
Changing the preposition 0.90 0.84
Adding a new preposition ? 0.86
Removing the preposition ? ?
Table 4: Deepfix thresholds
change to the preposition can also involve chang-
ing the morphological case of the noun, as each
preposition typically requires a certain morpho-
logical case.
For some combinations of a change type and a
model, as in case of the preposition removing, we
never perform a fix because we observed that it
nearly never improves the translation. E.g., if a
verb-noun pair can be correct both with and with-
out a preposition, the preposition-less variant is
usually much more frequent than the prepositional
variant (and thus is assigned a much higher prob-
ability by the model). However, the preposition
often bears a meaning that is lost by removing it
? in Czech, which is a relatively free-word-order
language, the semantic roles of verb arguments
are typically distinguished by prepositions, as op-
posed to English, where they can be determined
by their relative position to the verb.
5.4 Implementation
The whole Deepfix pipeline is implemented in
Treex, a modular NLP framework (Popel and
Z?abokrtsky?, 2010) written in Perl, which provides
wrappers for many state-of-the-art NLP tools. For
the analysis of the English sentence, we use the
Morc?e tagger (Spoustova? et al, 2007) and the
MST parser (McDonald et al, 2005). The Czech
sentence is analyzed by the Featurama tagger2 and
the RUR parser (Rosa et al, 2012a) ? a parser
adapted to parsing of SMT outputs. The word
alignment is created by GIZA++ (Och and Ney,
2003); the intersection symmetrization is used.
6 Evaluation
6.1 Automatic Evaluation
We evaluated our method on three datasets:
WMT10 (2489 parallel sentences), WMT11 (3003
parallel sentences), and WMT12 (3003 parallel
sentences) by Callison-Burch et al (2010; 2011;
2012). For evaluation, we used outputs of a
state-of-the-art SMT system, Moses (Koehn et al,
2http://featurama.sourceforge.net/
2007), tuned for English-to-Czech translation (Bo-
jar et al, 2012a). We used the WMT10 dataset
and its Moses translation as our development data
to tune the thresholds. In Table 5, we report the
achieved BLEU scores (Papineni et al, 2002),
NIST scores (Doddington, 2002), and PER (Till-
mann et al, 1997).
The improvements in automatic scores are low
but consistently positive, which suggests that
Deepfix does improve the translation quality.
However, the changes performed by Deepfix are
so small that automatic evaluation is unable to re-
liably assess whether they are positive or negative
? it can only be taken as an indication.
6.2 Manual Evaluation
To reliably assess the performance of Deepfix,
we performed manual evaluation on the WMT12
dataset translated by the Moses system.
The dataset was evenly split into 4 parts and
each of the parts was evaluated by one of two an-
notators (denoted ?A? and ?B?). For each sentence
that was modified by Deepfix, the annotator de-
cided whether the Deepfix correction had a posi-
tive (?improvement?) or negative (?degradation?)
effect on the translation quality, or concluded that
this cannot be decided (?indefinite?) ? either be-
cause both of the sentences are correct variants, or
because both are incorrect.3
The results in Table 6 prove that the overall ef-
fect of Deepfix is positive: it modifies about 20%
of the sentence translations (569 out of 3003 sen-
tences), improving over a half of them while lead-
ing to a degradation in only a quarter of the cases.
We measured the inter-annotator agreement on
100 sentences which were annotated by both an-
notators. For 60 sentence pairs, both of the anno-
tators were able to select which sentence is better,
i.e. none of the annotators used the ?indefinite?
marker. The inter-annotator agreement on these
60 sentence pairs was 97%.4
3The evaluation was done in a blind way, i.e. the annota-
tors did not know which sentence is before Deepfix and which
is after Deepfix. They were also provided with the source En-
glish sentences and the reference human translations.
4If all 100 sentence pairs are taken into account, requiring
that the annotators also agree on the ?indefinite? marker, the
inter-annotator agreement is only 65%. This suggests that
deciding whether the translation quality differs significantly
is much harder than deciding which translation is of a higher
quality.
176
Dataset BLEU score (higher is better) NIST score (higher is better) PER (lower is better)Baseline Deepfix Difference Baseline Deepfix Difference Baseline Deepfix Difference
WMT10* 15.66 15.74 +0.08 5.442 5.470 +0.028 58.44% 58.26% -0.18
WMT11 16.39 16.42 +0.03 5.726 5.737 +0.011 57.17% 57.09% -0.08
WMT12 13.81 13.85 +0.04 5.263 5.283 +0.020 60.04% 59.91% -0.13
Table 5: Automatic evaluation of Deepfix on outputs of the Moses system on WMT10, WMT11 and
WMT12 datasets. *Please note that WMT10 was used as the development dataset.
Part Annotator Changed sentences Improvement Degradation Indefinite
1 A 126 57 (45%) 35 (28%) 34 (27%)
2 B 112 62 (55%) 29 (26%) 21 (19%)
3 A 150 88 (59%) 29 (19%) 33 (22%)
4 B 181 114 (63%) 42 (23%) 25 (14%)
Total 569 321 (56%) 135 (24%) 113 (20%)
Table 6: Manual evaluation of Deepfix on outputs of Moses Translate system on WMT12 dataset.
6.3 Discussion
When a formeme change was performed, it was
usually either positive or at least not harmful (sub-
stituting one correct variant for another correct
variant).
However, we also observed a substantial
amount of cases where the change of the formeme
was incorrect. Manual inspection of a sample of
these cases showed that there can be several rea-
sons for a formeme change to be incorrect:
? incorrect analysis of the Czech sentence
? incorrect analysis of the English sentence
? the original formeme is a correct but very rare
variant
The most frequent issue is the first one. This is
to be expected, as the Czech sentence is often er-
roneous, whereas the NLP tools that we used are
trained on correct sentences; in many cases, it is
not even clear what a correct analysis of an incor-
rect sentence should be.
7 Conclusion and Future Work
On the English-Czech pair, we have shown that
statistical post-editing of statistical machine trans-
lation outputs is possible, even when translating
from a morphologically poor to a morphologi-
cally rich language, if it is grounded by deep lin-
guistic knowledge. With our tool, Deepfix, we
have achieved improvements on outputs of two
state-of-the-art SMT systems by correcting verb-
noun valency errors, using two simple probabilis-
tic valency models computed on large-scale data.
The improvements have been confirmed by man-
ual evaluation.
We encountered many cases where the per-
formance of Deepfix was hindered by errors of
the underlying tools, especially the taggers, the
parsers and the aligner. Because the use of the
RUR parser (Rosa et al, 2012a), which is partially
adapted to SMT outputs parsing, lead to a reduc-
tion of the number of parser errors, we find the ap-
proach of adapting the tools for this specific kind
of data to be promising.
We believe that our method can be adapted
to other language pairs, provided that there is a
pipeline that can analyze at least the target lan-
guage up to deep syntactic trees. Because we only
use a small subset of information that a tectogram-
matical tree provides, it is sufficient to use only
simplified tectogrammatical trees. These could be
created by a small set of rules from shallow-syntax
dependency trees, which can be obtained for many
languages using already existing parsers.
Acknowledgments
This research has been supported by the 7th FP
project of the EC No. 257528 and the project
7E11042 of the Ministry of Education, Youth and
Sports of the Czech Republic.
Data and some tools used as a prerequisite
for the research described herein have been pro-
vided by the LINDAT/CLARIN Large Infrastruc-
tural project, No. LM2010013 of the Ministry of
Education, Youth and Sports of the Czech Repub-
lic.
We would like to thank two anonymous review-
ers for many useful comments on the manuscript
of this paper.
177
References
Hanna Be?chara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. MT Summit XIII, pages 308?315.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.
2012a. Probes in a taxonomy of factored phrase-
based models. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 253?
260, Montre?al, Canada. Association for Computa-
tional Linguistics.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The joy of parallelism with CzEng
1.0. In Proceedings of the 8th International Confer-
ence on Language Resources and Evaluation (LREC
2012), pages 3921?3928, I?stanbul, Turkey. Euro-
pean Language Resources Association.
Ondr?ej Bojar. 2011a. Rich morphology and what
can we expect from hybrid approaches to MT. In-
vited talk at International Workshop on Using Lin-
guistic Information for Hybrid Machine Translation
(LIHMT-2011), November.
Ondr?ej Bojar. 2011b. Analyzing error types in
English-Czech machine translation. Prague Bulletin
of Mathematical Linguistics, 95:63?76, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145. Morgan Kauf-
mann Publishers Inc.
Ondr?ej Dus?ek, Zdene?k Z?abokrtsky?, Martin Popel, Mar-
tin Majlis?, Michal Nova?k, and David Marec?ek.
2012. Formemes in English-Czech deep syntac-
tic MT. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 267?274,
Montre?al, Canada. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP,
Barcelona, Spain.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91?98. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, pages 25?32. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL 2002,
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: modular NLP framework. In Proceedings of
the 7th international conference on Advances in nat-
ural language processing, IceTAL?10, pages 293?
304, Berlin, Heidelberg. Springer-Verlag.
Rudolf Rosa, Ondr?ej Dus?ek, David Marec?ek, and Mar-
tin Popel. 2012a. Using parallel features in pars-
ing of machine-translated sentences for correction of
grammatical errors. In Proceedings of Sixth Work-
shop on Syntax, Semantics and Structure in Statis-
tical Translation (SSST-6), ACL, pages 39?48, Jeju,
Korea. ACL.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.
2012b. DEPFIX: A system for automatic correction
of Czech MT outputs. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
178
pages 362?368, Montre?al, Canada. Association for
Computational Linguistics.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The meaning of the sentence in its semantic and
pragmatic aspects. Springer.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical phrase-based post-editing. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics; Proceedings of the
Main Conference, pages 508?515, Rochester, New
York, April. Association for Computational Linguis-
tics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of
two worlds: Cooperation of statistical and rule-
based taggers for Czech. In Proceedings of the
Workshop on Balto-Slavonic Natural Language Pro-
cessing 2007, pages 67?74, Praha, Czechia. Uni-
verzita Karlova v Praze, Association for Computa-
tional Linguistics.
Lucien Tesnie`re and Jean Fourquet. 1959. Ele?ments de
syntaxe structurale. E?ditions Klincksieck, Paris.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated dp based search for statistical translation.
In European Conf. on Speech Communication and
Technology, pages 2667?2670.
179
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201?206,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Maximum Entropy Translation Model
in Dependency-Based MT Framework
David Marec?ek, Martin Popel, Zdene?k Z?abokrtsky?
Charles University in Prague, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{marecek,popel,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Maximum Entropy Principle has been
used successfully in various NLP tasks. In
this paper we propose a forward transla-
tion model consisting of a set of maxi-
mum entropy classifiers: a separate clas-
sifier is trained for each (sufficiently fre-
quent) source-side lemma. In this way
the estimates of translation probabilities
can be sensitive to a large number of fea-
tures derived from the source sentence (in-
cluding non-local features, features mak-
ing use of sentence syntactic structure,
etc.). When integrated into English-to-
Czech dependency-based translation sce-
nario implemented in the TectoMT frame-
work, the new translation model signif-
icantly outperforms the baseline model
(MLE) in terms of BLEU. The perfor-
mance is further boosted in a configuration
inspired by Hidden Tree Markov Mod-
els which combines the maximum entropy
translation model with the target-language
dependency tree model.
1 Introduction
The principle of maximum entropy states that,
given known constraints, the probability distri-
bution which best represents the current state of
knowledge is the one with the largest entropy.
Maximum entropy models based on this princi-
ple have been widely used in Natural Language
Processing, e.g. for tagging (Ratnaparkhi, 1996),
parsing (Charniak, 2000), and named entity recog-
nition (Bender et al, 2003). Maximum entropy
models have the following form
p(y|x) =
1
Z(x)
exp
?
i
?ifi(x, y)
where fi is a feature function, ?i is its weight, and
Z(x) is the normalizing factor
Z(x) =
?
y
exp
?
i
?ifi(x, y)
In statistical machine translation (SMT), trans-
lation model (TM) p(t|s) is the probability that the
string t from the target language is the translation
of the string s from the source language. Typical
approach in SMT is to use backward translation
model p(s|t) according to Bayes? rule and noisy-
channel model. However, in this paper we deal
only with the forward (direct) model.1
The idea of using maximum entropy for con-
structing forward translation models is not new. It
naturally allows to make use of various features
potentially important for correct choice of target-
language expressions. Let us adopt a motivat-
ing example of such a feature from (Berger et al,
1996) (which contains the first usage of maxent
translation model we are aware of): ?If house ap-
pears within the next three words (e.g., the phrases
in the house and in the red house), then dans might
be a more likely [French] translation [of in].?
Incorporating non-local features extracted from
the source sentence into the standard noisy-
channel model in which only the backward trans-
lation model is available, is not possible. This
drawback of the noisy-channel approach is typi-
cally compensated by using large target-language
n-gram models, which can ? in a result ? play a
role similar to that of a more elaborate (more con-
text sensitive) forward translation model. How-
ever, we expect that it would be more beneficial to
exploit both the parallel data and the monolingual
data in a more balance fashion, rather than extract
only a reduced amount of information from the
parallel data and compensate it by large language
model on the target side.
1A backward translation model is used only for pruning
training data in this paper.
201
A deeper discussion on the potential advantages
of maximum entropy approach over the noisy-
channel approach can be found in (Foster, 2000)
and (Och and Ney, 2002), in which another suc-
cessful applications of maxent translation models
are shown. Log-linear translation models (instead
of MLE) with rich feature sets are used also in
(Ittycheriah and Roukos, 2007) and (Gimpel and
Smith, 2009); the idea can be traced back to (Pap-
ineni et al, 1997).
What makes our approach different from the
previously published works is that
1. we show how the maximum entropy trans-
lation model can be used in a dependency
framework; we use deep-syntactic depen-
dency trees (as defined in the Prague Depen-
dency Treebank (Hajic? et al, 2006)) as the
transfer layer,
2. we combine the maximum entropy transla-
tion model with target-language dependency
tree model and use tree-modified Viterbi
search for finding the optimal lemmas label-
ing of the target-tree nodes.
The rest of the paper is structured as follows. In
Section 2 we give a brief overview of the trans-
lation framework TectoMT in which the experi-
ments are implemented. In Section 3 we describe
how our translation models are constructed. Sec-
tion 4 summarizes the experimental results, and
Section 5 contains a summary.
2 Translation framework
We use tectogrammatical (deep-syntactic) layer of
language representation as the transfer layer in the
presented MT experiments. Tectogrammatics was
introduced in (Sgall, 1967) and further elaborated
within the Prague Dependency Treebank project
(Hajic? et al, 2006). On this layer, each sentence
is represented as a tectogrammatical tree, whose
main properties (from the MT viewpoint) are fol-
lowing: (1) nodes represent autosemantic words,
(2) edges represent semantic dependencies (a node
is an argument or a modifier of its parent), (3) there
are no functional words (prepositions, auxiliary
words) in the tree, and the autosemantic words ap-
pear only in their base forms (lemmas). Morpho-
logically indispensable categories (such as number
with nouns or tense with verbs, but not number
with verbs as it is only imposed by agreement) are
stored in separate node attributes (grammatemes).
The intuition behind the decision to use tec-
togrammatics for MT is the following: we be-
lieve that (1) tectogrammatics largely abstracts
from language-specific means (inflection, agglu-
tination, functional words etc.) of expressing
non-lexical meanings and thus tectogrammatical
trees are supposed to be highly similar across lan-
guages,2 (2) it enables a natural transfer factor-
ization,3 (3) and local tree contexts in tectogram-
matical trees carry more information (especially
for lexical choice) than local linear contexts in the
original sentences.4
In order to facilitate transfer of sentence ?syn-
tactization?, we work with tectogrammatical nodes
enhanced with the formeme attribute (Z?abokrtsky?
et al, 2008), which captures the surface mor-
phosyntactic form of a given tectogrammatical
node in a compact fashion. For example, the
value n:pr?ed+4 is used to label semantic nouns
that should appear in an accusative form in a
prepositional group with the preposition pr?ed in
Czech. For English we use formemes such as
n:subj (semantic noun (SN) in subject position),
n:for+X (SN with preposition for), n:X+ago (SN
with postposition ago), n:poss (possessive form of
SN), v:because+fin (semantic verb (SV) as a sub-
ordinating finite clause introduced by because),
v:without+ger (SV as a gerund after without), adj:attr
(semantic adjective (SA) in attributive position),
adj:compl (SA in complement position).
We have implemented our experiments in the
TectoMT software framework, which already of-
fers tool chains for analysis and synthesis of Czech
and English sentences (Z?abokrtsky? et al, 2008).
The translation scenario proceeds as follows.
1. The input English text is segmented into sen-
tences and tokens.
2. The tokens are lemmatized and tagged with
Penn Treebank tags using the Morce tagger
(Spoustova? et al, 2007).
2This claim is supported by error analysis of output of
tectogrammatics-based MT system presented in (Popel and
Z?abok/rtsky?, 2009), which shows that only 8 % of translation
errors are caused by the (obviously too strong) assumption
that the tectogrammatical tree of a sentence and the tree rep-
resenting its translation are isomorphic.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
?denser?, especially when translating from/to a language with
rich inflection such as Czech.
4Recall the house-is-somewhere-around feature in the in-
troduction; again, the fact that we know the dominating (or
dependent) word should allow to construct a more compact
translation model, compared to n-gram models.
202
Figure 1: Intermediate sentence representations when translating the English sentence ?However, this
very week, he tried to find refuge in Brazil.?, leading to the Czech translation ?Pr?esto se tento pra?ve?
ty?den snaz?il naj??t u?toc?is?te? v Braz??lii.?.
3. Then the Maximum Spanning Tree parser
(McDonald et al, 2005) is applied and a
surface-syntax dependency tree (analytical
tree in the PDT terminology) is created for
each sentence (Figure 1a).
4. This tree is converted to a tectogrammatical
tree (Figure 1b). Each autosemantic word
with its associated functional words is col-
lapsed into a single tectogrammatical node,
labeled with lemma, formeme, and seman-
tically indispensable morphologically cate-
gories; coreference is also resolved. Collaps-
ing edges are depicted by wider lines in the
Figure 1a.
5. The transfer phase follows, whose most dif-
ficult part consists in labeling the tree with
target-side lemmas and formemes5 (changes
of tree topology are required relatively infre-
quently). See Figure 1c.
6. Finally, surface sentence shape (Figure 1d) is
synthesized from the tectogrammatical tree,
which is basically a reverse operation for the
5In this paper we focus on using maximum entropy
for translating lemmas, but it can be used for translating
formemes as well.
tectogrammatical analysis: adding punctua-
tion and functional words, spreading mor-
phological categories according to grammat-
ical agreement, performing inflection (using
Czech morphology database (Hajic?, 2004)),
arranging word order etc.
3 Training the two models
In this section we describe two translation mod-
els used in the experiments: a baseline translation
model based on maximum likelihood estimates
(3.2), and a maximum entropy based model (3.3).
Both models are trained using the same data (3.1).
In addition, we describe a target-language tree
model (3.4), which can be combined with both
the translation models using the Hidden Tree
Markov Model approach and tree-modified Viterbi
search, similarly to the approach of (Z?abokrtsky?
and Popel, 2009).
3.1 Data preprocessing common for both
models
We used Czech-English parallel corpus CzEng 0.9
(Bojar and Z?abokrtsky?, 2009) for training the
translation models. CzEng 0.9 contains about
8 million sentence pairs, and also their tectogram-
matical analyses and node-wise alignment.
203
We used only trees from training sections (about
80 % of the whole data), which contain around 30
million pairs of aligned tectogrammatical nodes.
From each pair of aligned tectogrammatical
nodes, we extracted triples containing the source
(English) lemma, the target (Czech) lemma, and
the feature vector.
In order to reduce noise in the training data,
we pruned the data in two ways. First, we dis-
regarded all triples whose lemma pair did not oc-
cur at least twice in the whole data. Second,
we computed forward and backward maximum
likelihood (ML) translation models (target lemma
given source lemma and vice versa) and deleted
all triples whose probability according to one of
the two models was lower than the threshold 0.01.
Then the forward ML translation model was
reestimated using only the remaining data.
For a given pair of aligned nodes, the feature
vector was of course derived only from the source-
side node or from the tree which it belongs to. As
already mentioned in the introduction, the advan-
tage of the maximum entropy approach is that a
rich and diverse set of features can be used, with-
out limiting oneself to linearly local context. The
following features (or, better to say, feature tem-
plates, as each categorical feature is in fact con-
verted to a number of 0-1 features) were used:
? formeme and morphological categories of the
given node,
? lemma, formeme and morphological cate-
gories of the governing node,
? lemmas and formemes of all child nodes,
? lemmas and formemes of the nearest linearly
preceding and following nodes.
3.2 Baseline translation model
The baseline TM is basically the ML translation
model resulting from the previous section, lin-
early interpolated with several translation models
making use of regular word-formative derivations,
which can be helpful for translating some less fre-
quent (but regularly derived) lemmas. For exam-
ple, one of the derivation-based models estimates
the probability p(zaj??mave?|interestingly) (possibly
unseen pair of deadjectival adverbs) by the value
of p(zaj??mavy?|interesting). More detailed descrip-
tion of these models goes beyond the scope of this
paper; their weights in the interpolation are very
small anyway.
3.3 MaxEnt translation model
The MaxEnt TM was created as follows:
1. training triples (source lemma, target lemma,
feature vector) were disregarded if the source
lemma was not seen at least 50 times (only
the baseline model will be used for such lem-
mas),
2. the remaining triples were grouped by the En-
glish lemma (over 16 000 groups),
3. due to computational issues, the maximum
number of triples in a group was reduced to
1000 by random selection,
4. a separate maximum entropy classifier
was trained for each group (i.e., one
classifier per source-side lemma) using
AI::MaxEntropy Perl module,6
5. due to the more aggressive pruning of the
training data, coverage of this model is
smaller than that of the baseline model; in or-
der not to loose the coverage, the two mod-
els were combined using linear interpolation
(1:1).
Selected properties of the maximum entropy
translation model (before the linear interpolation
with the baseline model) are shown in Figure 2.
We increased the size of the training data from
10 000 training triples up to 31 million and eval-
uated three relative quantities characterizing the
translation models:
? coverage - relative frequency of source lem-
mas for which the translation model offers at
least one translation,
? first - relative frequency of source lemmas for
which the target lemmas offered as the first
by the model (argmax) are the correct ones,
? oracle - relative frequency of source lemmas
for which the correct target lemma is among
the lemmas offered by the translation model.
As mentioned in Section 3.1, there are context
features making use both of local linear context
and local tree context. After training the MaxEnt
model, there are about 4.5 million features with
non-zero weight, out of which 1.1 million features
6http://search.cpan.org/perldoc?AI::
MaxEntropy
204
Figure 2: Three measures characterizing the Max-
Ent translation model performance, depending on
the training data size. Evaluated on aligned node
pairs from the dtest portion of CzEng 0.9.
are derived from the linear context and 2.4 million
features are derived from the tree context. This
shows that the MaxEnt translation model employs
the dependency structure intensively.
A preliminary analysis of feature weights seems
to support our intuition that the linear context
is preferred especially in the case of more sta-
ble collocations. For example, the most impor-
tant features for translating the lemma bare are
based on the lemma of the following noun: tar-
get lemma bosy? (barefooted) is preferred if the fol-
lowing noun on the source side is foot, while holy?
(naked, unprotected) is preferred if hand follows.
The contribution of dependency-based features
can be illustrated on translating the word drop.
The greatest weight for choosing kapka (a droplet)
as the translation is assigned to the feature captur-
ing the presence of a node with formeme n:of+X
among the node?s children. The greatest weights
in favor of odhodit (throw aside) are assigned to
features capturing the presence of words such as
gun or weapon, while the greatest weights in favor
of klesnout (to come down) are assigned to fea-
tures saying that there is the lemma percent or the
percent sign among the children.
Of course, the lexical choice is influenced also
by the governing lemmas, as can be illustrated
with the word native. One can find a high-
value feature for rodily? (native-born) saying that
the source-side parent is speaker; similarly for
mater?sky? (mother) with governing tongue, and
rodny? (home) with land.
Linear and tree features are occasionally used
simultaneously: there are high-valued positive
configuration BLEU NIST
baseline TM 10.44 4.795
MaxEnt TM 11.77 5.135
baseline TM + TreeLM 11.77 5.038
MaxEnt TM + TreeLM 12.58 5.250
Table 1: BLEU and NIST evaluation of four con-
figurations of our MT system; the WMT 2010 test
set was used.
weights for translating order as objednat (reserve,
give an order for st.) assigned both to tree-based
features saying that there are words such as pizza,
meal or goods and to linear features saying that the
very following word is some or two.
3.4 Target-language tree model
Although the MaxEnt TM captures some contex-
tual dependencies that are covered by language
models in the standard noisy-channel SMT, it may
still be beneficial to exploit target-language mod-
els, because these can be trained on huge mono-
lingual corpora. We use a target-language depen-
dency tree model differing from standard n-gram
model in two aspects:
? it uses tree context instead of linear context,
? it predicts tectogrammatical attributes (lem-
mas and formemes) instead of word forms.
In particular, our target-language tree model
(TreeLM) predicts the probability of node?s
lemma and formeme given its parent?s lemma and
formeme. The optimal (lemma and formeme) la-
beling is found by tree-modified Viterbi search;
for details see (Z?abokrtsky? and Popel, 2009).
4 Experiments
When included into the above described transla-
tion scenario, the MaxEnt TM outperforms the
baseline TM, be it used together with or with-
out TreeLM. The results are summarized in Ta-
ble 1. The improvement is statistically signif-
icant according to paired bootstrap resampling
test (Koehn, 2004). In the configuration without
TreeLM the improvement is greater (1.33 BLEU)
than with TreeLM (0.81 BLEU), which confirms
our hypothesis that MaxEnt TM captures some of
the contextual dependencies resolved otherwise by
language models.
205
5 Conclusions
We have introduced a maximum entropy transla-
tion model in dependency-based MT which en-
ables exploiting a large number of feature func-
tions in order to obtain more accurate translations.
The BLEU evaluation proved significant improve-
ment over the baseline solution based on the trans-
lation model with maximum likelihood estimates.
However, the performance of this system still be-
low the state of the art (which is around BLEU 16
for the English-to-Czech direction).
Acknowledgments
This research was supported by the grants
MSM0021620838, MS?MT C?R LC536, FP7-ICT-
2009-4-247762 (Faust), FP7-ICT-2007-3-231720
(EuroMatrix Plus), GA201/09/H057, and GAUK
116310. We thank two anonymous reviewers for
helpful comments.
References
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proceedings of CoNLL 2003, pages
148?151.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39?71.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathemati-
cal Linguistics, 92:63?83.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the ACL conference, pages
132?139, San Francisco, USA.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52, Morristown, USA.
Association for Computational Linguistics.
Kevin Gimpel and Noah A. Smith. 2009. Feature-
rich translation by quasi-synchronous lattice pars-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 219?228, Morristown, USA. Association for
Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Abraham Ittycheriah and Salim Roukos. 2007. Direct
translation model 2. In Candace L. Sidner, Tanja
Schultz, Matthew Stone, and ChengXiang Zhai, edi-
tors, HLT-NAACL, pages 57?64. The Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, volume 4, pages 388?395.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of HLT / EMNLP, pages 523?530, Vancouver,
Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295?302.
Kishore A. Papineni, Salim Roukos, and Todd R.
Ward. 1997. Feature-based language understand-
ing. In European Conference on Speech Commu-
nication and Technology (EUROSPEECH), pages
1435?1438, Rhodes, Greece, September.
Martin Popel and Zdene?k Z?abok/rtsky?. 2009.
Improving English-Czech Tectogrammatical MT.
The Prague Bulletin of Mathematical Linguistics,
(92):1?20.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In In Proceedings
of EMNLP?96, pages 133?142.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k Z?abokrtsky? and Martin Popel. 2009. Hidden
markov tree model in dependency-based machine
translation. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 145?148, Sun-
tec, Singapore.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL, pages 167?170.
206
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 426?432,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Two-step translation with grammatical post-processing?
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova? and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{marecek,rosa,galuscakova,bojar}@ufal.mff.cuni.cz
Abstract
This paper describes an experiment in which
we try to automatically correct mistakes in
grammatical agreement in English to Czech
MT outputs. We perform several rule-based
corrections on sentences parsed to dependency
trees. We prove that it is possible to improve
the MT quality of majority of the systems par-
ticipating in WMT shared task. We made both
automatic (BLEU) and manual evaluations.
1 Introduction
This paper is a joint report on two English-to-Czech
submissions to the WMT11 shared translation task.
The main contribution is however the proposal and
evaluation of a rule-based post-processing system
DEPFIX aimed at correcting errors in Czech gram-
mar applicable to any MT system. This is somewhat
the converse of other approaches (e.g. Simard et al
(2007)) where a statistical system was applied for
the post-processing of a rule-based one.
2 Our phrase-based systems
This section briefly describes our underlying phrase-
based systems. One of them (CU-BOJAR) was sub-
mitted directly to the WMT11 manual evaluation,
the other one (CU-TWOSTEP) was first corrected by
the proposed method (Section 3 below) and then
submitted under the name CU-MARECEK.
?This research has been supported by the European Union
Seventh Framework Programme (FP7) under grant agreement
n? 247762 (Faust), n? 231720 (EuroMatrix Plus), and by the
grants GAUK 116310 and GA201/09/H057.
2.1 Data for statistical systems
Our training parallel data consists of CzEng 0.9
(Bojar and Z?abokrtsky?, 2009), the News Commen-
tary corpus v.6 as released by the WMT11 orga-
nizers, the EMEA corpus, a corpus collected from
the transcripts of TED talks (http://www.ted.com),
the parallel news and separately some of the par-
allel web pages of the European Commission
(http://ec.europa.eu), and the Official Journal of the
European Union as released by the Apertium con-
sortium (http://apertium.eu/data).
A custom web crawler was used for the European
Commission website. English and Czech websites
were matched according to their URLs. Unfortu-
nately, Czech websites very often contain untrans-
lated parts of English texts. Because of this, we
aimed especially at the news articles, which are very
often translated correctly and also more relevant for
the shared task. Texts were segmented using train-
able tokenizer (Klyueva and Bojar, 2008) and dedu-
plicated. Processed texts were automatically aligned
by Hunalign (Varga and others, 2005).
The data from the Official Journal were first con-
verted from XML to plain text. The documents were
paired according to their filenames. To better han-
dle the nature of these data, we decided to divide
the documents into two classes based on the aver-
age number of words per sentence: ?lists? are docu-
ments with less than 2.8 words per sentence, other
documents are called ?texts?. The corresponding
?lists? were aligned line by line. The corresponding
?texts? were automatically segmented by trainable
tokenizer and aligned automatically by Hunalign.
We use the following two Czech language mod-
426
els, their weights are optimized in MERT:
? 5-gram LM from the Czech side of CzEng (ex-
cluding the Navajo section). The LM was con-
structed by interpolating LMs of the individual do-
mains (news, EU legislation, technical documenta-
tion, etc.) to achieve the lowest perplexity on the
WMT08 news test set.
? 6-gram LM from the monolingual data supplied by
WMT11 organizers (news of the individual years
and News Commentary), the Czech National Cor-
pus and a web collection of Czech texts. Again, the
final LM is constructed by interpolating the smaller
LMs1 for the WMT08 news test set.
2.2 Baseline Moses (CU-BOJAR)
The system denoted CU-BOJAR for English-to-
Czech is simple phrase-based translation, i.e. Moses
without factors. We tokenized, lemmatized and
tagged all texts using the tools wrapped in TectoMT
(Popel and Z?abokrtsky?, 2010). We further tokenize
e.g. dashed words (?23-year?) after all the process-
ing is finished. Phrase-based MT is then able to
handle such expressions both at once, or decompose
them as needed to cover unseen variations. We use
lexicalized reordering (orientation-bidirectional-fe).
The translation runs in ?supervised truecase?, which
means that we use the output of our lemmatizers
to decide whether the word should be lowercased
or should preserve uppercasing. After the transla-
tion, the first letter in the output is simply upper-
cased. The model is optimized using Moses? stan-
dard MERT on the WMT09 test set.
The organizers of WMT11 encouraged partici-
pants to apply simple normalization to their data
(both for training and testing).2 The main purpose
of the normalization is to improve the consistency of
typographical rules. Unfortunately, some of the au-
tomatic changes may accidentally damage the mean-
ing of the expression.3 We therefore opted to submit
1The interpolated LM file (gzipped ARPA format) is 5.1 GB
so we applied LM pruning as implemented in SRI toolkit with
the threshold 10?14 to reduce the file size to 2.3 GB.
2http://www.statmt.org/wmt11/normalize-punctuation.perl
3Fixing the ordering of the full stop and the quote is wrong
because the order (at least in Czech typesetting) depends on
whether it is the full sentence or a final phrase that is captured
in the quotes. Even riskier are rules handling decimal and thou-
sand separators in numbers. While there are language-specific
conventions, they are not always followed and the normaliza-
tion can in such cases confuse the order of magnitude by 3.
the output based on non-normalized test sets as our
primary English-to-Czech submission.
We invested much less effort into the submission
called CU-BOJAR for Czech-to-English. The only
interesting feature there is the use of alternative de-
coding paths to translate either from the Czech form
or from the Czech lemma equipped with meaning-
bearing morphological properties, e.g. the number
of nouns. Bojar and Kos (2010) used the same setup
with simple lemmas in the fallback decoding path.
The enriched lemmas perform marginally better.
2.3 Two-step translation
Our two-step translation is essentially the same
setup as detailed by Bojar and Kos (2010): (1)
the English source is translated to simplified Czech,
and (2) the simplified Czech is monotonically trans-
lated to fully inflected Czech. Both steps are sim-
ple phrase-based models. Instead of word forms, the
simplified Czech uses lemmas enriched by a sub-
set of morphological features selected manually to
encode only properties overt both in English and
Czech such as the tense of verbs or number of nouns.
Czech-specific morphological properties indicating
various agreements (e.g. number and gender of ad-
jectives, gender of verbs) are imposed in the second
step solely on the basis of the language model.
The first step uses the same parallel and mono-
lingual corpora as CU-BOJAR, except the LMs being
trained on the enriched lemmas, not on word forms.
The second step uses exactly the same LM as CU-
BOJAR but the phrase-table is extracted from all our
Czech monolingual data (phrase length limit of 1.)
3 Grammatical post-processing
Phrase-based machine translation systems often
have problems with grammatical agreement, espe-
cially on longer dependencies. Sometimes, there is
a mistake in agreement even between adjacent words
because each one belongs to a different phrase. The
goal of our post-processing is to correct forms of
some words so that they do not violate grammatical
rules (eg. grammatical agreement).
The problem is how to find the correct syntactic
relations in the output of an MT system. Parsers
trained on correct sentences can rely on grammat-
ical agreement, according to which they determine
427
the dependencies between words. Unfortunately, the
agreement in MT outputs is often wrong and the
parser fails to produce a correct parse tree. There-
fore, we would need a parser trained on a manually
annotated treebank consisting of specific outputs of
machine translation systems. Such a treebank does
not exist and we do not even want to create one, be-
cause the MT systems are changing constantly and
also because manual annotation of texts that are of-
ten not even understandable would be almost a su-
perhuman task.
The DEPFIX system was implemented in TectoMT
framework (Popel and Z?abokrtsky?, 2010). MT out-
puts were tagged by Morc?e tagger (Spoustova? et al,
2007) and then parsed with MST parser (McDon-
ald et al, 2005) that was trained on the Prague De-
pendency Treebank (Hajic? and others, 2006), i.e.
on correct Czech sentences. We used an improved
implementation with some additional features es-
pecially tuned for Czech (Nova?k and Z?abokrtsky?,
2007). The parser accuracy is much lower on the
?noisy? MT output sentences, but a lot of dependen-
cies in which we are to correct grammatical agree-
ment are determined correctly. Adapting the parser
for outputs of MT systems will be addressed in the
coming months.
A typical example of a correction is the agreement
between the subject and the predicate: they should
share the morphological number and gender. If they
do not, we simply change the number and gender
of the predicate in agreement with the subject.4 An
example of such a changed predicate is in Figure 1.
Apart from the dependency tree of the target sen-
tence, we can also use the dependency tree of the
source sentence. Source sentences are grammat-
ically correct and the accuracy of the tagger and
the parser is accordingly higher there. Words in
the source and target sentences are aligned using
GIZA++5 (Och and Ney, 2003) but verbose outputs
of the original MT systems would be possibly a bet-
ter option. The rules for fixing grammatical agree-
ment between words can thus consider also the de-
pendency relations and morphological caregories of
their English counterparts in the input sentence.
4In this case, we suppose that the number of the subject has
a much higher chance to be correct.
5GIZA++ was run on lemmatized texts in both directions
and intersection symmetrization was used.
Some
people
came
later
Atr
Sb
Pred
Advplpl
.AuxK
p?i?liPredpl
N?kte??
lid?
p?i?el
pozd?ji
Atr
Sb
Pred
Advsg, mpl
.AuxK
Figure 1: Example of fixing subject-predicate agreement.
The Czech word pr?is?el [he came] has a wrong morpho-
logical number and gender.
3.1 Grammatical rules
We have manually devised a set of the following
rules. Their input is the dependency tree of a Czech
sentence (MT output) and its English source sen-
tence (MT input) with the nodes aligned where pos-
sible. Each of the rules fires if the specified con-
ditions (?IF?) are matched, executes the command
(?DO?) , usually changing one or more morphologi-
cal categories of the word, and generates a new word
form for any word which was changed.
The rules make use of several morphological cat-
egories of the word (node:number, node:gender...),
its syntactic relation to its parent in the dependency
tree (node:afun) and the same information for its
English counterpart (node:en) and other nodes in
the dependency trees.
The order of the rules in this paper follows the
order in which they are applied; this is important, as
often a rule changes a morphological category of a
word which is then used by a subsequent rule.
3.1.1 Noun number (NounNum)
In Czech, a word in singular sometimes has the
same form as in plural. Because the tagger often
fails to tag the word correctly, we try to correct the
tag of a noun tagged as singular if its English coun-
terpart is in plural, so that the subsequent rules can
work correctly.
We trust the form of the word but changing the
number may also require to change the morphologi-
cal case (i.e. the tagger was wrong with both number
and case). In such cases we choose the first (linearly
428
from nominative to instrumentative) case matching
the form. The rule is:
IF: node:pos = noun &
node:number = singular &
node:en:number = plural
DO: node:number := plural;
node:case := find case(node:form, plural);
3.1.2 Subject case (SubjCase)
The subject of a Czech sentence must be in the
nominative case. Since the parser often fails in
marking the correct word as a subject, we use the
English source sentence and presuppose that the
Czech counterpart of the English subject is also a
subject in the Czech sentence.
IF: node:en:afun = subject
DO: node:case := nominative;
3.1.3 Subject-predicate agreement (SubjPred)
Subject and predicate in Czech agree in their mor-
phological number. To identify a Czech Subject, we
trust the subject in the English sentence. Then we
copy the number from the (Czech) Subject to the
Czech Predicate.
IF: node:en:afun = subject &
parent:afun = predicate
DO: parent:number := node:number;
3.1.4 Subject-past participle agreement (SubjPP)
Czech past participles agree with subject in
morphological gender.
IF: node:pos = noun|pronoun &
node:en:afun = subject &
parent:pos = verb past participle
DO: parent:number := node:number;
parent:gender := node:gender;
3.1.5 Preposition without children (PrepNoCh)
In our dependency trees, the preposition is the
parent of the words it belongs to (usually a noun). A
preposition without children is incorrect so we find
nodes aligned to its English counterpart?s children
and rehang them under the preposition.
IF: node:afun = preposition &
!node:has children &
node:en:has children
DO: foreach node:en:child;
node:en:child:cs:parent := node;
3.1.6 Preposition-noun agreement (PrepNoun)
Every prepositions gets a morphological case as-
signed to it by the tagger, with which the dependent
noun should agree.
IF: parent:pos = preposition &
node:pos = noun
DO: node:case := parent:case;
3.1.7 Noun-adjective agreement (NounAdj)
Czech adjectives and nouns agree in morpholog-
ical gender, number and case. We assume that the
noun is correct and change the adjective accordingly.
IF: node:pos = adjective &
parent:pos = noun
DO: node:gender := parent:gender;
node:number := parent:number;
node:case := parent:case;
3.1.8 Reflexive particle deletion (ReflTant)
Czech reflexive verbs are accompanied by reflex-
ive particles (?se? and ?si?). We delete particles not
beloning to any verb (or adjective derived from a
verb).
IF: node:form = ?se?|?si? &
node:pos = pronoun &
parent:pos != verb|verbal adjective
DO: remove node;
4 Experiments and results
We tested our CU-TWOSTEP system with DEPFIX
post-processing on both WMT10 and WMT11 test-
ing data. This combined system was submitted to
shared translation task as CU-MARECEK. We also
ran the DEPFIX post-processing on all other partici-
pating systems.
4.1 Automatic evaluation
The achieved BLEU scores are shown in Tables 1
and 2. They show the scores before and after the
DEPFIX post-processing. It is interesting that the
improvements are quite different between the years
2010 and 2011 in terms of their BLEU score. While
the average improvement on WMT10 test set was
0.21 BLEU points, it was only 0.05 BLEU points on
the WMT11 test set. Even the results of the same
TWOSTEP system differ in a similar way, so it must
have been caused by the different data.
429
system before after improvement
cu-twostep 15.98 16.13 0.15 (0.05 - 0.26)
cmu-heaf. 16.95 17.04 0.09 (-0.01 - 0.20)
cu-bojar 15.85 16.09 0.24 (0.14 - 0.36)
cu-zeman 12.33 12.55 0.22 (0.12 - 0.32)
dcu 13.36 13.59 0.23 (0.13 - 0.37)
dcu-combo 18.79 18.90 0.11 (0.02 - 0.23)
eurotrans 10.10 10.11 0.01 (-0.04 - 0.07)
koc 11.74 11.91 0.17 (0.08 - 0.26)
koc-combo 16.60 16.86 0.26 (0.16 - 0.37)
onlineA 11.81 12.08 0.27 (0.17 - 0.38)
onlineB 16.57 16.79 0.22 (0.11 - 0.33)
potsdam 12.34 12.57 0.23 (0.14 - 0.35)
rwth-combo 17.54 17.79 0.25 (0.15 - 0.35)
sfu 11.43 11.83 0.40 (0.29 - 0.52)
uedin 15.91 16.19 0.28 (0.18 - 0.40)
upv-combo 17.51 17.73 0.22 (0.10 - 0.34)
Table 1: Depfix improvements on the WMT10 systems
in BLEU score. Confidence intervals, which were com-
puted on 1000 bootstrap samples, are in brackets.
system before after improvement
cu-twostep 16.57 16.60 0.03 (-0.07 - 0.13)
cmu-heaf. 20.24 20.32 0.08 (-0.03 - 0.19)
commerc2 09.32 09.32 0.00 (-0.04 - 0.04)
cu-bojar 16.88 16.85 -0.03 (-0.12 - 0.07)
cu-popel 14.12 14.11 -0.01 (-0.06 - 0.03)
cu-tamch. 16.32 16.28 -0.04 (-0.14 - 0.06)
cu-zeman 14.61 14.80 0.19 (0.09 - 0.29)
jhu 17.36 17.42 0.06 (-0.03 - 0.16)
online-B 20.26 20.31 0.05 (-0.06 - 0.16)
udein 17.80 17.88 0.08 (-0.02 - 0.17)
upv-prhlt. 20.68 20.69 0.01 (-0.08 - 0.11)
Table 2: Depfix improvements on the WMT11 systems
in BLEU score. Confidence intervals are in brackets.
4.2 Manual evaluation
Two independent annotators evaluated DEPFIX man-
ually on the outputs of CU-TWOSTEP and ONLINE-
B. We randomly selected 1000 sentences from the
newssyscombtest2011 data set and the appropri-
ate translations made by these two systems. The
annotators got the outputs before and after DEPFIX
post-processing and their task was to decide which
translation6 from these two is better and label it by
the letter ?a?. If it was not possible to determine
6They were also provided with the source English sentence
and the reference translation. The options were shuffled and
indentical candidate sentences were collapsed.
A / B improved worsened indefinite total
improved 273 20 15 308
worsened 12 59 7 78
indefinite 53 35 42 130
total 338 114 64 516
Table 5: Matrix of the inter-annotator agreement
rule fired impr. wors. % impr.
SubjCase 51 46 5 90.2
SubjPP 193 165 28 85.5
NounAdj 434 354 80 81.6
NounNum 156 122 34 78.2
PrepNoun 135 99 36 73.3
SubjPred 68 48 20 70.6
ReflTant 15 10 5 66.7
PrepNoCh 45 29 16 64.4
Table 6: Rules and their utility.
which is better, they labeled both by ?n?.
Table 3 below shows that about 60% of sentences
fixed by DEPFIX were improved and only about 20%
were worsened. DEPFIX worked a little better on the
ONLINE-B, making fewer changes but also fewer
wrong changes. It is probably connected with the
fact that overall better translations by ONLINE-B are
easier to parse.
The matrix of inter-annotator agreement is in Ta-
ble 5. Our two annotators agreed in 374 sentences
(out of 516), that is 72.5%. On the other hand, if
we consider only cases where both annotators chose
different translation as better (no indefinite marks),
we get only 8.8% disagreement (32 out of 364).
Using the manual evaluation, we can also measure
performance of the individual rules. Table 6 shows
the number of all, improved or worsened sentences
where a particular rule was applied. Definitely, the
most useful rule (used often and quite reliable) was
the one correcting noun-adjective agreement, fol-
lowed by the subject-pastparticiple agreement rule.
In each changed sentence, two rules (not neces-
sarily related ones) were applied on average.
4.3 Manual evaluation across data sets
The fact that the improvements in BLEU scores on
WMT10 test set are much higher has led us to one
more experiment: we compare manual annotations
of 330 sentences from each of the WMT10 and
430
system annotator changed improved worsened indefinite
count % count % count %
cu-bojar-twostep A 269 152 56.5 39 14.5 78 29.0
cu-bojar-twostep B 269 173 64.3 50 18.6 46 17.1
online-B A 247 156 63.1 39 15.9 52 21.1
online-B B 247 165 66.8 64 25.9 18 7.3
Table 3: Manual evaluation of the DEPFIX post-processing on 1000 randomly chosen sentences from WMT11 test set.
test set changed improved worsened indefinite BLEU
count % count % count % before after diff
newssyscombtest2010 104 52 50.0 20 19.2 32 30.8 16.99 17.38 0.39
newssyscombtest2011 101 66 65.3 19 18.8 16 15.8 13.99 13.87 -0.12
Table 4: Manual and automatic evaluation of the DEPFIX post-processing on CU-TWOSTEP system across different
datasets. 330 sentences were randomly selected from each of the WMT10 and WMT11 test sets. Both manual scores
and BLEU are computed only on the sentences that were changed by the DEPFIX post-processing.
WMT11 sets as translated by CU-TWOSTEP and cor-
rected by DEPFIX. Table 4 shows that WMT10 and
WMT11 are comparable in manually estimated im-
provement (50?65%). BLEU does not indicate that
and even estimates a drop in quality on this subset
WMT11. (The absolute BLEU scores differ from
BLEUs on the whole test sets but we are interested
only in the change of the scores.) BLEU is thus not
very suitable for the evaluation of DEPFIX.
5 Conclusions and future work
Manual evaluation shows that our DEPFIX approach
to improving MT output quality is sensible. Al-
though it is unable to correct many serious MT er-
rors, such as wrong lexical choices, it can improve
the grammaticality of the output in a way that the
language model often cannot, which leads to out-
put that is considered to be better by humans. We
also suggest that BLEU is not appropriate metric
for measuring changes in grammatical correctness
of sentences, especially with inflective languages.
An advantage of our method is that it is possible
to apply it on output of any MT system (although it
works better for phrase-based MT systems). While
DEPFIX has been developed using the output of CU-
BOJAR, the rules we devised are not specific to any
MT system. They simply describe several grammat-
ical rules of Czech language that can be machine-
checked and if errors are found, the output can be
corrected. Moreover, our method only requires the
source sentence and the translation output for its op-
eration ? i.e. it is not necessary to modify the MT
system itself.
We are now considering modifications of the
parser so that it is able to parse the incorrect sen-
tences produced by MT. Theoretically it would be
possible to train the parser on annotated ungrammat-
ical sentences, but we do not want to invest such an-
notation labour. Instead, when parsing the Czech
sentence we will make the parser utilize the infor-
mation contained in the parse tree of the English
sentence, which is usually correct. We will proba-
bly also have to make the parser put less weight to
the often incorrect tagger output. An alternative is
to avoid parsing of the target and project the source
parse to the target side using word alignments, if
provided by the MT system.
Because some of our rules are able to work using
only the tagger output, we will also try to apply them
before the parsing as they might help the parser by
correcting some of the tags.
We will also try several modifications of the tag-
ger, but the English sentence does not help us so
much here, because it does not contain any infor-
mation regarding the most common errors ? in-
correct assignment of morphological gender and
case. However, it could help with part of speech
and morphological number disambiguation. More-
over, it would be probably helpful for us if the tag-
ger included several most probable hypotheses, as
the single-output-only disambiguation is often erro-
neous on ungrammatical sentences.
431
References
Ondrej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T0 1, Philadelphia.
Natalia Klyueva and Ondr?ej Bojar. 2008. UMC 0.1:
Czech-Russian-English Multilingual Corpus. In Pro-
ceedings of International Conference Corpus Linguis-
tics, pages 188?195, Saint-Petersburg.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, edi-
tors, Lecture Notes in Artificial Intelligence, Proceed-
ings of the 10th I nternational Conference on Text,
Speech and Dialogue, Lecture Notes in Computer Sci-
ence, pages 92?98, Pilsen, Czech Republic. Springer
Science+Business Media Deutschland GmbH.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007.
Statistical phrase-based post-editing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 508?515, Rochester, New York, April.
Association for Computational Linguistics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Da?niel Varga et al 2005. Parallel corpora for medium
density languages. In Proceedings of the Recent Ad-
vances in Natural Language Processing, pages 590?
596, Borovets, Bulgaria.
432
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 433?439,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Influence of Parser Choice on Dependency-Based MT
Martin Popel, David Marec?ek, Nathan Green and Zdene?k Z?abokrtsky?
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{popel,marecek,green,zabokrtsky}@ufal.mff.cuni.cz
Abstract
Accuracy of dependency parsers is one of the
key factors limiting the quality of dependency-
based machine translation. This paper deals
with the influence of various dependency pars-
ing approaches (and also different training
data size) on the overall performance of an
English-to-Czech dependency-based statisti-
cal translation system implemented in the
Treex framework. We also study the relation-
ship between parsing accuracy in terms of un-
labeled attachment score and machine transla-
tion quality in terms of BLEU.
1 Introduction
In the last years, statistical n-gram models domi-
nated the field of Machine Translation (MT). How-
ever, their results are still far from perfect. Therefore
we believe it makes sense to investigate alternative
statistical approaches. This paper is focused on an
analysis-transfer-synthesis translation system called
TectoMT whose transfer representation has a shape
of a deep-syntactic dependency tree. The system has
been introduced by Z?abokrtsky? et al (2008). The
translation direction under consideration is English-
to-Czech.
It has been shown by Popel (2009) that the current
accuracy of the dependency parser employed in this
translation system is one of the limiting factors from
the viewpoint of its output quality. In other words,
the parsing phase is responsible for a large portion
of translation errors. The biggest source of trans-
lation errors in the referred study was (and prob-
ably still is) the transfer phase, however the pro-
portion has changed since and the relative impor-
tance of the parsing phase has grown, because the
tranfer phase errors have already been addressed by
improvements based on Hidden Markov Tree Mod-
els for lexical and syntactic choice as shown by
Z?abokrtsky? and Popel (2009), and by context sensi-
tive translation models based on maximum entropy
as described by Marec?ek et al (2010).
Our study proceeds along two directions. First,
we train two state-of-the-art dependency parsers on
training sets with varying size. Second, we use
five parsers based on different parsing techniques.
In both cases we document the relation between
parsing accuracy (in terms of Unlabeled Attachment
Score, UAS) and translation quality (estimated by
the well known BLEU metric).
The motivation behind the first set of experiments
is that we can extrapolate the learning curve and try
to predict how new advances in dependency parsing
can affect MT quality in the future.
The second experiment series is motivated by
the hypothesis that parsers based on different ap-
proaches are likely to have a different distribution
of errors, even if they can have competitive perfor-
mance in parsing accuracy. In dependency parsing
metrics, all types of incorrect edges typically have
the same weight,1 but some incorrect edges can be
more harmful than others from the MT viewpoint.
For instance, an incorrect attachment of an adverbial
node is usually harmless, while incorrect attachment
of a subject node might have several negative conse-
1This issue has been tackled already in the parsing literature;
for example, some authors disregard placement of punctuation
nodes within trees in the evaluation (Zeman, 2004).
433
quences such as:
? unrecognized finiteness of the governing verb,
which can lead to a wrong syntactization on the
target side (an infinitive verb phrase instead of
a finite clause),
? wrong choice of the target-side verb form (be-
cause of unrecognized subject-predicate agree-
ment),
? missing punctuation (because of wrongly rec-
ognized finite clause boundaries),
? wrong placement of clitics (because of wrongly
recognized finite clause boundaries),
? wrong form of pronouns (personal and posses-
sive pronouns referring to the clause?s subject
should have reflexive forms in Czech).
Thus it is obvious that the parser choice is im-
portant and that it might not be enough to choose a
parser, for machine translation, only according to its
UAS.
Due to growing popularity of dependency syntax
in the last years, there are a number of dependency
parsers available. The present paper deals with
five parsers evaluated within the translation frame-
work: three genuine dependency parsers, namely the
parsers described in (McDonald et al, 2005), (Nivre
et al, 2007), and (Zhang and Nivre, 2011), and two
constituency parsers (Charniak and Johnson, 2005)
and (Klein and Manning, 2003), whose outputs were
converted to dependency structures by Penn Con-
verter (Johansson and Nugues, 2007).
As for the related literature, there is no published
study measuring the influence of dependency parsers
on dependency-based MT to our knowledge.2
The remainder of this paper is structured as fol-
lows. The overall translation pipeline, within which
the parsers are tested, is described in Section 2. Sec-
tion 3 lists the parsers under consideration and their
main features. Section 4 summarizes the influence
of the selected parsers on the MT quality in terms of
BLEU. Section 5 concludes.
2However, the parser bottleneck of the dependency-based
MT approach was observed also by other researchers (Robert
Moore, personal communication).
2 Dependency-based Translation in Treex
We have implemented our experiments in the Treex
software framework (formerly TectoMT, introduced
by Z?abokrtsky? et al (2008)), which already offers
tool chains for analysis and synthesis of Czech and
English sentences.
We use the tectogrammatical (deep-syntactic)
layer of language representation as the transfer layer
in the presented MT experiments. Tectogrammat-
ics was introduced by Sgall (1967) and further
elaborated within the Prague Dependency Treebank
project (Hajic? et al, 2006). On this layer, each
sentence is represented as a tectogrammatical tree,
whose main properties (from the MT viewpoint) are
the following:
1. nodes represent autosemantic words,
2. edges represent semantic dependencies (a node
is an argument or a modifier of its parent),
3. there are no functional words (prepositions,
auxiliary words) in the tree, and the autose-
mantic words appear only in their base forms
(lemmas). Morphologically indispensable cat-
egories (such as number with nouns or tense
with verbs, but not number with verbs as it is
only imposed by agreement) are stored in sep-
arate node attributes (grammatemes).
The intuitions behind the decision to use tec-
togrammatics for MT are the following: we be-
lieve that (1) tectogrammatics largely abstracts from
language-specific means (inflection, agglutination,
functional words etc.) of expressing non-lexical
meanings and thus tectogrammatical trees are sup-
posed to be highly similar across languages, (2)
it enables a natural transfer factorization,3 (3) and
local tree contexts in tectogrammatical trees carry
more information (especially for lexical choice) than
local linear contexts in the original sentences.
The translation scenario is outlined in the rest of
this section.
3Morphological categories can be translated almost inde-
pendently from lemmas, which makes parallel training data
?denser?, especially when translating from/to a language with
rich inflection such as Czech.
434
2.1 Analysis
The input English text is segmented into sentences
and tokens. The tokens are lemmatized and tagged
with Penn Treebank tags using the Morce tagger
(Spoustova? et al, 2007). Then one of the studied
dependency parsers is applied and a surface-syntax
dependency tree (analytical tree in the PDT termi-
nology) is created for each sentence.
This tree is converted to a tectogrammatical tree.
Each autosemantic word with its associated func-
tional words is collapsed into a single tectogram-
matical node, labeled with a lemma, formeme,4 and
semantically indispensable morphologically cate-
gories; coreference is also resolved.
2.2 Transfer
The transfer phase follows, whose most difficult part
consists especially in labeling the tree with target-
side lemmas and formemes. There are also other
types of changes, such as node addition and dele-
tion. However, as shown by Popel (2009), changes
of tree topology are required relatively infrequently
due to the language abstractions on the tectogram-
matical layer.
Currently, translation models based on Maxi-
mum Entropy classifiers are used both for lemmas
and formemes (Marec?ek et al, 2010). Tree label-
ing is optimized using Hidden Tree Markov Mod-
els (Z?abokrtsky? and Popel, 2009), which makes
use of target-language dependency tree probabilistic
model.
All models used in the transfer phase are trained
using training sections of the Czech-English parallel
corpus CzEng 0.9 (Bojar and Z?abokrtsky?, 2009).
2.3 Synthesis
Finally, surface sentence shape is synthesized from
the tectogrammatical tree, which is basically the
reverse operation of the tectogrammatical analy-
sis. It consists of adding punctuation and functional
4Formeme captures the morphosyntactic means which are
used for expressing the tectogrammatical node in the surface
sentence shape. Examples of formeme values: v:that+fin ?
finite verb in a subordinated clause introduced with conjunction
that, n:sb ? semantic noun in a subject position, n:for+X ?
semantic noun in a prepositional group introduced with prepo-
sition for, adj:attr ? semantic adjective in an attributive po-
sition.
words, spreading morphological categories accord-
ing to grammatical agreement, performing inflection
(using Czech morphology database (Hajic?, 2004)),
arranging word order etc.
The difference from the analysis phase is that
there is not very much space for optimization in the
synthesis phase. In other words, final sentence shape
is determined almost uniquely by the tectogrammat-
ical tree (enriched with formemes) resulting from
the transfer phase. However, if there are not enough
constraints for a unique choice of a surface form of
a lemma, then a unigram language model is used for
the final decision. The model was trained using 500
million words from the Czech National Corpus.5
3 Involved Parsers
We performed experiments with parsers from
three families: graph-based parsers, transition-
based parsers, and phrase-structure parsers (with
constituency-to-dependency postprocessing).
3.1 Graph-based Parser
In graph-based parsing, we learn a model for scoring
graph edges, and we search for the highest-scoring
tree composed of the graph?s edges. We used Max-
imum Spanning Tree parser (Mcdonald and Pereira,
2006) which is capable of incorporating second or-
der features (MST for short).
3.2 Transition-based Parsers
Transition-based parsers utilize the shift-reduce al-
gorithm. Input words are put into a queue and
consumed by shift-reduce actions, while the out-
put parser is gradually built. Unlike graph-based
parsers, transition-based parsers have linear time
complexity and allow straightforward application of
non-local features.
We included two transition-based parsers into our
experiments:
? Malt ? Malt parser introduced by Nivre et al
(2007) 6
5http://ucnk.ff.cuni.cz
6We used stackeager algorithm, liblinear learner, and
the enriched feature set for English (the same configu-
ration as in pretrained English models downloadable at
http://maltparser.org.
435
? ZPar ? Zpar parser7 which is basically an al-
ternative implementation of the Malt parser,
employing a richer set of non-local features as
described by Zhang and Nivre (2011).
3.3 CFG-based Tree Parsers
Another option how to obtain dependency trees is
to apply a constituency parser, recognize heads in
the resulting phrase structures and apply a recur-
sive algorithm for converting phrase-structure trees
into constituency trees (the convertibility of the two
types of syntactic structures was studied already by
Gaifman (1965)).
We used two constituency parsers:
? Stanford ? The Stanford parser (Klein and
Manning, 2003),8
? CJ ? a MaxEnt-based parser combined with
discriminative reranking (Charniak and John-
son, 2005).9
Before applying the parsers on the text, the system
removes all spaces within tokens. For instance U. S.
becomes U.S. to restrict the parsers from creating
two new tokens. Tokenization built into both parsers
is bypassed and the default tokenization in Treex is
used.
After parsing, Penn Converter introduced by Jo-
hansson and Nugues (2007) is applied, with the
-conll2007 option, to change the constituent
structure output, of the two parsers, into CoNLL de-
pendency structure. This allows us to keep the for-
mats consistent with the output of both MST and
MaltParser within the Treex framework.
There is an implemented procedure for cre-
ating tectogrammatical trees from the English
phrase structure trees described by Kuc?erova? and
Z?abokrtsky? (2002). Using the procedure is more
straightforward, as it does not go through the
CoNLL-style trees; English CoNLL-style trees dif-
fer slightly from the PDT conventions (e.g. in at-
taching auxiliary verbs) and thus needs additional
7http://sourceforge.net/projects/zpar/ (version 0.4)
8Only the constituent, phrase based, parsed output is used in
these experiments.
9We are using the default settings from the August 2006 ver-
sion of the software.
postprocessing for our purposes. However, we de-
cided to stick to Penn Converter, so that the similar-
ity of the translation scenarios is maximized for all
parsers.
3.4 Common Preprocessing: Shallow Sentence
Chunking
According to our experience, many dependency
parsers have troubles with analyzing sentences that
contain parenthesed or quoted phrases, especially if
they are long.
We use the assumption that in most cases the con-
tent of parentheses or quotes should correspond to
a connected subgraph (subtree) of the syntactic tree.
We implemented a very shallow sentence chunker
(SentChunk) which recognizes parenthesed word
sequences. These sequences can be passed to a
parser first, and be parsed independently of the rest
of the sentence. This was shown to improve not only
parsing accuracy of the parenthesed word sequence
(which is forced to remain in one subtree), but also
the rest of the sentence.10
In our experiments, SentChunk is used only
in combination with the three genuine dependency
parsers.
4 Experiments and Evaluation
4.1 Data for Parsers? Training and Evaluation
The dependency trees needed for training the parsers
and evaluating their UAS were created from the
Penn Treebank data (enriched first with internal
noun phrase structure applied via scripts provided
by Vadas and Curran (2007)) by Penn Converter (Jo-
hansson and Nugues, 2007) with the -conll2007
option (PennConv for short).
All the parsers were evaluated on the same data ?
section 23.
All the parsers were trained on sections 02?21,
except for the Stanford parser which was trained
on sections 01?21. We were able to retrain the
parser models only for MST and Malt. For the
other parsers we used pretrained models available on
the Internet: CJ?s default model ec50spfinal,
Stanford?s wsjPCFG.ser.gz model, and
10Edge length is a common feature in dependency parsers, so
?deleting? parenthesed words may give higher scores to correct
dependency links that happened to span over the parentheses.
436
ZPar?s english.tar.gz. The model of ZPar
is trained on data converted to dependencies using
Penn2Malt tool,11 which selects the last member of
a coordination as the head. To be able to compare
ZPar?s output with the other parsers, we postpro-
cessed it by a simple ConjAsHead code that con-
verts this style of coordinations to the one used in
CoNLL2007, where the conjuction is the head.
4.2 Reference Translations Used for Evaluation
Translation experiments were evaluated using refer-
ence translations from the new-dev2009 data set,
provided by the organizors of shared translation task
with the Workshop on Statistical Machine Transla-
tion.
4.3 Influence of Parser Training Data Size
We trained a sequence of parser models for MST and
Malt, using a roughly exponentially growing se-
quence of Penn Treebank subsets. The subsets are
contiguous and start from the beginning of section
02. The results are collected in Tables 1 and 2.12
#tokens UAS BLEU NIST
100 0.362 0.0579 3.6375
300 0.509 0.0859 4.3853
1000 0.591 0.0995 4.6548
3000 0.623 0.1054 4.7972
10000 0.680 0.1130 4.9695
30000 0.719 0.1215 5.0705
100000 0.749 0.1232 5.1193
300000 0.776 0.1257 5.1571
990180 0.793 0.1280 5.1915
Table 1: The effect of training data size on parsing accu-
racy and on translation performance with MST.
The trend of the relation between the training data
size and BLEU is visible also in Figure 1. It is ob-
vious that increasing the training data has a positive
effect on the translation quality. However, the pace
of growth of BLEU is sublogarithmic, and becomes
unconvincing above 100,000 training tokens. It in-
dicates that given one of the two parsers integrated
11http://w3.msi.vxu.se/?nivre/research/
Penn2Malt.html
12To our knowledge, the best system participating in the
shared task reaches BLEU 17.8 for this translation direction.
#tokens UAS BLEU NIST
100 0.454 0.0763 4.0555
300 0.518 0.0932 4.4698
1000 0.591 0.1042 4.6769
3000 0.616 0.1068 4.7472
10000 0.665 0.1140 4.9100
30000 0.695 0.1176 4.9744
100000 0.723 0.1226 5.0504
300000 0.740 0.1238 5.1005
990180 0.759 0.1253 5.1296
Table 2: The effect of training data size on parsing accu-
racy and on translation performance with Malt.
 
0.05
 
0.06
 
0.07
 
0.08
 
0.09 0.1
 
0.11
 
0.12
 
0.13
 
100
 
100
0
 
100
00
 
100
000
 
1e+
06
BLEU
train
ing 
toke
ns
MS
T Mal
t
Figure 1: The effect of parser training data size of BLEU
with Malt and MST parsers.
into our translation framework, increasing the parser
training data alone would probably not lead to a sub-
stantial improvement of the translation performance.
4.4 Influence of Parser Choice
Table 3 summarizes our experiments with the five
parsers integrated into the tectogrammatical transla-
tion pipeline. Two configurations (with and without
SentChunk) are listed for the genuine dependency
parsers. The relationship between UAS and BLEU
for (the best configurations of) all five parsers is de-
picted also in Figure 2.
Additionally, we used paired bootstrap 95% con-
fidence interval testing (Zhang et al, 2004), to check
which BLEU differences are significant. For the
five compared parser (with SentChunk if appli-
cable), only four comparisons are not significant:
MST-CJ, MST-Stanford, Malt-Stanford,
and CJ-Stanford.
437
Parser Training data Preprocessing Postprocessing UAS BLEU NIST TER
MST PennTB + PennConv SentChunk ? 0.793 0.1280 5.192 0.735
MST PennTB + PennConv ? ? 0.794 0.1236 5.149 0.739
Malt PennTB + PennConv SentChunk ? 0.760 0.1253 5.130 0.740
Malt PennTB + PennConv ? ? 0.761 0.1214 5.088 0.744
Zpar PennTB + Penn2Malt SentChunk ConjAsHead 0.793 0.1176 5.039 0.749
Zpar PennTB + Penn2Malt ? ConjAsHead 0.792 0.1127 4.984 0.754
CJ PennTB ? PennConv 0.904 0.1284 5.189 0.737
Stanford PennTB ? PennConv 0.825 0.1277 5.137 0.740
Table 3: Dependency parsers tested in the translation pipeline.
 
0.1
 
0.10
5
 
0.11
 
0.11
5
 
0.12
 
0.12
5
 
0.13
 
0.13
5
 
0.14
 
0.14
5
 
0.15
 
0.74
 
0.76
 
0.78
 
0.8
 
0.82
 
0.84
 
0.86
 
0.88
 
0.9
 
0.92
BLEU
UAS
MS
T Mal
t
Zpa
r
Sta
nfor
d CJ
Figure 2: Unlabeled Attachment Score versus BLEU.
Even if BLEU grows relatively smoothly with
UAS for different parsing models of the same parser,
one can see that there is no obvious relation be-
tween UAS and BLEU accross all parsers. MST and
Zpar have the same UAS but quite different BLEU,
whereas MST and CJ have very similar BLEU but
distant UAS. It confirms the original hypothesis that
it is not only the overall UAS, but also the parser-
specific distribution of errors what matters.
4.5 Influence of Shallow Sentence Chunking
Table 3 confirms that parsing the contents paren-
theses separately from the rest of the sentence
(SentChunk) has a positive effect with all three
dependency parsers. Surprisingly, even if the effect
on UAS is negligible, the improvement is almost
half of BLEU point which is significant for all the
three parsers.
4.6 Discussion on Result Comparability
We tried to isolate the effects of the properties of
selected parsers, however, the separation from other
influencing factors is not perfect due to several tech-
nical issues:
? So far, we were not able to retrain the models
for all parsers ourselves and therefore their pre-
trained models (one of them based on slightly
different Penn Treebank division) must have
been used.
? Some parsers make their own choice of POS
tags within the parsed sentences, while other
parsers require the sentences to be tagged al-
ready on their input.
? The trees in the CzEng 0.9 parallel treebank
were created using MST. CzEng 0.9 was used
for training translation models used in the
transfer phase of the translation scenario; thus
these translation models might compensate for
some MST?s errors, which might handicap other
parsers. So far we were not able to reparse 8
million sentence pairs in CzEng 0.9 by all stud-
ied parsers.
5 Conclusions
This paper is a study of how the choice of a de-
pendency parsing technique influences the quality of
English-Czech dependency-based translation. Our
main observations are the following. First, BLEU
grows with the increasing amount of training depen-
dency trees, but only in a sublogarithmic pace. Sec-
ond, what seems to be quite effective for translation
438
is to facilitate the parsers? task by dividing the sen-
tences into smaller chunks using parenthesis bound-
aries. Third, if the parsers are based on different
approaches, their UAS does not correlate well with
their effect on the translation quality.
Acknowledgments
This research was supported by the
grants MSM0021620838, GAUK 116310,
GA201/09/H057, and by the European Com-
mission?s 7th Framework Program (FP7) under
grant agreements n? 238405 (CLARA), n? 247762
(FAUST), and n? 231720 (EuroMatrix Plus).
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng
0.9, Building a Large Czech-English Automatic Par-
allel Treebank. The Prague Bulletin of Mathematical
Linguistics, 92:63?83.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of
Association for Computational Linguistics, ACL ?05,
pages 173?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Haim Gaifman. 1965. Dependency systems and phrase-
structure systems. Information and Control, pages
304?337.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia.
Jan Hajic?. 2004. Disambiguation of Rich Inflection ?
Computational Morphology of Czech. Charles Uni-
versity ? The Karolinum Press, Prague.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105?112,
Tartu, Estonia, May 25-26.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of Association for Computational Lin-
guistics, pages 423?430.
Ivona Kuc?erova? and Zdene?k Z?abokrtsky?. 2002. Trans-
forming Penn Treebank Phrase Trees into (Praguian)
Tectogrammatical Dependency Trees. The Prague
Bulletin of Mathematical Linguistics, (78):77?94.
David Marec?ek, Martin Popel, and Zdene?k Z?abokrtsky?.
2010. Maximum entropy translation model in
dependency-based MT framework. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 201?201, Uppsala,
Sweden. Association for Computational Linguistics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
HLT / EMNLP, pages 523?530, Vancouver, Canada.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Martin Popel. 2009. Ways to Improve the Quality of
English-Czech Machine Translation. Master?s thesis,
Institute of Formal and Applied Linguistics, Charles
University, Prague, Czech Republic.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska? dek-
linace. Academia, Prague.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The Best of Two
Worlds: Cooperation of Statistical and Rule-Based
Taggers for Czech. In Proceedings of the Workshop
on Balto-Slavonic Natural Language Processing, ACL
2007, pages 67?74, Praha.
David Vadas and James Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Zdene?k Z?abokrtsky? and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 145?148, Suntec, Sin-
gapore.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceedings
of the 3rd Workshop on Statistical Machine Transla-
tion, ACL, pages 167?170.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph.D. thesis, Faculty of Mathematics
and Physics, Charles University in Prague.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In To
appear in the Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores: How much improvement
do we need to have a better system. In Proceedings of
LREC, volume 4, pages 2051?2054.
439
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267?274,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Formemes in English-Czech Deep Syntactic MT ?
Ondr?ej Du?ek, Zdene?k ?abokrtsk?, Martin Popel,
Martin Majli?, Michal Nov?k, and David Marec?ek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?me?st? 25, Prague
{odusek,zabokrtsky,popel,majlis,mnovak,marecek}@ufal.mff.cuni.cz
Abstract
One of the most notable recent improve-
ments of the TectoMT English-to-Czech trans-
lation is a systematic and theoretically sup-
ported revision of formemes?the annotation
of morpho-syntactic features of content words
in deep dependency syntactic structures based
on the Prague tectogrammatics theory. Our
modifications aim at reducing data sparsity,
increasing consistency across languages and
widening the usage area of this markup.
Formemes can be used not only in MT, but in
various other NLP tasks.
1 Introduction
The cornerstone of the TectoMT tree-to-tree ma-
chine translation system is the deep-syntactic lan-
guage representation following the Prague tec-
togrammatics theory (Sgall et al, 1986), and its ap-
plication in the Prague Dependency Treebank (PDT)
2.01 (Hajic? et al, 2006), where each sentence is
analyzed to a dependency tree whose nodes corre-
spond to content words. Each node has a number
of attributes, but the most important (and difficult)
for the transfer phase are lemma?lexical informa-
tion, and formeme?surface morpho-syntactic infor-
? This research has been supported by the grants
FP7-ICT-2009-4-247762 (FAUST), FP7-ICT-2009-4-249119
(Metanet), LH12093 (Kontakt II), DF12P01OVV022 (NAKI),
201/09/H057 (Czech Science Foundation), GAUK 116310, and
SVV 265 314. This work has been using language resources de-
veloped and/or stored and/or distributed by the LINDAT-Clarin
project of the Ministry of Education of the Czech Republic
(project LM2010013).
1http://ufal.mff.cuni.cz/pdt2.0
mation, including selected auxiliary words (Pt?c?ek
and ?abokrtsk?, 2006; ?abokrtsk? et al, 2008).
This paper focuses on formemes?their definition
and recent improvements of the annotation, which
has been thoroughly revised in the course of prepa-
ration of the CzEng 1.0 parallel corpus (Bojar et al,
2012b), whose utilization in TectoMT along with the
new formemes version has brought the greatest ben-
efit to our English-Czech MT system in the recent
year. However, the area of possible application of
formemes is not limited to MT only or to the lan-
guage pair used in our system; the underlying ideas
are language-independent.
We summarize the development of morpho-
syntactic annotations related to formemes (Sec-
tion 2), provide an overview of the whole TectoMT
system (Section 3), then describe the formeme an-
notation (Section 4) and our recent improvements
(Section 5), as well as experimental applications, in-
cluding English-Czech MT (Section 6). The main
asset of the formeme revision is a first systematic re-
organization of the existing practical aid, providing
it with a solid theoretical base, but still bearing its
intended applications in mind.
2 Related Work
Numerous theoretical approaches had been made
to morpho-syntactic description, mainly within va-
lency lexicons, starting probably with the work by
Helbig and Schenkel (1969). Perhaps the best one
for Czech is PDT-VALLEX (Hajic? et al, 2003), list-
ing all possible subtrees corresponding to valency
arguments (Ure?ov?, 2009). ?abokrtsk? (2005)
gives an overview of works in this field.
267
This kind of information has been most exploited
in structural MT systems, employing semantic re-
lations (Menezes and Richardson, 2001) or surface
tree substructures (Quirk et al, 2005; Marcu et al,
2006). Formemes, originally developed for Natural
Language Generation (NLG) (Pt?c?ek and ?abokrt-
sk?, 2006), have been successfully applied to MT
within the TectoMT system. Our revision of for-
meme annotation aims to improve the MT perfor-
mance, keeping other possible applications in mind.
3 The TectoMT English-Czech Machine
Translation System
The TectoMT system is a structural machine trans-
lation system with deep transfer, first introduced
by ?abokrtsk? et al (2008). It currently supports
English-to-Czech translation. Its analysis stage
follows the Prague tectogrammatics theory (Sgall,
1967; Sgall et al, 1986), proceeding over two layers
of structural description, from shallow (analytical)
to deep (tectogrammatical) (see Section 3.1).
The transfer phase of the system is based on Max-
imum Entropy context-sensitive translation models
(Marec?ek et al, 2010) and Hidden Tree Markov
Models (?abokrtsk? and Popel, 2009). It is factor-
ized into three subtasks: lemma, formeme and gram-
matemes translation (see Sections 3.2 and 3.3).
The subsequent generation phase consists of rule-
based components that gradually change the deep
target language representation into a shallow one,
which is then converted to text (cf. Section 6.1).
The version of TectoMT submitted to WMT122
builds upon the WMT11 version. Several rule-based
components were slightly refined. However, most of
the effort was devoted to creating a better and bigger
parallel treebank?CzEng 1.03 (Bojar et al, 2012b),
and re-training the statistical components on this re-
source. Apart from bigger size and improved filter-
ing, one of the main differences between CzEng 0.9
(Bojar and ?abokrtsk?, 2009) (used in WMT11) and
CzEng 1.0 (used in WMT12) is the revised annota-
tion of formemes.
2http://www.statmt.org/wmt12
3http://ufal.mff.cuni.cz/czeng
3.1 Layers of structural analysis
There are two distinct structural layers used in the
TectoMT system:
? Analytical layer. A surface syntax layer, which
includes all tokens of the sentence, organized
into a labeled dependency tree. The labels cor-
respond to surface syntax functions.
? Tectogrammatical layer. A deep syntax/se-
mantic layer describing the linguistic meaning
of the sentence. Its dependency trees include
only content words as nodes, assigning to each
of them a deep lemma (t-lemma), a semantic
role label (functor), and other deep linguistic
features (grammatemes), such as semantic part-
of-speech, person, tense or modality.
The analytical layer can be obtained using differ-
ent dependency parsers (Popel et al, 2011); the tec-
togrammatical representation is then created by rule-
based modules from the analytical trees.
In contrast to the original PDT annotation,
the TectoMT tectogrammatical layer also includes
formemes describing the surface morpho-syntactic
realization of the nodes (cf. also Section 3.3).
3.2 Transfer: Translation Factorization and
Symmetry
Using the tectogrammatical representation in struc-
tural MT allows separating the problem of translat-
ing a sentence into relatively independent simpler
subtasks: lemma, functors, and grammatemes trans-
lation (Bojar et al, 2009; ?abokrtsk?, 2010). Since
topology changes to deep syntax trees are rare in MT
transfer, each of these three subtasks allows a vir-
tually symmetric source-target one-to-one mapping,
thus simplifying the initial n-to-m mapping of word
phrases or surface subtrees.
?abokrtsk? et al (2008) obviated the need for
transfer via functors (i.e. semantic role detection)
by applying a formeme transfer instead. While
formeme values are much simpler to obtain by au-
tomatic processing, this approach preserved the ad-
vantage of symmetric one-to-one value translation.
Moreover, translations of a given source morpho-
syntactic construction usually follow a limited num-
ber of patterns in the target language regardless of
268
their semantic functions, e.g. a finite clause will
most often be translated as a finite clause.
3.3 Motivation for the Introduction of
Formemes
Surface-oriented formemes have been introduced
into the semantics-oriented tectogrammatical layer,
as it proves beneficial to combine the deep syntax
trees, smaller in size and more consistent across lan-
guages, with the surface morphology and syntax to
provide for a straightforward transition to the surface
level (?abokrtsk?, 2010).
The three-fold factorization of the transfer phase
(see Section 3.2) helps address the data sparsity is-
sue faced by today?s MT systems. As the translation
of lemmas and their morpho-syntactic forms is sepa-
rated, combinations unseen in the training data may
appear on the output.
To further reduce data sparsity, only minimal in-
formation needed to reconstruct the surface form is
stored in formemes; morphological categories deriv-
able from elsewhere, i.e. morphological agreement
or grammatemes, are discarded.
4 Czech and English Formemes in
TectoMT
A formeme is a concise description of relevant
morpho-syntactic features of a node in a tectogram-
matical tree (deep syntactic tree whose nodes usu-
ally correspond to content words). The general
shape of revised Czech and English formemes, as
implemented within the Treex4 NLP framework
(Popel and ?abokrtsk?, 2010) for the TectoMT sys-
tem, consists of three main parts:
1. Syntactic part-of-speech.5 The number of syn-
tactic parts-of-speech is very low, as only con-
tent words are used on the deep layer and the
categories of pronouns and numerals have been
divided under nouns and adjectives accord-
ing to syntactic behavior (?evc??kov?-Raz?mov?
and ?abokrtsk?, 2006). The possible values are
v for verbs, n for nouns, adj for adjectives,
and adv for adverbs.
4http://ufal.mff.cuni.cz/treex/,
https://metacpan.org/module/Treex
5Cf. Section 5.2 for details.
2. Subordinate conjunction/preposition. Applies
only to formemes of prepositional phrases and
subordinate clauses introduced by a conjunc-
tion and contains the respective conjunction or
preposition; e.g. if, on or in_case_of.
3. Form. This part represents the morpho-
syntactic form of the node in question and de-
pends on the part-of-speech (see Table 1).
The two or three parts are concatenated into
a human-readable string to facilitate usage in
hand-written rules as well as statistical systems
(?abokrtsk?, 2010), producing values such as
v:inf, v:if+fin or n:into+X. Formeme val-
ues of nodes corresponding to uninflected words are
atomic.
Formemes are detected by rule-based modules op-
erating on deep and surface trees. Example deep
syntax trees annotated with formemes are shown in
Fig. 1. A listing of all possible formeme values is
given in Table 1.
Verbal formemes remain quite consistent in both
languages, except for the greater range of forms in
English (Czech uses adjectives or nouns instead of
gerunds and verbal attributes). Nominal formemes
differ more significantly: Czech is a free-word order
language with rich morphology, where declension
is important to syntactic relations?case is therefore
included in formemes. As English makes its syntac-
tic relations visible rather with word-order than with
morphology, English formemes indicate the syntac-
tic position instead. The same holds for adjecti-
val complements to verbs. Posession is expressed
mostly using nouns in English and adjectives in
Czech, which is also reflected in formemes.
5 Recent Markup Improvements
Our following markup innovations address several
issues found in the previous version and aim to adapt
the range of values more accurately to the intended
applications.
5.1 General Form Changes
The relevant preposition and subordinate conjunc-
tion nodes had been selected based on their depen-
dency labels; we use a simple part-of-speech tag fil-
ter instead in order to minimize the influence of pars-
ing errors and capture more complex prepositions,
269
Figure 1: An example English and Czech deep sentence structure annotated with formemes (in typewriter font).
Formeme Language Definition
v:(P+)fin both Verbs as heads of finite clauses
v:rc both Verbs as heads of relative clauses
v:(P+)inf both Infinitive clauses; typically with the particle to in English?
v:(P+)ger EN Gerunds, e.g. I like reading (v:ger), but I am tired of arguing (v:of+ger).
v:attr EN Present or past participles (i.e. -ing or -ed forms) in the attributive syntactic
position, e.g. Striking (v:attr) teachers hate bored (v:attr) students.
n:[1..7] CS Bare nouns; the numbers indicate morphological case?
n:X CS Bare nouns that cannot be inflected
n:subj EN Nouns in the subject position (i.e. in front of the main verb of the clause)
n:obj EN Nouns in the object position (i.e. following the verb with no preposition)
n:obj1, n:obj2 EN Nouns in the object position; distinguishing the two objects of ditransitive
verbs (e.g. give, consider)
n:adv EN Nouns in an adverbial position, e.g. The sales went up by 1 % last month
n:P+X EN Prepositional phrases
n:P+[1..7] CS Prepositional phrases; the preposition surface form is combined with the re-
quired case?
n:attr both Nominal attributes, e.g. insurance company or president Smith in English
and prezident Smith in Czech
n:poss EN English possessive pronouns and nouns with the ?s suffix
adj:attr both Adjectival attributes (Czech inflection forms need not be stored thanks to
congruency with the parent noun)
adj:compl EN Direct adjectival complements to verbs
adj:[1..7] CS Direct adjectival complements to verbs (morphological case must be stored
in Czech, as it is determined by valency)
adj:poss CS Czech possesive adjectives and pronouns; a counterpart to English n:poss
adv both Adverbs (not inflected, can take no prepositions etc.)
x both Coordinating conjunctions, other uninflected words
drop both Deep tree nodes which do not appear on the surface (e.g. pro-drop pronouns)
?I.e. infinitives as head of clauses, not infinitives as parts of compound verb forms with finite auxiliary verbs.
?Numbers are traditionally used to mark morphological case in Czech; 1 stands for nominative, 2 for genitive etc.
?Since many prepositions may govern multiple cases in Czech, the case number is necessary.
Table 1: A listing of all possible formeme values, indicating their usage in Czech, English or both languages. ?P+?
denotes the (lowercased) surface form of a preposition or a subordinate conjunction. Round brackets denote optional
parts, square brackets denote a set of alternatives.
270
e.g. in case of. Our revision also allows combining
prepositions with all English gerunds and infinitives,
preventing a loss of important data.
We also use the lowercased surface form in the
middle formeme part instead of lemmas to allow for
a more straightforward surface form generation.
5.2 Introducing Syntactic Part-of-Speech
Formemes originally contained the semantic part-of-
speech (sempos) (Raz?mov? and ?abokrtsk?, 2006)
as their first part. We replaced it with a syntac-
tic part-of-speech (syntpos), since it proved compli-
cated to assign sempos reliably by a rule-based mod-
ule and morpho-syntactic behavior is more relevant
to formemes than semantics.
The syntpos is assigned in two steps:
1. A preliminary syntpos is selected, using our
categorization based on the part-of-speech tag
and lemma.
2. The final syntpos is selected according to the
syntactic position of the node, addressing nom-
inal usage of adjectives and cardinal numerals
(see Sections 5.4 and 5.5).
5.3 Capturing Czech Nominal Attributes
Detecting the attributive usage of nouns is straight-
forward for English, where any noun depending di-
rectly on another noun is considered an attribute.
In Czech, one needs to distinguish case-congruent
attributes from others that have a fixed case. We
aimed at assigning the n:attr formeme only in the
former case and thus replaced the original method
based on word order with a less error-prone one
based on congruency and named entity recognition.
5.4 Numerals: Distinguishing Usage and
Correcting Czech Case
The new formemes now distinguish adjectival and
nominal usage of cardinal numerals (cf. also Sec-
tion 5.2), e.g. the number in 5 potatoes is now as-
signed the adj:attr formeme, whereas Apollo 11
is given n:attr. The new situation is analogous
in Czech, with nominal usages of numerals having
their morphological case marked in formemes.
To reduce data sparsity in the new formemes ver-
sion, we counter the inconsistent syntactic behavior
of Czech cardinal numerals, where 1-4 behave like
The word ban?n is in genitive (n:2), but would have an ac-
cusative (n:4) form if the numeral behaved like an adjective.
Figure 2: Case correction with numerals in Czech.
adjectives but other numerals behave like nouns and
shift their semantically governing noun to the po-
sition of a genitive attribute. An example of this
change is given in Fig. 2.
5.5 Adjectives: Nominal Usage and Case
The new formemes address the usage of adjectives
in the syntactic position of nouns (cf. Section 5.2),
which occurs only rarely, thus preventing sparse val-
ues, namely in these syntactic positions:
? The subject. We replaced the originally as-
signed adj:compl value, which was impos-
sible to tell from adjectival objects, with the
formeme a noun would have in the same po-
sition, e.g. in the sentence Many of them were
late, the subject many is assigned n:subj.
? Prepositional phrases. Syntactic behavior of
adjectives is identical to nouns here; we thus
assign them the formeme values a noun would
receive in the same position, e.g. n:of+X in-
stead of adj:of+X in He is one of the best at
school.
In Czech, we detect nominal usage of adjectives
in verbal direct objects as well, employing large-
coverage valency lexicons (Lopatkov? et al, 2008;
Hajic? et al, 2003).
Instead of assigning the compl value in Czech,
our formemes revision includes the case of adjecti-
val complements, which depends on the valency of
the respective verb.
5.6 Mutual Information Across Languages
The changes described above have been motivated
not only by theoretical linguistic description of the
languages in question, but also by the intended us-
age within the TectoMT translation system. Instead
271
of retraining the translation model after each change,
we devised a simpler and faster estimate to measure
the asset of our innovations: using Mutual Informa-
tion (MI) (Manning and Sch?tze, 1999, p. 66) of
formemes in Czech and English trees.
We expect that an inter-language MI increase will
lead to lower noise in formeme-to-formeme transla-
tion dictionary (Bojar et al, 2009, cf. Section 3.2),
thus achieving higher MT output quality.
Using the analysis pipeline from CzEng1.0, we
measured the inter-language MI on sentences from
the Prague Czech-English Dependency Treebank
(PCEDT) 2.0 (Bojar et al, 2012a). The overall re-
sults show an MI increase from 1.598 to 1.687 (Bo-
jar et al, 2012b). Several proposed markup changes
have been discarded as they led to an inter-language
MI drop; e.g. removing the v:rc relative clause
formeme or merging the v:attr and adj:attr
values in English.
6 Experimental Usage
We list here our experiments with the newly de-
veloped annotation: an NLG experiment aimed at
assessing the impact of formemes on the synthesis
phase of the TectoMT system, and the usage in the
English-Czech MT as a whole.
6.1 Czech Synthesis
The synthesis phase of the TectoMT system relies
heavily on the information included in formemes, as
its rule-based blocks use solely formemes and gram-
mar rules to gradually change a deep tree node into
a surface subtree.
To directly measure the suitability of our changes
for the synthesis stage of the TectoMT system, we
used a Czech-to-Czech round trip?deep analysis of
Czech PDT 2.0 development set sentences using the
CzEng 1.0 pipeline (Bojar et al, 2012b), followed
directly by the synthesis part of the TectoMT sys-
tem. The results were evaluated using the BLEU
metric (Papineni et al, 2002) with the original sen-
tences as reference; they indicate a higher suitability
of the new formemes for deep Czech synthesis (see
Table 2).
6.2 English-Czech Machine Translation
To measure the influence of the presented formeme
revision on the translation quality, we compared
Version BLEU
Original formemes 0.6818
Revised formemes 0.7092
Table 2: A comparison of formeme versions in Czech-to-
Czech round trip.
Version BLEU
Original formemes 0.1190
Revised formemes 0.1199
Table 3: A comparison of formeme versions in English-
to-Czech TectoMT translation on the WMT12 test set.
two translation scenarios?one using the origi-
nal formemes and the second using the revised
formemes in the formeme-to-formeme translation
model. Due to time reasons, we were able to
train both translation models only on 1/2 of the
CzEng 1.0 training data.
The results in Table 3 demonstrate a slight6 BLEU
gain when using the revised formemes version. The
gain is expected to be greater if several rule-based
modules of the transfer phase are adapted to the re-
visions.
7 Conclusion and Further Work
We have presented a systematic and theoretically
supported revision of a surface morpho-syntactic
markup within a deep dependency annotation sce-
nario, designed to facilitate the TectoMT transfer
phase. Our first practical experiments proved the
merits of our innovations in the tasks of Czech syn-
thesis and deep structural MT as a whole. We have
also experimented with formemes in the functor as-
signment (semantic role labelling) task and gained
moderate improvements (ca. 1-1.5% accuracy).
In future, we intend to tune the rule-based parts
of our MT transfer for the new version of formemes
and examine further possibilities of data sparsity re-
duction (e.g. by merging synonymous formemes).
We are also planning to create formeme annotation
modules for further languages to widen the range of
language pairs used in the TectoMT system.
6Significant at 90% level using pairwise bootstrap resam-
pling test (Koehn, 2004).
272
References
O. Bojar and Z. ?abokrtsk?. 2009. CzEng 0.9: Large
Parallel Treebank with Rich Annotation. Prague Bul-
letin of Mathematical Linguistics, 92.
O. Bojar, D. Marec?ek, V. Nov?k, M. Popel, J. Pt?c?ek,
J. Rou?, and Z. ?abokrtsk?. 2009. English-Czech MT
in 2008. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 125?129. As-
sociation for Computational Linguistics.
O. Bojar, J. Hajic?, E. Hajic?ov?, J. Panevov?, P. Sgall,
S. Cinkov?, E. Fuc??kov?, M. Mikulov?, P. Pajas,
J. Popelka, J. Semeck?, J. ?indlerov?, J. ?te?p?nek,
J. Toman, Z. Ure?ov?, and Z. ?abokrtsk?. 2012a.
Announcing Prague Czech-English Dependency Tree-
bank 2.0. In Proceedings of LREC 2012, Istanbul,
Turkey, May. ELRA, European Language Resources
Association. In print.
O. Bojar, Z. ?abokrtsk?, O. Du?ek, P. Galu?c??kov?,
M. Majli?, D. Marec?ek, J. Mar??k, M. Nov?k,
M. Popel, and A. Tamchyna. 2012b. The Joy of Par-
allelism with CzEng 1.0. In Proceedings of LREC
2012, Istanbul, Turkey, May. ELRA, European Lan-
guage Resources Association. In print.
J. Hajic?, J. Panevov?, Z. Ure?ov?, A. B?mov?,
V. Kol?rov?, and P. Pajas. 2003. PDT-VALLEX: Cre-
ating a large-coverage valency lexicon for treebank an-
notation. In Proceedings of The Second Workshop on
Treebanks and Linguistic Theories, volume 9, pages
57?68.
J. Hajic?, J. Panevov?, E. Hajic?ov?, P. Sgall, P. Pajas,
J. ?te?p?nek, J. Havelka, M. Mikulov?, Z. ?abokrtsk?,
and M. ?evc??kov?-Raz?mov?. 2006. Prague Depen-
dency Treebank 2.0. CD-ROM LDC2006T01, LDC,
Philadelphia.
G. Helbig and W. Schenkel. 1969. W?rterbuch zur
Valenz und Distribution deutscher Verben. VEB Bib-
liographisches Institut, Leipzig.
P. Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
M. Lopatkov?, Z. ?abokrtsk?, V. Kettnerov?, and
K. Skwarska. 2008. Valenc?n? slovn?k c?esk?ch sloves.
Karolinum, Prague.
C.D. Manning and H. Sch?tze. 1999. Foundations of
statistical natural language processing. MIT Press.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
SPMT: Statistical machine translation with syntacti-
fied target language phrases. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing, pages 44?52. Association for
Computational Linguistics.
D. Marec?ek, M. Popel, and Z. ?abokrtsk?. 2010. Maxi-
mum entropy translation model in dependency-based
MT framework. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and Met-
rics (MATR), pages 201?206. Association for Compu-
tational Linguistics.
A. Menezes and S. D. Richardson. 2001. A best-first
alignment algorithm for automatic extraction of trans-
fer mappings from bilingual corpora. In Proceed-
ings of the workshop on Data-driven methods in ma-
chine translation - Volume 14, DMMT ?01, pages 1?8,
Stroudsburg, PA. Association for Computational Lin-
guistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
M. Popel and Z. ?abokrtsk?. 2010. TectoMT: modular
NLP framework. Advances in Natural Language Pro-
cessing, pages 293?304.
M. Popel, D. Marec?ek, N. Green, and Z. ?abokrtsk?.
2011. Influence of parser choice on dependency-based
MT. In Chris Callison-Burch, Philipp Koehn, Christof
Monz, and Omar Zaidan, editors, Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 433?439, Edinburgh, UK. Association for Com-
putational Linguistics.
J. Pt?c?ek and Z. ?abokrtsk?. 2006. Synthesis of
Czech sentences from tectogrammatical trees. In Text,
Speech and Dialogue, pages 221?228. Springer.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 271?279. Association for Computational
Linguistics.
M. Raz?mov? and Z. ?abokrtsk?. 2006. Annotation
of grammatemes in the Prague Dependency Treebank
2.0. In Proceedings of the LREC 2006 Workshop on
Annotation Science, pages 12?19.
M. ?evc??kov?-Raz?mov? and Z. ?abokrtsk?. 2006. Sys-
tematic parameterized description of pro-forms in the
Prague Dependency Treebank 2.0. In J. Hajic? and
J. Nivre, editors, Proceedings of the Fifth Workshop on
Treebanks and Linguistic Theories (TLT), pages 175?
186, Prague.
P. Sgall, E. Hajic?ov?, J. Panevov?, and J. Mey. 1986. The
meaning of the sentence in its semantic and pragmatic
aspects. Springer.
P. Sgall. 1967. Generativn? popis jazyka a c?esk? dekli-
nace. Academia, Prague.
Z. Ure?ov?. 2009. Building the PDT-VALLEX valency
lexicon. In On-line proceedings of the fifth Corpus
Linguistics Conference. University of Liverpool.
273
Z. ?abokrtsk?, J. Pt?c?ek, and P. Pajas. 2008. Tec-
toMT: highly modular MT system with tectogrammat-
ics used as transfer layer. In Proceedings of the Third
Workshop on Statistical Machine Translation, StatMT
?08, pages 167?170, Stroudsburg, PA. Association for
Computational Linguistics.
Z. ?abokrtsk?. 2005. Valency Lexicon of Czech Verbs.
Ph.D. thesis, Charles University in Prague.
Z. ?abokrtsk?. 2010. From Treebanking to Machine
Translation. Habilitation thesis, Charles University in
Prague.
Z. ?abokrtsk? and M. Popel. 2009. Hidden Markov
Tree Model in Dependency-based Machine Transla-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 145?148, Suntec, Singa-
pore.
274
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 362?368,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DEPFIX: A System for Automatic Correction of Czech MT Outputs?
Rudolf Rosa, David Marec?ek and Ondr?ej Dus?ek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{rosa,marecek,odusek}@ufal.mff.cuni.cz
Abstract
We present an improved version of DEPFIX
(Marec?ek et al, 2011), a system for auto-
matic rule-based post-processing of English-
to-Czech MT outputs designed to increase
their fluency. We enhanced the rule set used
by the original DEPFIX system and measured
the performance of the individual rules.
We also modified the dependency parser of
McDonald et al (2005) in two ways to adjust
it for the parsing of MT outputs. We show that
our system is able to improve the quality of the
state-of-the-art MT systems.
1 Introduction
The today?s outputs of Machine Translation (MT)
often contain serious grammatical errors. This
is particularly apparent in statistical MT systems
(SMT), which do not employ structural linguistic
rules. These systems have been dominating the area
in the recent years (Callison-Burch et al, 2011).
Such errors make the translated text less fluent and
may even lead to unintelligibility or misleading
statements. The problem is more evident in lan-
guages with rich morphology, such as Czech, where
morphological agreement is of a relatively high im-
portance for the interpretation of syntactic relations.
The DEPFIX system (Marec?ek et al, 2011) at-
tempts to correct some of the frequent SMT sys-
?This research has been supported by the European Union
Seventh Framework Programme (FP7) under grant agree-
ment n? 247762 (Faust), and by the grants GAUK116310,
GA201/09/H057 (Res-Informatica), and LH12093.
tems? errors in English-to-Czech translations.1 It an-
alyzes the target sentence (the SMT output in Czech
language) using a morphological tagger and a de-
pendency parser and attempts to correct it by apply-
ing several rules which enforce consistency with the
Czech grammar. Most of the rules use the source
sentence (the SMT input in English language) as a
source of information about the sentence structure.
The source sentence is also tagged and parsed, and
word-to-word alignment with the target sentence is
determined.
In this paper, we present DEPFIX 2012, an im-
proved version of the original DEPFIX 2011 system.
It makes use of a new parser, described briefly in
Section 3, which is adapted to handle the generally
ungrammatical target sentences better. We have also
enhanced the set of grammar correction rules, for
which we give a detailed description in Section 4.
Section 5 gives an account of the experiments per-
formed to evaluate the DEPFIX 2012 system and
compare it to DEPFIX 2011. Section 6 then con-
cludes the paper.
2 Related Work
Our approach can be regarded as converse to the
more common way of using an SMT system to auto-
matically post-edit the output of a rule-based transla-
tion system, as described e.g. in (Simard et al, 2007)
or (Lagarda et al, 2009).
The DEPFIX system is implemented in the
1Although we apply the DEPFIX system just to SMT systems
in this paper as it mainly targets the errors induced by this type
of MT systems, it can be applied to virtually any MT system
(Marec?ek et al, 2011).
362
TectoMT/Treex NLP framework (Popel and
Z?abokrtsky?, 2010),2 using the Morc?e tagger (Spous-
tova? et al, 2007) and the MST parser (McDonald
et al, 2005) trained on the CoNLL 2007 Shared
Task English data (Nivre et al, 2007) to analyze the
source sentences. The source and target sentences
are aligned using GIZA++ (Och and Ney, 2003).
3 Parsing
The DEPFIX 2011 system used the MST parser (Mc-
Donald et al, 2005) with an improved feature set
for Czech (Nova?k and Z?abokrtsky?, 2007) trained on
the Prague Dependency Treebank (PDT) 2.0 (Hajic?
and others, 2006) to analyze the target sentences.
DEPFIX 2012 uses a reimplementation of the MST
parser capable of utilizing parallel features from the
source side in the parsing of the target sentence.
The source text is usually grammatical and there-
fore is likely to be analyzed more reliably. The
source structure obtained in this way can then pro-
vide hints for the target parser. We use local features
projected through the GIZA++ word alignment ? i.e.
for each target word, we add features computed over
its aligned source word, if there is one.
To address the differences between the gold stan-
dard training data and SMT outputs, we ?worsen?
the treebank used to train the parser, i.e. introduce
errors similar to those found in target sentences:
The trees retain their correct structure, only the word
forms are modified to resemble SMT output.
We have computed a ?part-of-speech tag er-
ror model? on parallel sentences from the Prague
Czech-English Dependency Treebank (PCEDT) 2.0
(Bojar et al, 2012), comparing the gold standard
Czech translations to the output of an SMT system
(Koehn et al, 2007) and estimating the Maximum
Likelihood probabilities of errors for each part-of-
speech tag. We then applied this error model to the
Czech PCEDT 2.0 sentences and used the resulting
?worsened? treebank to train the parser.
4 Rules
DEPFIX 2012 uses 20 hand-written rules, address-
ing various frequent errors in MT output. Each
rule takes an analyzed target sentence as its in-
put, often together with its analyzed source sen-
2http://ufal.mff.cuni.cz/treex
tence, and attempts to correct any errors found ?
usually by changing morphosyntactic categories of
a word (such as number, gender, case, person and
dependency label) and regenerating the correspond-
ing word form if necessary, more rarely by deleting
superfluous particles or auxiliary words or changing
the target dependency tree structure. However, nei-
ther word order problems nor bad lexical choices are
corrected.
Many rules were already present in DEPFIX 2011.
However, most were modified in DEPFIX 2012 to
achieve better performance (denoted as modified),
and new rules were added (new). Rules not modified
since DEPFIX 2011 are denoted as reused.
The order of rule application is important as there
are dependencies among the rules, e.g. FixPrepo-
sitionNounAgreement (enforcing noun-preposition
congruency) depends on FixPrepositionalCase (fix-
ing incorrectly tagged prepositional case). The rules
are applied in the order listed in Table 2.
4.1 Analysis Fixing Rules
Analysis fixing rules try to detect and rectify tagger
and parser errors. They do not change word forms
and are therefore invisible on the output as such;
however, rules of other types benefit from their cor-
rections.
FixPrepositionalCase (new)
This rule corrects part-of-speech-tag errors in
prepositional phrases. It looks for all words that de-
pend on a preposition and do not match its part-of-
speech tag case. It tries to find and assign a com-
mon morphological case fitting for both the word
form and the preposition. Infrequent preposition-
case combinations are not considered.
FixReflexiveTantum (new)
If the word form ?se? or ?si? is classified as reflex-
ive tantum particle by the parser, but does not be-
long to an actual reflexive tantum verb (or a dever-
bative noun or an adjective), its dependency label is
changed to a different value, based on the context.
FixNounNumber (reused)
If a noun is tagged as singular in target but as plu-
ral in source, the tag is likely to be incorrect. This
rule tries to find a tag that would match both the
363
source number and the target word form, changing
the target case if necessary.
FixPrepositionWithoutChildren (reused)
A target preposition with no child nodes is clearly
an analysis error. This rule tries to find children for
childless prepositions by projecting the children of
the aligned source preposition to the target side.
FixAuxVChildren (new)
Since auxiliary verbs must not have child nodes,
we rehang all their children to the governing full
verb.
4.2 Agreement Fixing Rules
These rules relate to morphological agreement re-
quired by Czech grammar, which they try to enforce
in case it is violated. Czech grammar requires agree-
ment in morphological gender, number, case and
person where applicable.
These rules typically use the source sentence only
for confirmation.
FixRelativePronoun (new)
The Czech word relative pronoun ?ktery?? is as-
signed gender and number identical to the closest
preceding noun or pronoun, if the source analysis
confirms that it depends on this noun/pronoun.
FixSubject (modified)
The subject (if the subject dependency label is
confirmed by the source analysis) will have its case
set to nominative; the number is changed if this leads
to the word form staying unchanged.
FixVerbAuxBeAgreement (modified)
If an auxiliary verb is a child of an infinitive, the
auxiliary verb receives the gender and number of the
subject, which is a child of the infinitive (see also
FixAuxVChildren).
FixSubjectPredicateAgreement (modified)
An active verb form receives the number and per-
son from its subject (whose relation to the verb must
be confirmed by the source).
FixSubjectPastParticipleAgreement (modified)
A past participle verb form receives the number
and gender from its subject (confirmed by the source
analysis).
FixPassiveAuxBeAgreement (modified)
An auxiliary verb ?by?t? (?to be?) depending on a
passive verb form receives its gender and number.
FixPrepositionNounAgreement (modified)
A noun or adjective depending on a preposition
receives its case. The dependency must be con-
firmed in the source.
FixNounAdjectiveAgreement (modified)
An adjective (or an adjective-like pronoun or nu-
meral) preceding its governing noun receives its
gender, number and case.
4.3 Translation Fixing Rules
The following rules detect and correct structures of-
ten mistranslated by SMT systems. They usually de-
pend heavily on the source sentence.
FixBy (new)
English preposition ?by? is translated to Czech us-
ing the instrumental case (if modifying a verb, e.g.
?built by David?: ?postaveno Davidem?) or using the
genitive case (if modifying a noun, e.g. ?songs by
David?: ?p??sne? Davida?).
FixPresentContinuous (modified)
If the source sentence is in a continuous tense (e.g.
?Ondr?ej isn?t experimenting.?), the auxiliary verb ?to
be? must not appear on the output, which is often
the case (e.g. *?Ondr?ej nen?? experimentovat.?). This
rule deletes the auxiliary verb in target and transfers
its morphological categories to the main verb (e.g.
?Ondr?ej neexperimentuje.?).
FixVerbByEnSubject (new)
If the subject of the source sentence is a personal
pronoun, its following morphological categeries are
propagated to the target predicate:
? person
? number (except for ?you?, which does not ex-
hibit number)
? gender (only in case of ?he? or ?she?, which ex-
hibit the natural gender)
FixOf (new)
English preposition ?of? modifying a noun is
translated to Czech using the genitive case (e.g. ?pic-
tures of Rudolf?: ?obra?zky Rudolfa?).
364
FixAuxT (reused)
Reflexive tantum particles ?se? or ?si? not belong-
ing to any verb or adjective are deleted. This situa-
tion usually occurs when the meaning of the source
verb/adjective is lost in translation and only the par-
ticle is produced.
4.4 Other Rules
VocalizePrepos (reused)
Prepositions ?k?, ?s?, ?v?, ?z? are vocalized (i.e.
changed to ?ke?, ?se?, ?ve?, ?ze?) where neces-
sary. The vocalization rules in Czech are similar to
?a?/?an? distinction in English.
FixFirstWordCapitalization (new)
If the first word of source is capitalized and the
first word of target is not, this rule capitalizes it.
5 Experiments and Results
For parameter tuning, we used datasets from the
WMT10 translation task and translations by ON-
LINEB and CU-BOJAR systems.
5.1 Manual Evaluation
Manual evaluation of both DEPFIX 2011 and DEP-
FIX 2012 was performed on the WMT113 test set
translated by ONLINEB. 500 sentences were ran-
domly selected and blind-evaluated by two indepen-
dent annotators, who were presented with outputs of
ONLINEB, DEPFIX 2011 and DEPFIX 2012. (For
246 sentences, at least one of the DEPFIX setups
modified the ONLINEB translation.) They provided
us with a pairwise comparison of the three setups,
with the possibility to mark the sentence as ?indef-
inite? if translations were of equal quality. The re-
sults are given in Table 1.
In Table 2, we use the manual evaluation to mea-
sure the performance of the individual rules in DEP-
FIX 2012. For each rule, we ran DEPFIX 2012 with
this rule disabled and compared the output to the
output of the full DEPFIX 2012. The number of
affected sentences on the whole WMT11 test set,
given as ?changed?, represents the impact of the
rule. The number of affected sentences selected for
manual evaluation is listed as ?evaluated?. Finally,
the annotators? ratings of the ?evaluated? sentences
3http://www.statmt.org/wmt11
A / B
Setup 1 Setup 2
Indefinite
better better
Setup 1 better 55% 1% 11%
Setup 2 better 1% 8% 4%
Indefinite 3% 2% 15%
Table 3: Inter-annotator agreement matrix for ONLINEB
+ DEPFIX 2012 as Setup 1 and ONLINEB as Setup 2.
(suggesting whether the rule improved or worsened
the translation, or whether the result was indefinite)
were counted and divided by the number of anno-
tators to get the average performance of each rule.
Please note that the lower the ?evaluated? number,
the lower the confidence of the results.
The inter-annotator agreement matrix for com-
parison of ONLINEB + DEPFIX 2012 (denoted as
Setup 1) with ONLINEB (Setup 2) is given in Ta-
ble 3. The results for the other two setup pairs were
similar, with the average inter-annotator agreement
being 77%.
5.2 Automatic Evaluation
We also performed several experiments with auto-
matic evaluation using the standard BLEU metric
(Papineni et al, 2002). As the effect of DEPFIX in
terms of BLEU is rather small, the results are not as
confident as the results of manual evaluation.4
In Table 4, we compare the DEPFIX 2011 and
DEPFIX 2012 systems and measure the contribution
of parser adaptation (Section 3) and rule improve-
ments (Section 4). It can be seen that the com-
bined effect of applying both system modifications
is greater than when they are applied alone. The im-
provement of DEPFIX 2012 over ONLINEB without
DEPFIX is statistically significant at 95% confidence
level.
The effect of DEPFIX 2012 on the outputs of some
of the best-scoring SMT systems in the WMT12
Translation Task5 is shown in Table 5. Although
DEPFIX 2012 was tuned only on ONLINEB and CU-
BOJAR system outputs, it improves the BLEU score
of all the best-scoring systems, which suggests that
4As already noted by Marec?ek et al (2011), BLEU seems
not to be very suitable for evaluation of DEPFIX. See (Kos and
Bojar, 2009) for a detailed study of BLEU performance when
applied to evaluation of MT systems with Czech as the target
language.
5http://www.statmt.org/wmt12
365
Setup 1 Setup 2
Differing
Annotator
Setup 1 Setup 2
Indefinite
sentences better better
ONLINEB
ONLINEB 169
A 58% 13% 29%
+ DEPFIX 2011 B 47% 11% 42%
ONLINEB
ONLINEB 234
A 65% 14% 21%
+ DEPFIX 2012 B 59% 11% 30%
ONLINEB ONLINEB
148
A 54% 24% 22%
+ DEPFIX 2012 + DEPFIX 2011 B 56% 22% 22%
Table 1: Manual pairwise comparison on 500 sentences from WMT11 test set processed by ONLINEB, ONLINEB +
DEPFIX 2011 and ONLINEB + DEPFIX 2012. Evaluated by two independent annotators.
Sentences
Rule changed evaluated impr. % wors. % indef. %
FixPrepositionalCase 34 5 3 60 2 40 0 0
FixReflexiveTantum 1 0 ? ? ? ? ? ?
FixNounNumber 80 11 5 45 5 45 1 9
FixPrepositionWithoutChildren 16 6 3 50 3 50 0 0
FixBy 75 13 10.5 81 1 8 1.5 12
FixAuxVChildren 26 6 4.5 75 0 0 1.5 25
FixRelativePronoun 56 8 6 75 2 25 0 0
FixSubject 142 18 13.5 75 3 17 1.5 8
FixVerbAuxBeAgreement 8 2 1 50 1 50 0 0
FixPresentContinuous 30 7 5.5 79 1 14 0.5 7
FixSubjectPredicateAgreement 87 10 5.5 55 1 10 3.5 35
FixSubjectPastParticipleAgreement 396 63 46.5 74 9.5 15 7 11
FixVerbByEnSubject 25 6 5 83 0 0 1 17
FixPassiveAuxBeAgreement 43 8 6 75 0.5 6 1.5 19
FixPrepositionNounAgreement 388 62 40 65 13 21 9 15
FixOf 84 13 11.5 88 0 0 1.5 12
FixNounAdjectiveAgreement 575 108 69.5 64 20 19 18.5 17
FixAuxT 38 7 4 57 1 14 2 29
VocalizePrepos 53 12 6 50 2.5 21 3.5 29
FixFirstWordCapitalization 0 0 ? ? ? ? ? ?
Table 2: Impact and accuracy of individual DEPFIX 2012 rules using manual evaluation on 500 sentences from
WMT11 test set translated by ONLINEB. The number of changed sentences is counted on the whole WMT11 test
set, i.e. 3003 sentences. The numbers of improved, worsened and indefinite translations are averaged over the annota-
tors.
366
DEPFIX setup BLEU
without DEPFIX 19.37
DEPFIX 2011 19.41
DEPFIX 2011 + new parser 19.42
DEPFIX 2011 + new rules 19.48
DEPFIX 2012 19.56
Table 4: Performance of ONLINEB and various DEPFIX
setups on the WMT11 test set.
System BLEU
ONLINEB 16.25
ONLINEB + DEPFIX 2012 16.31
UEDIN 15.54
UEDIN + DEPFIX 2012 15.75
CU-BOJAR 15.41
CU-BOJAR + DEPFIX 2012 15.45
CU-TAMCH-BOJ 15.35
CU-TAMCH-BOJ + DEPFIX 2012 15.39
Table 5: Comparison of BLEU of baseline system output
and corrected system output on WMT12 test set.
it is able to improve the quality of various SMT
systems when applied to their outputs. (The im-
provement on UEDIN is statistically significant at
95% confidence level.) We submitted the ONLINEB
+ DEPFIX 2012 system to the WMT12 Translation
Task as CU-DEPFIX.
6 Conclusion
We have presented two improvements to DEPFIX,
a system of rule-based post-editing of English-to-
Czech Machine Translation outputs proven by man-
ual and automatic evaluation to improve the qual-
ity of the translations produced by state-of-the-art
SMT systems. First, improvements in the existing
rules and implementation of new ones, which can be
regarded as an additive, evolutionary change. Sec-
ond, a modified dependency parser, adjusted to pars-
ing of SMT outputs by training it on a parallel tree-
bank with worsened word forms on the Czech side.
We showed that both changes led to a better perfor-
mance of the new DEPFIX 2012, both individually
and combined.
In future, we are planning to incorporate deeper
analysis, devising rules that would operate on the
deep-syntactic, or tectogrammatical, layer. The
Czech and English tectogrammatical trees are more
similar to each other, which should enable us to ex-
ploit more information from the source sentences.
We also hope to be able to perform more complex
corrections, such as changing the part of speech of a
word when necessary.
Following the success of our modified parser, we
would also like to modify the tagger in a similar way,
since incorrect analyses produced by the tagger of-
ten hinder the correct function of our rules, some-
times leading to a rule worsening the translation in-
stead of improving it.
As observed e.g. by Groves and Schmidtke (2009)
for English-to-German and English-to-French trans-
lations, SMT systems for other language pairs also
tend to produce reoccurring grammatical errors. We
believe that these could be easily detected and cor-
rected in a rule-based way, using an approach similar
to ours.
References
Ondr?ej Bojar, Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?,
Petr Sgall, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing Prague Czech-English Dependency Treebank 2.0.
In Proceedings of LREC 2012, Istanbul, Turkey, May.
ELRA, European Language Resources Association.
In print.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Declan Groves and Dag Schmidtke. 2009. Identification
and analysis of post-editing patterns for MT. Proceed-
ings of MT Summit XII, pages 429?436.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T0 1, Philadelphia.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
367
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of ma-
chine translation metrics for czech as the target lan-
guage. The Prague Bulletin of Mathematical Linguis-
tics, 92(-1):135?148.
Antonio L. Lagarda, Vicent Alabau, Francisco Casacu-
berta, Roberto Silva, and Enrique Diaz-de Liano.
2009. Statistical post-editing of a rule-based ma-
chine translation system. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 217?220. Association for
Computational Linguistics.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
426?432. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), June.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, edi-
tors, Lecture Notes in Artificial Intelligence, Proceed-
ings of the 10th I nternational Conference on Text,
Speech and Dialogue, Lecture Notes in Computer Sci-
ence, pages 92?98, Pilsen, Czech Republic. Springer
Science+Business Media Deutschland GmbH.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007.
Statistical phrase-based post-editing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 508?515, Rochester, New York, April.
Association for Computational Linguistics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
368
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39?48,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Parallel Features in Parsing of Machine-Translated Sentences for
Correction of Grammatical Errors ?
Rudolf Rosa, Ondr?ej Dus?ek, David Marec?ek, and Martin Popel
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{rosa,odusek,marecek,popel}@ufal.mff.cuni.cz
Abstract
In this paper, we present two dependency
parser training methods appropriate for pars-
ing outputs of statistical machine transla-
tion (SMT), which pose problems to standard
parsers due to their frequent ungrammatical-
ity. We adapt the MST parser by exploiting
additional features from the source language,
and by introducing artificial grammatical er-
rors in the parser training data, so that the
training sentences resemble SMT output.
We evaluate the modified parser on DEP-
FIX, a system that improves English-Czech
SMT outputs using automatic rule-based cor-
rections of grammatical mistakes which re-
quires parsed SMT output sentences as its in-
put. Both parser modifications led to im-
provements in BLEU score; their combina-
tion was evaluated manually, showing a sta-
tistically significant improvement of the trans-
lation quality.
1 Introduction
The machine translation (MT) quality is on a steady
rise, with mostly statistical systems (SMT) dominat-
ing the area (Callison-Burch et al, 2010; Callison-
Burch et al, 2011). Most MT systems do not employ
structural linguistic knowledge and even the state-
of-the-art MT solutions are unable to avoid making
serious grammatical errors in the output, which of-
ten leads to unintelligibility or to a risk of misinter-
pretations of the text by a reader.
?This research has been supported by the EU Seventh
Framework Programme under grant agreement n? 247762
(Faust), and by the grants GAUK116310 and GA201/09/H057.
This problem is particularly apparent in target lan-
guages with rich morphological inflection, such as
Czech. As Czech often conveys the relations be-
tween individual words using morphological agree-
ment instead of word order, together with the word
order itself being relatively free, choosing the cor-
rect inflection becomes crucial.
Since the output of phrase-based SMT shows fre-
quent inflection errors (even in adjacent words) due
to each word belonging to a different phrase, a
possible way to address the grammaticality prob-
lem is a combination of statistical and structural ap-
proach, such as SMT output post-editing (Stymne
and Ahrenberg, 2010; Marec?ek et al, 2011).
In this paper, we focus on improving SMT output
parsing quality, as rule-based post-editing systems
rely heavily on the quality of SMT output analy-
sis. Parsers trained on gold standard parse trees of-
ten fail to produce the expected result when applied
to SMT output with grammatical errors. This is
partly caused by the fact that when parsing highly in-
flected free word-order languages the parsers have to
rely on morphological agreement, which, as stated
above, is often erroneous in SMT output.
Training a parser specifically by creating a man-
ually annotated treebank of MT systems? outputs
would be very expensive, and the application of such
treebank to other MT systems than the ones used
for its generation would be problematic. We address
this issue by two methods of increasing the quality
of SMT output parsing:
? a different application of previous works on
bitext parsing ? exploiting additional features
from the source language (Section 3), and
39
? introducing artificial grammatical errors in the
target language parser training data, so that the
sentences resemble the SMT output in some
ways (Section 4). This technique is, to our
knowledge, novel with regards to its applica-
tion to SMT and the statistical error model.
We test these two techniques on English-Czech
MT outputs using our own reimplementation of the
MST parser (McDonald et al, 2005) named RUR1
parser. and evaluate their contribution to the SMT
post-editing quality of the DEPFIX system (Marec?ek
et al, 2011), which we outline in Section 5. We
describe the experiments carried out and present the
most important results in Section 6. Section 7 then
concludes the paper and indicates more possibilities
of further improvements.
2 Related Work
Our approach to parsing with parallel features is
similar to various works which seek to improve the
parsing accuracy on parallel texts (?bitexts?) by us-
ing information from both languages. Huang et
al. (2009) employ ?bilingual constraints? in shift-
reduce parsing to disambiguate difficult syntac-
tic constructions and resolve shift-reduce conflicts.
Chen et al (2010) use similar subtree constraints to
improve parser accuracy in a dependency scenario.
Chen et al (2011) then improve the method by ob-
taining a training parallel treebank via SMT. In re-
cent work, Haulrich (2012) experiments with a setup
very similar to ours: adding alignment-projected
features to an originally monolingual parser.
However, the main aim of all these works is to im-
prove the parsing accuracy on correct parallel texts,
i.e. human-translated. This paper applies similar
methods, but with a different objective in mind ? in-
creasing the ability of the parser to process ungram-
matical SMT output sentences and, ultimately, im-
prove rule-based SMT post-editing.
Xiong et al (2010) use SMT parsing in translation
quality assessment, providing syntactic features to a
classifier detecting erroneous words in SMT output,
yet they do not concentrate on improving parsing ac-
curacy ? they employ a link grammar parser, which
1The abbreviation ?RUR? parser stands for ?Rudolph?s Uni-
versal Robust? parser.
is robust, but not tuned specifically to process un-
grammatical input.
There is also another related direction of research
in parsing of parallel texts, which is targeted on pars-
ing under-resourced languages, e.g. the works by
Hwa et al (2005), Zeman and Resnik (2008), and
McDonald et al (2011). They address the fact that
parsers for the language of interest are of low qual-
ity or even non-existent, whereas there are high-
quality parsers for the other language. They ex-
ploit common properties of both languages and de-
lexicalization. Zhao et al (2009) uses information
from word-by-word translated treebank to obtain ad-
ditional training data and boost parser accuracy.
This is different from our situation, as there ex-
ist high performance parsers for Czech (Buchholz
and Marsi, 2006; Nivre et al, 2007; Hajic? et al,
2009). Boosting accuracy on correct sentences is
not our primary goal and we do not intend to re-
place the Czech parser by an English parser; instead,
we aim to increase the robustness of an already ex-
isting Czech parser by adding knowledge from the
corresponding English source, parsed by an English
parser.
Other works in bilingual parsing aim to parse the
parallel sentences directly using a grammar formal-
ism fit for this purpose, such as Inversion Trans-
duction Grammars (ITG) (Wu, 1997). Burkett et
al. (2010) further include ITG parsing with word-
alignment in a joint scenario. We concentrate here
on using dependency parsers because of tools and
training data availability for the examined language
pair.
Regarding treebank adaptation for parser robust-
ness, Foster et al (2008) introduce various kinds of
artificial errors into the training data to make the fi-
nal parser less sensitive to grammar errors. How-
ever, their approach concentrates on mistakes made
by humans (such as misspellings, word repetition or
omission etc.) and the error models used are hand-
crafted. Our work focuses on morphology errors of-
ten encountered in SMT output and introduces sta-
tistical error modelling.
3 Parsing with Parallel Features
This section describes our SMT output parsing setup
with features from analyzed source sentences. We
40
explain our motivation for the inclusion of parallel
features in Section 3.1, then provide an account of
the parsers used (including our RUR parser) in Sec-
tion 3.2, and finally list all the monolingual and par-
allel features included in the parser training (in Sec-
tions 3.3 and 3.4, respectively).
3.1 Motivation
An advantage of SMT output parsing over general
dependency parsing is that one can also make use of
source ? English sentences in our case. Moreover,
although SMT output is often in many ways ungram-
matical, source is usually grammatical and therefore
easier to process (in our case especially to tag and
parse). This was already noticed in Marec?ek et al
(2011), who use the analysis of source sentence to
provide additional information for the DEPFIX rules,
claiming it to be more reliable than the analysis of
SMT output sentence.
We have carried this idea further by having de-
vised a simple way of making use of this information
in parsing of the SMT output sentences: We parse
the source sentence first and include features com-
puted over the parsed source sentence in the set of
features used for parsing SMT output. We first align
the source and SMT output sentences on the word
level and then use alignment-wise local features ?
i.e. for each SMT output word, we add features com-
puted over its aligned source word, if applicable (cf.
Section 3.4 for a listing).
3.2 Parsers Used
We have reimplemented the MST parser (McDonald
et al, 2005) in order to provide for a simple insertion
of the parallel features into the models.
We also used the original implementation of the
MST parser by McDonald et al (2006) for com-
parison in our experiments. To distinguish the two
variants used, we denote the original MST parser
as MCD parser,2 and the new reimplementation as
RUR parser.
We trained RUR parser in a first-order non-
projective setting with single-best MIRA. Depen-
dency labels are assigned in a second stage by a
2MCD uses k-best MIRA, does first- and second-order
parsing, both projectively and non-projectively, and can be
obtained from http://sourceforge.net/projects/
mstparser.
MIRA-based labeler, which has been implemented
according to McDonald (2006) and Gimpel and Co-
hen (2007).
We used the Prague Czech-English Dependency
Treebank3 (PCEDT) 2.0 (Bojar et al, 2012) as the
training data for RUR parser ? a parallel treebank
created from the Penn Treebank (Marcus et al,
1993) and its translation into Czech by human trans-
lators. The dependency trees on the English side
were converted from the manually annotated phrase-
structure trees in Penn Treebank, the Czech trees
were created automatically using MCD. Words of
the Czech and English sentences were aligned by
GIZA++ (Och and Ney, 2003).
We apply RUR parser only for SMT output pars-
ing; for source parsing, we use MCD parser trained
on the English CoNLL 2007 data (Nivre et al,
2007), as the performance of this parser is sufficient
for this task.
3.3 Monolingual Features
The set of monolingual features used in RUR parser
follows those described by McDonald et al (2005).
For parsing, we use the features described below.
The individual features are computed for both the
parent node and the child node of an edge and con-
joined in various ways. The coarse morphological
tag and lemma are provided by the Morc?e tagger
(Spoustova? et al, 2007).
? coarse morphological tag ? Czech two-letter
coarse morphological tag, as described in
(Collins et al, 1999),4
? lemma ? morphological lemma,
? context features: preceding coarse morpholog-
ical tag, following coarse morphological tag
? coarse morphological tag of a neighboring
node,
? coarse morphological tags in between ? bag of
coarse morphological tags of nodes positioned
between the parent node and the child node,
3http://ufal.mff.cuni.cz/pcedt
4The first letter is the main POS (12 possible values), the
second letter is either the morphological case field if the main
POS displays case (i.e. for nouns, adjectives, pronouns, numer-
als and prepositions; 7 possible values), or the detailed POS if
it does not (22 possible values).
41
? distance ? signed bucketed distance of the par-
ent and the child node in the sentence (in # of
words), using buckets 1, 2, 3, 4, 5 and 11.
To assign dependency labels, we use the same
set as described above, plus the following features
(called ?non-local? by McDonald (2006)), which
make use of the knowledge of the tree structure.
? is first child, is last child ? a boolean indicating
whether the node appears in the sentence as the
first/last one among all the child nodes of its
parent node,
? child number ? the number of syntactic chil-
dren of the current node.
3.4 Parallel Features
Figure 1: Example sentence for parallel features illustra-
tion (see Table 1).
In RUR parser we use three types of parallel fea-
tures, computed for the parent and child node of an
edge, which make use of the source English nodes
aligned to the parent and child node.
? aligned tag: morphological tag following the
Penn Treebank Tagset (Marcus et al, 1993) of
the English node aligned to the Czech node
Feature Feature value on
parent node child node
word form jel Martin
aligned tag VBD NNP
aligned dep. label Pred Sb
aligned edge existence true
word form jel autem
aligned tag VBD NN
aligned dep. label Pred Adv
aligned edge existence false
word form do zahranic???
aligned tag ? RB
aligned dep. label ? Adv
aligned edge existence ?
word form #root# .
aligned tag #root# .
aligned dep. label AuxS AuxK
aligned edge existence true
Table 1: Parallel features for several edges in Figure 1.
? aligned dependency label: dependency label of
the English node aligned to the Czech node in
question, according to the PCEDT 2.0 label set
(Bojar et al, 2012)
? aligned edge existence: a boolean indicating
whether the English node aligned to the Czech
parent node is also the parent of the English
node aligned to the Czech child node
The parallel features are conjoined with the
monolingual coarse morphological tag and lemma
features in various ways.
If there is no source node aligned to the parent
or child node, the respective feature cannot be com-
puted and is skipped.
An example of a pair of parallel sentences is given
in Figure 1 with the corresponding values of parallel
features for several edges in Table 1.
4 Worsening Treebanks to Simulate Some
of the SMT Frequent Errors
Addressing the issue of great differences between
the gold standard parser training data and the actual
analysis input (SMT output), we introduced artificial
inconsistencies into the training treebanks, in order
to make the parsers more robust in the face of gram-
mar errors made by SMT systems. We have concen-
42
trated solely on modeling incorrect word flection,
i.e. the dependency trees retained their original cor-
rect structures and word lemmas remained fixed, but
the individual inflected word forms have been modi-
fied according to an error model trained on real SMT
output. We simulate thus, with respect to morphol-
ogy, a treebank of parsed MT output sentences.
In Section 4.1 we describe the steps we take to
prepare the worsened parser training data. Sec-
tion 4.2 contains a description of our monolingual
greedy alignment tool which is needed during the
process to map SMT output to reference transla-
tions.
4.1 Creating the Worsened Parser Training
Data
The whole process of treebank worsening consists
of five steps:
1. We translated the English side of PCEDT5 to
Czech using SMT (we chose the Moses sys-
tem (Koehn et al, 2007) for our experiments)
and tagged the resulting translations using the
Morc?e tagger (Spoustova? et al, 2007).
2. We aligned the Czech side of PCEDT, now
serving as a reference translation, to the SMT
output using our Monolingual Greedy Aligner
(see Section 4.2).
3. Collecting the counts of individual errors, we
estimated the Maximum Likelihood probabili-
ties of changing a correct fine-grained morpho-
logical tag (of a word from the reference) into
a possibly incorrect fine-grained morphological
tag of the aligned word (from the SMT output).
4. The tags on the Czech side of PCEDT were
randomly sampled according to the estimated
?fine-grained morphological tag error model?.
In those positions where fine-grained morpho-
logical tags were changed, new word forms
were generated using the Czech morphological
generator by Hajic? (2004).6
5This approach is not conditioned by availability of parallel
treebanks. Alternatively, we might translate any text for which
reference translations are at hand. The model learned in the
third step would then be applied (in the fourth step) to a different
text for which parse trees are available.
6According to the ?fine-grained morphological tag error
We use the resulting ?worsened? treebank to train
our parser described in Section 3.2.
4.2 The Monolingual Greedy Aligner
Our monolingual alignment tool, used in treebank
worsening to tie reference translations to MT out-
put (see Section 4.1), scores all possible alignment
links and then greedily chooses the currently highest
scoring one, creating the respective alignment link
from word A (in the reference) to word B (in the
SMT output) and deleting all scores of links from A
or to B, so that one-to-one alignments are enforced.
The process is terminated when no links with a score
higher than a given threshold are available; some
words may thus remain unaligned.
The score is computed as a linear combination of
the following four features:
? word form (or lemma if available) similar-
ity based on Jaro-Winkler distance (Winkler,
1990),
? fine-grained morphological tag similarity,
? similarity of the relative position in the sen-
tence,
? and an indication whether the word following
(or preceding) A was already aligned to the
word following (or preceding) B.
Unlike bilingual word aligners, this tool needs no
training except for setting weights of the four fea-
tures and the threshold.7
5 The DEPFIX System
The DEPFIX system (Marec?ek et al, 2011) applies
various rule-based corrections to Czech-English
SMT output sentences, especially of morphological
agreement. It also employs the parsed source sen-
tences, which must be provided on the input together
with the SMT output sentences.
The corrections follow the rules of Czech gram-
mar, e.g. requiring that the clause subject be in the
model?, about 20% of fine-grained morphological tags were
changed. In 4% of cases, no word form existed for the new
fine-grained morphological tag and thus it was not changed.
7The threshold and weights were set manually using just ten
sentence pairs. The resulting alignment quality was found suf-
ficient, so no additional weights tuning was performed.
43
nominative case or enforcing subject-predicate and
noun-attribute agreements in morphological gender,
number and case, where applicable. Morphological
properties found violating the rules are corrected and
the corresponding word forms regenerated.
The source sentence parse, word-aligned to the
SMT output using GIZA++ (Och and Ney, 2003),
is used as a source of morpho-syntactic information
for the correction rules. An example of a correction
rule application is given in Figure 2.
Some
people
came
later
Atr
Sb
Pred
Advplpl
.AuxK
p?i?liPredpl
N?kte??
lid?
p?i?el
pozd?ji
Atr
Sb
Pred
Advsg, mpl
.AuxK
Figure 2: Example of fixing subject-predicate agreement.
The Czech word pr?is?el [he came] has a wrong morpho-
logical number and gender. Adapted from Marec?ek et al
(2011).
The system is implemented within the
TectoMT/Treex NLP framework (Popel and
Z?abokrtsky?, 2010). Marec?ek et al (2011) feed the
DEPFIX system with analyses by the MCD parser
trained on gold-standard treebanks for parsing of
English source sentences as well as Czech SMT
output.
6 Experiments and Results
We evaluate RUR parser indirectly by using it in the
DEPFIX system and measuring the performance of
the whole system. This approach has been chosen
instead of direct evaluation of the SMT output parse
trees, as the task of finding a correct parse tree of
a possibly grammatically incorrect sentence is not
well defined and considerably difficult to do.
We used WMT10, WMT11 and WMT12 En-
glish to Czech translation test sets, newssyscomb-
test2010, newssyscombtest2011 and news-
test2012,8 (denoted as WMT10, WMT11 and
8http://www.statmt.org/wmt10,
WMT12) for the automatic evaluation. The data sets
include the source (English) text, its reference trans-
lation and translations produced by several MT sys-
tems. We used the outputs of three SMT systems:
GOOGLE,9 UEDIN (Koehn et al, 2007) and BOJAR
(Bojar and Kos, 2010).
For the manual evaluation, two sets of 1000 ran-
domly selected sentences from WMT11 and from
WMT12 translated by GOOGLE were used.
6.1 Automatic Evaluation
Table 2 shows BLEU scores (Papineni et al, 2002)
for the following setups of DEPFIX:
? SMT output: output of an SMT system without
applying DEPFIX
? MCD: parsing with MCD
? RUR: parsing with RUR (Section 3.2)
? RUR+PARA: parsing with RUR using parallel
features (Section 3.4)
? RUR+WORS: parsing with RUR trained on
worsened treebank (Section 4)
? RUR+WORS+PARA: parsing with RUR
trained on worsened treebank and using
parallel features
It can be seen that both of the proposed ways of
adapting the parser to parsing of SMT output of-
ten lead to higher BLEU scores of translations post-
processed by DEPFIX, which suggests that they both
improve the parsing accuracy.
We have computed 95% confidence intervals
on 1000 bootstrap samples, which showed that
the BLEU score of RUR+WORS+PARA was sig-
nificantly higher than that of MCD and RUR
parser in 4 and 3 cases, respectively (results
where RUR+WORS+PARA achieved a significantly
higher score are marked with ?*?). On the other
hand, the score of neither RUR+WORS+PARA nor
RUR+WORS and RUR+PARA was ever signifi-
cantly lower than the score of MCD or RUR parser.
This leads us to believe that the two proposed meth-
ods are able to produce slightly better SMT output
parsing results.
http://www.statmt.org/wmt11,
http://www.statmt.org/wmt12
9http://translate.google.com
44
Test set WMT10 WMT11 WMT12
SMT system BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDIN
SMT output *15.85 *16.57 *15.91 *16.88 *20.26 *17.80 14.36 16.25 *15.54
MCD 16.09 16.95 *16.35 *17.02 20.45 *18.12 14.35 16.32 *15.65
RUR 16.08 *16.85 *16.29 17.03 20.42 *18.09 14.37 16.31 15.66
RUR+PARA 16.13 *16.90 *16.35 17.05 20.47 18.19 14.35 16.31 15.72
RUR+WORS 16.12 16.96 *16.45 17.06 20.53 18.21 14.40 16.31 15.71
RUR+WORS+PARA 16.13 17.03 16.54 17.12 20.53 18.25 14.39 16.30 15.74
Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and
UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and
processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores
marked with ?*? on the same data.
6.2 Manual Evaluation
Performance of RUR+WORS+PARA setup was man-
ually evaluated by doing a pairwise comparison with
other setups ? SMT output, MCD and RUR parser.
The evaluation was performed on both the WMT11
(Table 4) and WMT12 (Table 5) test set. 1000 sen-
tences from the output of the GOOGLE system were
randomly selected and processed by DEPFIX, using
the aforementioned SMT output parsers. The anno-
tators then compared the translation quality of the
individual variants in differing sentences, selecting
the better variant from a pair or declaring two vari-
ants ?same quality? (indefinite). They were also pro-
vided with the source sentence and a reference trans-
lation. The evaluation was done as a blind test, with
the sentences randomly shuffled.
The WMT11 test set was evaluated by two inde-
pendent annotators. (The WMT12 test set was eval-
uated by one annotator only.) The inter-annotator
agreement and Cohen?s kappa coefficient (Cohen
and others, 1960), shown in Table 3, were computed
both including all annotations (?with indefs?), and
disregarding sentences where at least one of the an-
notators marked the difference as indefinite (?with-
out indefs?) ? we believe a disagreement in choos-
ing the better translation to be more severe than a
disagreement in deciding whether the difference in
quality of the translations allows to mark one as be-
ing better.
For both of the test sets, RUR+WORS+PARA sig-
nificantly outperforms both MCD and RUR base-
line, confirming that a combination of the proposed
modifications of the parser lead to its better perfor-
mance. Statistical significance of the results was
RUR+WORS+PARA with indefs without indefs
compared to IAA Kappa IAA Kappa
SMT output 77% 0.54 92% 0.74
MCD 79% 0.66 95% 0.90
RUR 75% 0.60 94% 0.85
Table 3: Inter-annotator agreement on WMT11 data set
translated by GOOGLE
confirmed by a one-sided pairwise t-test, with the
following differences ranking: RUR+WORS+PARA
better = 1, baseline better = -1, indefinite = 0.
6.3 Inspection of Parser Modification Benefits
For a better understanding of the benefits of using
our modified parser, we inspected a small number of
parse trees, produced by RUR+WORS+PARA, and
compared them to those produced by RUR.
In many cases, the changes introduced by
RUR+WORS+PARA were clearly positive. We
provide two representative examples below.
Subject Identification
Czech grammar requires the subject to be in nom-
inative case, but this constraint is often violated in
SMT output and a parser typically fails to identify
the subject correctly in such situations. By wors-
ening the training data, we make the parser more ro-
bust in this respect, as the worsening often switches
the case of the subject; by including parallel fea-
tures, especially the aligned dependency label fea-
ture, RUR+WORS+PARA parser can often identify
the subject as the node aligned to the source subject.
45
Out of the differing sentences
Annotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinite
count percent count percent count percent
SMT output 422 301 71% 79 19% 42 10%
A MCD 211 120 57% 65 31% 26 12%
RUR 217 123 57% 64 29% 30 14%
SMT output 422 284 67% 69 16% 69 16%
B MCD 211 107 51% 56 26% 48 23%
RUR 217 118 54% 53 24% 46 21%
Table 4: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT11 data set
translated by GOOGLE, evaluated by two independent annotators.
Out of the differing sentences
Annotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinite
count percent count percent count percent
SMT output 420 270 64% 88 21% 62 15%
A MCD 188 86 45% 64 34% 38 20%
RUR 187 96 51% 57 30% 34 18%
Table 5: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT12 data set
translated by GOOGLE.
Governing Noun Identification
A parser for Czech typically relies on morpho-
logical agreement between an adjective and its gov-
erning noun (in morphological number, gender and
case), which is often violated in SMT output. Again,
RUR+WORS+PARA is more robust in this respect,
aligned edge existence now being the crucial feature
for the correct identification of this relation.
7 Conclusions and Future Work
We have studied two methods of improving the pars-
ing quality of Machine Translation outputs by pro-
viding additional information to the parser.
In Section 3, we propose a method of integrat-
ing additional information known at runtime, i.e.
the knowledge of the source sentence (source), from
which the sentence being parsed (SMT output) has
been translated. This knowledge is provided by
extending the parser feature set with new features
from the source sentence, projected through word-
alignment.
In Section 4, we introduce a method of utilizing
additional information known in the training phase,
namely the knowledge of the ways in which SMT
output differs from correct sentences. We provide
this knowledge to the parser by adjusting its training
data to model some of the errors frequently encoun-
tered in SMT output, i.e. incorrect inflection forms.
We have evaluated the usefulness of these two
methods by integrating them into the DEPFIX rule-
based MT output post-processing system (Marec?ek
et al, 2011), as MT output parsing is crucial for the
operation of this system. When used with our im-
proved parsing, the DEPFIX system showed better
performance both in automatic and manual evalua-
tion on outputs of several, including state-of-the-art,
MT systems.
We believe that the proposed methods of improv-
ing MT output parsing can be extended beyond their
current state. The parallel features used in our setup
are very few and very simple; it thus remains to
be examined whether more elaborate features could
help utilize the additional information contained in
the source sentence to a greater extent. Modeling
other types of SMT output inconsistencies in parser
training data is another possible step.
We also believe that the methods could be adapted
for use in other applications, e.g. automatic classifi-
cation of translation errors, confidence estimation or
multilingual question answering.
46
References
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar, Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?,
Petr Sgall, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing Prague Czech-English Dependency Treebank 2.0.
In Proceedings of LREC 2012, Istanbul, Turkey, May.
ELRA, European Language Resources Association.
In print.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 127?135. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 21?29. Association for Computational Lin-
guistics.
Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshi-
masa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro
Torisawa, and Haizhou Li. 2011. SMT helps bitext
dependency parsing. In EMNLP, pages 73?83. ACL.
Jacob Cohen et al 1960. A coefficient of agreement for
nominal scales. Educational and psychological mea-
surement, 20(1):37?46.
Michael Collins, Lance Ramshaw, Jan Hajic?, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 505?512,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jennifer Foster, Joachim Wagner, and Josef Van Gen-
abith. 2008. Adapting a WSJ-trained parser to gram-
matically noisy text. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 221?224. Association for Computa-
tional Linguistics.
Kevin Gimpel and Shay Cohen. 2007. Discriminative
online algorithms for sequence labeling- a comparative
study.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, et al 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Associa-
tion for Computational Linguistics.
Jan Hajic?. 2004. Disambiguation of rich inflection: com-
putational morphology of Czech. Karolinum.
Martin Haulrich. 2012. Data-Driven Bitext Dependency
Parsing and Alignment. Ph.D. thesis.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 3-Volume 3, pages 1222?1231. Association for
Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11:311?325, September.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
47
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Comput. Lin-
guist., 19:313?330, June.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Chris Callison-Burch,
Philipp Koehn, Christof Monz, and Omar Zaidan, edi-
tors, Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 426?432, Edinburgh, UK.
Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
62?72. Association for Computational Linguistics.
Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency parsing.
Ph.D. thesis, Philadelphia, PA, USA. AAI3225503.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), June.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Sara Stymne and Lars Ahrenberg. 2010. Using a gram-
mar checker for evaluation and postprocessing of sta-
tistical machine translation. In Proceedings of LREC,
pages 2175?2181.
William E. Winkler. 1990. String comparator met-
rics and enhanced decision rules in the Fellegi-Sunter
model of record linkage. In Proceedings of the Section
on Survey Research Methods (American Statistical As-
sociation), pages 354?359.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 604?611. Association for Computational
Linguistics.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. NLP for
Less Privileged Languages, page 35.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
55?63. Association for Computational Linguistics.
48
