The hyperonym problem revisited: 
Concep, tual and::lexical.:hierarchies.in:,Janguage,,generation::  
Manfred Stede 
Technical University of Berlin 
Dept. of Computer Science 
KIT Project Group 
10587 Berl in/Germany 
st ede~cs, tu-berlin, de 
Abstract 
When a lexical item is selected in the language 
production process, it needs to be explained 
why none of its superordinates gets selected in- 
stead, since their applicability conditions are 
fulfilled all the same. This question has received 
much attention in cognitive modelling and not 
as much in other branches of NLG. This pa- 
per describes the various approaches taken, dis- 
cusses the reasons why they are so different, and 
argues that production models using symbolic 
representations should make a distinction be- 
tween conceptual nd lexical hierarchies, which 
can be organized along fixed levels as studied in 
(some branches of) lexical semantics. 
1 In t roduct ion  
Representations used in language processing 
owe much to the tradition of 'semantic net- 
works', which nowadays have been successfully 
formalized and organized especially around one 
particular kind of link between odes: the ISA- 
link, which connects entities to subordinate en- 
tities. This link is, by definition, the root of 
the so-called 'hyperonym 1 problem': When a 
speaker utters a word, she presumably needs 
to retrieve a lemma from her mental lexicon, 
and the 'applicability conditions" of the lemma 
automatically render the lemma's hyperonyms 
also applicable, thus raising the question how 
the choice among a set of more or less specific 
words is made. 
In this paper, I briefly review approaches 
to the hyperonym problem in psycholinguis- 
tics, natural language generation, and lexical 
semantics. In doing that, I will refer to differ- 
ent branches of NLG according to their roots 
I Alternatively called 'hypernym' in many publica- 
tions: 'hyperonym" seems preferable, as the Greek root 
is 'hyper" (super) + 'onoma'  (name). 
. . . .  ~ . . . . . . . . . . . . . . . . . . . . . . . . .  ? . . . . . . .  : . . . . . . . .  ~ . :  : . . . . . .  
and main motivations. Generally acknowl- 
edged are the two poles of 'cognition-inspired' 
and 'engineering-inspired' language production: 
Cognition-inspired work (CI-NLG, for short) 
seeks to build models that replicate perfor- 
mance data and explain phenomena of human 
language production with the help of psycholog- 
ical experiments; engineering-inspired work (EI- 
NLG) seeks to build programs that provide lin- 
guistic output to some particular computer ap- 
plication. These goals are extremely different, 
and it seems that the gap between the respec- 
tive methodologies will persist for quite some 
time. In between the two, however, I would 
situate a third category, which may be called 
'linguistics-inspired'. For this branch, here ab- 
breviated as LI-NLG, the primary motivation 
is neither in modelling human performance nor 
in efficiently performing a technical application; 
rather, LI-NLG seeks production models that 
replicate 'competence data', i.e. that account for 
observed linguistic regularities, without con> 
miting to statements about the human produc- 
tion p~vcess. 
Arguing that progress hinges on a better un- 
derstanding of the structure of the mental vo- 
cabulary, which includes a clear picture of the 
nature of the ISA-link, I will sketch a framework 
of distinct (but related) conceptual and lexical 
hierarchies, which offers possibilities to account 
for at least some of the phenomena to be dis- 
cussed. 
2 The  hyperonym prob lem 
Following tile psycholinguistics literature, the 
hyperonym problem is regarded as all aspect of 
lemrna retrieval. Roelofs \[1996, p. 308\] describes 
a 'lemma' as a representation f the meaning 
and the syntactic properties of a word, and the 
task of lemma retrieval as a crucial step in the 
93 
process of grammatical encoding, where build- situations of utterance. More concrete, given 
ing of a phrasal, clausal, or sentential structure a conceptual specification (in a wide sense, in- 
-requires the syntacti~information :thattemmas. : :ctuding,:eontextual. p rameters=andcommun:iCa- - 
contain. 
Thus abstracting from .the other steps of lan- 
guage production (formulation, articulation) as 
well as from possible influences of context, the 
task is confined to retrieve a lemma that cor- 
responds to the Conceptual specification that 
is represented in some adequate way. For the 
psycholinguist, the~geneya~!_.prgb!em is  that of 
convergence from an under-specified conceptual 
representation to one word that the speaker ut- 
ters. Levelt \[1989, p. 20I\] characterizes the hy- 
peronym problem: 
"There is one particularly nasty con- 
vergence problem that has not been 
solved by any theory of lexical access. 
l will call it the hyperonym problem 
\[...\]: When lemma A's meaning entails 
lemma B's meaning, B is a hyperonym 
of A. If A's conceptual conditions are 
met, then B's are necessarily also satis- 
fied. Hence, if A is the correct lemma, 
B will (also) be retrieved." 
The relation of hyperonymy is generally re- 
garded as transitive: If A is a hyperonym of 
B, and B is a hyperonym of C, then A is a hy- 
peronym of C. Following common practice, we 
call A a direct hyperonym of B, while it is only 
an indirect hyperonym of C. The same holds for 
the inverse relation, hyponymy. 
For CI-NLG, which is concerned with find- 
ing models that resolve the convergence prob- 
lem with the impressive speed displayed by hu- 
man speakers, the hyperonym problem is im- 
portant because it. serves to put implemented 
models of spreading activation to the test. For 
EI-NLG. on the other hand, it can usually be 
ignored, as most of today's practical applica- 
tions either do not require the production of a 
more general word (i.e.. there is a one-to-one 
mapping from concept to word) or can rely on 
fairly simple mechanisms that.,avoid ,lexical rep- 
etitions bv choosing from a fixed, pre-defined set 
of near-synonyms. For LI-NLG, the challenge 
of the hyperonynl problem is to explain how a 
sentence can be paraphrased by others that re- 
place a word by a hyperonym, and why speakers 
select from candidate hyperonyms in different 
rive goals), the task is to find the best candidate 
from a set of valid paraphrases, here especially 
on the grounds of replacing content words with 
hyperonyms. 
3 Psycholinguistic product ion  
mode ls  
.... Lan gu age' prod n'ction ~m o dels~deve\[oped in- psy:--, 
cholinguistics are nowadays couched in neural 
network theory. Under debate are the computa- 
tional properties of the networks, i.e., the modes 
of activation spreading, tile existence of feed- 
back, of inhibitory links, etc. The main method- 
ological concern is to construct he models in 
such a way that they account for data gathered 
in human speech production experiments, of- 
ten involving production errors, which can shed 
light on the underlying mechanisms. 
A central point of content is the ques- 
tion whether the meaning of concepts and/or 
words is represented in a decomposed fashion 
or not. Here, the hyperonym problem is some- 
times used as evidence by proponents of non- 
decompositional models. Roelofs \[1996\], for in- 
stance, argues that if a number of nodes repre- 
senting semantic features are the basis for lex- 
ical access, in lemma retrieval it becomes ex- 
tremely difficult to control the activation spread 
in such a way that only the most specific lexical 
unit that combines these features gets selected. 
Roelofs concludes that a non-decompositional 
model is to be favoured: When lemma retrieval 
starts with activation of the 'lexical concept' 
FATHER, rather than with tile features MALE 
and PARENT, the output word will be father, 
without the danger of being outranked by a 
higher activation of parent (or person, or entity. 
presumably). 
This line is continued in a recent compre- 
hensive theory of speech production by Lev- 
elt. Roelofs, and Meyer \[1999\]. The focus of 
.this. theory_is more _on. the side. of.articulation, 
but their approach to (non-) decompos'itionan/:t 
hyperonyms follows the basic assumption just 
sketched. The model consists of three layers of 
nodes: A layer of concept nodes with labelled 
concept links, a layer of lemma nodes, and a 
layer of word form nodes that include morpho- 
94 
logical information. When a lexical concept is 
activated, the mechanism of activation spread- 
:ing ensures that ~the::~directly:..ecm:nected::lemma.... 
receives tile highest activation, and not a lemma 
associated with a hyperonym of the lexical con- 
cept (which is connected by an ISA-link). 
Working out the mechanics to ensure this 
behaviour is important for the implementa- 
tion, but from the particular viewpoint of word 
choice, approaches of this kind are not very ex- 
planatory. Levelt. et.al. :\[1999, ~..~,4\]i istate that  
"there is not the slightest evidence that speak- 
ers tend to produce hyperonyms of intended 
target words." But when lexical access starts 
with an appropriately activated lexical concept, 
the problem is effectively moved away, into the 
realm of conceptualization. The authors ac- 
knowledge the need for a component hat es- 
tablishes a 'perspective' by selecting a specific 
set of words, but have not incorporated such a 
component into their model. Thus, why and 
how the lexical concept receives its activation, 
and where the intention of using a word arises 
from, is not covered by the theory. For these 
questions, we have to turn to work in natural 
language generation. 
4 Hyperonyms in NLG sys tems 
In contrast to psycholinguistics-inspired work, 
the vast majority of natural language genera- 
tion systems uses computations based on sym- 
bol manipulation, often connected with sym- 
bolic knowledge representation and reasoning 
techniques. In these systems, the hyperonym 
problem as one aspect of the general task of 
lexical choice arises only in systems that em- 
ploy a sufficiently rich model of the lexicon and 
tile concept-lexicon link. involving some sort of 
hierarchy information. As pointed out above, 
from an application-oriented perspective (i.e.. 
in EI-NLG) it is often sufficient o work with 
rather limited mechanisms that largely eschew 
the lexical choice task. 
The earliest and very influential device for 
performing lexical choice, Goldman's-\[.1.-975\] 
discr imination net hard-wires the sequence of 
choice points leading to a specific lexical item, 
which is in fact the general strategy taken in the 
majority of NLG systems: if you have a choice. 
then prefer the most specific term. 
The most substantial criticism on the prefer- 
the-specific heuristic has been voiced in the 
work of Reiter \[1991\]. One of his examples 
:is. ~  system., ~as~zerhlg~:the-N.uestio n .*Is; .Ter~y:a 
woman?  Even if the system has the specific 
knowledge that Terry is a bachelor, the response 
No, Terry is a bachelor would not be appropri-.. 
ate here; the less specific No, Terry is a man 
is better since it does not prompt the hearer to 
draw ally conclusions as to tile particular ele- 
vance of Terry's marital status for the present 
Lc0:n~ersa, tion, Reiter?s-. main -pointis:to distin- 
guish the knowledge a generation system has at 
its disposal from the communicative goals fol- 
lowed in producing an utterance. The latter 
are explicitly represented in his system as a. list 
of attributes 'to communicate about an entity', 
which is a subset of the overall knowledge the 
system has of that entity. In the Terry-example, 
the goal is to inform the hearer that Terry 
has the attributes {Human, Age-status:adult, 
Sex:Male}. 
In the KL-ONE \[Brachman, Schmolze 1985\]) 
style knowledge representation used by Reiter, 
concepts can be marked as 'basic-level' in the 
sense of \[Rosch 1978\]. Thus, on the taxonomic 
path Tweety (instance-of) Robin - Bird - Ver- 
tebrate - An imal  - Object, the concept Bird is 
a basic-level one, which leads to a preference 
for using the corresponding lexical item when 
referring to some kind of bird (i.e., some con- 
cept or instance subsumed by it). Simultane- 
ous to Rosch's work, Cruse \[1977\] (who in turn 
was building on earlier research by Roger Brown 
in tile 1960s) had pointed out that tile failure 
to use items of "inherently neutral specificity" 
(a notion that closely corresponds to the basic 
level) results in unwanted conversational impli- 
ca.tures I tile hearer will surmise the existence 
of some reason why the neutra.1 term could not 
be used in the specific situation of utterance. 
But using the basic level is not mandatory. 
of course. Given a suitable context where at- 
tention is directed to particular attributes of 
entitities, a speaker moves to a more specific 
or sometimes to a more ~ general : evel. ~:Reiter's 
mechanism of to-communicate attributes tries 
to capture this: Covering these attributes with 
a suitable term can override the preference for 
the basic level. Other kinds of preferences are 
also accounted for, such as favouring shorter 
rather than longer words, which typically (but 
95 
not always) co-incides with the basic-level pref- 
erence. Reiter notes that humans also employ 
. . . . . .  - some preferences.t:hat can~otbe xplained ~wi,th 
the parameters investigated so far. He gives 
the example \[Reiter 1991, p. 248\] of a speaker 
pointing the hearer to a cow and a horse with 
the utterance Look at the animals / mammals / 
vertebrates, t None of the terms is basic-level or 
signigificantly shorter than the others, yet there 
is a clear order of-'normality' in the sequence of 
the three candidates. 
In my own work on lexical choice in  the 
'Moose' generator \[Stede 1999\], I used language- 
neutral conceptual hierarchies and the sub- 
sumption relation, inter alia to account for the 
fact that different languages occasionally dis- 
play preferences for different levels of specificity. 
For example, in hi-lingual instructional text we 
find a regular correspondence between the gen- 
eral English to remove and numerous more spe- 
cific German lexemes ( abziehen, abnehmen, her- 
ausdrehen, ...); this might very well be a genre- 
specific tendency. Furthermore, Moose employs 
a model of lexical connotations that can over- 
ride the general preference for a more specific 
lexical item. For example, when referring to a 
POODLE in a derogatory manner, Moose can 
choose the appropriately connotated word mutt, 
which requires moving up the taxonomy to the 
DOG concept, where a range of near-synonyms 
(differing in their connotations) are attached. 
Another reason for considering hyperonyms in 
the lexical choice process is to avoid repeated 
usage of the same term when referring to some 
object multiple times. 
In the present Moose implementation, all 
more general words are inherited to the concept- 
to-be-lexicalized, and the preference mechanism 
selects one of them (in case of absence of any de- 
cisive factors, it chooses the most specific word). 
This mechanism is certainly not cognitively ad- 
equate (it was not intended to be) and also not 
particularly efficient: The range of candidates 
under consideration should be constrained be- 
forehand. 
-In conclusion, NLG systems, employ a mix- 
ture of constraints and preferences in their ap- 
proaches to hyperonymy. The factors used by 
various systems in the choice process are: 
o User's vocabulary and knowledge (e.g.. 
\[Mcl(eown et al 199:\]\]) 
. Successul reference, i.e., discrimination 
from other candidate entities (e.g., \[Dale, 
Re i te r1995\ ] )  .:: :- ........ ~' 
? Basic-level and entry-level effects, conver- 
sational implicatures 
? Length of words 
? Stylistic features such as formality, posi- 
tive/negative attitude 
? Language, genre _, 
? Givenness of item, avoid repetition or "say- 
ing the very obvious" 
Not surprisingly, there is no generator yet that 
would incorporate al l  these factors within a 
single system. It is not clear which general 
lexical items should be inherited down to the 
concept-to-be-lexicMized and enter the prefer- 
ential choice mechanism; it is also not clear how 
exactly the various preferences would interact 
and which would take precedence in a particu- 
lar situation of utterance. 
5 Hyperonymy in lex ica l  semant ics  
Linguists studying lexical semantics are to a 
good extent concerned with sense relations be- 
tween words, and hyp(er)onymy is certainly one 
of the relations receiving the most attention. 
While the intuitive decision whether some en- 
tity is subordinate to some other entity is in 
most cases not difficult to make, spelling out 
the precise definition of hyponymy (and thus 
hyperonymy) and its consequences is anything 
but trivial. Lyons \[1977\], for example, proposes 
that fish and bird share the direct hyperonym 
creature-  but not animal. That is, when I say 
There were plenty of fish in the creek, tile al- 
ternative sentence There. were plenty of animals 
in the creek would not be a felicitous utterance. 
even thougil it is "trutl>conditionally correct". 
And hence, there is a difference between fish 
ISA creature and fish ISA animal. 
An interesting distinction in this respect is 
offered by Cruse \[1986\], who separates hy- 
ponymy_ from the more constrained relation .of 
taxonym, y. A diagnosis for the latter is the ut- 
terance frame X is a kind o f / type  of Y. Exam- 
pies that "work" in this frame are: spaniel-dog, 
rose-flou, er, mango-fruit. Examples that seem 
not to work are: kitten-cat, queen-monarch, 
spinster-woman, u aiter-man. Notice t hat bot h 
96 
groups are perfectly compatible with the ISA- 
test, though: No one would doubt that a waiter 
IS A man, a.q-ueen IS A'.monarch. 
Taxonomies, as Cruse proposes, typically 
have no more than five levels, and frequently 
have fewer. The levels are commonly labelled 
as 'unique beginner' - 'life form' - 'generic' -
'specific' - 'varietal'. (The origin of these term 
in biology is obvious, but they can be t rans -  
creature creature 
animal bird 
/N 
dog cat dog cat b~ 
& A 
collie spaniel robin blackbM slarling collie spaniel robin blackbird starling 
Figure 1: Variants of taxonomy, reproduced 
from \[Cruse 1986, p. 146\] 
.6 Synthes is :  Toward  a mode l  o f  
ferred to otherweatms, as-t3ruse notes.)  Most ..... .- ::.~..:.coneepCu:at.van@-:lexical inhe~i tance  
important is the generic level, which holds or -  Due to the very different motivations, different 
dinary everyday names like cat, apple, church, 
cup. These items tend to be morphologically 
simple and are not metaphorically transferred 
from elsewhere. Most branches of hierarchies 
terminate at the generic level, and hence this 
is the level with the largest number of items. 
Items at specific and varietal evels are particu- 
larly likely to be morphologically complex, and 
compound words are frequent here. 
From the notion of explicitly defined levels, 
it follows that hierarchies do not need to have 
nodes at each level. Consider the examples in 
figure 1. Depending on what items people place 
on the generic level, they end up with one of the 
two variants; according to Cruse, most people 
subscribe to the second, which holds dog, cat, 
bird on the same, generic level. Another ex- 
ample are musical instruments: Most of them 
belong to a kind such as strings, woodwind, 
brass, percussion, but there is no obvious kind 
for bagpipes or concertina, which are thus di- 
rectly linked to musical instrument. 
Cruse elaborated the importance of the 
generic level in \[Cruse 1977\], where he states 
that for every line of noun taxonomy, there is 
one term that is 'inherently neutral' (cf. the no- 
tion of basic level mentioned above). There is 
a general rule that requires speakers to use this 
term in order to obtain an unmarked utterance 
in a given context:-:-.unless.this would- result 
in an 'abnormal communication', in which case 
the speaker should deviate from neutral level, 
but only to the minimum degree required to en- 
sure normality. Cruse then offers several condi- 
tions that would license such over- and under- 
specification, which we do not reproduce here. 
kinds of NLG have very different approaches to 
the hyperonym problem. EI-NLG can basically 
ignore or finess it. In CI-NLG, it is reduced 
to a merely technical question: getting the me- 
chanics of spreading activation right, so that 
lexical convergence enables the subsequent pro- 
cesses of syntactization and articulation (which 
the CI-NLG models place their emphasis on). 
A broader view is necessarily based on reason- 
ing with speaker's goals and contextual features, 
which for the time being is the realm of LI- 
NLG. Thus, before embarking on building more 
comprehensive con.nectionist models, the hyper- 
onym problem is best studied in the frameworks 
of LI-NLG - -  but with the motivation of mod- 
elling human performance taken into account. 
Thus adopting the perspective outlined in 
section 4, we are interested in choosing words 
between more or less specific alternatives as well 
as between near-synonyms of the same speci- 
ficity. We thereby open the door to both 'ver- 
tical' and 'horizontal' lexical choice within a hi- 
erarchy, which raises a number of questions: 
* What is the granularity of conceptual, and 
that of lexical knowledge? 
? How are tile differences between near- 
synonyms represented? 2 
? Given an activated concept, which more 
general lexical items are considered in tile 
choice process; are there any restrictions on 
.-lexical inheritance-?- . . . . . . . . .  
o How is the eventual choice from the set of 
candidate lexical items being made? 
2This question is beyond the scope of this paper; the 
kind of approach I have in mind here is represented in
\[DiMarco et al 1993\], \[Hirst 1995\], \[Edmonds 1999\]. 
97 
collie -- (a silky-coated sheepdog with a long ruff and long narrow head developed in Scotland) 
=> shepherd dog, sheepdog, sheep dog -- (any .of various usually long-haird breeds,of do.g, ~ .. 
reared to herd-and guard sheep) 
=> working dog -- (any of several breeds of usually large powerful dogs bred to work as 
draft animals and guard and guide dogs) 
=> dog, domestic dog, .Canis familiaris -- (a member of the genus Canis"(probably... 
=> canine, canid -- (any of various fissiped mammals with nonretractile claws and 
typically long muzzles) 
=> carnivore -- (terrestrial or aquatic flesh-eating mammal; terrestrial carnivores 
have four or five clawed digits on each limb) 
=> placental, placental mammal, eutherian, eutherianmammal -- (mammals having a 
placenta; all mammals except monotremes and marsupials) 
.`=>~mamma1~-~a~amm~c~.~ded~er~eb~rte.having.~t~he`~in.~mur~.`~r~ess~?.~Yered.~ 
=> vertebrate, craniate -- (animals having a bony or cartilaginous skeleton... 
=> chordate -- (any animal of the phylum Chordata having a notochord or 
spinal column) 
=> animal, animate being, beast, brute, creature, fauna -- (a living 
organism characterized by voluntary movement) 
=> life form, organism, being, living thing -- (any living entity) 
=> entity, something -- (anything having existence (living or nonliving)) 
Figure 2: Hyperonyms for collie from WordNet 
As we have seen, present models that admit 
hyperonyms into the choice process (in particu- 
lar those of Reiter \[1991\] and Stede \[1999\]) run 
into the problem of overgeneration: Too many 
candidates have to be compared for their prefer- 
ential features, and it is not clear that a decision 
can always be made. 
To illustrate the question of granularity and 
range of hyperonymic alternatives, contrast he 
path from collie to creature given by Cruse 
\[1986\] in figure 1 with the hyperonym chain 
for collie offered by WordNet \[Fellbaum 1998\], 
shown in figure 2. The WordNet chain includes 
many items that clearly do not show up in ev- 
eryday language use, and that a lexical choice 
process hould prefer not to consider when pro- 
ducing an utterance about a collie. Chordate, 
for example, would in the vast majority of utter- 
ance situations not be an option. On tile other 
hand, all these terms are certainly 'correct', and 
a system should be able to respond affirmatively 
to the question Is a collie a chordate ?
This divergence points to the need for a dis- 
tinction between conceptuaJ,and lexicalg:ranu- 
laritv and inheritance: The WordNet chain rep- 
resents rather a series of concepts than of words 
entering the lexical choice process, which ap- 
pears to be better represented by a Cruse-type 
chain with few designated levels (but needs to 
be augmented with near-synonyms for tile 'hor- 
i ng ,  . . .  . . . . . . . . . . .  ffntity 
creature  . . . .  " . . . . . . . .  li~q form 
an imal ,  beast ,  . . .  . . . . .  animal 
c>rdate  
ve~ebrate 
~ammal 
p~cental 
c~71ivore 
7~_1 ine 
dog J/ . wyking dog 
shepherd dog 
/ 
co l l i e  . . . . . . . . . .  collie 
Figure 3: Active-lexical and conceptual hierar- 
chy 
izontal' aspects of choice). 
The resulting situation is sketched in figure 3. 
On the right hand side, the nodes of the concep- 
tual chain also are linguistic units, but in lan- 
guage production they would be accessed only 
, if tile. '.to~com unical~e".attdbutes ex.plicitly, call 
for it, e.g., when comparing chordates to verte- 
brates. Otherwise, only items oll tile left hand 
side (tentatively called 'active-lexical') enter tile 
lexical choice process, which are characterized 
by their particular level in the vocabulary struc- 
ture, and further differentiated by stylistic and 
98 
other features. The generic, or basic, level is P. Downing. "Factors influencing lexical choice 
marked by a box. in narrative." In: W. Chafe (ed.): The pear 
When a hyperonym chain is thus not.merely -- .... stories:, cognitive,: c,ultural~ .and:li.nguistic~as-. 
an ordered list, but the signficance of the levels 
is recognized (assuming that Cruse's proposal of 
level structure indeed scales up to other areas of 
vocabulary), rules for deviating from the generic 
level can be stated that map contextual param- 
eters onto 'level movement instructions'. These 
rules would extend the lexicalisation framework 
of Reiter \[1991\], w.he~e tthe?fivsg:Gon~tion .~is..ad- 
hering t~ the hard constraints (the word must 
convey the essential attributes that are to be 
communicated), and the second is a preference 
for the basic level. Adding the instructions 
for level movement would "contextualize" this 
framework. 
The rules for moving between levels have to 
consider the specific function of the NP (refer, 
inform about category membership, etc.) and 
other factors as indicated in the previous sec- 
tions (and others mentioned by Cruse \[1977\]). 
Since the roles and interactions of these fac- 
tors are not well understood yet, at this point 
CI-NLG can make important contributions by 
designing experiments hat shed more light on 
the parameters that prompt speakers to deviate 
from the basic level; one example here is the 
study on speaker's lexical choices in narrative 
by Downing \[1980\]. 
Re ferences  
R. Brachman, J. Schmolze. "An overview of the 
KL-ONE knowledge representation system." 
In: Cognitive Science 9 (2), 1985. 
D. Cruse. "The pragmaties of lexical speci- 
ficity." In: Journal of Linguistics 13, pp. 153- 
164, 1977. 
D. Cruse. Lexieal semantics. Cambridge, \[_l\[(: 
Cambridge University Press, 1986. 
R. Dale, E. Reiter. "Computational Interpreta- 
tions of the Gricean Maxims in the Genera- 
tion of Referring Expressions." In: Cognitive 
Science 19:233-263, 1995. 
C. DiMarco, G. Hirst, M. Stede. "The semantic 
and stylistic differentiation of synonyms and 
near-synonyms." In: Working notes of the 
AAAI Spring Symposium on Building Lexi- 
cons for Machine Translation. Stanford Uni- 
versity, March 1993. 
pects of narrative production. Norwood/N J:
Ablex, 1980 
P. Edmonds. "Semantic representations of near- 
synonyms for automatic lexical choice." PhD 
thesis, Department of Computer Science, 
University of Toronto, September 1999. 
C. Fellbaum. WordNet -- An Electronic Lexical 
:Database~C~mb~idge /MA : MI T . l~,ress, 199.8. 
N.M. Goldman. "Conceptual generation." In: 
R.C. Schank (ed.): Conceptual informa- 
tion processing. Amsterdam: North-Holland, 
1975. 
G. Hirst. "Near-synonymy and the structure of 
lexical knowledge." In: Working notes of the 
AAAI Spring Symposium on Representation 
and Acquisition of Lexica\] Knowledge. Stan- 
ford University, 1995. 
J. Lyons. Semantics. Volume I. Cambridge/UK: 
Cambridge University Press, 1977. 
K. McKeown, J. Robin, M. Tanenblatt. "Tai- 
loring lexical choice to the user's vocabulary 
in multimedia explanation generation." In: 
Proceedings of the 31st Annual Meeting of 
the Association for Computational Linguis- 
t ics (ACL). Columbus, OH, 1993. 
W. Levelt. Speaking: From Intention to Articu- 
lation. Cambridge/MA: MIT Press, 1989. 
W. Levelt, A. Roelofs, A. Meyer. "A theory 
of lexical access in speech production." In: 
Behavioral and Brain Sciences 22, pp. 1-75, 
1999. 
E. Reiter. "A new model of lexical choice for 
nouns." In: Computational Intelligence 7, 
240-251, 1991. 
A. Roelofs. "Computational Models of Lemlna 
Retrieval." In: T. Dijkstra, K. de Smedt 
(eds.): Computational Psycholir~gui.~tic.~. 
London: Taylor & Francis. 1996. 
E. Rosch. "Principles of categorization." In: E. 
Rosch, B. Lloyd (eds.): Cognition and cate- 
gorization. Hilldale, N J: Lawrence Erlbaum, 
1978. 
? M: Stede. Lexicai semantics and tcrmwledge r p- 
resentation in multilingual text generations.. 
Dordrecht/Boston: Kluwer, 1999. 
99 
 	
	Machine-Assisted Rhetorical Structure Annotation
Manfred Stede and Silvan Heintze
University of Potsdam
Dept. of Linguistics
Applied Computational Linguistics
D-14415 Potsdam
Germany
stede|heintze@ling.uni-potsdam.de
Abstract
Manually annotating the rhetorical struc-
ture of texts is very labour-intensive. At the
same time, high-quality automatic analysis
is currently out of reach. We thus propose to
split the manual annotation in two phases:
the simpler marking of lexical connectives
and their relations, and the more difficult
decisions on overall tree structure. To this
end, we developed an environment of two
analysis tools and XML-based declarative
resources. Our ConAno tool allows for effi-
cient, interactive annotation of connectives,
scopes and relations. This intermediate re-
sult is exported to O?Donnell?s ?RST Tool?,
which facilitates completing the tree struc-
ture.
1 Introduction
A number of approaches tackling the difficult
problem of automatic discourse parsing have
been proposed in recent years (e.g., (Sumita
et al, 1992) (Marcu, 1997), (Schilder, 2002)).
They differ in their orientation toward sym-
bolic or statistical information, but they all ?
quite naturally ? share the assumption that
the lexical connectives or discourse markers are
the primary source of information for construct-
ing a rhetorical tree automatically. The den-
sity of discourse markers in a text depends on
its genre (e.g., commentaries tend to have more
than narratives), but in general, it is clear that
only a portion of the relations holding in a text
is lexically signalled.1 Furthermore, it is well-
known that discourse markers are often ambigu-
ous; for example, the English but can, in terms
of (Mann, Thompson, 1988), signal any of the
relations Antithesis, Contrast, and Concession.
Accordingly, automatic discourse parsing focus-
ing on connectives is bound to have its limita-
tions.
1In our corpus of newspaper commentaries (Stede,
2004), we found that 35% of the coherence relations are
signalled by a connective.
Our position is that progress in discourse
parsing relies on the one hand on a more thor-
ough understanding of the underlying issues,
and on the other hand on the availability of
human-annotated corpora, which can serve as
a resource for in-depth studies of discourse-
structural phenomena, and also for training
statistical analysis programs. Two examples
of such corpora are the RST Tree Corpus by
(Marcu et al, 1999) for English and the Pots-
dam Commentary Corpus (Stede, 2004) for
German. Producing such resources is a labour-
intensive task that requires time, trained anno-
tators, and clearly specified guidelines on what
relation to choose under which circumstances.
Nonetheless, rhetorical analysis remains to be
in part a rather subjective process (see section
2). In order to eventually arrive at more objec-
tive, comparable results, our proposal is to split
the annotation process into two parts:
1. Annotation of connectives, their scopes
(the two related textual units), and ? op-
tionally ? the signalled relation
2. Annotation of the remaining (unsignalled)
relations between larger segments
Step 1 is inspired by work done for English
in the Penn Discourse TreeBank2 (Miltsakaki
et al, 2004). In our two-step scenario, it is the
easier part of the whole task in that connectives
can be quite clearly identified, their scopes are
often (but not always, see below) transparent,
and the coherence relation is often clear. We see
the result of step 1 as a corpus resource in its
own right (it can be used for training statistical
classifiers, for instance) and at the same time
as the input for step 2, which ?fills the gaps?:
now annotators have to decide how the set of
small trees produced in step 1 is best arranged
in one complete tree, which involves assigning
2http://www.cis.upenn.edu/?pdtb/
relations to instances without any lexical sig-
nals and also making more complicated scope
judgements across larger spans of text ? the
more subjective and also more time-consuming
step.3
Our approach is as follows. To speed up the
annotation process in step 1, we have devel-
oped an XML format and a dedicated analysis
tool called ConAno, which will be introduced
in Section 4. ConAno can export the anno-
tated text in the ?rs3? format that serves as in-
put to O?Donnell?s RST Tool (O?Donnell, 1997).
His original idea was that manual annotation be
done completely with his tool; we opted however
to use it only for step 2, and will motivate the
reasons for this overall architecture in Section
5.
The net result is a modular, XML-based
annotation environment for machine-assisted
rhetorical analysis, which we see as on the one
hand less ambitious than fully-automatic dis-
course parsing and on the other hand as more
efficient than completely ?manual? analysis.
2 Approaches to rhetorical analysis
There are two different perspectives on the task
of discourse parsing: an ?ideal? one that aims
at modelling a systematic, incremental process;
and an ?empirical? one that takes the experi-
ences of human annotators into account. ?Ide-
ally?, discourse analysis proceeds incrementally
from left to right, where for each new segment,
an attachment point and a relation (or more
than one of each, cf. SDRT) are computed and
the discourse structure grows step by step. This
view is taken for instance in SDRT (Asher, Las-
carides, 2003), which places emphasis on the no-
tion of ?right frontier? (also discussed recently by
(Webber et al, 2003)).
However, when we trained two (experienced)
students to annotate the 171 newspaper com-
mentaries of the Potsdam Commentary Corpus
(Stede, 2004) and upon completion of the task
asked them about their experiences, a very dif-
ferent picture emerged. Both annotators agreed
that a strict left-to-right approach is highly im-
practical, because the intended argumentative
structure of the text often becomes clear only
in retrospect, after reflecting the possible con-
tributions of the segments to the larger scheme.
3This assessment of relative difficulty does not carry
over to PDTB, where the annotations are more complex
than in our step 1 but do not go as far as building rhetor-
ical structures.
Thus they very soon settled on a bottom-up ap-
proach: First, mark the transparent cases, in
which a connective undoubtedly signals a rela-
tion between two segments.4 Then, see how the
resulting pieces fit together into a structure that
mirrors the argument presented.
The annotators used RST Tool (O?Donnell,
1997), which worked reasonably well for the pur-
pose. However, since we also have in our group
an XML-based lexicon of German connectives
at our disposal (Berger et al, 2002), why not
use this resource to speed up the first phase of
the annotation?
3 Annotating connectives and their
scopes
In our definition of ?connective?, we largely fol-
low (Pasch et al, 2003) (a substantial catalogue
and classification of German connectives), who
require them to take two arguments that can
potentially be full clauses and that semantically
denote two-place relations between eventualities
(but they need not always be spelled out as
clauses). From the syntactic viewpoint, they are
a rather inhomogeneous group consisting of sub-
ordinating and coordinating conjunctions, some
prepositions, and a number of sententence ad-
verbials. We refer to the two related units as
an ?internal? and an ?external? one, where the
?internal? one is the unit of which the connec-
tive is actually a part. For example, in Despite
the heavy rain we had a great time, the noun
phrase the heavy rain is the internal unit, since
it forms a syntactic phrase together with the
preposition. Notice that this is a case where
the eventuality (a state of weather) is not made
explicit by a verb.
As indicated, this step of annotating connec-
tives and units is closely related to the idea
of the PDTB project, which seeks to develop
a large corpus annotated with information on
discourse structure for English texts. For this
purpose, annotators are provided with detailed
annotation guidelines, which point out various
challenges in the annotation process for explicit
as well as empty connectives and their respec-
tive arguments. They include, among others,
? words/phrases that look like connectives,
but prove not to take two propositional ar-
guments
4The clearest cases are subjunctors, which always
mark a relation between matrix clause and embedded
clause.
? words/phrases as preposed predicate com-
plements
? pre- and post-modified connectives
? co-occurring connectives
? single and multiple clauses/sentences as ar-
guments of connectives
? annotation of discontinuous connective ar-
guments
Annotators have to also make syntactic judge-
ments, which is not the case in our approach
(where syntax would be done on a different an-
notation layer, see (Stede, 2004)).
In the following, we briefly explain the most
important problematic issues with annotating
German connectives and the way we deal with
them, using our annotation scheme for Con-
Ano.
3.1 Issues with German connectives
Connective or not: Some words can be used
as connective or in other functions, such as und
(?and?), which can for example conjoin clauses
(connective) or items in a list (no connective).
Which relation: Some connectives can sig-
nal more than one relation, as the above-
mentioned but and its German counterpart aber.
Complex connectives: Connectives can be
phrasal (e.g., aus diesem Grund, ?for this rea-
son?) or even discontinuous (e.g., entweder
. . . oder, ?either . . . or?). A fortiori, some may
be used in more than one order (wenn A, dann
B / dann B, wenn A / dann, wenn A, B; ?if
. . . then . . . ?).
Multiple connectives/relations: Some
connectives can be joined to form a complex
one, which might then signal more than one
relation (e.g., combinations with und and aber,
such as aber dennoch, ?but still?).
Modified connectives: Some but not all
connectives are subject to modification (e.g.,
nur dann, wenn, ?only then, if?; besonders weil,
?especially because?).
Embedded segments: The minimal units
linked by the connective may be embedded
rather than adjacent: Wir mu?ssen, weil die Zeit
dra?ngt, uns Montag treffen (?We have to, be-
cause time is short, meet on Monday?).
3.2 A DTD and an Example
As the first step toward an annotation tool, we
defined an XML format for texts with connec-
tives and their scopes. Figure 1 shows the DTD,
and Figure 2 a short sample annotation of a
single ? yet complex ? sentence: Auch Berlin
koennte, jedenfalls dann, wenn der Bund sich
erkenntlich zeigt, um die Notlage seiner Haupt-
stadt zu lindern, davon profitieren. (?Berlin,
too, could ? at least if the federation shows
some gratitude in order to alleviate the emer-
gency of its capital ? profit from it.?) The DTD
introduces XML tags for each of the connec-
tives (<connective>), their possible modifiers
(<modifier>) and respective discourse units
(<unit>, where the type can be filled by int or
ext), as well as the entire text (<discourse>).
Henceforth, we will refer to the text unit con-
taining the connective as the internal, ?int-unit?
and to the other, external, one as ?ext-unit?. Us-
ing this DTD, it is possible to represent the
range of problematic phenomena discussed in
the previous section.
Connective or not: Only those words actu-
ally used as connectives will be marked with the
<connective> tag, while others such as the fre-
quently occurring und (?and?) or oder (?or?) will
remain unmarked, if they merely conjoin items
in a list.
Which relation: The <connective> tag in-
cludes a rel attribute for optional specification
of the rhetorical relation that holds between the
connected clauses.
Complex connectives: Using an XML
based annotation scheme, we can easily mark
phrasal connectives such as aus diesem Grund
(?for this reason?) using the <connective> tag.
In order for discontinuous connectives to be an-
notated correctly, we introduce an id attribute
that provides every connective with a distinct
reference number. This way connectives such as
entweder . . . oder, (?either . . . or?) can be repre-
sented as belonging together. (see <connective
id="4" rel="condition"> tags in Figure 2)
Multiple connectives/relations: In our
annotation scheme, complex connectives such
as aber dennoch, (?but still?) are treated as two
distinct connectives that indicate different rela-
tions holding between the same units.
Modified connectives: Connective modi-
fiers are marked with a special <modifier> tag,
which is embedded inside the <connective>
tag, as shown with jedenfalls modifying dann in
our example. Hence an additional id attribute
for this tag is not necessary.
Embedded segments: Discourse units are
marked using the <unit> tag, which also pro-
vides an id attribute. On the one hand, this
is used for assigning discourse units to their re-
spective connectives, on the other hand it pro-
vides a way of dealing with discountinuous dis-
course units, as the example shows.
<?xml version=?1.0? encoding=?UTF-8??>
<!ELEMENT modifier (#PCDATA)>
<!ELEMENT connective (#PCDATA|modifier)>
<!ATTLIST connective
id CDATA #IMPLIED
rel CDATA #IMPLIED>
<!ELEMENT unit
(#PCDATA|connective|unit)*>
<!ATTLIST unit
id CDATA #IMPLIED
type CDATA #IMPLIED>
<!ELEMENT discourse (#PCDATA|unit)*>
Figure 1: The DTD for texts-with-connectives
<?xml version="1.0"?>
<!DOCTYPE discourse SYSTEM "discourse.dtd">
<discourse>
<unit type="ext" id="4">
Auch Berlin koennte,
<connective id="4"
relation="condition">
<modifier>jedenfalls</modifier>
dann
</connective>
,
</unit>
<unit type="int" id="4">
<connective id="4"
relation="condition">
wenn
</connective>
der Bund sich erkenntlich zeigt, um
die Notlage seiner Hauptstadt zu
lindern,
</unit>
<unit type="ext" id="4">
davon profitieren.
</unit>
</discourse>
Figure 2: Sample annotation in XML-format
4 The ConAno annotation tool
A range of relatively generic linguistic annota-
tion tools are available today, but none of them
turned out suitable for our purposes: We seek a
very easy-to-use, platform-independent tool for
mouse-marking ranges of text and having them
associated with one another. Consequently, we
decided to implement our own Java-based tool,
ConAno, which is geared especially to connec-
tive/scope annotation and thus can have a very
intuitive user interface.
Just like discourse parsers do, ConAno ex-
ploits the fact that connectives are the most re-
liable source of information. Rather than at-
tempting an automatic disambiguation, how-
ever, ConAno merely makes suggestions to the
human analyst, which she might follow or dis-
card. In particular, ConAno loads a list of
(potential) connectives, and when annotation
of a text begins, highlights each candidate so
that the user can either confirm (by marking its
scope) or discard (by mouse-click) it if it not
used as a connective. Furthermore, the connec-
tive list optionally may contain associated co-
herence relations, which are then also offered to
the user for selection. This annotation phase
is thus purely data-driven: Attention is paid
only to the connectives and their specific rela-
tion candidates.
To elaborate a little, the annotation process
proceeds as follows. The text is loaded into the
annotation window, and the first potential con-
nective is automatically highlighted. Potential
preposed or postposed modifiers, if any, of the
connective are also highlighted (in a different
color). The user moves with the mouse from
one connective to the next and
? can with a mouseclick discard a highlighted
item (it is not a connective or not a modi-
fier),
? can call up a help window explaining the
syntactic behavior and the relations of this
connective,
? can call up a suggestion for the int-unit
(i.e., text portion is highlighted),
? can analogously call up a suggestion for the
ext-unit,
? can choose from a menu of the relations
associated with this connective.
A screenshot is given in Figure 4. The sugges-
tions for int-unit and ext-unit are made by Co-
nAno on the basis of the syntactic category of
Figure 3: Screenshot of ConAno
the connective; we use simple rules like ?search
up to the next comma? to find the likely int-
unit for a subjunctor, or ?search the preceding
two full-stops? to find the ext-unit for an adver-
bial (the preceding sentence). The suggestions
may be wrong; then the user discards them and
marks them with the mouse herself. The result
of this annotation phase is an XML file like the
(very short) one shown in Figure 2.
5 Overall annotation environment
A central design objective is to keep the envi-
ronment neutral with respect to the languages
of the text, the connectives to be annotated, and
the coherence relations associated with them.
Accordingly, the list of connectives is external
and read into ConAno upon startup. In our
case, we use an XSLT sheet to map our ?Dis-
course Marker Lexicon? (see below) to the input
format of ConAno. The text to be annotated
is expected in plain ASCII. When annotation is
complete, the result (or an intermediate result)
can be saved in our XML-format introduced in
section 3.2. Optionally, it can be exported to
the ?rs3? format developed by (O?Donnell, 1997)
for his RSTTool. This allows for a smooth tran-
rs3
ConAno
RSTTool
DiMLex
xslt raw text
text with connectives,
scopes (and relations)
rhetorical tree
text with full
Figure 4: Overview of annotation environment
sition to a tool for constructing complete rhetor-
ical trees. Rather than starting from scratch,
the RSTTool user can now open the file pro-
duced by ConAno, which amounts to a partial
rhetorical analysis of the text, and which the
user can now complete to a full tree.
Our Discourse Marker Lexikon ?DiMLex?
(Berger et al, 2002) assembles information on
140 German connectives, giving a range of syn-
tactic, semantic, and pragmatic features, in-
cluding the coherence relations along the lines of
(Mann, Thompson, 1988). They are encoded in
an application-neutral XML format (see Figure
5), which are mapped with XSLT sheets to vari-
ous NLP applications. Our new proposal here is
to use it also for interactive connective annota-
tion. Hence, we wrote an XSLT sheet that maps
DiMLex to a reduced list, where each connective
is associated with syntactic labels coordination,
subordination or adverb and <coh-relation>
entries for its potential relations ? see Figure
6 for DTD and 7 for an example. The, for
these purposes quite simple, syn value has been
mapped from the more complex classification in
DiMLex under kat (German for category). This
format is the input to ConAno.
As indicated above, we do not see the tran-
sition to RSTTool as a necessary step. Rather,
the intermediate result of connective/scope an-
notation is useful in its own right, as it encodes
those aspects of rhetorical structure that are in-
dependent of the chosen set of coherence rela-
tions and the conditions of assigning them.
6 Summary
With our work on German discourse connec-
tives, the structure of their argument units,
and the indicated rhetorical relations, we seek
a better understanding of underlying linguistic
issues on the one hand, and an easier way of
developing rhetorical structure-annotated cor-
pora for German texts on the other hand. For
<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl"
href="short_dictionary.xsl" ?>
<!DOCTYPE dictionary SYSTEM "dimlex.dtd">
<dictionary>
<entry id="41">
<orth phrasal="0">denn</orth>
<syn>
<kat>konj</kat>
<position>vorvorfeld</position>
<!-- . . . -->
</syn>
<semprag>
<relation>cause</relation>
<relation>explanation</relation>
<presupp>int-unit</presupp>
<!-- . . . -->
</semprag>
<example>
Das Konzert muss ausfallen,
*denn* die Saengerin ist erkrankt.
</example>
<example>
Die Blumen auf dem Balkon sind
erfroren, *denn* es hat heute
nacht Frost gegeben.
</example>
</entry>
</dictionary>
Figure 5: DiMLex extract
this purpose, we present an annotation envi-
ronment, including our ConAno tool, which
helps human annotators to mark discourse con-
nectives and their argument units by finding
possible connectives and making suggestions on
their estimated argument structure. We pointed
out several challenges in the connective annota-
tion process of German texts and introduced an
XML based annotation scheme to handle the
difficulties. For one thing, the results of this
step provide elobarate information about the
structure of German texts with respect to dis-
course connectives, but furthermore they can be
used as input to O?Donnell?s RST Tool, in or-
der to complete the annotation of the rhetorical
tree structure. The overall scenario is then one
of machine-assisted rhetorical structure anno-
tation. Since ConAno is based on an external
list of connectives (with associated syntactic la-
bels and relations), the tool is not dedicated to
one particular theory of discourse structure, let
alone to a specific set of relations. Furthermore,
it can in principle deal with texts in various lan-
guages (it just relies on string matching between
<?xml version=?1.0? encoding=?UTF-8??>
<!ELEMENT example (#PCDATA)>
<!ELEMENT coh-relation (#PCDATA)>
<!ELEMENT sem (example|coh-relation)*>
<!ELEMENT syn (sem)*>
<!ATTLIST syn
type CDATA #IMPLIED>
<!ELEMENT part (#PCDATA)>
<!ATTLIST part
type CDATA #IMPLIED>
<!ELEMENT orth (part)*>
<!ATTLIST orth
type CDATA #IMPLIED>
<!ELEMENT entry (syn|orth)*>
<!ATTLIST entry
id CDATA #IMPLIED>
<!ELEMENT conanolex (entry)*>
Figure 6: DTD for connectives in ConAno in-
put format
<entry id="116">
<orth type="cont">
<part type="single">wenn</part>
</orth>
<orth type="discont">
<part type="single">wenn</part>
<part type="single">dann</part>
</orth>
<syn type="subordination">
<sem>
<coh-relation>condition
</coh-relation>
<example>*Wenn* man auf den Knopf
drueckt, oeffnet sich die Tuer
von selbst.
</example>
<example>*Wenn* du mich fragst,
*dann* wuerde ich die Finger
davon lassen.
</example>
</sem>
</syn>
</entry>
Figure 7: Connective information in ConAno
input format
connectives in the list and in the text), but we
have so far used it only for German.
Acknowledgements
We thank the anonymous reviewers for their
constructive comments and suggestions for im-
proving the paper.
References
Asher, N. and Lascarides, A. 2003. Logics of
Conversation. Cambridge University Press.
Berger, D.; Reitter, D. and Stede, M. 2002.
XML/XSL in the Dictionary: The Case of
Discourse Markers. In: Proc. of the Coling
Workshop ?NLPXML-2002?, Tapei.
O?Donnell, M. 1997. RST-Tool: An RST Analy-
sis Tool. Proc. of the 6th European Workshop
on Natural Language Generation, Duisburg.
Mann, W. and Thompson, S. 1988. Rhetorical
Structure Theory: A Theory of Text Organi-
zation. TEXT 8(3), 243-281.
Marcu, D. 1997. The rhetorical parsing of nat-
ural language texts. Proc. of the 35th Annual
Conference of the ACL, 96-103.
Marcu, D.; Amorrortu, E. and Romera, M.
1999. Experiments in Constructing a Corpus
of Discourse Trees. In: Proc. of ACL Work-
shop ?Towards Standards and Tools for Dis-
course Tagging?, University of Maryland.
Miltsakaki, E.; Prasad, R.; Joshi, A. and Web-
ber, B. 2004. Annotating Discourse Connec-
tives and their Arguments. In: Proc. of the
HLT/NAACL Workshop ?Frontiers in Corpus
Annotation?, Boston.
Pasch, R; Brausse, U.; Breindl, E. and Wass-
ner, H. 2003. Handbuch der deutschen Kon-
nektoren. Berlin: deGruyter.
Schilder, F. 2002. Robust Discourse Parsing via
Discourse Markers, Topicality and Position.
Natural Language Engineering 8 (2/3).
Stede, M. 2004. The Potsdam Commentary
Corpus. In: Proc. of the ACL Workshop ?Dis-
course Annotation?, Barcelona.
Sumita, K.; Ono, K.; Chino, T.; Ukita, T.;
Amano, S. 1992. A discourse structure ana-
lyzer for Japanese text. Proc. of the Interna-
tional Conference on Fifth Generation Com-
puter Systems, 1133-1140.
Webber, B.; Knott, A.; Stone, M. and Joshi,
2003. A. Anaphora and Discourse Structure.
Computational Linguistics 29(4), 545-588.
  
Rhetorical Parsing with Underspecification and Forests 
Thomas Hanneforth 
 Dept. of Linguistics 
University of Potsdam 
P.O. Box 601553 
14415 Potsdam, Germany 
tom@ling.uni-
potsdam.de 
Silvan Heintze 
Dept. of Linguistics 
University of Potsdam 
P.O. Box 601553 
14415 Potsdam, Germany 
heintze@ling.uni-
potsdam.de 
Manfred Stede 
Dept. of Linguistics 
University of Potsdam 
P.O. Box 601553 
14415 Potsdam, Germany 
stede@ling.uni-
potsdam.de 
 
Abstract 
We combine a surface based approach to dis-
course parsing with an explicit rhetorical 
grammar in order to efficiently construct an 
underspecified representation of possible dis-
course structures. 
1 Introduction 
The task of rhetorical parsing, i.e., automatically de-
termining discourse structure, has been shown to be 
relevant, inter alia, for automatic summarization (e.g., 
Marcu, 2000). Not surprisingly, though, the task is very 
difficult. Previous approaches have thus emphasized the 
need for heuristic or probabilistic information in the 
process of finding the best or most likely rhetorical tree. 
As an alternative, we explore the idea of strictly 
separating ?high-confidence? information from hypo-
thetical reasoning and of working with underspecified 
trees as much as possible. We create a parse forest on 
the basis of surface cues found in the text. This forest 
can then be subject to further processing. Depending on 
the application, such further steps can either calculate 
the ?best? tree out of the forest or continue working 
with a set of structured hypotheses.  
Section 2 briefly summarizes our proposal on under-
specified rhetorical trees; section 3 introduces our 
grammar approach to text structure; section 4 compares 
this strategy to earlier work. 
2 Parse forests and underspecification  
We will illustrate the underspecification of ambiguities 
with the following example: 
?(1) Yesterday the delegates elected their new rep-
resentative by a narrow margin. Even though (2) Smith 
got only 234 votes, (3) he accepted the position. But (4) 
his predecessor was rather irritated by the results.? 
We take it that even though unambiguously marks a 
CONCESSION between the embedded clause (2, satellite) 
and the matrix clause (3, nucleus). For the purpose of 
illustration, we also assume that ?but? can only signal a 
bi-nuclear CONTRAST relation with the second nucleus 
(4); the span of the first nucleus is in this case ambigu-
ous (1-3 or 2-3). For linking (1) to the remaining mate-
rial, we suppose that either ELABORATION (with nucleus 
(1)) or SEQUENCE holds. Further relations are possible, 
which will add to the possibilities, but our points can be 
made with the situation as just described. 
Instead of enumerating all possible rhetorical trees 
for our example text, we use a parse forest representa-
tion which compactly encodes the different analysises. 
A parse forest is basically an attributed And-Or-graph 
with the properties of subtree sharing and containment 
of ambiguities. The first property means that a subtree, 
which plays different roles in some bigger structure, is 
represented only once. The second property ensures that 
two subtrees which have in common the same category 
and the same terminal yield, but which differ in the first 
step of a leftmost derivation are unified together. 
Fig. 1 shows a simplified parse forest for the exam-
ple text. 
 
Fig.1: Parse forest for the input text 
 
Subtree sharing is indicated by nodes (e.g. ?1?) 
which have several incoming edges. Containment of 
ambiguities is exemplified in fig. 1 by the upper left  
contrast node which represents a disjunctive hypothesis 
concerning the span of the relation.  
Reitter and Stede (to appear) developed an XML-
DTD scheme to represent such parse forests in XML 
notation. 
  
3 Discourse structure parsing 
In our approach, we combine a standard chunk parser 
which identifies the relevant units for discourse process-
ing with a feature-based grammar which builds larger 
rhetorical trees out of these chunks. The categories and 
features we use are summarized in table 1. 
 
Cat. Feat. 
 
Values Comment 
  RST-tree 
cat macro_seg, 
s, ip, pp, ? 
The category of the 
RST-tree: macro 
segments, phrases 
sentences etc. 
type ns,  
nn,  
term 
Type of RST-tree: 
nuc-sat, multi-
nuclear or terminal 
role nuc, sat Nucleus or satellite 
relation elaboration, 
contrast, 
cause, ? 
The relation which 
combines the 
daughters of the 
RST-tree. 
rst 
dp no_dp, 
but, al-
though,  
? 
The discourse par-
ticle triggering the 
relation, or no_dp, 
if absent. 
dp See 
above 
 Discourse particle 
chunk   Phrase or sentence 
punct   Punctuation 
Table 1: Grammar categories and features 
 
There are three groups of grammar rules: 
1. Rules combining chunks to terminal RST-trees  
2. Rules combining discourse particles and sentence 
fragments to non-primitives RST-trees 
3. Rules combining sentences or groups of sentences 
(so called macro segments) to non-primitive RST-
trees. 
 
An example for a rule in group 1 is the one which 
builds a terminal RST-tree of category mc (main clause) 
out of a discourse particle, and sentence fragment and a 
full stop (all examples are given in Prolog-style nota-
tion, with curly brackets indicating feature structures): 
(1) 
rst({cat:mc, dp:DP,  type:term}) ---> 
dp({cat:pav, dp:DP}), 
chunk({cat:ip}), 
punct({cat:fullstop}). 
 
Rules like this one are used to build terminal RST-
trees for sentences like (4) in our example text. 
The second group of rules is exemplified by a rule 
which combines two terminal RST-trees - a subordinate 
clause containing a conjunction like even though and 
another clause - to a hypotactic RST-tree: 
(2) 
rst({cat:mc, rel:concession, dp:no_dp, type:ns}) --->  
rst({cat:sc, dp:even_though, role:sat}), 
rst({cat:mc, dp:no_dp, role:nuc}). 
 
The macro segment building rules of the third group 
can be divided into two subclasses. The first class is 
constituted by rules which construct RST-trees on the 
basis of a relation that is triggered by a discourse parti-
cle. An example of this type is the possible contrast-
relation between segments 4 and 2-3 in (1), which is 
triggered by the discourse particle but. 
(3) 
rst({cat:macro_seg, rel:contrast, 
    dp:no_dp, type:ns}) ---> 
rst({cat:macro_seg, role:sat}), 
rst({cat:macro_seg, role:nuc, dp:but}). 
 
The other subclass contains rules which freely con-
struct branching RST-trees without the overt evidence 
of discourse particles. The relations which are typically 
involved here are SEQUENCE and ELABORATION. Rela-
tions which have in common the same type of nucleus-
satellite-configuration are unified into a single rule us-
ing the list-valued form of the relation-feature: 
(4) 
rst({cat:macro_seg, rel:[sequence,elaboration],  
     dp:no_dp, type:nn}) ---> 
rst({cat:macro_seg, role:nuc, dp:no_dp}), 
rst({cat:macro_seg, role:nuc, dp:no_dp}). 
 
Fig. 2 shows a parse tree which reflects one analysis 
of our example text. Note that the segments into which 
the input is broken usually smaller than sentences. 
 
Yesterday
the
delegates
elected
their
new
representative
by a
narrow
margin
chunk [cat:s]
.
punct
rst [cat:mc]
Even
though
dp [cat:kous]
Smith
got only
234
votes
chunk [cat:ip]
,
punct
rst [cat:sc]
he
accepted
the
position
chunk [cat:ip]
.
punct
rst [cat:mc]
2-7
concession
0-7
sequence
But
dp [cat:pav]
his
predecessor
was
rather
irritated
by the
results
chunk [cat:ip]
.
punct
rst [cat:mc]
0-10
contrast
 
Fig.2: Sample parse tree for the input text 
 
Rules like (4) ensure the robustness of the grammar 
as they can be used to combine partial structures with-
out any structure triggering discourse particles. 
  
Furthermore, rules of the kind shown in (4) are on 
the one hand necessary to produce all possible branch-
ing structure over a given sequence of terminal ele-
ments. On the other hand they introduce massive 
ambiguities into the grammar which causes the number 
of analyses to grow according to the Catalan numbers 
(cf. Aho and Ullman, 1972, p. 165). 
It is therefore crucial that during parsing the con-
struction of parse trees is strictly avoided because that 
would turn an otherwise polynomial parsing algorithm 
like chart parsing into an exponential one. Instead we 
incrementally build the parse forest mentioned in sec-
tion 2. This is done by assigning a unique id to each 
edge introduced into the chart and by storing the ids of 
the immediate daughters within the edge. After parsing 
the parse forest is constructed by partitioning the set of 
edges into equivalence classes. Two chart edges E1 and 
E2 are in the same equivalence class if they a) have 
identical start and end positions and b) the categories of 
E1 and E2 subsume each other. For the subsumption test 
it is necessary to ignore the role-feature, because this 
feature is an attribute of the parse forest edges and not 
of the parse forest nodes. 
Besides keeping the parsing algorithm polynomial it 
is of equal importance to keep the grammar constant 
low. For example, rule (4) which establishes a 
SEQUENCE/ELABORATION relation between two macro 
segments also connects two simple clauses (of category 
mc), a macro segment and a simple clause, or a simple 
clause and a macro segment. The standard move to 
avoid this kind of rule multiplication is to introduce an 
unary chain rule of the form  
rst({cat:macro_seg}) ---> rst({cat:mc}) 
which ensures the desired level shifting. 
Because of the inherent relational nature of RST trees 
this solution is blocked. Instead we use an inheritance 
hierarchy like that in fig. 3 and replace rule (4) with the 
following one, which is underspecified w.r.t to the cate-
gory feature. 
(5) 
rst({cat:macro_seg, rel:[sequence,elaboration],  
     dp:no_dp, type:nn}) ---> 
rst({cat:rst_tree, role:nuc, dp:no_dp}), 
rst({cat:rst_tree, role:nuc, dp:no_dp}). 
 
segment
rst_tree
mc macro_seg
non_rst_tree
pp sc  
Fig 3: Simplified inheritance hierarchy for cat  
4 Related work 
Similar to Marcu (2000) we assume discourse markers 
as indicators for rhetorical relations. 
But contrary to Marcu (1999) and also to Schilder 
(2002) we use a full-fledged discourse grammar and a 
standard parsing algorithm, which makes it, in our opin-
ion, unnecessary to propose special rhetorical tree build-
ing operations, as suggested e.g. by Marcu (1999). 
By using the chart parsing algorithm combined with 
the construction of an underspecified parse forest, it can 
easily be shown that our method is of cubic complexity. 
This is a crucial property, because it is commonly as-
sumed that the number of distinct structures that can be 
constructed over a sequence of n discourse units is ex-
ponential in n, (as it is for example implicit in the DCG 
based algorithm proposed by Schilder, 2002). 
Our system is robust in the same way as the one in 
Schilder (2002) because the grammar admits under-
specified rhetorical trees in the absence of overt dis-
course markers. 
5 Conclusion 
We have shown that a grammar based approach to rhe-
torical parsing is suitable for efficient and robust con-
struction of underspecified rhetorical structures. 
References 
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory 
of Parsing, Translating and Compiling. Volume 1. 
Prentice-Hall, Englewood Cliffs, NJ. 
Daniel Marcu. 1999. A decision-based approach to rhe-
torical parsing. The 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'99), 
pages 365-372, Maryland, June 1999.  
Daniel Marcu. 2000. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-Based Approach. Computa-
tional Linguistics, 26 (3), pages 395-448. 
David Reitter and Manfred Stede. to appear. Step by 
Step: Underspecified Markup in Incremental Rhe-
torical Analysis. To appear in: Proc. Of the 4th Inter-
national Workshop on Linguistically Interpreted 
Corpora (LINC-03). Budapest. 
Frank Schilder. 2002. Robust Discourse Parsing via 
Discourse Markers, Topicality and Position. Natural 
Language Engineering 8 (2/3). 
XML/XSL in the Dictionary: The Case of Discourse Markers
Daniela Berger and David Reitter and Manfred Stede
University of Potsdam
Dept. of Linguistics / Applied Computational Linguistics
P.O. Box 601553 / D-14415 Potsdam / Germany
{berger|reitter|stede}@ling.uni-potsdam.de
Abstract
We describe our ongoing work on an application
of XML/XSL technology to a dictionary, from
whose source representation various views for
the human reader as well as for automatic text
generation and understanding are derived. Our
case study is a dictionary of discourse markers,
the words (often, but not always, conjunctions)
that signal the presence of a disocurse relation
between adjacent spans of text.
1 Overview
Electronic dictionaries have made extensive use
of SGML encoding in the past, but to our
knowledge, the advantages of contemporary
frameworks such as XML/XSL are only be-
ginning to be explored. We are applying this
framework to our work on a lexicon of ?dis-
course markers? and will outline the advantages
of deriving a variety of views from the common
underlying lexical resource: different views for
different demands by the human eye, but also
application-specific views that tailor the dictio-
nary to either the parsing or the generation task
(a third one would be machine translation but
is not covered in this paper), and that respect
the conventions of the specific underlying pro-
gramming language. Using XSL style sheets for
producing the views automatically is especially
useful when the lexicon is still under develop-
ment: the ramifications of particular modifica-
tions or extensions can be made visible easily
by running the conversion and testing the ap-
plications in question.
Discourse markers are words (predominantly
conjunctions) that signal the kind of semantic or
rhetorical relationship between adjacent spans
of text. In text generation, when given a rep-
resentation of propositions and relations hold-
ing between them, the task is to select an ap-
propriate discourse marker in the context. In
text understanding, discourse markers are the
most important clues for inferring the ?rhetori-
cal structure? of the text, a task that has lately
been called ?rhetorical parsing?. While these
discourse markers are a somewhat idiosyncratic
class of lexical items, we believe that our general
approach to applying XML/XSL can be fruitful
to other branches of the dictionary as well (in
particular to the open-class ?content words?).
After reviewing some earlier work on XML-
based dictionaries (Section 2) and discussing the
notion of discourse markers (Section 3), we pro-
ceed to outline the particular requirements on
a discourse marker lexicon from both the text
generation and the text understanding perspec-
tive (Section 4). Then, Section 5 describes our
XML/XSL encoding of the source lexicon and
the views for the human eye, for automatic text
generation, and for text understanding. Finally,
Section 6 draws some conclusions.
2 Dictionaries and XML: Related
work
Recent research in lexicology has been focused
on two different goals: the mark-up process
for existing print dictionaries, and the success-
ful construction of machine-readable dictionar-
ies from scratch.
The first approach has received more at-
tention in the past. This is partly due to
the fact that the transformation of existing
print dictionaries into modules for NLP applica-
tions promises to be less time-consuming than
the construction of a new machine-readable
database. Lexicologists agree on the fact that a
dictionary entry is inherently hierarchical, i.e.,
it consists out of atomic elements grouped to-
gether within non-atomic elements in a tree-like
hierarchy. Many approaches place orthograph-
ical and phonological information together in
one group, while grammatical information is put
in a different group. This hierarchical approach
also allows to denote scope by inserting informa-
tion at different levels of the hierarchy. Again,
information about orthography and phonology
generally applies to every facet of the headword
and are thus placed high in the hierarchy, while
other information might only apply to single
definitions and thus ranks lower hierarchically
(Amsler/Tompa, 1988; Ide, Ve?ronis, 1995; Ide
et al, 2000).
A common problem of lexicologists working
with print dictionaries is the fact that there is
a certain variation between entries in any two
given dictionaries or even within the same dic-
tionary. This results in a neccessary trade-off
between the descriptive power and the gener-
ality of an approach, i.e. to design a SGML
application that is both descriptive enough to
be of practical value and general enough to ac-
comodate the variation.
There has been, on the other hand, only little
research on machine-readable dictionaries that
are not based on print dictionaries. To our
knowledge, only (Ide et al, 1993) deals with
this issue by reviewing several approaches to-
wards encoding machine-readable dictionaries.
One of these is the use of text models that ap-
ply a rather flat hierarchy to mark up dictio-
nary entries. These text models might chiefly
use typographical or grammatical information.
Another approach is using relational databases,
in which the information contained in a dictio-
nary entry is distributed over several databases.
A third approach is based on feature structures
that impose a rich hierarchical structure on the
data. The authors finally describe an example
application that uses feature structures encoded
in SGML to set up a machine-readable dictio-
nary.
The papers mentioned above agree on us-
ing SGML for the mark-up. We found that
their SGML code is, however, in general XML-
compliant.
3 Discourse markers
Several contemporary discourse theories posit
that important aspects of a text?s coherence
can be formally described (and represented) by
means of discourse relations holding between
adjacent spans of text (e.g. Asher, 1993; Mann,
Thompson, 1988). We use the term discourse
marker for those lexical items that (in addition
to non-lexical means such as punctuation, as-
pectual and focus shifts, etc.) can signal the
presence of such a relation at the linguistic sur-
face. Typically, a discourse relation is associ-
ated with a wide range of such markers; con-
sider, for instance, the following variety of Con-
cessions, which all express the same underly-
ing propositional content. The words that we
treat as discourse markers are underlined.
We were in SoHo; {nevertheless | nonetheless
| however | still | yet}, we found a cheap bar.
We were in SoHo, but we found a cheap bar
anyway.
Despite the fact that we were in SoHo, we
found a cheap bar.
Notwithstanding the fact that we were in
SoHo, we found a cheap bar.
Although we were in SoHo, we found a cheap
bar.
If one accepts these sentences as paraphrases,
then the various discourse markers all need to
be associated with the information that they
signal a concessive relationship between the two
propositions involved. Notice that the markers
belong to different syntactic categories and thus
impose quite different syntactic constraints on
their environment in the sentence. Discourse
markers do not form a homogeneous class from
the syntactican?s viewpoint, but from a func-
tional perspective they should nonetheless be
treated as alternatives in a paradigmatic choice.
A detailled characterization of discourse
markers, together with a test procedure for
identifying them in text, has been provided for
English by (Knott, 1996). Recently, (Grote, to
appear) adapted Knott?s procedure for the Ger-
man language. Very briefly, to identify a dis-
course marker (e.g., because) in a text, isolate
the clause containing a candidate from the text,
resolve any anaphors and make elided items ex-
plicit; if the resulting text is incomplete (e.g.,
because the woman bought a Macintosh), then
the candidate is indeed a ?relational phrase?, or
for our purposes, a two-place discourse marker.
In addition to the syntactic features, the dif-
ferences in meaning and style between similar
markers need to be discerned; one such differ-
ence is the degree of specificity: for example,
but can mark a general Contrast or a more
specific Concession. Another one is the no-
table difference in formality between, say but ...
anyway and notwithstanding.
From the perspective of text generation, not
all paraphrases listed above are equally felici-
tous in specific contexts. In order to choose
the most appropriate variant, a generator needs
knowledge about the fine-grained differences be-
tween similar markers for the same relation.
Furthermore, it needs to account for the interac-
tions between marker choice and other genera-
tion decisions and hence needs knowledge about
the syntagmatic constraints associated with dif-
ferent markers. We will discuss this perspective
in Section 4.1
From the perspective of text understanding,
discourse markers can be used as one source of
information for guessing the rhetorical structure
of a text, or automatic rhetorical parsing. We
will characterize this application in Section 4.2.
4 Requirements on a discourse
marker lexicon
As the following two subsections will show, text
generation and understanding have quite dif-
ferent preferences on the information coded in
a discourse marker lexicon, or ?DiMLex? for
short. In addition, different systems employ dif-
ferent programming languages, and the format
of the lexicon has to be adapted accordingly.
Yet we want to avoid coding different lexicons
manually and thus seek a common ?core rep-
resentation? for DiMLex from which the var-
ious application-specific instantiations can be
derived. Before proposing such a representa-
tion, though, we have to examine in more detail
the different requirements.
4.1 The text generation perspective
Present text generation systems are typically
not very good at choosing discourse mark-
ers. Even though a few systems have incor-
porated some more sophisticated mappings for
specific relations (e.g., in DRAFTER (Paris et
al., 1995)), there is still a general tendency to
treat discourse marker selection as a task to
be performed as a ?side effect? by the gram-
mar, much like for other function words such as
prepositions.
To improve this situation, we propose to view
discourse marker selection as one subtask of the
general lexical choice process, so that ? to con-
tinue the example given above ? one or another
form of Concession can be produced in the
light of the specific utterance parameters and
the context. Obviously, marker selection also
includes the decision whether to use any marker
at all or leave the relation implicit. When these
decisions can be systematically controlled, the
text can be tailored much better to the specific
goals of the generation process.
The generation task imposes a particular view
of the information coded in DiMLex: the en-
try point to the lexicon is the discourse relation
to be realized, and the lookup yields the range
of alternatives. But many markers have more
semantic and pragmatic constraints associated
with them, which have to be verified in the gen-
erator?s input representation for the marker to
be a candidate. Then, discourse markers place
(predominantly syntactic) constraints on their
immediate context, which affects the interac-
tions between marker choice and other realiza-
tion decisions. And finally, markers that are still
equivalent after evaluating these constraints are
subject to a choice process that can utilize pref-
erential (e.g. stylistic or length-based) criteria.
Therefore, under the generation view, the infor-
mation in DiMLex is grouped into the following
three classes:
? Applicability conditions: The necessary
conditions for using a discourse marker, i.e., the
features or structural configurations that need
to be present in the input specification.
? Syntagmatic constraints: The constraints
regarding the combination of a marker and the
neighbouring constituents; most of them are
syntactic and appear at the beginning of the list
given above (part of speech, linear order, etc.).
? Paradigmatic features: Features that label
the differences between similar markers sharing
the same applicability conditions, such as stylis-
tic features and degrees of emphasis.
Very briefly, we see discourse marker choice
as one aspect of the sentence planning task
(e.g. (Wanner, Hovy, 1996)). In order to
account for the intricate interactions between
marker choice and other generation decisions,
the idea is to employ DiMLex as a declara-
tive resource supporting the sentence planning
process, which comprises determining sentence
boundaries and sentence structure, linear order-
ing of constituents (e.g. thematizations), and
lexical choice. All these decisions are heavily
interdependent, and in order to produce truly
adequate text, the various realization options
need to be weighted against each other (in con-
trast to a simple, fixed sequence of making the
types of decisions), which presupposes a flexible
computational mechanism based on resources
as declarative as possible. This generation ap-
proach is described in more detail in (Grote,
Stede, 1998).
4.2 The text understanding perspective
In text understanding, discourse markers serve
as cues for inferring the rhetorical or semantic
structure of the text. In the approach proposed
in (Marcu, 1997), for example, the presence of
discourse markers is used to hypothesize indi-
vidual textual units and relations holding be-
tween them. Then, the overall discourse struc-
ture tree is built using constraint satisfaction
techniques. Our analysis method uses the lexi-
con for an initial identification and disambigua-
tion of discourse markers. They serve as one
of several other shallow features that determine
through a statistical, learned language model
the optimal rhetorical analysis.
In contrast to the use of markers in genera-
tion, the list of cues is significantly longer and
includes phrasal items like aus diesem Grund
(for this reason) or genauer genommen (more
precisely).
5 Our XML/XSL solution
In the following we show some sample represen-
tations and style sheets that have been abridged
for presentation purposes.
5.1 Source representation
In our hierarchical XML structure, the
<dictionary> root tag encloses the entire file,
and every single entry rests in an <entry>
tag, which unambigously identifies every entry
with its id attribute. Within the <entry> tag
there are four subordinate tags: <form>, <syn>,
<sem>, and <examples>.
The <form> tag contains the orthographic
form of the headword; at present this amounts
to two slots for alternative orthographies. The
<syn> area contains the syntactic information
about the headword. In this shortened exam-
ple, there is only the <init field> tag present,
<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl"
href="short_dictionary.xsl" ?>
<!DOCTYPE dictionary SYSTEM "DTD.dtd">
5 <dictionary>
<entry id="05">
<form>
<orth>denn</orth>
<alt_orth>none</alt_orth>
10 <!-- . . . -->
</form>
<syn>
<init_field>-</init_field>
<!-- . . . -->
15 </syn>
<sem>
<function>causal</function>
<!-- . . . -->
</sem>
20 <examples>
<example>Das Konzert muss ausfallen,
*denn* die S&auml;ngerin ist erkrankt.
</example>
<example>Die Blumen auf dem Balkon sind
25 erfroren, *denn* es hat heute nacht
Frost gegeben.</example>
</examples>
</entry>
<entry>
30 <!-- more entries -->
</entry>
</dictionary>
Figure 1: The XML structure
which shows whether the headword can be used
in the initial field of a sentence. Correspond-
ingly, <sem> contains semantic features such as
the <function> tag, which contains the seman-
tic/discourse relation expressed by the head-
word. Finally, <examples>, contains one or
more <example> tags that may each give an ex-
ample sentence.
We have shortened this presentation consider-
ably; the full lexicon contains more fine-grained
features for all three areas: within <form>, in-
formation on pronounciation, syllable structure,
and hyphenation; within <syn>, information
about syntactic subcategorization and possible
positions in the clause; within <sem>, for exam-
ple the feature whether the information subor-
dinated by the marker is presupposed or not.
5.2 HTML views
The listing in Figure 4 shows a style sheet that
provides an HTML by listing the XML data in
a format that roughly resembles a print dictio-
nary. Figure 2 shows the output that results
from applying this XSL file to the XML source
in figure 1.
05: denn
Occurrences: middle field / Nullstelle
Semantics: kausal
Related markers: weil da
Examples: Das Konzert muss ausfallen, *denn* die
Sa?ngerin ist erkrankt.
Die Blumen auf dem Balkon sind erfroren, *denn* es hat
heute nacht Frost gegeben.
Figure 2: One HTML view of the data
We assume that the general structure of the
formatting part of XSL is familiar to the reader.
We would like to highlight some details.
XLINK is used to ensure that the entry con-
tains an HTML-anchor named after the head-
word (ll. 14-20). This way it is possible to link
to a certain entry from the <rel> tag of a dif-
ferent entry (39-45).
We also employ the XSL equivalent to
an if/then/else construct (24-31). The
<xsl:choose> tag encloses the choices to be
made. The <xsl:when> tag contains the con-
dition match=".[alt orth=?none?]" that does
nothing if the <alt orth> tag contains the
data none. Every other case is covered by
the <xsl:otherwise> tag that prints out the
<alt orth> information if it is not no entry.
Entry alt orth init field mid field . . .
denn none - + . . .
da none + + . . .
zumal none - - . . .
weil none - - . . .
als none - - . . .
Figure 3: Another possible HTML view of the
data
Figure 3 shows another possible view for the
data. In this case the data is presented in table
form, ordered by the value of the mid field tag.
It would be easy to show that it is possible to
use a <xsl:choose> construct as shown in the
example before to print out only those entries
that satisfy a certain condition.
5.3 The text generation view
For the lexicon to be applied in our text
generation system ?Polibox? (Stede, 2002),
we need a Lisp-based version of DiM-
Lex. Using the (defstruct <name> <slot1>
<?xml version="1.0"?>
<xsl:stylesheet
xmlns:xsl=
"http://www.w3.org/1999/XSL/Transform">
5 <xsl:template match="/">
<FONT SIZE="-2">
<xsl:apply-templates/>
</FONT>
</xsl:template>
10 <xsl:template match="dictionary">
<xsl:apply-templates/>
</xsl:template>
<xsl:template match="entry">
<P><font size="2"><B><A>
15 <xsl:attribute name="NAME">
<xsl:value-of select="form/orth"/>
</xsl:attribute>
<xsl:value-of select="./@id"/>:
<xsl:value-of select="form/orth"/>
20 </A></b></font>
<xsl:apply-templates/></P>
</xsl:template>
<xsl:template match="form">
<xsl:choose>
25 <xsl:when match=".[alt_orth=?none?]">
</xsl:when>
<xsl:otherwise>
<BR/><B>Alternative orthography:</B>
<xsl:value-of select="alt_orth"/>
30 </xsl:otherwise>
</xsl:choose>
</xsl:template>
<xsl:template match="sem">
<BR/><B>Semantics:</B>
35 <xsl:value-of select="ko_sub"/>
/ <xsl:value-of select="function"/>
<br/><b>Related markers:</b>
<xsl:for-each select="rel">
<A><xsl:attribute name="HREF">
40 #<xsl:value-of select="."/>
</xsl:attribute>
<xsl:value-of select="."/></A>
</xsl:for-each>
</xsl:template>
45 <xsl:template match="syn">
<BR/><B>Occurrences:</B>
<xsl:choose>
<xsl:when match=".[init\_field=?-?]">
</xsl:when>
50 <xsl:otherwise>
initial field /
</xsl:otherwise>
</xsl:choose>
</xsl:template>
55 <xsl:template match="examples">
<BR/><B>Examples:</B>
<xsl:for-each select="example">
<xsl:value-of select="."/><BR/>
</xsl:for-each>
60 </xsl:template>
</xsl:stylesheet>
Figure 4: The XSL file for the HTML view
shown in Figure 2
.. <slotn>) construct, we define a class of ob-
jects for discourse markers, where the features
needed for generation are stored in the slots.
Again, we abbreviate slightly:
(defstruct DiscMarker
Relation N-Complexity S-Complexity
Ortho POS ... Style)
Now, a Lisp-object for each individual dis-
course marker entry is created with the func-
tion make-Discmarker, which provides the val-
ues for the slots. Figure 5 shows the shape of the
entry for denn, whose XML-source was given in
figure 1.
Again, we aim at deriving these entries au-
tomatically via an XSL sheet (which we do not
show here). Notice that the mapping task is
now somewhat different from the HTML cases,
since the transformation part of XSL (XSLT)
comes into play here. Instead of merely display-
ing the data in a web browser as in the examples
before, an XSLT processor may transform data
for use in some XML based client application.
As explained in Section 4.1, in the generation
scenario we are given a tree fragment consist-
ing of a discourse relation node and two daugh-
ters representing the related material, the nu-
cleus and the satellite of the relation. In order
to decide whether a particular marker can be
used, one important constraint is the ?size? of
the daughter material, which can be a single
proposition or an entire sub-tree. The gener-
ator needs to estimate whether it will fit into
a single phrase, clause, sentence, or into a se-
quence of sentences; a subordinating conjunc-
tion, for instance, can only be used if the ma-
terial can be expressed within a clause. Thus,
the Lisp-entry contains slots N-Complexity and
S-Complexity, which are highly application-
specific and thus do not have a simple corre-
sponding feature in the XML source represen-
tation of the dictionary. The XSL sheet thus
inspects certain combinations of daughter at-
tributes of <syn> and maps them to new names
for the fillers of the two Complexity slots in
the Lisp structure. (Similar mappings occur in
other places as well, which we do not show here.)
5.4 The text understanding view
Our analysis method recasts rhetorical parsing
as a set of classification decisions, where a pars-
(make-Discmarker
:Relation cause
:N-Complexity sent
:S-Complexity sent
5 :Ortho denn
:POS coordconj
:Style unmarked)
Figure 5: Lisp-version of generation-oriented
dictionary entry for denn (abridged)
ing framework builds a tree structured analy-
sis. Each of the decisions is based on a set of
features. Feature types range from syntactical
configuration to the presence of a certain dis-
course marker. The mapping from a pattern of
observed features to a rhetorical relation may be
learned automatically by a classification learn-
ing algorithm.
Learning and analysis applications use a pars-
ing framework that gives us a set of text span
pairs. Every two text spans are subject to a
classification learning algorithm (during train-
ing) or the actual classifier. So, a rhetorical rela-
tion is assigned to these two spans of text along
with a score so that the parsing framework may
decide which of several competing classifications
to accept.
Learning and actual rhetorical analysis are
accomplished by a set of distinct tools that add
specific annotations to a given input text, be-
fore resulting relations are learned or guessed.
These tools include a data mining component, a
part-of-speech tagger and a segmenter. They all
access data organized in an XML syntax. The
central learning and parsing application makes
use of a Document Object Model (DOM) repre-
sentation of the corpus. This data structure is
effectively used for information interchange be-
tween several components, because it allows us
to easily visualize and modify the current data
at each processing step during development.
With the present corpus data, the learning al-
gorithm is theoretically able to identify rhetori-
cal markers automatically and could thus com-
pile a marker lexicon. However, markers are
highly ambiguous. Even though many of them
can be tagged as adverbials or conjunctions,
markers often lack distinctive syntactic and/or
positional properties; some of them are phrasal,
some are discontinuous. To identify significant
cue - relation correlations, a lot of annotated
data is necessary: more than is usually avail-
able. In a sparse data situation, we want to
easen the learning task for the rhetorical lan-
guage model: It makes sense to use a discourse
marker lexicon.
On the other hand, we do not expect a hand-
crafted lexicon to contain all contextual con-
straints that would enable us to assign a sin-
gle rhetorical relation. These constraints can be
very subtle; some of them should be represented
as probabilistic scalar information.
Thus, DiMLex contributes to initial dis-
course marker disambiguation. From each en-
try, we interpret syntactic positioning informa-
tion, morphosyntactic contextual information
and a scope class (sentential, phrasal, discourse-
level) as a conjunction of constraints. The pres-
ence of a certain discourse marker in a specified
configuration is one of the features to be ob-
served in the text.
Depending on the depth of the syntactic and
semantic analysis carried out by the text un-
derstanding system, different features provided
by DiMLex can be taken into account. Certain
structural configurations can be tested with-
out any deep understanding; for instance, the
German marker wa?hrend is generally ambigu-
ous between a Contrast and a Temporal-
Cooccurrence reading, but when followed by
a noun phrase, only the latter reading is avail-
able (wa?hrend corresponds not only to the En-
glish while but also to during).
In the parsing client application, DiMLex
serves as resource for the identification of cue
phrases in specific structural configurations.
Rhetorical information from the DiMLex entries
may serve as one of several cues for the classi-
fication engine. The final linking from cue pat-
terns to rhetorical relations is learned from a
corpus annotated with rhetorical structures.
6 Summary
We have presented our ongoing work on con-
structing an XML-based dictionary of discourse
markers, from which a variety of views are de-
rived by XSL sheets: For the dictionary de-
signer or application developer, we present the
dictionary in tabular form or in a form resem-
bling print dictionaries, but with hyperlinks in-
cluded for easy cross-referencing. Similarly, text
generation and understanding systems are on
the one hand written in different programming
languages and thus expect different dictionary
formats; on the other hand, the information
needed for generation and parsing is also not
identical, which is accounted for by the XSL
sheets. Evaluation of the approach will depend
on the client applications. Their implementa-
tion will determine the final shape of DiMLex.
References
R.A. Amsler and F.W. Tompa, ?An SGML-
Based Standard for English Monolingual Dic-
tionaries.? In: Information in Text: Fourth
Annual Conference of the UW Center for the
New Oxford English Dictionary, University of
Waterloo Center for the New Oxford English
Dictionary, Waterloo, Ontario, 1988, pp. 61-
79.
N. Asher. Reference to Abstract Objects in Dis-
course. Dordrecht: Kluwer, 1993.
B. Grote, M. Stede. ?Discourse marker choice
in sentence planning.? In: Proceedings of
the Ninth International Workshop on Natural
Language Generation, Niagara-on-the-Lake,
Canada, 1998.
B. Grote. ?Signalling coherence relations: tem-
poral markers and their role in text genera-
tion.? Doctoral dissertation, Universita?t Bre-
men, forthcoming.
N. M. Ide, J. Le Maitre, and J. Ve?ronis, ?Out-
line of a Model for Lexical Databases.? In-
formation Processing and Management, 29, 2
(1993), 159-186.
N. M. Ide and J. Ve?ronis. Encoding Dictionar-
ies.Computers and the Humanities, 29:167-
180, 1995
N. M. Ide, Kilgarriff, A., and Romary, L.A For-
mal Model of Dictionary Structure and Con-
tent. In Proceedings of EURALEX 2000, pp.
113-126, Stuttgart.
A. Knott. ?A data-driven methodology for mo-
tivating a set of coherence relations.? Doc-
toral dissertation, University of Edinburgh,
1996.
W. Mann, S. Thompson. ?Rhetorical structure
theory: Towards a functional theory of text
organization.? In: TEXT, 8:243-281, 1988.
D. Marcu. ?The Rhetorical Parsing of Un-
restricted Natural Language Texts.?
Proceedings of the Thirty-Fifth Annual
Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of
the European Chapter of the Association for
Computational Linguistics, Somerset, New
Jersey, 1997.
C. Paris, K. Van der Linden, M. Fischer, A.
Hartley, L. Pemberton, R. Power and D.R.
Scott, ?Drafter: A Drafting Tool for Produc-
ing Multilingual Instructions?, Proceedings of
the 5th European Workshop on Natural Lan-
guage Generation, pp. 239-242, Leiden, the
Netherlands, 1995.
M. Stede. ?Polibox: Generating, descriptions,
comparisons, and recommendations from a
database.? In Proceedings of COLING-2002.
L. Wanner, E. Hovy. ?The HealthDoc Sentence
Planner.? Proceedings of the 8th Interna-
tional Workshop on Natural Language Gen-
eration, Hearstmonceux Castle, 1996.
Web References
Domain Object Model
W3C Recommendation, 13 November 2000
http://www.w3.org/TR/DOM-Level-2-Core
Extensible Stylesheet Language (XSL) 1.0
W3C Recommendation, 15 October 2001
http://www.w3.org/TR/xsl
XML Base
W3C Recommendation 27 June 2001
http://www.w3.org/TR/xmlbase
XSL Transformations (XSLT) 1.0
W3C Recommendation, 16 November 1999
http://www.w3.org/TR/xslt
Surfaces and Depths in Text Understanding:
The Case of Newspaper Commentary
Manfred Stede
University of Potsdam
Dept. of Linguistics
Applied Computational Linguistics
D-14415 Potsdam
Germany
stede@ling.uni-potsdam.de
Abstract
Using a specific example of a newspaper com-
mentary, the paper explores the relationship
between ?surface-oriented? and ?deep? analysis
for purposes such as text summarization. The
discussion is followed by a description of our
ongoing work on automatic commentary un-
derstanding and the current state of the imple-
mentation.
1 Introduction
Generally speaking, language understanding for some
cognitive agent means reconstructing the presumed
speaker?s goals in communicating with her/him/it. An
application-specific automatic system might very well
hard-wire some or most of the aspects of this reconstruc-
tion process, but things get more interesting when the
complexity is acknowledged and paid attention to. When
moving from individual utterances to understanding con-
nected discourse, an additional problem arises: that of
partitioning the material into segments (usually at vari-
ous levels) and that of inferring the connections between
text segments (or between their underlying illocutions).
In recent years, some surface-based approaches to
?rhetorical parsing? have been proposed, which try to re-
cover a text?s discourse structure, following the general
layout of Rhetorical Structure Theory (Mann, Thompson,
1988). Starting from this idea, in this paper, we imag-
ine to push the goal of rhetorical parsing a bit further.
The idea is that of a system that can take a newspaper
commentary and understand it to the effect that it can,
amongst other things, produce the ?most concise sum-
mary? of it:
  the topic of the commentary
  the position the author is taking toward it
This goal does not seem reachable with methods of shal-
low analysis alone. But why exactly is it not, and what
methods are needed in addition? In the following, we
work through a sample commentary and analyse the steps
and the knowledge necessary to arrive at the desired re-
sult, i.e., a concise summary. Thereafter, we sketch the
state of our implementation work, which follows the goal
of fusing surface-based methods with knowledge-based
analysis.
2 Sample commentary
Figure 1 shows a sample newspaper commentary, taken
from the German regional daily ?Ma?rkische Allgemeine
Zeitung? in October 2002, along with an English trans-
lation. To ease reference, numbers have been inserted
in front of the sentences. Let us first move through the
text and make some clarifications so that the reader can
get the picture of what is going on. Dagmar Ziegler is
the treasury secretary of the German state of Branden-
burg. A plan for early retirement of teachers had been
drafted collectively by her and the education secretary,
whose name is Reiche. Sentence 5 points out that the plan
had intended education to be exempt from the cutbacks
happening all over the various ministries ? Reiche?s col-
leagues in 6 are thus the other secretaries. While the mid-
dle part of the text provides some motivation for the with-
drawal, 9-14 state that the plan nonetheless should be im-
plemented, for the reasons given in 10-12. Our intended
?most concise summary? then would be:
  Topic: Treasury secretary delays decision on teacher
staff plan
  Author?s opinion: Government has to decide quickly
and give priority to education, thus implement the
plan
Notice that a statistical summarization technique (i.e.,
a sentence extraction approach) is very unlikely to yield
(1) Dagmar Ziegler sitzt in der Schuldenfalle. (2) Auf Grund der dramatischen Kassenlage in Brandenburg hat sie
jetzt eine seit mehr als einem Jahr erarbeitete Kabinettsvorlage u?berraschend auf Eis gelegt und vorgeschlagen, erst
2003 daru?ber zu entscheiden. (3) ?Uberraschend, weil das Finanz- und das Bildungsressort das Lehrerpersonalkonzept
gemeinsam entwickelt hatten. (4) Der Ru?ckzieher der Finanzministerin ist aber versta?ndlich. (5) Es du?rfte derzeit
schwer zu vermitteln sein, weshalb ein Ressort pauschal von ku?nftigen Einsparungen ausgenommen werden soll -
auf Kosten der anderen. (6) Reiches Ministerkollegen werden mit Argusaugen daru?ber wachen, dass das Konzept
wasserdicht ist. (7) Tatsa?chlich gibt es noch etliche offene Fragen. (8) So ist etwa unklar, wer Abfindungen erhalten
soll, oder was passiert, wenn zu wenig Lehrer die Angebote des vorzeitigen Ausstiegs nutzen. (9) Dennoch gibt
es zu Reiches Personalpapier eigentlich keine Alternative. (10) Das Land hat ku?nftig zu wenig Arbeit fu?r zu viele
Pa?dagogen. (11) Und die Zeit dra?ngt. (12) Der gro?e Einbruch der Schu?lerzahlen an den weiterfu?hrenden Schulen
beginnt bereits im Herbst 2003. (13) Die Regierung muss sich entscheiden, und zwar schnell. (14) Entweder sparen
um jeden Preis - oder Priorita?t fuer die Bildung.
(1) Dagmar Ziegler is up to her neck in debt. (2) Due to the dramatic fiscal situation in Brandenburg she now sur-
prisingly withdrew legislation drafted more than a year ago, and suggested to decide on it not before 2003. (3)
Unexpectedly, because the ministries of treasury and education both had prepared the teacher plan together. (4) This
withdrawal by the treasury secretary is understandable, though. (5) It is difficult to motivate these days why one min-
istry should be exempt from cutbacks ? at the expense of the others. (6) Reiche?s colleagues will make sure that the
concept is waterproof. (7) Indeed there are several open issues. (8) For one thing, it is not clear who is to receive
settlements or what should happen in case not enough teachers accept the offer of early retirement. (9) Nonetheless
there is no alternative to Reiche?s plan. (10) The state in future has not enough work for its many teachers. (11) And
time is short. (12) The significant drop in number of pupils will begin in the fall of 2003. (13) The government has to
make a decision, and do it quickly. (14) Either save money at any cost - or give priority to education.
Figure 1: Sample text with translation
a result along these lines, because word frequency is of
little help in cases where the line of the argument has to
be pulled out of the text, and might make some synthesis
necessary. Just to illustrate the point, the Microsoft Word
?25 percent? summarization reads as follows:
?Uberraschend, weil das Finanz- und das
Bildungsressort das Lehrerpersonalkonzept
gemeinsam entwickelt hatten. Reiches Minis-
terkollegen werden mit Argusaugen daru?ber
wachen, dass das Konzept wasserdicht ist. En-
tweder sparen um jeden Preis - oder Priorita?t
fu?r die Bildung.
Unexpectedly, because the ministries of trea-
sury and education both had prepared the
teacher plan together. Reiche?s colleagues will
make sure that the concept is waterproof. Ei-
ther save money at any cost - or give priority to
education.
It includes the final sentence (most probably because it is
the final sentence), but in the context of the other two ex-
tracted sentences it does not convey the author?s position
? nor the precise problem under discussion.
3 Rhetorical Structure
Since RST (Mann, Thompson 1988) has been so in-
fluential in discourse-oriented computational linguistics,
we start our analysis with a ?man-made? RST anal-
ysis, which was produced collectively by two RST-
experienced students. See Figure 2.1 (The English reader
can relatively easy map the German segments to their
translations in Fig. 1 with the help of the sentence num-
bers added to the text in the tree).
Some considerations motivating this analysis (in terms
of segment numbers, not sentence numbers): 1 is seen
as the general Background for the satellite of the over-
all Concession, which discusses the problem arising from
the debt situation. Arguably, it might as well be treated
as Background to the entire text. The Evaluation between
2-6 and 7-12 is a relation often found in opinion texts; an
alternative to be considered here is Antithesis ? in this
case, however, 7-12 would have to be the nucleus, which
seems to be problematic in light of the situation that 3-4
is the main portion that is being related to the material in
13-16.
8-12 explains and elaborates the author?s opinion that
the withdrawal is understandable (7). The distinctions
between the relations Explanation, Elaboration, and Ev-
idence were mostly based on surface cues, such as
tatsa?chlich (?indeed?) signalling Evidence. The Elabora-
1Visualization by the RST Tool (O?Donnell, 1997). Nota-
tion follows Mann and Thompson (1988): vertical bars and in-
coming arrows denote nuclear segments, outgoing arrows de-
note satellites. Numbers at leaves are sentence numbers; seg-
ment numbers are given at internal nodes.
tions,
o
n
the
otherhand,tak
e
up
o
n
e
aspectfrom
the
pre-
vious
utterance
and
pro
vide
additionalinform
ation,
su
ch
as
the
tw
o
open
questionsin
10-12.
13-16
then
o
v
erw
rites
this
ackno
w
ledged
?u
nderstand-
ing?
ofZiegler?s
m
o
v
e
and
states
thatherplan
should
be
im
plem
ented
anyw
ay
,
and
thatthe
m
atteris
u
rgent.Itis
here
w
here
the
k
ern
el
ofthe
author?s
opinion
o
n
the
m
at-
teris
located(and
argued
forby
14-16).
The
finalpart
17-20
then
is
a
little
less
decisiv
e,
re-states
the
u
rgency
,
and
u
ses
a
?rhetorical
alternativ
e?in
19-20
to
indirectly
indicate
thatthe
plan
should
be
im
plem
ented,
education
be
giv
en
priority
.
Rhetorical
an
alysis
is
anything
b
ut
an
u
n
co
ntro
v
ersial
m
atter
.F
o
r
o
u
rpurposes,though,let
u
s
tak
e
theproposed
an
alysis
as
the
point
ofdeparture
for
subsequent
co
n
sid-
erations.
W
e
firsthav
e
to
ask
w
hether
su
ch
an
RST
tree
is
indeed
significant
and
u
sefulfor
the
goals
of
text
u
n
-
derstanding
as
o
utlined
in
Section
1
?
and
should
this
question
receiv
e
an
affirm
ativ
e
an
sw
er
,w
e
n
eed
to
turn
to
the
prospectsfor
autom
ating
the
an
alysis.
4
The
role
ofR
ST
treesin
text
u
nderstanding
D
oes
the
inform
ation
en
coded
in
Figure
2
m
ak
e
a
co
n
-
trib
ution
to
o
u
r
n
eeds?
Y
es,fortunately
itdoes.
First
of
all,in
v
estigating
the
lengths
of
the
linesbeginning
from
the
top,
w
e
n
otice
thattheRST
tree
co
ntains
a
u
seful
seg-
m
entation
of
the
text.Its
m
ain
co
n
stituents
are
segm
ents
1,2-6,7-12,13-16,and
17-20.N
ext,w
e
aregiv
en
a
set
of
central
n
u
clei
co
m
ing
from
these
co
n
stituents:3/4,7,13,
and
17.
Finally
,
w
e
find
the
m
o
st
ob
viousingredient
of
an
RST
an
alysis:
coherence
relations.W
hen
w
e
proceed
to
extractthe
relations
that
co
n
n
ect
o
u
r
m
ain
co
n
stituents
and
then
replace
each
co
n
stituent
w
ith(a
paraphrase
of)
its
central
n
u
cleus,w
e
areleft
w
ith
theRST
tree
sho
w
n
in
Figure
3.
This
tree,
assu
m
ing
thatit
also
determ
ines
the
linear
o
rder
ofthe
text
u
nits,can
be
v
erbalized
in
English
forinstancelik
e
this:
That
Ziegler
withdrew
the
legislation
o
n
teacher
staff
is
u
nderstandable;
n
o
n
etheless,
there
is
n
o
alternative
to
it.
The
Bra
ndenb
u
rg
go
vern
m
ent
m
u
st
m
ake
a
decision
n
o
w
.
This,it
seem
s,is
n
otbad
for
a
co
n
cise
su
m
m
ary
of
the
text.N
oticefurtherm
ore
that
additional
m
aterialfrom
the
o
riginaltree
can
be
added
to
the
extracted
tree
w
hen
de-
sired,
su
ch
as
the
reaso
n
for
actA
being
u
nderstandable
(incrementally
segm
ents8,9,10,11-12).
W
e
initially
co
n
clude
that
a
rhetorical
tree
seem
s
to
be
u
seful
as
a
backbone
for
a
text
representation,based
o
n
w
hich
w
e
can
perform
operations
su
ch
as
su
m
m
ariza-
tion.
W
hile
w
e
are
n
ot
the
first
to
point
this
o
ut(see,
e.g.,M
arcu
1999),
w
e
shall
n
o
w
m
o
v
e
o
n
to
ask
ho
w
o
n
e
13?20
17?20
17?18
(13) Die Regierung
muss sich
entscheiden,
und zwar schnell.
Elaboration
19?20
Elaboration
(14) Entweder
sparen um jeden
Preis ?
Disjunction
oder Prioritaet fuer
die Bildung.
13?16
Explanation
(9) Dennoch gibt
es zu Reiches
Personalpapier
eigentlich keine
Alternative.
14?16
Explanation
(10) Das Land hat
kuenftig zu wenig
Arbeit fuer zu viele
Paedagogen.
15?16
Elaboration
(11) Und die Zeit
draengt.
Explanation
1?12
Concession
2?12(1) Dagmar Ziegler
sitzt in der
Schuldenfalle.
Background
2?6
2?4
3?4(2) Auf Grund der
dramatischen
Kassenlage in
Brandenburg
Explanation
hat sie jetzt eine
seit mehr als einem
Jahr erarbeitete
Kabinettsvorlage
ueberraschend auf
Eis gelegt
Sequence
und
vorgeschlagen,
erst 2003 darueber
zu entscheiden.
5?6
Elaboration
weil das Finanz?
und das
Bildungsressort
das
Lehrerpersonalkon
zept gemeinsam
entwickelt hatten.
(3)
Ueberraschend,
Nonvolitional?result
7?12
Evaluation
(4) Der
Rueckzieher der
Finanzministerin ist
aber verstaendlich.
8?12
Explanation
(5) Es duerfte
derzeit schwer zu
vermitteln sein,
weshalb ein
Ressort pauschal
von kuenftigen
Einsparungen
ausgenommen
werden soll ? auf
Kosten der
anderen.
9?12
Elaboration
(6) Reiches
Ministerkollegen
werden mit
Argusaugen
darueber wachen,
dass das Konzept
wasserdicht ist.
10?12
Evidence
(7) Tatsaechlich
gibt es noch etliche
offene Fragen.
11?12
Elaboration
(8) So ist etwa
unklar, wer
Abfindungen
erhalten soll,
Disjunction
oder was passiert,
wenn zu wenig
Lehrer die
Angebote des
vorzeitigen
Ausstiegs nutzen.
(12) Der gro?e
Einbruch der
Schuelerzahlen an
den
weiterfuehrenden
Schulen beginnt
bereits im Herbst
2003.
1?20
Figure2:RST
tree
for
sam
ple
text
There is no
alternative to B.
Explanation
1?2
Concession
(action A =) Ziegler
withdrew (object B
=) legislation on
teacher staff.
A is
understandable.
Evaluation
Brandenburg
government must
now make decision
on B.
3?4
1?4
Figure 3: Desired ?summary tree? for sample text
would arrive at such a tree ? more specifically, at a for-
mal representation of it.
What kind of information is necessary beyond assign-
ing relations, spans and nuclei? In our representation of
the summary tree, we have implicitly assumed that refer-
ence resolution has been worked out - in particular that
the legislation can be identified in the satellite of the Ex-
planation, and also in its nucleus, where it figures implic-
itly as the object to be decided upon. Further, an RST tree
does not explicitly represent the topic of the discourse, as
we had asked for in the beginning. In our present exam-
ple, things happen to work out quite well, but in general,
an explicit topic identification step will be needed. And
finally, the rhetorical tree does not have information on
illocution types (1-place rhetorical relations, so to speak)
that distinguish reported facts (e.g., segments 3 and 4)
from author?s opinion (e.g., segment 7). We will return
to these issues in Section 6, but first consider the chances
for building up rhetorical trees automatically.
5 Prospects for Rhetorical Parsing
Major proponents of rhetorical parsing have been (Sumita
et al, 1992), (Corston-Oliver, 1998), (Marcu, 1997), and
(Schilder, 2002). All these approaches emphasise their
membership in the ?shallow analysis? family; they are
based solely on surface cues, none tries to work with
semantic / domain / world knowledge. (Corston-Oliver
and Schilder use some genre-specific heuristics for pref-
erential parsing, though.) In general, our sample text be-
longs to a rather ?friendly? genre for rhetorical parsing,
as commentaries are relatively rich in connectives, which
are the most important source of information for making
decisions ? but not the only one: Corston-Oliver, for
example, points out that certain linguistic features such
as modality can sometimes help disambiguating connec-
tives. Let us now hypothesize what an ?ideal? surface-
oriented rhetorical parser, equipped with a good lexicon
of connectives, part-of-speech tagger and some rough
rules of phrase composition, could do with our example
text.
5.1 Segmentation
As we are imagining an ?ideal? shallow analyser, it might
very well produce the segmentation that is underlying the
human analysis in Figure 2. The obvious first step is to
establish a segment boundary at every full stop that ter-
minates a sentence (no ambiguities in our text). Within
sentences, there are six additional segment boundaries,
which can be identified by considering connectives and
part-of-speech tags of surrounding words, i.e. by a vari-
ant of ?chunk parsing?: Auf Grund (?due to?) has to be
followed by an NP and establishes a segment up to the
finite verb (hat). The und (?and?) can be identified to
conjoin complete verb phrases and thus should trigger a
boundary. In the following sentence, weil (?because?) has
to be followed by a full clause, forming a segment. The
next intra-sentential break is between segments 11 and
12; the oder (?or?) can be identified like the und above. In
segment 17-18, und zwar (?and in particular?) is a strict
boundary marker, as is the entweder ? oder (?either ? or?)
construction in 19-20.
5.2 Relations, scopes, nuclei
The lexical boundary markers just mentioned also indi-
cate (classes of) rhetorical relationships. Auf Grund ?
when used in its idiomatic reading ? signals some kind
of Cause with the satellite following in an NP. Because
the und in 3-4 co-occurs with the temporal expressions
jetzt (?now?) and erst 2003 (?not before 2003?), it can be
taken as a signal of Sequence here, with the boundaries
clearly identifiable, so that the RST subtree 2-4 can be
derived fully. Furthermore, 5 takes up a single adver-
bial u?berraschend from 3, and in conjunction with the
weil-clause in 6, the Elaboration can be inferred. weil
(?because?) itself signals some Cause, but the nuclearity
decision (which in the ?real? tree in Fig. 2 leads to choos-
ing Result) is difficult here; since 5 merely repeats a lex-
eme from 3, we might assign nuclearity status to 6 on
the ?surface? grounds that it is longer and provides new
material. We thus have derived a rhetorical structure for
the entire span 2-6. In 7, aber (?but?) should be expected
to signal either Contrast or Concession; how far the left-
most span reaches can not be determined, though. Both 8
and 9 provide no reliable surface clues. In 10, tatsa?chlich
(?indeed?) can be taken as an adverbial indicating Evi-
dence; again the scope towards the left is not clear. So ..
etwa (?thus .. for instance?) in 11 marks an Elaboration,
and the oder in 12 a Disjunction between the two clauses.
Span 10-12 therefore receives an analysis. In 13, dennoch
(?nonetheless?) is a clear Concession signal, but its scope
cannot be reliably determined. Finally, the only two re-
maining decisions to be made from surface observations
are the Elaboration 17-18 (und zwar, ?and in particular?)
and the Disjunction 19-20. Then, making use of RST?s
?empty? relation Join, we can bind together the assem-
bled pieces and are left with the tree shown in Fig. 4.
Dagmar Ziegler
sitzt in der
Schuldenfalle.
Der Rueckzieher
der Finanzministerin
ist aber
verstaendlich.
2?6
Concession
2?4
3?4
hat sie jetzt eine
seit mehr als einem
Jahr erarbeitete
Kabinettsvorlage
ueberraschend auf
Eis gelegt
Sequence
und
vorgeschlagen,
erst 2003 darueber
zu entscheiden.
Auf Grund der
dramatischen
Kassenlage in
Brandenburg
Cause
5?6
Elaboration
weil das Finanz?
und das
Bildungsressort
das
Lehrerpersonalkon
zept gemeinsam
entwickelt hatten.
Ueberraschend,
Cause
2?7 9?12
Reiches
Ministerkollegen
werden mit
Argusaugen
darueber wachen,
dass das Konzept
wasserdicht ist.
10?12
Evidence
Tatsaechlich gibt es
noch etliche offene
Fragen.
11?12
Elaboration
Disjunction
oder was passiert,
wenn zu wenig
Lehrer die
Angebote des
vorzeitigen
Ausstiegs nutzen.
Es duerfte derzeit
schwer zu
vermitteln sein,
weshalb ein
Ressort pauschal
von kuenftigen
Einsparungen
ausgenommen
werden soll ? auf
Kosten der
anderen.
Dennoch gibt es zu
Reiches
Personalpapier
eigentlich keine
Alternative.
Das Land hat
kuenftig zu wenig
Arbeit fuer zu viele
Paedagogen.
Und die Zeit
draengt.
Der gro?e Einbruch
der Schuelerzahlen
an den
weiterfuehrenden
Schulen beginnt
bereits im Herbst
2003.
17?18
Die Regierung
muss sich
entscheiden,
und zwar schnell.
Elaboration
Joint
19?20
Entweder sparen
um jeden Preis ?
Disjunction
oder Prioritaet fuer
die Bildung.
So ist etwa unklar,
wer Abfindungen
erhalten soll,
1?20Figure4:R
esult
of
?su
rfaceparsing?
of
sam
ple
text
5.3
H
euristics
o
r
statistics
In
the
an
alysisjustproposed,
w
e
u
sed
lexicalkno
w
ledge
(connectiv
es
?
relations)
as
w
ell
as
so
m
e
linguistic
cu
es.
In
addition,
rhetoricalparsers
can
either
apply
dom
ain-
o
r
genre-specific
heuristics,
o
r
hypothesize
further
re-
lations
by
em
ploying
probabilistic
kno
w
ledge
gathered
from
training
w
ith
an
n
otated
co
rpora.
W
hat
can
be
ex
-
pected
to
be
gained
in
this
w
ay
for
o
u
r
sam
ple
text?
Since
the
u
n
an
alysed1
isfollo
w
edby
alarger
segm
ent,
w
e
m
ighthypothesize1
to
be
aBackground
forfollo
w
ing
m
aterial;thisis
certainly
co
m
m
o
n
in
co
m
m
entaries.The
satellite
ofContrast/Concession
to
theleft
of7
can
be
as-
su
m
ed
to
be
the
larger
segm
entpreceding
it;ho
w
far
the
n
u
cleus
stretches
to
the
rightis
difficult
to
see,
though.
Statistically
,it
w
illlik
ely
be
o
nly
segm
ent8.
The
situa-
tion
is
sim
ilar
w
ith
theConcession
hypothesized
at13
?
itis
so
m
ew
hatlik
ely(though
w
ro
ng
in
this
case!)
thatthe
n
u
cleus
w
illbe
o
nly
the
segm
enthosting
the
co
n
n
ectiv
e,
b
ut
aboutthe
satellite
span
n
othing
can
be
said
here.Fi-
n
ally
,atthe
v
ery
end
ofthe
co
m
m
entary
,aheuristic
m
ight
tellthatit
should
n
otterm
inate
w
ith
a
binucleardisjunc-
tion
as
a
prom
inent
n
u
cleus(such
a
co
m
m
entary
w
o
uld
probably
failto
m
ak
e
a
point),
and
hence
it
seem
s
advis-
able
to
treat19-20
as
a
satellite
of
a
larger
span
17-20,
and
a
?defensiv
e?
relation
guess
w
o
uld
be
Elaboration.
Returning
to
theissue
of
segm
entation,w
e
can
also
try
to
apply
su
rface-based
heuristic
m
ethodsto
finding
larger
segm
ents,i.e.,to
splitthe
textinto
its
m
ajorparts,
w
hich
has
so
m
etim
es
been
called
?text
tiling?.
F
o
rinstance,
a
boundary
betw
een
?m
acro
segm
ents?
13-16
and
17-20
is
hinted
atby
the
definite
N
P
D
ie
Regierung(?the
go
v
ern
-
m
ent?)
at
the
beginning
of17,
w
hich
has
n
o
antecedent
N
P
in
thepreceding
segm
entand
hence
can
beinterpreted
as
a
change
ofdiscourse
topic.
Such
co
n
siderations
can
be
u
n
reliable,though.
Schuldenfalle(?up
to
the
n
eck
in
debt?)
and
dra
m
atische
K
assenlag
e(?dramatic
fiscal
sit-
u
ation?)
seem
to
bind
1
and
2
closely
together
,
and
yet
there
is
a
m
ajor
segm
entboundary
in
o
u
rtree
in
Fig.2.
5.4
A
ssessm
ent
U
nder
the
assu
m
ption
that
o
u
rdiscussion
reaso
n
ably
re-
flects
the
state
of
the
art
in
su
rface-o
riented
an
alysis
m
ethods,
w
e
n
o
w
hav
e
to
co
m
pare
its
resultto
o
u
r
o
v
er
-
all
target,
the
su
m
m
ary
tree
in
Figure
3.
W
e
hav
e
su
c-
cessfully
found
segm
ent3-4
as
the
central
n
u
cleus
of
the
span
2-6,
and
w
e
hav
e
hypothesized
itbeing
related
to
7
(withoutidentifying
the
Ev
aluation
relation).
A
s
for
the
otherhalf
of
the
targettree,17
hasbeen
hypothesized
as
an
im
portant
n
u
cleus,b
ut
w
e
hav
e
n
o
clear
co
n
n
ection
to
13(its
target
satellite),
as
the
?staircase?
of
Elabora-
tions
and
Explanations13-16
co
uld
n
otbeidentified.N
or
co
uld
w
edeterm
ine
the
centralrole
oftheConcession
that
co
m
bines
the
k
ey
n
u
clei.
A
t
this
point,
w
e
can
draw
three
interm
ediate
co
n
clu-
sions. First, rhetorical parsing should allow for under-
specified representations as ? intermediate or final ?
outcome; see (Hanneforth et al, submitted). Second,
text understanding aiming at quality needs to go further
than surface-oriented rhetorical parsing. With the help
of additional domain/world-knowledge sources, attempts
should be made to fill gaps in the analysis. It is then
an implementation decision whether to fuse these addi-
tional processes into the rhetorical parser, or to use a
pipeline approach where the parser produces an under-
specified rhetorical tree that can afterwards be further en-
riched. Third, probabilistic or statistical knowledge can
also serve to fill gaps, but the information drawn from
such sources should be marked with its status being inse-
cure. As opposed to decisions based on lexical/linguistic
knowledge (in 5.2), the tentative decisions from 5.3 may
be overwritten by later knowledge-based processes.
6 Knowledge-Based Understanding
?Understanding a text? for some cognitive agent means to
fuse prior knowledge with information encountered in the
text. This process has ramifications for both sides: What
I know or believe influences what exactly it is that I ?take
away? from a text, and my knowledge and beliefs will
usually to a certain extent be affected by what I read. Nat-
urally, the process varies from agent to agent: They will
understand different portions of a text in different ways
and to different degrees. Thus, when we endeavour to
devise and implement models of text understanding, the
target should not be to arrive at ?the one and only? result,
but rather to account for the mechanics of this variability:
the mechanism of understanding should be the same, but
the result depend on the type and amount of prior knowl-
edge that the agent carries. In the end, a representation
of text meaning should therefore be designed to allow for
this flexibility.
6.1 KB Design
In line with many approaches to using knoweldge for
language processing, we adopt the framework of termi-
nological logic as the vehicle for representing both the
background knowledge necessary to bootstrap any under-
standing process, and the content of the text. Thus the ba-
sic idea is to encode prior, general knowledge in the TBox
(concepts) and the information from the text in the ABox
(instances). For our example, the subworld of govern-
ment, ministries and legislation has to be modelled in the
TBox, so that entities referred to in the text can instantiate
the appropriate concepts. We thus map the rhetorical tree
built up by shallow analysis to an ABox in the LOOM
language (MacGregor, Bates, 1987); for a sketch of rep-
resenting rhetorical structure in LOOM, see (Stede, 1999,
ch. 10).
6.2 ?Ideal? text understanding
Each leaf of the tree is now subject to detailled semantic
analysis and mapped to an enriched predicate/argument
structure that instantiates the relevant portions of the
TBox (quite similar to the ?Text Meaning Representation?
of (Mahesh, Nirenburg, 1996)). ?Enriched? indicates that
beyond the plain proposition, we need information such
as modality but also the type of illocution; e.g., does the
utterance represent a factual statement, the author?s opin-
ion, or a proposal? This is necessary for analyzing the
structure of an argument (but, of course, often it is very
difficult to determine).
One central task in text understanding is reference
resolution. Surface-based methods can perform initial
work here, but without some background knowledge,
the task can generally not be completed. In our sample
text, understanding the argument depends on recogniz-
ing that Kabinettsvorlage in (2), Lehrerpersonalkonzept
in (3), Konzept in (6), and Reiches Personalpapier in (9)
all refer to the same entity; that Ziegler in (1) and Fi-
nanzministerin in (4) are co-referent; that Finanz- und
Bildungsressort in (3), Reiches Ministerkollegen in (6),
and die Regierung in (13) refer to portions of or the com-
plete Brandenburg government, respectively. Once again,
hints can be derived from the surface words (e.g., by com-
pund analysis of Lehrerpersonalkonzept), but only back-
ground knowledge (an ontology) about the composition
of governments and their tasks enables the final decisions.
Knowledge-based inferences are necessary to infer
rhetorical relations such as Explanation or Evaluation.
Consider for example segment 15-16, where the rela-
tionship between ?time is short? (a subjective, evaluative
statement) and ?begin already in the fall of 2003? (a state-
ment of a fact), once recognized, prompts us to assign
Explanation. Similarly, the Elaboration between this seg-
ment and the preceeding 14 can be based on the fact that
14 makes a statement about the ?future situation? in Bran-
denburg, which is made more specific by time being short
and the fall of 2003. More complex inferences are nec-
essary to attach 14-16 then to 13 (and similarly in the
segment 7-12).
6.3 ?Realistic? text understanding
Even if it were possible to hand-code the knowledge base
such that for our present sample text the complete repre-
sentation can be constructed ? for the general text analy-
sis situation, achieving a performance anywhere near the
?complete and correct solution? is beyond reach. As in-
dicated at the beginning of the section, though, this is not
necessarily bad news, as a notion of partial understand-
ing, or ?mixed-depth encoding? as suggested by Hirst
and Ryan (1992), should be the rule rather than the ex-
ception. Under ideal circumstances, a clause at a leaf of
the rhetorical tree might be fully analyzed, with all refer-
ences resolved and no gaps remaining. In the worst case,
however, understanding might fail entirely. Then, follow-
ing Hirst and Ryan, the text portion itself should simply
be part of the representation. In most cases, the repre-
sentation will be somewhere in-between: some aspects
fully analyzed, but others not or incompletely understood.
For example, a sentence adverbial might be unknown and
thus the modality of the sentence not be determined. The
ABox then should reflect this partiality accordingly, and
allow for appropriate inferences on the different levels of
representation.
The notion of mixed depth is relevant not only for the
tree?s leaves: Sometimes, it might not be possible to de-
rive a unique rhetorical relation between two segments,
in which case a set of candidates can be given, or none
at all, or just an assignment of nucleus and satellite seg-
ments, if there are cues allowing to infer this. In (Reitter
and Stede, 2003) we suggest an XML-based format for
representing such underspecified rhetorical structures.
Projecting this onto the terminological logic scheme,
and adding the treatment of leaves, we need to provide
the TBox not only with concepts representing entities of
?the world? but also with those representing linguistic
objects, such as clause or noun group, and for the case
of unanalyzed material, string. To briefly elaborate the
noun group example, consider Reiches Ministerkollegen
(?Reiche?s colleagues?) in sentence 6. Shallow analysis
will identify Reiche as some proper name and thus the
two words as a noun group. An ABox istance of this
type is created, and it depends on the knowledge held by
the TBox whether additional types can be inferred. Re-
iche has not been mentioned before in the text, because
from the perspective auf the author the name is prominent
enough to be identified promptly by the (local) readers.
If the system?s TBox contains a person of that name in
the domain of the Brandenburg government, the link can
be made; otherwise, Reiche will be some un-identified
object about which the ABox collects some information
from the text.
Representations containing material with different de-
grees of analysis become useful when accompanied by
processes that are able to work with them (?mixed-depth
processing?). For summarization, this means that the task
becomes one of fusing extraction (of unanalyzed portions
that have been identified as important nuclei) with gener-
ation (from the representations of analyzed portions). Of
course, this can lead to errors such as dangling anaphors
in the extracted portions, but that is the price we pay for
robustness ? robustness in this refined sense of ?anal-
yse as deeply as you can? instead of the more common
?extract something rather than fail.?
7 Implementation Strategy
Finally, here is a brief sketch of the implementation work
that is under way in the Computational Linguistics group
at Potsdam University. Newspaper commentaries are
the genre of choice for most of our current work. We
have assembled a corpus of some 150 commentaries from
?Ma?rkische Allgemeine Zeitung?, annotated with rhetor-
ical relations, using the RST Tool by O?Donnell (1997).
It uses an XML format that we convert to our format
of underspecified rhetorical structure (?URML? Reitter &
Stede 2003).
This data, along with suitable retrieval tools, informs
our implementation work on automatic commentary un-
derstanding and generation. Focusing here on under-
standing, our first prototype (Hanneforth et al, submit-
ted) uses a pipeline of modules performing
1. tokenization
2. sentence splitting and segmentation into clauses
3. part-of-speech tagging
4. chunk parsing
5. rhetorical parsing
6. knowledge-based processing
The tagger we are using is the Tree Tagger by Schmid
(1994); the chunk parser is CASS (Abney 1996). The re-
maining modules, as well as the grammars for the chunk
parser, have been developed by our group (including stu-
dent projects).2 The rhetorical parser is a chart parser and
uses a discourse grammar leading to a parse forest, and
is supported by a lexicon of discourse markers (connec-
tives). We have started work on reference resolution (in
conjunction with named-entity recognition). Addition of
the knowledge-based component, as sketched in the pre-
vious section, has just begun. The main challenge is to
allow for the various kinds of underspecification within
the LOOM formalism and to design appropriate inference
rules.
As implementation shell, we are using GATE
(http://www.gate.ac.uk), which proved to be a very use-
ful environment for this kind of incremental system con-
struction.
8 Conclusions
Knowledge-based text understanding and surface-based
analysis have in the past largely been perceived as very
different enterprises that do not even share the same
2In addition to this ?traditional? pipeline approach, Reit-
ter (2003) performed experiments with machine learning tech-
niques based on our MAZ corpus as training data.
goals. The paper argued that a synthesis can be useful, in
particular: that knowledge-based understanding can ben-
efit from stages of surface-based pre-processing. Given
that
  pre-coded knowledge will almost certainly have
gaps when it comes to understanding a ?new? text,
and
  surface-based methods yield ?some? analysis for
any text, however sparse, irrelevant or even wrong
that analysis may be,
a better notion of robustness is needed that explains how
language understanding can be ?as good (deep) as pos-
sible or as necessary?. The proposal is to first employ
?defensive? surface-based methods to provide a first, un-
derspecified representation of text structure that has gaps
but is relatively trustworthy. Then, this representation
may be enriched with the help of statistical, probabilistic,
heuristic information that is added to the representation
(and marked as being less trustworthy). Finally, a ?deep?
analysis can map everything into a TBox/ABox scheme,
possibly again filling some gaps in the text representa-
tion (Abox) on the basis of prior knowledge already en-
coded in the TBox. The deep analysis should not be an
all-or-nothing step but perform as good as possible ? if
something cannot be understood entirely, then be content
with a partial representation or, in the worst case, with a
portion of the surface string.
Acknowledgements
Thanks to: Thomas Hanneforth and all the students of
our Systemkonstruktion seminar for the implementation
of the rhetorical parser prototype; anonymous review-
ers for helpful comments on the paper; Ma?rkische Allge-
meine Zeitung for providing us with plenty of commen-
taries.
References
Abney, S. 1996. Partial Parsing via Finite-State Cascades.
In: Proceedings of the ESSLLI ?96 Robust Parsing
Workshop.
Corston-Oliver, S. 1998. Computing representations of
the structure of written discourse. Ph.D. Thesis. Uni-
versity of California, Santa Barbara.
Hanneforth, T.; Heintze, S.; Stede, M. Rhetorical parsing
with underspecification and forests. Submitted.
Hirst, G.; Ryan, M. 1992. Mixed-depth representations
for natural language text. In: P. Jacobs (ed.): Text-
based intelligent systems. Lawrence Erlbaum, Hills-
dale.
MacGregor, R.; Bates, R. 1987. The LOOM Knowledge
Representation Language. Technical Report ISI/RS-
87-188, USC Information Sciences Institute.
Mahesh, K.; Nirenburg, S.; 1996. Meaning representation
for knowledge sharing in practical machine translation.
Proc. of the FLAIRS-96 track on information inter-
change; Florida AI Research Symposium, Key West.
Mann, W.; Thompson, S. 1988. Rhetorical Structure The-
ory: A Theory of Text Organization. TEXT 8(3), 243-
281.
Marcu, D. 1997. The rhetorical parsing of natural lan-
guage texts. Proc. of the 35th Annual Conference of
the ACL, 96-103.
Marcu, D. 1999. Discourse trees are good indicators of
importance in text. In: I. Mani and M. Maybury (eds.):
Advances in Automatic Text Summarization, 123-136,
The MIT Press.
O?Donnell, M. 1997. RST-Tool: An RST Analysis Tool.
Proc. of the 6th European Workshop on Natural Lan-
guage Generation, Duisburg.
Reitter, D. 2003. Rhetorical analysis with rich-feature
support vector models. Diploma Thesis, Potsdam Uni-
versity, Dept. of Linguistics.
Reitter, D.; Stede, M. 2003. Step by step: underspeci-
fied markup in incremental rhetorical analysis In: Proc.
of the Worksop on Linguistically Interpreted Corpora
(LINC-03), Budapest.
Schilder, F. 2002. Robust Discourse Parsing via Dis-
course Markers, Topicality and Position. Natural Lan-
guage Engineering 8 (2/3).
Schmid, H. 1994. Probabilistic part-of-speech tagging us-
ing decision trees. Proc. of the Int?l Conference on
New Methods in Language Processing.
Stede, M. 1999. Lexical Semantics and Knowledge Rep-
resentation in Multilingual Text Generation. Kluwer,
Dordrecht/Boston.
Sumita, K.; Ono, K.; Chino, T.; Ukita, T.; Amano, S.
1992. A discourse structure analyzer for Japanese text.
Proc. of the International Conference on Fifth Genera-
tion Computer Systems, 1133-1140.
The Potsdam Commentary Corpus
Manfred Stede
University of Potsdam
Dept. of Linguistics
Applied Computational Linguistics
D-14415 Potsdam
Germany
stede@ling.uni-potsdam.de
Abstract
A corpus of German newspaper commentaries
has been assembled and annotated with differ-
ent information (and currently, to different de-
grees): part-of-speech, syntax, rhetorical struc-
ture, connectives, co-reference, and information
structure. The paper explains the design deci-
sions taken in the annotations, and describes a
number of applications using this corpus with
its multi-layer annotation.
1 Introduction
A corpus of German newspaper commentaries
has been assembled at Potsdam University, and
annotated with different linguistic information,
to different degrees. Two aspects of the corpus
have been presented in previous papers ((Re-
itter, Stede 2003) on underspecified rhetorical
structure; (Stede 2003) on the perspective of
knowledge-based summarization). This paper,
however, provides a comprehensive overview of
the data collection effort and its current state.
At present, the ?Potsdam Commentary Cor-
pus? (henceforth ?PCC? for short) consists of
170 commentaries from Ma?rkische Allgemeine
Zeitung, a German regional daily. The choice
of the genre commentary resulted from the fact
that an investigation of rhetorical structure,
its interaction with other aspects of discourse
structure, and the prospects for its automatic
derivation are the key motivations for building
up the corpus. Commentaries argue in favor
of a specific point of view toward some polit-
ical issue, often dicussing yet dismissing other
points of view; therefore, they typically offer a
more interesting rhetorical structure than, say,
narrative text or other portions of newspapers.
The choice of the particular newspaper was
motivated by the fact that the language used in
a regional daily is somewhat simpler than that
of papers read nationwide. (Again, the goal of
automatic analysis was responsible for this deci-
sion.) This is manifest in the lexical choices but
also in structural features. As an indication, in
our core corpus, we found an average sentence
length of 15.8 words and 1.8 verbs per sentence,
whereas a randomly taken sample of ten com-
mentaries from the national papers Su?ddeutsche
Zeitung and Frankfurter Allgemeine has 19.6
words and 2.1 verbs per sentence. The com-
mentaries in PCC are all of roughly the same
length, ranging from 8 to 10 sentences. For il-
lustration, an English translation of one of the
commentaries is given in Figure 1.
The paper is organized as follows: Section 2
explains the different layers of annotation that
have been produced or are being produced. Sec-
tion 3 discusses the applications that have been
completed with PCC, or are under way, or are
planned for the future. Section 4 draws some
conclusions from the present state of the effort.
2 Layers of Annotation
The corpus has been annotated with six differ-
ent types of information, which are character-
ized in the following subsections. Not all the
layers have been produced for all the texts yet.
There is a ?core corpus? of ten commentaries,
for which the range of information (except for
syntax) has been completed; the remaining data
has been annotated to different degrees, as ex-
plained below.
All annotations are done with specific tools
and in XML; each layer has its own DTD.
This offers the well-known advantages for inter-
changability, but it raises the question of how
to query the corpus across levels of annotation.
We will briefly discuss this point in Section 3.1.
2.1 Part-of-speech tags
All commentaries have been tagged with
part-of-speech information using Brants? TnT1
tagger and the Stuttgart/Tu?bingen Tag Set
1www.coli.uni-sb.de/?thorsten/tnt/
Dagmar Ziegler is up to her neck in debt. Due to the dramatic fiscal situation in Brandenburg
she now surprisingly withdrew legislation drafted more than a year ago, and suggested to decide
on it not before 2003. Unexpectedly, because the ministries of treasury and education both had
prepared the teacher plan together. This withdrawal by the treasury secretary is understandable,
though. It is difficult to motivate these days why one ministry should be exempt from cutbacks
? at the expense of the others. Reiche?s colleagues will make sure that the concept is waterproof.
Indeed there are several open issues. For one thing, it is not clear who is to receive settlements or
what should happen in case not enough teachers accept the offer of early retirement. Nonetheless
there is no alternative to Reiche?s plan. The state in future has not enough work for its many
teachers. And time is short. The significant drop in number of pupils will begin in the fall of 2003.
The government has to make a decision, and do it quickly. Either save money at any cost - or give
priority to education.
Figure 1: Translation of PCC sample commentary
(STTS)2.
2.2 Syntactic structure
Annotation of syntactic structure for the core
corpus has just begun. We follow the guide-
lines developed in the TIGER project (Brants
et al 2002) for syntactic annotation of German
newspaper text, using the Annotate3 tool for in-
teractive construction of tree structures.
2.3 Rhetorical structure
All commentaries have been annotated with
rhetorical structure, using RSTTool4 and the
definitions of discourse relations provided by
Rhetorical Structure Theory (Mann, Thomp-
son 1988). Two annotators received training
with the RST definitions and started the pro-
cess with a first set of 10 texts, the results of
which were intensively discussed and revised.
Then, the remaining texts were annotated and
cross-validated, always with discussions among
the annotators. Thus we opted not to take the
step of creating more precise written annotation
guidelines (as (Carlson, Marcu 2001) did for En-
glish), which would then allow for measuring
inter-annotator agreement. The motivation for
our more informal approach was the intuition
that there are so many open problems in rhetor-
ical analysis (and more so for German than for
English; see below) that the main task is qual-
itative investigation, whereas rigorous quanti-
tative analyses should be performed at a later
stage.
One conclusion drawn from this annotation
effort was that for humans and machines alike,
2www.sfs.nphil.uni-tuebingen.de/Elwis/stts/
stts.html
3www.coli.uni-sb.de/sfb378/negra-corpus/annotate.
html
4www.wagsoft.com/RSTTool
assigning rhetorical relations is a process loaded
with ambiguity and, possibly, subjectivity. We
respond to this on the one hand with a format
for its underspecification (see 2.4) and on the
other hand with an additional level of annota-
tion that attends only to connectives and their
scopes (see 2.5), which is intended as an inter-
mediate step on the long road towards a system-
atic and objective treatment of rhetorical struc-
ture.
2.4 Underspecified rhetorical structure
While RST (Mann, Thompson 1988) proposed
that a single relation hold between adjacent
text segments, SDRT (Asher, Lascarides 2003)
maintains that multiple relations may hold si-
multaneously. Within the RST ?user commu-
nity? there has also been discussion whether two
levels of discourse structure should not be sys-
tematically distinguished (intentional versus in-
formational).
Some relations are signalled by subordinat-
ing conjunctions, which clearly demarcate the
range of the text spans related (matrix clause,
embedded clause). When the signal is a coordi-
nating conjunction, the second span is usually
the clause following the conjunction; the first
span is often the clause preceding it, but some-
times stretches further back. When the connec-
tive is an adverbial, there is much less clarity as
to the range of the spans.
Assigning rhetorical relations thus poses
questions that can often be answered only sub-
jectively. Our annotators pointed out that very
often they made almost random decisions as to
what relation to choose, and where to locate
the boundary of a span. (Carlson, Marcu 2001)
responded to this situation with relatively pre-
cise (and therefore long!) annotation guidelines
that tell annotators what to do in case of doubt.
Quite often, though, these directives fulfill the
goal of increasing annotator agreement without
in fact settling the theoretical question; i.e., the
directives are clear but not always very well mo-
tivated.
In (Reitter, Stede 2003) we went a different
way and suggested URML5, an XML format for
underspecifying rhetorical structure: a number
of relations can be assigned instead of a sin-
gle one, competing analyses can be represented
with shared forests. The rhetorical structure
annotations of PCC have all been converted to
URML. There are still some open issues to be re-
solved with the format, but it represents a first
step. What ought to be developed now is an
annotation tool that can make use of the for-
mat, allow for underspecified annotations and
visualize them accordingly.
2.5 Connectives with scopes
For the ?core? portion of PCC, we found that on
average, 35% of the coherence relations in our
RST annotations are explicitly signalled by a
lexical connective.6 When adding the fact that
connectives are often ambiguous, one has to
conclude that prospects for an automatic anal-
ysis of rhetorical structure using shallow meth-
ods (i.e., relying largely on connectives) are not
bright ? but see Sections 3.2 and 3.3 below.
Still, for both human and automatic rhetori-
cal analysis, connectives are the most important
source of surface information. We thus decided
to pay specific attention to them and introduce
an annotation layer for connectives and their
scopes. This was also inspired by the work on
the Penn Discourse Tree Bank7, which follows
similar goals for English.
For effectively annotating connec-
tives/scopes, we found that existing annotation
tools were not well-suited, for two reasons:
? Some tools are dedicated to modes of anno-
tation (e.g., tiers), which could only quite
un-intuitively be used for connectives and
scopes.
? Some tools would allow for the desired
annotation mode, but are so complicated
(they can be used for many other purposes
as well) that annotators take a long time
getting used to them.
5?Underspecified Rhetorical Markup Language?
6This confirms the figure given by (Schauer, Hahn
2001), who determined that in their corpus of German
computer tests, 38% of relations were lexically signalled.
7www.cis.upenn.edu/?pdtb/
Consequently, we implemented our own anno-
tation tool ConAno in Java (Stede, Heintze
2004), which provides specifically the function-
ality needed for our purpose. It reads a file
with a list of German connectives, and when
a text is opened for annotation, it highlights all
the words that show up in this list; these will
be all the potential connectives. The annotator
can then ?click away? those words that are here
not used as connectives (such as the conjunc-
tion und (?and?) used in lists, or many adverbials
that are ambiguous between connective and dis-
course particle). Then, moving from connective
to connective, ConAno sometimes offers sugges-
tions for its scope (using heuristics like ?for sub-
junctor, mark all words up to the next comma
as the first segment?), which the annotator can
accept with a mouseclick or overwrite, marking
instead the correct scope with the mouse. When
finished, the whole material is written into an
XML-structured annotation file.
2.6 Co-reference
We developed a first version of annotation
guidelines for co-reference in PCC (Gross 2003),
which served as basis for annotating the core
corpus but have not been empirically evaluated
for inter-annotator agreement yet. The tool we
use is MMAX8, which has been specifically de-
signed for marking co-reference.
Upon identifying an anaphoric expression
(currently restricted to: pronouns, preposi-
tional adverbs, definite noun phrases), the an-
notator first marks the antecedent expression
(currently restricted to: various kinds of noun
phrases, prepositional phrases, verb phrases,
sentences) and then establishes the link between
the two. Links can be of two different kinds:
anaphoric or bridging (definite noun phrases
picking up an antecedent via world-knowledge).
? Anaphoric links: the annotator is asked
to specify whether the anaphor is a repe-
tition, partial repetition, pronoun, epithet
(e.g., Andy Warhol ? the PopArt artist), or
is-a (e.g., Andy Warhol was often hunted
by photographers. This fact annoyed espe-
cially his dog...).
? Bridging links: the annotator is asked to
specify the type as part-whole, cause-effect
(e.g., She had an accident. The wounds
are still healing.), entity-attribute (e.g., She
8www.eml-research.de/english/Research/NLP/
Downloads
had to buy a new car. The price shocked
her.), or same-kind (e.g., Her health in-
surance paid for the hospital fees, but
the automobile insurance did not cover the
repair.).
2.7 Information structure
In a similar effort, (Go?tze 2003) developed a
proposal for the theory-neutral annotation of
information structure (IS) ? a notoriously dif-
ficult area with plenty of conflicting and over-
lapping terminological conceptions. And in-
deed, converging on annotation guidelines is
even more difficult than it is with co-reference.
Like in the co-reference annotation, Go?tze?s pro-
posal has been applied by two annotators to the
core corpus but it has not been systematically
evaluated yet.
We use MMAX for this annotation as well.
Here, annotation proceeds in two phases: first,
the domains and the units of IS are marked as
such. The domains are the linguistic spans that
are to receive an IS-partitioning, and the units
are the (smaller) spans that can play a role as a
constituent of such a partitioning. Among the
IS-units, the referring expressions are marked
as such and will in the second phase receive
a label for cognitive status (active, accessible-
text, accessible-situation, inferrable, inactive).
They are also labelled for their topicality (yes
/ no), and this annotation is accompanied by
a confidence value assigned by the annotator
(since it is a more subjective matter). Finally,
the focus/background partition is annotated,
together with the focus question that elicits the
corresponding answer. Asking the annotator to
also formulate the question is a way of arriving
at more reproducible decisions.
For all these annotation taks, Go?tze devel-
oped a series of questions (essentially a decision
tree) designed to lead the annotator to the ap-
propriate judgement.
3 Past, Present, Future Applications
Having explained the various layers of annota-
tion in PCC, we now turn to the question what
all this might be good for. This concerns on
the one hand the basic question of retrieval, i.e.
searching for information across the annotation
layers (see 3.1). On the other hand, we are in-
terested in the application of rhetorical analy-
sis or ?discourse parsing? (3.2 and 3.3), in text
generation (3.4), and in exploiting the corpus
for the development of improved models of dis-
course structure (3.5).
3.1 Retrieval
For displaying and querying the annoated text,
we make use of the Annis Linguistic Database
developed in our group for a large research effort
(?Sonderforschungsbereich?) revolving around
information structure.9 The implementation is
basically complete, yet some improvements and
extensions are still under way. The web-based
Annis imports data in a variety of XML formats
and tagsets and displays it in a tier-oriented
way (optionally, trees can be drawn more ele-
gantly in a separate window). Figure 2 shows a
screenshot (which is of somewhat limited value,
though, as color plays a major role in signalling
the different statuses of the information). In the
small window on the left, search queries can be
entered, here one for an NP that has been anno-
tated on the co-reference layer as bridging. The
portions of information in the large window can
be individually clicked visible or invisible; here
we have chosen to see (from top to bottom)
? the full text,
? the annotation values for the activated an-
notation set (co-reference),
? the actual annotation tiers, and
? the portion of text currently ?in focus?
(which also appears underlined in the full
text).
Different annotations of the same text are
mapped into the same data structure, so that
search queries can be formulated across annota-
tion levels. Thus it is possible, for illustration,
to look for a noun phrase (syntax tier) marked
as topic (information structure tier) that is in
a bridging relation (co-reference tier) to some
other noun phrase.
3.2 Stochastic rhetorical analysis
In an experiment on automatic rhetorical pars-
ing, the RST-annotations and PoS tags were
used by (Reitter 2003) as a training corpus
for statistical classification with Support Vector
Machines. Since 170 annotated texts constitute
a fairly small training set, Reitter found that
an overall recognition accuracy of 39% could
be achieved using his method. For the En-
glish RST-annotated corpus that is made avail-
able via LDC, his corresponding result is 62%.
Future work along these lines will incorporate
other layers of annotation, in particular the syn-
tax information.
9www.ling.uni-potsdam.de/sfb/
Figure 2: Screenshot of Annis Linguistic Database
3.3 Symbolic and knowledge-based
rhetorical analysis
We are experimenting with a hybrid statisti-
cal and knowledge-based system for discourse
parsing and summarization (Stede 2003), (Han-
neforth et al 2003), again targeting the genre
of commentaries. The idea is to have a pipeline
of shallow-analysis modules (tagging, chunk-
ing, discourse parsing based on connectives) and
map the resulting underspecified rhetorical tree
(see Section 2.4) into a knowledge base that may
contain domain and world knowledge for enrich-
ing the representation, e.g., to resolve references
that cannot be handled by shallow methods,
or to hypothesize coherence relations. In the
rhetorical tree, nuclearity information is then
used to extract a ?kernel tree? that supposedly
represents the key information from which the
summary can be generated (which in turn may
involve co-reference information, as we want to
avoid dangling pronouns in a summary). Thus
we are interested not in extraction, but actual
generation from representations that may be de-
veloped to different degrees of granularity.
In order to evaluate and advance this ap-
proach, it helps to feed into the knowledge base
data that is already enriched with some of the
desired information ? as in PCC. That is, we
can use the discourse parser on PCC texts, em-
ulating for instance a ?co-reference oracle? that
adds the information from our co-reference an-
notations. The knowledge base then can be
tested for its relation-inference capabilities on
the basis of full-blown co-reference information.
Conversely, we can use the full rhetorical tree
from the annotations and tune the co-reference
module. The general idea for the knowledge-
based part is to have the system use as much
information as it can find at its disposal to pro-
duce a target representation as specific as pos-
sible and as underspecified as necessary. For
developing these mechanisms, the possibility to
feed in hand-annotated information is very use-
ful.
3.4 Salience-based text generation
Text generation, or at least the two phases
of text planning and sentence planning, is a
process driven partly by well-motivated choices
(e.g., use this lexeme X rather than that more
colloquial near-synonym Y ) and partly by con-
ventionalized patterns (e.g., order of informa-
tion in news reports). And then there are deci-
sions that systems typically hard-wire, because
the linguistic motivation for making them is
not well understood yet. Preferences for con-
stituent order (especially in languages with rel-
atively free word order) often belong to this
group. Trying to integrate constituent ordering
and choice of referring expressions, (Chiarcos
2003) developed a numerical model of salience
propagation that captures various factors of au-
thor?s intentions and of information structure
for ordering sentences as well as smaller con-
stituents, and picking appropriate referring ex-
pressions.10 Chiarcos used the PCC annota-
tions of co-reference and information structure
to compute his numerical models for salience
projection across the generated texts.
3.5 Improved models of discourse
structure
Besides the applications just sketched, the over-
arching goal of developing the PCC is to build
up an empirical basis for investigating phe-
nomena of discourse structure. One key issue
here is to seek a discourse-based model of in-
formation structure. Since Danes?? proposals
of ?thematic development patterns?, a few sug-
gestions have been made as to the existence
of a level of discourse structure that would
predict the information structure of sentences
within texts. (Hartmann 1984), for example,
used the term Reliefgebung to characterize the
distibution of main and minor information in
texts (similar to the notion of nuclearity in
RST). (Brandt 1996) extended these ideas to-
ward a conception of kommunikative Gewich-
tung (?communicative-weight assignment?). A
different notion of information structure, is used
in work such as that of (?), who tried to char-
acterize felicitous constituent ordering (theme
choice, in particular) that leads to texts pre-
senting information in a natural, ?flowing? way
rather than with abrupt shifts of attention. ?
In order to ground such approaches in linguistic
observation and description, a multi-level anno-
10For an exposition of the idea as applied to the task
of text planning, see (Chiarcos, Stede 2004).
tation like that of PCC can be exploited to look
for correlations in particular between syntactic
structure, choice of referring expressions, and
sentence-internal information structure.
A different but supplementary perspective on
discourse-based information structure is taken
by one of our partner projects11, which is inter-
ested in correlations between prosody and dis-
course structure. A number of PCC commen-
taries will be read by professional news speak-
ers and prosodic features be annotated, so that
the various annotation layers can be set into
correspondence with intonation patterns. In fo-
cus is in particular the correlation with rhetor-
ical structure, i.e., the question whether spe-
cific rhetorical relations ? or groups of relations
in particular configurations ? are signalled by
speakers with prosodic means.
Besides information structure, the second
main goal is to enhance current models of
rhetorical structure. As already pointed out in
Section 2.4, current theories diverge not only on
the number and definition of relations but also
on apects of structure, i.e., whether a tree is
sufficient as a representational device or gen-
eral graphs are required (and if so, whether
any restrictions can be placed on these graph?s
structures ? cf. (Webber et al, 2003)). Again,
the idea is that having a picture of syntax,
co-reference, and sentence-internal information
structure at one?s disposal should aid in find-
ing models of discourse structure that are more
explanatory and can be empirically supported.
4 Conclusions
The PCC is not the result of a funded project.
Instead, the designs of the various annotation
layers and the actual annotation work are re-
sults of a series of diploma theses, of students?
work in course projects, and to some extent of
paid assistentships. This means that the PCC
cannot grow particularly quickly. After the first
step towards breadth had been taken with the
PoS-tagging, RST annotation, and URML con-
version of the entire corpus of 170 texts12, em-
phasis shifted towards depth. Hence we decided
to select ten commentaries to form a ?core cor-
pus?, for which the entire range of annotation
levels was realized, so that experiments with
multi-level querying could commence. Cur-
11www.ling.uni-potsdam.de/sfb/projekt a3.php
12This step was carried out in the course of the
diploma thesis work of David Reitter (2003), which de-
serves special mention here.
rently, some annotations (in particular the con-
nectives and scopes) have already moved be-
yond the core corpus; the others will grow step
by step.
The kind of annotation work presented here
would clearly benefit from the emergence of
standard formats and tag sets, which could lead
to sharable resources of larger size. Clearly this
poses a number of research challenges, though,
such as the applicability of tag sets across dif-
ferent languages. Nonetheless, the prospect of a
network of annotated discourse resources seems
particularly promising if not only a single anno-
tation layer is used but a whole variety of them,
so that a systematic search for correlations be-
tween them becomes possible, which in turn can
lead to more explanatory models of discourse
structure.
References
N. Asher, A. Lascarides. 2003. Logics of Con-
versation. Cambridge University Press.
M. Brandt. 1996. Subordination und Parenthese
als Mittel der Informationsstrukturierung in
Texten. In. W. Motsch (ed.): Ebenen der
Textstruktur. Tu?bingen: Niemeyer.
S. Brants, S. Dipper, S. Hansen, W. Lezius, G.
Smith. 2002. The TIGER Treebank. In: Proc.
of the Workshop on Treebanks and Linguistic
Theories. Sozopol.
L. Carlson, D. Marcu. 2001. Discourse Tagging
Reference Manual. Ms., Univ. of Southern
California / Information Sciences Institute.
C. Chiarcos. 2003. Eine Satzplanungskompo-
nente fu?r die Textgenerierung. In: Uta
Seewald-Heeg (ed.): Sprachtechnologie fu?r die
multilinguale Kommunikation. Bonn: gardez.
(Short version of Diploma Thesis, Technische
Universita?t Berlin)
C. Chiarcos, M. Stede. 2004. Salience-Driven
Text Planning. To appear in: Proc. of the
Third Int?l Conference on Natural Language
Generation (INLG), Careys Manor (UK).
P. Fries. 1981. On the Status of Theme in En-
glish. Forum Linguisticum 6.1:1-38.
M. Go?tze. 2003. Zur Annotation von Infor-
mationsstruktur. Diploma thesis, Universita?t
Potsdam, Inst. of Linguistics.
J. Gross. 2003. Algorithmen zur Behandlung
von Anaphora in Zeitungskommentaren.
Diploma thesis, Technische Universita?t
Berlin.
T. Hanneforth, S. Heintze, M. Stede. 2003.
Rhetorical Parsing with Underspecification
and Forests. In: Proc. of the HLT/NAACL
Conference (Companion Volume), Edmon-
ton/AL.
D. Hartmann. 1984. Reliefgebung: Infor-
mationsvordergrund und Informationshinter-
grund in Texten als Problem von Textlinguis-
tik und Stilistik. In: Wirkendes Wort 34:305-
323.
W. Mann, S. Thompson. 1988. Rhetorical
Structure Theory: A Theory of Text Orga-
nization. TEXT 8(3):243-281.
D. Reitter. 2003. Simple signals for complex
rhetorics: on rhetorical analysis with rich-
feature support vector modelds. In: Uta
Seewald-Heeg (ed.): Sprachtechnologie fu?r die
multilinguale Kommunikation. Bonn: gardez.
(Short version of Diploma Thesis, Universita?t
Potsdam, Inst. of Linguistics)
D. Reitter, M. Stede. 2003. Step by step: un-
derspecified markup in incremental rhetorical
analysis. In: Proc. of the 4th Int?l Workshop
on Linguistically Interpreted Corpora (LINC-
03), Budapest.
H. Schauer, U. Hahn. 2001. Anaphoric cues
for coherence relations. In: Proc. of ?Recent
Advances in Natural Language Processing?-
RANLP 2001. Tzigov Chark, Bulgaria.
M. Stede. 2003. Surfaces and depths in text un-
derstanding: the case of newspaper commen-
tary. In: Proc. of the HLT/NAACL Work-
shop on Text Meaning, Edmonton/AL.
M. Stede, S. Heintze. 2004. Machine-assisted
rhetorical structure annotation. To appear in:
Proc. of the 20th Int?l Conference on Compu-
tational Linguistics (COLING), Geneva.
B. Webber, A. Knott, M. Stone, A. Joshi. 2003.
Anaphora and Discourse Structure. Compu-
tational Linguistics 29(4):545-588.
Feeding OWL: Extracting and Representing
the Content of Pathology Reports
David Schlangen and Manfred Stede
Department of Linguistics
University of Potsdam
P.O. Box 601553
D-14415 Potsdam, Germany
{das|stede}@ling.uni-potsdam.de
Elena Paslaru Bontas
Institute for Computer Science
Freie Universita?t Berlin
Takustr.9
D-14195 Berlin, Germany
paslaru@inf.fu-berlin.de
Abstract
This paper reports on an ongoing project that com-
bines NLP with semantic web technologies to sup-
port a content-based storage and retrieval of medical
pathology reports. We describe the NLP component
of the project (a robust parser) and the background
knowledge component (a domain ontology repre-
sented in OWL), and how they work together during
extraction of domain specific information from nat-
ural language reports. The system provides a good
example of how NLP techniques can be used to pop-
ulate the Semantic Web.
1 Introduction
Clinical pathologists work with and produce vast
amounts of data: images of biological samples and
written reports of their findings. Digital Pathology
is the cover term for a number of efforts to intro-
duce digital processing into the work-flow of the
pathologist. While previous projects have focussed
on storage and distribution of images and reports
(e.g. in Tele-Pathology-projects, (Slodowksa et al,
2002; Demichellis et al, 2002)), the work reported
here explores the use of Natural Language Process-
ing (NLP) and Semantic Web technologies to sup-
port a content-based storage and retrieval of case
reports. The system that we are building, LUPUS
(Lung Pathology System), consists of an NLP com-
ponent (a robust parser) and a Semantic Web com-
ponent (a domain ontology represented in OWL, and
a Description Logic reasoner), which work closely
together, with the domain ontology guiding the in-
formation extraction process.
The remainder of the paper is organised as fol-
lows. In the next section we describe the context
and intended application of the system, we discuss
linguistic properties of the input material we are
working with, and we give some details of the back-
ground ontology we are using. In Section 3 we go
into the technical details of the process of extracting
information from natural language reports and rep-
resenting it in an OWL representation, after which
we describe a preliminary evaluation. We close with
discussing related work, and planned future work.
2 Digital Pathology
2.1 The Application
LUPUS is intended to support the pathologist in two
ways. First, it is used to semantically annotate a
large archive of case reports, turning them into a
valuable resource for diagnosis and teaching. The
system uses the case reports produced by experts
(the pathologists) to extract information about the
accompanying images (of the tissue samples), and
thus produces semantic annotation both for the re-
port and for those images.
This corpus of cases can then be searched in a
fast, content-based manner to retrieve case reports
(the textual reports together with the images of tis-
sue samples) that might be relevant for a case the
pathologist is working on. The search is content-
based in that it can make use of semantic relation-
ships between search concepts and those occuring
in the text. We also encode in rules knowledge
about certain diagnostics tasks, so that for example
queries asking for ?differential diagnosis? (?show
me cases of diagnoses which are known to be easily
confusable with the diagnosis I am thinking of for
the present case?) can be processed?tasks which
normally require consultation of textbooks. These
search capabilities are useful both during diagnosis
and for teaching, where it makes interesting exam-
ples immediately available to students.
Another use case is quality control during input
of new reports. Using our system, such reports can
be entered in a purpose-built editor (which com-
bines digital microscopy facilities (Saeger et al,
2003) with our semantic annotator / search engine),
where they are analysed on-the-fly, and potential
inconsistencies with respect to the background do-
main ontology are spotted.1 During the develop-
ment phase of the system, we are using this feature
1Naturally, to gain acceptance by working pathologists, this
process has to be ?minimally invasive?.
to detect where the coverage of the system must be
extended.
The present paper focuses on the process of ex-
tracting the relevant information from natural lan-
guage reports and representing it in a semantic
web-ready format as a precondition for performing
searches; we leave the description of the search and
retrieval functions to another paper. To give an idea
of the kind of data we are dealing with, and of the in-
tended target representation, Figure 1 shows an ex-
ample report (at the top of the figure) and the repre-
sentation of its content computed by our system (at
the bottom).2 We discuss the input format in the fol-
lowing subsection, and the target representation to-
gether with the domain knowledge available to us in
Subsection 2.3; discussion of the intermediate for-
mat that is also shown in the figure is deferred until
Section 3.
2.2 Pathology Reports
During the development phase of the system, we
are using a corpus of 90 randomly selected case re-
ports (ca. 13,000 words; i.e. the average length of
the reports is ca. 140 words, with a standard devia-
tion of 12 words) for testing and grammar develop-
ment. Linguistically, these reports are quite distin-
guished: they are written in a ?telegram?-style, with
verbs largely being absent (a rough examination of
the corpus showed that only about every 43rd token
is a verb, compared to every 11th in a comparable
corpus of German newspaper). Also, the vocabulary
is rather controlled, with very little variation?this
of course is good news for automatically process-
ing such input. On the discourse level we also find
a strict structure, with a fixed number of semanti-
cally grouped sections. E.g., information about the
diagnosis made will normally be found in the sec-
tion ?Kritischer Bericht? (critical report), and the in-
formation in the ?Makroskopie? and ?Mikroskopie?
sections (macroscopy and microscopy, respectively)
will be about the same parts of the sample, but on
different levels of granularity.
The last peculiarity we note is the relatively high
frequency of compound nouns. These are especially
important for our task, since technical concepts in
German tend to be expressed by such compound
nouns (rather than by noun groups). While some
2What is shown in the figure is actually already the result
of a preprocessing step; the cases as stored in the database con-
tain patient data as well, and are formatted to comply with the
HL7 standard for medical data (The HL7 Consortium, 2003).
Moreover, the italicisation in the input representation and the
numbers in square brackets are added here for ease of refer-
ence and are not part of the actual representations maintained
by the system.
of those will denote individual concepts and hence
will be recorded in the domain lexicon, others must
be analysed and their semantics must be composed
out of that of their parts (see below).
2.3 Lung Pathology Knowledge in OWL
The result of processing such reports with LUPUS
is a representation of (relevant aspects of) their con-
tent. This representation has the form of instances
of concepts and assertions of properties that are de-
fined in an ontology, which constitutes the domain
knowledge of the system (at the moment focussed
on pathologies of the lung). This ontology is spec-
ified in OWL DL (W3C WebOnt WG, 2004), a ver-
sion of OWL with a formal semantics and a complete
and decidable calculus. Consequently, the content
of the texts is represented in OWD DL as well, and
so the knowledge base of the system consists of the
ontology and the instances.
The ontology we use is compiled out of sev-
eral medical sources (such as UMLS (The UMLS
Consortium, 2003) and SNOMED (SNOMED Inter-
national, 2004)), but since these sources often were
not intended for machine reasoning (i.e., are not
necessarily consistent, and use rather loosely de-
fined relations), considerable effort has been spent
(and is being spent) on cleaning them up.3 At the
moment, about 1,000 domain-level concepts and
ca. 160 upper-level concepts have been identified,
which are connected by about 50 core relation types.
To our knowledge, this makes it one of the biggest
OWL-ontologies currently in use.
Besides representing concepts relevant to our do-
main, the ontology also lists properties that in-
stances of these concepts can have. These proper-
ties are represented as two-place relations; to give
an example, the property ?green? attributed to an
entity x will in our system not be represented as
?green(x)?, but rather as something like ?colour(x,
green)?. This allows us to enforce consistency
checks, by demanding that for each second-order
predicate (colour, malignity, consistency, etc.) ap-
propriate for a given concept only one value is
chosen.4 This choice of representation has conse-
quences for the way the semantics of adjectives is
represented in the lexicon, as we will see presently.
3There are several current research projects with a similar
aim of extracting stricter ontologies from sources like those
mentioned above (see e.g. (Schulz and Hahn, 2001; Burgun and
Bodenreider, 2001)), and this is by no means a trivial task. The
present paper, however, focuses on a different (but of course in-
terdependent) problem, namely that of extracting information
such that it can be represented in the way described here.
4Technically, these constraints are realised by functional
data-properties relating entities to enumerated data types.
An example report (with translation):
<befund>
<makroskopie>
Stanzzylinder von 15 mm La?nge und 1 mm Durchmesser. [1]
</makroskopie>
<mikroskopie>
Stanzbiopsat [2] eingenommen durch Infiltrate einer soliden malignen epithelialen Neoplasie. [3]
Die Tumorzellen mit distinkten Zellgrenzen [4], zum Teil interzellula?r Spaltra?ume [5], zwischen
denen stellenweise kleine Bru?cken [6] nachweisbar sind. Das Zytoplasma leicht basophil,
z.T. auch breit und eosinphil, [7] die Zellkerne hochgradig polymorph mit zum Teil
multiplen basophilen Nukleolen. [8] Deutliche desmoplastische Stromareaktion. [9]
</mikroskopie>
<kritischer bericht>
Stanzbiopsat aus einer Manifestation eines soliden Karzinoms [10]
(klinisch rechte Lunge apikal).
</kritischer bericht>
<kommentar>
...
</kommentar>
</befund>
( Biopsy cylinder of 15 mm length and 1 mm diameter. | Biobsy infiltrated by a solid
malignant epithelial neoplasia. The tumor cells with distinct cell borders, partially intercel-
lular spatia, between which sporadically small bridges are verifiable. The cytoplasm lightly
basophil, in part also broad and eosinphile, the nuclei highly polymorphic, partially with
multiple basophile nucleoli. Distinct desmoplastic stroma reaction. | Biopsy cylinder from
a manifestation of a solid carcinoma (clinical right lung apical). )
?
Intermediate Representation (excerpt):
[2] unspec det(x2) ? punch biopsat(x2) [3] unspec plur det(x3) ? infiltrate(x3, x4) ?
indef det(x4) ? solid(x4) ?malign(x4) ? epithelial(x4) ? neoplasia(x4)
[4] def plur det(x5)?tumorcell(x5)?with rel(x5, x6)?unspec plur det(x6)?distinctive(x6)?
cell borders(x6) [7] spec det(x9) ? low degree(d1) ? basophile(x9, d1) ? partially(d2) ?
broad(x9, d2) ? eosinphile(x9, d2) ? cytoplasm(x9)
[8] def plur det(x10) ? high degree(d3) ? polymorpheous(x10, d3) ? nucleus(x10) ?
with rel(x10, x11)?unspec plur det(x11)?partially(d4)?multiple(x11, d4)?basophile(x11)?
nucleoli(x11)
?
Target Representation (excerpt):
<Malignant Epithelial Neoplasm C0432650 rdf:ID=?neoplasia x4?>
<solidity rdf:datatype=?http://www.w3.org/2001/XMLSchema#float?>1.0</solidity>
</Malignant Epithelial Neoplasm>
<Cell Border C0032743 rdf:ID=?cell border x61?/>
<Tumor cells C0431085 rdf:ID=?tumor cell x52?>
<hasBoundary rdf:resource=?file:...#cell boundary x61?/>
</Tumor cells C0431085>
<cytoplasm C0326583 rdf:ID=?cytoplasm1?>
<broad rdf:datatype=?http://www.w3.org/2001/XMLSchema#float?>1.0</broad>
<eosinphil rdf:datatype=?http://www.w3.org/2001/XMLSchema#float?>1.0</eosinphil>
<basophil rdf:datatype=?http://www.w3.org/2001/XMLSchema#float?>0.5</basophil>
</cytoplasm>
Figure 1: Input, Intermediate and Target Representation
Figure 2: Flowchart
Using OWL DL as a representation format for
natural language content means certain limitations
have to be accepted. Being a fragment of FOL, it
is not expressive enough to represent certain finer
semantic details, as will be discussed below. How-
ever, the advantage of using an emerging standard
for delivering and sharing information outweighs
these drawbacks.
3 Implementation
3.1 Overview
As mentioned above, most of the sentences in our
corpus do not contain a finite verb; i.e., according to
standard rules of grammar they are elliptical. While
a theoretically motivated approach should strive to
resolve this ellipsis contextually (for example as de-
scribed in (Schlangen, 2003)), in view of the in-
tended application and for reasons of robustness we
have decided to focus only on extracting informa-
tion about the entities introduced in the reports?
that is, on recognising nominal phrases, leaving
aside the question of how verbal meanings are to
be resolved.
Our strategy is to combine a ?shallow? prepro-
cessing stage (based on finite-state methods and sta-
tistical approaches) with a symbolic phase, in which
the semantics of the NPs is assembled.5 A require-
ment for the processing is that it must be robust, in
two ways: it must be able to deal with unknown
tokens (i.e., ?out of vocabulary? items) and with un-
known structure (i.e., ?out of grammar? construc-
tions), degrading gracefully and not just failing.
Figure 2 shows a flow chart of the system; the
individual modules are described in the following
sections.
5This strategy sits somewhere between Information Extrac-
tion, where also only certain phrases are extracted, for which,
however, normally no compositional semantics is computed,
and ?full? parsing, where such a semantics is computed only if
the whole input can be parsed.
3.2 Preprocessing
The first step, tokenising and sentence splitting, is
fairly standard, and so we skip over it here. The
second step, morpho-syntactic analysis, is more in-
teresting. It is performed by an independently de-
veloped module called TAGH, a huge finite-state
machine that makes use of a German word-stem
lexicon (containing about 90,000 entries for nouns,
17,000 for verbs, 20,000 adjectives and adverbs,
and about 1,500 closed class word forms). The
transducer is implemented in C++ and has a very
high throughput (about 20,000 words per second
on modern machines). The coverage achieved on
a balanced corpus of German is around 96% (Ju-
rish, 2003), for our domain the lexicon had to be
extended with some domain specific vocabulary.
To give an example of the results of the analysis,
Figure 3 shows (excerpts of) the output for Sentence
2 of the example report. Note that this is already the
POS-disambiguated output, and we only show one
analysis for each token. In most cases, we will get
several analyses for each token at this stage, differ-
ing with respect to their part of speech tag or other
morphological features (e.g., case) that are not fully
determined by their form. (The average is 5.7 anal-
yses per token.) Note also that the actual output of
the module is in an XML format (as indeed are all
intermediate representations); only for readability is
it presented here as a table.
Another useful feature of TAGH is that it pro-
vides derivational information about compound
nouns. To give an example, (1) shows one analysis
of the noun ?Untersuchungsergebnis? (examination
result).
(1) Untersuchungsergebnis
untersuch(V)?ung(n)/s#Ergebnis
As this shows, the analysis gives us information
about the stems of the compounds; this can be used
to guide the computation of the meaning of the com-
plex noun. However, this meaning is not fully com-
Token Type Analysis
Stanzbiopsat Stanzbiopsat [NN Gender=neut Number=sg Case=nom]
eingenommen ein|nehm?en [VVPP2]
durch durch [APPR]
Infiltrate Infiltrat [NN Gender=neut Number=pl Case=acc]
einer eine [ARTINDEF Number=sg Case=gen Gender=fem]
soliden solid [ADJA Degree=pos Number=sg Case=gen Gender=* ADecl=mixed]
malignen maligne [ADJA Degree=pos Number=sg Case=gen Gender=* ADecl=mixed]
epithelialen epithelial [ADJA Degree=pos Number=sg Case=gen Gender=* ADecl=mixed]
Neoplasie Neoplasie [NN Gender=fem Number=sg Case=*]
Figure 3: Result of Morphological Analysis / POS-tag disambiguation for Sentence 2
positional, as the nature of the relation between the
compounds is underspecified. We represent this by
use of an underspecified relation rel that holds be-
tween the compounds, and which has to be specified
later on in the processing chain.
The output of this module is then fed into a statis-
tically trained POS-disambiguator, which finds the
most likely path through the lattice of morpholog-
ical analyses (Jurish, 2003) (with an accuracy of
96%). In cases where morphology failed to provide
an analysis, the syntagmatically most likely POS tag
is chosen. At the end of this stage all analyses for
a given token agree on its part of speech; however,
other features (number, person, case, etc.) might
still not be disambiguated.
At the next stage, certain sequences of tokens
are grouped together, namely multi-word expres-
sion that denote a single concept in our ontology
(e.g., ?anthrakotische Lymphknoten? denotes a sin-
gle concept, and hence is marked as one token of
type NN at this step), and certain other phrases (e.g.
specifications of spatial dimensions) which can be
recognised easily but would require very specialised
grammar rules later on.6
Then, the domain-specific lexicon is accessed,
which maps ?concept names? (nouns, or phrases as
recognised in the previous step) to the concept IDs
used in the ontology.7 Tokens for which there is no
entry in that lexicon, and which are hence deemed
?irrelevant? for the domain, are assigned a ?dummy?
semantics appropriate for their part of speech, so
that they do not confuse the later parsing stage.
(More details about this kind of robustness will be
given shortly.)
6See for example (Grover et al, 2002) for a discussion of
the utility of a named entitiy recognition preprocessing stage
for robust symbolic parsing.
7Note that this lexicon is one single resource out of which
also the domain specfic additions to the morphology-lexicon
and the list of multi-word expressions are compiled.
3.3 Chunk Parsing
Next, the analyses of the tokens are transformed
into a feature structure format, and are passed to
the parsing component.8 The output of this stage
is an intermediate semantic representation of (as-
pects of) the content (of which the notation shown
in 1 is a variant). This format is akin to traditional
logical forms and still has to be mapped into OWL;
we decided on this strategy because such a format
is closer to surface structure and hence easier to
build compositionally (see discussion below in Sec-
tion 3.5). Also note that the semantics is ?flat?, and
does not represent scope of quantifiers (which only
very rarely occur in our data, and cannot be repre-
sented OWL in any case).
To get an idea of the feature geometry used by the
grammar see Figure 4; this figure also shows the se-
mantic representations generated at this stage (in a
different notation than in Figure fig:reps). Note the
?simulation? of typing of feature structures, and the
representation of properties via second order prop-
erties as discussed above. Chunk parsing is per-
formed by a chart parser running a grammar that is
loosely inspired by HPSG (Pollard and Sag, 1994).9
The grammar contains context-free rules for fairly
complex NPs (allowing arguments of Ns, modifi-
cation by PPs, and coordination). When extracting
chunks, the strategy followed by the system is to al-
ways extract the largest non-overlapping chunks.10
An example might help to illustrate the robust-
8Up until here, all steps are performed in one go for the
whole document. The subsequent steps, on the other hand, are
performed incrementally for each sentence. This allows the
system to remove ambiguity when it occurs, rather than having
to maintain and later filter out different analyses.
9The parser is implemented in PROLOG, and based on the
simple algorithm given in (Gazdar and Mellish, 1989). It also
uses code by Michael Covington for dealing with feature struc-
tures in PROLOG, which is described in (Covington, 1994).
10That strategy will prefer lenght of individual chunks over
coverage of input, for example when there is one big chunk and
two overlapping smaller chunks at each side of that chunk, that
however together span more input.
??
?
?
?
?
?
?
?
?
?
?
?
?
SYN
?
?
?
?
?
CAT np
HEAD
?
?
CASE nom
AGR
[
NUM sg
PER dr
GEN neu
]
?
?
COMP nil
?
?
?
?
?
SEM
?
?
?
RESTR
?
[
RELTYPE det
TYPE unspec
ARG x3
][
RELTYPE ent
TYPE stanzbiopsat
INST x3
]
?
INDEX x3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
SYN
?
?
?
?
?
CAT np
HEAD
?
?
CASE acc
AGR
[
NUM pl
PER dr
GEN fem
]
?
?
COMP nil
?
?
?
?
?
SEM
?
?
?
?
?
?
?
?
?
RESTR
?
[
RELTYPE det
TYPE unspec plur
ARG x1
]
[
RELTYPE ent
TYPE infiltrat
ARG x2
INST x1
]
[
RELTYPE det
TYPE indef
ARG x2
]
[
RELTYPE prop
TYPE consistency
ARG x2
VALUE solid
]
[
RELTYPE prop
TYPE malignity
ARG x2
VALUE malign
][
RELTYPE prop
TYPE position
ARG x2
VALUE epithelial
]
[
RELTYPE ent
TYPE neoplasia
INST x2
]
?
INDEX x1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 4: The chunks extracted from Sentence 2
ness of the system. (2) shows a full syntactic analy-
sis of our example sentence. Our system only recog-
nises the chunks indicated by the brackets printed
in bold typeface: since it can?t recognise the pred-
icative use of the verb here, it is satisfied with just
building parses for the NPs it does recognise. (The
round brackets around the analysis of the first word
indicate that this parse is strictly speaking not cor-
rect if the full structure is respected.)
(2) [NP ([NP) [NOM Stanzbiopsat] (]), [ADJP [VVPP2
eingenommen] [PP [P durch] [NP Infiltrate einer
soliden malignen epithelialen Neoplasie.]]]]?
This is an example of the system?s tolerance to un-
known structure; (3) shows a (constructed) exam-
ple of an NP where the structure is covered by the
grammar, but there are ?unknown? (or rather, irrele-
vant) lexical items. As described above, we assign
a ?dummy semantics? (here, a property that is true
of all entities) to words that are irrelevant to the do-
main, and so parsing can proceed.
(3) Solid, hardly detectable tumor cells. ?
solid(x) ? true(x) ? tumor cell(x)
A few last remarks about the grammar. First, as
shown in Figure 4, NPs without determiner intro-
duce an underspecified relation unspec det, and in-
formation about definiteness and number of deter-
miners is represented. This means that all infor-
mation to do discourse processing (bridging of def-
inites to antecedents) is there; we plan to exploit
such information in later incarnations of the sys-
tem. Secondly, it can of course occur that there is
more than one analysis spanning the same input;
i.e., we can have syntactic ambiguity. This will be
dealt with in the transformation component, where
domain knowledge is used to only let through ?plau-
sible? analyses.
Lastly, prepositions are another source for under-
specification. For instance, given as input the string
(4), the parser will compute a semantics where an
underspecified with rel connects the two entities
tumor and alveolar; this relation will be specified
in the next step, using domain knowledge, to a rela-
tion contains.
(4) Ein Tumor mit freien Alveolaren.
A tumor with free alveolars.
3.4 Resolution of Underspecification using
Ontologies
As described in the previous sections, the output of
the parser (and of the morphological analysis) might
still contain underspecified relations. These are re-
solved in the module described in this section. This
module sends a query to a reasoning component that
can perform inference over the ontology, asking for
possible relations that can hold between (instances
of) entities. For example (4) above, this will return
the answer contains, since the ontology specifies
that ?alveolars? are parts of tumours (via a chain of
is-a-relations linking tumours with cells, and cells
with alveolars). In a similar way the underspecifi-
cation of compound nouns is resolved. This process
proceeds recursively, ?inside-out?, since compound
nouns can of course be embedded in NPs that are
parts of PPs, and so on.
3.5 Mapping LF to OWL
In the final step, the logical forms produced by the
parser and specified by the previous module are
transformed into OWL-compliant representations.
This process is fairly straightforward, as should be
clear from comparing the intermediate representa-
tion in Figure 1 with the target representation: a)
unique identifiers for the instances of concepts are
generated; b) in cases of plural entities (?three sam-
ples? ? card(x, 3) ? sample(x)), several separate
instances are created; and c) appropriateness condi-
tions for properties are applied: if a property is not
defined for a certain type of entity, the analysis is
rejected.
This translation step also handles potential syn-
tactic ambiguity, since it can filter out analyses
if they specify inconsistent information. Note
also that certain information, e.g. about second
order properties, might be lost, due to the re-
stricted expressivity of OWL. E.g., an expres-
sion like ?highly polymorpheous? in Figure 1 ei-
ther has to be converted into a representation like
polymorphism : high, or the modification is lost
(polymorpheous(x)).
This ends our brief description of the system. We
now discuss a preliminary evaluation of the mod-
ules, related work, and further extensions of the sys-
tem we are currently working on or which we are
planning.
4 Evaluation
At the moment, we have only evaluated the mod-
ules individually, and?since the system is still un-
der developement?this evaluation only provides a
snapshot of the current state of developement. A
full-scale evaluation of the whole system in its ap-
plication context is planned as soon as the modules
are finalised; plans for this are discussed below.
The coverage of the morphology module and the
POS-tagger have already been reported above, so we
concentrate here on the chunk-parser. To evaluate
this module, we have manually annotated the NPs
in a randomly selected test set of 20 reports (ca.
2,800 words; we found about 500 NPs). The re-
ports were then morphologically analysed and POS-
filtered, and the results were manually checked and
corrected, to ensure that the input was optimal and
really only the performance of the chunker was eval-
uated. We then computed precision and recall based
on two different matching criteria: for exact match-
ing, where only exact congruence of chunks counts,
a precision of 48% and a recall of 63% was com-
puted; the numbers improve when partial matches,
i.e. smaller chunks within the target chunk, receive
partial credit (by a factor of .25), resulting in a (re-
laxed) precision of 61% and a (relaxed) recall of
80%. This difference can be explained by the fact
that some of the more complex NP-constructions
(with quite complex modifications) in our data are
not yet covered by the grammar, and only their con-
stituent NPs are recognised.
Note that this evaluation just takes into account
the boundaries of the chunks and not the correct-
ness of the computed semantic representations. For
a full-scale evaluation, we will manually annotate
these NPs with semantic representations, and we
will use this to compute precision and recall also
with respect to semantics, and ultimately with re-
spect to sample search queries. This annotation,
however, is very resource-intensive, and so will only
be done once the modules have been finalised.
5 Related Work
Acquisition of information from texts especially
from the medical domain is a lively research area.
Among the many projects in that field, we share
some of our central concerns with the medSyn-
diKAte system (Hahn et al, 2002): robust text anal-
ysis of medical reports; a background knowledge
base for guiding the analysis and storing the text?s
content; emphasis on handling co-reference phe-
nomena. What distinguishes LUPUS from medSyn-
diKAte, though, is foremost the parsing scheme: the
language used in the reports analysed by Hahn et al
is much closer to ?natural? language in that it con-
tains sentences with tensed verbs. Accordingly, they
use a variant of dependency parsing which is driven
by verb information. As described in Section 2.2
above, this is not an option for us, given the style of
our input texts, and hence our data renders a bottom-
up chart parsing approach much more promising.
Besides this difference, the work in medSynDiKAte
predates the emergence of XML/web ontology stan-
dards and thus uses an earlier description logic
knowledge representation language; we are hoping
that by using a standard we will be able to allow
even future semantic web technologies to work with
our data.
As for the robust analysis side, (Grover et al,
2002), also use a similar preprocessing pipeline
in combination with parsing. However, they also
focus on more ?natural? input texts (Medline ab-
stracts), and they use statistical rather than sym-
bolic/ontology based methods for computing the
meaning of compound nouns.
6 Summary and Further Work
We have described LUPUS, an NLP system that
makes use of a domain ontology to guide extraction
of information about entities from medical texts,
and represents this information as instances of con-
cepts from that ontology. Besides its direct use for
content-based search on these texts, the fact that the
system relies entirely on emerging semantic web
standards will make the resulting annotated infor-
mation usable for all kinds of agents working with
such data.
As a next step, we plan to add discourse process-
ing to the pipeline (see e.g. (Hahn et al, 1998) for
a discussion why such a step is required even for
such relatively simple texts). As mentioned above,
the prerequisite information (about definite articles,
for example) is already there; we plan to use the
available domain knowledge to guide the search for
antecedents for bridging. As a more technical im-
provement we are investigating ways of making the
architecture less pipeline-y, and to integrate domain
reasoning in computing edges in the chart. Lastly,
we are also working on a large-scale evaluation of
the system, by manually annotating reports to com-
pute precision and recall.
Acknowledgements
We thank the anonymous reviewers for their helpful
comments. Thanks are also due to Thomas Hanneforth
and Bryan Jurish for their help with integrating their
modules, and to our student assistant Sebastian Maar for
doing much of the actual coding.
References
Anita Burgun and Oliver Bodenreider. 2001. Mapping
the UMLS semantic network into general ontologies.
In Proceedings of the AMIA Symposium.
Michael A. Covington. 1994. GULP 3.1: An extension
of prolog for unification-based grammar. Technical
Report AI-1994-06, University of Georgia.
F. Demichellis, V. Della Mea, S. Forti, P. Dalla Palma,
and C.A. Beltrami. 2002. Digital storage of glass
slide for quality assurance in histopathology and cy-
topathology. Telemedicine and Telecare, 8(3):138?
142.
Gerald Gazdar and Chris Mellish. 1989. Natural Lan-
guage Processing in PROLOG. Addison-Wesley,
Wokingham, England.
Claire Grover, Ewan Klein, Mirella Lapata, and
Alex Lascarides. 2002. XML-based NLP tools for
analysing and annotating medical language. In Pro-
ceedings of the 2nd Workshop on NLP and XML,
Taipei, Taiwan, September.
Udo Hahn, Martin Romacker, and Stefan Schulz. 1998.
Why discourse structures in medical reports matter
for the validity of automatically generated text knowl-
edge bases. In MedInfo ?98 ? Proceedings of the 9th
World Congress on Medical Informatics, pages 633?
638, Seoul, Korea, August.
Udo Hahn, Martin Romacker, and Stefan Schulz. 2002.
Creating knowledge repositories from biomedical re-
ports: The medsyndikate text mining system. In Pa-
cific Symposium on Biocomputing, pages 338?349,
Hawai, USA, January.
Bryan Jurish. 2003. Part-of-speech tagging with finite
state morphology. In Proceedings of the Workshop on
Collocations and Idioms: Linguistic, Computational
and Psycholinguistic Perspectives, Berlin, Germany,
September.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. CSLI / The University of
Chicago Press, Chicago and London.
Kai Saeger, Karsten Schlu?ns, Thomas Schrader, and Pe-
ter Hufnagl. 2003. The virtual microscope for routine
pathology based on a pacs system for 6 gb images.
In Proceedings of the 17th International Congress on
Computer Assisted Radiology and Surgery (CARS),
pages 299?304, London, UK, June.
David Schlangen. 2003. A Coherence-Based Approach
to the Interpretation of Non-Sentential Utterances in
Dialogue. Ph.D. thesis, School of Informatics, Uni-
versity of Edinburgh, Edinburgh, UK.
Stefan Schulz and Udo Hahn. 2001. Medical knowledge
engineering?converting major portions of the umls
into a terminological knowledge base. International
Journal of Medical Informatics.
J. Slodowksa, K. Kayser, and P. Hasleton. 2002. Tele-
consultation in the chest disorders. European Journal
for Medical Research, 7(Suppl.I):80.
SNOMED International. 2004. SNOMED clinical terms.
http://www.snomed.org/index.html.
The HL7 Consortium. 2003. HL7 version 2.5 ANSI
standard, June. http://www.hl7.org.
The UMLS Consortium. 2003. UMLS release 2003AC.
http://www.nlm.nih.gov/research/umls/.
W3C WebOnt WG. 2004. OWL web ontology language
overview. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-owl-features-
20040210/.
Proceedings of the Linguistic Annotation Workshop, pages 191?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Panel Session: Discourse Annotation
Manfred Stede
Dept. of Linguistics
University of Potsdam
stede@ling.uni-potsdam.de
Janyce Wiebe
Dept. of Computer Science
University of Pittsburgh
wiebe@cs.pitt.edu
Eva Hajic?ova?
Faculty of Math. and Physics
Charles University
hajicova@ufal.ms.mff.cuni.cz
Brian Reese
Dept. of Linguistics
Univ. of Texas at Austin
bjreese@mail.utexas.edu
Simone Teufel
Computer Laboratory
Univ. of Cambridge
sht25@cl.cam.uk
Bonnie Webber
School of Informatics
Univ. of Edinburgh
bonnie@inf.ed.ac.uk
Theresa Wilson
Dept. of Comp. Science
Univ. of Pittsburgh
twilson@cs.pitt.edu
1 Introduction
The classical ?success story? of corpus annotation
are the various syntax treebanks that provide struc-
tural analyses of sentences and have enabled re-
searchers to develop a range of new and highly suc-
cessful data-oriented approaches to sentence pars-
ing. In recent years, however, a number of corpora
have been constructed that provide annotations on
the discourse level, i.e. information that reaches be-
yond the sentence boundaries. Phenomena that have
been annotated include coreference links, the scope
of connectives, and coherence relations. Many of
these are phenomena on whose handling there is
not a general agreement in the research community,
and therefore the question of ?recycling? corpora by
other people and for other purposes is often diffi-
cult. (To some extent, this is due to the fact that dis-
course annotation deals ?only? with surface reflec-
tions of underlying, abstract objects.) At the same
time, the efforts needed for building high-quality
discourse corpora are considerable, and thus one
should be careful in deciding how to invest those ef-
forts. One aspect of providing added-value with an-
notation projects is that of shared corpora: If a vari-
ety of annotation efforts is executed on the same pri-
mary data, the series of annotation levels can yield
insights that the creators of the individual levels had
not explicitly planned for. A clear case is the rela-
tionship between coherence relations and connective
use: When both levels are marked individually and
with independent annotation guidelines, then after-
wards the correlations between coherence relations,
cue usage (and possibly other factors, if annotated)
can be studied systematically. This conception of
multi-level annotation presupposes, of course, that
the technical problems of setting annotation levels
in correspondence to one another be resolved.
The panel on discourse annotation is organized
by Manfred Stede and Janyce Wiebe. It aims at
surveying the scene of discourse corpora, exploring
chances for synergy, and identifying desiderata for
future corpus creation projects. In preparation for
the panel, the participants have provided the follow-
ing short descriptions of the various copora in whose
construction they have been involved.
2 Prague Dependency Treebank
(Eva Hajic?ova?, Prague)
One of the maxims of the work on the Prague De-
pendency Treebank is that one should not overlook,
disregard and thus lose what the sentence structure
offers when one attempts to analyze the structure of
discourse, thus moving from ?the trees? to ?the for-
est?. Therefore, we emphasize that discourse anno-
tation should make use of every possible detail the
annotation of the component parts of the discourse,
namely the sentences, puts at our disposal. This
is, of course, not only true for the surface shape of
the sentence (i.e., the surface means of expression),
but (and most importantly) for the underlying repre-
sentation of sentences. The panel contribution will
introduce the (multilayered) annotation scenario of
the Prague Dependency Treebank and illustrate the
point using some of the particular features of the un-
derlying structure of sentences that can be made use
of in planning the scenario of discourse ?treebanks?.
191
3 SDRT in Newspaper Text
(Brian Reese, Austin)
We are currently working under the auspices of
an NSF grant to build and train a discourse parser
and codependent anaphora resolution program to
test discourse theories empirically. The training re-
quires the construction of a corpus annotated with
discourse structure and coreference information. So
far, we have annotated the MUC61 corpus for dis-
course structure and are in the process of annotating
the ACE22 corpus; both corpora are already anno-
tated for coreference. One of the goals of the project
is to investigate whether using the right frontier con-
straint improves the system?s performance in resolv-
ing anaphors. Here we detail some experiences we
have had with the discourse annotation process.
An implementation of the extant SDRT (Asher and
Lascarides, 2003) glue logic for building discourse
structures is insufficient to deal with open domain
text, and we cannot envision an extended version
at the present time able to deal with the problem.
Thus, we have opted for a machine learning based
approach to discourse parsing based on superficial
features, like BNL. To build an implementation to
test these ideas, we have had to devise a corpus of
texts annotated for discourse structure in SDRT.
Each of the 60 texts in the MUC6 corpus, and now
18 of the news stories in ACE2, were annotated by
two people familiar with SDRT. The annotators then
conferred and agreed upon a gold standard. Our
annotation effort took the hierarchical structure of
SDRT seriously and built graphs in which the nodes
are discourse units and the arcs represent discourse
relations between the units. The units could either be
simple (elementary discourse units: EDUs) or they
could be complex. We assumed that in principle the
units were recursively generated and could have an
arbitrary though finite degree of complexity.
4 Potsdam Commentary Corpus
(Manfred Stede, Potsdam)
Construction of the Potsdam Commentary Corpus
(PCC) began in 2003 and is still ongoing. It is a
1The Message Understanding Conference, www-nlpir.
nist.gov/related projects/muc/.
2The Automated Content Extraction program,
www.nist.gov/speech/tests/ace/.
genre-specific corpus of German newspaper com-
mentaries, taken from the daily papers Ma?rkische
Allgemeine Zeitung and Tagesspiegel. One central
aim is to provide a tool for studying mechanisms
of argumentation and how they are reflected on the
linguistic surface. The corpus on the one hand is a
collection of ?raw? data, which is used for genre-
oriented statistical explorations. On the other hand,
we have identified two sub-corpora that are subject
to a rich multi-level annotation (MLA).
The PCC176 (Stede, 2004) is a sub-corpus that
is available upon request for research purposes. It
consists of 176 relatively short commentaries (12-
15 sentences), with 33.000 tokens in total. The
sentences have been PoS-tagged automatically (and
manually checked); sentence syntax was anno-
tated semi-automatically using the TIGER scheme
(Brants et al, 2002) and Annotate3 tool. In addition,
we annotated coreference (PoCos (Krasavina and
Chiarcos, 2007)) and rhetorical structure according
to RST (Mann and Thompson, 1988). Our anno-
tation software architecture consists of a variety of
standard, external tools that can be used effectively
for the different annotation types. Their XML output
is then automatically converted to a generic format
(PAULA, (Dipper, 2005)), which is read into the lin-
guistic database ANNIS (Dipper et al, 2004), where
the annotations are aligned, so that the data can be
viewed and queried across annotation levels.
The PCC10 is a sub-corpus of 10 commentaries
that serves as ?testbed? for further developing the
annotation levels. On the one hand, we are apply-
ing recent guidelines on annotation of information
structure (Go?tze et al, 2007). On the other hand,
based on experiences with the RST annotation, we
are replacing the rhetorical trees with a set of dis-
tinct, simpler annotation layers: thematic structure,
conjunctive relations (Martin, 1992), and argumen-
tation structure (Freeman, 1991); these are comple-
mented by the other levels mentioned above for the
PCC176. The primary motivation for this step is the
high degree of arbitrariness that annotators reported
when producing the RST trees (see (Stede, 2007)).
By separating the thematic from the intentional in-
formation, and accounting for the surface-oriented
3www.coli.uni-saarland.de/projects/
sfb378/negra-corpus/annotate.html
192
conjunctive relations (which are similar to what is
annotated in the PDTB, see Section 6), we hope to
? make annotation easier: handling several ?sim-
ple? levels individually should be more effec-
tive than a single, very complex annotation
step;
? end up with less ambiguity in the annotations,
since the reasons for specific decisions can be
made explicit (by annotations on ?simpler? lev-
els);
? be more explicit than a single tree can be: if a
discourse fulfills, for example, a function both
for thematic development and for the writer?s
intention, they can both be accounted for;
? provide the central information that a ?tradi-
tional? rhetorical tree conveys, without loosing
essential information.
5 AZ Corpus
(Simone Teufel, Cambridge)
The Argumentative Zoning (AZ) annotation scheme
(Teufel, 2000; Teufel and Moens, 2002) is con-
cerned with marking argumentation steps in scien-
tific articles. One example for an argumentation step
is the description of the research goal, another an
overt comparison of the authors? work with rival ap-
proaches. In our scheme, these argumentation steps
have to be associated with text spans (sentences or
sequences of sentences). AZ?Annotation is the la-
belling of each sentence in the text with one of these
labels (7 in the original scheme in (Teufel, 2000)).
The AZ labels are seen as relations holding between
the meanings of these spans, and the rhetorical act
of the entire paper. (Teufel et al, 1999) reports on
interannotator agreement studies with this scheme.
There is a strong interrelationship between the ar-
gumentation in a paper, and the citations writers use
to support their argument. Therefore, a part of the
computational linguistics corpus has a second layer
of annotation, called CFC (Teufel et al, 2006) or
Citation Function Classification. CFC? annotation
records for each citation which rhetorical function it
plays in the argument. This is following the spirit of
research in citation content analysis (e.g., (Moravc-
sik and Murugesan, 1975)). An example for a ci-
tation function would be ?motivate that the method
used is sound?. The annotation scheme contains
12 functions, clustered into ?superiority?, ?neutral
comparison/contrast?, ?praise or usage? and ?neu-
tral?.
One type of research we hope to do in the future
is to study the relationship between these rhetori-
cal phonemena with more traditional discourse phe-
nomena, e.g. anaphoric expressions.
The CmpLg/ACL Anthology corpora consist of
320/9000 papers in computational linguistics. They
are partially annotated with AZ and CFC markup. A
subcorpus of 80 parallelly annotated papers (AZ and
CFF) can be obtained from us for research (12000
sentences, 1756 citations). We are currently port-
ing both schemes to chemistry in the framework
of the EPSRC-sponsored project SciBorg. In the
course of this work a larger, more general AZ an-
notation scheme was developed. The SciBorg effort
will result in an AZ/CFC?annotated chemistry cor-
pus available to the community in 2009.
In terms of challenges, the most time-consuming
aspects of creating this annotated corpus were for-
mat conversions on the corpora, and cyclic adapta-
tions of scheme and guidelines. Another problem is
the simplification of annotating only full sentences;
sometimes, annotators would rather mark a clause
or sometimes even just an NP. However, we found
these cases to be relatively rare.
6 Penn Discourse Treebank
(Bonnie Webber, Edinburgh)
The Penn Discourse TreeBank (Miltsakaki et al,
2004; Prasad et al, 2004; Webber, 2005) anno-
tates discourse relations over the Wall Street Jour-
nal corpus (Marcus et al, 1993), in terms of dis-
course connectives and their arguments. Following
the approach towards discourse structure in (Webber
et al, 2003), the PDTB takes a lexicalized approach,
treating discourse connectives as the anchors of the
relations and thus as discourse-level predicates tak-
ing two Abstract Objects as their arguments. An-
notated are the text spans that give rise to these ar-
guments. There are primarily two types of connec-
tives in the PDTB: explicit and implicit, the latter
being inserted between adjacent paragraph-internal
sentence pairs not related by an explicit connective.
193
Also annotated in the PDTB is the attribution of
each discourse relation and of its arguments (Dinesh
et al, 2005; Prasad et al, 2007). (Attribution itself
is not considered a discourse relation.) A prelimi-
nary version of the PDTB was released in April 2006
(PDTB-Group, 2006), and is available for download
at http://www.seas.upenn.edu/?pdtb. This release only has
implicit connectives annotated in three sections of
the corpus. The annotation of all implicit connec-
tives, along with a hierarchical semantic classifica-
tion of all connectives (Miltsakaki et al, 2005), will
appear in the final release of the PDTB in August
2007.
Here I want to mention three of the challenges we
have faced in developing the PDTB:
(I) Words and phrases that can function as con-
nectives can also serve other roles. (Eg, when can be
a relative pronoun, as well as a subordinating con-
junction.) It has been difficult to identify all and
only those cases where a token functions as a dis-
course connective, and in many cases, the syntactic
analysis in the Penn TreeBank (Marcus et al, 1993)
provides no help. For example, is as though always a
subordinating conjunction (and hence a connective)
or do some tokens simply head a manner adverbial
(eg, seems as though . . . versus seems more rushed
as though . . . )? Is also sometimes a discourse con-
nective relating two abstract objects and other times,
an adverb that presupposes that a particular property
holds of some other entity? If so, when one and
when the other? In the PDTB, annotation has erred
on the side of false positives.
(II) In annotating implicit connectives, we discov-
ered systematic non-lexical indicators of discourse
relations. In English, these include cases of marked
syntax (eg, Had I known the Queen would be here,
I would have dressed better.) and cases of sentence-
initial PPs and adjuncts with anaphoric or deictic
NPs such as at the other end of the spectrum, adding
to that speculation. These cases labelled ALTLEX,
for ?alternative lexicalisation? have not been anno-
tated as connectives in the PDTB because they are
fully productive (ie, not members of a more eas-
ily annotated closed set of tokens). They comprise
about 1% of the cases the annotators have consid-
ered. Future discourse annotation will benefit from
further specifying the types of these cases.
(III) The way in which spans are annotated as ar-
guments to connectives also raises a challenge. First,
because the PDTB annotates both structural and
anaphoric connectives (Webber et al, 2003), a span
can serve as argument to >1 connective. Secondly,
unlike in the RST corpus (Carlson et al, 2003) or the
Discourse GraphBank (Wolf and Gibson, 2005), dis-
course segments are not separately annotated, with
annotators then identifying what discourse relations
hold between them. Instead, in annotating argu-
ments, PDTB annotators have selected the minimal
clausal text span needed to interpret the relation.
This could comprise an embedded, subordinate or
coordinate clause, an entire sentence, or a (possi-
bly disjoint) sequence of sentences. As a result,
there are fairly complex patterns of spans within and
across sentences that serve as arguments to differ-
ent connectives, and there are parts of sentences that
don?t appear within the span of any connective, ex-
plicit or implicit. The result is that the PDTB pro-
vides only a partial but complexly-patterned cover
of the corpus. Understanding what?s going on and
what it implies for discourse structure (and possibly
syntactic structure as well) is a challenge we?re cur-
rently trying to address (Lee et al, 2006).
7 MPQA Opinion Corpus
(Theresa Wilson, Pittsburgh)
Our opinion annotation scheme (Wiebe et al, 2005)
is centered on the notion of private state, a gen-
eral term that covers opinions, beliefs, thoughts, sen-
timents, emotions, intentions and evaluations. As
Quirk et al (1985) define it, a private state is a state
that is not open to objective observation or verifica-
tion. We can further view private states in terms of
their functional components ? as states of experi-
encers holding attitudes, optionally toward targets.
For example, for the private state expressed in the
sentence John hates Mary, the experiencer is John,
the attitude is hate, and the target is Mary.
We create private state frames for three main types
of private state expressions (subjective expressions)
in text:
? explicit mentions of private states, such as
?fears? in ?The U.S. fears a spill-over?
? speech events expressing private states, such as
?said? in ?The report is full of absurdities,?
194
Xirao-Nima said.
? expressive subjective elements, such as ?full of
absurdities? in the sentence just above.
Frames include the source (experiencer) of the
private state, the target, and various properties such
as polarity (positive, negative, or neutral) and inten-
sity (high, medium, or low). Sources are nested. For
example, for the sentence ?China criticized the U.S.
report?s criticism of China?s human rights record?,
the source is ?writer, China, U.S. report?, reflecting
the facts that the writer wrote the sentence and the
U.S. report?s criticism is the target of China?s criti-
cism. It is common for multiple frames to be created
for a single clause, reflecting various levels of nest-
ing and the type of subjective expression.
The annotation scheme has been applied to a
corpus, called the ?Multi-Perspective Question An-
swering (MPQA) Corpus,? reflecting its origins in
the 2002 NRRC Workshop on Multi-Perspective
Question Answering (MPQA) (Wiebe et al, 2003)
sponsored by ARDA AQUAINT (it is also called
?OpinionBank?). It contains 535 documents and a
total of 11,114 sentences. The articles in the cor-
pus are from 187 different foreign and U.S. news
sources, dating from June 2001 to May 2002. Please
see (Wiebe et al, 2005) and Theresa Wilson?s forth-
coming PhD dissertation for further information, in-
cluding the results of inter-coder agreement studies.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In J. van
Kuppevelt & R. Smith, editor, Current Directions in
Discourse and Dialogue. Kluwer, New York.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005. At-
tribution and the (non-)alignment of syntactic and dis-
course arguments of connectives. In ACL Workshop
on Frontiers in Corpus Annotation, Ann Arbor MI.
Stefanie Dipper, Michael Go?tze, Manfred Stede, and Till-
mann Wegst. 2004. Annis: A linguistic database for
exploring information structure. In Interdisciplinary
Studies on Information Structure, ISIS Working papers
of the SFB 632 (1), pages 245?279.
Stefanie Dipper. 2005. XML-based stand-off represen-
tation and exploitation of multi-level linguistic annota-
tion. In Rainer Eckstein and Robert Tolksdorf, editors,
Proceedings of Berliner XML Tage, pages 39?50.
James B. Freeman. 1991. Dialectics and the
Macrostructure of Argument. Foris, Berlin.
Michael Go?tze, Cornelia Endriss, Stefan Hinterwimmer,
Ines Fiedler, Svetlana Petrova, Anne Schwarz, Stavros
Skopeteas, Ruben Stoel, and Thomas Weskott. 2007.
Information structure. In Information structure in
cross-linguistic corpora: annotation guidelines for
morphology, syntax, semantics, and information struc-
ture, volume 7 of ISIS Working papers of the SFB 632,
pages 145?187.
Olga Krasavina and Christian Chiarcos. 2007. Potsdam
Coreference Scheme. In this volume.
Alan Lee, Rashmi Prasad, Aravind Joshi, Nikhil Dinesh,
and Bonnie Webber. 2006. Complexity of dependen-
cies in discourse. In Proc. 5th Workshop on Treebanks
and Linguistic Theory (TLT?06), Prague.
William Mann and Sandra Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. TEXT, 8:243?281.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large scale anno-
tated corpus of English: The Penn TreeBank. Compu-
tational Linguistics, 19:313?330.
James R. Martin. 1992. English text: system and struc-
ture. John Benjamins, Philadelphia/Amsterdam.
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. Annotating discourse connec-
tives and their arguments. In NAACL/HLT Workshop
on Frontiers in Corpus Annotation, Boston.
Eleni Miltsakaki, Nikhil Dinesh, Rashmi Prasad, Ar-
avind Joshi, and Bonnie Webber. 2005. Experiments
on sense annotation and sense disambiguation of dis-
course connectives. In 4t Workshop on Treebanks and
Linguistic Theory (TLT?05), Barcelona, Spain.
Michael J. Moravcsik and Poovanalingan Murugesan.
1975. Some results on the function and quality of ci-
tations. Soc. Stud. Sci., 5:88?91.
The PDTB-Group. 2006. The Penn Discourse TreeBank
1.0 annotation manual. Technical Report IRCS 06-01,
University of Pennsylvania.
195
Rashmi Prasad, Eleni Miltsakaki, Aravind Joshi, and
Bonnie Webber. 2004. Annotation and data mining
of the Penn Discourse TreeBank. In ACL Workshop
on Discourse Annotation, Barcelona, Spain, July.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Aravind Joshi,
and Bonnie Webber. 2007. Attribution and its annota-
tion in the Penn Discourse TreeBank. TAL (Traitement
Automatique des Langues.
Randolph Quirk, Sidney Greenbaum, Geoffry Leech, and
Jan Svartvik. 1985. A Comprehensive Grammar of the
English Language. Longman, New York.
Manfred Stede. 2004. The Potsdam commentary corpus.
In Proceedings of the ACL Workshop on Discourse An-
notation, pages 96?102, Barcelona.
Manfred Stede. 2007. RST revisited: disentangling nu-
clearity. In Cathrine Fabricius-Hansen and Wiebke
Ramm, editors, ?Subordination? versus ?coordination?
in sentence and text ? from a cross-linguistic perspec-
tive. John Benjamins, Amsterdam. (to appear).
Simone Teufel and Marc Moens. 2002. Summaris-
ing scientific articles ? experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?446.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumenta-
tion in research articles. In Proceedings of the 9th Eu-
ropean Conference of the ACL (EACL-99), pages 110?
117, Bergen, Norway.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. An annotation scheme for citation function. In
Proceedings of SIGDIAL-06, Sydney, Australia.
Simone Teufel. 2000. Argumentative Zoning: Infor-
mation Extraction from Scientific Text. Ph.D. thesis,
School of Cognitive Science, University of Edinburgh,
Edinburgh, UK.
Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-
istair Knott. 2003. Anaphora and discourse structure.
Computational Linguistics, 29:545?587.
Bonnie Webber. 2005. A short introduction to the Penn
Discourse TreeBank. In Copenhagen Working Papers
in Language and Speech Processing.
Janyce Wiebe, Eric Breck, Chris Buckley, Claire Cardie,
Paul Davis, Bruce Fraser, Diane Litman, David Pierce,
Ellen Riloff, Theresa Wilson, David Day, and Mark
Maybury. 2003. Recognizing and organizing opinions
expressed in the world press. In Working Notes of the
AAAI Spring Symposium in New Directions in Ques-
tion Answering, pages 12?19, Palo Alto, California.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
39(2/3):164?210.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31:249?287.
196
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35?43,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
By all these lovely tokens...?
Merging Conflicting Tokenizations
Christian Chiarcos, Julia Ritz and Manfred Stede
Sonderforschungsbereich 632 ?Information Structure?
University of Potsdam
Karl-Liebknecht-Str. 24-25, 14476 Golm, Germany
{chiarcos|julia|stede}@ling.uni-potsdam.de
Abstract
Given the contemporary trend to modular
NLP architectures and multiple annotation
frameworks, the existence of concurrent
tokenizations of the same text represents
a pervasive problem in everyday?s NLP
practice and poses a non-trivial theoretical
problem to the integration of linguistic an-
notations and their interpretability in gen-
eral. This paper describes a solution for
integrating different tokenizations using a
standoff XML format, and discusses the
consequences for the handling of queries
on annotated corpora.
1 Motivation
1.1 Tokens: Functions and goals
For most NLP tasks and linguistic annotations,
especially those concerned with syntax (part-of-
speech tagging, chunking, parsing) and the inter-
pretation of syntactic structures (esp., the extrac-
tion of semantic information), tokens represent
the minimal unit of analysis: words (lexemes,
semantic units, partly morphemes) on the one
hand and certain punctuation symbols on the other
hand. From a corpus-linguistic perspective, tokens
also represent the minimal unit of investigation,
the minimal character sequence that can be ad-
dressed in a corpus query (e.g. using search tools
like TIGERSearch (Ko?nig and Lezius, 2000) or
CWB (Christ, 1994)). Tokens also constitute the
basis for ?word? distance measurements. In many
annotation tools and their corresponding formats,
the order of tokens provides a timeline for the
sequential order of structural elements (MMAX
(Mu?ller and Strube, 2006), GENAU (Rehm et al,
2009), GrAF (Ide and Suderman, 2007), TIGER
XML (Ko?nig and Lezius, 2000)). In several multi-
?Taken from the poem September by Helen Hunt Jackson.
layer formats, tokens also define the absolute po-
sition of annotation elements, and only by refer-
ence to a common token layer, annotations from
different layers can be related with each other
(NITE (Carletta et al, 2003), GENAU).
Thus, by their function, tokens have the fol-
lowing characteristics: (i) tokens are totally or-
dered, (ii) tokens cover the full (annotated portion
of the) primary data, (iii) tokens are the smallest
unit of annotation, and (iv) there is only one sin-
gle privileged token layer. The last aspect is es-
pecially relevant for the study of richly annotated
data, as an integration and serialization of anno-
tations produced by different tools can be estab-
lished only by reference to the token layer. From
a corpus-linguistic perspective, i.e., when focus-
ing on querying of annotated corpora, tokens need
to be well-defined and all information annotated
to a particular text is to be preserved without any
corruption. We argue that for this purpose, char-
acteristic (iii) is to be abandoned, and we will de-
scribe the data format and an algorithm for merg-
ing different tokenizations and their respective an-
notations.
Our goal is a fully automated merging of anno-
tations that refer to different tokenizations (hence-
forth T ? and T ?) of the same text. We regard the
following criteria as crucial for this task:
Information preservation. All annotations ap-
plied to the original tokenizations should be pre-
served.
Theoretically well-defined notion of token. It
should be possible to give a plausible list of posi-
tive criteria that define character sequences as to-
kens. Knowledge about the token definition is es-
sential for formulating queries for words, e.g. in a
corpus search interface.
Integrative representation. All annotations that
are consistent with the merged tokenization should
refer to the merged tokenization. This is necessary
in order to query across multiple annotations orig-
35
inating from different annotation layers or tools.
Unsupervised merging. The integration of con-
flicting tokenizations should not require manual
interference.
1.2 Tokenization
Tokenization is the process of mapping sequences
of characters to sequences of words (cf. Guo
1997). However, different research questions or
applications induce different conceptions of the
term ?word?. For a shallow morphosyntactic anal-
ysis (part of speech tagging), a ?simple? tokeniza-
tion using whitespaces and punctation symbols as
delimiters seems acceptable for the examples in
(1). A full syntactic analysis (parsing), however,
could profit from the aggregation of complex nom-
inals into one token each.
(1) a. department store
b. Herzog-von der Heide1
c. Red Cross/Red Crescent movement
Similarly, examples (2a) and (2b) can be ar-
gued to be treated as one token for (mor-
pho)syntactic analyses, respectively. Despite in-
tervening whitespaces and punctuation symbols,
they are complex instances of the ?classical? part-
of-speech adjective. For certain semantic analyses
such as in information extraction, however, it may
be useful to split these compounds in order to ac-
cess the inherent complements (E 605, No. 22).
(2) a. E 605-intoxicated
b. No. 22-rated
Finally, (3) illustrates a morphology-based tok-
enization strategy: the principle of splitting at
morpheme boundaries (Marcus et al, 1993, PTB)
(token boundaries represented by square brack-
ets). Morphological tokenization may help distri-
butional (co-occurrence-based) semantics and/or
parsing; however, the resulting tokens might be
argued as being less intuitive to users of a corpus
search tool.
(3) a. [Mitchell][?s], [they][?ve], [do][n?t]
b. [wo][n?t], [ca][n?t], [ai][n?t]
These examples show that different applications
(tagging, parsing, information extraction) and the
focus on different levels of description (morphol-
ogy, syntax, semantics) require specialized tok-
enization strategies. When working with multiple
1Double surname consisting of Herzog and von der Heide.
tools for standard NLP tasks, thus, it is the norm
rather than the exception that they disagree in their
tokenization, as shown in ex. (4).
(4) doesn?t
a. [does][n?t] (Marcus et al, 1993, PTB)
b. [doesn][?][t] (Brants, 2000, TnT)
When creating a corpus that is annotated at multi-
ple levels and/or using several tools, different tok-
enizations are not always avoidable, as some tools
(automatic NLP tools, but also tools for manual
annotation) have integrated tokenizers. Another
challenge is the representation of token bound-
aries. Commonly, token boundaries are repre-
sented by a line break (?\n?) or the whitespace
?character? (? ?) ? in which case token-internal
whitespaces are replaced, usually by an under-
score (? ?) ?, thereby corrupting the original data.
This practice makes reconciling/merging the data
a difficult enterprise.
Given this background, we suggest an XML-
based annotation of token boundaries, such that
token boundaries are marked without affecting the
original primary data. In a straightforward XML
model, tokens are represented by XML elements
enclosing primary text slices (c.f. the BNC encod-
ing scheme (Burnard, 2007)). However, treating
tokens as spans of text by means of the XML hier-
archy is impossible for tokenization conflicts as in
(4.a) and (4.b).
2 Conflicting tokenizations:
Straightforward strategies
By ?straightforward strategies?, we mean ap-
proaches that aim to preserve the definition of to-
kens as atomic, minimal, unambiguous units of
annotation when unifying different tokenizations
(henceforth T ? and T ?) of the same text. By ?un-
supervised straightforward strategies?, we mean
tokenization strategies that operate on the primary
data only, without consulting external resources
such as dictionaries or human expertise.
Unsupervised straightforward strategies to the
task include:
1. no merging In a conservative approach, we
could create independent annotation projects for
every tokenization produced, and thus represent
all tokenizations independently. This, however,
rules out any integration or combined evaluation
of annotations to T ? and annotations to T ?.
36
2. normalization Adopt one of the source tok-
enizations, say T ?, as the ?standard? tokenization.
Preserve only the information annotated to T ? that
is consistent with T ?. Where tokenization T ? de-
viates from T ?, all annotations to T ? are lost.2
3. maximal tokens For every token boundary
in T ? that is also found in T ?, establish a token
boundary in the merged tokenization (cf. Guo?s
1997 ?critical tokenization?). However, with to-
kens assumed to be the minimal elements of anno-
tation, we lose linguistic analyses of fine-grained
tokens. With respect to (4.a) and (4.b), the max-
imal token would be the whole phrase doesn?t.
Again, this results in a loss of information, as all
annotations applied to does, doesn, n?t, ? and t re-
fer to units that are smaller than the resulting to-
ken.
4. maximal common substrings For every
token boundary in T ? or T ?, establish a token
boundary, thereby producing minimal tokens:
one token for every maximal substring shared
between T ? and T ? (cf. Guo?s 1997 ?shortest
tokenization?). By defining the original tokens
(?supertokens?) as annotations spanning over
tokens, all annotations are preserved. However,
the concept of ?token? loses its theoretical motiva-
tion; there is no guarantee that maximal common
substrings are meaningful elements in any sense:
The maximum common substring tokenization
of 4.a and 4.b is [does][n][?][t], but [n] is not
a well-defined token. It is neither defined with
respect to morphology (like PTB tokens) nor is
it motivated from orthography (like TnT tokens),
but it is just the remainder of their intersection.
As shown in Table 1, none of the strategies
sketched above fulfills all criteria identified in Sec-
tion 1.1: Avoiding a merging process counteracts
data integration; token normalization and maximal
tokens violate information preservation, and maxi-
mal common substrings violate the requirement to
specify a theoretically well-defined notion of to-
ken.
As an alternative, we propose a formalism for
the lossless integration and representation of con-
2Alternatively, transformation rules to map annotations
from T ? to T ? would have to be developed. This does, how-
ever, not guarantee information preservation, and, addition-
ally, it requires manual work, as such transformations are
annotation-specific. Thus, it is not an option for the fully
automated merging of tokenizations.
Table 1: Deficits of ?straightforward? merging ap-
proaches
no normalize max. max. common
merge tokens substrings
information preservation
+ ? ? +
well-defined tokens
+ + (?) ?
integrative
? + + +
unsupervised
(+) + + +
flicting tokenizations by abandoning the assump-
tion that tokens are an atomic, primitive con-
cept that represents the minimal unit of annota-
tion. Rather, we introduce annotation elements
smaller than the actual token ? so-called termi-
nals or terms for short ? that are defined accord-
ing to the maximum common substrings strategy
described above.
Then, tokens are defined as nodes that span
over a certain range of terms similar to phrase
nodes that dominate other nodes in syntax annota-
tions. The representation of conflicting tokeniza-
tions, then, requires a format that is capable to
express conflicting hierarchies. For this purpose,
we describe an extension of the PAULA format, a
generic format for text-oriented linguistic annota-
tions based on standoff XML.
3 Conflicting tokenizations in the
PAULA format
3.1 Annotation structures in PAULA 1.0
The PAULA format (Dipper, 2005; Dipper and
Go?tze, 2005) is a generic XML format, used as a
pivot format in NLP pipelines (Stede et al, 2006)
and in the web-based corpus interface ANNIS
(Chiarcos et al, 2008). It uses standoff XML rep-
resentations, and is conceptually closely related to
the formats NITE XML (Carletta et al, 2003) and
GraF (Ide and Suderman, 2007).
PAULA was specifically designed to support the
lossless representation of different types of text-
oriented annotations (layer-based/timeline anno-
tations, hierarchical annotations, pointing rela-
tions), optimized for the annotation of multiple
layers, including conflicting hierarchies and sim-
ple addition/deletion routines for annotation lay-
ers. Therefore, primary data is stored in a separate
37
Table 2: PAULA 1.0 data types
nodes (structural units of annotation)
token character spans in the primary data that form the basis
for higher-level annotation
markable (spans of) token(s) that can be annotated with lin-
guistic information. Markables represent flat, layer-based
annotations defined with respect to the sequence of tokens
as a general timeline.
struct hierarchical structures (DAGs or trees) are formed by
establishing a dominance relation between a struct (e.g.,
a phrase) node as parent, and tokens, markables, or other
struct nodes as children.
edges (relational units of annotation, connecting tokens,
markables, structs)
dominance relation directed edge between a struct
and its children
pointing relations directed edge between nodes in
general (tokens, markables, structs)
labels (annotations: node or edge labels)
features represent annotations attached to a particular
(structural or relational) unit of annotation
file. Multiple annotations are also stored in sepa-
rate files to avoid interference between concurrent
annotations. Annotations refer to the primary data
or to other annotations by means of XLinks and
XPointers.
As types of linguistic annotation, we distinguish
nodes (token, markable, struct), edges (dominance
and pointing relations) and labels (annotations), as
summarized in Table 2. Each type of annotation
is stored in a separate file, so that competing or
ambiguous annotations can be represented in an
encapsulated way.
PAULA 1.0 is already sufficiently expressive for
capturing the data-heterogeneity sketched above,
including the representation of overlapping seg-
ments, intersecting hierarchies, and alternative an-
notations (e.g., for ambiguous annotations), but
only for annotations above the token level. Fur-
ther, PAULA 1.0 relies on the existence of a
unique layer of non-overlapping, atomic tokens as
minimal units of annotation: For all nodes, their
position and sequential order is defined with re-
spect to the absolute position of tokens that they
cover; and for the special case of markables, these
are defined solely in terms of their token range.
Finally, PAULA 1.0 tokens are totally ordered,
they cover the (annotated) primary data com-
pletely, and they are non-overlapping. Only on
this basis, the extension and (token-)distance of
annotated elements can be addressed; and only
by means of unambiguous reference, information
from different layers of annotation can be com-
bined and evaluated.
3.2 Introducing terminal nodes
In our extension of the PAULA format, we in-
troduce the new concept of term nodes: atomic
terminals that directly point to spans of primary
data. Terms are subject to the same constraints as
tokens in PAULA 1.0 (total order, full coverage,
non-overlapping). So, terms can be used in place
of PAULA 1.0 tokens to define the extension and
position of super-token level and sub-token level
annotation elements.
Markables are then defined with respect to
(spans of) terminal nodes rather than tokens, such
that alternative tokenizations can be expressed as
markables in different layers that differ in their ex-
tensions.
Although terms adopt several functions for-
merly associated with tokens, a privileged token
layer is still required: In many query languages,
including ANNIS-QL (Chiarcos et al, 2008), to-
kens define the application domain of regular ex-
pressions on the primary data. More impor-
tantly, tokens constitute the basis for conventional
(?word?) distance measurements and (?word?)
coverage queries. Consequently, the constraints
on tokens (total order, full coverage and absence
of overlap) remain.
The resulting specifications for structural units
of annotation are summarized in Table 3. Distin-
guishing terminal elements and re-defining the to-
ken layer as a privileged layer of markables al-
lows us to disentangle the technical concept of
?atomic element? and ?token? as the convention-
ally assumed minimal unit of linguistic analysis.
3.3 A merging algorithm
In order to integrate annotations on tokens, it is
not enough to represent two tokenizations side by
side with reference to the same layer of terminal
nodes. Instead, a privileged token layer is to be es-
tablished and it has to be ensured that annotations
can be queried with reference to the token layer.
38
Table 3: PAULA extensions: revised node types
terms specify character spans in the primary data
that form the basis for higher-level annota-
tion
markable defined as above, with terms taking the
place of tokens
structs defined as above, with terms taking the
place of tokens
tokens sub-class of structs that are non-
overlapping, arranged in a total order,
and cover the full primary data
Then, all annotations whose segmentation is con-
sistent with the privileged token layer are directly
linked with tokens.
Alg. 3.1 describes our merging algorithm, and
its application to the four main cases of conflict-
ing tokenization is illustrated in Figure 1.3 The
following section describes its main characteris-
tics and the consequences for querying.
4 Discussion
Alg. 3.1 produces a PAULA project with one sin-
gle tokenization. So, it is possible to define queries
spanning across annotations with originally differ-
ent tokenization:
Extension and precedence queries are
tokenization-independent: Markables refer to
the term layer, not the tok layer, structs also
(indirectly) dominate term nodes.
Dominance queries for struct nodes and tokens
yield results whenever the struct node dominates
only nodes with tok-compatible source tokeniza-
tion: Structs dominate tok nodes wherever the
original tokenization was consistent with the
privileged tokenization tok (case A and C in Fig.
1).
Distance queries are defined with respect to the
tok layer, and are applicable to all elements that
are are defined with reference to the tok layer (in
figure 1: tok?a, tok?a, tok?b, tok?b in case A; tokab
in case B; toka, tokb, tokab in case C; tokab, tokc
in case D). They are not applicable to elements
that do not refer to the tok layer (B: toka, tokb; D:
toka, tokbc).
3Notation: prim ? primary data / tok, term ? annota-
tion layers / t ? L ? t is a node on a layer L / a..b ? con-
tinuous span from tok/term a to tok/term b / a, b ? list of
tok/term/markable nodes a, b / t = [a] ? t is a node (struct,
markable, tok) that points to a node, span or list a
The algorithm is unsupervised, and the token
concept of the output tokenization is well-defined
and consistent (if one of the input tokenizations
is adopted as target tokenization). Also, as shown
below, it is integrative (enabling queries across dif-
ferent tokenizations) and information-preserving
(reversible).
4.1 Time complexity
After a PAULA project has been created, the time
complexity of the algorithm is quadratic with re-
spect to the number of characters in the primary
data n. This is due to the total order of tokens:
Step 2 and 3.a are applied once to all original to-
kens from left to right. Step 5 can be reformulated
such that for every terminal node, the relationship
between the directly dominating tok? and tok? is
checked. Then, Step 5 is also in O(n). In terms of
the number of markables m, the time complexity
in Step 3.b is in O(n m): for every markable, the
corresponding term element is to be found, tak-
ing at most n repositioning operations on the term
layer. Assuming that markables within one layer
are non-overlapping4 and that the number of lay-
ers is bound by some constant c5, then m ? n c,
so that 3.b is in O(n? c).
For realistic scenarios, the algorithm is thus
quadratic.
4.2 Reversibility
The merging algorithm is reversible ? and, thus,
lossless ? as shown by the splitting algorithm in
Alg. 3.2. For reasons of space, the correctness
of this algorithm cannot be demonstrated here, but
broadly speaking, it just removes every node that
corresponds to an original token of the ?other? tok-
enization, plus every node that points to it, so that
only annotations remain that are directly applied
to the target tokenization.
4.3 Querying merged tokenizations
We focus in this paper on the merging of analy-
ses with different tokenizations for the purpose of
users querying a corpus across multiple annota-
4Although PAULA supports overlapping markables
within one single layer, even with identical extension, this is
a reasonable assumption: In practice, overlapping markables
within one single layer are rare. More often, there is even a
longer sequence of primary data between one markable of a
particular layer and the next. In our experience, such ?gaps?
occur much more often than overlapping markables.
5Again, this is a practical simplication. Theoretically, the
number of layers is infinite.
39
Alg. 3.1 Merging different tokenizations
0. assume that we have two annotations analysis? and analysis? for the same primary data, but with different tokenizations
1. create PAULA 1.0 annotation projects for analysis? and analysis? with primary data files prim? and prim? and token
layers tok? and tok? respectively.
2. harmonize primary data
if prim? equals prim?, then
(i) rename prim? to prim
(ii) set al references in analysis? from prim? to prim
(iii) create a new annotation project analysis by copying prim and all annotation layers from analysis? and analysis?
otherwise terminate with error msg
3. harmonize terminal nodes
create a new annotation layer term, then
(a) for all overlapping tokens t? ? tok? and t? ? tok?: identify the maximal common substrings of t? and t?
for every substring s, create a new element terms pointing to the corresponding character span in the primary data
for every substring s, redefine t? and t? as markables referring to terms
(b) redefine markable spans as spans of terminal nodes
for every token t = [terms? ..terms? ] ? tok? ? tok? and every markable m = [w..xty..z]: set m =
[w..xterms? ..terms?y..z]
4. select token layer
rename tok? to tok, or rename tok? to tok, (cf. the normalization strategy in Sect. 2) or
rename term to tok (cf. the minimal tokens strategy in Sect. 2)
5. token integration
for every original token ot = [a..b] ? (tok? ? tok?) \ tok:
if there is a token t ? tok such that t = [a..b], then define ot as a struct with ot = [t], else
if there are tokens t?, .., tn ? tok such that t?..tn form a continuous sequence of tokens and t? = [a..x] and tn = [y..b],
then define ot as a struct such that ot = [t?, .., tn],
otherwise: change nothing
Figure 1: Merging divergent tokenizations
40
Alg. 3.2 Splitting a PAULA annotation project
with two different tokenizations
0. given a PAULA annotation project analysis with token
layer tok, terminal layer term, and two layers l? and l?
(that may be identical to term or tok) that convey the
information of the original token layers tok? and tok?
1. create analysis? and analysis? as copies of analysis
2. if l? represents a totally ordered, non-overlapping list of
nodes that cover the primary data completely, then modify
analysis?:
a. for every node in l?: substitute references to tok? by
references to term?
b. remove l? from analysis?
c. if l? 6= tok?, remove tok? from analysis?
d. for every annotation element (node/relation) e in
analysis? that directly or indirectly points to another
node in analysis? that is no longer present, remove e
from analysis?
e. remove every annotation layer from analysis? that
does not contain an annotation element
f. for every markable in l?: remove references to term?,
define the extension of l? nodes directly in terms of
spans of text in prim?
g. if l? 6= term?, remove term?
3. perform step 2. for l? and analysis?
tion layers. Although the merging algorithm pro-
duces annotation projects that allow for queries in-
tegrating annotations from analyses with different
tokenization, the structure of the annotations is al-
tered, such that the behaviour of merged and un-
merged PAULA projects may be different. Obvi-
ously, token-level queries must refer to the priv-
ileged tokenization T ?. Operators querying for
the relative precedence or extension of markables
are not affected: in the merged annotation project,
markables are defined with reference to the layer
term: originally co-extensional elements E? and
E? (i.e. elements covering the same tokens in the
source tokenization) will also cover the same ter-
minals in the merged project. Distance operators
(e.g. querying for two tokens with distance 2, i.e.
with two tokens in between), however, will oper-
ate on the new privileged tokenization, such that
results from queries on analysis may differ from
those on analysis?. Dominance operators are
also affected, as nodes that directly dominated a
token in analysis? or analysis? now indirectly
dominate it in analysis, with a supertoken as an
intermediate node.
Alg. 3.3 Iterative merging: modifications of Alg.
3.1, step.3
if analysis? has a layer of terminal nodes term?, then let
T ? = term?, otherwise T ? = tok?
if analysis? has a layer of terminal nodes term?, then let
T ? = term?, otherwise T ? = tok?
create a new annotation layer term, then
1. for all overlapping terminals/tokens t? ? T ? and t? ?
T ?: identify the maximal common substrings of t? and
t?
for every substring s, create a new element terms
pointing to the corresponding character span in the pri-
mary data
for every substring s, redefine t? and t? as markables
referring to terms
2. redefine markable spans as spans of terminal nodes
for every node t = [terms? ..terms? ] ? T ? ? T ?
and every markable m = [w..xty..z]: set
m = [w..xterms? ..terms?y..z]
3. for all original terminals t ? T ??T ?: if t is not directly
pointed at, remove t from analysis
Accordingly, queries applicable to PAULA
projects before the merging are not directly appli-
cable to merged PAULA projects. Users are to be
instructed to keep this in mind and to be aware of
the specifications for the merged tokenization and
its derivation.6
5 Extensions
5.1 Merging more than two tokenizations
In the current formulation, Alg. 3.1 is applied to
two PAULA 1.0 projects and generates extended
PAULA annotation projects with a term layer.
The algorithm, however, may be applied itera-
tively, if step 3 is slightly revised, such that ex-
tended PAULA annotation projects can also be
merged, see Alg. 3.3.
5.2 Annotation integration
The merging algorithm creates a struct node for
every original token. Although this guarantees re-
versibility, one may consider to remove such re-
dundant structs. Alg. 3.4 proposes an optional
postprocessing step for the merging algorithm.
This step is optional because these operations are
6The information, however, is preserved in the format and
may be addressed by means of queries that, for example, op-
erate on the extension of terminals.
41
Alg. 3.4 Annotation integration: Optional post-
processing for merging algorithm
6.a. remove single-token supertoken
for every original token ot = [t] ? tok? ? tok? with
t ? tok: replace all references in analysis to ot by
references to t, remove ot
6.b. merging original token layers tok? and tok? (if
tok? 6= tok and tok? 6= tok)
define new ?super token? layer stok.
for every ot ? tok? ? tok?:
if ot = [t] for some t ? tok, then see 6.a
if ot = [t?, .., tn] for some t?, .., tn ? tok, and
there is ot? = [t?, .., tn] ? tok? ? tok? ? stok,
then replace all references in analysis to ot? by
references to ot, move ot to layer stok, remove
ot? from analysis
move all remaining ot ? tok? ? tok? to stok, remove
layers tok? and tok?
6.c. unify higher-level annotations
for every markable mark? = [term?..termn] and
term?, .., termn ? term:
if there is a markable mark? in analysis such
that mark? = [term?..termn], then replace all
references in analysis to mark? by references to
mark?, remove mark?
for every struct struct? = [c?, .., cn] that covers ex-
actly the same children as another struct struct? =
[c?, .., cn], replace all references to struct? by refer-
ences to struct?, remove struct?
destructive: We lose the information about the ori-
gin (analysis? vs. analysis?) of stok elements
and their annotations.
6 Summary and Related Reasearch
In this paper, we describe a novel approach for the
integration of conflicting tokenizations, based on
the differentiation between a privileged layer of
tokens and a layer of atomic terminals in a stand-
off XML format: Tokens are defined as structured
units that dominate one or more terminal nodes.
Terminals are atomic units only within the re-
spective annotation project (there is no unit ad-
dressed that is smaller than a terminal). By iter-
ative applications of the merging algorithm, how-
ever, complex terms may be split up in smaller
units, so that they are not atomic in an absolute
sense.
Alternatively, terms could be identified a priori
with the minimal addressable unit available, i.e.,
characters (as in the formalization of tokens as
charspans and charseqs in the ACE information
extraction annotations, Henderson 2000). It is not
clear, however, how a character-based term defini-
tion would deal with sub-character and zero exten-
sion terms: A character-based definition of terms
that represent traces is possible only by corrupt-
ing the primary data.7 Consequently, a character-
based term definition is insufficient unless we re-
strict ourselves to a particular class of languages,
texts and phenomena.
The role of terminals can thus be compared to
timestamps: With reference to a numerical time-
line, it is always possible to define a new event
between two existing timestamps. Formats specif-
ically designed for time-aligned annotations, e.g.,
EXMARaLDA (Schmidt, 2004), however, typi-
cally lack a privileged token layer and a formal
concept of tokens. Instead, tokens, as well as
longer or shorter sequences, are represented as
markables, defined by their extension on the time-
line.
Similarly, GrAF (Ide and Suderman, 2007), al-
though being historically related to PAULA, does
not have a formal concept of a privileged token
layer in the sense of PAULA.8 We do, however,
assume that terminal nodes in GrAF can be com-
pared to PAULA 1.0 tokens.
For conflicting tokenizations, Ide and Suderman
(2007) suggest that ?dummy? elements are defined
covering all necessary tokenizations for controver-
sially tokenized stretches of primary data. Such
dummy elements combine the possible tokeniza-
tions for strategies 1 (no merging) and 3 (maxi-
mal tokens), so that the information preservation
deficit of strategy 3 is compensated by strategy 1,
and the integrativity deficit of strategy 1 is com-
pensated by strategy 3 (cf. Table 1). However, to-
kens, if defined in this way, are overlapping and
thus only partially ordered, so that distance opera-
tors are no longer applicable.9
7Similarly, phonological units that are not expressed in
the primary data can be subject to annotations, e.g., short e
and o in various Arabic-based orthographies, e.g., the Ajami
orthography of Hausa. A term with zero extension at the po-
sition of a short vowel can be annotated as having the phono-
logical value e or o without having character status.
8https://www.americannationalcorpus.
org/graf-wiki/wiki/WikiStart#GraphModel,
2009/05/08
9This can be compensated by marking the base segmen-
tation differently from alternative segmentations. In the ab-
stract GrAF model, however, this can be represented only by
means of labels, i.e., annotations. A more consistent con-
42
Another problem that arises from the introduc-
tion of dummy nodes is their theoretical status, as
it is not clear how dummy nodes can be distin-
guished from annotation structured on a concep-
tual level. In the PAULA formalization, dummy
nodes are not necessary, so that this ambiguity is
already resolved in the representation.
References
Thorsten Brants. 2000. TnT A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
ANLP-2000. Seattle, WA.
Lou Burnard (ed.). 2007. Reference Guide
for the British National Corpus (XML Edi-
tion). http://www.natcorp.ox.ac.uk/
XMLedition/URG/bnctags.html.
Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan
Kilgour, Judy Robertson, and Holger Voormann.
2003. The NITE XML Toolkit: Flexible Annotation
for Multi-modal Language Data. Behavior Research
Methods, Instruments, and Computers 35(3), 353-
363.
Christian Chiarcos, Stefanie Dipper, Michael Go?tze,
Ulf Leser, Anke Lu?deling, Julia Ritz, and Manfred
Stede. 2009. A Flexible Framework for Integrating
Annotations from Different Tools and Tagsets TAL
(Traitement automatique des langues) 49(2).
Oli Christ. 1994. A modular and flexible architec-
ture for an integrated corpus query system. COM-
PLEX?94, Budapest, Hungary.
Stefanie Dipper. 2005. XML-based Stand-off Repre-
sentation and Exploitation of Multi-Level Linguistic
Annotation. In Rainer Eckstein and Robert Tolks-
dorf (eds:): Proceedings of Berliner XML Tage,
pages 39-50.
Stefanie Dipper and Michael Go?tze. 2005. Accessing
Heterogeneous Linguistic Data ? Generic XML-
based Representation and Flexible Visualization. In
Proceedings of the 2nd Language & Technology
Conference 2005, Poznan, Poland, pages 23?30.
Stefanie Dipper, Michael Go?tze. 2006. ANNIS:
Complex Multilevel Annotations in a Linguistic
Database. Proceedings of the 5th Workshop on NLP
and XML (NLPXML-2006): Multi-Dimensional
Markup in Natural Language Processing. Trento,
Italy.
Jin Guo. 1997. Critical Tokenization and its Proper-
ties, Computational Linguistic, 23(4), pp.569-596.
ception would encode structural information on the structural
level, and only linguistic annotation and metadata on the con-
tents level.
John C. Henderson. 2000. A DTD for Reference Key
Annotation of EDT Entities and RDC Relations
in the ACE Evaluations (v. 5.2.0, 2000/01/05),
http://projects.ldc.upenn.edu/ace/
annotation/apf.v5.2.0.dtd (2009/06/04)
Nancy Ide and Keith Suderman. 2007. GrAF: A
Graph-based Format for Linguistic Annotations. In
Proceedings of the Linguistic Annotation Work-
shop,held in conjunction with ACL 2007, Prague,
June 28-29, 1-8.
Esther Ko?nig and Wolfgang Lezius. 2000. A descrip-
tion language for syntactically annotated corpora.
In: Proceedings of the COLING Conference, pp.
1056-1060, Saarbru?cken, Germany.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank. Computa-
tional Linguistics 19, pp.313-330.
Christoph Mu?ller and Michael Strube. 2006. Multi-
Level Annotation of Linguistic Data with MMAX2.
In: S. Braun et al (eds.), Corpus Technology and
Language Pedagogy. New Resources, New Tools,
New Methods. Frankfurt: Peter Lang, 197?214.
Georg Rehm, Oliver Schonefeld, Andreas Witt, Chris-
tian Chiarcos, and Timm Lehmberg. 2009.
SPLICR: A Sustainability Platform for Linguistic
Corpora and Resources. In: Text Resources and
Lexical Knowledge. Selected Papers the 9th Confer-
ence on Natural Language Processing (KONVENS
2008), Berlin, Sept. 30 ? Oct. 2, 2008. Mouton de
Gruyter.
Helmut Schmid. 2002. Tokenizing & Tagging. In
Lu?deling, Anke and Kyto?, Merja (Hrsg.) Corpus
Linguistics. An International Handbook. (HSK Se-
ries). Mouton de Gryuter, Berlin
Thomas Schmidt. 2004. Transcribing and Annotat-
ing Spoken Language with Exmaralda. Proceedings
of the LREC-workshop on XML Based Richly Anno-
tated Corpora. Lisbon, Portugal. Paris: ELRA.
Manfred Stede, Heike Bieler, Stefanie Dipper, and
Arthit Suriyawongkul. 2006. SUMMaR: Combin-
ing Linguistics and Statistics for Text Summariza-
tion. Proceedings of the 17th European Conference
on Artificial Intelligence (ECAI-06). pp 827-828.
Riva del Garda, Italy.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw
and Linnea Micciulla. 2006. OntoNotes Release
1.0. Linguistic Data Consortium, Philadelphia.
43
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62?70,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Genre-Based Paragraph Classification for Sentiment Analysis 
 
 
Maite Taboada 
Department of Linguistics 
Simon Fraser University 
Burnaby, BC, Canada 
mtaboada@sfu.ca 
Julian Brooke 
Department of Computer Science 
University of Toronto 
Toronto, ON, Canada 
jbrooke@cs.toronto.edu 
Manfred Stede 
Institute of Linguistics 
University of Potsdam 
Potsdam, Germany 
stede@ling.uni-
potsdam.de 
 
  
 
 
Abstract 
We present a taxonomy and classification 
system for distinguishing between differ-
ent types of paragraphs in movie reviews: 
formal vs. functional paragraphs and, 
within the latter, between description and 
comment. The classification is used for 
sentiment extraction, achieving im-
provement over a baseline without para-
graph classification. 
1 Introduction 
Much of the recent explosion in sentiment-
related research has focused on finding low-level 
features that will help predict the polarity of a 
phrase, sentence or text. Features, widely unders-
tood, may be individual words that tend to ex-
press sentiment, or other features that indicate 
not only sentiment, but also polarity. The two 
main approaches to sentiment extraction, the se-
mantic or lexicon-based, and the machine learn-
ing or corpus-based approach, both attempt to 
identify low-level features that convey opinion. 
In the semantic approach, the features are lists of 
words and their prior polarity, (e.g., the adjective 
terrible will have a negative polarity, and maybe 
intensity, represented as -4; the noun masterpiece 
may be a 5). Our approach is lexicon-based, but 
we make use of information derived from ma-
chine learning classifiers. 
Beyond the prior polarity of a word, its local 
context obviously plays an important role in 
conveying sentiment. Polanyi and Zaenen (2006) 
use the term ?contextual valence shifters? to refer 
to expressions in the local context that may 
change a word?s polarity, such as intensifiers, 
modal verbs, connectives, and of course negation. 
Further beyond the local context, the overall 
structure and organization of the text, influenced 
by its genre, can help the reader determine how 
the evaluation is expressed, and where it lies. 
Polanyi and Zaenen (2006) also cite genre con-
straints as relevant factors in calculating senti-
ment.  
Among the many definitions of genre, we take 
the view of Systemic Functional Linguistics that 
genres are purposeful activities that develop in 
stages, or parts (Eggins and Martin, 1997), which 
can be identified by lexicogrammatical proper-
ties (Eggins and Slade, 1997). Our proposal is 
that, once we have identified different stages in a 
text, the stages can be factored in the calculation 
of sentiment, by weighing more heavily those 
that are more likely to contain evaluation, an ap-
proach also pursued in automatic summarization 
(Seki et al, 2006). 
To test this hypothesis, we created a taxonomy 
of stages specific to the genre of movie reviews, 
and annotated a set of texts. We then trained 
various classifiers to differentiate the stages. 
Having identified the stages, we lowered the 
weight of those that contained mostly description. 
Our results show that we can achieve improve-
ment over a baseline when classifying the polar-
ity of texts, even with a classifier that can stand 
to improve (at 71.1% accuracy). The best per-
formance comes from weights derived from the 
output of a linear regression classifier. 
We first describe our inventory of stages and 
the manual annotation (Section 2), and in Sec-
tion 3 turn to automatic stage classification. After 
describing our approach to sentiment classifica-
tion of texts in Section 4, we describe experi-
ments to improve its performance with the in-
formation on stages in Section 5. Section 6 dis-
62
cusses related work, and Section 7 provides con-
clusions.  
2 Stages in movie reviews 
Within the larger review genre, we focus on 
movie reviews. Movie reviews are particularly 
difficult to classify (Turney, 2002), because large 
portions of the review contain description of the 
plot, the characters, actors, director, etc., or 
background information about the film. 
Our approach is based on the work of Bieler et 
al. (2007), who identify formal and functional 
zones (stages) within German movie reviews. 
Formal zones are parts of the text that contribute 
factual information about the cast and the credits, 
and also about the review itself (author, date of 
publication and the reviewer?s rating of the mov-
ie). Functional zones contain the main gist of the 
review, and can be divided roughly into descrip-
tion and comment. Bieler et al showed that func-
tional zones could be identified using 5-gram 
SVM classifiers built from an annotated German 
corpus.  
2.1 Taxonomy 
In addition to the basic Describe/Comment dis-
tinction in Bieler et al, we use a De-
scribe+Comment label, as in our data it is often 
the case that both description and comment are 
present in the same paragraph. We decided that a 
paragraph could be labeled as De-
scribe+Comment when it contained at least a 
clause of each, and when the comment part could 
be assigned a polarity (i.e., it was not only sub-
jective, but also clearly positive or negative).  
Each of the three high-level tags has a subtag, 
a feature also present in Bieler et al?s manual 
annotation. The five subtags are: overall, plot, 
actors/characters, specific and general. ?Specific? 
refers to one particular aspect of the movie (not 
plot or characters), whereas ?general? refers to 
multiple topics in the same stage (special effects 
and cinematography at the same time). Outside 
the Comment/Describe scale, we also include 
tags such as Background (discussion of other 
movies or events outside the movie being 
reviewed), Interpretation (subjective but not 
opinionated or polar), and Quotes. Altogether, 
the annotation system includes 40 tags, with 22 
formal and 18 functional zones. Full lists of 
zone/stage labels are provided in Appendix A. 
2.2 Manual annotation 
We collected 100 texts from rottentomatoes.com, 
trying to include one positive and one negative 
review for the same movie. The reviews are part 
of the ?Top Critics? section of the site, all of 
them published in newspapers or on-line maga-
zines. We restricted the texts to ?Top Critics? 
because we wanted well-structured, polished 
texts, unlike those found in some on-line review 
sites. Future work will address those more in-
formal reviews. 
The 100 reviews contain 83,275 words and 
1,542 paragraphs. The annotation was performed 
at the paragraph level. Although stages may span 
across paragraphs, and paragraphs may contain 
more than one stage, there is a close relationship 
between paragraphs and stages. The restriction 
also resulted in a more reliable annotation, per-
formed with the PALinkA annotation tool (Ora-
san, 2003). 
The annotation was performed by one of the 
authors, and we carried out reliability tests with 
two other annotators, one another one of the au-
thors, who helped develop the taxonomy, and the 
third one a project member who read the annota-
tion guidelines1, and received a few hours? train-
ing in the labels and software. We used Fleiss? 
kappa (Fleiss, 1971), which extends easily to the 
case of multiple raters (Di Eugenio and Glass, 
2004). We all annotated four texts. The results of 
the reliability tests show a reasonable agreement 
level for the distinction between formal and 
functional zones (.84 for the 3-rater kappa). The 
lowest reliability was for the 3-way distinction in 
the functional zones (.68 for the first two raters, 
and .54 for the three raters). The full kappa val-
ues for all the distinctions are provided in Ap-
pendix B. After the reliability test, one of the 
authors performed the full annotation for all 100 
texts. Table 1 shows the breakdown of high-level 
stages for the 100 texts.  
 
Stage Count 
Describe 347 
Comment 237 
Describe+Comment 237 
Background 51 
Interpretation 22 
Quote 2 
Formal 646 
Table 1. Stages in 100 text RT corpus 
                                                 
1Available from http://www.sfu.ca/~mtaboada/nserc-
project.html 
63
3 Classifying stages 
Our first classification task aims at distinguishing 
the two main types of functional zones, Com-
ment and Describe, vs. Formal zones.  
3.1 Features 
We test two different sets of features. The first, 
following Bieler et al (2007), consists of 5-
grams (including unigrams, bigrams, 3-grams 
and 4-grams), although we note in our case that 
there was essentially no performance benefit 
beyond 3-grams. We limited the size of our fea-
ture set to n-grams that appeared at least 4 times 
in our training corpus. For the 2 class task (no 
formal zones), this resulted in 8,092 binary fea-
tures, and for the 3 and 4 class task there were 
9,357 binary n-gram features. 
The second set of features captures different 
aspects of genre and evaluation, and can in turn 
be divided into four different types, according to 
source. With two exceptions (features indicating 
whether a paragraph was the first or last para-
graph in text), the features were numerical (fre-
quency) and normalized to the length of the pa-
ragraph. 
The first group of genre features comes from 
Biber (1988), who attempted to characterize di-
mensions of genre. The features here include fre-
quency of first, second and third person pro-
nouns; demonstrative pronouns; place and time 
adverbials; intensifiers; and modals, among a 
number of others. 
The second category of genre features in-
cludes discourse markers, primarily from Knott 
(1996), that indicate contrast, comparison, causa-
tion, evidence, condition, and similar relations. 
The third type of genre features was a list of 
500 adjectives classified in terms of Appraisal 
(Martin and White, 2005) as indicating Apprec-
iation, Judgment or Affect. Appraisal categories 
have been shown to be useful in improving the 
performance of polarity classifiers (Whitelaw et 
al., 2005).  
Finally, we also include text statistics as fea-
tures, such as average length of words and sen-
tences and position of paragraphs in the text.  
3.2 Classifiers 
To classify paragraphs in the text, we use the 
WEKA suite (Witten and Frank, 2005), testing 
three popular machine learning algorithms: 
Na?ve Bayes, Support Vector Machine, and Li-
near Regression (preliminary testing with Deci-
sion Trees suggests that it is not appropriate for 
this task). Training parameters were set to default 
values. 
In order to use Linear Regression, which pro-
vides a numerical output based on feature values 
and derived feature weights, we have to conceive 
of Comment/Describe/Describe+Comment not as 
nominal (or ordinal) classes, but rather as corres-
ponding to a Comment/Describe ratio, with 
?pure? Describe at one end and ?pure? Comment 
at the other. For training, we assign a 0 value (a 
Comment ratio) to all paragraphs tagged De-
scribe and a 1 to all Comment paragraphs; for 
Describe+Comment, various options (including 
omission of this data) were tested. The time re-
quired to train a linear regression classifier on a 
large feature set proved to be prohibitive, and 
performance with smaller sets of features gener-
ally quite poor, so for the linear regression clas-
sifier we present results only for our compact set 
of genre features. 
3.3 Performance 
Table 2 shows the performance of classifi-
er/feature-set combinations for the 2-, 3-, and 4-
class tasks on the 100-text training set, with 10-
fold cross-validation, in terms of precision (P), 
recall (R) and F-measure 2 . SVM and Na?ve 
Bayes provide comparable performance, al-
though there is considerable variation, particular-
ly with respect to the feature set; the SVM is a 
significantly (p<0.05) better choice for our genre 
features 3 , while for the n-gram features the 
Bayes classification is generally preferred. The 
SVM-genre classifier significantly outperforms 
the other classifiers in the 2-class task; these ge-
nre features, however, are not as useful as 5-
grams at identifying Formal zones (the n-gram 
classifier, by contrast, can make use of words 
such as cast). In general, formal zone classifica-
tion is fairly straightforward, whereas identifica-
tion of Describe+Comment is quite difficult, and 
the SVM-genre classifier, which is more sensi-
tive to frequency bias, elects to (essentially) ig-
nore this category in order to boost overall accu-
racy.  
To evaluate a linear regression (LR) classifier, 
we calculate correlation coefficient ?, which re-
flects the goodness of fit of the line to the da-
ta. Table 3 shows values for the classifiers built 
from the corpus, with various Comment ratios
                                                 
2 For the 2- and 3-way classifiers, Describe+Comment pa-
ragraphs are treated as Comment. This balances the num-
bers of each class, ultimately improving performance. 
3 All significance tests use chi-square (?2). 
64
Classifier 
Comment Describe Formal Desc+Comm Overall 
Accuracy P R F P R F P R F P R F 
2-class-5-gram-Bayes .66 .79 .72 .70 .55 .62 - - - - - - 68.0 
2-class-5-gram-SVM .53 .63 .64 .68 .69 .69 - - - - - - 66.8 
2-class-genre-Bayes .66 .75 .70 .67 .57 .61 - - - - - - 66.2 
2-class-genre-SVM .71 .76 .74 .71 .65 .68 - - - - - - 71.1 
3-class-5-gram-Bayes .69 .49 .57 .66 .78 .71 .92 .97 .95 - - - 78.1 
3-class-5-gram-SVM .64 .63 .63 .68 .65 .65 .91 .97 .94 - - - 77.2 
3-class-genre-Bayes .68 .68 .66 .67 .46 .55 .84 .96 .90 - - - 74.0 
3-class-genre-SVM .66 .71 .68 .67 .56 .61 .90 .94 .92 - - - 76.8 
4-class-5-gram-Bayes .46 .35 .38 .69 .47 .56 .92 .97 .95 .42 .64 .51 69.0 
4-class-5-gram-SVM .43 .41 .44 .59 .62 .60 .91 .97 .94 .45 .41 .42 69.6 
4-class-genre-Bayes .38 .31 .34 .66 .30 .41 .86 .97 .90 .33 .60 .42 62.3 
4-class-genre-SVM .46 .32 .38 .53 .82 .65 .87 .94 .90 .26 .03 .06 67.4 
Table 2. Stage identification performance of various categorical classifiers 
 
(C) assigned to paragraphs with the De-
scribe+Comment tag, and with De-
scribe+Comment paragraphs removed from con-
sideration. 
 
Classifier ? 
LR, Des+Com C = 0 .37 
LR, Des+Com C = 0.25 .44 
LR, Des+Com C = 0.5 .47 
LR, Des+Com C = 0.75 .46 
LR, Des+Com C = 1 .43 
LR, No Des+Com .50 
Table 3. Correlation coefficients for LR 
classifiers 
The drop in correlation when more extreme 
values are assigned to Describe+Comment sug-
gests that Describe+Comment paragraphs do in-
deed belong in the middle of the Comment spec-
trum. Since there is a good deal of variation in 
the amount of comment across De-
scribe+Comment paragraphs, the best correlation 
comes with complete removal of these somewhat 
unreliable paragraphs. Overall, these numbers 
indicate that variations in relevant features are 
able to predict roughly 50% of the variation in 
Comment ratio, which is fairly good considering 
the small number and simplistic nature of the 
features involved. 
4 Sentiment detection: SO-CAL 
In this section, we outline our semantic orienta-
tion calculator, SO-CAL. SO-CAL extracts 
words from a text, and aggregates their semantic 
orientation value, which is in turn extracted from 
a set of dictionaries. SO-CAL uses five dictionar-
ies: four lexical dictionaries with 2,257 adjec-
tives, 1,142 nouns, 903 verbs, and 745 adverbs, 
and a fifth dictionary containing 177 intensifying 
expressions. Although the majority of the entries 
are single words, the calculator also allows for 
multiword entries written in regular expression-
like language.  
The SO-carrying words in these dictionaries 
were taken from a variety of sources, the three 
largest a corpus of 400 reviews from Epin-
ions.com, first used by Taboada and Grieve 
(2004), a 100 text subset of the 2,000 movie re-
views in the Polarity Dataset (Pang and Lee, 
2004), and words from the General Inquirer dic-
tionary (Stone, 1997). Each of the open-class 
words were given a hand-ranked SO value be-
tween 5 and -5 (neutral or zero-value words are 
not included in the dictionary) by a native Eng-
lish speaker. The numerical values were chosen 
to reflect both the prior polarity and strength of 
the word, averaged across likely interpretations. 
For example, the word phenomenal is a 5, nicely 
a 2, disgust a -3, and monstrosity a -5. The dic-
tionary was later reviewed by a committee of 
three other researchers in order to minimize the 
subjectivity of ranking SO by hand. 
Our calculator moves beyond simple averag-
ing of each word?s semantic orientation value, 
and implements and expands on the insights of 
Polanyi and Zaenen (2006) with respect to con-
textual valence shifters. We implement negation 
by shifting the SO value of a word towards the 
opposite polarity (not terrible, for instance, is 
calculated as -5+4 = -1). Intensification is mod-
eled using percentage modifiers (very engaging: 
4x125% = 5). We also ignore words appearing 
within the scope of irrealis markers such as cer-
tain verbs, modals, and punctuation, and de-
crease the weight of words which appear often in 
the text. In order to counter positive linguistic 
65
bias (Boucher and Osgood, 1969), a problem for 
lexicon-based sentiment classifiers (Kennedy and 
Inkpen, 2006), we increase the final SO of any 
negative expression appearing in the text. 
The performance of SO-CAL tends to be in 
the 76-81% range. We have tested on informal 
movie, book and product reviews and on the Po-
larity Dataset (Pang and Lee, 2004). The perfor-
mance on movie reviews tends to be on the lower 
end of the scale. Our baseline for movies, de-
scribed in Section 5, is 77.7%. We believe that 
we have reached a ceiling in terms of word- and 
phrase-level performance, and most future im-
provements need to come from discourse fea-
tures. The stage classification described in this 
paper is one of them.  
5 Results 
The final goal of a stage classifier is to use the 
information about different stages in sentiment 
classification. Our assumption is that descriptive 
paragraphs contain less evaluative content about 
the movie being reviewed, and they may include 
noise, such as evaluative words describing the 
plot or the characters. Once the paragraph clas-
sifier had assigned labels we used those labels to 
weigh paragraphs. 
5.1 Classification with manual tags 
Before moving on to automatic paragraph classi-
fication, we used the 100 annotated texts to see 
the general effect of weighting paragraphs with 
the ?perfect? human annotated tags on sentiment 
detection, in order to show the potential im-
provements that can be gained from this ap-
proach.  
Our baseline polarity detection performance 
on the 100 annotated texts is 65%, which is very 
low, even for movie reviews. We posit that for-
mal movie reviews might be particularly difficult 
because full plot descriptions are more common 
and the language used to express opinion less 
straightforward (metaphors are common). How-
ever, if we lower the weight on non-Comment 
and mixed Comment paragraphs (to 0, except for 
Describe+Comment, which is maximized by a 
0.1 weight), we are able to boost performance to 
77%, an improvement which is significant at the 
p<0.05 level. Most of the improvement (7%) is 
due to disregarding Describe paragraphs, but 2% 
comes from Describe+Comment, and 1% each 
from Background, Interpretation, and (all) For-
mal tags. There is no performance gain, however, 
from the use of aspect tags (e.g., by increasing 
the weight on Overall paragraphs), justifying our 
decision to ignore subtags for text-level polarity 
classification.  
5.2 Categorical classification 
We evaluated all the classifiers from Table 2, but 
we omit discussion of the worst performing. The 
evaluation was performed on the Polarity Dataset 
(Pang and Lee, 2004), a collection of 2,000 on-
line movie reviews, balanced for polarity. The 
SO performance for the categorical classifiers is 
given in Figure 1. When applicable, we always 
gave Formal Zones (which Table 2 indicates are 
fairly easy to identify) a weight of 0, however for 
Describe paragraphs we tested at 0.1 intervals 
between 0 and 1. Testing all possible values of 
Describe+Comment was not feasible, so we set 
the weights of those to a value halfway between 
the weight of Comment paragraphs (1) and the 
weight of the Describe paragraph. 
Most of the classifiers were able to improve 
performance beyond the 77.7% (unweighted) 
baseline. The best performing model (the 2-
class-genre-SVM) reached a polarity identifica-
tion accuracy of 79.05%, while the second best 
(the 3-class 5-gram-SVM) topped out at 78.9%. 
Many of the classifiers showed a similar pattern 
with respect to the weight on Describe, increas-
ing linearly as weight on Describe was decreased 
before hitting a maximum in the 0.4-0.1 range, 
and then dropping afterwards (often precipitous-
ly). Only the classifiers which were more con-
servative with respect to Describe, such as the 4-
class-5-gram-Bayes, avoided the drop, which can 
be attributed to low precision Describe identifi-
cation: At some point, the cost associated with 
disregarding paragraphs which have been mis-
tagged as Describe becomes greater that the ben-
efit of disregarding correctly-labeled ones. In-
deed, the best performing classifier for each class 
option is exactly the one that has the highest pre-
cision for identification of Describe, regardless 
of other factors. This suggests that improving 
precision is key, and, in lieu of that, weighting is 
a better strategy than simply removing parts of 
the text. 
In general, increasing the complexity of the 
task (increasing the number of classes) decreases 
performance. One clear problem is that the iden-
tification of Formal zones, which are much more 
common in our training corpus than our test cor-
pus, does not add important information, since 
most Formal zones have no SO valued words. 
The delineation of an independent De-
scribe+Comment class is mostly ineffective, 
66
 
Figure 1. SO Performance with various paragraph tagging classifiers, by weight on Describe 
 
probably because this class is not easily distin-
guishable from Describe and Comment (nor in 
fact should it be). 
We can further confirm that our classifier is 
properly distinguishing Describe and Comment 
by discounting Comment paragraphs rather than 
Describe paragraphs (following Pang and Lee 
2004). When Comment paragraphs tagged by the 
best performing classifier are ignored, SO-CAL?s 
accuracy drops to 56.65%, just barely above 
chance. 
5.3 Continuous classification 
Table 4 gives the results for the linear regression 
classifier, which assigns a Comment ratio to each 
paragraph used for weighting.  
 
Model Accuracy 
LR, Des+Com C = 0 78.75 
LR, Des+Com C = 0.25 79.35 
LR, Des+Com C = 0.5 79.00 
LR, Des+Com C = 0.75 78.90 
LR, Des+Com C = 1 78.95 
LR, No Des+Com 79.05 
Table 4. SO Performance with linear regression 
 
The linear regression model trained with a 
0.25 comment ratio on Describe+Comment para-
graphs provides the best performance of all clas-
sifiers we tested (an improvement of 1.65% from 
baseline). The correlation coefficients noted 
in Table 4 are reflected in these results, but the 
spike at C = 0.25 is most likely related to a gen-
eral preference for low (but non-zero) weights on 
Describe+Comment paragraphs also noted when 
weights were applied using the manual tags; 
these paragraphs are unreliable (as compared to 
pure Comment), but cannot be completely dis-
counted. There were some texts which had only 
Describe+Comment paragraphs.  
Almost a third of the tags assigned by the 2-
class genre feature classifier were different than 
the corresponding n-gram classifier, suggesting 
the two classifiers might have different strengths. 
However, initial attempts to integrate the various 
high performing classifiers?including collaps-
ing of feature sets, metaclassifiers, and double 
tagging of paragraphs?resulted in similar or 
worse performance. We have not tested all poss-
ible options (there are simply too many), but we 
think it unlikely that additional gains will be 
made with these simple, surface feature sets. Al-
though our testing with human annotated texts 
and the large performance gap between movie 
reviews and other consumer reviews both sug-
gest there is more potential for improvement, it 
will probably require more sophisticated and 
precise models. 
6 Related work 
The bulk of the work in sentiment analysis has 
focused on classification at either the sentence 
level, e.g., the subjectivity/polarity detection of 
Wiebe and Riloff (2005), or alternatively at the 
level of the entire text. With regards to the latter, 
two major approaches have emerged: the use of 
machine learning classifiers trained on n-grams 
77
78
79
80
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
SO
?C
al
cu
la
to
r?
A
cc
ur
ac
y
Weight?on?Describe?Paragraph
No?tagging?Baseline
2?Class?5?gram?SVM
2?Class?5?gram?Bayes
2?Class?genre?Bayes
2?Class?genre?SVM
3?Class?5?gram?Bayes
3?Class?5?gram?SVM
3?Class?genre?Bayes
4?Class?5?gram?Bayes
4?Class?5?gram?SVM
4?Class?genre?Bayes
67
or similar features (Pang et al, 2002), and the 
use of sentiment dictionaries (Esuli and Sebas-
tiani, 2006; Taboada et al, 2006). Support Vec-
tor Machine (SVM) classifiers have been shown 
to out-perform lexicon-based models within a 
single domain (Kennedy and Inkpen, 2006); 
however they have trouble with cross-domain 
tasks (Aue and Gamon, 2005), and some re-
searchers have argued for hybrid classifiers (An-
dreevskaia and Bergler, 2008). 
Pang and Lee (2004) attempted to improve the 
performance of an SVM classifier by identifying 
and removing objective sentences from the texts. 
Results were mixed: The improvement was mi-
nimal for the SVM classifier (though the perfor-
mance of a na?ve Bayes classifier was signifi-
cantly boosted), however testing with parts of the 
text classified as subjective showed that the elim-
inated parts were indeed irrelevant. In contrast to 
our findings, they reported a drop in performance 
when paragraphs were taken as the only possible 
boundary between subjective and objective text 
spans. 
Other research that has dealt with identifying 
more or less relevant parts of the text for the pur-
poses of sentiment analysis include Taboada and 
Grieve (2004), who improved the performance of 
a lexicon-based model by weighing words to-
wards the end of the text; Nigam and Hurst 
(2006), who detect polar expressions in topic 
sentences; and Voll and Taboada (2007), who 
used a topic classifier and discourse parser to 
eliminate potentially off-topic or less important 
sentences. 
7 Conclusions 
We have described a genre-based taxonomy for 
classifying paragraphs in movie reviews, with 
the main classification being a distinction be-
tween formal and functional stages, and, within 
those, between mainly descriptive vs. comment 
stages. The taxonomy was used to annotate 100 
movie reviews, as the basis for building classifi-
ers.  
We tested a number of different classifiers. 
Our results suggest that a simple, two-way or 
continuous classification using a small set of lin-
guistically-motivated features is the best for our 
purposes; a more complex system is feasible, but 
comes at the cost of precision, which seems to be 
the key variable in improving sentiment analysis. 
Ultimately, the goal of the classification was 
to improve the accuracy of SO-CAL, our seman-
tic orientation calculator. Using the manual an-
notations, we manage to boost performance by 
12% over the baseline. With the best automatic 
classifier, we still show consistent improvement 
over the baseline. Given the relatively low accu-
racy of the classifiers, the crucial factor involves 
using fine-grained weights on paragraphs, rather 
than simply ignoring Describe-labeled para-
graphs, as Pang and Lee (2004) did for objective 
sentences.  
An obvious expansion to this work would in-
volve a larger dataset on which to train, to im-
prove the performance of the classifier(s). We 
would also like to focus on the syntactic patterns 
and verb class properties of narration, aspects 
that are not captured with simply using words 
and POS labels. Connectives in particular are 
good indicators of the difference between narra-
tion (temporal connectives) and opinion (contras-
tive connectives). There may also be benefit to 
combining paragraph- and sentence-based ap-
proaches. Finally, we would like to identify 
common sequences of stages, such as plot and 
character descriptions appearing together, and 
before evaluation stages. This generic structure 
has been extensively studied for many genres 
(Eggins and Slade, 1997). 
Beyond sentiment extraction, our taxonomy 
and classifiers can be used for searching and in-
formation retrieval. One could, for instance, ex-
tract paragraphs that include mostly comment or 
description. Using the more fine-grained labels, 
searches for comment/description on actors, di-
rectors, or other aspects of the movie are possible. 
Acknowledgements 
This work was supported by SSHRC (410-2006-
1009) and NSERC (261104-2008) grants to 
Maite Taboada. 
References 
Andreevskaia, Alina & Sabine Bergler. 2008. When 
specialists and generalists work together: Domain 
dependence in sentiment tagging. Proceedings of 
46th Annual Meeting of the Association for Com-
putational Linguistics (pp. 290-298). Columbus, 
OH. 
Aue, Anthony & Michael Gamon. 2005. Customizing 
sentiment classifiers to new domains: A case study. 
Proceedings of the International Conference on 
Recent Advances in Natural Language Processing. 
Borovets, Bulgaria. 
Biber, Douglas. 1988. Variation across Speech and 
Writing. Cambridge: Cambridge University Press. 
68
Bieler, Heike, Stefanie Dipper & Manfred Stede. 
2007. Identifying formal and functional zones in 
film reviews. Proceedings of the 8th SIGdial 
Workshop on Discourse and Dialogue (pp. 75-78). 
Antwerp, Belgium. 
Boucher, Jerry D. & Charles E. Osgood. 1969. The 
Pollyanna hypothesis. Journal of Verbal Learning 
and Verbal Behaviour, 8: 1-8. 
Di Eugenio, Barbara & Michael Glass. 2004. The 
kappa statistic: A second look. Computational Lin-
guistics, 30(1): 95-101. 
Eggins, Suzanne & James R. Martin. 1997. Genres 
and registers of discourse. In Teun A. van Dijk 
(ed.), Discourse as Structure and Process. Dis-
course Studies: A Multidisciplinary Introduction 
(pp. 230-256). London: Sage. 
Eggins, Suzanne & Diana Slade. 1997. Analysing 
Casual Conversation. London: Cassell. 
Esuli, Andrea & Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for 
opinion mining. Proceedings of 5th International 
Conference on Language Resources and Evaluation 
(LREC) (pp. 417-422). Genoa, Italy. 
Fleiss, Joseph L. 1971. Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 76: 378-382. 
Kennedy, Alistair & Diana Inkpen. 2006. Sentiment 
classification of movie and product reviews using 
contextual valence shifters. Computational Intelli-
gence, 22(2): 110-125. 
Knott, Alistair. 1996. A Data-Driven Methodology for 
Motivating a Set of Coherence Relations. Edin-
burgh, UK: University of EdinburghThesis Type. 
Martin, James R. & Peter White. 2005. The Language 
of Evaluation. New York: Palgrave. 
Nigam, Kamal & Matthew Hurst. 2006. Towards a 
robust metric of polarity. In Janyce Wiebe (ed.), 
Computing Attitude and Affect in Text: Theory 
and Applications (pp. 265-279). Dordrecht: Sprin-
ger. 
Orasan, Constantin. 2003. PALinkA: A highly custo-
mizable tool for discourse annotation. Proceedings 
of 4th SIGdial Workshop on Discourse and Dialog 
(pp. 39 ? 43). Sapporo, Japan. 
Pang, Bo & Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. Proceedings of 
42nd Meeting of the Association for Computation-
al Linguistics (pp. 271-278). Barcelona, Spain. 
Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
Machine Learning techniques. Proceedings of Con-
ference on Empirical Methods in NLP (pp. 79-86). 
Polanyi, Livia & Annie Zaenen. 2006. Contextual 
valence shifters. In James G. Shanahan, Yan Qu & 
Janyce Wiebe (eds.), Computing Attitude and Af-
fect in Text: Theory and Applications (pp. 1-10). 
Dordrecht: Springer. 
Seki, Yohei, Koji Eguchi & Noriko Kando. 2006. 
Multi-document viewpoint summarization focused 
on facts, opinion and knowledge. In Janyce Wiebe 
(ed.), Computing Attitude and Affect in Text: 
Theory and Applications (pp. 317-336). Dordrecht: 
Springer. 
Stone, Philip J. 1997. Thematic text analysis: New 
agendas for analyzing text content. In Carl Roberts 
(ed.), Text Analysis for the Social Sciences. Mah-
wah, NJ: Lawrence Erlbaum. 
Taboada, Maite, Caroline Anthony & Kimberly Voll. 
2006. Creating semantic orientation dictionaries. 
Proceedings of 5th International Conference on 
Language Resources and Evaluation (LREC) (pp. 
427-432). Genoa, Italy. 
Taboada, Maite & Jack Grieve. 2004. Analyzing ap-
praisal automatically. Proceedings of AAAI Spring 
Symposium on Exploring Attitude and Affect in 
Text (AAAI Technical Report SS-04-07) (pp. 158-
161). Stanford University, CA. 
Turney, Peter. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews. Proceedings of 40th Meeting 
of the Association for Computational Linguistics 
(pp. 417-424). 
Voll, Kimberly & Maite Taboada. 2007. Not all 
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. Pro-
ceedings of the 20th Australian Joint Conference 
on Artificial Intelligence (pp. 337-346). Gold 
Coast, Australia. 
Whitelaw, Casey, Navendu Garg & Shlomo Arga-
mon. 2005. Using Appraisal groups for sentiment 
analysis. Proceedings of ACM SIGIR Conference 
on Information and Knowledge Management 
(CIKM 2005) (pp. 625-631). Bremen, Germany. 
Wiebe, Janyce & Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unan-
notated texts. Proceedings of Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics (CICLing-2005). Mex-
ico City, Mexico. 
Witten, Ian H. & Eibe Frank. 2005. Data Mining: 
Practical Machine Learning Tools and Techniques 
(2nd edn.). San Francisco: Morgan Kaufmann. 
 
69
Appendix A: Full lists of formal and functional zones 
 
 
Figure A1. Functional zones 
 
 
Figure A2. Formal zones 
 
Describe
Comment
Plot
Character
Specific
General
Content
Plot
Actors+characters
Specific
General
Overall
Plot
Actors+characters
Specific
General
Content
Structural
elements
Information
about the
film
Tagline
Structure
Off-topic
Title, Title+year, Runtime,
Country+year, Director,
Genre, Audience-restriction,
Cast, Credits, Show-Loc+date,
Misc-Movie-Info
Source, Author, Author-Bio,
Place, Date, Legal-Notice,
Misc-Review-Info, Rating
 
 
Appendix B: Kappa values for annotation task 
 
Classes 2-rater 
kappa 
3-rater 
kappa 
Describe/Comment/Describe+Comment/Formal .82 .73 
Describe/Comment/Formal .92 .84 
Describe/Comment/Describe+Comment .68 .54 
Describe/Comment .84 .69 
Table B1. Kappa values for stage annotations 
 
 
70
Tutorials, NAACL-HLT 2013, pages 4?6,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Discourse Processing
Manfred Stede
Universita?t Potsdam
stede@uni-potsdam.de
1 Overview
The observation that discourse is more than a mere sequence of utterances or sen-
tences amounts to a truism. But what follows from this? In what way does the
?value added? arise when segments of discourse are juxtaposed - how does hierar-
chical structure originate from a linearized discourse?
While many discourse phenomena apply to dialogue and monologue alike, this
tutorial will center its attention on monologue written text. The perspective taken is
that of practical language processing: We study methods for automatically deriving
discourse information from text, and point to aspects of their implementation. The
emphasis is on breadth rather than depth, so that the attendees will get an overview
of the central tasks of discourse processing, with pointers to the literature for study-
ing the individual problems in more depth. Much of the tutorial will follow the line
of the recent book M. Stede: Discourse Processing. Morgan & Claypool 2011.
Specifically, we will study the most important ways of ascribing structure to
discourse. This is, first, a breakdown into functional units that are characteristic for
the genre of the text. A news message, for example, is conventionally structured in
a different way than a scientific paper is. For grasping this level of structure, the
patterns that are characteristic for the specific genre need to be modeled.
Second, an ongoing text, unless it is very short, will cover different topics and
address them in a sensible linear order. This is largely independent of genre, and
since the notion of topic is relatively vague, it is harder to describe and sometimes
difficult to identify. The common approach is to track the distribution of content
words across the text, but in addition, overt signals for topic switches can be ex-
ploited.
Third, the identification of coreference links is a central aspect of discourse
processing, and has received much attention in computational linguistics. We will
survey the corpus-based methods that have dominated the field in recent years, and
4
then look at the ramifications that the set of all coreference links in a text has for
its structure.
Fourth, we investigate the structure resulting from establishing coherence rela-
tions (e.g., Cause, Contrast) among adjacent text segments. The term ?discourse
parsing? is often used for the task of identifying such relations (by exploiting more
or less explicit linguistic signals) and building tree structures that reflect the se-
mantic or pragmatic scaffolding of a (portion of) text.
Thus emerges a picture of a text as a series of different, yet related, layers
of analysis. The final part of the tutorial addresses the issue of inter-connections
between these levels. As a tool for accessing such multi-layered text corpora, we
will see how the (open-source) ANNIS2 database allows for querying the data
across different layers, and for visualizing different structural layers in appropriate
ways.
2 Outline
1. Introduction: Coherence and cohesion. How does a text differ from a ?non-
text??
2. Discourse structure as induced by the genre. Not all texts are created equal:
The genre can determine text structure to a large extent. We look at three
examples: Court decisions, film reviews, scientific papers.
3. Topics and text structure. Few texts keep talking about just one thing: Meth-
ods for finding topic breaks.
4. Coreference and its role for text structure. For understanding a text, we need
to know who and what is being referred to: Methods for coreference analysis.
5. Coherence relations and ?rhetorical structure?. Trees resulting from seman-
tic or pragmatic links between text segments: Methods for discourse parsing.
6. Synopsis: Text analysis on multiple levels
7. Accessing multi-layer corpora: The ANNIS2 Database
3 Speaker Bio
Manfred Stede1, University of Potsdam. After completing his dissertation on the
role of lexical semantics in multilingual text generation, Manfred Stede shifted
1http://www.ling.uni-potsdam.de/?stede/
5
his research focus towards problems of discourse structure and its role in various
applications of text understanding. For discourse structure, his work centered on
coherence relations and associated structural descriptions of text, and on the lin-
guistic signals of such relations, especially connectives. From the early 2000s on,
he developed the Potsdam Commentary Corpus as an example of (German) texts
analyzed simultaneously on multiple levels, including sentential syntax, corefer-
ence, and rhetorical structure; in parallel, the technical infrastructure of a database
for querying and visualizing multi-layer corpora was developed. In recent years,
more analysis levels have been added to the corpus (e.g., content zones, connectives
and their arguments). As for applications, Manfred worked on text summarization
and various tasks of information extraction; more recently, his focus has been on
issues of subjectivity and sentiment analysis.
6
Connective-based Local
Coherence Analysis: A Lexicon
for Recognizing Causal
Relationships
Manfred Stede
University of Potsdam (Germany)
email: stede@ling.uni-potsdam.de
Abstract
Local coherence analysis is the task of deriving the (most likely) coher-
ence relation holding between two elementary discourse units or, recur-
sively, larger spans of text. The primary source of information for this
step is the connectives provided by a language for, more or less explic-
itly, signaling the relations. Focusing here on causal coherence relations,
we propose a lexical resource that holds both lexicographic and corpus-
statistic information on German connectives. It can serve as the cen-
tral repository of information needed for identifying and disambiguating
connectives in text, including determining the coherence relations being
signaled. We sketch a procedure performing this task, and describe a
manually-annotated corpus of causal relations (also in German), which
serves as reference data.
221
222 Stede
1 Introduction
?Text parsing? aims at deriving a structural description of a text, often a tree in the
spirit of Rhetorical Structure Theory (Mann and Thompson, 1988). For automating
this task (see, e.g., Sumita et al (1992); Corston-Oliver (1998); Marcu (2000)), the
central source of information are the connectives that the author employed to more
or less specifically signal the type of coherence relation between adjacent spans. For
illustration, consider this short text:1.
Because well-formed XML does not permit raw less-than signs and ampersands, if you use a character
reference such as &#60; or the entity reference &lt; to insert the < character, the formatter will
output &lt; or perhaps &#60;.
Supposing that we are able to identify the connectives and punctuation symbols cor-
rectly (here in particular: note that to is not a spatial preposition; distinguish between
commas in enumerations and those finishing clauses), we can identify the ?scaffold?
of this short text as the following:
Because A, if B or C to D, E or F
with A to F representing the minimal units of analysis. Next, fairly simple rules will
be sufficient to guess the most likely overall bracketing of this string:
(Because A, (if ((B or C) to D)), (E or F))
And finally, it happens that the connectives because, if, to and or are quite reliable sig-
nals of the coherence relations Reason, Condition, Purpose and Disjunction, respec-
tively. Combining this information with the bracketing, we can obtain a tree structure
in spirit of RST.
Texts of this level of complexity could be handled by early text parsers (see Sec-
tion 2). But, obviously, not too many texts behave as nicely as our example does. In
general, constructing a discourse tree is highly complicated even without trying to find
semantic/pragmatic labels for the relationships; the discussion by Polanyi et al (2004)
demonstrates that just the structural decisions are often very difficult to make. Taking
a different viewpoint, this author argues in Stede (2008) that constructing ?the? tree
structure for a text should not be regarded as such an important goal and that coher-
ence should rather be explained as the interplay of different levels of (possibly partial)
description, such as referential and thematic structure, intentional structure, and a level
of local coherence analysis that records the clearly recognizable relationships between
adjacent text spans but does not aim at constructing a complete and well-formed tree.
In the present paper, this viewpoint is taken to the task of automatic analysis, which
aims at identifying individual coherence relations and the spans related. We restrict
ourselves here to causal relationships and moreover to those that are explicitly sig-
naled by a connective. The central resource used in our approach is a lexicon that
collects the information associated with individual connectives and makes it available
to applications such as a coherence analysis or text generation.
The paper is organized as follows. After reviewing some earlier research on text
parsing in Section 2, we turn to connectives in Section 3 and point out a number of
problems that sophisticated coherence analyzers have to reckon with. Then, Section 4
explains the connective lexicon we developed, and Section 5 describes a corpus we
collected and annotated manually for causal connectives and the relations they signal.
1Source: http://www.cafeconleche.org/books/bible2/chapters/ch17.html
Connective-based Local Coherence Analysis 223
It serves as a reference for designing the analysis procedure, which is finally sketched
in Section 6. Our analysis and implementation target German text, but most of the
phenomena apply equally to English.
2 Related Work
In the late 1990s. the best-known work on ?text parsing? was that of Marcu, which
is collected in Marcu (2000). He had used surface-based and statistical methods to
identify elementary discourse units, hypothesize coherence relations between adjacent
segments, and finally compute the most likely overall ?rhetorical tree? for the text.
Surface-based methods were highly popular at the time, but with the recent advances
in robust and wide-coverage sentence parsing, it seems sensible to cast local coherence
analysis as a problem of linguistic analysis, drawing on the results of syntactic parsing
(or even, on top of that, semantic analysis).
An early approach in this spirit was implemented in the RASTA analyzer (Corston-
Oliver, 1998). It perused the output of the ?Microsoft English Grammar? to guess the
presence of coherence relations on the basis of accumulated evidence from a variety
of more or less deep linguistic features. For instance, a hypotactic clause would al-
ways figure as the satellite of some nucleus-satellite relation in RST terms. For some
relations (e.g., Elaboration), the type of referring expressions, especially in subject
position, was considered a predictive feature. In general, RASTA employed a set of
necessary criteria for each relation to hold in a particular context, and for those rela-
tions passing the filter, a voting scheme accumulated evidence to decide on the most
likely relation. The system worked on Encarta articles, hence on expository text; 13
relations were being used.
While RASTA employed a relation-centric approach, the recent work by L?ngen
et al (2006) places the connectives at the center of the analysis, recording information
about them in a specific lexicon (similar to our own earlier work (Stede, 2002)). In the
lexicon used by L?ngen et al, an entry consists of three zones: the identification zone
gives the textual representation of the connective, its lemma and part-of-speech tag;
the filter zone encodes necessary conditions for particular discourse relations, in the
form of context descriptions; the allocation zone then specifies a default relation to be
assumed if no other relation can be derived on the basis of further (soft) conditions. It
also encodes constraints on the size of units to be related, the nuclearity assignment,
and the information whether the segment including the connective attaches to the left
or to the right in the text. Each entry gives rise to a rule used by a shift-reduce parser
that tries to build a complete rhetorical tree. This parser works in close cooperation
with a module identifying logical document structure, and the context conditions spec-
ified in lexicon entries often refer to this level of structure, or to a syntactic dependency
analysis provided by the Connexor parser2.
We share with these approaches (and with that of Polanyi et al (2004)) the desire to
derive as much information about discourse relations as possible without resorting to
non-linguistic knowledge, so that the role of local coherence analysis in effect can be
seen as extending the realm of robust sentence parsing. Our approach is to represent
as much of the necessary information as possible in a declarative resource: a lexicon
2http://www.connexor.com
224 Stede
of connectives.
3 Complications with Connectives
Connectives are closed-class lexical items that can belong to four different syntactic
categories: coordinating and subordinating conjunction, adverbial, and preposition
(such as despite or due to). They have in common that semantically they denote two-
place relations, and the text spans they relate can at least potentially be expressed as
full clauses (Pasch et al, 2003). As mentioned in the beginning, they are not always
as easy to interpret as in our ?well-formed XML? example. In this section, we suggest
an inventory of the complications that a thorough local coherence analysis procedure
needs to deal with. We group them into four categories.
Ambiguity. Here we need to distinguish two kinds: (i) ambiguity as to whether a
word is used as a connective or not, and (ii) ambiguity as to the semantic reading of a
connective. Certain cases of (i) correspond to the distinction between ?sentential use?
and ?discourse use? that Hirschberg and Litman (1994) had proposed not for connec-
tives but more generally for ?cue phrases? in spoken language. For example, German
denn can be a coordinating conjunction (sentential use) or a particle often used in
questions without a recognizable semantic effect (discourse use). Other cases of (i)
reflect ambiguity between different ?sentential? uses. Sometimes this coincides with a
syntactic difference (e.g., English as is a connective only when used as subordinator),
but with many adverbials it does not (e.g., German daher can be a locative adver-
bial ?from there? or a causal adverbial ?therefore?). Also, sometimes the distinction
coincides with semantic scope, as with the focus particle / connective nur (?only?):
(1) Es war ein sch?ner Sommertag. Nur die V?gel sangen nicht.
(?It was a nice summer day. Only the birds weren?t singing.?)
In a narrow-scope reading of ?only?, the message is that everybody was singing except
for the birds; in a wide-scope reading, ?only? connects the two sentences and signals
a restrictive elaboration. Ambiguity of type (i) is more widespread than one might
think; in Dipper and Stede (2006), we report that 42 out of 135 frequent German
connectives also have a non-connective reading, and we point out that many of the
problems cannot be handled with off-the-shelf part-of-speech taggers.
Concerning ambiguity (ii), some connectives can have more than one semantic
reading, which we regard as a difference in the coherence relation being signaled.
Sometimes, the relation can be established on different levels of linguistic description
(see, e.g., Sweetser (1990)). For example, finally can be used to report the last one
in a sequence of events, or it can be used by the author as a device for structuring
the discourse (?and my last point is...?). Interestingly, the very similar German word
schlie?lich in addition has a third reading: It can also be an argumentative marker
conveying that a presented reason is definitive or self-evident, which in English may
be signaled with ?after all?: Vertraue ihr. Sie ist schlie?lich die Chefin. (?Trust her.
She is the boss, after all.?)
Pragmatic features. In addition to the relational differences, connectives can some-
times be distinguished by more fine-grained pragmatic features, which are usually not
modeled as a difference in coherence relation. A well-known case in point is the dif-
ference between because and since (corresponding to German weil / da), where only
Connective-based Local Coherence Analysis 225
the latter has a tendency to mark the following information as hearer-old (not neces-
sarily discourse-old). The same pair of connectives serves to illustrate the feature of
non-/occurrence within the scope of focus particles:
(2) Nur weil/?da es regnet, nehme ich das Auto
(?Only because/?since it?s raining, I take the car.?)
While in German, the da variant is hardly acceptable at all, in English there is a ten-
dency for since to be interpreted in its temporal reading when used within the scope
of only.
Also, connectives can convey largely the same information yet differ in terms of
stylistic nuances, for instance in degree of formality. Thus a concessive relation in
English may be signaled in a standard way with although, or with a rather formal, and
in that sense ?marked? notwithstanding construction.
Form. While the majority of connectives consist of a single word, some of them have
two parts. Well-known instances are either .. or and if .. then. For the German version
of the latter (wenn .. dann), a coherence analyzer must account for the possibility
of its occurring in reverse order: Dann nehme ich eben das Auto, wenn Du so bet-
telst. (?Then I?ll take the car, if you?re begging so much?.) Further, looking at highly
frequent collocations such as even though or even if, it is difficult to decide whether
we are dealing with a single-word connective and a focus particle, or with a complex
connective; one solution is to check in such cases whether the meaning is in fact de-
rived compositionally and then to prefer the focus particle analysis. From ?regular?
two-word connectives it is only a small step to the shady area of phrasal connectives,
which can allow for almost open-ended variation and modification: for this reason /
for these reasons / for all these very good reasons / ....
For German, we have dealt with the issue of differentiating between types of multi-
token connectives in a separate paper Stede and Irsig (2008).
Discourse structure. As is well-known, the structural description of a text can also
bemore complicated than in our ?well-formedXML? example shown at the beginning.
For one thing, discourse units can be embedded into one another, using parenthetical
material or appositions. Further, connectives can occasionally link text segments that
are non-adjacent ? a phenomenon that has been studied intensively by Webber et al
(2003) and also by Wolf and Gibson (2005). An example from Webber et al: John
loves Barolo. So he ordered three cases of the ?97. But he had to cancel the order
because then he discovered he was broke. Here, the then is to be understood as linking
the discovery event back to the ordering event rather than to the (adjacent) canceling.
In German, many adverbial connectives have an overt anaphoric affix (e.g., deswegen,
daher, trotzdem), and the ability to link non-adjacent segments appears to be restricted
to these. Non-adjacency also leads to the issue of crossing dependencies, which is also
discussed by the two teams of authors mentioned above. It correlates with the problem
of two connectives occurring in the same clause, as it happens in the Barolo example
(because then), which renders the parsing task significantly more complex than in the
?well-formed XML? example.
A different problem is to be found in situations where a single coherence relation
is signaled twice, by two different connectives, where one typically is to be read cat-
aphorically:
226 Stede
(3) Ich nehme deshalbi das Auto, weili Du so bettelst.
(?I take the car (for that reason)i becausei you?re begging so much.?)
This phenomenon is difficult to reproduce in English; again, in German it is also
limited to a certain class of connectives that can serve as cataphoric ?correlates?. Ob-
viously, in such examples, a coherence analyzer will have to be very careful not to
hypothesize two separate causal relationships. The same danger applies when multi-
ple causes are enumerated for the same consequence, or multiple consequences arising
from the same cause. The mere insertion of the focus particle auch (?also?) in example
3 can fundamentally change the discourse structure to stating two reasons for taking
the car:
(4) Es regnet sehr stark. Ich nehme deshalb das Auto, auch weil Du so bettelst.
(?It?s raining heavily. I therefore take the car, also because you?re begging so
much.?)
Finally, it is to be noted that certain connectives convey information about the dis-
course structure beyond the local relation between two segments. A case in point is
the first word of this paragraph, which not only makes a ?List? or ?Enumeration? rela-
tion explicit, but also provides the information that this very list is now coming to an
end. A smart coherence analyzer could thus reduce the search space for linking the
subsequent text segment ? it will definitely not be part of the same ?List? configura-
tion.
4 A Rich Lexical Resource for Connectives
For building programs to perform local coherence analysis on texts that display the
complexities discussed above, our approach is to clearly divide the labor between
a declarative connective lexicon on the one hand, and a flexible analysis procedure
on the other. In this section, we describe our Discourse Marker Lexicon (DIMLEX),
whose first version was described in Stede (2002). At the time, it was used for rela-
tively simple text parsing as outlined at the beginning of the paper, and also for a lan-
guage generation application. The multi-functionality results from using a rather ab-
stract XML encoding for the ?master? lexicon, which is transformed by XSLT scripts
to the format needed by a specific application ? both in terms of technical format
(e.g., programming language) and the amount and granularity of information needed
for the application. With our current focus on causal relations, we extended the DIM-
LEX entries of the causal connectives to a richer scheme, which will gradually be
transferred to the remaining connectives as well.
It is not trivial to define an inventory of causal connectives, due to the grey area
of words marking a semantic relationship that readers can also interpret causally ?
after all, causality is very often not explicitly signaled but being left for the reader to
reconstruct. For example, in The wind shook the shed for a few seconds, and then
it collapsed there certainly is causality involved in the relationship between the sen-
tences, but we would not want to treat and or then as causal connectives. With the
help of the ?Handbook of German Connectives? (Pasch et al, 2003), we determined a
set of 66 German connectives that primarily convey causality.
Connective-based Local Coherence Analysis 227
The DIMLEX entries for these connectives consist of the following zones of infor-
mation: (1) orthography, syntax, and structural features; (2) non-/connective disam-
biguation rules; (3) semantic and pragmatic features, including information on dis-
ambiguating different readings, and on role linking. As for the type of information,
entries contain both binary features and probabilities derived from corpus analyses.
Orthography and syntax. Orthographic variants that we store in the lexicon result
from the recent official German spelling reform and from frequent mistakes made
by speakers/authors (as found in corpora). Also, we list both upper and lower case
spellings because this difference plays a role in many disambiguation rules (see be-
low). Each variant has a unique identifier that is being used in those rules. Also, one
of the variants is marked as ?canonical? for co-reference purposes. Here is a sample
excerpt from the entry for aufgrund, corresponding to the English due to:
<orth type="simple" canon="1" onr="k2v1">
<part type="cont">aufgrund</part> </orth>
<orth type="complex" canon="0" onr="k2v2">
<part type="cont">auf Grund</part> </orth>
<orth type="simple" canon="0" onr="k2v3">
<part type="cont">Aufgrund</part> </orth>
<orth type="complex" canon="0" onr="k2v4">
<part type="cont">Auf Grund</part> </orth>
Each orth is of type ?simple? or ?complex?, depending on the number of tokens
involved. For simple connectives (single tokens), the part type is always ?cont?
(continuous), whereas for complex connectives it may also be ?discontinuous? if lin-
guistic material can intervene between the parts (which is not the case for the two
complex variants above).
Syntactically, connectives can be subordinating conjunctions; Postponierer; pre-
, post- and circumpositions; and adverbials, some of which can occur only in spe-
cific positions (characterized in accordance with the Feldermodell that is often used
to describe German sentence structure in terms of Vorfeld, Mittelfeld, Nachfeld). We
encode this information following the classification by Pasch et al (2003)), whose
primary criterion is whether the connective can be integrated into the clause, and if
so, at what positions it can occur. Here is the information for the prepositional adverb
(?padv?) dadurch (?by means of this?):
<padv>
<vorfeld>1</vorfeld>
<mittelfeld>1</mittelfeld>
<nacherst>0</nacherst>
<nachfeld>1</nachfeld>
<nullstelle>0</nullstelle>
<nachnachfeld>0</nachnachfeld>
<satzklammer>0</satzklammer>
</padv>
The binary features say that the connective can be in the Vorfeld (preceding the
finite verb or auxiliary: Dadurch ist es geschehen), Mittelfeld (between auxiliary and
228 Stede
verb: Es ist dadurch geschehen), and Nachfeld (following the verb phrase: Es ist
geschehen dadurch).
As a representation more directly usable for computational purposes, we also spec-
ify patterns of the connective being situated in a syntax tree in TIGER format (Brants
et al, 2004). This format is used both in large hand-annotatedGerman corpora as well
as in an automatic parser3. The idea of the patterns in the lexical entry thus is to find
instances of the word in a TIGER-tree, whether coming from a treebank or from a
parser. For illustration, here is the pattern for the complex connective so .. dass (?so ..
that?):
(#avp:[cat="AVP"] > [lemma="so"])
&
((#avp > #s:[cat="S"])
|
((#avp > #cs:[cat="CS"]) &
(#cs > #s:[cat="S"]))
)
&
(#s > [lemma=("dass")])
This expression looks for an adverbial phrase (AVP) that dominates both so and a sen-
tence (S), or a coordination of sentences (CS) that in turn dominate dass. Between the
so and dass, any material can intervene. An examples matched by this expression in
the TIGER corpus is: Der Kanzler hat China so gern , da? er ihm sogar die h?chsten
Berge der Welt zu schenken verm?chte. (?The chancellor likes China so much, that he
even wants to give the world?s highest mountains as a present to the country.?)
Besides the syntactic structure of individual conjuncts, we also need to represent
the possibilities on linear order of the conjuncts. This is also based on the terminology
of Pasch et al (2003), who distinguish between the internal conjunct (the clause or
phrase that the connective syntactically belongs to) and the external one. Sometimes,
this a hard constraint: With the conjunction denn (causal ?for?), the internal conjunct
can only follow the external one. With other connectives, e.g., weil (?because?), both
orderings are possible, i.e., the because-clause giving a reason can precede or follow
the clause giving the effect. In these cases we include probabilities derived from a
corpus analysis, which the coherence analysis module can use for disambiguating
scope when it has no other information available.
The syntactic representations become somewhat more complicated in case of com-
plex connectives. For instance, there is a variant of dadurch that co-occurs with a sub-
sequent (but not necessarily adjacent!) complement clause headed by dass (?that?).
Similarly, as shown in the previous section, certain causal conjunctions and adverbials
can co-occur and redundantly mark the same relation. Our lexicon entries contain fea-
tures representing those possible pairings. For a more general discussion on German
complex connectives, see Stede and Irsig (2008).
Finally, we include a feature stating whether the connective can be in the scope of a
focus particle. This information can sometimes support non-/connective disambigua-
tion.
3http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html
Connective-based Local Coherence Analysis 229
Non-/connective disambiguation. In Dipper and Stede (2006), we reported on an
approach to disambiguating non-/connective use for nine connectives by incremen-
tally training a Brill tagger, which lead to F-measures of 81% (+connective) and 95%
(?connective) in the best of four training scenarios. During this work it became clear
that the part-of-speech context of the word often indeed provides enough information
for making the decision. The main reason why off-the-shelf taggers, however, do not
perform very well is that tagsets do not reflect the distinction ? recall the syntactic
heterogeneity of the ?class? of connectives. From our findings we thus constructed for
each connective a set of patterns over part-of-speech and lemma information, leading
to regular expressions associated with probabilites (again gathered from corpus stud-
ies). These expressions become part of the DiMLex entries and can be used by the co-
herence analyzer. Starting from the Dipper/Stede results, we manually created classes
of connectives with apparently-equivalent behavior, rather than studying each of the
66 connectives in detail. For illustration, here is the pattern set for daher, which can
be a causal connective (?therefore?) or a locative adverb (?from there?):
<conn-disambi>
<pros>
<pro value="90" ref="k5v2"> $. $$/PROAV </pro>
<pro value="90" ref="k5v1"> VVFIN $$/PROAV </pro>
</pros>
<cons>
<con value="99" ref="k5v1 k5v2">
$$/PROAV $, {?dass?}/KOUS
</con>
<con value="95" ref="k5v2">
$. $$/PROAV .* {?kommen? ?ruehren?} .+ $, {?dass?}/KOUS
</con>
<con value="99" ref="k5v1"> $$/PROAV $. </con>
</cons>
</conn-disambi>
Weights range from 0 to 100, so 99 represents basically a strict rule. Notice the ref
attribute, which restricts the rules to orthographic variants (in this case to upper and
lower case ones). The first two rules support a +connective reading: daher tagged as
pronominal adverb (PROAV) following a full stop or a finite verb, respectively. The
following three rules support a ?connective reading: daher followed by the subordi-
nating conjunction (KOUS) dass; occurring in a collocation like kommt daher, dass
(?stems from?); occurring before a full stop, i.e., sentence-final.
Semantics and Pragmatics. As stated earlier, we identify a difference in readings
with a difference in coherence relation signaled by the connective. As for the in-
ventory of relations, we take inspiration from Mann and Thompson (1988), Asher
and Lascarides (2003), and especially for the causal relations, from the taxonomic
approach of Sanders et al (1992). Not every distinction made in the literature can
be traced to connectives; so we do for instance not follow RST?s distinction between
?Volitional Cause? and ?Non-volitional Cause? in DIMLEX. But we find differences in
connective use for semantic versus pragmatic causal relations (Sanders et al, 1992).
230 Stede
For instance, the denn used in (4) below is quite typical for pragmatic relations (see,
e.g. Pasch, 1989).
(5) Er wird bestimmt p?nktlich kommen, denn er ist doch immer so gewissenhaft.
(?Surely he will arrive on time, for he is always so assiduous.?)
Thus, in the realm of causality we use coherence relations labeled ?Argument-Claim?
(pragmatic) and ?Reason-Consequence? (semantic). Further, if the consequence is a
yet-unrealized intended effect, we assign the relation ?Purpose? as it has been sug-
gested by Mann and Thompson (1988). The connectives associated with Purpose are
mostly quite specific (e.g., English in order to; German um .. zu), but there can also
be ambiguity between Purpose and ?other? causality (e.g., English so that; German
damit).
Disambiguation between the semantic and the pragmatic relation is usually very
difficult and thus a matter of heuristically weighing the evidence. Similar to our han-
dling non-/connective disambiguation (see above), we use a scheme of weight ac-
cumulation for features indicating the presence of a relation. For example, for the
connective schlie?lich we found that with the main verb of the clause elided, the prag-
matic reading is very unlikely; on the other hand, if the verb is in present tense and the
Aktionsart is ?state?, it very likely signals the pragmatic ?Claim-Argument? relation.
Other evidence for this relation includes modal particles signaling the epistemic status
of the proposition(s), often in conjunction with present or future tense. This is illus-
trated in example 4 above, where the speaker expresses her confidence that the event
will materialize with bestimmt (?surely?), while doch a?in the second clause marks the
information has hearer-old, so that the difference between claim and argument in this
case is quite transparent. Other features we modeled are inspired by the empirical
work of Frohning (2007). They include position, tense and aspect of the clause, mood
and modality, and lexical collocations; Frohning derived their weights from corpus
analyses.
Often, however, no compelling evidence for either of the three relations can be
found, and for these cases we use a neutral relation called ?Cause-Caused?, which is
thus meant to subsume the two others.
In addition to relation(s), a lexicon entry specifies the role linking for connectives:
the mapping from the syntactically internal or external conjunct (see above) to its
function in the relation. We label these functions in accordance with the relations:
?Argument?, ?Claim?, ?Reason?, and so forth. Since causal relations are directed, and
the mapping cannot be predicted from syntactic features, it is crucial to represent this
information explictly.
Besides, we use a number of more idosyncratic features to represent information
that is relevant only for certain connectives, in particular to distinguish very similar
ones. An example mentioned in the previous section is the information-structural dif-
ference between weil (?because?) and da (?since?). For other families of connectives,
this ?miscelleneous features? section is more important; with temporal connectives,
for instance, we specify in addition to the coarse-grained coherence relation more
fine-grained distinctions such as whether the time spans of the related events meet or
not, etc.
Connective-based Local Coherence Analysis 231
Having discussed our treatment of syntax and semantics separately, we now have
to attend to the relationship between the two, i.e., to the issues of ambiguity and pol-
ysemy. The majority of connectives has one syntactic description and can convey
one or two similar coherence relations (the typical ambiguity between semantic and
pragmatic reading). We do, however, also find other configurations:
? Two syntactic descriptions: weil used to be a subordinating conjunction, but in
spoken German is now widely accepted as a coordinating conjunction as well.
Since the meaning is the same, it suffices to simply list both syntactic variants
in DIMLEX.
? One syntactic description, many coherence relations: When used as an adverb,
the connective damit can signal Purpose (?so that?) or Reason-Consequence
(?thus?). This situation is similar to the previous one: We provide a disjunction
of semantic readings (including the disambiguation information) and a single
syntactic description.4
? Two syntactic descriptions, several coherence relations: These cases are the
only serious complications, as a difference in syntax can correlate with one in
semantics, so that we cannot simply specify disjunctions for the syntactic and
semantic descriptions. Instead, we use multiple lexical entries, in accordance
with the intuition that we are dealing with fairly unrelated items (polysemy).
An example is dann (?then?), which on the one hand is a temporal adverbial,
and on the other hand can express a Condition relation (optionally with a corre-
sponding wenn (?if?) in the other clause). In the latter case, it does not behave
as an adverb, though, but it governs a verb-second clause. So, distributing the
information across two separate lexicon entries seems to be appropriate.
Finally, to enhance the maintainability of DIMLEX, we inlcude with the entries
a range of linguistic examples that illustrate the relevant distinctions, and we also
citee information that is provided by standard dictionaries ? especially in those cases
where our formalization is not yet complete. One of the XSLT scripts for converting
DIMLEX maps the base lexicon to an HTML format that allows for inspecting the
entries, including the information just mentioned, which is intended for the human
eye rather than for automatic parsers or generators.
5 A Corpus Annotated with Causal Relations
As a preparatory step for implementing a local coherence analyzer that aims specifi-
cally at identifying causal relations, we built a corpus with causal connectives anno-
tated manually. We selected 200 short texts from a product review web site5, where
travelers comment on various tourist destinations. Since they often give reasons for
the opinions they express, this genre offers more instances of causal connectives than,
say, newspaper text. On the other hand, there is the undeniable drawback of frequent
4As a matter of fact, the situation is more difficult: Damit is one of the most complicated words in
our lexicon, as it also has a reading as subordinator where it signals Purpose, as well as a non-connective
adverbial reading (?with it/that?).
5http://www.dooyoo.de
232 Stede
mistakes in grammar and orthography, which makes any automatic analyis quite hard,
and also sometimes poses challenges to the human annotator.
Creating the corpus involved several steps. First, potential causal connectives were
searched automatically (using the list from DIMLEX) and manually filtered. Subse-
quently, identifying causal connectives was not an issue for the annotation process, as
they were already presented to annotators as ?anchors? for their task. We then de-
signed annotation guidelines with instructions for identifying causes and effects. As
for the length of spans, annotators were encouraged to prefer a shorter span in cases
where the boundary of a cause or effect is not quite clear. At the same time, they were
asked to mark two discontinuous spans in cases where a cause/effect was interrupted
by extraneous material such as authors? remarks on their own text production. Thus,
in the following example, the C1 and C2 indices mark the intended cause, and E the
intended effect.
(6) [The beach was not very pleasant]E , as [it was,]C1 I just have to say this here,
[utterly littered with remains of picnics.]C2
When multiple reasons are given for the same effect (or vice versa), annotators had to
mark them separately, so that each cause-effect pair can be derived individually from
the annotated data. Sometimes this multiplicity can involve separate connectives, as
in the following example. In such cases, annotators had to choose a central connective
(the one linking the adjacent cause and effect) and then add additional ones as sec-
ondary connectives, possibly forming a chain. This ensures easy retrievability of all
pairs from the data.
(7) [We reached the hotel late]E [due to]Co1 [the flight?s delay]Ca and also
[because]Co2 [it took so long to find a cab.]Cb
Further, annotators had to identify possible redundant markings of the same cause-
effect pair (as with the cataphoric correlates discussed above) as well as focus particles
that modify connectives. Thus, in example (7), they would mark also and link it to the
modified because.
Our first version of the annotation guidelines was subject to an informal evalua-
tion with annotators who had not been involved in the project. On the basis of the
results we clarified several aspects in the guidelines and thus wrote the final version.
Furthermore, we prepared two instructional videos: one for using the annotation tool
MMAX26, and one for our specific annotation scenario, illustrating the handling of a
fairly complicated text passage. In the formal evaluation with two annotators, they re-
ceived no training other than by the guidelines and the two videos. Of 78 connectives,
34 were analyzed identically. The vast majority of the mismatches (36 of 44) re-
sulted from different span length: There was overlap between the spans chosen by the
annotators, but the boundaries were not exactly identical. Other mismatches, which
occurred only a few times, included different decisions on secondary connectives and
the resulting chains of causes/effects.
Finally, with the guidelines having become stable, experienced annotators created
the ?official? annotation of the entire corpus of 200 texts (containing some 1,200
6http://mmax2.sourceforge.net
Connective-based Local Coherence Analysis 233
causal connectives). It is now is available as a resource for training and evaluation
of automatic procedures. We also developed a web-based viewer (essentially translat-
ing the MMAX2 format to HTML and Javascript) that allows for manually browsing
the corpus comfortably.7
6 Towards recognizing causal relations automatically
Having described DIMLEX as the central resource for local coherence analysis, and
the corpus as reference and evaluation tool, we now briefly sketch a procedure for
recognizing causal relations, whose implementation is currently under way in our text
analysis workbench (Chiarcos et al, 2008), a standoff XML architecture for fusing
linguistic annotations coming from different manual or automatic annotation tools. In
this highly modular approach, the output of each individual analysis module is stored
in a separate layer, using our standoff XML format PAULA (Dipper, 2005). Analysis
tools can use previously computed layers for their own task, which usually involves
creating one or more new layers.
In this setting, the task of local coherence analysis involves the following layers.
The first four are to be built in the pre-processing phase, and the last two are the result
of the coherence analyzer:
1. Token layer (including sentence boundaries)
2. Part-of-Speech
3. Logical document structure (headlines, paragraph breaks, etc.)
4. Dependency syntax analysis
5. Elementary discourse units
6. Connectives and (sets of) EDUs they relate
The procedure of coherence analysis consists of the following three sequential
steps, which at various points make use of information from DIMLEX:
Connective identification. All the words listed in DIMLEX as some orthographic
variant of a causal connective are identified in the text. This includes a check for
complex connectives as listed in the lexicon, i.e., two correspondingwords in adjacent
clauses (amongst others, the if .. then type). It also includes a check for correlates,
i.e., a connective that according to DIMLEX can be a correlate occurring in a clause
immediately preceding a subordinate clause governed by a connective that according
to DIMLEX can have a correlate (the deshalb .. weil type. For these checks, the syntax
layer (4) is used to identify adjacent clauses.
Next, the single-word connective candidates are run through the disambiguation
filters, i.e., the PoS/token regular expressions specified in their lexical entries are
matched against the text?s PoS representation on the corresponding layer (2). Those
items that appear to be words in non-connective use are removed from the connective
list. Finally, a new layer (6) is created, for now holding only the words that were
recognized as connectives.
7All material can be found at http://www.ling.uni-potsdam.de/~stede/kausalkorpus.html.
234 Stede
Segmentation. The basic idea of our approach follows that of the module imple-
mented by L?ngen et al (2006) for German. We first overgenerate, guessing segment
boundaries at every possible position, according to the dependency parse result; then,
contextual rules remove those boundaries that appear to be wrong (e.g., commas in
enumerations). We are, however, using somewhat different definitions of segments,
namely a variant of Jasinskaja et al (2007), and the corpus annotated according to
those segmentation guidelines will be used to evaluate our module. One issue where
we diverge from both L?ngen et al and from Jasinskaja et al is in our handling of
prepositions: We do admit certain prepositional phrases as elementary discourse units,
but only those that are headed by a preposition listed in DIMLEX, e.g., the causal
markers wegen (?due to?) or durch (?through?). The resulting sequence of segments is
represented on a new layer of analysis (5).
Relation and scope identification. Next, the connective layer (6) is extended with
information on relations and scopes: Every connective is associated with one or more
attribute-value structures listing possible coherence relations along with probabilities.
To this end, all relations stored with the connective in DIMLEX are recorded as hy-
potheses, and weights are accumulated as the result of evaluating the associated dis-
ambiguation rules, which largely operate on the syntax layer, as explained in Section 4.
Finally, for each relation we also hypothesize its scope: the thematic roles are as-
sociated with sequences of minimal units from layer (5). Given a reliable syntactic
analysis, scope determination is usually straightforward for coordinating and subordi-
nating conjunctions. For adverbials, we hypothesize different solutions and rank them
according to size: The most narrow interpretation is taken as most likely. In this step,
we also consider the layer of logical document structure in order to avoid segments
that would stretch across paragraphs or other kinds of boundaries. Similarly, a layer
with the results of ?text tiling? (breakdown of the text in terms of thematic units, in the
tradition of Hearst (1994)) could be used for this purpose, as well as as an ?attribution?
layer that identifies those modal contexts that attribute a span of text to a particular
source (as in indirect speech).
In this way, the module will generate hypotheses of coherence relations and related
spans, for the time being solely on the basis of connectives occurring in the text. As
explained, this information is represented in two additional analysis layers. Modules
following in the processing chain may combine the various hypotheses into the most
likely overall relational tree structure for the paragraph (or a set of such tree struc-
tures, see Reitter and Stede (2003)), or they may use the hypotheses directly for some
application that does not rely on a spanning tree.
7 Discussion
The central idea behind the separation of the declarative DIMLEX resource and the (on-
going) implementation of an analysis procedure is to facilitate a smooth extensibility
of the overall approach towards further kinds of connectives and coherence relations.
When the lexicon is extended ? while the underlying scheme remains unchanged
? coverage of the analyzer grows without adaptations to the analysis procedure. An
important benefit of the XML-based organization of the lexicon is its suitability for a
variety of applications (parsing, generation, lexicography), which can each select from
Connective-based Local Coherence Analysis 235
the master lexicon exactly those types of information that are relevant for them. On
the other hand, an obvious drawback of the present ?flat? XML format is a relatively
high degree of redundancy. The good reasons for introducing inheritance-based rep-
resentation formalisms in ?standard? computational lexicons of content words largely
apply to the realm of connectives (and possibly to other function words) as well. For
the time being, however, the more mundane task of lexical description still offers a
great many open questions for individual connectives and families thereof; the issue
of more intelligent storage should become prominent later, when the groundwork has
stabilized.
As with the vast majority of coherence relations, causal ones often need not be
explicitly signaled at the linguistic surface by a connective. Thus the approach pro-
posed in this paper will of course only partially solve the problem of local coherence
analysis. An important challenge for future work is to identify linguistic features of
discourse units other than connectives that can also serve to at least constrain the range
of admissible coherence relations (see, e.g. Asher and Lascarides, 2003). Investigating
these with empirical methods is an important next step in the overall program of par-
tially deriving coherence relations in authentic text without resorting to non-linguistic
knowledge.
Acknowledgments
The following people from the Potsdam Applied Computational Linguistics Group
contributed to the work described in this paper (in alphabetical order): Andr? Herzog
(causality corpus); Kristin Irsig (DiMLex entries for causal connectives); Andreas
Peldszus (causality corpus and annotation guidelines); Uwe K?ssner (implementation
of LCA module).
References
Asher, N. and A. Lascarides (2003). Logics of Conversation. Cambridge: Cambridge
University Press.
Brants, S., S. Dipper, P. Eisenberg, S. Hansen, E. K?nig, W. Lezius, C. Rohrer,
G. Smith, and H. Uszkoreit (2004). Tiger: Linguistic interpretation of a german
corpus. Research on Language and Computation 2(4), 597?620.
Chiarcos, C., S. Dipper, M. G?tze, J. Ritz, and M. Stede (2008). A flexible frame-
work for integrating annotations from different tools and tagsets. In Proc. of the
First International Conference on Global Interoperability for Language Resources,
Hongkong.
Corston-Oliver, S. (1998). Computing of Representations of the Structure of Written
Discourse. Ph. D. thesis, University of California at Santa Barbara.
Dipper, S. (2005). XML-based stand-off representation and exploitation of multi-
level linguistic annotation. In R. Eckstein and R. Tolksdorf (Eds.), Proceedings of
Berliner XML Tage, pp. 39?50.
236 Stede
Dipper, S. and M. Stede (2006). Disambiguating potential connectives. In M. Butt
(Ed.), Proceedings of KONVENS ?06, Konstanz, pp. 167?173.
Frohning, D. (2007). Kausalmarker zwischen Pragmatik und Kognition. Korpus-
basierte Analysen zur Variation im Deutschen. T?bingen: Niemeyer. (Im Er-
scheinen).
Hearst, M. A. (1994). Multi-paragraph segmentation of expository text. In Proceed-
ings of the 32nd Meeting of the Association for Computational Linguistics, Las
Cruces/NM.
Hirschberg, J. and D. J. Litman (1994). Empirical studies on the disambiguation of
cue phrases. Computational Linguistics 19(3), 501?530.
Jasinskaja, K., J. Mayer, J. Boethke, A. Neumann, A. Peldszus, and K. J. Rodr?guez
(2007). Discourse tagging guidelines for German radio news and newspaper com-
mentaries. Ms., Universit?t Potsdam.
L?ngen, H., H. Lobin,M. B?renf?nger,M. Hilbert, and C. Puskas (2006). Text parsing
of a complex genre. In B. Martens and M. Dobreva (Eds.), Proc. of the Conference
on Electronic Publishing (ELPUB 2006), Bansko, Bulgaria.
L?ngen, H., C. Puskas, M. B?renf?nger, M. Hilbert, and H. Lobin (2006). Discourse
segmentation of German written text. In T. Salakoski, F. Ginter, S. Pyysalo, and
T. Phikkala (Eds.), Proceedings of the 5th International Conference on Natural
Language Processing (FinTAL 2006), Berlin/Heidelberg/New York. Springer.
Mann, W. and S. Thompson (1988). Rhetorical structure theory: Towards a functional
theory of text organization. TEXT 8, 243?281.
Marcu, D. (2000). The theory and practice of discourse parsing and summarization.
Cambridge/MA: MIT Press.
Pasch, R. (1989). Adverbials?tze ? kommentars?tze ? adjungierte s?tze. eine hy-
pothese zu den typen der bedeutungen von ?weil?, ?da? und ?denn?. In W. Motsch
(Ed.), Wortstruktur und Satzstruktur, Linguistische Studien des ZISW: Reihe A ?
Arbeitsberichte 194, pp. 141?158. Berlin: Akademie der Wissenschaften der DDR.
Pasch, R., U. Brau?e, E. Breindl, and U. H. Wa?ner (2003). Handbuch der deutschen
Konnektoren. Berlin/New York: Walter de Gruyter.
Polanyi, L., C. Culy, M. van den Berg, G. L. Thione, and D. Ahn (2004). A rule
based approach to discourse parsing. In Proceedings of the SIGDIAL ?04 Work-
shop, Cambridge/MA. Assoc. for Computational Linguistics.
Reitter, D. and M. Stede (2003). Step by step: underspecified markup in incremental
rhetorical analysis. In Proceedings of the 4th International Workshop on Linguisti-
cally Interpreted Corpora (LINC), Budapest.
Sanders, T., W. Spooren, and L. Noordman (1992). Toward a taxonomy of coherence
relations. Discourse Processes 15, 1?35.
Connective-based Local Coherence Analysis 237
Stede, M. (2002). DiMLex: A lexical approach to discourse markers. In Exploring
the Lexicon - Theory and Computation. Alessandria: Edizioni dell?Orso.
Stede, M. (2008). RST revisited: Disentangling nuclearity. In C. Fabricius-Hansen
and W. Ramm (Eds.), ?Subordination? versus ?coordination? in sentence and text.
Amsterdam: John Benjamins.
Stede, M. and K. Irsig (2008). Identifying complex connectives: Complications for
local coherence analysis. In A. Benz, P. K?hnlein, and M. Stede (Eds.), Proceed-
ings of the Workshop on Constraints in Discourse, Potsdam, pp. 77?84.
Sumita, K., K. Ono, T. Chino, T. Ukita, and S. Amano (1992). A discourse structure
analyzer for Japanese text. In Proceedings of the International Conference on Fifth
Generation Computer Systems, pp. 1133?1140.
Sweetser, E. (1990). From etymology to pragmatics. Cambridge: Cambridge Univer-
sity Press.
Webber, B., M. Stone, A. Joshi, and A. Knott (2003). Anaphora and discourse struc-
ture. Computational Linguistics 29(4), 545?587.
Wolf, F. and E. Gibson (2005). Representing discourse coherence: a corpus-based
study. Computational Linguistics 31(2), 249?287.
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 81?86,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
From newspaper to microblogging: What does it take to find opinions?
Wladimir Sidorenko and Jonathan Sonntag and Manfred Stede
Applied Computational Linguistics
University of Potsdam/Germany
sidarenk|sonntag|stede@uni-potsdam.de
Nina Kru?ger and Stefan Stieglitz
Dept. of Information Systems
University of Mu?nster/Germany
nina.krueger|stefan.stieglitz@uni-muenster.de
Abstract
We compare the performance of two lexicon-
based sentiment systems ? SentiStrength
(Thelwall et al, 2012) and SO-CAL (Taboada
et al, 2011) ? on the two genres of newspaper
text and tweets. While SentiStrength has been
geared specifically toward short social-media
text, SO-CAL was built for general, longer
text. After the initial comparison, we suc-
cessively enrich the SO-CAL-based analysis
with tweet-specific mechanisms and observe
that in some cases, this improves the perfor-
mance. A qualitative error analysis then iden-
tifies classes of typical problems the two sys-
tems have with tweets.
1 Introduction: Twitter, SentiStrength and
SO-CAL
In recent years, microblogging has been an attrac-
tive new target for sentiment analysis. The question
studied in this paper is how the methods used for
?standard? newspaper text can be transferred to mi-
croblogs. We focused on the Twitter network be-
cause of its widespread use, and because Twitter
communication, in response to emerging issues, is
fast and especially ad hoc, making it an effective
platform for the sharing and discussion of crisis-
related information (Bruns/Burgess, 2011). Further-
more, Twitter is characterized by a high topicality of
content (Milstein al., 2008).
Specifically, we present experiments involving
two sentiment analysis systems that both employ
a combination of polarity lexicon and sentiment
composition rules: (i) SentiStrength (Thelwall et
al., 2012), a system that is geared toward short
social-media text, and (ii) SO-CAL (Taboada et al,
2011), ?Semantic Orientation Calculator?, a general-
purpose system that was designed primarily to work
on the level of complete texts. While both are
lexicon-based approaches, there are certain differ-
ences in the roles of the various submodules. For our
purposes here, it is important that SentiStrength was
designed to cope specifically with ?user-generated
content?. Among the features of the system, as
stated by Thelwall et al, the following four are espe-
cially important for tweets: (i) a simple spelling cor-
rection algorithm deletes repeated letters when the
word is not found in the dictionary; (ii) repeated let-
ters lead to a boost in sentiment value; (iii) an emoti-
con list supplements the polarity lexicon; (iv) pos-
itive sentences ending in an exclamation mark re-
ceive an additional boost, and multiple exclamation
marks further strengthen the polarity.
SO-CAL, on the other hand, does not include
social-media-specific measures. In contrast, it was
designed for determining semantic orientation on
the text level; in our experiments here, we are thus
using it for the non-intended purpose of sentence-
level sentiment, on tweet ?sentences?.
Next, we review related work on twitter sentiment
analysis (Section 2), and describe the data sets for
our experiments in Section 3. Then we investigate
the relative performance of SentiStrength and SO-
CAL on newspaper text and on tweets (Section 4),
including experiments with preprocessing steps. In
Section 5, we present observations from a qualitative
evaluation, and we interpret the results and conclude
in Section 6.
81
2 Related work
Following the work on ?standard? text, sentiment
classification on tweets is often treated as a two-step
task, e.g., (Barbosa/Feng, 2010): subjectivity classi-
fication followed by polarity classification. For sub-
jectivity classification, (Pak/Paroubek, 2010) found
that the distribution of POS tags is a useful feature,
due to, for example, the presence of modal verbs in
subjective tweets.
For polarity assignment, one approach is to au-
tomatically build large sets of training data and
then train classifiers on token n-grams; in this vein,
(Pak/Paroubek, 2010) found that in their approach,
bigrams outperform unigrams and trigrams, and
they report f-measures around 0.6 for the three-
way pos/neg/neutral classification. The other, non-
learning, approach is to rely on a polarity wordlist
(or a collection of several, as in (Joshi et al, 2011;
Mukherjee et al, 2012)). Mukherjee et al report
an accuracy of 66.69% for pos/neg, and 56.17% for
pos/neg/neut classification.
Typical preprocessing steps employed by the
approaches discussed are the correction of mis-
spellings, the replacement of URLs and hashtags,
the translation of emoticons and of slang words.
Sometimes, stop word removal and stemming is
used; sometimes deliberately not. Few authors eval-
uate the influence of the various measures; one ex-
ception is (Mukherjee et al, 2012).
A recent branch of research deals with fine-
grained target-specific analysis (as proposed re-
cently by (Jiang et al, 2011)). In our work, how-
ever, we tackle the more coarse-grained problem
of assigning a single sentiment value to a complete
tweet. However, we will return to the issue of target-
specificity in our conclusions.
An interesting result from analysing the state of
the art is that apparently no consensus has been
reached yet on the question of ?extra difficulty? of
tweet sentiment analysis. While everybody agrees
that tweets are noisy and can pose considerable diffi-
culty to any standard linguistically-inspired analysis
tool, it is not clear to what extent this is a problem
for sentiment analysis. Some authors argue that the
noise renders the task more difficult than the anal-
ysis of longer text, while others maintain that the
brevity of tweets is in fact an advantage, because ? as
(Bermingham/Smeaton, 2010) put it, ?the short doc-
ument length introduces a succinctness to the con-
tent?, and thus ?the focused nature of the text and
higher density of sentiment-bearing terms may ben-
efit automated sentiment analysis techniques.? In
their evaluation, the classification of microblogs in-
deed yields better results than that of blogs.
In correspondence with this open question, there
are only few investigations so far on the performance
differences for existing sentiment tools operating on
newspaper versus social media text. To shed more
light on the issue, we chose to run a set of com-
parative experiments with the two aforementioned
lexicon/rule-based systems, on both newspaper and
twitter corpora.
3 Data sets
MPQA The well-known MPQA corpus1 (Wiebe
et al, 2005) of newspaper text has fine-grained an-
notations of ?private states? at phrase level. For our
purposes these need to be reduced to a more coarse-
grained labelling of sentence-level sentiment. To
avoid ambiguity, we ignored those sentences that in-
clude both positive and negative sentiment annota-
tions. From the remaining sentences, we selected
100 positive and negative sentences each, where the
former target-specific sentiment is now taken to rep-
resent sentence-level sentiment. The data set is a
difficult one, given that we are dealing with isolated
sentences from newspaper reports.
Qantas To track Twitter data we used a self-
developed prototype (see (Stieglitz/Kaufhold,
2011)). We concentrate our analysis on Qantas, an
Australian leading carrier for long-haul air travel,
for which we assume substantial interest in public
communication. We furthermore expect that ?
caused by some management crises in 2011 ? online
communication around Qantas-related topics is
characterized by a strong emotional investment of
stakeholders.
The tracking tool captures all those tweets that
contain the keyword ?Qantas? in their content, in the
username of the sender, or in a URL. After spam re-
moval, we had a dataset of some 27,000 tweets, col-
lected between mid-May and mid-November 2011.
1http://mpqa.cs.pitt.edu/
82
Topic #pos #neut #neg #irrelevant
Apple 219 581 377 164
Google 218 604 61 498
Microsoft 93 671 138 513
Twitter 68 647 78 611
Table 1: Distribution of tweets and labels across subcor-
pora
For evaluation purposes, 300 Tweets have been man-
ually annotated by two annotators in parallel, using
a polarity scale ranging from -2 to 2. 190 Tweets of
those (63%) received identical labels, and we used
only this set in our experiments described below.
That means we also discarded cases of ?minor? dis-
agreement such as a -1/-2 annotation.
Sanders The Sanders corpus2 is a corpus consist-
ing of 5513 tweets of various languages which have
been annotated for sentiment. The tweets have been
sampled by the search terms ,,@apple?, ,,#google?,
,,#microsoft? and ,,#twitter?. Each tweet is accom-
panied by a date-time stamp and the target of its po-
larity. Possible polarity values are positive, negative,
neutral (simple factual statements / questions with-
out strong emotions / neither positive nor negative /
both positive and negative), and irrelevant (spam /
non-English). The positive and negative tweets thus
contain judgements on the companies or their prod-
ucts/services. Along with the corpus comes an anno-
tation scheme and statistics about the corpus. Some
numbers of the size and distribution within the cor-
pus are given in Table 1.
According to the annotation guidelines, positive
and negative labels were only assigned to clear cases
of sentiment. Ambigious tweets have been anno-
tated as neutral.
4 Experiments and results
4.1 Performance on MPQA sentences
In order to establish a basis for the comparison, we
first ran a small comparative evaluation on ?stan-
dard? text, i.e., on the sentences from the MPQA
newspaper corpus. The results, given in Table 2,
show that both systems perform considerably better
2http://www.sananalytics.com/lab/
twitter-sentiment/
SentiStrength SO-CAL
acc pos 0.2727 0.4717
acc neg 0.7071 0.6542
weighted avg 0.4899 0.5634
Table 2: Accuracy on MPQA sentences
Senti- SO-CAL SO-CAL
Strength preproc.
Qantas
acc 0.3754 0.3953 0.3887
acc pos 0.3091 0.2545 0.2545
acc neg 0.2857 0.2857 0.2857
acc neut 0.6164 0.6781 0.6644
avg sentiment 1.1075 1.2756 1.3316
Sanders total
acc 0.5945 0.5899 0.5790
acc pos 0.6171 0.5694 0.6032
acc neg 0.4572 0.5301 0.5519
acc neut 0.6230 0.6092 0.5802
avg sentiment 0.8517 1.3761 1.5233
Sanders twitter
acc 0.4985 0.5804 0.5387
acc pos 0.4286 0.3750 0.4821
acc neg 0.4590 0.4754 0.5246
acc neut 0.5099 0.6121 0.5245
avg sentiment 0.8393 1.4054 1.6978
Table 3: Accuracy on tweet corpora
on negative than on positive sentences, and overall
there is a slight advantage for SO-CAL.
4.2 Performance on Qantas and Sanders tweets
In Table 3, we show the system performance on the
Twitter corpora: Qantas, the complete Sanders cor-
pus, and the Sanders subcorpus with target ?Twit-
ter?. We ran evaluations on all four separate sub-
corpora, but only ?Twitter? showed interesting dif-
ferences from the results for the total corpus, and
that is why they are included in the table. The ?acc?
row gives the overall weighted accuracy. ?Avg senti-
ment? is the absolute value of the sentiment strength
determined by SentiStrength and SO-CAL; notice
that these should not be compared between the two
systems, as they do not operate on the same scale.
(We will return to the role of sentiment strength in
Section 6.)
83
4.3 Preprocessing steps
Since SO-CAL was not intended for analyzing Twit-
ter data, we implemented three preprocessing steps
to study whether noise effects of this text genre can
be reduced. Similarly to the steps suggested by
(Mukherjee et al, 2012), we first unified all URLs,
e-mail addresses and user names by replacing them
with unique tokens. Additionally, in step 1 all hash
marks were stripped from words, and emoticons
were mapped to special tokens representing their
emotion categories. These special tokens were then
added to the polarity lexicons used by SO-CAL.
In step 2, social media specific slang expressions
and abbreviations like ?2 b? (for ?to be?) or ?im-
sry? (for ?I am sorry?) were translated to their ap-
propriate standard language forms. For this, we used
a dictionary of 5,424 expressions that we gathered
from publicly available resources.3
In the last step, we tackled two typical spelling
phenomena: the omission of final g in gerund forms
(goin), and elongations of characters (suuuper). For
the former, we appended the character g to words
ending with -in if these words are unknown to vo-
cabulary,4 while the corresponding ?g?-forms are in-
vocabulary words (IVW). For the latter problem,
we first tried to subsequently remove each repeat-
ing character until we hit an IVW. For cases re-
sisting this treatment, we adopted the method sug-
gested by (Brody/Diakopoulos, 2011) and generated
a squeezed form of the prolongated word, subse-
quently looking it up in a probability table that has
previously been gathered from a training corpus.
Altogether, SO-CAL does not benefit from pre-
processing in the Qantas corpus, but it does help for
the pos/neg tweets from the Sanders corpus, espe-
cially for the Twitter subcorpus. The observation
that the accuracy on neutral tweets decreases while
the average sentiment increases will be discussed
in Section 6. We also measured the effects of the
three individual steps in isolation, and the only note-
worthy result is that SentiStrength, when subjected
to our ?extra? preprocessing, benefits slightly from
slang normalization for the Qantas corpus, and from
3http://www.noslang.com/dictionary/,
http://onlineslangdictionary.com/, http:
//www.urbandictionary.com/
4For vocabulary check, we used the open Hunspell dictio-
nary (http://hunspell.sourceforge.net/).
noise cleaning for some parts of the Sanders corpus.
5 Qualitative evaluation
Having computed the success rates, we then per-
formed a small qualitative evaluation: What are the
main reasons for the misclassifications on tweets? In
addition, we wanted to know why the Qantas corpus
yielded much worse results than the Sanders corpus,
and thus we looked into its results.
5.1 Problems for SO-CAL
We chose SO-CAL?s judgements as the basis for this
evaluation and randomly selected 120 tweets from
the Sanders corpus that were not correctly classi-
fied. The distribution across the manual annotations
pos/neg/neut was 40/40/40.
In Table 4, we provide a classification of the rea-
sons for problems. The first group are cases where
we would not agree with the annotation and thus
cannot blame SO-CAL. The second group includes
problems that are beyond the scope of the system
and hence, strictly speaking, not its fault. Among the
typos, there are cases of misspelled opinion words,
but also a few where the typo leads to problems with
SO-CALs linguistic analysis and in consequence to
a misclassification. The slang words include items
like ?wow!? but also shorthands such as ?thx?. Most
important are ?domain formulae?: expressions that
require inferences in order to identify the sentiment.
An example is ?I now use X instead of TARGET?.
We encounter these most often in negative tweets,
where complaints are expressed, as in ?My phone
can send but not receive texts.?
In the third group, we find problems that are or
could be in the scope of SO-CAL. Occasionally,
negation or irrealis rules misfire. Gaps in the lex-
icon are noticeable especially on the positive side
(examples: ?loving?, ?better?, ?thanks to?). ?Lex-
ical ambiguity? refers to words that may or may
not carry polarity; by far the most frequent example
here is ?new?, which SO-CAL labels positive, but in
technology-related tweets often is neutral. Also in
neutral tweets, we often find high complexity, i.e.,
cases where both positive and negative judgements
are mixed. And finally, a fair number of problems
stems from sentiment expressed on the wrong target
of the tweet.
84
Problem Pos Neg Neut
Annotation ambig. 15% 0% 2%
Typo 3% 5% 10%
Slang words 12% 10% 0%
Sarcasm 0% 2% 0%
Domain formula 23% 60% 5%
Wrong rule 3% 5% 3%
Lexicon gap 30% 12% 0%
Lexical ambiguity 5% 5% 50%
Complexity 0% 0% 18%
Wrong target 8% 0% 12%
Table 4: SO-CAL error types on 120 Sanders tweets
Problem Pos Neg Neut
Annotation ambig. 45% 25% 12%
Typo 18% 0% 0%
Slang words 0% 0% 0%
Sarcasm 0% 16% 0%
Domain formula 9% 42% 4%
Wrong rule 9% 0% 10%
Lexicon gap 9% 16% 0%
Lexical ambiguity 0% 0% 16%
Complexity 9% 0% 16%
Spam / news 0% 0% 41%
Table 5: Error types on 75 Qantas tweets
5.2 Observations on the Qantas corpus
The analysis of 75 Qantas tweets that have been mis-
classified by both SentiStength and SO-CAL yielded
the results in Table 5: Again, many annotation cases
are ambiguous, and domain formulae are the ma-
jor problem with negative tweets. Sarcasm is much
more frequent than in the Sanders corpus. The cen-
tral problem for neutral tweets stems from the fact
that spam and tweets containing headlines and URLs
of news messages have been annotated as neutral,
but these may very well contain polarity-bearing
words, which are then detected by the systems.
6 Interpretation and Conlusions
News versus tweets. Since the Sanders corpus is
much larger than Qantas, we regard it as the tweet
representative for the comparison to MPQA (a dif-
ficult data set, as argued above). For positive text,
both SentiStrength and SO-CAL yield better re-
sults on tweets, while for negative texts, the results
on tweets are much lower than on news sentences.
Within the news genre, however, both systems per-
form much better on negative than on positive text.
So we conclude a ?polarity flip? in the performance
of both systems when going from news to tweets.
Differences among tweets. Based on the Sanders
corpus, the SentiStrength and SO-CAL results are
a little better than those reported by (Mukherjee et
al., 2012), who achieved 56.17% accuracy for the
three-way classification. As SO-CAL does not in-
clude tweet-specific analysis, we may conclude that
the utility of such genre-specific measures is in fact
limited. ? An interesting question is why the ?Twit-
ter? subcorpus of Sanders behaves so different from
the others: While overall accuracy is the same, the
figures for the three categories differ widely. Also,
SO-CAL here benefits heavily from preprocessing
on the non-neutral tweets. One factor is the large
proportion of neutral tweets (see Table 1); besides,
we find that these tweets are not as target-related as
those for Apple, Google, Microsoft; it seems that
users often drop a ?#twitter? without actually talking
about Twitter or its service.
Preprocessing. Of the four measures taken by
SentiStrength to account for tweet problems (see
Sct. 1), SO-CAL already implements the exclama-
tion mark boost; the other three were added in our
own preprocessing, but we did only minimal spell-
checking. Overall, SO-CAL does not profit as much
as we had expected, but we find a fair improvement
(0.57?0.6) for the positive Sanders tweets. For neu-
tral tweets, performance actually decreases.
The role of targets An interesting observation is
that adding preprocessing to SO-CAL leads to de-
tecting ?more? sentiment: The average sentiment
values increase for all the corpora in Table 3. At the
same time, the accuracy on neutral tweets decreases,
which indicates that ?spurious? sentiment is being
detected. The most likely reason is that SO-CAL in-
deed profits from tweet-preprocessing but then de-
tects sentiment that is unrelated to the target and
therefore not annotated in the gold data. An im-
portant direction for future work therefore is to pay
more attention to target-specific sentiment identifi-
cation, cf. (Jiang et al, 2011).
85
Acknowledgments
This work was funded by German Ministry for Edu-
cation and Research (BMBF), grant 01UG1232D.
References
L. Barbosa and J. Feng. 2010. Robust sentiment detec-
tion on twitter from biased and noisy data. Proc. of
COLING (Posters), Beijing.
A. Bermingham and A. Smeaton. 2010. Classifying
Sentiment in Microblogs: Is Brevity an Advantage?
Proc. of the 20th ACM Conference on Information and
Knowledge Management (CIKM), Toronto.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using
Word Lengthening to Detect Sentiment in Microblogs.
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pp. 562?570, Edinburgh.
A. Bruns and J.E. Burgess. 2011. The Use of Twitter
Hashtags in the Formation of Ad Hoc Publics. 6th
European Consortium for Political Research General
Conference, Reykjavik, Iceland, pp. 25-27.
L. Jiang, M. Yu, M. Zhou, X. Liu and T. Zhao.
2011. Target-dependent twitter sentiment classifica-
tion. Proc. of the 49th Annual Meeting of the ACL,
pp. 151-160, Portland/OR.
A. Joshi, Balamurali A R, P. Bhattacharyya and R. Mo-
hanty. 2011. C-Feel-It: a sentiment analyzer for
micro-blogs. Proc. of the ACL-HLT 2011 System
Demonstrations, pp. 127-132, Portland/OR.
S. Milstein, A. Chowdhury, G. Hochmuth, B. Lorica and
R. Magoulas. 2008. Twitter and the Micro-Messaging
Revolution: Communication, Connections, and Imme-
diacy - 140 Characters at a Time.
S. Mukherjee, A. Malu, A.R. Balamurali and P. Bhat-
tacharyya. 2012. TwiSent: a multistage system for
analyzing sentiment in twitter. Proc. of the 21st ACM
Conference on Information and Knowledge Manage-
ment (CIKM).
A. Pak and P. Paroubek. 2010. Twitter as a corpus
for sentiment analysis and opinion mining. Proc. of
LREC, Valletta/Malta.
S. Stieglitz and C. Kaufhold. 2011. Automatic Full
Text Analysis in Public Social Media ? Adoption of
a Software Prototype to Investigate Political Commu-
nication. Proc. of the 2nd International Conference on
Ambient Systems, Networks and Technologies (ANT-
2011) / The 8th International Conference on Mobile
Web Information Systems (MobiWIS 2011), Procedia
Computer Science 5, Elsevier, 776-781.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M.
Stede. 2011. Lexicon-based methods for sentiment
analysis. Computational Linguistics, 37(2):267?307.
M. Thelwall, K. Buckley, and G. Paltoglou. 2012. Sen-
timent strength detection for the social Web. Journal
of the American Society for Information Science and
Technology, 63(1):163?173.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2?3):165?210.
86
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 98?102,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Importing MASC into the ANNIS linguistic database:
A case study of mapping GrAF
Arne Neumann
EB Cognitive Science and SFB 632
University of Potsdam
neumana@uni-potsdam.de
Nancy Ide
Department of Computer Science
Vassar College
ide@cs.vassar.edu
Manfred Stede
EB Cognitive Science and SFB 632
University of Potsdam
stede@uni-potsdam.de
Abstract
This paper describes the importation of
Manually Annotated Sub-Corpus (MASC)
data and annotations into the linguistic
database ANNIS, which allows users to vi-
sualize and query linguistically-annotated
corpora. We outline the process of
mapping MASC?s GrAF representation to
ANNIS?s internal format relANNIS and
demonstrate how the system provides ac-
cess to multiple annotation layers in the
corpus. This access provides information
about inter-layer relations and dependen-
cies that have been previously difficult to
explore, and which are highly valuable for
continued development of language pro-
cessing applications.
1 Introduction
Over the past decade, corpora with multiple lay-
ers of linguistic annotation have been developed
in order to extend the range of empirically-based
linguistic research and enable study of inter-layer
interactions. Recently created corpora include
OntoNotes (Pradhan et al, 2007), the Groningen
Meaning Bank (Basile et al, 2012), and the Man-
ually Annotated Sub-Corpus (MASC)1 (Ide et al,
2010). Typically, such corpora are represented
in idiosyncratic in-house formats, and developers
provide special software to access and query the
annotations (for example, the OntoNotes ?db tool?
and Groningen?s GMB Explorer). Access without
the use of developer-supplied software often re-
quires significant programming expertise, and as
a result, it is not easy?or even possible?for others
to add to or modify data and annotations in the re-
source.
This paper describes the importation of MASC
data and annotations into the linguistic database
1www.anc.org/MASC
ANNIS2 (Chiarcos et al, 2008; Zeldes et al,
2009), which was designed to visualize and query
linguistically-annotated corpora. Unlike most
other corpora with multi-layer annotations, no
special software has been developed for access
to MASC. Instead, all MASC data and annota-
tions are represented in GrAF (Ide and Suder-
man, 2007), the XML serialization of the abstract
model for annotations defined by ISO TC37 SC4?s
Linguistic Annotation Framework (ISO/LAF) (Ide
and Suderman, In press). GrAF is intended to
serve as a generic ?pivot? format that is isomor-
phic to annotation schemes conforming to the ab-
stract model and therefore readily mappable to
schemes used in available systems. We outline
the process of mapping GrAF to ANNIS?s internal
format relANNIS and demonstrate how the sys-
tem provides access to multiple annotation layers
in MASC.
2 The ANNIS Infrastructure
The ANNIS system is a linguistic database geared
toward the requirements of querying multi-layer
annotated corpora, and providing various visual-
ization means for layers with different structural
properties. In particular, the annotation types
supported are spans, DAGs with labelled edges,
and pointing relations between terminals or non-
terminals. For illustration, Figure 1 shows a
screenshot where various parallel annotations of
the same data are provided: dependency trees,
constituent trees (here with ?secondary edges? in
dotted lines), and a grid view for annotations that
assign labels to token spans. In addition, ANNIS
offers a ?discourse view? giving the complete text
with coreference relations indicated by color and
underlining. In the top of the screenshot, it can be
noted that the system also stored video (and au-
2http://www.sfb632.uni-potsdam.de/
annis/
98
Figure 1: Screenshot of ANNIS2
Figure 2: Querying MASC in ANNIS2 for an NP that includes both
a food frame element and a location named entity
99
dio) data, but that aspect shall not concern us in
this paper.
The system is web-based; the user interface is
written in Java and ExtJS. The backend is Post-
greSQL3. In general, all components are open
source under the Apache License 2.0, and you
can download ANNIS from the above-mentioned
URL. We offer two versions: A server version, and
the more lightweight ?ANNIS kickstarter?, which
can be installed locally, e.g., on laptops.
ANNIS is complemented by SaltNPepper, a
framework for converting annotations stemming
from various popular annotation tools (MMAX,
EXMARaLDA, annotate/Synpathy, RSTTool) ?
see Section 4.
3 MASC and GrAF
MASC is a fully open, half-million word corpus
covering nineteen diverse genres of American En-
glish drawn from the Open American National
Corpus (OANC)4. The corpus includes manually
produced or hand-validated annotations for mul-
tiple linguistic layers, including morphosyntax
(two different annotations), shallow parse (noun
and verb chunks), Penn Treebank syntax, and
named entities. Portions of the corpus are also
annotated for FrameNet frames, opinion, Prop-
Bank predicate-arguments, and WordNet 3.1 word
senses. Discourse-level annotation, including co-
reference, clauses, and discourse markers, will be
available in fall, 2013.
Like the OANC, all MASC annotations
are rendered in standoff form using GrAF,
the graph-based format developed as a part
of the ISO Linguistic Annotation Framework
(ISO/LAF)(ISO 24612, 2012). GrAF is an XML
serialization of the LAF abstract model for annota-
tions, a formalization of models used across mul-
tiple applications for associating (linking) infor-
mation, including not only directed-acyclic graphs
(DAGs) but also ER diagrams, the Universal Mod-
eling Language (UML), semantic and neural net-
works, RDF/OWL, and, more generally, hyper-
linked data on the World Wide Web. The model
is sufficiently general to represent any type of lin-
guistic annotation; any serialization of the model
can therefore serve as a pivot or intermediary
among diverse annotation formats that conform to
the abstract model. Thus, any sufficiently well-
3http://www.postgresql.org/
4www.anc.org/OANC
formed annotation scheme should be isomorphic
to a GrAF representation of the same information.
Problems arise only when a scheme does not spec-
ify information explicitly but rather embeds the in-
terpretation in processing software rather than in
the representation itself; for transduction to GrAF,
this information must be made explicit in the rep-
resentation.
Funding for MASC did not allow for extensive
software development; the expectation is that by
rendering the corpus in the ISO standard GrAF
format, access could rely on GrAF-aware software
developed by others, or transduction from GrAF
to appropriate alternative formats would be trivial.
We have already developed and deployed means
to import linguistic data represented in GrAF into
UIMA, GATE, and NLTK, and we provide trans-
ducers from GrAF to inline XML and the CoNLL
IOB format.5 Additionally, a GrAF-to-RDF trans-
ducer is near completion, which will enable inclu-
sion of MASC in the Linguistic Linked Open Data
(LLOD) cloud6. The incorporation of a GRAF
transducer for ANNIS provides another example
of the flexibility afforded via the GrAF represen-
tation.
4 Mapping GrAF to ANNIS via
SaltNPepper
Pepper is a software framework that converts lin-
guistic data among various formats, e.g. CoNLL,
EXMARaLDA, PAULA, TigerXML, RSTTool
and TreeTagger (Zipser et al, 2011). It is built
upon the graph-based Salt meta model (Zipser and
Romary, 2010), which is in turn based on the LAF
abstract model for linguistic annotation. Map-
ping GrAF to Salt extends the range of formats
into which annotations represented in GrAF can
be automatically transduced to those to which Salt
has been mapped, including ANNIS?s relational
database format relANNIS.
The following steps were taken to import the
MASC corpus into ANNIS: first, the MASC cor-
pus data was extracted with the GrAF API7. Sec-
ond, a mapping between GrAF and Salt data
structures was created. Most of the conversion
is straightforward, since both models are graph-
based. The only added processing is to provide
5Available from http://www.anc.org/MASC.
6http://linguistics.okfn.org/
resources/llod/
7http://sourceforge.net/projects/
iso-graf/
100
explicit edge labels in the Salt representation for
ordered constiuents: in GrAF, directed edges from
one to several other nodes by default represent sets
of ordered constituents and need not be explicitly
labeled as such, whereas in Salt, the role of all
edges must be specified explicitly. Explicit labels
in ANNIS are required in order to generate the ap-
propriate visualizations automatically (e.g. trees
for syntactic hierarchies and arc diagrams for syn-
tactic dependencies).
Finally, the code was structured as a plug-in
for Pepper and parameterized to make it usable
for GrAF-formatted corpora other than MASC. It
will be included in the next SaltNPepper release.
The code is currently available from our software
repository8.
5 MASC in ANNIS: Examples
The ANNIS Query Language (AQL) allows users
to search for specific token values and annotations
as well as relationships between them, even across
annotation level boundaries.9 Token values are
represented as text between quotes (e.g. "men"),
while annotations are specified as attribute-value
pairs (e.g. pos="NN", a part-of-speech attribute
with the value NN). A query for an annotation will
return all elements with that annotation. Where
necessary, namespaces10 can be added to any ele-
ment to disambiguate, e.g., ptb:cat="NP" sig-
nifies all annotation attribute-value pairs (attribute:
cat, value: NP) that are in the ptb (Penn Tree-
bank) namespace.
Relations among elements are specified by
back-referencing incremental variable numbers,
e.g. #1, #2 etc. Linguistically motivated opera-
tors bind the elements together; e.g. #1 > #2
means that the first element dominates the second
in a tree. Operators can express overlap and adja-
cency between annotation spans, as well as recur-
sive hierarchical relations that hold between nodes
(such as elements in a syntactic tree).
The following examples show AQL queries that
combine annotations from different layers:
8https://korpling.german.hu-berlin.
de/svn/saltnpepper/PepperModules/
GrAFModules/
9Note that ANNIS does not allow searching for arbitrary
strings from the primary data, but only for pre-identified seg-
ments such as tokens, named entities, etc.
10A namespace groups one or more types of annotation
into a logical unit, e.g all annotations produced by a specific
tool or project.
1. A VP that dominates a PP which contains a
named person at its right border:
cat="VP" & cat="PP" & NER="person" &
#1>#2 & #2 r #3
2. a VP of passive form in past tense that in-
cludes a mention of a FrameNet frame ele-
ment:
cat="VP" & voice="passive" &
tense="SimPas" & FE="Event" & #1 i #2
& #1 i #3 & #1 i #4
Figure 2 shows the results of a search for an
NP that includes both a named entity of the type
country and a FrameNet frame element of the type
Food:
cat="NP" & anc:type="country" &
FE="Food" & #1 i #2 & #1 i #3
6 Summary and Outlook
We explained the mapping of the MASC multi-
layer corpus to the ANNIS database by interpret-
ing the GrAF format via the Pepper framework.
Both MASC and ANNIS are freely available; a
portion of MASC will also be added to the online
demo version of ANNIS. We are also making the
Pepper converter module for GrAF available.
Version 3 of ANNIS is currently under devel-
opment11. Besides a new front-end and a REST-
based API, it offers improved tokenization support
(annotation on the level of subtokens; conflicting
tokenizations) and handles dialogue corpora with
simultaneous speakers as well as time-aligned au-
dio/video data.
The ability to query across multiple annota-
tion levels opens up significant new possibilities
for exploring linguistically annotated data. Most
commonly, language models are developed us-
ing information from at most one or two linguis-
tic layers; ANNIS enables user to explore inter-
dependencies that have been previously difficult
to detect. By providing tools and data that are
entirely free for use by the community, the AN-
NIS and MASC efforts contribute to the growing
trend toward transparent sharing and openness of
linguistic data and tools.
11Early development releases can be found at
http://www.sfb632.uni-potsdam.de/annis/
annis3.html
101
Acknowledgments
MASC and GrAF development was supported by
US NSF award CRI-0708952. The work of A.N.
and M.S. was supported by Deutsche Forschungs-
gemeinschaft as part of the Collaborative Research
Center ?Information Structure? (SFB 632) at Univ.
Potsdam and HU Berlin.
References
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Christian Chiarcos, Stefanie Dipper, Michael Go?tze,
Ulf Leser, Anke Lu?deling, Julia Ritz, and Manfred
Stede. 2008. A flexible framework for integrating
annotations from different tools and tag sets. Traite-
ment Automatique des Langues (TAL), 49(2).
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-
based format for linguistic annotations. In Proceed-
ings of the First Linguistic Annotation Workshop,
pages 1?8, Prague.
Nancy Ide and Keith Suderman. In press. The Linguis-
tic Annotation Framework: A Standard for Annota-
tion Interchange and Merging. Language Resources
and Evaluation.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The Manually Anno-
tated Sub-Corpus : A community resource for and
by the people. In Proceedings of the The 48th An-
nual Meeting of the Association for Computational
Linguistics, Uppsala, Sweden.
ISO 24612. 2012. Language Resource Management
? Linguistic Annotation Framework. International
Standards Organization, Geneva, Switzerland.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified relational
semantic representation. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 517?526, Washington, DC, USA.
IEEE Computer Society.
Amir Zeldes, Julia Ritz, Anke Lu?deling, and Christian
Chiarcos. 2009. ANNIS: A search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics 2009.
Florian Zipser and Laurent Romary. 2010. A model
oriented approach to the mapping of annotation for-
mats using standards. In Proceedings of the Work-
shop on Language Resource and Language Technol-
ogy Standards, LREC 2010, pages 7?18, Malta.
Florian Zipser, Amir Zeldes, Julia Ritz, Laurent Ro-
mary, and Ulf Leser. 2011. Pepper: Handling
a multiverse of formats. In 33. Jahrestagung
der Deutschen Gesellschaft fu?r Sprachwissenschaft,
Go?ttingen.
102
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 196?204,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Ranking the annotators: An agreement study on argumentation structure
Andreas Peldszus
Applied Computational Linguistics
University of Potsdam
peldszus@uni-potsdam.de
Manfred Stede
Applied Computational Linguistics
University of Potsdam
stede@uni-potsdam.de
Abstract
We investigate methods for evaluating
agreement among a relatively large group
of annotators who have not received exten-
sive training and differ in terms of ability
and motivation. We show that it is possi-
ble to isolate a reliable subgroup of anno-
tators, so that aspects of the difficulty of
the underlying task can be studied. Our
task is to annotate the argumentative struc-
ture of short texts.
1 Introduction
Scenarios for evaluating annotation experiments
differ in terms of the difficulty of the task, the
number of annotators, and the amount of training
that annotators receive. For simple tasks, crowd-
sourcing involving very many annotators has re-
cently attracted attention.1 For more difficult
tasks, the standard setting still is to work with
two or a few more annotators, train them well,
and compute agreement, usually in terms of the
kappa measure. In this paper, we study a dif-
ferent scenario, which may be called ?classroom
annotation?: The group of annotators is bigger
(in our example, 26), and there are no extensive
training sessions: Students receive detailed writ-
ten guidelines, there is a brief QA period, and an-
notation starts. In such a setting, one has to expect
some agreement problems that are due to different
abilities and different motivation of the students.
Our goal is to develop methods for systematically
studying the annotation results in such groups, to
identify more or less competent subgroups, yet at
the same time also learn about the difficulty of var-
ious aspects of the underlying annotation task. To
this end, we investigate ways of ranking and clus-
tering annotators.
1See, for instance, Snow et al (2008) or Bhardwaj et al
(2010) for strategies to analyse and cope with diverging per-
formance of annotators in that scenario.
Our task is the annotation of argumentation in
short texts, which is somewhat similar to mark-
ing the rhetorical structure, e.g. in terms of RST
(Mann and Thompson, 1988; Carlson et al, 2003).
Thus we are dealing with a relatively difficult task
involving text interpretation. We devised an an-
notation scheme (which is more fully described
elsewhere), and in order to study the feasibility,
first ran experiments with short hand-crafted texts
that collectively cover all the relevant phenom-
ena. This is the setting we report in this paper. A
separate step for future work is guideline revision
on the basis of the results, and then applying the
scheme to authentic argumentative text (e.g., user
generated content on various websites).
2 A theory of argumentation structure
Following up on Toulmin?s (1958) influential anal-
ysis of argument, Freeman (1991; 2011) worked
on integrating those ideas into the argument dia-
graming techniques of the informal logic tradition.
Freeman?s central idea is to model argumentation
as a hypothetical dialectical exchange between a
proponent, who presents and defends claims, and
a challenger (the ?opponent?), who critically ques-
tions them in a regimented fashion. Every move
in such a basic dialectical situation corresponds
to a structural element in the argument diagram.
The analysis of an argumentative text is thus con-
ceived as finding the corresponding critical ques-
tion of the challenger that is answered by a partic-
ular segment of the text.
Since the focus of this paper is on the evalu-
ation methodology, we provide here only a brief
sketch of the scheme; for a detailed description
with many examples, see Peldszus and Stede (to
appear). Premises and conclusions are proposi-
tions expressed in the text segments. We can
graphically present an argument as an argument
diagram, with propositions as nodes and the vari-
ous relations as arrows linking either two nodes or
196
Figure 1: Example of an argumentation structure
annotation for a short text
a node and a link2. See figure 1 for an example.
Notice that segments in favor of the proponent?s
position are drawn in circles, whereas the chal-
lenger?s perspective is given in boxes. The root
of an argument tree is the central statement made
in the text. In the example, it is expressed both in
segment 1 and in segment 8; the = indicates that
the annotator judges the contributions of the two
segments as equivalent, which can happen for any
node in the tree. Segments 2, 4, and 9 provide
support to the central statement, which is the most
simple configuration.
(1) [We should tear the building down.]1 [It is full
of asbestos.]2
Support can be serial (transitive), when a support-
ing statement in turn receives support from an-
other one. E.g., example (1) could be continued
with . . . [The report of the commission made that
very clear.]3.
If an argument involves multiple premises that
support the conclusion only if they are taken to-
gether, we have a linked structure in Freeman?s ter-
minology. On its own none of the linked premises
would be able to support the conclusion. In the
basic dialectical situation, a linked structure is in-
duced by the challenger?s question as to why a
premise is relevant to the claim. The proponent
then answers by presenting another premise expli-
cating the connection. Building linked structure is
thus to be conceived as completing an argument.
As an example, consider the following continu-
ation of example (1) . . . [All buildings with haz-
ardous materials should be demolished.]3 . Linked
support is shown in the diagram by connecting the
premises before they link to the conclusion.
Two more configurations, which turn up in Fig-
ure 1, are the attacking relations (all with a cir-
cled arrowhead): undercut and rebuttal. The for-
2When an artificial node is introduced in such places, a
standard tree representation results.
mer (segment 5) denies the relevance of a stated
relation, here: the support that 4 lends to 1=8. The
opponent does not dispute the truth of 4 itself but
challenges the idea that it can in fact lend support
to 1=8. We draw it as an attack arrow pointing
at the relation in question. In contrast, a rebut-
tal directly challenges the truth of a statement. In
the example, the annotator first decided that seg-
ments 6 and 7 play a joint role for the argumen-
tation (this is the step of merging two segments)
and then marked them as the proponent?s rebuttal
of the challenger?s statement 5.
3 Annotation Experiment
3.1 Guidelines
We developed annotation guidelines based on the
theory presented in Section 2. The guidelines
(6 pages) contain text examples and the cor-
responding graphs for all basic structures, and
they present different combinations of attack and
counter-attack. The annotation process is divided
into three steps: First, one segment is identified as
the central claim of the text. The annotator then
chooses the dialectical role (proponent or oppo-
nent) for all remaining segments. Finally, the argu-
mentative function of each segment (is it support-
ing or attacking) and the corresponding subtypes
have to be determined, as well as the targeted seg-
ment.
3.2 Data
Applying the scheme demands a detailed, deep un-
derstanding of the text, which is why we choose
to first evaluate this task on short and controlled
instances of argumentation. For this purpose we
built a set of 23 constructed German texts, where
each text consists of only five discourse segments.
While argumentative moves in authentic texts are
often surrounded by material that is not directly
relevant to the argumentation, such as factual
background information, elaborations or rhetori-
cal decoration, in the constructed texts all seg-
ments are clearly argumentative, i.e. they either
presents the central claim, a reason, an objection
or a counter-attack. Merging segments and identi-
fying restatements is thus not necessary. The texts
cover several combinations of the basic constructs
in different linearisations, typically one central
claim, two (simple, combined or exemplifying)
premises, one objection (rebutting a premise, re-
butting the conclusion or undercutting the link be-
197
tween them) and a possible reaction (rebutting or
undercutting counter-attacks, or a new reason that
renders the objection uncountered). A (translated)
example of a micro text is given in (2). In the
questionaire the order of the texts has been ran-
domized.
(2) [Energy-saving light bulbs contain a con-
siderable amount of toxic substances.]1 [A
customary lamp can for instance contain
up to five milligrams of quicksilver.]2 [For
this reason, they should be taken off the
market,]3 [unless they are virtually unbreak-
able.]4 [This, however, is simply not case.]5
3.3 Procedure
The annotation experiment was carried out in the
context of an undergraduate university course with
26 students, participation was obligatory. The an-
notators only received minimal training: A short
introduction (5 min.) was given to set the topic.
After studying the guidelines (?30 min.) and a
very brief question-answering, the subjects anno-
tated the 23 texts (?45 min.), writing their analysis
as an argumentative graph in designated areas of
the questionaire.
4 Evaluation
4.1 Preparations
Since the annotators were asked to assign one and
only one function to each segment, every node in
the argumentative graph has exactly one out-going
arc. The graph can thus be reinterpreted as a list
of segment labels.
Every segment is labeled on different levels:
The ?role?-level specifies the dialectical role (pro-
ponent or opponent). The ?typegen?-level specifies
the general type, i.e. whether the segment presents
the central claim (thesis) of the text, supports or
attacks another segment. The ?type?-level addi-
tionally specifies the kind of support (normal or
example) and the kind of attack (rebutter or un-
dercutter). Whether a segment?s function holds
only in combination with that of another segment
(combined) or not (simple) is represented on the
?combined?-level.3 The target is finally specified
by the segment identifier (1 . . . 5) or relation iden-
tifier (a . . . d) on the ?target?-level.
The labels of each separate level can be merged
to form a complex tagset. We interpret the result
3This is roughly equivalent to Freeman?s ?linked
premises?.
as a hierarchical tagset as it is presented in Fig-
ure 2.4 The label ?PSNC(3)? for example stands
for a proponent?s segment, giving normal support
to segment 3 in combination with another seg-
ment, while ?OAUS(b)? represents an opponent?s
segment, undercutting a relation b, not combined.
Due to space and readability constraints, we fo-
cus the detailed discussion of the experiment?s re-
sult on the ?role+type?-level. Still, general results
will be reported for all levels.
Another question that arises before evaluation,
especially in our setting, is how to deal with miss-
ing annotations, since measuring inter-annotator
agreement with a ?-like coefficient requires a deci-
sion of every annotator (or at least the same num-
ber of annotators) on each item. One way to cope
with this is to exclude annotators with missing an-
notations, another to exclude items that have not
been annotated by every subject. In our exper-
iment only 11 of the 26 subjects annotated ev-
ery segment. Another 10 annotated at least 90%
of the segments, five annotated less. Excluding
some annotators would be possible in our setting,
but keeping only 11 of 26 is unacceptable. Ex-
cluding items is also inconvenient given the small
dataset. We thus chose to mark segments with
missing annotations as such in the data, augment-
ing the tagset with the label ??? for missing anno-
tations. We are aware of the undesired possibility
that two annotators ?agree? on not assigning a cat-
egory to a segment. Still, we can decide to only
exclude those annotators who omitted many deci-
sions, and to measure agreement for the remaining
ones, thereby reducing the risk of false agreement.
4.2 IAA over all annotators
The agreement in terms of Fleiss?s ? (Fleiss,
1971)5 of all annotators on the different levels is
shown in Table 1. For the complex levels we ad-
ditionally report Krippendorff?s ? (Krippendorff,
1980) as a weighted measure of agreement. We
use the distance between two tags in the tag hier-
archy to weigh the confusion (similar to Geertzen
and Bunt (2006)), in order to capture the intuition
that confusing, e.g., PSNC with PSNS is less se-
vere than confusing it with OAUS.
According to the scale of Krippendorff (1980),
4Notice that this hierarchy is implicit in the annotation
process, yet the annotators were neither confronted with a
decision-tree version nor the labels of this tag hierarchy.
5A generalisation of Scott?s pi (Scott, 1955) for more than
two annotators, as Artstein and Poesio (2008) pointed out.
198
Figure 2: The hierarchy of segment labels.
level #cats ? AO AE ? DO DE
role 2 0.521 0.78 0.55
typegen 3 0.579 0.72 0.33
type 5 0.469 0.61 0.26
comb 2 0.458 0.73 0.50
target (9) 0.490 0.58 0.17
role+typegen 5 0.541 0.66 0.25 0.534 0.28 0.60
role+type 9 0.450 0.56 0.20 0.500 0.33 0.67
role+type+comb 15 0.392 0.49 0.16 0.469 0.38 0.71
role+type+comb+target (71) 0.384 0.44 0.08 0.425 0.45 0.79
Table 1: Agreement for all 26 annotators on 115 items for the different levels. The number of categories
on each level (without ???) is shown in the second column (possible target categories depend on text
length). We report Fleiss?s ? with the associated observed (AO) and expected agreement (AE). Weighted
scores were calculated using Krippendorff?s ?, with observed (DO) and expected disagreement (DE).
the annotators in our experiment did neither
achieve reliable (? ? 0.8) nor marginally reli-
able (0.67 ? ? < 0.8) agreement. On the scale
of Landis and Koch (1977), most results can be
interpreted to show moderate correlation (0.4 <
? ? 0.6), only the two most complex levels fall
out. Considering weighted scores for those com-
plex levels, all fall into the window of moderate
correlation.
While typical results in discourse structure tag-
ging usually reach or exceed the 0.7 threshold6 ,
we expected lower results for three reasons: first
the minimal training of the naive annotators only
based on the guidelines, second the varying com-
mitment to the task of the annotators in the con-
strained setting and finally the nature of the task,
which requires a precise specification of the anno-
tators interpretation of the texts.
When it comes to investigation of the reasons
of disagreement, the informativeness of a single
inter-annotator agreement value is limited. We
want to identify sources of disagreement in both
the set of annotators as well as the categories. To
6Agreement of professional annotators on 16 rhetorical
relations was ?=0.64 in the beginning and 0.82 after extensive
training (Carlson et al, 2003). Agreement on ?argumentative
zones? is reported ?=0.71 for trained annotators with detailed
guidelines, another study for untrained annotators with only
minimalistic guidelines reported values varying between 0.35
and 0.72 (depending on the text), see Teufel (2010).
cat. ?? n AO AE
PT +0.265 572 0.91 0.69
PSE +0.128 112 0.97 0.93
PSN +0.082 1075 0.79 0.54
OAR ?0.027 430 0.86 0.75
PAR ?0.148 173 0.92 0.89
OSN ?0.198 153 0.93 0.90
OAU ?0.229 172 0.92 0.89
PAU ?0.240 138 0.93 0.91
OSE ?0.451 2 0.99 0.99
Table 3: Krippendorff?s category definition diag-
nostic for the level ?role+type?, base ?=0.45.
this end, contingency tables (confusion matrices)
are studied, which show the number of category
agreements and confusions for a pair of annota-
tors. However, the high number of annotators in
our study makes this strategy infeasible, as there
are 325 different pairs of annotators. One solution
to still get an overview of typical category con-
fusions, is to build an aggregated confusion ma-
trix, which sums up the values of category pairs
across all 325 normal confusion matrices. As pro-
posed in Cinkova? et al (2012), we derive a confu-
sion probability matrix from this aggregated ma-
trix, which is shown in Table 2. It specifies the
conditional probability that one annotator will an-
notate an item with categorycolumn , given that an-
other has chosen categoryrow , so the rows sum up
to 1. The diagonal cells display the probability of
agreement for each category.
199
PT PSN PSE PAR PAU OSN OSE OAR OAU ?
PT 0.625 0.243 0.005 0.003 0.002 0.006 0.000 0.030 0.007 0.078
PSN 0.123 0.539 0.052 0.034 0.046 0.055 0.001 0.052 0.021 0.078
PSE 0.024 0.462 0.422 0.007 0.008 0.000 0.000 0.015 0.001 0.061
PAR 0.007 0.164 0.004 0.207 0.245 0.074 0.000 0.156 0.072 0.071
PAU 0.007 0.264 0.005 0.290 0.141 0.049 0.000 0.117 0.075 0.052
OSN 0.016 0.292 0.000 0.081 0.046 0.170 0.004 0.251 0.075 0.065
OSE 0.000 0.260 0.000 0.000 0.000 0.260 0.000 0.240 0.140 0.100
OAR 0.033 0.114 0.004 0.070 0.044 0.102 0.001 0.339 0.218 0.076
OAU 0.017 0.101 0.000 0.069 0.061 0.066 0.002 0.469 0.153 0.063
? 0.179 0.351 0.031 0.066 0.041 0.055 0.001 0.157 0.061 0.057
Table 2: Confusion probability matrix over all 26 annotators for the level ?role+type?.
category pair ?? AO AE
OAR+OAU +0.048 0.61 0.22
PAR+PAU +0.026 0.59 0.21
OAR+OSN +0.018 0.58 0.22
PSN+PSE +0.012 0.59 0.23
OAR+PAR +0.007 0.58 0.22
PSN+OSN +0.007 0.59 0.24
PAR+OSN +0.005 0.57 0.21
Table 4: Krippendorff?s category distinction diag-
nostic for the level ?role+type?, base ?=0.45.
Krippendorff (1980) proposed another way to
investigate category confusions by systematically
comparing the agreement on the original category
set with the agreement on a reduced category set.
There are two different methods to collapse cat-
egories: The first is the category definition test,
where all but the one category of interest are col-
lapsed together, yielding a binary category distinc-
tion. When measuring the agreement with this bi-
nary distinction only confusions between the cat-
egory of interest and the rest count, but no confu-
sions between the collapsed categories. If agree-
ment increases for the reduced set compared to the
original set, that category of interest is better dis-
tinguished than the rest of the categories. As Ta-
ble 3 shows, the highest distinguishability is found
for PT, PSN and PSE. Rebutters are better distin-
guished for the opponent role than for the propo-
nent role. Undercutters seem equally problematic
for both roles. The extreme value for OSE is not
surprising, given that this category was not sup-
posed to be found in the dataset and was only used
twice. It shows, though, that the results of this test
have to be interpreted with caution for rare cate-
gories, since in these cases the collapsed rest al-
ways leads to a very high chance agreement.
The other of Krippendorff?s diagnostics is the
category distinction test, where two categories are
collapsed in order to measure the impact of con-
fusions between them on the overall agreement
value. The higher the difference, the greater the
confusion between the two collapsed categories.
Table 4 shows the result for some category pairs.
The highest gain is found between rebutting and
undercutting attacks on the opponents side: Given
the base ?=0.45, the +0.048 increase means a po-
tential improvement of 10% if these confusions
could be reduced. However, distingishing rebut-
ters and undercutters often depends on interpreta-
tion and we consider it unlikely to reach perfect
agreement on that decision.
4.3 Comparison with gold data
We now compare the result of the annotation ex-
periment with the gold annotation. For each an-
notator and for each level of annotation, we cal-
culated the F1 score, macro-averaged over the cat-
egories of that level. Figure 3 shows the distri-
bution of those values as boxplots. We observe
varying degrees of difficulty on the basic levels:
While the scores on the ?role? and ?typegen? are
relatively dense between 0.8 and 0.9, the distribu-
tion is much wider and also generally lower for
?type?, ?comb? and ?target?. Especially remarkable
is the drop of the median when comparing ?type-
gen? with ?type?: For the simpler level, all values
of the better half of annotators lie above 0.85, but
for the more complex level, which also requires
the distinction between rebutters and undercutters,
the median drops to 0.67. The figure also shows
the pure F1 score for identifying the central claim
(PT). While the larger part of the annotators per-
forms well in this task, there are still some be-
low 0.7. This is remarkable, since identifying one
segment as the central claim of a five-segment text
does not appear to be a challenging task.
4.4 Ranking and clustering the annotators
Until now we have mainly investigated the tagset
as a factor in measuring agreement. The
widespread distribution of annotator scores in the
comparison with gold data however showed that
200
ro
l
e
t
y
p
e
g
e
n
t
y
p
e
c
o
m
b
t
a
r
g
e
t
r
o
l
e
+
t
y
p
e
g
e
n
r
o
l
e
+
t
y
p
e
r
o
+
t
y
+
c
o
r
o
+
t
y
+
c
o
+
t
a
c
e
n
t
r
a
l
-
c
l
a
i
m
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 3: Comparison with gold annotation: For
each level we show a boxplot of the F1 scores
of all annotators (each score macro-averaged over
categories of that level). Also, we present the F1
score for the recognition of the central claim.
their performance differs greatly. As described in
Section 3.3, participation in the study was obliga-
tory for our subjects (students in class). We thus
want to make sure that the differences in perfor-
mance are a result of the annotator?s varying com-
mitment to the task, rather than a result of pos-
sible ambiguities or flaws of the guidelines. The
inter-annotator agreement values presented in Ta-
ble 1 are not so helpful for answering this ques-
tion, as they only provide us with an average mea-
sure, but not with an upper and lower bound of
what is achievable with our annotators. Conse-
quently, the goal of this section is to give structure
to the set of annotators, to impose a (partial) or-
der on it or even divide it into different groups and
investigate their characteristic confusions.
Central claim: During the conversion of the
written graphs into segment label squences, it be-
came obvious that certain annotators nearly al-
ways chose the first segment of the text as the
central claim, even in cases where it was fol-
lowed by a consecutive clause with a discourse
marker. Therefore, our first heuristic was to im-
pose an order on the set of annotators according
to their F1 score in identifying the central claim.
This not only identifies those outliers but can ad-
ditionally serve as a rough indicator of text un-
derstanding. Although this ordering requires gold
data, producing gold data for the central claim of a
text is relatively simple and using them only gives
minimal bias in the evaluation (in contrast to e.g.
5 10 15 20 25
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
role+type+comb+target
role+type+comb
target
typegen
role
role+type
comb
type
role+typegen
Figure 4: Agreement in ? on the different levels
for the n-best annotators ordered by their F1 score
in identifying the central claim.
?role+type? F1 score as a sorting criterion). With
this ordering we can then calculate agreement on
different subsets of the annotators, e.g. only for
the two best annotators, for the ten best or for all.
Figure 4 shows ? on the different levels for all n-
best groups of annotators: From the two best to the
six best annotators the results are quite stable. The
six best annotators achieve an encouraging ?=0.74
on the ?role+type? level and likewise satisfactory
?=0.69 for the full task, i.e. on the maximally
complex ?role+type+comb+target? level. For in-
creasingly larger n-best groups, the agreement de-
creases steadily with only minor fluctuations. Al-
though the central claim F1 score proves to be a
useful sorting criterion here, it might not work as
well for authentic texts, due to the possibility of
restated, or even implicit central claims.
Category distributions: Investigating the an-
notator bias is also a promising way to impose
structure onto the group of annotators. A look
on the individual distribution of categories per an-
notator quickly reveals that there are some devia-
tions. Table 5 shows the individual distributions
for the ?role+type?-level, as well as the average
annotator distribution and that found in the gold
data. We focus on three peculiarities here. First,
both annotators A18 and A21 refrain from classi-
fying segments as attacking. Although they make
the distinction between the roles, they give only
supporting segments. Checking the annotations
shows that they must have mixed the concepts of
dialectical role and argumentative function. An-
other example is the group of A04, A20 and A23,
who refrain from using proponent attacks. Al-
201
anno PT PSN PSE PAR PAU OSN OSE OAR OAU ? ?gold ??
A01 23 40 5 13 0 6 0 24 0 4 17 15.6
A02 22 33 7 8 11 3 0 23 1 7 17 16.9
A03 23 40 6 4 12 5 0 16 9 0 7 11.8
A04 21 52 6 1 0 0 0 14 11 10 25 20.5
A05 23 42 5 15 2 5 0 20 3 0 10 14.2
A06 24 39 6 6 9 7 0 15 9 0 7 10.9
A07 22 41 1 12 8 5 0 13 8 5 13 9.4
A08 23 35 6 6 14 6 1 17 7 0 9 13.3
A09 23 43 2 6 7 7 0 15 12 0 9 10.8
A10 23 51 3 3 4 8 0 8 15 0 21 21.2
A11 21 41 3 2 1 1 0 22 9 15 21 16.6
A12 23 42 6 15 5 3 0 13 4 4 13 11.7
A13 23 40 4 16 0 7 0 17 8 0 14 13.3
A14 19 33 6 10 4 4 0 11 8 20 26 20.2
A15 19 37 2 6 7 3 0 18 3 20 20 16.9
A16 20 31 4 7 10 7 0 14 5 17 22 16.9
A17 22 53 2 4 3 0 0 20 6 5 17 15.1
A18 23 51 5 0 0 34 1 0 1 0 39 40.4
A19 24 41 7 13 2 5 0 20 3 0 10 14.5
A20 21 41 4 0 1 2 0 31 5 10 22 18.2
A21 16 40 0 1 0 20 0 0 1 37 52 44.8
A22 22 34 7 5 10 6 0 17 9 5 12 10.3
A23 23 52 0 1 0 0 0 32 6 1 24 27.1
A24 23 41 6 6 9 5 0 22 3 0 4 11.8
A25 23 38 4 5 15 0 0 7 23 0 24 27.1
A26 23 44 5 8 4 4 0 21 3 3 9 10.2
? 22.0 41.3 4.3 6.7 5.3 5.9 0.1 16.5 6.6 6.3
gold 23 42 6 6 8 5 0 19 6 0
Table 5: Distribution of categories for each annotator in absolute numbers for the ?role+type? level.
The last two rows display gold and average annotator distribution for comparison. The two right-
most columns specify for each annotator the total difference to gold or average distribution ?gold/? =
1
2
?
c
?gold/?c .
though they make the distinction between the ar-
gumentative functions of supporting and attack-
ing, they do not systematically attribute counter-
attacks to the proponent. Finally, as pointed out
before, there are several annotators with a different
amount of missing annotations. Note, that missing
annotations must not necessarily signal an unmo-
tivated annotator (who skips an item if deciding on
it is too tedious). It could very well also be a dili-
gent but slow annotator. Still, missing annotations
lead to lower agreement in most cases, so filtering
out the severe cases might be a good idea. Most
of the annotators showing deviations in category
distribution could be identified, if annotators are
sorted by deviation from average distribution ??,
which is shown in the last column of Table 5. Fil-
tering out the 7 worst annotators in terms of ??,
the resulting ? increases from 0.45 to 0.54 on the
?role+type?-level, which is nearly equal to the 0.53
achieved when using the same size of annotator set
in the central claim ordering. Although this order-
ing suffices to detect outliers in the set of annota-
tors without relying on gold data, it still has two
drawbacks: It only maximizes to the average and
will thus not garantuee best agreement scores for
the smaller n-best sets. Furthermore a more gen-
eral critique on total orders of annotators: There
are various ways in which a group agrees or dis-
A
2
1
A
2
0
A
0
4
A
1
8
A
2
5
A
1
0
A
0
9
A
1
1
A
1
5
A
1
6
A
0
7
A
2
3
A
1
4
A
2
2
A
1
7
A
0
1
A
1
3
A
2
6
A
0
6
A
0
2
A
0
8
A
2
4
A
0
3
A
1
2
A
0
5
A
1
9
0.9
0.8
0.7
0.6
0.5
0.4
Figure 5: Clustering of the annotators (on the x-
axis) for the ?role+type? level. The y-axis speci-
fies the distance between the clusters, i.e. the ?
reached by the annotators of both clusters.
agrees simultaneously that might not be linearized
this way. Luckily, a better solution is at hand.
Agglomerative hierarchical clustering: We
apply hierarchical clustering in order to investi-
gate the structure of agreement in the set of an-
notators. The clusters are initialized as singletons
for each annotator. Then agreement is calculated
for all possible pairs of those clusters. The pair of
clusters with highest agreement is merged. This
procedure is iterated until there is only one cluster
left. In contrast to normal clustering, the linkage
202
criterion does not determine the distance between
complex clusters indirectly as function of the dis-
tance between singleton clusters, but directly mea-
sures agreement for the unified set of annotators of
both clusters. Figure 5 shows the clustering on the
?role+type?-level. It not only gives an impression
of the possible range of agreement, but also allows
us to check for ambiguities in the guidelines: If
there were stable alternative readings in the guide-
lines, we would expect multiple larger clusters that
can only be merged at a lower level of ?. As the
Figure shows, the clustering grows steadily, maxi-
mally incorporating clusters of two annotators, so
we do not see the threat of ambiguity in the guide-
lines. Furthermore, the clustering conforms with
central claim ordering in picking out the same set
of six reliable and good annotators (with an aver-
age F1 of 0.76 for ?role+type? and of 0.67 for the
full task compared to gold) and it conforms with
both orderings in picking out similar sets of worst
annotators.
With this clustering we now have the possibility
to investigate the agreement for subgroups of an-
notators. Since the growth of the clusters is rather
linear, we choose to track the confusion over the
best path of growing clusters, i.e. starting from
the best scoring {A24,A03} cluster to the maximal
cluster. It would be interesting to see the change in
Krippendorff?s category distinction diagnostic for
selected confusion pairs. However, this value not
only depends on the amount of confusion but also
on the frequency of that categories7, which cannot
be assume to be identical for different sets of an-
notators. We thus investigate the confusion rate
confc1,c2 , i.e. the ratio of confusing assigments
pairs |c1 ? c2| in the total set of agreeing and con-
fusing assignments pairs for these two categories:
confc1,c2 =
|c1 ? c2|
|c1 ? c1|+ |c1 ? c2|+ |c2 ? c2|
Figure 6 shows the confusion rate for selected
category pairs over the path from the best scoring
to the maximal cluster. The confusion between re-
butters and undercutters is already at a high level
for the best six best annotators, but increases when
worse annotators enter the cluster. A constant
and relatively low confusion rate has PSN+PAU,
which means that distinguishing counter-attacks
from new premises is equally ?hard? for all annota-
tors. Distinguishing normal and example support,
720% confusion of frequent categories have a larger im-
pact on agreement than that of less frequent categories.
2 3 6 7 8 9 11 12 13 14 15 16 18 19 20 21 22 23 25 26
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
PAR+PAU
OAR+OAU
PT+PSN
PSN+PAU
PSN+PSE
OAU+OSN
Figure 6: Confusion rate for selected category
pairs in the growing clusters, with the numbers of
annotators in the cluster on the x axis.
as well as central claims and supporting segments
is not a problem for the six best annotators. It be-
comes slightly more confusing for more annota-
tors, yet ends at a relatively low level around 0.08
and 0.13 respectively. Confusing undercutters and
support on the opponents side is only a problem
of the low-agreeing annotators, the confusion rate
is nearly 0 for the first 21 annotators on the clus-
ter path. Finally note, that there is no confusion
typical for the high-agreeing annotators only.
5 Conclusions
We presented methods to systematically study the
agreement in a larger group of annotators. To
this end, we evaluated an annotation study, where
26 untrained annotators marked the argumentation
structure of small texts. While the overall agree-
ment showed only moderate correlation (as one
could expect from naive annotators in a text in-
terpretation task) we could identify a subgroup of
annotators reaching a reliable level of agreement
and good F1 scores in comparison with gold data
by different ranking and clustering approaches and
investigated which category confusions were char-
acteristic for the different subgroups.
Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. The first author was supported by a
grant from Cusanuswerk and the second author by
Deutsche Forschungsgemeinschaft (SFB 632).
203
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596, December.
Vikas Bhardwaj, Rebecca J. Passonneau, Ansaf Salleb-
Aouissi, and Nancy Ide. 2010. Anveshan: a frame-
work for analysis of multiple annotators? labeling
behavior. In Proceedings of the Fourth Linguistic
Annotation Workshop, LAW IV ?10, pages 47?55,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure The-
ory. In Jan van Kuppevelt and Ronnie Smith, edi-
tors, Current Directions in Discourse and Dialogue.
Kluwer, Dordrecht.
Silvie Cinkova?, Martin Holub, and Vincent Kr??z?. 2012.
Managing uncertainty in semantic tagging. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, EACL ?12, pages 840?850, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
James B. Freeman. 1991. Dialectics and the
Macrostructure of Argument. Foris, Berlin.
James B. Freeman. 2011. Argument Structure: Repre-
sentation and Theory. Argumentation Library (18).
Springer.
Jeroen Geertzen and Harry Bunt. 2006. Measuring
annotator agreement in a complex hierarchical di-
alogue act annotation scheme. In Proceedings of
the 7th SIGdial Workshop on Discourse and Dia-
logue, SigDIAL ?06, pages 126?133, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to its Methodology. Sage Publications,
Beverly Hills, CA.
J Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorial data.
Biometrics, 33(1):159?174, March.
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory: Towards a functional theory of
text organization. TEXT, 8:243?281.
Andreas Peldszus and Manfred Stede. to appear. From
argument diagrams to automatic argument mining:
A survey. International Journal of Cognitive Infor-
matics and Natural Intelligence, 7(1).
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opin-
ion Quarterly, 19(3):321?325.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 254?263, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Simone Teufel. 2010. The Structure of Scientific Arti-
cles: Applications to Citation Indexing and Summa-
rization. CSLI Studies in Computational Linguis-
tics. CSLI Publications.
Stephen Toulmin. 1958. The Uses of Argument. Cam-
bridge University Press, Cambridge.
204
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards a Tool for Interactive Concept Building for Large Scale Analysis
in the Humanities
Andre Blessing1 Jonathan Sonntag2 Fritz Kliche3
Ulrich Heid3 Jonas Kuhn1 Manfred Stede2
1Institute for Natural Language Processing
Universitaet Stuttgart, Germany
2Institute for Applied Computational Linguistics
University of Potsdam, Germany
3Institute for Information Science and Natural Language Processing
University of Hildesheim, Germany
Abstract
We develop a pipeline consisting of var-
ious text processing tools which is de-
signed to assist political scientists in find-
ing specific, complex concepts within
large amounts of text. Our main focus is
the interaction between the political scien-
tists and the natural language processing
groups to ensure a beneficial assistance for
the political scientists and new application
challenges for NLP. It is of particular im-
portance to find a ?common language? be-
tween the different disciplines. Therefore,
we use an interactive web-interface which
is easily usable by non-experts. It inter-
faces an active learning algorithm which
is complemented by the NLP pipeline to
provide a rich feature selection. Political
scientists are thus enabled to use their own
intuitions to find custom concepts.
1 Introduction
In this paper, we give examples of how NLP meth-
ods and tools can be used to provide support for
complex tasks in political sciences. Many con-
cepts of political science are complex and faceted;
they tend to come in different linguistic realiza-
tions, often in complex ones; many concepts are
not directly identifiable by means of (a small set
of) individual lexical items, but require some in-
terpretation.
Many researchers in political sciences either
work qualitatively on small amounts of data which
they interpret instance-wise, or, if they are in-
terested in quantitative trends, they use compara-
tively simple tools, such as keyword-based search
in corpora or text classification on the basis of
terms only; this latter approach may lead to im-
precise results due to a rather unspecific search as
well as semantically invalid or ambigious search
words. On the other hand, large amounts of e.g.
news texts are available, also over longer periods
of time, such that e.g. tendencies over time can
be derived. The corpora we are currently working
on contain ca. 700,000 articles from British, Irish,
German and Austrian newspapers, as well as (yet
unexplored) material in French.
Figure 1 depicts a simple example of a quantita-
tive analysis.1 The example shows how often two
terms, Friedensmission(?peace operation?), and
Auslandseinsatz(?foreign intervention?) are used
in the last two decades in newspaper texts about
interventions and wars. The long-term goal of the
project is to provide similar analysis for complex
concepts. An example of a complex concept is
the evocation of collective identities in political
contexts, as indirect in the news. Examples for
such collective identities are: the Europeans, the
French, the Catholics.
The objective of the work we are going to dis-
cuss in this paper is to provide NLP methods and
tools for assisting political scientists in the ex-
ploration of large data sets, with a view to both,
a detailed qualitative analysis of text instances,
and a quantitative overview of trends over time,
at the level of corpora. The examples discussed
here have to do with (possibly multiple) collective
identities. Typical context of such identities tend
to report communication, as direct or as indirect
speech. Examples of such contexts are given in 1.
(1) Die
The
Europa?er
Europeans
wu?rden
would
die
the
Lu?cke
gap
fu?llen,
fill,
1The figure shows a screenshot of our web-based
prototype.
55
Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency
of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was
predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention)
being now frequently used.
sagte
said
Ru?he.
Ru?he.
,,The Europeans would fill the gap, Ru?he said.?
The tool support is meant to be semi-automatic,
as the automatic tools propose candidates that
need to be validated or refused by the political sci-
entists.
We combine a chain of corpus processing tools
with classifier-based tools, e.g. for topic clas-
sifiers, commentary/report classifiers, etc., make
the tools interoperable to ensure flexible data ex-
change and multiple usage scenarios, and we em-
bed the tool collection under a web (service) -
based user interface.
The remainder of this paper is structured as fol-
lows. In section 2, we present an outline of the ar-
chitecture of our tool collection, and we motivate
the architecture. Section 3 presents examples of
implemented modules, both from corpus process-
ing and search and retrieval of instances of com-
plex concepts. We also show how our tools are re-
lated to the infrastructural standards in use in the
CLARIN community. In section 4, we exemplify
the intended use of the methods with case studies
about steps necessary for identifying evocation:
being able to separate reports from comments, and
strategies for identifying indirect speech. Section
6 is devoted to a conclusion and to the discussion
of future work.
2 Project Goals
A collaboration between political scientists and
computational linguists necessarily involves find-
ing a common language in order to agree on
the precise objectives of a project. For exam-
ple, social scientists use the term codebook for
manual annotations of text, similar to annotation
schemes or guidelines in NLP. Both disciplines
share methodologies of interactive text analysis
which combine term based search, manual an-
notation and learning-based annotation of large
amounts of data. In this section, we give a brief
56
summary of the goals from the perspective of each
of the two disciplines, and then describe the text
corpus that is used in the project. Section 3 will
describe our approach to devising a system archi-
tecture that serves to realize the goals.
2.1 Social Science Research Issue
Given the complexity of the underlying research
issues (cf. Section 1) and the methodological tra-
dition of manual text coding by very well-trained
annotators in the social science and particular in
political science, our project does not aim at any
fully-automatic solution for empirical issues in po-
litical science. Instead, the goal is to provide as
much assistance to the human text analyst as possi-
ble, by means of a workbench that integrates many
tasks that otherwise would have to be carried out
with different software tools (e.g., corpus prepro-
cessing, KWIC searches, statistics). In our project,
the human analyst is concerned specifically with
manifestations of collective identities in newspa-
per texts on issues of war and military interven-
tions: who are the actors in political crisis man-
agement or conflict? How is this perspective of
responsible actors characterized in different news-
papers (with different political orientation; in dif-
ferent countries)? The analyst wants to find doc-
uments that contain facets of such constellations,
which requires search techniques involving con-
cepts on different levels of abstraction, ranging
from specific words or named entities (which may
appear with different names in different texts) to
event types (which may be realized with different
verb-argument configurations). Thus the text cor-
pus should be enriched with information relevant
to such queries, and the workbench shall provide
a comfortable interface for building such queries.
Moreover, various types and (possibly concurrent)
layers of human annotations have to complement
the automatic analysis, and the manual annota-
tion would benefit from automatic control of code-
book2 compliance and the convergence of coding
decisions.
2.2 Natural Language Processing Research
Issue
Large collections of text provide an excellent op-
portunity for computational linguists to scale their
methods. In the scenario of a project like ours, this
becomes especially challenging, because standard
2or, in NLP terms: annotation scheme.
automatic analysis components have to be com-
bined with manual annotation or interactive inter-
vention of the human analyst.
In addition to this principled challenge, there
may be more mundane issues resulting from pro-
cessing corpora whose origin stretches over many
years. In our case, the data collection phase coin-
cided with a spelling reform in German-speaking
countries. Many aspects of spelling changed twice
(in 1996 and in 2006), and thus it is the responsi-
bility of the NLP branch of the project to provide
an abstraction over such changes and to enable to-
day?s users to run a homogeneous search over the
texts using only the current spelling. While this
might be less important for generic web search ap-
plications, it is of great importance for our project,
where the overall objective is a combination of
quantitative and qualitative text analysis.
In our processing chain, we first need to harmo-
nize the data formats so that the processing tools
operate on a common format. Rather than defin-
ing these from scratch, we aim at compatibility
with the standardization efforts of CLARIN3 and
DARIAH4, two large language technology infras-
tructure projects in Europe that in particular target
eHumanities applications. One of the objectives
is to provide advanced tools to discover, explore,
exploit, annotate, analyse or combine textual re-
sources. In the next section we give more details
about how we interact which the CLARIN-D in-
frastructure (Boehlke et al, 2013).
3 Architecture
The main goal is to provide a web-based user-
interface to the social scientist to avoid any soft-
ware installation. Figure 2 presents the workflow
of the different processing steps in this project.
The first part considers format issues that occur
if documents from different sources are used. The
main challenge is to recognize metadata correctly.
Date and source name are two types of metadata
which are required for analyses in the social sci-
ences. But also the separation of document con-
tent (text) and metadata is important to ensure that
only real content is processed with the NLP meth-
ods. The results are stored in a repository which
uses a relational database as a back-end. All fur-
ther modules are used to add more annotations to
the textual data. First a complex linguistic pro-
3http://www.clarin.eu/
4http://www.dariah.eu/
57
cessing chain is used to provide state-of-the-art
corpus linguistic annotations (see Section 3.2 for
details). Then, to ensure that statistics over oc-
currence counts of words, word combinations and
constructions are valid and not blurred by the mul-
tiple presence of texts or text passages in the cor-
pus, we filter duplicates. Duplicates can occur
if our document set contains the same document
twice or if two documents are very similar, e.g.
they differ in only one sentence.
Raw documents
Repository:MetadataStructural dataTextual data Topic filter
Duplicate filter
Linguistic analysisSentence splitter Tokenizer
Web-basedUserinterface
Tagger ParserCoref NER
ImportExploration Workbench
Concept detection
Complex Concept Builder
Figure 2: Overview of the complete processing
chain.
We split the workflow for the user into two
parts: The first part is only used if the user im-
ports new data into the repository. For that he
can use the exploration workbench (Section 3.1).
Secondly, all steps for analyzing the data are done
with the Complex Concept Builder (Section 3.2).
3.1 Exploration Workbench
Formal corpus inhomogeneity (e.g. various data
formats and inconsistent data structures) are a ma-
jor issue for researchers working on text corpora.
The web-based ?Exploration Workbench? allows
for the creation of a consistent corpus from vari-
ous types of data and prepares data for further pro-
cessing with computational linguistic tools. The
workbench can interact with to existing computa-
tional linguistic infrastructure (e.g. CLARIN) and
provides input for the repository also used by the
Complex Concept Builder.
The workbench converts several input formats
(TXT, RTF, HTML) to a consistent XML repre-
sentation. The conversion tools account for differ-
ent file encodings and convert input files to Uni-
code (UTF-8). We currently work on newspa-
per articles wrapped with metadata. Text mining
components read out those metadata and identify
text content in the documents. Metadata appear
at varying positions and in diverse notations, e.g.
for dates, indications of authors or newspaper sec-
tions. The components account for these varia-
tions and convert them to a consistent machine
readable format. The extracted metadata are ap-
pended to the XML representation. The result-
ing XML is the starting point for further compu-
tational linguistic processing of the source docu-
ments.
The workbench contains a tool to identify text
duplicates and semi-duplicates via similarity mea-
sures of pairs of articles (Kantner et al, 2011).
The method is based on a comparison of 5-grams,
weighted by significance (tf-idf measure (Salton
and Buckley, 1988)). For a pair of documents it
yields a value on a ?similarity scale? ranging from
0 to 1. Values at medium range (0.4 to 0.8) are
considered semi-duplicates.
Data cleaning is important for the data-driven
studies. Not only duplicate articles have a nega-
tive impact, also articles which are not of interest
for the given topic have to be filtered out. There
are different approaches to classify articles into a
range of predefined topics. In the last years LDA
(Blei et al, 2003; Niekler and Ja?hnichen, 2012)
is one of the most successful methods to find top-
ics in articles. But for social scientists the cate-
gories typically used in LDA are not sufficient. We
follow the idea of Dualist (Settles, 2011; Settles
and Zhu, 2012) which is an interactive method for
classification. The architecture of Dualist is based
on MALLET (McCallum, 2002) which is easily
integrable into our architecture. Our goal is to
design the correct feature to find relevant articles
for a given topic. Word features are not sufficient
since we have to model more complex features (cf.
Section 2.1).
The workbench is not exclusively geared to the
data of the current project. We chose a modular
set-up of the tools of the workbench and provide
user-modifiable templates for the extraction of var-
ious kinds of metadata, in order to keep the work-
bench adaptable to new data and to develop tools
suitable for data beyond the scope of the current
corpus.
58
3.2 Complex Concept Builder
A central problem for political scientists who in-
tend to work on large corpora is the linguistic va-
riety in the expression of technical terms and com-
plex concepts. An editorial or a politician cited
in a news item can mobilize a collective identity
which can be construed from e.g. regional or so-
cial affiliation, nationality or religion. A reason-
able goal in the context of the search for collec-
tive identity evocation contexts is therefore to find
all texts which (possibly) contain collective iden-
tities. Moreover, while we are training our inter-
active tools on a corpus on wars and military in-
terventions the same collective identities might be
expressed in different ways in a corpus i.e. on the
Eurocrisis.
From a computational point of view, many dif-
ferent tools need to be joined to detect interest-
ing texts. An example application could be a case
where a political scientist intends to extract news-
paper articles that cite a politician who tries to
rally support for his political party. In order to
detect such text, we need a system to identify di-
rect and indirect speech and a sentiment system to
determine the orientation of the statement. These
systems in turn need various kinds of preprocess-
ing starting from tokenization over syntactic pars-
ing up to coreference resolution. The Complex
Concept Builder is the collection of all these sys-
tems with the goal to assist the political scientists.
So far, the Complex Concept Builder imple-
ments tokenization (Schmid, 2009), lemmatisation
(Schmid, 1995), part-of-speech tagging (Schmid
and Laws, 2008), named entity detection (Faruqui
and Pado?, 2010), syntactical parsing (Bohnet,
2010), coreference analysis for German (Lappin
and Leass, 1994; Stuckardt, 2001), relation extrac-
tion (Blessing et al, 2012) and sentiment analysis
for English (Taboada et al, 2011).
It is important for a researcher of the humanities
to be able to adapt existing classification systems
according to his own needs. A common procedure
in both, NLP and political sciences, is to annotate
data. Therefore, one major goal of the project and
the Complex Concept Builder is to provide ma-
chine learning systems with a wide range of pos-
sible features ? including high level information
like sentiment, text type, relations to other texts,
etc. ? that can be used by non-experts for semi-
automatic annotation and text selection. Active
learning is used to provide immediate results that
can then be improved continuously. This aspect
of the Complex Concept Builder is especially im-
portant because new or adapted concepts that may
be looked for can be found without further help of
natural language processing experts.
3.3 Implementation
We decided to use a web-based platform for our
system since the social scientist needs no software
installation and we are independent of the used
operating system. Only a state-of-the-art web-
browser is needed. On the server side, we use a
tomcat installation that interacts with our UIMA
pipeline (Ferrucci and Lally, 2004). A HTML-
rendering component designed in the project (and
parametrizable) allows for a flexible presentation
of the data. A major issue of our work is interac-
tion. To solve this, we use JQuery and AJAX to
dynamically interact between client- and server-
side.
4 Case Study
In this section we explore the interaction between
various sub-systems and how they collaborate to
find complex political concepts. The following
Section 4.1 describes the detection of direct and
indirect speech and its evaluation follows in Sec-
tion 4.2. Section 4.3 is a general exploration of a
few selected sub-systems which require, or benefit
from direct and indirect speech. Finally, Section
4.4 discusses a specific usage scenario for indirect
speech.
4.1 Identifying Indirect Speech
The Complex Concept Builder provides analy-
ses on different linguistic levels (currently mor-
phosyntax, dependency syntax, named entities) of
annotation. We exploit this knowledge to identify
indirect speech along with a mentioned speaker.
Our indirect speech recognizer is based on three
conditions: i) Consider all sentences that contain
at least one word which is tagged as subjunctive
(i.e. ?*.SUBJ?) by the RFTagger. ii) This verb
has to be a direct successor of another verb in the
dependency tree. iii) This verb needs to have a
subject.
Figure 3 depicts the dependency parse tree of
sentence 2.
(2) Der Einsatz werde wegen der Risiken fu?r die
unbewaffneten Beobachter ausgesetzt, teilte
59
Einsatzmission
theDer
,,
ausgesetztstopped
werde
wegenbecause of
Risikorisks
teilteinformed
will be Missionschefhead of mission
MoodMood
RobertRobert
mitam
SaturdaySamstag
on
..
SBOC
VFIN.Aux.3.Sg.Pres.Subj
VFIN.Full.3.Sg.Past.IndRFTags
Figure 3: Dependency parse of a sentence that
contains indirect speech (see Sentence 2).
Missionschef Robert Mood am Samstag mit.
The mission will be stopped because of the risks to the
unarmed observers, informed Head of Mission Robert
Mood on Saturday.
The speaker of the indirect speech in Sentence
2 is correctly identified as Missionschef (Head of
Mission) and the corresponding verb is teilte mit
(from mitteilen) (to inform).
The parsing-based analysis helps to identify the
speaker of the citation which is a necessary in-
formation for the later interpretation of the cita-
tion. As a further advantage, such an approach
helps to minimize the need of lexical knowledge
for the identification of indirect speech. Our er-
ror analysis below will show that in some cases
a lexicon can help to avoid false positives. A lexi-
con of verbs of communication can easily be boot-
strapped by using our approach to identify candi-
dates for the list of verbs which then restrict the
classifier in order to achieve a higher precision.
4.2 Indirect Speech Evaluation
For a first impression, we present a list of sen-
tences which were automatically annotated as pos-
itive instances by our indirect speech detector.
The sentences were rated by political scientists.
Additionally, for each sentence we extracted the
speaker and the used verb of speech. We man-
ually evaluated 200 extracted triples (sentence,
speaker, verb of speech): The precision of our
system is: 92.5%
Examples 2, 3 and 4 present good candidates
which are helpful for further investigations on col-
lective identities. In example 3 Cardinal Lehmann
is a representative speaker of the Catholic commu-
nity which is a collective identity. Our extracted
sentences accelerate the search for such candidates
which amounts to looking manually for needles in
a haystack.
example speaker verb of speech
(2) Robert Mood teilte (told)
(3) Kardinal Karl Lehmann sagte (said)
(4) Sergej Ordzhonikidse sagte (said)
(5) Bild (picture) tru?ben (tarnish)
(6) sein (be) sein (be)
Examples 5 and 6 show problems of our first
approach. In this case, the speaker is not a person
or an organisation, and the verb is not a verb of
speech.
(3) Ein Angriffskrieg jeder Art sei ? sit-
tlich verwerflich ?, sagte der Vorsitzende
der Bischoffskonferenz, Kardinal Karl
Lehmann.
Any kind of war of aggression is ?morally reprehen-
sible,? said the chairman of the Bishops? Conference,
Cardinal Karl Lehmann.
(4) Derartige Erkla?rungen eines Staatschefs
seien im Rahmen der internationalen
Beziehungen inakzeptabel, sagte der UN-
Generaldirektor Sergej Ordzhonikidse
gestern in Genf.
Such statements of heads of states are unacceptable in
the context of international relations, said UN General
Director Sergei Ordzhonikidse in Geneva yesterday.
(5) Wu?rden die Wahlen verschoben, tru?bte sich
das gescho?nte Bild.
Would the elections be postponed, the embellished im-
age would tarnish.
(6) Dies sei alles andere als einfach, ist aus Of-
fizierskreisen zu ho?ren.
This is anything but simple, is to hear from military
circles.
60
EinsatzEimosheDrhsatzs,uDigao
psgasatzEdidsowshgbcdsatzhsu hdo
pgddsgDsatzEcihsoEbchsgwsatzfhgdso
ciwsatzciRsofshksatzfgDDo
wsd asatzspuciEglsoshlrcDsatzdsDDo
MSMMy.SMMy BMSMMyB.SMMyOMSMMyO.SMMyCMSMMyC.SMMyVMSMMy
p EdtFEsktEussbctRshwE
Figure 4: 10 most used verbs (lemma) in indirect
speech.
4.3 Using Indirect Speech
Other modules benefit from the identification of
indirect speech, as can be seen from Sentence 7.
The sentiment system assigns a negative polarity
of ?2.15 to the sentence. The nested sentiment
sources, as described by (Wiebe et al, 2005), of
this sentence require a) a direct speech with the
speaker ?Mazower? and b) an indirect speech with
the speaker ?no one? to be found.5
(7) ?There were serious arguments about what
should happen to the Slavs and Poles in east-
ern Europe,? says Mazower, ?and how many
of them should be sent to the camps and what
proportion could be Germanised . . . No one
ever came out and directly said Hitler had got
it wrong, but there was plenty of implied crit-
icism through comparisons with the Roman
empire. [...]?6
A collective identity evoked in Sentence 7 is
?the Germans?? although the term is not explic-
itly mentioned. This collective identity is de-
scribed as non-homogeneous in the citation and
can be further explored manually by the political
scientists.
The following are further applications of the
identified indirect speeches a) using the frequency
of speeches per text as a feature for classifica-
tion; e.g. a classification system for news re-
ports/commentaries as described in Section 4.4 b)
a project-goal is to find texts in which collective
5The reported sentiment value for the whole sentence is
applicable only to the direct speech. The indirect speech (i.e.
?Hitler had got it wrong?) needs a more fine-grained polarity
score. Since our Complex Concept Builder is very flexible, it
is trivial to score each component separately.
6http://www.guardian.co.uk/education/2008/jul
/01/academicexperts.highereducationprofile
identities are mobilised by entities of political de-
bate (i.e. persons, organisations, etc.); the detec-
tion of indirect speech is mandatory for any such
analysis.
4.4 Commentary/Report Classification
A useful distinction for political scientists dealing
with newspaper articles is the distinction between
articles that report objectively on events or back-
grounds and editorials or press commentaries.
We first extracted opinionated and objective
texts from DeReKo corpus (Stede, 2004; Kupietz
et al, 2010). Some texts were removed in order to
balance the corpus. The balanced corpus contains
2848 documents and has been split into a develop-
ment and a training and test set. 570 documents
were used for the manual creation of features. The
remaining 2278 documents were used to train and
evaluate classifiers using 10-fold cross-validation
with the WEKA machine learning toolkit (Hall et
al., 2009) and various classifiers (cf. Table 1).
The challenge is that the newspaper articles
from the training and evaluation corpus come from
different newspapers and, of course, from differ-
ent authors. Commentaries in the yellow press
tend to have a very different style and vocabulary
than commentaries from broadsheet press. There-
fore, special attention needs to be paid to the in-
dependence of the classifier from different authors
and different newspapers. For this reason, we use
hand-crafted features tailored to this problem. In
return, this means omitting surface-form features
(i.e. words themselves).
The support vector machine used the SMO al-
gorithm (Platt and others, 1998) with a polynomial
kernel K(x, y) =< x, y > e with e = 2. All other
algorithms were used with default settings.
precision recall f-score
SVM 0.819 0.814 0.813
Naive Bayes 0.79 0.768 0.764
Multilayer Percep-
tron
0.796 0.795 0.794
Table 1: Results of a 10-fold cross-validation for
various machine learning algorithms.
A qualitative evaluation shows that direct and
indirect speech is a problem for the classifier.
Opinions voiced via indirect speech should not
lead to a classification as ?Commentary?, but
should be ignored. Additionally, the number of
61
uses of direct and indirect speech by the author can
provide insight into the intention of the author. A
common way to voice one?s own opinion, without
having to do so explicitly, is to use indirect speech
that the author agrees with. Therefore, the number
of direct and indirect speech uses will be added
to the classifier. First experiments indicate that the
inclusion of direct and indirect speech increase the
performance of the classifier.
5 Related Work
Many approaches exist to assist social scientists in
dealing with large scale data. We discuss some
well-known ones and highlight differences to the
approach described above.
The Europe Media Monitor (EMM) (Stein-
berger et al, 2009) analyses large amounts of
newspaper articles and assists anyone interested in
news. It allows its users to search for specific top-
ics and automatically clusters articles from differ-
ent sources. This is a key concept of the EMM,
because it collects about 100, 000 articles in ap-
proximately 50 languages per day and it is impos-
sible to scan through these by hand. EMM users
are EU institutions, national institutions of the EU
member states, international organisations and the
public (Steinberger et al, 2009).
The topic clusters provide insight into ?hot?
topics by simply counting the amount of articles
per cluster or by measuring the amount of news on
a specific topic with regards to its normal amount
of news. Articles are also data-mined for geo-
graphical information, e.g. to update in which
geographical region the article was written and
where the topic is located. Social network infor-
mation is gathered and visualised as well.
Major differences between the EMM and our
approach are the user group and the domain of
the corpus. The complex concepts political sci-
entists are interested in are much more nuanced
than the concepts relevant for topic detection and
the construction of social networks. Additionally,
the EMM does not allow its users to look for their
own concepts and issues, while this interactivity
is a central contribution of our approach (cf. Sec-
tions 1, 2.1 and 3.2).
The CLARIN-D project also provides a web-
based platform to create NLP-chains. It is called
WebLicht (Hinrichs et al, 2010), but in its cur-
rent form, the tool is not immediately usable for
social scientists as the separation of metadata and
textual data and the encoding of the data is hard
for non-experts. Furthermore, WebLicht does not
yet support the combination of manual and au-
tomatic annotation needed for text exploration in
the social science. Our approach is based on the
webservices used by WebLicht. But in contrast to
WebLicht, we provide two additional components
that simplify the integration (exploration work-
bench) and the interpretation (complex concept
builder) of the research data. The former is in-
tended, in the medium term, to be made available
in the CLARIN framework.
6 Conclusion and Outlook
We developed and implemented a pipeline of var-
ious text processing tools which is designed to as-
sist political scientists in finding specific, complex
concepts within large amounts of text. Our case
studies showed that our approach can provide ben-
eficial assistance for the research of political sci-
entists as well as researcher from other social sci-
ences and the humanities. A future aspect will be
to find metrics to evaluate our pipeline. In recently
started annotation experiments on topic classifica-
tion Cohen?s kappa coefficient (Carletta, 1996) is
mediocre. It may very well be possible that the
complex concepts, like multiple collective identi-
ties, are intrinsically hard to detect, and the anno-
tations cannot be improved substantially.
The extension of the NLP pipeline will be an-
other major working area in the future. Examples
are sentiment analysis for German, adding world
knowledge about named entities (e.g. persons and
events), identification of relations between enti-
ties.
Finally, all these systems need to be evaluated
not only in terms of f-score, precision and recall,
but also in terms of usability for the political scien-
tists. This also includes a detailed investigation of
various political science concepts and if they can
be detected automatically or if natural language
processing can help the political scientists to de-
tect their concepts semi-automatically. The defini-
tion of such evaluation is an open research topic in
itself.
Acknowledgements
The research leading to these results has been
done in the project eIdentity which is funded from
the Federal Ministry of Education and Research
(BMBF) under grant agreement 01UG1234.
62
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Andre Blessing, Jens Stegmann, and Jonas Kuhn.
2012. SOA meets relation extraction: Less may be
more in interaction. In Proceedings of the Work-
shop on Service-oriented Architectures (SOAs) for
the Humanities: Solutions and Impacts, Digital Hu-
manities, pages 6?11.
Volker Boehlke, Gerhard Heyer, and Peter Wittenburg.
2013. IT-based research infrastructures for the hu-
manities and social sciences - developments, exam-
ples, standards, and technology. it - Information
Technology, 55(1):26?33, February.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional, pages 89?97.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Manaal Faruqui and Sebastian Pado?. 2010. Train-
ing and evaluating a german named entity recog-
nizer with semantic generalization. In Proceedings
of KONVENS 2010, Saarbru?cken, Germany.
D. Ferrucci and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information process-
ing in the corporate research environment. Natural
Language Engineering, 10(3-4):327?348.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Erhard W. Hinrichs, Marie Hinrichs, and Thomas Za-
strow. 2010. WebLicht: Web-Based LRT Services
for German. In Proceedings of the ACL 2010 System
Demonstrations, pages 25?29.
Cathleen Kantner, Amelie Kutter, Andreas Hilde-
brandt, and Mark Puettcher. 2011. How to get rid
of the noise in the corpus: Cleaning large samples
of digital newspaper texts. International Relations
Online Working Paper, 2, July.
Marc Kupietz, Cyril Belica, Holger Keibel, and An-
dreas Witt. 2010. The german reference corpus
dereko: a primordial sample for linguistic research.
In Proceedings of the 7th conference on interna-
tional language resources and evaluation (LREC
2010), pages 1848?1854.
Shalom Lappin and Herbert J Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational linguistics, 20(4):535?561.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Andreas Niekler and Patrick Ja?hnichen. 2012. Match-
ing results of latent dirichlet alocation for text.
In Proceedings of ICCM 2012, 11th International
Conference on Cognitive Modeling, pages 317?322.
Universita?tsverlag der TU Berlin.
John Platt et al 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector
machines. technical report msr-tr-98-14, Microsoft
Research.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513?
523.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Helmut Schmid, 2009. Corpus Linguistics: An In-
ternational Handbook, chapter Tokenizing and Part-
of-Speech Tagging. Handbooks of Linguistics and
Communication Science. Walter de Gruyter, Berlin.
Burr Settles and Xiaojin Zhu. 2012. Behavioral fac-
tors in interactive training of text classifiers. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
563?567. Association for Computational Linguis-
tics.
Burr Settles. 2011. Closing the loop: Fast, inter-
active semi-supervised annotation with queries on
features and instances. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1467?1478. Association for Com-
putational Linguistics.
Manfred Stede. 2004. The potsdam commentary
corpus. In Proceedings of the 2004 ACL Work-
shop on Discourse Annotation, DiscAnnotation ?04,
pages 96?102, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the europe me-
dia monitor family of applications. In Proceedings
of the Information Access in a Multilingual World-
Proceedings of the SIGIR 2009 Workshop, pages 1?
8.
63
Roland Stuckardt. 2001. Design and enhanced evalua-
tion of a robust anaphor resolution algorithm. Com-
putational Linguistics, 27(4):479?506.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
64
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 35?44,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Conceptual and Practical Steps in
Event Coreference Analysis of Large-scale Data
Fatemeh Torabi Asr
1
, Jonathan Sonntag
2
, Yulia Grishina
2
and Manfred Stede
2
1
MMCI Cluster of Excellence, Saarland University, Germany
fatemeh@coli.uni-saarland.de
2
Applied Computational Linguistics, University of Potsdam, Germany
sonntag|grishina|stede@uni-potsdam.de
Abstract
A simple conceptual model is employed
to investigate events, and break the task
of coreference resolution into two steps:
semantic class detection and similarity-
based matching. With this perspective an
algorithm is implemented to cluster event
mentions in a large-scale corpus. Results
on test data from AQUAINT TimeML,
which we annotated manually with coref-
erence links, reveal how semantic conven-
tions vs. information available in the con-
text of event mentions affect decisions in
coreference analysis.
1 Introduction
In a joint project with political scientists, we are
concerned with various tasks of indexing the con-
tent of a large corpus of newspaper articles. To
supplement other NLP tools and as an interest-
ing information for the political scientists by itself,
we are interested in keeping track of discussions
around headline events such as attacks and crises.
The main challenges in the project include:
1. proposing a definition of event identity, and
2. finding the actual mentions in natural text,
to construct clusters of, so-called, coreferential
events. We refer to the former task as a formal
convention, a vital step in order for useful results
to be delivered to the human text analysts. The lat-
ter is basically an information extraction task once
a clear problem specification is obtained.
The main objective of the paper is to shed
light on each of the above tasks by applying a
three-layer event ontology
1
. Terminologies from
1
The term ontology is used to refer to a conceptual model
of events and connections between them rather than a partic-
ular knowledge base implementation.
earlier theories (Davidson, 1969) up until recent
work (Hovy et al., 2013a) are combined to draw an
integrated picture of the event coreference prob-
lem. The semantic layer is established with the
help of WordNet synsets. Related entities and
timestamps are considered as fundamental event
attributes that in practice can be resolved from the
context of a mention. We implement an incremen-
tal event clustering algorithm with respect to the
adapted ontology of events and use a minimal lin-
guistic procedure to extract values from text for
every event attribute. This system is being devel-
oped to work within a pipeline annotation project
where incremental clustering performs efficiently
on large-scale data.
In order to evaluate our proposed method, we
have manually annotated a random selection of
event mentions in the AQUAINT TimeML cor-
pus (UzZaman et al., 2013). Performance of the
automatic system in pair-wise coreference reso-
lution is comparable to that of more sophisti-
cated clustering methods, which at the same time
consider a variety of linguistic features (Bejan
and Harabagiu, 2010). The differences between
the human annotator pair-wise decisions and the
output of our clustering algorithm reveal inter-
esting cases where coreference labeling is per-
formed based upon the adapted semantic conven-
tion rather than information available in the text
about time, location and participants of an event
instance. In the following, we provide an overview
of the adapted ontology, background on event
coreference, and finally our implementation and
experiments within the proposed framework on
real data as well as the annotated corpus. We point
to related work at the various appropriate places in
the paper.
2 An Object Oriented Ontology
The general impression one gets by a review of
the coreference literature, is that at the semantic
35
formalism level, events are engaged with a higher
degree of complexity and more variety than en-
tities. That is probably because of the concrete
nature of entities: intuitively, an event happens,
whereas, an entity exists. As a subject matter, the
latter is more straightforward to get decomposed
into smaller components and be identified by cer-
tain feature attributes. The ontology explained in
this chapter is general in the sense that one could
(perhaps should) start understanding it by exam-
ples about entities.
A realized entity belongs to a class of enti-
ties sharing the same set of attributes. For ex-
ample, president Obama, as long as being talked
in a political context is considered as an instance
of the class PRESIDENT, comprising attributes
such as Country, Party and Duration of
presidency. Any other president can be compared
against Obama, with respect to the attribute values
associated with them. Therefore, Bush is a differ-
ent instance of the class PRESIDENT regarding
the fact that a different political Party as well
as a different presidential Duration are assigned
to him. Detecting mentions of these PRESIDENT
instances in text corpora would be a technical task
once the semantic representation was fixed. At this
level, instead we face questions like, whether or
not a named entity somewhere in the text detected
by our text processor, e.g., ?Barack Hossein?, is
referring to the one PRESIDENT instance that we
named above as Obama.
Figure 1 illustrates similar levels of abstraction
for event classes, event instances, and event men-
tions. The distinction between the second and the
third layer are more obvious and previously con-
sidered as clearly in other frameworks. The dis-
tinction between the first and the second layer,
though, is often left implicit, even in recently pub-
lished event annotation guidelines. For example in
a Grounded Annotation for Events (GAF, Fokkens
et al. 2013), event mentions are clearly distin-
guished from instances. However, the first two
layers have been taken as one, i.e., the semantic
layer. In their work, event type which is an artifact
of the adapted semantic ontology (SEM, Klyne
and Carroll 2004), implicitly works similar to the
classes in our definition. Nevertheless, these three
layers are intuitively separable and familiar for lin-
guists working on event and entity recognition.
Bejan and Harabagiu (2010), for example, intro-
duce the event coreference resolution with an ex-
ample put into a similar three-layer hierarchy, de-
spite their purely data-driven approach leaving off
prior semantic specifications. Here, we explain
each layer of the model separately. Issues specific
to coreference detection will be presented in the
following section.
2.1 Event Classes
The first layer of the ontology determines event
type definitions. Each class can have totally dif-
ferent attributes depending on the interests of a
particular study. Some events might be identi-
fied only by their time and place, while others by
participants of prioritized importance. A very flat
semantic representation would attribute all types
of events with a fixed set of entities, e.g.: par-
ticipants, time and location. Note, however, that
structural and semantic differences exist among
events of different natures, even if these complex
phenomena are reduced into something more fa-
miliar and tangible such as verb frames (Fillmore
et al., 2003). For example, a KILLING event is es-
sentially attributed with its Agent and Patient,
while salient attributes of an EARTHQUAKE
include Location, Magnitude, Time and
Human Impacts, in a typical news context.
This becomes even more clear when event types
are taken and compared against one another from
different genres of text (Pivovarova et al., 2013;
Shaw, 2013). A scientific attitude toward the
analysis of EARTHQUAKE events might character-
ize them with Natural Impacts rather than
Human Impacts. Thus, the first layer of the
model needs to be designed with respect to the
specific information extraction goals of the partic-
ular study, be it a pure linguistic or an application-
oriented one.
Ambiguities about the granularity of attributes,
subevent-ness, scope and most importantly, iden-
tity between event instances are dealt with at the
definition layer for and between classes. For ex-
ample, if the modeler wants to allow coreference
between instances of KILLING and SHOOTING
to indicate some type of coreference between an
event and its possible subevent then this needs to
be introduced at the class level, along with a pro-
cedure to compare instances of the two classes,
which possess different sets of attribute
2
. Remarks
2
The same applies even to a more flexible case, when
the modeler wants to allow coreference between KILLING
and DYING instances (e.g., if a KILLING?s Patient is the
same as a DYING?s Theme).
36
  
Class KILLINGAgent;Patient;Time;Location;
Class SHOOTINGAgent;Patient;Time;Location;Weapon;
Class EARTQUAKEMagnitude;Human Impacts;Time;Location;
Shooting instance 1Agent: Lee Harvey OswaldPatient:  John Fitzgerald KennedyWeapon: a rifleTime: 22.11.1963Location: Dealey Plaza, Dallas
Mention 4? Shortly after noon on November 22, 1963, President John F. Kennedy was assassinated as he rode in a motorcade through Dealey Plaza. ?
Earthquake instance 1Magnitude: 6.6 to 7Human Impacts:  injury and deathTime: 20.04.2013Location: Sichuan, China
Mentions 2 and 3? Lushan, China (CNN) -- A strong earthquake that struck the southwestern Chinese province of Sichuan this weekend has killed 186 people, sent nearly 8,200 to hospitals and created a dire dearth of drinking water, Chinese state-run Xinhua reported Sunday. Earlier reports had said as many as 11,200 people were injured. ?
1 n m q
        Formalism                                  Realization                                           Text 
Killing instance 1Agent: Lee Harvey OswaldPatient:  John Fitzgerald KennedyTime: 22.11.1963Location: Dealey Plaza, Dallas
Killing instance 2Agent: an earthquakePatient: local peopleTime: 20.04.2013Location: Sichuan, China
Mention 1? President Kennedy was killed three days before he was to make these amendments public.?
Figure 1: A three-layer ontology of events: classes, instances and mentions
of Hovy et al. (2013b) on different types of iden-
tity according to lexicosyntactic similarity, syn-
onymy and paraphrasing indicate that the model-
ers have a wide choice of identity definition for
event types. In section 4.3 we explain how to adapt
an extended version of synonymy in order to de-
fine event classes prior to similarity-based cluster-
ing of the mentions.
2.2 Event Instances
Layer 2 indicates perfect instantiation, representa-
tive of the human common sense intuition of phe-
nomena in real world. Instances in this layer corre-
spond to the Davidsonian notion of events as con-
crete objects with certain locations in space-time,
something that is happening, happened, or will
happen at some point (Davidson, 1969). There-
fore, links from classes to instances represent
a one-to-many relation. Every instance of the
EARTHQUAKE is determined with a unique set
of attribute values. Two EARTHQUAKE instanti-
ations with exactly similar attribute values are just
identical. In order to keep a clear and simple rep-
resentation specific to the study of coreference,
the model does not allow any connection or rela-
tion between two event instances unless via their
classes. Note that in Figure 1, for each realized
object, only attributes included in the formalism
layer are presented with their values, while in re-
ality events occur with possibly infinite number of
attributes.
2.3 Event Mentions
Facing an event mention in the text, one should
first determine its class and then the unique event
instance, to which the mention points. Detection
of the class depends on the semantic layer defi-
nitions, while discovering the particular instance
that the mention is talking about relies on the at-
tribute values extractable from the mention con-
text.
Usually, mentions provide only partial informa-
tion about their target event instance. They can
be compared against one another and (if available)
against a fully representative mention, which most
clearly expresses the target event by providing all
necessary attribute values. Fokkens et al. (2013)
refer to such a mention as the trigger event. Some-
times it is possible that the context is even more in-
formative than necessary to resolve the unique real
world corresponding event (see details about the
impact of the earthquake in mention 3, Figure 1).
In natural text a mention can refer to more than
one event instance of the same type, for example
when a plural case is used: ? ... droughts, floods
and earthquakes cost China 421 billion yuan in
2013?. Hovy et al. (2013b) propose partial coref-
erence between singular and plural mentions. In
37
our model plural mentions are not treated seman-
tically differently, they only point to several in-
stances, thus, are coreferential with any single
mention of them as long as the attribute values al-
low
3
.
With respect to the above discussion, links from
layer 2 to 3 represent many-to-many relations: an
event instance can have several mentions in the
text, and a single mention can point to more than
one event instance at a time.
3 Towards Coreference Analysis
In terms of method, two different approaches have
been tried in the literature under the notion of
event coreference resolution (Chen and Ji, 2009;
Bejan and Harabagiu, 2010; Lee et al., 2012;
Hovy et al., 2013b). The first and most theoreti-
cally founded strategy is to decide for every pair
of event mentions, whether or not they refer to
the same event instance. Since in this approach
decisions are independently made for every pair
of event mentions, a clear formalism is needed to
determine exactly what types of coreference are
possible and how they are detected by looking
at textual mentions (Chen and Ji, 2009; Hovy
et al., 2013b). Some related work on predicate
alignment also fit into this category of research
(Roth and Frank, 2012; Wolfe et al., 2013).
Alternatively, in automatic event clustering, the
objective is basically discovering event instances:
all we know about an event in the world is the
collective information obtained from mentions
referring to that in a text corpus. Each cluster
in the end ideally represents a unique event in
reality with all its attribute values (Bejan and
Harabagiu, 2010; Lee et al., 2012). Some formal
and technical differences exist between the two
approaches.
Boolean choice: traditionally, clusters shape with
the idea that all mentions within a cluster are of
the same identity. Every randomly chosen pair
of mentions are coreferent if they are found in a
single cluster at the end, and non-coreferent oth-
erwise. Therefore, taking this approach implies a
level of formalism, which rules out partial coref-
erence. On the other hand, pair-wise classifica-
tion could consider partial coreference whenever
3
The other type of quasi-identity discussed by Hovy et al.
(2013b) engaged with sub-events is handled in the semantic
level.
two event mentions are neither identical nor totally
different (Hovy et al., 2013b). Soft-clustering can
compensate some deficiencies of traditional clus-
tering approaches
4
.
Transitivity: all mentions in a single cluster
are coreferential, whereas pair-wise labels allow
for non-transitive relations among event mentions.
Depending on the specific goal of a study, this
could be an advantage or a disadvantage. Lack
of transitivity could be considered as an error if it
is not consciously permitted in the underlying se-
mantic formalism.
Complexity and coverage: event mentions can
appear in noisy or sparse context where informa-
tion for detection of their target event instance is
not available. Dealing with such cases is usually
easier in a clustering framework where similarity
scores are calculated against the collective infor-
mation obtained from a population of mentions,
rather than an individual occurrence. Classifica-
tion approaches could comparatively handle this
only if sufficiently representative labeled data is
available for training.
Exploration: a general advantage of cluster anal-
ysis is that it provides an exploratory framework
to assess the nature of similar input records, and
at the end it results in a global distributional
representation. This is specially desired here,
since computational research on event coreference
is in its early ages. Evaluation corpora and
methodology are still not established, thus, the
problem is not yet in the phase of ?look for higher
precision?!
The method we are going to propose in the next
section combines a rule-based initial stage with a
similarity-based clustering procedure. This is par-
tially inspired by the work of Rao et al. (2010),
where entity coreference links are looked up in
high-volume streaming data. They employ a lex-
icon of named entities for cluster nomination to
reduce the search space. Once a mention is visited
only the candidates among all incrementally con-
structed clusters up to that point are examined. In-
cremental clustering strategies are in general suit-
able for a pipeline project by efficiently providing
single visits of every mention in its context. Fea-
ture values of a mention can be extracted from the
document text, used for clustering, and combined
4
For example, multi-assignment would allow plural men-
tions to take part in several different clusters, each represen-
tative of one event instance.
38
into the feature representation of the assigned clus-
ter in a compressed format.
4 Event Coreference System
The original data in our study is a text corpus au-
tomatically annotated with several layers of syn-
tactic and semantic information (Blessing et al.,
2013). The English portion includes news and
commentary articles of several British and Amer-
ican publishers from 1990 to 2012. An approx-
imate average of 100 event mentions per docu-
ment with the large number of total documents per
month (avg. 1200) requires us to think of different
ways to reduce the search space and also design a
low-complexity coreference resolution algorithm.
4.1 Partitioning
In cross-document analysis, typically, a topic-
based document partitioning is performed prior to
the coreference chain detection (Lee et al., 2012;
Cybulska and Vossen, 2013). Since we are in-
terested to track discussions about a certain event
possibly appearing in different contexts, this tech-
nique is not desired as coreference between men-
tions of a single real word event in two differ-
ent topics would remain unknown. For example,
when an articles reviews several instances of a cer-
tain event type such as different attacks that has
happened in a wide temporal range and in differ-
ent locations, such articles would not be included
in any of the individual topics each focused on
one event instance. As an alternative to the pre-
vious approach, we perform a time-window par-
titioning based on the article publication date be-
fore feeding the data into the coreference analysis
algorithm. Larger windows would capture more
coreference links: this is a parameter that can be
set with respect to the available resources in trade-
off with the desired search scope. In the future, we
would like to invent an efficient procedure to com-
bine the resulting clusters from consecutive time-
windows in order to further enhance the recall of
the system.
4.2 Event Mention and Feature Identification
In order to extract event mentions we use the
ClearTK UIMA library (Ogren et al., 2008), check
the PoS of the head word in the extracted text
span and take all verbal and nominal mentions
into account. In the current implementation all
event classes are identified by a fixed set of at-
tributes including Timestamps and Related
Entities. While being very coarse-grained,
this way of attribution is quite intuitive: events
are identified by times, places and participants
directly or vaguely attached to them. Temporal
expressions are extracted also by ClearTK and
normalized using SUTime (Chang and Manning,
2012). Named entities of all types except Date
are used which are obtained from previous work
on the same dataset (Blessing et al., 2013).
4.3 The Two-step Algorithm
Having all required annotations, we select a
time window and perform the following two
steps for event mentions of the TimeML classes
Occurrence, I-Action, Perception and
Aspectual
5
.
1) Semantic class identification: WordNet
synsets provide a rich resource in order to be
adapted as event classes (Fellbaum, 1999). They
cover a large lexicon and the variety of rela-
tional links between words enables us to specify
a clear semantic convention for the coreference
system. In addition to the mentions coming from
the same synset, we allow coreference between
events belonging to two different synsets that are
directly connected via hypernymy or morphose-
mantic links. While every WordNet synset com-
prises words only from a single part of speech,
morphosemantic relations allow the model to es-
tablish cross-PoS identity among words sharing
a stem with the same meaning which is desired
here: observe (verb) and observation (noun)
6
. A
Java library is employed to access WordNet anno-
tations (Finlayson, 2014).
2) Similarity-based clustering: A mention is
compared against previously constructed clus-
ters with respect to the attribute values that are
extractable from its context. In order to fill
the Timestamps attribute we have employed a
back-off strategy: first we look at all time expres-
sions in the same paragraph where the event men-
tion appears, if we found enough temporal infor-
mation, that would suffice. Otherwise, we look
into the content of the entire article for tempo-
ral expressions. The Related Entities at-
5
Other types, namely, Report, State and I-State
events are not interesting for us, therefore such mentions are
simply skipped.
6
When a mention is visited all compatible synsets accord-
ing to the head lemma are tried because in the current imple-
mentation we do not perform word sense disambiguation.
39
tribute is filled similarly by looking at the named
entities in the context of the event mention. The
first step is a procedure to candidate clusters con-
taining mentions of related types. If no cluster
is a candidate, a singleton cluster is created and
its class is added to the index of visited event
types (synsets). If candidate clusters already ex-
ist, we calculate the feature-based similarity score
for each. If the best score is below a threshold a
new singleton cluster is created but in this case for
the reason that, perhaps, not a new type but a new
event instance is visited.
5 Manual Annotation and Evaluation
The Event Coreference Bank, which is the largest
available corpus with cross-document corefer-
ence labels, supports only a within topic evalu-
ation (ECB, Bejan and Harabagiu 2010). In or-
der to perform a more realistic evaluation of the
method presented in this paper, we selected a sub-
set of events from the AQUAINT TimeML cor-
pus and annotated those with coreferentiality. The
AQUAINT TimeML data has recently served as
one of the benchmarks in the TempEval shared
task (UzZaman et al., 2013) and is available for
public use
7
. It contains 73 news report docu-
ments from four topics, annotated with 4431 event
mentions and 652 temporal expressions which
make it suitable for our task. Two main differ-
ences between our annotation and the ECB data
are: 1) event mentions here are selected semi-
randomly
8
and across topics rather than topic-
based, 2) they are shown pair-wise to the anno-
tator (in order to catch the transitivity patterns
after the analysis), whereas, in the ECB, event
mentions are clustered. Furthermore, the data
already comes with manually assigned mention
boundaries, event types, temporal expressions and
links between events and temporal expressions, all
according to the TimeML standards (Hobbs and
Pustejovsky, 2003). These serve exactly as fea-
tures that our algorithm uses for construction of
clusters. We only had to perform named entity
recognition automatically to have data ready for
evaluation of the model. The manual annotation
7
http://www.cs.york.ac.uk/
semeval-2013/task1
8
Since the number of coreferential mentions is much
smaller than non-coreferent ones, we adapted a heuristic mea-
sure to make sure that we will have some similar mentions
among the 100 records. Therefore, we would call it a semi-
random selection, still different from the fully selective strat-
egy employed for ECB.
of 4950 pairs resulting from 100 selected event
mentions (
100!
2!(100?2)!
) was done with the help of a
simple user interface, which showed each of the
two event mentions within its context to the an-
notator and asked for pushing yes, no or next
(undecided) button to proceed to the next pair.
After studying the annotation guideline published
by Cybulska and Vossen (2014), our expert spent
some hours during a week for the job. Decisions
made in shorter than 500 ms were revised after-
wards. There was one no answer which the an-
notator found unsure after revision, as it resulted
in a transitivity violation, but we left it unchanged
due to the nature of pair-wise decisions. In the end
we came up with a total of 36 yes, and 4914 no
pairs.
6 Experiments
This section provides an insight into how clusters
of event mentions are created for a portion of our
large news corpus. We also run the algorithm on
the manually annotated data to perform an error
analysis.
6.1 Construction of Event Clusters
News text from New York Times and Washing-
ton Post are combined to demonstrate a show-
case of clustering for a time-window of two weeks
(250 articles)
9
. Figure 2 shows the creation curve
of event classes (type index entries) and event
instances (clusters) as the number of the vis-
ited mentions increases. Comparison between the
number of mentions with that of clusters indicates
that a great deal of event instances are mentioned
only once in the text. Since, for each mention, all
compatible synsets are added to the type index (if
not there already) during the early stages of clus-
tering the number of the type index entries is times
the number of visited mentions. In the middle
to the end phases the type index contains a large
collection of event classes, also a decent number
of non-singleton clusters (repeatedly mentioned
event instances) are created. Statistics of the type
of clusters obtained after performing the algorithm
on the processed mentions are presented in Ta-
ble 1. A significant number of non-singleton clus-
ters contain mentions only from a single paragraph
or a single article, which is expected given the type
9
This collection is processed within a few minutes on a
normal PC by the proposed algorithm starting with zero clus-
ters.
40
Figure 2: Number of clusters and the type index entries as mentions are visited in 250 articles
of features; remember that Timestamps and
Named Entities are looked up in a paragraph
scope. Clusters containing mentions from several
articles, namely, the popular ones are most inter-
esting for us as they would be representative of the
systems performance on cross-document corefer-
ence analysis. By looking at those we found that
the named entities have a very important role in
finding similar subtopics within and between doc-
uments. Temporal expressions are less helpful as
they are rare, and otherwise introduce some noise
when documents are already being processed in
a specific publication time-window. For example,
the word today which appears in most articles of
the same day (and would be normalized to that
day?s date, e.g., ?1990.01.12?) would gather men-
tions of a general event type, e.g., meet, although,
they might not be pointing to the same instance.
The employed semantic convention establishes a
balance between efficiency and recall of the sys-
tem. Nevertheless, it sometimes allows clustering
of intuitively unrelated actions. In order to en-
hance the clustering performance in terms of the
precision, we have a parameter to give priority to
within synset coreference.
Cluster type Freq. Avg. content
Singleton 12895 1
Single paragraph 1360 2.36
Single article 807 3.95
Popular 182 2.99
Table 1: Different types of resulting clusters
6.2 Error Analysis
We fed all event mentions from the AQUAINT
TimeML corpus into the algorithm exactly in the
same way that we did in case of our large news
corpora. The algorithm has a few parameters
which we set by looking at samples of resulting
clusters prior to the measurement on the labeled
portion. This is a minimal NLP system given that
neither syntactic/semantic dependency of entities
to the event head word nor the type of attachment
to temporal expressions in the context are taken
into account. Nevertheless, we obtain 51.3% pre-
cision and 55.6% recall for the pair-wise corefer-
ence resolution task on the annotated data. The
resulting F-score of 53.4% is comparable with the
best F-scores reported in the work of Bejan and
Harabagiu (52.1% on ECB for the similar task)
while they use a rich linguistic feature set, as well
as a more sophisticated clustering method.
Coreference Total Related class Same doc.
True positive 20 100% 25%
True negative 4895 16% 2%
False positive 19 100% 36%
False negative 16 33% 7%
Total 4950 15% 2%
Table 2: Pair-wise decisions
Table 2 shows false positive and negative answers
separately. As reflected in the results, positive
labels are given only to mention pairs of related
classes (headwords need to share a synset, or are
related via hypernym and morphosemantic links
in WordNet). 36% of positive labels are given to
pairs within some article which is expected given
that common contextual features are easy to find
for them. In such cases, usually linguistic features
are needed to resolve participants or the relative
temporality of one mention against the other:
a. some people are born rich, some are born
poor.
b. the bullet bounced off a cabinet and rico-
cheted into the living room.
41
In some cases, on the other hand, the disagreement
depends on the semantic approach to the defini-
tion of identity, and therefore, is more controver-
sial. The human annotator has apparently been
more conservative to annotate coreference when
the head words of the mentions were a bit different
in meaning, whereas the system?s decision bene-
fited from some flexibility:
a. the immigration service decided the boy
should go home. / they made a reasonable
decision Wednesday in ruling that...
b. if he goes, he will immediately become...
It is not clear, for example, whether ruling is a sub-
event of the decision or exactly the same event. A
similar distinction needs to be made in case of the
false negative labels. The automatic clustering is
not able to detect coreference mostly in case of
sparse context, where enough information is not
available to resolve the similarity. That is why
false negative happens more frequently for men-
tions coming from different articles (specifically
paragraphs sharing few named entities) and only
7% of the time when they happen within a docu-
ment:
a. the Clinton administration has pushed for
the boy?s return. / his son said he didn?t
want to go.
Sparse context results either in the creation of a
singleton cluster for the mention or careless as-
signment to some wrong cluster, which in the fu-
ture would decrease the chance of meeting coref-
erent mentions. False negatives happening for
mentions of unrelated semantic classes are due to
the missing links between possibly synonym words
in WordNet, one of the issues that need to be in-
vestigated and cured in the future work.
7 Conclusion
This paper presented a variety of material concern-
ing event coreference resolution:
1. A general ontology is explained that can be
employed in different studies on events.
2. An algorithm is designed, regardingly, to
gather coreferential event in a large corpus.
3. A set of event mentions in AQUAINT
TimeML is annotated with pair-wise corefer-
ence tags within and between topics
10
.
4. An implementation of the method consider-
ing simple and scalable features is tested on
real data and the annotated corpus.
5. Finally, we performed an error analysis of the
automatically assigned labels to identify fu-
ture directions.
Separating the semantic layer definition of coref-
erence from textual attribution of event mentions
has two benefits in our framework. First, it pro-
vides us with an efficient partitioning procedure
to reduce the search space. Second, it makes the
model flexible to allow for different possible se-
mantic conventions which could vary from one
application to another. Our adaptation of Word-
Net synsets allows for integrative future exten-
sion of the model ? e.g., to capture metaphori-
cal and subevent relations based on Methonymy
and Entailment links. The intuition of using
named entities for identification of important real-
world events resulted in balanced precision and re-
call on the test data. In the future, we would like to
investigate the effect of linguistic features on im-
proving the performance of the algorithm. In par-
ticular, it would be interesting to see whether exact
specification of event head arguments would out-
perform the vague attribution with related entities.
The state-of-the-art result in the supervised predi-
cate alignment approach is a hint for rich linguistic
features to be helpful (Wolfe et al., 2013). On the
other hand, depending on the adapted event iden-
tity definition, coreferential events might not re-
ally share identical arguments (Hasler and Orasan,
2009). There are differences between real data
collections and the available annotated corpora,
including ours, which needs to be investigated as
well. For example, small collections do not in-
clude enough same-class event mentions pointing
to different event instances, and it brings about
unrealistic evaluations. Furthermore, annotation
guidelines are usually biased towards a specific
theory of event identity which affect the resulting
data in one way or another. Some applications de-
mand different semantic conventions perhaps with
broader/narrower definition of identity. This is a
dilemma that needs to be resolved through more
theoretical studies in touch with real world prob-
lems such as the one we introduced in this paper.
10
The annotation is available at: http://www.coli.
uni-saarland.de/
?
fatemeh/resources.htm
42
References
Bejan, C. A. and Harabagiu, S. (2010). Unsuper-
vised event coreference resolution with rich lin-
guistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 1412?1422. Associa-
tion for Computational Linguistics.
Blessing, A., Sonntag, J., Kliche, F., Heid, U.,
Kuhn, J., and Stede, M. (2013). Towards a
tool for interactive concept building for large
scale analysis in the humanities. In Proceed-
ings of the 7th Workshop on Language Technol-
ogy for Cultural Heritage, Social Sciences, and
Humanities, pages 55?64, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Chang, A. X. and Manning, C. (2012). Sutime:
A library for recognizing and normalizing time
expressions. In LREC, pages 3735?3740.
Chen, Z. and Ji, H. (2009). Graph-based event
coreference resolution. In Proceedings of the
2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 54?57.
Association for Computational Linguistics.
Cybulska, A. and Vossen, P. (2013). Semantic re-
lations between events and their time, locations
and participants for event coreference resolu-
tion. In RANLP, volume 2013, page 8.
Cybulska, A. and Vossen, P. (2014). Guidelines for
ecb+ annotation of events and their coreference.
Technical report, Technical Report NWR-2014-
1, VU University Amsterdam.
Davidson, D. (1969). The individuation of events.
In Essays in honor of Carl G. Hempel, pages
216?234. Springer.
Fellbaum, C. (1999). WordNet. Wiley Online Li-
brary.
Fillmore, C. J., Johnson, C. R., and Petruck, M. R.
(2003). Background to framenet. International
journal of lexicography, 16(3):235?250.
Finlayson, M. A. (2014). Java libraries for ac-
cessing the princeton wordnet: Comparison and
evaluation. In Proceedings of the 7th Global
Wordnet Conference, pages 78?85.
Fokkens, A., van Erp, M., Vossen, P., Tonelli, S.,
van Hage, W. R., SynerScope, B., Serafini, L.,
Sprugnoli, R., and Hoeksema, J. (2013). Gaf: A
grounded annotation framework for events. In
NAACL HLT, volume 2013, page 11.
Hasler, L. and Orasan, C. (2009). Do corefer-
ential arguments make event mentions corefer-
ential. In Proc. the 7th Discourse Anaphora
and Anaphor Resolution Colloquium (DAARC
2009).
Hobbs, J. and Pustejovsky, J. (2003). Annotating
and reasoning about time and events. In Pro-
ceedings of AAAI Spring Symposium on Logical
Formalizations of Commonsense Reasoning.
Hovy, E., Mitamura, T., and Palmer, M. (2013a).
The 1st workshop on events: Definition, detec-
tion, coreference, and representation.
Hovy, E., Mitamura, T., Verdejo, F., Araki, J.,
and Philpot, A. (2013b). Events are not sim-
ple: Identity, non-identity, and quasi-identity.
NAACL HLT 2013, page 21.
Klyne, G. and Carroll, J. J. (2004). Resource
description framework (rdf): Concepts and ab-
stract syntax. w3c recommendation, 10 feb.
2004.
Lee, H., Recasens, M., Chang, A., Surdeanu,
M., and Jurafsky, D. (2012). Joint entity and
event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural Lan-
guage Learning, pages 489?500. Association
for Computational Linguistics.
Ogren, P. V., Wetzler, P. G., and Bethard, S. J.
(2008). Cleartk: A uima toolkit for statisti-
cal natural language processing. Towards En-
hanced Interoperability for Large HLT Systems:
UIMA for NLP, 32.
Pivovarova, L., Huttunen, S., and Yangarber, R.
(2013). Event representation across genre.
NAACL HLT 2013, page 29.
Rao, D., McNamee, P., and Dredze, M. (2010).
Streaming cross document entity coreference
resolution. In Proceedings of the 23rd Inter-
national Conference on Computational Linguis-
tics: Posters, pages 1050?1058. Association for
Computational Linguistics.
Roth, M. and Frank, A. (2012). Aligning predicate
argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceed-
ings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Pro-
ceedings of the main conference and the shared
43
task, and Volume 2: Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation,
pages 218?227. Association for Computational
Linguistics.
Shaw, R. (2013). A semantic tool for historical
events. NAACL HLT 2013, page 38.
UzZaman, N., Llorens, H., Derczynski, L., Verha-
gen, M., Allen, J., and Pustejovsky, J. (2013).
Semeval-2013 task 1: Tempeval-3: Evaluat-
ing time expressions, events, and temporal rela-
tions. In Second joint conference on lexical and
computational semantics (* SEM), volume 2,
pages 1?9.
Wolfe, T., Van Durme, B., Dredze, M., Andrews,
N., Beller, C., Callison-Burch, C., DeYoung,
J., Snyder, J., Weese, J., Xu, T., et al. (2013).
Parma: A predicate argument aligner.
44
