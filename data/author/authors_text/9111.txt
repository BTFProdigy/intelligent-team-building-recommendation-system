Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 37?40,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity and Relation Identification System 
 
 
Tianfang Yao 
Department of Computer Science and 
Engineering 
Shanghai Jiao Tong University 
Shanghai, 200030, China 
yao-tf@cs.sjtu.edu.cn 
Hans Uszkoreit 
Department of Computational Linguistics and 
Phonetics 
Saarland University 
Saarbr?cken, 66041, Germany 
uszkoreit@coli.uni-sb.de 
 
  
 
Abstract 
In this interactive presentation, a Chinese 
named entity and relation identification 
system is demonstrated. The domain-
specific system has a three-stage pipeline 
architecture which includes word seg-
mentation and part-of-speech (POS) tag-
ging, named entity recognition, and 
named entity relation identitfication. The 
experimental results have shown that the 
average F-measure for word segmenta-
tion and POS tagging after correcting er-
rors achieves 92.86 and 90.01 separately. 
Moreover, the overall average F-measure 
for 6 kinds of name entities and 14 kinds 
of named entity relations is 83.08% and 
70.46% respectively. 
1 Introduction 
The investigation for Chinese information ex-
traction is one of the topics of the project COL-
LATE (DFKI, 2002) dedicated to building up the 
German Competence Center for Language Tech-
nology. The presented work aims at investigating 
automatic identification of Chinese named enti-
ties (NEs) and their relations in a specific domain.  
Information Extraction (IE) is an innovative 
language technology for accurately acquiring 
crucial information from documents. NE recog-
nition is a fundamental IE task, that detects some 
named constituents in sentences, for instance 
names of persons, places, organizations, dates, 
times, and so on. Based on NE recognition, the 
identification of Named Entity Relation (NER) 
can indicate the types of semantic relationships 
between identified NEs. e.g., relationships be-
tween person and employed organization; person 
and residing place; person and birthday; organi-
zation and seat, etc. The identified results for 
NEs and NERs can be provided as a resource for 
other application systems such as question-
answering system. Therefore, these two IE tasks 
are selected as our investigation emphases. 
Chinese has a very different structure from 
western languages. For example, it has a large 
character set involving more than 48,000 charac-
ters; there is no space between words in written 
texts; and Chinese words have fewer inflections, 
etc. In the past twenty years there have been sig-
nificant achievements in IE concerning western 
languages such as English. Comparing with that, 
the research on the relevant properties of Chinese 
for IE, especially for NER, is still insufficient. 
Our research focuses on domain-specific IE. 
We picked the sports domain, particularly, texts 
on soccer matches because the number and types 
of entities, relations and linguistic structures are 
representative for many applications. 
Based on the motivations above mentioned, 
our goals for the design and implementation of 
the prototype system called CHINERIS (Chinese 
Named Entity and Relation Identification System) 
are: 
? Establishing an IE computational model 
for Chinese web texts using hybrid tech-
nologies, which should to a great extent 
meet the requirements of IE for Chinese 
web texts; 
? Implementing a prototype system based 
on this IE computational model, which 
extracts information from Chinese web 
texts as accurately and quickly as possi-
ble; 
? Evaluating the performance of this sys-
tem in a specific domain. 
37
2 System Design 
In the model, the IE processing is divided into 
three stages: (i) word segmentation and part-of-
speech (POS) tagging; (ii) NE recognition; (iii) 
NER identification. Figure 1 demonstrates a Chi-
nese IE computational model comprised of these 
three stages. Each component in the system cor-
Figure 1. A three-s
responds to a stage. 
tage Chinese IE computa-
In general, the e first stage has 
co
e 
di
al., 2000). 
Th
ication performance for 
N
 defined a hierarchical tax-
on
mentation 
During the implementation, object-oriented de-
sign and programming methods are thoroughly 
tional model. 
 accuracy of th
nsiderable influence on the performance of the 
consequent two stages. It has been demonstrated 
by our experiments (Yao et al, 2002). In order to 
reduce unfavorable influence, we utilize a train-
able approach (Brill, 1995) to automatically gen-
erate effective rules, by which the first compo-
nent can repair different errors caused by word 
segmentation and POS tagging.  
At the second stage, there are two kinds of NE 
constructions to be processed (Yao et al, 2003). 
One is the NEs which involve trigger words; the 
other those without trigger words. For the former 
NEs, a shallow parsing mechanism, i.e., finite-
state cascades (FSC) (Abney, 1996) which are 
automatically constructed by sets of NE recogni-
tion rules, is adopted for reliably identifying dif-
ferent categories of NEs. For the latter NEs, 
however, some special strategies, such as the 
valence constraints of domain verbs, the con-
stituent analysis of NE candidates, the global 
context clues and the analysis for preposition 
objects etc., are designed for identifying them.  
After the recognition for NEs, NER identifica-
tion is performed in the last stage. Because of th
versity and complexity of NERs, at the same 
time, considering portability requirement in the 
identification, we suggest a novel supervised 
machine learning approach called positive and 
negative case-based learning (PNCBL) used in 
this stage (Yao and Uszkoreit, 2005).  
The learning in this approach is a variant of 
memory-based learning (Daelemans et 
e goal of that is to capture valuable informa-
tion from NER and non-NER patterns, which is 
implicated in different features. Because not all 
features we predefine are necessary for each 
NER or non-NER, we should select them by a 
reasonable measure mode. According to the se-
lection criterion we propose - self-similarity, 
which is a quantitative measure for the concen-
trative degree of the same kind of NERs or non-
NERs in the corresponding pattern library, the 
effective feature sets - General-Character Feature 
(GCF) sets for NERs and Individual-Character 
Feature (ICF) sets for non-NERs are built. More-
over, the GCF and ICF feature weighting serve 
as a proportion determination of feature?s degree 
of importance for identifying NERs against non-
NERs. Subsequently, identification thresholds 
can also be determined.  
Therefore, this approach pursues the im-
provement of the identif
ERs by simultaneously learning two opposite 
cases, automatically selecting effective multi-
level linguistic features from a predefined feature 
set for each NER and non-NER, and optimally 
making an identification tradeoff. Further, two 
other strategies, resolving relationship conflicts 
and inferring missing relationships, are also inte-
grated in this stage. 
Considering the actual requirements for do-
main knowledge, we
omy and constructed conceptual relationships 
among Object, Movement and Property concept 
categories under the taxonomy in a lexical sports 
ontology (Yao, 2005). Thus, this ontology can be 
used for the recognition of NEs with special con-
structions - without trigger words, the determina-
tion of NE boundaries, and the provision of fea-
ture values as well as the computation of the se-
mantic distance for two concepts during the iden-
tification of NERs. 
3 System Imple
Word Seg. and 
POS Tag. 
NE  
Recognition 
NER 
Identification 
Error Repair 
Resources 
Texts from Internet or Disk 
Word Seg. and 
POS Tag. 
Resources 
Texts with Word Seg. 
and POS Tags 
NER Identi-
fication  
Resources 
NE-Recognized Texts 
NE Recogni-
tion 
Resources
Lexical 
Ontology 
NER-Identified Texts 
38
used in the system development. In order to 
eriments for testing 
three components. Table 1 shows the experimen-
r f these compo-
avoid repeated development, we integrate other 
application system and resource, e.g., Modern 
Chinese Word Segmentation and POS Tagging 
System (Liu, 2000) and HowNet (Dong and 
Dong, 2000) into the system. Additionally, we 
utilize Prot?g?-2000 (version 1.9) (Stanford 
Medical Informatics, 2003) as a development 
environment for the implementation of lexical 
sports ontology. 
The prototype system CHINERIS has been 
implemented in Java. The system can automati-
cally identify 6 types of NEs1 and 14 types of 
NERs 2  in the sports domain. Furthermore, its 
run-time efficiency is acceptable and the system 
user interfaces are friendly. 
4 Testing and Evaluation 
We have finished three exp
tal esults for the performance o
nents.  
Stage Task (Total ) Ave. Rec. 
(Total) 
Ave. Pre. 
(Total) 
Ave. F-M
Word Seg. 95.08 90.74 92.86 
1st  
POS Tag. 92.39 87.75 90.01 
2nd N  E Ident. 83.38 82.79 83.08 
3rd  NER Ident. 78.50 63.92 70.46 
Table 1. Performance for the Sy  CHINERIS. 
In the first experiment, the training set consists 
of 94 texts including 3473 sentences collected 
                                                
stem
from the soccer matches of the Jie Fang Daily 
(http://www.jfdaily.com/) in 2001. During man-
ual error-correction, we adopted a double-person 
annotation method. After training, we obtain er-
ror repair rules. They can repair at least one error 
in the training corpus. The rules in the rule li-
brary are ranked according to the errors they cor-
rect. The testing set is a separate set that contains 
20 texts including 658 sentences. The texts in the 
 
entity identification which in-
cl
ing learning. They 
ha
1 Personal Name (PN); Date or Time (DT); Location Name 
(LN); Team Name (TN); Competition Title (CT); Personal 
Identity (PI). 
2 Person ? Team (PS_TM); Person ? Competition 
(PS_CP); Person ? City / Province / Country (PS_CPC); 
Person ? Identification (PS_ID); Home Team ? Visiting 
Team (HT_VT); Winning Team ? Losing Team (WT_LT); 
Draw Team ? Draw Team (DT_DT); Team ? Competi-
tion (TM_CP); Team ? City / Province / Country 
(TM_CPC); Identification ? Team (ID_TM); Competition 
? Date (CP_DA); Competition ? Time (CP_TI); Competi-
tion ? Location (CP_LOC); Location ? City / Province / 
Country (LOC_ CPC). 
testing set have been randomly chosen from the 
Jie Fang Daily from May 2002. In the testing, the 
usage of error repair rules with context con-
straints has priority over those without context 
constraints, and the usage of error repair rules for 
word segmentation has priority over those for 
POS tagging. Through experimental observation, 
this processing sequence can ensure that the rules 
repair many more errors. On the other hand, it 
can prevent new errors occurring during the re-
pair of existing errors. The results indicate that 
after the correction, the average F-measure of 
word segmentation has increased from 87.75 % 
to 92.86%; while that of POS tagging has even 
increased from 77.47% to 90.01%. That is to say, 
the performance of both processes has been dis-
tinctly enhanced. 
In the second experiment, we utilize the same 
testing set for the error repair component to 
check the named 
udes regular and special entity constructions. 
The rule sets provided for TN, CT, and PI recog-
nition have 35, 50, and 20 rules respectively. In 
lexical sports ontology, there are more than 350 
domain verbs used for the identification of TN 
with special constructions. Among six NEs, the 
average F-measure of DT, PI, and CT exceeds 
85%. Therefore, it specifies that the identifica-
tion performance of named entities after adding 
the special recognition strategies in this compo-
nent has reached a good level. 
In the third experiment, both pattern libraries 
are established in terms of the annotated texts 
and lexical sports ontology dur
ve 142 (534 NERs) and 98 (572 non-NERs) 
sentence groups respectively. To test the per-
formance of our approach, we randomly choose 
32 sentence groups from the Jie Fang Daily in 
2002 (these sentence groups are out of either 
NER or non-NER pattern library), which em-
body 117 different NER candidates. Table 1 
shows the total average recall, precision, and F-
measure for 14 different NERs by positive and 
negative case-based learning and identification. 
Among 14 types of NERs, the highest total aver-
age F-measure is 95.65 from the relation 
LOC_CPC and the lowest total average F-
measure is 34.09 from TM_CPC. The total aver-
age F-measure is 70.46. In addition, we also 
compared the performance between the total av-
erage recall, precision, and F-measure for all 
NERs only by positive and by positive and nega-
tive case-based learning and identification sepa-
rately. It shows the total average F-measure is 
enhanced from 63.61% to 70.46% as a whole, 
39
due to the adoption of both positive and negative 
cases.  
From the result, we also realize that the selec-
tion of relation features is critical. First, they 
should be selected from multiple linguistic levels, 
e.g
 
ap effective for Chinese named 
y cation in sports domain. 
wo
constraint 
sy
 
sit
Es, identi-
fic
 successful for the sample ap-
pl
 is a part of the COLLATE project un-
01B, which is supported 
ry for Education and Re-
search. 
g Workshop, pages 8-15. Prague, Czech Re-
 Transformation-Based Error-Driven 
W ans, A. Bosch, J. Zavrel, K. Van der Sloot, 
Netherlands. 
DF
Z. wNet. 
K.
T. . Ding, and G. Erbach. 2002. Correcting 
29-36. 
T. 
), 
T.
., morphology, syntax and semantics. Second, 
they should also embody the crucial information 
of Chinese language processing, such as word 
order, the context of words, and particles etc. 
Moreover, the proposed self-similarity is a rea-
sonable measure for selecting GCF and ICF for 
NERs and non-NERs identification respectively. 
5 Conclusion 
This three-stage IE prototype system CHINERIS
is propriate and 
entit  and relation identifi
In the first component, it is a beneficial explo-
ration to develop an error repairer which simul-
taneously enhances the performance of Chinese 
rd segmentation and POS tagging.  
In the second component, we theoretically ex-
tend the original definition of Finite State Auto-
mata (FSA), that is, we use complex 
mbols rather than atomic constraint symbols. 
With this extension, we improve the practicabil-
ity for the FSC mechanism. At the same time, the 
new issue for automatically constructing FSC 
also increases the flexibility of its maintenance. 
In order to improve the NE identification per-
formance, some special strategies for the identi-
fication of NEs without trigger words are added 
in this stage, which cannot be recognized by FSC.
In the third component, automatically select-
ing effectual multi-level linguistic features for 
each NER and non-NER and learning two oppo-
e types of cases simultaneously are two inno-
vative points in the PNCBL approach. 
The lexical sports ontology plays an important 
role in the identification of NEs and NERs, such 
as determination of the boundary of N
ation for NE with special constructons and 
calculation of similarity for the features (e.g. se-
mantic distance). 
The experimental results for the three compo-
nents in the prototype system show that the sys-
tem CHINERIS is
ication. 
Acknowledgement 
This work
der contract no. 01INA
by the German Minist
References 
S. Abney. 1996. Partial Parsing via Finite-State Cas-
cades. In Proceedings of the ESSLLI ?96 Robust 
Parsin
public. 
E. Brill. 1995.
Learning and Natural Language  Processing: A 
Case Study in Part of Speech Tagging. Computa-
tional Linguistics, 21(4): 543-565.  
. Daelem
and A. Vanden Bosch. 2000. TiMBL: Tilburg 
Memory Based Learner, Version 3.0, Reference 
Guide. Technical Report ILK-00-01, ILK, Tilburg 
University. Tilburg, The 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz. 
KI. 2002. COLLATE: Computational Linguistics 
and Language Technology for Real Life Applica-
tions. DFKI, Saarbr?cken, Germany. 
http://collate.dfki. de/. 
 Dong and Q. Dong. 2000. Ho
http://www.keenage.com/zhiwang/e_zhiwang.html. 
 Liu. 2000. Automatic Segmentation and Tagging 
for Chinese Text. The Commercial Press. Beijing, 
China. 
Stanford Medical Informatics. 2003. The Prot?g? On-
tology Editor and Knowledge Acquisition System. 
The School of Medicine, Stanford University. 
Stanford, USA. http://protege.stanford.edu/. 
 Yao, W
Word Segmentation and Part-of-Speech Tagging 
Errors for Chinese Named Entity Recognition. In G. 
Hommel and H. Sheng, editors, The Internet Chal-
lenge: Technology and Applications, pages 
Kluwer Academic Publishers. The Netherlands. 
Yao, W. Ding and G. Erbach. 2003. CHINERS: A 
Chinese Named Entity Recognition System for the 
Sports Domain. In: Proc. of the Second SIGHAN 
Workshop on Chinese Language Processing (ACL 
2003 Workshop), pages 55-62. Sapporo, Japan.  
T. Yao and H. Uszkoreit. 2005. A Novel Machine 
Learning Approach for the Identification of Named 
Entity Relations. In: Proc. of the Workshop on Fea-
ture Engineering for Machine Learning in Natural 
Language Processing (ACL 2005 Workshop
pages 1-8. Michigan, USA. 
 Yao. 2005. A Lexical Ontology for Chinese Infor-
mation Extraction. In M. Sun and Q. Chen, editors, 
Proc. of the 8th National Joint Symposium on 
Computational Linguistics (JSCL-2005), pages 
241-246. Nanjing, China. 
40
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 1?8,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Novel Machine Learning Approach for the Identification of 
Named Entity Relations 
 
 
 
Tianfang Yao Hans Uszkoreit 
Department of Computer Science and 
 Engineering 
Department of Computational Linguistics and 
Phonetics 
Shanghai Jiao Tong University Saarland University 
Shanghai, 200030, China Saarbruecken, 66041, Germany 
yao-tf@cs.sjtu.edu.cn uszkoreit@coli.uni-sb.de 
 
 
 
Abstract 
In this paper, a novel machine learning 
approach for the identification of named 
entity relations (NERs) called positive 
and negative case-based learning 
(PNCBL) is proposed. It pursues the im-
provement of the identification perform-
ance for NERs through simultaneously 
learning two opposite cases and auto-
matically selecting effective multi-level 
linguistic features for NERs and non-
NERs. This approach has been applied to 
the identification of domain-specific and 
cross-sentence NERs for Chinese texts. 
The experimental results have shown that 
the overall average recall, precision, and 
F-measure for 14 NERs are 78.50%, 
63.92% and 70.46% respectively. In addi-
tion, the above F-measure has been en-
hanced from 63.61% to 70.46% due to 
adoption of both positive and negative 
cases. 
1 Introduction 
The investigation for Chinese information extrac-
tion is one of the topics of the project COLLATE 
dedicated to building up the German Competence 
Center for Language Technology. After accom-
plishing the task concerning named entity (NE) 
identification, we go on studying identification 
issues for named entity relations (NERs). As an 
initial step, we define 14 different NERs based on 
six identified NEs in a sports domain based Chi-
nese named entity recognition system (Yao et al, 
2003). In order to learn NERs, we annotate the 
output texts from this system with XML. Mean-
while, the NER annotation is performed by an in-
teractive mode.  
The goal of the learning is to capture valuable 
information from NER and non-NER patterns, 
which is implicated in different features and helps 
us identify NERs and non-NERs. Generally speak-
ing, because not all features we predefine are im-
portant for each NER or non-NER, we should 
distinguish them by a reasonable measure mode. 
According to the selection criterion we propose - 
self-similarity, which is a quantitative measure for 
the concentrative degree of the same kind of NERs 
or non-NERs in the corresponding pattern library, 
the effective feature sets - general-character feature 
(GCF) sets for NERs and individual-character fea-
ture (ICF) sets for non-NERs are built. Moreover, 
the GCF and ICF feature weights serve as a pro 
portion determination of the features? degree of 
importance for identifying NERs against non-
NERs. Subsequently, identification thresholds can 
also be determined. 
 In the NER identification, we may be confronted 
with the problem that an NER candidate in a new 
case matches more than one positive case, or both 
positive and negative cases. In such situations, we 
have to employ a vote to decide which existing 
1
case environment is more similar to the new case. 
In addition, a number of special circumstances 
should be also considered, such as relation conflict 
and relation omission. 
2 Definition of Relations 
An NER may be a modifying / modified, dominat-
ing / dominated, combination, collocation or even 
cross-sentence constituent relationship between 
NEs. Considering the distribution of different 
kinds of NERs, we define 14 different NERs based 
on six identified NEs in the sports domain shown 
in Table 1. 
 
Table 1. NER Category 
 
In order to further indicate the positions of NEs 
in an NER, we define a general frame for the 
above NERs and give the following example using 
this description: 
 
Definition 1 (General Frame of NERs):  
NamedEntityRelation (NamedEntity1, Paragraph-
SentenceNamedEntityNo1; NamedEntity2, Para-
graphSentenceNamedEntityNo2) 
 
Example 1:  
 
?????1??????????????? 
The Guangdong Hongyuan Team defeated the Guangzhou 
Taiyangshen Team by 3: 0 in the guest field. 
 
In the sentence we observe that there exist two 
NERs. According to the general frame, the first 
NER description is HT_VT( ? ? ? ? ? ?
(Guangzhou Taiyangshen Team), 1-1-2; ????
?(Guangdong Hongyuan Team), 1-1-1) and the 
other is WT_LT( ? ? ? ? ? (Guangdong 
                                                          
1 The underlining of Chinese words means that an NE consists of these words.
Hongyuan Team), 1-1-1; ?????(Guangzhou 
Taiyangshen Team), 1-1-2). 
In this example, two NERs represent dominating 
/ dominated and collocation relationships sepa-
rately: namely, the first relation HT_VT gives the 
collocation relationship for the NE ?Guangdong 
Hongyuan Team? and the noun ?guest field?. This 
implies that ?Guangdong Hongyuan Team? is a 
guest team. Adversely, ?Guangzhou Taiyangshen 
Team? is a host team; the second relation WT_LT 
indicates dominating / dominated relationship be-
tween ?Guangdong Hongyuan Team? and 
?Guangzhou Taiyangshen Team? by the verb ?de-
feat?. Therefore, ?Guangdong Hongyuan Team? 
and ?Guangzhou Taiyangshen Team? are the win-
ning and losing team, respectively. 
NER Cate-
gory Explanation 
PS_TM The membership of a person in a sports team. 
PS_CP A person takes part in a sports competition. 
PS_CPC The origin location of a person. 
PS_ID A person and her / his position in a sports team or other occasions. 
HT_VT The home and visiting teams in a sports competition. 
WT_LT The winning and losing team name in a sports match. 
DT_DT The names of two teams which draw a match. 
TM_CP A team participates in a sports competition. 
TM_CPC It indicates where a sports team comes from. 
ID_TM The position of a person employed by a sports team. 
CP_DA The staged date for a sports competition. 
CP_TI The staged time for a sports competition. 
CP_LOC It gives the location where a sports match is held. 
LOC_ CPC The location ownership (LOC belongs to CPC). 
3 Positive and Negative Case-Based 
Learning 
The positive and negative case-based learning 
(PNCBL) belongs to supervised statistical learning 
methods (Nilsson, 1996). Actually, it is a variant of 
memory-based learning (Stanfill and Waltz, 1986; 
Daelemans, 1995; Daelemans et al, 2000). Unlike 
memory-based learning, PNCBL does not simply 
store cases in memory but transforms case forms 
into NER and non-NER patterns. Additionally, it 
stores not only positive cases, but also negative 
ones. Here, it should be clarified that the negative 
case we mean is a case in which two or more NEs 
do not stand in any relationships with each other, 
i.e, they bear non-relationships which are also in-
vestigated objects in which we are interested. 
During the learning, depending on the average 
similarity of features and the self-similarity of 
NERs (also non-NERs), the system automatically 
selects general or individual-character features 
(GCFs or ICFs) to construct a feature set. It also 
determines different feature weights and identifica-
tion thresholds for different NERs or non-NERs. 
Thus, the learning results provide an identification 
references for the forthcoming NER identification. 
3.1 Relation Features 
Relation features, by which we can effectively 
identify different NERs, are defined for capturing 
critical information of the Chinese language. Ac-
cording to the features, we can define NER / non-
2
NER patterns. The following essential factors mo-
tivate our definition for relation features: 
 
? The relation features should be selected 
from multiple linguistic levels, i.e.,  mor-
phology, grammar and semantics (Cardie, 
1996);  
? They can help us to identify NERs using 
positive and negative case-based machine 
learning as their information do not only 
deal with NERs but also with non-NERs; 
and 
? They should embody the crucial information 
of Chinese language processing (Dang et al, 
2002), such as word order, the context of 
words, and particles etc. 
 
There are a total of 13 relation features shown 
in Table 2, which are empirically defined accord-
ing to the above motivations. It should be ex-
plained that in order to distinguish feature names 
from element names of the NER / non-NER pat-
terns, we add a capital letter ?F? in the ending of 
feature names. In addition, a sentence group in 
the following definitions can contain one or mul-
tiple sentences. In other words, a sentence group 
must end with a stop, semicolon, colon, exclama-
tion mark, or question mark. 
 
Feature 
Category Explanation 
SGTF The type of a sentence group in which there exists a relation. 
NESPF The named entities of a relevant relation are located in the same sentence or different sentences. 
NEOF The order of the named entities of a relevant relation. 
NEVPF 
The relative position between the verbs and the named 
entities of a relevant relation. The verbs of a relevant 
relation mean that they occur in a sentence where the 
relation is embedded. 
NECF 
The context of named entities. The context only embod-
ies a word or a character preceding or following the 
current named entity. 
VSPF The verbs are located in the same sentence or different sentences in which there is a relevant relation. 
NEPPOF 
The relative order between parts-of-speech of particles 
and named entities. The particles occur within the 
sentences where the relation is embedded. 
NEPF The parts-of-speech of the named entities of a relevant relation. 
NECPF The parts-of-speech of the context for the named enti-ties associated with a relation. 
SPF The sequence of parts-of-speech for all sentence con-stituents within a relation range. 
VVF The valence expression of verbs in the sentence(s) where there is a relation embedded. 
NECTF The concepts of the named entities of a relevant relation from HowNet (Dong and Dong, 2000). 
VCTF The concepts of the verbs of a relevant relation from HowNet. 
 
Table 2. Feature Category 
 
In 13 features, three features (NECF, NECPF 
and NEPF) belong to morphological features, three 
features (NEOF, SPF and SGTF) are grammatical 
features, four features (NEPPOF, NESPF, NEVPF 
and VSPF) are associated with not only morphol-
ogy but also grammar, and three features (NECTF, 
VCTF and VVF) are semantic features.  
Every feature describes one or more properties 
of a relation. Through the feature similarity calcu-
lation, the quantitative similarity for two relations 
can be obtained, so that we can further determine 
whether a candidate relation is a real relation. 
Therefore, the feature definition plays an important 
role for the relation identification. For instance, 
NECF can capture the noun ?? (the guest field, 
it means that the guest team attends a competition 
in the host team?s residence.) and also determine 
that the closest NE by this noun is ????? 
(the Guangdong Hongyuan Team). On the other 
hand, NEOF can fix the sequence of two relation-
related NEs. Thus, another NE ?????? (the 
Guangzhou Taiyangshen Team) is determined. 
Therefore, these two features reflect the properties 
of the relation HT_VT. 
3.2 Relation and Non-Relation Patterns 
A relation pattern describes the relationships be-
tween an NER and its features. In other words, it 
depicts the linguistic environment in which NERs 
exist. 
 
Definition 2 (Relation Pattern): A relation pat-
tern (RP) is defined as a 14-tuple: RP = (NO, RE, 
SC, SGT, NE, NEC, VERB, PAR, NEP, NECP, SP, 
VV, NECT, VCT) where NO represents the num-
ber of a RP; RE is a finite set of relation expres-
sions; SC is a finite set for the words in the 
sentence group except for the words related to 
named entities; SGT is a sentence group type; NE 
is a finite set for named entities in the sentence 
group; NEC is a finite set that embodies the con-
text of named entities; VERB is a finite set that in-
cludes the sequence numbers of verbs and 
corresponding verbs; NEP is a finite set of named 
entities and their POS tags; NECP is a finite set 
which contains the POS tags of the context for 
named entities; SP is a finite set in which there are 
the sequence numbers as well as corresponding 
POS tags and named entity numbers in a sentence 
group; VV is a finite set comprehending the posi-
3
tion of verbs in a sentence and its valence con-
straints from Lexical Sports Ontology which is 
developed by us; NECT is a finite set that has the 
concepts of named entities in a sentence group; and 
VCT is a finite set which gives the concepts of 
verbs in a sentence group. 
 
Example 2: 
 
????????????????????????
????????????????????????
????????????????????????
?????? 
According to the news from Xinhua News Agency Beijing on 
March 26th: National Football Tournament (the First B 
League) today held five competitions of the second round, 
The Guangdong Hongyuan Team defeats the Guangzhou 
Taiyangshen Team by 3: 0 in the guest field, becoming the 
only team to win both matches, and temporarily occupying 
the first place of the entire competition. 
 
Relation Pattern: 
NO = 34; 
RE = {(CP_DA, NE1-3, NE1-2), (CP_TI, NE1-3, NE1-4), ?, 
(WT_LT, NE2-1, NE2-2)} 
SC = {(1, ?, according_to, Empty, AccordingTo), (2, ??
? , Xinhua/Xinhua_News_agency, Empty, institu-
tion/news/ProperName/China), ?, (42, ? , ., Empty, 
{punc})}; 
SGT = multi-sentences; 
NE = {(NE1-1, 3, LN, {(1, ??)}), (NE1-2, 4, Date, {(1, ?), 
(2, ?), (3, ??), (4, ?)}), ..., (NE2-2, 26, TN, {(1, ??), 
(2, ???), (3, ?)})}; 
NEC = {(NE1-1, ???,?), (NE1-2, ??, ?), ..., (NE2-2, 
??, ?) }; 
VERB = {(8, ??), (25, ??), ..., (39, ?)} 
PAR = {(1, ?), (9, ?), ..., (38, ?)}; 
NEP = {(NE1-1, {(1, N5)}), (NE1-2, {(1, M), (2, N), (3, M), 
(4, N)}), ..., (NE2-2, {(1, N5), (2, N), (3, N)})}; 
NECP = {(NE1-1, N, M), (NE1-2, N5, N), ?, (NE2-2, V, 
W)}; 
SP = {(1, P), (2, N), (3, NE1-1), ..., (42, W)}; 
VV = {(V_8, {Agent|fact/compete|CT, -Time|time|DT}), 
(V_25, {Agent|human/mass|TN, Patient|human/mass|TN}),..., 
(V_39, {Agent|human/sport|PN, Agent|human/mass|TN})}; 
NECT = {(NE1-1, place/capital/ProperName/China), (NE1-2, 
Empty+celestial/unit/time+Empty+ celestial/time/time/ 
morning), ?, (NE2-2, place/city/ProperName/China+ 
Empty+community/human/mass)}; 
VCT = {(V_8, GoForward/GoOn/Vgoingon), (V_25, de-
feat), ?, (V_39, reside/situated)} 
 
Analogous to the definition of the relation pat-
tern, a non-relation pattern is defined as follows:  
 
Definition 3 (Non-Relation Pattern): A non-
relation pattern (NRP) is also defined as a 14-tuple: 
NRP = (NO, NRE, SC, SGT, NE, NEC, VERB, 
PAR, NEP, NECP, SP, VV, NECT, VCT), where 
NRE is a finite set of non-relation expressions 
which specify the nonexistent relations in a sen-
tence group. The definitions of the other elements 
are the same as the ones in the relation pattern. For 
example, if we build an NRP for the above sen-
tence group in Example 2, the NRE is listed in the 
following: 
 
NRE = {(CP_LOC, NE1-3, NE1-1), (TM_CPC, NE2-1, 
NE1-1), ..., (DT_DT, NE2-1, NE2-2)} 
 
In this sentence group, the named entity (CT) ?
??????? (National Football Tournament 
(the First B League)) does not bear the relation 
CP_LOC to the named entity (LN) ?? (Beijing). 
This LN only indicates the release location of the 
news from Xinhua News Agency. 
As supporting means, the non-NER patterns also 
play an important role, because in the NER pattern 
library we collect sentence groups in which the 
NER exists. If a sentence group only includes non-
NERs, obviously, it is excluded from the NER pat-
tern library. Thus the impact of positive cases can-
not replace the impact of negative cases. With the 
help of non-NER patterns, we can remove misiden-
tified non-NERs and enhance the precision of NER 
identification. 
3.3 Similarity Calculation 
In the learning, the similarity calculation is a ker-
nel measure for feature selection.  
 
Definition 4 (Self-Similarity): The self-similarity 
of a kind of NERs or non-NERs in the correspond-
ing library can be used to measure the concentra-
tive degree of this kind of relations or non-relations. 
The value of the self-similarity is between 0 and 1. 
If the self-similarity value of a kind of relation or 
non-relation is close to 1, we can say that the con-
centrative degree of this kind of relation or non-
relation is very ?tight?. Conversely, the concentra-
tive degree of that is very ?loose?. 
The calculation of the self-similarity for the 
same kind of NERs is equal to the calculation for 
the average similarity of the corresponding relation 
features. Suppose R(i) is a defined NER in the 
NER set (1 ? i ? 14). The average similarity for 
this kind of NERs is defined as follows:  
 
                                               ? Sim (R(i)j, R(i)k)  
 1? j, k ? m; j ? k 
Simaverage(R(i)) =  ???????????                    (1)                                         
                                          Sumrelation_pair(R(i)j, R(i)k) 
 
where Sim (R(i)j, R(i)k) denotes the relation simi-
larity between the same kind of relations, R(i)j and 
4
R(i)k. 1 ? j, k ? m, j ? k; m is the total number of 
the relation R(i) in the NER pattern library. The 
calculation of Sim(R(i)j, R(i)k) depends on differ-
ent features. Sumrelation_pair(R(i)j, R(i)k) is the sum of 
calculated relation pair number. They can be calcu-
lated using the following formulas: 
 
                                   Sumf 
                               ? Sim (R(i)j, R(i)k) (ft)  
                                               t = 1 
Sim (R(i)j, R(i)k ) =  ???????????                (2)                                                            Sim                                   f(s)                                           Sumf
 
                                                                     1               m = 2 
      
Sumrelation_pair(R(i)j, R(i)k)  =              m !                              (3) 
                                                             ?????      m > 2                                         where R(i) is a defined relation in the NER set (1 ? 
i ? 14); n is the size of selected features, 1 ? s, t ? n; 
and 
                                                             (m-2) ! * 2 ! 
 
In the formula (2), ft is a feature in the feature 
set (1 ? t ? 13). Sumf is the total number of fea-
tures. The calculation formulas of Sim (R(i)j, R(i)k) 
(ft) depend on different features. For example, if ft 
is equal to NECF, Sim (R(i)j, R(i)k) (ft) is shown as 
follows: 
 
1  if all contexts of named  
entities for two relations 
                                                                are the same 
0.75 if only a  preceding or  
following context is not  
                                                                the same                                                         
Sim (R(i)
Sim (X(i)j, X(i)k) (NECF)  =       0.5      if two preceding and / or  
following contexts are 
                                                                not the same 
0.25     if three preceding and / or 
following contexts are 
                                               not the same 
0       if all contexts of named  
entities for two relations 
are not the same 
                                                                      (4) 
 
Notice that the similarity calculation for non-
NERs is the same as the above calculations.  
Before describing the learning algorithm, we 
want to define some fundamental conceptions re-
lated to the algorithm as follows: 
 
Definition 5 (General-Character Feature): If the 
average similarity value of a feature in a relation is 
greater than or equal to the self-similarity of this 
relation, it is called a General-Character Feature 
(GCF). This feature reflects a common characteris-
tic of this kind of relation. 
 
Definition 6 (Individual-Character Feature): An 
Individual-Character Feature (ICF) means its aver-
age similarity value in a relation is less than or 
equal to the self-similarity of this relation. This 
feature depicts an individual property of this kind 
of relation. 
 
Definition 7 (Feature Weight): The weight of a 
selected feature (GCF or ICF) denotes the impor-
tant degree of the feature in GCF or ICF set. It is 
used for the similarity calculation of relations or 
non-relations during relation identification.  
 
averagef(s)(R(i)) 
w(R(i)) = ?????????                                (5)                                         
                                        n 
                                       ? Simaveragef(t)(R(i)) 
                                      t = 1 
 
 
                                     ? Sim (R(i)j, R(i)k) (f(s)) 
                                      1? j, k ? m; j ? k 
Simaveragef(s)(R(i)) =  ???????????                 (6)                                        
                                              Sumrelation_pair(R(i)j, R(i)k) 
 
j, R(i)k) (f(s)) computes the feature simi-
larity of  the feature f(s) between same kinds of 
relations, R(i)j and R(i)k. 1 ? j, k ? m, j ? k; m is 
the total number of the relation R(i) in the NER 
pattern library. Sumrelation_pair(R(i)j, R(i)k) is the sum 
of calculated relation pair numbers, which can be 
calculated by the formula (3). 
 
Definition 8 (Identification Threshold): If a can-
didate relation is regarded as a relation in the rela-
tion pattern library, the identification threshold of 
this relation indicates the minimal similarity value 
between them. It is calculated by the average of the 
sum of average similarity values for selected fea-
tures: 
 
                                n 
                                        ? Simaveragef(t)(R(i)) 
                                       t  = 1                                         
            IdenThrh(R(i)) =  ????????                            (7) 
                                                        n              
       
where n is the size of selected features, 1 ? t ? n. 
Finally, the PNCBL algorithm is described as 
follows: 
1) Input annotated texts; 
2) Transform XML format of texts into internal 
data format; 
3) Build NER and non-NER patterns; 
4) Store both types of patterns in hash tables 
and construct indexes for them; 
5
5) Compute the average similarity for features 
and self-similarity for NERs and non-NERs; 
6) Select GCFs and ICFs for NERs and non-
NERs respectively; 
7) Calculate weights for selected features;  
8) Decide identification thresholds for every 
NER and non-NER;  
9) Store the above learning results.  
4 Relation Identification  
Our approach to NER identification is based on 
PNCBL, it can utilize the outcome of learning for 
further identifying NERs and removing non-NERs. 
4.1 Optimal Identification Tradeoff 
During the NER identification, the GCFs of NER 
candidates match those of all of the same kind of 
NERs in the NER pattern library. Likewise, the 
ICFs of NER candidates compare to those of non-
NERs in the non-NER pattern library. The comput-
ing formulas in this procedure are listed as follows: 
 
                
Sum(GCF)
i
Sim (R(i)can, R(i)j1 ) =  ? { wi (GCFk1) * Sim (R(i)can, R(i)j1 ) (GCFk1) }   
                     k1 = 1                
and                                                                     (8) 
                              
Sum(ICF)
i
Sim (R(i)can, NR(i)j2 ) =  ? { wi (ICFk2) * Sim (R(i)can, NR(i)j2 ) (ICFk2) }  
                        k2 = 1                
                                                                           (9) 
where R(i) represents the NERi, and NR(i) ex-
presses the non-NERi, 1? i ? 14. R(i)can is defined 
as a NERi candidate. R(i)j1 and NR(i)j2 are the j1-th 
NERi in the NER pattern library and the j2-th non-
NERi in the non-NER pattern library. 1 ? j1 ? Sum 
(R(i)) and 1 ? j2 ? Sum (NR(i)). Sum (R(i)) and 
Sum (NR(i)) are the total number of R(i) in the 
NER pattern library and that of NR(i) in non-NER 
pattern library respectively. wi (GCFk1) and wi 
(ICFk2) mean the weight of the k1-th GCF for the 
NERi and that of the k2-th ICF for the non-NERi. 
Sum (GCF)i and Sum (ICF)i are the total number of 
GCF for NERi and that of ICF for non-NERi sepa-
rately. 
In matching results, we find that sometimes the 
similarity values of a number of NERs or non-
NERs matched with NER candidates are all more 
than the identification threshold. Thus, we have to 
utilize a voting method to achieve an identification 
tradeoff in our approach. For an optimal tradeoff, 
we consider the final identification performance in 
two aspects: i.e., recall and precision. In order to 
enhance recall, as many correct NERs should be 
captured as possible; on the other hand, in order to 
increase precision, misidentified non-NERs should 
be removed as accurately as possible.  
 The voting refers to the similarity calculation 
results between an NER candidate and NER / non-
NER patterns. It pays special attention to circum-
stances in which both results are very close. If this 
happens, it exploits multiple calculation results to 
measure and arrive at a final decision. Additionally, 
notice that the impact of non-NER patterns is to 
restrict possible misidentified non-NERs. On the 
other hand, the voting assigns different thresholds 
to different NER candidates (e.g. HT_VT, WT_LT, 
and DT_DT or other NERs). Because the former 
three NERs have the same kind of NEs, the identi-
fication for these NERs is more difficult than for 
others. Thus, when voting, the corresponding 
threshold should be set more strictly. 
4.2 Resolving NER Conflicts 
In fact, although the voting is able to use similarity 
computing results for yielding an optimal tradeoff, 
there still remain some problems to be resolved. 
The relation conflict is one of the problems, which 
means that contradictory NERs occur in identifica-
tion results. For example:  
(i) The same kind of relations with different ar-
gument position: e.g., the relations HT_VT,  
 
HT_VT(ne1, no1; ne2, no2) and HT_VT(ne2, no2; ne1, no1) 
occur in an identification result at the same time. 
 
(ii)  The different kinds of relations with same or 
different argument positions: e.g., the relations 
WT_LT and DT_DT,  
 
WT_LT(ne1, no1; ne2, no2) and DT_DT(ne1, no1; ne2, no2) 
appear simultaneously in an identification result. 
 
The reason for a relation conflict lies in the si-
multaneous and successful matching of a pair of 
NER candidates whose NEs are the same kind. 
They do not compare and distinguish themselves 
further. Considering the impact of NER and non-
NER patterns, we organize the conditions to re-
move one of the relations, which has lower average 
similarity value with NER patterns or higher aver-
age similarity value with non-NER patterns.  
4.3 Inferring Missing NERs 
6
Due to a variety of reasons, some relations that 
should appear in an identification result may be 
missing. However, we can utilize some of the iden-
tified NERs to infer them. Of course, the prerequi-
site of the inference is that we suppose identified 
NERs are correct and non-contradictory. For all 
identified NERs, we should first examine whether 
they contain missing NERs. After determining the 
type of missing NERs, we may infer them - con-
taining the relation name and its arguments. For 
instance, in an identification result, two NERs are: 
 
PS_ID (ne1, no1; ne2, no2) and PS_TM (ne1, no1; ne3, no3) 
 
In the above NER expressions, ne1 is a personal 
name, ne2 is a personal identity, and ne3 is a team 
name, because if a person occupies a position, i.e., 
he / she has a corresponding identity in a sports 
team, that means the position or identity belongs to 
this sports team. Accordingly, we can infer the fol-
lowing NER: 
 
ID_TM (ne2, no2; ne3, no3) 
5 Experimental Results and Evaluation 
The main resources used for learning and identifi-
cation are NER and non-NER patterns. Before 
learning, the texts from the Jie Fang Daily2 in 2001 
were annotated based on the NE identification. 
During learning, both pattern libraries are estab-
lished in terms of the annotated texts and Lexical 
Sports Ontology. They have 142 (534 NERs) and 
98 (572 non-NERs) sentence groups, respectively.  
To test the performance of our approach, we 
randomly choose 32 sentence groups from the Jie 
Fang Daily in 2002, which embody 117 different 
NER candidates.  
For evaluating the effects of negative cases, we 
made two experiments. Table 3 shows the average 
and total average recall, precision, and F-measure 
for the identification of 14 NERs only by positive 
case-based learning. Table 4 demonstrates those by 
PNCBL. Comparing the experimental results, 
among 14 NERs, the F-measure values of the 
seven NERs (PS_ID, ID_TM, CP_TI, WT_LT, 
PS_CP, CP_DA, and DT_DT) in Table 4 are 
higher than those of corresponding NERs in Table 
3; the F-measure values of three NERs (LOC_CPC, 
TM_CP, and PS_CP) have no variation; but the F-
measure values of other four NERs (PS_TM, 
                                                          
2 This is a local newspaper in Shanghai, China.
CP_LOC, TM_CPC, and HT_VT) in Table 4 are 
lower than those of corresponding NERs in Table 3. 
This shows the performances for half of NERs are 
improved due to the adoption of both positive and 
negative cases. Moreover, the total average F-
measure is enhanced from 63.61% to 70.46% as a 
whole. 
 
Relation 
Type 
Average 
Recall 
Average 
Precision 
Average 
F-measure
LOC_CPC 100 91.67 95.65 
TM_CP 100 87.50 93.33 
PS_ID 100 84.62 91.67 
PS_TM 100 72.73 84.21 
CP_LOC 88.89 69.70 78.13 
ID_TM 90.91 66.67 76.93 
CP_TI 83.33 71.43 76.92 
PS_CP 60 75 66.67 
TM_CPC 100 42.50 59.65 
HT_VT 71.43 38.46 50 
WT_LT 80 30.77 44.45 
PS_CPC 33.33 66.67 44.44 
CP_DA 0 0 0 
DT_DT 0 0 0 
Total Ave. 71.99 56.98 63.61 
  
Table 3:  Identification Performance for 14 NERs 
only by Positive Case-Based Learning 
 
Relation 
Type 
Average 
Recall 
Average 
Precision 
Average 
F-measure
LOC_CPC 100 91.67 95.65 
TM_CP 100 87.50 93.33 
CP_TI 100 75 85.71 
PS_CPC 100 68.75 81.48 
ID_TM 90.91 68.19 77.93 
PS_ID 72.22 81.67 76.65 
CP_LOC 88.89 66.67 76.19 
PS_TM 80 65 71.72 
CP_DA 100 50 66.67 
DT_DT 66.67 66.67 66.67 
PS_CP 60 75 66.67 
WT_LT 60 37.50 46.15 
HT_VT 42.86 30 35.30 
TM_CPC 37.50 31.25 34.09 
Total Ave. 78.50 63.92 70.46 
 
Table 4:  Identification Performance  
for 14 NERs by PNCBL 
 
Finally, we have to acknowledge that it is diffi-
cult to compare the performance of our method to 
others because the experimental conditions and 
corpus domains of other NER identification efforts 
are quite different from ours. Nevertheless, we 
would like to use the performance of Chinese NER 
identification using memory-based learning (MBL) 
(Zhang and Zhou, 2000) for a comparison with our 
approach in Table 5. In the table, we select similar 
NERs in our domain to correspond to the three 
types of the relations (employee-of, product-of, and 
location-of). From the table we can deduce that the 
7
identification performance of relations for PNCBL 
is roughly comparable to that of the MBL. 
 
Method Relation Type Recall Precision F-measure
employee-of 75.60 92.30 83.12 
product-of 56.20 87.10 68.32 MBL&I 
location-of 67.20 75.60 71.15 
PS_TM 
PS_CP 
PS_ID 
80 
60 
72.22 
65 
75 
81.67 
71.72 
66.67 
76.65 
ID_TM 
TM_CP 
90.91 
100 
68.19 
87.50 
77.93 
93.33 PNCBL&I 
CP_LOC 
PS_CPC 
TM_CPC 
88.89 
100 
37.50 
66.67 
68.75 
31.25 
76.19 
81.48 
34.09 
 
Table 5:  Performances for Relation Identification  
(PNCBL&I vs. MBL&I) 
6 Conclusion 
In this paper, we propose a novel machine learning 
and identification approach PNCBL&I. This ap-
proach exhibits the following advantages: (i) The 
defined negative cases are used to improve the 
NER identification performance as compared to 
only using positive cases;  (ii) All of the tasks, 
building of NER and non-NER patterns, feature 
selection, feature weighting and identification 
threshold determination, are automatically com-
pleted. It is able to adapt the variation of NER and 
non-NER pattern library; (iii) The information 
provided by the relation features deals with multi-
ple linguistic levels, depicts both NER and non-
NER patterns, as well as satisfies the requirement 
of Chinese language processing; (iv) Self-
similarity is a reasonable measure for the concen-
trative degree of the same kind of NERs or non-
NERs, which can be used to select general-
character and individual-character features for 
NERs and non-NERs respectively; (v) The strate-
gies used for achieving an optimal NER identifica-
tion tradeoff, resolving NER conflicts, and 
inferring missing NERs can further improve the 
performance for NER identification; (vi) It can be 
applied to sentence groups containing multiple sen-
tences. Thus identified NERs are allowed to cross 
sentences boundaries.  
The experimental results have shown that the 
method is appropriate and effective for improving 
the identification performance of NERs in Chinese. 
 
Acknowledgement 
This work is a part of the COLLATE project 
under contract no. 01INA01B, which is supported 
by the German Ministry for Education and Re-
search. 
References 
C. Cardie. 1996. Automating Feature Set Selection for 
Case-Based Learning of Linguistic Knowledge. In 
Proc. of the Conference on Empirical Methods in 
Natural Language Processing. University of Pennsyl-
vania, Philadelphia, USA. 
W. Daelemans. 1995. Memory-based lexical acquisition 
and processing. In P. Steffens, editor, Machine 
Translations and the Lexicon, Lecture Notes in Arti-
ficial Intelligence, pages 85-98. Springer Verlag. 
Berlin, Germany. 
W. Daelemans, A. Bosch, J. Zavrel, K. Van der Sloot, 
and A. Vanden Bosch. 2000. TiMBL: Tilburg Mem-
ory Based Learner, Version 3.0, Reference Guide. 
Technical Report ILK-00-01, ILK, Tilburg Univer-
sity. Tilburg, The Netherlands. 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz. 
H. Dang, C. Chia, M. Palmer and F. Chiou. 2002. Sim-
ple Features for Chinese Word Sence Disambigua-
tion. In Proc. of the 19th International Conference on 
Computational Linguistics (COLING 2002), pages 
204-210. Taipei, Taiwan. 
Z. Dong and Q. Dong. 2000. HowNet. 
http://www.keenage.com/zhiwang/e_zhiwang.html. 
N. Nilsson. 1996. Introduction to Machine Learning: An 
Early Draft of a Proposed Textbook. Pages 175-188. 
http://robotics.stanford.edu/people/nilsson/mlbook.ht
ml. 
C. Stanfill and D. Waltz. 1986. Toward memory-based 
reasoning. Communications of the ACM, Vol.29, 
No.12, pages 1213-1228. 
T. Yao, W. Ding and G. Erbach. 2003. CHINERS: A 
Chinese Named Entity Recognition System for the 
Sports Domain. In: Proc. of the Second SIGHAN 
Workshop on Chinese Language Processing (ACL 
2003 Workshop), pages 55-62. Sapporo, Japan.  
Y. Zhang and J. Zhou. 2000. A trainable method for 
extracting Chinese entity names and their relations. 
In Proc. of the Second Chinese Language Processing 
Workshop (ACL 2000 Workshop), pages 66-72. 
Hongkong, China. 
8
CHINERS: A Chinese Named Entity Recognition System
for the Sports Domain
Tianfang Yao    Wei Ding    Gregor Erbach
Department of Computational Linguistics
Saarland University
Germany
yao@coli.uni-sb.de  wding@mpi-sb.mpg.de
gor@acm.org

Abstract
In the investigation for Chinese named
entity (NE) recognition, we are con-
fronted with two principal challenges.
One is how to ensure the quality of word
segmentation and Part-of-Speech (POS)
tagging, because its consequence has an
adverse impact on the performance of NE
recognition. Another is how to flexibly,
reliably and accurately recognize NEs. In
order to cope with the challenges, we pro-
pose a system architecture which is di-
vided into two phases. In the first phase,
we should reduce word segmentation and
POS tagging errors leading to the second
phase as much as possible. For this pur-
pose, we utilize machine learning tech-
niques to repair such errors. In the second
phase, we design Finite State Cascades
(FSC) which can be automatically con-
structed depending on the recognition rule
sets as a shallow parser for the recogni-
tion of NEs. The advantages of that are
reliable, accurate and easy to do mainte-
nance for FSC. Additionally, to recognize
special NEs, we work out the correspond-
ing strategies to enhance the correctness
of the recognition. The experimental
evaluation of the system has shown that
the total average recall and precision for
six types of NEs are 83% and 85% re-
spectively. Therefore, the system architec-
ture is reasonable and effective.
1 Introduction
The research for Chinese information extraction is
one of the topics in the project COLLATE1 (Com-
putational Linguistics and Language Technology
for Real World Applications). The main motivation
is to investigate the strategies for information ex-
traction for such language, especially in some spe-
cial linguistic phenomena, to build a reasonable
information extraction model and to implement an
application system. Chinese Named Entity Recog-
nition System (CHINERS) is a component of Chi-
nese information extraction system which is being
developed. CHINERS is mainly based on machine
learning and shallow parsing techniques. We adopt
football competition news as our corpus, because
there exist a variety of named entities (NEs) and
relations in the news. Among the NEs we select six
of them as the recognized objects, that is, personal
name (PN), date or time (DT), location name (LN),
team name (TN), competition title (CT) and per-
sonal identity (PI). e.g.    (Mo Chenyue), 

 (Carlos); 	 
 	  (Sept. 19), 
(this Friday),   (former 70 minutes); Active Learning Based Corpus Annotation 
Hongyan Song1 and Tianfang Yao2
Shanghai Jiao Tong University 
Department of Computer Science and Engineering 
Shanghai, China 200240 
1songhongyan@sjtu.org 
2yao-tf@cs.sjtu.edu.cn 
Abstract
Opinion Mining aims to automatically acquire 
useful opinioned information and knowledge 
in subjective texts. Research of Chinese Opin-
ioned Mining requires the support of annotated 
corpus for Chinese opinioned-subjective texts. 
To facilitate the work of corpus annotators, 
this paper implements an active learning based 
annotation tool for Chinese opinioned ele-
ments which can identify topic, sentiment, and 
opinion holder in a sentence automatically. 
1 Introduction 
Opinion Mining is a novel and important re-
search topic, aiming to automatically acquire 
useful opinioned information and knowledge in 
subjective texts (Liu et al 2008). This technique 
has wide and many real world applications, such 
as e-commerce, business intelligence, informa-
tion monitoring, public opinion poll, e-learning, 
newspaper and publication compilation, and 
business management. For instance, a typical 
opinion mining system produces statistical re-
sults from online product reviews, which can be 
used by potential customers when deciding 
which model to choose, by manufacturers to find 
out the possible areas of improvement, and by 
dealers for sales plan evaluation (Yao et al 
2008).
   According to Kim and Hovy (2004), an opin-
ion is composed of four parts, namely, topic, 
holder, sentiment, and claim, in which the holder 
expresses the claim including positive or nega-
tive sentiment towards the topic. For example, in 
the sentence I like this car, I is the holder, like is 
the positive sentiment, car is the topic, and the 
whole sentence is the claim. 
   Research on Chinese opinion mining technol-
ogy requires the support of annotated corpus for 
Chinese opinioned-subjective text. Since the cor-
pus includes deep level information related to 
word segmentation, part-of-speech, syntax, se-
mantics, opinioned elements, and some other 
information, the finished annotation is very com-
plicated. Hence, it is necessary to develop an 
automatic tool to facilitate the work of annotators 
so that the efficiency and accuracy of annotation 
can be improved. 
   When developing the automatic annotation tool, 
we find it is most difficult for the tool to annotate 
opinioned elements automatically. Because 
unlike other elements such as part-of-speech, and 
dependency relationship that needed to be anno-
tated in the corpus, there is no available tool that 
can identify opinioned elements automatically. 
Special classifiers should be constructed to solve 
this problem. 
   In traditional supervised learning tasks, train-
ing process consumes all the available annotated 
training instances, so a classifier with high classi-
fication accuracy might be constructed. When 
training a classifier for opinioned elements, it is 
very expensive and time-consuming to get anno-
tated instances. On the other hand, unannotated 
instances are abundant in this case, because all 
the texts in the corpus can be regarded as unan-
notated instances before being annotated. This 
scenario is very appropriate for active learning 
application. An active learning algorithm picks 
up the instances which will improve the per-
formance of the classifier to the largest extent 
into the training set, and often produce classifier 
with higher accuracy using less training instances. 
   Active learning algorithm is featured with 
smaller training set size, less influence from un-
balanced training data and better classification 
performance comparing to classical learning al-
gorithm. This paper experimentally demonstrates 
the validity of active learning algorithm when 
used for opinioned elements identification and 
proposes a computational method for overall sys-
tem performance evaluation which consists of F-
measure, training time, and number of training 
instances.
2 Related Work 
Common active learning algorithms can be di-
vided into two classes, membership query and 
selective sampling (Dagan and Engelson, 1995).
For membership query, algorithm constructs 
learning instances by itself according to the 
knowledge learnt, and submits the instances for 
human processing (Angluin, 1988) (Sammut and 
Banerji, 1986) (Shapiro, 1982). Although this 
method has proved high learning efficiency (Da-
gan and Engelson, 1995), it can be applied in 
fewer scenarios. Since constructing meaningful 
training instance without the knowledge of target 
concept is rather difficult. As to selective sam-
pling, algorithm picks up training instances 
which can improve the performance of the classi-
fier to the largest extent from a large variety of 
available instances. Algorithm in this class can 
be further divided into stream-based algorithm 
and pool-based algorithm according to how in-
stances are saved (Long et al 2008). For stream-
based algorithm (Engelson and Dagon, 1999) 
(Freund et al 1997), unannotated instances are 
submitted to the system successively. All the 
instances not selected by the algorithms will be 
discarded. As to pool-based algorithm (Muslea et 
al, 2006) (McCallum and Nigam, 1998) (Lewis
and Gail, 1994), the algorithm choose the most 
appropriate training instances from all the avail-
able instances. Instance not selected might have 
chance to be picked up in the next round. Though 
its computational complexity is higher, selective 
sampling is widely used as an active learning 
method for no prior knowledge of the target con-
cept is required. 
Although much research has been made in 
the field, we found no case which deals with 
multi-classification problem in active learning. 
Besides, there is no available method to evaluate 
the performance of active learning in information 
extraction.
3 Active Learning Based Corpus Anno-
tation
3.1 System Structure 
The pool-based active learning algorithm is 
composed of two main parts: a learning engine 
and a selecting engine (Figure 1). The learning 
engine uses instances in the training set to im-
prove the performance of the classifier. The se-
lecting engine picks up unannotated instances 
according to preset rules, submits these instances 
for human annotation, and incorporates these 
instances into the training set after the annotation 
is completed. The learning engine and the select-
ing engine work in turns. The performance of the 
classifier tends to improve with the increasing of 
the training set size. When the preset condition is 
met, the training process will finish. 
Figure 1 System Workflow 
For our active learning based annotation tool, 
the workflow is as follows. 
1. Convert raw texts into the format which 
the algorithm can deal with. 
2. Selecting engine picks up instances which 
are expected to improve the performance of the 
classifier to the largest extent. 
3. Annotate these instances manually. 
4. Learning engine incorporate these anno-
tated instances into the training set, and use the 
new training set to train the classifier. 
5. Find out whether the performance of the 
classifier satisfies the preset standard. If not, go 
to step 2. 
6. Use the classifier to identify the opinioned 
element in the unannotated dataset. 
7. Convert the result into the required format. 
3.2 Learning Engine 
The learning engine maintains the classifier by 
iteratively training classifiers with new training 
sets. The classifier adopted determines the up 
limit of the system performance. We use Support 
Vector Machine (SVM) (Vapnik, 1995) (Boser et
al, 1992) (Chang and Lin, 1992) as the classifier 
for our system for its high generalization per-
formance even with feature vectors of high di-
mension and its ability to manage kernel func-
tions that map input data to higher dimensional 
space without increasing computational com-
plexity. 
3.3 Selecting Engine 
In our system, selecting engine picks up in-
stances for human annotation, and puts the anno-
tated instance into the training set. The strategy 
adopted when selecting training instance is criti-
cal to the overall performance of the active learn-
ing algorithm. A good strategy will more likely 
to produce a classifier with high accuracy from 
less training instances. 
The strategy we adopted here is to choose the 
instances which the classifier is most unsure 
about which class they belong to. For a linear bi-
classification SVM, these instances are the ones 
closest to the separating hyper plane. That means, 
the selecting engine will choose training in-
stances according to their geometric distances to 
the hyper plane. The instance with least distance 
will be selected as the next instance to be added 
into the training set while the other instances will 
be saved for future reference. 
The computational complexity of getting the 
distance between an instance and the hyper plane 
is low. However, this method can not be applied 
to SVM with non-linear kernel for geometric 
distances are meaningless in these cases. We use 
radial basis function, which is non-linear, as the 
kernel function in our system for it outperforms 
linear kernel in the experiment. Hence, we must 
find another method to pick up training instances. 
Non-linear SVM decides the class an in-
stance belongs to according to its decision func-
tion value.
S
( ) ( )
s
s s s
x
y x y K x x bD
?
 ??&
& & &               (1)       
The instance will be classified into one cer-
tain class if , or the other class 
if . However, it will be difficult to clas-
sify the instance according to SVM theory 
if
( ) 0y x !&
( ) 0y x &
( ) 0y x  & . Hence, we may deduce that SVM is 
most unsure when classifying an instance with 
least absolute decision function value. 
We define the Predict Value (PV) as the 
value based on which selecting engine picks up 
training instances. 
For bi-classification SVM, we have PV 
equals to the absolute decision function value, 
namely, 
PV( ) ( )x
&
y x
&
                                       (2) 
Instances with the minimum PV will be selected 
into the training set before other instances. 
For example, if we want to identify all the 
topics in the sentence,  
I like this car very much, but the price is a little 
bit too high. 
????????????????
The PV of each instance in the sentences are 
listed in Table 1. They are calculated from the 
decision function of the SVM gained from the 
last round of iteration.  
Instances PV
?   I 0.260306643320642 
?   very 0.553855024703612 
?? like 0.427269428974918 
?   this 0.031682276068012 
?   type 0.366598504697780 
?   car 0.095961213527654 
? 0.178633448748979 
?? but 0.092571306234562 
?? price 0.052164989563922 
?   high 0.539913276317129 
?   (auxiliary word) 0.458036102580422 
?   a little bit 0.439936293288062 
? 0.375263535139242 
Table 1 Example of 2-Classification SVM 
Predict Value 
Suppose all the instances in this sentence 
have not been added into the training set. This
(0.0316), price (0.0521), and but (0.0925) will be 
selected into the training set successively for 
they have the minimal PVs. 
For multi-classification SVM, it will be more 
complicated to find the training instances. Be-
cause common multi-classification SVM is im-
plemented by voting process (Hsu and Lin, 2002),
there are
1
( 1)
2
t t?  decision function values in t-
classification SVM. 
In our system, we need to classify instances 
into 4 classes, namely, topic, holder, sentiment
and other. So a 4-classification SVM is adopted. 
Suppose for an instance, we get 6 Decision Func-
tion Values from 6 bi-classification SVMs as in 
Table 2. 
No. Classification Decision Function Value Result
1 Class 0 Vs Class 1 1.00032792289507 0
2 Class 0 Vs Class 2 0.999999993721249 0
3 Class 0 Vs Class 3 1.00032792289507 0
4 Class 1 Vs Class 2 0.106393804825973 1
5 Class 1 Vs Class 3 -5.20417042793042E-18 3
6 Class 2 Vs Class 3 -0.106393804825973 3
Table 2 Example of 4-Classification SVM Decision 
Process
For each bi-classification SVM, the class in-
stance belongs to is determined by whether the 
decision function value is greater than or less 
than zero. The instance in Table 2 belongs to 
Class 0 since there 3 votes out of 6 votes for 
Class 0. When deciding which class an instance 
belongs to, only the decision function values 
from bi-classification SVMs with correct votes 
will work on the certainty of the final result. 
Hence, we define Predict Value for multi-
classification SVMs as the arithmetic mean value 
of the absolute decision function value of every 
bi-classification SVM with correct vote, 
          
^
t
1, bi classification SVMs with correct votes
1
( ) y ( )
k
t t `
x x
k  ?
?
&

39  
&
ir
      (3) 
For the instance in Table2, the value is calculated 
from the decision function values from bi-
classification SVMs numbered 1, 2, and 3. 
3.4 Experiments 
To prove the validity of active learning algorithm 
and find out the relations between the perform-
ance of the classifiers and the way the classifiers 
are trained, we carried out batches of experi-
ments.
In most information extraction tasks, a word 
and its context are considered a learning sample, 
and encoded as feature vectors. In our experi-
ments, context data includes the part-of-speech 
tag, dependency relation, word semantic mean-
ing, and word disambiguation information of the 
word being classified, its neighboring words and 
its parent word in dependency grammar. Part-of-
speech tag and dependency relation are common 
features for Chinese Natural Language Process-
ing (NLP) tasks1. We get word semantic mean-
ing from HowNet, which is an online common-
sense knowledge base unveiling inter-conceptual
relations and inter-attribute relations of concepts
as connoting in lexicons of the Chinese and the
English equivalents (Zhendong Dong and Qiang 
Dong, 1999). Given an occurrence of a word in 
natural language text, word sense disambiguation 
is the process of identifying which sense of the 
word is intended if the word has a number of dis-
tinct senses. According to Song and Yao (2009), 
this information may help in Chinese NLP tasks 
such as topic identification. 
Lack of explicit boundary between training 
instances and testing instances is a great differ-
ence between common machine learning algo-
rithm and learning algorithm designed for corpus 
annotation. For common machine learning algo-
rithm such as human face recognition, the quan-
tity of training instances is limited while the test-
ing instances could be infinite. It is unnecessary 
and impossible to annotate all the testing in-
stances. However, when annotating a corpus, all 
the texts need to be annotated are decided be-
forehand. Although tools automated part of the 
annotation process, the results still need to be 
reviewed for several times to ensure the quality 
of annotation. That means in an annotation sce-
nario, all the data to be processed are available 
during the training stage. 
The raw texts used in our experiments are 
taken from forums of chinacars.com. These texts 
include explicit subjective opinion and informal 
network language, which are necessary for opin-
ion mining research. Most of them are comments 
composed of one or more sentences on certain 
type of vehicle. The detailed opinion elements 
distributions are showed in table 3. 
We use all the texts as testing data set and a 
subset of it as a training data set. First of all, we 
pick up 10 instances for each class, and train a 
simple classification model with them. Then, the 
baseline system picks up k instances in sequence 
and adds them into the training data set to train a 
new classification model iteratively until the 
training data set is as large as the testing data set, 
1 We use Language Technology Platform (LTP), developed 
by Center for Information Retrieval, Harbin Institute of 
Technology, for part-of-speech tagging, dependency rela-
tionship analysis and word sense disambiguation in our 
experiment.
while the active learning system picks up in-
stances according to the strategy in Chapter 3.3.  
Type No. of Instances
Topic 638
Sentiment 769
Holder 46
Other 1500
Total 2953
Table 3 Detailed Information of the Data Set 
We use three bi-classification model to test 
the performance of the active learning system on 
topic, sentiment, and holder identification sepa-
rately and a four-classification model to identify 
the three opinion elements simultaneously. The 
results of the experiments are illustrated in Fig-
ure 2, 3, 4, and 5 respectively. Table 4, 5, and 6 
provide the detailed F-measure trends while dif-
ferent numbers of instances are added into the 
training data set in each rounds. For each ex-
periment, we try to compare the performances 
when we add different number of instances into 
the training data set in each round of iteration. 
Figure 2 Topic Identification 
Figure 3 Sentiment Identification 
Figure 4 Holder Identification 
Figure 5 All Opinion Elements Identification 
As are illustrated in the figures, the active 
learning system can always achieve better or at 
least no worse performance than baseline system. 
For example, when adding 200 instances in each 
round for topic identification task (Figure2 and 
Table 4), the active learning system reaches its 
peak value in F-measure (0.8644) with only 600 
training instances. This F-measure value is even 
higher than the value the baseline system get 
(0.8604) after taking all the 2953 training in-
stances.
The active learning system outperforms the 
baseline system greatly especially when dealing 
with unbalanced data set (Figure 4 and Table 4). 
In opinion holder identification task, the baseline 
system can not find any holder until 1600 train-
ing instances are taken while the active learning 
system reaches its peak F-measure value (0.8810) 
with only 600 training instances. That means 
when using active learning algorithm, it is possi-
ble for us to save some time for optimizing the 
parameters when dealing with unbalanced data. 
The number of instances added to the training 
data set in each round (k) influences the perform-
ance of the active learning algorithm in a large 
extent. When a smaller value is assigned to k, the 
active learning system will tend to achieve better 
F-measure (Table 4) with less training instances 
comparing to the baseline system. Advantages of 
the active learning system will be diminished by 
the increase in k (Table 6). 
4 Evaluation of Active Learning Algo-
rithm
For active learning algorithm based on member-
ship query, its training process will probably take 
longer time by the time the optimum classier is 
found, since the training process consists of sev-
eral rounds of iteration. At the beginning of the 
iteration, the classification speed of the model is 
much faster due to less training instances are 
used and the model is simple. With more and 
more training instances are added into the train-
ing data set, the model will become more com-
plex and more time will be needed for classifica-
? 
Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
200 0.7118 0.6221  0.6481 0.0103 0.0000 0.0000 0.6968  0.3874 
400 0.8072 0.8287  0.7344 0.6239 0.0000 0.0000 0.7691  0.7336 
600 0.8237 0.8644  0.7845 0.7860 0.0000 0.8810 0.7907  0.7979 
800 0.8250 0.8625  0.7876 0.8133 0.0000 0.8810 0.8020  0.8240 
1000 0.8386 0.8613  0.7878 0.8189 0.0000 0.8810 0.8101  0.8378 
1200 0.8389 0.8588  0.7992 0.8153 0.0000 0.8810 0.8128  0.8377 
1400 0.8489 0.8588  0.8011 0.8141 0.0000 0.8810 0.8178  0.8471 
1600 0.8450 0.8581  0.8033 0.8150 0.0426 0.8810 0.8211  0.8468 
1800 0.8521 0.8581  0.8059 0.8183 0.1224 0.8810 0.8271  0.8479 
2000 0.8528 0.8585  0.8169 0.8197 0.6857 0.8810 0.8348  0.8481 
2200 0.8560 0.8583  0.8109 0.8200 0.8101 0.8810 0.8372  0.8468 
2400 0.8592 0.8592  0.8186 0.8195 0.8395 0.8810 0.8404  0.8474 
2600 0.8620 0.8610  0.8165 0.8205 0.8675 0.8810 0.8440  0.8463 
2800 0.8578 0.8610  0.8138 0.8177 0.8810 0.8810 0.8464  0.8443 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 
Table 4 F-measure Trends when k=200 

Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
500 0.8198 0.7730  0.7616 0.1369 0.0000 0.0000 0.7831  0.5173 
1000 0.8386 0.8508  0.7878 0.7566 0.0000 0.8837 0.8101  0.7776 
1500 0.8468 0.8592  0.8039 0.8175 0.0833 0.8810 0.8194  0.8398 
2000 0.8528 0.8610  0.8169 0.8183 0.6857 0.8810 0.8348  0.8484 
2500 0.8626 0.8583  0.8168 0.8205 0.8395 0.8810 0.8427  0.8463 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 

Table 5  F-measure Trends when k=500

Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
1000 0.8386 0.8335  0.7878 0.3514 0.0000 0.0000 0.8101  0.7534 
2000 0.8528 0.8581  0.8169 0.8170 0.6857 0.8810 0.8348  0.8376 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 

Table 6  F-measure Trends when k=1000
tion. On account of the features of active learn-
ing algorithm, we believe it is necessary to find a 
way to balance the performance of the classifier 
and the time it take in training process for a thor-
ough evaluation of the algorithm. 
We define the measurement for time as: 
k
T
C
                                                 (4) 
where C is the number of all the possible training 
instances available, k is the number of training 
instances added into the training data set in each 
round of iteration. T is the approximate value of 
the inverse ratio of the time it takes for training 
process. T will have a greater value if the training 
process takes less time. Its range is (0, 1] just 
similar to F-measure. 
We define the measurement for the training 
instances used as: 
(1 )
n
K
C
  (5) 
where n is the number of the training instances 
actually used. K will have a greater value if less 
training instances are used in the training process. 
The range of K is [0, 1). 
To judge the overall performance of an active 
learning algorithm, we consider the F-measure 
(F) of the classifier, the time it takes during the 
training process, and the training instances used. 
We define the Active Learning Performance 
(ALP) as the harmonic mean of the three aspects: 
1
( )
(6)
( ) ( )
ALP
K F T
F k C n
F C k k C n F C C n
D E J
D E J
 
 
? ?  
? ? ?  ?   ? ? 
where + + =1D E J , and > @, , 0,1D E J ? . They 
are the weights for the three measurements. The 
greater the value of a certain weight is, the more 
important the measurement is in the overall per-
formance. The greater the value of the ALP is, 
the better the performance of the active learning 
algorithm. For instance, when training a classi-
fier for sentiment identification using active 
learning algorithm, we get a classifier with F-
measure of 0.8189 using 1000 training instances 
and a classifier with F-measure of 0.8200 using 
2200 training instances (Table 4). Sup-
pose
1
= = =
3
D E J , we calculate the value of ALP
for the two cases according to equation (6) and 
get 0.1714 and 0.1507 as results respectively. 
That means a people with no preference among 
F-measure, the number of training instances 
adopted and the time used during training proc-
ess will choose to get a classifier with less train-
ing instances, less training time and less F-
measure value. 
5 Conclusion
This paper experimentally demonstrates the va-
lidity of active learning algorithm when used for 
opinioned elements identification and proposed a 
computational method for overall system per-
formance evaluation which consists of F-
measure, training time, and number of training 
instances. According to our tests, active learning 
algorithm outperforms the base line system in 
most of the cases especially when fewer in-
stances are added into the training data set in 
each round of iteration. However, the method 
could extent the training time in a large scale. To 
balance the pros and cons of active learning algo-
rithm, it might be helpful to adjust the number of 
training instances added in each round dynami-
cally in the training process. For instance, add 
less training instances at the beginning of the 
training process to ensure a high peak value of F-
measure could be achieved and add more train-
ing instances later so that time spent on training 
process could be reduced. 
Acknowledgments  
The author of this paper would like to thank In-
formation Retrieval Lab, Harbin Institute of 
Technology for providing the tool (LTP) used in 
experiments. This research was supported by 
National Natural Science Foundation of China 
Grant No.60773087.  
References
Andrew K. McCallum, Kamal Nigam. 1998. Employ-
ing EM in Pool-based Active Learning for Text 
Classification. In Proceedings of the 15th Interna-
tional Conference on Machine Learning.
Bernhard E. Boser, Isabelle M. Guyon, and Vladimir 
N. Vapnik. 1992. A Training Algorithm for Opti-
mal Margin Classifiers. In Proceedings of the Fifth 
Annual Workshop on Computational Learning 
Theory.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/ 
libsvm 
Chih-Wei Hsu and Chih-Jen Lin. 2002. A Compari-
son of Methods for Multi-class Support Vector 
Machines. IEEE Transactions on Neural Networks.
Claude Sammut and Ranan B. Banerji. 1986. Learn-
ing Concepts by Asking Questions. Machine 
Learning: An Artificial Intelligence Approach,
1986, 2: 167-191 
Dana Angluin. 1988. Queries and Concept Learning. 
Machine Learning, 1988, 2(4): 319-342 
David D. Lewis, William A. Gail. 1994. A Sequential 
Algorithm for Training Text Classifiers. In Pro-
ceedings of the 17th Annual International ACM 
SIGIR Conference on Research and Development 
in Information Retrieval.
Ehud Y. Shapiro. 1982. Algorithmic Program Debug-
ging. M.I.T. Press. 
Ido Dagan, Sean P. Engelson. 1995. Committee-
Based Sampling for Training Probabilistic Classi-
fiers. In Proceedings of the International Confer-
ence on Machine Learning.
Ion Muslea, Steven Minton, Craig A. Knoblock. 2006. 
Active Learning with Multiple Views. Journal of 
Artificial Intelligence Research, 2006, 27(1): 203-
233. 
Quansheng Liu, Tianfang Yao, Gaohui Huang, Jun 
Liu, Hongyan Song. 2008. A Survey of Opinion 
Mining for Texts. Journal of Chinese Information 
Processing. 2008, 22(6):63-68. 
Jun Long, Jianping Yin, En Zhu, and Wentao Zhao. A 
Survey of Active Learning. 2008. Journal of Com-
puter Research and Development, 2008, 45(z1): 
300-304. 
Shlomo A. Engelson, Ido Dagon. 1999. Committee-
based Sample Selection for Probabilistic Classifi-
ers. Journal of Artificial Intelligence Research,
1999, 11: 335-360. 
Hongyan Song, Jun Liu, Tianfang Yao, Quansheng 
Liu, Gaohui Huang. 2009. Construction of an An-
notated Corpus for Chinese Opinioned-Subjective 
Texts. Journal of Chinese Information Processing,
2009, 23(2): 123-128. 
Hongyan Song and Tianfang Yao. 2009. Improving 
Chinese Topic Extraction Using Word Sense Dis-
ambiguation Information. In Proceedings of the 4th 
International Conference on Innovative Computing, 
Information and Control.
Soo-Min Kim and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. In Proceedings of the 
Conference on Computational Linguistics: 1367-
1373. 
Tianfang Yao, Xiwen Cheng, Feiyu Xu, Hans 
Uszkoreit, and Rui Wang. 2008. A Survey of Opin-
ion Mining for Texts. Journal of Chinese Informa-
tion Processing, 2008, 22(3): 71-80. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer.  
Yoav Freund, H.Sebastian Seung, Eli Shamir, Naftali 
Tishby. 1997. Selective Sampling Using the Query 
by Committee Algorithm. Machine Learning,
28(2-3): 133-168 
Zhendong Dong and Qiang Dong. 1999. HowNet. 
http://www.keenage.com 
