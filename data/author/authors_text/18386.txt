Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 13?18,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Open Source Toolkit for Quantitative Historical Linguistics
Johann-Mattis List
Research Center Deutscher Sprachatlas
Philipps-University Marburg
mattis.list@uni-marburg.de
Steven Moran
Department of General Linguistics
University of Zurich
steven.moran@uzh.ch
Abstract
Given the increasing interest and devel-
opment of computational and quantitative
methods in historical linguistics, it is im-
portant that scholars have a basis for doc-
umenting, testing, evaluating, and shar-
ing complex workflows. We present a
novel open-source toolkit for quantitative
tasks in historical linguistics that offers
these features. This toolkit also serves
as an interface between existing software
packages and frequently used data for-
mats, and it provides implementations of
new and existing algorithms within a ho-
mogeneous framework. We illustrate the
toolkit?s functionality with an exemplary
workflow that starts with raw language
data and ends with automatically calcu-
lated phonetic alignments, cognates and
borrowings. We then illustrate evaluation
metrics on gold standard datasets that are
provided with the toolkit.
1 Introduction
Since the turn of the 21st century, there has been an
increasing amount of research that applies compu-
tational and quantitative approaches to historical-
comparative linguistic processes. Among these
are: phonetic alignment algorithms (Kondrak,
2000; Proki? et al, 2009), statistical tests for ge-
nealogical relatedness (Kessler, 2001), methods
for phylogenetic reconstruction (Holman et al,
2011; Bouckaert et al, 2012), and automatic de-
tection of cognates (Turchin et al, 2010; Steiner et
al., 2011), borrowings (Nelson-Sathi et al, 2011),
and proto-forms (Bouchard-C?t? et al, 2013).
In contrast to traditional approaches to language
comparison, quantitative methods are often em-
phasized as advantageous with regard to objectiv-
ity, transparency and replicability of results. It
is striking then that given the multitude of new
approaches, very few are publicly available as
executable code. Thus in order to replicate a
study, researchers have to rebuild workflows from
published descriptions and reimplement their ap-
proaches and algorithms. These challenges make
the replication of results difficult, or even impos-
sible, and they hinder not only the evaluation and
comparison of existing algorithms, but also the de-
velopment of new approaches that build on them.
Another problem is that quantitative approaches
that have been released as software are largely in-
compatible with each other and they show great
differences in regard to their input and out for-
mats, application range and flexibility.1 Given the
breadth of research questions involved in deter-
mining language relatedness, this is not surprising.
Furthermore, the linguistic datasets upon which
many analyses and tools are based are only ? if at
all ? available in disparate formats that need man-
ual or semi-automatic re-editing before they can
be used as input elsewhere. Scholars who want
to analyze a dataset with different approaches of-
ten have to (time-consumingly) convert it into var-
ious input formats and they have to familiarize
themselves with many different kinds of software.
As a result, errors may occur during data conver-
sion processes and the output from different tools
must also be converted into a comparable format.
For the comparison of different output formats or
1There is the STARLING database program for lexicosta-
tistical and glottochronological analyses (Starostin, 2000).
TheRug/L04 software aligns sound sequences and calculates
phonetic distances using the Levensthein distance (Kleiweg,
2009; Levenshtein, 1966). The ASJP-Software also com-
putes the Levenshtein distance (Holman et al, 2011), but its
results are based on previously executed phonetic analyses.
The ALINE software carries out pairwise alignment analy-
ses (Kondrak, 2000). There are also software packages from
evolutionary biology, which are adapted for linguistic pur-
poses, such as MrBayes (Ronquist and Huelsenbeck, 2003),
PHYLIP (Felsenstein, 2005), and SplitsTree (Huson, 1998).
13
for the evaluation of competing quantitative ap-
proaches, gold standard datasets are desirable.
Towards a solution to these problems, we have
developed a toolkit that (a) serves as an interface
between existing software packages and data for-
mats frequently used in quantitative approaches,
(b) provides high-quality implementations of new
and existing approaches within a homogeneous
framework, and (c) offers a solid basis for test-
ing, documenting, evaluating, and sharing com-
plex workflows in quantitative historical linguis-
tics. We call this open source toolkit LingPy.
2 Lingpy
LingPy is written in Python3 and is freely avail-
able online.2 The Lingpy website contains an API,
documentation, tutorials, example scripts, work-
flows, and datasets that can be used for training,
testing, and comparing results from different algo-
rithms. We use Python because it is flexible and
object-oriented, it is easy to write C extensions
for scientific computing, and it is approachable
to non-programmers (Knight et al, 2007). Apart
from a large number of different functions for com-
mon automatic tasks, LingPy offers specific mod-
ules for implementing general workflows that are
used in historical linguistics and which partially
mimic the basic aspects of the traditional compar-
ative method (Trask, 2000, 64-67). Figure 1 il-
lustrates the interaction between different modules
along with the data they produce. In the following
subsections, these modules will be introduced in
the order of a typical workflow to illustrate the ba-
sic capabilities of the LingPy toolkit in more detail.
2.1 Input Formats
The basic input format read by LingPy is a tab-
delimited text file in which the first line (the
header) indicates the values of the columns and all
words are listed in the following rows. The for-
mat is very flexible. No specific order of columns
or rows is required. Any additional data can be
specified by the user, as long as it is in a separate
column. Each row represents a word that has to be
characterized by a minimum of four values that are
given in separate columns: (1) ID, an integer that
is used to uniquely identify the word during calcu-
lations, (2) CONCEPT, a gloss which indicates the
meaning of the word and which is used to align the
words semantically, (3) WORD, the orthographic
2http://lingpy.org
Raw data
Tokenized
data
Orthographic parsing
Cognate
sets
Alignments
Cognate
detection
Phonetic
alignment (PA)
Output 
formats
PA
Patchy 
cognate 
sets
borrowing
detection
PA
Figure 1: Basic Workflow in LingPy
representation of the word,3 and (4) TAXON, the
name of the language (or dialect) inwhich theword
occurs. Basic output formats are essentially the
same, the difference being that the results of cal-
culations are added as separate columns. Table 1
illustrates the basic structure of the input format
for a dataset covering 325 concepts translated into
18 Dogon language varieties taken from the Do-
gon comparative lexical spreadsheet (Heath et al,
2013).4
2.2 Parsing and Unicode Handling
Given a dataset in the basic LingPy input for-
mat, the first step towards sound-based normal-
ization for automatically identifying cognates and
sound changes with quantitative methods is to
parse words into tokens. Orthographic tokeniza-
tion is a non-trivial task, but it is needed to at-
3By this we mean a textual representation of the word,
whether in a document or language-specific orthography or
in some form of broad or narrow transcription, etc.
4This tokenized dataset and analyses that are discussed in this
work are available for download from the LingPy website.
14
ID CONCEPT WORD TAXON
... ... ... ...
1239 file (tool) ki?:ra? Toro_Tegu
1240 file (tool) di?:s?: Ben_Tey
1241 file (tool) ki?r?l Bankan_Tey
1242 file (tool) di?:ju? Jamsay
... ... ... ...
1249 file (tool) bi?mbu? Tommo_So
1250 file (tool) bi?mbu? Dogul_Dom
1251 file (tool) di?:zu? Yanda_Dom
1252 file (tool) bi?:mbye? Mombo
... ... ... ...
Table 1: Basic Input Format of LingPy
tain interoperability across different orthographies
or transcription systems and to enable the com-
parative analysis of languages. LingPy includes
a parser that takes as input a dataset and an op-
tional orthography profile, i.e. a description of
the Unicode code points, characters, graphemes
and orthographic rules that are needed to ade-
quately model a writing system for a language va-
riety as described in a particular document (Moran,
2012, 331). The LingPy parser first normalizes all
strings into UnicodeNormalization FormD,which
decomposes all character sequences and reorders
them into one canonical order. This step is nec-
essary because sequences of Unicode characters
may differ in their visual and logical orders. Next,
if no orthography profile is specified, the parser
will use a regular expression match \X for Uni-
code grapheme clusters, i.e. combining character
sequences typified by a base character followed by
one or more Combing Diacritical Marks. How-
ever, another layer of tokenization is usually re-
quired to match linguistic graphemes, or what Uni-
code calls ?tailored grapheme clusters?. Table 2 il-
lustrates the different technological and linguistic
levels involved in orthographic parsing.5
code points t s h o ? ? ? ? ? ? s h i
?characters? t s h o??? s h i
graphemes tsh o??? sh i
Table 2: Tokens for the string <tsh???shi>
So, given the dataset illustrated in Table 1 and
an orthography profile that defines the phone-
mic units in the Dogon comparative lexicon, the
5Note that even when a linguist transcribes a word with the
International Phonetic Alphabet (IPA; a transcription system
with one-to-one symbol-to-sound correspondences), explicit
definitions for phonemes are needed because some IPA dia-
critics are encoded as Unicode Spacing Modifier Letters, i.e.
characters that are not specified as how they combine with a
base character, such as aspiration.
LingPy parser produces the IPA tokenized output
shown in Table 3.
ID ... WORD TOKENS ...
... ... ... ... ...
1239 ... ki?:ra? # k i?: r a? # ...
1240 ... di?:s?: # d i?: s ?: # ...
1241 ... ki?r?l # k i? r ? l # ...
1242 ... di?:ju? # d i?: ? u? # ...
... ... ... ... ...
1249 ... bi?mbu? # b i? m b u? # ...
1250 ... bi?mbu? # b i? m b u? # ...
1251 ... di?:zu? # d i?: z u? # ...
1252 ... bi?:mbye? # b i?: m b j e? # ...
... ... ... ... ...
Table 3: Orthographic Parsing in LingPy
2.3 Phonetic Alignments
Although less common in traditional historical lin-
guistics, phonetic alignment plays a crucial role
in automatic approaches, with alignment analyses
being currently used in many different subfields,
such as dialectology (Proki? et al, 2009), phyloge-
netic reconstruction (Holman et al, 2011) and cog-
nate detection (List, 2012a). Furthermore, align-
ment analyses are very useful for data visualiza-
tion, since they directly show which sound seg-
ments correspond in cognate words.
LingPy offers implementations for many dif-
ferent approaches to pairwise and multiple pho-
netic alignment. Among these, there are stan-
dard approaches that are directly taken from evo-
lutionary biology and can be applied to linguistic
data with only slight modifications, such as the
Needleman-Wunsch algorithm (Needleman and
Wunsch, 1970) and the Smith-Waterman algo-
rithm (Smith and Waterman, 1981). Furthermore,
there are novel approaches that use more com-
plex sequence models in order to meet linguistic-
specific requirements, such as the Sound-Class-
based phonetic Alignment (SCA) method (List,
2012b). Figure 2 shows a plot of the multi-
ple alignment of the counterparts of the concept
?stool? in eight Dogon languages. The color
scheme for the sound segments follows the sound
class distinction of Dolgopolsky (1964).
2.4 Automatic Cognate Detection
The identification of cognates plays an impor-
tant role in both traditional and quantitative ap-
proaches in historical linguistics. Most quantita-
tive approaches dealing with phylogenetic recon-
struction are based on previously identified cog-
nate sets distributed over the languages being in-
15
Taxon AlignmentBen_Tey t u? ? g u? r - u? mBankan_Tey t u? ? g u? r - u? -Jamsay t u? ? - u? r? - u? -Perge_Tegu t u? ? - u? r? - u? mGourou t u? m - u? r - u? -Yorno_So t ?? ? - ?? - - - -Tommo_So t u? ? g u? r - u? -Tebul_Ure t u? ? g u? r g ?? -XXX XXX XXX XXX XXX XXX XXX XXX XXX
Figure 2: Multiple Phonetic Alignment in LingPy
vestigated (Bouckaert et al, 2012; Bouchard-C?t?
et al, 2013). Since the traditional approach to cog-
nate detection within the framework of the com-
parative method is very time-consuming and diffi-
cult to evaluate for the non-expert, automatic ap-
proaches to cognate detection can play an impor-
tant role in objectifying phylogenetic reconstruc-
tions.
Currently, LingPy offers four alternative ap-
proaches to cognate detection in multilingual
wordlists. Themethod by Turchin et al (2010) em-
ploys sound classes as proposed by Dolgopolsky
(1964) and assigns words that match in their first
two consonant classes to the same cognate set. The
NED method calculates the normalized edit dis-
tance between words and groups them into cognate
sets using a flat cluster algorithm.6 The SCA and
the LexStat methods (List, 2012a) use the same
strategy for clustering, but the distances for the
SCA method are calculated with help of the SCA
alignment method (List, 2012b), and the distances
for the LexStat method are derived from previ-
ously identified regular sound correspondences.
Table 4 shows a small section of the results from
the LexStat analysis of the Dogon data. As shown,
LingPy follows the STARLING approach in dis-
playing cognate judgments by assigning cognate
words the same cognate ID (COGID). In Table
4, the words judged to be cognate are shaded in
the same color. The full results are posted on the
LingPy website.
2.5 Automatic Borrowing Detection
Automatic approaches for borrowing detection
are still in their infancy in historical linguistics.
LingPy provides a full reimplementation (along
with specifically linguistic modifications) of the
minimal lateral network (MLN) approach (Nelson-
Sathi et al, 2011). This approach searches for cog-
nate sets which are not compatible with a given ref-
6The normalized edit distance is calculated by dividing the
edit distance (Levenshtein, 1966) by the length of the smaller
sequence, see Holman et al (2011) for details.
ID CONCEPT WORD TAXON COGID
... ... ... ... ...
1239 file (tool) ki?:ra? Toro_Tegu 68
1240 file (tool) di?:s?: Ben_Tey 69
1241 file (tool) ki?r?l Bankan_Tey 68
1242 file (tool) di?:ju? Jamsay 69
... ... ... ... ...
1249 file (tool) bi?mbu? Tommo_So 70
1250 file (tool) bi?mbu? Dogul_Dom 70
1251 file (tool) di?:zu? Yanda_Dom 69
1252 file (tool) bi?:mbye? Mombo 70
... ... ... ... ...
Table 4: Cognate Detection in LingPy
erence tree topology. Incompatible (patchy) cog-
nate sets often point to either borrowings or wrong
cognate assessments in the data. The results can
be visualized by connecting all taxa of the refer-
ence tree for which patchy cognate sets can be in-
ferred with lateral links. In Figure 3, the method
has been applied again to the Dogon dataset. Cog-
nate judgments for this analysis were carried out
with help of LingPy?s LexStat method. The tree
topology was calculated using MrBayes.
2.6 Output Formats
The output formats supported by LingPy can be di-
vided into three different classes. The first class
consists of text-based formats that can be used
for manual correction and inspection by import-
ing the data into spreadsheet programs, or sim-
ply editing and reviewing the results in a text
editor. The second class consists of specific
formats for third-party toolkits, such as PHY-
LIP, SplitsTree, MrBayes, or STARLING. LingPy
currently offers support for PHYLIP?s distance
calculations (DST-format), for tree-representation
(Newick-format), for complex representations of
character data (Nexus-format), and for the im-
port into STARLING databases (CSV with STAR-
LING markup). The third class consists of new
approaches to the visualization of phonetic align-
ments, cognate sets, and phylogenetic networks.
In fact, all plots in this paper were created with
LingPy?s output formats.
3 Evaluation
In order to improve the performance of quantita-
tive approaches, it is of crucial importance to test
and evaluate them. Evaluation is usually done by
comparing how well a given approach performs
on a reference dataset, i.e. a gold standard, where
the results of the analysis are known in advance.
LingPy comes with a module for the evaluation of
16
Ben Tey
Tomo Kan Diangassagou
Toro Tegu
Tebul Ure
Jamsay Mondoro
Yanda DomNanga
Tiranige
Bankan Tey
Jamsay
Perge Tegu
Gourou
Bunoge
Dogul Dom
Mombo
Yorno So
Tommo SoTogo Kan
1
9
19
Inf
err
ed
Lin
ks
Figure 3: Borrowing Detection in LingPy
basic tasks in historical linguistics, such as pho-
netic alignment and cognate detection. This mod-
ule offers both common evaluation measures that
are used to assess the accuracy of the respective
methods and gold standard datasets encoded in the
LingPy input format.
In Figure 4, the performance of the four above-
mentioned approaches to automatic cognate de-
tection are compared with the gold standard cog-
nate judgments of a dataset covering 207 con-
cepts translated into 20 Indo-European languages
taken from the Indo-European Lexical Cognacy
(IELex) database (Bouckaert et al, 2012).7 The
pair scores, implemented in LingPy after the de-
scription in Bouchard-C?t? et al (2013), were used
as an evaluation measure. For all approaches we
chose the respective thresholds that tend to yield
the best results on all of the gold standards. As
shown in Figure, both the SCA and LexStat meth-
ods show a higher accuracy than the Turchin and
NED methods, with LexStat slightly outperform-
ing SCA. However, the generally bad performance
7Gold standard here means that the cognate judgments were
carried out manually by the compilers of the IELex database.
of all approaches on this dataset shows that there is
a clear need for improving automatic cognate de-
tection approaches, especially in cases of remote
relationship, such as Indo-European.
Precision Recall F-Score0.2
0.3
0.4
0.5
0.6
0.7
0.8 TurchinNEDSCALexStat
Figure 4: Evaluating Cognate Detection Methods
4 Conclusion
Quantitative approaches in historical linguistics
are still in their infancy, far away from being able
to compete with the intuition of trained historical
17
linguists. The toolkit we presented is a first at-
tempt to close the gap between quantitative and
traditional methods by providing a homogeneous
framework that serves as an interface between ex-
isting packages and at the same time provides high-
quality implementations of new approaches.
References
A. Bouchard-C?t?, D. Hall, T. L. Griffiths, and
D. Klein. 2013. Automated reconstruction of an-
cient languages using probabilistic models of sound
change. PNAS, 110(11):4224?4229.
R. Bouckaert, P. Lemey, M. Dunn, S. J. Greenhill,
A. V. Alekseyenko, A. J. Drummond, R. D. Gray,
M. A. Suchard, and Q. D. Atkinson. 2012. Map-
ping the origins and expansion of the Indo-European
language family. Science, 337(6097):957?960, Aug.
A. B. Dolgopolsky. 1964. Gipoteza drevnej?ego rod-
stva jazykovych semej Severnoj Evrazii s verojatnos-
tej to?ky zrenija [A probabilistic hypothesis concern-
ing the oldest relationships among the language fam-
ilies of Northern Eurasia]. Voprosy Jazykoznanija,
2:53?63.
J. Felsenstein. 2005. Phylip (phylogeny inference
package) version 3.6. Distributed by the author. De-
partment of Genome Sciences, University of Wash-
ington, Seattle.
J. Heath, S Moran, K. Prokhorov, L. McPherson, and
B. Canslter. 2013. Dogon comparative lexicon.
URL: http://www.dogonlanguages.org.
E. W. Holman, C. H. Brown, S. Wichmann, A. M?ller,
V. Velupillai, H. Hammarstr?m, S. Sauppe, H. Jung,
D. Bakker, P. Brown, O. Belyaev, M. Urban,
R. Mailhammer, J.-M. List, and D. Egorov. 2011.
Automated dating of the world?s language families
based on lexical similarity. Current Anthropology,
52(6):841?875.
D. H. Huson. 1998. SplitsTree. Analyzing and visu-
alizing evolutionary data. Bioinformatics, 14(1):68?
73.
B. Kessler. 2001. The significance of word lists. Sta-
tistical tests for investigating historical connections
between languages. CSLI Publications, Stanford.
P. Kleiweg. 2009. RuG/L04. Software for dialecto-
metrics and cartography. Distributed by the Author.
Rijksuniversiteit Groningen. Faculteit der Letteren,
September.
R. Knight, P. Maxwell, A. Birmingham, J. Carnes,
J. G. Caporaso, B. Easton, M. Eaton, M. Hamady,
H. Lindsay, Z. Liu, C. Lozupone, D. McDonald,
M. Robeson, R. Sammut, S. Smit, M. Wakefield,
J. Widmann, S. Wikman, S. Wilson, H. Ying, and
G. Huttley. 2007. PyCogent. A toolkit for making
sense from sequence. Genome Biology, 8(8):R171.
G. Kondrak. 2000. A new algorithm for the align-
ment of phonetic sequences. In Proceedings of
the 1st North American chapter of the Association
for Computational Linguistics conference, NAACL
2000, pages 288?295, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
J.-M. List. 2012a. LexStat. Automatic detection of
cognates in multilingual wordlists. InProceedings of
the EACL 2012 Joint Workshop of LINGVIS & UN-
CLH, pages 117?125. Association for Computational
Linguistics.
J.-M. List. 2012b. SCA. Phonetic alignment based
on sound classes. In M. Slavkovik and D. Las-
siter, editors, New directions in logic, language, and
computation, number 7415 in LNCS, pages 32?51.
Springer, Berlin and Heidelberg.
S. Moran. 2012. Phonetics information base and lexi-
con. Ph.D. thesis, University of Washington.
S. B. Needleman and C. D. Wunsch. 1970. A gene
method applicable to the search for similarities in
the amino acid sequence of two proteins. Journal
of Molecular Biology, 48:443?453, July.
S. Nelson-Sathi, J.-M. List, H. Geisler, H. Fangerau,
R. D. Gray, W. Martin, and T. Dagan. 2011. Net-
works uncover hidden lexical borrowing in Indo-
European language evolution. Proceedings of the
Royal Society B, 278(1713):1794?1803.
J. Proki?, M. Wieling, and J. Nerbonne. 2009. Multi-
ple sequence alignments in linguistics. In Proceed-
ings of the EACL 2009 Workshop on Language Tech-
nology and Resources for Cultural Heritage, Social
Sciences, Humanities, and Education, pages 18?25.
Association for Computational Linguistics.
F. Ronquist and J. P. Huelsenbeck. 2003. MrBayes 3.
Bayesian phylogenetic inference under mixed mod-
els. Bioinformatics, 19(12):1572?1574.
T. F. Smith and M. S. Waterman. 1981. Identifica-
tion of common molecular subsequences. Journal of
Molecular Biology, 1:195?197.
S. A. Starostin. 2000. The STARLING database pro-
gram. URL: http://starling.rinet.ru.
L. Steiner, P. F. Stadler, and M. Cysouw. 2011.
A pipeline for computational historical linguistics.
Language Dynamics and Change, 1(1):89?127.
R. L. Trask. 2000. The dictionary of historical
and comparative linguistics. Edinburgh University
Press, Edinburgh.
P. Turchin, I. Peiros, and M. Gell-Mann. 2010. An-
alyzing genetic connections between languages by
matching consonant classes. Journal of Language
Relationship, 3:117?126.
18
Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 117?125,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
LexStat: Automatic Detection of Cognates in Multilingual Wordlists
Johann-Mattis List
Institute for Romance Languages and Literature
Heinrich Heine University D?sseldorf, Germany
listm@phil.uni-duesseldorf.de
Abstract
In this paper, a new method for automatic
cognate detection in multilingual wordlists
will be presented. The main idea behind the
method is to combine different approaches to
sequence comparison in historical linguistics
and evolutionary biology into a new frame-
work which closely models the most impor-
tant aspects of the comparative method. The
method is implemented as a Python program
and provides a convenient tool which is pub-
licly available, easily applicable, and open
for further testing and improvement. Testing
the method on a large gold standard of IPA-
encoded wordlists showed that its results are
highly consistent and outperform previous
methods.
1 Introduction
During the last two decades there has been an in-
creasing interest in automatic approaches to his-
torical linguistics, which is reflected in the large
amount of literature on phylogenetic reconstruc-
tion (e.g. Ringe et al, 2002; Gray and Atkin-
son, 2003; Brown et al, 2008), statistical aspects
of genetic relationship (e.g. Baxter and Manaster
Ramer, 2000; Kessler, 2001; Mortarino, 2009),
and phonetic alignment (e.g. Kondrak, 2002;
Proki? et al, 2009; List, forthcoming).
While the supporters of these new automatic
methods would certainly agree that their greatest
advantage lies in the increase of repeatability and
objectivity, it is interesting to note that the most
crucial part of the analysis, namely the identifica-
tion of cognates in lexicostatistical datasets, is still
almost exclusively carried out manually. That this
may be problematic was recently shown in a com-
parison of two large lexicostatistical datasets pro-
duced by different scholarly teams where differ-
ences in item translation and cognate judgments
led to topological differences of 30% and more
(Geisler and List, forthcoming). Unfortunately,
automatic approaches to cognate detection still
lack the precision of trained linguists? judgments.
Furthermore, most of the methods that have been
proposed so far only deal with bilingual as op-
posed to multilingual wordlists.
The LexStat method, which will be presented
in the following, is a convenient tool which not
only closely renders the most important aspects of
manual approaches but also yields transparent de-
cisions that can be directly compared with the re-
sults achieved by the traditional methods.
2 Identification of Cognates
2.1 The Comparative Method
In historical linguistics, cognacy is traditionally
determined within the framework of the compar-
ative method (Trask, 2000, 64-67). The final
goal of this method is the reconstruction of proto-
languages, yet the basis of the reconstruction it-
self rests on the identification of cognate words or
morphemes within genetically related languages.
Within the comparative method, cognates in a
given set of language varieties are identified by
applying a recursive procedure. First an initial list
of putative cognate sets is created by comparing
semantically and phonetically similar words from
the languages to be investigated. In most of the lit-
erature dealing with the comparative method, the
question of which words are most suitable for the
initial compilation of cognate lists is not explic-
itly addressed, yet it seems obvious that the com-
paranda should belong to the basic vocabulary of
the languages. Based on this cognate list, an ini-
117
tial list of putative sound correspondences (corre-
spondence list) is created. Sound correspondences
are determined by aligning the cognate words and
searching for sound pairs which repeatedly oc-
cur in similar positions of the presumed cognate
words. After these initial steps have been made,
the cognate list and the correspondence list are
modified by
1. adding and deleting cognate sets from the
cognate list depending on whether or not they
are consistent with the correspondence list,
and
2. adding and deleting sound correspondences
from the correspondence list, depending on
whether or not they find support in the cog-
nate list.
These steps are repeated until the results seem sat-
isfying enough such that no further modifications,
neither of the cognate list, nor of the correspon-
dence list, seem to be necessary.
The specific strength of the comparativemethod
lies in the similarity measure which is applied for
the identification of cognates: Sequence similar-
ity is determined on the basis of systematic sound
correspondences (Trask, 2000, 336) as opposed to
similarity based on surface resemblances of pho-
netic segments. Thus, comparing English token
[t??k?n] and German Zeichen [?a???n] ?sign?,
the words do not really sound similar, yet their
cognacy is assumed by the comparative method,
since their phonetic segments can be shown to cor-
respond regularly within other cognates of both
languages.1 Lass (1997, 130) calls this notion
of similarity genotypic as opposed to a pheno-
typic notion of similarity, yet the most crucial as-
pect of correspondence-based similarity is that it is
language-specific: Genotypic similarity is never
defined in general terms but always with respect
to the language systems which are being com-
pared. Correspondence relations can therefore
only be established for individual languages, they
can never be taken as general statements. This
may seem to be a weakness, yet it turns out that
the genotypic similarity notion is one of the most
crucial strengths of the comparative method: Not
1Compare, for example, English weak [wi?k] vs. Ger-
man weich [va??] ?soft? for the correspondence of [k] with
[?], and English tongue [t??] vs. German Zunge [????]
?tongue? for the correspondence of [t] with [?].
only does it allow us to dive deeper in the his-
tory of languages in cases where phonetic change
has corrupted the former identity of cognates to
such an extent that no sufficient surface similarity
is left, it also makes it easier to distinguish bor-
rowed from commonly inherited items, since the
former usually come along with a greater degree
of phenotypic similarity.
2.2 Automatic Approaches
In contrast to the language-specific notion of simi-
larity that serves as the basis for cognate detection
within the framework of the comparative method,
most automatic methods seek to determine cog-
nacy on the basis of surface similarity by calcu-
lating the phonetic distance or similarity between
phonetic sequences (words, morphemes).
The most popular distance measures are based
on the paradigm of sequence alignment. In align-
ment analyses two ormore sequences are arranged
in a matrix in such a way that all correspond-
ing segments appear in the same column, while
empty cells of the matrix, resulting from non-
corresponding segments, are filled with gap sym-
bols (Gusfield, 1997, 216). Table 1 gives an ex-
ample for the alignment of German Tochter [t?x-
t?r] ?daughter? and English daughter [d??t?r]:
Here, all corresponding segments are inserted in
the same columns, while the velar fricative [x] of
the German sequence which does not have a cor-
responding segment in the English word is repre-
sented by a gap symbol.
German t ? x t ? r
English d ?? - t ? r
Table 1: Alignment Analysis
In order to retrieve a distance or a similar-
ity score from such an alignment analysis, the
matched residue pairs, i.e. the segments which
appear in the same column of the alignment, are
compared and given a specific score depending on
their similarity. How the phonetic segments are
scored depends on the respective scoring function
which is the core of all alignment analyses. Thus,
the scoring function underlying the edit distance
only distinguishes identical from non-identical
segments, while the scoring function used in the
ALINE algorithm of Kondrak (2002) assigns in-
dividual similarity scores for the matching of pho-
netic segments based on phonetic features.
118
Using alignment analyses, cognacy can be de-
termined by converting the distance or similarity
scores to normalized distance scores and assum-
ing cognacy for distances beyond a certain thresh-
old. The normalized edit distance (NED) of two
sequencesA andB is usually calculated by divid-
ing the edit distance by the length of the small-
est sequence. The normalized distance score of
algorithms which yield similarities (such as the
ALINE algorithm) can be calculated by the for-
mula of Downey et al (2008):
(1) 1? 2SAB
SA + SB
,
where SA and SB are the similarity scores of the
sequences aligned with themselves, and SAB is
the similarity score of the alignment of both se-
quences. For the alignment given in Table 1, the
normalized edit distance is 0.6, and the ALINE
distance is 0.25.
A certain drawback of most of the common
alignment methods is that their scoring function
defines segment similarity on the basis of phe-
notypic criteria. The similarity of phonetic seg-
ments is determined on the basis of their phonetic
features and not on the basis of the probability
that their segments occur in a correspondence re-
lation in genetically related languages. An alter-
native way to calculate phonetic similarity which
comes closer to a genotypic notion of similarity
is to compare phonetic sequences with respect to
their sound classes. The concept of sound classes
goes back to Dolgopolsky (1964). The original
idea was ?to divide sounds into such groups, that
changes within the boundary of the groups are
more probable than transitions from one group
into another? (Burlak and Starostin, 2005, 272)2.
In his original study, Dolgopolsky proposed ten
fundamental sound classes, based on an empirical
analysis of sound-correspondence frequencies in a
sample of 400 languages. Cognacy between two
words is determined by comparing the first two
consonants of both words. If the sound classes are
identical, the words are judged to be cognate. Oth-
erwise no cognacy is assumed. Thus, given the
words German Tochter [t?xt?r] ?daughter? and
English daughter [d??t?r], the sound class rep-
resentation of both sequences will be TKTR and
2My translation, original text: ?[...] ???????? ?????
?????? ??????, ??? ????????? ? ???????? ?????? ?????
????????, ??? ???????? ?? ????? ?????? ? ???????.
TTR, respectively. Since the first two consonants
of both words do not match regarding their sound
classes, the words are judged to be non-cognate.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Distances
400
200
0
200
400
Seq
uen
ce P
airs
SCA cog.
SCA non-cog.
NED cog.
NED non-cog.
1
Figure 1: SCA Distance vs. NED
In recent studies, sound classes have also been
used as an internal representation format for
pairwise and multiple alignment analyses. The
method for sound-class alignment (SCA, cf. List,
forthcoming) combines the idea of sound classes
with traditional alignment algorithms. In contrast
to the original proposal by Dolgopolsky, SCA
employs an extended sound-class model which
also represents tones and vowels along with a re-
fined scoring scheme that defines specific transi-
tion probabilities between sound classes. The ben-
efits of the SCA distance compared to NED can
be demonstrated by comparing the distance scores
the methods yield for the comparison of the same
data. Figure 1 contrasts the scores of NED with
SCA distance for the alignment of 658 cognate
and 658 non-cognate word pairs between English
and German (see Sup. Mat. A). As can be seen
from the figure, the scores for NED do not show
a very sharp distinction between cognate and non-
cognate words. Even with a ?perfect? threshold
of 0.8 that minimizes the number of false positive
and false negative decisions there are still 13% of
incorrect decisions. The SCA scores, on the other
hand, show a sharper distinction between scores
for cognates and non-cognates. With a threshold
of 0.5 the percentage of incorrect decisions de-
creases to 8%.
There are only three recent approaches known
to the author which explicitly deal with the task of
cognate detection in multilingual wordlists. All
methods take multilingual, semantically aligned
119
wordlists as input data. Bergsma and Kondrak
(2007) first calculate the longest common sub-
sequence ratio between all word pairs in the in-
put data and then use an integer linear program-
ming approach to cluster the words into cognate
sets. Unfortunately, their method is only tested
on a dataset containing alphabetic transcriptions;
hence, no direct comparison with the method
proposed in this paper is possible. Turchin et
al. (2010) use the above-mentioned sound-class
model and the cognate-identification criterion by
Dolgopolsky (1964) to identify cognates in lexi-
costatistical datasets. Their method is also imple-
mented within LexStat, and the results of a direct
comparisonwill be reported in section 4.3. Steiner
et al (2011) propose an iterative approach which
starts by clustering words into tentative cognate
sets based on their alignment scores. These pre-
liminary results are then refined by filtering words
according to similar meanings, computing multi-
ple alignments, and determining recurrent sound
correspondences. The authors test their method
on two large datasets. Since no gold standard for
their test set is available, they only report interme-
diate results, and their method cannot be directly
compared to the one proposed in this paper.
3 LexStat
LexStat combines the most important aspects of
the comparative method with recent approaches
to sequence comparison in historical linguistics
and evolutionary biology. The method employs
automatically extracted language-specific scor-
ing schemes for computing distance scores from
pairwise alignments of the input data. These
language-specific scoring schemes come close to
the notion of sound correspondences in traditional
historical linguistics.
The method is implemented as a part of the
LingPy library, a Python library for automatic
tasks in quantitative historical linguistics.3 It can
either be used in Python scripts or directly be
called from the Python prompt.
The input data are analyzed within a four-step
approach: (1) sequence conversion, (2) scoring-
scheme creation, (3) distance calculation, and (4)
sequence clustering. In stage (1), the input se-
quences are converted to sound classes and their
3Online available under http://lingulist.de/
lingpy/.
sonority profiles are determined. In stage (2), a
permutation method is used to create language-
specific scoring schemes for all language pairs.
In stage (3) the pairwise distances between all
word pairs, based on the language-specific scor-
ing schemes, are computed. In stage (4), the se-
quences are clustered into cognate sets whose av-
erage distance is beyond a certain threshold.
3.1 Input and Output Format
The method takes multilingual, semantically
aligned wordlists in IPA transcription as input.
The input format is a CSV-representation of the
way multilingual wordlists are represented in the
STARLING software package for lexicostatistical
analyses.4 Thus, the input data are specified in a
simple tab-delimited text file with the names of the
languages in the first row, an ID for the semantic
slots (basic vocabulary items in traditional lexico-
statistic terminology) in the first column, and the
language entries in the columns corresponding to
the language names. The language entries should
be given either in plain IPA encoding. Addition-
ally, the file can contain headwords (items) for se-
mantic slots corresponding to the IDs. Synonyms,
i.e. multiple entries in one language for a given
meaning are listed in separate rows and given the
same ID. Table 2 gives an example for the possible
structure of an input file.
ID Items German English Swedish
1 hand hant h?nd hand
2 woman fra? w?m?n kvina
3 know k?n?n n?? ??na
3 know v?s?n - ve?ta
Table 2: LexStat Input Format
The output format is the same as the input for-
mat except that each language column is accom-
panied by a column indicating the cognate judg-
ments made by LexStat. Cognate judgments are
displayed by assigning a cognate ID to each entry.
If entries in the output file share the same cognate
ID, they are judged to be cognate by the method.
3.2 Sequence Conversion
In the stage of sequence conversion, all input se-
quences are converted to sound classes, and their
4Online available under http://starling.
rinet.ru/program.php; a closer description of the
software is given in Burlak and Starostin (2005, 270-275)
120
respective sonority profiles are calculated. Lex-
Stat uses the SCA sound-class model by default,
yet other sound class models are also available.
The idea of sonority profiles was developed
in List (forthcoming). It accounts for the well-
known fact that certain types of sound changes are
more likely to occur in specific prosodic contexts.
Based on the sonority hierarchy of Geisler (1992,
30), the sound segments of phonetic sequences
are assigned to different prosodic environments,
depending on their prosodic context. The cur-
rent version of SCA distinguishes seven different
prosodic environments.5 The information regard-
ing sound classes and prosodic context are com-
bined, and each input sequence is further repre-
sented as a sequence of tuples, consisting of the
sound class and the prosodic environment of the
respective phonetic segment. During the calcula-
tion, only those segments which are identical re-
garding their sound class as well as their prosodic
context are treated as identical.
3.3 Scoring-Scheme Creation
In order to create language specific scoring
schemes, a permutation method is used (Kessler,
2001). The method compares the attested distri-
bution of residue pairs in phonetic alignment anal-
yses of a given dataset to the expected distribution.
The attested distribution of residue pairs is de-
rived from global and local alignment analyses of
all word pairs whose distance is beyond a cer-
tain threshold. The threshold is used to reflect the
fact that within the comparative method, recurrent
sound correspondences are only established with
respect to presumed cognate words, whereas non-
cognate words or borrowings are ignored. Tak-
ing only the best-scoring word pairs for the cal-
culation of the attested frequency distribution in-
creases the accuracy of the approach and helps to
avoid false positive matches contributing to the
creation of the scoring scheme. Alignment analy-
ses are carried out with help of the SCA method.
While the attested distribution is derived from
alignments of semantically aligned words, the ex-
pected distribution is calculated by aligning word
pairs without regard to semantic criteria. This
is achieved by repeatedly shuffling the wordlists
5The different environments are: # (word-initial, cons.),
V (word-initial, vow.), C (ascending sonority, cons.), v (max-
imum sonority, vow.), c (descending sonority, cons.), $
(word-final, cons.), and > (word-final, vow.).
and aligning them with help of the same methods
which were used for the calculation of the attested
distributions. In the default settings, the number
of repetitions is set to 1000, yet many tests showed
that even the number of 100 repetitions is suffi-
cient to yield satisfying results that do not vary
significantly.
Once the attested and the expected distributions
for the segments of all language pairs are cal-
culated, a language-specific score sx,y for each
residue pair x and y in the dataset is created us-
ing the formula
(2) sx,y =
1
r1 + r2
(
r1 log2
(
a2x,y
e2x,y
)
+ r2dx,y
)
,
where ax,y is the attested frequency of the segment
pair, ex,y is the expected frequency, r1 and r2 are
scaling factors, and dx,y is the similarity score of
the original scoring function which was used to
retrieve the attested and the expected distributions.
Formula (2) combines different approaches
from the literature on sequence comparison in his-
torical linguistics and biology. The idea of squar-
ing the frequencies of attested and expected fre-
quencies was adopted from Kessler (2001, 150),
reflecting ?the general intuition among linguists
that the evidence of phoneme recurrence grows
faster than linearly?. Using the binary loga-
rithm of the division of attested and expected fre-
quencies of occurrence is common in evolution-
ary biology to retrieve similarity scores (?log-
odds scores?) which are apt for the computation
of alignment analyses (Henikoff and Henikoff,
1992). The incorporation of the alignment scores
of the original language-independent scoring-
scheme copes with possible problems resulting
from small wordlists: If the dataset is too small to
allow the identification of recurrent sound corre-
spondences, the language-independent alignment
scores prevent the method from treating gener-
ally probable and generally improbable matchings
alike. The ratio of language-specific to language-
independent alignment scores is determined by the
scaling factors r1 and r2.
As an example of the computation of language-
specific scoring schemes, Table 3 shows attested
and expected frequencies along with the resulting
similarity scores for the matching of word-initial
and word-final sound classes in the KSL testset
(see Sup. Mat. B and C). The word-initial and
word-final classes T = [t, d], C = [?], S = [?, s, z]
121
English German Att. Exp. Score
#[t,d] #[t,d] 3.0 1.24 6.3
#[t,d] #[?] 3.0 0.38 6.0
#[t,d] #[?,s,z] 1.0 1.99 -1.5
#[?,?] #[t,d] 7.0 0.72 6.3
#[?,?] #[?] 0.0 0.25 -1.5
#[?,?] #[s,z] 0.0 1.33 0.5
[t,d]$ [t,d]$ 21.0 8.86 6.3
[t,d]$ [?]$ 3.0 1.62 3.9
[t,d]$ [?,s]$ 6.0 5.30 1.5
[?,?]$ [t,d]$ 4.0 1.14 4.8
[?,?]$ [?]$ 0.0 0.20 -1.5
[?,?]$ [?,s]$ 0.0 0.80 0.5
Table 3: Attested vs. Expected Frequencies
in German are contrasted with the word-initial and
word-final sound classes T = [t, d] and D = [?, ?]
in English. As can be seen from the table, the scor-
ing scheme correctly reflects the complex sound
correspondences between English and German re-
sulting from the High German Consonant Shift
(Trask, 2000, 300-302), which is reflected in such
cognate pairs as English town [ta?n] vs. Ger-
man Zaun [?aun] ?fence?, English thorn [???n]
vs. German Dorn [d?rn] ?thorn?, English dale
[de?l] vs. German Tal ?valley? [ta?l], and English
hot [h?t] vs. German hei? [ha?s] ?hot?. The spe-
cific benefit of representing the phonetic segments
as tuples consisting of their respective sound class
along with their prosodic context also becomes
evident: The correspondence of English [t] with
German [s] is only attested in word-final position,
correctly reflecting the complex change of former
[t] to [s] in non-initial position in German. If it
were not for the specific representation of the pho-
netic segments by both their sound class and their
prosodic context, the evidence would be blurred.
3.4 Distance Calculation
Once the language-specific scoring scheme is
computed, the distances between all word pairs
are calculated. Here, LexStat uses the ?end-space
free variant? (Gusfield, 1997, 228) of the tradi-
tional algorithm for pairwise sequence alignments
which does not penalize gaps introduced in the be-
ginning and the end of the sequences. This mod-
ification is useful when words contain prefixes or
suffixes which might distort the calculation. The
alignment analysis requires no further parameters
such as gap penalties, since they have already been
calculated in the previous step. The similarity
scores for pairwise alignments are converted to
distance scores following the approach of Downey
et al (2008) which was described in section 2.2.
Word Pair SCA LexStat
German Schlange [?la??]
English Snake [sne?k] 0.44 0.67
German Wald [valt]
English wood [w?d] 0.40 0.64
German Staub [?taup]
English dust [d?st] 0.43 0.78
Table 4: SCA Distance vs. LexStat Distance
The benefits of the language-specific distance
scores become obvious when comparing them
with general ones. Table 4 gives some exam-
ples for non-cognate word pairs taken from the
KSL testset (see Sup. Mat. B and C). While the
SCA distances for these pairs are all considerably
low, as it is suggested by the surface similarity of
the words, the language-specific distances are all
much higher, resulting from the fact that no fur-
ther evidence for the matching of specific residue
pairs can be found in the data.
3.5 Sequence Clustering
In the last step of the LexStat algorithm all se-
quences occurring in the same semantic slot are
clustered into cognate sets using a flat cluster vari-
ant of the UPGMA algorithm (Sokal and Mich-
ener, 1958) which was written by the author. In
contrast to traditional UPGMA clustering, this al-
gorithm terminates when a user-defined threshold
of average pairwise distances is reached.
Ger. Eng. Dan. Swe. Dut. Nor.
Ger. [frau] 0.00 0.95 0.81 0.70 0.34 1.00
Eng. [w?m?n] 0.95 0.00 0.78 0.90 0.80 0.80
Dan. [kven?] 0.81 0.78 0.00 0.17 0.96 0.13
Swe. [kvin?a] 0.70 0.90 0.17 0.00 0.86 0.10
Dut. [vr?u?] 0.34 0.80 0.96 0.86 0.00 0.89
Nor. [k?in?] 1.00 0.80 0.13 0.10 0.89 0.00
Clusters 1 2 3 3 1 3
Table 5: Pairwise Distance Matrix
Table 5 shows pairwise distances of German,
English, Danish, Swedish, Dutch, and Norwegian
entries for the itemWOMAN taken from the GER
dataset (see Sup. Mat. B) along with the resulting
122
cluster decisions of the algorithm when setting the
threshold to 0.6.
4 Evaluation
4.1 Gold Standard
In order to test the method, a gold standard was
compiled by the author. The gold standard con-
sists of 9 multilingual wordlists conforming to the
input format required by LexStat (see Supplemen-
tary Material B). The data was collected from dif-
ferent publicly available sources. Hence, the se-
lection of language entries as well as the man-
ually conducted cognate judgments were carried
out independently of the author. Since not all the
original sources provided phonetic transcriptions
of the language entries, the respective alphabetic
entries were converted to IPA transcription by the
author. The datasets differ regarding the treatment
of borrowings. In some datasets they are explic-
itly marked as such and treated as non-cognates, in
other datasets no explicit distinction between bor-
rowing and cognacy is drawn. Information on the
structure and the sources of the datasets is given
in Table 6.
File Family Lng. Itm. Entr. Source
GER Germanic 7 110 814 Starostin (2008)
ROM Romance 5 110 589 Starostin (2008)
SLV Slavic 4 110 454 Starostin (2008)
PIE Indo-Eur. 18 110 2057 Starostin (2008)
OUG Uralic 21 110 2055 Starostin (2008)
BAI Bai 9 110 1028 Wang (2006)
SIN Sinitic 9 180 1614 H?u (2004)
KSL varia 8 200 1600 Kessler (2001)
JAP Japonic 10 200 1986 Shir? (1973)
Table 6: The Gold Standard
4.2 Evaluation Measures
Bergsma andKondrak (2007) test their method for
automatic cognate detection by calculating the set
precision (PRE), the set recall (REC), and the set
F-score (FS): The set precision p is the proportion
of cognate sets calculated by the method which
also occurs in the gold standard. The set recall r is
the proportion of cognate sets in the gold standard
which are also calculated by the method, and the
set F-score f is calculated by the formula
(3) f = 2 pr
p+ r
.
A certain drawback of these scores is that they
only check for completely identical decisions re-
garding the clustering of words into cognate sets
while neglecting similar tendencies. The similar-
ity of decisions can be evaluated by calculating the
proportion of identical decisions (PID)when com-
paring the test results with those of the gold stan-
dard. Given all pairwise decisions regarding the
cognacy of word pairs inherent in the gold stan-
dard and in the testset, the differences can be dis-
played using a contingency table, as shown in Ta-
ble 7.
Cognate Non-Cognate
Gold Standard Gold Standard
Cognate
Testset true positives false positives
Non-Cognate
Testset false negatives true negatives
Table 7: Comparing Gold Standard and Testset
The PID score can then simply be calculated by
dividing the sum of true positives and true nega-
tives by the total number of decisions. In an analo-
gous way the proportion of identical positive deci-
sions (PIPD) and the proportion of identical nega-
tive decisions (PIND) can be calculated by divid-
ing the number of true positives by the sum of true
positives and false negatives, and by dividing the
number of false positives by the sum of false pos-
itives and true negatives, respectively.
4.3 Results
Based on the new method for automatic cognate
detection, the 9 testsets were analyzed by Lex-
Stat, using a gap penalty of -2 for the alignment
analysis, a threshold of 0.7 for the creation of
the attested distribution, and 1:1 as the ratio of
language-specific to language-independent simi-
larity scores. The threshold for the clustering of
sequences into cognate sets was set to 0.6. In order
to compare the output of LexStat with other meth-
ods, three additional analyses of the datasets were
carried out: The first two analyses were based on
the calculation of SCA and NED distances of all
language entries. Based on these scores all words
were clustered into cognate sets using the flat clus-
ter variant of UPGMA with a threshold of 0.4 for
SCA distances and a threshold of 0.7 for NED,
since these both turned out to yield the best results
for these approaches. The third analysis was based
on the above-mentioned approach by Turchin et
al. (2010). Since in this approach all decisions re-
123
garding cognacy are either positive or negative, no
specific cluster algorithm had to be applied.
Score LexStat SCA NED Turchin
PID 0.85 0.82 0.76 0.74
PIPD 0.78 0.75 0.66 0.56
PIND 0.93 0.90 0.86 0.94
PRE 0.59 0.51 0.39 0.39
REC 0.68 0.57 0.47 0.55
FS 0.63 0.55 0.42 0.46
Table 8: Performance of the Methods
The results of the tests are summarized in Ta-
ble 8. As can be seen from the table, LexStat
outperforms the other methods in almost all re-
spects, the only exception being the proportion of
identical negative decisions (PIND). Since non-
identical negative decisions point to false posi-
tives, this shows that ? for the given settings of
LexStat ? the method of Turchin et al (2010) per-
forms best at avoiding false positive cognate judg-
ments, but it fails to detect many cognates cor-
rectly identified by LexStat.6 Figure 2 gives the
separate PID scores for all datasets, showing that
LexStat?s good performance is prevalent through-
out all datasets. The fact that all methods per-
form badly on the PIE dataset may point to prob-
lems resulting from the size of the wordlists: if
the dataset is too small and the genetic distance of
the languages too large, one may simply lack the
evidence to prove cognacy without doubt.
SLV KSL GER BAI SIN PIE ROM JAP OUG
0.6
0.7
0.8
0.9
1.0
LexStat
SCA
NED
Turchin
1
Figure 2: PID Scores of the Methods
6LexStat can easily be adjusted to avoid false positives
by lowering the threshold for sequence clustering. Using a
threshold of 0.5 will yield a PIND score of 0.96, yet the PID
score will lower down to 0.82.
The LexStat method was designed to distin-
guish systematic from non-systematic similarities.
The method should therefore produce less false
positive cognate judgments resulting from chance
resemblances and borrowings than the other meth-
ods. In the KSL dataset borrowings are marked
along with their sources. Out of a total of 5600
word pairs, 72 exhibit a loan relation, and 83 are
phonetically similar (with an NED score less then
0.6) but unrelated. Table 9 lists the number and the
percentage of false positives resulting from unde-
tected borrowings or chance resemblances for the
different methods (see also Sup. Mat. D). While
LexStat outperforms the other methods regarding
the detection of chance resemblances, it is not
particularly good at handling borrowings. Lex-
Stat cannot per se deal with borrowings, but only
with language-specific as opposed to language-
independent similarities. In order to handle bor-
rowings, other methods (such as, e.g., the one by
Nelson-Sathi et al, 2011) have to be applied.
LexStat SCA NED Turchin
Borr. 36 / 50% 44 / 61% 35 / 49% 38 / 53%
Chance R. 14 / 17% 35 / 42% 74 / 89% 26 / 31%
Table 9: Borrowings and Chance Resemblances
5 Conclusion
In this paper, a new method for automatic cognate
detection in multilingual wordlists has been pre-
sented. The method differs from other approaches
in so far as it employs language-specific scoring
schemes which are derived with the help of im-
proved methods for automatic alignment analy-
ses. The test of the method on a large dataset of
wordlists taken from different language families
shows that it is consistent regardless of the lan-
guages being analyzed and outperforms previous
approaches.
In contrast to the black box character of many
automatic analyses which only yield total scores
for the comparison of wordlists, the method yields
transparent decisions which can be directly com-
pared with the traditional results of the compar-
ative method. Apart from the basic ideas of the
procedure, which surely are in need of enhance-
ment through reevaluation and modification, the
most striking limit of the method lies in the data:
If the wordlists are too short, certain cases of cog-
nacy are simply impossible to be detected.
124
References
William H. Baxter and Alexis Manaster Ramer. 2000.
Beyond lumping and splitting. Probabilistic issues
in historical linguistics. In Colin Renfrew, April
McMahon, and Larry Trask, editors, Time depth in
historical linguistics, pages 167?188. McDonald In-
stitute for Archaeological Research, Cambridge.
Shane Bergsma and Grzegorz Kondrak. 2007. Mul-
tilingual cognate identification using integer lin-
ear programming. In RANLP Workshop on Acqui-
sition and Management of Multilingual Lexicons,
Borovets, Bulgaria.
Cecil H. Brown, Eric W. Holman, S?ren Wich-
mann, Viveka Velupillai, and Michael Cysouw.
2008. Automated classification of the world?s
languages. Sprachtypologie und Universalien-
forschung, 61(4):285?308.
Svetlana A. Burlak and Sergej A. Starostin.
2005. Sravnitel?no-istori?eskoe jazykoznanie
[Comparative-historical linguistics]. Akademia,
Moscow.
Aron B. Dolgopolsky. 1964. Gipoteza drevne-
j?ego rodstva jazykovych semej Severnoj Evrazii s
verojatnostej to?ky zrenija [A probabilistic hypoth-
esis concerning the oldest relationships among the
language families of Northern Eurasia]. Voprosy
Jazykoznanija, 2:53?63.
Sean S. Downey, Brian Hallmark, Murray P. Cox, Pe-
ter Norquest, and Stephen Lansing. 2008. Com-
putational feature-sensitive reconstruction of lan-
guage relationships: Developing the ALINE dis-
tance for comparative historical linguistic recon-
struction. Journal of Quantitative Linguistics,
15(4):340?369.
Hans Geisler and Johann-Mattis List. forthcoming.
Beautiful trees on unstable ground. Notes on the data
problem in lexicostatistics. In Heinrich Hettrich, ed-
itor,Die Ausbreitung des Indogermanischen. Thesen
aus Sprachwissenschaft, Arch?ologie und Genetik.
Reichert, Wiesbaden.
Hans Geisler. 1992. Akzent und Lautwandel in der
Romania. Narr, T?bingen.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426(6965):435?439.
Dan Gusfield. 1997. Algorithms on strings, trees
and sequences. Cambridge University Press, Cam-
bridge.
Steven Henikoff and Jorja G. Henikoff. 1992. Amino
acid substitution matrices from protein blocks.
PNAS, 89(22):10915?10919.
J?ng H?u, editor. 2004. Xi?nd?i H?ny? f?ngy?n
y?nk? [Phonological database of Chinese dialects].
Sh?ngh?i Ji?oy?, Shanghai.
Brett Kessler. 2001. The significance of word lists.
Statistical tests for investigating historical connec-
tions between languages. CSLI Publications, Stan-
ford.
Grzegorz Kondrak. 2002. Algorithms for language
reconstruction. Dissertation, University of Toronto,
Toronto.
Roger Lass. 1997. Historical linguistics and language
change. Cambridge University Press, Cambridge.
Johann-Mattis List. forthcoming. SCA: Phonetic
alignment based on sound classes. In Marija
Slavkovik and Dan Lassiter, editors, New direc-
tions in logic, language, and computation. Springer,
Berlin and Heidelberg.
Cinzia Mortarino. 2009. An improved statistical test
for historical linguistics. Statistical Methods and
Applications, 18(2):193?204.
Shijulal Nelson-Sathi, Johann-Mattis List, Hans
Geisler, Heiner Fangerau, Russell D. Gray, William
Martin, and Tal Dagan. 2011. Networks uncover
hidden lexical borrowing in Indo-European lan-
guage evolution. Proceedings of the Royal Society
B, 278(1713):1794?1803.
Jelena Proki?, Martijn Wieling, and John Nerbonne.
2009. Multiple sequence alignments in linguis-
tics. In Proceedings of the EACL 2009 Workshop on
Language Technology and Resources for Cultural
Heritage, Social Sciences, Humanities, and Educa-
tion, pages 18?25, Stroudsburg, PA. Association for
Computational Linguistics.
Donald Ringe, Tandy Warnow, and Ann Taylor. 2002.
Indo-european and computational cladistics. Trans-
actions of the Philological Society, 100(1):59?129.
Hattori Shir?. 1973. Japanese dialects. In Henry M.
Hoenigswald and Robert H. Langacre, editors, Di-
achronic, areal and typological linguistics, pages
368?400. Mouton, The Hague and Paris.
Robert. R. Sokal and Charles. D. Michener. 1958.
A statistical method for evaluating systematic rela-
tionships. University of Kansas Scientific Bulletin,
28:1409?1438.
George Starostin. 2008. Tower of Babel. An etymo-
logical database project. Online ressource. URL:
http://starling.rinet.ru.
Lydia Steiner, Peter F. Stadler, and Michael Cysouw.
2011. A pipeline for computational historical
linguistics. Language Dynamics and Change,
1(1):89?127.
Robert L. Trask, editor. 2000. The dictionary of his-
torical and comparative linguistics. Edinburgh Uni-
versity Press, Edinburgh.
Peter Turchin, Ilja Peiros, and Murray Gell-Mann.
2010. Analyzing genetic connections between lan-
guages by matching consonant classes. Journal of
Language Relationship, 3:117?126.
Feng Wang. 2006. Comparison of languages in
contact. Institute of Linguistics Academia Sinica,
Taipei.
125
