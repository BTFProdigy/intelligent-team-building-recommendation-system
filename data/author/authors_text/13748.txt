Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 3?10,
Beijing, August 2010
Filtering news for epidemic surveillance:
towards processing more languages with fewer resources
Gae?l Lejeune1, Antoine Doucet1,
1GREYC, University of Caen
first.last@info.unicaen.fr
Roman Yangarber2, Nadine Lucas1
2CS department, University of Helsinki
yangarbe@cs.helsinki.fi
Abstract
Processing content for security be-
comes more and more important since
every local danger can have global
consequences. Being able to collect
and analyse information in different
languages is a great issue. This pa-
per addresses multilingual solutions
for analysis of press articles for epi-
demiological surveillance. The sys-
tem described here relies on pragmat-
ics and stylistics, giving up ?bag of
sentences? approach in favour of dis-
course repetition patterns. It only
needs light resources (compared to
existing systems) in order to process
new languages easily. In this pa-
per we present here results in En-
glish, French and Chinese, three lan-
guages with quite different character-
istics. These results show that simple
rules allow selection of relevant doc-
uments in a specialized database im-
proving the reliability of information
extraction.
1 Multilingual techniques in
information extraction
In natural language processing, information
extraction is a task where, given raw text, a
system is to give precise information fitting
in a predefined semantic template.
1.1 Epidemic surveillance
Automated news surveillance is an important
application of information extraction. The
detection of terrorist events and economic
surveillance were the first applications, in
particular in the framework of the evalua-
tion campaigns of the Message Understand-
ing Conference (MUC) (MUC, 1992; MUC,
1993). In MUC-3 (1991) and MUC-4 (1992),
about terrorism in Latin American countries,
the task of participants was, given a collec-
tion of news feed data, to fill in a prede-
termined semantic template containing the
name of the terrorist group that perpetrated
a terrorist event, the name of the victim(s),
the type of event, and the date and location
where it occurred. In economic surveillance,
one can for instance extract mergers or cor-
porate management changes.
An application of information extraction
that lately gained much importance is that
of epidemiological surveillance, with a spe-
cial emphasis on the detection of disease out-
breaks. Given news data, the task is to de-
tect epidemiological events, and extract the
location where they occurred, the name of
the disease, the number of victims, and the
?case?, that is, a text description of the
event, that may be the ?status? of victims
(sick, injured, dead, hospitalised . . . ) or a
written description of symptoms. Epidemio-
logical surveillance has become a crucial tool
with increasing world travel and the latest
crises of SARS, avian flu, H1N1 . . .
In this paper, we present an application to
epidemic surveillance, but it may be equally
applied to any subdomain of news surveil-
lance.
1.2 Multilingual information
extraction
As in many fields of NLP, most of the work in
information extraction long focused on En-
glish data (Etzioni et al, 2008).Multilingual
has often been understood as adding many
3
monolingual systems, except in pioneer mul-
tilingual parsing (Vergne, 2002). Whereas
English is nowadays the lingua franca in
many fields (in particular, business), we will
see that for several applications, this is not
sufficient. Most news agencies are translat-
ing part of their feed into English (e.g., AFP1
and Xinhua2 for which the source languages
are respectively French and Chinese), but a
good deal of the data is never translated,
while for the part that is, the translation
process naturally incurs a delay that is, by
essence, problematic in a field where exhaus-
tivity and early detection are crucial aspects.
Subsequently, the ability to simultane-
ously handle documents written in different
languages is becoming a more and more im-
portant feature (Poibeau et al, 2008; Gey et
al., 2009). Indeed, in the field of epidemio-
logical surveillance, it is especially important
to detect a new event the very first time it is
mentioned, and this very first occurrence will
almost always happen in the local language
(except for countries like Iraq for instance).
Therefore, it is not enough to be able to deal
with several languages : It is necessary to
handle many. For instance, the Medical In-
formation System (Medisys) of the European
Community gathers news data in 42 differ-
ent languages (Atkinson and der Goot, 2009)
(now 453).
1.3 Current approaches
There are currently 2 main approaches to
multilingual information extraction. The
first approach relies on the prior transla-
tion of all the documents into one com-
mon language (usually English), for which a
well-performing information extraction sys-
tem has been developed (Linge et al, 2009).
Whereas the simple design of this solution
is attractive, the current state of the art in
machine translation only allows for mediocre
results. Most monolingual information ex-
traction systems indeed rely on a combina-
1http://www.afp.com/afpcom/en
2http://www.xinhuanet.com/english2010/
3http://medusa.jrc.it/medisys/aboutMediSys.html
tion of grammatical patterns and specialized
lexicons (Grishman et al, 2002; Riloff, 1996).
The second main approach consists in leav-
ing documents in their original language but
to translate the lexicons and extraction pat-
terns into that language (Efimenko et al,
2004; Linge et al, 2009). However, the
same problems occur as in the first approach
because the patterns are strongly language-
related. Yet, to ?translate the system? seems
more realistic than to translate the docu-
ments, as it can be done manually, and of-
fline (once and for all, and not as docu-
ments arrive). The bottleneck is then that
the amount of work for each language is
enormous: it naturally requires the com-
plete translation of the lexicon (for all trig-
ger words), but the more challenging is-
sue is the translation of patterns, whose
language-dependence might well mean that
the amount of work needed to translate them
comes close to that required for writing them
from scratch. In addition, this task must
necessarily be achieved by a domain expert,
with excellent skills in the languages at hand.
One could want to tackle this problem by us-
ing machine learning but she will need train-
ing data in many languages. In practice,
this will often mean that only a few major
languages will be dealt with, whilst all the
others (amongst which all low-resource lan-
guages), will again be totally discarded. One
can then only wish that epidemics will chose
to occur in locations handled by surveillance
systems. . .
Both approaches additionally require a
number of linguistic processing tools, in a
number comparable to the number of lan-
guages to be dealt with: tokenizer, stem-
mer, syntactic analyzer, . . . One might there-
fore conclude that such techniques are not
properly multilingual but rather monolingual
methods that may be adapted to other lan-
guages individually.
In this paper, we explore a third approach
to multilingual information extraction. We
restrain ourselves to the sole use of truly mul-
4
tilingual elements, facts that are equally true
for any language. The approach hence relies
on universals, relying, e.g., on stylistics and
rhetorics.
2 Rationale of the experiment
The objective of the system is to monitor
news in a variety of languages to detect dis-
ease outbreaks which is an important issue
for an alert system in epidemic surveillance.
For this task a simple and clear framework is
needed in order to limit the amount of work
for new languages while keeping good relia-
bility. The main idea of our work is using
text granularity and discourse properties to
write rules that may be language indepen-
dent, fast and reliable (Vergne, 2002). For
this study, regularities at text level are ex-
ploited. These phenomena can be related to
stylistics and pragmatics. It has already been
shown that news discourse has its own con-
straints reflected in press articles of different
languages (Van Dijk, 1988; Lucas, 2004).
2.1 Stylistic rules
Journalists all over the world know how to
hook their potential readers. These meth-
ods are described in journalism schools (Itule
and Anderson, 2006). One very important
rule for journalists seems to be the?5W rule?
which emphasise on the fact that answering
to the questions?What?,?Where?,?When?,
?Why? and ?Who? is a priority at the start
of a paper. Only after that can journal-
ists develop and give secondary information.
This phenomenon is genre dependent and is
exploited for processing texts by searching
for repetitions.
Example 1 shows a piece of news where the
disease name is found in the beginning of the
news article and developed later on. No local
pattern is needed to detect what the article
is about, repetition phenomena is sufficient.
Example 2 is a counter example, where
a disease name is found but not repeated.
This French document reports on a pop mu-
sic band being the ?coqueluche? of Hip-Hop,
which can mean ?pertussis?, but here means
?fashion? in a figurative sense (underlining
the fast spread of the band?s popularity).
Usually, figurative meanings are not used
twice in the same article (Itule and Ander-
son, 2006) and hence the repetition criteria
allows one to rightfully ignore this article.
2.2 Pragmatic rules
As press articles are made for humans, strong
effort is exerted to ensure that readers will
understand the main information with as few
inferences as possible (Sperber and Wilson,
1998). In fact, the more inferences the reader
has to make, the more errors he is likely to
make and the more probability he will get
confused and not read the full article. Rep-
etitions are there to relieve the memory ef-
fort. A point that journalists pay much at-
tention to is leaving as few ambiguities on
main facts as possible. It means that poten-
tially unknown or complicated terms will be
used quite rarely. Only one main story will
be developed in an article, other facts that
are important will be developed elsewhere as
main stories.
3 Our system
The system is based on the comparison of
repetitions in the article to find documents
relevant for epidemic surveillance and extract
where the disease occurs and how many peo-
ple are concerned.
3.1 String repetitions: relevant
content
A system is not a human reader, so objec-
tive discourse marks are used by the sys-
tem. Repetitions are known since the an-
cient times as reflecting discourse structure.
A press article is divided into two parts,
roughly the head and the rest of the news.
The title and the first two sentences form
the head or thematic part and the rest of
the text is considered to be a development in
an expository discourse.
5
Measles outbreak spreads north in B.C.
Number of cases hits 44 provincewide B.C.?s measles outbreak appears to have spread to
northeastern areas of the province, after doctors confirmed two new cases of the disease
in the Fort St. John and Fort Nelson areas on Thursday.
The new cases bring the total number of confirmed cases in the province to 44, not
including suspected but unconfirmed cases, said the B.C. Centre for Disease Control.
Northern Health spokeswoman Eryn Collins said the virus had not been detected in the
north in more than six years and the two new cases involve people who weren?t immunized.
[...] ?It is suspected that at least two out-of-country visitors brought measles into Van-
couver sometime in February or early March, as two separate strains of the virus have been
identified,? said a statement from the B.C. Centre for Disease Control earlier this week. So
far, 17 cases of the measles have been detected in the Fraser Valley, 17 in the Vancouver
area, seven in the southern Interior, two in northern B.C. and one on Vancouver island.
Figure 1: Example in English: repetition of disease name and cases
Cameroun/Musique : X-Maleya nouvelle coqueluche du Hip-Hop camerounais !
Le trio Hip-Hop Cameounais X-Maleya, a le vent en poupe. Le groupe qui s?illustre dans
la tendance Hip-Hop, est aujourd?hui l?une des valeurs sres musicales gra?ce son second
opus Yelele.
Derrie`re ces trois pre?noms : Roger, Auguste et Ha??s, se cachent un trio camerounais
qui s?illustre dans le monde du Hip-Hop. [etc.] C?est donc, une nouvelle valeur su?re
qu?incarnent eux trois Roger, Auguste et Ha??s. Le groupe rencontre en effet, une ascension
fulgurante. Les trois faiseurs de Hip-Hop, ont une seule ide?e en te?te, continuer de se
produire pour ceux qui les appre?cient, toujours composer de belles me?lodies et, ne pas
oublier d?ou` ils viennent.
Figure 2: Example in French: no repetition
Strings that are present in both parts will
be referred to as?relevant content?. They are
found in the beginning of the news and re-
peated in the development. To process as
many languages as possible, repeated char-
acter strings will be searched (not words
because Chinese for instance does not use
graphic words).
3.2 Defining epidemic event
Epidemic events are captured through these
information slots:
? Disease (What)
? Location (Where)
? Case, i.e.,People concerned (Who)
3.3 Selecting potentially relevant
documents
This discourse related heuristic rule limits re-
sources needed by the system. Many char-
acter strings that are repeated in the text
reflect important terms. However, repeti-
tion alone does not allow to fill IE templates
with detailed information as required. Ac-
cordingly, a lexical filter is applied on the re-
peated strings. 200 common disease names
are used to filter information and find dis-
ease names. The idea behind the restricted
list is that a journalist will use a common
name to help his readers understand the mes-
sage. Similarly, for locations, a list of coun-
try names and capitals provided by UN is
6
WHO checks smallpox reports in Uganda
LONDON, Thursday
The World Health Organisation said today it was investigating reports of suspected cases
of the previously eradicated disease smallpox in eastern Uganda.
Smallpox is an acute contagious disease and was one of the worlds most feared sicknesses
until it was officially declared eradicated worldwide in 1979.
?WHO takes any report of smallpox seriously, Gregory Hartl, a spokesman for the Geneva-
based United Nations health agency, told Reuters via email.
?WHO is aware of the reports coming out of Uganda and is taking all the necessary
measures to investigate and verify.?[etc.]
Figure 3: Example in English: repetition and location
used (about 500 items). Finally, in order to
comply with a specific demand of partners,
blacklist terms were used to detect less rel-
evant articles (vaccination campaign for in-
stance).
When a disease name is found in the rele-
vant content, the article is selected as poten-
tially relevant and the system tries to extract
location and cases.
3.4 Extracting location and cases
To extract the location, the following heuris-
tic is applied: the relevant location corre-
sponds to a string in the?relevant content?.
For instance, Example 3 shows that it allows
for the system to find that the main event
concerns Uganda but not London.
If numerous locations match, the system
compares frequencies in the whole document:
if one location is more than twice as frequent
as others, it is considered as the relevant one.
If no location is found, the location of the
source is selected by default. In fact accord-
ing to pragmatic rules when one reads an
article in the Washington Post, she will be
sure that it is about the United States even
if it is not explicitly mentioned. To the con-
trary if the article is about Argentina it will
be clearly mentioned so the reader has less
chances of misunderstanding.
Concerning the cases, they are related to
the first numeric information found in the
document, provided the figures are not re-
lated to money or date (this is checked by a
blacklist and simple regular expressions).
Furthermore the extracted cases are con-
sidered more relevant if they appear twice in
the document, the system uses regular ex-
pressions to round up and compare them.
See Example 4 where the number of dead
people ?55? is the first numeric information
in the beginning and is repeated in the de-
velopment (we chose an example where it is
easy even for a non Chinese speaker to see
the repetition). One can also note that the
second repeated figure is ?19488? which is
the number of infected people.
4 Evaluation
It is important to insist on the fact that our
system extracts the main event from one ar-
ticle, considering that secondary events have
been or will be mentioned in another article.
Often, the more topics are presented in one
article, the less important each one is. In the
case of epidemic surveillance, review articles
or retrospectives are not first-hand, fresh and
valuable information.
4.1 Corpus and Languages
For each language we randomly extracted
documents from the Medisys website.
Medisys documents are gathered using key-
words: medical terms (including scientific
disease names), but also weaker keywords
such as casualties, hospital. . . This implies
that some news document not related
7
Figure 4: Example in Chinese: 55 deaths from H1N1
to epidemic surveillance, but to accident
reports for instance, are liable to be found
in the database.
We must underline that in this framework,
recall can only be estimated, notably because
the news documents are keyword-filtered be-
forehand. However, our aim is not to provide
an independent system, but to provide quick
sorting of irrelevant news, prior to detailed
analysis, which is the key issue of a surveil-
lance and alert system. 200 documents were
extracted for each language and manually
tagged by native speakers with the following
instructions:
? Is this article about an epidemic?
? If it is, please give when possible:
Disease(s)
Country (or Worldwide)
Number of cases
100 of these annotated documents were used
for fine-tuning the system, 100 others for
evaluating. We chose for this study 3 fairly
different languages for checking the generic-
ity of the approach
? French, with its rather rich morphology,
? English,a rather isolating language with
poor morphology,
? Chinese, a strict isolating language with
poor morphology.
4.2 Results
These results were computed from a set of
100 annotated documents, as described in
section 4. Table 1 shows recall, precision and
F-measure for document selection(more ex-
amples are available online 4 ) Table 2 com-
pares automatically extracted slots and hu-
man annotated slots, therefore if an event is
not detected by the system it will count as
an error for each slot.
Table 1 shows that selection of documents
is quite satisfactory and that recall is better
than precision. This is mostly due to the fact
that the system still extracts documents with
low relevance. We found it impossible to pre-
dict if this is a general bias and whether it
can be improved. The result analysis showed
that many false negatives are due to cases
when the piece of news is quite small, see
for instance Example 5 where ?Swine flu? is
only found in the first two sentences, which
implies the repetition criteria does not apply
(and the system misses the document).
Table 2 shows the accuracy of the infor-
mation entered into semantic slots, respec-
4http://sites.google.com/site/iesystemcoling2010
8
China has 100 cases of swine flu: state media
China has 100 confirmed cases of swine flu, state media said Tuesday, as data from the
World Health Organization showed the disease had spread to 73 countries.
?The health ministry has reported that so far, China has 100 confirmed cases of A(H1N1)
flu,? said a news report on state television CCTV. The report said the 100 cases were in
mainland China, which does not include Hong Kong or Macau.
Figure 5: Example in English: Disease name not in ?relevant content?
Language Recall Precision F-measure
French 93% 88% 90%
English 88% 84% 86%
Chinese 92% 85% 88%
Table 1: Selecting documents
Language Diseases Locations Cases
French 88% 87% 81%
English 81% 81% 78%
Chinese 82% 79% 77%
Table 2: Accuracy in filling slots
tively name of disease, location and number
of cases. It is important to say that the de-
scriptors extracted are really reliable in spite
of the fact that the annotated set used for
evaluation is fairly small: 100 documents per
language, 30 to 40 of which were marked as
relevant. The extraction of cases performs a
bit worse than that of locations but the loca-
tion is the most important to our end-users.
5 Discussion and Conclusion
Most research in Information Extraction (IE)
focuses on building independent systems for
each language, which is time and resource
consuming. To the contrary, using common
features of news discourse saves time. The
system is not quite independent, but it al-
lows filtering news feeds and it provides rea-
sonable information even when no resources
at all are available. Our results on English
are worse than some existing systems (about
93% precision for Global health Monitor for
instance) but these systems need strong re-
sources and are not multilingual. We then
really need a multilingual baseline to com-
pare both approaches.
Recall is important for an alert system,
but is very difficult to assess in the case of
epidemiological surveillance. This measure
is always problematic for web based docu-
ments, due to the fact that any randomly
checked sample would only by sheer luck con-
tain all the positive documents. The assump-
tion here is that no important news has been
missed by Medisys, and that no important
news filtered from Medisys has been rejected.
One explanation for missed articles lies
in the definition of the article header: it is
too rigid. While this is fine for standard
size news, it is inappropriate for short news,
hence meaningful repetitions are missed in
the short news. This is a flaw, because first
alerts are often short news. In the future, we
may wish to define a discourse wise detection
rule to improve the location slot filling. The
extraction of locations is currently plagued
by a very long list of countries and capitals,
most of which is not useful. Locations are ac-
tually mentioned in data according to states,
provinces, prefectures, etc. The country list
might be abandoned, since we do not favour
external resources.
The methods that are presented here
maintain good reliability in different lan-
guages, and the assumption that genre laws
are useful has not been challenged yet. Light
resources, about 750 items (to be compared
to tens of thousands in classical IE sys-
tems), make it possible to strongly divide the
amount of work needed for processing new
languages. It might be attempted to refine
the simple hypotheses underlying the pro-
9
gram and build a better system for filtering
relevant news. This approach is best suited
when combined with elaborate pattern-based
IE modules when available. Repetition can
be checked for selecting documents prior to
resource intensive semantic processing. It
can also provide a few, easily fixable and effi-
cient preliminary results where language re-
sources are scarce or not available at all.
References
Atkinson, Martin and Erik Van der Goot. 2009.
Near real time information mining in multilin-
gual news. In 18th International World Wide
Web Conference (WWW2009).
Efimenko, Irina, Vladimir Khoroshevsky, and
Victor Klintsov. 2004. Ontosminer family:
Multilingual ie systems. In SPECOM 2004:
9th Conference Speech and Computer.
Etzioni, Oren, Michele Banko, Stephen Soder-
land, and Daniel S. Weld. 2008. Open in-
formation extraction from the web. Commun.
ACM, 51(12):68?74.
Gey, Fredric, Jussi Karlgren, and Noriko Kando.
2009. Information access in a multilingual
world: transitioning from research to real-
world applications. SIGIR Forum, 43(2):24?
28.
Grishman, Ralph, Silja Huttunen, and Roman
Yangarber. 2002. Information extraction for
enhanced access to disease outbreak reports.
Journal of Biomedical Informatics, 35(4):236?
246.
Itule, Bruce and Douglas Anderson. 2006. News
Writing and Reporting for Today?s Media.
McGraw-Hill Humanities.
Linge, JP, R Steinberger, T P Weber, R Yan-
garber, E van der Goot, D H Al Khudhairy,
and N I Stilianakis. 2009. Internet surveil-
lance systems for early alerting of threats. Eu-
rosurveillance, 14.
Lucas, Nadine. 2004. The enunciative structure
of news dispatches, a contrastive rhetorical
approach. Language, culture, rhetoric, pages
154?164.
MUC. 1992. Proceedings of the 4th Confer-
ence on Message Understanding, MUC 1992,
McLean, Virginia, USA, June 16-18, 1992.
MUC. 1993. Proceedings of the 5th Conference
on Message Understanding, MUC 1993, Balti-
more, Maryland, USA, August 25-27, 1993.
Poibeau, Thierry, Horacio Saggion, and Roman
Yangarber, editors. 2008. MMIES ?08: Pro-
ceedings of the Workshop on Multi-source Mul-
tilingual Information Extraction and Summa-
rization, Morristown, NJ, USA. Association
for Computational Linguistics.
Riloff, Ellen. 1996. Automatically generating
extraction patterns from untagged text. In
AAAI/IAAI, Vol. 2, pages 1044?1049.
Sperber, Dan and Deirdre Wilson. 1998. Rele-
vance: Communication and cognition. Black-
well press, Oxford U.K.
Van Dijk, T.A. 1988. News as discourse.
Lawrence Erlbaum Associates, Hillsdale N.J.
Vergne, Jacques. 2002. Une me?thode pour
l?analyse descendante et calculatoire de corpus
multilingues: application au calcul des rela-
tions sujet-verbe. In TALN 2002, pages 63?74.
10
JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 41?48,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
D?tection de mots-cl?s par approches au grain caract?re et au
grain mot
Ga?lle Doualan, Mathieu Boucher, Romain Brixtel, Ga?l Lejeune, Ga?l Dias
?quipe HULTECH (GREYC, Universit? de Caen), Bd Mar?chal juin, 14032 Caen Cedex
prenom.nom@unicaen.fr
R?SUM?
Nous pr?sentons dans cet article les m?thodes utilis?es par l??quipe HULTECH pour sa partici-
pation au D?fi Fouille de Textes 2012 (Deft 2012). La t?che de cette ?dition du d?fi consiste ?
retrouver dans des articles scientifiques, les mots-cl?s choisis par les auteurs. Nous nous appuyons
sur la d?tection de cha?nes r?p?t?es maximales (rst rmax), au grain caract?re et au grain mot. Lam?thode d?velopp?e est simple et non supervis?e. Elle a permis ? notre syst?me d?atteindre la 3e
place (sur 10 ?quipes) sur la premi?re piste du d?fi.
ABSTRACT
Keywords extraction by repeated string analysis
We present here the HULTECH(Human Language Technology) team approach for the Deft 2012
(french text mining challenge). The aim of the challenge is to retrieve the keywords given by
the authors of scientific articles. Our method relies on a text algorithmics technic : detection of
maximal repeated strings. This technic is applied at character level and word level. We achieved
the third rank (over 10) of the first track.
MOTS-CL?S : Recherche d?information, extraction de mots-cl?s, algorithmique du texte.
KEYWORDS: Information retrieval, keywords extraction, string algorithmics.
1 Introduction
La t?che propos?e dans le cadre du D?fi Fouille de Textes 2012 consiste ? retrouver dans des
articles de sciences humaines les mots-cl?s propos?s par les auteurs. Le corpus de travail est
scind? en deux pistes, la premi?re comportant 140 articles et la seconde 141. Une terminologie
qui regroupe tous les mots-cl?s des articles est propos?e avec la premi?re piste. Dans cet article
nous proposerons deux approches : une bas?e sur la connaissance de la terminologie, une autre
adapt?e ? l?absence de cette terminologie. Ce sera pour nous l?occasion de comparer les deux
approches et leurs r?sultats. Nos deux approches s?appuient sur un algorithme de recherche
de cha?nes r?p?t?es maximales, ci-apr?s rst rmax 1. Dans la premi?re approche, bas?e sur laterminologie, nous prenons comme grain d?analyse le caract?re. Dans la seconde approche nous
prenons comme grain d?analyse le mot graphique, sans appui sur la terminologie ni pour la
piste 1 ni pour la piste 2. Dans la section 2, nous proc?dons ? une analyse du corpus qui nous
permet d?appr?hender le mat?riau sur lequel nous travaillons. Dans la section 3, nous d?taillons
1. L?implantation en python utilis?e est disponible ? l?url suivante : code.google.com/p/py-rstr-max
41
nos deux approches. Ensuite, nous pr?senterons les r?sultats dans la section 4et proposons une
confrontation de ces deux approches dans la section 5.
2 Description du corpus
Le corpus utilis? comporte des articles de sciences humaines provenant de quatre revues diffus?es
sur le site Erudit 2. Nous pr?senterons ici plus pr?cis?ment les articles2.1 ? traiter et les mots-cl?s
qui leur sont associ?s2.2.
2.1 Les articles du corpus DEFT 2012
Le corpus DEFT 2012 est constitu? de 300 articles r?partis sur 4 revues de sciences humaines :
? Anthropologie et Soci?t? (AS)
? Revue des Sciences de l?Education (RSE)
? Traduction, Terminologie et R?daction (TTR)
? M?ta : journal des traducteurs (META)
2.1.1 Configuration des articles
Les articles sont au format xml. Ils sont constitu?s d?un identifiant, de la liste des mots-cl?s
fournis par l?auteur, d?un r?sum? et du corps de l?article lui-m?me. Le nom de la revue n?appara?t
pas dans le fichier xml mais dans le nom du fichier. De m?me, le nom de l?auteur et le titre de
l?article ne figurent pas dans le fichier xml. Ceci a rendu plus complexe la recherche des mots-cl?s
notamment du fait que le nom de l?auteur figurait syst?matiquement parmi les mots-cl?s des
articles de la revue Anthropologie et Soci?t?.
Nous pr?sentons dans la figure 1 un exemple d?article du corpus afin de montrer sa configuration
et sa structure, notons que les titres et sous-titres des articles n??taient pas disponibles.
2.1.2 Statistiques sur les articles
Nous avons effectu? des statistiques sur les articles afin de pouvoir mieux les appr?hender
(Tableau 2).
Nombre de documents Taille moyenne en paragraphes Taille moyenne en caract?res
Piste 1 94 67,8 41235
Piste 2 93 80,2 39153
Tableau 1 ? Statistiques sur les documents du corpus d??valuation
Le nombre moyen de paragraphes ne varie pas particuli?rement en fonction de la revue, ?
l?exception de certains articles de META pour lequel le d?coupage en paragraphes ?tait mauvais.
2. http://www.erudit.org
42
< ?xml version="1.0" encoding="UTF-8" ?>
-<doc id="0001">
-<motscles>
<nombre>4</nombre>
<mots>Labrecque ;?conomie politique ;f?minisme ;ethnographie</mots>
</motscles>
-<article>
-<resume>
<p>Tout en poursuivant l?objectif de la pr?sentation du num?ro,
. . . . . . . . . . . . . . . . . .
la consolidation de la th?orie.</p>
</resume>
-<corps>
<p>Qui sape l?ethnographie ?branle la th?orie
. . . . . . . . . . . . . . . . . . . . . . . .
d?une anthropologie engag?e, d?autre part.</p>
</corps>
</article>
</doc>
FIGURE 1 ? Un exemple d?article du jeu d?entra?nement
43
2.2 Les mots-cl?s
Nous avons remarqu? que les articles ne comportent pas le m?me nombre de mots-cl?s : en
moyenne 5,4 95,2 sur la piste 2 et 5,7 sur la piste 1). Mais une grande disparit? peut exister
d?un texte ? l?autre, l??tendue ?tant de 9 (1 ? 10 mots cl?s par article). Nous avons not? que le
premier mot-cl? est syst?matiquement le nom de l?auteur de l?article pour la revue Anthropologie
et Soci?t?. C?est dans un tel cas que la mention du nom de l?auteur dans le fichier nous aurait ?t?
utile.
2.2.1 Nature des mots-cl?s
? Noms propres : nom de l?auteur (ex : Labrecque), auteur faisant l?objet de l?article (ex : Jack
Kerouac), lieu g?ographique (ex : Japon)
? Noms communs : des noms communs seuls ou parfois accompagn?s d?adjectifs, mais jamais de
verbes ni d?adverbes (ex : f?minisme, ?conomie politique)
? Parfois les noms sont compl?t?s par des compl?ments du noms, formant des motifs tels que
celui-ci : N de art N (ex : traitement de l?information sociale)
? Cas particuliers : des noms coordonn?s (ex : traduction scientifique et technique)
Nous avons remarqu? que plus les mots-cl?s ?taient longs, moins on avait de chances de les
retrouver tels quels dans le texte. Lorsque l?on a la chance de les rencontrer dans le texte, ils y
sont peu fr?quents. Globalement 79% des mots-cl?s sont pr?sents tels quels dans le corps du
texte, 44,5% dans le r?sum? et 42% et dans le corps et dans le r?sum?.
3 Description des approches
Notre premi?re approche bas?e sur le grain caract?re utilise la terminologie afin de s?attaquer ? la
piste 1. Notre seconde approche n?utilise pas la terminologie et a ?t? utilis?e sur les deux pistes.
3.1 Approche au grain caract?re
Nous reprenons ici les principes de la m?thode utilis?e pour le Deft 2011 (Lejeune et al, 2011).
On suppose que les segments communs entre le r?sum? et le reste du texte constituent de bons
descripteurs. Pour s?lectionner les descripteurs pertinents nous nous fondons sur leur proximit?
avec des ?l?ments terminologie, technique utilis?e dans le domaine de l?Extraction d?Information
multilingue (Lejeune et al, 2010).
La m?thode rst rmax L?analyse au grain caract?re est effectu? en recherchant des motifs sanstrous (ci-apr?s motifs) tels que d?finis par (Ukkonen, 2009). Ces motifs sont des sous-cha?nes du
texte ayant les caract?ristiques suivantes 3 :
r?p?t?s : les motifs apparaissent au moins deux fois ;
maximaux : les motifs ne sont pas inclus dans des motifs plus grands et de m?me effectif
3. Pour une description plus formelle voir code.google.com/p/py-rstr-max
44
Nous comparons les deux segments textuels(r?sum? et corps) et l?ensemble de la terminologie
en une seule op?ration. Nous conservons les rst r ?max apparaissant dans ces deux segments et
dans un ?l?ment de la terminologie. Seuls les motifs respectant un crit?re de longueur donn?
sont consid?r?s comme pertinents. Pour tenir compte des variations morphologiques du fran?ais,
nous avons fix? la proximit? minimale entre un motif trouv? et un ?l?ment de la terminologie ?
0.9. Autrement dit, un ?l?ment t de la terminologie est consid?r? comme mot-cl? du texte s?il
existe une cha?ne c telle que :
? c est pr?sent dans le r?sum? et dans le corps de l?article
? c est une sous cha?ne de t
? len(c)len(t) ? 910 avec len le nombre de caract?res dans c et t
Nous n?avons pas appliqu? cette m?thode ? la seconde piste car la s?lection de cha?nes de
caract?res adapt?es ? l??valuation ?tait malais?e. Il aurait fallu un grand nombre d?heuristiques
pour retrouver des mots-cl?s comparables ? la r?f?rence. Nous avons pr?f?r? garder la "puret?"
de cette m?thode. En effet le seul pr?-traitement effectu? est le d?coupage en deux segments
textuels (r?sum? et corps). Aucun outillage linguistique (lemmatisation, ?tiquetage. . .) n?est
n?cessaire. Par ailleurs, aucun post-traitement n?est effectu?.
3.2 Approche au grain mot
Pour notre seconde approche, nous proc?dons ? un d?coupage plus classique en mots. Cette
m?thode est con?ue pour fonctionner en l?absence de terminologie de r?f?rence. Nous appliquons
l?algorithme de d?tection des rst rmax (section : 3.1) mais en l?appliquant cette fois sur des mots.
L?algorithme rst rmax est appliqu? ? tout ce qui est compris entre les balises <article> ce quicorrespond au r?sum? et au corps de l?article. Nous consid?rons le tout comme une cha?ne. Nous
obtenons ainsi un ensemble de cha?nes de mots r?p?t?es et maximales. Un grand nombre de
motifs sont d?tect?s dont certains sont partiellement redondants. Par exemple, on a les motif
ABC D et BC DF et on souhaite souvent ne garder que la partie centrale BC D. Pour am?liorer la
pr?cision, nous appliquons donc une seconde fois rst rmax sur la liste des cha?nes obtenues.
3.2.1 IDF
Pour am?liorer la pr?cision de nos r?sultats, nous voulons r?duire encore le nombre de cha?nes
obtenues. Cependant, il nous faut conserver un rappel correct. Pour ce faire nous avons choisi
de calculer l?IDF (Inverse Document Frequency) de chaque cha?ne. Cette mesure fait ressortir
les cha?nes sp?cifiques ? un texte par rapport au corpus. L?IDF est l?inverse de la fr?quence de
la cha?ne dans un ensemble de documents. Cette mesure est g?n?ralement coupl?e avec le TF
(term frequency ou effectif du mot dans un document) en Voici comment se calcule le T F ? I DF
d?une cha?ne C dans un document D 4 :
T F ? I DF = f req(C ,D)t(D) ?? log2 nd(C)N
45
Avec :
? freq(C,D) le nombre de fois que la cha?ne C appara?t dans le document D
? t(D) le nombre de mots du document D
? nd(C) le nombre de documents contenant C dans le corpus
? N la taille du corpus en documents
Cependant, nous ne conservons que l?IDF. Dans notre cas, il n??tait pas n?cessaire l?appliquer le
TF. En effet, gr?ce ? la m?thode rst rmax , nous obtenons les cha?nes maximales r?p?t?es, ce quisignifie qu?elles ont d?j? une certaine fr?quence dans le document. Par ailleurs, le TF a tendance
? privil?gier les cha?nes tr?s fr?quentes d?un texte, autrement dit des mots vides peu susceptibles
d??tre des mots-cl?s.
Pour calculer l?IDF, nous consid?rons l?ensemble des articles d?une piste. Cela nous permet de
caract?riser un article par rapport ? une piste. Cela se justifie si nous nous repla?ons dans
le s?mantique textuelle de Fran?ois Rastier : " le texte pour une linguistique ?volu?e l?unit?
minimale, et le corpus l?ensemble dans lequel cette unit? prend son sens "(Rastier, 2002). Ainsi,
un article ne prend son sens que dans le corpus de travail si bien que nous devons caract?riser
ces cha?nes et ces mots-cl?s par rapport ? l?ensemble du corpus. Lorsque nous calculons l?IDF des
cha?nes nous obtenons des r?sultats compris entre 0 et 5. Nous classons les cha?nes en ordre
d?croissant de leur IDF. Le but ?tant de r?duire le nombre de cha?nes, nous ne conservons que
celles dans l?IDF est sup?rieure ? 2.
3.2.2 Pond?ration des cha?nes
L?IDF constitue un premier filtrage par pond?ration mais ce n?est pas suffisant. Nous proc?dons
donc ? un second filtrage par pond?ration en attribuant un poids aux cha?nes restantes en
fonction des crit?res suivants :
? IDF
? fr?quence de la cha?ne dans l?article
? fr?quence de la cha?ne dans le r?sum?
? longueur de la cha?ne
? pr?sence de la cha?ne dans le premier paragraphe (a priori : introduction
? pr?sence de la cha?ne dans la dernier paragraphe (a priori : conclusion)
A chacune de ces mesures est attribu? un coefficient qui pond?re leur importance. Nous avons
effectu? des statistiques sur le corpus afin d?anticiper les places occup?es par les mots-cl?s dans
les articles. Ainsi, si une cha?ne est fr?quente dans le r?sum?, elle a davantage de chance d??tre
un mot-cl? qu?une autre cha?ne. Nous attribuons donc un certain poids ? ces mesures en fonction
de leur capacit? ? traduire le comportement des mots-cl?s. Notons que l?absence des titres dans
les documents analys?s rend difficile la d?tection des segments introductifs et conclusifs . Les
cha?nes sont rang?es en ordre d?croissant de poids et nous s?lectionnons les 7 premi?res cha?nes
en guise de mots-cl?s. Ce seuil a ?t? fix? ? partir des meilleurs r?sultats obtenus sur le corpus
d?entra?nement.
46
4 R?sultats
R?sultat piste 1 R?sultat piste 2
Approche 1 : rst rmax au grain caract?re 0.44, 3e/10 Approche 2 : rst rmax au grain mot 0,12 0,13, 7e/9Baseline : tf-idf simple 0,08 0,07
Tableau 2 ? R?sultats et rangs pour nos 2 approches et notre baseline
La premi?re approche donne de bons r?sultats en raison de l?appui de la terminologie, bien
meilleurs qu?avec l?approche par poids. Sans doute ces r?sultats auraient pu ?tre am?lior?s avec
quelques heuristiques, par exemple : chercher ? affecter chaque mot-cl? de la terminologie ? au
moins un document. Mais nous n?avons pas souhait? complexifier la proc?dure utilis?e.
Concernant la seconde approche, elle aurait sans doute eu de meilleurs r?sultats sur la piste 1 en
s?appuyant sur la terminologie mais nous avons souhait? pour les deux pistes conserver l?aspect
?sans ressources externes".
5 Discussion
Nous avons opt? pour des m?thodes simples ? mettre en place et peu co?teuses en temps,
peut ?tre au d?triment de la qualit? des r?sultats. La premi?re approche se voulait avant tout
ind?pendante de la langue consid?r?e. Travailler sur le grain caract?re permet de d?passer
les probl?mes de d?coupage des textes en mots. Toutefois pour se conformer aux modalit?s
d??valuation, le soutien de la terminologie s?est av?r? n?cessaire. La seconde approche se voulait
ind?pendante de tout support ext?rieur. En effet, ne pas utiliser la terminologie permet d?extraire
des informations nouvelles ? partir d?un document brut.
Nos deux approches ont en commun l?utilisation d?une m?thode d?algorithmique du texte :
rst rmax . L?algorithme recherche des cha?nes r?p?t?es maximales, suppos?es caract?ristiques d?untexte. Nos approches diff?rent par le grain d?analyse : caract?re pour l?une, mot pour l?autre.
La premi?re m?thode pr?sente l?avantage de la simplicit?, elle ne n?cessite aucun param?tre mais
e base sur la terminologie. La seconde m?thode ne n?cessite pas de terminologie mais impose
des traitements suppl?mentaires.
Nos deux m?thodes pr?sentent par ailleurs l?avantage de d?tecter facilement des unit?s multi-
mots, souvent plus pertinentes pour des t?ches d?indexation documentaire et de recherche
d?information.
Enfin, nos deux approches sont ind?pendantes de tout module d?analyse linguistique (lemma-
tisation, ?tiquetage. . .) ce qui les rend a priori moins sensibles ? une utilisation sur d?autres
langues que le fran?ais. Il serait donc int?ressant d?exp?rimenter ces techniques sur des corpus
multilingues.
47
R?f?rences
LEJEUNE, G., BRIXTEL, R., GIGUET, E. et LUCAS, N. (2011). Deft2011 : appariement de r?sum?s et
d?articles scientifiques fond? sur les cha?nes de caract?res. In D?fi Fouille de Textes/TALN 2011,
pages 53?64.
LEJEUNE, G., DOUCET, A., YANGARBER, R. et LUCAS, N. (2010). Filtering news for epidemic
surveillance : towards processing more languages with fewer resources. In 4th Workshop on
Cross Lingual Information Access, pages 3?10.
RASTIER, F. (2002). Enjeux ?pist?mologiques de la linguistique de corpus. In 2?me journ?es de la
linguistique de corpus.
UKKONEN, E. (2009). Maximal and minimal representations of gapped and non-gapped motifs
of a string. Theor. Comput. Sci., 410(43):4341?4349.
48
