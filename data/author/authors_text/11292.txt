Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 222?226,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Revised Arabic PropBank 
Wajdi Zaghouani? , Mona Diab? , Aous Mansouri?, 
Sameer Pradhan? and Martha Palmer? 
 
?Linguistic Data Consortium, ?Columbia University,  
?University of Colorado, ?BBN Technologies 
 
wajdiz@ldc.upenn.edu, mdiab@ccls.columbia.edu, aous.mansouri@colorado.edu,  
pradhan@bbn.com, martha.palmer@colorado.edu 
 
Abstract 
The revised Arabic PropBank (APB) reflects 
a number of changes to the data and the proc-
ess of PropBanking. Several changes stem 
from Treebank revisions. An automatic proc-
ess was put in place to map existing annota-
tion to the new trees. We have revised the 
original 493 Frame Files from the Pilot APB 
and added 1462 new files for a total of 1955 
Frame Files with 2446 framesets. In addition 
to a heightened attention to sense distinctions 
this cycle includes a greater attempt to ad-
dress complicated predicates such as light 
verb constructions and multi-word expres-
sions. New tools facilitate the data tagging 
and also simplify frame creation. 
1 Introduction 
Recent years have witnessed a surge in available 
automated resources for the Arabic language. 1 
These resources can now be exploited by the 
computational linguistics community with the 
aim of improving the automatic processing of 
Arabic. This paper discusses semantic labeling. 
  
Shallow approaches to semantic processing are 
making large advances in the direction of effi-
ciently and effectively deriving application rele-
vant explicit semantic information from text 
(Pradhan et al, 2003; Gildea and Palmer, 2002; 
Pradhan et al, 2004; Gildea and Jurafsky, 2002; 
Xue and Palmer, 2004; Chen and Rambow, 
2003; Carreras and Marquez, 2005; Moschitti, 
2004; Moschitti et al, 2005; Diab et al, 2008). 
Indeed, the existence of semantically annotated 
resources in English such as FrameNet (Baker et 
al., 1998) and PropBank (Kingsbury and Palmer, 
2003; Palmer et al, 2005) corpora have marked a 
surge in efficient approaches to automatic se-
                                               
1 In this paper, we use Arabic to refer to Modern Standard 
Arabic (MSA). 
mantic labeling of the English language. For ex-
ample, in the English sentence, ?John enjoys 
movies?, the predicate is ?enjoys? and the first 
argument, the subject, is ?John?, and the second 
argument, the object, is ?movies?. ?John? would 
be labeled as the agent/experiencer and ?movies? 
would be the theme/content. According to Prop-
Bank, ?John? is labeled Arg0 (or enjoyer) and 
?movies? is labeled Arg1 (or thing enjoyed). Cru-
cially, that independent of the labeling formalism 
adopted, the labels do not vary in different syn-
tactic constructions, which is why proposition 
annotation is different from syntactic Treebank 
annotation. For instance, if the example above 
was in the passive voice, ?Movies are enjoyed by 
John?, ?movies? is still the Theme/Content (Arg1) 
and (thing enjoyed), while ?John? remains the 
Agent/Experiencer (Arg0) and (enjoyer). Like-
wise for the example ?John opened the door? vs. 
?The door opened?, in both of these examples 
?the door? is the Theme (Arg1). In addition to 
English, there are PropBank efforts in Chinese 
(Xue et al, 2009), Korean (Palmer et al 2006) 
and Hindi (Palmer et al, 2009), as well as Fra-
meNet annotations in Chinese, German, Japa-
nese, Spanish and other languages (Hans 2009). 
Being able to automatically apply this level of 
analysis to Arabic is clearly a desirable goal, and 
indeed, we began a pilot Arabic PropBank effort 
several years ago (Palmer et al, 2008). 
  
In this paper, we present recent work on adapting 
the original pilot Arabic Proposition Bank (APB) 
annotation to the recent changes that have been 
made to the Arabic Treebank (Maamouri et al, 
2008). These changes have presented both lin-
guistic and engineering challenges as described 
in the following sections. In Section 2 we discuss 
major linguistics changes in the Arabic Treebank 
annotation, and any impact they might have for 
the APB effort. In Section 3 we discuss the engi-
neering ramifications of adding and deleting 
nodes from parse trees, which necessitates mov-
222
ing all of the APB label pointers to new tree lo-
cations. Finally, in Section 4 we discuss the cur-
rent APB annotation pipeline, which takes into 
account all of these changes. We conclude with a 
statement of our current goals for the project.  
2 Arabic Treebank Revision and APB 
The Arabic syntactic Treebank Part 3 v3.1 was 
revised according to the new Arabic Treebank 
Annotation Guidelines. Major changes have af-
fected the NP structure and the classification of 
verbs with clausal arguments, as well as im-
provements to the annotation in general.2  
  
The Arabic Treebank (ATB) is at the core of the 
APB annotations. The current revisions have re-
sulted in a more consistent treebank that is closer 
in its analyses to traditional Arabic grammar. 
The ATB was revised for two levels of linguistic 
representation, namely morphological informa-
tion and syntactic structure. Both of these 
changes have implications for APB annotations.  
 
The new ATB introduced more consistency in 
the application of morphological features to POS 
tags, hence almost all relevant words in the ATB 
have full morphological features of number, 
gender, case, mood, and definiteness associated 
with them. This more comprehensive application 
has implications on agreement markers between 
nouns and their modifiers and predicative verbs 
and their arguments, allowing for more consis-
tent semantic analysis in the APB. 
  
In particular, the new ATB explicitly marks the 
gerunds in Arabic known as maSAdir (singular 
maSdar.) MaSAdirs, now annotated as VN, are 
typically predicative nouns that take arguments 
that should receive semantic roles. The nouns 
marked as VN are embedded in a new kind of 
syntactic S structure headed by a VN and having 
subject and object arguments similar to verbal 
arguments. This syntactic structure, namely S-
NOM, was present in previous editions/versions 
of the ATB but it was headed by a regular noun, 
hence it was difficult to find. This explicit VN 
annotation allows the APB effort to take these 
new categories into account as predicates. For 
instance [????]VN [-??]ARG0 [????? ?????]ARG1, 
transliterated as takab~udi-,  meaning 'suffered' 
                                               
2 For a complete description of the new Treebank annotation 
guidelines, see (Arabic Treebank Morphological and Syn-
tactic Annotation Guidelines 2008) at 
http://projects.ldc.upenn.edu/ArabicTreebank/. 
is an example of predicative nominal together 
with its semantically annotated arguments ARG0 
transliterated as -him, meaning 'they' and ARG1 
transliterated as xasA}ira kabiyrap, meaning 
'heavy losses'. 
 
Other changes in the ATB include idafa con-
structions (a means of expressing possession) 
and the addition of a pseudo-verb POS tag for a 
particular group of particles traditionally known 
as ?the sisters of  ??? <in~a 'indeed' ?. These have 
very little impact on the APB annotation. 
3 Revised Treebank processing 
One of the challenges that we faced during the 
process of revising the APB was the transfer of 
the already existing annotation to the newly re-
vised trees -- especially since APB data encoding 
is tightly coupled with the explicit tree structure. 
Some of the ATB changes that affected APB 
projection from the old pilot effort to the new 
trees are listed as follows:  
i. Changes to the tree structure 
ii. Changes to the number of tokens -- both 
modification (insertion and deletion) of 
traces and modification to some tokeni-
zation 
iii. Changes in parts of speech 
iv. Changes to sentence breaks 
The APB modifications are performed within the 
OntoNotes project (Hovy et al 2006), we have 
direct access to the OntoNotes DB Tool, which 
we extended to facilitate a smooth transition. The 
tool is modified to perform a three-step mapping 
process: 
 
a) De-reference the existing (tree) node-level 
annotations to the respective token spans; 
 
b) Align the original token spans to the best pos-
sible token spans in the revised trees. This was 
usually straight forward, but sometimes the to-
kenization affected the boundaries of a span in 
which case careful heuristics had to be employed 
to find the correct mapping. We incorporated the 
standard "diff" utility into the API. A simple 
space separated token-based diff would not com-
pletely align cases where the tokenization had 
been changed in the new tree. For these cases we 
had to back-off to a character based alignment to 
recover the alignments. This two-pass strategy 
works better than using character-based align-
223
ment as a default since the diff tool does not have 
any specific domain-level constraints and gets 
spurious alignments; 
 
c) Create the PropBank (tree) node-pointers for 
the revised spans. 
 
As expected, this process is not completely 
automatic. There are cases where we can deter-
ministically transfer the annotations to the new 
trees, and other cases (especially ones that in-
volve decision making based on newly added 
traces) where we cannot. We automatically trans-
ferred all the annotation that could be done de-
terministically, and flagged all the others for hu-
man review. These cases were grouped into mul-
tiple categories for the convenience of the anno-
tators. Some of the part of speech changes in-
validated some existing annotations, and created 
new predicates to annotate. In the first case, we 
simply dropped the existing annotations on the 
affected nodes, and in the latter we just created 
new pointers to be annotated. We could auto-
matically map roughly 50% of the annotations. 
The rest are being manually reviewed. 
4 Annotation Tools and Pipeline 
4.1 Annotation process 
APB consists of two major portions: the lexicon 
resource of Frame Files and the annotated cor-
pus. Hence, the process is divided into framing 
and annotation (Palmer et al, 2005). 
 
Currently, we have four linguists (framers) creat-
ing predicate Frame Files. Using the frame crea-
tion tool Cornerstone, a Frame File is created for 
a specific lemma found in the Arabic Treebank. 
The information in the Frame File must include 
the lemma and at least one frameset.  
 
Previously, senses were lumped together into a 
single frame if they shared the same argument 
structure. In this effort, however, we are attempt-
ing to be more sensitive to the different senses 
and consequently each unique sense has its own 
frameset. A frameset contains an English defini-
tion, the argument structure for the frameset, a 
set of (parsed) Arabic examples as an illustration, 
and it may include Arabic synonyms to further 
help the annotators with sense disambiguation.  
 
Figure 1 illustrates the Frameset for the verb 
????? } isotamaE 'to listen' 
 
Predicate: {isotamaE ????? 
Roleset id: f1, to listen 
Arg0: entity listening 
Arg1: thing listened 
 
Figure 1. The frameset of the verb {isotamaE 
         
 
Rel: {isotamaE, ????? 
Arg0: -NONE- * 
Gloss: He 
Arg1: ??? ??????? 
Gloss: to their demands 
Example: ?????  ??? ??????? 
 
Figure 2. An example annotation for a sentence 
containing the verb {isotamaE 
 
In addition to the framers, we also have five na-
tive Arabic speakers as annotators on the team, 
using the annotation tool Jubilee (described be-
low). Treebanked sentences from the ATB are 
clearly displayed in Jubilee, as well as the raw 
text for that sentence at the bottom of the screen. 
The verb that needs to be tagged is clearly 
marked on the tree for the annotators. A drop-
down menu is available for the annotators to use 
so that they may choose a particular frameset for 
that specific instance. Once a frameset is chosen 
the argument structure will be displayed for them 
to see. As a visual aid, the annotators may also 
click on the ?example? button in order to see the 
examples for that particular frameset. Finally, the 
complements of the predicate are tagged directly 
on the tree, and the annotators may move on to 
the next sentence. Figure 2 illustrates a sample 
annotation. 
 
Once the data has been double-blind annotated, 
the adjudication process begins. An adjudicator, 
a member of the framing team, provides the Gold 
Standard annotation by going over the tagged 
instances to settle any differences in the choices. 
Occasionally a verb will be mis-lemmatized (e.g. 
the instance may actually be ????? sah~al 'to cause 
to become easy' but it is lemmatized under ????? 
sahul-u 'to be easy' which looks identical without 
vocalization.) At this point the lemmas are cor-
rected and sent back to the annotators to tag be-
fore the adjudicators can complete their work. 
 
The framers and annotators meet regularly at 
least every fortnight. These meetings are impor-
tant for the framers since they may need to con-
vey to the annotators any changes or issues with 
the frames, syntactic matters, or anything else 
that may require extra training or preparation for 
224
the annotators. It is important to note that while 
the framers are linguists, the annotators are not. 
This means that the annotators must be instructed 
on a number of things including, but not limited 
to, how to read trees, and what forms a constitu-
ent, as well as how to get familiar with the tools 
in order to start annotating the data. Therefore, 
little touches, such as the addition of Arabic 
synonyms to the framesets (especially since not 
all of the annotators have the same level of flu-
ency in English), or confronting specific linguis-
tic phenomena via multiple modalities are a nec-
essary part of the process. To these meetings, the 
annotators mostly bring their questions and con-
cerns about the data they are working on. We 
rely heavily on the annotator?s language skills. 
They take note of whether a frame appears to be 
incorrect, is missing an argument, or is missing a 
sense. And since they go through every instance 
in the data, annotators are instrumental for point-
ing out any errors the ATB. Since everything is 
discussed together as a group people frequently 
benefit from the conversations and issues that are 
raised. These bi-monthly meetings not only help 
maintain a certain level of quality control but 
establish a feeling of cohesion in the group. 
 
The APB has decided to thoroughly tackle light 
verb constructions and multi-word expressions as 
part of an effort to facilitate mapping between 
the different languages that are being Prop-
Banked. In the process of setting this up a num-
ber of challenges have surfaced which include: 
how can we cross-linguistically approach these 
phenomena in a (semi) integrated manner, how 
to identify one construction from the other, figur-
ing out a language specific reliable diagnostic 
test, and whether we deal with these construc-
tions as a whole unit or as separate parts; and 
how? (Hwang, et al, 2010) 
4.2 Tools 
Frameset files are created in an XML format. 
During the Pilot Propbank project these files 
were created manually by editing the XML file 
related to a particular predicate. This proved to 
be time consuming and prone to many formatting 
errors. The Frame File creation for the revised 
APB is now performed with the recently devel-
oped Cornerstone tool (Choi et al, 2010a), which 
is a PropBank frameset editor that allows the 
creation and editing of Propbank framesets with-
out requiring any prior knowledge of XML. 
Moreover, the annotation is now performed by 
Jubilee, a new annotation tool, which has im-
proved the annotation process by displaying sev-
eral types of relevant syntactic and semantic in-
formation at the same time. Having everything 
displayed helps the annotator quickly absorb and 
apply the necessary syntactic and semantic in-
formation pertinent to each predicate for consis-
tent and efficient annotation (Choi et al, 
20010b). Both tools are available as Open Source 
tools on Google code.3 
4.3 Current Annotation Status and Goals 
We have currently created 1955 verb predicate 
Frame Files which correspond to 2446 framesets, 
since one verb predicate Frame File can contain 
one or more framesets. We will reconcile the 
previous Arabic PropBank with the new Tree-
bank and create an additional 3000 Frame files to 
cover the rest of the ATB3 verb types.  
5 Conclusion  
This paper describes the recently revived and 
revised APB. The changes in the ATB have af-
fected the APB in two fundamentally different 
ways. More fine-grained POS tags facilitate the 
tasks of labeling predicate argument structures. 
However, all of the tokenization changes have 
rendered the old pointers obsolete, and new 
pointers to the new constituent boundaries have 
to be supplied. This task is underway, as well as 
the task of creating several thousand additional 
Frame Files to complete predicate coverage of 
ATB3. 
 
Acknowledgments 
 
We gratefully acknowledge a grant from the De-
fense Advanced Research Projects Agency 
(DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No. HR0011-06-C-
0022, subcontract from BBN, Inc. We also thank 
Abdel-Aati Hawwary and Maha Saliba Foster 
and our annotators for their invaluable contribu-
tions to this project. 
References  
Boas, Hans C. 2009. Multilingual FrameNets. In 
Computational Lexicography: Methods and Appli-
cations. Berlin: Mouton de Gruyter. pp. x+352 
Carreras, Xavier & Llu?s M?rquez. 2005. Introduction 
to the CoNLL-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI, USA. 
                                               
3 http://code.google.com/p/propbank/ 
225
Chen, John & Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Choi, Jinho D., Claire Bonial, & Martha Palmer. 
2010a. Propbank Instance Annotation Guidelines 
Using a Dedicated Editor,Cornerstone. In Proceed-
ings of the 7th International Conference on Lan-
guage Resources and Evaluation 
(LREC'10),Valletta, Malta. 
Choi, Jinho D., Claire Bonial, & Martha Palmer. 
2010b. Propbank Instance Annotation Guidelines 
Using a Dedicated Editor,Jubilee. In Proceedings 
of the 7th International Conference on Language 
Resources and Evaluation (LREC'10),Valletta, 
Malta. 
Diab, Mona, Alessandro Moschitti, & Daniele Pighin. 
2008. Semantic Role Labeling Systems for Arabic 
using Kernel Methods. In Proceedings of ACL. As-
sociation for Computational Linguistics, Colum-
bus, OH, USA. 
Gildea, Daniel & Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Gildea, Daniel & Martha Palmer. 2002. The necessity 
of parsing for predicate argument recognition. In 
Proceedings of the 40th Annual Conference of the 
Association for Computational Linguistics (ACL-
02), Philadelphia, PA, USA. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences. Cambridge University Press, 
Cambridge, UK. 
Habash, Nizar & Owen Rambow. 2007. Arabic dia-
critization through full morphological tagging. In 
HLT-NAACL 2007; Companion Volume, Short Pa-
pers, Association for Computational Linguistics, 
pages 53?56, Rochester, NY, USA. 
Hovy, Eduard, Mitchell Marcus, Martha Palmer, 
Lance Ramshaw & Ralph Weischedel. 2006. 
OntoNotes: The 90% Solution. In Proceedings of 
HLT-NAACL 2006, New York, USA. 
Hwang, Jena D., Archna Bhatia, Clare Bonial, Aous 
Mansouri, Ashwini Vaidya, Nianwen Xue & Mar-
tha Palmer. 2010. PropBank Annotation of Multi-
lingual Light Verb Constructions. In Proceedings 
of the LAW-ACL 2010. Uppsala, Sweden. 
Maamouri, Mohamed, Ann Bies, Seth Kulick. 2008. 
Enhanced Annotation and Parsing of the Arabic 
Treebank. In Proceedings of INFOS 2008, Cairo, 
Egypt. 
M?rquez, Llu?s. 2009. Semantic Role Labeling. Past, 
Present and Future . TALP Research Center. Tech-
nical University of Catalonia. Tutorial at ACL-
IJCNLP 2009. 
Moschitti, Alessandro. 2004. A study on convolution 
kernels for shallow semantic parsing. In proceed-
ings of the 42th Conference on Association for 
Computational Linguistic (ACL-2004), Barcelona, 
Spain.  
Moschitti, Alessandro, Ana-Maria Giuglea, Bonaven-
tura Coppola, & Roberto Basili. 2005. Hierarchical 
semantic role labeling. In Proceedings of CoNLL-
2005, Ann Arbor, MI, USA. 
Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona 
Diab, Mohamed Maamouri, Aous Mansouri, Wajdi 
Zaghouani. 2008. A Pilot Arabic Propbank. In 
Proceedings of LREC 2008, Marrakech, Morocco. 
Palmer, Martha, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, & Fei Xia. 
2009. Hindi Syntax: Annotating Dependency, 
Lexical Predicate-Argument Structure, and Phrase 
Structure. In The 7th International Conference on 
Natural Language Processing (ICON-2009), Hy-
derabad, India. 
Palmer, Martha, Daniel Gildea, & Paul Kingsbury.  
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 31, 
1 (Mar. 2005), 71-106. 
Palmer, Martha, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, & Yeongmi Jeon. 2006. LDC Catalog 
LDC2006T03. 
Pradhan, Sameer, Kadri Hacioglu, Wayne Ward, 
James H. Martin, & Daniel Jurafsky. 2003. Seman-
tic role parsing: Adding semantic structure to un-
structured text. In Proceedings of ICDM-2003, 
Melbourne, USA. 
Pradhan, Sameer S., Wayne H Ward, Kadri Hacioglu, 
James H Martin, & Dan Jurafsky. 2004. Shallow 
semantic parsing using support vector machines. In 
Susan Dumais, Daniel Marcu, & Salim Roukos, 
editors, HLT-NAACL 2004: Main Proceedings, 
pages 233?240, Boston, MA, USA. 
Xue, Nianwen & Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
& Dekai Wu, editors, Proceedings of ACL-EMNLP 
2004, pages 88?94, Barcelona, Spain.  
Xue, Nianwen & Martha Palmer. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural 
Language Engineering, 15 Jan. 2009, 143-172. 
226
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 127?135,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Developing ARET: An NLP-based  Educational Tool Set for Arabic 
Reading Enhancement 
 
Mohamed Maamouri1, Wajdi Zaghouani1, Violetta Cavalli-Sforza2, 
Dave Graff1 and Mike Ciul1 
 
1 Linguistic Data Consortium, University of Pennsylvania, 3600 Market St., Suite 810,  
Philadelphia, PA 19104. 
           2 School of Science and Engineering, Al Akhawayn University, Ifrane 53000, Morocco. 
maamouri@ldc.upenn.edu, wajdiz@ldc.upenn.edu, 
v.cavallisforza@aui.ma, graff@ldc.upenn.edu, mciul@ldc.upenn.edu 
 
 
 
Abstract 
This paper describes a novel Arabic Reading 
Enhancement Tool (ARET) for classroom use, 
which has been built using corpus-based Natu-
ral Language Processing in combination with 
expert linguistic annotation. The NLP tech-
niques include a widely used morphological 
analyzer for Modern Standard Arabic to pro-
vide word-level grammatical details, and a rela-
tional database index of corpus texts to provide 
word concordances. ARET also makes use of a 
commercial Arabic text-to-speech (TTS) sys-
tem to add a speech layer (with male and fe-
male voices) to the Al-Kitaab language 
textbook resources. The system generates test 
questions and distractors, offering teachers and 
students an interesting computer-aided lan-
guage learning tool. We describe the back-
ground and the motivation behind the building 
of ARET, presenting the various components 
and the method used to build the tools. 
1 Introduction 
Reading is an essential skill for learners of Modern 
Standard Arabic (MSA). For most of learners it is 
the most important skill to master in order to en-
sure success in learning. With strengthened reading 
skills, learners of Arabic tend to make greater 
progress in other areas of language learning. Read-
ing should be an active, fluent process that in-
volves the reader and the reading material in build-
ing meaning. Often, however, it is not. The average 
learner?s second language reading ability is usually 
well below that of the first language. This can im-
pede academic progress in the second language. 
Arabic language teachers and learners face many 
challenges in the classroom. Teaching students 
how to utilize the skills and knowledge they bring 
from their first language, develop vocabulary 
skills, improve reading comprehension and rate, 
and monitor their own improvement are just some 
of the issues that teachers must consider in prepar-
ing for an Arabic language reading class. With 
these issues in mind, we set out to create a web-
based service that would provide efficient and 
pedagogically relevant access to instructional texts 
in Modern Standard Arabic, with the goal of creat-
ing a resource that would serve both instructors 
and students, by presenting novel modes of infor-
mation access. We received valuable support from 
Georgetown University Press, which gave permis-
sion for us to use the reading passages from the 3-
volume textbook publication Al-Kitaab (Al-Batal 
et al, 2001;2004 and 2006), which is the most 
popular publication in the USA for teaching Arab-
ic. 
2 Motivation 
Using technology in classrooms can make the les-
sons more efficient. There are many technology 
127
tools that can be used in English as a Second Lan-
guage (ESL) classes to improve foreign students? 
English and technology skills. According to Wang 
(2005) there are many advantages integrating tech-
nology in classrooms especially for ESL students. 
To be able to improve their language skills, like 
writing, reading, listening and speaking, English 
language learners use pedagogical computer appli-
cations to check their work and improve their lan-
guage skills; they also use web browsers and e-
mail to search for information, join in online dis-
cussions, publish their work, read technology texts, 
communicate each other even worldwide. He also 
says that, ?Technology integration in foreign lan-
guage teaching demonstrates the shift in educa-
tional paradigms from a behavioral to a 
constructivist learning approach? (p. 2). Gone are 
the days in which learning foreign language voca-
bulary and grammar rules relied largely on repeti-
tive drills; more and more, foreign language 
learners are asked to engage directly with authentic 
materials and take more initiative in their learning. 
However, finding appropriate, authentic reading 
materials is a challenge for language instructors. 
The Web is a vast resource of texts, but most pages 
are not suitable for reading practice, and commer-
cial search engines are not well suited to finding 
texts that satisfy pedagogical constraints such as 
reading level, length, text quality, and presence of 
target vocabulary. We present a system that uses 
various language technologies to facilitate the se-
lection , presentation and study of authentic read-
ing materials from the widely used textbook series 
Al-Kitaab (Al-Batal et al, 2001;2004 and 2006). In 
the next section we review some of the related 
work. In section 4 we discuss some of the specific 
challenges faced when learning the Arabic lan-
guage.  
3 Related work 
Many studies have shown that an on-line learning 
environment that supplements classroom instruc-
tion with additional study materials at an appropri-
ate level for the learner may enhance language 
learning and development (Ware, 2004; Chiu et al, 
2007; Yuan, 2003; Wang, 2005;). As a result, a 
number of recent projects have aimed to dynami-
cally provide a supply of accessible authentic texts 
to language learners by drawing from online re-
sources. WERTi (Meurers et al 2010) is an intelli-
gent automatic workbook that uses texts from the 
Web to increase knowledge of English grammati-
cal forms and functions. READ-X (Miltsakaki and 
Troutt, 2007) is a tool for finding texts at specified 
reading levels. SourceFinder (Sheehan et al,2007) 
is an authoring tool for finding suitable texts for 
standardized test items on verbal reasoning and 
reading comprehension. Project REAP (Reader-
Specific Lexical Practice) (Brown and Eskenazi, 
2004; Heilman et al, 2006) takes a different ap-
proach. Rather than teachers choosing texts, in 
REAP the system selects individualized practice 
readings from a digital library according to specific 
lexical constraints. Readings are chosen to contain 
vocabulary words that a given student needs to 
learn, while limiting the number of words the stu-
dent does not know. The choice of texts is there-
fore driven by a curriculum model, informed by a 
student model, and constrained by the availability 
of suitable texts, as described by their text model.  
While a user-adapted tool has the potential to 
better match individual needs, since each student 
can work with different texts, a drawback of this 
approach is that instructors may have difficulty 
coordinating group discussion about readings and 
integrating the tool into their curriculum. An ad-
vantage of a tool containing a search system, how-
ever, is that teachers can find texts that match the 
needs and interests of the class as a whole. While 
some degree of individualization is lost, the advan-
tages of better coordinated support from teachers 
and classroom integration are gained. In the early 
stages of this project, we had planned to use REAP 
software after adapting it to handle the complex 
morphology of MSA. Unfortunately, while the 
system was already being tested in the field, REAP 
project leaders did not consider the code base ma-
ture enough to be released to other research 
groups. As a result, we chose to develop our own 
database and access method to texts, foregoing 
adaptation to individual users. 
4 Challenges of Arabic reading 
It has never been an easy transition from ?learning 
to read? to ?reading to learn? for Arabs and other 
Arabic learners. In Meynet (1971) and according to 
father Anastase Al-Karmali, a member of the 
Arabic Language Academy in Cairo, Egypt. ?The 
Arabs study the rules of the Arabic language in 
order to learn to read, whereas others read in order 
128
to learn ??. Indeed, reading in Arabic as a first or 
second language presents special challenges due to 
its script and its rich and complex morphology. 
Also, Arabic texts lack short vowels and other 
diacritics that distinguish words and grammatical 
functions. These linguistic complexities result in 
significant reading difficulties. Typically, Arabic 
as a second language learners face difficulties in 
word recognition, word disambiguation and the 
acquisition of decoding skills, including recogniz-
ing letter and word boundaries, decoding unvoca-
lized words and identifying these words. In order 
to understand Arabic text, the novice reader must 
learn to insert short vowels and other diacritics 
based on grammatical rules not yet learned. The 
ambiguity associated with a lack of diacritization is 
shown for instance in the lemma ??? /Elm/ which 
has the following nine possible reading interpreta-
tions shown in Table 1. 
???? ?Science, learning? 
????? ?flag? 
?3 ?????rd P. Masc. Sing. Perf. V. (MSA V. I) 
?he learned/knew? 
?3 ?????rd P. Sing. Pass. V. (MSA V. I) ?it/he 
was learned?  
??????? Intensifying, Caus. V. (MSA V. II) ?he 
taught 
??????? Causative V. Pass (MSA V. II) ?he was 
taught? 
??????/????  (NOM Noun + Definite and Indefinite) 
????? (ACCU Noun + Definite) 
??????/????  (GEN Noun + Definite and Indefinite) 
Table 1.  Various interpretations for the lemma ??? 
 
5 The Arabic reading enhancement tools 
To address these challenges, we developed an 
Arabic Reading Enhancement Tool (ARET) for 
classroom use with support from the U.S. Depart-
ment of Education?s International Research Study 
Program (IRS). The ARET tool is rather similar in 
intent to the foreign language learning tool, 
GLOSSER-RuG built by Nerbonne and Smit 
(1996) for Dutch, but targets explicitly the particu-
larities of MSA. ARET has two subparts tools : the 
Arabic Reading Facilitation Tool (ARFT) and the 
Arabic Reading Assessment Tool (ARAT). A ma-
jor achievement of this project was to create a col-
lection of fully annotated texts for learners of 
Arabic, using materials included in an authoritative 
textbook series that spans several competence le-
vels. In this section, we describe the creation, 
structure and content of the Arabic corpus/lexicon 
database, and then describe the ARFT and ARAT 
tools in more detail. 
5.1 The Al-Kitaab corpus database 
The ARET system uses the full text of Arabic 
reading passages from the Georgetown University 
Press Al-Kitaab textbook series, which represents a 
60,000 word corpus. Each passage was submitted 
to a combined automatic/manual annotation 
process in order to create a version of the text that 
was completely diacritized and thoroughly seg-
mented and labeled to identify all morphemes for 
each word, including their part-of-speech labels 
and English glosses. 
We first applied the Standard Arabic Morpho-
logical Analyzer (SAMA) (Maamouri et al, 2010), 
to enumerate all possible solutions for each word 
token in a given passage. The entire passage, with 
the full set of possible SAMA solutions for each 
word token, was then presented to a native Arabic 
speaker experienced in the morphological analysis 
of MSA, and their task was to select the particular 
SAMA solution for each word based on their un-
derstanding of the context; where necessary, the 
annotator would manually edit the details of POS 
tags or glosses to fill gaps in SAMA?s coverage of 
the vocabulary. This is a standard approach used in 
the annotation of numerous Arabic text corpora, 
including the Arabic Treebank Project (Maamouri 
and Bies 2004). As described in section 5.2, the 
resulting annotation was fully reviewed by expert 
Arabic linguists using our reading facilitation tool, 
to identify and repair errors. 
A relational database was created to store the 
corpus and annotations. Separate tables were used 
to enumerate (a) the reading passages (keeping 
track of the book volume, chapter and page num-
ber of each passage), (b) the sequence of sentences 
in each passage, (c) the word token sequence for 
each sentence, (d) the inventory of distinct word 
types (i.e. orthographic word forms with their con-
text-dependant analyses), and (e) the inventory of 
distinct ?headwords? (lemmas) and affix mor-
phemes (clitics). 
Using this relational table structure, a full pas-
sage could be assembled for display by querying 
129
for the sequence of sentences and the word tokens 
for each sentence. The information returned by the 
query could include, for each word token, the orig-
inal and/or diacritized spelling, and an index for 
looking up the context-dependent morphological 
analysis plus gloss for the token. This in turn also 
provided access to a dictionary entry for the lemma 
from which the token was derived. Table 2 sum-
marizes the contents of the database.  The number 
of distinct lemmas refers to the number of citation 
forms for content words (nouns, verbs, etc) that are 
referenced by the all the inflected stems found in 
the reading texts; the number of glossary entries 
refers to the manually edited dictionary descrip-
tions for lemmas / citation forms, including their 
consonantal roots. In cases where a lemma does 
not have a corresponding glossary entry, the fully-
detailed morphological analysis provides an Eng-
lish gloss (but not the root) for each word token 
containing the lemma. 
 
Type No. of 
Entries 
Sentences, titles and sub-headings 
3,692 
Arabic word tokens 53,411 
Distinct undiacritized Arabic ortho-
graphic forms 
17,209 
Distinct diacritized orthographic forms 20,725 
Distinct morphology/POS/gloss anno-
tations on word forms 
22,304 
Distinct clitic and inflected-stem mor-
phemes 
16,774 
Distinct lemmas 6,829 
Glossary entries for lemmas 3,436 
 Table 2. Corpus quantities in ARET database 
5.2 The Arabic reading facilitation tool 
The Arabic Reading Facilitation Tool (ARFT) 
provides the user with direct access to the Al-
Kitaab text corpus, organized by volume, chapter 
and page number. In addition to presenting the full 
text for a given passage, the user can click on any 
word in the passage to bring up in a side-bar the 
full morphological analysis and gloss for the word 
in that context, along with a glossary entry for the 
associated lemma, and a summary of other Arabic 
citation forms that are related by root. Two other 
important functions are also provided: (a) toggling 
the presence vs. absence of all diacritic marks in 
the full display of the reading passage, and (b) the 
ability to view a concordance of all occurrences for 
any selected word. The tool also provides a "tool-
tip" pop-up window whenever the mouse cursor 
hovers over an Arabic word in the text passage; if 
the page is showing undiacritized text, the pop-up 
shows the diacritized form of the word, and vice-
versa. This is a very useful feature for the new 
learners of the Arabic language. 
As soon as the annotated version of the corpus 
was loaded into the database, there was a sustained 
effort involving native Arabic speakers and Arabic 
faculty to carefully review the database content, as 
displayed by the ARFT, and validate it against the 
original textbook content. This effort involved 
numerous repairs of all sorts that stemmed from all 
stages of corpus preparation: typing mistakes from 
the original keyboarding of the text, problems in 
morphological annotation, and difficulties in the 
loading of the tables. Customized tools and proce-
dures were developed to facilitate the updates that 
were needed to apply all the corrections directly to 
the database.  
A glossary for use in the ARFT was added to 
the database, with the relational linkage needed to 
support glossary lookups triggered by the user 
clicking on any word in a text passage. The word-
to-glossary relation is based on the "lemma_ID" of 
the stem in each word. The lemma_ID is a string 
identifier assigned by the Standard Arabic Mor-
phological Analyzer (SAMA), which was used for 
the morphological annotation of the entire corpus; 
all verbs in a given conjugation paradigm share the 
same lemma_ID, as do all nouns or adjectives in a 
given declensional (case) paradigm, so every dis-
tinct inflected form of a noun, adjective or verb is 
linked by the annotation to its corresponding glos-
sary entry. The glossary table (with indexing by 
Semitic root) was a special, additional annotation 
specifically for ARFT, so not all lemmas were 
covered in the glossary; when a term not in the 
glossary is clicked, the side-bar display area in the 
ARFT shows the message "Refer to Morphology 
Information"; the morphology information is the 
full set of annotation data for each word based on 
SAMA, and this always includes an English gloss 
for the stem (except in the case of proper nouns, 
which always have "Proper Noun" as their part-of-
speech label).  
The ARFT is intended for use with a modern 
web browser over a reasonably fast internet con-
130
nection. The tool has a flexible and intuitive web 
interface to navigate the texts via several key fea-
tures: 
 
1. Source Panel, featuring Al-Kitaab text 
2. Highlighted Sentence 
3. Highlighted Word 
4. Audio Player for highlighted sentence 
5. Audio Player for highlighted word 
6. Morphological Data Panel 
7. Lexical Data Panel 
8. Tabbed browsing for convenient access to 
multiple screens 
 
Figure 1. below illustrates an example of the tool 
using a passage of text from Al-Kitab Volume 2, 
Page 61. 
 
 
Figure 1. Arabic Reading Facilitation Tool featuring 
function labels 
 
5.3 The Text to speech module 
An Arabic Text-to-Speech technology module was 
licensed from RDI1. This technology has been used 
to add an audio feature to the ARFT, and can be 
used to render audio of arbitrary Arabic text. So 
the users will be able to listen to individual words 
or passages of text spoken by a high quality syn-
thesized voice. The RDI module, reads text files or 
literal text in Windows Arabic encoding and gene-
rates WAV audio data either as files or for direct 
output to an audio device. It has a C++ API that 
may be employed in Microsoft Visual Studio. The 
                                                          
1<http://www.rdi-eg.com/Technologies/speech.htm> 
voice rendering quality is excellent. Moreover, the 
module analyzes diacritized or undiacritized Arab-
ic text to determine pronunciation, rhythm and 
inflection of speech. Many variables of speech 
production can be controlled, most significantly 
the gender of the speaker. We developed a simple 
console-based executable that reads a list of Arabic 
text files and generates a WAV file of speech cor-
responding to each one, using a male voice, female 
voice, or one of each.  
5.4 The Arabic Reading Assessment Tool 
(ARAT) 
In order to support the creation of tests and quizzes 
for specific Arabic reading skills the Arabic Read-
ing Assessment Tool (ARAT) has been built 
around an existing open-source web application 
framework called Moodle (http://moodle.org). This 
framework was developed as a ?Content Manage-
ment System?, and provides built-in support for 
many of the ?infrastructure? functions that ARAT 
would need, including: registration of faculty and 
student user accounts; creation of courses with 
schedule plans and content-based resources; crea-
tion, presentation and scoring of tests and quizzes; 
and overall record-keeping of resources, activities 
and test scores. Custom software modules were 
developed to augment the Moodle code base in 
order to provide functions that are specific to the 
ARAT: 
 - communicating with and importing data from the 
annotated Al-Kitaab passage database; 
 - defining specialized question types (the first 
three types described below) based on annotations 
in the database, such that answers to the questions 
can be scored automatically by reference to the 
corpus annotations.  
 
The three types of annotation-based questions 
were defined and implemented in the prototype 
ARAT: 
 - Cloze-Test Question: given a reading passage in 
Arabic, one or more words are chosen as test items 
and are replaced in the text by an underlined empty 
slot; the student is given a multiple-choice question 
to identify the correct Arabic word to fill each slot. 
 - English Gloss Question: given a reading passage, 
one or more words are chosen as test items and 
highlighted in the text; the student is given a mul-
131
tiple-choice question to identify the correct English 
gloss for each test word. 
 - Case-Ending Question: given reading passage, 
one or more nouns and/or adjectives are chosen as 
test items and highlighted in the text; the student is 
given a multiple-choice question of the six possible 
cases in Arabic to identify the correct case ending 
for each test word. Mood ending could also be 
considered for verbs. 
- Yes/No questions: these are fully  developed by 
teachers, who must enter questions and answers 
into the program in order to have Moodle give the 
student/teacher the appropriate final scores and 
correct answers feedback.  
The implementation allows an instructor to se-
lect what text passage to use for a given quiz, and 
also allows for either manual and automatic selec-
tion of words to use as test items from the text, as 
well as either manual or automatic selection of 
distractor items for the Cloze and Gloss tests. By 
providing automatic selection of test items and 
distractors based on available annotations in the 
corpus database, ARAT allows a student to prac-
tice each task any number of times on a given text 
passage, be challenged by novel questions on each 
attempt, and receive a tally of right and wrong 
answers, without the instructor having to create or 
score each attempt as shown in Figure 2. 
 
 
 
Figure 2. A sample question created with ARAT 
 
5.5 The test set creation procedure 
The procedure for creating a test set within ARAT 
breaks down to the following ?top-level? steps: 
 
1. Provide or select a text passage to be used as the 
source from which test questions are derived. 
 
2. For questions that will be based on specific word 
tokens in the text, identify the tokens that will be 
basis for test questions; these token-specific ques-
tions will always involve a particular task with a 
multiple-choice response, so for each selected to-
ken: select the task (word choice, gloss choice, 
case-ending), identify a correct answer and provide 
or select a set of three distractors. 
 
3. For questions not based on specific tokens, the 
instructor must supply the following: prompting 
text for the question, the type of response (y/n, t/f, 
type-in, multiple choice) and the correct answer 
(and three distractors for multiple choice). Figure 3 
shows the test set main screen. 
 
 
Figure 3. Test set main screen. 
 
6 Classroom usage and tool evaluation 
The ARFT was presented to Arabic faculty at the 
University of Pennsylvania; the tool was an-
nounced in Arabic courses and students were asked 
to use it. Several lists of student enrollments for 
many Arabic courses have been imported into the 
Moodle-based system.  
An informal evaluation was also performed, in 
the Summer of 2010, with Arabic instructors teach-
ing in the ARAbic and North African Studies 
(ARANAS) program at Al Akhawayn University, 
in Ifrane, Morocco. Unfortunately, due to the very 
rushed schedule and time pressure that instructors 
work under during this intensive program, the tools 
did not receive the desired attention. Only a hand-
ful of instructors actually explored the tools. Two 
132
instructors filled out an evaluation questionnaire 
concerning various aspects of the tools and their 
use of computer technology for language teaching 
in general. The feedback was generally positive 
and included some detailed suggestion for improv-
ing the tools; they also revealed some issues with 
inconsistent response time (partly due to the net-
work infrastructure of the university at that time) 
and ease of use (for non technology-savy instruc-
tors). The biggest obstacles to using the tools, 
however, appeared to be lack of time on the part of 
the instructors to acquire sufficient familiarity with 
the tools and devise effective ways of introducing 
them in the curriculum. We are investigating the 
possibility of using the tools with exchange stu-
dents during the regular academic year, even 
though the numbers in Arabic classes at all levels 
is much lower than in the Summer program. 
Recently, the use of the ARFT and its compa-
nion the ARAT has been mandated by the Arabic 
Section at the University of Pennsylvania and we 
hope that a more consistent use is going to be 
made. As of now, 118 students are registered 
representing four 1st Year classes (total: 63 stu-
dents), two 2nd Year classes (total:3 students), one 
3rd Year class ( total: 13 students) and One 4th 
Year class ( 11students).At this point, the tool im-
pact on the classroom has not been evaluated, but it 
is in our future plans to do a comprehensive class-
room evaluation of the tool. 
As part of the effort to introduce the ARFT and 
the ARAT to faculty, we obtained three short read-
ing passage texts, totaling 1022 Arabic word to-
kens, selected by a faculty member from news 
sources. These were submitted to annotation to 
disambiguate and diacritize the content based on 
SAMA analysis, just as was done for the Al-Kitaab 
passages. The annotated texts have been added into 
the database corpus and are available for use in the 
ARAT, but are not accessible for general browsing 
via the ARFT. The annotation and database import 
went quickly, demonstrating that these procedures 
have matured, and providing resources for building 
quizzes and tests based on materials that are ?un-
seen? by students who use both the ARFT and the 
ARAT. 
7 Conclusion 
We have described computational tools and lin-
guistic resources that enable students to enhance 
their Arabic reading skills by helping them with 
the difficulties they face in word recognition, word 
disambiguation and general decoding skills during 
the Arabic reading process. These computational 
tools and resources provide the needed correct and 
meaningful vocalizations by using natural lan-
guage processing (NLP) technologies namely a 
Standard Arabic Morphological Analyzer 
(SAMA), a concordance, a Text-to-Speech module 
and various interfaces. The time gained by students 
who use our Reading Enhancement Tools could be 
put to good use in the current ASL (Arabic as a 
Second Language) classroom which, following the 
ACTFL proficiency movement puts a primary 
emphasis on communication with less concern for 
accuracy as reflected in morphology or syntax, 
particularly at the initial stages of ASL learning. 
We reiterate at this point that our choice of the 
GUP Al-Kitaab textbook series was not fortuitous. 
We could have chosen any other pedagogical text 
but Al-Kitaab distinguishes itself by being widely 
used in the United States and abroad, and provid-
ing an extensive curriculum with a wide variety of 
texts. We are thankful that GUP gave us permis-
sion to use this resource, as it enabled us to create a 
tool that can accompany many English-speaking 
students studying MSA in many classrooms around 
the world.  
In addition to answering learners? reading needs 
in MSA, our efforts went beyond the specificities 
of this language by allowing us to demonstrate that 
our tools and the methodology we followed was in 
fact ?portable? to other languages which had a 
morphologically complex nature such as, for in-
stance, the Nahuatl Learning Environment (NLE) 
project based on the ARET infrastructure 2. Future 
efforts will continue experimentation of the use of 
available and robust Arabic NLP technologies to 
extend the enhancement of Arabic reading to better 
understanding of authentic reading text that the 
reader could download from the Internet for in-
stance. Progress in that direction is desirable and 
possible because it would increase the motivation 
of Modern Standard Arabic learners and will boost 
access by students and other professionals to au-
thentic real world language text in new genres and 
topics. In this way, the contribution of NLP tech-
                                                          
2 The Nahuatl learning tool project prepared by Jonathan 
Amith (n.d) and a team of Nahuatl speakers can be accessed 
online through a Beta version of the Nahuatl Learning Envi-
ronment at the LDC : http://nahuatl.ldc.upenn.edu/. 
133
nologies to the teaching and learning of languages 
may become more significant and more compelling 
to all concerned, teachers, learners and computer 
NLP specialists alike.  
Acknowledgements 
We gratefully acknowledge the sponsorship of the 
U.S. Department of Education, whose International 
Research Study (IRS) Grant No. P017A050040-
07-05 supported our work on this project. The 
views, opinions and/or findings contained in this 
article are those of the authors and should not be 
interpreted as representing the official views or 
policies, either expressed or implied, of the U.S. 
Department of Education's International Research 
Study program.  We also acknowledge the help 
and support of Georgetown University Press who 
allowed us to use their Al-Kitaab series as a testing 
ground for our tools.  Thanks and appreciation go 
Professor Roger Allen and his team of Arabic 
teachers at the University of Pennsylvania for their 
warm reception of our tools in their teaching struc-
ture.  Thanks go finally, to all the programmers 
and annotators who worked on the project.  They 
are numerous and we cannot give them all the cre-
dit they deserve but without them our achievement 
would not have been so significant.  
References 
Mahmoud Al-Batal, Kristen Brustad & Abbas Al-Tonsi. 
2006. Al-Kitaab fii tacallum al-cArabiyya, Volume II 
(with DVDs, Second Edition).Washington, D.C.: 
Georgetown University Press, 2006.  
Mahmoud Al-Batal, Kristen Brustad & Abbas Al-Tonsi. 
2004. Al-Kitaab fii tacallum al-cArabiyya, A Text-
book for Beginning Arabic, Volume I (with DVDs, 
Second Edition). Washington, D.C.: Georgetown 
University Press, 2004. 
Mahmoud Al-Batal, Kristen Brustad & Abbas Al-Tonsi. 
2001. Al-Kitaab fii tacallum al-cArabiyya,Volume 
III.Washington, D.C.: Georgetown University Press, 
2001. 
Jonathan Amith.  n.d. Nahuatl Learning Environment. 
Available online at : http://nahuatl.ldc.upenn.edu/. 
Jon Brown and Maxine Eskenazi. 2004. Retrieval of 
authentic documents for reader-specific lexical prac-
tice. In Proceedings of  InSTIL/ICALL Symposium 
2004. Venice, Italy. 
Tsuo-Lin Chiu, Hsien-Chin Liou and Yuli Yeha. 2007. 
A study of web-based oral activities enhanced byau-
tomatic speech recognition for EFL college learning. 
Computer Assisted Language Learning, 20 (3), 209? 
233. 
Michael Heilman, Kevyn Collins-Thompson, Jamie 
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cess of an Intelligent Tutoring System for lexical 
practice and reading comprehension. In Proceedings 
of the Ninth International Conference on Spoken 
LanguageProcessing. Pittsburgh, PA. 
Mohamed Maamouri, David Graff, Basma Bouziri, 
Sondos Krouna, Ann Bies and Seth Kulick. 2010. 
Standard Arabic Morphological Analyzer (SAMA) 
Version 3.1. Linguistic Data Consortium, Catalog 
No.: LDC2010L01. 
Mohamed Maamouri and Ann Bies. 2004. Developing 
an Arabic Treebank: Methods, Guidelines, Proce-
dures, and Tools. In Proceedings of the Workshop 
Computational Approaches to Arabic Script-based 
Languages. Pages 2-9./20th International Conference 
on Computational Linguistics/. COLING Geneva, 
Switzerland. 
Detmar Meurers, Ramon Ziai, Luiz Amaral, Adriane 
Boyd, Aleksandar Dimitrov, Vanessa Metcalf, Niels 
Ott. 2010. Enhancing Authentic Web Pages for Lan-
guage Learners. In Proceedings of the 5th Workshop 
on Innovative Use of NLP for Building Educational 
Applications, NAACL-HLT 2010, Los Angeles. 
Roland Meynet. 1971. L'?criture arabe en question: les 
projets de l'Acad?mie de Langue Arabe du Caire de 
1938 ? 1968.Beirut: Dar el-Machreq, 1971. 142 pp 
Eleni Miltsakaki and Audrey Troutt. 2007. Read-X: 
Automatic Evaluation of Reading Difficulty of Web 
Text. In Proceedings of  E-Learn 2007, sponsored by 
the Association for theAdvancement of Computing in 
Education. Quebec, Canada. 
John Nerbonne and Petra Smit. 1996. GLOSSER-RuG: 
in Support of Reading. In Proceedings of the 16th In-
ternational Conference on Computational Linguistics 
(COLING 1996). 
Kathleen M. Sheehan, Irene Kostin, Yoko Futagi. 2007. 
SourceFinder: A Construct-Driven Approach for Lo-
cating Appropriately Targeted Reading Comprehen-
sion Source Texts. In Proceedings of the SLaTE 
Workshop on Speech and Language Technology in 
Education.Carnegie Mellon University and Interna-
tional Speech Communication Association (ISCA). 
Li Wang. 2005. The advantages of using technology in 
second language education. T.H.E. Journal, 32 (10), 
1-6. 
134
Paige D. Ware. 2004. Confidence and competition on-
line: ESL student perspectives on web based discus-
sions in the classroom. Computers and Composition, 
21, 451?468. 
Yi Yuan. 2003. The use of chat rooms in an ESL set-
ting. Computers and Composition, 20, 194?206. 
135
Workshop on Computational Linguistics for Literature, pages 78?83,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
A Pilot PropBank Annotation for Quranic Arabic 
 
                  Wajdi Zaghouani 
            University of Pennsylvania 
                Philadelphia, PA USA 
     wajdiz@ldc.upenn.edu 
               Abdelati Hawwari and Mona Diab    
              Center for Computational Learning Systems 
            Columbia University, NYC, USA 
      {ah3019,mdiab}@ccls.columbia.edu 
 
 
 
  
Abstract 
The Quran is a significant religious text written in a 
unique literary style, close to very poetic language 
in nature. Accordingly it is significantly richer and 
more complex than the newswire style used in the 
previously released Arabic PropBank (Zaghouani 
et al, 2010; Diab et al, 2008). We present prelimi-
nary work on the creation of a unique Arabic prop-
osition repository for Quranic Arabic. We annotate 
the semantic roles for the 50 most frequent verbs in 
the Quranic Arabic Dependency Treebank (QATB) 
(Dukes and Buckwalter 2010). The Quranic Arabic 
PropBank (QAPB) will be a unique new resource 
of its kind for the Arabic NLP research community 
as it will allow for interesting insights into the 
semantic use of classical Arabic, poetic literary 
Arabic, as well as significant religious texts. More-
over, on a pragmatic level QAPB will add approx-
imately 810 new verbs to the existing Arabic 
PropBank (APB). In this pilot experiment, we 
leverage our knowledge and experience from our 
involvement in the APB project. All the QAPB 
annotations will be made freely available for re-
search purposes. 
1 Introduction 
Explicit characterization of the relation between 
verbs and their arguments has become an impor-
tant issue in sentence processing and natural lan-
guage understanding. Automatic Semantic role 
labeling [SRL] has become the correlate of this 
characterization in natural language processing 
literature (Gildea and Jurafsky 2002). In SRL, the 
system automatically identifies predicates and their 
arguments and tags the identified arguments with 
meaningful semantic information. SRL has been 
successfully used in machine translation, summari-
zation and information extraction. In order to build 
robust SRL systems there is a need for significant 
resources the most important of which are seman-
tically annotated resources such as proposition 
banks. Several such resources exist now for differ-
ent languages including FrameNet (Baker et al, 
1998), VerbNet (Kipper et al 2000) and PropBank 
(Palmer et al, 2005). These resources have marked 
a surge in efficient approaches to automatic SRL of 
the English language. Apart from English, there 
exist various PropBank projects in Chinese (Xue et 
al., 2009), Korean (Palmer et al 2006) and Hindi 
(Ashwini et al, 2011). These resources exist on a 
large scale spearheading the SRL research in the 
associated languages (Carreras and Marquez, 
2005), Surdeanu et al (2008). However, resources 
created for Arabic are significantly more modest. 
The only Arabic Propank [APB] project (Zaghoua-
ni et al, 2010; Diab et al, 2008) based on the 
phrase structure syntactic Arabic Treebank (Maa-
mouri et al 2010) comprises a little over 4.5K 
verbs of newswire modern standard Arabic. Apart 
from the modesty in size, the Arabic language 
genre used in the APB does not represent the full 
scope of the Arabic language. The Arabic culture 
has a long history of literary writing and a rich 
linguistic heritage in classical Arabic. In fact all 
historical religious non-religious texts are written 
in Classical Arabic. The ultimate source on clas-
sical Arabic language is the Quran. It is considered 
the Arabic language reference point for all learners 
of Arabic in the Arab and Muslim world. Hence 
understanding the semantic nuances of Quranic 
Arabic is of significant impact and value to a large 
population. This is apart from its significant differ-
ence from the newswire genre, being closer to 
poetic language and more creative linguistic ex-
78
pression. Accordingly, in this paper, we present a 
pilot annotation project on the creation a Quranic 
Arabic PropBank (QAPB) on layered above the 
Quranic Arabic Dependency Treebank (QATB) 
(Dukes and Buckwalter 2010).  
2 The PropBank model  
The PropBank model is a collection of annotated 
propositions where each verb predicate is anno-
tated with its semantic roles. An existing syntactic 
treebank is typically a prerequisite for this shallow 
semantic layer. For example consider the following 
English sentence: ?John likes apples?, the predicate 
is ?likes? and the first argument, the subject, is 
?John?, and the second argument, the object, is 
?apples?. ?John? would be semantically annotated 
as the agent and ?apples? would be the theme. Ac-
cording to PropBank, ?John? is labeled ARG0 and 
?apples? is labeled ARG1. Crucially, regardless of 
the adopted semantic annotation formalism (Prop-
Bank, FrameNet, etc), the labels do not vary in 
different syntactic constructions, which is why 
proposition annotation is different from Treebank 
annotation. For instance, if the example above was 
in the passive voice, ?Apples are liked by John?, 
John is still the agent ARG0, and Apples are still 
the theme ARG1.  
3 Motivation and Background 
The main goal behind this project is to extend cov-
erage of the existing Arabic PropBank (APB) to 
more verbs and genres (Zaghouani et al 2010; 
Diab et al 2008). APB is limited to the newswire 
domain in modern standard Arabic (MSA). It sig-
nificantly lags behind the English PropBank (EPB) 
in size. EPB consists of 5413 verbs corresponding 
to 7268 different verb senses, the APB only covers 
2127 verb types corresponding to 2657 different 
verb senses. According to El-Dahdah (2008) Arab-
ic Dictionary, there are more than 16,000 verbs in 
the Arabic language. The Quran corpus comprises 
a total of 1466 verb types including 810 not 
present in APB. Adding the 810 verbs to the APB 
is clearly a significant boost to the size of the APB 
(38% amounting to 2937 verb types).  
In the current paper however we address the 
annotation of the Quran as a stand alone resource 
while leveraging our experience in the APB anno-
tation process. The Quran consists of 1466 verb 
types corresponding to 19,356 verb token in-
stances. The language of the Quran is Classical 
Arabic (CA) of 77,430 words, sequenced in chap-
ters and verses, dating back to over 1431 years. It 
is considered a reference text on both religious as 
well as linguistic matters. The language is fully 
specified with vocalic and pronunciation markers 
to ensure faithful oration. The language is poetic 
and literary in many instances with subtle allusions 
(Zahri 1990). It is the source of many other reli-
gious and heritage writings and a book of great 
importance to muslims worldwide, including non 
speakers of Arabic.  
Dukes and Buckwalter (2010) started the Qu-
ranic Arabic Corpus, an annotated linguistic re-
source which marks the Arabic grammar, syntax 
and morphology for each word. The QATB pro-
vides two levels of analysis: morphological annota-
tion and syntactic representation. The syntax of 
traditional Arabic grammar is represented in the 
Quranic Treebank using hybrid dependency graphs 
as shown in Figure 1.1 To the best of our know-
ledge, this is the first PropBank annotation of a 
religious and literary style text. 
The new verbs added from the Quran are also 
common verbs widely used today in MSA but the 
Quranic context adds more possible senses to these 
verbs. Having a QAPB allows for a more semantic 
level of analysis to the Quran. Currently the Qu-
ranic Corpus Portal2 comprises morphological 
annotations, syntactic treebanks, and a semantic 
ontology. Adding the QAPB will render it a unique 
source for Arabic language scholars worldwide 
(more than 50,000 unique visitors per day).  
Linguistic studies of the Quranic verbs such as 
verbal alternations, verb valency, polysemy and 
verbal ambiguity are one of the possible research 
directions that could be studied with this new re-
source. On the other hand, the Arabic NLP re-
search community will benefit from the increased 
coverage of the APB verbs, and the new domain 
covered (religious) and the new writing style (Qu-
ranic Arabic). Furthermore, Quranic citations are 
commonly used today in MSA written texts 
(books, newspapers, etc.), as well as Arabic social 
media intertwined with dialectal writings. This 
                                                 
1This display is different from the other existing Arabic Tree-
bank, the Prague Arabic Dependency Treebank (PADT) (Smr? 
et al, 2008). 
2http://corpus.quran.com/ 
79
makes the annotation of a Quranic style a rare and 
relevant resource for the building of Arabic NLP 
applications.  
4 Methodology 
We leverage the approach used with the previous 
APB (Zaghouani et al 2010; Diab et al 2008). We 
pay special attention to the polysemic nature of 
predicates used in Quranic Arabic. An Arabic root 
meaning tool is used as a reference to help in iden-
tifying different senses of the verb. More effort is 
dedicated to revision of the final product since 
unlike the APB, the QAPB is based on a depen-
dency Treebank (QATB) not a phrase structure 
Treebank.3 
For this pilot annotation experiment, we on-
ly annotate the 50 most frequent verbs in the cor-
pus corresponding to 7227 verbal occurrences in 
the corpus out of 19,356 total verbal instances. In 
the future plans, the corpus will cover eventually 
all the 1466 verbs in the whole Quranic corpus. 
Ultimately, it is our plan to perform a merging 
between the new frame files of the QAPB and the 
existing 1955 Frame files of the Arabic PropBank 
 
4.1 The annotation process 
 
The PropBank annotation process is divided into 
two steps: a. creation of the frame files for verbs 
occurring in the data, and b. annotation of the ver-
bal instances with the frame file ids. During the 
creation of the Frame Files, the usages of the verbs 
in the data are examined by linguists (henceforth, 
?framers?). During the frameset creation process, 
verbs that share similar semantic and syntactic 
characteristics are usually framed similarly). Once 
a predicate (in this case a verb) is chosen, framer-
look at an average sample size of 60-70 instances 
per predicate found in the Quranic corpus in order 
to get an idea of its syntactic behavior. Based on 
these observations and their linguistic knowledge 
and native-speaker intuition, the framers create a 
Frame File for each verb containing one or more 
framesets, which correspond to coarse-grained 
senses of the predicate lemma. Each frameset spe-
cifies the PropBank core labels (i.e., ARG0, 
                                                 
3 The Propbank style of annotation are already used with other 
languages on top of dependency Treebank structures such as 
the Hindi  Treebank project (Ashwini et al, 2011).  
ARG1,?ARG4) corresponding to the argument 
structure of the verb. Additionally, illustrative ex-
amples are included for each frameset, which will 
later be referenced by the annotators. Note that in 
addition to these core, numbered roles, PropBank 
also includes annotations of a variety of modifier 
roles, prefixed by ARGM labels from a list of 15 
arguments (ARGM-ADV, ARGM-BNF, ARGM-
CAU,ARGM-CND, ARGM-DIR, ARGM-DIS, 
ARGM-EXT, ARGM-LOC, ARGM-MNR, 
ARGM-NEG, ARGM-PRD, ARGM-PRP, ARGM-
REC, ARGM-TMP, ARGM-PRD). Unlike the 
APB frame files creation, where no specific Arabic 
reference is used, for this project, an Arabic root 
meaning reference tool developed by Swalha 
(2011) is used by the framers to ensure that all 
possible meanings of the verbs in the corpus are 
covered and all various senses are taken into ac-
count. The Arabic root-meaning search tool is free-
ly available online.4 The search is done by root, the 
tool displays all possible meanings separated by a 
comma with citation examples from many sources 
including the Quran. Once the Frame files are 
created, the data that have the identified predicate 
occurrences are passed on to the annotators for a 
double-blind annotation process using the pre-
viously created framesets. Each PropBank entry 
represents a particular instance of a verb in a par-
ticular sentence in the Treebank and the mapping 
of numbered roles to precise meanings is given on 
a verb-by-verb basis in a set of frames files during 
the annotation procedure. To ensure consistency, 
the data is double annotated and finally adjudicated 
by a third annotator. The adjudicator resolves dif-
ferences between the two annotations if present to 
produce the gold annotation. A sample Frameset 
and a related annotation example from the QAPB 
are shown in Table 1. During the annotation 
process, the data is organized by verb such that 
each verb with all its instances is annotated at 
once. In doing so, we firstly ensure that the frame-
sets of similar verbs, and in turn, the annotation of 
the verbs, will both be consistent across the data. 
Secondly, by tackling annotation on a verb-by-verb 
basis, the annotators are able to concentrate on a 
single verb at a time, making the process easier and 
faster for the annotators. 
                                                 
4 Available at :<http://www.comp.leeds.ac.uk/cgi-
bin/scmss/arabic_roots.py> 
 
80
 
FrameSet Example Annotation Example 
Predicate: wajada?????   
 ?
Roleset id: f1, to find 
Arg0: the finder 
Arg1: thing found 
 
Rel: wajada, ?????? 
Arg0: -NONE- * 
Gloss: You 
Arg1: ?? 
Gloss: it 
ArgM-LOC: ??????? ???????? 
Gloss: with Allah 
 
Example in Arabic: 
 ?????? ??????????? ?????????????? ???? ?????? ????????? ?????
????????? 
Gloss: and whatever good you 
put forward for yourselves - 
you will find it with Allah 
Table 1. The frameset / Annotation of wajada 
 
4.2 Tools 
 
Frameset files are created in an XML format. We 
use tools used in the APB project. The Frame File 
editing is performed by the Cornerstone tool (Choi 
et al, 2010a), which is a PropBank frameset editor 
that allows creation and editing of PropBank fra-
mesets without requiring any prior knowledge of 
XML. Moreover, we use Jubilee5 as the annotation 
tool (Choi et al, 20010b). Jubilee is a recent anno-
tation tool which improves the annotation process 
of the APB by displaying several types of relevant 
syntactic and semantic information simultaneously. 
Having everything displayed helps the annotator 
quickly absorb and apply the necessary syntactic 
and semantic information pertinent to each predi-
cate for consistent and efficient annotation. Both 
tools are currently being modified in order to han-
dle the Dependency TreeBank structure, originally 
the tool was designed specifically to handle phrase 
structure Tree format. Moreover, since the file 
formats and the tree formats in the dependency 
Treebank are different from the previous APB 
effort, a revision in the Quranic Treebank output 
had to be done. This involves mainly a change in 
the annotated data format in order to add the role 
labels in the annotation file. For the moment, all of 
the 50 XML Frame files have been created and 
some manual annotation is performed to illustrate 
the feasibility of the experiment. 
                                                 
5 Cornerstone and Jubilee are available as Open Source tools 
on Google code. 
 
4.3 Impact of the dependency structure 
Treebank  
 
Having The Quran corpus annotated using a de-
pendency structure Treebank has some advantages. 
First, semantic arguments can be marked explicitly 
on the syntactic trees (such as the Arg0 Pron. In 
Figure 1), so annotations of the predicate argument 
structure can be more consistent with the depen-
dency structure as shown in Figure 1.  
 
 
Figure 1. Semantic role labels to the QATB 
 
Secondly, the Quranic Arabic Dependency 
Treebank (QATB) provides a rich set of dependen-
cy relations that capture the syntactic-semantic 
information. This facilitates possible mappings 
between syntactic dependents and semantic argu-
ments. A successful mapping would reduce the 
annotation effort.  
It is worth noting the APB comprises 1955 ver-
bal predicates corresponding to 2446 framesets 
with an ambiguity ratio of 1.25. This is in contrast 
to the QAPB where we found that the 50 verbal 
predicate types we annotated corresponded to 71 
framesets thereby an ambiguity ratio of 1.42. 
Hence these results suggest that the QAPB is more 
ambiguous than the newswire genre annotated in 
the APB. By way of contrast, the EPB comprises 
6089 verbal predicates corresponding to 7268 fra-
mesets with an ambiguity ratio of 1.19. 
21 verb types of the 50 verbs we annotated are 
present in both corpora corresponding to 31 frame-
sets in QAPB (a 1.47 ambiguity ratio) and 25 fra-
mesets in APB (1.19 ambiguity ratio).  The total 
verbal instances in the QAPB is 2974. 29 verb 
81
types with their corresponding 40 framesets occur 
only in the QAPB (58% of the list of 50 verbs). 
This translated to a 1.38 ambiguity ratio. 
In the common 21 verb types shared between 
APB and QAPB corpora we note that 12 predicates 
share the same exact frame sets indicating no 
change in meaning between the use of the predi-
cates in the Quran and MSA. However, 9 of the 
verbal predicates have more framesets in QAPB 
than APB. None of the verbal predicates have 
more framesets in APB than QAPB. Below is an 
example of a verbal predicate with two different 
framesets.  
 
FrameSet  
Example 
Annotation Example 
Predicate: >anozal ??????? 
Roleset id: f1, to reveal 
Arg0: revealer 
Arg1: thing revealed 
Arg2: start point 
Arg3: end point, recipient 
Rel: >anozal 
Arg0: ??? 
Gloss: we 
Arg1: ??????? ?????????  
Gloss:  clear verses 
Arg3: ????????  
Gloss: to you 
 
Example in Arabic: 
????????? ??????????? ???????? ?????? ?????????  
We have certainly revealed to you 
verses [which are] clear proofs 
Table 2. The frameset / Annotation of  >anozal 
(QAPB)  
 
FrameSet  
Example 
Annotation Example 
Predicate:  
>anozal ??????? 
Roleset id: f1, to 
release 
Arg0: agent re-
leasing 
Arg1:thing re-
leased 
Rel: >anozal 
Arg0: ????   
Gloss: Zyad 
Arg1:NONE-* 
Gloss: He 
ARGM-TMP: ????? ?????????? 
Gloss: the mid-eighties 
 
Example in Arabic: 
 ???? ????? ??? ????? ???? ??? ?? ???? ???? ?????
???? ????? ?????????? 
The songs of the Album I am not a disbe-
liever released by Ziad during the eighties 
are popular again.  
Table 3. The frameset / Annotation of  >anozal 
(APB)  
The two frames of verb ?? >anozal ? can clarify 
the meaning differences between MSA and QA as 
used in the Quran. Although  both APB and QAPB 
have this verb, they have different senses leading 
to different semantic frames. In the QAPB the 
sense of revealed is only associated with religious 
texts, while in MSA it has the senses of released or 
dropped.   
5 Conclusion 
We have presented a pilot Quranic Arabic 
PropBank experiment with the creation of frame 
files for 50 verb types. At this point, our initial 
study confirms that building a lexicon and tagging 
the Arabic Quranic Corpus with verbal sense and 
semantic information following the PropBank 
model is feasible. In general, the peculiarities of 
the Quranic Arabic language did not seem to cause 
problems for the PropBank annotation model. We 
plan to start the effective annotation of the resource 
in order to finalize the creation of a QAPB that 
covers all 1466 verbal predicates. Once released, 
the data will be freely available for research pur-
pose. 
References  
Vaidya Ashwini, Jinho Choi, Martha Palmer, and Bhu-
vana Narasimhan. 2011. Analysis of the Hindi Prop-
osition Bank using Dependency Structure. In 
Proceedings of the fifth Linguistic Annotation Work-
shop. ACL 2011, pages 21-29. 
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING-ACL ?98, the University of Mon-
treal, pages 86?90. 
Xavier Carreras and Llu?s Marquez. 2005. Introduction 
to the CoNLL-2005 shared task: Semantic role labe-
ling. In Proceedings of the Ninth (CoNLL-2005), 
pages 152?164. 
Jinho Choi, Claire Bonial, and Martha Palmer.2010a. 
PropBank Instance Annotation Guidelines Using a 
Dedicated Editor, Cornerstone.In Proceedings of the 
(LREC'10), pages 3650-3653. 
Jinho Choi, Claire Bonial, and Martha Palmer.2010b. 
PropBank Instance Annotation Guidelines Using a 
Dedicated Editor, Jubilee.In Proceedings of the 
(LREC'10), pages 1871-1875. 
Mona Diab, Aous Mansouri, Martha Palmer, Olga Bab-
ko-Malaya,Wajdi Zaghouani, Ann Bies, and Mo-
82
hammed Maamouri. 2008. A Pilot Arabic PropBank. 
In Proceedings of the (LREC'08), pages 3467-3472. 
Kais Dukes and Tim Buckwalter. 2010. A Dependency 
Treebank of the Quran using Traditional Arabic 
Grammar. In Proceedings of the 7th International 
Conference on Informatics and Systems (INFOS). 
Antoine El-Dahdah. 2008. A Dictionary of Arabic Verb 
Conjugation. Librairie du Liban, Beirut, Lebanon. 
Daniel Gildea, and Daniel Jurafsky. 2002. Automatic-
Labeling of Semantic Roles. Computational Linguis-
tics 28:3, 245-288 
Karin Kipper, HoaTrang Dang, and Martha Palmer. 
2000. Class-Based Construction of a Verb Lexicon. 
In Proceedings of the AAAI-2000 Seventeenth Na-
tional Conference on Artificial Intelligence, pages 
691-696. 
 Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma-
Gaddeche, Wigdan Mekki, Sondos Krouna, Basma-
Bouziri, and Wajdi Zaghouani. 2011. Arabic 
Treebank: Part 2 v 3.1. LDC Catalog 
No.:LDC2011T09 
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. 
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics Journal, 
31:1 
Martha Palmer, Shijong Ryu, Jinyoung Choi, Sinwon 
Yoon, and Yeongmi Jeon. 2006. LDC Catalog 
LDC2006T03. 
Otakar Smr?, Viktor Bielick?, IvetaKou?ilov?, Jakub 
Kr??mar, Jan Haji? and Petr Zem?nek. 2008. Prague 
Arabic Dependency Treebank: A Word on the Mil-
lion Words.In Proceedings of the Workshop on 
Arabic and Local Languages (LREC 2008). 
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s Marquez, and Joakim Nivre. 2008. The 
CoNLL-2008 shared task on joint parsing on syntac-
tic and semantic dependencies. In Proceedings of 
CoNLL?08, pages 159?177. 
Majdi Swalha. 2011. Open-source Resources and Stan-
dards for Arabic Word Structure Analysis: Fine 
Grained Morphological Analysis of Arabic Text Cor-
pora. PhD thesis, Leeds University. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94. 
Wajdi Zaghouani , Mona Diab, Aous Mansouri, Sameer 
Pradhan, and Martha Palmer. 2010. The revised 
Arabic PropBank. In Proceedings of the Fourth Lin-
guistic Annotation Workshop (LAW IV '10), ACL, 
pages 222-226. 
Maysoon Zahri. 1990. Metaphor and translation. PhD 
thesis, University of Salford. 
 
 
83
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39?47,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The First QALB Shared Task on Automatic Text Correction for Arabic
Behrang Mohit
1?
, Alla Rozovskaya
2?
, Nizar Habash
3
, Wajdi Zaghouani
1
, Ossama Obeid
1
1
Carnegie Mellon University in Qatar
2
Center for Computational Learning Systems, Columbia University
3
New York University Abu Dhabi
behrang@cmu.edu, alla@ccls.columbia.edu, nizar.habash@nyu.edu
wajdiz@qatar.cmu.edu,owo@qatar.cmu.edu
Abstract
We present a summary of the first shared
task on automatic text correction for Ara-
bic text. The shared task received 18 sys-
tems submissions from nine teams in six
countries and represented a diversity of ap-
proaches. Our report includes an overview
of the QALB corpus which was the source
of the datasets used for training and eval-
uation, an overview of participating sys-
tems, results of the competition and an
analysis of the results and systems.
1 Introduction
The task of text correction has recently gained a
lot of attention in the Natural Language Process-
ing (NLP) community. Most of the effort in this
area concentrated on English, especially on errors
made by learners of English as a Second Lan-
guage. Four competitions devoted to error cor-
rection for non-native English writers took place
recently: HOO (Dale and Kilgarriff, 2011; Dale
et al., 2012) and CoNLL (Ng et al., 2013; Ng et
al., 2014). Shared tasks of this kind are extremely
important, as they bring together researchers who
focus on this problem and promote development
and dissemination of key resources, such as bench-
mark datasets.
Recently, there have been several efforts aimed
at creating data resources related to the correc-
tion of Arabic text. Those include human anno-
tated corpora (Zaghouani et al., 2014; Alfaifi and
Atwell, 2012), spell-checking lexicon (Attia et al.,
2012) and unannotated language learner corpora
(Farwaneh and Tamimi, 2012). A natural exten-
sion to these resource production efforts is the cre-
ation of robust automatic systems for error correc-
tion.
* These authors contributed equally to this work.
In this paper, we present a summary of the
QALB shared task on automatic text correction
for Arabic. The Qatar Arabic Language Bank
(QALB) project
1
is one of the first large scale data
and system development efforts for automatic cor-
rection of Arabic which has resulted in annota-
tion of the QALB corpus. In conjunction with the
EMNLP Arabic NLP workshop, the QALB shared
task is the first community effort for construction
and evaluation of automatic correction systems for
Arabic.
The results of the competition indicate that the
shared task attracted a lot of interest and generated
a diverse set of approaches from the participating
teams.
In the next section, we present the shared task
framework. This is followed by an overview of
the QALB corpus (Section 3). Section 4 describes
the shared task data, and Section 5 presents the ap-
proaches adopted by the participating teams. Sec-
tion 6 discusses the results of the competition. Fi-
nally, in Section 7, we offer a brief analysis and
present preliminary experiments on system com-
bination.
2 Task Description
The QALB shared task was created as a forum for
competition and collaboration on automatic error
correction in Modern Standard Arabic. The shared
task makes use of the QALB corpus (Zaghouani et
al., 2014), which is a manually-corrected collec-
tion of Arabic texts. The shared task participants
were provided with training and development data
to build their systems, but were also free to make
use of additional resources, including corpora, lin-
guistic resources, and software, as long as these
were publicly available.
For evaluation, a standard framework devel-
1
http://nlp.qatar.cmu.edu/qalb/
39
Original Corrected

??

K @Q? @

HCJ


?j

J? @

?
	
Y?

?

K @Q

? Y
	
J? ?



GXA?? ?


Y? @?P??

J

K B
?


X

?

@
	
?@ ?

<? @
	
?? ?


	
??

JK
.

I
	
J? ? H
.
A

? ?


	
G @

B

??
Q

j?
?
@ ?
YJ


?K
.
@
	
Y?
	
?@ @?YJ
.
K


	
?A? ? ?


??

?B@ Yj
.
??
?
AK
.
@P?Q?

?Q???@
	
???
?
?
	
K @ ??

?J


K
.
	
?A?

?J


	
J?B@ ???


Yg ?


	
? A? ??
	
? ?A
	
J?
?
@
.

??J


j

?? ?

J


	
J?@
	
?

BA??

?

?m
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137?142,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
CMUQ@QALB-2014: An SMT-based System
for Automatic Arabic Error Correction
Serena Jeblee
1
, Houda Bouamor
2
, Wajdi Zaghouani
2
and Kemal Oflazer
2
1
Carnegie Mellon University
sjeblee@cs.cmu.edu
2
Carnegie Mellon University in Qatar
{hbouamor,wajdiz}@qatar.cmu.edu, ko@cs.cmu.edu
Abstract
In this paper, we describe the CMUQ sys-
tem we submitted to The ANLP-QALB 2014
Shared Task on Automatic Text Correction
for Arabic. Our system combines rule-based
linguistic techniques with statistical language
modeling techniques and machine translation-
based methods. Our system outperforms the
baseline and reaches an F-score of 65.42% on
the test set of QALB corpus. This ranks us 3rd
in the competition.
1 Introduction
The business of text creation and editing represents a
large market where NLP technologies might be applied
naturally (Dale, 1997). Today?s users of word proces-
sors get surprisingly little help in checking spelling,
and a small number of them use more sophisticated
tools such as grammar checkers, to provide help in en-
suring that a text remains grammatically accurate after
modification. For instance, in the Arabic version of Mi-
crosoft Word, the spelling checker for Arabic, does not
give reasonable and natural proposals for many real-
word errors and even for simple probable errors (Had-
dad and Yaseen, 2007).
With the increased usage of computers in the pro-
cessing of natural languages comes the need for cor-
recting errors introduced at different stages. Natu-
ral language errors are not only made by human op-
erators at the input stage but also by NLP systems
that produce natural language output. Machine trans-
lation (MT), or optical character recognition (OCR),
often produce incorrect output riddled with odd lexi-
cal choices, grammar errors, or incorrectly recognized
characters. Correcting human/machine-produced er-
rors, or post-editing, can be manual or automated. For
morphologically and syntactically complex languages,
such as Modern Standard Arabic (MSA), correcting
texts automatically requires complex human and ma-
chine processing which makes generation of correct
candidates a challenging task.
For instance, the Automatic Arabic Text Correction
Shared Task is an interesting testbed to develop and
evaluate spelling correction systems for Arabic trained
either on naturally occurring errors in texts written by
humans (e.g., non-native speakers), or machines (e.g.,
MT output). In such tasks, participants are asked to
implement a system that takes as input Modern Stan-
dard Arabic texts with various spelling errors and au-
tomatically correct them. In this paper, we describe
the CMUQ system we developed to participate in the
The First Shared Task on Automatic Text Correction
for Arabic (Mohit et al., 2014). Our system combines
rule-based linguistic techniques with statistical lan-
guage modeling techniques and machine translation-
based methods. Our system outperforms the baseline,
achieves a better correction quality and reaches an F-
score of 62.96% on the development set of QALB cor-
pus (Zaghouani et al., 2014) and 65.42% on the test set.
The remainder of this paper is organized as follows.
First, we review the main previous efforts for automatic
spelling correction, in Section 2. In Section 3, we de-
scribe our system, which consists of several modules.
We continue with our experiments on the shared task
2014 dev set (Section 4). Then, we give an analysis of
our system output in Section 5. Finally, we conclude
and hint towards future improvement of the system, in
Section 6.
2 Related Work
Automatic error detection and correction include auto-
matic spelling checking, grammar checking and post-
editing. Numerous approaches (both supervised and
unsupervised) have been explored to improve the flu-
ency of the text and reduce the percentage of out-
of-vocabulary words using NLP tools, resources, and
heuristics, e.g., morphological analyzers, language
models, and edit-distance measure (Kukich, 1992;
Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan
et al., 2003; Haddad and Yaseen, 2007; Hassan et al.,
2008; Habash, 2008; Shaalan et al., 2010). There has
been a lot of work on error correction for English (e.g.,
(Golding and Roth, 1999)). Other approaches learn
models of correction by training on paired examples
of errors and their corrections, which is the main goal
of this work.
For Arabic, this issue was studied in various direc-
tions and in different research work. In 2003, Shaalan
et al. (2003) presented work on the specification and
classification of spelling errors in Arabic. Later on,
Haddad and Yaseen (2007) presented a hybrid ap-
proach using morphological features and rules to fine
137
tune the word recognition and non-word correction
method. In order to build an Arabic spelling checker,
Attia et al. (2012) developed semi-automatically, a dic-
tionary of 9 million fully inflected Arabic words us-
ing a morphological transducer and a large corpus.
They then created an error model by analyzing error
types and by creating an edit distance ranker. Finally,
they analyzed the level of noise in different sources of
data and selected the optimal subset to train their sys-
tem. Alkanhal et al. (2012) presented a stochastic ap-
proach for spelling correction of Arabic text. They used
a context-based system to automatically correct mis-
spelled words. First of all, a list is generated with pos-
sible alternatives for each misspelled word using the
Damerau-Levenshtein edit distance, then the right al-
ternative for each misspelled word is selected stochas-
tically using a lattice search, and an n-gram method.
Shaalan et al. (2012) trained a Noisy Channel Model
on word-based unigrams to detect and correct spelling
errors. Dahlmeier and Ng (2012a) built specialized de-
coders for English grammatical error correction. More
recently, (Pasha et al., 2014) created MADAMIRA,
a system for morphological analysis and disambigua-
tion of Arabic, this system can be used to improve the
accuracy of spelling checking system especially with
Hamza spelling correction.
In contrast to the approaches described above, we
use a machine translation (MT) based method to train
an error correction system. To the best of our knowl-
edge, this is the first error correction system for Arabic
using an MT approach.
3 Our System
Our system is a pipeline that consists of several dif-
ferent modules. The baseline system uses a spelling
checking module, and the final system uses a phrase-
based statistical machine translation system. To
preproces the text, we use the provided output of
MADAMIRA (Pasha et al., 2014) and a rule-based
correction. We then do a rule-based post-processing
to fix the punctuation.
3.1 Baseline Systems
For the baseline system, we try a common spelling
checking approach. We first pre-process the data us-
ing the features from MADAMIRA (see Feature 14
Replacement), then we use a noisy channel model for
spelling checking.
Feature 14 Replacement
The first step in the pipeline is to extract
MADAMIRA?s 14th feature from the .column file
and replace each word in the input text with this form.
MADAMIRA uses morphological disambiguation and
SVM analysis to select the most likely fully diacritized
Arabic word for the input word. The 14th feature
represents the undiacritized form of the most likely
word. This step corrects many Hamza placement or
omission errors, which makes a good base for other
correction modules.
Spelling Correction
The spelling checker is based on a noisy channel model
- we use a word list and language model to determine
the most probable correct Arabic word that could have
generated the incorrect form that we have in the text.
For detecting spelling errors we use the AraComLex
word list for spelling checking (Attia et al., 2012),
which contains about 9 million Arabic words.
1
We
look up the word from the input sentence in this list,
and attempt to correct those that are not found in the
list. We also train a mapping of incorrect words and
possible corrections from the edits in the training data.
If the word is in this map, the list of possible correc-
tions from the training data becomes the candidate list.
If the word is not in the trained map, the candidate list
is created by generating a list of words with common
insertions, substitutions, and deletions, according to the
list in (Attia et al., 2012). Each candidate is generated
by performing these edits and has a weight according to
the edit distance weights in the list. We then prune the
candidate list by keeping only the lowest weight words,
and removing candidates that are not found in the word
list. The resulting sentence is scored with a 3-gram lan-
guage model built with KenLM (Heafield et al., 2013)
on the correct side of the training data. The top one
sentence is then kept and considerd as the ?corrected?
one.
This module handles spelling errors of individual
words; it does not handle split/merge errors or word
reordering. The spelling checker sometimes attempts
to correct words that were already correct, because
the list does not contain named entities or translitera-
tions, and it does not contain all possible correct Arabic
words. Because the spelling checker module decreased
the overall performance, it is not included in our final
system.
3.2 Final System
Feature 14 Replacement
The first step in our final system is Feature 14 Replace-
ment, as described above.
Rule-based Clitic Correction
With the resulting data, we apply a set of rules to reat-
tach clitics that may have been split apart from the base
word. After examining the train dataset, we realized
that 95% of word merging cases involve ??? attach-
ment. When found by themselves, the clitics are at-
tached to either the previous word or next word, based
on whether they generally appear as prefixes or suf-
fixes. The clitics handled by this module are specified
in Table 2.
We also remove extra characters by replacing a se-
quence of 3 or more of the same character with a single
1
http://sourceforge.net/projects/
arabic-wordlist/
138
Dev
Exact Match No Punct
Precision Recall F1 Precision Recall F1
Feature 14 0.7746 0.3210 0.4539 0.8100 0.5190 0.6326
Feature 14 + Spelling checker (baseline) 0.4241 0.3458 0.3810 0.4057 0.4765 0.4382
Feature 14 + Clitic Rules 0.7884 0.3642 0.4983 0.8149 0.5894 0.6841
Feature 14 + Phrase-based MT 0.7296 0.5043 0.5964 0.7797 0.6397 0.7028
Feature 14 + Clitic Rules + Phrase-based MT 0.7571 0.5389 0.6296 0.8220 0.6850 0.7473
Test
Feature 14 + Clitic Rules + Phrase-based MT 0.7797 0.5635 0.6542 0.7438 0.6855 0.7135
Table 1: System results on the dev set (upper part) and on the test set (lower part).
Attach clitic to... Clitics
Beginning of next word {?, ?@, H
.
,
	
?, ?}
End of previous word {?, A?, A
	
K, ?


	
G, ?


, ??, @}
Table 2: Clitics handled by the rule-based module.
instance of that character (e.g. !!!!!!! would be replaced
with !).
Statistical Phrase-based Model
We use the Moses toolkit (Koehn et al., 2007) to
create a statistical phrase-based machine translation
model built on the best pre-processed data, as described
above. We treat this last step as a translation prob-
lem, where the source language is pre-processed in-
correct Arabic text, and the reference is correct Ara-
bic. Feature 14 extraction, rule-based correction, and
character de-duplication are applied to both the train
and dev sets. All but the last 1,000 sentences of the
train data are used at the training set for the phrase-
based model, the last 1,000 sentences of the train data
are used as a tuning set, and the dev set is used for
testing and evaluation. We use fast align, the aligner
included with the cdec decoder (Dyer et al., 2010) as
the word aligner with grow-diag as the symmetrization
heuristic (Och and Ney, 2003), and build a 5-gram lan-
guage model from the correct Arabic training data with
KenLM (Heafield et al., 2013). The system is evaluated
with BLEU (Papineni et al., 2002) and then scored for
precision, recall, and F1 measure against the dev set
reference.
We tested several different reordering window sizes
since this is not a standard translation task, so we may
want shorter distance reordering. Although 7 is the de-
fault size, we tested 7, 5, 4, 3, and 0, and found that a
window of size 4 produces the best result according to
BLEU score and F1 measure.
4 Experiments and Results
We train and evaluate our system with the train-
ing and development datasets provided for the shared
task and the m2Scorer (Dahlmeier and Ng, 2012b).
These datasets are extracted from the QALB corpus
of human-edited Arabic text produced by native speak-
ers, non-native speakers and machines (Zaghouani et
al., 2014).
We conducted a small scale statistical study on the
950K tokens training set used to build our system. We
realized that 306K tokens are affected by a correction
action which could be a word edit, insertion, deletion,
split or merge. 169K tokens were edited to correct the
spelling errors and 99K tokens were inserted (mostly
punctuation marks). Furthermore, there is a total of
6,7K non necessary tokens deleted and 10.6K attached
tokens split and 18.2 tokens merged. Finally, there are
only 427 tokens moved in the sentence and 1563 mul-
tiple correction action.
We experiment with different configurations and
reach the sweet spot of performance when combining
the different modules.
4.1 Results
To evaluate the performance of our system on the de-
velopment data, we compare its output to the reference
(gold annotation). We then compute the usual mea-
sures of precision, recall and f-measure. Results for
various system configurations on the dev and test sets
are given in Table 1. Using the baseline system con-
sisting in replacing words by their non diacritized form
(Feature 14), we could correct 51.9% of the errors oc-
curring in the dev set, when punctuation is not consid-
ered. This result drops when we consider the punctua-
tion errors which seem to be more complex to correct:
Only 32.1% of the errors are corrected in the dev set. It
is important to notice that adding the clitic rules to the
Feature 14 baseline yields an improvement of + 5.15 in
F-measure. We reach the best F-measure value when
using the phrase-based MT system after pre-processing
the data and applying the Feature 14 and clitic rules.
Using this combination we were able to correct 68.5%
of the errors (excluding punctuation) on the develop-
ment set with a precision of 82.2% and 74.38% on the
test set. When we consider the punctuation, 53.89%
of the errors of different types were corrected on the
dev set and 56.35% on the test set with a precision of
75.71% and 77.97%, respectively.
139
5 Error Analysis and Discussion
When building error correction systems, minimizing
the number of cases where correct words are marked
as incorrect is often regarded as more important than
covering a high number of errors. Therefore, a higher
precision is often preferred over higher recall. In order
to understand what was affecting the performance, we
took a closer look at our system output and translation
tables to present some samples of errors that our system
makes on development set.
5.1 Out-of-vocabulary Words
This category includes words that are not seen by our
system during the training which is a common problem
in machine translation systems. In our system, most of
out-of-vocabulary words were directly transferred un-
changed from source to target. For example the word

?J


??

????
	
? @ was not corrected to

?J


??

???
?
@.
5.2 Unnecessary Edits
In some cases, our system made some superfluous edits
such as adding the definite article in cases where it is
not required such as :
Source

?
	
JK


Y?
?
@
	
?AJ


?

@
Hypothesis

?
	
JK


Y?
?
@
	
?AJ


?

B@
Reference (unchanged)

?
	
JK


Y?
?
@
	
?AJ


?

@
Table 3: An example of an unnecessary addition of the
definite article.
5.3 Number Normalization
We observed that in some cases, the system did not nor-
malize the numbers such as in the following case which
requires some knowledge of the real context to under-
stand that these numbers require normalization.
Source

H@?A
	
?J


? 450000
Hypothesis

H@?A
	
?J


? 450000
Reference

H@?A
	
?J


? 450
Table 4: An example of number normalization.
5.4 Hamza Spelling
Even though our system corrected most of the Hamza
spelling errors, we noticed that in certain cases they
were not corrected, especially when the words without
the Hamza were valid entries in the dictionary. These
cases are not always easy to handle since only context
and semantic rules can handle them.
5.5 Grammatical Errors
In our error analysis we encountered many cases of un-
corrected grammatical errors. The most frequent type
Source

?J


	
J???@ X@?
Hypothesis

?J


	
J???@ X@?
Reference

?J


	
J???@ X

@?
Table 5: A sentence where the Hamza was not added
above the Alif in the first word because both versions
are valid dictionary entries.
is the case endings correction such as correcting the
verbs in jussive mode when there is a prohibition par-
ticle (negative imperative) like the (B) in the following
examples :
Source ??E


XAK



@
??
? @?K
.
Q?
	
?


B
Hypothesis ??E


XAK



@
??
? @?K
.
Q?
	
?


B
Reference ??E


XAK



@
??
?
	
??K
.
Q?
	
?


B
Table 6: An example of a grammatical error.
5.6 Unnecessary Word Deletion
According to the QALB annotation guidelines, ex-
tra words causing semantic ambiguity in the sentence
should be deleted. The decision to delete a given word
is usually based on the meaning and the understanding
of the human annotator, unfortunately this kind of er-
rors is very hard to process and our system was not able
to delete most of the unnecessary words.
Source Q
	
k

@ A
	
J



?? A??E


YK



@ A?
	
?? Y?D

?
	
J? ??
Hypothesis Q
	
k

@ A
	
J



?? A??E


YK



@ A?
	
?? Y?D

?
	
J? ??
Reference Q
	
k

@ A
	
J



?? A?
	
?? Y?D

?
	
J? ??
Table 7: An example of word deletion.
5.7 Adding Extra Words
Our analysis revealed cases of extra words introduced
to some sentences, despite the fact that the words added
are coherent with the context and could even improve
the overall readability of the sentence, they are uncred-
ited correction since they are not included in the gold
standard. For example :
Source ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Hypothesis Qm
?
'@ ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Reference ?


P???@

?


m
.
?
'@

???
?
?
H
.
Q?
	
?
Table 8: An example of the addition of extra words.
5.8 Merge and Split Errors
In this category, we show some sample errors of neces-
sary word splits and merge not done by our system. The
140
word Y?K
.
A???
	
k should have been split as Y?K
.
A???
	
k
and the word YK
.
B should have been merged to appear
as one word as in YK
.
B.
5.9 Dialectal Correction Errors
Dialectal words are usually converted to their Modern
Standard Arabic (MSA) equivalent in the QALB cor-
pus, since dialectal words are rare, our system is unable
to detect and translate the dialectal words to the MSA
as in the expression
	
?K


	
P I
.
? that is translated in the
gold standard to
	
?K


	
P
Q



	
?.
6 Conclusion
We presented our CMUQ system for automatic Ara-
bic text correction. Our system combines rule-based
linguistic techniques with statistical language model-
ing techniques and a phrase-based machine transla-
tion method. We experiment with different configu-
rations. Our experiments have shown that the system
we submitted outperforms the baseline and we reach
an F-score of 74.73% on the development set from
the QALB corpus when punctuation is excluded, and
65.42% on the test set when we consider the punctu-
ation errors . This placed us in the 3rd rank. We be-
lieve that our system could be improved in numerous
ways. In the future, we plan to finalize a current mod-
ule that we are developing to deal with merge and split
errors in a more specific way. We also want to focus in
a deeper way on the word movement as well as punc-
tuation problems, which can produce a more accurate
system. We will focus as well on learning further error
correction models from Arabic Wikipedia revision his-
tory, as it contains natural rewritings including spelling
corrections and other local text transformations.
Acknowledgements
This publication was made possible by grants NPRP-
09-1140-1-177 and NPRP-4-1058- 1-168 from the
Qatar National Research Fund (a member of the Qatar
Foundation). The statements made herein are solely the
responsibility of the authors.
References
Mohamed I. Alkanhal, Mohamed Al-Badrashiny, Man-
sour M. Alghamdi, and Abdulaziz O. Al-Qabbany.
2012. Automatic Stochastic Arabic Spelling Correc-
tion With Emphasis on Space Insertions and Dele-
tions. IEEE Transactions on Audio, Speech & Lan-
guage Processing, 20(7):2111?2122.
Mohammed Attia, Pavel Pecina, Younes Samih,
Khaled Shaalan, and Josef van Genabith. 2012. Im-
proved Spelling Error Detection and Correction for
Arabic. In Proceedings of COLING 2012: Posters,
pages 103?112, Mumbai, India.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A Beam-
Search Decoder for Grammatical Error Correction.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 568?578, Jeju Island, Korea.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Bet-
ter Evaluation for Grammatical Error Correction. In
NAACL HLT ?12 Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 568?572.
Robert Dale. 1997. Computer Assistance in Text Cre-
ation and Editing. In Survey of the state of the art
in Human Language Technology, chapter 7, pages
235?237. Cambridge University Press.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A Decoder, Alignment, and Learning Frame-
work for Finite-state and Context-free Translation
Models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7?12, Uppsala, Sweden.
A. R. Golding and D. Roth. 1999. A Winnow Based
Approach to Context-Sensitive Spelling Correction.
Machine Learning, 34(1-3):107?130.
Nizar Habash. 2008. Four Techniques for Online Han-
dling of Out-of-Vocabulary Words in Arabic-English
Statistical Machine Translation. In Proceedings of
ACL-08: HLT, Short Papers, pages 57?60, Colum-
bus, Ohio.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-words in Arabic: a Hybrid
Approach. International Journal of Computer Pro-
cessing of Oriental Languages, 20(04):237?257.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction using
Finite State Automata. In Proceedings of the Third
International Joint Conference on Natural Language
Processing (IJCNLP 2008), pages 913?918, Hyder-
abad, India.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modfied
Kneser-Ney Language Model Estimation. In In Pro-
ceedings of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
141
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Surveys
(CSUR), 24(4):377?439.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. In Computational Linguistics, page 1951.
Kemal Oflazer. 1996. Error-Tolerant Finite-State
Recognition with Applications to Morphological
Analysis and Spelling Correction. Computational
Linguistics, 22(1):73?89.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the Association for Computational Linguis-
tics, Philadelphia, Pennsylvania.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC?14), pages 1094?1101, Reykjavik, Iceland.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Proceedings of the 4th Conference on Lan-
guage Engineering, Egyptian Society of Language
Engineering (ELSE), Cairo, Egypt.
Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An
Approach for Analyzing and Correcting Spelling Er-
rors for Non-native Arabic Learners. In Proceedings
of The 7th International Conference on Informatics
and Systems, INFOS2010, the special track on Nat-
ural Language Processing and Knowledge Mining,
pages 28?30, Cairo, Egypt.
Khaled Shaalan, Mohammed Attia, Pavel Pecina,
Younes Samih, and Josef van Genabith. 2012.
Arabic Word Generation and Modelling for Spell
Checking. In Proceedings of the Eighth Inter-
national Conference on Language Resources and
Evaluation (LREC-2012), pages 719?725, Istanbul,
Turkey.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines
and Framework. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC?14), Reykjavik, Iceland.
Chiraz Zribi and Mohammed Ben Ahmed. 2003. Ef-
ficient Automatic Correction of Misspelled Arabic
Words Based on Contextual Information. In Pro-
ceedings of the Knowledge-Based Intelligent Infor-
mation and Engineering Systems Conference, pages
770?777, Oxford, UK.
142
