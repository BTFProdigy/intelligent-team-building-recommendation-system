Proceedings of the 12th Conference of the European Chapter of the ACL, pages 24?32,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Contextual Phrase-Level Polarity Analysis using Lexical Affect Scoring
and Syntactic N-grams
Apoorv Agarwal
Department of Computer Science
Columbia University
New York, USA
aa2644@columbia.edu
Fadi Biadsy
Department of Computer Science
Columbia University
New York, USA
fadi@cs.columbia.edu
Kathleen R. Mckeown
Department of Computer Science
Columbia University
New York, USA
kathy@cs.columbia.edu
Abstract
We present a classifier to predict con-
textual polarity of subjective phrases in
a sentence. Our approach features lexi-
cal scoring derived from the Dictionary of
Affect in Language (DAL) and extended
through WordNet, allowing us to automat-
ically score the vast majority of words in
our input avoiding the need for manual la-
beling. We augment lexical scoring with
n-gram analysis to capture the effect of
context. We combine DAL scores with
syntactic constituents and then extract n-
grams of constituents from all sentences.
We also use the polarity of all syntactic
constituents within the sentence as fea-
tures. Our results show significant im-
provement over a majority class baseline
as well as a more difficult baseline consist-
ing of lexical n-grams.
1 Introduction
Sentiment analysis is a much-researched area that
deals with identification of positive, negative and
neutral opinions in text. The task has evolved from
document level analysis to sentence and phrasal
level analysis. Whereas the former is suitable for
classifying news (e.g., editorials vs. reports) into
positive and negative, the latter is essential for
question-answering and recommendation systems.
A recommendation system, for example, must be
able to recommend restaurants (or movies, books,
etc.) based on a variety of features such as food,
service or ambience. Any single review sentence
may contain both positive and negative opinions,
evaluating different features of a restaurant. Con-
sider the following sentence (1) where the writer
expresses opposing sentiments towards food and
service of a restaurant. In tasks such as this, there-
fore, it is important that sentiment analysis be done
at the phrase level.
(1) The Taj has great food but I found their ser-
vice to be lacking.
Subjective phrases in a sentence are carriers of
sentiments in which an experiencer expresses an
attitude, often towards a target. These subjective
phrases may express neutral or polar attitudes de-
pending on the context of the sentence in which
they appear. Context is mainly determined by con-
tent and structure of the sentence. For example, in
the following sentence (2), the underlined subjec-
tive phrase seems to be negative, but in the larger
context of the sentence, it is positive.1
(2) The robber entered the store but his efforts
were crushed when the police arrived on time.
Our task is to predict contextual polarity of sub-
jective phrases in a sentence. A traditional ap-
proach to this problem is to use a prior polarity
lexicon of words to first set priors on target phrases
and then make use of the syntactic and semantic
information in and around the sentence to make
the final prediction. As in earlier approaches, we
also use a lexicon to set priors, but we explore
new uses of a Dictionary of Affect in Language
(DAL) (Whissel, 1989) extended using WordNet
(Fellbaum, 1998). We augment this approach with
n-gram analysis to capture the effect of context.
We present a system for classification of neutral
versus positive versus negative and positive versus
negative polarity (as is also done by (Wilson et al,
2005)). Our approach is novel in the use of fol-
lowing features:
? Lexical scores derived from DAL and ex-
tended through WordNet: The Dictionary
of Affect has been widely used to aid in in-
terpretation of emotion in speech (Hirschberg
1We assign polarity to phrases based on Wiebe (Wiebe et
al., 2005); the polarity of all examples shown here is drawn
from annnotations in the MPQA corpus. Clearly the assign-
ment of polarity chosen in this corpus depends on general
cultural norms.
24
et al, 2005). It contains numeric scores as-
signed along axes of pleasantness, activeness
and concreteness. We introduce a method for
setting numerical priors on words using these
three axes, which we refer to as a ?scoring
scheme? throughout the paper. This scheme
has high coverage of the phrases for classi-
fication and requires no manual intervention
when tagging words with prior polarities.
? N-gram Analysis: exploiting automatically
derived polarity of syntactic constituents
We compute polarity for each syntactic con-
stituent in the input phrase using lexical af-
fect scores for its words and extract n-grams
over these constituents. N-grams of syntactic
constituents tagged with polarity provide pat-
terns that improve prediction of polarity for
the subjective phrase.
? Polarity of Surrounding Constituents: We
use the computed polarity of syntactic con-
stituents surrounding the phrase we want to
classify. These features help to capture the
effect of context on the polarity of the sub-
jective phrase.
We show that classification of subjective
phrases using our approach yields better accuracy
than two baselines, a majority class baseline and a
more difficult baseline of lexical n-gram features.
We also provide an analysis of how the differ-
ent component DAL scores contribute to our re-
sults through the introduction of a ?norm? that
combines the component scores, separating polar
words that are less subjective (e.g., Christmas ,
murder) from neutral words that are more subjec-
tive (e.g., most, lack).
Section 2 presents an overview of previous
work, focusing on phrasal level sentiment analy-
sis. Section 3 describes the corpus and the gold
standard we used for our experiments. In sec-
tion 4, we give a brief description of DAL, dis-
cussing its utility and previous uses for emotion
and for sentiment analysis. Section 5 presents, in
detail, our polarity classification framework. Here
we describe our scoring scheme and the features
we extract from sentences for classification tasks.
Experimental set-up and results are presented in
Section 6. We conclude with Section 7 where we
also look at future directions for this research.
2 Literature Survey
The task of sentiment analysis has evolved from
document level analysis (e.g., (Turney., 2002);
(Pang and Lee, 2004)) to sentence level analy-
sis (e.g., (Hu and Liu., 2004); (Kim and Hovy.,
2004); (Yu and Hatzivassiloglou, 2003)). These
researchers first set priors on words using a prior
polarity lexicon. When classifying sentiment at
the sentence level, other types of clues are also
used, including averaging of word polarities or
models for learning sentence sentiment.
Research on contextual phrasal level sentiment
analysis was pioneered by Nasukawa and Yi
(2003), who used manually developed patterns to
identify sentiment. Their approach had high preci-
sion, but low recall. Wilson et al, (2005) also ex-
plore contextual phrasal level sentiment analysis,
using a machine learning approach that is closer to
the one we present. Both of these researchers also
follow the traditional approach and first set priors
on words using a prior polarity lexicon. Wilson
et al (2005) use a lexicon of over 8000 subjec-
tivity clues, gathered from three sources ((Riloff
and Wiebe, 2003); (Hatzivassiloglou and McKe-
own, 1997) and The General Inquirer2). Words
that were not tagged as positive or negative were
manually labeled. Yi et al (2003) acquired words
from GI, DAL and WordNet. From DAL, only
words whose pleasantness score is one standard
deviation away from the mean were used. Na-
sukawa as well as other researchers (Kamps and
Marx, 2002)) also manually tag words with prior
polarities. All of these researchers use categorical
tags for prior lexical polarity; in contrast, we use
quantitative scores, making it possible to use them
in computation of scores for the full phrase.
While Wilson et al (2005) aim at phrasal level
analysis, their system actually only gives ?each
clue instance its own label? [p. 350]. Their gold
standard is also at the clue level and assigns a
value based on the clue?s appearance in different
expressions (e.g., if a clue appears in a mixture of
negative and neutral expressions, its class is neg-
ative). They note that they do not determine sub-
jective expression boundaries and for this reason,
they classify at the word level. This approach is
quite different from ours, as we compute the po-
larity of the full phrase. The average length of
the subjective phrases in the corpus was 2.7 words,
with a standard deviation of 2.3. Like Wilson et al
2http://www.wjh.harvard.edu/ inquirer
25
(2005) we do not attempt to determine the bound-
ary of subjective expressions; we use the labeled
boundaries in the corpus.
3 Corpus
We used the Multi-Perspective Question-
Answering (MPQA version 1.2) Opinion corpus
(Wiebe et al, 2005) for our experiments. We
extracted a total of 17,243 subjective phrases
annotated for contextual polarity from the corpus
of 535 documents (11,114 sentences). These
subjective phrases are either ?direct subjective?
or ?expressive subjective?. ?Direct subjective?
expressions are explicit mentions of a private state
(Quirk et al, 1985) and are much easier to clas-
sify. ?Expressive subjective? phrases are indirect
or implicit mentions of private states and therefore
are harder to classify. Approximately one third of
the phrases we extracted were direct subjective
with non-neutral expressive intensity whereas the
rest of the phrases were expressive subjective. In
terms of polarity, there were 2779 positive, 6471
negative and 7993 neutral expressions. Our Gold
Standard is the manual annotation tag given to
phrases in the corpus.
4 DAL
DAL is an English language dictionary built to
measure emotional meaning of texts. The samples
employed to build the dictionary were gathered
from different sources such as interviews, adoles-
cents? descriptions of their emotions and univer-
sity students? essays. Thus, the 8742 word dictio-
nary is broad and avoids bias from any one par-
ticular source. Each word is given three kinds of
scores (pleasantness ? also called evaluation, ee,
activeness, aa and imagery, ii) on a scale of 1 (low)
to 3 (high). Pleasantness is a measure of polarity.
For example, in Table 1, affection is given a pleas-
antness score of 2.77 which is closer to 3.0 and
is thus a highly positive word. Likewise, active-
ness is a measure of the activation or arousal level
of a word, which is apparent from the activeness
scores of slug and energetic in the table. The third
score, imagery, is a measure of the ease with which
a word forms a mental picture. For example, af-
fect cannot be imagined easily and therefore has a
score closer to 1, as opposed to flower which is a
very concrete and therefore has an imagery score
of 3.
A notable feature of the dictionary is that it has
different scores for various inflectional forms of a
word ( affect and affection) and thus, morphologi-
cal parsing, and the possibility of resulting errors,
is avoided. Moreover, Cowie et al, (2001) showed
that the three scores are uncorrelated; this implies
that each of the three scores provide complemen-
tary information.
Word ee aa ii
Affect 1.75 1.85 1.60
Affection 2.77 2.25 2.00
Slug 1.00 1.18 2.40
Energetic 2.25 3.00 3.00
Flower 2.75 1.07 3.00
Table 1: DAL scores for words
The dictionary has previously been used for de-
tecting deceptive speech (Hirschberg et al, 2005)
and recognizing emotion in speech (Athanaselis et
al., 2006).
5 The Polarity Classification Framework
In this section, we present our polarity classifi-
cation framework. The system takes a sentence
marked with a subjective phrase and identifies the
most likely contextual polarity of this phrase. We
use a logistic regression classifier, implemented
in Weka, to perform two types of classification:
Three way (positive, negative, vs. neutral) and
binary (positive vs. negative). The features we
use for classification can be broadly divided into
three categories: I. Prior polarity features com-
puted from DAL and augmented using WordNet
(Section 5.1). II. lexical features including POS
and word n-gram features (Section 5.3), and III.
the combination of DAL scores and syntactic fea-
tures to allow both n-gram analysis and polarity
features of neighbors (Section 5.4).
5.1 Scoring based on DAL and WordNet
DAL is used to assign three prior polarity scores
to each word in a sentence. If a word is found in
DAL, scores of pleasantness (ee), activeness (aa),
and imagery (ii) are assigned to it. Otherwise, a
list of the word?s synonyms and antonyms is cre-
ated using WordNet. This list is sequentially tra-
versed until a match is found in DAL or the list
ends, in which case no scores are assigned. For
example, astounded, a word absent in DAL, was
scored by using its synonym amazed. Similarly,
in-humane was scored using the reverse polarity of
26
its antonym humane, present in DAL. These scores
are Z-Normalized using the mean and standard de-
viation measures given in the dictionary?s manual
(Whissel, 1989). It should be noted that in our cur-
rent implementation all function words are given
zero scores since they typically do not demonstrate
any polarity. The next step is to boost these nor-
malized scores depending on how far they lie from
the mean. The reason for doing this is to be able
to differentiate between phrases like ?fairly decent
advice? and ?excellent advice?. Without boosting,
the pleasantness scores of both phrases are almost
the same. To boost the score, we multiply it by
the number of standard deviations it lies from the
mean.
After the assignment of scores to individual
words, we handle local negations in a sentence by
using a simple finite state machine with two states:
RETAIN and INVERT. In the INVERT state, the
sign of the pleasantness score of the current word
is inverted, while in the RETAIN state the sign of
the score stays the same. Initially, the first word in
a given sentence is fed to the RETAIN state. When
a negation (e.g., not, no, never, cannot, didn?t)
is encountered, the state changes to the INVERT
state. While in the INVERT state, if ?but? is en-
countered, it switches back to the RETAIN state.
In this machine we also take care of ?not only?
which serves as an intensifier rather than nega-
tion (Wilson et al, 2005). To handle phrases like
?no better than evil? and ?could not be clearer?,
we also switch states from INVERT to RETAIN
when a comparative degree adjective is found after
?not?. For example, the words in phrase in Table
(2) are given positive pleasantness scores labeled
with positive prior polarity.
Phrase has no greater desire
POS VBZ DT JJR NN
(ee) 0 0 3.37 0.68
State RETAIN INVERT RETAIN RETAIN
Table 2: Example of scoring scheme using DAL
We observed that roughly 74% of the content
words in the corpus were directly found in DAL.
Synonyms of around 22% of the words in the cor-
pus were found to exist in DAL. Antonyms of
only 1% of the words in the corpus were found in
DAL. Our system failed to find prior semantic ori-
entations of roughly 3% of the total words in the
corpus. These were rarely occurring words like
apartheid, apocalyptic and ulterior. We assigned
zero scores for these words.
In our system, we assign three DAL scores, us-
ing the above scheme, for the subjective phrase
in a given sentence. The features are (1) ?ee, the
mean of the pleasantness scores of the words in the
phrase, (2) ?aa, the mean of the activeness scores
of the words in the phrase, and similarly (3) ?ii,
the mean of the imagery scores.
5.2 Norm
We gave each phrase another score, which we call
the norm, that is a combination of the three scores
from DAL. Cowie et al (2001) suggest a mecha-
nism of mapping emotional states to a 2-D contin-
uous space using an Activation-Evaluation space
(AE) representation. This representation makes
use of the pleasantness and activeness scores from
DAL and divides the space into four quadrants:
?delightful?, ?angry?, ?serene?, and ?depressed?.
Whissel (2008), observes that tragedies, which
are easily imaginable in general, have higher im-
agery scores than comedies. Drawing on these ap-
proaches and our intuition that neutral expressions
tend to be more subjective, we define the norm in
the following equation (1).
norm =
?
ee2 + aa2
ii
(1)
Words of interest to us may fall into the follow-
ing four broad categories:
1. High AE score and high imagery: These
are words that are highly polar and less sub-
jective (e.g., angel and lively).
2. Low AE score and low imagery: These are
highly subjective neutral words (e.g., gener-
ally and ordinary).
3. High AE score and low imagery: These are
words that are both highly polar and subjec-
tive (e.g., succeed and good).
4. Low AE score and high imagery: These are
words that are neutral and easily imaginable
(e.g., car and door).
It is important to differentiate between these
categories of words, because highly subjective
words may change orientation depending on con-
text; less subjective words tend to retain their prior
orientation. For instance, in the example sentence
from Wilson et al(2005)., the underlined phrase
27
seems negative, but in the context it is positive.
Since a subjective word like succeed depends on
?what? one succeeds in, it may change its polar-
ity accordingly. In contrast, less subjective words,
like angel, do not depend on the context in which
they are used; they evoke the same connotation as
their prior polarity.
(3) They haven?t succeeded and will never succeed
in breaking the will of this valiant people.
As another example, AE space scores of good-
ies and good turn out to be the same. What differ-
entiates one from the another is the imagery score,
which is higher for the former. Therefore, value of
the norm is lower for goodies than for good. Un-
surprisingly, this feature always appears in the top
10 features when the classification task contains
neutral expressions as one of the classes.
5.3 Lexical Features
We extract two types of lexical features, part of
speech (POS) tags and n-gram word features. We
count the number of occurrences of each POS in
the subjective phrase and represent each POS as
an integer in our feature vector.3 For each subjec-
tive phrase, we also extract a subset of unigram,
bigrams, and trigrams of words (selected automat-
ically, see Section 6). We represent each n-gram
feature as a binary feature. These types of features
were used to approximate standard n-gram lan-
guage modeling (LM). In fact, we did experiment
with a standard trigram LM, but found that it did
not improve performance. In particular, we trained
two LMs, one on the polar subjective phrases and
another on the neutral subjective phrases. Given a
sentence, we computed two perplexities of the two
LMs on the subjective phrase in the sentence and
added them as features in our feature vectors. This
procedure provided us with significant improve-
ment over a chance baseline but did not outper-
form our current system. We speculate that this
was caused by the split of training data into two
parts, one for training the LMs and another for
training the classifier. The resulting small quantity
of training data may be the reason for bad perfor-
mance. Therefore, we decided to back off to only
binary n-gram features as part of our feature vec-
tor.
3We use the Stanford Tagger to assign parts of speech tags
to sentences. (Toutanova and Manning, 2000)
5.4 Syntactic Features
In this section, we show how we can combine the
DAL scores with syntactic constituents. This pro-
cess involves two steps. First, we chunk each
sentence to its syntactic constituents (NP, VP,
PP, JJP, and Other) using a CRF Chunker.4 If
the marked-up subjective phrase does not contain
complete chunks (i.e., it partially overlaps with
other chunks), we expand the subjective phrase to
include the chunks that it overlaps with. We term
this expanded phrase as the target phrase, see Fig-
ure 1.
Second, each chunk in a sentence is then as-
signed a 2-D AE space score as defined by Cowie
et al, (2001) by adding the individual AE space
scores of all the words in the chunk and then nor-
malizing it by the number of words. At this point,
we are only concerned with the polarity of the
chunk (i.e., whether it is positive or negative or
neutral) and imagery will not help in this task; the
AE space score is determined from pleasantness
and activeness alone. A threshold, determined
empirically by analyzing the distributions of posi-
tive (pos), negative (neg) and neutral (neu) expres-
sions, is used to define ranges for these classes of
expressions. This enables us to assign each chunk
a prior semantic polarity. Having the semantic ori-
entation (positive, negative, neutral) and phrasal
tags, the sentence is then converted to a sequence
of encodings [Phrasal ? Tag]polarity. We mark
each phrase that we want to classify as a ?target? to
differentiate it from the other chunks and attach its
encoding. As mentioned, if the target phrase par-
tially overlaps with chunks, it is simply expanded
to subsume the chunks. This encoding is illus-
trated in Figure 1.
After these two steps, we extract a set of fea-
tures that are used in classifying the target phrase.
These include n-grams of chunks from the all
sentences, minimum and maximum pleasantness
scores from the chunks in the target phrase itself,
and the syntactic categories that occur in the con-
text of the target phrase. In the remainder of this
section, we describe how these features are ex-
tracted.
We extract unigrams, bigrams and trigrams of
chunks from all the sentences. For example, we
may extract a bigram from Figure 1 of [V P ]neu
followed by [PP ]targetneg . Similar to the lexical
4Xuan-Hieu Phan, ?CRFChunker: CRF English Phrase
Chunker?, http://crfchunker.sourceforge.net/, 2006.
28
?? ??????? ?? ?? ?? ?? ??
?????????
????????????????????? ?????????????????????
? ? ? ? ? ?
Figure 1: Converting a sentence with a subjective phrase to a sequence of chunks with their types and polarities
n-grams, for the sentence containing the target
phrase, we add binary values in our feature vec-
tor such that the value is 1 if the sentence contains
that chunk n-gram.
We also include two features related to the tar-
get phrase. The target phrase often consists of
many chunks. To detect if a chunk of the target
phrase is highly polar, minimum and maximum
pleasantness scores over all the chunks in the tar-
get phrase are noted.
In addition, we add features which attempt to
capture contextual information using the prior se-
mantic polarity assigned to each chunk both within
the target phrase itself and within the context of the
target phrase. In cases where the target phrase is
in the beginning of the sentence or at the end, we
simply assign zero scores. Then we compute the
frequency of each syntactic type (i.e., NP, VP, PP,
JJP) and polarity (i.e., positive, negative, neutral)
to the left of the target, to the right of the target
and for the target. This additional set of contextual
features yields 36 features in total: three polari-
ties: {positive, negative, neutral} * three contexts:
{left, target, right} * four chunk syntactic types:
{NP, VP, PP, JJP}.
The full set of features captures different types
of information. N-grams look for certain patterns
that may be specific to either polar or neutral senti-
ments. Minimum and maximum scores capture in-
formation about the target phrase standalone. The
last set of features incorporate information about
the neighbors of the target phrase. We performed
feature selection on this full set of n-gram related
features and thus, a small subset of these n-gram
related features, selected automatically (see sec-
tion 6) were used in the experiments.
6 Experiments and Results
Subjective phrases from the MPQA corpus were
used in 10-fold cross-validation experiments. The
MPQA corpus includes gold standard tags for each
Feature Types Accuracy Pos.* Neg.* Neu.*
Chance baseline 33.33% - - -
N-gram baseline 59.05% 0.602 0.578 0.592
DAL scores only 59.66% 0.635 0.635 0.539
+ POS 60.55% 0.621 0.542 0.655
+ Chunks 64.72% 0.681 0.665 0.596
+ N-gram (all) 67.51% 0.703 0.688 0.632
All (unbalanced) 70.76% 0.582 0.716 0.739
Table 3: Results of 3 way classification (Positive, Negative,
and Neutral). In the unbalanced case, majority class baseline
is 46.3% (*F-Measure).
Feature Types Accuracy Pos.* Neg.*
Chance baseline 50% - -
N-gram baseline 73.21% 0.736 0.728
DAL scores only 77.02% 0.763 0.728
+ POS 79.02% 0.788 0.792
+ Chunks 80.72% 0.807 0.807
+ N-gram (all) 82.32% 0.802 0.823
All (unbalanced) 84.08% 0.716 0.889
Table 4: Positive vs. Negative classification results. Baseline
is the majority class. In the unbalanced case, majority class
baseline is 69.74%. (* F-Measure)
phrase. A logistic classifier was used for two po-
larity classification tasks, positive versus negative
versus neutral and positive versus negative. We
report accuracy, and F-measure for both balanced
and unbalanced data.
6.1 Positive versus Negative versus Neutral
Table 3 shows results for a 3-way classifier. For
the balanced data-set, each class has 2799 in-
stances and hence the chance baseline is 33%. For
the unbalanced data-set, there are 2799 instances
of positive, 6471 instances of negative and 7993
instances of neutral phrases and thus the baseline
is about 46%. Results show that the accuracy in-
creases as more features are added. It may be
seen from the table that prior polarity scores do
not do well alone, but when used in conjunction
with other features they play an important role
in achieving an accuracy much higher than both
baselines (chance and lexical n-grams). To re-
29
Figure 2: (a) An example sentence with three annotated subjective phrases in the same sentence. (b) Part of the sentence with
the target phrase (B) and their chunks with prior polarities.
confirm if prior polarity scores add value, we ex-
perimented by using all features except the prior
polarity scores and noticed a drop in accuracy by
about 4%. This was found to be true for the
other classification task as well. The table shows
that parts of speech and lexical n-grams are good
features. A significant improvement in accuracy
(over 4%, p-value = 4.2e-15) is observed when
chunk features (i.e., n-grams of constituents and
polarity of neighboring constituents) are used in
conjunction with prior polarity scores and part of
speech features.5 This improvement may be ex-
plained by the following observation. The bi-
gram ?[Other]targetneu [NP ]neu? was selected as a
top feature by the Chi-square feature selector. So
were unigrams, [Other]targetneu and [Other]
target
neg .
We thus learned n-gram patterns that are char-
acteristic of neutral expressions (the just men-
tioned bigram and the first of the unigrams) as
well as a pattern found mostly in negative ex-
pressions (the latter unigram). It was surpris-
ing to find another top chunk feature, the bigram
?[Other]targetneu [NP ]neg? (i.e., a neutral chunk of
syntactic type ?Other? preceding a negative noun
phrase), present in neutral expressions six times
more than in polar expressions. An instance where
these chunk features could have been responsi-
ble for the correct prediction of a target phrase is
shown in Figure 2. Figure 2(a) shows an exam-
ple sentence from the MPQA corpus, which has
three annotated subjective phrases. The manually
labeled polarity of phrases (A) and (C) is negative
and that of (B) is neutral. Figure 2(b) shows the
5We use the binomial test procedure to test statistical sig-
nificance throughout the paper.
relevant chunk bigram which is used to predict the
contextual polarity of the target phrase (B).
It was interesting to see that the top 10 features
consisted of all categories (i.e., prior DAL scores,
lexical n-grams and POS, and syntactic) of fea-
tures. In this and the other experiment, pleasant-
ness, activation and the norm were among the top
5 features. We ran a significance test to show the
importance of the norm feature in our classifica-
tion task and observed that it exerted a significant
increase in accuracy (2.26%, p-value = 1.45e-5).
6.2 Positive versus Negative
Table 4 shows results for positive versus negative
classification. We show results for both balanced
and unbalanced data-sets. For balanced, there are
2779 instances of each class. For the unbalanced
data-set, there are 2779 instances of positive and
6471 instances of neutral, thus our chance base-
line is around 70%. As in the earlier classification,
accuracy and F-measure increase as we add fea-
tures. While the increase of adding the chunk fea-
tures, for example, is not as great as in the previous
classification, it is nonetheless significant (p-value
= 0.0018) in this classification task. The smaller
increase lends support to our hypothesis that po-
lar expressions tend to be less subjective and thus
are less likely to be affected by contextual polar-
ity. Another thing that supports our hypothesis that
neutral expressions are more subjective is the fact
that the rank of imagery (ii), dropped significantly
in this classification task as compared to the previ-
ous classification task. This implies that imagery
has a much lesser role to play when we are dealing
with non-neutral expressions.
30
7 Conclusion and Future Work
We present new features (DAL scores, norm
scores computed using DAL, n-gram over chunks
with polarity) for phrasal level sentiment analysis.
They work well and help in achieving high accu-
racy in a three-way classification of positive, neg-
ative and neutral expressions. We do not require
any manual intervention during feature selection,
and thus our system is fully automated. We also
introduced a 3-D representation that maps differ-
ent classes to spatial coordinates.
It may seem to be a limitation of our system that
it requires accurate expression boundaries. How-
ever, this is not true for the following two reasons:
first, Wiebe et al, (2005) declare that while mark-
ing the span of subjective expressions and hand
annotating the MPQA corpus, the annotators were
not trained to mark accurate expression bound-
aries. The only constraint was that the subjective
expression should be within the mark-ups for all
annotators. Second, we expanded the marked sub-
jective phrase to subsume neighboring phrases at
the time of chunking.
A limitation of our scoring scheme is that it
does not handle polysemy, since words in DAL
are not provided with their parts of speech. Statis-
tics show, however, that most words occurred with
primarily one part of speech only. For example,
?will? occurred as modal 1272 times in the corpus,
whereas it appeared 34 times as a noun. The case
is similar for ?like? and ?just?, which mostly occur
as a preposition and an adverb, respectively. Also,
in our state machine, we haven?t accounted for the
impact of connectives such as ?but? or ?although?;
we propose drawing on work in argumentative ori-
entation to do so ((Anscombre and Ducrot, 1983);
(Elhadad and McKeown, 1990)).
For future work, it would be interesting to do
subjectivity and intensity classification using the
same scheme and features. Particularly, for the
task of subjectivity analysis, we speculate that the
imagery score might be useful for tagging chunks
with ?subjective? and ?objective? instead of posi-
tive, negative, and neutral.
Acknowledgments
This work was supported by the National Science
Foundation under the KDD program. Any opin-
ions, ndings, and conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reect the views of the National
Science Foundation. score.
We would like to thank Julia Hirschberg for use-
ful discussion. We would also like to acknowledge
Narayanan Venkiteswaran for implementing parts
of the system and Amal El Masri, Ashleigh White
and Oliver Elliot for their useful comments.
References
J.C. Anscombre and O. Ducrot. 1983. Philosophie et
langage. l?argumentation clans la langue. Bruxelles:
Pierre Mardaga.
T. Athanaselis, S. Bakamidis, , and L. Dologlou. 2006.
Automatic recognition of emotionally coloured
speech. In Proceedings of World Academy of Sci-
ence, Engineering and Technology, volume 12, ISSN
1307-6884.
R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Vot-
sis, S. Kollias, and W. Fellenz et al 2001. Emo-
tion recognition in human-computer interaction. In
IEEE Signal Processing Magazine, 1, 32-80.
M. Elhadad and K. R. McKeown. 1990. Generating
connectives. In Proceedings of the 13th conference
on Computational linguistics, pages 97?101, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. In MIT press.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of ACL.
J. Hirschberg, S. Benus, J.M. Brenier, F. Enos, and
S. Friedman. 2005. Distinguishing deceptive from
non-deceptive speech. In Proceedings of Inter-
speech, 1833-1836.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of KDD.
J. Kamps and M. Marx. 2002. Words with attitude. In
1st International WordNet Conference.
S. M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In In Coling.
T. Nasukawa and J. Yi. 2003. Sentiment analysis:
Capturing favorability using natural language pro-
cessing. In Proceedings of K-CAP.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of ACL.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A comprehensive grammar of the english lan-
guage. Longman, New York.
31
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
EMNLP.
K. Toutanova and C. D. Manning. 2000. Enriching
the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pp. 63-70.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of ACL.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors,
Emotion: theory research and experience, volume 4,
Acad. Press., London.
C. M. Whissell. 2008. A psychological investiga-
tion of the use of shakespeare=s emotional language:
The case of his roman tragedies. In Edwin Mellen
Press., Lewiston, NY.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
In Language Resources and Evaluation, volume 39,
issue 2-3, pp. 165-210.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recog-
nizing contextual polarity in phrase level sentiment
analysis. In Proceedings of ACL.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In Proceedings of IEEE ICDM.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP.
32
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 1?5, Dublin, Ireland, August 23-29 2014.
An Error Analysis Tool for Natural Language Processing and Applied
Machine Learning
Apoorv Agarwal
Department of Computer Science
Columbia University
New York, NY, USA
apoorv@cs.columbia.edu
Ankit Agarwal
NextGen Invent Corp.
Shrewsbury, MA, USA
ankit.agarwal@ngicorporation.com
Deepak Mittal
NextGen Invent Corp.
Shrewsbury, MA, USA
deepak.mittal@ngicorporation.com
Abstract
In this paper we present a simple to use web based error analysis tool to help computational
linguists, researchers building language applications, and non-technical personnel managing de-
velopment of language tools to analyze the predictions made by their machine learning models.
The only expectation is that the users of the tool convert their data into an intuitive XML format.
Once the XML is ready, several error analysis functionalities that promote principled feature
engineering are a click away.
1 Introduction
A typical machine learning (ML) pipeline involves conversion of examples into a structured represen-
tations, followed by training a model on these examples, followed by testing the model. Most Natural
Language Processing (NLP) tasks involve (broadly) two types of structured representations: feature vec-
tors (in which examples are represented as vectors in R
n
) and abstract representations such as strings and
trees. Classification models assign each test example an integer, corresponding to the predicted class.
Regression models assign each test example a real number. Throughout this process, frequently asked
questions include: is there a bug in the code that converts text into a feature vector or structured repre-
sentation, which models (trained using different learning algorithms) make the right prediction on what
kind of examples? Which models trained on which set of features/structures make the right prediction on
what kind of examples? Is a pair of models statistically significantly different? Answers to these ques-
tions give us a deeper understanding of the learning process, which in turn results in principled feature
engineering and model selection.
We spend a lot of time writing quick and dirty scripts to make connections between different aspects
of the learning process (features, structures, predictions, models). These scripts are often task dependent
and need to be re-written for each task. More time is spent in compiling a report so our findings may be
shared with other collaborators. Frustrated with this day-to-day and repetitive script writing, we decided
to design and implement an easy-to-use error analysis tool that helps in answering the aforementioned
questions in a few clicks. The following are the two main contributions of this work:
1. Design: the tool provides a common framework for performing error analysis of a wide range of
NLP tasks. In designing and implementing this tool, we had to abstract away from specific task
definitions, feature representations, and structure representations in order to bring different aspects
of a task into a unified interface.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
1
2. Web based: the tool is web-based, meaning that users can access a URL and upload files to access
the functionalities of the tool. Each user can optionally generate an identifier, which can be shared
with other users, enabling other users to access the same error analysis session.
2 The Tool
In the following sub-sections, we give details of the basic functionalities of the tool, followed by details
of a few advanced functionalities. We explicate the functionalities using the ACE relation extraction task
as a running example.
Figure 1: A screenshot of the tool loaded with ACE relation extraction task data.
Figure 2: Screenshots of pop-ups on sample ACE data. The first pop-up shows the gold class distribution.
The second pop-up shows evaluation metrics for one of the models.
2.1 ACE Relation Extraction
ACE (Automatic Content Extraction) relation extraction is a popular and well-established NLP task
(Doddington et al., 2004). Given a sentence, and two entity mentions (usually referred to as target
2
entities), the goal is to detect a relation between the two entities (relation detection), and if a relation
exists, to identify the type of the relation (relation classification). There is a pre-defined list of relations
for the ACE relation extraction task: ART, DISC, EMP-ORG, GPE-AFF, PER-SOC, PHYS, OTHER.
Researchers have developed a wide range of features for the task (Kambhatla, 2004; Zhao and Grishman,
2005; Zhang et al., 2006). There is also a large body of work that has explored the space of tree structure
representations (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhou et al., 2007; Nguyen et al., 2009;
Agarwal and Rambow, 2010; Agarwal et al., 2014). Relation extraction is a complex task, with features
ranging from simple bag-of-words to more complex semantic features, and with tree structures ranging
from simple parse trees to more complicated tree structures.
2.2 Basic Functionalities
Figure 1 presents a screenshot of the tool loaded with the ACE relation extraction data. The column
labeled EID is the example identifier assigned to each example, the column labeled Text has the example
in text format, the column labeled Gold has the gold class of each example (may be a real number for
regression type tasks), the columns labeled FEATUREVECTORMODEL and SYNTREEMODEL have
predictions of respective models. The column labeled FEATUREVECTORS has the different types of
feature vector representations, and the column labeled TREES has the different types of tree structure
representations for each example. Note that the number of columns and their names are not hard-coded.
The number of columns, their names and their order is automatically inferred from the XML input
specified by the user (section 2.4).
The column labeled Text in Figure 1 shows the sentence with the target entities highlighted. The
relations may be directed from one target entity to the other. The numbers next to the highlighted entities
specify the direction of the relation (from entity marked as [1] to entity marked as [2]). Highlighting
certain parts of input text with a tag (in this case target identifiers) is a general feature of the tool. There
are other popular NLP tasks in which this functionality may come in handy. For example, for parts-of-
speech tagging and named entity recognition tasks, features are extracted with respect to part of a text,
which may be highlighted to understand the input example.
The column labeled Gold in Figure 1 shows the gold label for each example. Clicking on the column
label pops up a window with a histogram that shows the distribution of the gold class (Figure 2).
The column labeled FEATUREVECTORMODEL shows the predictions made by a model that was
trained only on feature vectors. The column labeled SYNTREEMODEL shows the predictions made by
a model that was trained only on syntactic tree structures. There is an icon next to the column name,
marked as R. Clicking on this icon pops up a window with a result table that summarizes the performance
of that model (Figure 2). Built-in metrics include precision, recall, and F1-measure (with respect to each
class), alongside the macro- and micro-F1 measures and percentage accuracy. This list of metrics may
be easily extended in the code.
The way in which a user specifies the predictions per model is simple ? the user is required to create
a two column file (EID, prediction) and load the file into the tool using the ?Browse? and ?Upload File?
buttons (upper left corner of Figure 1). The tool uses the EID to assign each prediction a row and
therefore the predictions may be specified in any order.
In section 2.3 we discuss an advanced functionality of the tool that allows the user to filter the loaded
set of examples based on boolean queries on the correctness of predictions of various models.
The column labeled Meta contains meta-data associated with each example. In our research, we are
currently experimenting with features and tree structures derived from the output of a semantic frame
parser called Semafor (Chen et al., 2010). Semafor labels the input text with frame information ? frame
evoking elements, frame elements, their spans and types. The output of a Semafor parse is quite complex.
Visualizing the annotations produced by Semafor, along with other features and dependency trees, is
helping us design novel features and tree structures for the task of relation extraction. The user of the
tool has full control over the type of meta-data, and the number of types of meta-data that (s)he might
want to upload in the error analysis interface.
The column labeled FEATUREVECTORS lists the different types of features that one may design for
3
a task. Clicking one of the feature vector types pops up a window that shows the value of features for a
particular example. For instance, clicking ?BOW1? for the first example will show a pop-up that contains
the following: ?some:0.2 administration:0.6?. These are words (and their tf-idf scores) that appear in the
text between the start of the sentence and the occurrence of the first target.
The column labeled TREES shows the different types of tree structures that a user may design for a
task. Clicking on a particular type of tree (button) will bring up a picture of the tree. Note ? we do not
require the user to provide the pdf with the tree diagram. The tool converts a tree specified in a standard
text format, such as this ?(ROOT (A B))?, into a dot file, which is automatically converted into a pdf file.
2.3 Other Functionalities
Notice the text box labeled ?Filter Data? in Figure 1. Users of the tool may specify complex
queries to filter out the examples that do not satisfy the filter. For example, a query such as
?FEATUREVECTORMODEL AND NOT SYNTREEMODEL? will filter out examples that do not sat-
isfy the following condition: examples that FEATUREVECTORMODEL predicted correctly but SYN-
TREEMODEL predicted incorrectly. Similarly a query such as ?FEATUREVECTORMODEL OR SYN-
TREEMODEL? will filter out examples that both the models predicted incorrectly. We have implemented
two binary operations (AND and OR) and a unary operation (NOT). These operations may be combined
with model names to form complex queries. The tool automatically saves the past 10 filter conditions,
which may be accessed through a drop down menu under the filter text box.
Notice the button labeled ?McNemar?s Statistical Significance? in Figure 1 (upper right corner). Click-
ing this button pops up a window that shows, in tabular format, the McNemar?s statistical significance
p-value for all pairs of models loaded in the interface.
Notice the ?Generate ID? button on the top right corner of Figure 1. If a user of the tool wants to
share his/her session (and analysis) with other collaborators, the user can generate a unique identifier
by clicking this button. This identifier may be shared with other collaborators who may visit the tool
website, enter the identifier in the text box labeled ?ID? and gain access to the same session. Of course,
a user might want to save the generated identifier for him/her-self for returning to an older session. The
web service handles concurrent requests.
2.4 Input XML Representation
Figure 3: XML format to be specified by the user.
Figure 3 shows the input XML schema expected from the user. Each example may be specified within
the XML tag ?example?. Example identifier and its gold class may be specified as attributes of the
element ?example?. Each example may have associated text, and a number of ?Meta?, ?Feat?, and
?Tree? prefixed tags. We use this prefix to determine how to render the content of each element. For
example, the content of the tag prefixed by ?Meta? and ?Feat? is shown as is on the interface, whereas
the content of the tag prefixed by ?Tree? is converted to a pdf with a picture of the tree.
3 Related Work
Stymne (2011) present an error analysis tool for machine translation. El Kholy and Habash (2011)
present an error analysis tool, Ameana, for NLP tasks that use morphologically rich languages. Both
these tools are specific in terms of the NLP tasks they tackle. While such tools are important (because
tasks such as machine translation are quite complex and require customized solutions), the goal of the
4
tool we present in this paper is different. The goal of our tool is to replace the day-to-day, quick and dirty
script writing process (required to make connections between different aspects of an NLP task) by a web
based user friendly solution. The design of this tool is novel, and to the best of our knowledge there is
no such publicly available web based error analysis tool.
4 License and Contact Information
The error analysis tool is available
1
for free for research purposes under the GNU General Public License
as published by the Free Software Foundation.
Acknowledgements
We would like to thank Jiehan Zheng and Aakash Bishnoi for contributing to the user interface code. We
would also like to thank Caronae Howell for her insightful comments.
References
Apoorv Agarwal and Owen Rambow. 2010. Automatic detection and classification of social events. In Pro-
ceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024?1034,
Cambridge, MA, October. Association for Computational Linguistics.
Apoorv Agarwal, Sriramkumar Balasubramanian, Anup Kotalwar, Jiehan Zheng, and Owen Rambow. 2014.
Frame semantic tree kernels for social network extraction from text. 14th Conference of the European Chapter
of the Association for Computational Linguistics.
Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A. Smith. 2010. Semafor: Frame argument resolution
with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264?
267, Uppsala, Sweden, July. Association for Computational Linguistics.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational Linguistics (ACL?04), Main Volume, pages 423?429,
Barcelona, Spain, July.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. 2004. The automatic
content extraction (ace) program?tasks, data, and evaluation. LREC, pages 837?840.
A. El Kholy and N. Habash. 2011. Automatic error analysis for morphologically rich languages. In MT Summit
XIII, September.
Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for
extracting relations. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 22.
Association for Computational Linguistics.
Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent,
dependency and sequential structures for relation extraction. Conference on Empirical Methods in Natural
Language Processing.
Sara Stymne. 2011. Blast: A tool for error analysis of machine translation output. In Proceedings of the ACL-HLT
2011 System Demonstrations, pages 56?61, Portland, Oregon, June. Association for Computational Linguistics.
Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The
Journal of Machine Learning Research, 3:1083?1106.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A composite kernel to extract relations between entities
with both flat and structured features. In Proceedings of COLING-ACL.
Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods.
In Proceedings of the 43rd Meeting of the ACL.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoMing Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In Proceedings of EMNLP-CoNLL.
1
www.ngicorporation.com/NEAT
5
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024?1034,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Detection and Classification of Social Events
Apoorv Agarwal
Department of Computer Science
Columbia University
New York, U.S.A.
apoorv@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, U.S.A.
rambow@ccls.columbia.edu
Abstract
In this paper we introduce the new task of
social event extraction from text. We distin-
guish two broad types of social events depend-
ing on whether only one or both parties are
aware of the social contact. We annotate part
of Automatic Content Extraction (ACE) data,
and perform experiments using Support Vec-
tor Machines with Kernel methods. We use a
combination of structures derived from phrase
structure trees and dependency trees. A char-
acteristic of our events (which distinguishes
them from ACE events) is that the participat-
ing entities can be spread far across the parse
trees. We use syntactic and semantic insights
to devise a new structure derived from depen-
dency trees and show that this plays a role in
achieving the best performing system for both
social event detection and classification tasks.
We also use three data sampling approaches
to solve the problem of data skewness. Sam-
pling methods improve the F1-measure for the
task of relation detection by over 20% abso-
lute over the baseline.
1 Introduction
This paper introduces a novel natural language pro-
cessing (NLP) task, social event extraction. We are
interested in this task because it contributes to our
overall research goal, which is to extract a social
network from written text. The extracted social net-
work can be used for various applications such as
summarization, question-answering, or the detection
of main characters in a story. For example, we man-
ually extracted the social network of characters in
Alice in Wonderland and ran standard social network
analysis algorithms on the network. The most influ-
ential characters in the story were correctly detected.
Moreover, characters occurring in a scene together
were given same social roles and positions. Social
network extraction has recently been applied to lit-
erary theory (Elson et al, 2010) and has the potential
to help organize novels that are becoming machine
readable.
We take a ?social network? to be a network con-
sisting of individual human beings and groups of hu-
man beings who are connected to each other by the
virtue of participating in social events. We define
social events to be events that occur between peo-
ple where at least one person is aware of the other
and of the event taking place. For example, in the
sentence John talks to Mary, entities John and Mary
are aware of each other and the talking event. In
the sentence John thinks Mary is great, only John is
aware of Mary and the event is the thinking event.
In the sentence Rabbit ran by Alice there is no evi-
dence about the cognitive states of Rabbit and Alice
(because the Rabbit could have run by Alice without
any one of them noticing each other). A text can de-
scribe a social network in two ways: explicitly, by
stating the type of relationship between two individ-
uals (e.g. husband-wife), or implicitly, by describing
an event which creates or perpetuates a social rela-
tionship (e.g. John talked to Mary). We will call
these types of events social events. We define two
types of social events: interaction, in which both
parties are aware of the social event (e.g., a conver-
sation), and observation, in which only one party
is aware of the interaction (e.g., thinking about or
1024
spying on someone). Note that the notion of cogni-
tive state is crucial to our definition. This paper is
the first attempt to detect and classify social events
present in text.
Our task is different from related tasks, notably
from the Automated Content Extraction (ACE) rela-
tion and event extraction tasks because the events are
different (they are a class of events defined through
the effect on participants? cognitive state), and the
linguistic realization is different. Mentions of enti-
ties1 engaged in a social event are often quite distant
from each other in the sentence (unlike in ACE rela-
tions where about 70% of relations are local, in our
social event annotation, only 25% of the events are
local. In fact, the average number of words between
entities participating in any social event is 9.)
We use tree kernel methods (on structures derived
from phrase structure trees and dependency trees) in
conjunction with Support Vector Machines (SVMs)
to solve our tasks. For the design of structures and
type of kernel, we take motivation from a system
proposed by Nguyen et al (2009) which is a state-
of-the-art system for relation extraction. Data skew-
ness turns out to be a big challenge for the task of
relation detection since there are many more pairs
of entities without a relation as compared to pairs of
entities that have a relation. In this paper we dis-
cuss three data sampling techniques that deal with
this skewness and allow us to gain over 20% in F1-
measure over our baseline system. Moreover, we
introduce a new sequence kernel that outperforms
previously proposed sequence kernels for the task of
social event detection and plays a role to achieve the
best performing system for the task of social event
detection and classification.
The paper is structured as follows. In Section 2,
we compare our work to existing work, notably the
ACE extraction literature. In Section 3, we present
our task in detail, and explain how we annotated our
corpus. We also show why this is a novel task, and
how it is different from the ACE extraction tasks.
We then discuss kernel methods and the structures
we use, and introduce our new structure in Section 4.
In Section 5, we present the sampling methods used
for experiments. In Section 6 we present our exper-
1An entity mention is a reference of an entity in text. Also,
we use entities and people interchangeably since the only enti-
ties we are interested in are people or groups of people.
iments and results for social event detection and so-
cial event classification tasks. We conclude in Sec-
tion 7 and mention our future direction of research.
2 Literature Survey
There has not been much work in developing tech-
niques for ACE event extraction as compared to
ACE relation extraction. The most salient work for
event extraction is Grishman et al (2005) and Ji and
Grishman (2008). To solve the task for event ex-
traction, Grishman et al (2005) mainly use a combi-
nation of pattern matching and statistical modeling
techniques. They extract two kinds of patterns: 1)
the sequence of constituent heads separating anchor
and its arguments and 2) a predicate argument sub-
graph of the sentence connecting anchor to all the
event arguments. In conjunction they use a set of
Maximum Entropy based classifiers for 1) Trigger
labeling, 2) Argument classification and 3) Event
classification. Ji and Grishman (2008) further ex-
ploit a correlation between senses of verbs (that are
triggers for events) and topics of documents.
Our work shares some similarities. However, in-
stead of building different classifiers, we use kernel
methods with SVMs that ?naturally? combine vari-
ous patterns. The structures we use for kernel meth-
ods are a super-set of the patterns used by Grishman
et al (2005). Moreover, in our work, we take gold
annotation for entity mentions, and do not deal with
the task of named entity detection or resolution. Fi-
nally, our social events are a broad class of event
types, and they involve linguistic expressions for ex-
pressing interactions and cognition that do not seem
to have a correlation with the topics of documents.
There has been much work in extracting ACE re-
lations. The supervised approaches used for relation
extraction can broadly be divided into three main
categories: 1) feature-based approaches 2) kernel-
based approaches and 3) a combination of feature
and kernel based approaches. The state-of-the-art
feature based approach is that of GuoDong et al
(2005). They use diverse lexical, syntactic and se-
mantic knowledge for the task. The lexical fea-
tures they use are words between, before, and af-
ter target entity mentions, the type of entity (Per-
son, Organization etc.), the type of mention (named,
nominal or pronominal) and a feature called overlap
1025
that counts the number of other entity mentions and
words between the target entities. To incorporate
syntactic features they use features extracted from
base phrase chunking, dependency trees and phrase
structure trees. To incorporate semantic features,
their approach uses resources like a country list and
WordNet. GuoDong et al (2005) report that 70% of
the entities are embedded within each other or sep-
arated by just one word. This is a major difference
to our task because most of our relations span over a
long distance in a sentence.
Collins and Duffy (2002) are among the earliest
researchers to propose the use of tree kernels for
various NLP tasks. Since then kernels have been
used for the task of relation extraction (Zelenko et
al., 2002; Zhao and Grishman, 2005; Zhang et al,
2006; Moschitti, 2006b; Nguyen et al, 2009). For
an excellent review of these techniques, see Nguyen
et al (2009). In addition, there has been some work
that combines feature and kernel based methods
(Harabagiu et al, 2005; Culotta and Jeffrey, 2004;
Zhou et al, 2007). Apart from using kernels over de-
pendency trees, Culotta and Jeffrey (2004) incorpo-
rate features like words, part of speech (POS) tags,
syntactic chunk tag, entity type, entity level, rela-
tion argument and WordNet hypernym. Harabagiu
et al (2005) leverage this approach by adding more
semantic feature derived from semantic parsers for
FrameNet and PropBank. Zhou et al (2007) use a
context sensitive kernel in conjunction with features
they used in their earlier publication (GuoDong et
al., 2005). However, we take an approach similar
to Nguyen et al (2009). This is because it incorpo-
rates many of the features suggested in feature-based
approaches by using combinations of various struc-
tures derived from phrase structure trees and depen-
dency trees. In addition we use data sampling tech-
niques to deal with the problem of data skewness.
We not only try the structures suggested by Nguyen
et al (2009) but also introduce a new sequence struc-
ture on dependency trees. We discuss their struc-
tures and kernel method in detail in Section 4.
3 Social Event Annotation Data
3.1 Social Event Annotation
There has been much work in the past on annotat-
ing entities, relations and events in free text, most
notably the ACE effort (Doddington et al, 2004).
We leverage this work by annotating social events on
the English part of ACE 2005 Multilingual Training
Data2 that has already been annotated for entities,
relations and events. In Agarwal et al (2010), we in-
troduce a comprehensive set of social events which
are conceptually different from the event annotation
that already exists for ACE. Since our annotation
task is complex and layered, in Agarwal et al (2010)
we present confusion matrices, Cohen?s Kappa, and
F-measure values for each of the decision points that
the annotators go through in the process of select-
ing a type and subtype for an event. Our annota-
tion scheme is reliable, achieving a moderate kappa
for relation detection (0.68) and a high kappa for re-
lation classification (0.86). We also achieve a high
global agreement of 69.7% using a measure which
is inspired by Automated Content Extraction (ACE)
inter-annotator agreement measure. This compares
favorably to the ACE annotation effort.
Following are the two broad types of social events
that were annotated:
Interaction event (INR): When both entities par-
ticipating in an event are aware of each other and of
the social event, we say they have an INR relation.
Consider the following Example (1).
(1) [Toujan Faisal], 54, {said} [she] was
{informed} of the refusal by an [Interior
Ministry committee] overseeing election
preparations. INR
As is intuitive, if one person informs the other
about something, both have to be cognizant of each
other and of the informing event in which they are
both participating.
Observation event (OBS): When only one person
(out of the two people that are participating in an
event) is aware of the other and of the social event,
we say they have an OBS relation. Of the type OBS,
there are three subtypes: Physical Proximity (PPR),
Perception (PCR) and Cognition (COG). PPR re-
quires that one entity can observe the other entity in
real time not through a broadcast medium, in con-
trast to the subtype PCR, where one entity observes
the other through media (TV, radio, magazines etc.)
Any other observation event that is not PPR or PCR
2Version: 6.0, Catalog number: LDC2005E18
1026
is COG. Consider the aforementioned Example (1).
In this sentence, the event said marks a COG re-
lation between Toujan Faisal and the committee.
This is because, when one person talks about another
person, the other person must be present in the first
person?s cognitive state without any requirement on
physical proximity or external medium.
As the annotations revealed, PPR and PCR oc-
curred only twice and once, respectively, in the part
of ACE corpus we annotated. (They occur more fre-
quently in another genre we are investigating such
as literary texts.) We omit these extremely low-
frequency categories from our current study; in this
paper we build classifiers to detect and classify only
INR and COG events.
3.2 Comparison Between Social Events and
ACE Annotations
The ACE effort is about entity, relation and event
annotation. We use their annotations for entity types
PER.Individual and PER.Group and add our social
event annotations. Our event annotations are dif-
ferent from ACE event annotations because we an-
notate text that expresses the cognitive states of the
people involved, or allows the annotator to infer it.
Therefore, at the top level of classification we dif-
ferentiate between events in which only one entity
is cognizant of the other (observation) versus events
when both entities are cognizant of each other (in-
teraction). This distinction is, we believe, novel in
event or relation annotation.
Now we present statistics and examples to make
clear how our annotations are different from ACE
event annotations. The statistics are based on 62
documents from the ACE corpus. These files con-
tain a total of 212 social events. We found a total of
63 candidate ACE events that had at least two Person
entities involved. Out of these 63 candidate events,
54 match our annotations. The majority of social
events that match the ACE events are of type INR.
On analysis, we found that most of these correspond
to the ACE event type CONTACT. Specifically, the
?meeting? event, which is an ACE CONTACT event
and an INR event according to our definition, is the
major cause of overlap. However, our type INR has
a broader definition than ACE type CONTACT. For
example, in Example 1, we recorded an INR event
between Toujan Faisal and committee (event span:
informed). ACE does not record any event between
these two entities because informed does not entail
a CONTACT event for ACE event annotations. An-
other example that will clarify the difference is the
following:
(2) In central Baghdad, [a Reuters cameraman] and
[a cameraman for Spain?s Telecinco] died when
an American tank fired on the Palestine Hotel
ACE has annotated the above example as an event
of type CONFLICT in which there are two entities
that are of type person: the Reuters cameraman
and the cameraman for Spain?s Telecinco, both of
which are arguments of type ?Victim?. Being an
event that has two person entities involved makes
the above sentence a potential social event. How-
ever, we do not record any event between these enti-
ties since the text does not reveal the cognitive states
of the two entities; we do not know whether one was
aware of the other.
ACE defines a class of social relations (PER-
SOC) that records named relations like friendship,
co-worker, long lasting etc. Also, there already exist
systems that detect and classify these relations well.
Therefore, even though these relations are directly
relevant to our overall goal of social event extrac-
tion, we do not annotate, detect or classify these re-
lations in this paper.
4 Tree Kernels, Discrete Structures, and
Language
In this section, we give details of the structures and
kernel we use for our classification tasks. We also
discuss our motivation behind using these methods.
Linear learning machines are one of the most popu-
lar machines used for classification problems. The
objective of a typical classification problem is to
learn a function that separates the data into differ-
ent classes. The data is usually in the form of fea-
tures extracted from abstract objects like strings,
trees, etc. A drawback of learning by using com-
plex functions is that complex functions do not gen-
eralize well and thus tend to over-fit. The research
community therefore prefers linear classifiers over
other complex classifiers. But more often than not,
the data is not linearly separable. It can be made
linearly separable by increasing the dimensionality
of data but then learning suffers from the curse of
1027
dimensionality and classification becomes computa-
tionally intractable. This is where kernels come to
the rescue. The well-known kernel trick aids us in
finding similarity between feature vectors in a high
dimensional space without having to write down the
expanded feature space. The essence of kernel meth-
ods is that they compare two feature vectors in high
dimensional space by using a dot product that is a
function of the dot product of feature vectors in the
lower dimensional space. Moreover, Convolution
Kernels (first introduced by Haussler (1999)) can
be used to compare abstract objects instead of fea-
ture vectors. This is because these kernels involve
a recursive calculation over the ?parts? of a discrete
structure. This calculation is usually made computa-
tionally efficient using Dynamic Programming tech-
niques. Therefore, Convolution Kernels alleviate the
need of feature extraction (which usually requires
domain knowledge, results in extraction of incom-
plete information and introduces noise in the data).
Therefore, we use convolution kernels with a linear
learning machine (Support Vector Machines) for our
classification task.
Now we present the ?discrete? structures followed
by the kernel we used. We use the structures pre-
viously used by Nguyen et al (2009), and propose
one new structure. Although we experimented with
all of their structures,3 here we only present the ones
that perform best for our classification task. All the
structures and their combinations are derived from a
variation of the underlying structures, Phrase Struc-
ture Trees (PST) and Dependency Trees (DT). For
all trees we first extract their Path Enclosed Tree,
which is the smallest common subtree that contains
the two target entities (Moschitti, 2004). We use the
Stanford parser (Klein and Manning, 2003) to get
the basic PSTs and DTs. Following are the struc-
tures that we refer to in our experiments and results
section:
PET: This refers to the smallest common phrase
structure tree that contains the two target entities.
Dependency Words (DW) tree: This is the smallest
common dependency tree that contains the two tar-
get entities. In Figure 1, since the target entities are
at the leftmost and rightmost branch of the depen-
3We omitted SK6, which is the worst performing sequence
kernel in (Nguyen et al, 2009).
committee
...
by
T2-Group
pobj
refusal
the
det
T1-Individual
Toujan_Faisal
nsubj
said
informed
ccomp
of
pobj
Individual
she
nsubjpass
prepprep
was
auxpass
54
appos
Figure 1: Dependency parse tree for the sentence (in
the ACE corpus): ?[Toujan Faisal], 54, {said} [she]
was {informed} of the refusal by an [Interior Min-
istry committee] overseeing election preparations.?
dency tree, this is in fact a DW (ignoring the gram-
matical relations on the arcs).
Grammatical Relation (GR) tree: If we replace the
words at the nodes by their relation to their corre-
sponding parent in DW, we get a GR tree. For exam-
ple, in Figure 1, replacing Toujan Faisal by nsubj,
54 by appos, she by nsubjpass and so on.
Grammatical Relation Word (GRW) tree: We get
this tree by adding the grammatical relations as sep-
arate nodes between a node and its parent. For ex-
ample, in Figure 1, adding nsubj as a node between
T1-Individual and Toujan Faisal, appos as a node
between 54 and Toujan Faisal, and so on.
Sequence Kernel of words (SK1): This is the se-
quence of words between the two entities, including
their tags. For our example in Figure 1, it would
be T1-Individual Toujan Faisal 54 said she was in-
formed of the refusal by an T2-Group Interior Min-
istry committee.
Sequence in GRW tree (SqGRW): This is the new
structure that we introduce which, to the best of
1028
our knowledge, has not been used before for sim-
ilar tasks. It is the sequence of nodes from one
target to the other in the GRW tree. For example,
in Figure 1, this would be Toujan Faisal nsubj T1-
Individual said ccomp informed prep by T2-Group
pobj committee.
We also use combinations of these structures
(which we refer to as ?combined-structures?). For
example, PET GR SqGRW means we used the three
structures (PET, GR and SqGRW) together with a
kernel that calculates similarity between forests.
We use the Partial Tree (PT) kernel, first proposed
by Moschitti (2006a), for structures derived from de-
pendency trees and Subset Tree (SST) kernel, pro-
posed by Collins and Duffy (2002), for structures
derived from phrase structure trees. PT is a relaxed
version of the SST; SST measures the similarity be-
tween two PSTs by counting all subtrees common to
the two PSTs. However, there is one constraint: all
daughter nodes of a node must be included. In PTs
this constraint is removed. Therefore, in contrast
to SSTs, PT kernels compare many more substruc-
tures. They have been used successfully by (Mos-
chitti, 2004) for the task of semantic role labeling.
The choices we have made are motivated by
the following considerations. We are interested
in modeling classes of events which are charac-
terized by the cognitive states of participants?who
is aware of whom. The predicate-argument struc-
ture of verbs can encode much of this information
very efficiently, and classes of verbs express their
predicate-argument structure in similar ways. For
example, many verbs of communication can ex-
press their arguments using the same pattern: John
talked/spoke/lectured/ranted/testified to Mary about
Percy. Independently of the verb, John is in a COG
relation with Percy and in an INR relation with
Mary. All these verbs allow us to drop either or
both of the prepositional phrases, without altering
the interpretation of the remaining constituents. And
even more strikingly, any verb that can be put in that
position is likely to have this interpretation; for ex-
ample, we are likely to interpret the neologistic John
gazooked to Mary about Percy as a similarly struc-
tured social event.
The regular relation between verb alternations and
meaning components has been extensively studied
(Levin, 1993; Schuler, 2005). This regularity in
the syntactic predicate-argument structure allows us
to overcome lexical sparseness. However, in or-
der to exploit such regularities, we need to have ac-
cess to a representation which makes the predicate-
argument structure clear. Dependency representa-
tions do this. Phrase structure representations also
represent predicate-argument structure, but in an in-
direct way through the structural configurations, and
we expect this to increase the burden on the learner.
(In some phrase structure representations, some ar-
guments and adjuncts are not disambiguated.) When
using dependency structures, the SST kernel is far
less appealing, since it forces us to always consider
all daughter nodes of a node. However, as we have
seen, it is certain daughter nodes, such as the pres-
ence of a to PP and a about PP, which are important,
while other daughters, such as temporal or locative
adjuncts, should be disregarded. The PT kernel al-
lows us to do this.
5 Sampling Methods
In this section we present the data sampling meth-
ods we use to deal with data skewness. We em-
ploy two well-known data sampling methods on the
training data before creating a model for test data;
random under-sampling and random over-sampling
(Kotsiantis et al, 2006; Japkowicz, 2000; Weiss and
Provost, 2001). These techniques are non-heuristic
sampling methods that aim at balancing the class
proportions by removing examples of the major-
ity class and by duplicating instances of the minor-
ity class respectively. The reason for using these
techniques is that learning is usually optimized to
achieve high accuracy. Therefore, when presented
with skewed training data, a classifier may learn the
target concept with a high accuracy by only predict-
ing the majority class. But if one looks at the preci-
sion, recall, and F-measure, of such a classifier, they
will be very low for the minority class. Since, like
other researchers, we are evaluating the goodness of
a model based on its precision, recall and F-measure
and not on the accuracy on the test set, either we
should change the optimization function of the clas-
sifier or employ data sampling techniques. We em-
ploy the latter because by balancing the class ratio,
we are presenting the classifier with a more chal-
lenging task of achieving a good accuracy when the
1029
majority base class is about 50%. The major draw-
backs of the two techniques is that under-sampling
throws away important information whereas over-
sampling is prone to over-fitting (due to data dupli-
cation). As our results show, throwing away infor-
mation about the majority class is much better than
the system that tries to learn in an unbalanced sce-
nario, but it performs worse than an approach using
data duplication. Since we are using SVMs as a clas-
sifier, over-fitting is unlikely as reported by Kolcz et
al. (2003).
In order to be sure that we are not over-fitting,
we tried another sampling method proposed by Ha
and Bunke (1997), which is shown to be good so-
lution to avoid over-fitting by Chawla et al (2002).
This sampling technique proposes to generate syn-
thetic examples of the minority class by ?perturb-
ing? the training data. Specifically, Ha and Bunke
(1997) produced new synthetic examples for the task
of handwritten character recognition by doing op-
erations like rotation and skew on characters. The
basic idea is to produce synthetic examples that are
?close? to the real example from which these syn-
thetic points are generated. Analogously, we tried
two transformations on our dependency tree struc-
tures to produce synthetic examples. The first trans-
formation is based on the observation that in con-
trol verb constructions, the matrix verb typically
does not contribute to the interpretation as a social
event or not. In this transformation, we lower the
subject to an argument verb if it does not have a
subject, and repeat this procedure iteratively. As
it turned out, this transformation only occurred 15
times, and therefore it does not serve the purpose
of over-sampling. We tried a more relaxed trans-
formation on the rightmost target in the tree. Here,
the observation is that for the COG social events,
the second target may be very deeply embedded in
the tree. For example, in Example 1, Toujan Faisal
and the Interior Ministry Committee participate in a
COG event (because Faisal is aware of the Commit-
tee during the saying event). However, the contents
of what Faisal said is only relevant to the extent that
it pertains to the committee. The depth of the em-
bedding of the second target creates issues of data
sparseness, as the path-enclosed trees become very
large and very diverse. Our transformation, there-
fore, is to move the second target to its grandmother
node, attaching it on the left, and to recalculate the
path-enclosed tree, which is now smaller. This is re-
peated iteratively, so that a sentence with a deeply
embedded second target can yield a large number of
synthesized structures.
6 Experiments And Results
In this section we present experiments and results for
our two tasks: social event detection and classifica-
tion. For the social event detection task, we wish to
validate the following research hypotheses. First, we
aim to show the importance of using data sampling
when evaluating on F-measure; specifically, we ex-
pect under-sampling to outperform no sampling,
over-sampling to outperform under-sampling, and
over-sampling with transformations to out perform
over-sampling without transformations. In contrast,
the social event classification task does not suffer
from data skewness because the INR and COG rela-
tions; both occur almost the same number of times.
Therefore, sampling methods may not be applied for
this task. Second, for both tasks, we expect that a
combination of kernels will out-perform individual
kernels. Moreover, we expect that dependency trees
will have a crucial role in achieving the best perfor-
mance.
6.1 Experimental Set-up
We use part of ACE data that we annotated for social
events. In all, we annotated 138 ACE documents.
We retained the ACE entity annotations. We con-
sider all entity mention pairs in a sentence. If our
annotators recorded a relation between a pair of en-
tity mentions, we say there is a relation between the
corresponding entities. If there are any other pairs of
entity mentions for the same pair of entity, we dis-
card those. For all other pairs of entity mentions,
we say there is no relation. Out of 138 files, four
files did not have any positive or negative examples
(because there were very few and sparse entity men-
tions in these four files). We found a total of 1291
negative examples, 172 examples belonging to class
INR and 174 belonging to class COG.
We use Jet?s sentence splitter4 and the Stanford
Parser (Klein and Manning, 2003) for phrase struc-
ture trees and dependency parses. For classifica-
4http://cs.nyu.edu/grishman/jet/jetDownload.html
1030
tion, we used Alessandro Moschitti?s SVM-Light-
TK package (Moschitti, 2006b) which is built on
the SVM-Light implementation of Joakhims (1999).
For all our experiments, we perform 5-fold cross-
validation. We randomly divide the whole corpus
into 5 equal parts, such that no news story (or docu-
ment) gets divided among two parts. For each fold,
we then merge 4 parts to create a training corpus and
treat the remaining part as a test corpus. By keep-
ing individual news stories intact, we make sure that
vocabulary specific to one story does not unrealisti-
cally improve the performance.
6.2 Social Event Detection
Social event detection is the task of detecting if any
social event exists between a pair of entities in a sen-
tence. We formulate the problem as a binary classi-
fication task by labeling an example that does not
have a social event as class -1 and by labeling an ex-
ample that either has an INR or COG social event
as class 1. First we present results for our baseline
system. Our baseline system uses various structures
and their combinations but without any data balanc-
ing. 5
Kernel P R F1
PET 70.28 21.46 32.38
GR 87.79 15.21 25.55
GRW 76.42 8.26 14.8
SqGRW 48.78 6.08 10.38
PET GR 70.21 27.76 38.89
PET GR SqGRW 71.06 26.74 38.02
GR SqGRW 82.0 24.47 36.12
GRW SqGRW 68.19 17.01 25.06
GR GRW SqGRW 79.81 21.99 32.57
Table 1: Baseline System for the task of social event
detection. The proportion of positive data in training
and test set is 21.1% and 20.6% respectively.
Table 1 presents results for our baseline system.
Grammatical relation tree structure (GR), a struc-
ture derived from dependency tree by replacing the
words by their grammatical relations achieves the
best precision. This is probably because the clas-
5Although we experimented with many more structures and
their combinations, due to space restrictions we mention only
the top results.
sifier learns that if both the arguments of a predi-
cate contain target entities then it is a social event.
Among kernels for single structures, the path en-
closed tree for PSTs (PET) achieves the best re-
call. Furthermore, a combination of structures de-
rived from PSTs and DTs performs best. The se-
quence kernels, perform much worse than SqGRW
(F1-measure as low as 0.45). Since it is the same
case for all subsequent experiments, we omit them
from the discussion.
Kernel P R F1
PET 28.89 77.06 41.96
GR 35.68 72.47 47.37
GRW 29.7 83.6 43.6
SqGRW 34.31 84.15 48.61
PET GR 34.38 83.94 48.52
PET GR SqGRW 34.34 83.66 48.52
GR SqGRW 33.45 81.73 47.27
GRW SqGRW 32.87 84.44 47.11
GR GRW SqGRW 32.73 83.26 46.82
Table 2: Under-sampled system for the task of rela-
tion detection. The proportion of positive examples
in the training and test corpus is 50.0% and 20.6%
respectively.
We now turn to experiments involving sampling.
Table 2 presents results for under-sampling, i.e. ran-
domly removing examples belonging to the negative
class until its size matches the positive class. Table 2
shows a large gain in F1-measure of 9.72% abso-
lute over the baseline system (Table 1). We found
that worst performing kernel with under-sampling
is SK1 with an F1-measure of 39.2% which is
better than the best performance without under-
sampling. These results make it clear that doing
under-sampling greatly improves the performance of
the classifier, despite the fact that we are using less
training data (fewer negative examples). This is as
expected because we are evaluating on F1-measure
and the classifier is optimizing for accuracy.
Table 3 presents results for over-sampling i.e.
replicating positive examples to achieve an equal
number of examples belonging to the positive and
negative class. Table 3 shows that the gain over
the baseline system now is 22.2% absolute. Also,
the gain over the under-sampled system is 12.5%
1031
Kernel P R F1
PET 50.9 57.21 53.62
GR 43.57 67.21 52.59
GRW 46.05 64.15 53.31
SqGRW 42.4 72.75 53.5
PET GR 56.42 66.2 60.63
PET GR SqGRW 57.28 66.26 61.11
GR SqGRW 44.35 71.17 54.52
GRW SqGRW 44.77 68.79 54.12
GR GRW SqGRW 46.79 71.54 56.45
Table 3: Over-sampled system for the task of rela-
tion detection. The proportion of positive examples
in the training and test corpus is 50.0% and 20.6%
respectively.
absolute. As in the baseline system, a combina-
tion of structures performs best. As in the under-
sampled system, when the data is balanced, SqGRW
(sequence kernel on dependency tree in which gram-
matical relations are inserted as intermediate nodes)
achieves the best recall. Here, the PET and GR ker-
nel perform similar: this is different from the results
of (Nguyen et al, 2009) where GR performed much
worse than PET for ACE data. This exemplifies
the difference in the nature of our event annotations
from that of ACE relations. Since the average dis-
tance between target entities in the surface word or-
der is higher for our events, the phrase structure trees
are bigger. This means that implicit feature space is
much sparser and thus not the best representation.
PET 37.04 66.49 47.28
GR 40.39 71.14 51.27
GRW 45.16 66.82 53.47
SqGRW 42.88 70.67 53.22
PET GR 45.33 70.26 54.71
PET GR SqGRW 45.26 72.97 55.67
GR SqGRW 43.73 71.47 54.06
GRW SqGRW 45.70 71.30 55.32
GR GRW SqGRW 45.91 71.90 55.70
Table 4: Over-sampled System with transformation
for relation detection. The proportion of positive ex-
amples in the training and test corpus is 51.7% and
20.6% respectively.
Table 4 presents results for using the over-
sampling method with transformation that produces
synthetic positive examples by using a transforma-
tion on dependency trees such that the new syn-
thetic examples are ?close? to the original exam-
ples. This method achieves a gain 16.78% over the
baseline system. We expected this system to per-
form better than the over-sampled system but it does
not. This suggests that our over-sampled system is
not over-fitting; a concern with using oversampling
techniques.
6.3 Social Event Classification
For the social event classification task, we only con-
sider pairs of entities that have an event. Since these
events could only be INR or COG, this is a binary
classification problem. However, now we are inter-
ested in both outcomes of the classification, while
earlier we were only interested in knowing how well
we were finding relations (and not in how well we
were finding ?non-relations?). Therefore, accuracy
is the relevant metric (Table 5).
Kernel Acc
PET 76.85
GR 71.04
GRW 76.22
SqGRW 75.78
PET GR 76.34
PET GR SqGRW 78.72
GR SqGRW 75.60
GRW SqGRW 76.96
GR GRW SqGRW 77.29
Table 5: System for the task of relation classifica-
tion. The two classes are INR and COG, and we
evaluate using accuracy (Acc.). The proportion of
INR relations in training and test set is 49.7% and
49.63% respectively.
Even though the task of reasoning if an event
is about one-way or mutual cognition seems hard,
our system beats the chance baseline by 28.72%.
These results show that there are significant clues
in the lexical and syntactic structures that help in
differentiating between interaction and cognition so-
cial events. Once again we notice that the combi-
nation of kernels works better than single kernels
1032
alone, though the difference here is less pronounced.
Among the combined-structure approaches, com-
binations with dependency-derived structures con-
tinue to outperform those not including dependency
(the best all-phrase structure performer is PET SK1
with 75.7% accuracy, not shown in Table 5).
7 Conclusion And Future Work
In this paper, we have introduced the novel tasks of
social event detection and classification. We show
that data sampling techniques play a crucial role
for the task of relation detection. Through over-
sampling we achieve an increase in F1-measure of
22.2% absolute over a baseline system. Our exper-
iments show that as a result of how language ex-
presses the relevant information, dependency-based
structures are best suited for encoding this informa-
tion. Furthermore, because of the complexity of
the task, a combination of phrase based structures
and dependency-based structures perform the best.
This revalidates the observation of Nguyen et al
(2009) that phrase structure representations and de-
pendency representations add complimentary value
to the learning task. We also introduced a new se-
quence structure (SqGRW) which plays a role in
achieving the best accuracy for both, social event de-
tection and social event classification tasks.
In the future, we will use other parsers (such as
semantic parsers) and explore new types of linguis-
tically motivated structures and transformations. We
will also investigate the relation between classes of
social events and their syntactic realization.
Acknowledgments
The work was funded by NSF grant IIS-0713548.
We thank Dr. Alessandro Moschitti and Truc-Vien
T. Nguyen for helping us with re-implementing their
system. We acknowledge Boyi Xie for his assistance
in implementing the system. We would also like to
thank Dr. Claire Monteleoni and Daniel Bauer for
useful discussions and feedback.
References
Apoorv Agarwal, Owen Rambow, and Rebecca J Passon-
neau. 2010. Annotation scheme for social network
extraction from text. In Fourth Linguistic Annotation
Workshop, ACL.
N V Chawla, L O Hall, K W Bowyer, and W P
Kegelmeyer. 2002. Smote: Synthetic minority over-
sampling technique. In Journal of Artificial Intelli-
gence Research.
M. Collins and N. Duffy. 2002. Convolution kernels for
natural language. In Advances in neural information
processing systems.
Aron Culotta and Sorensen Jeffrey. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 423?429,
Barcelona, Spain, July.
G Doddington, A Mitchell, M Przybocki, L Ramshaw,
S Strassel, and R Weischedel. 2004. The automatic
content extraction (ace) program?tasks, data, and eval-
uation. LREC, pages 837?840.
David K. Elson, Nicholas Dames, and Kathleen R. McK-
eown. 2010. Extracting social networks from literary
fiction. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL
2010), Uppsala, Sweden.
Ralph Grishman, David Westbrook, and Adam Meyers
Proc. 2005. Nyu?s english ace 2005 system descrip-
tion. In ACE Evaluation Workshop.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extrac-
tion. In Proceedings of 43th Annual Meeting of the
Association for Computational Linguistics.
T. M. Ha and H Bunke. 1997. Off-line, handwritten nu-
merical recognition by perturbation method. In Pat-
tern Analysis and Machine Intelligence.
Sanda Harabagiu, Cosmin Adrian Bejan, and Paul
Morarescu. 2005. Shallow semantics for relation ex-
traction. In International Joint Conference On Artifi-
cial Intelligence.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of California
at Santa Cruz.
Nathalie Japkowicz. 2000. Learning from imbalanced
data sets: Comparison of various strategies. In AAAI
Workshop on Learning from Imbalanced Data Sets.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through unsupervised cross-document infer-
ence. In Proceedings of ACL.
Thorsten Joakhims. 1999. Making large-scale svm
learning practical. In B. Scholkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning.
1033
Dan Klein and Chistopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS).
Aleksander Kolcz, Abdur Chowdhury, and Joshua Al-
spector. 2003. Data duplication: An imbalance
problem. In Workshop on Learning from Imbalanced
Datasets, ICML.
Sotiris Kotsiantis, Dimitris Kanellopoulos, and Panayio-
tis Pintelas. 2006. Handling imbalanced datasets: A
review. In GESTS International Transactions on Com-
puter Science and Engineering.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University of
Chicago Press.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceedings
of the 42nd Conference on Association for Computa-
tional Linguistic.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Ma-
chine Learning.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proceedings of
European chapter of Association for Computational
Linguistics.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Conference on Empirical Methods
in Natural Language Processing.
Karin Kipper Schuler. 2005. Verbnet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
upenncis.
Gary M Weiss and Foster Provost. 2001. The effect of
class distribution on classifier learning: an empirical
study. Technical Report ML.TR-44, Rutgers Univer-
sity, August.
D. Zelenko, C. Aone, and A. Richardella. 2002. Kernel
methods for relation extraction. In Proceedings of the
EMNLP.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations between
entities with both flat and structured features. In Pro-
ceedings of COLING-ACL.
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Proceedings of the 43rd Meeting of the ACL.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoM-
ing Zhu. 2007. Tree kernel-based relation extraction
with context-sensitive structured parse tree informa-
tion. In Proceedings of EMNLP-CoNLL.
1034
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 211?219,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Frame Semantic Tree Kernels for Social Network Extraction from Text
Apoorv Agarwal
Dept. of Computer Science
Columbia University
New York, NY, USA
Sriramkumar Balasubramanian
Dept. of Computer Science
Columbia University
New York, NY, USA
Anup Kotalwar
Microsoft, Inc.
Redmonad, WA, USA
Jiehan Zheng
The Peddie School
Hightstown, NJ, USA
Owen Rambow
CCLS
Columbia University
New York, NY, USA
apoorv@cs.columbia.edu
Abstract
In this paper, we present work on ex-
tracting social networks from unstructured
text. We introduce novel features de-
rived from semantic annotations based on
FrameNet. We also introduce novel se-
mantic tree kernels that help us improve
the performance of the best reported sys-
tem on social event detection and classi-
fication by a statistically significant mar-
gin. We show results for combining the
models for the two aforementioned sub-
tasks into the overall task of social net-
work extraction. We show that a combina-
tion of features from all three levels of ab-
stractions (lexical, syntactic and semantic)
are required to achieve the best performing
system.
1 Introduction
Social network extraction from text has recently
been gaining a considerable amount of attention
(Agarwal and Rambow, 2010; Elson et al., 2010;
Agarwal et al., 2013a; Agarwal et al., 2013b; He
et al., 2013). One of the reason for this attention,
we believe, is that being able to extract social net-
works from unstructured text may provide a pow-
erful new tool for historians, political scientists,
scholars of literature, and journalists to analyze
large collections of texts around entities and their
interactions. The tool would allow researchers to
quickly extract networks and assess their size, na-
ture, and cohesiveness, a task that would otherwise
be impossible with corpora numbering millions of
documents. It would also make it possible to make
falsifiable claims about these networks, bringing
the experimental method to disciplines like his-
tory, where it is still relatively rare.
In our previous work (Agarwal et al., 2010),
we proposed a definition of a network based on
interactions: nodes are entities and links are so-
cial events. We defined two broad types of links:
one-directional links (one person thinking about
or talking about another person) and bi-directional
links (two people having a conversation, a meet-
ing, etc.). For example, in the following sen-
tence, we would add two links to the network: a
one-directional link between Toujan Faisal and
the committee, triggered by the word said (be-
cause Toujan is talking about the committee) and
a bi-directional link between the same entities trig-
gered by the word informed (a mutual interaction).
(1) [Toujan Faisal], 54, said [she] was informed
of the refusal by an [Interior Ministry com-
mittee] overseeing election preparations.
In this paper, we extract networks using the
aforementioned definition of social networks. We
introduce and add tree kernel representations and
features derived from frame-semantic parses to
our previously proposed system. Our results show
that hand-crafted frame semantic features, which
are linguistically motivated, add less value to
the overall performance in comparison with the
frame-semantic tree kernels. We believe this is due
to the fact that hand-crafted features require frame
parses to be highly accurate and complete. In con-
trast, tree kernels are able to find and leverage less
strict patterns without requiring the semantic parse
to be entirely accurate or complete.
Apart from introducing semantic features and
tree structures, we evaluate on the task of social
network extraction, which is a combination of two
sub-tasks: social event detection and social event
classification. In our previous work (Agarwal and
Rambow, 2010), we presented results for the two
211
sub-tasks, but no evaluation was presented for the
task of social network extraction. We experiment
with two different designs of combining models
for the two sub-tasks: 1) One-versus-All and 2)
Hierarchical. We find that the hierarchical de-
sign outperforms the more commonly used One-
versus-All by a statistically significant margin.
Following are the contributions of this paper:
1. We design and propose novel frame semantic
features and tree-based representations and
show that tree kernels are well suited to work
with noisy semantic parses.
2. We show that in order to achieve the best
performing system, we need to include fea-
tures and tree structures from all levels of
abstractions, lexical, syntactic, and semantic,
and that the convolution kernel framework is
well-suited for creating such a combination.
3. We combine the previously proposed sub-
tasks (social event detection and classifica-
tion) into a single task, social network ex-
traction, and show that combining the mod-
els using a hierarchical design is significantly
better than the one-versus-all design.
The rest of the paper is structured as follows:
In Section 2, we give a precise definition of the
task and describe the data. In Section 3, we give
a brief overview of frame semantics and motivate
the need to use frame semantics for the tasks ad-
dressed in this paper. In Section 4, we present
semantic features and tree kernel representations
designed for the tasks. In Section 5, we briefly
review tree kernels and support vector machines
(SVM). In Section 6 we present experiments and
discuss the results. In Section 7 we discuss related
work. We conclude and give future directions of
work in Section 8.
2 Data and Task Definition
In Agarwal et al. (2010), we presented the annota-
tion details of social events on a well-known cor-
pus ? Automated Content Extraction
1
(ACE2005).
We defined a social event to be a happening be-
tween two entities (of type person) E1 and E2
(E1 6= E2), in which at least one entity is cog-
nitively aware of the other and of the happen-
ing taking place. We defined two broad cate-
1
Version: 6.0, Catalog number: LDC2005E18
No-Event INR OBS
# of Examples 1,609 199 199
Table 1: Data distribution; INR are interaction so-
cial events. OBS are observation social events.
gories of social events: Interaction (INR) and Ob-
servation (OBS). In a social event of type INR,
the two participating entities are mutually aware
of each other, i.e., INR is a bi-directional social
event. For example, meetings and dinners are so-
cial events of type interaction. In a social event of
type OBS, only one of the two participating enti-
ties is aware of the other and therefore, OBS is a
one-directional social event, directed from the en-
tity that is aware of the other to the other entity.
For example, thinking about someone, or missing
someone are social events of type OBS. Table 1
shows the distribution of the data. There are 199
INR type of social events, 199 OBS events, and
1,609 pairs of entity mentions have no event be-
tween them.
Task definition : The task is, given a pair of en-
tity mentions in a sentence, to predict if the en-
tities are participating in a social event or not
(social event detection, SED), and if they are, to
further predict the type of social event (INR or
OBS, social event classification, SEC). In this pa-
per, we evaluate our system on the above tasks as
well as a combined task: social network extraction
(SNE): given a sentence and a pair of entity men-
tions, predict the class of the example from one of
the following three categories: {No-Event, INR,
OBS}.
For the purposes of this paper, we use gold
named entity mentions to avoid errors caused due
to named entity recognition systems. This is a
common practice used in the literature for re-
porting relation extraction systems (Zelenko et
al., 2003; Kambhatla, 2004; Zhao and Grishman,
2005; GuoDong et al., 2005; Harabagiu et al.,
2005; Nguyen et al., 2009). We use standard ter-
minology from the literature to refer to the pair of
entities mentions as target entities T
1
and T
2
.
3 Frame Semantics and FrameNet
FrameNet (Baker et al., 1998) is a resource which
associates words of English with their meaning.
Word meanings are based on the notion of ?se-
mantic frame?. A frame is a conceptual descrip-
tion of a type of event, relation, or entity, and it
212
includes a list of possible participants in terms of
the roles they play; these participants are called
?frame elements?. Through the following exam-
ple, we present the terminology and acronyms that
will be used throughout the paper.
Example (2) shows the frame annotations for
the sentence Toujan Faisal said she was informed
of the refusal by an Interior Ministry committee.
One of the semantic frames in the sentence is
Statement. The frame evoking element (FEE) for
this frame is said. It has two frame elements (FE):
one of type Speaker (Toujan Faisal) and the other
of type Message (she was informed ... by an Inte-
rior Ministry committee).
(2) [
FE?Speaker
Toujan Faisal] [
FEE?Statement
said] [
FE?Message
she was informed of the
refusal by an Interior Ministry committee]
In example (2), the speaker of the message (Toujan
Faisal) is mentioning another group of people (the
Interior Ministry committee) in her message. By
definition, this is a social event of type OBS. In
general, there is an OBS social event between any
Speaker and any person mentioned in the frame
element Message of the frame Statement. This
close relation between frames and social events is
the reason for our investigation and use of frame
semantics for the tasks addressed in this paper.
4 Feature space and data representation
We convert examples
2
into two kinds of structured
representations: feature vectors and tree struc-
tures. Each of these structural representations may
broadly be categorized into one or more of the fol-
lowing levels of abstraction: {Lexical, Syntactic,
Semantic}. Table 2 presents this distribution. Our
final results show that all of our top performing
models use a data representation that is a combi-
nation of features and structures from all levels of
abstraction. We review previously proposed fea-
tures and tree structures in subsections 4.1, 4.2,
and 4.3. To the best of our knowledge, the re-
maining features and structures presented in this
section are novel.
4.1 Bag of words (BOW)
We create a vocabulary from our training data
by using the Stanford tokenizer (Klein and Man-
ning, 2003) followed by removal of stop words
2
An input example is a sentence with a pair of entity men-
tions between whom we predict and classify social events.
and Porter Stemming. We convert each example
(~x) to a set of three boolean vectors: {
~
b
1
,
~
b
2
,
~
b
3
}.
~
b
1
is the occurrence of words before the first tar-
get,
~
b
2
between the two targets and
~
b
3
after the sec-
ond target. Here the first target and second target
are defined in terms of the surface order of words.
Though these features have been previously pro-
posed for relation extraction on ACE (GuoDong
et al., 2005), they have not been utilized for the
task we address in this paper.
4.2 Syntactic structures (AR2010)
In Agarwal and Rambow (2010), we explored
a wide range of syntactic structures for the two
tasks of social event detection (SED) and classi-
fication (SEC). All our previous structures were
derived from a variation of two underlying tree
structures: phrase structure trees and depen-
dency trees. The best structure we proposed was
PET_GR_SqGRW, which was a linear combina-
tion of two tree kernels and one word kernel: 1)
a structure derived from a phrase structure tree
(PET); 2) a grammatical role tree (GR), which is
a dependency tree in which words are replaced
with their grammatical roles; and 3) a path from
one entity to the other in a dependency tree, in
which grammatical roles of words are inserted as
additional nodes between the dependent and par-
ent (SqGRW). We refer the reader to Agarwal
and Rambow (2010) for details of these structures.
For the rest of the paper, we refer to this struc-
ture, PET_GR_SqGRW, as ?AR2010?. We use
AR2010 as one of our baselines.
4.3 Bag of frames (BOF)
We use Semafor (Chen et al., 2010) for obtaining
the semantic parse of a sentence. Semafor found
instances of 1,174 different FrameNet frames in
our corpus. Each example (~x) is converted to a
vector of dimension 1,174, in which x
i
(the i
th
component of vector ~x) is 1 if the frame number
i appears in the example, and 0 otherwise.
4.4 Hand-crafted semantic features (RULES)
We use the manual of the FrameNet resource to
hand-craft 199 rules that are intended to detect the
presence and determine the type of social events
between two entities mentioned in a sentence. An
example of one such rule is given in section 3,
which we reformulate here. We also present an-
other example:
213
Feature Vectors Tree Structures
BOW BOF RULES AR2010 FrameForest FrameTree FrameTreeProp
Lexical ! ! !
Syntactic ! !
Semantic (novel) ! ! ! ! !
Table 2: Features and tree structures and the level of abstraction they fall into.
(3) If the frame is Statement, and the first tar-
get entity mention is contained in the FE
Speaker, and the second is contained in the
FE Message, then there is an OBS social
event from the first entity to the second.
(4) If the frame is Commerce_buy, and one tar-
get entity mention is contained in the FE
Buyer, and the other is contained in the FE
Seller, then there is an INR social event be-
tween the two entities.
Each rule corresponds to a binary feature: it
takes a value 1 if the rule fires for an input ex-
ample, and 0 otherwise. Consider the following
sentence:
(5) [Coleman]
T1?Ind
{claimed}
[he]
T1
?
?Ind
{bought} drugs from the
[defendants]
T2?Grp
.
In this sentence, there are two social events:
1) an OBS event triggered by the word claimed
between Coleman and defendants and 2) an INR
event triggered by the word bought between he
(co-referential with Coleman) and the defendants.
Semafor correctly detects two frames in this
sentence: 1) the frame Statement, with Coleman
as Speaker, and he bought ... defendants as Mes-
sage, and 2) the frame Commerce_buy, with he as
the Buyer, drugs as the Goods and the defendants
as the Seller. Both hand-crafted rules (3 and 4)
fire and the corresponding feature values for these
rules is set to 1. Firing of these rules (and thus
the effectiveness these features) is of course highly
dependent on the fact that Semafor provides an ac-
curate frame parse for the sentence.
4.5 Semantic trees (FrameForest,
FrameTree, FrameTreeProp)
Semafor labels text spans in sentences as frame
evoking elements (FEE) or frame elements (FE).
A sentence usually has multiple frames and the
frame annotations may overlap. There may be two
ways in which spans overlap (Figure 1): (a) one
Figure 1: Two overlapping scenarios for frame an-
notations of a sentence, where F1, F2 are frames.
frame annotation is completely embedded in the
other frame annotation and (b) some of the frame
elements overlap (in terms of text spans). We now
present the three frame semantic tree kernel rep-
resentations that handle these overlapping issues,
along with providing a meaningful semantic ker-
nel representation for the tasks addressed in this
paper.
For each of the following representations,
we assume that for each sentence s, we have
the set of semantic frames, F
s
= {F =
?FEE, [FE
1
, FE
2
, . . . , FE
n
]?} with each frame
F having an FEE and a list of FEs. . We illustrate
the structures using sentence (5).
4.5.1 FrameForest Tree Representation
We first create a tree for each frame annota-
tion F in the sentence. Consider a frame,
F = ?FEE, [FE
1
, FE
2
, . . . , FE
n
]?. For the
purposes of tree construction, we treat FEE as
another FE (call it FE
0
) of type Target. For
each FE
i
, we choose the subtree from the de-
pendency parse tree that is the smallest subtree
containing all words annotated as FE
i
by Se-
mafor. Call this subtree extracted from the de-
pendency parse DepTree_FE
i
. We then cre-
ate a larger tree by adding DepTree_FE
i
as
a child of a new node labeled with frame el-
ement FE
i
: (FE
i
DepTree_FE
i
). Call this
resulting tree SubTree_FE
i
. We then connect
all the SubTree_FE
i
(i ? {0, 1, 2, . . . , n}) to
a new root node labeled with the frame F :
(F SubTree_FE
0
. . . SubTree_FE
n
). This
is the tree for a frame F . Since the sentence
could have multiple frames, we connect the for-
est of frame trees to a new node called ROOT .
214
ROOT
Commerce_buy
Target
4
Buyer
T1-Ind
Seller
from
T2-Grp
Statement
Target
claimed
4
Speaker
T1?-Ind
Message
4
Statement
Speaker
T1-Ind
Coleman
Message
Commerce_buy
Buyer
T1?-Ind
he
Seller
T2-Grp
defendants
Figure 2: Semantic trees for the sentence ?Coleman claimed [he]
T1?Ind
bought drugs from the
[defendants]
T2?Grp
.?. The tree on the left is FrameForest and the tree on the right is FrameTree. 4
in FrameForest refers to the subtree (bought (T1-Ind) (from T2-Grp)). Ind refers to individual and Grp
refers to group.
We prune away all subtrees that do not contain
the target entities. We refer to the resulting tree
as FrameForest.
For example, in Figure 2, the left tree is the
FrameForest tree for sentence (5). There are two
frames in this sentence that appear in the final tree
because both these frames contain the target enti-
ties and thus are not pruned away. The two frames
are Commerce_buy and Statement. We first cre-
ate trees for each of the frames. For the Com-
merce_buy frame, there are three frame elements:
Target (the frame evoking element), Buyer and
Seller. For each frame element, we get the sub-
tree from the dependency tree that contains all the
words belonging to that frame element. The sub-
tree for FEE Target is (bought T1-Ind (from T2-
Grp)). The subtree for FE Buyer is (T1-Ind) and
the subtree for FE Seller is (from T2-Grp). We
connect these subtrees to their respective frame el-
ements and connect the resulting subtrees to the
frame (Commerce_buy). Similarly, we create a
tree for the frame Statement. Finally, we connect
all frame trees to the ROOT .
In this representation, we have avoided the
frame overlapping issues by repeating the com-
mon subtrees: the subtree (bought T1-Ind (from
T2-Grp)) is repeated under the FEE Target of the
Statement frame as well as under the FE Message
of the Statement frame.
4.5.2 FrameTree Tree Representation
For the design of this tree, we deal with the two
overlapping conditions shown in Figure 1 differ-
ently. If one frame is fully embedded in another
frame, we add the former as a child of the latter
frame. In Figure 2, the frame Commerce_buy is
fully embedded in the frame element Message of
the frame Statement. Therefore, the frame sub-
tree for Commerce_buy appears as a subtree of
Message.
If the frames overlap partially, we copy over the
overlapping portions of the structures to each of
the frame sub-trees.
For the design of this representation, we remove
all lexical nodes (struck out nodes in Figure 2) and
trees that do not span any of the target entities (not
shown in the figure). As a result, this structure
is the smallest semantic structure that contains the
two target entities. The right tree in Figure 2 is the
FrameTree tree for sentence (5).
4.5.3 FrameTreeProp Tree Representation
We are using a partial tree kernel (PTK) for calcu-
lating the similarity of two trees (as detailed in sec-
tion 5). The PTK does not skip over nodes of the
tree that lie on the same path. For establishing an
OBS social event between Coleman and the defen-
dants, all the structure needs to encode is the fact
that one target appears as a Speaker and the other
appears in the Message (of the speaker). In Frame-
Tree, this information is encoded but in an unclear
manner ? there are two nodes (Commerce_buy
and Seller) that come in between the node Mes-
sage and T2-Grp.
For this reason, we copy the nodes labeled with
the target annotations (T1??, T2??) to all nodes
(that are frame elements of a frame) on the path
from them to the root in FrameTree. We call this
215
variation of FrameTree, in which we propagate
T1 ? ?, T2 ? ? nodes to the root, FrameTreeP-
rop. For the running example, FrameTreeProp
will be: (Statement (Speaker T1-Ind) (Message
(Commerce_buy ...) (T2-Grp))). Using this tree
representation, one of the sub-trees in the implicit
feature space will be (Statement (Speaker T1-Ind)
(Message (T2-Grp)), which encodes the relation
between the two targets in a more direct manner
as compared to FrameTree.
5 Machine Learning
We represent our data in form of feature vectors
and tree structures. We use convolution kernels
(Haussler, 1999) that make use of the dual form
of Support Vector Machines (SVMs). In the dual
form, the optimization problem that SVM solves
is the following (Burges, 1998):
max ?
i
?
i
? ?
i,j
?
i
?
j
y
i
y
j
K(x
i
, x
j
)
s.t. ?
i
?
i
y
i
= 0
?
i
? 0 ?i = 1, 2, . . . , l
Here, x
i
is the input example, y
i
is the class of
the example x
i
, ?
i
is the Lagrange multiplier as-
sociated with example x
i
, l is the number of train-
ing examples, and K is the kernel function that
returns a similarity between two examples. More
formally, K is the function, K : X ? X ? R,
that maps a pair of objects belonging to the set X
to a real number. For example, if we represent our
input examples as feature vectors, the setX would
be the set of feature vectors. For feature vectors,
we use a linear kernel, i.e. K(x
i
, x
j
) = x
i
? x
j
(dot product of the two vectors). For our tree rep-
resentations, we use a Partial Tree Kernel (PTK),
first proposed by Moschitti (2006). PTK is a re-
laxed version of the Subset Tree (SST) kernel pro-
posed by Collins and Duffy (2002). A subset
tree kernel measures the similarity between two
trees by counting all subtrees common to the two
trees. However, there is one constraint: all daugh-
ter nodes of a parent node must be included (in
the sub-trees). In PTK, this constraint is removed.
Therefore, in contrast to SST, PT kernels compare
many more substructures. For a combination of
feature vectors and tree representations, we sim-
ply use the linear combination of their respective
kernels.
6 Experiments and Results
We present 5-fold cross-validation results on the
ACE2005 corpus annotated for social events.
Since the number of types of features and struc-
tures is not large (Table 2), we run an exhaustive
set of 2
7
? 1 = 127 experiments for each of three
tasks: Social Event Detection (SED), Social Event
Classification (SEC) and Social Network Extrac-
tion (SNE). To avoid over-fitting to a particular
partition into folds, we run each 5-fold experi-
ment 50 times, for 50 randomly generated parti-
tions. The results reported in the following tables
are all averaged over these 50 partitions. The ab-
solute standard deviation on an average is less than
0.004. This means that the performance of our
models across 50 random folds does not fluctuate
and hence the system is robust. We use McNe-
mar?s significance test and refer to statistical sig-
nificance as p < 0.05.
6.1 Social event detection (SED) and
classification (SEC)
We report precision (P), recall (R) and F1 measure
for the detection task, and % accuracy for the clas-
sification task. For both these tasks, our previous
best performing system was PET_GR_SqGRW
(which we refer to as AR2010). We use this as
a baseline, and introduce two new baselines: the
bag-of-words (BOW) baseline and a linear com-
bination of BOW and AR2010, referred to as
BOW_AR2010.
Table 3 presents the results for these two tasks
for various features and structures. The results
show that our purely semantic models (RULES,
BOF, FrameTree, FrameTreeProp) do not perform
well alone. FrameForest, which encodes some
lexical and syntactic level features (but is primar-
ily semantic), also performs worse than the base-
lines when used alone. However, a combination
of lexical, syntactic and semantic structures im-
proves the performance by an absolute of 1.1% in
F1-measure for SED (from 0.574 to 0.585). This
gain is statistically significant. For SEC, the abso-
lute gain from our best baseline (BOW_AR2010)
is 0.8% in F1-measure (from 82.3 to 83.1), which
is not statistically significant. However, the gain
of 2% from our previously proposed best system
(AR2010) is statistically significant.
216
SED SEC SNE Hierarchical
Model P R F1 %Acc P R F1
BOW 0.343 0.391 0.365 70.9 0.247 0.277 0.261
AR2010 0.464 0.751 0.574 81.1 0.375 0.611 0.465
BOW_AR2010 0.488 0.645 0.555 82.3 0.399 0.532 0.456
RULES 0.508 0.097 0.164 60.2 0.301 0.059 0.099
BOF 0.296 0.416 0.346 64.4 0.183 0.266 0.217
FrameForest 0.331 0.594 0.425 74.5 0.247 0.442 0.317
FrameTree 0.295 0.594 0.395 68.3 0.206 0.405 0.273
FrameTreeProp 0.308 0.554 0.396 70.7 0.217 0.390 0.279
All 0.494 0.641 0.558 82.5 0.405 0.531 0.460
BOW_AR2010_FrameForest_FrameTreeProp 0.490 0.633 0.552 83.1 0.405 0.528 0.459
AR2010_FrameTreeProp 0.484 0.740 0.585 82.0 0.397 0.608 0.480
Table 3: Results for three tasks: ?SED? is Social Event Detection, ?SEC? is Social Event Classification,
?SNE? is Social Network Extraction. The first three models are the baseline models. The next five
models are the novel semantic features and structures we propose in this paper. ?All? refers to the
model that uses all the listed structures together. ?BOW_AR2010_FrameForest_FrameTreeProp? refers
to the model that uses a linear combination of mentioned structures. AR2010_FrameTreeProp is a linear
combination of AR2010 and FrameTreeProp.
6.2 Social network extraction (SNE)
Social network extraction is a multi-way classifi-
cation task, in which, given an example, we clas-
sify it into one of three categories: {No-Event,
INR, OBS}. A popular technique of performing
multi-way classification using a binary classifier
like SVM, is one-versus-all (OVA). We try this
along with a less commonly used technique, in
which we stack two binary classifiers in a hier-
archy. For the hierarchical design, we train two
models: (1) the SED model ({INR + OBS} ver-
sus No-Event) and (2) the SEC model (INR versus
OBS). Given a test example, it is first classified us-
ing the SED model. If the prediction is less than
zero, we label it as No-Event. Otherwise, the test
example is passed onto SEC and finally classified
into either INR or OBS.
We see that none of the semantic features and
structures alone outperform the baseline. How-
ever, a combination of structures from different
levels of abstraction achieve the best performance:
an absolute gain of 1.5% in F1 (statistically sig-
nificant) when we use a hierarchical design (from
0.465 to 0.480).
Comparing hierarchical verus OVA approaches,
we observe that the hierarchical approach
outperforms the OVA approach for all our
models by a statistically significant margin.
The performance for our best reported model
(AR2010_FrameTreeProp) for OVA in terms
precision, recall, and F1-measure is 0.375, 0.592,
0.459 respectively. This is statistically signifi-
cantly worse than hierarchical approach (0.397,
0.608, 0.480).
6.3 Discussion of results
Performing well on SED is more important than
SEC, because if a social event is not detected in
the first place, the goodness of the SEC model is
irrelevant. Therefore, the best feature and struc-
ture combination we report in this paper is a com-
bination of AR2010 and FrameTreeProp.
To gain insight into the how each type of se-
mantic feature and structure contribute to our
previously proposed lexical and syntactic model
(AR2010), we perform experiments in which we
add one semantic feature/structure at a time to
AR2010. Table 4 presents the results for this
study. We see that the hand-crafted RULES do
not help in the overall task. We investigated the
reason for RULES not being as helpful as we had
expected. We found that when there is no social
event, the rules fire in 7% of the cases. When
there is a social event, they fire in 17% of cases.
So while they fire more often when there is a so-
cial event, the percentage of cases in which they
fire is small. We hypothesize that this is due the
dependence of RULES on the correctness of se-
217
mantic parses. For example, Rule (4) correctly
detects the social event in sentence (5), since Se-
mafor correctly parses the input. In contrast, Se-
mafor does not correctly parse the input sentence
(1): it correctly identifies the Statement frame and
its Message frame element, but it fails to find the
Speaker. As a result, Rule (3) does not fire, even
though the semantic structure is partially identi-
fied. This, we believe, highlights the main strength
of tree kernels ? they are able to learn seman-
tic patterns, without requiring correctness or com-
pleteness of the semantic parse.
Out of the semantic structures we propose,
FrameTreeProp adds the most value to the base-
line system as compared to other semantic features
and structures. This supports our intuition that we
need to reduce unbounded semantic dependencies
between the target entities by propagating the tar-
get entity tags to the top of the semantic tree.
Model SED
(F1)
SEC
(%A)
SNE Hier.
(F1)
AR2010 0.574 81.1 0.465
+ RULES 0.576 80.8 0.465
+ BOF 0.569 80.7 0.459
+ FrameForest 0.571 82.6 0.472
+ FrameTree 0.579 81.5 0.473
+ FrameTreeProp 0.585 82.0 0.480
Table 4: A study to show which semantic features
and structures add the most value to the baseline.
The top row gives the performance of the base-
line. Each consecutive row shows the result of
the baseline plus the feature/structure mentioned
in that row.
7 Related Work
There have been recent efforts to extract net-
works from text (Elson et al., 2010; He et al.,
2013). However, these efforts extract a different
type of network: a network of only bi-directional
links, where the links are triggered by quotation
marks. For example, Elson et al. (2010) and He
et al. (2013) will extract an interaction link be-
tween Emma and Harriet in the following sen-
tence. However, their system will not detect any
interaction links in the other examples mentioned
in this paper.
(6) ?Take it,? said Emma, smiling, and pushing
the paper towards Harriet ?it is for you. Take
your own.?
Our approach to extract and classify social
events builds on our previous work (Agarwal and
Rambow, 2010), which in turn builds on work
from the relation extraction community (Nguyen
et al., 2009). Therefore, the task of relation extrac-
tion is most closely related to the tasks addressed
in this paper. Researchers have used other notions
of semantics in the literature such as latent se-
mantic analysis (Plank and Moschitti, 2013) and
relation-specific semantics (Zelenko et al., 2003;
Culotta and Sorensen, 2004). To the best of our
knowledge, there is only one work that uses frame
semantics for relation extraction (Harabagiu et al.,
2005). Harabagiu et al. (2005) propose a novel se-
mantic kernel that incorporates frame parse infor-
mation in the kernel computation that calculates
similarity between two dependency trees. They,
however, do not propose data representations that
are based on frame parses and the resulting ar-
borescent structures, instead adding features to
syntactic trees. We believe the implicit feature
space of kernels based on our data representation
encode a richer and larger feature space than the
one proposed by Harabagiu et al. (2005).
8 Conclusion and Future Work
This work has only scratched the surface of possi-
bilities for using frame semantic features and tree
structures for the task of social event extraction.
We have shown that tree kernels are well suited to
work with possibly inaccurate semantic parses in
contrast to hand-crafted features that require the
semantic parses to be completely accurate. We
have also extended our previous work by design-
ing and evaluating a full system for social network
extraction.
A more natural data representation for seman-
tic parses is a graph structure. We are actively
exploring the design of semantic graph structures
that may be brought to bear with the use of graph
kernels (Vishwanathan et al., 2010).
Acknowledgments
We would like to thank CCLS?s IT heads, Hatim
Diab and Manoj Pooleery, for providing the infras-
tructure support. This paper is based upon work
supported in part by the DARPA DEFT Program.
The views expressed are those of the authors and
do not reflect the official policy or position of the
Department of Defense or the U.S. Government.
218
References
Apoorv Agarwal and Owen Rambow. 2010. Auto-
matic detection and classification of social events.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1024?1034, Cambridge, MA, October. Association
for Computational Linguistics.
Apoorv Agarwal, Owen C. Rambow, and Rebecca J.
Passonneau. 2010. Annotation scheme for social
network extraction from text. In Proceedings of the
Fourth Linguistic Annotation Workshop.
Apoorv Agarwal, Anup Kotalwar, and Owen Ram-
bow. 2013a. Automatic extraction of social net-
works from literary text: A case study on alice in
wonderland. In the Proceedings of the 6th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2013).
Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and
Owen Rambow. 2013b. Sinnet: Social interaction
network extractor from text. In Sixth International
Joint Conference on Natural Language Processing,
page 33.
Collin F. Baker, J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In 36th Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL?98), pages 86?90,
Montr?al.
Chris Burges. 1998. A tutorial on support vector ma-
chines for pattern recognition. Data mining and
knowledge discovery.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 264?267, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263?270. Asso-
ciation for Computational Linguistics.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
423?429, Barcelona, Spain, July.
David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 138?147.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of 43th Annual Meeting of
the Association for Computational Linguistics.
Sanda Harabagiu, Cosmin Adrian Bejan, and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In International Joint Conference On Ar-
tificial Intelligence.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia at Santa Cruz.
Hua He, Denilson Barbosa, and Grzegorz Kondrak.
2013. Identification of speakers in novels. The
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013).
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22. Association for Computa-
tional Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. Conference on Empirical
Methods in Natural Language Processing.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1498?1507, Sofia, Bulgaria, August.
Association for Computational Linguistics.
SVN Vishwanathan, Nicol N Schraudolph, Risi Kon-
dor, and Karsten M Borgwardt. 2010. Graph ker-
nels. The Journal of Machine Learning Research,
11:1201?1242.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083?1106.
Shubin Zhao and Ralph Grishman. 2005. Extract-
ing relations with integrated information using ker-
nel methods. In Proceedings of the 43rd Meeting of
the ACL.
219
Proceedings of the ACL-HLT 2011 Student Session, pages 111?116,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Social Network Extraction from Texts: A Thesis Proposal
Apoorv Agarwal
Department of Computer Science
Columbia University
apoorv@cs.columbia.edu
Abstract
In my thesis, I propose to build a system that
would enable extraction of social interactions
from texts. To date I have defined a compre-
hensive set of social events and built a prelim-
inary system that extracts social events from
news articles. I plan to improve the perfor-
mance of my current system by incorporating
semantic information. Using domain adapta-
tion techniques, I propose to apply my sys-
tem to a wide range of genres. By extracting
linguistic constructs relevant to social interac-
tions, I will be able to empirically analyze dif-
ferent kinds of linguistic constructs that peo-
ple use to express social interactions. Lastly, I
will attempt to make convolution kernels more
scalable and interpretable.
1 Introduction
Language is the primary tool that people use for es-
tablishing, maintaining and expressing social rela-
tions. This makes language the real carrier of social
networks. The overall goal of my thesis is to build a
system that automatically extracts a social network
from raw texts such as literary texts, emails, blog
comments and news articles. I take a ?social net-
work? to be a network consisting of individual hu-
man beings and groups of human beings who are
connected to each other through various relation-
ships by the virtue of participating in social events.
I define social events to be events that occur be-
tween people where at least one person is aware
of the other and of the event taking place. For ex-
ample, in the sentence John talks to Mary, entities
John and Mary are aware of each other and of the
talking event. In the sentence John thinks Mary is
great, only John is aware of Mary and the event is
the thinking event. My thesis will introduce a novel
way of constructing networks by analyzing text to
capture such interactions or events.
Motivation: Typically researchers construct a so-
cial network from various forms of electronic in-
teraction records like self-declared friendship links,
sender-receiver email links and phone logs etc. They
ignore a vastly rich network present in the content
of such sources. Secondly, many rich sources of
social networks remain untouched simply because
there is no meta-data associated with them (literary
texts, new stories, historical texts). By providing a
methodology for analyzing language to extract in-
teraction links between people, my work will over-
come both these limitations. Moreover, by empiri-
cally analyzing large corpora of text from different
genres, my work will aid in formulating a compre-
hensive linguistic theory about the types of linguistic
constructs people often use to interact and express
their social interactions with others. In the follow-
ing paragraphs I will explicate these impacts.
Impact on current SNA applications: Some
of the current social network analysis (SNA) ap-
plications that utilize interaction meta-data to con-
struct the underlying social network are discussed
by Domingos and Richardson (2003), Kempe et al
(2003), He et al (2006), Rowe et al (2007), Lin-
damood et al (2009), Zheleva and Getoor (2009).
But meta-data captures only part of all the interac-
tions in which people participate. There is a vastly
rich network present in text such as the content of
emails, comment threads on online social networks,
transcribed phone calls. My work will enrich the
111
social network that SNA community currently uses
by complementing it with the finer interaction link-
ages present in text. For example, Rowe et al (2007)
use the sender-receiver email links to connect peo-
ple in the Enron email corpus. Using this network,
they predict the organizational hierarchy of the En-
ron Corporation. Their social network analysis for
calculating centrality measure of people does not
take into account interactions that people talk about
in the content of emails. Such linkages are relevant
to the task for two reasons. First, people talk about
their interactions with other people in the content of
emails. By ignoring these interaction linkages, the
underlying communication network used by Rowe
et al (2007) to calculate various features is incom-
plete. Second, sender-receiver email links only rep-
resent ?who talks to whom?. They do not represent
?who talks about whom to whom.? This later infor-
mation seems to be crucial to the task presumably
because people at the lower organizational hierarchy
are more likely to talk about people higher in the hi-
erarchy. My work will enable extraction of these
missing linkages and hence offers the potential to
improve the performance of currently used SNA al-
gorithms. By capturing alternate forms of commu-
nications, my system will also overcome a known
limitation of the Enron email corpus that a signifi-
cant number of emails were lost at the time of data
creation (Carenini et al, 2005).
Impact on study of literary and journalistic
texts: Sources of social networks that are primar-
ily textual in nature such as literary texts, historical
texts, or news articles are currently under-utilized
for social network analysis. In fact, to the best of
my knowledge, there is no formal comprehensive
categorization of social interactions. An early effort
to illustrate the importance of such linkages is by
Moretti (2005). In his book, Graphs, Maps, Trees:
Abstract Models for a Literary History, Moretti
presents interesting insights into a novel by looking
at its interaction graph. He notes that his models
are incomplete because they neither have a notion
of weight (number of times two characters interact)
nor a notion of direction (mutual or one-directional).
There has been recent work that partially addresses
these concerns (Elson et al, 2010; Celikyilmaz et
al., 2010). They only extract mutual interactions
that are signaled by quoted speech. My thesis will
go beyond quoted speech and will extract interac-
tions signaled by any linguistic means, in particular
verbs of social interaction. Moreover, my research
will not only enable extraction of mutual linkages
(?who talks to whom? ) but also of one-directional
linkages (?who talks about whom?). This will give
rise to new applications such as characterization of
literary texts based on the type of social network that
underlies the narrative. Moreover, analyses of large
amounts of related text such as decades of news ar-
ticles or historical texts will become possible. By
looking at the overall social structure the analyst or
scientist will get a summary of the key players and
their interactions with each other and the rest of net-
work.
Impact on Linguistics: To the best of my knowl-
edge, there is no cognitive or linguistic theory that
explains how people use language to express social
interactions. A system that detects lexical items and
syntactic constructions that realize interactions and
then classifies them into one of the categories, I de-
fine in Section 2, has the potential to provide lin-
guists with empirical data to formulate such a the-
ory. For example, the notion of social interactions
could be added to the FrameNet resource (Baker and
Fillmore, 1998) which is based on frame semantics.
FrameNet records possible semantic frames for lexi-
cal items. Frames describe lexical meaning by speci-
fying a set of frame elements, which are participants
in a typical event or state of affairs expressed by the
frame. It provides lexicographic example annota-
tions that illustrate how frames and frame elements
can be realized by syntactic constructions. My cate-
gorization of social events can be incorporated into
FrameNet by adding new frames for social events
to the frame hierarchy. The data I collect using
the system can provide example sentenctes for these
frames. Linguists can use this data to make gen-
eralizations about linguistic constructions that real-
ize social interactions frames. For example, a pos-
sible generalization could be that transitive verbs in
which both subject and object are people, frequently
express a social event. In addition, it would be in-
teresting to see what kind social interactions occur
in different text genres and if they are realized dif-
ferently. For example, in a news corpus we hardly
found expressions of non-verbal mutual interactions
(like eye-contact) while these are frequent in fiction
112
texts like Alice in Wonderland.
2 Work to date
So far, I have defined a comprehensive set of social
events and have acquired reliable annotations on a
well-known news corpus. I have built a preliminary
system that extracts social events from news articles.
I will now expand on each of these in the following
paragraphs.
Meaning of social events: A text can describe
a social network in two ways: explicitly, by stat-
ing the type of relationship between two individuals
(e.g. Mary is John?s wife), or implicitly, by describ-
ing an event which initiates or perpetuates a social
relationship (e.g. John talked to Mary). I call the
later types of events ?social events? (Agarwal et al,
2010). I defined two broad types of social events:
interaction, in which both parties are aware of each
other and of the social event, e.g., a conversation,
and observation, in which only one party is aware
of the other and of the interaction, e.g., thinking of
or talking about someone. For example, sentence
1, contains two distinct social events: interaction:
Toujan was informed by the committee, and observa-
tion: Toujan is talking about the committee. I have
also defined sub-categories for each of these broad
categories based on physical proximity, verbal and
non-verbal interactions. For details and examples of
these sub-categories please refer to Agarwal et al
(2010)
(1) [Toujan Faisal], 54, {said} [she] was
{informed} of the refusal by an [Interior
Ministry committee] overseeing election
preparations.
As a pilot test to see if creating a social network
based on social events can give insight into the so-
cial structures of a story, I manually annotated a
short version of Alice in Wonderland. On the man-
ually extracted network, I ran social network anal-
ysis algorithms to answer questions like: who are
the most influential characters in the story, which
characters have the same social roles and positions.
The most influential characters in the story were de-
tected correctly. Another finding was that characters
appearing in the same scene like Dodo, Lory, Ea-
glet, Mouse and Duck were assigned the same social
roles and positions. This pointed out the possibility
of using my method to identify separate scenes or
sub-plots in a narrative, which is crucial for a better
understanding of the text under investigation.
Motivated by this pilot test I decided to anno-
tate social events on the Automatic Content Extrac-
tion (ACE) dataset (Doddington et al, 2004), a well
known news corpus. My annotations extend previ-
ous annotations for entities, relations and events that
are present in the 2005 version of the corpus. My an-
notations revealed that about 80% of the times, en-
tities mentioned together in the same sentence were
not linked with any social event. Therefore, a sim-
ple heuristic of connecting entities that are present
in the same sentence with a link will not reveal a
meaningful network. Hence I saw a need for a more
sophisticated analysis.
Extraction of social events: To perform such an
analysis, I built models for two tasks: social event
detection and social event classification (Agarwal
and Rambow, 2010). Both were formulated as bi-
nary tasks: the first one being about detecting ex-
istence of a social event between a pair of entities
in a sentence and the second one being about dif-
ferentiating between the interaction and observation
type events (given there is an event between the en-
tities). I used tree kernels on structures derived from
phrase structure trees and dependency trees in con-
junction with Support Vector Machines (SVMs) to
solve the tasks. For the design of structures and type
of kernel, I took motivation from a system proposed
by Nguyen et al (2009) which is a state-of-the-art
system for relation extraction. I tried all the kernels
and their combinations proposed by Nguyen et al
(2009). I used syntactic and semantic insights to de-
vise a new structure derived from dependency trees
and showed that this plays a role in achieving the
best performance for both social event detection and
classification tasks. The reason for choosing such
representations is motivated by extensive studies
about the regular relation between verb alternations
and meaning components (Levin, 1993; Schuler,
2005). This regularity provides a useful generaliza-
tion that helps to overcome lexical sparseness. How-
ever, in order to exploit such regularities, there is a
need to have access to a representation which makes
the predicate-argument structure clear. Dependency
representations do this. Phrase structure represen-
tations also represent predicate-argument structure,
113
but in an indirect way through the structural config-
urations. These experiments showed that as a result
of how language expresses the relevant information,
dependency-based structures are best suited for en-
coding this information. Furthermore, because of
the complexity of the task, a combination of phrase-
based structures and dependency-based structures
perform the best. To my surprise, the system per-
formed extremely well on a seemingly hard task of
differentiating between interaction and observation
type social events. This result showed that there are
significant clues in the lexical and syntactic struc-
tures that help in differentiating mutual and one-
directional interactions.
3 Future Work
Currently I am working on incorporating semantic
resources to improve the performance of my prelim-
inary system. I will work on making convolution
kernels scalable and interpretable. These two steps
will meet my goal of building a system that will ex-
tract social networks from news articles. My next
step will be to survey and incorporate domain adap-
tation techniques that will allow me port my system
to other genres like literary and historical texts, blog
comments, emails etc. These steps will allow me to
extract social networks from a wide range of textual
data. At the same time I will be able to empirically
analyze the types of linguistic patterns, both lexi-
cal and syntactic, that perpetuate social interactions.
Now I will expand on the aforementioned future di-
rections.
Adding semantic information: Currently I am
exploring linguistically motivated enhancements of
dependency and phrase structure trees to formulate
new kernels. Specifically, I am exploring ways of in-
corporating semantic information from VerbNet and
FrameNet. This will help me reduce data sparse-
ness and thus improve my current system. I am
interested in modeling classes of events which are
characterized by the cognitive states of participants?
who is aware of whom. The predicate-argument
structure of verbs can encode much of this infor-
mation very efficiently, and classes of verbs express
their predicate-argument structure in similar ways.
Levin?s verb classes, and Palmer?s VerbNet (Levin,
1993; Schuler, 2005), are based on syntactic simi-
larity between verbs: two verbs are in the same class
if and only if they can realize their arguments in the
same syntactic patterns. By the Levin Hypothesis,
this is because they share meaning elements, and
meaning and syntactic realizations of arguments are
related. However, this does not mean that verbs in
the same Levin or VerbNet class are synonyms; for
example, to deliberate and to play are both in Verb-
Net class meet-36.3-1. But from a social event per-
spective, I am not interested in exact synonymy, and
in fact it is quite possible that what I am interested
in (awareness of the interaction by the event partici-
pants) is the same among verbs of the same VerbNet
class. In this case, VerbNet will provide a useful ab-
straction. Future work will also explore FrameNet,
which provides a different type of semantic abstrac-
tion and explicit semantic relations that are not di-
rectly based on syntactic realizations.
Scaling convolution kernels: Convolution ker-
nels, first proposed by Haussler (1999), are a con-
venient way of ?naturally? combining a variety of
features without having to do fine-grained feature
engineering. Collins and Duffy (2002) presented a
way of successfully using them for NLP tasks such
as parsing and tagging. Since then they have been
used for various NLP tasks such as relation extrac-
tion (Zelenko et al, 2002; Culotta and Jeffrey, 2004;
Nguyen et al, 2009), semantic role labeling (Mos-
chitti et al, 2008), question-answer classification
(Moschitti et al, 2007) etc. Convolution kernels cal-
culate the similarity between two objects, like trees
or strings, by a recursive calculation over the ?parts?
(substrings, subtrees) of objects. This calculation
is usually made computationally efficient by using
dynamic programming. But there are two limita-
tions: 1) the computation is still quadratic and hence
slow and 2) the features (or parts) that are given high
weights at the time of learning remain inaccessible
i.e. interpretability of the model becomes difficult.
One direction I will explore to make convolution
kernels more scalable is the following: The deci-
sion function for the classifier (SVM in dual form)
is given in equation 1 (Burges, 1998, Eq 61). In
this equation, yi denotes the class of the ith support
vector (si), ?i denotes the Lagrange multiplier of
si, K(si, x) denotes the kernel similarity between si
and a test example x, b denotes the bias. The kernel
definition proposed by Collins and Duffy (2002) is
given in equation 2, where hs(T ) is the number of
114
times the sth subtree appears in tree T . The kernel
function K(T1, T2) therefore calculates the similar-
ity between trees T1 and T2 by counting the common
subtrees in them. By combining equations 1 and 2
I get equation 3 which can be re-written as equation
4.
f(x) =
Ns?
i=1
?iyiK(si, x) + b (1)
K(T1, T2) =
?
s
hs(T1)hs(T2) (2)
f(x) =
Ns?
i=1
?iyi
?
s
hs(si)hs(x) (3)
f(x) =
?
s
Ns?
i=1
?iyihs(si)hs(x) (4)
The motivation for exchanging these summation
signs is that the contribution of larger subtrees to
the kernel similarity is strictly less than the contri-
bution of the smaller subtrees. I will investigate the
possibility of approximating the decision function of
SVM without having to compare all subtrees, in par-
ticular large subtrees. I will also investigate if this
summation can be calculated in parallel to make the
calculation more scalable. Pelossof and Ying (2010)
have done recent work on speeding up the Percep-
tron by stopping the evaluation of features at an early
stage if they have high confidence that the example
will be classified correctly. Another relevant work to
improve the scalability of linear classifiers is due to
Clarkson et al (2010). However, to the best of my
knowledge, there is no work that addresses approxi-
mation of kernel evaluation for convolution kernels.
Interpretability of convolution kernels: As
mentioned in the previous paragraph, another dis-
advantage of using convolution kernels is that inter-
pretability of a model is difficult. Recently, Pighin
and Moschitti (2009) proposed an algorithm to lin-
earize convolution kernels. They show that by ef-
ficiently encoding the ?relevant? fragments gener-
ated by tree kernels, it is possible to get insight into
the substructures that were given high weights at the
time of learning a model. But their system currently
returns thousands of such fragments. I will inves-
tigate if there is a way of summarizing these frag-
ments into a meaningful set of syntactic and lexical
classes. By doing so I will be able to empirically see
what types of linguistic constructs are used by peo-
ple to express different types of social interactions
thus aiding in formulating a theory of how people
express social interactions.
Domain adaptation: To be able to extract social
networks from literary and historical texts, I will ex-
plore domain adaptation techniques. A notable work
in this direction is by Daume? III (2007). This work is
especially useful for me because Daume? III presents
a straightforward kernelized version of his domain
adaptation approach which readily fits the machine
learning paradigm I am using for my problem. I will
explore the literature to see if better domain adap-
tation techniques have been suggested since then.
Domain adaptation will conclude my overall goal of
creating a system that can extract social networks
from a wide variety of texts. I will then attempt to
extract social networks from the increasing amount
of text that is becoming machine readable.
Sentiment Analysis:1 A natural step to try once I
have linkages associated with snippets of text is sen-
timent analysis. I will use my previous work (Agar-
wal et al, 2009) on contextual phrase-level senti-
ment analysis to analyze snippets of text and add
polarity to social event linkages. Sentiment analy-
sis will make the social network representation even
richer by indicating if people are connected with
positive, negative or neutral sentiments. This will
not only give us information about the protagonists
and antagonists in the text but will also affect the
analysis of flow of information through the network.
Acknowledgments
This work was funded by NSF grant IIS-0713548. I
would like to thank Dr. Owen Rambow and Daniel
Bauer for useful discussions and feedback.
References
Apoorv Agarwal and Owen Rambow. 2010. Automatic
detection and classification of social events. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing.
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis using
1I do not mention sentiment analysis anywhere else in my
proposal since I will simply use my earlier work.
115
lexical affect scoring and syntactic n-grams. Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 24?32.
Apoorv Agarwal, Owen C. Rambow, and Rebecca J. Pas-
sonneau. 2010. Annotation scheme for social network
extraction from text. In Proceedings of the Fourth Lin-
guistic Annotation Workshop.
C. Baker and C. Fillmore. 1998. The berkeley framenet
project. Proceedings of the 17th international confer-
ence on Computational linguistics, 1.
Chris Burges. 1998. A tutorial on support vector
machines for pattern recognition. Data mining and
knowledge discovery.
G. Carenini, R. T. Ng, and X. Zhou. 2005. Scalable dis-
covery of hidden emails from large folders. Proceed-
ing of the eleventh ACM SIGKDD international con-
ference on Knowledge discovery in data mining, pages
544?549.
Asli Celikyilmaz, Dilek Hakkani-Tur, Hua He, Greg
Kondrak, and Denilson Barbosa. 2010. The actor-
topic model for extracting social networks in literary
narrative. NIPS Workshop: Machine Learning for So-
cial Computing.
K. L. Clarkson, E. Hazan, and D. P. Woodruff. 2010.
Sublinear optimization for machine learning. 51st An-
nual IEEE Symposium on Foundations of Computer
Science, pages 449 ?457.
M. Collins and N. Duffy. 2002. Convolution kernels for
natural language. In Advances in neural information
processing systems.
Aron Culotta and Sorensen Jeffrey. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42ndMeeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 423?429,
Barcelona, Spain, July.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The automatic
content extraction (ace) program?tasks, data, and eval-
uation. LREC, pages 837?840.
P. Domingos and M. Richardson. 2003. Mining the net-
work value of customers. In Proceedings of the 7th In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 57?66.
David K. Elson, Nicholas Dames, and Kathleen R. McK-
eown. 2010. Extracting social networks from literary
fiction. Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
138?147.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of California
at Santa Cruz.
Jianming He, Wesley W. Chu, and Zhenyu (Victor) Liu.
2006. Inferring privacy information from social net-
works. Intelligence and Security Informatics, pages
154?165.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. Annual Meeting-Association For Computational
Linguistics.
D. Kempe, J. Kleinberg, and E. Tardos. 2003. Maximiz-
ing the spread of influence through a social network.
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 137?146.
Beth Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation. The University of
Chicago Press.
J. Lindamood, R. Heatherly, M. Kantarcioglu, and
B. Thuraisingham. 2009. Inferring private informa-
tion using social network dataset. WWW.
Franco Moretti. 2005. Graphs, Maps, Trees: Abstract
Models for a Literary History. Verso.
A. Moschitti, S. Quarteroni, and R. Basili. 2007. Ex-
ploiting syntactic and shallow semantic kernels for
question answer classification. Proceedings of the
45th Conference of the Association for Computational
Linguistics (ACL).
A. Moschitti, D. Pighin, and R. Basili. 2008. Tree ker-
nels for semantic role labeling. Computational Lin-
guistics, 34.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Conference on Empirical Methods
in Natural Language Processing.
Raphael Pelossof and Zhiliang Ying. 2010. The attentive
perceptron. CoRR, abs/1009.5972.
D. Pighin and A. Moschitti. 2009. Reverse engineering
of tree kernel feature spaces. Proceedings of the Con-
ference on EMNLP, pages 111?120.
Ryan Rowe, German Creamer, Shlomo Hershkop, and
Salvatore J Stolfo. 2007. Automated social hierar-
chy detection through email network analysis. Pro-
ceedings of the 9th WebKDD and 1st SNA-KDD 2007
workshop on Web mining and social network analysis,
pages 109?117.
Karin Kipper Schuler. 2005. Verbnet: a broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
AAI3179808.
D. Zelenko, C. Aone, and A. Richardella. 2002. Kernel
methods for relation extraction. In Proceedings of the
EMNLP.
Elena Zheleva and Lise Getoor. 2009. To join or not
to join: the illusion of privacy in social networks with
mixed public and private user profiles. Proceedings of
the 18th international conference on World wide web,
pages 531?540.
116
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 161?165,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comprehensive Gold Standard for the Enron Organizational Hierarchy
Apoorv Agarwal1* Adinoyi Omuya1** Aaron Harnly2? Owen Rambow3?
1 Department of Computer Science, Columbia University, New York, NY, USA
2 Wireless Generation Inc., Brooklyn, NY, USA
3 Center for Computational Learning Systems, Columbia University, New York, NY, USA
* apoorv@cs.columbia.edu ** awo2108@columbia.edu
?aaron@cs.columbia.edu ?rambow@ccls.columbia.edu
Abstract
Many researchers have attempted to predict
the Enron corporate hierarchy from the data.
This work, however, has been hampered by
a lack of data. We present a new, large, and
freely available gold-standard hierarchy. Us-
ing our new gold standard, we show that a
simple lower bound for social network-based
systems outperforms an upper bound on the
approach taken by current NLP systems.
1 Introduction
Since the release of the Enron email corpus, many
researchers have attempted to predict the Enron cor-
porate hierarchy from the email data. This work,
however, has been hampered by a lack of data about
the organizational hierarchy. Most researchers have
used the job titles assembled by (Shetty and Adibi,
2004), and then have attempted to predict the rela-
tive ranking of two people?s job titles (Rowe et al,
2007; Palus et al, 2011). A major limitation of the
list compiled by Shetty and Adibi (2004) is that it
only covers those ?core? employees for whom the
complete email inboxes are available in the Enron
dataset. However, it is also interesting to determine
whether we can predict the hierarchy of other em-
ployees, for whom we only have an incomplete set
of emails (those that they sent to or received from
the core employees). This is difficult in particular
because there are dominance relations between two
employees such that no email between them is avail-
able in the Enron data set. The difficulties with the
existing data have meant that researchers have ei-
ther not performed quantitative analyses (Rowe et
al., 2007), or have performed them on very small
sets: for example, (Bramsen et al, 2011a) use 142
dominance pairs for training and testing.
We present a new resource (Section 3). It is a large
gold-standard hierarchy, which we extracted manu-
ally from pdf files. Our gold standard contains 1,518
employees, and 13,724 dominance pairs (pairs of
employees such that the first dominates the second
in the hierarchy, not necessarily immediately). All
of the employees in the hierarchy are email corre-
spondents on the Enron email database, though ob-
viously many are not from the core group of about
158 Enron employees for which we have the com-
plete inbox. The hierarchy is linked to a threaded
representation of the Enron corpus using shared IDs
for the employees who are participants in the email
conversation. The resource is available as a Mon-
goDB database.
We show the usefulness of this resource by inves-
tigating a simple predictor for hierarchy based on
social network analysis (SNA), namely degree cen-
trality of the social network induced by the email
correspondence (Section 4). We call this a lower
bound for SNA-based systems because we are only
using a single simple metric (degree centrality) to
establish dominance. Degree centrality is one of
the features used by Rowe et al (2007), but they
did not perform a quantitative evaluation, and to our
knowledge there are no published experiments us-
ing only degree centrality. Current systems using
natural language processing (NLP) are restricted to
making informed predictions on dominance pairs for
which email exchange is available. We show (Sec-
tion 5) that the upper bound performance of such
161
NLP-based systems is much lower than our SNA-
based system on the entire gold standard. We also
contrast the simple SN-based system with a specific
NLP system based on (Gilbert, 2012), and show that
even if we restrict ourselves to pairs for which email
exchange is available, our simple SNA-based sys-
tems outperforms the NLP-based system.
2 Work on Enron Hierarchy Prediction
The Enron email corpus was introduced by Klimt
and Yang (2004). Since then numerous researchers
have analyzed the network formed by connecting
people with email exchange links (Diesner et al,
2005; Shetty and Adibi, 2004; Namata et al, 2007;
Rowe et al, 2007; Diehl et al, 2007; Creamer et al,
2009). Rowe et al (2007) use the email exchange
network (and other features) to predict the domi-
nance relations between people in the Enron email
corpus. They however do not present a quantitative
evaluation.
Bramsen et al (2011b) and Gilbert (2012) present
NLP based models to predict dominance relations
between Enron employees. Neither the test-set nor
the system of Bramsen et al (2011b) is publicly
available. Therefore, we compare our baseline SNA
based system with that of Gilbert (2012). Gilbert
(2012) produce training and test data as follows: an
email message is labeled upward only when every
recipient outranks the sender. An email message is
labeled not-upward only when every recipient does
not outrank the sender. They use an n-gram based
model with Support Vector Machines (SVM) to pre-
dict if an email is of class upward or not-upward.
They make the phrases (n-grams) used by their best
performing system publicly available. We use their
n-grams with SVM to predict dominance relations
of employees in our gold standard and show that a
simple SNA based approach outperforms this base-
line. Moreover, Gilbert (2012) exploit dominance
relations of only 132 people in the Enron corpus for
creating their training and test data. Our gold stan-
dard has dominance relations for 1518 Enron em-
ployees.
3 The Enron Hierarchy Gold Standard
Klimt and Yang (2004) introduced the Enron email
corpus. They reported a total of 619,446 emails
taken from folders of 158 employees of the Enron
corporation. We created a database of organizational
hierarchy relations by studying the original Enron
organizational charts. We discovered these charts
by performing a manual, random survey of a few
hundred emails, looking for explicit indications of
hierarchy. We found a few documents with organi-
zational charts, which were always either Excel or
Visio files. We then searched all remaining emails
for attachments of the same filetype, and exhaus-
tively examined those with additional org charts. We
then manually transcribed the information contained
in all org charts we found.
Our resulting gold standard has a total of 1518
nodes (employees) which are described as be-
ing in immediate dominance relations (manager-
subordinate). There are 2155 immediate dominance
relations spread over 65 levels of dominance (CEO,
manager, trader etc.) From these relations, we
formed the transitive closure and obtained 13,724
hierarchal relations. For example, if A immediately
dominates B and B immediately dominates C, then
the set of valid organizational dominance relations
are A dominates B, B dominates C and A domi-
nates C. This data set is much larger than any other
data set used in the literature for the sake of predict-
ing organizational hierarchy.
We link this representation of the hierarchy to the
threaded Enron corpus created by Yeh and Harnley
(2006). They pre-processed the dataset by combin-
ing emails into threads and restoring some missing
emails from their quoted form in other emails. They
also co-referenced multiple email addresses belong-
ing to one person, and assigned unique identifiers
and names to persons. Therefore, each person is a-
priori associated with a set of email addresses and
names (or name variants), but has only one unique
identifier. Our corpus contains 279,844 email mes-
sages. These messages belong to 93,421 unique per-
sons. We use these unique identifiers to express our
gold hierarchy. This means that we can easily re-
trieve all emails associated with people in our gold
hierarchy, and we can easily determine the hierar-
chical relation between the sender and receivers of
any email.
The whole set of person nodes is divided into two
parts: core and non-core. The set of core people are
those whose inboxes were taken to create the Enron
162
email network (a set of 158 people). The set of non-
core people are the remaining people in the network
who either send an email to and/or receive an email
from a member of the core group. As expected, the
email exchange network (the network induced from
the emails) is densest among core people (density of
20.997% in the email exchange network), and much
less dense among the non-core people (density of
0.008%).
Our data base is freely available as a MongoDB
database, which can easily be interfaced with using
APIs in various programming languages. For infor-
mation about how to obtain the database, please con-
tact the authors.
4 A Hierarchy Predictor Based on the
Social Network
We construct the email exchange network as fol-
lows. This network is represented as an undirected
weighted graph. The nodes are all the unique em-
ployees. We add a link between two employees if
one sends at least one email to the other (who can
be a TO, CC, or BCC recipient). The weight is
the number of emails exchanged between the two.
Our email exchange network consists of 407,095
weighted links and 93,421 nodes.
Our algorithm for predicting the dominance rela-
tion using social network analysis metric is simple.
We calculate the degree centrality of every node in
the email exchange network, and then rank the nodes
by their degree centrality. Recall that the degree cen-
trality is the proportion of nodes in the network with
which a node is connected. (We also tried eigenvalue
centrality, but this performed worse. For a discus-
sion of the use of degree centrality as a valid indica-
tion of importance of nodes in a network, see (Chuah
and Coman, 2009).) Let CD(n) be the degree cen-
trality of node n, and let DOM be the dominance re-
lation (transitive, not symmetric) induced by the or-
ganizational hierarchy. We then simply assume that
for two people p1 and p2, if CD(p1) > CD(p2),
then DOM(p1,p2). For every pair of people who
are related with an organizational dominance rela-
tion in the gold standard, we then predict which per-
son dominates the other. Note that we do not pre-
dict if two people are in a dominance relation to be-
gin with. The task of predicting if two people are
Type # pairs %Acc
All 13,724 83.88
Core 440 79.31
Inter 6436 93.75
Non-Core 6847 74.57
Table 1: Prediction accuracy by type of predicted organi-
zational dominance pair; ?Inter? means that one element
of the pair is from the core and the other is not; a negative
error reduction indicates an increase in error
in a dominance relation is different and we do not
address that task in this paper. Therefore, we re-
strict our evaluation to pairs of people (p1, p2) who
are related hierarchically (i.e., either DOM(p1,p2) or
DOM(p2,p1) in the gold standard). Since we only
predict the directionality of the dominance relation
of people given they are in a hierarchical relation,1
the random baseline for our task performs at 50%.
We have 13,724 such pairs of people in the gold
standard. When we use the network induced simply
by the email exchanges, we get a remarkably high
accuracy of 83.88% (Table 1). We denote this sys-
tem by SNAG.
In this paper, we also make an observation crucial
for the task of hierarchy prediction, based on the dis-
tinction between the core and the non-core groups
(see Section 3). This distinction is crucial for this
task since by definition the degree centrality mea-
sure (which depends on how accurately the underly-
ing network expresses the communication network)
suffers from missing email messages (for the non-
core group). Our results in table 1 confirm this in-
tuition. Since we have a richer network for the core
group, degree centrality is a better predictor for this
group than for the non-core group.
We also note that the prediction accuracy is by far
the highest for the inter hierarchal pairs. The in-
ter hierarchal pairs are those in which one node is
from the core group of people and the other node
is from the non-core group of people. This is ex-
plained by the fact that the core group was chosen
by law enforcement because they were most likely
to contain information relevant to the legal proceed-
ings against Enron; i.e., the owners of the mailboxes
1This style of evaluation is common (Diehl et al, 2007;
Bramsen et al, 2011b).
163
were more likely more highly placed in the hierar-
chy. Furthermore, because of the network character-
istics described above (a relatively dense network),
the core people are also more likely to have a high
centrality degree, as compared to the non-core peo-
ple. Therefore, the correlation between centrality
degree and hierarchal dominance will be high.
5 Using NLP and SNA
In this section we compare and contrast the per-
formance of NLP-based systems with that of SNA-
based systems on the Enron hierarchy gold standard
we introduce in this paper. This gold standard al-
lows us to notice an important limitation of the NLP-
based systems (for this task) in comparison to SNA-
based systems in that the NLP-based systems require
communication links between people to make a pre-
diction about their dominance relation, whereas an
SNA-based system may predict dominance relations
without this requirement.
Table 2 presents the results for four experiments.
We first determine an upper bound for current NLP-
based systems. Current NLP-based systems pre-
dict dominance relations between a pair of people
by using the language used in email exchanges be-
tween these people; if there is no email exchange,
such methods cannot make a prediction. Let G be
the set of all dominance relations in the gold stan-
dard (|G| = 13, 723). We define T ? G to be
the set of pairs in the gold standard such that the
people involved in the pair in T communicate with
each other. These are precisely the dominance rela-
tions in the gold standard which can be established
using a current NLP-based approach. The number
of such pairs is |T | = 2, 640. Therefore, if we
consider a perfect NLP system that correctly pre-
dicts the dominance of 2, 640 tuples and randomly
guesses the dominance relation of the remaining
11, 084 tuples, the system would achieve an accu-
racy of (2640 + 11084/2)/13724 = 59.61%. We
refer to this number as the upper bound on the best
performing NLP system for the gold standard. This
upper bound of 59.61% for an NLP-based system is
lower (24.27% absolute) than a simple SNA-based
system (SNAG, explained in section 4) that predicts
the dominance relation for all the tuples in the gold
standard G.
As explained in section 2, we use the phrases
provided by Gilbert (2012) to build an NLP-based
model for predicting dominance relations of tuples
in set T ? G. Note that we only use the tu-
ples from the gold standard where the NLP-based
system may hope to make a prediction (i.e. peo-
ple in the tuple communicate via email). This sys-
tem, NLPGilbert achieves an accuracy of 82.37%
compared to the social network-based approach
(SNAT ) which achieves a higher accuracy of
87.58% on the same test set T . This comparison
shows that SNA-based approach out-performs the
NLP-based approach even if we evaluate on a much
smaller part of the gold standard, namely the part
where an NLP-based approach does not suffer from
having to make a random prediction for nodes that
do not comunicate via email.
System Test set # test points %Acc
UBNLP G 13,724 59.61
NLPGilbert T 2604 82.37
SNAT T 2604 87.58
SNAG G 13,724 83.88
Table 2: Results of four systems, essentially comparing
performance of purely NLP-based systems with simple
SNA-based systems.
6 Future Work
One key challenge of the problem of predicting
domination relations of Enron employees based on
their emails is that the underlying network is incom-
plete. We hypothesize that SNA-based approaches
are sensitive to the goodness with which the underly-
ing network represents the true social network. Part
of the missing network may be recoverable by an-
alyzing the content of emails. Using sophisticated
NLP techniques, we may be able to enrich the net-
work and use standard SNA metrics to predict the
dominance relations in the gold standard.
Acknowledgments
We would like to thank three anonymous reviewers
for useful comments. This work is supported by
NSF grant IIS-0713548. Harnly was at Columbia
University while he contributed to the work.
164
References
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011a. Extracting social power rela-
tionships from natural language. In ACL, pages 773?
782. The Association for Computer Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011b. Extracting social power rela-
tionships from natural language. ACL.
Mooi-Choo Chuah and Alexandra Coman. 2009. Iden-
tifying connectors and communities: Understand-
ing their impacts on the performance of a dtn pub-
lish/subscribe system. International Conference on
Computational Science and Engineering (CSE ?09).
Germa?n Creamer, Ryan Rowe, Shlomo Hershkop,
and Salvatore J. Stolfo. 2009. Segmentation
and automated social hierarchy detection through
email network analysis. In Haizheng Zhang, Myra
Spiliopoulou, Bamshad Mobasher, C. Lee Giles, An-
drew Mccallum, Olfa Nasraoui, Jaideep Srivastava,
and John Yen, editors, Advances in Web Mining and
Web Usage Analysis, pages 40?58. Springer-Verlag,
Berlin, Heidelberg.
Christopher Diehl, Galileo Mark Namata, and Lise
Getoor. 2007. Relationship identification for social
network discovery. AAAI ?07: Proceedings of the
22nd National Conference on Artificial Intelligence.
Jana Diesner, Terrill L Frantz, and Kathleen M Carley.
2005. Communication networks from the enron email
corpus it?s always about the people. enron is no dif-
ferent. Computational & Mathematical Organization
Theory, 11(3):201?228.
Eric Gilbert. 2012. Phrases that signal workplace hierar-
chy. In Proceedings of the ACM 2012 conference on
Computer Supported Cooperative Work (CSCW).
Bryan Klimt and Yiming Yang. 2004. Introducing the
enron corpus. In First Conference on Email and Anti-
Spam (CEAS).
Galileo Mark S. Namata, Jr., Lise Getoor, and Christo-
pher P. Diehl. 2007. Inferring organizational titles
in online communication. In Proceedings of the 2006
conference on Statistical network analysis, ICML?06,
pages 179?181, Berlin, Heidelberg. Springer-Verlag.
Sebastian Palus, Piotr Brodka, and Przemys?aw
Kazienko. 2011. Evaluation of organization structure
based on email interactions. International Journal of
Knowledge Society Research.
Ryan Rowe, German Creamer, Shlomo Hershkop, and
Salvatore J Stolfo. 2007. Automated social hierar-
chy detection through email network analysis. Pro-
ceedings of the 9th WebKDD and 1st SNA-KDD 2007
workshop on Web mining and social network analysis,
pages 109?117.
Jitesh Shetty and Jaffar Adibi. 2004. Ex employee
status report. http://www.isi.edu/?adibi/
Enron/Enron_Employee_Status.xls.
Jen Yuan Yeh and Aaron Harnley. 2006. Email thread
reassembly using similarity matching. In Proceedings
of CEAS.
165
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 198?202,
Dublin, Ireland, August 23-24, 2014.
Columbia NLP: Sentiment Detection of Sentences and Subjective Phrases
in Social Media
Sara Rosenthal
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
sara@cs.columbia.edu
Apoorv Agarwal
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
apoorv@cs.columbia.edu
Kathleen McKeown
Dept. of Computer Science
Columbia University
New York, NY 10027, USA
kathy@cs.columbia.edu
Abstract
We present two supervised sentiment de-
tection systems which were used to com-
pete in SemEval-2014 Task 9: Senti-
ment Analysis in Twitter. The first sys-
tem (Rosenthal and McKeown, 2013) clas-
sifies the polarity of subjective phrases as
positive, negative, or neutral. It is tai-
lored towards online genres, specifically
Twitter, through the inclusion of dictionar-
ies developed to capture vocabulary used
in online conversations (e.g., slang and
emoticons) as well as stylistic features
common to social media. The second sys-
tem (Agarwal et al., 2011) classifies entire
tweets as positive, negative, or neutral. It
too includes dictionaries and stylistic fea-
tures developed for social media, several
of which are distinctive from those in the
first system. We use both systems to par-
ticipate in Subtasks A and B of SemEval-
2014 Task 9: Sentiment Analysis in Twit-
ter. We participated for the first time in
Subtask B: Message-Level Sentiment De-
tection by combining the two systems to
achieve improved results compared to ei-
ther system alone.
1 Introduction
In this paper we describe two prior sentiment de-
tection algorithms for social media. Both systems
(Rosenthal and McKeown, 2013; Agarwal et al.,
2011) classify the polarity of sentence phrases and
This work is licensed under a Creative Commons At-
tribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
tweets as positive, negative, or neutral. These al-
gorithms were used to participate in the the expres-
sion level task (Subtask A) and message level task
(Subtask B) of the SemEval-2014 Task 9: Senti-
ment Analysis in Twitter (Rosenthal et al., 2014)
which one of the authors helped organize.
We first show improved results compared to our
participation in the prior year in the expression-
level task (Subtask A) by incorporating a new dic-
tionary and new features into the system. Our fo-
cus this year was on Subtask B which we partici-
pated in for the first time. We integrated two sys-
tems to achieve improved results compared to ei-
ther system alone. Our analysis shows that the first
system performs better on recall while the second
system performs better on precision. We used con-
fidence metrics outputted by the systems to deter-
mine which answer should be used. This resulted
in a slight improvement in the Twitter dataset com-
pared to either system alone. In this rest of this
paper, we discuss related work, the methods for
each system, and experiments and results for each
subtask using the data provided by Semeval-2014
Task 9: Sentiment Analysis in Twitter (Rosenthal
et al., 2014).
2 Related Work
Several recent papers have explored sentiment
analysis in Twitter. Go et al (2009) and Pak
and Paroubek (2010) classify the sentiment of
tweets containing emoticons using n-grams and
POS. Barbosa and Feng (2010) detect sentiment
using a polarity dictionary that includes web vo-
cabulary and tweet-specific social media features.
Bermingham and Smeaton (2010) compare polar-
ity detection in twitter to blogs and movie reviews
using lexical features.
Finally, there is a large amount of related work
198
through the participants of Semeval 2013 Task
2, and Semeval 2014 Task9: Sentiment Analysis
in Twitter (Nakov et al., 2013; Rosenthal et al.,
2014). A full list of teams and results can be found
in the task description papers.
3 Phrased-Based Sentiment Detection
We developed a phrase based sentiment detection
system geared towards Social Media by augment-
ing the state of the art system developed by Agar-
wal et al. (2009) to include additional dictionar-
ies such as Wiktionary and new features such as
word lengthening (e.g. helllllloooo) and emoti-
cons (e.g. :)) (Rosenthal and McKeown, 2013).
We initially evaluated our system through our par-
ticipation in the first Sentiment Analysis in Twitter
task (Nakov et al., 2013). We have improved our
system this year by adding a new dictionary and
additional features.
3.1 Lexicons
We assign a prior polarity score to each word by
using the scores provided by the Dictionary of
Affect in Language (DAL) (Whissel, 1989) aug-
mented with WordNet (Fellbaum, 1998) to im-
prove coverage. We additionally augment it with
Wiktionary, emoticon, and acronym dictionaries
to improve coverage in social media (Rosenthal
and McKeown, 2013). The DAL covers 50.1% of
the vocabulary, 16.5% are proper nouns which we
exclude due to their lack of polarity. WordNet cov-
ers 8.7% of the vocabulary and Wiktionary covers
12.5% of the vocabulary. Finally, 3.6% of the vo-
cabulary are emoticons, acronyms, word length-
ening, and forms of punctuation. 8.6% of the vo-
cabulary is not covered which means we find a
prior polarity for 96.4% of the vocabulary. In ad-
dition to these dictionaries we also use SentiWord-
Net (Baccianella et al., 2010) as a new distinct fea-
ture that is used in addition to the prior polarity
computed from the DAL scores.
3.2 Method
We include POS tags and the top n-gram fea-
tures as described in prior work (Agarwal et al.,
2009; Rosenthal and McKeown, 2013). The DAL
and other dictionaries are used along with a nega-
tion state machine (Agarwal et al., 2009) to deter-
mine the polarity for each word in the sentence.
We include all the features described in the orig-
inal system (Agarwal et al., 2009) such as the
Data Set Majority 2013 2014
Twitter Dev 38.14 77.6 81.5
Twitter Test 42.22 N/A 76.54
Twitter Sarcasm 39.81 N/A 61.76
SMS 31.45 73.3 74.55
LiveJournal 33.42 N/A 78.19
Table 1: A comparison between the 2013 and 2014
results for Subtask A using the SemEval Twitter
training corpus. All results exceed the majority
baseline of the positive class significantly.
DAL scores, polar chunk n-grams, and count of
syntactic chunks with their prior polarity based
on the chunks position. Finally, we include sev-
eral lexical-stylistic features that can occur in all
datasets. We divide these features into two groups,
general: ones that are common across online and
traditional genres (e.g. exclamation points), and
social media: one that are far more common in
online genres (e.g. emoticons). The features are
described in further detail in the precursor to this
work (Rosenthal and McKeown, 2013). Feature
selection was performed using chi-square in Weka
(Hall et al., 2009).
In addition we introduce some new features
that were not used in the prior year. SentiWord-
Net (Baccianella et al., 2010) is a sentiment dic-
tionary built upon WordNet that contains scores
for each word where scores > 0 indicate the word
is positive and scores < 0 indicate the word is neg-
ative. We sum the scores for each word in the
phrase and use this as a single polarity feature.
We found that this feature alone gave us a 2% im-
provement over our best results from last year. We
also include some other minor features: tweet and
phrase length and the position of the phrase within
the tweet.
3.3 Experiments and Results
We ran all of our experiments in Weka (Hall et al.,
2009) using Logistic Regression. We also exper-
imented with other learning methods (e.g. SVM
and Naive Bayes) but found that Logistic Regres-
sion worked the same or better than other methods.
All results are shown using the average F-measure
of the positive and negative class. The results are
compared against the majority baseline of the pos-
itive class. We do not use neutral/objective as the
majority class because it is not included in the av-
erage F-score in the Semeval task.
The full results in the participation of SemEval
2014: Sentiment Analysis in Twitter, Subtask A,
199
are shown in Table 1. Our system outperforms the
majority baseline significantly in all classes. Our
submitted system was trained using 3-way clas-
sification (positive/negative/polarity). It included
all the dictionaries from prior years and the top
100 n-grams with feature selection. In addition,
it included SentiWordNet and the other new fea-
tures added in 2014 which provided a 4% increase
compared to our best results during the prior year
(77.6% to 81.5%) and a rank of 10/20 amongst the
constrained systems which used no external data.
Our results on the new test set is 76.54% for a rank
of 14/20. We do not do well in detecting the po-
larity of phrases in sarcastic tweets. This is consis-
tent with the other teams as sarcastic tweets tend to
have their polarity flipped. The improvements to
our system provided a 1% boost in the SMS data
with a rank of 15/20. Finally, in the LiveJournal
dataset we had an F-Score of 78.19% for a rank of
12/20.
4 Message-Level Sentiment Detection
Our message-level system combines two prior sys-
tems to achieve improved results. The first system
inputs an entire tweet as a ?phrase? to the phrase-
level sentiment detection system described in Sec-
tion 3. The second system is described below.
4.1 Lexicons
The second system (Agarwal et al., 2011) makes
use of two dictionaries distinctive from the other
system: 1) an emoticon dictionary and 2) an
acronym dictionary. The emoticon dictionary was
prepared by hand-labeling 170 emoticons listed on
Wikipedia.
1
For example, :) is labeled as positive
whereas :=( is labeled as negative. Each emoticon
is assigned a label from the following set of labels:
Extremely-positive, Extremely-negative, Positive,
Negative, and Neutral. We compile an acronym
dictionary from an on-line resource.
2
The dictio-
nary has translations for 5,184 acronyms. For ex-
ample, lol is translated to laughing out loud.
4.2 Prior Polarity Scoring
A number of our features are based on prior po-
larity of words. As in the phrase-based system we
too build off of prior work (Agarwal et al., 2009)
by using the DAL and augmenting it with Word-
net. However, we do not follow the earlier method
1
http://en.wikipedia.org/wiki/List of emoticons
2
http://www.noslang.com/
but use it as motivation. We consider words with
with a polarity score (using the pleasantness met-
ric from the DAL) of less than 0.5 as negative,
higher than 0.8 as positive and the rest as neutral.
If a word is not directly found in the dictionary, we
retrieve all synonyms from Wordnet. We then look
for each of the synonyms in the DAL. If any syn-
onym is found in the DAL, we assign the original
word the same pleasantness score as its synonym.
If none of the synonyms is present in the DAL, the
word is not associated with any prior polarity. For
the given data we directly found the prior polar-
ity of 50.1% of the words. We find the polarity of
another 8.7% of the words by using WordNet. So
we find prior polarity of about 58.7% of English
language words.
4.3 Features
We propose a set of 50 features. We calculate these
features for the whole tweet and for the last one-
third of the tweet. In total, we get 100 additional
features. Our features may be divided into three
broad categories: ones that are primarily counts
of various features and therefore the value of the
feature is a natural number ? N. Second, we in-
clude features whose value is a real number ? R.
These are primarily features that capture the score
retrieved from DAL. The third category is features
whose values are boolean ? B. These are bag of
words, presence of exclamation marks and capital-
ized text. Each of these broad categories is divided
into two subcategories: Polar features and Non-
polar features. We refer to a feature as polar if we
calculate its prior polarity either by looking it up in
DAL (extended through WordNet) or in the emoti-
con dictionary. All other features which are not
associated with any prior polarity fall in the Non-
polar category. Each of the Polar and Non-polar
features is further subdivided into two categories:
POS and Other. POS refers to features that cap-
ture statistics about parts-of-speech of words and
Other refers to all other types of features.
A more detailed explanation of the system can
be found in Agarwal et al (2011).
4.4 Combined System
Our analysis showed that the first system performs
better on recall while the second system performs
better on precision. We also found that there were
785 tweets in the development set where one sys-
tem got it correct and the other one got it incorrect.
This leaves room for a significant improvement
200
Experiment Twitter SMS LiveJournal
Dev Test Sarcasm
Majority 29.19 34.64 27.73 19.03 27.21
Phrase-Based System 62.09 64.74 40.75 56.86 62.22
Tweet-Level System 62.4 63.73 42.41 60.54 69.44
Combined System 64.6 65.42 40.02 59.84 68.79
Table 2: A comparison between the different systems using the Twitter training corpus provided by the
SemEval task for Subtask B. All results exceed the majority baseline of the positive class significantly.
compared to using each system independently. We
combined the two systems for the evaluation by
using the confidence provided by the phrase-based
system. If the phrase-based system was < 70%
confident we use the message-level system.
4.5 Experiments and Results
This task was evaluated on the Twitter dataset pro-
vided by Semeval-2013 Task 2, Subtask B. All re-
sults are shown using the average F-measure of the
positive and negative class. The full results in the
participation of SemEval 2014: Sentiment Anal-
ysis in Twitter, Subtask B, are shown in Table 2.
All the results outperform the majority baseline of
the more prominent positive polarity class signifi-
cantly. The combined system outperforms the in-
dividual systems for the Twitter development and
test set. It does not outperform the sarcasm test set,
but this may be due to the small size; it contains
only 100 tweets. The Tweet-Level system outper-
forms the phrase-based and combined system for
the LiveJournal and SMS test sets. A closer look at
the results indicated that the phrase-based system
has particular difficulty with the short sentences
which are more common in SMS and LiveJour-
nal. For example, the average number of charac-
ters in a tweet is 120 whereas it is 95.6 in SMS
messages (Nakov et al., 2013). Short sentences
are harder because there are fewer polarity words
which causes the phrase-based system to incor-
rectly pick neutral. In addition, short sentences are
harder because the BOW feature space, which is
huge and already sparse, becomes sparser and in-
dividual features start to over-fit. Part of this prob-
lem is handled by using Senti-features so the space
will be less sparse.
Our ranking in the Twitter 2013 and SMS 2013
development data is 18/50 and 20/50 respectively.
Our rank in the Twitter 2014 test set is 15/50 and
our rank in the LiveJournal test set is 19/50. Based
on our rankings it is clear that our systems are
geared more towards Twitter than other social me-
dia. Finally our ranking in the Sarcasm test set is
41/50. Although this ranking is quite low, it is in
fact encouraging. It indicates that the sarcasm has
switched the polarity of the tweet. In the future we
would like to include a system (e.g. (Gonz?alez-
Ib?a?nez et al., 2011)) that can detect whether the
tweet is sarcastic.
5 Discussion and Future Work
We participated in Semeval-2014 Task 9: Senti-
ment Analysis in Twitter Subtasks A and B. In
Subtask A, we show that adding additional fea-
tures related to location and using SentiWord-
Net gives us improvement compared to our prior
system. In Subtask B, we show that combining
two systems achieves slight improvements over
using either system alone. Combining the two
system achieves greater coverage as the systems
use different emoticon and acronym dictionar-
ies and the phrase-based system uses Wiktionary.
The message-level system is geared toward entire
tweets whereas the phrase-based is geared toward
phrases (even though, in this case we consider the
entire tweet to be a ?phrase?). This is reflective in
several features, such as the position of the target
phrase and the syntactic chunk scores in the phrase
based system and the features related to the last
third of the tweet in the message-level system. In
the future, we?d like to perform an error analysis to
determine the source of our errors and specific ex-
amples of the kind of differences found in the two
systems. Finally, we have found that at times the
scores of the DAL do not line up with polarity in
social media. Therefore, we would like to explore
including more sentiment dictionaries instead of,
or in addition to, the DAL.
6 Acknowledgements
This research was funded by the DARPA DEFT
Program. All statements of fact, opinion or con-
clusions contained herein are those of the authors
and should not be construed as representing the
official views, policies, or positions of the Depart-
ment of Defense, or the U.S. Government.
201
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-
own. 2009. Contextual phrase-level polarity anal-
ysis using lexical affect scoring and syntactic n-
grams. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL ?09, pages 24?32, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
analysis of twitter data. In Proceedings of the Work-
shop on Language in Social Media (LSM 2011),
pages 30?38, Portland, Oregon, June. Association
for Computational Linguistics.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In COLING (Posters), pages 36?44.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: is brevity an advan-
tage? In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, CIKM, pages 1833?1836. ACM.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1?6.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in
twitter: A closer look. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies:
Short Papers - Volume 2, HLT ?11, pages 581?586,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan
Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC?10), Valletta, Malta, may. European
Language Resources Association (ELRA).
Sara Rosenthal and Kathleen McKeown. 2013.
Columbia nlp: Sentiment detection of subjective
phrases in social media. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 478?482, Atlanta, Georgia, USA, June. Asso-
ciation for Computational Linguistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. Semeval-2014 task 9: Sen-
timent analysis in twitter. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014), Dublin, Ireland, August. The COL-
ING 2014 Organizing Committee.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors,
Emotion: theory research and experience, volume 4,
London. Acad. Press.
202
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 20?28,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Annotation Scheme for Social Network Extraction from Text
Apoorv Agarwal
Computer Science Department
Columbia University
New York, U.S.A.
apoorv@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, U.S.A.
rambow@ccls.columbia.edu
Rebecca J. Passonneau
CCLS
Columbia University
New York, U.S.A.
becky@cs.columbia.edu
Abstract
We are interested in extracting social net-
works from text. We present a novel an-
notation scheme for a new type of event,
called social event, in which two people
participate such that at least one of them
is cognizant of the other. We compare
our scheme in detail to the ACE scheme.
We perform a detailed analysis of inter-
annotator agreement, which shows that
our annotations are reliable.
1 Introduction
Our task is to extract a social network from written
text. The extracted social network can be used for
various applications such as summarization, ques-
tion answering, or the detection of main charac-
ters in a story. We take a ?social network? to be
a network consisting of individual human beings
and groups of human beings who are connected
to each other through various relationships by the
virtue of participating in events. A text can de-
scribe a social network in two ways: explicitly, by
stating the type of relationship between two indi-
viduals (Example ??); or implicitly, by describ-
ing an event which creates or perpetuates a so-
cial relationship (Example 2). We are interested in
the implicit description of social relations through
events. We will call these types of events so-
cial events. Crucially, many social relations are
described in text largely implicitly, or even en-
tirely implicitly. This paper presents an annotation
project for precisely such social events.
To introduce the terminology and conventions
we use throughout the paper, consider the follow-
ing Example 2. In this example, there are two
entities: Iraqi officials and Timothy McVeigh.
These entities are present in text as nominal
and named entity mentions respectively (within
[. . .]). Furthermore, these entities are related by
an event, whose type we call INR.NONVERBAL-
NEAR (a non-verbal interaction that occurs in
physical proximity), and whose textual mention is
the extent (or span of text) provided money and
training.1
(1) [[Sharif]?s {wife} Tahari Shad Tabussum],
27, (. . .) made no application for bail at the
court, according to local reports. PER-SOC
(2) The suit claims [Iraqi officials] {provided
money and training} to [convicted bomber
Timothy McVeigh] (. . .) INR.Nonverbal-
Near
One question that immediately comes to mind
is how would these annotations be useful? Let
us consider the problem of finding the hierarchy
of people in the Enron Email corpus (Klimt and
Yang, 2004; Diesner et al, 2005). Much work to
solve this problem has focused on using social net-
work analysis algorithms for calculating the graph
theoretical quantities (like degree centrality, clus-
tering coefficient (Wasserman and Faust, 1994))
of people in the email sender-receiver network
(Rowe et al, 2007). Attempts have been made to
incorporate the content of emails usually by us-
ing topic modeling techniques (McCallum et al,
2007; Pathak et al, 2008). These techniques con-
sider a distribution of words in emails to classify
the interaction between people into topics and then
cluster together people that talk about the same
topic. Researchers also map relationships among
individuals based on their patterns of word use
in emails (Keila and Skillicorn, 2005). But these
techniques do not attempt to create an accurate so-
cial network in terms of interaction or cognitive
states of people. In comparison, our data allows
1Throughout this paper we will follow this representation
scheme for examples ? entity mentions will be enclosed in
square brackets [. . .] and relation mentions will be enclosed
in set brackets {. . .}
20
Sender? Receiver Email content
Kate? Sam [Jacob], the City attorney had a couple of questions which [I] will {attempt to
relay} without having a copy of the documents.
Sam? Kate, Mary Can you obtain the name of Glendale?s bond counsel (lawyer?s name, phone
number, email, etc.)?
Kate? Sam Glendale?s City Attorney is Jacob. Please let [me] {know} if [you] need any-
thing else.
Mary? Sam I do not see a copy of an opinion in the file nor have we received one since [I]
{sent} the execution copies of the ISDA to [Jacob].
Kate? Jacob Jacob, could you provide the name, phone number, etc. of your bond council
for our attorney, Sam?
Kate? Sam [I] will {work on this for} [you] - and will be in touch.
Figure 1: An email thread from the Enron Email Corpus. (For space concerns some part of the conversation is removed. The
missing conversation does not affect our discussion.)
Kate
Sam
MaryJacob
Figure 2: Network formed by considering email exchanges
as links. Identical color or shape implies structural equiva-
lence. Only Sam and Mary are structurally equivalent
for such a technique to be created. This is because
our annotations capture interactions described in
the content of the email such as face-to-face meet-
ings, physical co-presence and cognizance.
To explore if this is useful, we analyzed an En-
ron thread which is presented in Figure 1. Fig-
ure 2 shows the network formed when only the
email exchange is considered. It is easy to see
that Sam and Mary are structurally equivalent and
thus have the same role and position in the so-
cial network. When we analyze the content of the
thread, a link gets added between Mary and Ja-
cob since Mary in her email to Sam talks about
sending something to Jacob. This link changes
the roles and positions of people in the network. In
the new network, Figure 3, Kate and Mary appear
structurally equivalent to each other, as do Sam
and Jacob. Furthermore, Mary now emerges as
a more important player than the email exchange
on its own suggests. This rather simple example is
an indication of the degree to which a link may af-
fect the social network analysis results. In emails
where usually a limited number of people are in-
volved, getting an accurate network seems to be
crucial to the hierarchal analysis.
There has been much work in the past on an-
Kate
Sam
MaryJacob
Figure 3: Network formed by augmenting the email ex-
change network above with links that occur in the content of
the emails. Now, Kate and Mary are structurally equivalent,
as are Sam and Jacob.
notating entities, relations and events in free text,
most notably the ACE effort (Doddington et al,
2004). We intend to leverage this work as much
as possible. The task of social network extrac-
tion can be broadly divided into 3 tasks: 1) en-
tity extraction; 2) social relation extraction; 3) so-
cial event extraction. We are only interested in the
third task, social event extraction. For the first two
tasks, we can simply use the annotation guidelines
developed by the ACE effort. Our social events,
however, do not clearly map to the ACE events:
we introduce a comprehensive set of social events
which are very different from the event annotation
that already exists for ACE. This paper is about the
annotation of social events.
The structure of the paper is as follows. In Sec-
tion 2 we present a list of social relations that we
annotate. We also talk about some design deci-
sions and explain why we took them. We com-
pare this annotation to existing annotation, notably
the ACE annotation, in Section 3. In section 4
we present the procedure of annotation. Section 5
gives details of our inter-annotator agreement cal-
culation procedure and shows the inter-annotator
agreement on our task. We conclude in section 6
21
and mention future direction of research.
2 Social Event Annotation
In this section we define the social events that the
annotators were asked to annotate. Here, we are
interested in the meaning of the annotation; de-
tails of the annotation procedure can be found in
Section 4. Note that in this annotation effort, we
do not consider issues related to the truth of the
claims made in the text we are analyzing ? we
are interested in finding social events whether they
are claimed as being true, presented as specula-
tion, or presented as wishful thinking. We assume
that other modules will be able to determine the
factive status of the described social events, and
that social events do not differ from other types of
events in this respect.
A social event is an event in which two or more
entities relate, communicate or are associated such
that for at least one participant, the interaction is
deliberate and conscious. Put differently, at least
one participant must be aware of relating to the
other participant. In this definition, what consti-
tutes a social relation is an aspect of cognitive
state: an agent is aware of being in a particular re-
lation to another agent. While two people passing
each other on a street without seeing each other
may be a nice plot device in a novel, it is not a
social event in our sense, since it does not entail a
social relation.
Following are the four types of social events that
were annotated:2
Interaction event (INR): When both entities
participating in an event have each other in their
cognitive state (i.e., are aware of the social re-
lation) we say they have an INR relation. The
requirement is actually deeper: it extends to the
transitive closure under mutual awareness, what in
the case of belief is called ?mutual belief?. An
INR event could either be of sub-type VERBAL or
NONVERBAL. Note that a verbal interaction event
does not mean that all participants must actively
communicate verbally, it is enough if one partic-
ipant communicates verbally and the others are
aware of this communication.3 Furthermore, the
interaction can be in physical proximity or from a
distance. Therefore, we have further subtypes of
2Details of the annotation guidelines can be found in the
unpublished annotation manual, which we will refer to in the
final version of the paper.
3For this reason we explicitly annotate legal events as
VERBAL because legal interactions usually involve words
INR relation: NEAR and FAR. In all, INR has
four subtypes: VERBAL-NEAR, VERBAL-FAR,
NONVERBAL-NEAR, NONVERBAL-FAR. Con-
sider the following Example (3). In this sen-
tence, our annotators recorded an INR.VERBAL-
FAR between entities Toujan Faisal and the com-
mittee.
(3) [Toujan Faisal], 54, {said} [she] was
{informed} of the refusal by an [Inte-
rior Ministry committee] overseeing election
preparations. INR.Verbal-Far
As is intuitive, if one person informs the other
about something, both have to be cognizant of
each other and of the informing event. Also, the
event of informing involves words, therefore, it is a
verbal interaction. From the context it is not clear
if Toujan was informed personally, in which case
it would be a NEAR relation, or not. We decided
to default to FAR in case the physical proximity is
unclear from the context. We decided this because,
on observation, we found that if the author of the
news article was reporting an event that occurred
in close proximity, the author would explicitly say
so or give an indication. INR is the only relation
which is bi-directional.
Cognition event (COG): When only one person
(out of the two people that are participating in an
event) has the other in his or her cognitive state,
we say there exists a cognition relationship be-
tween entities. Consider the aforementioned Ex-
ample (3). In this sentence, the event said marks
a COG relation between Toujan Faisal and the
committee. This is because, when one person
talks about the other person, the other person must
be present in the first person?s cognitive state.
COG is a directed event from the entity which
has the other entity in its cognitive state to the
other entity. In the example under consideration,
it would be from Toujan Faisal to the committee.
There are no subtypes of this relation.
Physical Proximity event (PPR): We record a
PPR event when both the following conditions
hold: 1) exactly one entity has the other entity in
their cognitive state (this is the same requirement
as that for COG) and 2) both the entities are
physically proximate. Consider the following
Example (4). Here, one can reasonably assume
that Asif Muhammad Hanif was aware of being
in physical proximity to the three people killed,
while the inverse was not necessarily true.
22
(4) [Three people] were killed when (. . .), [Asif
Muhammad Hanif], (. . .), {detonated explo-
sives strapped to [his] body} PPR
PPR is a directed event like COG. There are no
subtypes of this relation. Note that if there exists
a PPR event then of course there would also be
a COG event. In such cases, the PPR event sub-
sumes COG, and we do not separately record a
COG event.
Perception event (PCR): The Perception Rela-
tionship is the distant equivalent of the Physi-
cal Proximity event. The point is not physical
distance; rather, the important ingredient is the
awareness required for PPR, except that physical
proximity is not required, and in fact physical dis-
tance is required. This kind of relationship usually
exists if one entity is watching the other entity on
TV broadcast, listening to him or her on the radio
or using a listening device, or reading about the
other entity in a newspaper or magazine etc. Con-
sider the following Example (5). In this example,
we record a PCR relation between the pair and
the Nepalese babies. This is because, the babies
are of course not aware of the pair. Moreover, the
pair heard about the babies so there is no physical
proximity. It is not COG because there was an ex-
plicit external information source which brought
the babies to the attention of the pair.
(5) [The pair] flew to Singapore last year af-
ter {hearing} of the successful surgery on
[Nepalese babies] [Ganga] and [Jamuna
Shrestha], (. . .). PCR
PCR is a directed event like COG. There are no
subtypes of this relation. Note that if there exists
a PCR event then we do not separately record a
COG event.
Figure 4 represents the series of decisions that
an annotator is required to take before reaching a
terminal node (or an event annotation label). The
interior nodes of the tree represent questions that
annotators answer to progress downwards in the
tree. Each question has a binary answer. For ex-
ample, the first question the annotators answer to
get to the type and subtype of an event is: ?Is
the relation directed (1-way) or bi-directional (2-
way)?? Depending on the answer, they move to
the left or the right in the tree respectively. If its a
2-way relation, then it has to one of the sub-types
of INR because only INR requires that both enti-
ties be aware of each other.
	 ?
Event	 ?Present	 ?Event	 ?Absent	 ?
Verbal	 ?
2-??Way	 ?
Nonverbal	 ?
1-??Way	 ?
Mind	 ?Far	 ?	 ?	 ?	 ?	 ?	 ?Near	 ?
Near	 ? Far	 ?
Near	 ? 	 ?	 ?	 ?Far	 ?
Figure 4: Tree representation of decision points for select-
ing an event type/subtype out of the list of social events. Each
decision point is numbered for easy reference. We refer to
these number later when we present our results. The num-
bers in braces ([. . .]) are the number of examples that reach a
decision point.
3 Comparison Between Social Events
and ACE Annotations
In this section, we compare our annotations
with existing annotation efforts. To the best of
our knowledge, no annotation effort has been
geared towards extracting social events, or to-
wards extracting expressions that convey social
relations in text. The Automated Content Ex-
traction (ACE) annotations are the most similar
to ours because ACE also annotates Person Enti-
ties (PER.Individual, PER.Group), Relations be-
tween people (PER-SOC), and various types of
Events. Our annotation scheme is different, how-
ever, because the focus of our event annotation is
on events that occur only between people. Fur-
thermore, we annotate text that expresses the cog-
nitive states of the people involved, or allows the
annotator to infer it. Therefore, at the top level
of classification we differentiate between events
in which only one entity is cognizant of the other
versus events when both entities are cognizant of
each other. This distinction is, we believe, novel
in event or relation annotation. In the remainder
of this section, we will present statistics and de-
tailed examples to highlight differences between
our event annotations and the ACE event annota-
tions.
The statistics we present are based on 62 docu-
ments from the ACE-2005 corpus that one of our
annotator also annotated.4 Since our event types
and subtypes are not directly comparable to the
4Due to space constraints we do not give statistics for the
other annotator.
23
ACE event types, we say there is a ?match? when
both the following conditions hold:
1. The span of text that represents an event in
the ACE event annotations overlap with ours.
2. The entities participating in the ACE event
are same as the entities participating in our
event.5
Our annotator recorded a total of 212 events
in 62 documents. We found a total of 63 can-
didate ACE events that had at least two Per-
son entities involved. Out of these 63 candi-
date events, 54 match both the aforementioned
conditions and hence our annotations. A clas-
sification of all of the events (those found by
our annotators and the ACE events involving at
least two persons) into our social event categories
and into the ACE categories is given in Fig-
ure 5. The figure shows that the majority of so-
cial events that match the ACE events are of type
INR.VERBAL-NEAR. On analysis, we found that
most of these correspond to the ACE type/subtype
CONTACT.MEET. It should be noted, how-
ever, our type/subtype INR.VERBAL-NEAR has a
broader definition than ACE type/subtype CON-
TACT.MEET, as will become apparent later in this
section. In the following, we discuss the 9 ACE
events that are not social events, and then we dis-
cuss the 158 social events that are not ACE events.
Out of the nine candidate ACE events which did
not match our social event annotation, we found
five are our annotation errors, i.e. when we an-
alyzed manually and looked for ACE events that
did not correspond to our annotations, we found
that our annotator missed these events. The re-
maining four, in contrast, are useful for our dis-
cussion because they highlight the differences in
ACE and our annotation perspectives. This will
become clearer with the following example:
(6) In central Baghdad, [a Reuters cameraman]
and [a cameraman for Spain?s Telecinco]
died when an American tank fired on the
Palestine Hotel
ACE has annotated the above example as an
event of type CONFLICT-ATTACK in which there
are two entities that are of type person: the
Reuters cameraman and the cameraman for
5Recall that our event annotations are between exactly
two entities of type PER.Individual or PER.Group.
Spain?s Telecinco, both of which are arguments
of type ?Victim?. Being an event that has two per-
son entities involved makes the above sentence a
valid candidate (or potential) ACE event that we
match with our annotations. However, it fails to
match our annotations, since we do not annotate
an event in this sentence. The reason is that this
example does not reveal the cognitive states of the
two entities ? we do not know whether one was
aware of the other.
We now discuss social events that are not ACE
events. From Figure 5 we see that most of the
events that did not overlap with ACE event anno-
tations were Cognition (COG) social events. In
the following, our annotator records a COG rela-
tion between Digvijay Singh and Abdul Kalam
(also Atal Behari Vajpayee and Varuna). The
reason is that by virtue of talking about the two
entities, Digvijay Singh?s cognitive state contains
those entities. However, the sentence does not re-
veal the cognitive states of the other two entities
and therefore it is not an INR event. In contrast,
ACE does not have any event annotation for this
sentence.
(7) The Times of India newspaper quoted [Digvi-
jay Singh] as {saying} that [Prime Minister
Atal Behari Vajpayee] and [President Abdul
Kalam] had offended [the Hindu rain God
Varuna] by remaining bachelors. COG
It is easy to see why COG relations are not usu-
ally annotated as ACE events. But it is counter-
intuitive for INR social events not to be annotated
as ACE events. We explain this using Example (3)
in Section 2. Our annotator recorded an INR re-
lation between Toujan Faisal and the commit-
tee (event span: informed). ACE did not record
any event between the two entities.6 This exam-
ple highlights the difference between our defini-
tion of Interaction events and ACE?s definition of
Contact events. For this reason, in Figure 5, 51 of
our INR relations do not overlap with ACE event
categories.
4 Annotation Procedure
We used Callisto (a configurable workbench) (Day
et al, 2004) to annotate the ACE-2005 corpus for
6The ACE event annotated in the sentence is of type
?Personell-Elect? (span election) which is not recorded as an
event between two or more entities and is not relevant here.
24
62 Documents Conflict (5) Contact (32) Justice-* (13) Life (7) Transaction (2) Not FoundAttack Meet Phone-Write Die Divorce Injure Transfer-Money
 INR 
 Verbal  Near (66) 0 26 0 9 0 0 0 0 31 Far (17) 0 0 3 3 0 1 0 0 10
 NonVerbal  Near (14) 3 0 0 0 2 0 0 1 8 Far (3) 0 0 0 0 0 0 0 1 2
 COG (109) 2 0 0 0 1 0 0 0 106
 PPR (2)  0  0  0  0  1  0  1  0  0 
 PCR (1)  0  0  0  0  0  0  0  0  1 
 Errors  0  3  0  1  1  0  0  0 
Figure 5: This table maps the type and subtype of ACE events to our types and subtypes of social events. The columns have
ACE event types and sub-types. The rows represent our social event types and sub-types. The last column is the number of our
events that are not annotated as ACE events. The last row has the number of social events that our annotator missed but are
ACE events.
the social events we defined earlier. The ACE-
2005 corpus has already been annotated for enti-
ties as part of the ACE effort. The entity anno-
tation is therefore not part of this annotation ef-
fort. We hired two annotators. Annotators opened
ACE-2005 files one by one in Callisto. They could
see the whole document at one time (top screen
of Figure 6) with entities highlighted in blue (bot-
tom screen of Figure 6). These entities were only
of type PER.Individual and PER.Group and be-
longed to class SPC. All other ACE entity annota-
tions were removed. The annotators were required
to read the whole document (not just the part that
has entities) and record a social event span (high-
lighted in dark blue in Figure 6), social event type,
subtype and the two participating entities in the
event.
The span of a event mention is the minimum
span of text that best represents the presence of the
type of event being recorded. It can also be viewed
as the span of text that evokes the type of event be-
ing recorded. The span may be a word, a phrase
or the whole sentence. For example, the span in
Example (4) in Section 2 includes strapped to his
body because that confirms the physical proximity
of the two entities. We have, however, not paid
much attention to the annotation of the span, and
will not report inter-annotator agreement on this
part of the annotation. The reason for this is that
we are interested in annotating the underlying se-
mantics; we will use machine learning to find the
linguistics clues to each type of social event, rather
than relying on the annotators? ability to deter-
mine these. Also note that we did not give precise
instructions on which entity mentions to choose
in case of multiple mentions of the same entity.
Again, this is because we are interested in anno-
tating the underlying semantics, and we will rely
on later analysis to determine which mentions par-
ticipate in signaling the annotated social events.
Figure 6: Snapshot of Callisto. Top screen has the text
from a document. Bottom screen has tabs for Entities, Entity
Mentions etc. An annotator selected text said, highlighted
in dark blue, as an event of type COG between Entities with
entity ID E1 and E9.
Both our annotators annotated 46 common doc-
uments. Out these, there was one document that
had no entity annotations, implying no social event
annotation. The average number of entities in the
remaining 45 documents was 6.82 per document,
and the average number of entity mentions per
document was 23.78. The average number of so-
cial events annotated per document by one anno-
25
tator was 3.43, whereas for the other annotator it
was 3.69. In the next section we present our inter-
annotator agreement calculations for these 45 doc-
uments.
5 Inter-annotator Agreement
Annotators consider all sentences that contain at
least two person entities (individuals or group),
but do not always consider all possible labels, or
annotation values. As represented in the decision
tree in Figure 5, many of the labels are conditional.
At each next depth of the tree, the number of in-
stances can become considerably pruned. Due to
the novelty of the annotation task, and the condi-
tional nature of the labels, we want to assess the
reliability of the annotation of each decision point.
For this, we report Cohen?s Kappa (Cohen, 1960)
for each independent decision. We use the stan-
dard formula for Cohen?s Kappa given by:
Kappa =
P (a)? P (e)
1? P (e)
where P (a) is probability of agreement and P (e)
is probability of chance agreement. These proba-
bilities can be calculated from the confusion ma-
trix represented as follows:
In addition, we present the confusion matrix for
each decision point to show the absolute number
of cases considered, and F-measure to show the
proportion of cases agreed upon. For most de-
cision points, the Kappa scores are at or above
the 0.67 threshold recommended by Krippen-
dorff (1980) with F-measures above 0.90. Where
Kappa is low, F-measure remains high. As dis-
cussed below, we conclude that the annotation
schema is reliable.
We note that in the ACE annotation effort, inter-
annotator agreement (IAA) was measured by a
single number, but this number did not take chance
agreement into account: it simply used the eval-
uation metric to compare systems against a gold
standard. Furthermore, this metric is composed
of distinct parts which were weighted in accor-
dance with research goals from year to year, mean-
ing that the results of applying the metric changed
from year to year. We have also performed an
ACE-style IAA evaluation, which we report at the
end of this section.
Figure 7 shows the results for the seven binary
decision points, considered separately. The num-
ber of the decision point in the table corresponds
to the decision points in Figure 4. The (flattened)
confusion matrices in column two present annota-
tor two?s choices by annotator one?s, with positive
agreement in the upper left (cell A) and negative
agreement in the lower right (cell D). In all cases
the cell values on the agreement diagonal (A, D)
are much higher than the cells for disagreement
(B, C). The upper left cell (A) of the matrix for
decision 1 represents the positive agreements on
the presence of a social event (N=133), and these
are the cases considered for decision 2. For the
remaining decisions, agreement is always unbal-
anced towards agreement on the positive cases,
with few negative cases. In the case of decision
4, for example, this reflects the inherent unlike-
lihood of the NONVERBAL-FAR event. In other
cases, it reflects a property of the genre. For ex-
ample, when we apply this annotation schema to
fiction, we find a much higher frequency of phys-
ically proximate events (PPR), corresponding to
the lower left cell (D) of the confusion matrix for
decision 6.
For decision 4 (NONVERBAL-NEAR) and 7
(PCR/COG), kappa scores are low but the con-
fusion matrices and high F-measures demonstrate
that the absolute agreement is very high. Kappa
measures the amount of agreement that would not
have occurred by chance, with values in [-1,1]. For
binary data and two annotators, values of -1 can
occur, indicating that the annotators have perfectly
non-random disagreements. The probability of an
annotation value is estimated by its frequency in
the data (the marginals of the confusion matrix).
It does not measure the actual amount of agree-
ment among annotators, as illustrated by the rows
for decisions 4 and 7. Because NONVERBAL-
FAR is chosen so rarely by either annotator (never
by annotator 2), the likelihood that both annota-
tors will agree on NONVERBAL-NEAR is close to
one. In this case, there is little room for agreement
above chance, hence the Kappa score of zero. We
should point out, however, that this skewness was
revealed from the annotated corpus. We did not
bias our annotators to look for a particular type of
relation.
The five cases of high Kappa and high F-
26
measure indicate aspects of the annotation where
annotators generally agree, and where the agree-
ment is unlikely to be accidental. We conclude that
these aspects of the annotation can be carried out
reliably as independent decisions. The two cases
of low Kappa and high F-measure indicate aspects
of the annotation where, for this data, there is rel-
atively little opportunity for disagreement.
Decision Point Confusion Matrix Kappa F1A B C D
1 (+/- Relation) 133 31 34 245 0.68 0.80
2 (1 or 2 way) 51 8 1 73 0.86 0.91
3 (Verbal/NonV) 40 4 0 7 0.73 0.95
4 (NonV-Near/Far) 6 0 1 0 0.00 0.92
5 (Verbal-Near/Far) 30 1 2 7 0.77 0.95
6 (+/- PPR) 71 0 1 1 0.66 0.99
7 (PCR/COG) 69 1 1 0 -0.01 0.98
Figure 7: This table presents the Inter-annotator agreement
measures. Column 1 is the decision point corresponding to
the decision tree. Column 2 represents a flattened confusion
matrix where A corresponds to top left corner, D corresponds
to the bottom right corner, B corresponds to top right corner
and C corresponds to the bottom left corner of the confusion
matrix. We present values for Cohen?s Kappa in column 3
and F-measure in the last column.
Now, we present a measure of % agreement
for our annotators by using the ACE evaluation
scheme.7 We considered one annotator to be the
gold standard and the other to be a system being
evaluated against the gold standard. For the cal-
culation of this measure we first take the union of
all event spans. As in the ACE evaluation scheme,
we associate penalties with each wrong decision
annotators take about the entities participating in
an event, type and sub-type of an event. Since
these penalties are not public, we assign our own
penalties. We choose penalties that are not biased
towards any particular event type or subtype. We
decide the penalty based on the number of options
an annotator has to consider before taking a cer-
tain decision. For example, we assign a penalty
of 0.5 if one annotator records an event which the
other annotator does not. If annotators disagree
on the relation type, the penalty is 0.25 because
there are four options to select from (INR, COG,
PPR, PCR). Similarly, we assign a penalty of 0.2
7http://www.itl.nist.gov/iad/mig//tests/ace/2007/doc/ace07-
evalplan.v1.3a.pdf
if the annotators disagree on the relation sub-types
(VERBAL-NEAR, VERBAL-FAR, NONVERBAL-
NEAR, NONVERBAL-FAR, No sub-type). We as-
sign a penalty of 0.5 if the annotators disagree on
the participating entities (incorporating the direc-
tionality in directed relations). Using these penal-
ties, we get % agreement of 69.74%. This is a high
agreement rate as compared to that of ACE?s event
annotation, which was reported to be 31.5% at the
ACE 2005 meeting.
6 Conclusion and Future Work
We have presented a new annotation scheme for
extracting social networks from text. We have
argued, social network created by the sender -
receiver links in Enron Email corpus can ben-
efit from social event links extracted from the
content of emails where people talk about their
?implicit? social relations. Our annotation task
is novel in that we are interested in the cogni-
tive states of people: who is aware of interact-
ing with whom, and who is aware of whom with-
out interacting. Though the task requires detec-
tion of events followed by conditional classifica-
tion of events into four types and subtypes, we
achieve high Kappa (0.66-0.86) and F-measure
(0.8-0.9). We also achieve a high global agree-
ment of 69.74% which is inspired by Automated
Content Extraction (ACE) inter-annotator agree-
ment measure. These measures indicate that our
annotations are reliable.
In future work, we will apply our annotation
effort to other genres, including fiction, and to
text from which larger social networks can be
extracted, such as extended journalistic reporting
about a group of people.
Please contact the second author of the paper
about the availability of the corpus.
Acknowledgments
This work was funded by NSF grant IIS-0713548.
We thank Dr. David Day for help with adapting
the annotation interface (Callisto) to our require-
ments. We would like to thank David Elson for
extremely useful discussions and feedback on the
annotation manual and the inter-annotator calcu-
lation scheme. We would also like to thank the
Natural Language Processing group at Columbia
University for their feedback on our classification
of social events.
27
References
Jacob Cohen. 1960. A coeffiecient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
David Day, Chad McHenry, Robyn Kozierok, and Lau-
rel Riek. 2004. Callisto: A configurable annotation
workbench. International Conference on Language
Resources and Evaluation.
Jana Diesner, Terrill L Frantz, and Kathleen M Car-
ley. 2005. Communication networks from the enron
email corpus it?s always about the people. enron is
no different. Computational & Mathematical Orga-
nization Theory, 11(3):201?228.
G Doddington, A Mitchell, M Przybocki, L Ramshaw,
S Strassel, and R Weischedel. 2004. The auto-
matic content extraction (ace) program?tasks, data,
and evaluation. LREC, pages 837?840.
P S Keila and D B Skillicorn. 2005. Structure in the
enron email dataset. Computational & Mathemati-
cal Organization Theory, 11 (3):183?199.
Bryan Klimt and Yiming Yang. 2004. Introducing
the enron corpus. In First Conference on Email and
Anti-Spam (CEAS).
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications.
Andrew McCallum, Xuerui Wang, and Andres
Corrada-Emmanuel. 2007. Topic and role discovery
in social networks with experiments on enron and
academic email. Journal of Artificial Intelligence
Research, 30 (1):249?272.
Nishith Pathak, Colin DeLong, Arindam Banerjee, and
Kendric Erickson. 2008. Social topic models for
community extraction. Proceedings of SNA-KDD.
Ryan Rowe, German Creamer, Shlomo Hershkop, and
Salvatore J Stolfo. 2007. Automated social hi-
erarchy detection through email network analysis.
Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web mining and social network
analysis, pages 109?117.
Stanley Wasserman and Katherine Faust. 1994. Social
Network Analysis: Methods and Applications. New
York: Cambridge University Press.
28
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 30?38,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Sentiment Analysis of Twitter Data
Apoorv Agarwal Boyi Xie Ilia Vovsha Owen Rambow Rebecca Passonneau
Department of Computer Science
Columbia University
New York, NY 10027 USA
{apoorv@cs, xie@cs, iv2121@, rambow@ccls, becky@cs}.columbia.edu
Abstract
We examine sentiment analysis on Twitter
data. The contributions of this paper are: (1)
We introduce POS-specific prior polarity fea-
tures. (2) We explore the use of a tree kernel to
obviate the need for tedious feature engineer-
ing. The new features (in conjunction with
previously proposed features) and the tree ker-
nel perform approximately at the same level,
both outperforming the state-of-the-art base-
line.
1 Introduction
Microblogging websites have evolved to become a
source of varied kind of information. This is due to
nature of microblogs on which people post real time
messages about their opinions on a variety of topics,
discuss current issues, complain, and express posi-
tive sentiment for products they use in daily life. In
fact, companies manufacturing such products have
started to poll these microblogs to get a sense of gen-
eral sentiment for their product. Many times these
companies study user reactions and reply to users on
microblogs. One challenge is to build technology to
detect and summarize an overall sentiment.
In this paper, we look at one such popular mi-
croblog called Twitter and build models for classify-
ing ?tweets? into positive, negative and neutral senti-
ment. We build models for two classification tasks:
a binary task of classifying sentiment into positive
and negative classes and a 3-way task of classi-
fying sentiment into positive, negative and neutral
classes. We experiment with three types of models:
unigram model, a feature based model and a tree
kernel based model. For the feature based model
we use some of the features proposed in past liter-
ature and propose new features. For the tree ker-
nel based model we design a new tree representa-
tion for tweets. We use a unigram model, previously
shown to work well for sentiment analysis for Twit-
ter data, as our baseline. Our experiments show that
a unigram model is indeed a hard baseline achieving
over 20% over the chance baseline for both classifi-
cation tasks. Our feature based model that uses only
100 features achieves similar accuracy as the uni-
gram model that uses over 10,000 features. Our tree
kernel based model outperforms both these models
by a significant margin. We also experiment with
a combination of models: combining unigrams with
our features and combining our features with the tree
kernel. Both these combinations outperform the un-
igram baseline by over 4% for both classification
tasks. In this paper, we present extensive feature
analysis of the 100 features we propose. Our ex-
periments show that features that have to do with
Twitter-specific features (emoticons, hashtags etc.)
add value to the classifier but only marginally. Fea-
tures that combine prior polarity of words with their
parts-of-speech tags are most important for both the
classification tasks. Thus, we see that standard nat-
ural language processing tools are useful even in
a genre which is quite different from the genre on
which they were trained (newswire). Furthermore,
we also show that the tree kernel model performs
roughly as well as the best feature based models,
even though it does not require detailed feature en-
gineering.
We use manually annotated Twitter data for our
30
experiments. One advantage of this data, over pre-
viously used data-sets, is that the tweets are col-
lected in a streaming fashion and therefore represent
a true sample of actual tweets in terms of language
use and content. Our new data set is available to
other researchers. In this paper we also introduce
two resources which are available (contact the first
author): 1) a hand annotated dictionary for emoti-
cons that maps emoticons to their polarity and 2)
an acronym dictionary collected from the web with
English translations of over 5000 frequently used
acronyms.
The rest of the paper is organized as follows. In
section 2, we discuss classification tasks like sen-
timent analysis on micro-blog data. In section 3,
we give details about the data. In section 4 we dis-
cuss our pre-processing technique and additional re-
sources. In section 5 we present our prior polarity
scoring scheme. In section 6 we present the design
of our tree kernel. In section 7 we give details of our
feature based approach. In section 8 we present our
experiments and discuss the results. We conclude
and give future directions of research in section 9.
2 Literature Survey
Sentiment analysis has been handled as a Natural
Language Processing task at many levels of gran-
ularity. Starting from being a document level classi-
fication task (Turney, 2002; Pang and Lee, 2004), it
has been handled at the sentence level (Hu and Liu,
2004; Kim and Hovy, 2004) and more recently at
the phrase level (Wilson et al, 2005; Agarwal et al,
2009).
Microblog data like Twitter, on which users post
real time reactions to and opinions about ?every-
thing?, poses newer and different challenges. Some
of the early and recent results on sentiment analysis
of Twitter data are by Go et al (2009), (Bermingham
and Smeaton, 2010) and Pak and Paroubek (2010).
Go et al (2009) use distant learning to acquire senti-
ment data. They use tweets ending in positive emoti-
cons like ?:)? ?:-)? as positive and negative emoti-
cons like ?:(? ?:-(? as negative. They build mod-
els using Naive Bayes, MaxEnt and Support Vec-
tor Machines (SVM), and they report SVM outper-
forms other classifiers. In terms of feature space,
they try a Unigram, Bigram model in conjunction
with parts-of-speech (POS) features. They note that
the unigram model outperforms all other models.
Specifically, bigrams and POS features do not help.
Pak and Paroubek (2010) collect data following a
similar distant learning paradigm. They perform a
different classification task though: subjective ver-
sus objective. For subjective data they collect the
tweets ending with emoticons in the same manner
as Go et al (2009). For objective data they crawl
twitter accounts of popular newspapers like ?New
York Times?, ?Washington Posts? etc. They re-
port that POS and bigrams both help (contrary to
results presented by Go et al (2009)). Both these
approaches, however, are primarily based on ngram
models. Moreover, the data they use for training and
testing is collected by search queries and is therefore
biased. In contrast, we present features that achieve
a significant gain over a unigram baseline. In addi-
tion we explore a different method of data represen-
tation and report significant improvement over the
unigram models. Another contribution of this paper
is that we report results on manually annotated data
that does not suffer from any known biases. Our
data is a random sample of streaming tweets unlike
data collected by using specific queries. The size
of our hand-labeled data allows us to perform cross-
validation experiments and check for the variance in
performance of the classifier across folds.
Another significant effort for sentiment classifica-
tion on Twitter data is by Barbosa and Feng (2010).
They use polarity predictions from three websites as
noisy labels to train a model and use 1000 manually
labeled tweets for tuning and another 1000 manu-
ally labeled tweets for testing. They however do
not mention how they collect their test data. They
propose the use of syntax features of tweets like
retweet, hashtags, link, punctuation and exclamation
marks in conjunction with features like prior polar-
ity of words and POS of words. We extend their
approach by using real valued prior polarity, and by
combining prior polarity with POS. Our results show
that the features that enhance the performance of our
classifiers the most are features that combine prior
polarity of words with their parts of speech. The
tweet syntax features help but only marginally.
Gamon (2004) perform sentiment analysis on
feeadback data from Global Support Services sur-
vey. One aim of their paper is to analyze the role
31
of linguistic features like POS tags. They perform
extensive feature analysis and feature selection and
demonstrate that abstract linguistic analysis features
contributes to the classifier accuracy. In this paper
we perform extensive feature analysis and show that
the use of only 100 abstract linguistic features per-
forms as well as a hard unigram baseline.
3 Data Description
Twitter is a social networking and microblogging
service that allows users to post real time messages,
called tweets. Tweets are short messages, restricted
to 140 characters in length. Due to the nature of this
microblogging service (quick and short messages),
people use acronyms, make spelling mistakes, use
emoticons and other characters that express special
meanings. Following is a brief terminology associ-
ated with tweets. Emoticons: These are facial ex-
pressions pictorially represented using punctuation
and letters; they express the user?s mood. Target:
Users of Twitter use the ?@? symbol to refer to other
users on the microblog. Referring to other users in
this manner automatically alerts them. Hashtags:
Users usually use hashtags to mark topics. This
is primarily done to increase the visibility of their
tweets.
We acquire 11,875 manually annotated Twitter
data (tweets) from a commercial source. They have
made part of their data publicly available. For infor-
mation on how to obtain the data, see Acknowledg-
ments section at the end of the paper. They collected
the data by archiving the real-time stream. No lan-
guage, location or any other kind of restriction was
made during the streaming process. In fact, their
collection consists of tweets in foreign languages.
They use Google translate to convert it into English
before the annotation process. Each tweet is labeled
by a human annotator as positive, negative, neutral
or junk. The ?junk? label means that the tweet can-
not be understood by a human annotator. A man-
ual analysis of a random sample of tweets labeled
as ?junk? suggested that many of these tweets were
those that were not translated well using Google
translate. We eliminate the tweets with junk la-
bel for experiments. This leaves us with an unbal-
anced sample of 8,753 tweets. We use stratified sam-
pling to get a balanced data-set of 5127 tweets (1709
tweets each from classes positive, negative and neu-
tral).
4 Resources and Pre-processing of data
In this paper we introduce two new resources for
pre-processing twitter data: 1) an emoticon dictio-
nary and 2) an acronym dictionary. We prepare
the emoticon dictionary by labeling 170 emoticons
listed on Wikipedia1 with their emotional state. For
example, ?:)? is labeled as positive whereas ?:=(? is
labeled as negative. We assign each emoticon a label
from the following set of labels: Extremely-positive,
Extremely-negative, Positive, Negative, and Neu-
tral. We compile an acronym dictionary from an on-
line resource.2 The dictionary has translations for
5,184 acronyms. For example, lol is translated to
laughing out loud.
We pre-process all the tweets as follows: a) re-
place all the emoticons with a their sentiment po-
larity by looking up the emoticon dictionary, b) re-
place all URLs with a tag ||U ||, c) replace targets
(e.g. ?@John?) with tag ||T ||, d) replace all nega-
tions (e.g. not, no, never, n?t, cannot) by tag ?NOT?,
and e) replace a sequence of repeated characters by
three characters, for example, convert coooooooool
to coool. We do not replace the sequence by only
two characters since we want to differentiate be-
tween the regular usage and emphasized usage of the
word.
Acronym English expansion
gr8, gr8t great
lol laughing out loud
rotf rolling on the floor
bff best friend forever
Table 1: Example acrynom and their expansion in the
acronym dictionary.
We present some preliminary statistics about the
data in Table 3. We use the Stanford tokenizer (Klein
and Manning, 2003) to tokenize the tweets. We use
a stop word dictionary3 to identify stop words. All
the other words which are found in WordNet (Fell-
baum, 1998) are counted as English words. We use
1http://en.wikipedia.org/wiki/List of emoticons
2http://www.noslang.com/
3http://www.webconfs.com/stop-words.php
32
Emoticon Polarity
:-) :) :o) :] :3 :c) Positive
:D C: Extremely-Positive
:-( :( :c :[ Negative
D8 D; D= DX v.v Extremely-Negative
: | Neutral
Table 2: Part of the dictionary of emoticons
the standard tagset defined by the Penn Treebank for
identifying punctuation. We record the occurrence
of three standard twitter tags: emoticons, URLs and
targets. The remaining tokens are either non English
words (like coool, zzz etc.) or other symbols.
Number of tokens 79,152
Number of stop words 30,371
Number of English words 23,837
Number of punctuation marks 9,356
Number of capitalized words 4,851
Number of twitter tags 3,371
Number of exclamation marks 2,228
Number of negations 942
Number of other tokens 9047
Table 3: Statistics about the data used for our experi-
ments.
In Table 3 we see that 38.3% of the tokens are stop
words, 30.1% of the tokens are found in WordNet
and 1.2% tokens are negation words. 11.8% of all
the tokens are punctuation marks excluding excla-
mation marks which make up for 2.8% of all tokens.
In total, 84.1% of all tokens are tokens that we ex-
pect to see in a typical English language text. There
are 4.2% tags that are specific to Twitter which in-
clude emoticons, target, hastags and ?RT? (retweet).
The remaining 11.7% tokens are either words that
cannot be found in WordNet (like Zzzzz, kewl) or
special symbols which do not fall in the category of
Twitter tags.
5 Prior polarity scoring
A number of our features are based on prior po-
larity of words. For obtaining the prior polarity of
words, we take motivation from work by Agarwal
et al (2009). We use Dictionary of Affect in Lan-
guage (DAL) (Whissel, 1989) and extend it using
WordNet. This dictionary of about 8000 English
language words assigns every word a pleasantness
score (? R) between 1 (Negative) - 3 (Positive). We
first normalize the scores by diving each score my
the scale (which is equal to 3). We consider words
with polarity less than 0.5 as negative, higher than
0.8 as positive and the rest as neutral. If a word is not
directly found in the dictionary, we retrieve all syn-
onyms from Wordnet. We then look for each of the
synonyms in DAL. If any synonym is found in DAL,
we assign the original word the same pleasantness
score as its synonym. If none of the synonyms is
present in DAL, the word is not associated with any
prior polarity. For the given data we directly found
prior polarity of 81.1% of the words. We find po-
larity of other 7.8% of the words by using WordNet.
So we find prior polarity of about 88.9% of English
language words.
6 Design of Tree Kernel
We design a tree representation of tweets to combine
many categories of features in one succinct conve-
nient representation. For calculating the similarity
between two trees we use a Partial Tree (PT) ker-
nel first proposed by Moschitti (2006). A PT ker-
nel calculates the similarity between two trees by
comparing all possible sub-trees. This tree kernel
is an instance of a general class of convolution ker-
nels. Convolution Kernels, first introduced by Haus-
sler (1999), can be used to compare abstract objects,
like strings, instead of feature vectors. This is be-
cause these kernels involve a recursive calculation
over the ?parts? of abstract object. This calculation
is made computationally efficient by using Dynamic
Programming techniques. By considering all possi-
ble combinations of fragments, tree kernels capture
any possible correlation between features and cate-
gories of features.
Figure 1 shows an example of the tree structure
we design. This tree is for a synthesized tweet:
@Fernando this isn?t a great day for playing the
HARP! :). We use the following procedure to con-
vert a tweet into a tree representation: Initialize the
main tree to be ?ROOT?. Then tokenize each tweet
and for each token: a) if the token is a target, emoti-
con, exclamation mark, other punctuation mark, or a
negation word, add a leaf node to the ?ROOT? with
33
VBGfor
EW
POSplayingPOS
STOP
NN dayPOS
EW EW
NN CAPS harp
EXC ||P||STOP
this
||T||
great
ROOT
NOTSTOP EW
is JJ
VB
GG forEW POS
VB
GG f orE
VBGf
orEW VBG ffoff ffrff
Figure 1: Tree kernel for a synthesized tweet: ?@Fernando this isn?t a great day for playing the HARP! :)?
the corresponding tag. For example, in the tree in
Figure 1 we add tag ||T || (target) for ?@Fernando?,
add tag ?NOT? for the token ?n?t?, add tag ?EXC?
for the exclamation mark at the end of the sentence
and add ||P || for the emoticon representing positive
mood. b) if the token is a stop word, we simply add
the subtree ? (STOP (?stop-word?))? to ?ROOT?. For
instance, we add a subtree corresponding to each of
the stop words: this, is, and for. c) if the token is
an English language word, we map the word to its
part-of-speech tag, calculate the prior polarity of the
word using the procedure described in section 5 and
add the subtree (EW (?POS? ?word? ?prior polarity?))
to the ?ROOT?. For example, we add the subtree
(EW (JJ great POS)) for the word great. ?EW? refers
to English word. d) For any other token <token>
we add subtree ?(NE (<token>))? to the ?ROOT?.
?NE? refers to non-English.
The PT tree kernel creates all possible subtrees
and compares them to each other. These subtrees
include subtrees in which non-adjacent branches be-
come adjacent by excising other branches, though
order is preserved. In Figure 1, we show some of
the tree fragments that the PT kernel will attempt to
compare with tree fragments from other trees. For
example, given the tree (EW (JJ) (great) (POS)), the
PT kernel will use (EW (JJ) (great) (POS)), (EW
(great) (POS)), (EW (JJ) (POS)), (EW (JJ) (great)),
(EW (JJ)), (EW (great)), (EW (POS)), (EW), (JJ),
(great), and (POS). This means that the PT tree ker-
nel attempts to use full information, and also ab-
stracts away from specific information (such as the
lexical item). In this manner, it is not necessary to
create by hand features at all levels of abstraction.
7 Our features
We propose a set of features listed in Table 4 for our
experiments. These are a total of 50 type of features.
We calculate these features for the whole tweet and
for the last one-third of the tweet. In total we get
100 additional features. We refer to these features as
Senti-features throughout the paper.
Our features can be divided into three broad cat-
egories: ones that are primarily counts of various
features and therefore the value of the feature is a
natural number ? N. Second, features whose value
is a real number ? R. These are primarily features
that capture the score retrieved from DAL. Thirdly,
features whose values are boolean ? B. These are
bag of words, presence of exclamation marks and
capitalized text. Each of these broad categories is
divided into two subcategories: Polar features and
Non-polar features. We refer to a feature as polar
if we calculate its prior polarity either by looking
it up in DAL (extended through WordNet) or in the
emoticon dictionary. All other features which are
not associated with any prior polarity fall in the Non-
polar category. Each of Polar and Non-polar features
is further subdivided into two categories: POS and
Other. POS refers to features that capture statistics
about parts-of-speech of words and Other refers to
all other types of features.
In reference to Table 4, row f1 belongs to the cat-
egory Polar POS and refers to the count of number
of positive and negative parts-of-speech (POS) in a
tweet, rows f2, f3, f4 belongs to the category Po-
34
lar Other and refers to count of number of negation
words, count of words that have positive and neg-
ative prior polarity, count of emoticons per polarity
type, count of hashtags, capitalized words and words
with exclamation marks associated with words that
have prior polarity, row f5 belongs to the category
Non-Polar POS and refers to counts of different
parts-of-speech tags, rows f6, f7 belong to the cat-
egory Non-Polar Other and refer to count of num-
ber of slangs, latin alphabets, and other words with-
out polarity. It also relates to special terms such as
the number of hashtags, URLs, targets and newlines.
Row f8 belongs to the category Polar POS and cap-
tures the summation of prior polarity scores of words
with POS of JJ, RB, VB and NN. Similarly, row f9
belongs to the category Polar Other and calculates
the summation of prior polarity scores of all words,
row f10 refers to the category Non-Polar Other and
calculates the percentage of tweet that is capitalized.
Finally, row f11 belongs to the category Non-
Polar Other and refers to presence of exclamation
and presence of capitalized words as features.
8 Experiments and Results
In this section, we present experiments and results
for two classification tasks: 1) Positive versus Nega-
tive and 2) Positive versus Negative versus Neutral.
For each of the classification tasks we present three
models, as well as results for two combinations of
these models:
1. Unigram model (our baseline)
2. Tree kernel model
3. 100 Senti-features model
4. Kernel plus Senti-features
5. Unigram plus Senti-features
For the unigram plus Senti-features model, we
present feature analysis to gain insight about what
kinds of features are adding most value to the model.
We also present learning curves for each of the mod-
els and compare learning abilities of models when
provided limited data.
Experimental-Set-up: For all our experiments we
use Support Vector Machines (SVM) and report av-
eraged 5-fold cross-validation test results. We tune
the C parameter for SVM using an embedded 5-fold
cross-validation on the training data of each fold,
i.e. for each fold, we first run 5-fold cross-validation
only on the training data of that fold for different
values of C. We pick the setting that yields the best
cross-validation error and use that C for determin-
ing test error for that fold. As usual, the reported
accuracies is the average over the five folds.
8.1 Positive versus Negative
This is a binary classification task with two classes
of sentiment polarity: positive and negative. We use
a balanced data-set of 1709 instances for each class
and therefore the chance baseline is 50%.
8.1.1 Comparison of models
We use a unigram model as our baseline. Re-
searchers report state-of-the-art performance for
sentiment analysis on Twitter data using a unigram
model (Go et al, 2009; Pak and Paroubek, 2010).
Table 5 compares the performance of three models:
unigram model, feature based model using only 100
Senti-features, and the tree kernel model. We report
mean and standard deviation of 5-fold test accuracy.
We observe that the tree kernels outperform the uni-
gram and the Senti-features by 2.58% and 2.66% re-
spectively. The 100 Senti-features described in Ta-
ble 4 performs as well as the unigram model that
uses about 10,000 features. We also experiment with
combination of models. Combining unigrams with
Senti-features outperforms the combination of ker-
nels with Senti-features by 0.78%. This is our best
performing system for the positive versus negative
task, gaining about 4.04% absolute gain over a hard
unigram baseline.
8.1.2 Feature Analysis
Table 6 presents classifier accuracy and F1-
measure when features are added incrementally. We
start with our baseline unigram model and subse-
quently add various sets of features. First, we add
all non-polar features (rows f5, f6, f7, f10, f11 in Ta-
ble 4) and observe no improvement in the perfor-
mance. Next, we add all part-of-speech based fea-
tures (rows f1, f8) and observe a gain of 3.49% over
the unigram baseline. We see an additional increase
in accuracy by 0.55% when we add other prior po-
larity features (rows f2, f3, f4, f9 in Table 4). From
35
N
Polar
POS # of (+/-) POS (JJ, RB, VB, NN) f1
Other # of negation words, positive words, negative words f2
# of extremely-pos., extremely-neg., positive, negative emoticons f3
# of (+/-) hashtags, capitalized words, exclamation words f4
Non-Polar
POS # of JJ, RB, VB, NN f5
Other # of slangs, latin alphabets, dictionary words, words f6
# of hashtags, URLs, targets, newlines f7
R
Polar
POS For POS JJ, RB, VB, NN,
?
prior pol. scores of words of that POS f8
Other
?
prior polarity scores of all words f9
Non-Polar Other percentage of capitalized text f10
B Non-Polar Other exclamation, capitalized text f11
Table 4: N refers to set of features whose value is a positive integer. They are primarily count features; for example,
count of number of positive adverbs, negative verbs etc. R refers to features whose value is a real number; for example,
sum of the prior polarity scores of words with part-of-speech of adjective/adverb/verb/noun, and sum of prior polarity
scores of all words. B refers to the set of features that have a boolean value; for example, presence of exclamation
marks, presence of capitalized text.
Model Avg. Acc (%) Std. Dev. (%)
Unigram 71.35 1.95
Senti-features 71.27 0.65
Kernel 73.93 1.50
Unigram +
Senti-features
75.39 1.29
Kernel +
Senti-features
74.61 1.43
Table 5: Average and standard deviation for test accuracy
for the 2-way classification task using different models:
Unigram (baseline), tree kernel, Senti-features, unigram
plus Senti-features, and tree kernel plus senti-features.
these experiments we conclude that the most impor-
tant features in Senti-features are those that involve
prior polarity of parts-of-speech. All other features
play a marginal role in achieving the best performing
system. In fact, we experimented by using unigrams
with only prior polarity POS features and achieved a
performance of 75.1%, which is only slightly lower
than using all Senti-features.
In terms of unigram features, we use Information
Gain as the attribute evaluation metric to do feature
selection. In Table 7 we present a list of unigrams
that consistently appear as top 15 unigram features
across all folds. Words having positive or negative
prior polarity top the list. Emoticons also appear as
important unigrams. Surprisingly though, the word
for appeared as a top feature. A preliminary analy-
Features Acc.
F1 Measure
Pos Neg
Unigram baseline 71.35 71.13 71.50
+ f5, f6, f7, f10, f11 70.1 69.66 70.46
+ f1, f8 74.84 74.4 75.2
+ f2, f3, f4, f9 75.39 74.81 75.86
Table 6: Accuracy and F1-measure for 2-way classifica-
tion task using Unigrams and Senti-features. All fi refer
to Table 4 and are cumulative.
Positive words love, great, good, thanks
Negative words hate, shit, hell, tired
Emoticons ||P || (positive emoticon),
||N || (negative emoticon)
Other for, ||U || (URL)
Table 7: List of top unigram features for 2-way task.
sis revealed that the word for appears as frequently
in positive tweets as it does in negative tweets. How-
ever, tweets containing phrases like for you and for
me tend to be positive even in the absence of any
other explicit prior polarity words. Owing to previ-
ous research, the URL appearing as a top feature is
less surprising because Go et al (2009) report that
tweets containing URLs tend to be positive.
8.1.3 Learning curve
The learning curve for the 2-way classification
task is in Figure 2. The curve shows that when lim-
36
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 162
64
66
68
70
72
74
76
Percentage of training data
Ac
cur
acy
 (%
)
 
 
Unigram
Unigram + Our Features
Tree Kernel
Figure 2: Learning curve for two-way classification task.
ited data is used the advantages in the performance
of our best performing systems is even more pro-
nounced. This implies that with limited amount of
training data, simply using unigrams has a critical
disadvantage, while both tree kernel and unigram
model with our features exhibit promising perfor-
mance.
8.2 Positive versus Negative versus Neutral
This is a 3-way classification task with classes
of sentiment polarity: positive, negative and neu-
tral. We use a balanced data-set of 1709 instances
for each class and therefore the chance baseline is
33.33%.
8.2.1 Comparison of models
For this task the unigram model achieves a gain
of 23.25% over chance baseline. Table 8 compares
the performance of our three models. We report
mean and standard deviation of 5-fold test accuracy.
We observe that the tree kernels outperform the un-
igram and the Senti-features model by 4.02% and
4.29% absolute, respectively. We note that this dif-
ference is much more pronounced comparing to the
two way classification task. Once again, our 100
Senti-features perform almost as well as the unigram
baseline which has about 13,000 features. We also
experiment with the combination of models. For
this classification task the combination of tree ker-
nel with Senti-features outperforms the combination
of unigrams with Senti-features by a small margin.
Model Avg. Acc (%) Std. Dev. (%)
Unigram 56.58 1.52
Senti-features 56.31 0.69
Kernel 60.60 1.00
Unigram +
Senti-features
60.50 2.27
Kernel +
Senti-features
60.83 1.09
Table 8: Average and standard deviation for test accuracy
for the 3-way classification task using different models:
Unigram (baseline), tree kernel, Senti-features, unigram
plus Senti-features, and Senti-features plus tree kernels.
This is our best performing system for the 3-way
classification task, gaining 4.25% over the unigram
baseline.
The learning curve for the 3-way classification
task is similar to the curve of the 2-way classifica-
tion task, and we omit it.
8.2.2 Feature Analysis
Table 9 presents classifier accuracy and F1-
measure when features are added incrementally. We
start with our baseline unigram model and subse-
quently add various sets of features. First, we add all
non-polar features (rows f5, f6, f7, f10 in Table 4)
and observe an small improvement in the perfor-
mance. Next, we add all part-of-speech based fea-
tures and observe a gain of 3.28% over the unigram
baseline. We see an additional increase in accuracy
by 0.64% when we add other prior polarity features
(rows f2, f3, f4, f9 in Table 4). These results are in
line with our observations for the 2-way classifica-
tion task. Once again, the main contribution comes
from features that involve prior polarity of parts-of-
speech.
Features Acc.
F1 Measure
Pos Neu Neg
Unigram baseline 56.58 56.86 56.58 56.20
+
f5, f6, f7, f10, f11
56.91 55.12 59.84 55
+ f1, f8 59.86 58.42 61.04 59.82
+ f2, f3, f4, f9 60.50 59.41 60.15 61.86
Table 9: Accuracy and F1-measure for 3-way classifica-
tion task using unigrams and Senti-features.
The top ranked unigram features for the 3-way
37
classification task are mostly similar to that of the
2-way classification task, except several terms with
neutral polarity appear to be discriminative features,
such as to, have, and so.
9 Conclusion
We presented results for sentiment analysis on Twit-
ter. We use previously proposed state-of-the-art un-
igram model as our baseline and report an overall
gain of over 4% for two classification tasks: a binary,
positive versus negative and a 3-way positive versus
negative versus neutral. We presented a comprehen-
sive set of experiments for both these tasks on manu-
ally annotated data that is a random sample of stream
of tweets. We investigated two kinds of models:
tree kernel and feature based models and demon-
strate that both these models outperform the unigram
baseline. For our feature-based approach, we do fea-
ture analysis which reveals that the most important
features are those that combine the prior polarity of
words and their parts-of-speech tags. We tentatively
conclude that sentiment analysis for Twitter data is
not that different from sentiment analysis for other
genres.
In future work, we will explore even richer lin-
guistic analysis, for example, parsing, semantic
analysis and topic modeling.
10 Acknowledgments
Agarwal and Rambow are funded by NSF grant
IIS-0713548. Vovsha is funded by NSF grant
IIS-0916200. We would like to thank NextGen
Invent (NGI) Corporation for providing us with
the Twitter data. Please contact Deepak Mit-
tal (deepak.mittal@ngicorportion.com) about ob-
taining the data.
References
Apoorv Agarwal, Fadi Biadsy, and Kathleen Mckeown.
2009. Contextual phrase-level polarity analysis using
lexical affect scoring and syntactic n-grams. Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 24?32, March.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 36?44.
Adam Bermingham and Alan Smeaton. 2010. Classify-
ing sentiment in microblogs: is brevity an advantage is
brevity an advantage? ACM, pages 1833?1836.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. MIT Press.
Michael Gamon. 2004. Sentiment classification on cus-
tomer feedback data: noisy data, large feature vectors,
and the role of linguistic analysis. Proceedings of the
20th international conference on Computational Lin-
guistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of California
at Santa Cruz.
M Hu and B Liu. 2004. Mining and summarizing cus-
tomer reviews. KDD.
S M Kim and E Hovy. 2004. Determining the sentiment
of opinions. Coling.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pages 423?430.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Ma-
chine Learning.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
Proceedings of LREC.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity analysis using sub-
jectivity summarization based on minimum cuts. ACL.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification of
reviews. ACL.
C M Whissel. 1989. The dictionary of Affect in Lan-
guage. Emotion: theory research and experience,
Acad press London.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recognizing
contextual polarity in phrase level sentiment analysis.
ACL.
38
Workshop on Computational Linguistics for Literature, pages 88?96,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Social Network Analysis of Alice in Wonderland
Apoorv Agarwal1* Augusto Corvalan1** Jacob Jensen1? Owen Rambow2?
1 Department of Computer Science, Columbia University, New York, NY, USA
2 Center for Computational Learning Systems, Columbia University, New York, NY, USA
* apoorv@cs.columbia.edu ** ac3096@columbia.edu
?jej2120@columbia.edu ?rambow@ccls.columbia.edu
Abstract
We present a network analysis of a literary
text, Alice in Wonderland. We build novel
types of networks in which links between
characters are different types of social events.
We show that analyzing networks based on
these social events gives us insight into the
roles of characters in the story. Also, static
network analysis has limitations which be-
come apparent from our analysis. We propose
the use of dynamic network analysis to over-
come these limitations.
1 Introduction
In recent years, the wide availability of digitized lit-
erary works has given rise to a computational ap-
proach to analyzing these texts. This approach has
been used, sometimes in conjunction with more tra-
ditional literary analysis techniques, to better grasp
the intricacies of several literary works. As the
field matured, new approaches and ideas gave rise
to the use of techniques, like social networks, usu-
ally reserved for quantitive fields in order to gain
new insights into the works. Recently, Elson et al
(2010) extracted networks from a corpus of 19th
century texts in order to debunk long standing hy-
potheses from comparative literature (Elson et al,
2010). Moretti (2011) examined a social event net-
work constructed from Hamlet in order to delve
deeper into its infamously dense character network.
While this approach is clearly powerful, it is not
without drawbacks. As Moretti (2011) points out,
undirected and unweighted networks are blunt in-
struments and limited in their use. While, as dis-
cussed below, some researchers have sought to rec-
tify these limitations, few have done so with a strict
and specific rubric for categorizing interactions.
In this paper, we annotate Lewis Carroll?s Alice in
Wonderland using a well-defined annotation scheme
which we have previously developed on newswire
text Agarwal et al (2010). It is well suited to deal
with the aforementioned limitations. We show that
using different types of networks can be useful by al-
lowing us to provide a model for determining point-
of-view. We also show that social networks allow
characters to be categorized into roles based on how
they function in the text, but that this approach is
limited when using static social networks. We then
build and visualize dynamic networks and show that
static networks can distort the importance of char-
acters. By using dynamic networks, we can build a
fuller picture of how each character works in a liter-
ary text.
Our paper uses an annotation scheme that is well-
defined and has been used in previous computational
models that extract social events from news articles
(Agarwal and Rambow, 2010). This computational
model may be adapted to extract these events from
literary texts. However, the focus of this paper is
not to adapt the previously proposed computational
model to a new domain or genre, but to first demon-
strate the usefulness of this annotation scheme for
the analysis of literary texts, and the social networks
derived from it. All results reported in this paper
are based on hand annotation of the text. Further-
more, we are investigating a single text, so that we
do cannot draw conclusions about the usefulness of
our methods for validating theories of literature.
We summarize the contributions of this paper:
? We manually extract a social network from Al-
88
ice in Wonderland based on the definition of so-
cial events as proposed by us in (Agarwal et al,
2010).
? We use static network analysis (in a bottom-up
approach) for creating character sketches. We
show that exploiting the distinction between
different types of social events (interaction and
observation), we are able to gain insights into
the roles characters play in this novel.
? We point certain limitations of the static net-
work analysis and propose the use of dynamic
network analysis for literary texts.
The rest of the paper is organized as follows. In
Section 2, we present previous work. In Section 3,
we present a brief overview of social events. In Sec-
tion 4, we discuss the data and annotation scheme.
In Section 6, we present results on static network
analysis, and results on dynamic network analysis in
Section 7. We conclude and present future direction
of research in Section 8.
2 Literature Review
The power of network analysis in the field of liter-
ature is evidenced by the rapid rise of work and in-
terest in the field in recent years. Network extrac-
tion and analysis has been performed on subjects
as varied as the Marvel universe (Alberich et al,
2002), Les Mise?rables (Newman and Girvan, 2004),
and ancient Greek tragedies (Rydberg-Cox, 2011).
Elson et al (2010) has looked at debunking com-
parative literature theories by examining networks
for sixty 19th-century novels. Elson et al (2010)
used natural language processing techniques to at-
tribute quoted speech to characters in the novels,
and then used this data to create networks that al-
lowed the researchers to make novel observations
about the correlation between setting and the num-
ber of characters. Because the study was limited to
quoted speech, however, a large chunk of interac-
tions (such as non-quoted dialog, observations and
thoughts) were missing from the network and sub-
sequent analysis. Our work specifically addresses
these missed cases, and in that sense our technique
for creating social networks is complementary to
that of Elson et al (2010).
Several other researchers have found network the-
ory to be useful in the study of literature. In his study
of Dicken?s Bleak House, Sack refines the granu-
larity of interaction types by breaking down links
by the purpose of the interaction, differentiating be-
tween conversations meant, for example, for legal
investigation vs. philanthropy. Sack (2006) also ex-
pands on the definition of ties, including face-to-face
interaction as well as what he terms ?weak ties?,
which includes interactions like being involved in
the same legal suit. His links are a hybrid of quanti-
tative and qualitative. Characters are linked by inter-
action, but how these interactions are then classified
are subjective according to Sack (2006). Thus, they
do not follow a strictly defined rubric. Celikyilmaz
et al (2010) have also worked along a similar track,
analyzing networks built based on topical similarity
in actor speech.
A theorist who has grappled with the limitations
of network analysis is Franco Moretti. In Network
Theory Plot Analysis, Moretti (2011) takes a sim-
ilar path as Elson et al (2010), where the act of
speech signifies interaction. Moretti (2011) points
out that his close reading of the network extracted
from Hamlet is limited by several factors. First,
edges are unweighted, giving equal importance to
interactions that are a few words and long, more
involved conversations. Second, edges have no di-
rection, which eliminates who initiated each inter-
action. Moretti (2011) concludes that more rigorous
network analysis tools are needed in order to make
further headway in the field. In this paper we ex-
tract two types networks from Alice in Wonderland,
one directed and the other undirected, both of which
are weighted. We show that indeed discriminating
between uni-directional and bi-directional linkages
gives us insight into the character profiles and their
role in the novel.
Overall, the previous work has primarily focused
on turning time into space, flattening out the action
in order to bring to light something that was ob-
fuscated previously. However, time and its passage
plays a crucial role in literature. Literature is, after
all, built in layers, with successive scenes stacking
up on each other. Texts reveal information not all
at once, like a network, but in spurts. This is not
merely an unfortunate side-effect of the medium, but
a central element that is manipulated by authors and
89
is central in extracting ?meaning? (Perry, 1979).
However, the static social network (SSN) medium
itself is not suited to clearly reveal these changes.
Dynamic social networks (DSN), on the other hand,
can go beyond the summary statistics of SSN. More-
over, because of their flattening effect, SSNs can
lead to inaccurate or inexact information (Berger-
Wolf et al, 2006). The DSN approach has many ap-
plications, from analyzing how terrorist cells evolve
over time (Carley, 2003), to mapping the interac-
tions in the writing community (Perry-Smith and
Shalley, 2003). One of the obstacles to using DSNs
is that they are not as straight-forward to visualize as
SSNs. In this paper, we use a visualization outlined
in Moody et al (2005). While the visualization may
not be novel, to the best of our knowledge, DSNs
have not yet been used to observe networks extracted
from literary texts. Our goal is to push beyond the
limitations of static network analysis of literature by
adding the crucial element it lacks: dynamism.
3 Social Events
A text may describe a social network in two ways:
explicitly, by stating the type of relationship between
two individuals (e.g. Mary is John?s wife), or implic-
itly, by describing an event whose repeated instanti-
ation may lead to a stronger social relationship (e.g.
John talked to Mary). These latter types of events
are called social events (Agarwal et al, 2010). Agar-
wal et al (2010) defined two broad types of social
events: interaction (INR), in which both parties are
aware of each other and of the social event, e.g.,
a conversation, and observation (OBS), in which
only one party is aware of the other and of the inter-
action, e.g., thinking of or talking about someone.
An important aspect of annotating social events is
taking into consideration the intention of the author:
does the author want us to notice an event between
characters or is he/she simply describing a setting of
a plot? Since our definition of social events is based
on cognitive states of characters, as described by the
author, we do not annotate a social event in Exam-
ple (2) below since there is no evidence that either
Alice or the Rabbit are aware of each other. How-
ever, in Example (1), there is clear evidence that Al-
ice notices the Rabbit but there is no evidence that
the Rabbit notices Alice as well. Therefore, there
in only a one-directional social event between these
entities called the observation (OBS) event.
1. (1) Then [Alice] {saw} the [White Rabbit] run
by her. OBS
2. (2) The [White Rabbit] ran by [Alice]. No
social event
Agarwal et al (2010) have defined finer sub-types
of these two coarse types of events. These sub-types
include recording physical proximity of characters,
verbal and non-verbal interactions, recording if the
thought process of thinking about the other entity is
initiated by a previous event or by reading a mag-
azine or other social medium. Many of these sub-
types are irrelevant for this literary text simply be-
cause it does not describe use of technology. There
are no emails being sent (which would be a verbal
interaction which does not happen in close physical
proximity), no one is watching the other on televi-
sion etc. Therefore, for this paper, we only focus
on two broad social event types: interaction versus
observation. For details and examples of other sub-
categories please refer to (Agarwal et al, 2010).
4 Data
We annotate an abridged version of Alice in Wonder-
land from project Gutenberg.1 This version has ten
chapters, 270 paragraphs and 9611 words.
Agarwal et al (2010) trained two annotators to
annotate social events in a well known news corpus ?
Automated Content Extraction (ACE2005, (Walker,
2005)). Once trained, we used one of the annotators
to annotate the same events in Alice in Wonderland.
Unlike the ACE corpus, we did not have previous
gold annotations for entity mentions or mention res-
olution. However, since we are primarily interested
only in social events, we instructed the annotator to
all and only record entity mentions that participate
in a social event.
Since the text is fairly short, the authors of this pa-
per checked the quality of annotations during the an-
notation process. After the annotation process was
complete, one of the authors went over the annota-
tions as an adjudicator. He did not propose deletion
of any annotation. However, he proposed adding a
1http://www.gutenberg.org/ebooks/19551
90
couple of annotations for chapter 3 for the mouse
drying ceremony. In this scene, the mouse instructs
a group of birds to dry themselves. Lewis Carroll
refers to groups of birds using them, they. Our an-
notation manual does not handle such group forma-
tions. Do we introduce a part-of relation and asso-
ciate each bird in the group with the group mention
(marking the group mention as a separate entity) or
not? If yes, and if the group loses one entity (bird
in this case), do we mark another group entity and
associate the remaining birds with this new group
or not? In general, the problem of such groups is
hard and, to the best of our knowledge, not handled
in current entity recognition manuals. We postpone
handling the annotation of such groups for future
work.
Another point that the adjudicator raised, which
is out of scope for our current annotation manual, is
the way of handling cases where one entity interacts
with the other but mistakenly thinking that the entity
is someone else. For example, the Rabbit interacts
with Alice thinking that she is Mary Ann.
5 Social Network Analysis (SNA) metrics
In this section we briefly describe some of the
widely used SNA metrics that we use throughout the
paper for drawing conclusions about the social net-
work of Alice in Wonderland.
Notation: A network or graph, G = (N,E) is
given by a set of nodes in the network, N and a set
of edges, E. G can be represented as an adjacency
matrix A such that Ai,j = I((i, j) ? E). Following
are the metrics we use:
Degree centrality (Newman, 2010): A node?s degree
centrality is equal to the total number of its incoming
and outgoing edges. The number of connections is
often a good proxy for a node?s importance.
In-degree centrality (Newman, 2010): Degree cen-
trality, but summing only a node?s incoming edges.
In the undirected case, this reduces to Degree cen-
trality.
Out-Degree centrality (Newman, 2010): Degree
centrality, but summing only a node?s outgoing
edges. In the undirected case, this reduces to Degree
centrality.
Hubs (Kleinberg, 1999): A node?s hub score is its el-
ement in the largest eigenvector of AA?. This quan-
tifies how well it reliably points to high-scoring au-
thorities. Intuitively, a high Hub score means a good
directory of important nodes.
Authorities (Kleinberg, 1999): A node?s authority
score is its element in the largest Eigenvector of
A?A. This quantifies how much attention it gets
from high-scoring hubs. Intuitively, a high author-
ity score means a node of importance.
6 Static Network Analysis
In this section we present results for static network
analysis of the different types of networks extracted
from Alice in Wonderland. We use a bottom-up ap-
proach. We extract different types of social networks
and look at the profiles of characters based on these
networks and network analysis metrics. We observe
that the profiles of some characters are strikingly dif-
ferent. In this paper, we discuss three characters
whose profiles we found most interesting. We are
able to show that making a distinction between types
of networks based on directionality (who is observ-
ing whom) is indeed useful.
6.1 Data Visualization
We calculate hubs and authority weights of all the
characters in Alice in Wonderland. Since we are
using a bottom-up approach, there is a lot of data
to look at along different dimensions. We develop
a data visualization scheme that makes it easy for
us to compare profiles of characters along different
dimensions and to compare their profiles with each
other.
Following are the different dimensions that we
are interested in: 1) type of network, denoted by
set N = {OBS, INR}, 2) network analysis metric,
denoted by the set M = {Hub weight, Authority
weight}, 3) rank of a character based on type of net-
work and network analysis metric used, denoted by
the set R = {1, 2, 3, . . . 52}, and 4) absolute sepa-
ration of consecutively ranked characters for a par-
ticular network analysis metric, denoted by a con-
tinuous set S = [0, 1]. We need this last dimension
since one character may be ranked higher than an-
other, yet the separation between the absolute values
of the network analysis metric is fairly small. We
treat characters with such small separations in abso-
lute values as having the same rank. There are a to-
91
(a) Alice (b) White Rabbit (c) Mouse
Figure 1: Static networks analysis plots for three characters of Alice in Wonderland. X-axis denotes network types,
OBS, INR, Verbal and Non-verbal (inorder), Y-axis denotes network analysis metrics, Authority weight and Hub
weight. Color coding: Blue = rank 1, Green = rank 2, Red = rank 3 and all other ranks are color Black. Size of the dot
is directly proportional to separation from next lower rank, in terms of the network analysis value.
tal of four dimensions for each character, and a total
of 2 ? 2 ? 52 = 208 data points to look at (ignor-
ing the last dimension, absolute separation from the
consecutively ranked character). We represent these
four dimensions dimensions in a 2-D scatter plot as
follows:
X-axis: We plot the network types along the X-axis.
Y-axis: We plot the network analysis metric along
the Y-axis.
Color: Color of a dot denotes the rank of the char-
acter. We choose the following color coding. Blue
denotes rank one, Green denotes rank two, Red de-
notes rank three and all the remaining ranks are de-
noted by color Black. After rank three the absolute
value of the metrics plummet and are very close to
one another i.e. the separation between absolute val-
ues (of network analysis metrics) for consecutively
ranked characters is less than 0.001.
Size: The size of a dot denotes the fourth dimen-
sion i.e. the absolute separation in network analy-
sis metric of the character under consideration to the
next lower ranked character. For example, in Fig-
ure 1, rank of the Rabbit for network type OBS when
looking at the authority weight is 1 and the sepa-
ration from ranked 2 character, the Mouse, is high,
as denoted by the larger circle. Alternatively, when
looking at rank for Rabbit as a hub for network type
OBS, he is ranked 3, but there is very little separa-
tion between him and the next lowest ranked char-
acter.
This visualization enables us to compare a lot of
numbers conveniently, out of which arise three in-
teresting character profiles. These profiles yield in-
formation as to how each character functions in the
story.
6.2 Point-of-View
Alice: Alice has the highest centrality for every
network which, using the definition of protago-
nist given by Moretti (2011), makes her the pro-
tagonist of the text. However, from our analysis
we are also able to conclude that the story is be-
ing told from Alice?s perspective. Note that pro-
tagonist and perspective-holder are not always the
same. For example, The Great Gatsby is narrated
by Nick Carraway, but the protagonist is Jay Gatsby.
Even though to a reader of the text, the perspective
holder(s) might be easy to identify, to the best of our
knowledge there are no network analysis approaches
that can do this. We show that by treating interac-
tion and observation events in isolation, we are able
to conclude that Alice is the only perspective holder
in the story.
The perspective, or point of view, is the ?mode (or
modes) established by an author by means of which
the reader is presented with the characters, dialog,
92
actions, setting and events? (Abrams, 1999). There
are four of these:
1. First-Person: The story is being told from the
perspective of a narrator that refers to itself as
?I? (or ?we?).
2. Second-Person: Similar to first-person, but the
narrator refers to a character(s) in the story as
?you?. This form of narration is not common.
3. Third-Person Limited: Here, the narrator is
not a character in the story, but an out-
side entity that refers to other characters as
?he/she/it/they?. However, in limited, this en-
tity is limited to one focal character that the
narrator follows.
4. Third-Person Omniscient: A type of third-
person narration where the narrator has access
to the thoughts and actions of multiple charac-
ters.
For first, second and third-person limited, it is
expected that the character who is observing other
characters is the perspective holder. In order to iso-
late observations from mentions, the OBS network
should be built ignoring quoted speech. Computa-
tionally, we believe this would be a fairly easy task.
In terms of the terminology we introduce, the per-
spective holder will have observation links point-
ing to other characters but will not receive observa-
tion links. In a first-person narration, this character
will be an ?I? or a name if the ?I? is named. The
same case for second-person and ?you.? In third-
person limited, while an entity is narrating the story,
there is one focal character whose perspective lim-
its and sometimes colors the narration. Thus, that
character will still be the one with observation links
emanating but not receiving. In third-person omni-
scient, since the narrator has access to every charac-
ter?s thoughts and actions, it is expected that many
characters would receive and emanate observation
links, while there would be an absence of charac-
ters who are emanating observation links but not re-
ceiving any. Therefore, the behavior of perspective
holding character is consistent across different types
of narrations ? it is the character that emanates ob-
servation type of links but does not receive any. This
analysis extends to the case where there are multiple
character perspectives being used by seeing which
characters are sending but not receiving OBS links
and which are not. However, in the rare case where
an actor whose point-of-view is being received over-
hears himself being mentioned, this will be anno-
tated as having him receive a OBS link, thereby
throwing off the categorization. We ignore this rare
case for now.
Looking at hub and authority weights of Alice?s
OBS network (Figure 1(a)), it is apparent that all the
observation links are pointing outwards from Alice.
Alice is ranked one (color of the dot is blue) and
has a high separation from the second ranked en-
tity (size of the dot) for Hub-weight metric. A high
hub-weight rank means that most of the links are
emanating from this character. In comparison, Al-
ice?s authority-weight of OBS network is low. This
means that other characters are not talking about Al-
ice. Thus, the story must be being told from the
point-of-view of Alice.
It should be noted that for concluding who is the
perspective holder, it is important to only look at the
OBS network. The same conclusion cannot be made
if we look at the INR network. This supports our
effort to make a distinction between uni-directional
versus bi-directional links.
6.3 Character Sketch for Minor Characters
White Rabbit: The White Rabbit has a very different
profile when we look at its OBS network in com-
parison to Alice (figure 1(b)). Rabbit is ranked one
but as an authority, instead of as a hub, in the OBS
network. This means that most of the observation
links are leading to Rabbit i.e. Rabbit is being ob-
served or talked about by other characters. On the
other hand Rabbit is ranked third in INR (for which
hub and authority have the same value, since INR
is non-directional). Thus, Rabbit is frequently ob-
served and talked about, yet remains insular in his
interactions with other characters. This suggests that
Rabbit is playing some sort of unique role in the text,
where importance is being placed on his being ob-
served rather than his interactions.
Mouse: Mouse has yet another kind of profile. For
Mouse, both hub and authority weights are ranked
two and have a clear separation from the next ranked
character. We may observe that Mouse not only in-
teracts with many characters, but mentions and is
93
mentioned in abundance as well. This makes him
a very important and well-connected character in
the story, behind only Alice. Thus, we can suggest
that his role in the text is as a connector between
many characters. Mouse mentions many characters
to other characters, interacts with them and is in turn
mentioned by them.
6.4 Need for Dynamic Analysis
The need for a dynamic analysis model is made
clear in the case of Mouse. His huge importance
(overshadowing more traditionally popular charac-
ters such as the Queen and Mad Hatter) was an un-
expected result. However, this is not the whole story:
Mouse actually only appears in one scene in chap-
ters 2-3. In the scene, Alice has created a large lake
with her tears and meets Mouse, who introduces her
to many minor characters during a drying ceremony.
Outside of this ceremony, Mouse does not reappear
in the text. This one scene, while important, should
not be enough to overshadow characters such as the
Queen, who is responsible for Alice?s life or death
during the climax of the text. Thus, it is clear from
the formation of these character profiles that certain
information is being skewed by static network anal-
ysis. Most notably, the importance of time as it flows
in text is being lost. This observation is the impetus
for a new model that addresses these issues, as out-
lined in the following section.
7 Dynamic Network Analysis
Figure 2 presents plots for dynamic network analy-
sis of the different types of networks extracted from
Alice in Wonderland. We look at interaction (INR)
and observation (OBS) networks, as we did for the
previous section, except we do this for each of the
10 chapters independently of all other chapters. The
social network metrics we consider are: degree, in-
degree and out-degree centrality. Note that for an
undirected network (i.e. INR), all three network
analysis metrics are the same. In this section we
present insights about the three characters consid-
ered in the previous section (Alice, Mouse and Rab-
bit), that are lost in static network analysis.
From Figure 2, it is clear that Alice (dotted blue
line) is not the most central character in every chap-
ter, something that is lost in the static network. Con-
sider figure 2(a) i.e. degree centrality of INR net-
work. Alice ranks 2 in chapters 3, 4 (the drying
ceremony mentioned above) and 9. In chapter 9,
Alice is overshadowed by The Hatter and Rabbit.
This makes sense, as this chapter concerns Rabbit
and The Hatter being witnesses at Alice?s trial. By
breaking the story down chapter by chapter like this,
it becomes evident that although Alice is a very ac-
tive character throughout, there are moments, such
as the trial, where she is inactive, indeed powerless.
Yet as soon as the trial is over and Alice is back in
her own world in chapter 10, we see a spike as she
again takes an active role in her fate.
Figure 2(b) shows in-degree centrality for the
OBS network. This represents how often a character
is thought about or talked about by another charac-
ter. Notice that Alice is completely absent in this
network: no one thinks about or mentions her. This
is to be expected, as Alice is our guide through Won-
derland. No one mentions her because she is present
in every scene, thus any dialog about her will be-
come an interaction. Likewise, no one thinks of her
because the reader is not presented with other char-
acter?s thoughts, only Alice?s. This is consistent with
earlier observations made in the static network. In-
terestingly, Queen (solid black line) comes to dom-
inate the later chapters, as she becomes the focus of
Alice?s thoughts and mentions. Again, this spike in
Queen?s influence (Figure 2(b)) is lost in the static
network. But it is Queen who ultimately has the
power to decide the final punishment for Alice at the
end of the trial, so it is fitting that Alice?s thoughts
are fixated with her.
Figure 2(c) shows the out-degree centrality of the
OBS network, a starkly different picture. Here, we
see why Mouse (dashed red line) has such impor-
tance in the static network. Over the course of the
drying ceremony in chapter 2 and 3, he mentions a
very large number of characters. The dynamic net-
work allows us to see that while Mouse does play
a key role at one point of the story, his influence is
largely limited to that one section. Other characters
overshadow him for the rest of the text. Comparing
Mouse?s role in the in-degree centrality graph (fig-
ure 2(b)) vs. out-degree centrality (figure 2(c)), we
can see that much of Mouse?s influence comes not
from entities referring to him (in-degree), but rather
the number of entities he mentions. His importance
94
1 2 3 4 5 6 7 8 9 100
2
4
6
8
10
12
Chapter Number
Indegr
ee Ce
ntrality
Network TypeINR
 
 AliceMouseQueenRabbitHatter
(a) Degree centrality measure for INR
network
1 2 3 4 5 6 7 8 9 100
1
2
3
4
5
6
7
Chapter Number
Indegr
ee Ce
ntrality
Network TypeCOG
 
 AliceMouseQueenRabbitHatter
(b) In-degree centrality measure for OBS
network
1 2 3 4 5 6 7 8 9 100
2
4
6
8
10
12
14
16
18
20
Chapter Number
Outde
gree C
entrali
ty
Network TypeCOG
 
 AliceMouseQueenRabbitHatter
(c) Out-degree centrality measure for
OBS network
Figure 2: Dynamic network analysis plots for all 10 chapters of Alice in Wonderland. Each plot presents the change of
centrality values (Degree, In-degree, Out-degree) in different types of network (INR and OBS). X-axis has the chapter
numbers (one through ten) and Y-axis has the value of the relevant centrality measure.
in the piece, then, appears to be isolated to a key
chapter where he acts as a guide to introduce many
entities to the reader.
Likewise, tracing Rabbit (dash-dotted green line)
across in- and out-degree centrality of the OBS net-
work (figure 2(b) and 2(c)) gives a more fine-grained
view of how he works in the text. He is the most
mentioned in chapters 1 and 4, chapters that sand-
wich a big event, the drying ceremony of chapters
2 and 3. Likewise, he reemerges for another big
event, Alice?s trial (chapter 8, 9, 10). As previously
mentioned, Queen is the primary concern in Alice?s
mind during the length of the trial. However, Queen
is absent from the out-degree graph?she makes no
reference to off-screen characters. Rabbit, who has
a large spike in out-degree links during these chap-
ters, is the one who actually mentions a large number
of characters, while Queen focuses on interacting
with those already present. Thus, Rabbit is a charac-
ter that concerns Alice during large set-pieces, one
whose primary purpose comes in noticing and being
noticed.
We see that using a dynamic network can pro-
vide a more subtle view than using a static network.
Characters who are key in certain sections are no
longer overshadowed, like Queen, nor are their im-
portance exaggerated, like Mouse. It can also pro-
vide us with a better view of when and how a protag-
onist is most important throughout the text. Finally,
analyzing across data dimensions can provide a very
specific idea of how a character is functioning, as
seen with Rabbit.
8 Conclusion
In this paper we have motivated a computational ap-
proach to dynamic network analysis. We have hand-
annotated Lewis Carrol?s Alice in Wonderland using
a strict and well-defined annotation scheme and cre-
ated social event networks from these annotations.
From these, we have shown the usefulness of using
different types of networks to analyze different as-
pects of a text. We derive point-of-view from a so-
cial network. We also break down important charac-
ters into certain roles that describe how they function
in the text. Ultimately, we find that these roles are
limited by the static nature of social networks and
create dynamic networks. From these, we extract
a clearer picture of how these roles work, as well
as other characters overshadowed in the static net-
work. Having shown the value of such analysis, fu-
ture work will focus on adapting our computational
model (Agarwal and Rambow, 2010) for extracting
social events from a different domain (news articles)
to this new domain (literary text). We will then in-
vestigate a large number of literary texts and inves-
tigate how we can use our machinery to empirically
validate theories about literature.
Acknowledgments
We would like to thank three anonymous reviewers
for very useful comments and suggestions, some of
which we intend to pursue in future work. This work
is supported by NSF grant IIS-0713548.
95
References
M.H. Abrams. 1999. A Glossary of Literary Terms. Har-
court Brace College Publisher.
Apoorv Agarwal and Owen Rambow. 2010. Automatic
detection and classification of social events. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing.
Apoorv Agarwal, Owen C. Rambow, and Rebecca J. Pas-
sonneau. 2010. Annotation scheme for social network
extraction from text. In Proceedings of the Fourth Lin-
guistic Annotation Workshop.
R. Alberich, J. Miro-Julia, and F. Rossello. 2002. Marvel
universe looks almost like a real social network. eprint
arXiv:cond-mat/0202174, February.
Berger-Wolf, Tanya Y., and Jared Saia. 2006. A frame-
work for analysis of dynamic social networks. In
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ?06, pages 523?528, New York, NY, USA.
ACM.
K. M. Carley. 2003. Dynamic network analysis. In
R. Breiger, K. M. Carley, and P. Pattison, editors, Dy-
namic Social Network Modeling and Analysis: Work-
shop Summary and Papers, pages 133?145, Washing-
ton, DC.
Asli Celikyilmaz, Dilek Hakkani-Tur, Hua He, Greg
Kondrak, and Denilson Barbosa. 2010. The actor-
topic model for extracting social networks in literary
narrative. Proceedings of the NIPS 2010 Workshop ?
Machine Learning for Social Computing.
David K. Elson, Nicholas Dames, and Kathleen R. McK-
eown. 2010. Extracting social networks from literary
fiction. Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
138?147.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632,
September.
James Moody, Daniel McFarland, and Skye Benderde-
Moll. 2005. Dynamic network visualization. Ameri-
can Journal of Sociology, 110(4):1206?1241, January.
Franco Moretti. 2011. Network theory, plot analysis.
New Left Review.
M. E. J. Newman and M. Girvan. 2004. Finding and
evaluating community structure in networks. Phys.
Rev. E, 69(2), February.
Mark Newman. 2010. Networks: An Introduction. Ox-
ford University Press, Inc., New York, NY, USA.
Jill E. Perry-Smith and Christina E. Shalley. 2003. The
social side of creativity: A static and dynamic social
network perspective. The Academy of Management
Review, 28(1):89?106.
Menakhem Perry. 1979. Literary dynamics: How the
order of a text creates its meanings [with an analy-
sis of faulkner?s ?a rose for emily?]. Poetics Today,
1(1/2):35?361, October.
Jeff Rydberg-Cox. 2011. Social networks and the lan-
guage of greek tragedy. Journal of the Chicago Collo-
quium on Digital Humanities and Computer Science,
1(3).
Alexander Graham Sack. 2006. Bleak house and weak
social networks. unpublished thesis, Columbia Uni-
versity.
Christopher R Walker, 2005. ACE (Automatic Content
Extraction) English Annotation Guidelines for Events
Version 5.4.3 2005.07.01. Linguistic Data Consor-
tium.
96
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 51?55,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Artificial IntelliDance: Teaching Machine Learning through a
Choreography
Apoorv Agarwal
Department of Computer Science
Columbia University, New York, USA
apoorv@cs.columbia.edu
Caitlin Trainor
Department of Dance
Barnard College, Columbia University
caitlinmarytrainor@gmail.com
Abstract
In this paper we present a choreography that
explains the process of supervised machine
learning. We present how a perceptron (in its
dual form) uses convolution kernels to learn
to differentiate between two categories of ob-
jects. Convolution kernels such as string ker-
nels and tree kernels are widely used in Nat-
ural Language Processing (NLP) applications.
However, the baggage associated with learn-
ing the theory behind convolution kernels,
which extends beyond graduate linear algebra,
makes the adoption of this technology intrinsi-
cally difficult. The main challenge in creating
this choreography was that we were required
to represent these mathematical equations at
their meaning level before we could translate
them into the language of movement. By or-
chestrating such a choreography, we believe,
we have obviated the need for people to posses
advanced math background in order to appre-
ciate the core ideas of using convolution ker-
nels in a supervised learning setting.
1 Introduction
Natural Language Processing (NLP) and Machine
Learning (ML) are making a significant impact in
our day to day lives. Advancement in these ar-
eas of research is changing the way humans inter-
act with each other and with objects around them.
For example, speech to speech translation is making
it possible for people speaking different languages
to communicate seamlessly.1 In this eco-system,
where machines and objects around us are becom-
1http://www.bbn.com/technology/speech/
speech to speech translation
ing smarter, there is a need to make this complex
technology available to a general audience.
The Dance Your PhD competition2 is a recent ef-
fort that encourages doctoral students pursuing re-
search in Physics, Chemistry, Biology and Social
Sciences to explain the scientific ideas in their the-
ses through movement. The main advantage of this
approach is that the scientific ideas become avail-
able to a general audience through a medium that is
both visual and entertaining. The main challenge, of
course, is to abstract away from the technical vocab-
ulary and physicalize these scientific ideas.
In this paper, we present a choreography that ex-
plains the process of learning from data in a super-
vised setting. Through this choreography, we bring
out some of the main ideas of supervised machine
learning, including representing data as structured
objects and formulating similarity functions that a
machine uses to calculate distances between data
points. While these are general ideas, more rele-
vant to an audience that is not familiar with machine
learning, the choreography may also be used for ex-
plaining convolution kernels to researchers familiar
with machine learning but not necessarily familiar
with how a perceptron uses a convolution kernel in
its dual form.
The main challenge in creating this choreography
was that we were required to represent these mathe-
matical equations at the meaning level before trans-
lating them into the language of movement. In doing
so, our primary concerns were accuracy, aesthetics,
and legibility. The scientific ideas at hand could not
be compromised, and yet a literal representation of
the symbols would negate the intent of the project.
2http://gonzolabs.org/dance/
51
Equally vital to the success of the piece is the qual-
ity of the choreography on its own formal and aes-
thetic terms. The challenge of the translation was
both critical to the process and also enriching, be-
cause it deepened our understanding of convolution
kernels.
As Jason Eisner correctly notes in his paper on
interactive spreadsheets for teaching the forward-
backward algorithm (Eisner, 2002) ? They are con-
crete, visual, playful, sometimes interactive, and re-
main available to the students after the lecture ends
? we believe this choreography shares the same
spirit. Artificial IntelliDance functions to explain a
relatively sophisticated machine learning paradigm
in an accessible and entertaining format that can be
viewed repeatedly.3
The rest of the paper is structured as follows: In
section 2, we review the perceptron algorithm, its
dual form and convolution kernels. In section 3 we
present details of the choreography, focusing on the
aspects that explain the process of supervised ma-
chine learning and bring out the strengths and weak-
nesses of kernel learning. We conclude in Section 4.
2 The Perceptron algorithm and
Convolution Kernels
The perceptron algorithm is an online learning algo-
rithm invented by Frank Rosenblatt in 1958 (Rosen-
blatt, 1958). Given a set of training data points,
D = {(xi, yi)}, where yi ? {1,?1}, the algorithm
works as follows:4
1. Start with the all-zeroes weight vector w1 = 0,
and initialize t to 1.
2. Given xi, predict positive if wt ? xi > 0
3. On a mistake, update as follows:
wt+1 ? wt + yixi
4. t? t + 1
In natural language, a perceptron maintains a
weight vector wt at time instance t. The weight
3The video is available at the following URL:
http://tinyurl.com/mte8wda
4From lecture notes of Avrim Blum:
http://www.cs.cmu.edu/?avrim/ML09/lect0126.pdf. Mod-
ified for our purposes.
vector is initialized to zero at the start of the algo-
rithm. The perceptron receives one data point after
the other. For each data point, it predicts the cate-
gory of the data point by calculating its dot product
with the weight vector. If the dot product is greater
than zero, it predicts the category of the data point
as 1, and -1 otherwise. On a mistake, the perceptron
updates the weight vector by adding the product of
the data point (xi) and its category (1 or -1).
The key idea here is that the weight vector is a
linear combination of the training data points whose
categories the perceptron predicted incorrectly at the
time of training. The algorithm remembers these
incorrectly classified data points by marking them
with their true category (1 or -1). Abusing terminol-
ogy, we refer to these incorrectly classified training
data points as support vectors. Notationally, the final
weight vector then is w =
?Ns
k=1 ykxk, where Ns is
the number of support vectors.
This simple fact that the weight vector is a linear
combination of the data points has a deeper conse-
quence ? to predict the category of an unseen exam-
ple, call it x, all we need is a dot product of x with
all the support vectors: w ? x =
?Ns
k=1 yk(xk ? x).
This is usually referred to as the dual form of the
perceptron. The dual form allows for the use of ker-
nels because the dot product between two examples
can be replaced by a kernel as follows: w ? x =
?Ns
k=1 ykK(xk,x). This is exactly where convolu-
tion kernels come into the picture. We review those
next.
Convolution kernels, first introduced by David
Haussler (1999), can be viewed as functions that
calculate similarities between abstract objects, K :
X ? X ? R, where X is the set of abstract ob-
jects. Since their introduction, convolution kernels
have been widely used in many NLP applications
(Collins and Duffy, 2002; Lodhi et al, 2002; Ze-
lenko et al, 2003; Culotta and Sorensen, 2004; Mos-
chitti, 2004; Zhou et al, 2007; Moschitti et al, 2008;
Agarwal and Rambow, 2010; Agarwal et al, 2011).
The reason for their popular use in NLP applica-
tions is that text has natural representations such as
strings, trees, and graphs. Representing text in its
natural representation alleviates the need for fine-
grained feature engineering and is therefore a con-
venient way of data representation. Using this natu-
ral data representation, convolution kernels calculate
52
the similarity between two objects by recursively di-
viding the objects into ?parts?, calculating the simi-
larity between smaller parts, and aggregating these
similarities to report a similarity between objects.
For example, the way a string kernel will calcu-
late the similarity between two strings (say ?abc?
and ?aec?) is by mapping each string into an im-
plicit feature space and then calculating the similar-
ity between the two strings by taking a dot product
of the mappings (see Table 1). The feature space
is called implicit because the kernel never explicitly
writes out these features (or sub-structures). It calcu-
lates the similarity by using a dynamic program that
recurses over these structures to find similar sub-
structures.
a b c e ab ac bc ae ec
?abc? 1 1 1 0 1 1 1 0 0
?aec? 1 0 1 1 0 1 0 1 1
~v 1 0 1 0 0 1 0 0 0
Table 1: An example showing how a string kernel will
calculate the similarity between two strings. The implicit
feature space is {a, b, c, e, ab, ac, bc, ae, ec }. ~v refers to
the dot product of the vectors of the two strings. Similar-
ity between these two strings is
?9
i=1 vi = 3
Thus, convolution kernels allow the learner to
make similarity calculations without compromising
the original structure of the objects (unlike feature
engineering, where every object is represented as
a vector in a finite dimensional space, thus losing
the original structure of objects). This was the key
observation that lead us to define objects as dance
forms, and to our choice of using convolution ker-
nels for explaining the machine learning process
through a choreography. We discuss this in detail
in the next section.
3 Artificial IntelliDance
In 2011, we created a choreography to present the
idea of how a machine goes through the process of
learning from data. We presented a perceptron, in
its dual form, that uses convolution kernels to learn
how to differentiate between two categories of ob-
jects. The 15 minute choreography is supported by
a narrative, which is an interaction between a ma-
chine, depicted by a dancer, and a user, whose voice
is heard but who remains unseen.
One of the main and early challenges we ran into
during the ideation of the choreography had to do
with the definition of objects. Though the central
goal of the choreography was to explain a scientific
idea, we wanted the choreography to maintain its
aesthetic value. As a consequence of this constraint,
we decided to stay away from defining objects as
things that would restrict the dancers from moving
freely in a natural way.
As discussed in the previous section, since con-
volution kernels allow for a natural representation
of objects, we define our objects to be two dance
forms: Ballet and Modern dance. Much like string
kernels, where the implicit feature space is the space
of sub-strings (that form a string), in our case, the
high dimensional kernel space is the space of sub-
movements (that form a movement). Each dancer is
a data point, seen as a sequence of movements in an
infinite dimensional space.
Figure 1: Above is a scene from one of the performances
in which the machine, represented by the dancer in sil-
ver, ?considers? the data. Prominently featured are data
point dancers in red and yellow, both of whom have been
marked with category-differentiating shapes (round for
Ballet and diamond for Modern).
The choreography is broken into multiple phases.
In the first phase, we motivate the need for ma-
chine learning, or pattern recognition, by presenting
an apparently chaotic scene; all of the dancers are
onstage at once, performing unique movement se-
quences, with only brief moments of synchronized
action. The cacophonous dancing conveys the over-
whelming difficulty for data scientists to find pat-
terns in data using the naked eye. The dialogue ad-
vances the choreography to the next phase, where
we sketch out the learning process.
In the learning phase, the machine starts by mak-
53
ing a prediction on the first data point. Since the
machine has no prior knowledge (w1 = 0), it makes
a random prediction and gets the category wrong.
The machine marks the dancer with a symbol in or-
der to remember the data point (wt ? wt + yixi).
The machine is then asked to make a prediction on a
second data point. The machine compares this new
data point with the data point it marked and makes
a prediction (w ? x =
?Ns
k=1 ykK(xk,x)). Once
again, it gets the category wrong and marks the sec-
ond data point as well. This process continues until
the machine has seen all the training instances and
has selected data points it thinks encode structures
important for classification.
Marking of dancers is done explicitly where the
machine dancer attaches a round or triangular sym-
bol to the data points: round is for Ballet and trian-
gle is for Modern (see Figure 1). This is analogous
to how a perceptron attaches positive and negative
weights to the data points belonging to positive and
negative categories respectively.
The narration points out a big limitation of con-
volution kernel methods, which is, in the worst case,
every data-point is compared with every other data
point in the training data, thus making the learn-
ing process slow (because the machine needs to go
through the training data twice).
We also differentiate between the low dimen-
sional feature space, in which the machine is un-
able to separate the data, and the high dimensional
space, which offers distinguishability. The set of in-
active training data points, i.e. the data points in
a low dimensional feature space, is depicted by a
clump of dancers in a corner who are hardly mov-
ing. The set of data points that are actively moving
lie in a high dimensional feature space in which the
machine learns a linear separator.
The next phase is testing, in which the machine
compares the test dancers with the dancers it marked
in the learning phase. After comparing each test
point with all the support vectors, the machine
makes a prediction. This phase concludes by show-
ing that the machine has in fact learned to differen-
tiate between the two categories.
The user is impressed and asks the machine to
reveal the secret sauce. In this part of the chore-
ography we visually describe how convolution ker-
nels go about calculating similarities between two
abstract objects, by breaking the object into parts,
and recursing over the parts to calculate similarity.
This action is illustrated by a comparison of simi-
lar movements and sub-movements as executed by a
ballet and modern dancer. Situated side by side, the
two dancers fragment the movements into increas-
ingly smaller bits so as to make differences in the
two forms of dance (objects) more visibly compa-
rable. We also highlight the reason for the machine
to look at a pair of data points instead of individ-
ual data points. The reason is that the machine does
not remember the sub-structures important for clas-
sification (because the implicit feature space is enor-
mous). By marking the data points, it only remem-
bers the data points that encode these sub-structures.
To this, the user voice points out another limita-
tion of using convolution kernels; interpretability of
models is hard. We can learn predictive models, but
the fine grained structures important for classifica-
tion remain hidden.
The piece ends with all the dancers linearly sepa-
rated into categories in a high dimensional implicit
feature space. Through the narration we point out
the main differences and similarities between the
two forms of dance, which are aesthetically visible
but are sometimes hard to articulate.
4 Conclusion
In this paper, we presented a choreography that il-
lustrates the process of supervised machine learn-
ing using a perceptron and convolution kernels. The
choreography is structured around a scene in which
the machine (represented by a dancer) learns to dif-
ferentiate between two categories of objects, ballet
and modern dance. The choreography not only ex-
plains the process of machine learning and how con-
volution kernels work, it also brings out two major
limitations of using convolution kernels visually ?
having to go through the data twice, which makes
the learning process slow, and that the interpretabil-
ity of the models is hard, because the important sub-
structures are not stored explicitly. While the gen-
eral ideas about supervised machine learning may
be more relevant to an audience that is not familiar
with machine learning, the choreography may also
be used to explain convolution kernels (in a visual
and entertaining way) to researchers familiar with
54
machine learning but not necessarily familiar with
how a perceptron uses a convolution kernel in its
dual form.
Artificial IntelliDance premiered at Barnard Col-
lege in April 2012, and has since been invited to
perform at the World Science Festival 2012 and
TEDx ColumbiaEngineering 2012. The audience
was comprised of a combination of scientists and
non-scientists, including dance artists, undergradu-
ate and graduate students, and the general public.
The primary concepts of the presentation were un-
derstood clearly by a number of viewers lacking fa-
miliarity with any machine learning paradigm as ev-
idenced by the post-presentation discussions. Unfor-
tunately, we do not have a more precise evaluation as
to how many people actually understood the scien-
tific ideas. The only ?evaluation? we have is that the
video continues to be showcased; we were recently
invited to showcase it at Australia?s National Sci-
ence Week 2013. However, we have not yet heard
of the video being used in a Machine Learning lec-
ture.
In addition to functioning as an educational tool,
a noteworthy outcome of the project is that it fosters
dialogue between the general public and the arts and
computer science communities.
Acknowledgments
We would like to thank Caronae Howell, Kapil
Thadani, and anonymous reviewers for useful com-
ments. We thank the dancers involved in the pro-
duction and performance of the piece (in no par-
ticular order): Aditi Dhruv, Jenna Simon, Mor-
gan Caglianone, Maddie James, Claire Salant, Anna
Brown Massey, Emily Craver, Mindy Upin, Tem-
ple Kemezis, Taylor Gordon, Chelsea Cusack, Lane
Halperin, Lisa Fitzgerald and Eleanor Barisser. We
would like to thank Robert Boston for music and
Marlon Cherry for voice of the user.
References
Apoorv Agarwal and Owen Rambow. 2010. Automatic
detection and classification of social events. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1024?
1034, Cambridge, MA, October. Association for Com-
putational Linguistics.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analy-
sis of twitter data. In Proceedings of the Workshop
on Language in Social Media (LSM 2011), pages 30?
38, Portland, Oregon, June. Association for Computa-
tional Linguistics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association for
computational linguistics, pages 263?270. Association
for Computational Linguistics.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42ndMeeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 423?429,
Barcelona, Spain, July.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective Tools and
Methodologies for Teaching Natural Language Pro-
cessing and Computational Linguistics, pages 10?18,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of California
at Santa Cruz.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Christianini, and Chris Watkins. 2002. Text classifi-
cation using string kernels. The Journal of Machine
Learning Research, 2:419?444.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Special Issue on Semantic Role Labeling, Computa-
tional Linguistics Journal,.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceedings
of the 42nd Conference on Association for Computa-
tional Linguistic.
Frank Rosenblatt. 1958. The perceptron. Psych. Rev,
65(6):386?408.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning
Research, 3:1083?1106.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoM-
ing Zhu. 2007. Tree kernel-based relation extraction
with context-sensitive structured parse tree informa-
tion. In Proceedings of EMNLP-CoNLL.
55
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 77?84,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Teaching the Basics of NLP and ML in an Introductory Course to
Information Science
Apoorv Agarwal
Department of Computer Science
Columbia University
New York, USA
apoorv@cs.columbia.edu
Abstract
In this paper we discuss our experience of
teaching basic Natural Language Processing
(NLP) and Machine Learning (ML) in an in-
troductory course to Information Science. We
discuss the challenges we faced while incor-
porating NLP and ML to the curriculum fol-
lowed by a presentation of how we met these
challenges. The overall response (of stu-
dents) to the inclusion of this new topic to the
curriculum has been positive. Students this
semester are pursuing NLP/ML projects, for-
mulating their own tasks (some of which are
novel and presented towards the end of the pa-
per), collecting and annotating data and build-
ing models for their task.
1 Introduction
An introductory course to Information Science has
been taught at Columbia University for over a
decade. The main goal of the course is to intro-
duce undergraduates at our university to applica-
tions of Computer Science. For most students, this
is their first course in the Computer Science de-
partment. The course has no pre-requisites such
as higher mathematics or programming. In fact,
through a survey we found that about 10% of the
class did not know the meaning of a programming
language.
Traditionally, the computer science applications
that have been taught in this course include HTML
(creating a website), Spreadsheets, Database Sys-
tems, World Wide Web and the Internet, Algorithms
and programming in Python. Given the importance
of understanding how humans are building smart
machines and the amount of excitement around
Natural Language Processing (NLP) and Machine
Learning (ML) applications, we decided to include a
social media analysis application ? sentiment analy-
sis of Twitter ? in the curriculum last year. The over-
all response to this inclusion has been positive. One
outcome of this inclusion is that the students are now
able to build basic models for popular NLP applica-
tions such as sentiment analysis of Twitter, spam de-
tection of emails, and document classification. But
a more significant outcome of this inclusion is that
the students seemed to have gained a general idea of
how machine learning works, as a result, they find
Watson playing Jeopardy! against the humans, and
Google?s self-driving car less ?magical?.
There were two main challenges in incorporating
an introduction to NLP and ML to the curriculum:
(1) we wanted to include this topic without compro-
mising the traditionally covered material, which put
a constraint on the number of lectures we could use
for introducing NLP and ML and (2) we were re-
quired to abstract away from the inherently math-
ematical jargon used to explain NLP and ML. In
this paper we present the way we met these chal-
lenges. We present our lecture, homework and ex-
amination design that enabled us to get some of the
most important ideas of NLP and ML across in one
lecture. The students performed exceptionally well
on the NLP/ML section of the examination. More-
over, some students are pursuing projects related to
these topics formulating their own tasks, collecting
and annotating data, and building models to answer
their hypotheses. These are signs that undergrad-
uates with a broad spectrum of educational back-
grounds and interests are not only capable of tack-
ling the basics of NLP and ML, but that they may
even be doing so with relish.
77
There has been successful and fruitful effort by
researchers in the NLP community to share their
experiences and course design through this work-
shop in the past (Lee, 2002; Eisner, 2002; Liddy
and McCracken, 2005; Freedman, 2005; Zinsmeis-
ter, 2008). Steven Bird (2008) notes that an intro-
ductory course needs to serve some common, basic
needs ? ?For some students, it will be the first step
in a pathway leading to specialized courses, grad-
uate research, or employment in this field. For stu-
dents who do not continue, the introductory course
will be their main exposure to the field. Naturally,
this course is also a prime opportunity to promote
the field to newcomers and encourage them to pur-
sue advanced studies in this area.? We share the
same motivation (as (Bird, 2008)) ? our target audi-
ence is in fact ?newbies? in Computer Science, who
may or may not continue with more advanced topics
in Computer Science, in which case this course will
be their main exposure to the field and thus offers a
great opportunity for us to promote the field.
The rest of the paper is structured as follows: In
section 2, we give details of the course and stu-
dent demographics. Section 3 presents the NLP/ML
lecture organization and content. In section 4 we
present the problems on the mid term examination
and performance of the students on the NLP/ML
part of the exam. Section 5 describes some of the
most interesting student projects that have come out
of the course. We conclude in Section 6.
2 Student demographics
Students enrolling in this introductory course on In-
formation Science come from a wide variety of aca-
demic backgrounds. A majority of the class is un-
dergraduates who have never taken any course in the
Computer Science department before. The course is
taught over a period of 4 months, consisting of 24
lectures of 75 minute duration each.
Figure 1 and Figure 2 present a distribution of 61
students based on their college rank and major (aca-
demic background) respectively.
Figure 1 shows that a large majority of students
are freshman and sophomores (50%). While these
students have an idea of what they would like to
major in, they are not required to finalize their ma-
jors until the final semester of their sophomore year.
This introductory course is therefore a great oppor-
tunity to promote the field by exposing the students
to some of the most exciting applications of Com-
puter Science. In the first class of the course, we
showed the students a video of Watson playing the
popular gameshow Jeopardy! against the humans.
It was surprising that only a few students knew of
Watson. But even the ones who knew about it were
excited and curious to learn how Watson actually
works.
Figure 1: Student distribution based on College rank.
20% Freshman, 30% Sophomore, 16% Junior, 21% Se-
nior, 8% Graduate and 5% Other
Figure 2 presents a distribution of students based
on the majors they are pursuing or intend to pursue.
For this figure, we grouped the reported majors into
the following broader categories: Math/Engineering
(Math, Computer Science, Information Science,
Electrical Engineering), Basic sciences (Biology,
Zoology, Chemistry, Physics, Neuroscience), Polit-
ical Science, Social Science, Language (German,
French, Yiddish, English, Linguistics), Arts and Hu-
manities (including Literature, Film, Theatre), Re-
gional studies (Asian, American, Middle Eastern),
and Other (Finance, History, International Relations,
Marketing, Philosophy, Psychology).
The distribution of majors shows that the students
come from a wide variety of academic backgrounds.
Only about 12% of the students are pursuing or in-
tend to pursue a major in Math/Engineering. There
is a large majority of students who have only taken
78
Figure 2: Student distribution based on majors. 16%
Economics, 14% Political Science, 14% Basic Sciences,
12% Math/Engineering, 11% Arts and Humanities, 11%
Other, 9% Social Science, 8% Language, 6% Regional
Studies.
SAT level mathematics. The majority of these stu-
dents have never used any programming language
before. Therefore, one of the main challenges of
teaching this course, especially introducing NLP and
ML, was to abstract away from mathematical jar-
gon and convey the fundamentals through the use
of analogies and concrete illustrations.
3 Lecture organization and content
To meet the aforementioned challenges, we spent
one lecture introducing the class to some basic con-
cepts of NLP and ML. Through homework and ex-
amination, we introduced the students to more NLP
applications that also helped them appreciate the
strengths and weaknesses of the simple ML tech-
nique we introduced in class. We geared the Python
part of course towards text processing preparing the
students to implement an end-to-end pipeline of a
popular NLP application on another homework.
We started the lecture by introducing a concrete
and motivating NLP application ? sentiment analy-
sis of Twitter. In line with Reva Freedman?s (2005)
observation, we found that starting with a concrete
application is important. We first defined sentiment
analysis as the task of building a machine that is able
classify the polarity of opinions in text into one of
two categories: positive and negative.1 We moti-
vated this application by briefly discussing some of
its use cases: predicting the outcome of a presiden-
tial election, gauging how a company or a product
of a company is performing in the market, finding
on average how people are feeling based on gender,
location, age and weather.2
After posing the question ? how would a machine
learn to predict if a tweet has a positive or a nega-
tive sentiment ? we first drew an analogy of how hu-
mans learn new concepts. Humans learn through ex-
amples and counter-examples. When we see a new
object or learn a new concept, our instinct is to com-
pare the new with the familiar. Our first attempt is to
find similarities and dissimilarities between this new
object with the objects we have already seen. Simi-
larly, to train a machine, we first need to provide it
with some labeled examples. For the task at hand,
examples are tweets and their labels are manually
annotated sentiment polarity (positive or negative).
Using these training examples, the machine learns
patterns of words that signify a particular sentiment.
We started with a small list of words, calling them
?features?. The training data and features are pre-
sented in Table 1. We asked the students to fill out
each cell in Table 1 by putting a 1 if a tweet contains
a particular word and 0 if it does not contain that
word. We mentioned that this process is called ?fea-
ture extraction?, in which we convert unstructured
data into a structured representation. This represen-
tation is structured because each tweet is represented
as an ordered and fixed list of features.
We asked the students how they would calculate
the similarity between two tweets. And we got an
obvious answer ? count the number of words they
have in common.
The next question we asked was ?how might the
machine calculate the similarity using the structured
representation?? The answer to this question was
less obvious but once we gave them the formula,
1We defined the italicized words and gave examples to help
students understand the definitions. We intentionally kept the
definition of sentiment analysis simple and restricted to classi-
fying polarity of opinions into positive and negative categories.
2http://www.wefeelfine.org
79
Tweet ID Tweet good bad not pretty great Label
T1 It?s a good day :) 1 0 0 0 0 +1
T2 The weather is pretty bad 0 1 0 1 0 -1
T3 Alice is pretty 0 0 0 1 0 +1
T4 Bieber is not all that great 0 0 1 0 1 -1
S1 It is a good day for biking 1 0 0 0 0 ?
S2 The situation is not pretty 0 0 1 1 0 ?
S3 Such a great show :) 0 0 0 0 1 ?
Table 1: Training and test data used in class to illustrate how a machine will learn to predict the polarity of tweets.
the students were able to grasp it quickly. We in-
troduced the formula as a bit-wise multiplication of
list of features followed by the summation of the re-
sulting bits.
Sim(T, S) =
d?
i=1
ti ? si
where T, S are tweets, d is the number of features in
the list of features, ti, si are the ith bit of tweets T
and S respectively.
The next question we asked was given a tweet,
whose polarity is unknown (an unseen tweet), how
might they use the training data to predict its po-
larity. This was a harder question, and though we
did not expect an answer, we posed this question
nonetheless to serve as a pause in the lecture and
indicate that a key idea was coming.
Before revealing the secret sauce, we made the
analogy of how humans would do a similar task.
Given two kinds of fish, say sea bass and salmon, the
way we would classify a new fish into one of these
two categories would be by comparing ?features? of
the new fish with the features of sea bass and with
the features of salmon followed by observing if the
new fish is ?closer? to sea bass or salmon. Similarly,
the machine will compare the list of features of the
unseen tweet with the list of features of the positive
and the list of features of the negative tweets and
compute a similarity score that will allow the ma-
chine to make a prediction about the polarity of this
unseen tweet.
We then introduced the following formula:
s =
N?
i=1
Sim(Ti, S)? Labeli
where N is the total number of training examples,
Ti is the ith training example, S is the test tweet and
Labeli is the human annotated polarity of Ti.
The machine uses this score to make a final pre-
diction. If the score is less than or equal to 0, the ma-
chine predicts the polarity of the tweet as negative.
If the score is greater than 0, the machine predicts
the polarity of the tweet as positive.
We illustrated this by working out a few examples
of how the machine will go about predicting the po-
larity of the following unseen tweets:
1. ?It is a good day for biking?
2. ?The situation is not pretty?
3. ?Such a great show :)?
We worked out the first example on the board and
asked the students to work out the remaining two on
their own. Following is the way in which we worked
out the first example on the board.
1. First the machine converts the test tweet S1
= ?It is a good day for biking? into the same
structured representation as that of the training
tweets. The list of features for S1 is [1,0,0,0,0]
(see Table 1).
2. Then the machine compares the list of features
for S1 with each of the training tweets as fol-
lows:
(a) Comparing the list of features for tweets
T1 and S1, the machine finds the bit-
wise multiplication of their feature lists
[1, 0, 0, 0]? [1, 0, 0, 0] = [1, 0, 0, 0]. Then
the machine adds all the bits 1+0+0+0 =
1. We point out there is only one word in
80
common between the two tweets (namely
?good?). The similarity score between the
first training example and the test example
s1 = 1? (+1) = 1.
(b) Similarly, comparing the feature lists for
T2 and S1, we get a similarity score s2 =
([0, 1, 0, 1, 0]? [1, 0, 0, 0, 0])? (?1) = 0
(c) Comparing the feature lists for T3 and
S1, we get a similarity score s3 =
([0, 0, 0, 1, 0]? [1, 0, 0, 0, 0])? (+1) = 0
(d) Finally, comparing the feature lists for T4
and S1, we get a similarity score s4 =
([0, 0, 1, 0, 0]? [1, 0, 0, 0, 0])? (?1) = 0
3. Next, the machine adds all the similarity scores
together to get an aggregated score for the test
tweet s = s1 + s2 + s3 + s4 = 1. Since s > 0,
the machine predicts this test tweet T1, ?It is a
good day for biking?, has a positive polarity.
Having the students work out the other two exam-
ples in class on their own and interacting with their
neighbors, they began to see the meaning of pattern
recognition. Bringing their attention to Table 1, we
pointed out that the word ?good? is associated with
a positive polarity by virtue of appearing in a posi-
tively labeled tweet. The word ?pretty? is associated
with a neutral polarity because it appears both in a
positive and in a negative tweet. This means that
the machine has learned that it cannot make a pre-
diction simply based on the word ?pretty?. The test
tweet ?The situation is not pretty? makes this point
explicit. This tweet is classified correctly as negative
but only because of the presence of the word ?not?,
which appears in a negative tweet.
In summary, through these worked out examples,
we were able to drive home the following points:
1. The machine automatically learns the connota-
tion of words by looking at how often certain
words appear in positive and negative tweets.
2. The machine also learns more complex patterns
that have to do with the conjunction and dis-
junction of features.
3. The quality and amount of training data is im-
portant ? for if the training data fails to encode
a substantial number of patterns important for
classification, the machine is not going to learn
well.
Students asked the following questions, which
helped us build on the aforementioned points.3
1. Good and great are synonyms. Shouldn?t we
count them as one feature?
2. Could we create and use a dictionary that lists
the prior polarity of commonly used words?
3. If the prediction score for the tweet is high,
does that mean we the machine is more con-
fident about the prediction?
4. In this approach, the sequence of words does
not matter. But clearly, if ?not? does not negate
the words containing opinion, then won?t the
machine learn a wrong pattern?
5. If we have too many negative tweets in our
training data (as compared to the positive
tweets), then would the machine not be pre-
disposed to predict the polarity of an unseen
tweet as negative?
Building on these concepts, we had the students
work through an end-to-end example of classifying
movie reviews into positive and negative on their
homework. What appeared to be a promising ma-
chine learning technique in class, seemed to fail for
this task. They realized that classifying movie re-
views is much harder because of the words used
in plot descriptions that mislead the classifier. We
used examples from the seminal paper by Peter Tur-
ney (2002) for this homework problem.
4 Problem and performance on the Mid
term examination
We further built on the fundamentals, by asking
the students to classify emails into ?important? and
?unimportant? by using the same machine learning
technique (used for sentiment analysis of Twitter)
on their mid term examination. This helped them
see that the ML technique learned in class may be
used, in general, for other NLP applications. As
Heike Zinsmeister (2008) notes, redundancy and it-
erative re-introduction could be helpful for students,
3Questions are reformulated for succinctness and clarity.
81
we found that by having the students work out differ-
ent NLP applications using the same ML approach
helped them grasp the concepts better and appreci-
ate the strengths and weaknesses of this simple ML
approach.
Table 2 presents the training data along with the
features. Following are the problems from their mid-
term examination.
1. Extract features from the emails in the training
data, i.e. fill Table 2 with ones and zeros. (5
points)
2. What will be the prediction of the machine for
this new incoming email ?It is important that
you register for this meeting. ? your phd advi-
sor?. Say if, this is an important email, is the
prediction made by your machine correct? (4 +
1 points)
3. What will be the prediction of the machine for
this new incoming email ?Bill, what up??. Say
if, this is an unimportant email, is the prediction
made by your machine correct? (4 + 1 points)
4. What is the performance of your current ma-
chine learning model on all the test data? (2
points)
5. What other feature(s) will you add to the list
of features to improve the performance of your
machine learning model? How will this change
the prediction of the two incoming emails?
What will be the performance of your new
model? (3 + (2 + 2) + 1 points)
For problem 5 on the exam, most of the students
came up with the answer of adding the words ?your?
and ?advisor? to the list of features. But some stu-
dents devised more complex features. One student
proposed to add the capitalization feature to distin-
guish between ?Bill? and ?bill?. Another student ex-
tended this feature to additionally check if ?Bill? is
a proper noun or not. The only type of feature we
introduced in class was the binary occurrence and
non-occurrence of words. It was promising to see
the students expand on the preliminary feature set to
create novel and more advanced set of features.
The duration of the exam was 75 min and it con-
sisted of 6 extended problems. The first two prob-
lems were compulsory and the students were asked
to do any two out of the remaining four problems
(NLP/ML, Logic Gates, Database Design, Machine
Instructions). Each of the remaining four problems
was worth 25 points. Table 3 shows their perfor-
mance on the four problems. The table shows that
the students did extremely well on the NLP/ML
problem ? averaging 20.54 out of 25 with a stan-
dard deviation of 4.46. Note, students unanimously
attempted the NLP/ML part of the exam ? only 2 stu-
dents scored a zero for this problem as compared to
17, 11 and 23 students, who scored a zero on Logic
Gates, Database Design and Machine Instructions
respectively.4
The performance of students on the mid term ex-
amination assured us that they were comfortable
with the terminology and the process of machine
learning. We decided to build on this foundation by
introducing them to basic text processing, indexing,
and stemming in the Python part of the course. On
their Python homework, they implemented a com-
plete pipeline, starting from creating a vocabulary
from the training data, then extracting features, and
finally implementing a simple version of the per-
ceptron algorithm to predict sentiment polarity of
tweets. The average on this homework was 87.8 out
of 115 with about 60% of the students scoring over
100 points.
5 Student project descriptions
The most exciting outcome of including NLP and
ML to the course has been that some students have
signed up for a course project in their demanding
curriculum. For the course project, the students were
asked to formulate their own tasks, collect and anno-
tate data and build machine learning models for their
tasks. Following are the two most novel task formu-
lations (in students? own language) followed by a list
of other projects.5
Detecting liberal or conservative biases (Allen
Lipson and Tyler H. Dratch): Critics on both sides
of the political spectrum often accuse their adver-
saries of employing biased language to promote
4The grading was generous and students were given partial
credit for their attempt. Therefore, we approximate the number
of students who attempted a problem by counting the number
of students who scored a zero on that problem.
5The project reports are available at
www.cs.columbia.edu/?apoorv/Teaching/ProjectReports
82
Email ID Email meeting register unsubscribe bill Label
E1 Meeting at 4, hurry! ? your advisor. ... ... ... ... +1
E2 Free event. To register click here. To un-
subscribe click here.
... ... ... ... -1
E3 According to our register, your bill is yet
to be paid
... ... ... ... +1
E4 Register for this useless meeting. ... ... ... ... -1
Table 2: Structured representation of the training data or examples from which the machine will learn to differentiate
between important and unimportant emails.
Problem Average Std-dev Median Count (Score < 5) Count (Score == 0)
NLP/ML 20.54 4.46 22 2 2
Logic Gates 16.94 6.48 20 20 17
Database Design 13.63 6.48 14 14 11
Machine Instructions 12.8 6.81 14.5 27 23
Table 3: Distribution of scores for 53 students on problems on the mid term exam. Students were required to do any
two out of these four problems. Each problem was worth 25 points. Count (Score < 5) means the number of students
out of 53 that scored less than 5 points on a problem. Average, standard deviation and median values exclude students
who scored a 0.
an agenda. Nowhere in politics is the usage of
language more contentious than in the immigration
debate. Conservatives lambast ?illegal aliens?;
liberals defend ?undocumented workers.? Liberals
promote a ?path to citizenship?; conservatives decry
?criminal amnesty?. But is this bias also present
in major news sources, the supposedly impartial
sources of society?s information? Or are papers like
the New York Times and the Wall Street Journal
firmly on opposite sides of the immigration debate?
We want to put this question to the test. We are
constructing a machine learning algorithm to detect
liberal or conservative biases on immigration in the
New York Times and the Wall Street Journal.
The Bechdel Test (Michelle Adriana Marguer
Cheripka and Christopher I. Young): The Bechdel
Test is a measure by which it is possible to iden-
tify gender bias in fiction. In order to pass the test,
the work of fiction must pass three criteria: there
must be two main female characters, they must have
a conversation, and they must be speaking about
something other than a man. Though primarily used
in film, the Bechdel test can also be applied to lit-
erature. In previous Bechdel experiments, the re-
sults indicated traditional, heteronormative pattern.
While a text does not necessarily need to be explic-
itly feminist in order to pass the test, the test itself is
an important gauge for the social roles that societies
uphold and perpetuate. This particular experiment
was created in order to determine if this trend was
consistent across mediums. Considering that chil-
dren?s books provide the foundation for a person?s
interaction with literature, the test could identify pat-
terns that emerge from an early stages of literature
and address their future impact.
Some of the other project proposals are as fol-
lows: Gim Hong Lee built a sentiment analysis en-
gine to rate a professor based on his/her reviews
available on CULPA.info (Columbia Underground
Listing of Professor Ability). Xueying (Alice) Lin
built a recommendation system for Yelp. She ac-
quired the data-set from kaggle.com.6 A group
of three students (Roni Saporta, Moti Volpo and
Michal Schestowitz) experimented with the breast
cancer data-set available at the UCI data-repository.7
They used scikit-learn?s8 implementation of the lo-
gistic regression algorithm.
It is heartening to see that students who had very
limited (or no) idea about how machines learn at the
start of the course are now formulating tasks and at-
6http://www.kaggle.com/c/yelp-recruiting
7http://archive.ics.uci.edu/ml/
8http://scikit-learn.org/stable/
83
tempting to build their own machine learning mod-
els. What they still do not know, we believe, is that
they are mapping each document into a finite dimen-
sional feature space and calculating dot products be-
tween feature vectors to calculate similarity between
documents. While this math vocabulary is probably
required to make more progress and dive deeper into
NLP and ML, we believe it is not required to convey
the essence of pattern recognition.
6 Conclusion
In this paper, we presented a lecture, homework and
examination design, through which we were able to
get some basic ideas of Natural Language Process-
ing and Machine Learning across to students who
came from a wide variety of academic backgrounds,
majority of whom did not have an advanced math
background. Apart from the challenge of having to
abstract away from the inherently mathematical con-
cepts, we faced another challenge at the onset of de-
signing the lecture ? we had to deliver the NLP and
ML material in one or two lectures so that we do not
compromise on the traditionally covered topics.
We believe that the lecture, homework and exami-
nation design presented in this paper may be used by
lecturers teaching introductory course such as ours
or by researchers who are interested in presenting
a simplified explanation of NLP and ML to general
popular science audiences.
Acknowledgments
We would like to thank Kapil Thadani, Caronae
Howell, Kshitij Yadav, Owen Rambow, Meghna
Agarwala, Sara Rosenthal, Daniel Bauer and anony-
mous reviewers for useful comments.
References
Steven Bird. 2008. Defining a core body of knowledge
for the introductory computational linguistics curricu-
lum. In Proceedings of the Third Workshop on Issues
in Teaching Computational Linguistics, pages 27?35,
Columbus, Ohio, June. Association for Computational
Linguistics.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective Tools and
Methodologies for Teaching Natural Language Pro-
cessing and Computational Linguistics, pages 10?18,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Reva Freedman. 2005. Concrete assignments for teach-
ing NLP in an M.S. program. In Proceedings of the
Second ACL Workshop on Effective Tools and Method-
ologies for Teaching NLP and CL, pages 37?42, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
Lillian Lee. 2002. A non-programming introduction
to computer science via nlp,ir,and ai. In Proceed-
ings of the ACL-02 Workshop on Effective Tools and
Methodologies for Teaching Natural Language Pro-
cessing and Computational Linguistics, pages 33?38,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Elizabeth Liddy and Nancy McCracken. 2005. Hands-on
NLP for an interdisciplinary audience. In Proceedings
of the Second ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 62?
68, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Peter D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In the Proceedings of the 40th meet-
ing of Association of Computational Linguisitcs (ACL
2002).
Heike Zinsmeister. 2008. Freshmen?s CL curriculum:
The benefits of redundancy. In Proceedings of the
Third Workshop on Issues in Teaching Computational
Linguistics, pages 19?26, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
84
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 50?58,
Gothenburg, Sweden, April 27, 2014.
c?2014 Association for Computational Linguistics
Parsing Screenplays for Extracting Social Networks from Movies
Apoorv Agarwal
?
, Sriramkumar Balasubramanian
?
, Jiehan Zheng
?
, Sarthak Dash
?
?
Dept. of Computer Science
Columbia University
New York, NY, USA
?
Peddie School
Hightstown, NJ, USA
apoorv@cs.columbia.edu jzheng-14@peddie.org
Abstract
In this paper, we present a formalization
of the task of parsing movie screenplays.
While researchers have previously moti-
vated the need for parsing movie screen-
plays, to the best of our knowledge, there
is no work that has presented an evalua-
tion for the task. Moreover, all the ap-
proaches in the literature thus far have
been regular expression based. In this pa-
per, we present an NLP and ML based
approach to the task, and show that this
approach outperforms the regular expres-
sion based approach by a large and statis-
tically significant margin. One of the main
challenges we faced early on was the ab-
sence of training and test data. We pro-
pose a methodology for using well struc-
tured screenplays to create training data
for anticipated anomalies in the structure
of screenplays.
1 Introduction
Social network extraction from unstructured text
has recently gained much attention (Agarwal and
Rambow, 2010; Elson et al., 2010; Agarwal et al.,
2013a; Agarwal et al., 2013b; He et al., 2013). Us-
ing Natural Language Processing (NLP) and Ma-
chine Learning (ML) techniques, researchers are
now able to gain access to networks that are not
associated with any meta-data (such as email links
and self-declared friendship links). Movies, which
can be seen as visual approximations of unstruc-
tured literary works, contain rich social networks
formed by interactions between characters. There
has been some effort in the past to extract social
networks from movies (Weng et al., 2006; Weng
et al., 2007; Weng et al., 2009; Gil et al., 2011).
However, these approaches are primarily regular
expression based with no evaluation of how well
they work.
In this paper we introduce a formalization of the
task of parsing screenplays and present an NLP
and ML based approach to the task. By parsing
a screenplay, we mean assigning each line of the
screenplay one of the following five tags: ?S? for
scene boundary, ?N? for scene description, ?C?
for character name, ?D? for dialogue, and ?M? for
meta-data. We expect screenplays to conform to
a strict grammar but they often do not (Gil et al.,
2011). This disconnect gives rise to the need for
developing a methodology that is able to handle
anomalies in the structure of screenplays. Though
the methodology proposed in this paper is in the
context of movie screenplays, we believe, it is gen-
eral and applicable to parse other kinds of noisy
documents.
One of the earliest challenges we faced was the
absence of training and test data. Screenplays, on
average, have 7,000 lines of text, which limits the
amount of annotated data we can obtain from hu-
mans. We propose a methodology for using well
structured screenplays to create training data for
anticipated anomalies in the structure of screen-
plays. For different types of anomalies, we train
separate classifiers, and combine them using en-
semble learning. We show that our ensemble out-
performs a regular-expression baseline by a large
and statistically significant margin on an unseen
test set (0.69 versus 0.96 macro-F1 measure for the
five classes). Apart from performing an intrinsic
evaluation, we also present an extrinsic evaluation.
We show that the social network extracted from
the screenplay tagged by our ensemble is closer
to the network extracted from a screenplay tagged
50
by a human, as compared to the network extracted
from a screenplay tagged by the baseline.
The rest of the paper is structured as follows: in
section 2, we present common terminology used
to describe screenplays. We survey existing liter-
ature in section 3. Section 4 presents details of
our data collection methodology, along with the
data distribution. Section 5 gives details of our
regular-expression based system, which we use as
a baseline for evaluation purposes. In section 6,
we present our machine learning approach. In sec-
tion 7, we give details of the features we use for
machine learning. In section 8, we present our ex-
periments and results. We conclude and give fu-
ture directions of research in section 9.
2 Terminology
Turetsky and Dimitrova (2004) describe the
structure of a movie screenplay as follows: a
screenplay describes a story, characters, action,
setting and dialogue of a film. Additionally,
they report that the structure of a screenplay
follows a (semi) regular format. Figure 1 shows
a snippet of a screenplay from the film ? The
Silence of the Lambs. A scene (tag ?S?) starts
with what is called the slug line (or scene bound-
ary). The slug line indicates whether the scene
is to take place inside or outside (INT, EXT),
the name of the location (?FBI ACADEMY
GROUNDS, QUANTICO, VIRGINIA?), and
can potentially specify the time of day (e.g.
DAY or NIGHT). Following the scene boundary
is a scene description. A scene description
is followed by a character name (tag ?C?),
which is followed by dialogues (tag ?D?).
Character names are capitalized, with an optional
(V.O.) for ?Voice Over? or (O.S.) for ?Off-screen.?
Dialogues, like scene descriptions, are not asso-
ciated with any explicit indicators (such as INT,
V.O.), but are indented at a unique level (i.e.
nothing else in the screenplay is indented at this
level). Screenplays may also have other elements,
such as ?CUT TO:?, which are directions for the
camera, and text describing the intended mood
of the speaker, which is found within parentheses
in the dialogue. For lack of a name for these
elements, we call them ?Meta-data? (tag ?M?).
3 Literature Survey
One of the earliest works motivating the need for
screenplay parsing is that of Turetsky and Dim-
itrova (2004). Turetsky and Dimitrova (2004)
proposed a system to automatically align written
screenplays with their videos. One of the crucial
steps, they noted, is to parse a screenplay into its
different elements: scene boundaries, scene de-
scriptions, character names, and dialogues. They
proposed a grammar to parse screenplays and
show results for aligning one screenplay with its
video. Weng et al. (2009) motivated the need for
screenplay parsing from a social network analy-
sis perspective. They proposed a set of opera-
tions on social networks extracted from movies
and television shows in order to find what they
called hidden semantic information. They pro-
posed techniques for identifying lead roles in bi-
lateral movies (movies with two main characters),
for performing community analysis, and for au-
tomating the task of story segmentation. Gil et
al. (2011) extracted character interaction networks
from plays and movies. They were interested in
automatically classifying plays and movies into
different genres by making use of social network
analysis metrics. They acknowledged that the
scripts found on the internet are not in consistent
formats, and proposed a regular expression based
system to identify scene boundaries and character
names.
While there is motivation in the literature to
parse screenplays, none of the aforementioned
work addresses the task formally. In this paper, we
formalize the task and propose a machine learning
based approach that is significantly more effec-
tive and tolerant of anomalous structure than the
baseline. We evaluate our models on their ability
to identify scene boundaries and character names,
but also on their ability to identify other important
elements of a screenplay, such as scene descrip-
tions and dialogues.
4 Data
We crawled the Internet Movie Script Database
(IMSDB) website
1
to collect movie screenplays.
We crawled a total of 674 movies. Movies
that are well structured have the property that
scene boundaries and scene descriptions, charac-
ter names, and dialogues are all at different but
fixed levels of indentation.
2
For example, in the
movie in Figure 1, all scene boundaries and scene
1
http://www.imsdb.com
2
By level of indentation we mean the number of spaces
from the start of the line to the first non-space character.
51
Figure 1: Example screenplay: first column shows the tags we assign to each line in the screenplay. M
stands for ?Meta-data?, S stands for ?Scene boundary?, N stands for ?Scene description?, C stands for
?Character name?, and D stands for ?Dialogue.? We also show the lines that are at context -2 and +3 for
the line ?CRAWFORD.?
descriptions are at the same level of indentation,
equal to five spaces. All character names are at
a different but fixed level of indentation, equal to
20 spaces. Dialogues are at an indentation level
of eight spaces. These indentation levels may vary
from one screenplay to the other, but are consis-
tent within a well formatted screenplay. Moreover,
the indentation level of character names is strictly
greater than the indentation level of dialogues,
which is strictly greater than the indentation level
of scene boundaries and scene descriptions. For
each crawled screenplay, we found the frequency
of unique indentation levels in that screenplay. If
the top three unique frequencies constituted 90%
of the total lines of a screenplay, we flagged that
the movie was well-structured, and assigned tags
based on indentation levels. Since scene bound-
aries and scene descriptions are at the same level
of indentation, we disambiguate between them by
utilizing the fact that scene boundaries in well-
formatted screenplays start with tags such as INT.
and EXT. We programmatically checked the sanity
of these automatically tagged screenplays by using
the following procedure: 1) check if scene descrip-
tions are between scene boundaries and character
names, 2) check if dialogues are between charac-
ter names, and 3) check if all character names are
within two scene boundaries. Using this method-
ology, we were able to tag 222 movies that pass
the sanity check.
Data # S # N # C # D # M
TRAIN 2,445 21,619 11,464 23,814 3,339
DEV1 714 7,495 4,431 9,378 467
DEV2 413 5,431 2,126 4,755 762
TEST 164 845 1,582 3,221 308
Table 1: Data distribution
Table 1 gives the distribution of our training, de-
velopment and test sets. We use a random sub-
set of the aforementioned set of 222 movies for
training purposes, and another random subset for
development. We chose 14 movies for the train-
ing set and 9 for the development set. Since hu-
man annotation for the task is expensive, instead of
getting all 23 movies checked for correctness, we
asked an annotator to only look at the development
set (9 movies). The annotator reported that one
out of 9 movies was not correctly tagged. We re-
moved this movie from the development set. From
the remaining 8 movies, we chose 5 as the first de-
velopment set and the remaining 3 as the second
development set. For the test set, we asked our
annotator to annotate a randomly chosen screen-
play (Silver Linings Playbook) from scratch. We
chose this screenplay from the set of movies that
52
we were unable to tag automatically, i.e. not from
the set of 222 movies.
5 Baseline System
Gil et al. (2011) mention the use of regular expres-
sions for tagging screenplays. However, they do
not specify the regular expressions or their exact
methodology. We use common knowledge about
the structure of the screenplay (underlined text in
section 2) to build a baseline system, that uses reg-
ular expressions and takes into account the gram-
mar of screenplays.
Since scene descriptions, characters and dia-
logues are relative to the scene boundary, we do
a first pass on the screenplay to tag scene bound-
aries. We created a dictionary of words that are
expected to indicate scene boundaries. We use
this dictionary for tagging lines in the screenplay
with the tag ?S?. We tag all the lines that con-
tain tags indicating a character (V.O., O.S.) with
?C?. We built a dictionary of meta-data tags that
contains patterns such as ?CUT TO:, DISSOLVE
TO.? We tag all the remaining untagged lines con-
taining these patterns with the tag ?M.? This ex-
hausts the list of regular expression matches that
indicate a certain tag.
In the next pass, we incorporate prior knowl-
edge that scene boundaries and character names
are capitalized. For this, we tag all the untagged
lines that are capitalized, and that have more than
three words as scene boundaries (tag ?S?). We tag
all the untagged lines that are capitalized, and that
have less than four words as character (tag ?C?).
The choice of the number four is not arbitrary;
we examined the set of 222 screenplays that was
tagged using indentation information and found
that less than two percent of the character names
were of length greater than three.
Finally, we incorporate prior knowledge about
relative positions of dialogues and scene descrip-
tions to tag the remaining untagged lines with one
of two tags: ?D? or ?N?. We tag all the untagged
lines between a scene boundary and the first char-
acter occurrence as ?N?. We tag all the lines be-
tween consecutive character occurrences, the last
character occurrence and the scene boundary as
?D?.
We use this baseline system, which incorporates
all of the prior knowledge about the structure of
screenplays, to tag movies in our first development
set DEV1 (section 8). We report a macro-F1 mea-
sure for the five tags as 0.96. This confirms that
our baseline is well suited to parse screenplays that
are well structured.
6 Machine Learning Approach
Note that our baseline system is not dependent on
the level of indentation (it achieves a high macro-
F1 measure without using indentation informa-
tion). Therefore, we have already dealt with one
common problem with screenplays found on the
web: bad indentation. However, there are other
problems, some of which we noticed in the lim-
ited data we manually examined, and others that
we anticipate: (1) missing scene boundary spe-
cific patterns (such as INT./EXT.) from the scene
boundary lines, (2) uncapitalized scene boundaries
and (3) uncapitalized character names. These are
problems that a regular expression based system
is not well equipped to deal with. In this sec-
tion, we discuss a strategy for dealing with screen-
plays, which might have anomalies in their struc-
ture, without requiring additional annotations.
We synthesize training and development data
to learn to handle the aforementioned three types
of anomalies. We create eight copies of our
TRAIN set: one with no anomalies, represented as
TRAIN_000,
3
one in which character names are
uncapitalized, represented as TRAIN_001, one in
which both scene boundaries and character names
are uncapitalized, represented as TRAIN_011,
and so on. Similarly, we create eight copies
of our DEV1 set: {DEV1_000, DEV1_001, ...,
DEV1_111}. Now we have eight training and
eight development sets. We train eight models,
and choose the parameters for each model by tun-
ing on the respective development set. However,
at test time, we require one model. Moreover, our
model should be able to handle all types of anoma-
lies (all of which could be present in a random or-
der). We experiment with three ensemble learning
techniques and choose the one that performs the
best on the second development set, DEV2. We
add all three types of anomalies, randomly, to our
DEV2 set.
For training individual models, we use Support
Vector Machines (SVMs), and represent data as
feature vectors, discussed in the next section.
3
Each bit refers to the one type of anomaly described
in the previous paragraph. If the least significant bit is 1,
this means, the type of anomaly is uncapitalized characters
names.
53
7 Features
We have six sets of features: bag-of-words fea-
tures (BOW), bag-of-punctuation-marks features
(BOP), bag-of-terminology features (BOT), bag-
of-frames features (BOF), bag-of-parts-of-speech
features (POS), and hand-crafted features (HAND).
We convert each line of a screenplay (input ex-
ample) into a feature vector of length 5,497: 3,946
for BOW, 22 for BOP, 2*58 for BOT, 2*45 for
POS, 2*651 for BOF, and 21 for HAND.
BOW, BOP, and BOT are binary features; we
record the presence or absence of elements of each
bag in the input example. The number of ter-
minology features is multiplied by two because
we have one binary vector for ?contains term?,
and another binary vector for ?is term.? We have
two sets of features for POS and BOF. One set
is binary and similar to other binary features that
record the presence or absence of parts-of-speech
and frames in the input example. The other set
is numeric. We record the normalized counts of
each part-of-speech and frame respectively. The
impetus to design this second set of features for
parts-of-speech and frames is the following: we
expect some classes to have a characteristic dis-
tribution of parts-of-speech and frames. For ex-
ample, scene boundaries contain the location and
time of scene. Therefore, we expect them to have
a majority of nouns, and frames that are related to
location and time. For the scene boundary in Fig-
ure 1 (EXT. FBI ACADEMY ... - DAY), we find
the following distribution of parts of speech and
frames: 100% nouns, 50% frame LOCALE (with
frame evoking element grounds), and 50% frame
CALENDRIC_UNIT (with frame evoking element
DAY). Similarly, we expect the character names to
have 100% nouns, and no frames.
We use Stanford part-of-speech tagger
(Toutanova et al., 2003) for obtaining the
part-of-speech tags and Semafor (Chen et al.,
2010) for obtaining the FrameNet (Baker et
al., 1998) frames present in each line of the
screenplay.
We devise 21 hand-crafted features. Six-
teen of these features are binary (0/1). We
list these features here (the feature names
are self-explanatory): has-non-alphabetical-
chars, has-digits-majority, has-alpha-majority,
is-quoted, capitalization (has-all-caps, is-all-
caps), scene boundary (has-INT, has-EXT),
date (has-date, is-date), number (has-number,
is-number), and parentheses (is-parenthesized,
starts-with-parenthesis, ends-with-parenthesis,
contains-parenthesis). We bin the preceding
number of blank lines into four bins: 0 for no
preceding blank lines, 1 for one preceding blank
line, 2 for two preceding blank lines, and so on.
We also bin the percentage of capitalized words
into four bins: 0 for the percentage of capitalized
words lying between 0-25%, 1 for 25-50%, and
so on. We use three numeric features: number of
non-space characters (normalized by the maxi-
mum number of non-space characters in any line
in a screenplay), number of words (normalized
by the maximum number of words in any line in
a screenplay), and number of characters (normal-
ized by the maximum number of characters in any
line in a screenplay).
For each line, say line
i
, we incorporate con-
text up to x lines. Figure 1 shows the lines
at context -2 and +3 for the line contain-
ing the text CRAWFORD. To do so, we ap-
pend the feature vector for line
i
by the fea-
ture vectors of line
i?1
, line
i?2
, . . . line
i?x
and
line
i+1
, line
i+2
, . . . line
i+x
. x is one of the pa-
rameters we tune at the time of training. We refer
to this parameter as CONTEXT.
8 Experiments and Results
In this section, we present experiments and results
for the task of tagging the lines of a screenplay
with one of five tags: {S, N, C, D, M}. Table 1
shows the data distribution. For parameter tun-
ing, we use DEV1 (section 8.1). We train sepa-
rate models on different types of known and antici-
pated anomalies (as discussed in section 6). In sec-
tion 8.2, we present strategies for combining these
models. We select the right combination of mod-
els and features by tuning on DEV2. Finally, we
show results on the test set, TEST. For all our ex-
periments, we use the default parameters of SVM
as implemented by the SMO algorithm of Weka
(Hall et al., 2009). We use a linear kernel.
4
8.1 Tuning learning parameters
We tune two parameters: the amount of train-
ing data and the amount of CONTEXT (section 7)
required for learning. We do this for each of
the eight models (TRAIN_000/DEV1_000, ...,
TRAIN_111/DEV1_111). We merge training
4
We tried the polynomial kernel up to a degree of four and
the RBF kernel. They performed worse than the linear kernel.
54
10 20 30 40 50 60 70 80 90 1000.7
0.75
0.8
0.85
0.9
0.95
1
% TRAINING DATA USED
MAC
RO F
?ME
ASU
RE
TRAIN?DEV1 (000)
 
 
context?0context?1context?2context?3context?4context?5
Figure 2: Learning curve for training on
TRAIN_000 and testing on DEV1_000. X-axis
is the % of training data, in steps of 10%. Y-axis
is the macro-F1 measure for the five classes.
data from all 14 movies into one (TRAIN). We
then randomize the data and split it into 10 pieces
(maintaining the relative proportions of the five
classes). We plot a learning curve by adding 10%
of training data at each step.
Figure 2 shows the learning curve for train-
ing a model on TRAIN_000 and testing on
DEV1_000.
5
The learning curve shows that the
performance of our classifier without any context
is significantly worse than the classifiers trained
on context. Moreover, the learning saturates early,
and stabilizes at about 50% of the training data.
From the learning curves, we pick CONTEXT
equal to 1, and the amount of training data equal
to 50% of the entire training set.
Table 2 shows a comparison of our rule based
baseline with the models trained using machine
learning. For the 000 setting, when there is no
anomaly in the screenplay, our rule based base-
line performs well, achieving a macro-F1 measure
of 0.96. However, our machine learning model
outperforms the baseline by a statistically signif-
icant margin, achieving a macro-F1 measure of
0.99. We calculate statistical significance using
McNemar?s significance test, with significance de-
fined as p < 0.05.
6
Results in Table 2 also show
that while a deterministic regular-expression based
system is not well equipped to handle anomalies,
there is enough value in our feature set, that our
machine learning based models learn to adapt to
any combination of the three types of anomalies,
achieving a high F1-measure of 0.98 on average.
5
Learning curves for all our other models were similar.
6
We use the same test for reporting other statistically sig-
nificance results in the paper.
8.2 Finding the right ensemble and feature
selection
We have trained eight separate models, which
need to be combined into one model that we will
make predictions at the test time. We explore the
following ways of combining these models:
1. MAJ: Given a test example, we get a vote
from each of our eight models, and take a ma-
jority vote. At times of a clash, we pick one
randomly.
2. MAX: We pick the class predicted by the
model that has the highest confidence in its
prediction. Since the confidence values are
real numbers, we do not see any clashes.
3. MAJ-MAX: We use MAJ but at times of a
clash, we pick the class predicted by the clas-
sifier that has the highest confidence (among
the classifiers that clash).
Table 3 shows macro-F1 measures for the three
movies in our DEV2 set. Note, we added the three
types of anomalies (section 6) randomly to the
DEV2 set for tuning the type of ensemble. We
compare the performance of the three ensemble
techniques with the individual classifiers (trained
on TRAIN_000, ... TRAIN_111).
The results show that all our ensembles (ex-
cept MAX for the movie The Last Temptation of
Christ) perform better than the individual models.
Moreover, the MAJ-MAX ensemble outperforms
the other two by a statistically significant margin.
We thus choose MAJ-MAX as our final classifier.
Table 4 shows results for removing one of all
feature sets, one at a time. These results are for our
final model, MAJ-MAX. The row ?All? shows the
results when we use all our features for training.
The consecutive rows show the result when we re-
move the mentioned feature set. For example, the
row ?- BOW? shows the result for our classifier
that was trained without the bag of words feature
set.
Table 4 shows that the performance drops the
most for bag of words (BOW) and for our hand-
crafted features (HAND). The next highest drop
is for the bag of frames feature set (BOF). Er-
ror analysis revealed that the major drop in per-
formance because of the removal of the BOF fea-
tures was not due the drop in the performance
of scene boundaries, counter to our initial intu-
ition. The drop was because the recall of dia-
55
000 001 010 011 100 101 110 111
Rule based 0.96 0.49 0.70 0.23 0.93 0.46 0.70 0.24
ML model 0.99 0.99 0.98 0.99 0.97 0.98 0.98 0.98
Table 2: Comparison of performance (macro-F1 measure) of our rule based baseline with our machine
learning based models on development sets DEV1_000, DEV1_001, ..., DEV1_111. All models are
trained on 50% of the training set, with the feature space including CONTEXT equal to 1.
Movie 000 001 010 011 100 101 110 111 MAJ MAX MAJ-MAX
LTC 0.87 0.83 0.79 0.94 0.91 0.86 0.79 0.96 0.97 0.95 0.98
X-files 0.87 0.84 0.79 0.93 0.86 0.84 0.79 0.92 0.94 0.94 0.96
Titanic 0.87 0.87 0.81 0.94 0.86 0.83 0.82 0.93 0.94 0.95 0.97
Average 0.87 0.85 0.80 0.94 0.88 0.84 0.80 0.94 0.95 0.95 0.97
Table 3: Macro-F1 measure for the five classes for testing on DEV2 set. 000 refers to the model trained
on data TRAIN_000, 001 refers to the model trained on data TRAIN_001, and so on. MAJ, MAX, and
MAJ-MAX are the three ensembles. The first column is the movie name. LTC refers to the movie ?The
Last Temptation of Christ.?
Feature set LTC X-files Titanic
All 0.98 0.96 0.97
- BOW 0.94 0.92 0.94
- BOP 0.98 0.97 0.97
- BOT 0.97 0.95 0.96
- BOF 0.96 0.93 0.96
- POS 0.98 0.96 0.95
- HAND 0.94 0.93 0.93
Table 4: Performance of MAJ-MAX classifier with
feature removal. Statistically significant differ-
ences are in bold.
logues decreases significantly. The BOF features
were helping in disambiguating between the meta-
data, which usually have no frames associated
with them, and dialogues. Removing bag of punc-
tuation (BOP) results in a significant increase in
the performance for the movie X-files, with a small
increase for other two movies. We remove this
feature from our final classifier. Removing parts
of speech (POS) results in a significant drop in the
overall performance for the movie Titanic. Error
analysis revealed that the drop in performance here
was in fact due the drop in performance of scene
boundaries. Scene boundaries almost always have
100% nouns and the POS features help in cap-
turing this characteristic distribution indicative of
scene boundaries. Removing bag of terminology
(BOT) results in a significant drop in the overall
performance of all movies. Our results also show
that though the drop in performance for some fea-
Baseline MAJ-MAX
Tag P R F1 P R F1
S 0.27 1.00 0.43 0.99 1.00 0.99
N 0.21 0.06 0.09 0.88 0.95 0.91
C 0.89 1.00 0.94 1 0.92 0.96
D 0.99 0.94 0.96 0.98 0.998 0.99
M 0.68 0.94 0.79 0.94 0.997 0.97
Avg 0.61 0.79 0.69 0.96 0.97 0.96
Table 5: Performance comparison of our rule
based baseline with our best machine learning
model on the five classes.
N
B
N
MAJ-MAX
N
G
# Nodes 202 37 41
# Links 1252 331 377
Density 0.036 0.276 0.255
Table 6: A comparison of network statistics for
the three networks extracted from the movie Silver
Linings Playbook.
ture sets is larger than the others, it is the conjunc-
tion of all features that is responsible for a high
F1-measure.
8.3 Performance on the test set
Table 5 shows a comparison of the performance
of our rule based baseline with our best machine
learning based model on our test set, TEST. The
results show that our machine learning based mod-
els outperform the baseline with a large and sig-
56
Model Degree Weighted Degree Closeness Betweenness PageRank Eigen
N
B
0.919 0.986 0.913 0.964 0.953 0.806
N
MAJ-MAX
0.997 0.997 0.997 0.997 0.998 0.992
Table 7: A comparison of Pearson?s correlation coefficients of various centrality measures for N
B
and
N
MAJ-MAX
with N
G
.
nificant margin on all five classes (0.96 versus
0.69 macro-F1 measure respectively). Note, as ex-
pected, the recall of the baseline is generally high,
while the precision is low. Moreover, for this test
set, the baseline performs relatively well on tag-
ging character names and dialogues. However, we
believe that the performance of the baseline is un-
predictable. It may get lucky on screenplays that
are well-structured (in one way or the other), but
it is hard to comment on the robustness of its per-
formance. On the contrary, our ensemble is ro-
bust, hedging its bets on eight models, which are
trained to handle different types and combinations
of anomalies.
In tables 6 and 7, we present an extrinsic evalua-
tion on the test set. We extract a network from our
test movie screenplay (Silver Linings Playbook)
by using the tags of the screenplay as follows
(Weng et al., 2009): we connect all characters hav-
ing a dialogue with each other in a scene with
links. Nodes in this network are characters, and
links between two characters signal their partici-
pation in the same scene. We form three such net-
works: 1) based on the gold tags (N
G
), 2) based on
the tags predicted by MAJ-MAX (N
MAJ-MAX
),
and 3) based on the tags predicted by our base-
line (N
B
). Table 6 compares the number of nodes,
number of links, and graph density of the three
networks. It is clear from the table that the net-
work extracted by using the tags predicted by
MAJ-MAX is closer to the gold network.
Centrality measures are one of the most funda-
mental social network analysis metrics used by so-
cial scientists (Wasserman and Faust, 1994). Ta-
ble 7 presents a comparison of Pearson?s correla-
tion coefficient for various centrality measures for
{N
B
, N
G
}, and {N
MAJ-MAX
, N
G
} for the top
ten characters in the movie. The table shows that
across all these measures, the statistics obtained
using the network N
MAJ-MAX
are significantly
more correlated to the gold network (N
G
), as com-
pared the the baseline network (N
B
).
9 Conclusion and Future Work
In this paper, we presented a formalization of the
task of parsing movie screenplays. We presented
an NLP and ML based approach to the task, and
showed that this approach outperforms the regular
expression based approach by a large and signifi-
cant margin. One of the main challenges we faced
early on was the absence of training and test data.
We proposed a methodology for learning to han-
dle anomalies in the structure of screenplays with-
out requiring additional annotations. We believe
that the machine learning approach proposed in
this paper is general, and may be used for parsing
noisy documents outside of the context of movie
screenplays.
In the future, we will apply our approach to
parse other semi-structured sources of social net-
works such as television show series and theatrical
plays.
Acknowledgments
We would like to thank anonymous reviewers for
their useful comments. We would also like to
thank Caronae Howell for her insightful com-
ments. Agarwal was funded by IBM Ph.D. fellow-
ship 2013-2014. This paper is based upon work
supported in part by the DARPA DEFT Program.
The views expressed are those of the authors and
do not reflect the official policy or position of the
Department of Defense or the U.S. Government.
57
References
Apoorv Agarwal and Owen Rambow. 2010. Auto-
matic detection and classification of social events.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1024?1034, Cambridge, MA, October. Association
for Computational Linguistics.
Apoorv Agarwal, Anup Kotalwar, and Owen Ram-
bow. 2013a. Automatic extraction of social net-
works from literary text: A case study on alice in
wonderland. In the Proceedings of the 6th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2013).
Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and
Owen Rambow. 2013b. Sinnet: Social interaction
network extractor from text. In Sixth International
Joint Conference on Natural Language Processing,
page 33.
C. Baker, C. Fillmore, and J. Lowe. 1998. The berke-
ley framenet project. Proceedings of the 17th inter-
national conference on Computational linguistics, 1.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 264?267, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 138?147.
Sebastian Gil, Laney Kuenzel, and Suen Caroline.
2011. Extraction and analysis of character interac-
tion networks from plays and movies. Technical re-
port, Stanford University.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Hua He, Denilson Barbosa, and Grzegorz Kondrak.
2013. Identification of speakers in novels. The
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013).
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL.
Robert Turetsky and Nevenka Dimitrova. 2004.
Screenplay alignment for closed-system speaker
identification and analysis of feature films. In Mul-
timedia and Expo, 2004. ICME?04. 2004 IEEE In-
ternational Conference on, volume 3, pages 1659?
1662. IEEE.
Stanley Wasserman and Katherine Faust. 1994. Social
Network Analysis: Methods and Applications. New
York: Cambridge University Press.
Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2006.
Movie analysis based on roles? social network. In
Proceedings of IEEE Int. Conference Multimedia
and Expo., pages 1403?1406.
Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2007.
Rolenet: treat a movie as a small society. In Pro-
ceedings of the international workshop on Workshop
on multimedia information retrieval, pages 51?60.
ACM.
Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2009.
Rolenet: Movie analysis from the perspective of so-
cial networks. Multimedia, IEEE Transactions on,
11(2):256?271.
58
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 30?33,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Using Frame Semantics in Natural Language Processing
Apoorv Agarwal
Dept. of Computer Science
Columbia University
New York, NY
apoorv@cs.columbia.edu
Daniel Bauer
Dept. of Computer Science
Columbia University
New York, NY
bauer@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY
rambow@ccls.columbia.edu
Abstract
We summarize our experience using
FrameNet in two rather different projects
in natural language processing (NLP).
We conclude that NLP can benefit from
FrameNet in different ways, but we sketch
some problems that need to be overcome.
1 Introduction
We present two projects at Columbia in which we
use FrameNet. In these projects, we do not de-
velop basic NLP tools for FrameNet, and we do
not develop FramNets for new languages: we sim-
ply use FrameNet or a FrameNet parser in an NLP
application. The first application concerns the ex-
traction of social networks from narrative texts.
The second application aims at generating three-
dimensional pictures from textual descriptions.
The applications are very different: they differ
in terms of their goals, and they differ in terms
of how they use FrameNet. However, they have
in common that they can use FrameNet because it
provides a particular level of semantic abstraction
which is suited for both applications. Consider
verbs of saying, such as declare, deny, mention,
remark, tell, or say: they do not have the same
meaning. However, they share enough common
meaning, and in particular they share the same set
of participants, so that for our two applications
they can be considered as interchangeable: they
represent the communication of verbal informa-
tion (the Message) from a Speaker to an Ad-
dressee. This is precisely what the Statement
frame encodes. We will use this example in the
next two sections, in which we discuss our two
projects in more detail.
2 Using an Off-the-Shelf FrameNet
Parser
Our first application is SINNET, a system that ex-
tracts a social network from narrative text. It uses
the notion of a social event (Agarwal et al., 2010),
a particular kind of event which involves (at least)
two people such that at least one of them is aware
of the other person. If only one person is aware
of the event, we call it Observation (OBS): for
example, someone is talking about someone else
in their absence. If both people are aware of the
event, we call it Interaction (INR): for example,
one person is telling the other a story. Our claim
is that links in social networks are in fact made
up of social events: OBS social events give rise
to one-way links, and INR social events to two-
way links. For more information, see (Agarwal
and Rambow, 2010; Agarwal et al., 2013a; Agar-
wal et al., 2013b).
From an NLP point of view, we have a difficult
cluster of phenomena: we have a precise defini-
tion of what we want to find, but it is based on the
cognitive state of the event participants, which is
almost never described explicitly in the text. Fur-
thermore, the definitions cover a large number of
diverse situations such as talking, spying, having
lunch, fist fighting, or kissing. Furthermore, some
semantic differences are not relevant: verbs such
as talk, tell, deny, all have the same meaning with
respect to social events. Finally, in order to de-
code the events in terms of social events, we need
to understand the roles: if I am talking to Sudeep
about Mae, Sudeep and I have an INR social event
with each other, and we both have a OBS social
event with Mae. Thus, this problem sounds like
an excellent application for frame semantics!
We present initial results in (Agarwal et al.,
2014), and summarize them here. We use Semafor
(Chen et al., 2010) as a black box to obtain the se-
mantic parse of a sentence. However, there are
several problems:
? FrameNet does not yet have complete lexical
coverage.
? Semafor does not produce a single semantic
30
representation for a sentence, as we would
want in order to perform subsequent process-
ing. Instead, it annotates separate, discon-
nected frame structures for each frame evok-
ing element it finds.
? The data annotated with FrameNet consists
of the example sentences as well as a compar-
atively small corpus. For this reason, it is not
easy to use standard machine learning tech-
niques for frame semantic parsing. As a re-
sult, the output is fairly errorful (as compared
to, say, a state-of-the-art dependency parser
trained on nearly a million annotated words).
Errors include mislabeled frames, mislabeled
frame elements, and missing frame elements.
To overcome these problems, we constructed
several tree representations out of the partial an-
notations returned by Semafor. We then used tree
kernels on these syntactic and semantic tree rep-
resentations, as well as bags of words. The tree
kernels can automatically identify important sub-
structures in the syntactic and semantic trees with-
out the need for feature engineering on our part.
Our hypothesis is that the kernels can learn which
parts of the semantic structures are reliable and
can be used for prediction.
The tree structures are shown in Figure 1. The
structure on the left (FrameForest) is created by
taking all identified instances of frames, and col-
lecting them under a common root node. The
frame elements are filled in with dependency syn-
tax. The structure on the right (FrameTree) is our
attempt to create a single arborescent structure to
capture the semantics of the whole sentence. Our
third structure, FrameTreeProp (not shown), is de-
rived from FrameTree by multiplying the nodes of
interest up the path from their normal place to the
root. This allows us to overcome problems with
the limited locality of the tree kernels.
We present some results in Table 1. Compar-
ing lines ?Syntax? with ?Synt FrameTreeProp?,
we see a slight but statistically significant increase.
This increase comes from using FrameNet seman-
tics. When we look at only the semantic structures,
we see that they all perform worse than syntax on
its own. ?BOF? is simply a bag of frames; we
see that the arborescent structures outperform it,
so semantic structure is useful in addition to se-
mantic tags. ?RULES? is a comprehensive set of
hand-written rules we attached to frames; if frame
Detection
Model P R F1
Syntax 0.464 0.751 0.574
RULES 0.508 0.097 0.164
BOF 0.296 0.416 0.346
FrameForest 0.331 0.594 0.425
FrameTree 0.295 0.594 0.395
FrameTreeProp 0.308 0.554 0.396
All 0.494 0.641 0.558
Synt FrameTreeProp 0.484 0.740 0.585
Table 1: Results for Social Event Detection.
?Syntax? is an optimized model using various
syntactic representations (Agarwal and Rambow,
2010). The next five models are the novel se-
mantic features and structures. ?All? refers to the
model that uses all the listed structures together.
?Synt FrameTreeProp? is a linear combination of
?Syntax? and FrameTreeProp.
semantic parsing were perfect, these rules should
perform pretty well. They do in fact achieve the
best precision of all our systems, but the recall is
so low that overall they are not useful. We inter-
pret this result as supporting our claim that part of
the problem with using frame-semantic parsers is
the high error rate.
Even though the gain so far from frame seman-
tic parsing is small, we are encouraged by the fact
that an off-the-shelf semantic parser can help at
all. We are currently exploring other semantic
structures we can create from the semantic parse,
including structures which are dags rather than
trees. We would like to point out that the com-
bination of the parser, the creation of our seman-
tic trees, and the training with tree kernels can be
applied to any other problem that is sensitive to
the meaning of text. Based on our experience, we
expect to see an increase in ?black box? uses of
FrameNet parsing for other applications in NLP.
3 Extending the FrameNet Resource
FrameNet can be a useful starting point for a richer
knowledge representation which is needed for a
specific task. In our example, we need a repre-
sentation that we can use in the WordsEye project
(Coyne and Sproat, 2001), in which pictures are
created automatically from text descriptions. This
can be understood as providing a particular type
of decompositional semantics for the input text.
31
ROOT
Commerce buy
Target
4
Buyer
T1-Ind
Seller
from
T2-Grp
Statement
Target
claimed
4
Speaker
T1?-Ind
Message
4
Statement
Speaker
T1-Ind
Coleman
Message
Commerce buy
Buyer
T1?-Ind
he
Seller
T2-Grp
defendants
Figure 1: Semantic trees for the sentence ?Coleman claimed [he]
T1?Ind
bought drugs from the
[defendants]
T2?Grp
.?. The tree on the left is FrameForest and the tree on the right is FrameTree. 4
in FrameForest refers to the subtree (bought (T1-Ind) (from T2-Grp)). Ind refers to individual and Grp
refers to group.
We extend FrameNet in two ways to obtain the re-
source we need, which we call VigNet (Coyne et
al., 2011).
The pictures created by the WordsEye system
are based on spatial arrangements (scenes) of pre-
defined 3D models. At a low level, scenes are de-
scribed by primitive spatial relations between sets
of these models (The man is in front of the woman.
He is looking at her. His mouth is open.). We
would like to use WordsEye to depict scenarios,
events, and actions (John told Mary his life story).
These can be seen as complex relations between
event participants.
We turn to FrameNet frames as representations
for such relations. FrameNet offers a large in-
ventory of frames, together with additional struc-
tured information about them in the form of frame
relations. Most importantly, FrameNet provides
example annotations illustrating the patterns in
which frames are evoked and syntactic arguments
are mapped to frame elements.
However, there are two main problems if we
want to turn frame annotations into pictures. First,
in frame annotations frame elements are only filled
with text spans, not with semantic objects. Anno-
tations are therefore restricted to individual predi-
cate/argument structures and do not represent the
meaning of a full sentence. To address this prob-
lem we essentially use FrameNet frames as an in-
ventory of predicates in a graph-based semantic
representation. We use semantic nodes, which are
identifiers representing events and entities that fill
frame elements. Frame instances then describe re-
lations between these semantic nodes, building a
graph structure that can represent a full text frag-
ment (including coreference). We are planning
to develop parsers that convert text directly into
such graph-based representations, inspired by re-
cent work on semantic parsing (Jones et al., 2012).
Second, FrameNet frames usually describe
functional relationships between frame elements,
not graphical ones. To turn a frame into its graphi-
cal representation we therefore need (a) a set of of
graphical frames and a formal way of decompos-
ing these frames into primitives and (b) a mech-
anism for relating FrameNet frames to graphi-
cal frames. Our solution is VigNet (Coyne et
al., 2011), an extension of FrameNet. VigNet
makes use of existing frame-to-frame relations
to extend FrameNet with a number of graphical
frames called Vignettes. Vignettes are subframes
of FrameNet frames, each representing a specific
way in which a frame can be realized based on the
specific lexical unit or on context. For instance,
a proper visualization of the INGESTION frame
will depend on the INGESTOR (human vs. ani-
mals of different sizes), the INGESTIBLE (differ-
ent types of foods and drinks are ingested accord-
ing to different social conventions, each a differ-
ent Vignette). Note however, that many FrameNet
frames provide useful abstractions that allow us
to use a single Vignette as a good default visu-
alization for the entire frame. For instance, all
lexical units in the STATEMENT frame can be de-
picted as the SPEAKER standing opposite of the
ADDRESSEE with an open mouth.
A new frame-to-frame relation, called subframe
parallel, is used to decompose a Vignette into
32
graphical sub-relations, which are in turn frames
(either graphical primitives or other vignettes).
Like any frame-to-frame relation, it maps frame
elements of the source frame to frame elements
of the target frame. New frame elements can also
be introduced. For instance, one Vignette for IN-
GESTION that can be used if the INGESTIBLE is a
liquid contains a new frame element CONTAINER.
The INGESTOR is holding the container and the
liquid is in the container.
We have populated the VigNet resource us-
ing a number of different approaches (Coyne et
al., 2012), including multiple choice questions on
Amazon Mechanical Turk to define vignettes for
locations (rooms), using the system itself to define
locations, and a number of web-based annotation
tools to define vignettes for actions.
An ongoing project is exploring the use of
WordsEye and VigNet as a tool for field linguists
and for language documentation and preserva-
tion. The WordsEye Linguistics Toolkit (WELT,
(Ulinski et al., 2014)) makes it easy to produce
pictures for field linguistic elicitation. It will
also provide an environment to essentially de-
velop language specific VigNets as models of the
syntax/semantics interface and conceptual cate-
gories. This work may be relevant to other projects
that aim to build non-English and multi-lingual
FrameNets.
4 Conclusion
We have tried to motivate the claim that FrameNet
provides the right layer of semantic abstraction
for many NLP applications by summarizing two
ongoing NLP projects at Columbia. We have
also suggested that part of the problem in using
FrameNet in NLP projects is the lack of a single
structure that is produced, either in manual anno-
tations, or in the output of a FrameNet parser. We
suspect that research into how to construct such
unified semantic representations will continue to
be a major component of the use of FrameNet in
NLP.
Acknowledgments
This paper is based upon work supported in part by
the NSF (grants IIS-0713548 and IIS-0904361),
and by the DARPA DEFT Program. We thank
our collaborators on the two projects used as ex-
amples in this extended abstract. We thank Chuck
Fillmore for FrameNet.
References
Apoorv Agarwal and Owen Rambow. 2010. Automatic de-
tection and classification of social events. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1024?1034, Cambridge,
MA, October. Association for Computational Linguistics.
Apoorv Agarwal, Owen C. Rambow, and Rebecca J. Passon-
neau. 2010. Annotation scheme for social network ex-
traction from text. In Proceedings of the Fourth Linguistic
Annotation Workshop.
Apoorv Agarwal, Anup Kotalwar, and Owen Rambow.
2013a. Automatic extraction of social networks from lit-
erary text: A case study on alice in wonderland. In the
Proceedings of the 6th International Joint Conference on
Natural Language Processing (IJCNLP 2013).
Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and Owen
Rambow. 2013b. Sinnet: Social interaction network ex-
tractor from text. In Sixth International Joint Conference
on Natural Language Processing, page 33.
Apoorv Agarwal, Sriramkumar Balasubramanian, Anup Ko-
talwar, Jiehan Zheng, and Owen Rambow. 2014. Frame
semantic tree kernels for social network extraction from
text. In Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden.
Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A.
Smith. 2010. Semafor: Frame argument resolution with
log-linear models. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 264?267, Up-
psala, Sweden, July. Association for Computational Lin-
guistics.
Bob Coyne and Richard Sproat. 2001. Wordseye: an au-
tomatic text-to-scene conversion system. In 28th annual
conference on Computer graphics and interactive tech-
niques.
Bob Coyne, Daniel Bauer, and Owen Rambow. 2011. Vi-
gnet: Grounding language in graphics using frame seman-
tics. In ACL Workshop on Relational Semantics (RELMS),
Portland, Oregon.
Bob Coyne, Alex Klapheke, Masoud Rouhizadeh, Richard
Sproat, and Daniel Bauer. 2012. Annotation tools and
knowledge representation for a text-to-scene system. In
COLING, Mumbai, India.
Bevan Jones, Jacob Andreas*, Daniel Bauer*, Karl Moritz
Hermann*, and Kevin Knight. 2012. Semantics-based
machine translation with hyperedge replacement gram-
mars. In COLING, Mumbai, India. *first authorship
shared.
Morgan Ulinski, Anusha Balakrishnan, Daniel Bauer, Bob
Coyne, Julia Hirschberg, and Owen Rambow. 2014. Doc-
umenting endangered languages with the wordseye lin-
guistics tool. In Proceedings of the ACL ComputEL work-
shop: The use of computational methods in the study of
endangered languages, Baltimore, MD, USA.
33
