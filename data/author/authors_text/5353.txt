
c? 2002 Association for Computational Linguistics
Efficiently Computed Lexical Chains as an
Intermediate Representation for
Automatic Text Summarization
H. Gregory Silber? Kathleen F. McCoy?
University of Delaware University of Delaware
While automatic text summarization is an area that has received a great deal of attention in recent
research, the problem of efficiency in this task has not been frequently addressed. When the size
and quantity of documents available on the Internet and from other sources are considered, the
need for a highly efficient tool that produces usable summaries is clear. We present a linear-time
algorithm for lexical chain computation. The algorithm makes lexical chains a computationally
feasible candidate as an intermediate representation for automatic text summarization. A method
for evaluating lexical chains as an intermediate step in summarization is also presented and carried
out. Such an evaluation was heretofore not possible because of the computational complexity of
previous lexical chains algorithms.
1. Introduction
The overall motivation for the research presented in this article is the development of
a computationally efficient system to create summaries automatically. Summarization
has been viewed as a two-step process. The first step is the extraction of important
concepts from the source text by building an intermediate representation of some sort.
The second step uses this intermediate representation to generate a summary (Sparck
Jones 1993).
In the research presented here, we concentrate on the first step of the summariza-
tion process and follow Barzilay and Elhadad (1997) in employing lexical chains to
extract important concepts from a document. We present a linear-time algorithm for
lexical chain computation and offer an evaluation that indicates that such chains are
a promising avenue of study as an intermediate representation in the summarization
process.
Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in
the text summarization process. Attempts to determine the benefit of this proposal
have been faced with a number of difficulties. First, previous methods for computing
lexical chains have either been manual (Morris and Hirst 1991) or automated, but
with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997).
Because of this, computing lexical chains for documents of any reasonable size has
been impossible. We present here an algorithm for computing lexical chains that is
linear in space and time. This algorithm makes the computation of lexical chains
computationally feasible even for large documents.
? Department of Computer and Information Sciences, Newark, DE 19711. E-mail: silber@udel.edu
? Department of Computer and Information Sciences, Newark, DE 19711. E-mail: mccoy@mail.eecis.
udel.edu
488
Computational Linguistics Volume 28, Number 4
A second difficulty faced in evaluating Barzilay and Elhadad?s proposal is that it
is a proposal for the first stage of the summarization process, and it is not clear how
to evaluate this stage independent of the second stage of summarization. A second
contribution of this article is a method for evaluating lexical chains as an intermedi-
ate representation. The intuition behind the method is as follows. The (strong) lexical
chains in a document are intended to identify important (noun) concepts in the docu-
ment. Our evaluation requires access to documents that have corresponding human-
generated summaries. We run our lexical chain algorithm both on the document and
on the summary and examine (1) how many of the concepts from strong lexical chains
in the document also occur in the summary and (2) how many of the (noun) concepts
appearing in the summary are represented in strong lexical chains in the document.
Essentially, if lexical chains are a good intermediate representation for text sum-
marization, we expect that concepts identified as important according to the lexical
chains will be the concepts found in the summary. Our evaluation of 24 documents
with summaries indicates that indeed lexical chains do appear to be a promising av-
enue of future research in text summarization.
1.1 Description of Lexical Chains
The concept of lexical chains was first introduced by Morris and Hirst. Basically, lex-
ical chains exploit the cohesion among an arbitrary number of related words (Morris
and Hirst 1991). Lexical chains can be computed in a source document by grouping
(chaining) sets of words that are semantically related (i.e., have a sense flow). Iden-
tities, synonyms, and hypernyms/hyponyms (which together define a tree of ?is a?
relations between words) are the relations among words that might cause them to be
grouped into the same lexical chain. Specifically, words may be grouped when:
? Two noun instances are identical, and are used in the same sense.
(The house on the hill is large. The house is made of wood.)
? Two noun instances are used in the same sense (i.e., are synonyms).
(The car is fast. My automobile is faster.)
? The senses of two noun instances have a hypernym/hyponym relation
between them. (John owns a car. It is a Toyota.)
? The senses of two noun instances are siblings in the hypernym/hyponyn
tree. (The truck is fast. The car is faster.)
In computing lexical chains, the noun instances must be grouped according to the
above relations, but each noun instance must belong to exactly one lexical chain. There
are several difficulties in determining which lexical chain a particular word instance
should join. For instance, a particular noun instance may correspond to several differ-
ent word senses, and thus the system must determine which sense to use (e.g., should
a particular instance of ?house? be interpreted as sense 1, dwelling, or sense 2, legisla-
ture). In addition, even if the word sense of an instance can be determined, it may be
possible to group that instance into several different lexical chains because it may be
related to words in different chains. For example, the word?s sense may be identical
to that of a word instance in one grouping while having a hypernym/hyponym rela-
tionship with that of a word instance in another. What must happen is that the words
must be grouped in such a way that the overall grouping is optimal in that it creates
the longest/strongest lexical chains. It is our contention that words are grouped into
a single chain when they are ?about? the same underlying concept.
489
Silber and McCoy Efficient Lexical Chains for Summarization
2. Algorithm Definition
We wish to extract lexical chains from a source document using the complete method
that Barzilay and Elhadad implemented in exponential time, but to do so in linear
time. Barzilay and Elhadad define an interpretation as a mapping of noun instances
to specific senses, and further, of these senses to specific lexical chains. Each unique
mapping is a particular ?way of interpreting? the document, and the collection of
all possible mappings defines all of the interpretations possible. In order to compute
lexical chains in linear time, instead of computing every interpretation of a source
document as Barzilay and Elhadad did, we create a structure that implicitly stores
every interpretation without actually creating it, thus keeping both the space and time
usage of the program linear. We then provide a method for finding that interpretation
which is best from within this representation. As was the case with Barzilay and
Elhadad, we rely on WordNet1 to provide sense possibilities for, and semantic relations
among, word instances in the document.
Before we could actually compute the interpretations, one issue we had to tackle
was the organization and speed of the WordNet dictionary. In order to provide expe-
dient access to WordNet, we recompiled the noun database into a binary format and
memory-mapped it so that it could be accessed as a large array, changing the WordNet
sense numbers to match the array indexes.
2.1 Chain Computation
Before computation can begin, the system uses a part-of-speech tagger2 to find the
noun instances within the document. Processing a document involves creating a large
array of ?metachains,? the size of which is the number of noun senses in WordNet
plus the number of nouns in the document, to handle the possibility of words not
found in WordNet. (This is the maximum size that could possibly be needed.) A
metachain represents all possible chains that can contain the sense whose number is
the index of the array. When a noun is encountered, for every sense of the noun in
WordNet, the noun sense is placed into every metachain for which it has an iden-
tity, synonym, or hyperonym relation with that sense. These metachains represent
every possible interpretation of the text. Note that each metachain has an index that
is a WordNet sense number, so in a very real way, we can say that a chain has an
?overriding sense.? Table 1 shows the structure of such an array, with each row being
a metachain based on the sense listed in the first column. In each node of a given
metachain, appropriate pointers are kept to allow fast lookup. In addition, in associa-
tion with each word, a list of the metachains to which it belongs is also kept (not shown
in table).
The second step, finding the best interpretation, is accomplished by making a
second pass through the source document. For each noun, each chain to which the
noun belongs is examined and a determination is made, based on the type of relation
and distance factors, as to which metachain the noun contributes to most. In the
event of a tie, the higher sense number is used, since WordNet is organized with more
specific concepts indexed with higher numbers. The noun is then deleted from all other
metachains. Once all of the nouns have been processed, what is left is the interpretation
whose score is maximum. From this interpretation, the best (highest-scoring) chains
can be selected. The algorithm in its entirety is outlined in Table 2.
1 WordNet is available at ?http://www.cogsci.princeton.edu/?wn?.
2 The part-of-speech tagger we used is available from ?http://www.rt66.com/gcooke/SemanTag?.
490
Computational Linguistics Volume 28, Number 4
Table 1
Example of metachains.
Index Meaning Chain
0 person John Machine
1 unit Computer IBM
2 device Computer Machine IBM
3 organization Machine IBM
4 unknown IBM
...
N
Note: Assume the sentences ?John has a computer.
The machine is an IBM.? and that the nouns have the
following senses (meanings): John (0), computer (1,2),
machine (0,2,3), IBM (1,2,3,4), and that words are put
in a chain if they have an identity relation. This table
then depicts the metachains after the first step.
Table 2
Basic linear-time lexical chains algorithm.
Step 1 For each noun instance
For each sense of the noun instance
Compute all scored metachains
Step 2 For each noun instance
For each metachain to which the noun belongs
Keep word instance in the metachain to which it contributes most
Update the scores of each other metachain
2.2 Scoring System
Our scoring system allows different types of relations within a lexical chain to con-
tribute to that chain differently. Further, our scoring system allows the distance be-
tween word instances in the original document to affect the word?s contribution to
a chain. Table 3 shows the scoring values used by our algorithm. These values were
obtained through empirical testing, and although not optimal, appear to give good
results.
Each noun instance is included in a chain because either it is the first noun instance
to be inserted or it is related to some word that is already in the chain. If it is the first
word, then the ?identical word? relation score is used. If not, then the type of relation is
determined, and the closest noun in the chain to which it is related is found. Using the
distance between these two words and the relation type, we look up the contribution
of the word instance to the overall chain score.
Once chains are computed, some of the high-scoring ones must be picked as
representing the important concepts from the original document. To select these, we
use the idea of ?strong chains? introduced by Barzilay and Elhadad (1997). They define
a strong chain as any chain whose score is more than two standard deviations above
the mean of the scores computed for every chain in the document.
491
Silber and McCoy Efficient Lexical Chains for Summarization
Table 3
Scoring system tuned by empirical methods.
One Sentence Three Sentences Same Paragraph Default
Identical word 1 1 1 1
Synonym 1 1 1 1
Hypernym 1 .5 .5 .5
Sibling 1 .3 .2 0
Table 4
Constants from WordNet 1.6.
Value Worst Case Average Case
C1 = # of senses for a given word 30 2
C2 = parent/child ?is a? relations of a word sense 45,147 14
C3 = # of nouns in WordNet 94,474 94,474
C4 = # of synsets in WordNet 66,025 66,025
C5 = # of siblings of a word sense 397 39
C6 = # of chains to which a word instance can belong 45,474 55
3. Linear Runtime Proof
In this analysis, we will not consider the computational complexity of part-of-speech
tagging, since it is quite fast. The runtime of the full algorithm will be O(pos tagging)
+ O(our algorithm). Also, as it does not change from execution to execution of the
algorithm, we shall take the size and structure of WordNet to be constant. We will
examine each phase of our algorithm to show that the extraction of these lexical chains
can indeed be performed in linear time. Table 4 defines constants for this analysis.
3.1 Collection of WordNet Information
For each noun in the source document that appears in WordNet, each sense that the
word can take must be examined. Additionally, for each sense, we must walk up and
down the hypernym/hyponym graph collecting all parent and child information. It
is important to note that we are interested not only in direct parents and children,
but in all parents and children in the graph from most specific to most general. Lastly
we must collect all of the senses in WordNet that are siblings (i.e., share immediate
parents) with the word being processed. All of the complexity in this step is related
to the size of WordNet, which is constant. Lookups in WordNet use a binary search;
hence a search in WordNet is O(log(C3)). The runtime is given by
n ? (log2(C3) + C1 ? C2 + C1 ? C5).
3.2 Building the Graph
The graph of all possible interpretations is nothing more than an array of sense values
(66, 025 + n in size) that we will call the sense array. For each word, we examine each
relation computed as above from WordNet. For each of these relations, we modify
the list that is indexed in the sense array by the sense number of the noun?s sense
involved in the relation. This list is then modified by adding the word to the list and
updating the list?s associated score. Additionally, we add the chain?s pointer (stored
in the array) to a list of such pointers in the word object. Lastly, we add the value of
492
Computational Linguistics Volume 28, Number 4
how this word affects the score of the chain based on the scoring system to an array
stored within the word structure. The runtime for this phase of the algorithm is
n ? C6 ? 4,
which is also clearly O(n).
3.3 Extracting the Best Interpretation
For each word in the source document, we look at each chain to which the word can
belong. A list of pointers to these chains is stored within the word object, so looking
them up takes O(1) time. For each of these, we simply look at the maximal score
component value in all of these chains. We then set the scores of all of the nodes that
did not contain the maximum to zero and update all the chain scores appropriately.
The operation takes
n ? C6 ? 4,
which is also O(n).
3.4 Overall Runtime Performance
The overall runtime performance of this algorithm is given by the sum of the steps
listed above, for an overall runtime of
n ? (1, 548, 216 + log2(94, 474) + 45, 474 ? 4) [worst case]
n ? (326 + log2(94, 474) + 55 ? 4) [average case].
Initially, we may be greatly concerned with the size of these constants; upon further
analysis, however, we see that most synsets have very few parent-child relations.
Thus the worst-case values may not reflect the actual performance of our application.
In addition, the synsets with many parent-child relations tend to represent extremely
general concepts such as ?thing? and ?object.? These synsets will most likely not
appear very often in a document.
Whereas in the worst case these constants are quite large, in the average case
they are reasonable. This algorithm is O(n) in the number of nouns within the source
document. Considering the size of most documents, the linear nature of this algorithm
makes it usable for generalized summarization of large documents (Silber and McCoy
2000). For example, in a test, our algorithm calculated a lexical chain interpretation of a
40,000-word document in 11 seconds on a Sparc Ultra 10 Creator. It was impossible to
compute lexical chains for such a document under previous implementations because
of computational complexity. Thus documents tested by Barzilay and Elhadad were
significantly smaller in size. Our method affords a considerable speedup for these
smaller documents. For instance, a document that takes 300 seconds using Barzilay
and Elhadad?s method takes only 4 seconds using ours (Silber and McCoy 2000).
4. Evaluation Design
Our algorithm now makes it feasible to use lexical chains as the method for identifying
important concepts in a document, and thus they may now form the basis of an
intermediate representation for summary generation, as proposed by Barzilay and
Elhadad. An important consequence of this is that Barzilay and Elhadad?s proposal
can now be evaluated on documents of substantial size. We propose an evaluation of
this intermediate stage that is independent of the generation phase of summarization.
493
Silber and McCoy Efficient Lexical Chains for Summarization
This said, we make no attempt to claim that a summary can actually be generated
from this representation; we do attempt, however, to show that the concepts found in
a human-generated summary are indeed the concepts identified by our lexical chains
algorithm.
The basis of our evaluation is the premise that if lexical chains are a good in-
termediate representation for summary generation, then we would expect that each
noun in a given summary should be used in the same sense as some word instance
grouped into a strong chain in the original document on which the summary is based.
Moreover, we would expect that all (most) strong chains in the document should be
represented in the summary.
For this analysis, a corpus of documents with their human-generated summaries
are required. Although there are many examples of document and summary types,
for the purposes of this experiment, we focus on two general categories of summaries
that are readily available. The first, scientific documents with abstracts, represents a
readily available class of summaries often discussed in the literature (Marcu 1999).
The second class of document selected was chapters from university level textbooks
that contain chapter summaries. To prevent bias, textbooks from several fields were
chosen.
In this analysis, we use the term concept to denote a noun in a particular sense (a
given sense number in the WordNet database). It is important to note that different
nouns with the same sense number3 are considered to be the same concept. It is also
important to note that for the purposes of this analysis, when we refer to the ?sense? of
a word, we mean the sense as determined by our lexical chain analysis. The basic idea
of our experiment is to try to determine whether the concepts represented by (strong)
lexical chains in an original document appear in the summary of that document and
whether the concepts appearing in the summary (as determined by the lexical chain
analysis of the summary) come from strong chains in the document. If both of these
give 100% coverage, this would mean that all and only the concepts identified by
strong lexical chains in the document occur in the summary. Thus the higher these
numbers turn out to be, the more likely it is that lexical chains are a good intermediate
representation of the text summarization task.
A corpus was compiled containing the two specific types of documents, ranging
in length from 2,247 to 26,320 words each. These documents were selected at random,
with no screening by the authors. The scientific corpus consisted of 10 scientific articles
(5 computer science, 3 anthropology, and 2 biology) along with their abstracts. The
textbook corpus consisted of 14 chapters from 10 university level textbooks in various
subjects (4 computer science, 6 anthropology, 2 history, and 2 economics), including
chapter summaries.
For each document in the corpus, the document and its summary were analyzed
separately to produce lexical chains. In both cases we output the sense numbers spec-
ified for each word instance as well as the overriding sense number for each chain.
By comparing the sense numbers of (words in) each chain in the document with the
computed sense of each noun instance in the summary, we can determine whether the
summary indeed contains the same ?concepts? as indicated by the lexical chains. For
the analysis, the specific metrics we are interested in are
? The number and percentage of strong chains from the original text that
are represented in the summary. Here we say a chain is represented if a
3 Recall that synonyms in the WordNet database are identified by a synset (sense) number.
494
Computational Linguistics Volume 28, Number 4
word occurs in the summary in the same sense as in the document
strong chain. (Analogous to recall)
? The number and percentage of noun instances in the summary that
represent strong chains in the document. (Analogous to precision)
By analyzing these two metrics, we can determine how well lexical chains represent
the information that appears in these types of human-generated summaries. We will
loosely use the terms recall and precision to describe these two metrics.
4.1 Experimental Results
Each document in the corpus was analyzed by running our lexical chain algorithm and
collecting the overriding sense number of each strong lexical chain computed. Each
summary in the corpus was analyzed by our algorithm, and the disambiguated sense
(i.e., the sense of the noun instance that was selected in order to insert it into a chain)
of each noun was collected. Table 5 shows the results of this analysis. The number of
strong chains computed for the document is shown in column 2. Column 3 shows the
Table 5
Evaluation results.
Total Total Strong Chains Noun Instances
Number Number with with
of Strong of Noun Corresponding Corresponding
Chains in Instances Noun Instances Strong Chains in
Document Document in Summary in Summary Document
CS Paper 1 10 22 7 (70.00%) 19 (86.36%)
CS Paper 2 7 19 6 (71.43%) 17 (89.47%)
CS Paper 3 5 31 4 (80.00%) 27 (87.19%)
CS Paper 4 6 25 5 (83.33%) 24 (96.00%)
CS Paper 5 8 16 6 (75.00%) 12 (75.00%)
ANTH Paper 1 7 20 7 (100.00%) 17 (85.00%)
ANTH Paper 2 5 17 4 (80.00%) 13 (76.47%)
ANTH Paper 3 7 21 6 (28.57%) 7 (33.33%)
BIO Paper 1 4 19 4 (100.00%) 17 (89.47%)
BIO Paper 2 5 31 5 (80.00%) 28 (90.32%)
CS Chapter 1 9 55 8 (88.89%) 49 (89.09%)
CS Chapter 2 7 49 6 (85.71%) 42 (85.71%)
CS Chapter 3 11 31 9 (81.82%) 25 (80.65%)
CS Chapter 4 14 47 5 (35.71%) 21 (44.68%)
ANTH Chapter 1 5 61 4 (80.00%) 47 (77.05%)
ANTH Chapter 2 8 74 7 (87.50%) 59 (79.73%)
ANTH Chapter 3 12 58 11 (91.67%) 48 (82.76%)
ANTH Chapter 4 13 49 11 (84.62%) 42 (85.71%)
ANTH Chapter 5 7 68 5 (71.43%) 60 (88.24%)
ANTH Chapter 6 9 59 8 (88.89%) 48 (81.36%)
HIST Chapter 1 12 71 10 (83.33%) 67 (94.37%)
HIST Chapter 2 8 65 7 (87.50%) 55 (84.62%)
ECON Chapter 1 14 68 12 (85.71%) 63 (92.65%)
ECON Chapter 2 9 51 7 (77.78%) 33 (64.71%)
Mean 79.12% 80.83%
Median 82.58% 85.35%
Note: ANTH?anthropology, BIO?biology, CS?computer science, ECON?economics,
HIST?history.
495
Silber and McCoy Efficient Lexical Chains for Summarization
total number of noun instances found in the summary. Column 4 shows the number,
and percentage overall, of strong chains from the document that are represented by
noun instances in the summary (recall). The number, and the percentage overall, of
nouns of a given sense from the summary that have a corresponding strong chain
with the same overriding sense number (representing the chain) in the original text
are presented in column 5 (precision). Summary statistics are also presented.
In 79.12% of the cases, lexical chains appropriately represent the nouns in the sum-
mary. In 80.83% of the cases, nouns in the summary would have been predicted by the
lexical chains. The algorithm performs badly on two documents, anthropology paper
3 and computer science chapter 4, under this analysis. Possible reasons for this will
be discussed below, but our preliminary analysis of these documents leads us to be-
lieve that they contain a greater number of pronouns and other anaphoric expressions
(which need to be resolved to compute lexical chains properly). These potential rea-
sons need to be examined further to determine why our algorithm performs so poorly
on these documents. Excluding these two documents, our algorithm has a recall of
83.39% and a precision of 84.63% on average. It is important to note that strong chains
represent only between 5% and 15% of the total chains computed for any document.
The evaluation presented here would be enhanced by having a baseline for com-
parison. It is not clear, however, what this baseline should be. One possibility would
be to use straight frequency counts as an indicator and use these frequency counts for
comparison.
5. Discussion and Future Work
Some problems that cause our algorithm to have difficulty, specifically proper nouns
and anaphora resolution, need to be addressed. Proper nouns (people, organization,
company, etc.) are often used in naturally occurring text, but since we have no in-
formation about them, we can only perform frequency counts on them. Anaphora
resolution, especially in certain domains, is a bigger issue. Much better results are
anticipated with the addition of anaphora resolution to the system.
Other issues that may affect the results we obtained stem from WordNet?s coverage
and the semantic information it captures. Clearly, no semantically annotated lexicon
can be complete. Proper nouns and domain-specific terms, as well as a number of
other words likely to be in a document, are not found in the WordNet database. The
system defaults to word frequency counts for terms not found. Semantic distance in
the ?is a? graph, a problem in WordNet, does not affect our implementation, since
we don?t use this information. It is important to note that although our system uses
WordNet, there is nothing specific to the algorithm about WordNet per se, and any
other appropriate lexicon could be ?plugged in? and used.
Issues regarding generation of a summary based on lexical chains need to be ad-
dressed and are the subject of our current work. Recent research has begun to look at
the difficult problem of generating a summary text from an intermediate representa-
tion. Hybrid approaches such as extracting phrases instead of sentences and recom-
bining these phrases into salient text have been proposed (Barzilay, McKeown, and
Elhadad 1999). Other recent work looks at summarization as a process of revision; in
this work, the source text is revised until a summary of the desired length is achieved
(Mani, Gates, and Bloedorn 1999). Additionally, some research has explored cutting
and pasting segments of text from the full document to generate a summary (Jing and
McKeown 2000). It is our intention to use lexical chains as part of the input to a more
classical text generation algorithm to produce new text that captures the concepts from
the extracted chains. The lexical chains identify noun (or argument) concepts for the
496
Computational Linguistics Volume 28, Number 4
summary. We are examining ways for predicates to be identified and are concentrating
on situations in which strong lexical chains intersect in the text.
6. Conclusions
In this article, we have outlined an efficient, linear-time algorithm for computing lexical
chains as an intermediate representation for automatic machine text summarization.
This algorithm is robust in that it uses the method proposed by Barzilay and Elhadad,
but it is clearly O(n) in the number of nouns in the source document.
The benefit of this linear-time algorithm is its ability to compute lexical chains
in documents significantly larger than could be handled by Barzilay and Elhadad?s
implementation. Thus, our algorithm makes lexical chains a computationally feasi-
ble intermediate representation for summarization. In addition, we have presented a
method for evaluating lexical chains as an intermediate representation and have eval-
uated the method using 24 documents that contain human-generated summaries. The
results of these evaluations are promising.
An operational sample of our algorithm is available on the Web; a search engine
that uses our algorithm can be accessed there as well (available at ?http://www.eecis.
udel.edu/?silber/research.htm?).
Acknowledgments
The authors wish to thank the Korean
Government, Ministry of Science and
Technology, whose funding, as part of the
Bilingual Internet Search Machine Project,
has made this research possible.
Additionally, special thanks to Michael
Elhadad and Regina Barzilay for their
advice and for generously making their data
and results available, and to the anonymous
reviewers for their helpful comments.
References
Barzilay, Regina and Michael Elhadad. 1997.
Using lexical chains for text
summarization. In Proceedings of the
Intelligent Scalable Text Summarization
Workshop (ISTS-97), Madrid, Spain.
Barzilay, Regina, Kathleen R. McKeown,
and Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Conference of the Association for
Computational Linguistics, College Park,
MD. Association for Computational
Linguistics, New Brunswick, NJ.
Hirst, Graeme and David St.-Onge. 1997.
Lexical chains as representation of context
for the detection and correction of
malapropisms. In Christiane Fellbaum,
editor, Wordnet: An electronic lexical database
and some of its applications. MIT Press,
Cambridge, pages 305?332.
Jing, H. and K. McKeown. 2000. Cut and
paste based text summarization. In
Proceedings of NAACL-00, Seattle.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Conference of the Association for
Computational Linguistics, College Park,
MD. Association for Computational
Linguistics, New Brunswick, NJ.
Marcu, Daniel. 1999. The automatic creation
of large scale corpora for summarization
research. In The 22nd International ACM
SIGIR Conference on Research and
Development in Information Retrieval,
Berkeley. ACM Press, New York.
Morris, J. and G. Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indecator of the structure of text.
Computational Linguistics, 18(1):21?45.
Silber, H. Gregory and Kathleen F. McCoy.
2000. Efficient text summarization using
lexical chains. In 2000 International
Conference on Intelligent User Interfaces,
New Orleans, January.
Sparck Jones, Karen. 1993. What might be in
summary? Information Retrieval ?93,
Regensburg, Germany, September, pages
9?26.
Proceedings of NAACL HLT 2007, Companion Volume, pages 173?176,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Effects of Word Prediction on Communication Rate for AAC
Keith Trnka, Debra Yarrington, John McCaw,
and Kathleen F. McCoy
Department of Computer and Information Sciences
University of Delaware Newark, DE 19716
trnka,yarringt,mccaw,mccoy@cis.udel.edu
Christopher Pennington
AgoraNet, Inc.
314 East Main Street, Suite 1
Newark, DE 19711
penningt@agora-net.com
Abstract
Individuals using an Augmentative and
Alternative Communication (AAC) de-
vice communicate at less than 10% of
the speed of ?traditional? speech, creat-
ing a large communication gap. In this
user study, we compare the communica-
tion rate of pseudo-impaired individuals
using two different word prediction algo-
rithms and a system without word pre-
diction. Our results show that word pre-
diction can increase AAC communication
rate and that more accurate predictions
significantly improve communication rate.
1 Introduction
Communication is a significant quality-of-life issue
for individuals with severe speech impairments. The
field of Augmentative and Alternative Communica-
tion (AAC) is concerned with mitigating commu-
nication barriers that would otherwise isolate indi-
viduals from society. Most high-tech AAC devices
provide the user with an electronic letter and word
board to input messages which are output via speech
synthesis. However, even with substantial user inter-
face optimization, communication rate is often less
than 10 words per minute (Newell et al, 1998) as
compared to about 150-200 words per minute for
unimpaired speech.
One way to improve communication rate is to de-
crease the number of keys entered to form a mes-
sage. Word prediction is an application of language
modeling to allowing the user to access words they
may be spelling at a cost of one keystroke. Many
commercial AAC devices use word prediction, such
as PRC?s PathfinderTM, Dynavox Technology?s Dy-
navox 4TM, and Saltillo?s ChatPCTM.
Although word prediction is used in AAC de-
vices, researchers have questioned whether it ac-
tually increases communication rate (Venkatagiri,
1993; Koester and Levine, 1997; Anson et al,
2004). These works note the additional cognitive
demands and cost of using word prediction in con-
junction with a letter-by-letter interface, such as the
need to shift the focus of attention to the prediction
list, the time to scan the prediction list, and the cog-
nitive effort required for making decisions about the
predicted words. Obviously the design of the par-
ticular interface (e.g., the ease of using word pre-
diction) will affect these results. In addition, these
studies used a single, simplistic method of generat-
ing predictions, and this may also be responsible for
some of their results.
In contrast, other researchers (Lesher and Hig-
ginbotham, 2005; Li and Hirst, 2005; Trnka et
al., 2006) have continued to investigate various im-
provements to language modeling for word pre-
diction in order to save the user more keystrokes.
Newer methods such as topic modeling yield sta-
tistically significant keystroke savings over previ-
ous methods. However, the question remains as to
whether improvements in prediction methods trans-
late to an enhanced communication rate. We hypoth-
esize that it will.
In this paper we study (1) whether a word pre-
diction interface increases communication rate over
173
letter-by-letter typing when a reasonable prediction
method is employed and (2) whether an advanced
word prediction method increases communication
rate over a basic word prediction method to a degree
greater than that afforded by the difference in theo-
retical keystroke savings between the two methods.
We expect that the communication rate gain due to
the better word prediction method will exceed the
gains from the poorer system. Our reasons for this
expectation has to do with not only users wasting
time scanning lists that do not contain the desired
word, but also the tendency for a user to give up on
such a system (i.e., choosing to ignore the predic-
tions) and thus missing the predicted word even if it
does appear in the list. Validating these hypotheses
will motivate continued improvements in word pre-
diction methods for increased communication rate.
The target population of our research is adult
AAC users without significant cognitive impair-
ments. Including actual AAC users in the study
poses several significant complications, perhaps the
largest of which concerns the user interface. AAC
devices vary significantly in the physical interfaces
available, in accordance with the variety of physi-
cal abilities of AAC users. This diversity has caused
different word prediction interfaces to be developed
for each physical interface. Moreover, it would be
impossible to mimic our word prediction layout in a
consistent fashion on all of the major AAC devices
used. Because of this, we conducted this pilot study
using subjects that are pseudo-impaired: the subjects
have no motor impairments but we have simulated
a motor impairment by providing an interface that
emulates the communication rate of a typical AAC
user. Future work includes the verification of the re-
sults using a smaller number of actual AAC users.
2 Approach
The purpose of the study was to measure the effects
of word prediction methods on communication rate.
To this end, the interface used for text entry was opti-
mized for ease-of-use and kept constant across trials.
Subjects were asked to enter text on a touchscreen
monitor using WivikTM, an on-screen keyboard. Be-
cause we wanted to simulate AAC users with mo-
tor impairments, we programmed a 1.5 second de-
lay between a key press and its registration in the
system. The artificial impairment gave the subjects
the same incentive to use word prediction that AAC
users face every day, whereas users with fine motor
control tend to ignore word prediction (e.g., in com-
mon word processing software). The delay slows the
input rate of our subjects down to a rate more typical
of AAC users (about 8-10 words per minute).
Seventeen adult, native speakers of English with
no visual, cognitive, or motor impairments partic-
ipated in the study. These subjects were asked to
type in three different excerpts from held-out data of
the Switchboard corpus on three different days.1 In
each of these sessions, a different prediction method
was used and the order of prediction methods was
randomized across subjects. Keystrokes and pre-
dictions were logged and then post-processed to
compute the words produced per minute, seconds
per keystroke, and keystroke savings, among other
statistics.
2.1 Independent variable: prediction methods
The independent variable in our study is the method
of text entry used: (1) letter-by-letter typing using
the Wivik keyboard with no word prediction, (2)
letter-by-letter typing augmented with word predic-
tions produced by a basic prediction method, (3)
letter-by-letter typing augmented with word predic-
tions produced by an advanced prediction method.
Basic prediction generates predictions from the
combination of a recency model of the text entered
so far in conjunction with a large word list. The
recency model is given priority in generating pre-
dictions. This model is similar to language models
used in AAC devices with the exception that many
devices use a unigram model in lieu of a word list.
Advanced prediction generates predictions on
the basis of a trigram model with backoff. A spe-
cial unigram model is used for the first word in
each sentence. This language model is constructed
from the transcribed telephone conversations of the
Switchboard corpus. If the prediction list isn?t filled
from this model?s predictions, then predictions are
selected from a recency model and then a word list,
as in the basic prediction method.
1Switchboard was chosen because our prediction models
were trained using another portion of the corpus. A copy task
was chosen for more controlled experimental conditions.
174
Adv. prediction Basic prediction No prediction
Words per minute (wpm) 8.09 5.50 5.06
Time (seconds) 1316s 1808s 2030s
Seconds per keystroke (spk) 2.92s 2.58s 2.28s
Keystroke savings (ks) 50.3% 18.2% -
Potential keystroke savings (pks) 55.2% 25.0% -
Prediction utilization (pru) 90.9% 73.3% -
Figure 1: Average statistics for each method.
3 Results
Once the data was collected, we post-processed the
logs and accumulated statistics. Average values for
each method are shown in Figure 1 and comparative
values are shown in Figure 2.
3.1 Communication rate (output rate)
The overall average words per minute and task com-
pletion time for each method is shown in Figure 1,
and Figure 2 shows comparative data for the three
methods. As hypothesized, advanced prediction was
found to be significantly faster than basic prediction
and basic prediction was found to be significantly
faster than no prediction (? = 0.01). For example,
users produced 59.9% more words per minute using
advanced prediction compared to no prediction. Ad-
vanced prediction was 44.4% faster than basic pre-
diction but basic prediction was only 10.1% faster
than no prediction.
Additionally, the relative task completion time is
shown in Figure 2. The copy tasks with advanced
prediction were completed in 64.5% of the time it
took to complete without word prediction. The trend
shown with relative task completion time reinforces
the trends shown with words per minute ? advanced
prediction offers a large speedup over no prediction
and basic prediction, but basic prediction offers a
much smaller increase over no prediction.
Our results show that basic word prediction sig-
nificantly boosts communication rate and that ad-
vanced word prediction substantially increases com-
munication rate beyond basic prediction.
3.2 Input rate (seconds per keystroke)
Figures 1 and 2 indicate that there were significant
differences (at ? = 0.01) in the methods in terms
of the rate at which keys were pressed. In partic-
ular, while overall communication rate was signif-
icantly faster with advanced prediction, users took
0.641 seconds longer for each key press from us-
ing advanced prediction compared to entry without
prediction. Similarly, users spent 0.345s longer to
enter each key using advanced as opposed to basic
prediction and basic prediction required more time
per keystroke than no prediction. The slower input
rate can be attributed to the additional demands of
searching through a prediction list and making a de-
cision about selecting a word from that list over con-
tinuing to type letters.
3.3 Keystroke savings / prediction utilization
The difference between the potential keystroke sav-
ings offered by advanced and basic prediction is sub-
stantial: 55.2% vs. 25.0%, as shown in Figure 1.
Accordingly, the actual keystroke savings that users
realized under each prediction method shows a wide
separation: 50.3% for advanced and 18.2% for ba-
sic. The keystroke savings that users of basic predic-
tion achieved seems quite a bit lower than the poten-
tial keystroke savings offered by the predictions. In
other words, the prediction utilization of basic pre-
diction was much lower than that of advanced pre-
diction. Comparative analysis shows a 17.1% im-
provement in prediction utilization from advanced
over basic prediction.
4 Discussion
The results show that communication rate increased
despite the decreased input rate due to a large reduc-
tion in the amount of input required (high keystroke
savings). In the past, researchers have noted that the
cognitive load of using word prediction was consid-
erable, so that the keystroke savings of word pre-
175
Adv. over None Adv. over Basic Basic over None
Relative task completion time 0.6451 0.7011 0.9191
Words per minute (wpm) 59.9% faster2 44.4% faster2 10.1% faster2
Seconds per keystroke (spk) 0.641s2 0.345s2 0.286s2
Prediction utilization (pru) 17.1%2
Figure 2: Average per-subject improvements. (1 Significance not tested. 2 Significant at ? = 0.01.)
diction was outweighed by the overhead of using
it. However, we have shown that despite significant
cognitive load, the reduction in keystroke savings
dominates the effect on output rate.
In contrast to earlier studies, our basic method
showed a significantly improved communication
rate over no prediction. One reason for this could
be the intuitiveness of our user interface. A second
reason could be related to the consistency of the ba-
sic prediction method. In particular, at least some
subjects using the basic prediction method learned
to scan the prediction list when the desired word was
recently used and mentioned it in the exit survey. At
other times they simply ignored the prediction list
and proceeded with letter-by-letter typing. This be-
havior would also explain why the input was sig-
nificantly slower with the advanced method over the
basic method ? users found that scanning the predic-
tion list more often was worth the added effort. This
also explains the significant difference in prediction
utilization between the methods.
The relationship between keystroke savings and
communication rate is a trend of increasing rate
enhancement with increasingly accurate prediction
methods. Improved prediction methods offer greater
potential keystroke savings to users and users see
increased keystroke savings in practice. Addition-
ally, users rely on better predictions more and thus
lose less of the potential keystroke savings offered
by the method. We expect that keystroke savings
will see substantial increases from improved poten-
tial keystroke savings until prediction utilization is
closer to 100%.
5 Conclusions
Word prediction in an experimental AAC device
with simulated AAC users significantly enhances
communication rate. The difference between an ad-
vanced and basic prediction method demonstrates
that further improvements in language modeling for
word prediction are likely to appreciably increase
communication rate. Therefore, further research in
improving word prediction is likely to have an im-
portant impact on quality-of-life for AAC users. We
plan to improve word prediction and validate these
results using AAC users as future work.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051.
References
Denis Anson, Penni Moist, Mary Przywars, Heather
Wells, Heather Saylor, and Hantz Maxime. 2004.
The effects of word completion and word prediction
on typing rates using on-screen keyboards. Assistive
Technology, 18.
Heidi Horstmann Koester and Simon P. Levine. 1997.
Keystroke-level models for user performance with
word prediction. Augmentative and Alternative Com-
munication, 13:239?257, December.
Gregory W. Lesher and D. Jeffery Higginbotham. 2005.
Using web content to enhance augmentative commu-
nication. In Proceedings of CSUN 2005.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS ?05, pages 121?
128.
Alan Newell, Stefan Langer, andMarianne Hickey. 1998.
The ro?le of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):1?16.
Keith Trnka, Debra Yarrington, Kathleen F. McCoy, and
Christopher A. Pennington. 2006. Topic modeling in
fringe word prediction for aac. In IUI ?06, pages 276?
278.
Horabail S. Venkatagiri. 1993. Efficiency of lexical
prediction as a communication acceleration technique.
Augmentative and Alternative Communication, 9:161?
167, September.
176
Error Profiling: Toward a Model of English Acquisition for Deaf
Learners
Lisa N. Michaud and Kathleen F. McCoy
Dept. of Computer and Info. Sciences, University of Delaware, Newark, DE 19716, USA
 
michaud,mccoy  @cis.udel.edu
http://www.eecis.udel.edu/research/icicle
Abstract
In this paper we discuss our approach
toward establishing a model of the ac-
quisition of English grammatical struc-
tures by users of our English language
tutoring system, which has been de-
signed for deaf users of American Sign
Language. We explore the correlation
between a corpus of error-tagged texts
and their holistic proficiency scores as-
signed by experts in order to draw ini-
tial conclusions about what language
errors typically occur at different levels
of proficiency in this population. Since
errors made at lower levels (and not
at higher levels) presumably represent
constructions acquired before those on
which errors are found only at higher
levels, this should provide insight into
the order of acquisition of English
grammatical forms.
1 Introduction
There have been many theories of language acqui-
sition proposing a stereotypical order of acquisi-
tion of language elements followed by most learn-
ers, and there has been empirical evidence of such
an order among morphological elements of lan-
guage (cf. (Bailey et al, 1974; Dulay and Burt,
1975; Larsen-Freeman, 1976)) and some syntac-
tic structures (cf. (Brown and Hanlon, 1970;
Gass, 1979)). There is indication that these re-
sults may be applied to any L1 group acquiring
English (Dulay and Burt, 1974; Dulay and Burt,
1975), and some research has focused on develop-
ing a general account of acquisition across a broad
range of morphosyntactic structures (cf. (Piene-
mann and Ha?kansson, 1999)). In this work, we
explore how our second language instruction sys-
tem, ICICLE, has generated the need for model-
ing such an account, and we discuss the results
of a corpus analysis we have undertaken to fulfill
that need.
1.1 ICICLE: an Overview
ICICLE (Interactive Computer Identification and
Correction of Language Errors) is an intelli-
gent tutoring system currently under development
(Michaud and McCoy, 1999; Michaud et al,
2000; Michaud et al, 2001). Its primary function
is to tutor deaf students on their written English.
Essential to performing that function is the ability
to correctly analyze user-generated language er-
rors and produce tutorial feedback to student per-
formance which is both correct and tailored to the
student?s language competence. Our target learn-
ers are native or near-native users of American
Sign Language (ASL), a distinct language from
English (cf. (Baker and Cokely, 1980)), so we
view the acquisition of skills in written English as
the acquisition of a second language for this pop-
ulation (Michaud et al, 2000).
Our system uses a cycle of user input and sys-
tem response, beginning when a user submits a
piece of writing to be reviewed by the system.
The system determines the grammatical errors in
the writing, and responds with tutorial feedback
aimed at enabling the student to perform correc-
tions. When the student has revised the piece, it
is re-submitted for analysis and the cycle begins
again. As ICICLE is intended to be used by an
individual over time and across many pieces of
writing, the cycle will be repeated with the same
individual many times.
user
interface
Error Identifcation
Module
Grammar Model
Domain Knowl.
Response Generation
Module
System History
Dialogue History Database of
Grammatical
Concepts
Augmented
Parsing
Grammar
text
highlighted errors
tutoring session
errors
User Model Domain KB
Figure 1: ICICLE system architecture.
Figure 1 contains a diagram of ICICLE?s over-
all architecture and the cycle we have described.
It executes between the User Interface, the Error
Identification Module (which performs the syn-
tactic analysis of user writing), and the Response
Generation Module (which builds the feedback to
the user based on the errors the user has commit-
ted). The work described in this paper focuses on
the development of one of the sources of knowl-
edge used by both of these processes, a compo-
nent of the User Model representing the user?s
grammatical competence in written English.
1.2 A Need for Modeling L2A Status
What currently exists of the ICICLE system is
a prototype application implemented in a graph-
ical interface connected to a text parser that uses
a wide-coverage English grammar augmented by
?mal-rules? capturing typical errors made by our
learner population. It can recognize and label
many grammatical errors, delivering ?canned?
one- or two-sentence explanations of each error
on request. The user can then make changes
and resubmit the piece for additional analysis.
We have discussed in (Schneider and McCoy,
1998) the performance of our parser and mal-rule-
augmented grammar and the unique challenges
?She is teach piano on Tuesdays.?
Beginner: Inappropriate use of auxiliary
and verb morphology problems.
?She teaches piano on Tuesdays.?
Intermediate: Missing appropriate +ing
morphology.
?She is teaching piano on Tuesdays.?
Advanced: Botched attempt at passive
formation.
?She is taught piano on Tuesdays.?
Figure 2: Possible interpretations of non-
grammatical user text.
faced when attempting to cover non-grammatical
input from this population.
In its current form, when the parser obtains
more than one possible parse of a user?s sentence,
the interface chooses arbitrarily which one it will
assume to be representative of which structures
the user was attempting. This is undesirable, as
one challenge that we face with this particular
population is that there is quite a lot of variabil-
ity in the level of written English acquisition. A
large percentage of the deaf population has read-
ing/writing proficiency levels significantly below
their hearing peers, and yet the population repre-
sents a broad range of ability. Among deaf 18-
year-olds, about half read at or below a fourth
grade level, while about 10% read above the
eighth-grade level (Strong, 1988). Thus, even
when focused on a subset of the deaf population
(e.g., deaf high school or college students), there
is significant variability in the writing proficiency.
The impact of this variability is that a particular
string of words may have multiple interpretations
and the most likely one may depend upon the pro-
ficiency level of the student, as illustrated in Fig-
ure 2. We are therefore currently developing a
user model to address the system?s need to make
these parse selections intelligently and to adapt
tutoring choices to the individual (Michaud and
McCoy, 2000; Michaud et al, 2001).
The model we are developing is called
SLALOM. It is a representation of the user?s abil-
ity to correctly use each of the grammatical ?fea-
tures? of English, which we define as incorpo-
rating both morphological rules such as plural-
izing a noun with +S and syntactic rules such
as the construction of prepositional phrases and
S V O sentence patterns. Intuitively, each unit
in SLALOM corresponds to a set of grammar
rules and mal-rules which realize the feature. The
information stored in each of these units repre-
sents observations based on the student?s perfor-
mance over the submission of multiple pieces of
writing. These observations will be abstracted
into three tags, representing performance that is
consistently good (acquired), consistently flawed
(unacquired), or variable (ZPD1) to record the
user?s ability to correctly execute each structure
in his or her written text.
1.3 An Incomplete Model
A significant problem that we must face in gen-
erating the tags for SLALOM elements is that
we would like to infer tags on performance ele-
ments not yet seen in a writer?s production, bas-
ing those tags on what performance we have been
able to observe so far. We have proposed (Mc-
Coy et al, 1996; Michaud and McCoy, 2000) that
SLALOM be structured in such a way as to cap-
ture these expectations by explicitly representing
the relationships between grammatical structures
in terms of when they are acquired; namely, indi-
cating which features are typically acquired be-
fore other features, and which are typically ac-
quired at the same time. With this information
available in the model, SLALOM will be able
to suggest that a feature typically acquired be-
fore one marked ?acquired? is most likely also
acquired, or that a feature co-acquired with one
marked ?ZPD? may also be something currently
being mastered by the student. The corpus anal-
ysis we have undertaken is meant to provide this
structure by indicating a partial ordering on the
acquisition of grammatical features by this popu-
lation of learners.
1.4 Applications
Having the SLALOM model marked with gram-
matical features as being acquired, unacquired, or
ZPD will be very useful in at least two different
1Zone of Proximal Development: see (Michaud and Mc-
Coy, 2000) for discussion. These are presumably the fea-
tures the learner is currently in the process of acquiring and
thus we expect to see variation in the user?s ability to execute
them.
ways. The first is when deciding which possi-
ble parse of the input best describes a particular
sentence produced by a learner. When there are
multiple parses of an input text, some may place
the ?blame? for detected errors on different con-
stituents. In order for ICICLE to deliver relevant
instruction, it needs to determine which of these
possibilities most likely reflects the actual perfor-
mance of the student. We intend for the parse se-
lection process to proceed on the premise that fu-
ture user performance can be predicted based on
the patterns of the past. The system can generally
prefer parses which use rules representing well-
formed constituents associated with ?acquired?
features, mal-rules from the ?unacquired? area,
and either correct rules or mal-rules for those fea-
tures marked ?ZPD.?
A second place SLALOM will be consulted is
in deciding which errors will then become the
subjects of tutorial explanations. This decision
is important if the instruction is to be effective.
It is our wish for ICICLE to ignore ?mistakes?
which are slip-ups and not indicative of a gap in
language knowledge (Corder, 1967) and to avoid
instruction on material beyond the user?s current
grasp. It therefore will focus on features marked
?ZPD??those in that ?narrow shifting zone di-
viding the already-learned skills from the not-yet-
learned ones? (Linton et al, 1996), or the frontier
of the learning process. ICICLE will select those
errors which involve features from this learner?s
learning frontier and use them as the topics of its
tutorial feedback.
With the partial order of acquisition repre-
sented in the SLALOM model as we have de-
scribed, these two processes can proceed on the
combination of the data contained in the previous
utterances supplied by a given learner and the ?in-
tuitions? granted by information on typical learn-
ers, supplementing empirical data on the specific
user?s mastery of grammatical forms with infer-
ences on what that means with respect to other
forms related to those through the order of acqui-
sition.
2 Profiling Language Errors
We have established the need for a description of
the general progress of English acquisition as de-
termined by the mastery of grammatical forms.
We have undertaken a series of studies to estab-
lish an order-of-acquisition model for our learner
population, native users of American Sign Lan-
guage.
In our first efforts, we have been guided by the
observation that the errors committed by learn-
ers at different stages of acquisition are clues to
the natural order that acquisition follows (Corder,
1967). The theory is that one expects to find er-
rors on elements currently being acquired; thus
errors made by early learners and not by more
advanced learners represent structures which the
early learners are working on but which the ad-
vanced learners have acquired. Having obtained
a corpus of writing samples from 106 deaf indi-
viduals, we sought to establish ?error profiles??
namely, descriptions of the different errors com-
mitted by learners at different levels of language
competence. These profiles could then be a piece
of evidence used to provide an ordering struc-
ture on the grammatical elements captured in the
SLALOM model.
This is an overview of the process by which we
developed our error profiles:
Goal : to have error profiles that indicate what
level of acquisition is most strongly associ-
ated with which grammatical errors. It is
important that the errors correspond to our
grammar mal-rules so that the system can
prefer parses which contain the errors most
consistent with the student?s level of acqui-
sition.
Method :
1. Collect writing samples from our user
population
2. Tag samples in a consistent manner
with a set of error codes (where these
codes have an established correspon-
dence with the system grammar)
3. Divide samples into the levels of acqui-
sition they represent
4. Statistically analyze errors within each
level and compare to the magnitude of
occurrence at other levels
5. Analyze resulting findings to determine
a progression of competence
In (Michaud et al, 2001) we discuss the initial
steps we took in this process, including the de-
velopment of a list of error codes documented by
a coding manual, the verification of our manual
and coding scheme by testing inter-coder reliabil-
ity in a subset of the corpus (where we achieved
a Kappa agreement score (Carletta, 1996) of
   )2, and the subsequent tagging of the en-
tire corpus. Once the corpus was annotated with
the errors each sentence contained, we obtained
expert evaluations of overall proficiency levels
performed by ESL instructors using the national
Test of Written English (TWE) ratings3 . The ini-
tial analysis we go on to describe in (Michaud
et al, 2001) confirmed that clustering algorithms
looking at the relative magnitude of different er-
rors grouped the samples in a manner which cor-
responded to where they appeared in the spectrum
of proficiency represented by the corpus. The
next step, the results of which we discuss here,
was to look at each error we tagged and the ability
of the level of the writer?s proficiency to predict
which errors he or she would commit. If we found
significant differences in the errors committed by
writers of different TWE scores, then we could
use the errors to help organize the SLALOM ele-
ments, and through that obtain data on which er-
rors to expect given a user?s level of proficiency.
2.1 Toward an error profile
Although our samples were scored on the six-
point TWE scale, we had sparse data at either end
of the scale (only 5% of the samples occurring in
levels 1, 5, and 6), so we concentrated our efforts
on the three middle levels (2, 3, and 4), which we
renamed low, middle, and high.
Our chosen method of data exploration was
Multivariate Analysis of Variance (MANOVA).
An initial concern was to put the samples on equal
footing despite the fact that they covered a broad
range in length?from 2 to 58 sentences?and
there was a danger that longer samples would tend
2We also discuss why we were satisfied with this score
despite only being in the range of what Carletta calls ?tenta-
tive conclusions.?
3Although these samples were relatively homogeneous
with respect to the amount of English training and the age
of the writer, we expected to see a range of demonstrated
proficiency for reasons discussed above. We discuss later
why the ratings were not as well spread-out as we expected.
no parse
plural +S
extra conjunction
adj placement
verb formation
extra preposition
infinitive use
wrong tense in context
"activity" phrase
adj formation
adverb placement
incorrect preposition
missing "to be" verb
missing object of verb
missing preposition
missing auxiliary
missing determiner
missing subject
extra auxiliary
adj/adv confusion
comparison phrase
extra relative pronoun
extra determiner
here/there as pronoun
voice confusion
no errors found
should be pronominalized
Significant Results
   
   
  
  






  
  
  
  
  



  
  
  


   
   
			
			

 
 


 
 



   
   





   
   
   
   
   



   
   
   


   
       





   
   
   
   
   







  
  
  
  
  
  
  






Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 261?264,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating Word Prediction: Framing Keystroke Savings
Keith Trnka and Kathleen F. McCoy
University of Delaware
Newark, DE 19716
trnka@cis.udel.edu
Abstract
Researchers typically evaluate word predic-
tion using keystroke savings, however, this
measure is not straightforward. We present
several complications in computing keystroke
savings which may affect interpretation and
comparison of results. We address this prob-
lem by developing two gold standards as a
frame for interpretation. These gold standards
measure the maximum keystroke savings un-
der two different approximations of an ideal
language model. The gold standards addition-
ally narrow the scope of deficiencies in a word
prediction system.
1 Introduction
Word prediction is an application of language mod-
eling to speeding up text entry, especially to entering
utterances to be spoken by an Augmentative and Al-
ternative Communication (AAC) device. AAC de-
vices seek to address the dual problem of speech and
motor impairment by attempting to optimize text in-
put. Even still, communication rates with AAC de-
vices are often below 10 words per minute (Newell
et al, 1998), compared to the common 130-200
words per minute speech rate of speaking people.
Word prediction addresses these issues by reducing
the number of keystrokes required to produce a mes-
sage, which has been shown to improve communi-
cation rate (Trnka et al, 2007). The reduction in
keystrokes also translates into a lower degree of fa-
tigue from typing all day (Carlberger et al, 1997).
Word prediction systems present multiple com-
pletions of the current word to the user. Systems
generate a list of W predictions on the basis of the
word being typed and a language model. The vo-
cabulary is filtered to match the prefix of the current
word and the language model ranks the words ac-
cording to their likelihood. In the case that no letters
of the current word have been entered, the language
model is the sole factor in generating predictions.
Systems often use a touchscreen or function/number
keys to select any of the predicted words.
Because the goal of word prediction systems is
to reduce the number of keystrokes, the primary
evaluation for word prediction is keystroke savings
(Garay-Vitoria and Abascal, 2006; Newell et al,
1998; Li and Hirst, 2005; Trnka and McCoy, 2007;
Carlberger et al, 1997). Keystroke savings (KS)
measures the percentage reduction in keys pressed
compared to letter-by-letter text entry.
KS =
keysnormal ? keyswith prediction
keysnormal
? 100%
A word prediction system that offers higher savings
will benefit a user more in practice.
However, the equation for keystroke savings has
two major deficiencies. Firstly, the equation alone
is not enough to compute keystroke savings ? actu-
ally computing keystroke savings requires a precise
definition of a keystroke and also requires a method
for determining howmany keystrokes are used when
predictions are available, discussed in Section 2. Be-
yond simply computing keystroke savings, the equa-
tion alone does not provide much in the way of inter-
pretation? is 60% keystroke savings good? Can we
do better? Section 3 will present two gold standards
to allow better interpretation of keystroke savings.
261
2 Computing Keystroke Savings
We must have a way to determine how many
keystrokes a user would take under both letter-
by-letter entry and word prediction to compute
keystroke savings. The common trend in research
is to simulate a ?perfect? user that will never make
typing mistakes and will select a word from the pre-
dictions as soon as it appears.
Implementation of perfect utilization of the pre-
dictions is not always straightforward. For exam-
ple, consider the predictive interface in Microsoft
WordTM: a single prediction is offered as an inline
completion. If the prediction is selected, the user
may backspace and edit the word. However, this
freedom makes finding the minimum sequence of
keys more difficult ? now the user may select a
prediction with the incorrect suffix and correct the
suffix as the optimal action. We feel that a more in-
tuitive interface would allow a user to undo the pre-
diction selection by pressing backspace, an interface
which does not support backspace-editing. In addi-
tion to backspacing, future research in multi-word
prediction will face a similar problem, analogous to
the garden-path problem in parsing, where a greedy
approach does not always give the optimal result.
The keystrokes used for training and testing word
prediction systems can affect the results. We at-
tempt to evaluate word prediction as realistically as
possible. Firstly, many corpora have punctuation
marks, but an AAC user in a conversational setting
is unlikely to use punctuation due to the high cost
of each key press. Therefore, we remove punctua-
tion on the outside of words, such as commas and
periods, but leave word-internal punctuation intact.
Also, we treat capital letters as a single key press,
reflecting the trend of many AAC users to avoid cap-
italization. Another problem occurs for a newline or
?speak key?, which the user would press after com-
pleting an utterance. In pilot studies, including the
simulation of a speak key lowered keystroke savings
by 0.8?1.0% for window sizes 1?10, because new-
lines are not able to be predicted in the system. How-
ever, we feel that the simulation of a speak key will
produce an evaluation metric that is closer to the ac-
tual user?s experience, therefore we include a speak
key in our evaluations.
An evaluation of word prediction must address
these issues, if only implicitly. The effect of these
potentially implicit decisions on keystroke savings
can make comparison of results difficult. However,
if results are presented in reference to a gold stan-
dard under the same assumptions, we can draw more
reliable conclusions from results.
3 Towards a Gold Standard
In trying to improve the state of word prediction,
several researchers have noted that it seems ex-
tremely difficult to improve keystroke savings be-
yond a certain point. Copestake (1997) discussed
the entropy of English to conclude that 50?60%
keystroke savings may be the most we can expect
in practice. Lesher et al (2002) replaced the lan-
guage model in a word prediction system with a
human to try and estimate the limit of keystroke
savings. They found that humans could achieve
59% keystroke savings with access to their ad-
vanced language model and that their advanced lan-
guage model alone achieved 54% keystroke savings.
They noted that one subject achieved nearly 70%
keystroke savings on one particular text, and con-
cluded that further improvements on current meth-
ods are possible. Garay-Vitoria and Abascal (2006)
survey many prediction systems, showing a wide
spectrum of savings, but no system offers more than
70% keystroke savings.
We investigated the problem of the limitations
of keystroke savings first from a theoretical per-
spective, seeking a clearly defined upper boundary.
Keystroke savings can never reach 100%? it would
mean that the system divined the entire text they in-
tended without a single key.
3.1 Theoretical keystroke savings limit
The minimum amount of input required corresponds
to a perfect system ? one that predicts every word
as soon as possible. In a word completion sys-
tem, the predictions are delayed until after the first
character of the word is entered. In such a sys-
tem, the minimum amount of input using a perfect
language model is two keystrokes per word ? one
for the first letter and one to select the prediction.
The system would also require one keystroke per
sentence. In a word prediction system, the predic-
tions are available immediately, so the minimal in-
262
put for a perfect system is one keystroke per word
(to select the prediction) and one keystroke per sen-
tence. We added the ability to measure the minimum
number of keystrokes and maximum savings to our
simulation software, which we call the theoretical
keystroke savings limit.
We evaluated a baseline trigram model under two
conditions with different keystroke requirements on
the Switchboard corpus. The simulation software
was modified to output the theoretical limit in ad-
dition to actual keystroke savings at various window
sizes. To demonstrate the effect of the theoretical
keystroke savings limit on actual savings, we eval-
uated the trigram model under conditions with two
different limits ? word prediction and word com-
pletion. The evaluation of the trigram model using
word completion is shown in Figure 1. The actual
keystroke savings is graphed by window size in ref-
erence to the theoretical limit. As noted by other re-
searchers, keystroke savings increases with window
size, but with diminishing returns (this is the effect
of placing the most probable words first). One of
0%
10%
20%
30%
40%
50%
60%
1 2 3 4 5 6 7 8 9 10
Key
stro
ke 
sav
ing
s
Window size
Word completionTheoretical limit
Figure 1: Keystroke savings and the limit vs. window
size for word completion.
the problems with word completion is that the the-
oretical limit is so close to actual performance ?
around 58.5% keystroke savings compared to 50.8%
keystroke savings with five predictions. At only five
predictions, the system has already realized 87% of
the possible keystroke savings. Under these circum-
stances, it would take a drastic change in the lan-
guage model to impact keystroke savings.
We repeated this analysis for word prediction,
shown in Figure 2 alongside word completion. Word
prediction is much higher than completion, both the-
oretically (the limit) and in actual keystroke savings.
0%
10%
20%
30%
40%
50%
60%
70%
80%
1 2 3 4 5 6 7 8 9 10
Key
stro
ke 
sav
ing
s
Window size
Word predictionWord prediction limitWord completionWord completion limit
Figure 2: Keystroke savings and the limit vs. window
size for word prediction compared to word completion.
Word prediction offers much more headroom in
terms of improvements in keystroke savings. There-
fore our ongoing research will focus on word pre-
diction over word completion.
This analysis demonstrates a limit to keystroke
savings, but this limit is slightly different than
Copestake (1997) and Lesher et al (2002) seek to
describe ? beyond the limitations of the user in-
terface, there seems to be a limitation on the pre-
dictability of English. Ideally, we would like to have
a gold standard that is a closer estimate of an ideal
language model.
3.2 Vocabulary limit
We can derive a more practical limit by simulating
word prediction using a perfect model of all words
that occur in the training data. This gold standard
will predict the correct word immediately so long as
it occurs in the training corpus. Words that never oc-
curred in training require letter-by-letter entry. We
call this measure the vocabulary limit and apply it to
evaluate whether the difference between training and
testing vocabulary is significant. Previous research
has focused on the percentage of out-of-vocabulary
(OOV) terms to explain changes in keystroke sav-
ings (Trnka and McCoy, 2007; Wandmacher and
Antoine, 2006). In contrast, the vocabulary limit
gives more guidance for research by translating the
problem of OOVs into keystroke savings.
Expanding the results from the theoretical limit,
the vocabulary limit is 77.6% savings, compared to
78.4% savings for the theoretical limit and 58.7%
actual keystroke savings with 5 predictions. The
practical limit is very close to the theoretical limit
263
in the case of Switchboard. Therefore, the remain-
ing gap between the practical limit and actual per-
formance must be due to other differences between
testing and training data, limitations of the model,
and limitations of language modeling.
3.3 Application to corpus studies
We applied the gold standards to our corpus study, in
which a trigram model was individually trained and
tested on several different corpora (Trnka and Mc-
Coy, 2007). In contrast to the actual trigram model
Corpus Trigram Vocab.
limit
Theor.
limit
AAC Email 48.92% 61.94% 84.83%
Callhome 43.76% 54.62% 81.38%
Charlotte 48.30% 65.69% 83.74%
SBCSAE 42.30% 60.81% 79.86%
Micase 49.00% 69.18% 84.08%
Switchboard 60.35% 80.33% 82.57%
Slate 53.13% 81.61% 85.88%
Table 1: A trigram model compared to the limits.
performance, the theoretical limits all fall within a
relatively narrow range, suggesting that the achiev-
able keystroke savings may be similar even across
different domains. The more technical and formal
corpora (Micase, Slate, AAC) show higher limits, as
the theoretical limit is based on the length of words
and sentences in each corpus. The practical limit
exhibits much greater variation. Unlike the Switch-
board analysis, many other corpora have a substan-
tial gap between the theoretical and practical limits.
Although the practical measure seems to match the
actual savings similarly to OOVs testing with cross-
validation (Trnka and McCoy, 2007), this measure
more concretely illustrates the effect of OOVs on
actual keystroke savings ? 60% keystroke savings
when training and testing on AAC Email would be
extraordinary.
4 Conclusions
Although keystroke savings is the predominant eval-
uation for word prediction, this evaluation is not
straightforward, exacerbating the problem of inter-
preting and comparing results. We have presented
a novel solution ? interpreting results alongside
gold standards which capture the difficulty of the
evaluation. These gold standards are also applica-
ble to drive future research ? if actual performance
is very close to the theoretical limit, then relaxing
the minimum keystroke requirements should be the
most beneficial (e.g., multi-word prediction). Sim-
ilarly, if actual performance is very close to the
vocabulary limit, then the vocabulary of the lan-
guage model must be improved (e.g., cache mod-
eling, adding general-purpose training data). In the
case that keystroke savings is far from either limit,
then research into improving the language model is
likely to be the most beneficial.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051.
References
Alice Carlberger, John Carlberger, Tina Magnuson,
M. Sharon Hunnicutt, Sira Palazuelos-Cagigas, and
Santiago Aguilera Navarro. 1997. Profet, a new gen-
eration of word prediction: An evaluation study. In
ACL-97 workshop on Natural Language Processing
for Communication Aids.
Ann Copestake. 1997. Augmented and alternative NLP
techniques for augmentative and alternative commu-
nication. In ACL-97 workshop on Natural Language
Processing for Communication Aids, pages 37?42.
Nestor Garay-Vitoria and Julio Abascal. 2006. Text pre-
diction systems: a survey. Univ Access Inf Soc, 4:183?
203.
Gregory W. Lesher, Bryan J. Moulton, D Jeffery Higgin-
botham, and Brenna Alsofrom. 2002. Limits of hu-
man word prediction performance. In CSUN.
Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS, pages 121?128.
Alan Newell, Stefan Langer, andMarianne Hickey. 1998.
The ro?le of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):1?16.
Keith Trnka and Kathleen F. McCoy. 2007. Corpus Stud-
ies in Word Prediction. In ASSETS, pages 195?202.
Keith Trnka, Debra Yarrington, JohnMcCaw, Kathleen F.
McCoy, and Christopher Pennington. 2007. The Ef-
fects of Word Prediction on Communication Rate for
AAC. In NAACL-HLT; Companion Volume: Short Pa-
pers, pages 173?176.
Tonio Wandmacher and Jean-Yves Antoine. 2006.
Training Language Models without Appropriate Lan-
guage Resources: Experiments with an AAC System
for Disabled People. In Eurospeech.
264
An Efficient Text Summarizer Using Lexical Chains 
H.Gregory  SHber  and  Kath leen  F.  McCoy  
Computer  and in fo rmat ion  Sciences 
Univers i ty of Delaware 
Newark,  DE  19711 
{ silber, mccoy} @cis. udel. edu 
? A b s t r a c t  . . . . . . . .  
We present a system which uses lexical chains as an 
intermediate r presentation for automatic text sum- 
marization. This system builds on previous research 
by implementing a lexical chain extraction algorithm 
in linear time. The system is reasonably domain in- 
dependent and takes as input any text or HTML 
document. The system outputs a short summary 
based on the most salient concepts from the origi- 
nal document. The length of the extracted summary 
can be either controlled automatically, or manually 
based on length or percentage ofcompression. While 
still under development, the system provides useful 
summaries which compare well in information con- 
tent to human generated summaries. Additionally, 
the system provides a robust est bed for future sum- 
mary generation research. 
1 In t roduct ion  
Automatic text summarization has long been viewed 
as a two-step process. First, an intermediate r pre- 
sentation of the summary must be created. Second, 
a natural language representation f the summary 
must be generated using the intermediate r presen- 
tation(Sparek Jones, 1993). Much of the early re- 
search in automatic text summarization has involved 
generation of the intermediate representation. The 
natural language generation problem has only re- 
cently received substantial attention in the context 
of summarization. 
1.1 Mot ivat ion 
In order to consider methods for generating natural 
text summaries from large documents, everal issues 
must be examined in detail. First, an analysis of the 
quality of the intermediate r presentation for use in 
generation must be examined. Second, a detailed 
examination of the processes which link the inter- 
mediate representation to a potential final summary 
must be undertaken. 
The system presented here provides a useful first 
step towards these ends. By developing a robust and 
efficient tool to generate these intermediate repre- 
sentations, we can both evaluate the representation 
......... andcormider the difficult problem of generatiiig nat- 
ural language texts from the representation. 
1.2 Background Research 
Much research as been conducted in the area of au- 
tomatic text summarization. Specifically, research 
using lexical chains and related techniques has re- 
ceived much attention. 
Early methods using word frequency counts did 
not consider the relations between similar words. 
Finding the aboutness of a document requires find- 
ing these relations. How these relations occur within 
a document is referred to as cohesion (Halliday 
and Hasan, 1976). First introduced by Morris and 
Hirst (1991), lexical chains represent lexical cohe- 
sion among related terms within a corpus. These 
relations can be recognized by identifying arbitrary 
size sets of words which are semantically related (i.e., 
have a sense flow). These lexical chains provide an 
interesting method for summarization because their 
recognition is easy within the source text and vast 
knowledge sources are not required in order to con> 
pure them. 
Later work using lexical chains was conducted by 
Hirst and St-Onge (1997) using lexical chains to cor- 
rect malapropisms. They used WordNet, a lexical 
database which contains ome semantic information 
(http://www.cs.princeton.edu/wn). 
Also using WordNet in their implenmntation. 
Barzilay and Elhadad (1997) dealt with some of tile 
limitations in Hirst and St-Onge's algorithm by ex- 
amining every possible lexical chain which could be 
computed, not just those possible at a given point 
in the text. That is to say, while Hirst and St.Onge 
would compute the chain in which a word should 
be placed when a word was first encountered, Barzi- 
lay and Elhadad computed ever:,' possible chain a 
word could become a member of when the word was 
encountered, and later determined the best interpre- 
tation. 
268 
2 A Linear T ime A lgor i thm for Intra Intra Adjacent Other 
Comput ing  Lexical  Chains Pgrph. Segment Segment 
2.1 Overv iew Same 1 1 1 1 
Our research on lexical chains as an intermediate Synonym 1 1 0 "O " 
representation forautomatic text summarization fol- Hypernym I 1 0 0 
lows the research of Barzilay and Elhadad (1997). Hyponym 1 1 0 0 
We use their results as a basis for the utility of Sibling 1 0 0 0 
the methodology. The most substantial difference is 
that Barzi lay and Elhadad create all possible chains 
explicit ly and then choose the best possible chain, 
whereas we compute them implicitly. 
Table 1: Dynamic Scoring Metrics Set to Mimic 
B+E's  Algorithm 
the word itself. These scores are dynamic and can . . . . . . .  2~2 
As mentioned above, WordNet is a lexical database 
that contains substantial semantic information. In 
order to  facilitate fficient access, the WordNet noun 
database was re-indexed by line number as opposed 
to file position and the file was saved in a binary in- 
dexed format. The database access tools were then 
rewritten to take advantage of this new structure. 
The result  of this work is that  accesses to the Word- 
Net noun database can be accomplished an order 
of magnitude faster than with the original imple- 
mentation. No additional changes to the WordNet 
databases were made. The re-indexing also provided 
a zero-based continuous numbering scheme that is 
important  o our linear time algorithm. This impor- 
tance will be noted below. 
Modifications ~to. Word.Net . . . . . . . . . . . . . .  be set ~ased ,on:segmentation information, dista.nce, 
2.3 Our  A lgor i thm 
Step 1 For each word instance that is a noun 
For every sense of that word 
Compute all scored "meta-chains" 
Step 2 For each word instance 
Figure out which "meta-chain" 
it contributes most to 
Keep the word instance in that chain 
and remove it from all other 
Chains updating the scores 
of each "meta-chain" 
Figure 1: Basic linear time Algorithm for Comput- 
ing Lexical Chains 
Our basic lexical chain algorithm is described 
briefly in Figure 1. The algorithm takes a part of 
speech tagged corpus and extracts the nouns. Us- 
ing WordNet to collect sense information for each of 
these noun instances, the algorithm then computes 
scored "nmta-chains" based on the collected infor- 
mation. A "meta-chain" is a representation f every 
possible lexical chain that can be computed start-  
ing with a word of a given sense. These meta-chains 
are scored in the following manner. As each word in- 
stance is added, its contribution, which is dependent 
on the scoring metrics used, is added to the "meta- 
chain" score. The contribution is then stored within 
and type of relation. 
Currently, segmentation is accomplished prior to 
using our algorithm by executing Hearst's text tiler 
(Hearst, 1994). The sentence numbers of each seg- 
ment boundary are stored for use by our algorithm. 
These sentence numbers are used in conjunction 
with relation type as keys into a table of potential 
scores. Table 1 denotes ample metrics tuned to sim- 
ulate the system devised by Barzilay and Elhadad 
(1997). 
At this point, the collection of "meta-chains" con- 
talns all possible interpretations of the source doc- 
ument. The problem is that  in our final represen- 
tation, each word instance can exist in only one 
chain. To figure out which chain is the correct one, 
each word is examined.using the score contribution 
stored in Step 1 to determine which chain the given 
word instance contributes to most. By deleting the 
word instance from all the other chains, a represen- 
tation where each word instance exists in precisely 
one chain remains. Consequently, the sum of the 
scores of all the chains is maximal. This method is 
analogous to finding a maximal spanning tree in a 
graph of noun senses. These noun senses are all of 
the senses of each noun instance in the document. 
From this representation, the highest scored 
chains correspond to the important concepts in the 
original document. These important concepts can 
be used to generate a summary from the source 
text. Barzilay and Elhadad use the notion of strong 
chains (i.e., chains whose scores are in excess of two 
standard eviations above the mean of all scores) to 
determine which chains to include in a summary. 
Our system can use this method, as well as sev- 
eral other methods including percentage compres- 
sion and number of sentences. 
For a more detailed description of our algorithm 
please consult our previous work (Silber and McCoy, 
2000). 
2.4 Runt ime Ana lys i s  
In this analysis, we will not consider the computa- 
tional complexity of part of speech tagging, as that is 
not the focus of this research. Also, because the size 
269 
Worst Average 
Case Case 
C1 =No. of senses 30 2 
C2 =Parent/chi ld isa relations ,45147 t4 
Ca =No. of nouns in WordNet 94474 94474 
C4 =No. of synsets in WordNet 66025 66025 
C5 =No. of siblings 397 39 
C6 =Chains word can belong to 45474 55 
Table 2: Constants from WordNet 
and structure of WordNet does not change from ex- 
ecution to execution of.aJae.algorit, hm, we shall take 
these aspects of WordNet to be constant. We will 
examine each phase of our algorithm to show that 
the extraction of these lexical chains can indeed be 
done in linear time. For this analysis, we define con- 
stants from WordNet 1.6 as denoted in Table 2. 
Extracting information from WordNet entails 
looking up each noun and extracting all synset, Hy- 
ponym/Hypernym, and sibling information. The 
runtime of these lookups over the entire document 
is: 
n * (log(Ca) + Cl * C2 + Cl * C5) 
When building the graph of all possible chains, we 
simply insert the word into all chains where a rela- 
tion exists, which is clearly bounded by a constant 
(C6). The only consideration is the computation 
of the chain score. Since we store paragraph num- 
bers represented within the chain as well as segment 
boundaries, we can quickly determine whether the 
relations are intra-paragraph, intra-segment, or ad- 
jacent segment. We then look up the appropriate 
score contribution from the table of metrics. There- 
fore, computing the score contribution of a given 
word is constant. The runtime of building the graph 
of all possible chains is: 
n*C6 .5  
Finding the best chain is equally efficient. For 
each word, each chain to which it belongs is exam- 
ined. Then, the word is marked as deleted from 
all but the single chain whose score the word con- 
tributes to most. In the case of a tie, the lower sense 
nmnber from WordNet is used, since this denotes a 
more general concept. The runtime for this step is: 
n*C6 .4  
This analysis gives an overall worst case runtime 
of: 
n * 1548216 + log(94474 ) + 227370 
and an average case runtime of: 
n ? 326 + log(94474) + 275 
While the constants are quite large, the algorithm 
is clearly O(n) in the number of nouns in the original 
document. 
A t  "first glance, "the'constants ~involved seem pro- 
hibitively large. Upon further analysis, however, we 
see that most synsets have very few parent child re- 
lations. Thus the worst case values maynot  reflect 
the actual performance of our application. In ad- 
dition, the synsets with many parent child relations 
tend to represent extremely general concepts. These 
synsets will most likely not appear very often as a 
direct synset for words appearing in a document. 
"2,;5 User  ~Interface 
Our system currently can be used as a command 
line utility. The arguments allow the user to specify 
scoring metrics, summary length, and whether or 
not to search for collocations. Additionally, a web 
CGI interface has been added as a front end which 
allows a user to specify not just text documents, but 
html documents as well, and summarize them from 
the Internet. Finally, our system has been attached 
to a search engine. The search engine uses data from 
existing search engines on the Internet o download 
and summarize ach page from the results. These 
summaries are then compiled and returned to the 
user on a single page.  The final result is that a 
search results page is returned with automatically 
generated summaries. 
2.6 Compar i son  w i th  P rev ious  Work  
As mentioned above, this research is based on the 
work of Barzilay and Elhadad (1997) on lexical 
chains. Several differences exist between our method 
and theirs. First and foremost, the linear run-time 
of our algorithm allows documents to be summarized 
much faster. Our algorithm can summarize a 40,000 
word document in eleven seconds on a Sun SPARC 
Ultra10 Creator. By comparison, our first version 
of the algorithm which computed lexical chains by 
building every possible interpretation like Barzilay 
and Elhadad took sLx minutes to extract chains from 
5,000 word documents. 
The linear nature of our algorithm also has sev- 
eral other advantages. Since our algorithm is also 
linear in space requirements, we can consider all pos- 
sible chains. Barzilay and Elhadad had to prune in- 
terpretations (enid thus chains) which did not seem 
promising. Our algorithm does not require pruning 
of chains. 
Our algorithm also allows us to analyze the iin- 
portance of segmentation. Barzilay and Elhadad 
used segmentation to reduce the complexity of the 
problem of extracting chains. They basically built 
chains within a segment and combined these chains 
later when chains across segment boundaries hared 
a word in the same sense in common. While we in- 
clude segmentation i formation in our algorithm, it 
270 
is merely because it might prove useful in disam- 
biguating chains. The fact that we can use it or not 
allows our algorithm to test the importance of seg- 
mentation to proper-word ~ense disambiguation. It 
is important o note that on short documents, like 
those analyzed by Barzilay and Elhadad, segmen- 
tation appears to have little effect. There is some 
linguistic justification for this fact. Segmentation 
is generally computed using word frequencies, and 
our lexical chains algorithm generally captures the 
same type of information. On longer documents, 
our research as shown segmentation to have a much 
greater effect. 
3 Cur rent  Research  and  Future  
D i rec t ions  
Some issues which are not currently addressed by 
this research are proper name disambiguation and 
anaphora resolution. Further, while we attempt o 
locate two-word collocations using WordNet, a more 
robust collocation extraction technique iswarranted. 
One of the goals of this research is to eventually 
create a system which generates natural language 
summaries. Currently, the system uses sentence se- 
lection as its method of generation. It is our con- 
tention that regardless of how well an algorithm for 
extracting sentences may be, it cannot possibly cre- 
ate quality summaries. It seems obvious that sen- 
tence selection will not create fluent, coherent text. 
Further, our research shows that completeness is a 
problem. Because information extraction is only at 
the sentence boundary, information which may be 
very important may be left out if a highly com- 
pressed summary is required. 
Our current research is examining methods of us- 
ing all of the important sentences determined by our 
lexical chains algorithm as a basis for a generation 
system. Our intent is to use the lexical chains algo- 
rithm to determine what to summarize, and then a 
more classical generation system to present he in- 
formation as coherent text. The goal is to combine 
and condense all significant information pertaining 
to a given concept which can then be used in gener- 
ation. 
4 Conc lus ions  
\Ve have described a domain independent summa- 
rization engine which allows for efficient summariza- 
tion of large documents. The algorithm described is 
clearly O(n) in the number of nouns in the original 
document. 
In their research, Barzilay and Elhadad showed 
that lexieal chains could be an effective tool for 
automatic text summarization (Barzilay and EI- 
hadad, 1997). By developing a linear time al- 
gorithm to compute these chains, we have pro- 
dueed a front end to a summarization system which 
can be implemented efficiently. An operational 
sample of this demo is available on the web at 
http://www.eecis.udel.edu/- silber/research.htm. 
..... While. ,usable currenlfly, the-system provides a 
platform for generation research on automatic text 
summarization by providing an intermediate r pre- 
sentation which has been shown to capture impor- 
tant concepts from the source text (Barzilay and 
Elhadad, 1997). The algorithm's peed and effec- 
tiveness allows research into summarization f larger 
documents. Moreover, its domain independence al- 
lows for research into the inherent differences be- 
tween domains. 
5 Acknowledgements  
The authors wish to thank the Korean Government, 
Ministry of Science and Technology, whose funding, 
as part of the Bilingual Internet Search Machine 
Project, has made this research possible. Addition- 
ally, special thanks to Michael Elhadad and Regina 
Barzilay for their advice, and for generously making 
their data and results available. 
Re ferences  
Regina Barzilay and Michael Elhadad. 1997. Us- 
ing lexical chains for text summarization. Pro- 
ceedings of the Intelligent Scalable Text Summa- 
rization Workshop, A CL Madrid. 
Michael Halliday and Ruqaiya Hasan. 1976. Cohe- 
sion in English. Longman, London. 
Marti A. Hearst. 1994. Multi-paragraph segmenta- 
tion of expository text. Proceedings o\] the 32nd 
Annual Meeting of the ACL. 
Gramme Hirst and David St-Onge. 1997. Lexical 
chains as representation f context for the detec- 
tion and correction of malapropisms. Wordnet: 
An electronic lexical database and some of its ap- 
plications. 
J. Morris and G. Hirst. 1991. Lexical cohesion com- 
puted by thesanral relations an an indecator of 
the structure of text. Computational Linguistics, 
18:21--45. 
H. Gregory Silber and Kathleen F. McCoy. 2000. 
Efficient text summarization using lexical chains. 
Conference on bztelligent User b~terfaces 2000. 
271 

Generation of single-sentence paraphrases from
predicate/argument structure using lexico-grammatical resources
Raymond Kozlowski, Kathleen F. McCoy, and K. Vijay-Shanker
Department of Computer and Information Sciences
University of Delaware
Newark, DE 19716, USA
kozlowsk,mccoy,vijay@cis.udel.edu
Abstract
Paraphrases, which stem from the va-
riety of lexical and grammatical means
of expressing meaning available in a
language, pose challenges for a sen-
tence generation system. In this
paper, we discuss the generation of
paraphrases from predicate/argument
structure using a simple, uniform gen-
eration methodology. Central to our
approach are lexico-grammatical re-
sources which pair elementary seman-
tic structures with their syntactic re-
alization and a simple but powerful
mechanism for combining resources.
1 Introduction
In natural language generation, producing some
realization of the input semantics is not the only
goal. The same meaning can often be expressed
in various ways using dierent lexical and syn-
tactic means. These dierent realizations, called
paraphrases, vary considerably in appropriate-
ness based on pragmatic factors and commu-
nicative goals. If a generator is to come up with
the most appropriate realization, it must be ca-
pable of generating all paraphrases that realize
the input semantics. Even if it makes choices on
pragmatic grounds during generation and pro-
duces a single realization, the ability to generate
them all must still exist.
Variety of lexical and grammatical forms
of expression pose challenges to a generator
((Stede, 1999); (Elhadad et al, 1997); (Nicolov
et al, 1995)). In this paper, we discuss the gen-
eration of single-sentence paraphrases realizing
the same semantics in a uniform fashion using a
simple sentence generation architecture.
In order to handle the various ways of realiz-
ing meaning in a simple manner, we believe that
the generation architecture should not be aware
of the variety and not have any special mech-
anisms to handle the dierent types of realiza-
tions
1
. Instead, we want all lexical and gram-
matical variety to follow automatically from the
variety of the elementary building blocks of gen-
eration, lexico-grammatical resources.
We have developed a fully-operational proto-
type of our generation system capable of gen-
erating the examples presented in this paper,
which illustrate a wide range of paraphrases.
As we shall see, the paraphrases that are pro-
duced by the system depend entirely on the
actual lexicon used in the particular applica-
tion. Determining the range of alternate forms
that constitute paraphrases is not the focus of
this work. Instead, we describe a framework in
which lexico-grammatical resources, if properly
dened, can be used to generate paraphrases.
2 Typical generation methodology
Sentence generation takes as input some seman-
tic representation of the meaning to be conveyed
in a sentence. We make the assumption that
1
Ability to handle variety in a uniform manner is also
important in multilingual generation as some forms avail-
able in one language may not be available in another.
ENJOY
EXPERIENCER THEME
AMY INTERACTION
Figure 1: The semantics underlying (2a-2b)
the input is a hierarchical predicate/argument
structure such as that shown in Fig. 1. The
output of this process should be a set of gram-
matical sentences whose meaning matches the
original semantic input.
One standard approach to sentence genera-
tion from predicate/argument structure (like the
semantic-head-driven generation in (Shieber et
al., 1990)) involves a simple algorithm.
1. decompose the input into the top predicate
(to be realized by a (single) lexical item that
serves as the syntactic head) and identify
the arguments and modiers
2. recursively realize arguments, then modi-
ers
3. combine the realizations in step 2 with the
head in step 1
In realizing the input in Fig. 1, the input can be
decomposed into the top predicate which can be
realized by a syntactic head (a transitive verb)
and its two arguments, the experiencer and the
theme. Suppose that the verb enjoy is chosen
to realize the top predicate. The two arguments
can then be independently realized as Amy and
the interaction. Finally, the realization of the
experiencer, Amy, can be placed in the subject
position and that of the theme, the interaction,
in the complement position, yielding (2a).
Our architecture is very similar but we argue
for a more central role of lexico-grammatical re-
sources driving the realization process.
3 Challenges in generating
paraphrases
Paraphrases come from various sources. In this
section, we give examples of some types of para-
phrases we handle and discuss the challenges
they pose to other generators. We also identify
types of paraphrases we do not consider.
3.1 Paraphrases we handle
Simple synonymy The simplest source of
paraphrases is simple synonymy. We take sim-
ple synonyms to be dierent words that have
the same meaning and are of the same syntactic
category and set up the same syntactic context.
(1a) Booth killed Lincoln.
(1b) Booth assassinated Lincoln.
A generation system must be able to allow
the same semantic input to be realized in dif-
ferent ways. Notice that the words kill and as-
sassinate are not always interchangeable, e.g.,
assassinate is only appropriate when the victim
is a famous person. Such constraints need to be
captured with selectional restrictions lest inap-
propriate realizations be produced.
Dierent placement of argument realiza-
tions Sometimes dierent synonyms, like the
verbs enjoy and please, place argument realiza-
tions dierently with respect to the head, as il-
lustrated in (2a-2b).
(2a) Amy enjoyed the interaction.
(2b) The interaction pleased Amy.
To handle this variety, a uniform generation
methodology should not assume a xed map-
ping between thematic and syntactic roles but
let each lexical item determine the placement of
argument realizations. Generation systems that
use such a xed mapping must override it for
the divergent cases (e.g., (Dorr, 1993)).
Words with overlapping meaning There
are often cases of dierent words that realize dif-
ferent but overlapping semantic pieces. The eas-
iest way to see this is in what has been termed
incorporation, where a word not only realizes a
predicate but also one or more of its arguments.
Dierent words may incorporate dierent argu-
ments or none at all, which may lead to para-
phrases, as illustrated in (3a-3c).
(3a) Charles ew across the ocean.
(3b) Charles crossed the ocean by plane.
(3c) Charles went across the ocean by plane.
Notice that the verb y realizes not only go-
ing but also the mode of transportation being a
plane, the verb cross with its complement real-
ize going whose path is across the object realized
by the complement, and the verb go only real-
izes going. For all of these verbs, the remaining
arguments are realized by modiers.
Incorporation shows that a uniform genera-
tor should use the word choices to determine 1)
what portion of the semantics they realize, 2)
what portions are to be realized as arguments
of the realized semantics, and 3) what portions
remain to be realized and attached as modiers.
Generation systems that assume a one-to-one
mapping between semantic and syntactic units
(e.g., (Dorr, 1993)) must use special processing
for cases of overlapping semantics.
Dierent syntactic categories Predicates
can often be realized by words of dierent syn-
tactic categories, e.g., the verb found and the
noun founding, as in (4a-4b).
(4a) I know that Olds founded GM.
(4b) I know about the founding of GM by Olds.
Words of dierent syntactic categories usu-
ally have dierent syntactic consequences. One
such consequence is the presence of additional
syntactic material. Notice that (4b) contains
the prepositions of and by while (4a) does not.
These prepositions might be considered a syn-
tactic consequence of the use of the noun found-
ing in this conguration. Another syntactic con-
sequence is a dierent placement of argument re-
alizations. The realization of the founder is the
subject of the verb found in (4a) while in (4b)
the use of founding leads to its placement in the
object position of the preposition by.
Grammatical alternations Words can be
put in a variety of grammatical alternations such
as the active and passive voice, as in (5a-5b), the
topicalized form, the it-cleft form, etc.
(5a) Oswald killed Kennedy.
(5b) Kennedy was killed by Oswald.
The choice of dierent grammatical alterna-
tions has dierent syntactic consequences which
must be enforced in generation, such as the pres-
ence or absence of the copula and the dierent
placement of argument realizations. In some
systems such as ones based on Tree-Adjoining
Grammars (TAG), including ours, these con-
sequences are encapsulated within elementary
structures of the grammar. Thus, such systems
do not have to specically reason about these
consequences, as do some other systems.
More complex alternations The same con-
tent of excelling at an activity can be realized by
the verb excel, the adverb well, and the adjective
good, as illustrated in (6a-6c).
(6a) Barbara excels at teaching.
(6b) Barbara teaches well.
(6c) Barbara is a good teacher.
This variety of expression, often called head
switching, poses a considerable diculty for
most existing sentence generators. The di-
culty stems from the fact that the realization
of a phrase (sentence) typically starts with the
syntactic head (verb) which sets up a syntactic
context into which other constituents are t. If
the top predicate is the excelling, we have to be
able to start generation not only with the verb
excel but also with the adverb well and the ad-
jective good, typically not seen as setting up an
appropriate syntactic context into which the re-
maining arguments can be t. Existing genera-
tion systems that handle this variety do so using
special assumptions or exceptional processing,
all in order to start the generation of a phrase
with the syntactic head (e.g., (Stede, 1999), (El-
hadad et al, 1997), (Nicolov et al, 1995), (Dorr,
1993)). Our system does not require that the se-
mantic head map to the syntactic head.
Dierent grammatical forms realizing se-
mantic content Finally, we consider a case,
which to our knowledge is not handled by other
generation systems, where grammatical forms
realize content independently of the lexical item
on which they act, as in (7a-7b).
(7a) Who rules Jordan?
(7b) Identify the ruler of Jordan!
The wh-question form, as used in (7a), real-
izes a request for identication by the listener
(in this case, the ruler of Jordan). Likewise, the
imperative structure (used in (7b)) realizes a re-
quest or a command to the listener (in this case,
to identify the ruler of Jordan).
3.2 Paraphrases we do not consider
Since our focus is on sentence generation and not
sentence planning, we only consider the genera-
tion of single-sentence paraphrases. Hence, we
do not have the ability to generate (8a-8b) from
the same input.
(8a) CS1 has a programming lab.
(8b) CS1 has a lab. It involves programming.
Since we do not reason about the semantic
input, including deriving entailment relations,
we cannot generate (9a-9b) from the same input.
(9a) Oslo is the capital of Norway.
(9b) Oslo is located in Norway.
4 Our generation methodology
Generation in our system is driven by the
semantic input, realized by selecting lexico-
grammatical resources matching pieces of it,
starting with the top predicate. The realization
of a piece containing the top predicate provides
the syntactic context into which the realizations
of the remaining pieces can be t (their place-
ment being determined by the resource).
The key to our ability to handle paraphrases
in a uniform manner is that our processing is
driven by our lexicon and thus we do not make
any a priori assumptions about 1) the amount
of the input realized by a lexical unit, 2) the re-
lationship between semantic and syntactic types
(and thus the syntactic rank or category of the
realization of the top piece), 3) the nature of
the mapping between thematic roles and syn-
tactic positions, and 4) the grammatical alter-
nation (e.g., there are dierent resources for the
same verb in dierent alternations: the active,
passive, topicalized, etc.). Because this informa-
tion is contained in each lexico-grammatical re-
source, generation can proceed no matter what
choices are specied about these in each indi-
vidual resource. Our approach is fundamen-
tally dierent from systems that reason directly
about syntax and build realizations by syntactic
rank ((Bateman, 1997), (Elhadad et al, 1997);
(Nicolov et al, 1995); (Stone and Doran, 1997)).
4.1 Our algorithm
Our generation algorithm is a simple, recursive,
semantic-head-driven generation process, con-
sistent with the approach described in section 2,
but one driven by the semantic input and the
lexico-grammatical resources.
1. given an unrealized input, nd a lexico-
grammatical resource that matches a por-
tion including the top predicate and satis-
es any selectional restrictions
2. recursively realize arguments, then modi-
ers
3. combine the realizations in step 2 with the
resource in step 1, as determined by the re-
source in step 1
Notice the prominence of lexico-grammatical re-
sources in steps 1 and 3 of this algorithm. The
standard approach in section 2 need not be
driven by resources.
4.2 Lexico-grammatical resources
The key to the simplicity of our algorithm lies in
the lexico-grammatical resources, which encap-
sulate information necessary to carry through
generation. These consist of three parts:
 the semantic side: the portion of seman-
tics realized by the resource (including the
predicate and any arguments; this part is
matched against the input semantics)
 the syntactic side: either word(s) in a syn-
tactic conguration or a grammatical form
without words, and syntactic consequences
 a mapping between semantic and syntactic
constituents indicating which constituent
on the semantic side is realized by which
constituent on the syntactic side
Consider the resources for the verbs enjoy and
please in Fig. 2. The semantic sides indicate
that these resources realize the predicate ENJOY
and the thematic roles EXPERIENCER and THEME.
The arguments lling those roles (which must be
realized separately, as indicated by dashed out-
lines) appear as variables X and Y which will be
matched against actual arguments. The syntac-
tic sides contain the verbs enjoy and please in
the active voice conguration. The mappings
include links between ENJOY and its realization
as well as links between the unrealized agent (X)
or theme (Y) and the subject or the complement.
Our mapping between semantic and syntactic
constituents bears resemblance to the pairings in
Synchronous TAG (Shieber and Schabes, 1990).
Just like in Synchronous TAG, the mapping is
VPNP
VP
NPV
enjoy
u
S
0 0
1
ENJOY
EXPERIENCER THEME
X Y
1
VPNP
VP
NPV
please
u
S
0 0
1
ENJOY
EXPERIENCER THEME
X Y
1
Figure 2: Two dierent resources for ENJOY
critical for combining realizations (in step 3 of
our algorithm in section 4.1). There are, how-
ever, advantages that our approach has. For
one, we are not constrained by the isomorphism
requirement in a Synchronous TAG derivation.
Also, the DSG formalism that we use aords
greater exibility, signicant in our approach, as
discussed later in this paper (and in more detail
in (Kozlowski, 2002b)).
4.3 The grammatical formalism
Both step 3 of our algorithm (putting re-
alizations together) and the needs of lexico-
grammatical resources (the encapsulation of
syntactic consequences such as the position
of argument realizations) place signicant de-
mands on the grammatical formalism to be used
in the implementation of the architecture. One
grammatical formalism that is well-suited for
our purposes is the D-Tree Substitution Gram-
mars (DSG, (Rambow et al, 2001)), a variant
of Tree-Adjoining Grammars (TAG). This for-
malism features an extended domain of locality
and exibility in encapsulation of syntactic con-
sequences, crucial in our architecture.
Consider the elementary DSG structures on
the right-hand-side of the resources for enjoy
and please in Fig. 2. Note that nodes marked
with # are substitution nodes corresponding to
syntactic positions into which the realizations of
S
u
0 0
1
1
VPNP
VP
please
NPVu
S
0 0
1
1
NP
the interactionAmy
VPNP
VP
V
enjoy
Figure 3: Combining argument realizations with
the resources for enjoy and please
arguments will be substituted. The positions of
both the subject and the complement are en-
capsulated in these elementary structures. This
allows the mapping between semantic and syn-
tactic constituents to be dened locally within
the resources. Dotted lines indicate domination
of length zero or more where syntactic material
(e.g., modiers) may end up.
4.4 Using resources in our algorithm
Step 1 of our algorithm requires matching the se-
mantic side of a resource against the top of the
input and testing selectional restrictions. A se-
mantic side matches if it can be overlaid against
the input. Details of this process are given
in (Kozlowski, 2002a). Selectional restrictions
(type restrictions on arguments) are associated
with nodes on the semantic side of resources.
In their evaluation, the appropriate knowledge
base instance is accessed and its type is tested.
More details about using selectional restrictions
in generation and in our architecture are given
in (Kozlowski et al, 2002).
Resources for enjoy and please which match
the top of the input in Fig. 1 are shown in
Fig. 2. In doing the matching, the arguments
AMY and INTERACTION are unied with X and
Y. The dashed outlines around X and Y indicate
that the resource does not realize them. Our al-
gorithm calls for the independent recursive real-
ization of these arguments and then putting to-
gether those realizations with the syntactic side
of the resource, as indicated by the mapping.
0u
fly
VPNP
VP
S
0
V
GO
1
PLANE
AGENT MODE
X
ACROSS
VPNP
VP
NPV
cross
u
S
0 0
1
GO
AGENT PATH
X
1
THEME
Y
Figure 4: Two dierent resources for GO
PATH MODE
ACROSS
OCEAN
THEME
AGENT
GO
PLANECHARLES
Figure 5: The semantics underlying (3a-3c) with
portion realized by cross in bold
This is shown in Fig. 3. The argument realiza-
tions, Amy and the interaction, are placed in the
subject and complement positions of enjoy and
please, according to the mapping in the corre-
sponding resources.
4.5 Driving decomposition by resources
The semantic side of a resource determines
which arguments, if any, are realized by the re-
source, while the matching done in step 1 of our
algorithm determines the portions that must be
realized by modiers. This is always done the
same way regardless of the resources selected
and how much of the input they realize, such
as the two resources realizing the predicate GO
shown in Fig. 4, one for y which incorporates
MODE PLANE and another for cross which incor-
porates PATH ACROSS.
YX
AGENT THEME
NP
FOUND
Y
1
X
THEMEAGENT
1
00
S
u
found
V
VP
NP VP
u
N?
the
u 0
NP
D N?
1
FOUND
2
PP
1by
P NP
founding
u
of
P NP
u PP
2
N
N?
Figure 6: Two dierent resources for FOUND
Suppose the semantic input underlying (3a-
3c) is as given in Fig. 5. The portion shown
in bold is realized by the resource for cross in
Fig. 4. The agent of GO and the theme of ACROSS
are to be realized as arguments. The remaining
thematic role MODE with the argument PLANE ll-
ing it, is to be realized by a modier.
4.6 Encapsulation of syntactic
consequences
All syntactic information should be encapsu-
lated within resources and transparent to the
algorithm. This includes the identication of ar-
guments, including their placement with respect
to the realization. Another example of a syn-
tactic consequence is the presence of additional
syntactic material required by the lexical item in
the particular syntactic conguration. The verb
found in the active conguration, as in (4a), does
not require any additional syntactic material.
On the other hand, the noun founding in the
conguration with prepositional phrases headed
by of and by, as in (4b), may be said to require
the use of the prepositions. The resources for
found and founding are shown in Fig. 6. Encap-
sulation of such consequences allows us to avoid
special mechanisms to keep track of and enforce
EXPERIENCER
EXCEL
THEME
[0]:
1
uV
VP
u
at
P
PP
[0]:
excel
VP
[0]:
[0]:
0
u
well
Adv
0Adv?
1Adv?
1
VP
AdvP
P
AGENT
NP
1
PRO
NP
S
P
THEMEEXPERIENCER
EXCEL
00
S
VP
Figure 7: Two dierent resources for EXCEL
them for individual resources.
4.7 Syntactic rank and category
No assumptions are made about the realization
of a piece of input semantics, including its syn-
tactic rank and category. For instance, the pred-
icate EXCEL can be realized by the verb excel,
the adverb well, and the adjective good, as illus-
trated in (6a-6c). The processing is the same:
a resource is selected and any argument realiza-
tions are attached to the resource.
Fig. 7 shows a resource for the predicate
EXCEL realized by the verb excel. What is in-
teresting about this case is that the DSG for-
malism we chose allows us to encapsulate the
PRO in the subject position of the complement
as a syntactic consequence of the verb excel in
this conguration. The other resource for EXCEL
shown in Fig. 7 is unusual in that the predicate
is realized by an adverb, well. Note the link be-
tween the uninstantiated theme on the semantic
side and the position for its corresponding syn-
tactic realization, the substitution node VP
1
2
.
Suppose the semantic input underlying (6a-
2
Also notice that the experiencer of EXCEL is consid-
ered realized by the well resource and coindexed with the
agent of the theme of EXCEL, to be realized by a separate
resource.
[1]
[1]: BARBARA
TEACH
[1]
AGENT
EXCEL
THEMEEXPERIENCER
Figure 8: The semantics underlying (6a-6c)
6c) is as given in Fig. 8 and the well resource in
Fig. 7 is selected to realize the top of the seman-
tics. The matching in step 1 of our algorithm
determines that the subtree of the input rooted
at TEACH must be recursively realized. The re-
alization of this subtree yields Barbara teaches.
Because of the link between the theme of EXCEL
and the VP
1
node of well, the realization Bar-
bara teaches is substituted to the VP
1
node of
well. This is a more complex substitution than
in regular TAG (where the substitution node is
identied with the root of the argument realiza-
tion), and is equivalent to the adjunction of well
to Barbara teaches. In DSG, we are able to treat
structures such as the well structure as initial
and not auxiliary, as TAG would. Thus, argu-
ment realizations are combined with all struc-
tures in a uniform fashion.
4.8 Grammatical forms
As discussed before, grammatical forms them-
selves can realize a piece of semantics. For in-
stance, the imperative syntactic form realizes a
request or a command to the listener, as shown
in Fig. 9. Likewise, the wh-question form real-
izes a request to identify, also shown in Fig. 9.
In our system, whether the realization has any
lexical items is not relevant.
4.9 The role of DSG
We believe that the choice of the DSG formal-
ism plays a crucial role in maintaining our sim-
ple methodology. Like TAG, DSG allows cap-
turing syntactic consequences in one elementary
structure. DSG, however, allows even greater
exibility in what is included in an elementary
structure. Note that in DSG we may have non-
immediate domination links between nodes of
[empty:+]
[subj?empty:+]
[0]:
[0]:
REQUEST
ACTION
ACTION
REQUEST
P
YOU
S
NP
(you)
IDENTIFY
THEME
S 1NP [inv:+]
S
NP
?who
uN
YOU SET?OF
THEME SUCH?THAT
P
AGENT
Figure 9: Two dierent resources for REQUEST
dierent syntactic categories (e.g., between the S
and NP in Fig. 9 and also in the excel at structure
in Fig. 7). DSG also allows uniform treatment
of complementation and modication using the
operations of substitution (regardless of the re-
alization of the predicate, e.g., the structures in
Fig. 7) and adjunction, respectively.
5 Conclusions
Although we only consider paraphrases with the
same semantics, there is still a wide variety of
expression which poses challenges to any genera-
tion system. In overcoming those challenges and
generating in a simple manner in our architec-
ture, our lexico-grammatical resources play an
important role in each phase of generation. En-
capsulation of syntactic consequences within ele-
mentary syntactic structures keeps our method-
ology modular. Whatever those consequences,
often very dierent for dierent paraphrases,
generation always proceeds in the same manner.
Both the algorithm and the constraints on
our lexico-grammatical resources place signif-
icant demands on the grammatical formalism
used for the architecture. We nd that the DSG
formalism meets those demands well.
References
John Bateman. 1997. Enabling technology for mul-
tilingual natural language generation: the KPML
development environment. Natural Language En-
gineering, 3(1):15{55.
Bonnie J. Dorr. 1993. Interlingual machine transla-
tion: a parametrized approach. Articial Intelli-
gence, 63(1):429{492.
Michael Elhadad, Kathleen McKeown, and Jacques
Robin. 1997. Floating constraints in lexical
choice. Computational Intelligence, 23:195{239.
Raymond Kozlowski, Kathleen F. McCoy, and
K. Vijay-Shanker. 2002. Selectional restrictions
in natural language sentence generation. In Pro-
ceedings of the 6th World Multiconference on Sys-
temics, Cybernetics, and Informatics (SCI'02).
Raymond Kozlowski. 2002a. Driving multilingual
sentence generation with lexico-grammatical re-
sources. In Proceedings of the Second Interna-
tional Natural Language Generation Conference
(INLG'02) - Student Session.
Raymond Kozlowski. 2002b. DSG/TAG - An appro-
priate grammatical formalism for exible sentence
generation. In Proceedings of the Student Research
Workshop at the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL'02).
Nicolas Nicolov, Chris Mellish, and Graeme Ritchie.
1995. Sentence Generation from Conceptual
Graphs. In Proceedings of the 3rd International
Conference on Conceptual Structures (ICCS'95).
Owen Rambow, K. Vijay-Shanker, and David Weir.
2001. D-Tree Substitution Grammars. Computa-
tional Linguistics, 27(1):87{122.
Stuart M. Shieber and Yves Schabes. 1990. Syn-
chronous Tree-Adjoining Grammars. In Proceed-
ings of the 13th International Conference on Com-
putational Linguistics.
Stuart M. Shieber, Gertjan van Noord, Fernando
C. N. Pereira, and Robert C. Moore. 1990.
Semantic-Head-Driven Generation. Computa-
tional Linguistics, 16(1):30{42.
Manfred Stede. 1999. Lexical semantics and knowl-
edge representation in multilingual text genera-
tion. Kluwer Academic Publishers, Boston.
Matthew Stone and Christine Doran. 1997. Sen-
tence Planning as Description Using Tree Adjoin-
ing Grammar. In Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics (ACL'97).
Understanding Information Graphics: A Discourse-Level Problem ?
?Sandra Carberry, ?Stephanie Elzer, ??Nancy Green, ?Kathleen McCoy, and ?Daniel Chester
?Dept. of Computer Science, University of Delaware, Newark, DE 19716
(carberry, elzer, mccoy, chester@cis.udel.edu)
??Dept. of Math. Sciences, Univ. of North Carolina at Greensboro, Greensboro, NC 27402
(nlgreen@uncg.edu)
Abstract
Keywords: graphics, understanding, dis-
course, plan-based models
Information graphics that appear in newspa-
pers and magazines generally have a message that
the viewer is intended to recognize. This paper ar-
gues that understanding such information graph-
ics is a discourse-level problem. In particular,
it requires assimilating information from multi-
ple knowledge sources to recognize the intended
message of the graphic, just as recognizing in-
tention in text does. Moreover, when an article
is composed of text and graphics, the intended
message of the information graphic (its discourse
intention) must be integrated into the discourse
structure of the surrounding text and contributes
to the overall discourse intention of the article.
This paper describes how we extend plan-based
techniques that have been used for understanding
traditional discourse to the understanding of in-
formation graphics. This work is part of a project
to develop an interactive natural language system
that provides sight-impaired users with access to
information graphics.
1 Introduction
Information graphics (non-pictorial graphics
such as bar charts and line graphs) are a variant of
language with many similarities to other forms of
communication. Information graphics are preva-
lent in information resources since they enable
complex information to be assimilated perceptu-
ally with ease. Unfortunately, knowledge sources
such as information graphics are not accessible to
some users. For example, individuals with im-
0The work of the third author was supported by the Na-
tional Science Foundation under Grant No. 0132821.
paired eyesight have limited access to information
graphics, thus preventing them from fully utiliz-
ing information resources.
Some information graphics are only intended
to display data values; (Yu et al, 2002) devel-
oped a pattern recognition algorithm for summa-
rizing interesting features of automatically gener-
ated graphics of time-series data from a gas tur-
bine engine. However, the overwhelming major-
ity of the graphics that we have examined (taken
from newspaper, magazine, and web articles) ap-
pear to have some underlying goal, such as get-
ting the viewer to believe that interest rates have
fallen substantially and that this would therefore
be a good time to refinance a mortgage. We
have found that understanding information graph-
ics is a discourse-level problem. In particular,
it requires assimilating information from multi-
ple knowledge sources to recognize the intended
message of the graphic, just as recognizing inten-
tion in text does. Moreover, the communicative
intention of the information graphic must be in-
tegrated into the discourse intentions of the sur-
rounding text.
We are developing an interactive natural lan-
guage system that infers the intended message
underlying an information graphic, augments it
with related interesting features of the graphic,
provides an initial summary of the graphic, and
then responds to followup questions from the
user. This paper presents the system architecture,
shows why interpreting information graphics is
a discourse-level problem, and outlines how we
extend techniques that have been used for under-
standing traditional discourse to the understand-
ing of information graphics.
2 A Natural Language Modality
Information is the key to knowledge and ef-
fective decision-making. But information is use-
ful only if it is accessible in a form that can be
easily assimilated. For sighted users, information
graphics capture complex information and enable
it to be assimilated perceptually with ease. For
individuals who have serious sight-impairments,
documents that contain information graphics pose
challenging problems. Although devices have
been developed for conveying information graph-
ics in alternative mediums such as musical tones
or tactile images, these approaches have serious
limitations. For example, systems that attempt to
convey graphics via a soundscape(Meijer, 1992)
do not facilitate easy comparison of two line
graphs linked in a single graphical display. More-
over, these approaches require the user to con-
struct a ?mental map? of the graphic, which is
difficult for congenitally blind users who do not
have the personal knowledge to assist them in the
interpretation of the image(Kennel, 1996). The
underlying hypothesis of our work is that alterna-
tive access to what the graphic looks like is not
enough ? the user should be provided with the
message and knowledge that one would gain from
viewing the graphic in order to enable effective
and efficient use of this information resource. To
accomplish this objective, we are developing an
interactive natural language system for communi-
cating the content of an information graphic. Our
methodology offers promise as a means of provid-
ing access to information graphics without expen-
sive equipment, with few limitations on the com-
plexity of the graphic that can be handled, and
with relatively little cognitive load on the user.
3 Architecture and Overview
Our current work is concerned with bar charts,
line graphs, and pie charts, although eventually
we will handle other kinds of graphics. Figure 1
shows the architecture of our system for convey-
ing information graphics. The visual extraction
component (VEC) analyzes the graphic and pro-
vides an XML representation of the graphic to
the intention recognition component (IRC). The
IRC is responsible for recognizing the intended
message of the information graphic and sending it
to the content planning component (CPC), which
will augment the intended message of the graphic
with related interesting features. The message or-
ganization component (MOC) then organizes the
most salient propositions into a coherent sum-
mary, which will be rendered in natural language
and conveyed to the user via speech synthesis.
The followup question component (FQC) will al-
low the user to interactively seek additional infor-
mation about the graphic.
Our work thus far (Section 4) has focused on
understanding an information graphic so that its
intended message can be conveyed to the user.
Section 4.1 discusses the extension of speech act
theory to the generation and understanding of in-
formation graphics. Section 4.2 argues that un-
derstanding information graphics is a discourse-
level problem in which the system must recog-
nize the intended message of the graphic and in-
tegrate it into the intentions of any surrounding
text; it further argues that understanding informa-
tion graphics requires similar kinds of knowledge
and processing as does the understanding of tra-
ditional textual discourse. Section 4.3 provides
a brief overview of the visual extraction compo-
nent that analyzes the graphical image and con-
structs an XML representation of the graphic for
use by the graphic understanding system. Sec-
tion 4.4 then describes how we have extended
techniques used for understanding traditional dis-
course and dialogue to the understanding of infor-
mation graphics. Section 5 gives a brief overview
of future work on the rest of the system. The Ap-
pendix contains information graphics that are part
of the corpus on which our work is based.
4 Understanding Information Graphics
4.1 Intention in Information Graphics
Information graphics are a variant of language.
As noted by Clark(Clark, 1996), language is more
than just words. It is any ?signal? (or lack of sig-
nal when one is expected), where a signal is a de-
liberate action that is intended to convey a mes-
sage. According to speech act theory, a speaker
or writer executes a speech act whose intended
meaning he expects the listener or reader to be
able to deduce(Searle, 1970; Grice, 1969; Clark,
1996). In their work on multimedia generation,
Figure 1: System Architecture
the AutoBrief group proposed that speech act the-
ory could be extended to cover the generation
of graphical representations(Kerpedjiev and Roth,
2000). They developed a multimedia presenta-
tion system that generated text and information
graphics. It included 1) an algorithm that could
map communicative goals to a set of perceptual
and cognitive tasks that must be enabled for a
viewer to recognize the goals and 2) an automatic
graph designer that used constraint satisfaction to
construct an information graphic that best facili-
tated those tasks, subject to competing constraints
among the tasks.
The overwhelming majority of information
graphics accompanying newspaper and magazine
articles appear to carry a message that the de-
signer intends to convey to the viewer by virtue
of the graphic?s design and the data presented in
the graphic. Consider the graphic in Figure 9. It
conveys the message that the salary of women
in science, mathematics, and engineering fields
is consistently less than that of men in the same
fields. Other messages could have been con-
veyed by a different graphic design. For ex-
ample, by grouping the bars for men together,
grouping the bars for women together, and or-
dering the bars for each group by height, the
graphic would have conveyed the message that
both men and women earn the least in the so-
cial sciences and the most in engineering. Or
if the bars for Computer/Mathematical Sciences
were highlighted in Figure 9 by coloring them
significantly differently from the other bars in the
graphic, the graphic would have invoked a com-
parison of the discrepancies between male and
female salaries in Computer/Mathematical Sci-
ences and the salary discrepancies between men
and women in other fields. Although a graphic?s
caption can be helpful in identifying its intended
message (as in Figure 8), Corio performed a large
corpus study(Corio and Lapalme, 1999) in which
he found that captions are often missing or fail
to provide any indication of what the information
graphic conveys (as in Figures 6 and 10). Thus
we cannot rely entirely on the presence of useful
captions to identify the intended message of an
information graphic.
Language research has posited that the listener
or reader who is interpreting a speech act identi-
fies its intended meaning by reasoning about the
observed signals and the mutual beliefs of author
and interpreter(Grice, 1969; Clark, 1996). Ap-
plying this to graphical displays, it is reasonable
to presume that the author of a graphic similarly
expects the viewer to use perceptual skills along
with other knowledge sources to deduce from the
graphic the message that he intended to convey.
Thus we are applying speech act theory in the re-
verse direction of the AutoBrief project, namely
to the recognition of the intended message under-
lying an information graphic.
4.2 A Discourse Level Problem
This section argues that interpreting informa-
tion graphics is a discourse-level problem ? not
only is it necessary to recognize the intention of
the graphic as noted in Section 4.1, but under-
standing an information graphic requires similar
kinds of knowledge and processing as does un-
derstanding traditional discourse.
Grosz and Sidner contended that discourse
has a structure comprised of discourse segments.
Each discourse segment has a discourse seg-
ment purpose that contributes to the discourse
purpose or intention underlying the overall dis-
course(Grosz and Sidner, 1986). When an arti-
cle is comprised of text and graphics, the graphic
generally expands on the text and contributes to
the discourse purpose of the article. Consider the
graphic and partial surrounding text reproduced
in Figure 6. Nowhere in the text is it stated that
the income of black women has risen dramati-
cally over the last decade and has reached the
level of white women. Yet this message is clearly
conveyed by the graphic and contributes to the
overall communicative intention of this portion
of the article ? namely, that there has been a
?monumental shifting of the sands? with regard
to the achievements of black women. Not only
does the intended message of the graphic (its dis-
course segment purpose) contribute to this overall
intention, but in fact the discourse intention of the
graphic helps to recognize the overall intention.
Even when the graphic stands in isolation as
in Figures 7 and 8, understanding the graphic
is a discourse-level problem. Grosz and Sid-
ner(Grosz and Sidner, 1986) claim that a robust
model of discourse understanding must use mul-
tiple knowledge sources in order to recognize the
complex relationships that utterances have to one
another. Information graphics have similar com-
plex relationships among their component ele-
ments. Not only might the graphic include mul-
tiple elements that must be related to one another
(such as multiple lines in a line graph, or individ-
ual bars in a bar chart), but information graphics
often include highlighting of certain elements to
make them particularly salient (as in Figure 10)
or include captions that might contribute to rec-
ognizing the graphic?s intention. The graphic in
Figure 8 includes such a helpful caption, although
many graphics, such as the ones in Figures 7
and 9, do not.
Furthermore, identifying the intended message
of a composite graphic (one comprised of multi-
ple individual graphics) requires relating the in-
dividual graphics to one another to identify the
intended message of the composite. Figure 11 il-
lustrates a composite information graphic. The
discourse purpose of the composite graphic is that
audits of affluent taxpayers are declining with re-
spect to audits of all taxpayers. This message can
only be deduced by relating the two individual
graphics and their underlying messages.
Moreover, understanding information graphics
requires the use of multiple knowledge sources.
In earlier work on recognizing expressions of
doubt, we developed an algorithm that combined
linguistic, contextual, and world knowledge and
applied it to the recognition of complex discourse
acts(Carberry and Lambert, 1999). In the case
of information graphics, the corollary to linguis-
tic knowledge is perceptual knowledge, by which
one recognizes the individual elements of the
graphic (for example, the bars in a bar chart), the
relation of the individual elements in the graphic
to one another, the type of graphic (line graph,
bar chart, pie chart, etc.), and what the different
graphic types can be used to convey. For exam-
ple, both a scatter plot and a pie chart can be
used to portray how an entity (such as govern-
ment income) is divided up among several cate-
gories (such as social welfare, military spending,
etc.); however, a graphic designer will choose a
pie chart if the intent is to convey the relative dis-
tributions as opposed to their absolute amounts.
Furthermore, a particular type of graphic (such as
a line graph) might be appropriate for conveying
several different intentions (maximum data point,
data trend, data variation, etc.).
Contextual and world knowledge are also es-
sential for understanding information graphics.
Contextual knowledge includes the caption as-
sociated with the graphic, any highlighting of
graphic elements that affects the focus of atten-
tion in the graphic, and the discourse structure and
focus of attention in any surrounding text. World
knowledge consists of mutual beliefs between de-
signer and viewer about entities of interest to the
intended viewing audience. For example, if an in-
formation graphic appears in a document targeted
at residents of New York City, then both the de-
signer and the viewer will mutually believe that
entities such as New York City, its football and
baseball teams, etc. will be particularly salient
to the viewer. Our methodology for understand-
ing information graphics takes these knowledge
sources into account.
4.3 The Visual Extraction Component
The visual extraction component (VEC) cap-
tures much of the perceptual knowledge discussed
in Section 4.2. It is responsible for recogniz-
ing the individual components comprising the
graphic, identifying the relationship of the differ-
ent components to one another and to the graphic
as a whole, and classifying the graphic as to type.
Extracted components include not only the bars,
lines, or wedges of a graphic but also the titles of
the axes, the legend, and the graphic?s title or cap-
tion. The present implementation deals only with
gray scale images (in pgm format) of bar charts,
pie charts, and line graphs, though eventually it
will be extended to handle color and other kinds
of information graphics. Words and numbers that
appear in the chart are associated with particular
bars, wedges and lines by their proximity to the
chart component in question. The output of the
visual extraction component is an XML file that
describes the chart and all of its components.
4.4 Applying Discourse Understanding
Strategies
Many researchers have cast the understanding
of discourse and dialogue as a plan recognition
problem ? that is, the writer or speaker (or char-
acters in the case of a story) has an underlying
goal and a plan for accomplishing that goal, and
understanding requires that the reader or listener
infer the plan and in turn the goal that the plan is
intended to achieve. (Perrault and Allen, 1980;
Wilensky, 1983; Litman and Allen, 1987; Car-
berry, 1990; Charniak and Goldman, 1993; Ardis-
onno and Sestero, 1996) are just a few examples
of such systems.
Since understanding information graphics is a
discourse-level problem, we are extending plan
inference techniques to recognizing the intended
message of an information graphic(Elzer et al,
2003) and to identifying its contribution to an
extended discourse that includes both text and
graphics. Planning and plan inference systems re-
quire knowledge about goals and how they can
be achieved. Typically, this is provided by a li-
brary of operators. Each operator encodes a goal
in its header; the body of the operator encodes
the subgoals that must be accomplished in order
to achieve the operator?s goal. A planning sys-
tem starts with a high-level goal, and uses oper-
ators to decompose the goal into a set of simpler
subgoals, which eventually decompose into prim-
itive subgoals that can be accomplished by prim-
itive actions in the domain. On the other hand,
a plan inference system starts with the primitive
goals associated with observed actions, and uses
the operators to chain backwards to higher-level
goals which the lower-level subgoals contribute to
achieving. In the case of traditional discourse and
dialogue, the subgoals in the plan operators are ei-
ther communicative or domain goals, and the ob-
served actions that start the plan inference process
are the speech acts represented by the utterances
in a story or a dialogue.
To extend plan inference to information graph-
ics, the plan operators must include goals that
can be accomplished by viewing an information
graphic, as opposed to being the recipient of an
utterance. As discussed in Section 4.1, the Auto-
Brief project(Kerpedjiev and Roth, 2000) devel-
oped an algorithm to map communicative goals to
a sequence of perceptual and cognitive tasks that
the graphic should support. Perceptual tasks are
tasks that can be performed by simply viewing the
graphic, such as finding the top of a bar in a bar
chart; cognitive tasks are tasks that are performed
via mental computations, such as computing the
difference between two numbers. We draw on
the AutoBrief notion of perceptual and cognitive
tasks enabled by an information graphic. Our plan
operators not only encode knowledge about how
to achieve domain and communicative goals (the
latter of which may require that the viewer per-
form perceptual and cognitive tasks) but they also
encode knowledge about how information-access
tasks, such as finding the value of an entity in
a graphic, can be decomposed into simpler sub-
goals. Figures 2 and 3 present two plan operators
for achieving the goal of finding the value <v> of
an attribute <att> for a graphical element <e>
(for example, the value associated with the top of
a bar in a bar chart). The body of the operator in
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Dependent-variable(<att>, <ds>)
Body: 1. Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Figure 2: Operator for achieving a goal perceptually
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Natural-quantitative-ordering(<att>)
Display-const: Ordered-values-on-axis(<g>, <axis>, <att>)
Body: 1. Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
2. Interpolate(<viewer>, <l1>, <l2>, <f>, <v>)
Figure 3: Operator that employs both perceptual and cognitive subgoals
Figure 2 specifies that the goal can be achieved
by a primitive perceptual task in which the viewer
just perceives the value; this could be done, for
example, if the element in the graphic is annotated
with its value, as are the bars in the bar chart in
Figure 8 of the Appendix. On the other hand, the
body of the operator in Figure 3 captures a differ-
ent way of finding the value, one that presumably
requires more effort. It specifies the perceptual
task of finding the values <l1> and <l2> sur-
rounding the desired value on the axis along with
the fraction <f> of the distance that the desired
value lies between <l1> and <l2>, followed by
the cognitive task of interpolating between the re-
trieved values <l1> and <l2>.
Our operators contain data requirements (la-
belled Data-req) which the data must satisfy in
order for the operator to be applicable in a graphic
planning paradigm; they may also contain display
constraints (labelled Display-const) which con-
strain how the information graphic is constructed
if this operator is part of a final plan. In the case
of plan recognition, these constraints are used
in reverse. The display constraints are used to
eliminate operators from consideration, since if
a graphic does not satisfy the operator?s display
constraints, then the operator could not be part of
a plan that led to the graphic. If a graphic meets
the display constraints of an operator, then the
data requirements are used to limit how the op-
erator?s parameters might be instantiated.
4.4.1 Beginning the Plan Inference Process
Traditional plan inference systems used for
language understanding start with the primitive
goal achieved by the speech act in the dialogue
or discourse. In the case of information graphics,
the role of the speech act is played by the primi-
tive perceptual tasks that the viewer performs on
the graphic. To limit the set of perceptual tasks
that are considered, we make two observations:
? The graphic designer has many alternative
ways of designing a graphic, and the de-
sign choices facilitate some perceptual tasks
more than others. Following the Auto-
Brief work(Kerpedjiev and Roth, 2000) on
generating graphics that fulfill communica-
tive goals, we hypothesize that the designer
chooses a design that best facilitates the
tasks that are most important to conveying
his intended message, subject to the con-
straints imposed by competing tasks.
? Entities may become particularly salient by
virtue of highlighting in the graphic (for ex-
ample, coloring certain elements different
from the others, annotating an element with
an asterisk, or exploding one piece of a pie
chart1), by their mention in the caption or
surrounding text, or via world knowledge
1(Mittal, 1997) discusses a variety of such design tech-
niques in the context of distorting the message inferred from
a graphic.
capturing mutual beliefs about entities of in-
terest to the intended audience. We hypoth-
esize that the designer relies on the viewer
recognizing particularly salient entities, in
order to make certain perceptual tasks more
salient to the viewer.
As noted in Section 4.1, one cannot rely on a
graphic?s caption to provide the intended mes-
sage of the graphic. Consequently, the plan in-
ference process starts with both the set of tasks
that are best enabled by the information graphic
and the set of tasks (if any) that are particularly
salient. These will be referred to as candidate
tasks. The next two subsections describe how
candidate tasks are identified.
Identifying the Best Enabled Tasks The
APTE (Analysis of Perceptual Task Effort) sub-
module, shown in Figure 1 as part of the Inten-
tion Recognition Component, captures perceptual
knowledge about performing primitive perceptual
tasks2, and it encapsulates the results of cognitive
psychology research to estimate the relative effort
required for different tasks. The output of APTE
is the set of perceptual tasks that are best enabled
by the graphic. These become candidate tasks.
Each APTE rule captures a primitive percep-
tual task that can be performed on a particu-
lar type of information graphic, the conditions
(graphic design choices) that affect the difficulty
of performing that task, and the estimated effort
expended by a viewer if those conditions are sat-
isfied in the graphic. The condition-computation
pairs are ordered so that the ones producing the
lowest effort estimates appear first in a rule.
To derive the effort estimates in the rules, we
have followed the GOMS approach(Card et al,
1983) by breaking down the tasks that are re-
garded as primitive in our plan operators into
even more basic component tasks, and then sum-
ming the effort estimates for these very basic
tasks. Lohse?s work(Lohse, 1993) is an exam-
ple of the GOMS architecture applied to predict-
ing performance on graph comprehension tasks,
and many of our effort estimates are based on
Lohse?s research. For example, Figure 4 dis-
2Primitive perceptual tasks are those that we do not de-
compose into a set of simpler subtasks; this is not to be con-
fused with the notion of a psychological primitive.
plays the APTE rule for the task of finding the
value associated with the top of a bar in a bar
chart. If the bar is annotated with its value,
then condition-computation pair B1-1 estimates
its effort as 150 units for discriminating the label
(based on work by Lohse(Lohse, 1993)) and 300
units for recognizing a 6-letter word (John and
Newell, 1990). If the bar is not annotated with its
value but is aligned with a tick mark on the axis,
then condition-computation pair B1-2 estimates
the perceptual effort in terms of the distance to the
dependent axis (in order to capture the degrees of
visual arc scanned(Kosslyn, 1989)) plus the effort
of discriminating and recognizing the label. Fig-
ure 5 displays the APTE rule associated with the
first subgoal in Figure 3. It estimates the effort for
the primitive task Perceive-info-to-interpolate as
the effort of the scan to the dependent axis (based
on (Kosslyn, 1989)), the effort of discriminating
the intersection location on the axis (150 units
based on (Lohse, 1993)), plus the effort of the sac-
cade to each label (230 units each (Russo, 1978))
along with the effort involved in discriminating
and recognizing the labels. Similarly, there is a
cognitive rule (not discussed here) for estimating
the effort associated with the cognitive task Inter-
polate (the second subgoal in the operator in Fig-
ure 3). (Elzer et al, 2003a) presents a more ex-
tensive discussion of the cognitive principles un-
derlying the APTE rules.
Given the XML representation of an informa-
tion graphic, each APTE rule that is applicable
to the graphic produces an effort estimate for the
task captured by the rule. When a task might be
instantiated in multiple ways and still satisfy the
conditions of a condition-computation pair (for
example, the task of finding the value of the top
of a bar could be instantiated for each bar in a
bar chart), only the instantiation that produces the
lowest effort estimate becomes a candidate task.
(If the bars are not annotated with values, then the
instantiation that will produce the lowest effort es-
timate for the task of finding the value of the top
of a bar in a bar chart would be the bar with the
shortest scan to the dependent axis.) This is con-
sistent with the idea that the graphic designer will
make the important tasks easy to perform. The
set of perceptual tasks that require the least effort
become candidate tasks.
Rule-1:Estimate effort for task Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Graphic-type: bar-chart
Gloss: Compute effort for finding the exact value <v> for attribute <att> represented by top <e>
of a bar <b> in graph <g>
B1-1: IF the top of bar <b> is annotated with a value,
THEN effort=150 + 300
B1-2: IF the top <e> of bar <b> aligns with a labelled tick mark on the dependent axis,
THEN effort=scan + 150 + 300
Figure 4: A rule for estimating effort for the primitive perceptual task Perceive-value
Rule-2:Estimate effort for task
Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
Graphic-type: bar-chart
Gloss: Compute effort for finding the information needed for interpolation, including the labels
<l1> and <l2> on either side of entity <e> on axis <axis> in graph <g>,
and the fraction <f> that is the distance between <l1> and entity <e> on <axis>
relative to the distance between <l1> and <l2>
B2-1: IF <axis> is labelled with values THEN effort=scan + 150 + ((230 + 150 + 300) x 2)
Figure 5: A rule for estimating effort for the primitive perceptual task Perceive-info-to-interpolate
Identifying Particularly Salient Tasks
Salient tasks are those that the viewer might
perform because they relate to entities that are
in the viewer?s current focus of attention, as
determined by contextual knowledge provided
by the caption, highlighting, and the surrounding
text and by world knowledge in the form of
mutual beliefs about items of particular interest
to the viewing audience.
Ideally, a caption will provide clues about the
message that an information graphic is intended
to convey, and thus noun phrases in captions rep-
resent salient entities.3 The graphic designer can
also call into focus certain aspects of the graphic
by using attention-getting devices such as col-
oring it differently from the rest of the graphic,
annotating it with an arrow, etc. Our working
hypothesis is that if the graphic designer goes
to the effort of employing such attention-getting
devices, then the highlighted items almost cer-
tainly contribute to the intended message. Thus
the attributes of these highlighted items (for ex-
ample, the attributes of a highlighted bar in a bar
chart), which are captured in the XML represen-
3Verb phrases in captions also provide evidence, but they
suggest particular operators of interest rather than instanti-
ated perceptual tasks, and thus we associate verbs with oper-
ators in the plan library.
tation of the graphic, are also regarded as salient
entities. Salient entities also include those that
world knowledge suggests are mutually believed
to be of interest to the viewing audience. We en-
vision in the future using the notion of lexical
chains(Silber and McCoy, 2000) to identify enti-
ties that the accompanying text makes particularly
salient. Perceptual tasks that are instantiated with
a salient entity and that can be performed on the
graphic are designated salient tasks.
4.4.2 The Search Process
Candidate tasks consist of the set of percep-
tual tasks that require the least effort and the set of
salient tasks. Once the set of candidate tasks has
been identified, plan inference begins. Initial can-
didate plans are constructed from each operator
in which a candidate task appears as a subgoal;
the root of the candidate plan is the goal of the
operator, and its children are the subgoals in the
body of the operator. Chaining from the root goal
to other operators whose body contains the root
goal as a subgoal produces larger candidate plans
with higher-level goals as the new root goal.
Plan inference systems have used a variety of
heuristics to evaluate candidate plans and to se-
lect the candidate plan to expand further. These
heuristics help to guide the search through the
space of candidate plans in order to hypothe-
size the plan that best represents the user?s in-
tentions. These heuristics have included increas-
ing the rating of partial plans as their arguments
become instantiated(Perrault and Allen, 1980),
preferring coherent discourse moves(Litman and
Allen, 1987; Carberry, 1990), and biasing the
plan inference process based on knowledge about
the user group(Gertner and Webber, 1996). We
have identified several kinds of evidence for guid-
ing plan inference from information graphics, in-
cluding the estimated effort required by a candi-
date plan, the basis for instantiating parameters in
the plan, adherence to the proximity compatibil-
ity principle from cognitive science research, and
the relation between a candidate plan and the es-
tablished discourse context.
Since our working hypothesis is that the
graphic designer tried to enable those tasks neces-
sary to recognize his intended message, candidate
plans that require substantially more effort than
other candidate plans are less likely to represent
the intentions of the designer. The effort associ-
ated with a candidate plan is measured as the sum
of the effort of the tasks comprising it.
There are many ways that a parameter in a task
or subgoal might become instantiated, and the ba-
sis for the instantiation provides evidence about
the likelihood that a hypothesized candidate plan
represents the graphic designer?s intentions. If
an instantiation is suggested by highlighting or
a caption or entities that are particularly salient
to the targeted audience, that partial plan should
be evaluated more favorably since the designer of
the graphic has provided reasons for the viewer to
use these instantiations in recognizing his inten-
tions. Similarly, if the instantiation is one of sev-
eral possible alternatives with no reason for pre-
ferring one over the other, then the partial plan
should be evaluated less favorably since the de-
signer did not give the viewer any reason to prefer
one over the other. This relates to Allen?s forking
heuristic(Perrault and Allen, 1980). The proxim-
ity compatibility principle(Wickens and Carswell,
1995) also suggests that candidate plans which
use similarly encoded elements (for example, all
red bars) in an integrated fashion should be eval-
uated more favorably than those that do not.
If there is a context established by the text pre-
ceding or surrounding the graphic, then candidate
plans whose root goal contributes to the exist-
ing discourse context should be preferred. If the
surrounding text has a reference to the graphic,
then focusing heuristics(Carberry, 1990) will pre-
fer candidate plans that relate most closely to the
current focus of attention at that point in the sur-
rounding text. However, the surrounding text of-
ten does not refer to accompanying graphics, as is
the case in the Newsweek article whose excerpt is
shown in Figure 6. Future work will investigate
how we should handle instances such as this.
5 Response Generation and Followup
The intended message of the graphic must be
augmented with additional propositions that con-
vey interesting features that a viewer would glean
from the graphic. For example, the intended mes-
sage of the graphic in Figure 6 appears to be that
the income of black women has risen dramati-
cally over the last decade and reached the level
of white women. But other interesting features of
the graphic might include the trends over the past
several decades, periods where they were closest,
etc. In future work, we anticipate developing a
methodology for identifying propositions that ex-
pand on the message of the graphic designer and
for including the most salient of these in the sum-
marization of the graphic. We also envision re-
sponding to followup requests for further infor-
mation about the graphic by selecting the highest
ranking propositions that were not included in the
initial message, organizing them into a coherent
response, and conveying it to the user.
6 Summary
This paper has argued that understanding
information graphics is a discourse-level prob-
lem. Not only must the system recognize the in-
tended message of the information graphic, but
the recognition process requires similar kinds of
knowledge sources and similar kinds of process-
ing as does the understanding of traditional dis-
course and dialogue. Moreover, when an article
is composed of text and graphics, the intended
message of the information graphic must be in-
tegrated into the discourse structure of the sur-
rounding text, and it contributes to the overall dis-
course intention of the article.
References
L. Ardisonno and D. Sestero. 1996. Using dynamic
user models in the recognition of the plans of the
user. User Modeling and User-Adapted Interac-
tion, 5(2):157?190.
S. Carberry and L. Lambert. 1999. A process model
for recognizing communicative acts and modeling
negotiation subdialogues. Computational Linguis-
tics, 25(1):1?53.
S. Carberry. 1990. Plan Recognition in Natural Lan-
guage Dialogue. ACL-MIT Press Series on Natural
Language Processing. MIT Press, Cambridge, MA.
S. Card, T. Moran, and A. Newell. 1983. The Psychol-
ogy of Human-Computer Interaction. Lawrence
Erlbaum Associates, Inc., Hillsdale, NJ.
E. Charniak and R. Goldman. 1993. A bayesian
model of plan recognition. Artificial Intelligence,
64:53?79.
H. Clark. 1996. Using Language. Cambridge Univer-
sity Press.
M. Corio and G. Lapalme. 1999. Generation of texts
for information graphics. In Proceedings of the 7th
European Workshop on Natural Language Genera-
tion EWNLG?99, pages 49?58.
S. Elzer, N. Green, and S. Carberry. 2003a. Exploit-
ing cognitive psychology research for recognizing
intention in information graphics. In Proceedings
of the 25th Annual Meeting of the Cognitive Science
Society. To appear.
S. Elzer, N. Green, S. Carberry, and K. McCoy. 2003.
Extending plan inference techniques to recognize
intentions in information graphics. In Proceedings
of the Ninth International Conference on User Mod-
eling. To appear.
A. Gertner and B. Webber. 1996. A Bias Towards
Relevance: Recognizing Plans Where Goal Mini-
mization Fails. In Proc. of the Thirteenth National
Conference on Artificial Intelligence, pages 1133?
1138.
H. P. Grice. 1969. Utterer?s Meaning and Intentions.
Philosophical Review, 68:147?177.
B. Grosz and C. Sidner. 1986. Attention, Intentions,
and the Structure of Discourse. Computational Lin-
guistics, 12(3):175?204.
B. John and A. Newell. 1990. Toward an engi-
neering model of stimulus response compatibility.
In R. Gilmore and T. Reeve, editors, Stimulus-
response compatibility: An integrated approach,
pages 107?115. North-Holland, New York.
A. Kennel. 1996. Audiograf: A diagram-reader for
the blind. In Second Annual ACM Conference on
Assistive Technologies, pages 51?56.
S. Kerpedjiev and S. Roth. 2000. Mapping com-
municative goals into conceptual tasks to generate
graphics in discourse. In Proc. of the International
Conference on Intelligent User Interfaces, pages
60?67.
S. Kosslyn. 1989. Understanding charts and graphs.
Applied Cognitive Psychology, 3:185?226.
D. Litman and J. Allen. 1987. A Plan Recognition
Model for Subdialogues in Conversation. Cognitive
Science, 11:163?200.
G. Lohse. 1993. A cognitive model for understand-
ing graphical perception. Human-Computer Inter-
action, 8:353?388.
Peter B. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions
on Biomedical Engineering, 39(2):291?300, Febru-
ary.
V. Mittal. 1997. Visual prompts and graphical design:
A framework for exploring the design space of 2-D
charts and graphs. In Proc. of the Fourteenth Na-
tional Conference on Artificial Intelligence, pages
57?63.
R. Perrault and J. Allen. 1980. A Plan-Based Anal-
ysis of Indirect Speech Acts. American Journal of
Computational Linguistics, 6(3-4):167?182.
J. Russo. 1978. Adaptation of cognitive processes to
eye movement systems. In J. Senders, D. Fisher,
and R. Monty, editors, Eye movements and higher
psychological functions. Lawrence Erlbaum, Hills-
dale, NJ.
J. Searle. 1970. Speech Acts: An Essay in the Phi-
losophy of Language. Cambridge University Press,
London.
G. Silber and K. McCoy. 2000. Efficient text summa-
rization using lexical chains. In Proc. of the Inter-
national Conference on Intelligent User Interfaces,
pages 252?255.
C. Wickens and M. Carswell. 1995. The proximity
compatibility principle: Its psychological founda-
tion and relevance to display design. Human Fac-
tors, 37(3):473?494.
R. Wilensky. 1983. Planning and Understanding.
Addison-Wesley.
J. Yu, J. Hunter, E. Reiter, and S. Sripada. 2002.
Recognising visual patterns to communicate gas
turbine time-series data. In ES2002, pages 105?
118.
Appendix of Graphics from our Corpus
Graphic from Newsweek Article
60 70 80 90 01
$15
10
5
Black women
White women
Median Income
In thousands of 2001 dollars
1948
Relevant Text from Newsweek Article
This is not to say that black women have
climbed the storied crystal stair. They remain
?in the proving stage?, observes Alabama ex-
ecutive Alice Gordon. Nearly 14 percent of
working black women remain below the poverty
level. And women don?t yet out-earn black men.
But the growing educational-achievement gap
portends a monumental shifting of the sands.
College-educated black women already earn
more than the median for all black working men
? or, for that matter, for all women. And as
women in general move up the corporate pyra-
mid, black women, increasingly, are part of the
parade. In 1995 women held less than 9 per-
cent of corporate-officer positions in Fortune
500 companies, according to Catalyst, a New
York-based organization that promotes the inter-
ests of women in business. Last year they held
close to 16 percent, a significant step up. Of
those 2,140 women, 163 were black ? a minus-
cule proportion, but one that is certain to grow.
Figure 6: Excerpt from Newsweek Magazine
How reliable adults think DNA tests are for identifying an individual:
Trusting DNA
Don?t know
Very unreliable
unreliable
Somewhat 
reliable
Somewhat
Very reliable
2%
3%
4%
68%
23%
Figure 7: Standalone Graphic from USA Today
Europe
Canada
African
Other
countries
South
United
States
Africa
21
metric tons
Leading producers in
in gold production
South Africa tops
428
355
187 155
Gold Production
M
et
ric
 T
on
s
Figure 8: Standalone Graphic from USA Today
010,000
Median Salaries (in dollars), Full?Time Employed SMET Doctorates, by Field and Gender, 1997
Computer/All SMET Physical Sciences
Sal
ari
es 
(in 
dol
lars
)
SciencesMathematical
Engineering Life Sciences Social Sciences
80,000
70,000
60,000
50,000
40,000
30,000
20,000
Male
Female
Figure 9: Graphic from Report of the NSF Committee on Equal Opportunities in Science & Engineering
personal filings
Delaware bankruptcy
3000
2500
1000
1500
2000
1998 1999 2000 2001
Figure 10: Graphic from Wilmington News
Journal
0.6%
?96 ?97 ?98 ?99 ?00 ?01
1.0%
2.0%
3.0%
0.8%
0
?96 ?97 ?98 ?99 ?00 ?01
All taxpayers
Affluent taxpayers
1.2%
continue to slide
Audits of affluent
were audited by the IRS:
Percentage of taxpayers who
0
0.6%
1.8%
Figure 11: Graphic from USA Today
Extending Document Summarization to Information Graphics
?Sandra Carberry, ??Stephanie Elzer, ? ? ?Nancy Green, ?Kathleen McCoy and ?Daniel Chester
?Dept. of Computer Science, University of Delaware, Newark, DE 19716
(carberry, mccoy, chester@cis.udel.edu)
??Dept. of Computer Science, Millersville Univ., Millersville, PA 17551
(elzer@cs.millersville.edu)
? ? ?Dept. of Math. Sciences, Univ. of North Carolina at Greensboro, Greensboro, NC 27402
(nlgreen@uncg.edu)
Abstract
Information graphics (non-pictorial graphics such
as bar charts or line graphs) are an important
component of multimedia documents. Often such
graphics convey information that is not contained
elsewhere in the document. Thus document summa-
rization must be extended to include summarization
of information graphics. This paper addresses our
work on graphic summarization. It argues that the
message that the graphic designer intended to con-
vey must play a major role in determining the con-
tent of the summary, and it outlines our approach
to identifying this intended message and using it to
construct the summary.
1 Introduction
Summarization work has focused primarily on the
written words in a document. However, graphics
are an important part of many documents, and they
often convey information that is not included else-
where in the document. Thus as text summarization
branches out, it is essential that it consider the sum-
marization of graphical information in documents.
Graph summarization has received some atten-
tion. (Yu et al, 2002) has used pattern recogni-
tion techniques to summarize interesting features of
automatically generated graphs of time-series data
from a gas turbine engine. (Futrelle and Nikolakis,
1995) developed a constraint grammar formalism
for parsing vector-based visual displays and produc-
ing structured representations of the elements com-
prising the display. The goal of Futrelle?s project
is to produce a graphic that summarizes one or
more graphics from a document (Futrelle, 1999).
The summary graphic might be a simplification of
a graphic or a merger of several graphics from the
document, along with an appropriate summary cap-
tion. Thus the end result of summarization will it-
self be a graphic.
Our project is concerned with information graph-
ics (non-pictorial graphics such as bar charts or line
graphs). Our current focus is on providing an ini-
tial summary of an information graphic, within a
larger interactive natural language system that can
respond to followup questions about the graphic.
There are several useful applications for a system
that can summarize information graphics. For dig-
ital libraries, the initial summary of the graphic
will be used in conjunction with the document
text/summary to provide a more complete represen-
tation of the content of the document to be used
for searching and indexing. In the case of environ-
ments with low-bandwidth transmission and minia-
ture viewing facilities, such as cellular telephones
for accessing the web, the initial summary and fol-
lowup capability will provide an alternative modal-
ity for access to the document.
However, the most compelling application of the
overall system is to provide effective access to in-
formation graphics for individuals with sight im-
pairments. The rapidly growing Information Infras-
tructure has had a major impact on society and the
development of technology. However, the growing
reliance on visual information display paradigms
obliges society to ensure that individuals with visual
impairments can access and assimilate information
resources as effectively as their sighted counter-
parts. The underlying hypothesis of our work is that
alternative access to what the graphic looks like is
not enough ? the user should be provided with the
message and knowledge that one would gain from
viewing the graphic in order to enable effective and
efficient use of this information resource. Thus our
system will present the user with an initial summary
that includes the primary message that the graphic
designer intended to convey, augmented with rel-
evant interesting features of the graphic, and then
interactively allow the user to access more detailed
summaries of information contained in the graphic.
As an example of the kinds of summaries that we
envision, consider the information graphic in Fig-
ure 1. The graphic designer?s communicative goal is
ostensibly to convey the sharp increase in bankrupt-
cies in 2001 compared with the previous decreasing
trend. More detailed features that might be of inter-
est include 1) that bankruptcies had been decreasing
at a steady rate since 1998, 2) that bankruptcies had
been decreasing slowly since 1998, 3) the percent-
age decrease each year, 4) the percentage increase
in bankruptcies in 2001, 5) the absolute increase in
bankruptcies in 2001, and 6) the total number of
bankruptcies in 2001. Thus the initial summary of
this graphic might be
This graphic shows that although
Delaware bankruptcy personal filings
decreased slowly and steadily from 1998
to 2000, they rose sharply in 2001.
Note that the proposed summary includes the hy-
pothesized intended message of the graphic, along
with the first two of the additional interesting fea-
tures of the graphic. The selection of additional fea-
tures to augment the summary is discussed further
in Section 3.3. The system would then respond to
user requests for additional information by present-
ing some or all of the other interesting features that
had been identified, as discussed in Section 3.4.
This paper provides an overview of our project.
Section 2 discusses the essential role of intention
recognition in graphics summarization. It argues
not only that the intended message of the graphic
designer must be inferred and included in a sum-
mary of a graphic, but also that the intended mes-
sage significantly influences the additional propo-
sitions that should be included in the summary.
Section 3 presents our approach to graph summa-
rization. It discusses how we use a computer vi-
sion module to construct an XML representation
that captures the components of the graphic and
their relationship to one another, and how we use
a Bayesian belief network to hypothesize the inten-
tions of the graph designer. The paper then dis-
cusses our plans for constructing a summary that
includes the graphic designer?s intended message
along with highly ranked additional propositions,
and how the lesser ranked propositions will be used
in an interactive natural language system that re-
sponds to the user?s requests for further summaries
of additional features of the graphic.
2 The Role of Intention in Graphics
Summarization
Text summarization has generally relied on statis-
tical techniques and identification and extraction
of key sentences from documents. However, it is
widely acknowledged that to truly understand a text
and produce the best summary, one must under-
stand the document and recognize the intentions of
the author. Recent work in text summarization has
personal filings
Delaware bankruptcy
3000
2500
1000
1500
2000
1998 1999 2000 2001
Figure 1: Graphic from a City Newspaper
60 70 80 90 01
$15
10
5
Black women
White women
Median Income
In thousands of 2001 dollars
1948
Figure 2: Graphic from Newsweek Magazine
begun to address this issue. For example, (Marcu,
2000) presents algorithms for automatically identi-
fying the rhetorical structure of a text and argues
that the hypothesized rhetorical structure can be
successfully used in text summarization.
Information graphics are an important component
of many documents. In some cases, information
graphics are stand-alone and constitute the entire
document. This is the case for many graphics ap-
pearing in newspapers, such as the graphic shown
in Figure 1. On the other hand, when an article is
comprised of text and graphics, the graphic gener-
ally expands on the text and contributes to the dis-
course purpose (Grosz and Sidner, 1986) of the arti-
cle. For example, Figure 2 illustrates a graphic from
Newsweek showing that the income of black women
has risen dramatically over the last decade and has
reached the level of white women. Although this in-
formation is not conveyed elsewhere in the article, it
contributes to the overall communicative intention
of this portion of the article ? namely, that there
has been a ?monumental shifting of the sands? with
regard to the achievements of black women.
Our project is concerned with the understand-
ing and summarization of information graphics: bar
charts, line graphs, pie charts, etc. We contend that
analyzing the data points underlying an informa-
tion graphic is insufficient. One must instead iden-
tify the message that the graphic designer intended
to convey via the design choices that were made
in constructing the graphic. (Although one might
suggest relying on captions to provide the intended
message of a graphic, Corio and Lapalme found
in a large corpus study (Corio and Lapalme, 1999)
that captions are often missing or are very general
and uninformative; our collected corpus of informa-
tion graphics supports their observations.) Design
choices include selection of chart type (bar chart,
pie chart, line graph, etc.), organization of informa-
tion in the chart (for example, aggregation of bars in
a bar chart), and attention-getting devices that high-
light certain aspects of a chart (such as coloring one
bar of a bar chart different from the others). Not
only should the graphic designer?s intended mes-
sage comprise the primary component of any sum-
mary, but this intended message has a strong influ-
ence on the salience of additional propositions that
might be included in the summary.
To see the importance of recognizing the graphic
designer?s intended message, consider the two
graphics in Figure 3. The one on the left, Fig-
ure 3a, appeared in an NSF publication. Both graph-
ics were constructed from the same data set. The
intended message of the graphic in Figure 3a is that
the salary of females is consistently less than that of
males for each of the science and engineering dis-
ciplines.1 Notice that the graphic designer selected
an organization for the graphic in Figure 3a that fa-
cilitated the comparison between male and female
salaries in each field. A different display of the
same data would facilitate different analyses. For
example, the graph in Figure 3b depicts the same
data as the graph in Figure 3a, yet the organiza-
tion tends to draw attention to comparisons within
male and female groups rather than between them,
1This graphic was constructed by a colleague who served
on the NSF panel that prepared the report. Thus we know the
intentions underlying the graphic.
and perhaps an integration/comparison of the mes-
sages conveyed by the two subgraphs. Thus the in-
tended message of the graphic in Figure 3b appears
to be that the ranking of the disciplines by salary are
about the same for both men and women. The dis-
tinctions between presentation formats illustrate the
extent to which the format can itself convey infor-
mation relevant to the graphic designer?s intended
message.
Now let us consider how the intended message
influences additional information that might be in-
cluded in a summary. Suppose that 1) the salary
differential between females and males was signif-
icantly larger in the life sciences than in other dis-
ciplines and 2) the average salary for both females
and males was much larger in engineering than in
any of the other disciplines. Feature 1) would be
particularly interesting and relevant to the intended
message of Figure 3a, and thus should be included
as part of the graphic?s summary. On the other hand,
this aspect would be less relevant to the intended
message of Figure 3b and thus not as important to
include. Similarly, Feature 2) would be particularly
relevant to the intended message of Figure 3b and
thus should be given high priority for inclusion in
its summary. Although an interactive system that
could analyze a graphic to any desired level of de-
tail might extract from the graphic the information
in both 1) and 2) above, we contend that a summary
of the graphic should prioritize content according to
its relevance to the designer?s intended message.
3 Graphic Summarization
Our architecture for graphic summarization consists
of modules for identifying the components of the
graphic, hypothesizing the graphic designer?s in-
tended message, planning the content of the sum-
mary, organizing a coherent summary, and interac-
tive followup. The following sections discuss four
of these modules.
3.1 Analyzing and Classifying a Graphic
The visual extraction module takes a screen image
of an information graphic. It is responsible for rec-
ognizing the individual components comprising the
graphic, identifying the relationship of the different
components to one another and to the graphic as a
whole, and classifying the graphic as to type. This
includes using heuristics (such as relative position
of a string of characters) to identify the axis labels
? for example, that the y-axis label is Delaware
2The source of the leftmost graph is the National Science
Foundation, Survey of Doctorate Recipients, 1997.
  
 
 
 
 
 
 

















 
 
 
 
 
 
 
 
 




































		
		
		
		
		
		
		
		
		





































80,000
70,000
60,000
50,000 50,000
60,000
70,000
80,000
40,000
30,000
20,000 20,000
30,000
40,000
FEMALE SALARIES MALE SALARIES
Computer/All
Math Sci
Engin. Phys.
Sci. Sci.
Social
Sci.
Life Sci.
Social Sci.
A
ll
Com
puter/M
ath Sci.
Phys Sci.
Engineering
Social Sci.
Life Sci.
Com
puter/M
ath Sci.
A
ll
Phys Sci.
Engineering
Life
Female
Male
(a) (b)
Figure 3: Two alternative graphs from the same data2
bankruptcy personal filings in Figure 1. Our cur-
rent implementation deals only with gray scale im-
ages (in pgm format) of bar charts, pie charts, and
line graphs, though eventually it will be extended to
handle color and other kinds of information graph-
ics. The output of the visual extraction component
is an XML file that describes the chart and all of its
components.
3.2 Identifying the Intended Message
The second module of our architecture is respon-
sible for inferring the graphic designer?s intended
message. In their work on multimedia generation,
the AutoBrief group proposed that speech act the-
ory can be extended to the generation of graphical
presentations (Kerpedjiev and Roth, 2000; Green et
al., 2004). They contended that the graphic design
was intended to convey its message by facilitating
requisite perceptual and cognitive tasks. By percep-
tual tasks we mean tasks that can be performed by
simply viewing the graphic, such as finding the top
of a bar in a bar chart; by cognitive tasks we mean
tasks that are done via mental computations, such as
computing the difference between two numbers.
The goal of our intention recognizer is the inverse
of the design process: namely, to use the displayed
graphic as evidence to hypothesize the communica-
tive intentions of its author. This is done by an-
alyzing the graphic to identify evidence about the
designer?s intended message and then using plan
recognition (Carberry, 1990) to hypothesize the au-
thor?s communicative intent.
3.2.1 Evidence about Intention
Following AutoBrief (Kerpedjiev and Roth, 2000),
we hypothesize that the graphic designer chooses
a design that makes important tasks (the ones that
the viewer is intended to perform in recognizing the
graphic?s message) as salient or as easy as possi-
ble. Thus salience and ease of performance should
be taken into account in reasoning about the graphic
designer?s intentions.
There are several ways that a task can be made
salient. The graphic designer can draw attention
to a component of a graphic (make it salient) by
an attention-getting or highlighting device, such as
by coloring a bar in a bar chart differently from
the other bars as in Figure 1 or by exploding a
wedge in a pie chart (Mittal, 1997). Attributes of
the highlighted graphic component are treated as
focused entities. Nouns in captions also serve to
establish focused entities. For example, a caption
such as ?Studying not top priority? would estab-
lish the noun studying as a focused entity. Focused
entities that appear as instantiations of parameters
in perceptual or cognitive tasks serve as evidence
that those tasks might be particularly salient. Sim-
ilarly, verbs that appear in captions serve as evi-
dence for the salience of particular tasks. For ex-
ample, the verb beats in a caption such as ?Canada
Beats Europe? serves as evidence for the salience
of a Recognize relative difference task. In the fu-
ture, we plan to capture the influence of surrounding
text by identifying the important concepts from the
text using lexical chains. Lexical chains have been
used in text summarization (Barzilay et al, 1999),
and our linear time algorithm (Silber and McCoy,
2002) makes their computation feasible even for
large texts. Whether a task is salient and the method
by which it was made salient are used as evidence
in our plan inference system.
The graphic design makes some tasks easier than
others. We use a set of rules, based on research by
cognitive psychologists, to estimate the relative ef-
fort of performing different perceptual and cogni-
tive tasks. These rules, described in (Elzer et al,
2004), have been validated by eye-tracking experi-
ments. Since the viewer is intended to recognize the
message that the graphic designer wants to convey,
we contend that the designer will choose a graphic
design that makes the requisite tasks easy to per-
form. This was illustrated in the two graphics in
Figure 3. The relative effort of performing a task is
thus used as another source of evidence in our plan
inference framework.
3.2.2 The Plan Inference Process
Our plan inference framework takes the form of
a Bayesian belief network. Bayesian belief net-
works have been applied to a variety of problems,
including reasoning about utterances (Charniak and
Goldman, 1993) and observed actions (Albrecht et
al., 1997). The belief network uses plan operators,
along with evidence that is gleaned from the infor-
mation graphic itself (as discussed in the preceding
section), to reason about the likelihood that vari-
ous hypothesized candidate plans represent the in-
tentions of the graphic designer.
Plan Operators for Information Graphics Our
system uses plan operators that capture knowledge
about how the graphic designer?s goal of conveying
a message can be achieved via the viewer perform-
ing certain perceptual and cognitive tasks, as well
as knowledge about how information-access tasks,
such as finding the value of an entity in a graphic,
can be decomposed into simpler subgoals. Our plan
operators consist of:
? Goal: the goal that the operator achieves
? Data-requirements: requirements that the data
must satisfy in order for the operator to be ap-
plicable in a graphic planning paradigm
? Display-constraints: features that constrain
how the graphic is eventually constructed if
this operator is part of the final plan
? Body: lower-level subgoals that must be ac-
complished in order to achieve the overall goal
of the operator.
Figures 4 and 5 present two plan operators for the
goal of finding the value <v> of an attribute <att>
for a graphical element <e> (for example, the value
associated with the top of a bar in a bar chart). The
body of the operator in Figure 4 specifies that the
goal can be achieved by a primitive perceptual task
in which the viewer just perceives the value; this
could be done, for example, if the element in the
graphic is annotated with its value. On the other
hand, the body of the operator in Figure 5 captures a
different way of finding the value, one that presum-
ably requires more effort. It specifies the perceptual
task of finding the values <l1> and <l2> surround-
ing the desired value on the axis along with the frac-
tion <f> of the distance that the desired value lies
between <l1> and <l2>, followed by the cogni-
tive task of interpolating between the retrieved val-
ues <l1> and <l2>.
Plan inference uses the plan operators to reasons
backwards from the XML representation of the ob-
served graphic (constructed by the visual extraction
module briefly described in Section 3.1). The dis-
play constraints are used to eliminate operators from
consideration ? if the graphic does not capture the
operator?s constraints on the display, then the opera-
tor could not have been part of a plan that produced
the graphic. The data requirements are used to in-
stantiate parameters in the operator ? the data must
have had certain characteristics for the operator to
have been included in the graphic designer?s plan,
and these often limit how the operator?s arguments
can be instantiated.
The Bayesian Belief Network The plan operators
are used to dynamically construct a Bayesian net-
work for each new information graphic. The net-
work includes the possible top level communicative
intentions (with uninstantiated parameters), such as
the intention to convey a trend, and the alternative
ways of achieving them via different plan opera-
tors. The perceptual tasks of lowest effort and the
tasks that are hypothesized as potentially salient are
added to the network. Other tasks are entered into
the network as they are inferred during chaining on
the plan operators; unification serves to instantiate
parameters in higher-level nodes. Evidence nodes
are added for each of the tasks entered into the net-
work, and they provide evidence (such as the degree
of perceptual effort required for a task or whether
a parameter of the task is a focused entity in the
graphic as discussed in Section 3.2.1) for or against
the instantiated tasks to which they are linked. Af-
ter propagation of evidence, the top-level intention
with the highest probability is hypothesized as the
graphic designer?s primary intention for the graphic.
Of course, a Bayesian network requires a set of
conditional probabilities, such as 1) the probability
that perceptual Task-A will be of low, medium, or
high effort given that the graphic designer?s plan in-
cludes the viewer performing Task-A, 2) the prob-
ability that parameter <x> of Task-A will be a fo-
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Dependent-variable(<att>, <ds>)
Body: 1. Perceive-dependent-value(<viewer>, <g>, <att>, <e>, <v>)
Figure 4: Operator for achieving a goal perceptually
Goal: Find-value(<viewer>, <g>, <e>, <ds>, <att>, <v>)
Gloss: Given graphical element <e> in graphic <g>, <viewer> can find the value <v>
in dataset <ds> of attribute <att> for <e>
Data-req: Natural-quantitative-ordering(<att>)
Display-const: Ordered-values-on-axis(<g>, <axis>, <att>)
Body: 1. Perceive-info-to-interpolate(<viewer>,<g>,<axis>,<e>,<l1>,<l2>,<f>)
2. Interpolate(<viewer>, <l1>, <l2>, <f>, <v>)
Figure 5: Operator that employs both perceptual and cognitive subgoals
cused entity in the caption given that the graphic de-
signer?s plan includes the viewer performing Task-
A, or 3) the probability that the viewer perform-
ing Task-B will be part of the designer?s intended
plan given that Task-A is part of his plan. (Note that
there may be several alternative ways of perform-
ing a particular task, as illustrated by the two plan
operators displayed in Figures 4 and 5.) We have
collected a rapidly expanding corpus of information
graphics, and have analyzed a small part of this cor-
pus to construct an initial set of probabilities. The
results suggest that our approach is very promising.
We will increase the number of analyzed graphics
to improve the probability estimates.
3.3 Planning the Content of the Summary
The recognized intention of the graphic designer,
such as to convey an overall increasing trend or to
compare salaries of females and males in different
disciplines as in Figure 3a, will provide one set of
highly salient propositions that should be included
in the graphic?s summary. Once the intentions have
been recognized, other visual features of the graphic
will influence the identification of additional salient
propositions.
We conducted a set of experiments in which sub-
jects were asked to write a brief summary of a set of
line graphs, each of which arguably could be said
to have the same high-level intention. Although
each summary included the high-level intention, the
summaries often differed significantly for different
graphs. By comparing these with summaries of the
same graph by different subjects, we have hypoth-
esized that certain features, such as the variance of
the data, can influence the generated summary, and
that the importance of including a specific feature in
a summary is related to the high-level intention of
the graphic. For example, variation in the data will
be relevant for an intention of conveying a trend,
but it will be less important than the overall slope
of the data points. This impact of the intended mes-
sage on the priority of including a specific feature
in a graphic was illustrated in Section 2, where we
showed how a significantly larger differential be-
tween female and male salaries for one particular
discipline would be more relevant to the summary of
the graphic in Figure 3a than for the graphic in Fig-
ure 3b. In addition, our experiments indicate that the
strength of a feature in the graphic also influences
its inclusion in a summary. For example, the more
ragged a sequence of line segments, the more salient
variance becomes for inclusion in a summary.
Once the content planning module has identified
and ranked interesting features that might augment
the intended message of the graphic, the most im-
portant propositions will be organized into a coher-
ent summary that can be stored for access in a digital
library or presented to a user. In the future, we will
also investigate integrating the summary of an infor-
mation graphic with the summary of its surrounding
text.
3.4 Interactive Followup
One of the primary goals of our work is an inter-
active natural language system that can convey the
content of an information graphic to a user with
sight impairments. For this application, the sum-
mary will be rendered in natural language and con-
veyed as an initial summary to the user via speech
synthesis. The system will then provide the user
with the opportunity to seek additional information.
We will utilize the propositions that were not in-
cluded in the initial message as indicative of ad-
ditional information about the graphic that might
be useful. Several kinds of followup will be pro-
vided. For example, if the user requests focused
followup, the system will categorize the remaining
propositions (for example, extreme values, trend de-
tail, etc.) and ask the user to select one of the cate-
gories of further information. The system will then
construct a followup message summarizing the most
important (often all) of the remaining propositions
in the selected category. This interactive followup
will continue until either all the propositions have
been conveyed or the user terminates the followup
cycle.
4 Summary
This paper extends document summarization to the
summarization of information graphics. It argues
that an effective summary must be based on the
message that the graphic designer intended to con-
vey in constructing the graphic, and that this in-
tended message strongly influences the relevance
of other propositions that might be included in the
summary. The paper describes our approach to
graphic summarization, including our plan infer-
ence system for inferring the intended message un-
derlying a graphic. This work has many applica-
tions. These include enabling information graphics
to be accessed via content in a digital library, allow-
ing access to information graphics via devices with
small bandwidth (such as cellular phones), and most
importantly making information graphics accessible
to individuals with sight impairments via an interac-
tive natural language system that can provide sum-
maries at various levels of detail.
References
David Albrecht, Ingrid Zukerman, Ann Nicholson,
and A. Bud. 1997. Towards a bayesian model
for keyhole plan recognition in large domains.
In Proceedings of the Sixth International Confer-
ence on User Modeling, pages 365?376.
R. Barzilay, K. McKeown, and M. Elhadad. 1999.
Information fusion in the context of multi-
document summarization. In Proc. of the 37th
Annual Meeting of the ACL, pages 550?557.
Sandra Carberry. 1990. Plan Recognition in Natu-
ral Language Dialogue. ACL-MIT Press Series
on Natural Language Processing. MIT Press.
Eugene Charniak and Robert Goldman. 1993. A
bayesian model of plan recognition. Artificial In-
telligence Journal, 64:53?79.
Marc Corio and Guy Lapalme. 1999. Generation of
texts for information graphics. In Proceedings of
the 7th European Workshop on Natural Language
Generation EWNLG?99, pages 49?58.
Stephanie Elzer, Nancy Green, Sandra Carberry,
and James Hoffman. 2004. Incorporating per-
ceptual task effort into the recognition of inten-
tion in information graphics. In Diagrammatic
Representation and Inference: Proceedings of
the Third International Conference on the Theory
and Application of Diagrams, LNAI 2980, pages
255?270.
Robert Futrelle and Nikos Nikolakis. 1995. Ef-
ficient analysis of complex diagrams using
constraint-based parsing. In Proceedings of the
Third International Conference on Document
Analysis and Recognition.
Robert Futrelle. 1999. Summarization of diagrams
in documents. In I. Mani and M. Maybury, edi-
tors, Advances in Automated Text Summarization.
MIT Press.
Nancy Green, Giuseppe Carenini, Stephan Kerped-
jiev, Joe Mattis, Johanna Moore, and Steven
Roth. 2004. Autobrief: An experimental system
for the automatic generation of briefings in inte-
grated text and graphics. International Journal of
Human-Computer Studies. to appear.
Barbara Grosz and Candace Sidner. 1986. Atten-
tion, Intentions, and the Structure of Discourse.
Computational Linguistics, 12(3):175?204.
Stephan Kerpedjiev and Steven Roth. 2000. Map-
ping communicative goals into conceptual tasks
to generate graphics in discourse. In Proceed-
ings of the International Conference on Intelli-
gent User Interfaces, pages 60?67.
Daniel Marcu. 2000. The rhetorical parsing of un-
restricted texts: A surface-based approach. Com-
putational Linguistics, 26(3):395?448.
Vibhu Mittal. 1997. Visual prompts and graphical
design: A framework for exploring the design
space of 2-d charts and graphs. In Proceedings
of the Fourteenth National Conference on Artifi-
cial Intelligence, pages 57?63.
Gregory Silber and Kathleen McCoy. 2002. Effi-
ciently computed lexical chains as an intermedi-
ate representation for automatic text summariza-
tion. Computational Linguistics, 28(4):487?496.
Jin Yu, Jim Hunter, Ehud Reiter, and Somaya-
julu Sripada. 2002. Recognising visual patterns
to communicate gas turbine time-series data. In
ES2002, pages 105?118.
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 101?102,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
UDel: Generating Referring Expressions
Guided by Psycholinguistic Findings
Charles Greenbacker and Kathleen McCoy
Dept. of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[charlieg|mccoy]@cis.udel.edu
Abstract
We present an approach to generating refer-
ring expressions in context utilizing feature se-
lection informed by psycholinguistic research.
Features suggested by studies on pronoun in-
terpretation were used to train a classifier sys-
tem which determined the most appropriate
selection from a list of possible references.
This application demonstrates one way to help
bridge the gap between computational and
empirical means of reference generation.
1 Introduction
This paper provides a system report on our submis-
sion for the GREC-MSR (Main Subject References)
Task, one of the two shared task competitions for
Generation Challenges 2009. The objective is to se-
lect the most appropriate reference to the main sub-
ject entity from a given list of alternatives. The cor-
pus consists of introductory sections from approxi-
mately 2,000 Wikipedia articles in which references
to the main subject have been annotated (Belz and
Varges, 2007). The training set contains articles
from the categories of cities, countries, mountains,
people, and rivers. The overall purpose is to develop
guidelines for natural language generation systems
to determine what forms of referential expressions
are most appropriate in a particular context.
2 Method
The first step of our approach was to perform a lit-
erature survey of psycholinguistic research related
to the production of referring expressions by human
beings. Our intuition was that findings in this field
could be used to develop a useful set of features
with which to train a classifier system to perform the
GREC-MSR task. Several common factors govern-
ing the interpretation of pronouns were identified by
multiple authors (Arnold, 1998; Gordon and Hen-
drick, 1998). These included Subjecthood, Paral-
lelism, Recency, and Ambiguity. Following (McCoy
and Strube, 1999), we selected Recency as our start-
ing point and tracked the intervals between refer-
ences measured in sentences. Referring expressions
which were separated from the most recent reference
by more than two sentences were marked as long-
distance references. To cover the Subjecthood and
Parallelism factors, we extracted the syntactic cate-
gory of the current and three most recent references
directly from the GREC data. This information also
helped us determine if the entity was the subject of
the sentence at hand, as well as the two previous
sentences. Additionally, we tracked whether the en-
tity was in subject position of the sentence where
the previous reference appeared. Finally, we made
a simple attempt at recognizing potential interfering
antecedents (Siddharthan and Copestake, 2004) oc-
curring in the current sentence and the text since that
last reference.
Observing the performance of prototyping sys-
tems led us to include boolean features indicat-
ing whether the reference immediately followed the
words ?and,? ?but,? or ?then,? or if it appeared be-
tween a comma and the word ?and.? We also found
that non-annotated instances of the entity?s name,
which actually serve as references to the name itself
rather than to the entity, factor into Recency. Fig-
ure 1 provides an example of such a ?non-referential
instance.? We added a feature to measure distance
to these items, similar to the distance between refer-
ences. Sentence and reference counters rounded out
101
the full set of features.
The municipality was abolished in 1928, and the
name ?Mexico City? can now refer to two things.
Figure 1: Example of non-referential instance. In this
sentence, ?Mexico City? is not a reference to the main en-
tity (Mexico City), but rather to the name ?Mexico City.?
3 System Description
A series of C5.0 decision trees (RuleQuest Research
Pty Ltd, 2008) were trained to determine the most
appropriate reference type for each instance in the
training set. Each tree used a slightly different sub-
set of features. It was determined that one decision
tree in particular performed the best on mountain
and person articles, and another tree on the remain-
ing categories. Both of these trees were incorporated
into the submitted system.
Our system first performed some preprocessing
for sentence segmentation and identified any non-
referential instances as described in Section 2. Next,
it marshalled all of the relevant data for the feature
set. These data points were used to represent the
context of the referring expression and were sent to
the decision trees to determine the most appropriate
reference type. Once the type had been selected, the
list of alternative referring expressions were scanned
using a few simple rules. For the first instance of a
name in an article, the longest non-emphatic name
was chosen. For subsequent instances, the shortest
non-emphatic name was selected. For the other 3
types, the first matching option in the list was used,
backing off to a pronoun or name if the preferred
type was not available.
4 Results
The performance of our system, as tested on the de-
velopment set and scored by the GREC evaluation
software, is offered in Table 1.
5 Conclusions
We?ve shown that psycholinguistic research can be
helpful in determining feature selection for gener-
ating referring expressions. We suspect the perfor-
mance of our system could be improved by employ-
Table 1: Scores from GREC evaluation software.
Component Score Value
total pairs 656
reg08 type matches 461
reg08 type accuracy 0.702743902439024
reg08 type precision 0.702743902439024
reg08 type recall 0.702743902439024
string matches 417
string accuracy 0.635670731707317
mean edit distance 0.955792682926829
mean normalised edit distance 0.338262195121951
BLEU 1 score 0.6245
BLEU 2 score 0.6103
BLEU 3 score 0.6218
BLEU 4 score 0.6048
ing more sophisticated means of sentence segmen-
tation and named entity recognition for identifying
interfering antecedents.
References
Jennifer E. Arnold. 1998. Reference Form and Discourse
Patterns. Doctoral dissertation, Department of Lin-
guistics, Stanford University, June.
Anja Belz and Sabastian Varges. 2007. Generation of re-
peated references to discourse entities. In Proceedings
of the 11th European Workshop on NLG, pages 9?16,
Schloss Dagstuhl, Germany.
Peter C. Gordon and Randall Hendrick. 1998. The rep-
resentation and processing of coreference in discourse.
Cognitive Science, 22(4):389?424.
Kathleen F. McCoy and Michael Strube. 1999. Gener-
ating anaphoric expressions: Pronoun or definite de-
scription. In Proceedings of Workshop on The Rela-
tion of Discourse/Dialogue Structure and Reference,
Held in Conjunction with the 38th Annual Meeting,
pages 63 ? 71, College Park, Maryland. Association
for Computational Linguistics.
RuleQuest Research Pty Ltd. 2008. Data mining
tools See5 and C5.0. http://www.rulequest.com/see5-
info.html.
Advaith Siddharthan and Ann Copestake. 2004. Gener-
ating referring expressions in open domains. In Pro-
ceedings of the 42th Meeting of the Association for
Computational Linguistics Annual Conference, pages
408?415, Barcelona, Spain.
102
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 105?106,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
UDel: Extending Reference Generation to Multiple Entities
Charles Greenbacker and Kathleen McCoy
Dept. of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[charlieg|mccoy]@cis.udel.edu
Abstract
We report on an attempt to extend a reference
generation system, originally designed only
for main subjects, to generate references for
multiple entities in a single document. This
endeavor yielded three separate systems: one
utilizing the original classifier, another with a
retrained classifier, and a third taking advan-
tage of new data to improve the identification
of interfering antecedents. Each subsequent
system improved upon the results of the pre-
vious iteration.
1 Introduction
This paper provides a system report on our submis-
sion for the GREC-NEG (Named Entity Generation)
Task, one of the two shared task competitions for
Generation Challenges 2009. The objective is to se-
lect the most appropriate reference to named entities
from a given list of alternatives. The corpus consists
of introductory sections from approximately 1,000
Wikipedia articles in which single and plural refer-
ences to all people mentioned in the text have been
annotated (Belz and Varges, 2007). The training set
contains articles from the categories of Chefs, Com-
posers, and Inventors. GREC-NEG differs from the
other challenge task, GREC-MSR (Main Subject
References), in that systems must now account for
multiple entities rather than a single main subject,
and the corpus includes only articles about persons
rather than a variety of topics.
2 System Description
Our GREC-NEG systems build upon our work for
the GREC-MSR task. Our original approach was
to consult findings in psycholinguistic research for
guidance regarding appropriate feature selection for
the production of referring expressions. We relied
upon several common factors recognized by multi-
ple authors (Arnold, 1998; Gordon and Hendrick,
1998), including Subjecthood, Parallelism, Recency,
and Ambiguity. We followed (McCoy and Strube,
1999) who stressed the importance of Recency in
reference generation. Finally, we made a prelimi-
nary attempt at identifying potential interfering an-
tecedents that could affect the Ambiguity of pro-
nouns (Siddharthan and Copestake, 2004).
As an initial attempt (UDel-NEG-1), we simply
extended our GREC-MSR submission. By adapt-
ing our system to account for multiple entities and
the slightly different data format, we were able to
use the existing classifier to generate references for
GREC-NEG. We suspected that accuracy could be
improved by retraining the classifier, so our next sys-
tem (UDel-NEG-2) added entity and mention num-
bers as features to train on. Presumably, this could
help distinguish between the main subject and sec-
ondary entities, as well as plural references. As
all named entities are tagged in the GREC-NEG
corpus, we leveraged this information to improve
our recognition of other antecedents interfering with
pronoun usage in a third new system (UDel-NEG-
3). As in our GREC-MSR submission, all three of
our GREC-NEG systems trained C5.0 decision trees
(RuleQuest Research Pty Ltd, 2008) on our set of
features informed by psycholinguistic research.
3 Results
System performance, as tested on the development
set and scored by the GREC evaluation software,
105
is offered in Tables 1, 2, and 3. Type accuracy
for UDel-NEG-1 remained close to our GREC-MSR
submission, and error rate was reduced by over 20%
for UDel-NEG-2 and UDel-NEG-3. However, string
accuracy was very low across all three systems, as
compared to GREC-MSR results.
Table 1: GREC scores for UDel-NEG-1 (unmodified).
Component Score Value
total pairs 907
reg08 type matches 628
reg08 type accuracy 0.69239250275634
reg08 type precision 0.688699360341151
reg08 type recall 0.688699360341151
string matches 286
string accuracy 0.315325248070562
mean edit distance 1.55126791620728
mean normalised edit dist. 0.657521668367265
BLEU 1 score 0.4609
BLEU 2 score 0.5779
BLEU 3 score 0.6331
BLEU 4 score 0.6678
Table 2: GREC scores for UDel-NEG-2 (retrained).
Component Score Value
total pairs 907
reg08 type matches 692
reg08 type accuracy 0.762954796030871
reg08 type precision 0.749466950959488
reg08 type recall 0.749466950959488
string matches 293
string accuracy 0.323042998897464
mean edit distance 1.4773980154355
mean normalised edit dist. 0.64564100951858
BLEU 1 score 0.4747
BLEU 2 score 0.6085
BLEU 3 score 0.6631
BLEU 4 score 0.6917
4 Conclusions
The original classifier performed well when ex-
tended to multiple entities, and showed marked im-
provement when retrained to take advantage of new
Table 3: GREC scores for UDel-NEG-3 (interference).
Component Score Value
total pairs 907
reg08 type matches 694
reg08 type accuracy 0.7651598676957
reg08 type precision 0.752665245202559
reg08 type recall 0.752665245202559
string matches 302
string accuracy 0.332965821389195
mean edit distance 1.46306504961411
mean normalised edit dist. 0.636499985162561
BLEU 1 score 0.4821
BLEU 2 score 0.6113
BLEU 3 score 0.6614
BLEU 4 score 0.6874
data. All three systems yielded poor scores for string
accuracy as compared to GREC-MSR results, sug-
gesting an area for improvement.
References
Jennifer E. Arnold. 1998. Reference Form and Discourse
Patterns. Doctoral dissertation, Department of Lin-
guistics, Stanford University, June.
Anja Belz and Sabastian Varges. 2007. Generation of re-
peated references to discourse entities. In Proceedings
of the 11th European Workshop on NLG, pages 9?16,
Schloss Dagstuhl, Germany.
Peter C. Gordon and Randall Hendrick. 1998. The rep-
resentation and processing of coreference in discourse.
Cognitive Science, 22(4):389?424.
Kathleen F. McCoy and Michael Strube. 1999. Gener-
ating anaphoric expressions: Pronoun or definite de-
scription. In Proceedings of Workshop on The Rela-
tion of Discourse/Dialogue Structure and Reference,
Held in Conjunction with the 38th Annual Meeting,
pages 63 ? 71, College Park, Maryland. Association
for Computational Linguistics.
RuleQuest Research Pty Ltd. 2008. Data mining
tools See5 and C5.0. http://www.rulequest.com/see5-
info.html.
Advaith Siddharthan and Ann Copestake. 2004. Gener-
ating referring expressions in open domains. In Pro-
ceedings of the 42th Meeting of the Association for
Computational Linguistics Annual Conference, pages
408?415, Barcelona, Spain.
106
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 600?609, Dublin, Ireland, August 23-29 2014.
Identifying Important Features for Graph Retrieval
Zhuo Li and Sandra Carberry and Hui Fang* and Kathleen F. McCoy
ivanka@udel.edu carberry@udel.edu hui@udel.edu mccoy@udel.edu
Department Computer and Information Science,
*Department of Electrical and Computer Engineering
University of Delaware
Abstract
Infographics, such as bar charts and line graphs, occur often in popular media and are a rich
knowledge source that should be accessible to users. Unfortunately, information retrieval re-
search has focused on the retrieval of text documents and images, with almost no attention specif-
ically directed toward the retrieval of information graphics. Our work is the first to directly tackle
the retrieval of infographics and to design a system that takes into account their unique charac-
teristics. Learning-to-rank algorithms are applied on a large set of features to develop several
models for infographics retrieval. Evaluation of the models shows that features pertaining to the
structure and the content of graphics should be taken into account when retrieving graphics and
that doing so results in a model with better performance than a baseline model that relies on
matching query words with words in the graphic.
1 Introduction
Infographics are non-pictorial graphics such as bar charts and line graphs. When such graphics appear in
popular media, they generally have a high-level message that they are intended to convey. For example,
the graphic in Figure 1 ostensibly conveys the message that Toyota has the highest profit among the
automobile companies listed. Thus infographics are a form of language since, according to Clark (Clark
and Curran, 2007), language is any deliberate signal that is intended to convey a message.
Although much research has addressed the retrieval of documents, very little attention has been given
to the retrieval of infographics. Yet research has shown that the content of an infographic is often not
included in the article?s text (Carberry et al., 2006). Thus infographics are an important knowledge
source that should be accessible to users of a digital library.
Techniques that have been effective for document or image retrieval are inadequate for the retrieval
of infographics. Current search engines employ strategies similar to those used in document retrieval,
relying primarily on the text surrounding a graphic and web link structures. But the text in the surround-
ing document generally does not refer explicitly to the infographic or even describe its content (Carberry
et al., 2006). An obvious extension to using the article text would be to collect all the words in an
infographic and use it as a bag of words. However, infographics have structure and often a high-level
message, and bag of words approaches ignore this structure and message content.
This paper explores the features that should be taken into account when ranking graphics for retrieval
in response to a user query. Using a learning-to-rank algorithm on a wide range of features (including
structural and content features), we produce a model that performs significantly better than a model that
ignores graph structure and content. Analysis of the model shows that features based on the structure
and content of graphs are very important and should not be ignored. To our knowledge, our research is
the first to take graph structure and content into account when retrieving infographics.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
600
Figure 1: An Example Infographic
2 Related Work
Information retrieval research has focused on the retrieval of text documents and images. Two popular
approaches to text retrieval are the vector space method and probabilistic methods. The vector space
method (Dubin, 2004) represents the document and the query each as a vector of weighted words and
then uses a similarity function to measure the similarity of each document to the query. Most weighting
mechanisms reward words that occur frequently in both the document and query but infrequently in
the overall collection of documents. Probabilistic retrieval models instead estimate the probability that
a document is relevant to a user query. In recent years, the language modeling approach has shown
promise as a retrieval strategy with sound statistical underpinnings (Lv and Zhai, 2009; Manning et al.,
2008). In all of the above approaches, query expansion techniques have been used to expand the query
with synonyms and related words before ranking documents for retrieval. Work on short document and
query expansion have shown improvements in retrieval performance (Arguello et al., 2008; Escalante et
al., 2008; Metzler and Cai, 2011).
Work in Content Based Image Retrieval (CBIR) (Datta et al., 2008) has progressed from systems that
retrieved images based solely on visual similarity, relying on low-level features such as color, texture and
shape ( (Flickner et al., 1995; Swain and Ballard, 1991; Smith and Chang, 1997; Gupta and Jain, 1997),
among others), to systems which attempt to classify and reason about the semantics of the images being
processed (Bradshaw, 2000; Smeulders et al., 2000; Datta et al., 2008). However, images are free-form
with relatively little inherent structure; thus it is extremely difficult to determine what is conveyed by an
image, other than to list the image?s constituent pieces. Most systems that retrieve infographics, such
as SpringerImages (http://www.springerimages.com) and Zanran (http://www.zanran.com), are based on
textual annotations of the graphics as in image retrieval (Gao et al., 2011) or on matching the user?s query
against the text surrounding the graphic. However, the structure and content of the graph are not taken
into consideration.
In this paper, we focus on natural language queries given that such queries allow users to express their
specific information need more clearly than keywords (Phan et al., 2007; Bendersky and Croft, 2009).
Previous work on verbose and natural language queries (Bendersky and Croft, 2008; Liu et al., 2013)
used probabilistic models and natural language processing techniques to identify the key contents in
such queries. Our query processing method not only extracts key entities but also further classifies the
extracted key entities into different components using a learned decision tree model.
3 Problem Formulation
Our research is currently limited to two kinds of infographics: simple bar charts and single line graphs.
We assume that our digital library contains an XML representation of each graphic that includes 1) the
graphic?s image, 2) its structural components: the set of independent axis (x-axis) labels
1
, the entity being
measured on the dependent axis (y-axis), and the text that appears in the graphic?s caption, referred to
as G
x
, G
y
, and G
c
respectively, and 3) the graphic?s intended message G
m
and any entities G
f
that the
1
We will refer to the independent axis as the x-axis and the dependent axis as the y-axis throughout this paper.
601
message focuses on. This paper is not concerned with the computer vision problem of recognizing the
bars, labels, colors, etc. in a graphic; other research efforts, such as the work in (Chester and Elzer, 2005;
Futrelle and Nikolakis, 1995) are addressing the parsing of electronic images such as bar charts and line
graphs.
Prior research on our project has addressed issues that arise in recognizing G
y
, G
m
, and G
f
. The
dependent axis of an infographic often does not explicitly label what is being measured, such as net
profit in Figure 1, and these must be inferred from other text in the graphic. Our prior work (Demir et
al., 2007) identified a hierarchy of graphic components in which pieces of the entity being measured
might appear; a set of heuristics were constructed that extracted these pieces and melded them together
to form what we refer to as a measurement axis descriptor and which is G
y
. The project?s prior work
also identified a set of 17 categories of intended message, such as Rank, Relative-difference, Maximum,
and Rising-trend, that might be conveyed by simple bar charts and line graphs; a Bayesian system (Elzer
et al., 2011; Wu et al., 2010) was developed that utilizes communicative signals in a graphic (such as the
coloring of one bar differently from the other bars) in order to recognize a graphic?s intended message,
including both the message category and the parameters of the message such as any focused entity. For
example, the intended message of the bar chart in Figure 1 is ostensibly that Toyota has the highest net
profit of any of the automobile manufacturers listed; thus its message falls into the Maximum message
category and its focused entity is Toyota.
Our vision is that since graphics have structure and content, the users whose particular information
needs could be satisfied by an infographic will formulate their queries to indicate the requisite structure
of the desired graphics. Thus we assume the use of full-sentence queries so that the semantics of the query
can be analyzed to identify characteristics of relevant graphics. For example, consider the following two
queries that contain similar keywords but represent different information needs:
Q
1
: Which countries have the highest occurrence of rare diseases?
Q
2
: Which rare diseases occur in the most countries?
These two queries contain almost identical words but are asking for completely different graphics. Query
Q
1
is asking for a comparison of countries (independent axis) according to their occurrence of rare dis-
eases (dependent axis) while query Q
2
is asking for a comparison of different rare diseases (independent
axis) according to the number of countries in which they occur (dependent axis). In addition, both queries
are asking for a graphic with a Rank message that ranks countries (query Q
1
) or rare diseases (query Q
2
)
as opposed to a graphic that shows the trend in rare diseases throughout the world.
4 Methodology
To retrieve relevant graphics in response to a user query, the query will first be analyzed to identify requi-
site characteristics of relevant infographics. We have developed learned decision trees (Li et al., 2013a;
Li et al., 2013b) for analyzing a query and identifying the requisite structure of relevant infographics (the
content of the independent axis or x-axis and dependent axis or y-axis, referred to as Q
x
and Q
y
), and
the category of intended message and focused entity, if any, (referred to as Q
m
and Q
f
) that will best
satisfy the user?s information need.
Given a new user query, it is parsed and noun phrases are extracted. Each query-phrase pair, consisting
of a query and an extracted noun phrase, is processed by a decision tree that determines whether the noun
phrase represents x-axis content, y-axis content, or neither. Attributes used by this decision tree include
whether the main verb of the query is a comparison verb (such as ?differ? and ?compare?) or a trend
verb (such as ?change? and ?decrease?), whether the noun phrase is preceded by a quantity phrase such
as ?the number of? suggesting that the noun phrase specifies y-axis content of relevant infographics, and
whether the noun phrase describes a period of time.
Similarly, another decision tree is constructed to identify the category of graph intended message
(such as Trend or Rank) that the query desires, using a subset of the attributes from the axes decision tree
combined with the classification results of the axes decision tree. An example of the reused attributes is
the class of the main verb in the user query; for example, a comparison main verb suggests that relevant
infographics will convey a comparison-based intended message, such as a Relative-difference or Rank
602
intended message. Other attributes include the presence of a superlative or comparative in the query
and attributes depending on the identified content of the x and y axes by the axes decision tree, such
as the number of x-axis entities, their plurality, and whether an x-axis entity describes a time interval.
A third decision tree is constructed for identifying whether a noun phrase describes a specific focused
x-axis entity. Then the infographics in the digital library must be rank-ordered according to how well
they satisfy the requirements of the user query.
This paper is concerned with identifying the most important features in a metric for rank-ordering
the graphics in response to a user query. We experiment with two learning-to-rank algorithms and 56
features that include both general features such as bag of words comparisons and structural and content
features. Our hypothesis is that structural and content-based features play an important role in graph
retrieval and cannot be ignored. Section 5 discusses the features used in our experiments, Section 6
discusses the learning algorithms, Section 6.1 compares the resultant models with a baseline that uses
just general features treating query and graphic each as one bag of words, and Section 6.2 discusses the
features that appear most influential in the models.
5 Features
We consider three kinds of features: 1) general features that compare words in the query with words in
the graphic, 2) structural features that compare the requisite structure hypothesized from the query with
the structure of candidate infographics, and 3) content-based features that compare the requisite message
hypothesized from the user query with the intended message of candidate graphics.
Query expansion is a commonly used strategy in information retrieval to bridge the vocabulary gap
between terms in a query and those in documents. The basic idea is to expand the original query with
terms that are semantically similar to the ones in the query. This addresses the problem encountered when
the query uses the word car but the document uses the term automobile. But retrieval of information
graphics presents an additional problem. Consider a query such as ?Which car manufacturer has the
highest net profit?? A graphic such as the one in Figure 1 displays a set of car manufacturers on the x-
axis (Toyota, Nissan, etc.) but nowhere in the graphic does the word car or a synonym appear. Identifying
the ontological category, such as car or automobile, of these labels is crucial since the user?s query often
generalizes the entities on the independent axis of relevant graphs rather than listing them.
To expand a given text string s, we use Wikimantic (Boston et al., 2013), a term expansion method
that uses Wikipedia articles as topic concepts. A topic concept is a unigram distribution built from words
in the Wikipedia article for that topic. A string s is interpreted by Wikimantic into a mixture concept that
is a weighted vector of topic concepts that capture the semantic meaning of the words in s. Each topic
concept is weighted by the likelihood that the concept (Wikipedia article) generates the text string s. The
weighted concepts are then used to produce a unigram distribution of words that serve as the expansion of
the terms in the string s. One issue in graph retrieval is correlating the requisite x-axis content specified
in the user query with the x-axis labels in graphs. A query such as ?Which car manufacturer has ... ??
is requesting a graph where ?car manufacturers? are listed on the x-axis. Thus we need to recognize
individual x-axis words which are often proper nouns (e.g., ?Ford?, ?Nissan?, ?Honda?) as instances of
car manufacturers. In the case of labels on the independent axis (such as Toyota, Nissan, Honda, etc.),
words such as car or automobile are part of the produced unigram distribution ? that is, as a side effect,
the ontological category of the individual entities becomes part of the term expansion.
We use Wikimantic to interpret and expand each of the graph components G
x
, G
y
, G
f
, and G
c
. The
expansion of the graph components (as opposed to the typical expansion of the query) accomplishes two
objectives: 1) it addresses the problem of sparse graphic text by adding semantically similar words and
2) it addresses the problem of terms in the query capturing general classes (such as car or automobile)
when the graphic instead contains an enumeration of members of the general class. Expansion of the
words in the graphics, unlike query expansion, has the added advantage that it is completed in advance
and off-line.
603
5.1 General Features
Our general feature set includes 17 general features capturing a variety of different kinds of relevance
scorings between two bags of words consisting respectively of words from the user query and words
from the candidate infographic:
? GF
1
: A modified version of Okapi-BM25 (Fang et al., 2004) calculated as:
Okapi-BM25 Score =
?
w?Q
log
|D|+1
df
w
+1
?
tf
w
?(1+k
1
)
tf
w
+k
1
where Q is a query, |D| is the number of graphs in the digital library, w is a query word in Q,
df
w
is the frequency of graphs containing word w in the digital library, tf
w
is the frequency of
word w in the text expansion of the given graphic, and k
1
is a parameter that is typically set to 1.2.
Okapi-BM25 is a bag-of-words ranking function used in many information retrieval systems. Our
modified version of Okapi-BM25 addresses the problem of negative values that can occur with the
original Okapi formula. In addition, our formula does not take text length or query term frequency
into account since graphics have relatively similar amounts of text and most terms in a query occur
only once.
? GF
2
: The term frequency-inverse document frequency (tf-idf) value of query words that appear in
the expanded graphic.
? GF
3
: The maximum, minimum, and arithmetic mean of the term frequency (tf) of query words that
appear in the expanded graphic.
? GF
4
: The maximum, minimum, and arithmetic mean of inverse document (graphic) frequency (idf)
of query words that appear in the expanded graphic.
5.2 Structural Features
Our structural feature set includes 35 features: 17 that address how well a graphic?s x-axis (independent
axis) relates to the requisite x-axis content hypothesized from the user?s query and 18 that address how
well a graphic?s y-axis (dependent axis) content captures the requisite dependent axis content hypothe-
sized from the query. The following are a few of the x-axis features:
? SFX
1
: The Okapi-BM25 value using the same modified formula as for general features, given the
query x-axis words and the text expansion of the x-axis labels in the graphic.
? SFX
2
: The tf-idf of x-axis words hypothesized from the query that appear in the expansion of the
x-axis labels in the graphic.
? SFX
3
: The maximum, minimum, and arithmetic mean of tf of x-axis words hypothesized from the
query that appear in the expansion of the x-axis labels in the graphic.
? SFX
4
: The maximum, minimum, and arithmetic mean of idf of x-axis words hypothesized from
the query that appear in the expansion of the x-axis labels in the graphic.
The y-axis features (SFY
1
, SFY
2
, SFY
3
, and SFY
4
) include the same relevance measurements as
used for the x-axis features; for example, feature SFY
1
captures the Okapi-BM25 score for the y-axis
content hypothesized from the query and the text expansion of the graphic y-axis words, and feature
SFY
2
is the tf-idf score for the y-axis content hypothesized from the query and the expansion of the
graphic y-axis words. One additional feature that is specific to the y-axis is:
? SFY
5
: The posterior probability of the Wikimantic (Boston et al., 2013) mixture concept
2
for the
y-axis words hypothesized from the query, given the Wikimantic mixture concept representing the
y-axis words in the graph, referred to as p(Q
y
|G
y
). Both query y-axis words and the graphic y-axis
2
A Wikimantic mixture concept is a set of weighted concepts (Boston et al., 2013).
604
descriptor are each interpreted by Wikimantic into a mixture concept, M
qy
and M
gy
respectively.
Recall from the introduction to Section 5 that a mixture concept is a weighted vector of topic con-
cepts that defines the semantic meaning of a term or set of terms. For example, the mixture concept
for the country China is represented by a vector of topic concepts such as ?China?, ?People?s Re-
public of China?, ?Mainland China?, and so on. Wikimantic estimates the probability of a concept
given another concept by the amount of overlapping words between the two concepts. For example,
the topic concept for the country ?United States? is likely to contain similar words to the concept
for ?China?, such as the words ?country?, ?nation?, ?region?, ?capital?, ?GDP?, etc. Therefore
the probability of United States given China is likely to be higher than that of United States given
the topic ?rugby?.
5.3 Content Features
Our content feature set contains four features that address how well the intended message of a graphic
captures the requisite message content hypothesized from the user?s query. Ideally, a relevant graphic?s
intended message G
m
will match the message category Q
m
hypothesized from the user?s query. When
the two do not match exactly, we use a hierarchy of message categories and the concept of relaxation
as the paradigm for estimating how much perceptual effort would be required to extract the message
specified by the query from the graphic. For example, suppose that the query requests a Rank message;
graphics with Rank messages will convey the rank of a specific entity by arranging the entities in order of
value and highlighting in some way the entity whose rank is being conveyed. Graphics with a Rank-all
intended message will convey the rank of a set of entities without highlighting any specific entity; the
Rank-all message category appears as a parent of Rank in the message hierarchy since it is less specific
than Rank. Although one can identify the rank of a specific entity from a graphic whose intended message
is a Rank-all message, it is perceptually more difficult since one must search through the graph for the
entity whose rank is desired. By moving up or down the message hierarchy from Q
m
to G
m
, Q
m
is
relaxed to match different G
m
. The greater the degree of relaxation involved, the less message-relevant
the infographic is to the user query. The four content-based features are:
? CF
1
: Whether the message category Q
m
hypothesized from the user?s query matches exactly the
intended message category G
m
of the graphic.
? CF
2
: The amount of relaxation needed to relax the message category Q
m
hypothesized from the
user?s query so that it matches the intended message category G
m
of the graphic.
? CF
3
: The Okapi-BM25 value given the intended message focused entity Q
f
(if any) hypothesized
from the user?s query and the focused entity G
f
in the graphic, if any.
? CF
4
: The Okapi-BM25 value given the intended message focused entity Q
f
(if any) hypothesized
from the user?s query and the non-focused x-axis entities G
nf
in the graphic.
6 Constructing a Ranking Model for Graph Retrieval
Learning-to-rank algorithms (Liu, 2009) construct a learned model that ranks objects based on partially
ordered training data. Tree-based ensemble methods have been shown to be very effective (Chapelle
and Chang, 2011). We experimented with two state-of-the-art tree-based learning-to-rank algorithms as
implemented in the RankLib library (http://people.cs.umass.edu/vdang/ranklib.html): Multiple Additive
Regression Trees abbreviated as MART (Friedman, 2001) and Random Forest (Breiman, 2001).
A human subject experiment was performed to collect a set of 152 full sentence user queries from
five topics. The queries were collected from 5 different tasks and covered a variety of topics involving
companies. Two sample queries are ?What credit card company made the most money in 2008?? and
?How does Avis rank compared to other car rental companies in revenue??. We used the collected
queries to search on popular commercial image search engines to get more infographics from the same
topics. These commercial search engines include Google Image, Microsoft Bing Image Search, and
Picsearch. This produced a set of 257 infographics that are in the topics of the collected queries. Each
605
query-infographic pair was assigned a relevance score on a scale of 0-3 by an undergraduate researcher.
A query-infographic pair was assigned 3 points if the infographic was considered highly relevant to the
query and 0 points if it was irrelevant. Query-infographic pairs where the graphic was somewhat relevant
to the query were assigned 1 or 2 points, depending on the judged degree of relevance of the graphic to
the query. This produced a corpus for training and testing.
Using MART and Random Forest, we developed four models from all 56 features, including the
structural and content features. Two of the models were built using our learned decision trees (Li et
al., 2013b; Li et al., 2013a) to analyze the queries and hypothesize the requisite x-axis content, y-axis
content, message category, and focused entity (if any); see the second row of Table 1. Since the learned
decision trees are not perfect, the other two models were built from hand-labelled data; see the last row of
Table 1. In addition, two baseline models were constructed using only the general features and omitting
the structural and content-based features.
6.1 Evaluating the Models
Normalized Discounted Cumulative Gain (NDCG) (Ja?rvelin and Keka?la?inen, 2002) is used to evaluate
the retrieval result. Table 1 displays the NDCG@10 results. In each case, we averaged together the
NDCG results of 10 runs using the Bootstrapping Method (Tan et al., 2006) in which the query data set
is sampled with replacement to select 152 queries; these 152 queries, and for each query the relevance
judgements assigned to each of the graphics, comprised the training set, with the unselected queries
and their relevance judgements comprising the testing set. The Bootstrapping method is a widely used
evaluation method for small datasets. Typically, approximately 63% of the dataset is selected for the
training set (with some items appearing more than once in the training set) and 37% for the testing set.
The second row of Table 1 provides results when each query is processed by our learned decision trees to
extract the structural content and message category that the query specifies. However, the decision trees
are imperfect. To determine whether our system could do even better if the decision trees were improved,
the third row of Table 1 reports results when each query was hand-labelled with the correctly extracted
structural and message content.
The models using all 56 features produced significantly better results than the baseline model that
used just the general features, indicating that structural and content-based features are very important
and must be taken into account in graph retrieval. In addition, the models built from the hand-labelled
data produced better results than the models where the structural and content features were automatically
extracted from the queries using the learned decision trees; this suggests that improving the decision trees
that process the queries would improve the accuracy of the learned graph retrieval models. In some cases,
the Random Forest learned model performed better than the MART model, but the improvement was not
significant. The experimental results show that both MART and Random Forest using all 56 features,
either using the hand-labelled query data or decision tree query data, provide significantly better results
than the baseline approach (p<0.0005).
Algorithm MART Random Forest
Baseline 0.4943 0.4935
Decision Tree Query Data 0.6239 0.6258
Hand-labelled Query Data 0.6723 0.6758
Table 1: NDCG@10 Results
Figure 2 displays the NDCG@k results for different values of k. The bottom solid line and the line
composed of triangles depict the baseline results, the middle dashed line and the line composed of circles
depict the results using the decision tree query data, and the top solid line and the line composed of
triangles depict the results using the hand-labelled data. All of the models improve as k increases. Most
important, both our MART and Random Forest models constructed from all 56 features perform much
better than the baseline models for all values of k. Thus we conclude that the use of structural and content
features helps in selecting the most relevant graphic as well as the most relevant sets of graphics.
606
Figure 2: NDCG@k for Various Values of n
6.2 Analysis of Influential Features
In both MART and Random Forest, features that are used at the top levels of each tree are more important
in ranking a graphic than features that appear lower in the tree. We analyzed the importance of each of
the 56 features based on the level in each tree where the feature is first used. 70% of the top ten most
important features in the trees produced by both MART and Random Forest were structural or content
features. The most influential two features in trees produced by MART were SFY
5
which captures
p(Q
y
| G
y
) and SFX
2
which captures the tf-idf of x-axis words hypothesized from the query that appear
in the expansion of the x-axis labels in the graphic. Although these two features were not the two
most influential features in the trees produced by Random Forest, they did appear among the top 5
features. Two content-based features appeared among the top ten most important features: CF
3
which
captures the relevance of the focused entity Q
f
(if any) hypothesized from the query to the focused
entity G
f
(if any) in the graphic and CF
4
which captures the relevance of the focused entity Q
f
(if any)
hypothesized from the query to the non-focused entities G
fx
in the graphic. The content features CF
1
and CF
2
that measure relevance of the message category hypothesized from the query to the intended
message category in a candidate graphic appeared among the top 20 features but not among the top 10
features. Further inspection of the trees and analysis of the queries and graphics leads us to believe
that message category relevance is influential in refining the ranking of graphics once graphics with
appropriate structural content have been identified. Our future work will examine these two features
more closely and determine whether modifications of them, or changes in how they are used, will improve
results.
Based on these results, we conclude that structural and content-based features are important when
ranking infographics for retrieval and must be taken into account in an effective graph retrieval system.
7 Conclusion and Future Work
To our knowledge, no other research effort has considered the use of structural and content-based fea-
tures when ranking graphics for retrieval from a digital library. We developed learned models that take
into account how well the structure and content of an infographic matches the requisite structure and con-
tent hypothesized from the user query, and showed that these models perform significantly better than
baseline models that ignore graph structure and message content. In addition, an analysis of the learned
models showed which structural and content features were most influential. In our future work, we will
improve our methods for hypothesizing requisite features of relevant graphics and will analyze our re-
laxation metric to determine whether an improved metric will play a more influential role in ranking
graphics for retrieval.
Acknowledgements
This work was supported by the National Science Foundation under grant III-1016916 and IIS-1017026.
607
References
Jaime Arguello, Jonathan L Elsas, Jamie Callan, and Jaime G Carbonell. 2008. Document representation and
query expansion models for blog recommendation. ICWSM, 2008(0):1.
Michael Bendersky and W Bruce Croft. 2008. Discovering key concepts in verbose queries. In Proceedings of the
31st annual international ACM SIGIR conference on Research and development in information retrieval, pages
491?498. ACM.
Michael Bendersky and W Bruce Croft. 2009. Analysis of long queries in a large scale search log. In Proceedings
of the 2009 workshop on Web Search Click Data, pages 8?14. ACM.
Christopher Boston, Hui Fang, Sandra Carberry, Hao Wu, and Xitong Liu. 2013. Wikimantic: Toward effective
disambiguation and expansion of queries. Data & Knowledge Engineering.
Ben Bradshaw. 2000. Semantic based image retrieval: a probabilistic approach. In Proceedings of the eighth ACM
international conference on Multimedia, pages 167?176. ACM.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Sandra Carberry, Stephanie Elzer, and Seniz Demir. 2006. Information graphics: an untapped resource for digital
libraries. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development
in information retrieval, pages 581?588. ACM.
Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Yahoo! Learning to Rank
Challenge, pages 1?24.
Daniel Chester and Stephanie Elzer. 2005. Getting computers to see information graphics so users do not have to.
In Foundations of Intelligent Systems, pages 660?668. Springer.
S. Clark and J.R. Curran. 2007. Wide-coverage efficient statistical parsing with ccg and log-linear models. Com-
putational Linguistics, 33(4):493?552.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. 2008. Image retrieval: Ideas, influences, and trends of the
new age. ACM Computing Surveys (CSUR), 40(2):5.
Seniz Demir, Sandra Carberry, and Stephanie Elzer. 2007. Effectively realizing the inferred message of an in-
formation graphic. In Proceedings of the International Conference on Recent Advances in Natural Language
Processing (RANLP), pages 150?156.
David Dubin. 2004. The most influential paper gerard salton never wrote.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman. 2011. The automated understanding of simple bar charts.
Artificial Intelligence, 175(2):526?555.
Hugo Jair Escalante, Carlos Herna?ndez, Aurelio Lo?pez, Heidy Mar??n, Manuel Montes, Eduardo Morales, Enrique
Sucar, and Luis Villasen?or. 2008. Towards annotation-based query and document expansion for image retrieval.
In Advances in Multilingual and Multimodal Information Retrieval, pages 546?553. Springer.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In Proceed-
ings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR ?04, pages 49?56, New York, NY, USA. ACM.
Myron Flickner, Harpreet Sawhney, Wayne Niblack, Jonathan Ashley, Qian Huang, Byron Dom, Monika Gorkani,
Jim Hafner, Denis Lee, Dragutin Petkovic, et al. 1995. Query by image and video content: The qbic system.
Computer, 28(9):23?32.
Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting machine. The Annals of Statis-
tics, 29(5):1189?1232, 10.
Robert P Futrelle and Nikos Nikolakis. 1995. Efficient analysis of complex diagrams using constraint-based
parsing. In Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on,
volume 2, pages 782?790. IEEE.
Y. Gao, M. Wang, H. Luan, J. Shen, S. Yan, and D. Tao. 2011. Tag-based social image search with visual-text
joint hypergraph learning. In Proceedings of the 19th ACM international conference on Multimedia, pages
1517?1520. ACM.
608
Amarnath Gupta and Ramesh Jain. 1997. Visual information retrieval. Communications of the ACM, 40(5):70?79.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf.
Syst., 20(4):422?446, October.
Zhuo Li, Matthew Stagitis, Sandra Carberry, and Kathleen F. McCoy. 2013a. Towards retrieving relevant informa-
tion graphics. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ?13, pages 789?792, New York, NY, USA. ACM.
Zhuo Li, Matthew Stagitis, Kathleen McCoy, and Sandra Carberry. 2013b. Towards finding relevant information
graphics: Identifying the independent and dependent axis from user-written queries.
Jingjing Liu, Panupong Pasupat, Yining Wang, Scott Cyphers, and Jim Glass. 2013. Query understanding en-
hanced by hierarchical parsing structures. In Automatic Speech Recognition and Understanding (ASRU), 2013
IEEE Workshop on, pages 72?77. IEEE.
Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval,
3(3):225?331.
Yuanhua Lv and ChengXiang Zhai. 2009. Positional language models for information retrieval. In Proceedings
of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages
299?306. ACM.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Schu?tze. 2008. Introduction to information retrieval,
volume 1. Cambridge university press Cambridge.
Donald Metzler and Congxing Cai. 2011. Usc/isi at trec 2011: Microblog track. In TREC.
Nina Phan, Peter Bailey, and RossWilkinson. 2007. Understanding the relationship of information need specificity
to search query length. In Proceedings of the 30th annual international ACM SIGIR conference on Research
and development in information retrieval, pages 709?710. ACM.
Arnold WM Smeulders, Marcel Worring, Simone Santini, Amarnath Gupta, and Ramesh Jain. 2000. Content-
based image retrieval at the end of the early years. Pattern Analysis and Machine Intelligence, IEEE Transac-
tions on, 22(12):1349?1380.
John R Smith and Shih-fu Chang. 1997. Querying by color regions using the visualseek content-based visual
query system. Intelligent multimedia information retrieval, 7(3):23?41.
Michael J Swain and Dana H Ballard. 1991. Color indexing. International journal of computer vision, 7(1):11?32.
Pang-Ning Tan, Michael Steinbach, Vipin Kumar, et al. 2006. Introduction to data mining. WP Co.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel Chester. 2010. Recognizing the intended message of line
graphs. In Diagrammatic Representation and Inference, pages 220?234. Springer.
609
Summarizing Information Graphics Textually
Seniz Demir?
TUBITAK-BILGEM
Sandra Carberry??
University of Delaware
Kathleen F. McCoy?
University of Delaware
Information graphics (such as bar charts and line graphs) play a vital role in many
multimodal documents. The majority of information graphics that appear in popular media
are intended to convey a message and the graphic designer uses deliberate communicative
signals, such as highlighting certain aspects of the graphic, in order to bring that message
out. The graphic, whose communicative goal (intended message) is often not captured by the
document?s accompanying text, contributes to the overall purpose of the document and cannot be
ignored. This article presents our approach to providing the high-level content of a non-scientific
information graphic via a brief textual summary which includes the intended message and the
salient features of the graphic. This work brings together insights obtained from empirical studies
in order to determine what should be contained in the summaries of this form of non-linguistic
input data, and how the information required for realizing the selected content can be extracted
from the visual image and the textual components of the graphic. This work also presents a
novel bottom?up generation approach to simultaneously construct the discourse and sentence
structures of textual summaries by leveraging different discourse related considerations such as
the syntactic complexity of realized sentences and clause embeddings. The effectiveness of our
work was validated by different evaluation studies.
1. Introduction
Graphical representations are widely used to depict quantitative data and the relations
among them (Friendly 2008). Although some graphics are constructed from raw data
only for visualization purposes, the majority of information graphics (such as bar charts
and line graphs) found in popular media (such as magazines and newspapers) are
? The Scientific and Technological Research Council of Turkey, Center of Research for Advanced
Technologies of Informatics and Information Security, Gebze, Kocaeli, TURKEY, 41470.
E-mail: senizd@uekae.tubitak.gov.tr. (This work was done while the author was a graduate student at
the Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.)
?? Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.
E-mail: carberry@cis.udel.edu.
? Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA 19716.
E-mail: mccoy@cis.udel.edu.
Submission received: 20 April 2010; revised submission received: 8 July 2011; accepted for publication:
6 September 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 3
Figure 1
Graphic conveying a maximum bar.
constructed to convey a message. For example, the graphic in Figure 1 ostensibly is
intended to convey that ?The United States has the highest number of hacker attacks
among the countries listed.? The graphic designer made deliberate choices in order to
bring that message out. For example, the bar representing the United States is high-
lighted with a different color from the other bars and the bars are sorted with respect to
their values instead of their labels so that the bar with the highest value can be easily
recognized. Such choices, we argue, are examples of communicative signals that graphic
designers use. Under Clark?s definition (1996), language is not just text and utterances,
but instead includes any deliberate signal (such as gestures and facial expressions) that
is intended to convey a message; thus an information graphic is a form of language.
In popular media, information graphics often appear as part of a multimodal
document. Carberry, Elzer, and Demir (2006) conducted a corpus study of information
graphics from popular media, where the extent to which the message of a graphic is
also captured by the text of the accompanying document was analyzed. One hundred
randomly selected graphics of different kinds (e.g., bar charts and line graphs) were
collected from newspapers and magazines along with their articles. It was observed
that in 26% of the instances, the text conveyed only a small portion of the graphic?s
message and in 35% of the instances, the text didn?t capture the graphic?s message
at all. Thus graphics, together with the textual segments, contribute to the overall
purpose of a document (Grosz and Sidner 1986) and cannot be ignored. We argue that
information graphics are an important knowledge resource that should be exploited,
and understanding the intention of a graphic is the first step towards exploiting it.
This article presents our novel approach to identifying and textually conveying
the high-level content of an information graphic (the message and knowledge that one
would gain from viewing a graphic) from popular media. Our system summarizes this
form of non-linguistic input data by utilizing the inferred intention of the graphic de-
signer and the communicative signals present in the visual representation. Our overall
goal is to generate a succinct coherent summary of a graphic that captures the intended
message of the graphic and its visually salient features, which we hypothesize as being
related to the intended message. Input to our system is the intention of the graphic
inferred by the Bayesian Inference System (Elzer, Carberry, and Zukerman 2011), and
an XML representation of the visual graphic (Chester and Elzer 2005) that specifies the
components of the graphic such as the number of bars and the heights of each bar. Our
work focuses on the generation issues inherent in generating a textual summary of a
graphic given this information. The current implementation of the system is applicable
to only one kind of information graphic, simple bar charts, but we hypothesize that the
overall summarization approach could be extended to other kinds of graphics.
528
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
In this article, we investigate answers to the following questions: (1) Among all
possible information that could be conveyed about a bar chart, what should be included
in its summary? (2) How should the content of a summary be organized into a coherent
text? (3) How should the text structure be best realized in natural language? Given the
intended message and the XML representation of a graphic, our system first determines
the content of the graphic?s summary (a list of propositions) by applying the content
identification rules constructed for that intended message category. Our system then
produces a coherent organization of the selected content by applying a bottom?up
approach which leverages a variety of considerations (such as the syntactic complexity
of the realized sentences and clause embeddings) in choosing how to aggregate informa-
tion into sentence-sized units. The system finally orders and realizes the sentence-sized
units in natural language and generates referring expressions for graphical elements
that are required in realization.
The rest of this article is structured as follows. Section 2 discusses related work
on summarization of non-linguistic input data and describes some natural language
applications which could benefit from summaries generated by our work. Section 3
outlines our summarization framework. Section 4 is concerned with identifying the
propositional content of a summary and presents our content-identification rules that
specify what should be included in the summary of a graphic. Section 5 describes
our bottom?up approach, which applies operators to relate propositions selected for
inclusion, explores aggregating them into sentence-sized units, and selects the best orga-
nization via an evaluation metric. Section 6 presents our sentence-ordering mechanism,
which incorporates centering theory to specify the order in which the sentence-sized
units should be presented. Section 7 describes how our system realizes the selected
content in natural language. Particular attention is devoted to our methodology for
generating referring expressions for certain graphical elements such as a descriptor
of what is being measured in the graphic. Section 8 presents a user study that was
conducted to evaluate the effectiveness of the generated summaries for the purposes
of this research by measuring readers? comprehension. Section 9 concludes the article
and outlines our future work.
2. Background
2.1 Related Work
There has been a growing interest in language systems that generate textual summaries
of non-linguistic input data (Reiter 2007). The overall goal of these systems, generally
referred to as data-to-text systems, is to enable efficient processing of large volumes
of numeric data by supporting traditional visualisation modalities and to reduce
the effort spent by human experts on analyzing the data. Various examples of data-
to-text systems in the literature include systems that summarize weather forecast
data (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich
1983), and georeferenced data (Turner, Sripada, and Reiter 2009).
One of the most successful data-to-text generation research efforts is the SumTime
project, which uses pattern recognition techniques to generate textual summaries of
automatically generated time-series data in order to convey the significant and inter-
esting events (such as spikes and oscillations) that a domain expert would recognize
by analyzing the data. The SumTime-Mousam (Somayajulu, Reiter, and Davy 2003)
and SumTime-Turbine (Yu et al 2007) systems were designed to summarize weather
forecast data and the data from gas turbine engines, respectively. More recently, the
529
Computational Linguistics Volume 38, Number 3
project was extended to the medical domain. The BabyTalk (Gatt et al 2009) project
produces textual summaries of clinical data collected for babies in a neonatal intensive
care unit, where the summaries are intended to present key information to medical staff
for decision support. The implemented prototype (BT-45) (Portet et al 2009) generates
multi-paragraph summaries from large quantities of heterogeneous data (e.g., time
series sensor data and the records of actions taken by the medical staff). The overall
goal of these systems (identifying and presenting significant events) is similar to our
goal of generating a summary that conveys what a person would get by viewing an
information graphic, and these systems contend with each of the generation issues we
must face with our system. Our generation methodology, however, is different from the
approaches deployed in these systems in various respects. For example, BT-45 produces
multi-paragraph summaries where each paragraph presents first a key event (of highest
importance), then events related to the key event (e.g., an event that causes the key
event), and finally other co-temporal events. Our system, on the other hand, produces
single-paragraph summaries where the selected propositions are grouped and ordered
with respect to the kind of information they convey. In addition, BT-45 performs a
limited amount of aggregation at the conceptual level, where the aggregation is used
to express the relations between events with the use of temporal adverbials and cue
phrases (such as as a result). Contrarily, our system syntactically aggregates the selected
propositions with respect to the entities they share.
There is also a growing literature on summarizing numeric data visualized via
graphical representations. One of the recent studies, the iGRAPH-Lite (Ferres et al
2007) system, provides visually impaired users access to the information in a graphic via
keyboard commands. The system is specifically designed for the graphics that appear
in ?The Daily? (Statistics Canada?s main dissemination venue) and presents the user
with a template-based textual summary of the graphic. Although this system is very
useful for in depth analysis of statistical graphs and interpreting numeric data, it is
not appropriate for graphics from popular media where the intended message of the
graphic is important. In the iGRAPH-Lite system, the summary generated for a graphic
conveys the same information (such as the title of the graphic, and the maximum and
minimum values) no matter what the visual features of the graphic are. The content of
the summaries that our system generates, however, is dependent on the intention and
the visual features of the graphic. Moreover, that system does not consider many of the
generation issues that we address in our work.
Choosing an appropriate presentation for a large amount of quantitative data is
a difficult and time-consuming task (Foster 1999). A variety of systems were built to
automatically generate presentations of statistical data?such as the PostGraphe sys-
tem (Corio and Lapalme 1999; Fasciano and Lapalme 2000), which generates graphics
and complementary text based on the information explicitly given by the user such
as the intention to be conveyed in the graphic and the data of special interest to the
user. The content of the accompanying text is determined according to the intention
of the graphic and the features of the data. Moreover, the generated texts are intended
to reinforce some important facts that are visually present in the graphic. In this re-
spect, the generation in PostGraphe is similar to our work, although the output texts
have a limited range and are heavily dependent on the information explicitly given
by the user.
2.2 Role of Graphical Summaries in Natural Language Applications
2.2.1 Accessibility. Electronic documents that contain information graphics pose chal-
lenging problems for visually impaired individuals. The information residing in the
530
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
text can be delivered via screen reader programs but visually impaired individuals are
generally stymied when they come across graphics. These individuals can only receive
the ALT text (human-generated text that conveys the content of a graphic) associated
with the graphic. Many electronic documents do not provide ALT texts and even in the
cases where ALT text is present, it is often very general or inadequate for conveying the
intended message of the graphic (Lazar, Kleinman, and Malarkey 2007).
Researchers have explored different techniques for providing access to the in-
formational content of graphics for visually impaired users, such as sound (Meijer
1992; Alty and Rigas 1998), touch (Ina 1996; Jayant et al 2007), or a combination
of the two (Kennel 1996; Ramloll et al 2000). Unfortunately, these approaches have
serious limitations such as requiring the use of special equipment (e.g., printers and
touch panels) or preparation work done by sighted individuals. Research has also
investigated language-based accessibility systems to provide access to graphics (Kurze
1995; Ferres et al 2007). As mentioned in Section 2.1, these language-based systems
are not appropriate for graphics in articles from popular media where the intended
message of the graphic is important. We hypothesize that providing alternative access
to what the graphic looks like is not enough and that the user should be provided
with the message and knowledge that one would gain from viewing the graphic. We
argue that the textual summaries generated by our approach could be associated with
graphics as ALT texts so that individuals with sight impairments would be provided
with the high-level content of graphics while reading electronic documents via screen
readers.
2.2.2 Document Summarization. Research has extensively investigated various techniques
for single (Hovy and Lin 1996; Baldwin andMorton 1998) and multi-document summa-
rization (Goldstein et al 2000; Schiffman, Nenkova, andMcKeown 2002). The summary
should provide the topic and an overview of the summarized documents by identifying
the important and interesting aspects of these documents. Document summarizers
generally evaluate and extract items of information from documents according to their
relevance to a particular request (such as a request for a person or an event) and address
discourse related issues such as removing redundancies (Radev et al 2004) and ordering
sentences (Barzilay, Elhadad, and McKeown 2002) in order to make the summary more
coherent.
It is widely accepted that to produce a good summary of a document, one must
understand the document and recognize the communicative intentions of the author.
Summarization work primarily focuses on the text of a document but, as mentioned
earlier, information graphics are an important part of many multimodal documents
that appear in popular media and these graphics contribute to the overall commu-
nicative intention of the document. We argue that document summarization should
capture the high-level content of graphics that are included in the document, because
information graphics often convey information that is not repeated elsewhere in the
document. We believe that the summary of a graphic generated by our system, which
provides the intended message of the graphic and the information that would be
perceived with a casual look at the graphic, might help in summarizing multi-modal
documents.1
1 Our colleagues are currently investigating how the findings from this work can be used in
communicating the content of multimodal documents.
531
Computational Linguistics Volume 38, Number 3
3. System Overview
Figure 2 provides an overview of the overall system architecture. The inputs to our
system are an XML representation of a bar chart and the intended message of the chart;
the former is the responsibility of a Visual Extraction System (Chester and Elzer 2005)
and the latter is the responsibility of a Bayesian Inference System (Elzer, Carberry, and
Zukerman 2011). Given these inputs, the Content Identification Module (CIM) first
identifies the salient and important features of a graphic that are used to augment its
inferred message in the summary. The propositions conveying the selected features and
the inferred message of the graphic are then passed to the Text Structuring and Aggre-
gation Module (TSAM). This module produces a partial ordering of the propositions
according to the kind of information they convey, and aggregates them into sentence-
sized units. The Sentence OrderingModule (SOM) then determines the final ordering of
the sentence-sized units. Finally, the Sentence Generation Module (SGM) realizes these
units in natural language, giving particular attention to generating referring expressions
for graphical elements when appropriate. In the rest of this section, we briefly present
the systems that provide input to our work and describe the corpus of bar charts
used for developing and testing our system. The following sections then describe the
modules implemented within our system in greater detail, starting from the Content
Identification Module.
Figure 2
System architecture.
532
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
3.1 Visual Extraction System
The Visual Extraction System (Chester and Elzer 2005) analyzes a graphic image (visual
image of a bar chart) and creates an XML representation specifying the components
of the graphic, such as the height and color of each bar, any annotations on a bar, the
caption of the graphic, and so forth. The current implementation handles vertical and
horizontal bar charts that are clearly drawn with specific fonts and no overlapping
characters. The charts can have a variety of textual components such as axis labels,
caption, further descriptive text, text inside the graphic, and text below the graphic.
The current system cannot handle 3D charts, charts where the bars are represented by
icons, or charts containing texts at multiple angles, however.
3.2 Bayesian Inference System for Intention Recognition
The Bayesian Inference System (Elzer, Carberry, and Zukerman 2011) treats an informa-
tion graphic as a form of language with a communicative intention, and reasons about
the communicative signals present in the graphic to recognize its intendedmessage. The
system is currently limited to simple bar charts and takes as input the XML representa-
tion of the chart produced by the Visual Extraction System described previously.
Three kinds of communicative signals that appear in bar charts are extracted from a
graphic and utilized by the system. The first kind of signal is the relative effort required
for various perceptual and cognitive tasks. The system adopts the AutoBrief (Kerpedjiev
and Roth 2000) hypothesis that the graphic designer chooses the best design to facilitate
the perceptual and cognitive tasks that a viewer will need to perform on the graphic.
Thus, the relative effort for different perceptual tasks serves as a communicative signal
about what message the graphic designer intended to convey (Elzer et al 2006). The
second and third types of communicative signals used in the system are salience and
the presence of certain verbs and adjectives in the caption that suggest a particular
message category. The presence of any of these three kinds of communicative signals
are entered into a Bayesian network as evidence. The top level of the network captures
one of the 12 message categories that have been identified as the kinds of messages that
can be conveyed by a bar chart, such as conveying a change in trend (Changing Trend)
or conveying the bar with the highest value (Maximum Bar). The system produces as
output the hypothesized intended message of a bar chart as one of these 12 message
categories, along with the instantiated parameters of the message category, in the form
of a logical representation such as Maximum Bar(first bar) for the graphic in Figure 1
and Increasing Trend(first bar, last bar) for the graphic in Figure 3a.
3.3 Corpus of Graphics
We collected 82 groups of graphics along with their articles from 11 different magazines
(such as Newsweek and Business Week) and newspapers. These groups of graphics
varied in their structural organization: 60% consisted solely of a simple bar chart (e.g.,
the graphic in Figure 1 on Page 2) and 40% were composite graphics (e.g., the graphic
in Figure 8a in Section 7.1.1) consisting of at least one simple bar chart along with
other bar charts or other kinds of graphics (e.g., stacked bar charts or line graphs). We
selected at least one simple bar chart from each group and our corpus contained a total
of 107 bar charts. The Bayesian Inference System had an overall success rate of 79.1% in
recognizing the correct intended message for the bar charts in our corpus using leave-
one-out cross-validation (Elzer, Carberry, and Zukerman 2011).
533
Computational Linguistics Volume 38, Number 3
Figure 3
(a) Graphic conveying an increasing trend. (b) Graphic conveying the ranking of all bars.
In the work described in this article, we only used the bar charts whose intended
message was correctly recognized by the Bayesian Inference System and associated each
chart with the inferred message category. Here, our intent is to describe a generation
approach that works through a novel problem from beginning to end by handling a
multitude of generation issues. Thus, using bar charts with the perfect intention is
reasonably appropriate within the scope of the present work. For each bar chart, we
also used the XML representation that was utilized by the Bayesian Inference System.
Slightly less than half of the selected bar charts were kept for testing the system per-
formance (which we refer to as the test corpus), and the remaining graphs were used
for developing the system (which we refer to as the development corpus). Because the
number of graphics in the development corpus was quite limited, we constructed a
number of bar charts2 in order to examine the effects of individual salient features
observed in the graphics from the development corpus. These graphs, most of which
were obtained by modifying original graphics, enabled us to increase the number of
graphics in the development corpus and to explore the system behavior in various new
cases.
4. Content Identification Module (CIM)
Our ultimate goal is to generate a brief and coherent summary of a graphic. Identifying
and realizing the high-level informational content of a graphic is not an easy task,
however. First, a graphic depicts a large amount of information and therefore it would
be impractical to attempt to provide all of this information textually to a user. Second, a
graphic is chosen as the communication medium because a reader can get information
from it at many different levels. A casual look at the graphic is likely to convey the
intended message of the graphic and its salient features. At the same time, a reader
could spend much more time examining the graphic to further investigate something
of interest or something they noticed during their casual glance.
In order to address the task of identifying the content of a summary, we extend
to simple bar charts the insights obtained from an informal experiment where human
participants were asked to write a brief summary of a series of line graphs with the
same high-level intention (McCoy et al 2001). The most important insight gained from
2 The graphics that we constructed were not used in any of the evaluation experiments with human
participants described throughout this article.
534
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
this study is that the intended message of a graphic was conveyed in all summaries no
matter what the visual features of the graphic were. It was observed that the participants
augmented the intended message with salient features of the graphic (e.g., if a line
graph is displaying an increasing trend and the variance in that trend is large, then
the variance is salient) and that what was found salient depended on the graphic?s
intended message. Because the participants generated similar summaries for a par-
ticular graphic, we hypothesize that they perceived the same salient features for that
graphic. Although the set of features that might be salient is the same for different
graphics sharing the same underlying intention, the differences observed between the
summaries generated for different graphics with the same intention can be explained
by whether or not the features are salient in those graphics. The fact that the summaries
did not include all information that could be extracted from the graphic (such as
the value of every point in a line graph) but only visually salient features, correlates
with Grice?s Maxim of Quantity (1975) which states that one?s discourse contribution
should be as informative as necessary for the purposes of the current exchange but not
more so.
To extend these observations to constructing brief summaries of bar charts, we
hypothesize that (1) the intended message of the bar chart should form the core of
its textual summary and (2) the most significant and salient features of the bar chart,
which are related to its intended message, should be identified and included in that
summary. The inferred intended message of a bar chart serves as a starting point for
our content identification approach. In the rest of this section, we first describe a series
of experiments that we conducted to identify what constitutes the salient features of
a given bar chart and in which circumstances these features should be included in its
textual summary. We then present the content identification rules that were constructed
to automatically select appropriate content for the summary of a bar chart.
4.1 Experiments
We conducted a set of formal experiments to find patterns between the intended mes-
sage of a graphic, salient visual features of the displayed data, and the propositions
selected for inclusion in a brief summary. We identified the set of all propositions
(PROPALL) that capture information that we envisioned someone might determine
by looking at a bar chart. This set included a wide variety of pieces of information
present in a bar chart and contained propositions common to all bar charts as well
as propositions which were applicable only to some of the message categories. The
following is a subset of the identified propositions. In this example, Propositions 1?4
are common to all bar charts; in contrast, Propositions 5?8 are only present when the
bar chart is intended to convey a trend:
 The labels of all bars (Proposition 1)
 The value of a bar (Proposition 2)
 The percentage difference between the values of two bars (Proposition 3)
 The average of all bar values (Proposition 4)
 The range of the bar values in the trend (Proposition 5)
 The overall percentage change in the trend (Proposition 6)
535
Computational Linguistics Volume 38, Number 3
 The change observed at a time period (Proposition 7)
 The difference between the largest and the smallest changes observed in
the trend (Proposition 8)
Some propositions, which we refer to as open propositions, require instantiation
(such as Propositions 2, 3, and 7 given here) and the information that they convey varies
according to their instantiations.3 In addition, the instantiation of an open proposition
may duplicate another proposition. For example, if the Proposition 3 is instantiatedwith
the first and the last bars of the trend, then the information conveyed by that proposition
is exactly the same as Proposition 6.
To keep the size of the experiment reasonable, we selected 8 message categories
from among the 12 categories that could be recognized by the Bayesian Inference Sys-
tem; these categories were the onesmost frequently observed in our corpus and could be
used as a model for the remaining message categories. These categories were Increasing
Trend, Decreasing Trend, Changing Trend, Contrast Point with Trend, Maximum Bar,
Rank Bar, Rank All, and Relative Difference. In the experiments, we did not use the
categoriesMinimumBar (which can bemodeled viaMaximumBar), Relative Difference
with Degree (which can be modeled via Relative Difference), Stable Trend (which was
not observed in the corpus), and Present Data (which is the default category selected
when the system cannot infer an intended message for the graphic).
For each message category, we selected two to three original graphics from the
development corpus, where the graphics with the same intended message presented
different visual features. For example, we selected two graphics conveying that a par-
ticular bar has the highest value among the bars listed, but only in one of these graphics
was the value of the maximum bar significantly larger than the values of the other bars
(such as the graphic in Figure 1). In total, 21 graphics were used in the experiments and
these graphics covered all selected intended message categories. Because the number of
propositions applicable to each message category was quite large, 10?12 propositions
were presented for each graphic. Each graphic was presented to at least four partici-
pants. Overall, the experiments covered all selected intended message categories and
all identified propositions.
Twenty participants, who were unaware of our system, participated in the experi-
ments. The participants were graduate students or recent Ph.D. graduates from a variety
of departments at the University of Delaware. Each experiment started with a brief
description of the task, where the participants were told to assume that in each case
the graphic was part of an article that the user is reading and that the most important
information depicted in the graphic should be conveyed in its summary. They were also
told that they would be given an information graphic along with a sentence conveying
the intended message of the graphic and a set of propositions, and would be asked to
classify these additional propositions into one of three classes according to how impor-
tant they felt it was to include that proposition in the textual summary:4 (1) Essential:
This proposition should be included in the brief textual summary, (2) Possible: This
proposition could be included in the brief textual summary but it?s not essential, and (3)
Not Important: This proposition should not be included in the brief textual summary.
3 We used open propositions in order to keep PROPALL within a manageable size.
4 The participants were also asked to instantiate the open propositions that they classified as Essential or
Possible.
536
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
4.2 Analysis
To analyze the experiment?s results, we first assigned a numeric score to each
class indicating the level of importance assigned by the participants: Essential = 3,
Possible = 1, Not-important = 0. We then calculated an ?importance level? (IL) for each
proposition with respect to a particular graphic, where the importance level estimates
how important it is for that proposition to be included in the graphic?s summary.
The importance level of a proposition was computed by summing the numeric
scores associated with the classes assigned by the participants. For example, if three
participants classified a proposition as Essential and two participants as Possible, the
importance level of that proposition in the graphic was (3? 3) + (2? 1) = 11. In cases
where a proposition (Prop A) and an instantiated open proposition which conveyed
the same information were classified by a participant into different classes for the same
graphic, the classification of the proposition that came earlier in the presentation was
used in computing the importance level of Prop A.
Given these computed scores, we needed to identify which propositions to con-
sider further for inclusion in a summary. Because there was a divergence between
the sets of propositions that were classified as essential by different participants, we
decided to capture the general tendency of the participants. For this purpose, we
defined majority importance level as a ranking criteria, which is the importance level
that would be obtained if half of the participants classify a proposition as essential.
For example, the majority importance level would be (6? 3)/2 = 9 if there were six
participants. We classified a proposition as a highly rated proposition if its importance
level was equal to or above the majority importance level.5 The propositions that were
classified as highly rated for the graphics with a message category formed the set of
highly rated propositions that should be considered for inclusion for that message
category.
We had to ensure that the propositions presented to the participants (PROPALL)
actually covered all information that is important enough to include in the summary of
a bar chart. Thus, for each graphic, we also asked participants if there was anything else
they felt should be included in the brief summary of the graphic. We received only a
few isolated suggestions such as a proposition conveying what type of a curve could fit
the trend. Moreover, these suggestions were not common among the participants, and
nothing was mentioned by more than one participant (indeed most did not make any
suggestions). Thus, we concluded that these suggestions were not appropriate for the
textual summary of a bar chart.
4.3 Content Identification Rules for Message Categories
Using the importance level scores, we needed to identify the subset of the highly rated
propositions that should be included in the textual summary in addition to the graphic?s
intended message. For each message category, we examined the similarities and the
differences between the sets of highly rated propositions identified for the graphics
5 The reason behind assigning particular scores (3,1,0) to the classes is to guarantee that a proposition will
not be selected as a highly rated proposition if none of the participants thought that it was essential.
Assume k participants classified a proposition (Prop A). The majority importance level of this proposition
(MIL(Prop A)) is (3? k)/2. A proposition is classified as highly rated if its importance level (IL(Prop A))
is equal to or greater than the majority importance level (IL(Prop A) ?MIL(Prop A)). If all of the
participants classified the proposition as Possible, the IL(Prop A) is 1? k, which is less than MIL(Prop A).
537
Computational Linguistics Volume 38, Number 3
associated with that message category, related these differences to the visual features
present in these graphics, and constructed a set of content identification rules for
identifying propositions to be included in the summary of a graphic from that message
category. If a proposition was marked as highly rated for all graphics in a particular
message category, then its selection was not dependent on particular visual features
present in these graphics. In such cases, our content identification rule simply states that
the proposition should be included in the textual summary for every graphic whose
inferred message falls into that message category. For the other propositions that are
highly rated for only a subset of the graphics in a message category, we identified a fea-
ture that was present in the graphics where the proposition was marked as highly rated
and was absent when it was not marked as highly rated, and our content identification
rules use the presence of this feature in the graphic as a condition for the proposition
to be included in the textual summary. In addition, we observed that a highly rated
proposition for a message category might require inclusion of another proposition for
realization purposes. For example, in the Rank All message category, the proposition
indicating the rank of each bar was identified as highly rated and thus could be included
in the textual summary. Because the rank of a bar cannot be conveyed without its label,
we added the proposition indicating the label of each bar to the content identification
rule containing the rank proposition, although this extra proposition was not explicitly
selected by the participants for inclusion. Notice that these steps?identifying features
that distinguish one subset of graphs from the other and identifying propositions that
need to be included for realizing other propositions?make it difficult to use machine
learning for this task. In our case the number of possible features that can be extracted
from a graphic is very large and it is difficult to know which features from among
those may be important/defining in advance. In addition, the number of graphics in
our development corpus is too small to expect machine learning to be effective.
The following are glosses of two partial sets of representative content identification
rules. The first set is applicable to a graphic conveying an increasing trend and the
second set is applicable to a graphic conveying the rankings of all bars present in the
graph:
 Increasing Trend message category:6
1. If (message category equals ?increasing trend?) then
include(proposition conveying the rate of increase of the trend):
Include the proposition conveying the rate of increase of the trend
2. If (message category equals ?increasing trend?) and
notsteady7(trend) then include(proposition conveying the
period(s) with a decrease):8
If the trend is not steady and has variability, then include the proposition
indicating where the trend varies
6 The ?notsteady? function returns true if its argument is not a steady trend; the ?value? function returns
the values of all members of its argument; the ?greaterthan? function returns true if the left argument is
greater than the right argument; the ?withinrange? function returns true if all members of its left
argument are within the range given by its right argument; the ?average? function returns the average of
the values of all members of its argument.
7 A trend is unsteady if there is at least one period with a decrease in contrast with the increasing trend.
8 The inclusion of propositions whose absence might lead the user to draw false conclusions is consistent
with Joshi, Webber, and Weischedel?s (1984) maxim, which states that a system should not only produce
correct information but should also prevent the user from drawing false inferences.
538
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
3. If (message category equals ?increasing trend?) and (value(last bar)
greaterthan (3*value(first bar))) then include(proposition
conveying the overall percentage increase in the trend):
If the overall percentage increase in the trend is significantly large, then
include the proposition conveying the percentage increase in the trend
 Rank All message category:
1. If (message category equals ?rank all?) then include(propositions
conveying the label and the value of the highest bar):
Include the propositions conveying the label and the value of the
highest bar
2. If (message category equals ?rank all?) and (value(all bars)
withinrange ((0.7*average(all bars)),(1.3*average(all bars)))) then
include(proposition indicating that the bar values vary slightly):
If the values of bars are close to each other, then include the proposition
indicating that the bar values vary slightly
3. If (message category equals ?rank all?) and (not(value(all bars)
withinrange ((0.7*average(all bars)),(1.3*average(all bars))))) then
include(propositions conveying the label and the value of the
lowest bar):
If the values of bars are not close to each other, then include the
propositions conveying the label and the value of the lowest bar
We defined the conditions of all content identification rules as a conjunction of one
or more expressions where some expressions required us to determine threshold values
to be used for comparison purposes. For example, we observed that the proposition
conveying the overall percentage change in the trend was marked as highly rated
only for graphics which depicted a significant change in the trend. We handled this
situation for graphics with an increasing trend by defining the third content identi-
fication rule (shown earlier) where we needed to set the lowest threshold at which
an overall increase observed in a trend can be accepted as significantly large. For
setting such threshold values, we examined all graphs in the development corpus
to which the corresponding content identification rule is applicable (i.e., the graphs
associated with the message category for which the rule is defined) and used our
intuitions about whether the proposition captured by the rule should be selected for
inclusion in the summaries of these graphs. We set the threshold values using the
results obtained from group discussions such that the final setting classified all of
the original graphics the way the participants did in the experiments described in
Section 4.1.
When the content identification rules constructed for the Increasing Trend message
category are applied to the bar chart in Figure 3a, the following pieces of information
are selected for inclusion in addition to the intended message of the graphic:
 The rate of increase of the trend, which is slight
 The small drop observed in the year 1999
 The overall percentage increase in the trend, which is 225%
539
Computational Linguistics Volume 38, Number 3
When the content identification rules constructed for the RankAll message category
are applied to the bar chart in Figure 3b, the following pieces of information are selected
for inclusion in addition to the intended message of the graphic:
 The label and the value of the highest bar, which is Army with 233,030
 The label and the value of the lowest bar, which is Other defense agencies
with 100,678
 The label and the ranking of each bar:9 Army is the highest, Navy is the
second highest, Air Force is the third highest, and Other defense agencies
is the lowest
4.4 Evaluation of the Content Identification Module
We conducted a user study to assess the effectiveness of our content identification
module in identifying the most important information that should be conveyed about
a bar chart. More specifically, the study had three goals: (1) to determine whether the
set of highly rated propositions that we identified for each message category contains
all propositions that should be considered for inclusion in the summaries of graphics
with that message category; (2) to determine how successful our content identification
rules are in selecting highly rated propositions for inclusion in the summary; and (3)
to determine whether the information conveyed by the highly rated propositions is
misleading or not.
Nineteen students majoring in different disciplines (such as Computer Science and
Materials Science and Engineering) at the University of Delaware were participants
in the study. These students neither participated in the earlier study described in Sec-
tion 4.1 nor were aware of our system. Twelve graphics from the test corpus (described
in Section 3.3) whose intended message was correctly identified by the Bayesian Infer-
ence System were used in the experiments. Once the intended message was recognized,
the corresponding content identification rules were executed in order to determine the
content of the graphic?s summary. Prior to the experiment, all participants were told
that they would be given a summary and that it should include the most important
information that they thought should be conveyed about the graphic. Each participant
was presented with three graphics from among the selected graphics such that each
graphic was viewed by at least four participants. For each graphic, the participants
were first given the summary of the graphic generated by our approach and then shown
the graphic. The participants were then asked to specify if there was anything omitted
that they thought was important and therefore should be included in the summary. In
addition, the participants were asked to specify whether or not they were surprised
or felt that the summary was misleading (i.e., whether the bar chart was similar to
what they expected to see after reading its summary). Note that our summaries with
relatively few propositions are quite short. Thus our evaluation focused on determin-
ing whether anything of importance was missing from the summary or whether the
summary was misleading. In the experiments, we did not ask the participants to rate
9 This piece of information is selected by a rule defined for the Rank All message category not shown in the
bulleted list on the previous page.
540
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
the content of summaries on a numeric scale in order to restrict them to evaluating only
the selected content as opposed to its presentation (i.e., the organization and realization
of the summary).
Feedback that we received from the participants was very promising. In most of
the cases (43 out of 57 cases), the participants were satisfied with the content that our
approach selected for the presented bar charts. There were a number of suggestions for
what should be added to the summaries in addition towhat had already been conveyed,
and in a couple of these cases, we observed that a highly rated proposition which was
not selected by the corresponding content identification rule was contrarily suggested
by the participants. There was no consensus in these suggestions, however, as none
was made by more than two participants. Some of the participants (3 out of 19) even
commented that we provided more information than they could easily get from just
looking at the graphic. In addition, a few participants (2 out of 19) commented that,
in some graphics, they didn?t agree with the degree (e.g., moderate or steep) assessed
by our approach for differences between bar values (e.g., the rate of change of the
trend), and therefore they thought the summary was misleading. Because there wasn?t
any common consensus among the participants, we didn?t address this very subjective
issue. Overall, we conclude that the sets of highly rated propositions that we identified
contain the most important information that should be considered for inclusion in the
summaries of bar charts and that our system effectively selects highly rated propositions
for inclusion when appropriate.
5. Text Structuring and Aggregation Module (TSAM)
A coherent text has an underlying structure where the informational content is pre-
sented in some particular order. Good text structure and information ordering have
proven to enhance the text?s quality by improving user comprehension. For example,
Barzilay, Elhadad, and McKeown (2002) showed that the ordering has a significant
impact on the overall quality of the summaries generated in theMULTIGEN system. Al-
though previous research highlights a variety of structuring techniques, there are three
prominent approaches that we looked to for guidance: top?down planning, application
of schemata, and bottom?up planning.
In top?down planning (Hovy 1988, 1993; Moore and Paris 1993), the assumption is
that a discourse is coherent if the hearer can recognize the communicative role of each
of its segments and the relation between these segments (generally mapped from the
set of relations defined in rhetorical structure theory (RST; Mann and Thompson 1987).
The discourse is usually represented as a tree-like structure and the planner constructs
a text plan by applying plan operators starting from the initial goal.
In the TEXT system (McKeown 1985), a collection of naturally occurring texts
were analyzed to identify certain discourse patterns for different discourse goals, and
these patterns were represented as schemas which are defined in terms of rhetorical
predicates. The schemas both specify what should be included in the generated texts
and how they should be ordered given a discourse goal. Lester and Porter (1997)
used explanation design packages, schema-like structures with procedural constructs
(for example, the inclusion of a proposition can be constrained by a condition), in
the KNIGHT system, which is designed to generate explanations from a large-scale
biology knowledge base. Paris (1988) applied the idea of schemata in the TAILOR
system to tailor object descriptions according to the user?s level of knowledge about
the domain.
541
Computational Linguistics Volume 38, Number 3
Marcu (1998) argued that text coherence can be achieved by satisfying local con-
straints on ordering and clustering of semantic units to be realized. He developed a
constraint satisfaction based approach to select the best plan that can be constructed
from a given set of textual units and RST relations between them, and showed that
such bottom?up planning overcomes the major weakness of top?down approaches
by guaranteeing that all semantic units are subsumed by the resulting text plan. The
ILEX system (O?Donnell et al 2001), which generates descriptions for exhibits in a
museum gallery, utilizes a similar bottom?up planning approach (Mellish et al 1998)
where the best rhetorical structure tree over the semantic units is used as the text
structure.
Because our content identification rules identify a set of propositions to be con-
veyed, it appears that a bottom?up approach that ensures that all propositions will be
included is in order. At the same time, it is important that our generated text adheres to
an overall discourse organization such as is provided by the top?down approaches.
Because of the nature of the propositions (the kinds of rhetorical relations that can
exist between propositions in a descriptive domain are arguably limited [O?Donnell
et al 2001]), however, a structure such as RST is not helpful here. Thus, the top?down
planning approach does not appear to fit. Although something akin to a schema might
work, it is not clear that our individual propositions fit into the kind of patterns used
in the schema-based approach. Instead we use what can be considered a combination
of a schema and a bottom?up approach to structure the discourse. In particular, we
use the notion of global focus (Grosz and Sidner 1986) and group together proposi-
tions according to the kind of information they convey about the graphic. We define
three proposition classes (message-related, specific, and computational) to classify the
propositions selected for inclusion in a textual summary. The message-related class
contains propositions that convey the intended message of the graphic. The specific
class contains the propositions that focus on specific pieces of information in the
graphic, such as the proposition conveying the period with an exceptional drop in a
graphic with an increasing trend or the proposition conveying the period with a change
which is significantly larger than the changes observed in other periods in a graphic
with a trend. Lastly, propositions in the computational class capture computations or
abstractions over the whole graphic, such as the proposition conveying the rate of
increase in a graphic with an increasing trend or the proposition conveying the overall
percentage change in the trend. In our system, all propositions within a class will
be delivered as a block. But we must decide how to order these blocks with respect
to each other. In order to emphasize the intended message of the graphic (the most
important piece of the summary), we hypothesize that the message-related propositions
should be presented first. We also hypothesize that it is appropriate to close the textual
summary by bringing the whole graphic back into the user?s focus of attention (Grosz
and Sidner 1986) (via the propositions in the computational class). Thus, we define an
ordering of the proposition classes (creating a partial ordering over the propositions)
and present first the message-related propositions, then the specific propositions, and
finally the computational propositions. Section 6 will address the issue of ordering the
propositions within these three classes.
5.1 Representing Summary Content
First we needed to have a representation of content that would provide us with the
most flexibility in structuring and realizing content. For this we used a set of basic
propositions. These were minimal information units that could be combined to form
542
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
the intended message and all of the propositions identified in our content identification
rules. This representation scheme increases the number of aggregation and realization
possibilities that could be explored by the system, which is described in the next
subsection. We defined two kinds of knowledge-base predicates to represent the basic
propositions:
(1) Relative Knowledge Base: These predicates are used to represent the basic
propositions which introduce graphical elements or express relations
between the graphical elements.
(2) Attributive Knowledge Base: These predicates are used to represent the
basic propositions which present an attribute or a characteristic of a
graphical element.
Each predicate contains at least two arguments and we refer to the first argument
as the main entity and the others as the secondary entities. The main entity of each
predicate is a graphical element and the secondary entities are either a string constant
or a graphical element. Some of the graphical elements that we used in this work are as
follows:
 graphic: ?the graphic itself?
 trend: ?the trend observed in the graphic?
 descriptor: ?a referring expression that represents what is being measured
in the graphic?10
 bar(x): ?a particular bar in the graphic?
1 <= x <= n where n = number of bars in the graph
 all bars: ?all bars depicted in the graphic? bset = {bar(x) | 1 <= x <= n}
 period(x,y): ?a period depicted in the graphic? 1 ? x < n and 1 < y ? n
 change(x,y): ? the change between the values of any two bars?
1 ? x < n and 1 < y ? n
 all changes: ?changes between all pairs of bars of the graphic?
cset = {change(x, y) | 1 ? x < n, 1 < y ? n}
 trend period: ?the period over which the trend is observed?
 graph period: ?the period which is depicted by the graphic?
 trend change: ?the overall change observed in the trend?
Table 1 presents sample instantiations of a subset of the predicates that we defined
for this work along with a possible realization for each instantiation. Although the
number of arguments in Relative Knowledge Base predicates (predicates 1 to 15) varies,
10 How that referring expression is extracted from the text associated with the graphic is described in
detail in Section 7.1. For example, the descriptor identified by our system for the graphic in Figure 4
is the dollar value of net profit.
543
Computational Linguistics Volume 38, Number 3
Table 1
Sample instantiations and possible realizations of a subset of our predicates.
1 shows(graphic,trend)
The graphic shows a trend
2 focuses(graphic,bar(3))
The graphic is about the third bar
3 covers(graphic, graph period)
The graphic covers the graph period
4 exists(trend,descriptor)
The trend is in the descriptor
5 has(trend,trend period)11
The trend is over the trend period
6 starts(trend period,?2001?)
The trend period starts at the year 2001
7 ends(trend period,?2010?)
The trend period ends at the year 2010
8 ranges(descriptor,?from?,?20 percent?,trend period)
The descriptor ranges from 20 percent over the trend period
9 hasextreme(descriptor,?largest?,change(3,4),period(3,4))
The descriptor shows the largest change between the third and the fourth bars
10 averages(descriptor,all bars,?55.4 billion dollars?)
The descriptor for all bars averages to 55.4 billion dollars
11 comprises(descriptor,trend change,trend period)
The descriptor comprises a trend change over the trend period
12 occurs(change(2,3),period(2,3))
A change occurs between the second and the third bars
13 hasdifference(change(1,5),bar(1),bar(5),descriptor)
A difference is observed between the descriptor of the first bar and that of the fifth bar
14 observed(all changes,?every?,interval,trend period)
Changes are observed every interval over the trend period
15 presents(descriptor,bar(3),?12 percent?)
The descriptor for the third bar is 12 percent
16 hasattr(trend change,?type?,?increase?)
The trend change is an increase
17 hasattr(change(2,3),?degree?,?moderate?)
The change is of degree moderate
18 hasattr(change(2,3),?amount?,?70 dollars?)
The change amount is 70 dollars
19 hasattr(trend change,?percentage amount?,?22 percent?)
The trend change percentage amount is 22 percent
20 hasattr(all changes,?rate?,?slight?)
Changes are slight changes
all Attributive Knowledge Base predicates (encoded as hasattr) consist of three argu-
ments, where the first argument is the graphical element being described, the second
is an attribute of the graphical element, and the third is the value of that attribute
(predicates 16 to 20).12
11 Notice that the graphical element trend period in 5 is the main entity in 6 and 7. These all might be
combined using the And operator to produce the realization The trend starts at the year 2001 and ends
at the year 2010.
12 Because all Attributive Knowledge Base predicates have the same form, the amount and unit of a change
are represented as a single string which is derived from the textual components of the graphic (such as
70 dollars in Predicate 18).
544
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 4
Graphic conveying a decreasing trend.
For example, consider how the propositions given in Section 4.1 can be represented
with the predicates shown in Table 1. Some of those propositions require a single
predicate. For example, the proposition conveying the value of a bar (Proposition 2)
can be represented via the predicate ?presents? (Predicate 15) and the proposition
conveying the average of all bar values (Proposition 4) via the predicate ?aver-
ages? (Predicate 10). On the other hand, some propositions require more than one
predicate. For example, the proposition conveying the overall percentage change
in the trend shown in Figure 4 (Proposition 6) can be represented via the predi-
cates ?comprises(descriptor,trend change,trend period)? (Predicate 11), ?starts(trend
period,?1998?)? (Predicate 6), ?ends(trend period,?2006?)? (Predicate 7), ?hasattr(trend
change,?type?,?decrease?)? (Predicate 16), and ?hasattr(trend change,?percentage
amount?,?65 percent?)?(Predicate 19). The same set of predicates can be used to rep-
resent the overall amount of change in the trend by replacing the constant ?percentage
amount? with the string ?amount? in Predicate 19.
As is shown by the possible realizations included in Table 1, each basic proposition
can be realized as a single sentence. Although we determined a couple of different
ways (i.e., simple sentences) of realizing each basic proposition, our current imple-
mentation always chooses a single realization (which we refer to as ?the realization
associated with the proposition?) and the main entity is always realized in subject
position.13
5.2 Aggregating Summary Content
The straightforward way of presenting the informational content of a summary is to
convey each proposition as a single sentence while preserving the partial ordering of
the proposition classes. The resultant text would not be very natural and coherent,
however. Aggregation is the process of removing redundancies during the generation
of a more concise and fluent text (Shaw 1998; Dalianis 1999). Aggregation (typically
syntactic aggregation [Reiter and Dale 2000]) has received considerable attention
from the NLG community (McKeown et al 1997; O?Donnell et al 2001; Barzilay and
Lapata 2006), and has been applied in various existing generation systems such as the
intelligent tutoring application developed by Di Eugenio et al (2005). Our aggregation
mechanism works to combine propositions into more complex structures. It takes
13 We leave it as a future work to explore how different realizations for a proposition, including ones
where the main entity is not in subject position, can be utilized by our approach.
545
Computational Linguistics Volume 38, Number 3
advantage of the two types of predicates (Relative Knowledge Base and Attributive
Knowledge Base predicates) and the shared entities between predicates. In order to
relate propositions and explore syntactically aggregating them, our mechanism treats
each proposition as a single node tree which can be realized as a sentence and attempts
to form more complex trees by combining individual trees via four kinds of operators
in such a way so that the more complex tree (containing multiple propositions) can still
be realized as a single sentence. The first operator (Attribute Operator) works only on
propositions with an Attributive Knowledge Base predicate and essentially identifies
opportunities to realize such a proposition as an adjective attached to a noun object
in the realization of another proposition. The remaining three operators, which do
not work on propositions with an Attributive Knowledge Base predicate, introduce
new nodes corresponding to operational predicates (And, Same, and Which) with a
single entity into the tree structures. Two of these operators (And Operator and Which
Operator) work on trees rooted by a proposition with a Relative Knowledge Base or an
And predicate. These operators look for opportunities for VP conjunction and relative
clauses, respectively. The third operator (Same Operator) works on trees rooted by a
proposition with a Relative Knowledge Base predicate and identifies opportunities for
NP conjunction. Although each predicate is associated with a unique realization in the
current implementation, none of these operators depend on how the corresponding
predicates or the entities in those predicates are realized.
Having defined the operators we next had to turn to the problem of determining
how these operators should be applied (e.g., which combinations are preferred). The
operators we defined are similar to the clause-combining operations used by the SPoT
sentence planner (Walker, Rambow, and Rogati 2002; Stent, Prasad, and Walker 2004;
Walker et al 2007) in the travel planner system AMELIA. In AMELIA, for each of
the 100 different input text plans, a set of possible sentence plans (up to 20 plans)
were generated by randomly selecting which operations to apply according to assumed
preferences for operations. These possible sentence plans were then rated by two judges
and the collected ratings were used to train the SPoT planner. Although we greatly
drew from the work on SPoT as we developed our aggregation method, we chose
not to follow their learning methodology. In the SPoT system, some of the features
were domain- and task-dependent and thus porting to a new domain would require
retraining. In addition, the judgments of the two raters were collected in isolation and
it is unclear how these would translate to the task situation the texts were intended
for. Although this methodology was innovative and necessary for SPoT because of
the large number of possible text plans, we chose to select the best text plan on the
basis of theoretically informed complexity features balancing sentence complexity and
number of sentences. Because our text plans are significantly more constrained, it is
possible to enumerate each of them and choose the one that best fits our rating cri-
teria.14 This has the added benefit of better understanding the complexity features by
evaluating the resulting text. In addition, our method would be open to both upgrad-
ing the selection criteria and adding further aggregation operators without requiring
retraining.15
14 Although in our implementation we do enumerate all plans before the rating criteria are applied to select
the best one, it is in principle possible to generate the text plans in an order that would allow maximizing
the scoring functions without first enumerating all possibilities. This is left for future work.
15 Such modifications and additions need to be empirically evaluated with empirical data, however.
546
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Operator: Attribute Operator
Gloss: This operator attaches a single node tree that consists solely of a proposition with
an Attributive Knowledge Base predicate, as a direct subchild of a node N with a Relative
Knowledge Base predicate in another tree, if the main entity of the Attributive Knowledge Base
predicate is an entity (main or secondary) for the proposition at node N.
Input: T1 and T2
Constraints:
1. (pred(T1-root)==?hasattr?)
2. ((pred(T2-node)!=?hasattr?) ? (pred(T2-node)!=?And?) ?
(pred(T2-node)!=?Which?))
3. ((main ent(T1-root)==main ent(T2-node)) ?
(main ent(T1-root)==secondary ent(T2-node)))
Output: A modified T2 such that
1. left child(T2-node)?T1
Glossary:
1. Tx-root: the root node of tree Tx
2. Tx-node: any node in tree Tx (including Tx-root)
3. pred(Tx-node): the predicate at Tx-node
4. left/right child(Tx-node): the leftmost/rightmost child of Tx-node
5. main/secondary ent(Tx-node): the main/secondary entity of the proposition at Tx-node
6. !=: not equal, ==: equal, !: not,?: assign
Operator: And Operator
Gloss: This operator combines two trees if the propositions at their root share the same main
entity. A proposition containing an And predicate with the same main entity forms the root of
the new tree and the trees that are combined form the immediate descendents of this root.
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?) ? (pred(T2-root)!=?hasattr?))
2. !((pred(T1-root)==?And?) ? (pred(T2-root)==?And?))
3. (main ent(T1-root)==main ent(T2-root))
Output: a new tree T3 where the root node has two immediate children such that
1. pred(T3-root)??And? ?main ent(T3-root)?main ent(T1-root)
2. left child(T3-root)?T1 ? right child(T3-root)?T2
Operator:Which Operator
Gloss: This operator attaches a tree (Tree A) as a descendent of a node N in another tree (Tree B)
via a Which predicate, if the main entity of the proposition at the root of Tree A is a secondary
entity for the proposition at node N of the other tree (Tree B). That particular entity forms the
main entity of the Which predicate. Thus, Tree A will be an immediate child of the node with
the Which predicate and the node with the Which predicate will be an immediate child of node
N in Tree B.
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?)?(pred(T2-node)!=?hasattr?))
2. (main ent(T1-root)==secondary ent(T2-node))
Output: A modified T2 via the addition of a new node (Node x) with a single immediate child
such that
1. pred(Node x)? ?Which? ?main ent(Node x)?main ent(T1-root)
2. right child(T2-node)?Node x ? left child(Node x)?T1
547
Computational Linguistics Volume 38, Number 3
Operator: Same Operator
Gloss: This operator combines two trees if the propositions at their root contain the same
predicate but the main entities of these predicates are different. A proposition with a Same
predicate forms the root of the new tree, and the trees that are combined form the descendents
of this root. Since the descendents of the new tree have different main entities, the main entity of
the Same predicate is some unique element not occurring elsewhere in the tree. For instance, in
our implementation this element is obtained by appending a unique number, which isn?t used
in another Same predicate in the current forest, to the term random (such as random0).
Input: T1 and T2
Constraints:
1. ((pred(T1-root)!=?hasattr?)?(pred(T2-root)!=?hasattr?) ?
(pred(T1-root)!=?And?) ? (pred(T2-root)!=?And?) ?
(pred(T1-root)!=?Same?) ? (pred(T2-root)!=?Same?))
2. (pred(T1-root)==pred(T2-root))
3. (main ent(T1-root)!=main ent(T2-root))
Output: a new tree T3 where the root node has two immediate children such that
1. pred(T3-root)??Same? ?main ent(T3-root)? a unique element
2. left child(T3-root)?T1 ? right child(T3-root)?T2
In our work, we thus developed a method that would choose a text plan on prin-
cipled reasoning concerning the resulting text. In particular, we looked to balance sen-
tence complexity and the number of sentences in the generated text. Moreover, whereas
such a method was not applicable in the case of SPoT (because of the significantly larger
set of operators with few constraints resulting in potential text plans too numerous to
evaluate), our work differs in several aspects that make it reasonable to generate all
text plans and apply an evaluation metric. First, our system has a small number of
aggregation operators and all operators cannot be applied to all kinds of predicates (e.g.,
the Attribute Operator cannot be applied to the Relative Knowledge Base predicates).
Second, the number of possible sets of basic propositions that our system needs to
organize is significantly lower than the number of possible text plans that the SPoT
planner needs to consider. Finally, although it is not practical in SPoT to list all possible
sentence plans that might be generated for a particular text plan (since the possibilities
are too great), generating all possible combinations of propositions in a proposition class
(such as message related class) is practical in our work. This is due to the fact that the
number of basic propositions in each class is fairly small (e.g., usually between 5 to
15 propositions) and that the nature of the operators and constraints that we put on
their application enable us to prune the space of possible combinations. Some of these
constraints are: (1) The And Operator produces only one complex tree from a pair of
trees and it cannot combine two trees if both trees have a proposition with an And
predicate at their roots (thus we limit the number of conjuncts in a conjoined sentence
to three at most16), and (2) the Attribute Operator produces only one complex tree in
cases where a single node tree (Tree A) can be attached as a direct subchild of more than
one node in another tree (Tree B); the parent of Tree A is the first such node found by
preorder traversal of Tree B.
Our implementation first generates all possible text plans for the propositions
within each class (message-related, specific, and computational). Each text plan is
represented as a forest where each tree in the forest represents a sentence. Initially,
each proposition class is treated as a forest consisting of all single node trees in that
class (initial candidate forest), and the operators are applied to that forest in order
to produce new candidate forests for the proposition class. Anytime two trees in a
16 We set this limit in order to avoid sentences that are too complex to comprehend.
548
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 5
A candidate forest for each proposition class.
candidate forest are combined via an operator, a new candidate forest is produced;
the new candidate forest is added to the existing set of candidate forests, thereby
increasing the number of candidate forests. Within each class, our approach first applies
the And Operator to all possible pairs of trees in the initial candidate forest, which
produces new candidate forests. The Same Operator is then applied to all possible
pairs of trees in each candidate forest. Similarly, the Which Operator and the At-
tribute Operator are applied to trees in the candidate forests produced earlier. The
result of this aggregation is a number of candidate forests with one or more trees
(each using different aggregation) for each of the proposition classes. For example,
Figure 5 shows one candidate forest that can be constructed for each proposition class
by applying these operators to the propositions selected for the graphic in Figure 4,
where each forest resulting from the aggregation consists of a single tree.17 In this
example, the Attributive Knowledge Base predicates (*) are attached to their parents
by the Attribute Operator, the nodes containing And predicates (**) are produced
by the And Operator, and the Which predicates (***) are produced by the Which
Operator.
17 The nodes represented with black circles correspond to the individual predicates. These individual
predicates within each class form the single node trees upon which the operators work.
549
Computational Linguistics Volume 38, Number 3
5.3 Evaluating a Text Structure
Different combinations of operators produce different candidate forests in each propo-
sition class and consequently lead to different realized text with a different complexity
of sentences. The set of candidate forests for each proposition class must be evaluated
to determine which one is best. Our objective is to find a forest that would produce
text which stands at a midpoint between two extremes: a text where each proposition
is realized as a single sentence and a text where groups of propositions are realized
with sentences that are too complex. Our evaluation metric to identify the best forest
leverages different considerations to balance these extremes. The first two criteria are
concerned with the number and syntactic complexity of sentences that will be used to
realize a forest. The third criteria takes into consideration how hard it is to comprehend
the relative clauses embedded in these sentences. The insights that we use in selecting
the best forest (e.g., balancing semantic importance, overall text structure, aggregation,
and readibility due to sentence complexity) represent our novel contributions to the
text structuring and aggregation literature. The theory that underlies our evaluation
metric (i.e., what it is we are balancing in the generation) is widely applicable to other
data-to-text generation domains because it uses general principles from the literature
and has the potential to be improved.
5.3.1 Sentence Complexity. Each tree (single node or complex) in a forest represents a
set of propositions that can be realized as a single sentence. Our aggregation rules
enable us to combine these simple sentences into more complex syntactic structures.
In the literature, different measures to assess syntactic complexity of written text and
spoken language samples have been proposed, with different considerations such as
the right branching nature of English (Yngve 1960) and dependency distance between
lexemes (Lin 1996). We apply the revised D-level sentence complexity scale (Covington
et al 2006) as the basis of our syntactic complexity measure. The D-level scale measures
the complexity of a sentence according to its syntactic structure and the sequence in
which children acquire the ability to use different types of syntactic structures. The
sentence types with the lowest score are those that children acquire first and there-
fore are the simplest types. Eight levels are defined in the study, some of which are
simple sentences, coordinated structures (conjoined phrases or sentences), non-finite
clause in adjunct positions, and sentences with more than one level of relative clause
embedding.
Among the eight levels defined in that study, the levels of interest in our work
are simple sentences, conjoined sentences, sentences with a relative clause modifying
the object of the main verb, non-finite clauses in adjunct positions, and sentences
with more than one level of embedding. However, the definition of sentence types at
each level is too general. For example, the sentences There is a trend and There is a
trend in the dollar value of net profit over the period from the year 1998 to the year 2006
are both classified as simple sentences with the lowest complexity score under the
D-level classification. We argue that although these sentences have a lower complexity
than the sentences with higher D-level scores, their complexities are not the same. We
make a finer distinction between sentence types defined in the D-level classification
and use the complexity levels shown in Table 2. For example, according to our clas-
sification, a simple sentence with more than one adjunct or preposition has a higher
complexity than a simple sentence without an adjunct. We preserve the ordering of
the complexity levels in the D-level classification. For example, in our classification,
550
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Table 2
Our syntactic complexity levels.
Complexity Syntactic Form
Level 0 Simple sentence with up to one prepositional phrase or adjunct
Level 1 Simple sentence with more than one prepositional phrase or adjunct
Level 2 Conjoined sentence (two simple sentences?Level 0 or 1)
Level 3 Conjoined sentence (more than two simple sentences?Level 0 or 1)
Level 4 Sentence with one level of embedding
(relative clause that is modifying object of main verb)
Level 5 Non-finite clause in adjunct positions
Level 6 Sentence with more than one level of embedding
Levels 0 and 1 correspond to the class of simple sentences in the D-level classification
and have a lower complexity than Levels 2 and 3, which correspond to the class of
coordinated structures with a higher complexity than simple sentences in the D-level
classification.
Each basic proposition in our system can be realized as a simple sentence containing
at most one prepositional phrase or adjunct.18 Each single node tree with a Relative or
Attributive Knowledge Base predicate at its root has the lowest syntactic complexity
(Level 0) in this classification.
Themost straightforwardway of realizing amore complex tree would be conjoining
the realizations of subtrees rooted by a proposition with an And or a Same predicate,
embedding the realization of a subtree rooted by a proposition with a Which predicate
as a relative clause, and realizing a subtree that consists solely of a proposition with
an Attributive Knowledge Base predicate as an adjective or a prepositional phrase.
For example, the tree rooted by shows(graphic,trend) in Figure 5 can be realized as
The graphic shows a decreasing trend, which is in the dollar value of net profit and is over
the period, which starts at the year 1998 and ends at the year 2006. The resultant text
is fairly complicated, however, and a more sophisticated realization would likely
lead to a lower syntactic complexity score. We defined a number of And predicate
and Which predicate complexity estimators to look for realization opportunities in
a complex tree structure so that a syntactic complexity score which is lower than
what the most straightforward realization would produce can be assigned to that tree.
These estimators compute the syntactic complexity of a complex tree by examining
the associated realizations of all aggregated propositions in that tree in a bottom?up
fashion. Because the complex trees that are rooted by a proposition with a Same
predicate would always be realized as a conjoined sentence (Level 2), we did not define
complexity estimators for this kind of predicate.
The And predicate complexity estimators check whether or not the realizations
of two subtrees rooted by a proposition with an And predicate can be combined into
a simple sentence (Level 1), or a conjoined sentence which consists of two independent
sentences (Level 2) if one of the subtrees is rooted by a proposition with an And
predicate. For example, the And predicate estimators can successfully identify the
18 In the current implementation, there is a single realization associated with each basic proposition with
the main entity in subject position.
551
Computational Linguistics Volume 38, Number 3
following realization opportunities (based on the representations of the sentences as
propositions):
 The period starts at the year 1998. AND The period ends at the year 2006.
can be combined into:
The period is from the year 1998 to the year 2006. (Level 1)
 The trend is in the dollar value of net profit. AND The trend is over the period
from the year 1998 to the year 2006. can be combined into:
The trend is in the dollar value of net profit over the period from the year 1998
to the year 2006. (Level 1)
 The dollar value of net profit ranges from 2.77 billion dollars over the period.
AND The dollar value of net profit ranges to 0.96 billion dollars over the
period. AND The dollar value of net profit shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001. can be
combined into:19
The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the
period and shows the largest drop of about 0.56 billion dollars between the
year 2000 and the year 2001. (Level 2)
The Which predicate complexity estimators check whether a tree rooted by a
proposition with a Which predicate can be realized as a simple adjunct or a prepo-
sitional phrase attached to the modified entity rather than a more complex relative
clause (which could increase the complexity level). For example, the Which predicate
estimators can successfully identify the following realization opportunities (based on
the representations of the sentences as propositions):
 The trend is over the period.WHICH The period is from the year 1998 to the
year 2006. can be realized as:
The trend is over the period from the year 1998 to the year 2006. (Level 1)
 The graphic shows a trend.WHICH The trend is in the dollar value of net profit
over the period from the year 1998 to the year 2006. can be realized as:
The graphic shows a trend in the dollar value of net profit over the period from
the year 1998 to the year 2006. (Level 1)
In our generation approach, multiple realizations for each proposition can be incor-
porated by defining new complexity estimators in addition to the estimators that are
used in the current implementation. Defining such estimators, which will not change
the task complexity or the underlying methodology, would add to the generalizability
of our approach.
19 In this case, the single node trees that correspond to the propositions conveying the range of the trend
form the immediate descendents of a tree rooted by a proposition with an And predicate, and that tree
with the And predicate at its root and the tree corresponding to the proposition conveying the largest
drop constitute the immediate descendents of a tree rooted by another proposition with an And
predicate.
552
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
5.3.2 Relative Clause Embedding. In cases where a tree rooted by a proposition with a
Which predicate cannot be realized as a simple adjunct or a prepositional phrase, it will
be realized by a relative clause. In the D-level classification, the complexity of a sentence
with an embedded clause is determined according to the grammatical role (subject
or object) of the entity that is modified by that clause, not the syntactic complexity
or position (center-embedded or right-branching) of the clause in the sentence. For
instance, a sentence with a complex center-embedded relative clause modifying an
object receives the same syntactic complexity score as a sentence with a simple right-
branching relative clause modifying an object. As argued in the literature, however,
center-embedded relative clauses are more difficult to comprehend than corresponding
right-branching clauses (Johnson 1998; Kidd and Bavin 2002). To capture this, our
evaluation metric for identifying the best structure penalizes Which predicates that
will be realized as a relative clause based on the clause?s syntactic complexity and
position in the sentence (which we refer to as ?comprehension complexity of a relative
clause?). For example, the following sentences receive different scores by our evaluation
metric with respect to clause embedding; the first one with a right-branching clause
(simpler) has a lower score than the second sentence with a center-embedded clause
(more complex):
 The graphic shows a decreasing trend over the period from the year 1998
to the year 2006 in the dollar value of net profit, which is 2.7 billion dollars
in the year 1999.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999, over the period from the year 1998 to the
year 2006.
The embedded clause (Level 0) in the first of the following sentences has a lower
syntactic score than the clause (Level 2) embedded in the second sentence. Because
our evaluation metric takes into consideration both the syntactic complexity of an
embedded clause and its position in the sentence, the first sentence receives a lower
score than the second sentence.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999, over the period from the year 1998 to the
year 2006.
 The graphic shows a decreasing trend in the dollar value of net profit, which is
2.7 billion dollars in the year 1999 and is 2.58 billion dollars in the year 2000,
over the period from the year 1998 to the year 2006.
5.3.3 Evaluation Metric. Our evaluation metric takes three criteria into account: the
number of sentences that will be used to realize a forest, the syntactic complexities of
these sentences, and the comprehension complexities of the embedded relative clauses.
Our metric evaluates the overall score of a candidate forest by summing the normalized
scores that the forest receives with respect to each criteria. The score of a forest (e.g.,
Forest A) is calculated as follows:
score(A) = nm1(sentence(A))+ nm2(complexity(A))+ nm3(clause(A)) (1)
553
Computational Linguistics Volume 38, Number 3
where:20
sentence(A): stands for the number of sentences that will be used to realize forest A
and equals the number of trees in that forest.
complexity(A): stands for the overall syntactic complexity of forest A and equals the
sum of the complexities of sentences that will be used to realize that forest.
clause(A): stands for the overall comprehension complexity of all relative clauses
in sentences used for realizing forest A and equals the sum of the comprehension
complexities of all clauses. The comprehension complexity of a relative clause equals
the product of its syntactic complexity and its position in the sentence, which is equal
to 2 if it is a center-embedded clause and is equal to 1 if it is a right-branching clause.
Consider, for example, the forest shown in Figure 6. Because the forest contains a
single tree, it receives a score of 1 for the sentence criteria. The syntactic complexity score
of the sentence that will be used to realize that tree is computed in a bottom?up fashion
as follows. The lowest syntactic complexity score (Level 0) is assigned to all leaf nodes,
and all inner nodes that only have single node trees with anAttributive Knowledge Base
predicate as descendents (as shown in Figure 6). Each of the remaining inner nodes is
then assessed with a syntactic complexity score once the complexity scores for all of
its descendents are computed (i.e., once the best realization possibility with the lowest
syntactic complexity for each descendent tree is determined). If an inner node contains
a proposition with an And predicate, its syntactic complexity score is computed via
the And predicate complexity estimators. Similarly, the Which predicate complexity
estimators are used to compute the syntactic complexity scores for all inner nodes with
aWhich predicate. The syntactic complexity score for the parent node of a tree rooted by
a proposition with aWhich predicate is computed based on whether or not that tree will
be realized as a relative clause (as indicated by the complexity score of the root node of
that tree). The forest shown in Figure 6 receives a score of 4 for the complexity criteria,
which is equal to the syntactic complexity score assigned to the parent node of the tree.
In Figure 6, only the tree rooted by Node 4 will be realized as a relative clause. Because
that relative clause, which receives a syntactic complexity score of 2, will be realized
as a center-embedded clause, the forest shown in Figure 6 receives a score of 4 for the
clause criteria.
In the current implementation, once the scores with respect to a criteria are com-
puted for each candidate forest, these scores (e.g., sentence(A)) are normalized with
respect to the maximum score (e.g., max(sentence(all forests))) by dividing each score
by the maximum of the computed scores. For instance, nm1(sentence(A)) is the nor-
malized score that the forest A receives with respect to the sentence criteria and is
equal to sentence(A)/max(sentence(all forests)). Thus, the normalized score that a forest
receives for each criteria is always between 0 and 1 and therefore each criteria has
an equal impact on the overall score of a forest.21 The normalized scores obtained
for a candidate forest are then summed to obtain the overall score for that forest. For
example, assume that three candidate forests, the first of which is shown in Figure 6,
20 The terms nm1, nm2, and nm3 stand for the normalized score of a given criteria.
21 The simplifying assumption of assigning equal weights to each criteria would be better optimized with
machine learning, as discussed in detail in Section 9.
554
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 6
A forest containing a single tree.
555
Computational Linguistics Volume 38, Number 3
Table 3
Overall evaluation scores.
Sentence Complexity Clause Overall Score
First Forest 1(0.5) 4(1) 4(1) 2.5
Second Forest 1(0.5) 4(1) 2(0.5) 2
Third Forest 2(1) 3(0.75) 0(0) 1.75
are constructed from a set of propositions. One possible way of realizing the forest in
Figure 6 would be The graphic shows a decreasing trend in the dollar value of net profit,
which shows the largest drop of about 0.56 billion dollars between the year 2000 and the
year 2001, and shows the smallest drop of nearly 0.07 billion dollars between the year 1998
and the year 1999, over the period from the year 1998 to the year 2006. Suppose that the
second forest is similar to Figure 6 except that the children (Node 2 and Node 3) of
the And(trend) node are swapped.22 Suppose also that the third forest is similar to
Figure 6 except that the tree is decomposed into two trees, which are rooted by Node 1
and Node 5, respectively. The first tree rooted by Node 1 consists of the nodes marked
with (*) and the second tree rooted by Node 5 consists of the nodes marked with (**).
Table 3 shows the actual and the normalized scores (shown in parentheses) for each
forest with respect to each criteria, and the overall score assigned by our evaluation
metric.
The number of sentences (1) and the overall sentence complexity (Level 4) are the
same for the first and second forests. The third forest has more sentences (2) but lower
overall sentence complexity (Level 3) than the other two forests. The first forest has a
center-embedded relative clause and receives a score of 4 for the clause criteria: the
product of the complexity of the relative clause (2) and its position (2). On the other
hand, the second forest has a right-branching relative clause and receives a score of 2
for the same criteria: the product of the complexity of the relative clause (2) and its
position (1). The third forest doesn?t have an embedded clause and receives a score of 0
for the clause criteria.
5.4 Identifying the Best Text Structure
Our approach selects the forest which receives the lowest evaluation score as the best
forest that can be obtained from a set of input propositions. For example, according
to the scores shown in Table 3, the third forest, which could be realized as The graphic
shows a decreasing trend in the dollar value of net profit over the period from the year 1998
to the year 2006. The dollar value of net profit shows the largest drop of about 0.56 billion
dollars between the year 2000 and the year 2001 and shows the smallest drop of nearly 0.07
billion dollars between the year 1998 and the year 1999., would be selected as the best
among the three forests. The initial overall text structure of a brief summary con-
sists of the best forests identified for the message related, specific, and computational
classes.
As a final step, we check whether we can improve the evaluation of the overall
structure of the summary by moving trees (i.e., trees rooted by a Level-0 node such as
22 This swapping would cause the relative clause rooted by Node 4 to be a right-branching clause.
556
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
And(descriptor) in Figure 5, Specific) or subtrees (i.e., trees rooted by a Level-1 node
with an And or a Relative Knowledge Base predicate such as hasextreme (descrip-
tor,?largest?,change(3,4),period(3,4)) in Figure 5, Specific) between the best forests for
the three proposition classes. For example, the best forest for the specific class might
contain a tree that conveys information about an entity introduced by a proposition
in the message related class. Moving this tree to the message related class and using an
operator to combine it with the tree introducing the entity might improve the evaluation
of the overall structure of the summary. For example, for the graphic in Figure 4, this
movement would allow our system to evaluate a structure where the tree rooted by the
specific proposition And(descriptor) (shown in Figure 5, Specific) is attached as a de-
scendent of the tree rooted by the message related proposition exists(trend,descriptor)
(shown in Figure 5,Message Related) via aWhich Operator.We explore all such possible
movements between best forests for the proposition classes (if any) and determine
the best overall text structure of the summary. To be consistent with the motivation
behind the initial groupings of the propositions, we do not allow movements out
of the message related class or any movement that will empty the computational
class.
5.5 Evaluation of the Text Structuring and Aggregation Module
Our text structuring and aggregation approach consists of several different components,
all of which contribute to the quality of the generated text. Our study focused on
whether or not our decisions with respect to these components contributed to the
perceived quality of the resultant summary: the organization and ordering (O) of
the content (partial ordering of the propositions within classes and classification of
the propositions), the aggregation (A) of the information into more complex tree struc-
tures (candidate forests constructed via operators), and the metric (E) used to evaluate
candidate forests that represent different possible aggregations of the informational
content.
We conducted an experiment with 15 participants (university students and gradu-
ate students) who were presented with six different summaries of twelve graphics from
the test corpus (described in Section 3.3). The participants neither participated in earlier
studies (described in Sections 4.1 and 4.4) nor were involved in this work. All presented
summaries were automatically produced by our generation approach. The participants
were not told how the presented summaries were produced (i.e., human-generated
or computer-generated), however. We focused on graphics with an increasing or a
decreasing trend, since these message categories exhibit the greatest variety of possible
summaries. For each of the graphics, the participants were given a set of summaries in
random order and asked to rank them in terms of their quality in conveying the content.
The summaries varied according to the test parameters as follows:23
 S O+A+E+: A summary that uses the ordering rules, the aggregation
rules, and receives the lowest (best) overall score by the evaluation
metric. This is the summary selected as best by the TSAMModule.
23 Although eight different summaries are logically possible with three different variables, we limited the
number to four (the second and the third in which exactly one of the components was turned off and the
fourth where all components were turned off) in order to keep the experiment within a manageable size.
557
Computational Linguistics Volume 38, Number 3
Table 4
Ranking of summary types.
Summary Type Best 2nd 3rd 4th
S O+A+E+ 65.6% 26.6% 6.7% 1.1%
S O+A+E- 16.7% 32.2% 33.3% 17.8%
S O-A+E+ 16.7% 30% 40% 13.3%
S O-A-E- 1% 11.2% 20% 67.8%
 S O+A+E-: A summary that uses the ordering and aggregation rules,
but does not receive the lowest overall score by the evaluation metric.
This is the summary that received the second lowest score.
 S O-A+E+: A summary where the propositions are randomly ordered,
but aggregation takes place, and it receives the lowest (best) overall
score by the evaluation metric.
 S O-A-E-: A summary consisting of single sentences that are randomly
ordered.
Table 4 presents the results of the experiment. It is particularly noteworthy that the
summary selected as the best by the Text Structuring andAggregationModulewasmost
often (65.6% of the time) rated as the best summary and overwhelmingly (92.2% of the
time) rated as one of the top two summaries. The table shows that omitting the eval-
uation metric (S O+A+E-) or omitting ordering of propositions (S O-A+E+) results in
summaries that are substantially less preferred by the participants. Overall, the results
shown in Table 4 validate our ordering, aggregation, and evaluation methodology.
6. Sentence Ordering Module (SOM)
With the use of different kinds of operators and an evaluation metric, our system
determines the partial ordering and the structure of sentences that will be used to realize
the selected content but doesn?t impose ordering constraints (final ordering) on the sen-
tences within each proposition class. To decide in which sequence the sentences should
be conveyed, we take advantage of the fact that each proposition has a defined main
entity, which will be realized in the subject position of the sentence that will be used to
realize the proposition. Identifying the subject of the realized sentences in advance al-
lows us to use centering theory (Grosz, Weinstein, and Joshi 1995) to generate a text that
is most coherent according to this theory.24 The theory outlines the principles of local
text coherence in terms of the way the discourse entities are introduced and discussed,
and the transitions between successive utterances in terms of the entities in the hearer?s
center of attention. Although some fundamental concepts of the theory, such as the
ranking of entities in an utterance, aren?t explicitly specified, various researchers have
applied centering theory to language generation (Kibble and Power 2004; Karamanis
et al 2009) with different interpretations. In our work, each sentence is regarded as an
24 If this assumption is relaxed, then centering theory would not be appropriate for an ordering component.
In that case, a focusing theory such as McCoy and Cheng (1991), or Suri and McCoy (1994) could be used
to order the sentences to be realized.
558
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
utterance. Following Brennan, Friedman, and Pollard (1987) and Grosz, Weinstein, and
Joshi (1995), we rank the entities with respect to their grammatical functions where the
entity in subject position is the most salient entity. When ordering sentences, we take
into account the preference order for centering transitions: continue is preferred over
retain, which is preferred over smooth shift, which is in turn preferred over rough shift.
For all message categories, the number of sentences in a proposition class would
be limited (less than five) even if all of the highly rated propositions identified for
that message category are selected for inclusion. Thus, a straightforward ?generate and
test? strategy is appropriate for ordering sentences in our case. For each proposition
class, all possible orderings of the sentences within that class are generated. We assign
different numeric scores to each centering transition, where continue receives a score
of 3, and retain and smooth shift receive scores of 2 and 1, respectively. The rough shift
transitions are assessed a score of 0. For each candidate ordering, we sum the scores
for the kinds of transitions observed between consecutive sentences. The ordering that
receives the highest score is selected as the best ordering for that proposition class. First,
the best ordering of the sentences in the message related proposition class is selected.
The subject of the last sentence in that ordering is used as the backward-looking center
of the previous utterance when determining the best ordering of the sentences in the
specific proposition class. Similarly, the subject of the last sentence in the best ordering
for the specific class is used as the backward-looking center when identifying the best
ordering for the computational proposition class.
For graphics that depict a time period, we also utilize the time periods mentioned in
each conjunct of a conjoined sentence in order to specify in which order these conjuncts
will be conveyed in the realized text. If the time periods mentioned in each conjunct of
a conjoined sentence are different, these conjuncts are ordered such that the time period
in focus in the first conjunct subsumes or precedes the time period in focus in the second
conjunct. Consider how individual sentences in the following compound sentences are
ordered by our approach:
 The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the
period from the year 1998 to the year 2006 and shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001.
 The dollar value of net profit shows the smallest drop of nearly 0.07 billion dollars
between the year 1998 and the year 1999 and shows the largest drop of about
0.56 billion dollars between the year 2000 and the year 2001.
Note that in the first conjoined sentence, the time period mentioned in the first con-
junct (from 1998 to 2006) subsumes the time period mentioned in the second conjunct
(between 2000 and 2001). On the other hand, in the second conjoined sentence, the time
period mentioned in the first conjunct (between 1998 to 1999) precedes the time period
mentioned in the second conjunct (between 2000 and 2001).
7. Sentence Generation Module (SGM)
To realize the summaries in natural language, we use the FUF/SURGE surface real-
izer (Elhadad and Robin 1999), which offers the richest knowledge of English syntax and
widest coverage among the publicly available realizers such as REALPRO (Lavoie and
Rambow 1997). The realization of the sentence-sized units requires referring expressions
559
Computational Linguistics Volume 38, Number 3
for certain graphical elements, however. Our system handles three different issues with
respect to referring expression generation:
 Generating a referring expression for the dependent axis. Information
graphics often do not label the dependent axis with a full descriptor of
what is being measured (which we call themeasurement axis descriptor).
In such a situation, a referring expression for this element must be
extracted from the text of the graphic. For example, to realize its summary,
a measurement axis descriptor (e.g., the dollar value of Chicago Federal Home
Loan Bank?s mortgage program assets) must be generated for the graphic in
Figure 7a, whose dependent axis is not explicitly labeled with the
descriptor.
 Generating a referring expression in order to refer to the bars on the
independent axis (e.g., the countries for the graphic in Figure 1). Such an
expression must be inferred from the bar labels or extracted from the text
of the graphic. This referring expression is often used in the summaries of
graphics in some message categories (e.g., Maximum Bar) that require
comparing a bar with others (e.g., distinguishing the bar with the
maximum value from all other bars).
 It was shown that people prefer less informative descriptions for
subsequent mentions of an entity (Krahmer and Theune 2002). In order to
generate more natural summaries, the syntactic forms of the subsequent
mentions of discourse entities should be constructed in a way that helps
text coherence.
7.1 Measurement Axis Descriptor
Generation of referring expressions (noun phrases) is one of the key problems explored
within the natural language generation literature. There is a growing body of research
in this area that, given a knowledge base of entities and their properties, deals with
Figure 7
(a) Graphic from Business Week. (b) Graphic from Business Week.
560
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
determining the set of properties that would single out the target entity (Dale and Reiter
1995; Krahmer, Van Erk, and Verleg 2003). More recently, the generation of referring
expressions has been proposed as a postprocessing technique to deal with the lack of
text coherence in extractive multidocument summarization (Belz et al 2008). Nenkova
and McKeown (2003) developed a method to improve the coherence of a multidoc-
ument summary of newswire texts by regenerating referring expressions for the first
and subsequent mentions of people?s names where the expressions are extracted from
the text of the input documents according to a set of rewrite rules. The task that we
face is similar to this recent body of research in that contextually appropriate referring
expressions for certain graphical elements should be extracted from the text of the
graphic. At the same time, our task is more complex in some respects. First, it is often the
case that the required referring expression isn?t explicitly given as a single unit and thus
must be constructed by extracting and combining pieces of information from the text of
the graphic. Second, in some cases where the dependent axis is explicitly labelled with a
descriptor, it still needs to be augmented. We undertook a corpus study in order to iden-
tify how a measurement axis descriptor could be generated from the text of a graphic;
the results of the analysis form the basis for the heuristics and augmentation rules
we developed for generating the measurement axis descriptor for a graphic. In Demir,
Carberry, and Elzer (2009), we outlined this problem as generating a graphical element
required for realizing the intended message of a graphic and thoroughly described the
technical details of our approach. Here, however, we treat this particular aspect as a
novel text-to-text generation methodology which is combined with other data-to-text
approaches in a complete NLG system. Thus, our focus in this section is to highlight
a new way of using the text associated with images which has been earlier exploited
by various NLP tasks such as indexing and retrieval of images (Pastra, Saggion, and
Wilks 2003).
7.1.1 Corpus Analysis. Graphic designers generally use text within and around a graphic
to present information related to the graphic. We started our analysis by examining how
texts are distributed around each group of graphics. We observed that graphics (indi-
vidual or composite) contain a set of component texts that are visually distinguished
from one another by blank lines, by different fonts/styles, or by different directions
and positions in the graphic. Although the number of component texts present in a
graphic may vary, our analysis recognized an alignment or leveling of text contained in
a graphic, which we refer to as ?text levels.?
We observed seven text levels which we refer to as Overall Caption, Overall
Description, Caption, Description, Dependent Axis Label, Text In Graphic, and Text
Under Graphic. Not every level appears in every graphic. Overall Caption and Over-
all Description apply to composite graphics that contain more than one individual
graphic (the graphics might be of different kinds) and refer to the entire collection
of graphics in the composite. In composite graphics, Overall Caption is the text that
appears at the top of the overall group and serves as a caption for the whole set (such
as Tallying Up the Hits in Figure 8a). In composite graphics, there is often another
text component placed under the Overall Caption but distinguished from it by a line
break or a change in font. This text component, if present, is also pertinent to all
individual graphics in the composite graphic and elaborates on them. We refer to such
text as the Overall Description (such as Yahoo once relied entirely on banner ads. Now it?s
broadened its business mix in Figure 8a). Caption and Description serve the same roles
for an individual graphic. For example, the Caption for the bar chart in Figure 8a is
Active Users and the Description is Registered users in millions. The Caption of Figure 8b
561
Computational Linguistics Volume 38, Number 3
Figure 8
(a) A composite graph from Newsweek.25 (b) Graphic from Business Week.
Table 5
Text levels in bar charts.
Text level Frequency of occurrence
Overall Caption 31.8% (?34/107)
Overall Description 17.8% (?19/107)
Caption 99.0% (?106/107)
Description 54.2% (?58/107)
Text In Graphic 39.3% (?42/107)
Dependent Axis Label 18.7% (?20/107)
Text Under Graphic 7.5% (?8/107)
is A Growing Biotech Market but this graphic does not have a Description. There may be
a label on the dependent axis itself and we refer to it as Dependent Axis Label (such
as Revenues (in billions) in Figure 8b). In addition to the text levels described so far
which appear outside the borders of a graphic, we have observed that there is often
a text component residing within the borders of a graphic which we refer to as Text
In Graphic (such as U.S. Biotech Revenues, 1992?2001 in Figure 8b). Finally, Text Under
Graphic is the text under a graphic which usually starts with a marker symbol (such
as *) and is essentially a footnote (such as U.S. only, one available seat flown one mile, year
ending June 2002 in Figure 7b). Each Text Under Graphic has a referrer elsewhere that
ends with the same marker and that referrer could be at any text level of the graphic. A
graphic might have more than one Text Under Graphic but each is differentiated with
a different marker. For each of the 107 graphics in our corpus (described in Section 3.3),
the Visual Extraction System extracts these text levels from the graphical image of the
bar chart and inserts them into the graph?s XML representation. Table 5 lists the various
text levels, along with how often they appeared in the graphics in our corpus.
25 This figure displays two of the five individual graphs constituting the composite graphic that appeared in
Newsweek.
562
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Two annotators first analyzed each graphic in our corpus and determined a mea-
surement axis descriptor for the graphic; the annotators used both the information
residing within the text components of the graphic and the article, and commonsense
knowledge. All ideal descriptors were noun phrases or wh-phrases26 (such asWhat?s the
most important issue affecting voters? vote? on a graphic depicting survey results). After the
descriptors were identified, we analyzed the graphics to see how the descriptors could
be generated from the text components of the graphic. We observed that an acceptable
measurement axis descriptor often cannot be extracted as a whole from a single text
level and instead must be put together by extracting pieces from one or several different
text levels; the pieces, though coming from a single level, might not be contiguous in that
level and still need to be melded into a coherent whole. In some cases, the information
is also retrieved from other graphics in the same composite or from the article?s text.
Our analysis has also led us to hypothesize that the ideal measurement axis descriptor
can be viewed as consisting of a core?a basic noun or wh-phrase from one text level
that is often augmented with text from another level (or in some cases, from text in the
accompanying article or other graphs in the same composite) to be more descriptive
and complete. For example, for the bar chart in Figure 8a, registered users is the core
of the ideal measurement axis descriptor which is Yahoo?s registered users. The core is
found in the Description and the augmentation to the core is found in the Overall
Description. When more than one text level is used, the text levels that contain pieces of
themeasurement axis descriptor also vary among the graphics.We observed thatmostly
text levels particular to a graphic (such as Text In Graphic and Description) contain the
pieces of the descriptor as opposed to the levels containing shared information (such
as Overall Description), and with the exception of Text Under Graphic, the ordering of
text levels in Table 5 forms a hierarchy of textual components, with Overall Caption
and Dependent Axis Label respectively at the top and bottom of the hierarchy, such that
the core generally appears in the lowest text level present in the graphic. During the
corpus analysis, we observed three ways in which a core extracted from one text level
was augmented with text from another text level:
 Expansion of the noun phrase: The nouns in the core of the descriptor
were replaced with a noun phrase at another text level which has the same
noun as its head. The replaced noun phrase appeared in a text level higher
in the precedence order than the text level at which the core appears.
Consider, for example, Figure 8b. The core of the descriptor is Revenues
(appearing in the Dependent Axis Label), which is reasonable enough
to be the core, but the noun Revenues should be replaced with U.S. Biotech
Revenues in order to be complete.
 Specialization of the noun phrase: The core was augmented with a
proper noun which specialized the descriptor to a specific entity.
Consider, for example, Figure 8a, which shows a composite graph where
individual graphics present different attributes of the same particular
entity (Yahoo). The ideal measurement axis descriptor of the bar chart
(Yahoo?s registered users) consists of the core registered users (appearing in
the Description) augmented with the proper noun Yahoo that appears in
the Overall Description.
26 Generally seen in graphics presenting the results of a survey.
563
Computational Linguistics Volume 38, Number 3
 Addition of detail: Text Under Graphic typically serves as a footnote to
give specialized detail about the graphic which is not as important as the
information given in other text levels. If the Text Under Graphic began
with a footnote marker, such as an asterisk, and the core was followed by
the same marker, then Text Under Graphic added detail to the core.
Consider, for example, Figure 7b, where unit costs is the core but the ideal
measurement axis descriptor (Unit costs, U.S. only, one available seat flown
one mile, year ending June 2002) also contains the information from the
Text Under Graphic.
7.1.2Methodology. First, preprocessing deletes the scale and unit indicators (phrases used
to give the unit and/or a scale of the values presented in the graphic), and the ontolog-
ical category of the bar labels (if explicitly marked by the preposition by) from the text
levels. Next, heuristics are used to identify the core of the measurement axis descriptor
by extracting a noun phrase or a wh-phrase from a text level of the graphic. Three kinds
of augmentation rules, corresponding to the three kinds of augmentation observed in
our corpus, are then applied to the core to produce the measurement axis descriptor.
If none of the augmentation rules are applicable, then the core of the descriptor forms
the measurement axis descriptor. Finally, if the measurement axis descriptor does not
already contain the unit of measurement (such as percent), the phrase indicating the unit
of measurement is appended to the front of the measurement axis descriptor.
For identifying the core of the measurement axis descriptor, we developed nine
heuristics that are dependent on the parses of the text levels. Two of these heuristics are
restricted to Dependent Axis Label and Text In Graphic, and the remaining heuristics
are applicable to all other text levels. The application of the heuristics gives preference
to text levels that are lower in the hierarchy and if a core is not identified at one text
level, the applicable heuristics are applied, in order, to the next higher text level in
the hierarchy. For example, suppose that the graphic contains only a Description and
a Caption and thus the first two heuristics are not applicable. The next seven heuristics
are first applied to the Description and then to the Caption. The following presents three
representative heuristics where the first two heuristics are applicable only to Dependent
Axis Label and Text In Graphic:27
 Heuristic-1: If the Dependent Axis Label consists of a single noun phrase
that is not a scale or unit indicator, that noun phrase is the core of the
measurement axis descriptor.
 Heuristic-2: If Text In Graphic consists solely of a noun phrase, then that
noun phrase is the core; otherwise, if Text In Graphic is a sentence, the
noun phrase that is the subject of the sentence is the core.
 Heuristic-6: If a fragment at the text level consists solely of a noun phrase,
and the noun phrase is not a proper noun, that noun phrase is the core.
Once the core is identified, augmentation rules are applied to fill out the descriptor.
For example, consider the graphic in Figure 8b where Heuristic-1 identifies Revenues
in Dependent Axis Label as the core. Because the core and the Text In Graphic, U.S.
Biotech Revenues, have the same head noun, the augmentation rule for expansion
27 All of the heuristics and augmentation rules can be found in Demir, Carberry, and Elzer (2009).
564
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
produces U.S. Biotech Revenues as the augmented core. After adding a phrase for
the unit of measurement, the referring expression for the dependent axis becomes
The dollar value of U.S. Biotech Revenues. As another example, consider the graphic in
Figure 7b. Our work uses Heuristic-2 and the augmentation rule for adding detail.
After adding a phrase representing the unit of measurement, the referring expression
for the dependent axis becomes The cent value of unit costs (U.S. only, one way available seat
flown one mile, year ending June 2002). Finally, consider the graphic in Figure 8a, where
Heuristic-6 identifies the noun phrase registered users as the core.28 The augmentation
rule for specialization finds that Yahoo is the only proper noun in the text levels and
does not match a bar label, and forms Yahoo?s registered users. After adding a phrase
representing the unit of measurement, the referring expression for the dependent axis
becomes The number of Yahoo?s registered users.
In order to evaluate our approach, we constructed a distinct test corpus consisting
of 205 randomly selected bar charts from 21 different newspapers and magazines;
only six of these sources were also used to gather the corpus described in Section 3.3.
For each graphic, we used our approach to generate the referring expression for the
dependent axis. Finally, the resultant output and three baselines were evaluated by
two evaluators (Demir, Carberry, and Elzer 2009). The evaluation results showed that
our approach performs much better than any of the baselines for the 205 graphics in
the corpus. The detailed analysis of the results also showed that our methodology is
applicable to a wider range of sources in popular media.
7.2 Generating an Expression for Referring to All Bars
For some message categories (for example, Maximum Bar), the identification of the
ontological category for the bar labels results in better natural language than merely
using a generic expression; for example, compare the phrase among the countries listed
with the phrase among the entities listed in producing natural language text for the
message conveyed by the graphic in Figure 1. There are a number of different pub-
licly available ontologies such as WordNet Fellbaum (1998) and OpenCyc (2011). In
our work, we need a knowledge base that offers both the semantic relations between
words and general commonsense knowledge. For example, WordNet could not iden-
tify Jacques Chirac, a former president of France, whereas OpenCyc ontology contains
this information. Therefore, we use OpenCyc ontology version v0.7.8b to identify the
ontological categories of bar labels in our work. Our implemented system currently
finds the most specific category that is a common category for at least two of the bar
labels and identifies it as the ontological category.
Grice?s Maxim of Quantity (1975) states that one?s discourse contribution should be
as informative as necessary for the purposes of the exchange but not more so. If our
system were to enumerate all entities involved in a comparison message, the realization
of the inferred message might be lengthy and the enumeration of little utility to the
user. Thus we set a cut-off C, such that if the number of entities involved in a Maximum
Bar or Rank Bar message exceeds C, they are not enumerated but rather we use the
generated referring expression for all bars. The cut-off value is currently set to 5 in our
implemented system.
28 The preprocessing of this text level would remove In millions because it is a scale indicator.
565
Computational Linguistics Volume 38, Number 3
7.3 Subsequent Mentions of Discourse Entities
In the current implementation of the system, the syntactic form of a subsequent mention
of an entity is determined based on its salience status in the context. In particular, the
backward-looking center of an utterance29 is replaced with a less informative definite
noun phrase after a continue or a retain transition because the backward-looking cen-
ter remains the same in the latter utterance. In such cases, the definite noun phrase
is constructed by adding the demonstrative this or these to the front of the head
noun of the backward-looking center, such as these revenues for the phrase U.S. biotech
revenues.
7.4 Example Summaries
For the graphic in Figure 1, our system generates the following textual summary: The
graphic shows that United States at 32,434 has the highest number of hacker attacks among the
countries Brazil, Britain, Germany, Italy, and United States. United States has 5.93 times more
attacks than the average of the other countries.
For the graphic in Figure 3a, the following textual summary is generated: The graphic
shows an increasing trend in the dollar value of Lands? End annual revenue over the period from
the year 1992 to the year 2001. The dollar value of Lands? End annual revenue shows an increase
of nearly 225 percent. Except for a small drop in the year 1999, slight increases are observed
almost every year.
For the graphic in Figure 3b, our system generates the following summary: The
graphic compares the defense agencies army, navy, air force, and other defense agencies, which
are sorted in descending order with respect to the number of civilian employees. The number
of civilian employees is highest for army at 233,030 and is lowest for other defense agencies at
100,678.30
For the graphic in Figure 4, the following textual summary is generated: The graphic
shows a decreasing trend in the dollar value of net profit over the period from the year 1998 to the
year 2006. The dollar value of net profit ranges from 2.77 to 0.96 billion dollars over the period
from the year 1998 to the year 2006 and shows the largest drop of about 0.56 billion dollars
between the year 2000 and the year 2001. Slight decreases are observed almost every year.
For the graphic in Figure 8b, our system generates the following summary: The
graphic shows an increasing trend in the dollar value of U.S. Biotech Revenues over the period
from the year 1992 to the year 2001. Increasing slightly every year, the dollar value of U.S.
Biotech Revenues shows an increase of nearly 265 percent and ranges from 7.87 to 28.52 billion
dollars.
For the graphic in Figure 7a, the following textual summary is generated: In the year
2003, the graphic shows a much higher rise in the dollar value of Chicago Federal Home Loan
Bank?s mortgage program assets in contrast with the moderate increases over the period from the
year 1998 to the year 2002. The dollar value of Chicago Federal Home Loan Bank?s mortgage
program assets reaches to 94.23 billion dollars in the year 2003. The dollar value of these assets
in the year 2003 is nearly 49.1 times higher than that in the year 1998.
For the graphic in Figure 9, the following textual summary is generated: This graphic
is about American Express. The graphic shows that American Express at 255 billion dollars is
29 The backward-looking center of a current utterance is the most highly ranked entity of the previous
utterance that is realized in the current utterance.
30 The reason for saying that the defense agencies are sorted in decreasing order is not to enable the reader
to visualize the graphic but rather that it subsumes giving the ranking of each bar.
566
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Figure 9
Graphic conveying the rank of a bar.
the third highest with respect to the dollar value of total credit-card purchases per year among
the entities Visa, Mastercard, Discover, Diner?s Club, and American Express.
8. Evaluation of the Effectiveness of the Textual Summaries
The earlier user studies (Sections 4.4 and 5.5) demonstrated the effectiveness of our gen-
eration methodology in identifying and presenting the high-level content of bar charts.
The success of a generation system depends not only on the quality of the produced text,
however, but also on whether the text achieves the impact that it is intended to make on
readers (such as enabling readers to perform a task or changing their opinions in some
context). We conducted an evaluation study to measure how adequate and effective
the summaries generated by our system are for our purpose of providing the message
and high-level knowledge that one would gain from viewing a graphic. Specifically, we
were interested in (1) what amount of information is retained by a reader from reading
the summary generated by our system, (2) whether someone reading the summary
garners the most important knowledge conveyed by the graphic, and (3) whether the
knowledge gleaned from the summary is consistent with the actual graphic.
In this study, we used four graphics from the test corpus (described in Section 3.3)
with different intended messages. These graphics conveyed an increasing trend (i.e.,
Figure 3a), a decreasing trend, the rank of the maximum bar (i.e., Figure 1), and the
rank of a bar (i.e., Figure 9) among all bars. In the first part of the study, each of the
18 participants (graduate students) was first presented with the summaries generated
by our system for these graphics; the participants neither saw the original graphics
(the graphical images of the bar charts) nor were aware of our system and how the
summaries were generated. For each summary, the participants were asked to draw
the bar chart being described in that summary. Although enabling a reader to redraw
the graphic is not a goal of our work, comparing a reader?s mental representation
of the graphic with the actual graphic allows us to identify whether there are any
inconsistencies between knowledge gleaned from the summary and the content of the
actual graphic.
In the second part of the study, we asked three evaluators not involved in this re-
search to evaluate the drawings that we collected from the participants. The evaluators
were Ph.D. students from the University of Delaware (none of them were the authors
of this work) and had an overall knowledge about our summarization approach (i.e.,
what is intended to be conveyed in the summaries of graphics). The evaluators were
567
Computational Linguistics Volume 38, Number 3
first told that a set of participants were presented with brief summaries of bar charts
and asked to draw the corresponding bar charts based on the information presented
in those summaries. They were also told that each summary only conveyed what is
identified by our system as the most important information that should be conveyed
about the bar chart being described. The evaluators were presented with the graphical
images of the four bar charts used in the study (none saw the summaries presented in
the first part of the study) and the drawings collected from the participants, and then
asked to rate each drawing using the following evaluation scores:
 5: The drawing is essentially the same as the original graphic
 4: The drawing captures all important information from the original
graphic but requires some minor modifications
 3: The drawing captures most of the important information from the
original graphic but is missing one significant piece of information
 2: The drawing reflects some information from the original graphic but
requires major modifications
 1: The drawing fails to reflect the original graphic
The average score that a drawing received from the evaluators ranged from 3 to 5.
The evaluators viewed the drawings drawn for the graphics with a trend more favor-
ably and assigned a score of 4 or more in most cases. For each graphic, we computed the
average score given by the evaluators to all drawings constructed from the summary
of that graphic (i.e., the average of the three scores given to each of the 18 drawings
drawn from the same summary). The graphics conveying an increasing (Figure 3a) and
a decreasing trend received a score of 4.22 and 4.63, respectively. The evaluators gave an
average score of 3.53 and 4.07 to the graphics which conveyed the rank of the maximum
bar (Figure 1) and the rank of a bar among all bars (Figure 9). Because we do not present
all features of a bar chart in its summary (such as all bar values), obtaining an average
score of less than 5 for all bar charts is not surprising.
We also asked the evaluators to specify a reason (i.e., what is missing or should be
changed) for the cases where they assigned a score of less than 4. Once we analyzed
their feedback for the drawings with a trend, we observed that missing values on
the dependent axis (i.e., tick mark labels) and missing measurement axis descriptors
(although given in the summaries) were the main reasons. We argue that presenting
tick mark labels is more appropriate for summaries that describe scientific graphics
(such as the summaries generated by the iGRAPH-Lite system [Ferres et al 2007]) in
contrast to the summaries that we generate for conveying the high-level content of
a graphic. The evaluators indicated incorrect rankings of some bars as the reason for
giving lower scores to the drawings that present the rank of the maximum bar or the
rank of a bar among all bars; this is due to the fact that our summaries did not convey
the rankings and the values of all bars in those cases. Because the intention of the
corresponding graphics is to convey the rank of a single bar (not all bars), we argue that
our summaries facilitate the readers to get the main purpose of these graphics. Overall,
this study demonstrated that our summarization approach is effective in conveying
the high-level content of bar charts so as to enable readers to correctly understand the
main point of the graphic. We are planning to conduct future studies to explore the
effectiveness of our approach further, however. One possible evaluation could be asking
568
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
a different set of participants to draw the bar charts that we used in this study by reading
their summaries produced by an appropriate baseline approach, and comparing the
scores that those new set of drawings received from the same three evaluators with our
current results. This evaluation will also allow us to determine whether the evaluators
judged our summarization approach favorably because they were aware of the overall
approach (i.e., brief summaries are generated by our system and these summaries do
not contain everything that can be conveyed by the corresponding bar charts).
Favorable results were also achieved when people with visual impairments were
presented with the brief summaries generated by our work in an interactive system
which also enabled the user to ask follow-up questions to learn more about about the
graphic. More on that system and study can be found in Demir et al (2010).
9. Conclusion and Future Work
The majority of information graphics from popular media are intended to convey a
message that is often not captured by the text of the document. Thus graphics, along
with the textual segments, contribute to the overall purpose of a multimodal document
and cannot be ignored. The work presented in this article is the first to apply natural lan-
guage generation technology to provide themessage and high-level knowledge that one
would gain by viewing graphics from popular media via brief textual summaries. Our
summarization approach treats a graphic as a form of language and utilizes the inferred
intention of the graphic designer, the communicative signals present in the graphic, and
the significant visual features of the underlying data in determining what to convey
about that graph. Our approach uses a set of content identification rules constructed for
each intended message category of a bar chart in determining the content of the sum-
maries. The propositions selected for inclusion by these rules are organized into a text
structure by applying a novel bottom?up approach which leverages different discourse
related considerations such as the number and syntactic complexity of sentences and
clause embeddings that will be used for realization. Following the generation of refer-
ring expressions for certain graphical elements, the structure of a summary is realized
in natural language via a publicly available realizer. Three different evaluation studies
validated the effectiveness of our approach in selecting and coherently organizing the
most important information that should be conveyed about a bar chart, and enabling
readers to correctly understand the high-level knowledge of the graphic.
In addition to the application area, this article makes contributions to two broad
areas of research: data-to-text generation systems and text-to-text generation of referring
expressions. Here we have viewed the generation of a summary of an information
graphic as a data-to-text generation problem. Any data-to-text generation system must
solve several important problems: (1) out of all of the information in the data, extract
out that information that is important enough to be included in the text, (2) structure
the information so it can be realized as a coherent text, (3) aggregate propositions to
be conveyed in the text into more complex yet understandable sentence structures, (4)
order the resulting sentence structures so as to maintain text fluency, and (5) realize the
information as English sentences and generate appropriate referring expressions. Our
work has addressed each of these issues in a systematic fashion maintaining modularity
of system components and following a development methodology that includes human
input for making system decisions, and a thorough evaluation of each module as well
as final system evaluation.
Although the specific implementations that we developed are geared toward gen-
erating summaries of bar charts, the groundwork described in this article is currently
569
Computational Linguistics Volume 38, Number 3
being used by our own group to investigate summarizing other types of graphics (such
as line graphs and grouped bar charts) from popular media. A Bayesian system for
recognizing the intended message of line graphs has already been developed (Wu et al
2010) and the work on constructing the content identification rules for line graphs is
under way (Greenbacker, Carberry, and McCoy 2011).
The module that generates referring expressions represents a sophisticated study of
text-to-text referring expression generation. Referring expression generation is a vibrant
field. Although the particular rules used for extracting an appropriate referring expres-
sion are unique to referring expressions for graphical elements inside an information
graphic (note: not just bar charts), the work has uncovered some rather interesting
properties in terms of extracting expressions from text which may generalize to other
domains as well. Our work is unique in that a full referring expression is pieced together
from text not directly referring to the item in question. In order to do this, we identified
text levels and rules for extracting the core expression along with potential important
modifiers. One can imagine this message carrying over to other types of data-to-text
activities as well as to more standard text-to-text generation problems.
In future work, we intend to build on the work reported here in several ways.
Corpus studies where human subjects tasked with writing brief summaries of graph-
ics (the kind of summaries that our system intends to generate) would be of great
potential in informing generation decisions that our system makes at different levels.
Moreover, experts with extensive knowledge of the domain or the targeted end users
were shown to be of greater supply to the development of manyNLG systems. Learning
from summaries written by subjects (especially expert writers) would be an exciting
area of future research. We also believe that our approach would benefit from these
corpus studies towards exploring how the fluency of the summaries can be further
improved particularly by reducing the occasional verbosity in order to achieve tex-
tual economy (Stone and Webber 1998). For example, these efforts might help us to
determine how measurement axis descriptors can be more appropriately phrased in
different situations. Improving the current evaluation metric for choosing a text plan is
also in our research agenda. We utilize three criteria in our evaluation metric for deter-
mining the best structure that can be obtained by applying the operators to the selected
propositions. In addition, each criteria (the number of sentences, the overall syntactic
complexity of sentences, and the overall comprehension complexity of all embedded
clauses) has the same impact on the selection process. We are considering conducting
more user studies in order to identify what other criteria should be taken into account
and how important each criteria should be in relation to all the others. Moreover,
exploring the broader applicability of the novel aspects of our work in other settings
is an interesting topic for future work. Finally, evaluating the utility of our theoretically
informed aggregation mechanism in comparison to or in conjunction with the more
surface-oriented mechanism of the SPoT system would be a promising area for further
research.
To our best knowledge, what kinds of summaries best serve the needs of visually
impaired individuals has not been throughly studied before. As mentioned in Sec-
tion 2.2.1, we believe that our summaries, once associated with graphics as ALT texts,
might be of help to these individuals while reading electronic documents via screen
readers. One fruitful research direction would be to present such individuals with our
summaries in real-time scenarios and to mine their informational and presentational
needs. Such a study would probably provide insights with regard to this question
and potentially lead to guidelines that human summarizers could follow in generating
summaries for people with visual impairments.
570
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
Acknowledgments
The authors would like to thank the study
volunteers for their time and willingness
to participate. This material is based upon
work supported by the National Institute on
Disability and Rehabilitation Research under
grant no. H133G080047. For all graphics that
were used in our evaluations and are given
as examples in this article, inputs (i.e., the
inferred intended message and the XML
representation of the graphic) and outputs
(i.e., the textual summary of the graphic) of
our system can be found at the following
Web page: www.cis.udel.edu/?carberry/
barcharts-corpus/.
References
Alty, James L. and Dimitrios I. Rigas. 1998.
Communicating graphical information
to blind users using music: the role of
context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing
Systems, pages 574?581, Los Angeles, CA.
Baldwin, Breck and Thomas Morton.
1998. Dynamic coreference-based
summarization. In Proceedings of the
3rd Conference on Empirical Methods in
Natural Language Processing, pages 1?6,
Granada.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata.
2006. Aggregation via set partitioning
for natural language generation.
In Proceedings of the Human Language
Technology Conference of the North
American Chapter of the Association of
Computational Linguistics, pages 359?366,
New York, NY.
Belz, Anja, Eric Kow, Jette Viethen, and
Albert Gatt. 2008. The Grec challenge
2008: Overview and evaluation results.
In Proceedings of the 5th International
Natural Language Generation Conference,
pages 183?191, Salt Fork, OH.
Brennan, Susan E., Marilyn W. Friedman,
and Carl J. Pollard. 1987. A centering
approach to pronouns. In Proceedings
of the Annual Meeting of the Association of
Computational Linguistics, pages 155?162,
Stanford, CA.
Carberry, Sandra, Stephanie Elzer, and Seniz
Demir. 2006. Information graphics: An
untapped resource for digital libraries.
In Proceedings of the ACM Special Interest
Group on Information Retrieval Conference,
pages 581?588, Seattle, WA.
Chester, Daniel and Stephanie Elzer. 2005.
Getting computers to see information
graphics so users do not have to. In
Proceedings of the 15th International
Symposium on Methodologies for Intelligent
Systems, pages 660?668, Saratoga
Springs, NY.
Clark, Herbert. 1996. Using Language.
Cambridge University Press, Cambridge.
Coch, Jose. 1998. Interactive generation and
knowledge administration in multimeteo.
In Proceedings of 9th International Workshop
on Natural Language Generation,
pages 300?303, Niagara-on-the-Lake.
Corio, Marc and Guy Lapalme. 1999.
Generation of texts for information
graphics. In Proceedings of the 7th European
Workshop on Natural Language Generation,
pages 49?58, Toulouse.
Covington, M., C. He, C. Brown, L. Naci,
and J. Brown. 2006. How complex is that
sentence? A proposed revision of the
rosenberg and abbeduto D-level scale.
Research Report, Artificial Intelligence
Center, University of Georgia.
Cycorp. Open Cyc. 2011.
http://www.cyc.com.
Dale, Robert and Ehud Reiter. 1995.
Computational interpretations of the
gricean maxims in the generation of
referring expressions. Cognitive Science,
19(2):233?263.
Dalianis, Hercules. 1999. Aggregation
in natural language generation.
Computational Intelligence, 15(4):384?414.
Demir, Seniz, Sandra Carberry, and
Stephanie Elzer. 2009. Issues in Realizing
the Overall Message of a Bar Chart,
John Benjamins, 5th edition.
Amsterdam, pages 311?320.
Demir, Seniz, David Oliver, Edward
Schwartz, Stephanie Elzer, Sandra
Carberry, Kathleen F. McCoy, and Daniel
Chester. 2010. Interactive sight: Textual
access to simple bar charts. The New
Review of Hypermedia and Multimedia,
16(3):245?279.
Di Eugenio, Barbara, Davide Fossati,
Dan Yu, Susan Haller, and Michael Glass.
2005. Aggregation improves learning:
Experiments in natural language
generation for intelligent tutoring
systems. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics, pages 50?57,
Ann Arbor, MI.
571
Computational Linguistics Volume 38, Number 3
Elhadad, M. and J. Robin. 1999. SURGE:
A comprehensive plug-in syntactic
realization component for text generation.
Technical Report, Department of
Computer Science, Ben Gurion
University. Beersheba, Israel.
Elzer, Stephanie, Sandra Carberry, and
Ingrid Zukerman. 2011. The automated
understanding of simple bar charts.
Artificial Intelligence, 175(2):526?555.
Elzer, Stephanie, Nancy Green, Sandra
Carberry, and James Hoffman. 2006. A
model of perceptual task effort for bar
charts and its role in recognizing intention.
International Journal on User Modeling and
User-Adapted Interaction, 16(1):1?30.
Fasciano, Massimo and Guy Lapalme. 2000.
Intentions in the coordinated generation
of graphics and text from tabular data.
Knowledge and Information Systems,
2(3):310?339.
Fellbaum, Christiane. 1998.WordNet:
An Electronic Lexical Database.
The MIT Press, Cambridge, MA.
Ferres, Leo, Petro Verkhogliad, Gitte
Lindgaard, Louis Boucher, Antoine
Chretien, and Martin Lachance. 2007.
Improving accessibility to statistical
graphs: the igraph-lite system.
In Proceedings of the 9th International
ACM SIGACCESS Conference on
Computers and Accessibility, pages 67?74,
Tempe, AZ.
Foster, Mary Ellen. 1999. Automatically
generating text to accompany
information graphics. Master?s Thesis,
University of Toronto.
Friendly, Michael. 2008. A brief history of
data visualization. In C. Chen, W. Ha?rdle,
and A. Unwin, editors, Handbook of
Computational Statistics: Data Visualization,
volume III. Springer-Verlag, Heidelberg,
pages 1?34.
Gatt, Albert, Francois Portet, Ehud Reiter,
Jim Hunter, Saad Mahamood, Wendy
Moncur, and Somayajulu Sripada. 2009.
From data to text in the neonatal intensive
care unit: Using NLG technology for
decision support and information
management. AI Communications,
22(3):153?186.
Goldberg, Eli, Norbert Driedger, and
Richard I. Kittredge. 1994. Using
natural-language processing to produce
weather forecasts. IEEE Expert: Intelligent
Systems and Their Applications, 9(2):45?53.
Goldstein, Jade, Vibhu Mittal, Jaime
Carbonell, and Mark Kantrowitz. 2000.
Multi-document summarization by
sentence extraction. In Proceedings
of the NAACL-ANLP Workshop on
Automatic Summarization, pages 40?48,
Seattle, WA.
Greenbacker, Charlie, Sandra Carberry, and
Kathleen F. McCoy. 2011. A corpus of
human-written summaries of line graphs.
In Proceedings of the EMNLP 2011 Workshop
on Language Generation and Evaluation
(UCNLG+Eval), pages 23?27, Edinburgh.
Grice, H. Paul. 1975. Logic and conversation.
Speech Acts, 3:41?58.
Grosz, Barbara and Candace Sidner. 1986.
Attention, intentions, and the structure
of discourse. Computational Linguistics,
12(3):175?204.
Grosz, Barbara J., Scott Weinstein, and
Aravind K. Joshi. 1995. Centering:
A framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203?225.
Hovy, Eduard H. 1988. Planning coherent
multisentential text. In Proceedings of
the 26th Annual Meeting of the Association
for Computational Linguistics,
pages 163?169, Buffalo, NY.
Hovy, Eduard H. 1993. Automated discourse
generation using discourse structure
relations. Artificial Intelligence,
63(1-2):341?385.
Hovy, Eduard and Chin-Yew Lin. 1996.
Automated text summarization and the
summarist system. In Proceedings of the
Workshop on TIPSTER Text Program,
pages 197?214, Vienna, VA.
Ina, Satoshi. 1996. Computer graphics for
the blind. SIGCAPH Computers and the
Physically Handicapped, 55:16?23.
Jayant, Chandrika, Matt Renzelmann,
Dana Wen, Satria Krisnandi, Richard
Ladner, and Dan Comden. 2007.
Automated tactile graphics translation: in
the field. In Proceedings of the 9th
International ACM SIGACCESS Conference
on Computers and Accessibility, pages 75?82,
Tempe, AZ.
Johnson, Mark. 1998. Proof nets and the
complexity of processing center embedded
constructions. Journal of Logic, Language
and Information, 7(4):433?447.
Joshi, Aravind, Bonnie Webber, and
Ralph Weischedel. 1984. Living up to
expectations: Computing expert responses.
In Proceedings of the National Conference on
Artificial Intelligence, pages 169?175,
Austin, TX.
Karamanis, Nikiforos, Chris Mellish,
Massimo Poesio, and Jon Oberlander.
2009. Evaluating centering for information
572
Demir, Carberry, and McCoy Summarizing Information Graphics Textually
ordering using corpora. Computational
Linguistics, 35(1):29?46.
Kennel, A. 1996. Audiograf: A
diagram-reader for the blind. In
Proceedings of the 2nd Annual ACM
Conference on Assistive Technologies,
pages 51?56, Vancover, BC, Canada.
Kerpedjiev, Stephan and Steven Roth.
2000. Mapping communicative goals
into conceptual tasks to generate
graphics in discourse. In Proceedings
of the International Conference on
Intelligent User Interfaces, pages 60?67,
New Orleans, LA.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Kidd, Evan and Edith Bavin. 2002.
English-speaking children?s
comprehension of relative clauses:
Evidence for general-cognitive and
language-specific constraints on
development. Journal of Psycholinguistic
Research, 31(6):599?617.
Krahmer, E. and M. Theune. 2002. Efficient
context-sensitive generation of referring
expressions. In K. van Deemter and
R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language
Generation and Interpretation, Center for the
Study of Language and Information-Lecture
Notes, volume 143 of CSLI Lecture Notes.
CSLI Publications, Stanford, CA,
pages 233?264.
Krahmer, Emiel, Sebastiaan Van Erk,
and Andre? Verleg. 2003. Graph-based
generation of referring expressions.
Computational Linguistics, 29(1):53?72.
Kukich, Karen. 1983. Design of a
knowledge-based report generator.
In Proceedings of the 21st Annual Meeting of
the Association for Computational
Linguistics, pages 145?150,
Cambridge, MA.
Kurze, Martin. 1995. Giving blind people
access to graphics (example: business
graphics). In Proceedings of the
Software-Ergonomie Workshop, Dannstadt,
Bremen.
Lavoie, Benoit and Owen Rambow. 1997.
A fast and portable realizer for text
generation systems. In Proceedings of
the 5th Conference on Applied Natural
Language Processing, pages 265?268,
Washington, DC.
Lazar, J., A. Allen, J. Kleinman, and
C. Malarkey. 2007. What frustrates screen
reader users on the web: A study of
100 blind users. International Journal of
Human-Computer Interaction, 22(3):247?269.
Lester, James C. and Bruce W. Porter. 1997.
Developing and empirically evaluating
robust explanation generators: The
KNIGHT experiments. Computational
Linguistics, 23(1):65?101.
Lin, Dekang. 1996. On the structural
complexity of natural language
sentences. In Proceedings of the International
Conference on Computational Linguistics,
pages 729?733, Copenhagen, Denmark.
Mann, William C. and Sandra A. Thompson.
1987. Rhetorical structure theory: A theory
of text organization. In Livia Polanyi,
editor, The Structure of Discourse. Ablex
Publishing Corporation, Norwood, NJ.
Marcu, Daniel. 1998. The Rhetorical Parsing,
Summarization, and Generation of Natural
Language Texts. Ph.D. thesis, Department of
Computer Science, University of Toronto.
McCoy, Kathleen F., Sandra Carberry, Tom
Roper, and Nancy Green. 2001. Towards
generating textual summaries of graphs.
In Proceedings of the 1st International
Conference on Universal Access in Human-
Computer Interaction, pages 695?699,
New Orleans, LA.
McCoy, Kathleen F. and Jeannette Cheng.
1991. Focus of attention: Constraining
what can be said next. In Cecile Paris,
William Swartout, and William Mann,
editors, Natural Language Generation in
Artificial Intelligence and Computational
Linguistics. Kluwer Academic Publishers,
Berlin, pages 103?124.
McKeown, Kathleen R. 1985. Discourse
strategies for generating natural-language
text. Artificial Intelligence, 27(1):1?41.
McKeown, Kathleen R., Shimei Pan, James
Shaw, Desmond A. Jordan, and Barry A.
Allen. 1997. Language generation for
multimedia healthcare briefings. In
Proceedings of the 5th Conference on Applied
Natural Language Processing, pages
277?282, Washington, DC.
Meijer, Peter B. 1992. An experimental
system for auditory image representations.
IEEE Transactions on Biomedical Engineering,
39(2):112?121.
Mellish, Chris, Alisdair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search
for text planning. In Proceedings of the
9th International Workshop on Natural
Language Generation, pages 98?107,
Niagara-on-the-Lake.
Moore, Johanna D. and Cecile Paris.
1993. Planning text for advisory
573
Computational Linguistics Volume 38, Number 3
dialogues: Capturing intentional and
rhetorical information. Computational
Linguistics, 19(4):651?694.
Nenkova, Ani and Kathleen McKeown. 2003.
References to named entities: a corpus
study. In Proceedings of the Conference of the
North American Chapter of the Association for
Computational Linguistics on Human
Language Technology, pages 70?72,
Edmonton.
O?Donnell, M., C. Mellish, J. Oberlander, and
A. Knott. 2001. Ilex: an architecture for a
dynamic hypertext generation system.
Natural Language Engineering, 7(3):225?250.
Paris, Cecile. 1988. Tailoring object
descriptions to a user?s level of expertise.
Computational Linguistics, 14(3):64?78.
Pastra, Katerina, Horacio Saggion, and
Yorick Wilks. 2003. Extracting relational
facts for indexing and retrieval of
crime-scene photographs. Knowledge-Based
Systems, 16(5-6):313?320.
Portet, Francois, Ehud Reiter, Albert Gatt,
Jim Hunter, Somayajulu Sripada, Yvonne
Freer, and Cindy Sykes. 2009. Automatic
generation of textual summaries from
neonatal intensive care data. Artificial
Intelligence, 173(7-8):789?816.
Radev, Dragomir R., Hongyan Jing,
Malgorzata Stys, and Daniel Tam. 2004.
Centroid-based summarization of multiple
documents. Information Processing and
Management: An International Journal,
40(6):919?938.
Ramloll, Rameshsharma, Wai Yu, Stephen
Brewster, Beate Riedel, Mike Burton, and
Gisela Dimigen. 2000. Constructing
sonified haptic line graphs for the blind
student: First steps. In Proceedings of the
4th International ACM Conference on
Assistive Technologies, pages 17?25,
Arlington, VA.
Reiter, Ehud. 2007. An architecture for
data-to-text systems. In Proceedings of
the 11th European Workshop on Natural
Language Generation, pages 97?104,
Schloss Dagstuhl.
Reiter, Ehud and Robert Dale. 2000. Building
Natural-language Generation Systems.
Cambridge University Press, Cambridge.
Schiffman, Barry, Ani Nenkova, and
Kathleen McKeown. 2002. Experiments
in multidocument summarization.
In Proceedings of the 2nd International
Conference on Human Language Technology
Research, pages 52?58, San Diego, CA.
Shaw, James. 1998. Clause aggregation using
linguistics knowledge. In Proceedings of
the 9th International Workshop on Natural
Language Generation, pages 138?147,
Niagara-on-the-Lake.
Somayajulu, Sripada, Ehud Reiter, and Ian
Davy. 2003. Sumtime-mousam:
Configurable marine weather forecast
generator. Expert Update, 6(3):4?10.
Stent, A., Rashmi Prasad, and Marilyn
Walker. 2004. Trainable sentence
planning for complex information
presentation in spoken dialog systems.
In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics,
pages 79?86, Barcelona.
Stone, Matthew and Bonnie Webber. 1998.
Textual economy through closely coupled
syntax and semantics. In Proceedings
of the International Natural Language
Generation Conference, pages 178?187,
Niagara-on-the-Lake.
Suri, Linda Z. and Kathleen F. McCoy. 1994.
Raft/rapr and centering: A comparison
and discussion of problems related to
processing complex sentences.
Computational Linguistics, 20(2):301?317.
Turner, Ross, Yaji Sripada, and Ehud Reiter.
2009. Generating approximate geographic
descriptions. In Proceedings of the 12th
European Workshop on Natural Language
Generation, pages 42?49, Athens.
Walker, M., O. Rambow, and M. Rogati. 2002.
Training a sentence planner for spoken
dialogue using boosting. Computer Speech
and Language: Special Issue on Spoken
Language Generation, 16(3):409?434.
Walker, M., A. Stent, F. Mairesse, and
R. Prasad. 2007. Individual and domain
adaptation in sentence planning for
dialogue. Journal of Artificial Intelligence
Research, 30(1):413?456.
Wu, Peng, Sandra Carberry, Stephanie Elzer,
and Daniel Chester. 2010. Recognizing
the intended message of line graphs.
In Proceedings of the International Conference
on the Theory and Application of Diagrams,
pages 220?234, Portland, OR.
Yngve, Victor H. 1960. A model and an
hypothesis for language structure.
American Philosophical Society, 104:444?466.
Yu, Jin, Ehud Reiter, Jim Hunter, and
Chris Mellish. 2007. Choosing the
content of textual summaries of large
time-series data sets. Natural Engineering,
13(1):25?49.
574
Generating Textual Summaries of Bar Charts
Seniz Demir Sandra Carberry Kathleen F. McCoy
Department of Computer Science
University of Delaware
Newark, DE 19716
{demir, carberry, mccoy}@cis.udel.edu
Abstract
Information graphics, such as bar charts and
line graphs, play an important role in mul-
timodal documents. This paper presents a
novel approach to producing a brief textual
summary of a simple bar chart. It outlines
our approach to augmenting the core message
of the graphic to produce a brief summary.
Our method simultaneously constructs both
the discourse and sentence structures of the
textual summary using a bottom-up approach.
The result is then realized in natural language.
An evaluation study validates our generation
methodology.
1 Introduction
Information graphics, such as bar charts and line
graphs, are an important component of a multi-
modal document. However, summarization has fo-
cused primarily on the text of a document. But as
shown in (Carberry et al, 2006), information graph-
ics in magazine and newspaper articles often convey
a message that is not repeated in the article?s text.
Thus information graphics cannot be ignored.
Individuals with sight impairments can access the
text of an electronic document via text to speech
systems. But such individuals are stymied when
they encounter information graphics. The SIGHT
system (Elzer et al, 2007) has the goal of provid-
ing the user with the message and knowledge that
one would gain from viewing the graphic, rather
than providing alternative access to what the graphic
looks like. In the current Bayesian network imple-
mentation, the system uses the communicative sig-
nals present in the graphic to recognize one of the
twelve message categories that can be conveyed by
a bar chart and produces a logical representation
of what we will refer to as the core message con-
veyed by the graphic; this representation is trans-
lated into natural language via templates. For exam-
ple, the system determines that the core message of
the graphic in Figure 1 is that the bar for the United
States has the maximum value of the entities listed,
and the system produces Maximum(First Bar) as the
logical representation of that message. However,
this is insufficient as a summary of the graphic since
it doesn?t convey the particularly significant features
of the graphic such as the fact that the number of
hacker attacks in the United States is far greater than
in the other countries listed.
In this paper, we explore the generation of an
effective initial summary of a bar chart within the
SIGHT system. Input to our system is a logical rep-
resentation of the graphic?s core message (as pro-
duced by SIGHT) and the XML representation of
the graphic which specifies the components of the
graphic such as the number of bars and the heights
of each bar. Our goal is to generate a succinct coher-
ent summary of a graphic that captures its core mes-
sage and the most important and significant features.
At the same time, the summary need not include all
information that could be extracted from the graphic
since future work on SIGHT includes a mechanism
for responding to follow-up questions from the user.
Our work is unique in that it generates a sum-
mary of the content of a bar chart with no domain
restriction and constructs a high-quality but brief
summary by incorporating the graphic?s core mes-
7
C o u n t r i e s  W i t h  t h e  M o s t  H a c k e r  
A t t a c k s ,  2 0 0 2
W o r l d w i d e ,  t h e  a t t a c k s  j u m p e d
U n i t e d  S t a t e s 2 4 , 4 3 4
B r a z i l
B r i t a i n
G e r m a n y
I ta ly
0 5 , 0 0 0 1 0 , 0 0 0 1 5 , 0 0 0 2 0 , 0 0 0
Figure 1: Graphic conveying a maximum bar.
sage and the most important and significant features
of the graphic. Our system produces a coherent or-
ganization of the content of the summary that we
hypothesize lends itself to follow-up questions, ap-
plies the notion of syntactic complexity in choosing
how to aggregate information into sentence-sized
pieces, utilizes a sentence realizer to convey the out-
put in natural language, and applies heuristics for
constructing referents for graphical elements. Thus,
our work addresses several generation problems.
Section 2 discusses related work in the area
of graph summarization. Section 3 presents our
methodology for identifying the content of initial
summaries. Section 4 describes our approach for or-
ganizing the summaries. Section 5 presents some
issues that we addressed in realizing the summaries
and a few example summaries generated by our sys-
tem. Section 6 discusses the results of an evaluation
study that validates our methodology.
2 Related Work
Graph summarization has received some attention.
The SumTime project uses pattern recognition tech-
niques to generate textual summaries of automati-
cally generated time-series data; different systems
have been designed in three domains, such as
SumTime-Turbine (Yu et al, 2007) for data from
gas turbine engines. However, each of these sys-
tems is domain-dependent. We view this as a very
different problem from the one that we address in
this paper, since we are working on domain indepen-
dent graphical representations. The iGRAPH-Lite
system (Ferres et al, 2007), whose main objective
is to make the information in a graphic accessible
to blind users via keyboard commands, uses tem-
plates to provide a short textual summary of what
the graphic looks like, but their summary is not con-
cerned with the higher level knowledge conveyed by
the graphic. The goal of Futrelle?s project (Futrelle,
1999) is to produce a summary graphic that captures
the content of one or more graphics. However, the
end result is itself a graphic, not text.
3 Informational Content of the Summaries
In (McCoy et al, 2001), we reported an informal
experiment in which human subjects were asked to
write a brief summary of a series of line graphs with
the same high-level intention. This experiment led
to three observations:
? the intended message of the graphic was con-
tained in all summaries.
? summaries of the same graphic by different
subjects were similar.
? summaries of different graphics in the same
message category (such as Rising Trend) var-
ied in the information provided.
Subjects included more than the core message of
the graphic in their summaries. The extra infor-
mation could be explained as capturing features of
the graphic that were visually salient. It was hy-
pothesized that what is taken as visually salient in a
graphic relates to the overall message of the graphic.
For example, in the line graphs analyzed, if the over-
all message of the graphic is an increasing trend and
the variance in that trend is large, then the variance
is salient. The fact that the summaries only included
the core message and the visually salient features,
correlates with Grice?s Maxim of Quantity (Grice,
1975) which states that one?s discourse contribution
should be as informative as necessary for the pur-
poses of the current exchange but not more so.
To extend these observations to other kinds of in-
formation graphics, particulary to simple bar charts,
we needed to identify the kinds of features that were
salient with respect to the graphic?s overall message.
For this purpose, we conducted a set of formal ex-
periments with human subjects. Our goal was a set
of rules that took into account the message category
(such as Rising Trend or Rank of Entity) and the vi-
sual features of the graphic to specify what proposi-
tions should be included in the initial summary.
8
3.1 Experimental Setup
Based on our experience with summarizing graph-
ics, we first identified the set of all propositions
(PROP ALL) that reflect information that we envi-
sioned someone might ask about a simple bar chart.
Twenty graduate students from a variety of depart-
ments participated in the experiments. Each subject
was given an information graphic along with a sen-
tence conveying the intended message of the graphic
(as identified by the SIGHT system) and a subset of
PROP ALL, and was asked to classify these addi-
tional propositions into one of three classes accord-
ing to how important they felt it was to include that
proposition in the initial summary: essential, pos-
sible, and not important. Subjects were told to
assume that the graphic was part of an article that
the user is reading and that he would be able to ask
follow-up questions after receiving the summary.
Twenty-one graphics were used in the experi-
ments and each participant was given six graphics
to view. The graphics were either selected from arti-
cles in popular media or constructed from scratch to
present a number of different salient features within
the same graphic. Eight different message categories
were tested in the experiments1 and the graphics
from the same message category reflected different
salient features that had been observed in a corpus
of collected graphics.
3.2 Computationalizing Proposition Selection
To analyze the experiment?s results, we assigned a
numeric score to each category (essential=3, possi-
ble=1, not important=0) indicating the level of im-
portance assigned by the subjects. We computed
an importance level (IL) for each proposition that
might be included in a summary of a graphic, esti-
mating how important it is to be included in the ini-
tial summary. The IL of a proposition is computed
based on the average of the importance assigned by
each subject. We classify a proposition as a highly-
rated proposition in a graphic (and therefore worthy
of being included in the initial summary) if its im-
portance level is equal to or above the importance
level that would be obtained if at least half of the
1We did not test message categories that are the opposite of
categories used in the experiments, such as Minimum which is
the opposite of Maximum and thus can be modeled similarly.
subjects had classified the proposition as essential.
The particular assignments of values to categories
shown above was used in order to emphasize in-
stances in which subjects placed a proposition in the
essential category.
For each message category, we analyzed the sets
of highly-rated propositions identified for the graph-
ics associated with that message category and hand
constructed a set of content identification rules spec-
ifying whether a proposition should be included in
the initial summary. If a proposition was classified
as highly-rated for all graphics in a particular mes-
sage category, the content identification rule stated
that the proposition should be included in the ini-
tial summary for every graphic whose core message
fell into that message category. For the other highly-
rated propositions for a message category, we iden-
tified a feature that was visually salient only in the
graphics where the proposition was marked as es-
sential, and our content identification rule used the
presence of this feature in the graphic as a condi-
tion for the proposition to be included in the initial
summary. Thirty content identification rules were
defined in the system.
Besides the propositions capturing salient features
of the graphic, we observed that the subjects se-
lected propositions whose absence might lead the
user to draw false conclusions by default (for exam-
ple, the propositions indicating that the trend start-
ing from 1997 does not cover the full range of bar
labels in the graphic in Figure 2). We constructed
rules to add such content. This correlates with the
maxim in (Joshi et al, 1984) which states that a sys-
tem should not only produce correct information but
should also prevent the user from drawing false in-
ferences.
The following are glosses of two representative
content identification rules applicable to a graphic
whose core message is an increasing trend:
? If (message category equals ?increas-
ing trend?) then include(propositions con-
veying the rate of increase of the trend)
? If (message category equals ?increas-
ing trend?) and (coverage(graphic) not equal
coverage(trend)) then include(propositions
indicating that the trend does not cover the full
range of bar labels)
To see how our rules might affect the generated
9
summary, consider the graphic in Figure 2. The
SIGHT system recognizes the core message as an
increasing trend from 1997 to 2002. The content
identification rules defined for increasing trend se-
lect the following pieces of additional information
to include in the initial summary of the graphic:
? The overall rate of increase of the trend, which
is moderate
? The range of the bar values in the trend:
$480,000 to $1,230,000
? The fact that there is an unusually steep rise be-
tween 2000 and 2001
? The period that the graphic covers, which is
from 1996 to 2002
4 Organizing Coherent Summaries
At this point in the processing, the system has iden-
tified the informational content to be conveyed in the
initial summary, which consists of the core message
and the set of propositions identified by the content
identification rules. We need to represent and orga-
nize the information to be communicated, determine
how the content should be aggregated into sentence-
sized pieces, and make decisions about referring ex-
pressions (Reiter and Dale, 1997).
To represent and help organize the propo-
sitions that should be included in the ini-
tial summary, we use two kinds of predicates.
Relative predicates, such as differs, are used to
express relations between the graphical elements.
For example, differs(bar(A),maximum bar,50%)
shows that the percentage difference between the
values of bar(A) and the maximum bar is 50%.
Attributive predicates, such as has attribute, are
used to elaborate the graphical elements. For ex-
ample, has attribute(trend,?type?,?increasing?)
shows that the trend observed in the graphic is an
increasing trend. We refer to the first argument of
a predicate as its main entity and the others as sec-
ondary entities.
Since a top-down planning approach does not
guarantee that all propositions will be covered by
the final text plan (Marcu, 1997), we use a bottom-
up planner for structuring the initial summaries. The
propositions selected for inclusion in the initial sum-
maries can be classified as message related, spe-
cific, or computational based on the type of in-
1 9 9 6 2 0 0 12 0 0 01 9 9 81 9 9 7 1 9 9 9 2 0 0 2
4 0 0
0
8 0 0
$ 1 , 2 0 0 , 0 0 0
R i s i n g  J u r y  A w a r d s
M e a n
4 8 0 , 0 0 0
1 , 2 3 0 , 0 0 0
Figure 2: Graphic with an increasing trend.
formation they convey. The core message of the
graphic is captured by the message related proposi-
tions. Specific propositions focus on specific pieces
of information in the graphic such as the propo-
sitions conveying the unusually large rise between
2000 and 2001 in the graphic in Figure 2. On the
other hand, computational propositions require com-
putations or abstractions over the whole graphic,
such as the propositions conveying the rate of in-
crease in the graphic in Figure 2. Once the propo-
sitions to be included in the summary are identified,
we assign them to one of these three classes.
We hypothesize that the message-related class
of propositions should be presented first since this
places emphasis on the core message of the graphic.
We anticipate that the user will ask follow-up ques-
tions after receiving the initial summary. Therefore,
it is appropriate to close the initial summary with
propositions from the computational class so that
the whole graphic is in the user?s focus of atten-
tion (Grosz and Sidner, 1986). Thus we hypothe-
size that a good ordering of propositions in the ini-
tial summary is the message-related class, the spe-
cific class, and finally the computational class. This
produces a partial ordering of the propositions to be
included in the summary.
Each proposition can be realized as a single sen-
tence. For example, shows(graphic,trend) can be
realized as ?The graphic shows a trend? or ?There
is a trend in the graphic?. Consequently, a set of
propositions can be viewed as a set of single sen-
tences. Figure 3 shows the propositions in the mes-
sage related class for the graphic in Figure 2, along
with a possible realization for each.
Although each proposition could be conveyed as
a single sentence, the result is unnatural and not very
10
shows(graphic,trend)
exists(trend,descriptor for dependent axis)
has_attr(trend,type,increasing)
has(trend,period)
starts(period,at,1997)
ends(period,at,2002)
The graphic shows a trend
There is a trend in the mean dollar
value of jury awards
The trend is an increasing trend
There is a trend over the period
The period starts at 1997
The period ends at 2002
Figure 3: Propositions in Message related Class.
coherent. Thus, we define operators to relate propo-
sitions and explore aggregating them. The opera-
tors work on trees; initially, each proposition is the
root of a single node tree. Each tree represents one
or more propositions that can be realized as a sin-
gle sentence, and operators combine individual trees
in a class into more complex trees. The following
are two such operators which work on relative pred-
icates:
? And Operator: This operator combines two
trees if their root propositions share the same
main entity. An And predicate with the same
main entity forms the root of the new tree, and
the trees that are combined form the descen-
dents of this root.
? Which Operator: This operator attaches one
tree as a descendent of another tree, connected
by a Which predicate, if the main entity of the
proposition at the root of the first tree is a sec-
ondary entity in the proposition at the root of
the second tree. That particular entity forms the
main entity of the Which predicate.
One possible result of applying our operators to
the set of propositions (single node trees) in Figure 3
produces the single but more complex tree shown
in Figure 4. The And predicate (***) in Figure 4
is produced by the And Operator and the resultant
subtree is attached to the predicate shows by the
Which Operator.
4.1 Evaluating Structures
Different combinations of operators produce differ-
ent sets of trees, each of which represents a differ-
ent text structure and consequently leads to differ-
shows(graphic,trend)
exists(trend,descriptor for dependent axis)
has_attr(trend,type,increasing)
has(trend,period)
starts(period,at,1997) ends(period,at,2002)
Which(trend)
And(trend)
Which(period)
And(period)
*
**
***
Figure 4: Best Structure of Message related Class.
ent realized text. These structures must be evalu-
ated to determine which one is the best. We don?t
want a structure where each proposition is realized
as a single sentence nor a structure where groups of
propositions are realized with sentences that are too
complex. Our objective is to find a structure which
stands at a mediatory point between these extremes.
Each tree in a structure represents a set of propo-
sitions that can be realized as a single sentence. The
most straightforward way of realizing a tree would
be conjoining the realizations of subtrees rooted by
an And predicate, embedding the realization of a
subtree rooted by a Which predicate as a relative
clause, and realizing a subtree that consists solely
of an attributive predicate as an adjective or a prepo-
sitional phrase. However, care must be taken that the
sentence realization of a tree is not too complex.
Research has used a number of different measures
to assess syntactic complexity of written text and
spoken language samples (Roark et al, 2007). We
apply the notion of syntactic complexity to evalu-
ate the semantic units (predicates) that will be re-
alized. The revised D-level sentence complexity
scale (Covington et al, 2006) forms the core of our
syntactic complexity measure. The D-Level scale
measures the complexity of a sentence according to
the sequence in which children acquire the ability
to use different types of sentences. The sentence
types with the lowest score are those that children
acquire first and therefore are the simplest types.
Among the seven levels defined in the revised D-
Level scale, the levels of interest in our work are
(in order of increasing complexity): simple sen-
tences, conjoined sentences, sentences with a rela-
11
Message Related:
The graphic shows an increasing trend in the mean dol-
lar value of jury awards over the period from 1997 to
2002.
Specific:
The value of these awards ranges from 480,000 dol-
lars to 1,230,000 dollars. The graphic covers a period
from 1996 to 2002. Between 2000 and 2001, an unusu-
ally steep rise is observed in the value of these awards.
Computational:
Moderate increases are observed every year during the
period from 1997 to 2002 in the value of these awards.
Table 1: Realization of the Initial Overall Structure.
Message Related:
Although the graphic covers a period from 1996 to 2002,
the graphic shows an increasing trend in the mean dollar
value of jury awards over the period from 1997 to 2002.
Specific:
The value of these awards ranges from 480,000 dollars to
1,230,000 dollars.
Computational:
Except for a steep rise between 2000 and 2001, moderate
increases are observed every year in the value of these
awards.
Table 2: Realization of the Final Overall Structure.
tive clause, and sentences with more than one level
of embedding. However, the definition of sentence
types at each level is too general. Therefore, we
use a finer distinction between sentence types within
each D-level, such as a simple sentence with more
than one preposition has a higher complexity than a
simple sentence with a single preposition.
To measure the complexity of sentences that will
realize a structure, we define a number of complex-
ity estimators. A tree consisting of a single node is
identified as having the lowest syntactic complex-
ity. We use And predicate and Which predicate es-
timators to estimate the complexity of the sentences
used to realize more complex trees. To do this, esti-
mators look for realization opportunities that would
produce lower complexity values than what the most
straightforward realization would produce. For in-
stance, the And predicate estimators check whether
or not the realizations of two subtrees rooted by
an And predicate can be combined into a single
sentence which is not a compound sentence con-
sisting of two independent sentences. These es-
timators have similar considerations to the clause-
combining operations used by Walker et.al (2002)
in the SPoT sentence planner. For example, one of
the And predicate estimators can successfully deter-
mine that the realizations ?There is a trend in the
mean dollar value of jury awards? and ?There is a
trend over the period from 1997 to 2002?2 can be
combined into ?There is a trend in the mean dollar
value of jury awards over the period from 1997 to
2Assume that the subtree in Figure 4 rooted by
Which(period) can be realized as ?from 1997 to 2002?.
2002? for the tree in Figure 4. This results in a com-
plexity score which is lower than the score of a com-
pound sentence consisting of two conjoined inde-
pendent sentences. The Which predicate estimators
check whether a tree rooted by a Which predicate
can be realized as an adjunct to the modified entity
rather than as a more complex relative clause. For
example, one of the Which predicate estimators can
identify that * and the subtree rooted at ** in Fig-
ure 4 can be realized as ?The graphic shows a trend
in the mean dollar value of jury awards over the pe-
riod from 1997 to 2002?.
Center-embedded relative clauses are more dif-
ficult to comprehend than corresponding right-
branching clauses (Kidd and Bavin, 2002). Our
complexity metric for evaluating the complexity of a
tree penalizes Which predicates that will be realized
as a relative clause in the middle of a sentence more
than one that appears at the end. Once the complex-
ity metric has evaluated the complexity of each tree
in a candidate structure, the complexities of the trees
are summed to get the complexity of the structure.
Our metric for selecting the best structure balances
the number of sentences and their complexity.
The initial overall structure of the summary con-
sists of the best structures for the message related,
specific, and computational classes. As a final step,
we check whether we can improve the evaluation of
the overall structure of the summary by moving trees
or subtrees between the best structures for the three
classes. For example, the best structure for the spe-
cific class might contain a tree that conveys infor-
mation about an entity introduced by a proposition
12
in the message related class. Moving this tree to the
message-related class and using the Which opera-
tor to combine it with the tree introducing the entity
(thereby realizing it as a relative clause) might im-
prove the evaluation of the overall structure of the
summary. To be consistent with the motivation be-
hind the initial groupings of the propositions, we do
not allow movements from the message related class
or any movement that will empty the computational
class. Table 1 presents the summary that our sys-
tem generates for the graphic in Figure 2 before the
movements between classes, and Table 2 presents
the summary after the movements.
5 Realizing Summaries
To realize the summaries in natural language, we
use the FUF/SURGE surface realizer (Elhadad and
Robin, 1996) with some changes made to address a
few problems encountered with respect to the use of
conjunctions and subject-ellipsises. Different strate-
gies are defined in the system for aggregating the
realizations of trees that are linked with operators.
The strategy selected by the system is based on the
relation (such as concession) that holds between the
propositions at the root of the trees and the syntac-
tic forms of their realization opportunities. For ex-
ample, the system uses different strategies for ag-
gregating the trees rooted by the And predicates in
Figure 4, where the tree rooted by And(period) is re-
alized as a combination of prepositional phrases and
the tree rooted by And(trend) is realized as a full
sentence containing a set of prepositional phrases.
5.1 A Descriptor for the Dependent Axis
The dependent axis of an information graphic is of-
ten not labelled with a full descriptor of what is be-
ing measured and therefore a mechanism for extract-
ing an appropriate descriptor had to be developed.
We undertook a corpus analysis and implemented a
system to realize the descriptors (Demir et al, 2007).
Our corpus analysis found that the full descriptor of-
ten must be built from pieces of text extracted from
different places in the graphic. We identified seven
text levels (text components) which form a hierar-
chy (the top being the Overall Caption and the bot-
tom being the Dependent Axis Label), and we ob-
served that the texts at the lower levels are more
? 95 ?00?99?97?96 ?98 ?01 ?02
6
4
2
0
1 0  p e r c e n t
8
A n n u a l  p e r c e n t  c h a n g e  i n  
g l o b a l  o u t p u t
Figure 5: Graphic conveying a contrast change.
likely to contribute to the descriptor than the texts
at the higher levels. We developed a set of heuristics
and augmentation rules for constructing the descrip-
tor for the dependent axis and validated them on a
previously unseen corpus of graphics.
5.2 Referent Generation
The descriptor that our system constructs is always a
noun phrase, but it may be quite long. Our referring
expression generator uses the full descriptor when
the dependent axis is first referenced in the text, but
only the head noun for subsequent references. To
relate the head noun to the descriptor in the text, the
demonstratives ?this? or ?these? is added to the front
of the head noun unless it follows a comparison such
as ?more?. This simple mechanism appears to work
well for our initial summaries.
5.3 Example Summaries
For the graphic in Figure 1, the SIGHT system posits
that the graphic?s core message is that the bar for
the United States has the maximum value among the
bars listed. Our system adds additional propositions
and produces the following summary:
?The graphic shows that United States with 24,434
has the highest number of hacker attacks among
the countries3 Brazil, Britain, Germany, Italy, and
United States. United States has 4.9 times more at-
tacks than the average of other countries.?
For the graphic in Figure 5, the SIGHT system
posits that the core message is that the change at the
last bar is in contrast with the previous increasing
trend. Our system generates the following summary:
3Our system has a module which identifies the ontological
category of the bar labels (Demir et al, 2007).
13
?The graphic shows a small drop in 2002 in contrast
with the increasing trend in annual percent change
in global output over the period from 1995 to 2001.
Except for a small drop in 1996, varying increases
are observed every year during the period from 1995
to 2001 in this change, which shows an overall in-
crease of 485.4 percent and shows a decrease of 8.7
percent between 2001 and 2002.?
6 Evaluation
The work presented in this paper consists of sev-
eral different components, all of which we claim
contribute to the quality of the graphical summary.
Our evaluation focused on three of these and eval-
uated whether or not our decisions with respect to
these components contributed to the perceived qual-
ity of the summary: the organization and ordering of
the informational content (O), the aggregation of the
information into more complex structures (A), and
the metric used to evaluate the structures that repre-
sent different possible aggregations of the informa-
tional content (E). We conducted an experiment with
fifteen participants where they were presented with
four different summaries of twelve graphics from a
variety of domains. We focused on increasing and
decreasing trend graphics since they have the great-
est variety of possible summaries. For each of the
graphics, the participants were asked to rank ran-
domly ordered summaries in terms of their quality
in conveying the informational content. The sum-
maries varied according to the test parameters:
? S O+A+E+: A summary that uses the ordering
rules, the aggregation rules, and is rated highest
by the evaluation metric. This is the summary
produced by our system.
? S O+A+E-: A summary that uses the ordering
and aggregation rules, but was not rated highest
by the evaluation metric.
? S O-A+E+: A summary where the proposi-
tions are randomly ordered, but aggregation
takes place, and the aggregation is rated best
by the evaluation metric.
? S O-A-E-: A summary consisting of single
sentences that are randomly ordered.
The results of the experiment are presented in Ta-
ble 3. It is particularly noteworthy that the summary
generated by our system was most often (65.6% of
Summary type Best 2nd 3rd 4th
S O+A+E+ 65.6% 26.6% 6.7% 1.1%
S O+A+E- 16.7% 32.2% 33.3% 17.8%
S O-A+E+ 16.7% 30% 40% 13.3%
S O-A-E- 1% 11.2% 20% 67.8%
Table 3: Ranking of Summary Types.
the time) rated as the best summary and overwhelm-
ingly (92.2% of the time) rated as one of the top two
summaries. The table shows that omitting the eval-
uation metric (S O+A+E-) or omitting ordering of
propositions (S O-A+E+) results in summaries that
are substantially less preferred by the subjects. We
gained insights into improving the system?s perfor-
mance by looking at the comments made by the sub-
jects when our summary was not selected as the best.
For example, it appears that the subjects did not like
summaries where exceptions were fronted on the
core message with an ?although? clause rather than
following the core message (S O+A+E-). We hy-
pothesize that fronting the exception detracted from
the core message. This is easily remedied in our
evaluation metric. Overall, the results shown in Ta-
ble 3 support our methodology for generating sum-
maries. In the future, we will test the summaries
with blind individuals to determine their effective-
ness in providing alternative access to graphics.
7 Conclusion
This paper has presented our methodology for gen-
erating brief and coherent summaries of simple bar
charts. Our work is the first to address the prob-
lem of summarizing the content of bar charts. We
have presented our approach for identifying the ap-
propriate content of an initial summary, ordering and
aggregating the included propositions, and evaluat-
ing the resultant summary structures to select the
best one. Overall, the results of an evaluation study
validate our ordering, aggregation, and evaluation
methodology.
Acknowledgements
We would like to thank Dr. Stephanie Elzer for her
advice, help, and implementation of the SIGHT sys-
tem upon which this work is built and Dr. Charles
Callaway for his valuable help in addressing the
problems encountered with the realizer.
14
References
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proc. of SIGIR?2006.
Michael A. Covington, Congzhou He, Cati Brown, Lo-
rina Naci, and John Brown 2006. How complex is that
sentence? A proposed revision of the Rosenberg and
Abbeduto D-Level scale. Research Report, AI Center,
University of Georgia.
Seniz Demir, Sandra Carberry, and Stephanie Elzer.
2007. Effectively realizing the inferred message of an
information graphic. In Proc. of RANLP?2007.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic real-
ization component. In Proc. of INLG?1996.
Stephanie Elzer, Edward Schwartz, Sandra Carberry,
Daniel Chester, Seniz Demir, and Peng Wu. 2007.
A browser extension for providing visually impaired
users access to the content of bar charts on the web. In
Proc. of WEBIST?2007.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proc. of ASSETS?2007.
Robert Futrelle. 1999. Summarization of diagrams in
documents. In Advances in Automated Text Summa-
rization. MIT Press, pp. 403-421.
Paul Grice. 1975. Logic and conversation. In Syntax and
Semantics, Speech Acts, 3:41-58.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. In Compu-
tational Linguistics, 12(3):175?204.
Aravind Joshi, Bonnie Webber, and Ralph Weischedel.
1984. Living up to expectations: computing expert
responses. In Proc. of NCAI?1984.
Evan Kidd and Edith Bavin. 2002. English-speaking
childrens comprehension of relative clauses: evidence
for general-cognitive and language-specific constraints
on development. In Journal of Psycholinguistic Re-
search, 31(6):599-617.
Daniel Marcu. 1997. The rhetorical parsing, summariza-
tion, and generation of natural language texts. PhD
thesis, Department of Computer Science, University of
Toronto.
Kathleen F. McCoy, Sandra Carberry, Tom Roper, and
Nancy Green. 2001. Towards generating textual sum-
maries of graphs. In Proc. of HCI?2001.
Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. In Natural Lan-
guage Engineering, 3(1):57?87.
Brian Roark, Margaret Mitchell, and Kristy Holling-
shead. 2007. Syntactic complexity measures for
detecting mild cognitive impairment. In Proc. of
BioNLP?2007.
Marilyn Walker, Owen Rambow, and Monica Rogati.
2002. Training a sentence planner for spoken dialogue
using boosting. In Computer Speech and Language:
Special Issue on Spoken Language Generation, 16(3-
4):409-433.
Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.
2007. Choosing the content of textual summaries
of large time-series data sets. In Natural Language
Engineering,13(1):25?49.
15
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 19?27,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Implications of Pragmatic and Cognitive Theories on the 
Design of Utterance-Based AAC Systems 
 
Kathleen F. McCoy Jan Bedrosian Linda Hoag 
Dept. of Computer 
 and Information Sciences 
Dept. of Speech Pathology and 
Audiology 
Dept. of Communication 
Sciences and Disorders 
University of Delaware Western Michigan University Kansas State University 
Newark, DE 19716, USA Kalamazoo, MI 49008, USA Manhattan, KS 66506, USA 
mccoy@cis.udel.edu jan.bedrosian@wmich.edu lhoag@ksu.edu 
 
 
Abstract 
Utterance-based AAC systems have the poten-
tial to significantly speed communication rate 
for someone who relies on a speech generat-
ing device for communication. At the same 
time, such systems pose interesting challenges 
including anticipating text needs, remember-
ing what text is stored, and accessing desired 
text when needed. Moreover, using such sys-
tems has profound pragmatic implications as a 
prestored message may or may not capture 
exactly what the user wishes to say in a par-
ticular discourse situation. In this paper we 
describe a prototype of an utterance-based 
AAC system whose design choices are driven 
by findings from theoretically driven studies 
concerning pragmatic choices with which the 
user of such a system is faced. These findings 
are coupled with cognitive theories to make 
choices for system design.  
1 Introduction 
There are more than 3.5 million Americans with 
disabilities who cannot effectively use speech to 
communicate (Beukelman & Mirenda, 2005). 
There are many conditions that can result in such 
severe speech impairments including cerebral pal-
sy, autism spectrum disorders, multiple sclerosis, 
amyotrophic lateral sclerosis (ALS), brain-stem 
stroke, Parkinson?s disease, and traumatic brain 
injury (TBI). Any one of these conditions can have 
a negative effect on the quality of life of these 
people. The field of Augmentative and Alternative 
Communication (AAC) has, especially over the 
last ten years, dramatically enhanced access to 
communication for these individuals through the 
use of high-tech systems. These electronic systems 
allow the entering of text that is then converted to 
natural-sounding synthetic speech. While the popu-
lation using AAC systems is quite diverse with 
regard to their linguistic and cognitive skills, here 
we focus on AAC systems for cognitively high-
functioning literate adults with motor impairments.   
Even with a focus on this population, the com-
munication rates of people who use AAC systems 
differ greatly based on their motor abilities and 
available interface choices (Trnka et al, 2009). 
Nevertheless, overall communication rates are 
slow to the extent that they are acknowledged as 
one of the most problematic areas of AAC interac-
tions.  Rates of 10-15 words per minute have been 
identified as upper limits for letter-by-letter selec-
tion on a keyboard (e.g., Wobbrock & Myers, 
2006)?a significant contrast to 130-200 words per 
minute for spoken communication. These slow 
rates and long pauses continue to be a major bar-
rier to the social, educational, and vocational suc-
cess, particularly when communicating with 
unfamiliar partners who have little or no expe-
rience in conversing with someone who uses AAC. 
One method that holds a great deal of promise 
for enhancing communication rate is the use of 
systems that offer a selection of prestored messag-
es. With these systems, a phrase or full sen-
tence/utterance can be selected at once. In such 
systems, sometimes called utterance-based AAC 
systems, people compose whole utterances in ad-
vance and store them for later use. These systems 
appear to be best suited for situations where rela-
tively predictable conversational routines take 
place.  Examples include short, transactional ex-
changes in stores, restaurants, or other public plac-
es where services are provided. 
Although it might appear that utterance-based 
technology could solve the problem of slow com-
19
munication, at least in these predictable exchanges, 
the individual who uses these prestored messages 
must deal with additional challenges to use the 
prestored messages that have been stored in their 
system. Users must be able to: 1) remember that 
they have messages prestored that are appropriate 
for a given situation; 2) remember where these 
messages are stored; and 3) access the desired 
prestored messages with few keystrokes. In addi-
tion, it must be recognized that the prestored mes-
sages are not always going to exactly fit the 
communicative situation in which the user finds 
him/herself (e.g., a prestored message may not 
have enough information for the needs of the part-
ner).This results in a fourth challenge to the user?
to decide if it is better to use the message as stored, 
or either edit or construct a new one. Each chal-
lenge, or trade-off choice, directly affects commu-
nication rate.  
An adequate solution to these challenges has 
proven elusive over the years, despite a long tradi-
tion of research in utterance-based technologies 
(e.g., Todman, 2000; Todman & Alm, 1997; Tod-
man et al, 2008; Vanderheyden et al, 1996). What 
has been lacking is a design process that employs a 
theoretical framework (or perspective) dealing 
with conversation conventions, empirical evidence 
to identify priorities, and systematic testing to de-
termine whether the design enables the communi-
cator to achieve the goals of an interaction.  
A hierarchy of conversational rule violations 
based on a series of experimental studies has a 
great deal of potential to positively influence the 
design of future utterance-based technologies. In 
this paper we first describe a set of such studies 
and the resulting hierarchy. We then discuss the 
implications of this hierarchy on the design of an 
utterance-based AAC system, while integrating 
considerations from cognition and Natural Lan-
guage Processing. Finally, we present our partially 
implemented prototype system and describe plans 
for evaluating this technology. 
2 Theoretical Background 
To shed light on the design of future utterance-
based technologies, studied conversational trade-
off choices that a person faces when using an ut-
terance-based system in goal-directed public situa-
tions with service providers who are unfamiliar 
with AAC, and how the particular choices made 
affect the attitudes and conversational behaviors of 
these providers (Bedrosian et al, 2003; Hoag et al, 
2007; Hoag et al, 2004, 2008; McCoy, et al, 
2007). We were interested in determining which 
message choices resulted in the most favorable 
attitudes and conversational responses leading to 
the success of the AAC customer?s goal in these 
transactional exchanges.  
Notice that no matter how well a user anticipates 
text need, it is inevitable that some prestored mes-
sages are not going to exactly fit the pragmatic 
context in which the user finds him or herself. Four 
public situations (i.e., bookstore, movie theater, 
small convenience store, hair salon) where such 
mismatches could occur were studied in a series of 
investigations. Possible pragmatic mismatches 
were characterized in terms of rule violations ac-
cording to Grice (1975) who articulated a set of 
classic conversational maxims that implicitly guide 
people in exchanging information. Using video-
taped interactions across experiments, these viola-
tions were scripted in messages that involved 
trade-off choices between prestored message use 
and real time message construction. Specifically, 
the trade-offs examined in these investigations 
were between speed of message delivery and a 
message with either: 1) repetitive information with 
repetitive words or phrases; 2) excessive informa-
tion, with more information than was needed by 
the listener but where the information was still top-
ically relevant; 3) inadequate information, lacking 
some of the information needed by the listener, or 
4) partly relevant information, where some of the 
content was not topically relevant. An example of 
such a trade-off involved the message choice of a 
quickly delivered (i.e., 4 seconds) prestored mes-
sage with excessive information or one that was 
delivered slowly (i.e., 90 seconds) to allow editing 
of the excessive information. 
In essence, these experiments simulated situa-
tions where the user was faced with a choice: 
whether to quickly deliver a prestored message that 
was not exactly what was desired because of the 
pragmatic mismatch, or whether to take the time to 
edit the message so that it was exactly what was 
needed. The experiments looked at goal oriented 
situations with unfamiliar partners. This is an ex-
tremely important set of circumstances where the 
attitudes and actions of the communication partner 
can greatly affect whether or not the user can inde-
pendently meet his or her goals. 
20
The experimental hypothesis was that there ex-
isted a hierarchy of conversational maxims involv-
ing the maxims of speed, relevance, repetition, and 
Informativeness, such that adherence to some of 
these maxims would result in more positive eval-
uations by public service providers than others. 
With regard to the results of the experiments, simi-
lar hierarchies of conversational rule violations  
were found across experiments, such that some 
violations, regardless of degree or particular public 
setting, were indeed consistently responded to 
more or less favorably than others. Consistently at 
the bottom of the hierarchy (i.e., responded to least 
favorably in all experimental situations, and with 
less success in meeting the target customer?s goal) 
were quickly delivered messages with only partly 
relevant information. The finding places a high 
priority on selecting entirely relevant messages. As 
such, it suggests the development of a system ar-
chitecture that makes it easy and fast to retrieve 
entirely relevant messages and difficult to retrieve 
messages that are only partly relevant to the cur-
rent exchange.  
On the other hand, consistently at the top of the 
hierarchy were quickly delivered messages with 
repetitive information. These messages were re-
sponded to the most favorably and with much suc-
cess in meeting the target customer?s goal. The 
limited negative impact of the messages with repe-
tition indicated that modification of system design 
to remedy this message flaw would yield less bene-
fit for the user.  
The other trade-off choices, the fast inadequate 
message, the slow adequate message, and the fast 
excessive message, occupied the middle of the hie-
rarchy across the experiments, although their posi-
tions with regard to each other were not exactly the 
same. Thus, the implications of these findings for 
system design are a little less clear, but suggest that 
users given options to edit or easily construct mes-
sages with respect to Informativeness.  
In sum, these findings have several important 
implications for future utterance-based technolo-
gies. A system design must provide a mechanisms 
to maximize the availability of situationally rele-
vant prestored messages.  Additionally, utterance-
based technologies must be integrated seamlessly 
into an AAC system design that allows these pres-
tored messages to be easily edited for their exces-
sive or inadequate information.  Finally, this 
design must also support the on-line construction 
of new messages, while still easily accessing pres-
tored messages when appropriate.  
3 Prototype Development 
The research findings cited above, particularly 
those regarding the critical role of relevance in 
conversation, led to the underlying structure of the 
prototype we are in the process of developing. 
Specifically, we are interested in a prototype that 
will support relevant conversation in familiar rou-
tine exchanges with relatively predictable content, 
such as those that occur in public settings, as it is 
these types of exchanges that provide the best situ-
ations in which to use prestored text. Schank and 
Abelson (1977) suggested that people develop 
mental scripts in such familiar situations (e.g., 
going to a restaurant), and that these scripts 
(representing typical sequences of events) are ac-
cessed by people in order to act appropriately in 
these situations, and understand/interpret what is 
being said. Each script consists of a series of 
scenes (subevents) that previous experience has led 
one to expect to occur. According to the cognitive 
theory, when faced with a new situation (e.g., 
going to a new restaurant), a person can pull up 
his/her mental script and step through the scenes in 
order to participate appropriately.  
We propose an underlying organizational struc-
ture for prestored utterances that leverages this 
mental script notion from cognitive science, as it 
nicely supports the Bedrosian, Hoag, McCoy, and 
Bedrosian findings about relevance. A slightly dif-
ferent notion of scripts has been used in previous 
research in utterance-based technologies (e.g., Dye 
et al, 1998; Alm et al, 2000). The notion referred 
to here is inspired by the early work of Vander-
heyden (1995). In particular, in our prototype sys-
tem the prestored utterances are organized 
(grouped and ordered) according to scenes within a 
script. For example, a ?going-to-a-restaurant? 
script may have scenes associated with entering, 
ordering drinks, ordering entree, paying, etc. Asso-
ciated with each of these scenes are the prestored 
utterances appropriate for use during that scene 
(e.g., utterances pertaining to entering might in-
clude, ?Hello.? ?Fine, thank you.?, ?Non-
smoking.?). 
Not only would this organization ensure the re-
levance of utterances to the current situation, but it 
would also significantly aid the user in remember-
21
ing where these messages are stored so that they 
can be accessed. Essentially the user could direct 
the system to step through messages appropriate 
for each scene of a given script as he/she is actual-
ly experiencing the scene. The utterance-based sys-
tem would have a ?now point? which corresponds 
to the scene in which the user is currently located 
in the script. Utterances useful for the conversation 
during that scene are easily available using very 
few keystrokes. Moreover, because the script mir-
rors the way a user thinks about a typical situation 
and how it flows from one scene to the next, the 
interface could lead the user to utterances appro-
priate for the next scenes to be encountered. Thus, 
users do not need to remember exactly which ut-
terances are stored; they need only to activate the 
appropriate scene in the script to be shown relevant 
messages that can be selected, as well as other 
scenes that may follow. 
At the same time, this underlying structure can 
also provide time-saving benefits to the user with 
respect to entering text. This is in part because of 
its hierarchical organization [see Figure 1, influ-
enced by Vanderheyden (1995)]. At the top of any 
given hierarchy, are the most general scripts which 
can be used in a multitude of new situations (e.g., a 
new type of restaurant that the user has never gone 
to). As shown in the figure, the most general script 
here involves a ?going-to-a-restaurant? script with 
scenes containing ?general purpose text?. For in-
stance, in the ordering scene, slot fillers appropri-
ate for many different kinds of restaurants are 
shown. Below this script, are scripts that pertain to 
more specific types of restaurants (only two are 
explicitly shown in the figure). In these scripts, 
notice some scenes and text are inherited verbatim 
from above, but text may also be added to or mod-
ified as appropriate for the situation and according 
to the preferences of the user. By inherited we 
mean that one or more scenes, with the corres-
ponding messages, from the most general script 
would automatically be made available in the more 
specific instances. Unavailable in other prestored 
text systems, this feature is a significant benefit to 
users, because they only have to enter the informa-
tion one time at the highest level of the hierarchy, 
and yet they will have access to it again in other 
scripts further down in the hierarchy.  
Another advantage of the inheritance is that it 
results in a consistent organization of messages 
across scripts. When accessing any script within 
the restaurant hierarchy, for example, not only can 
users expect to find the entering scene that was 
inherited from the ?parent? script, they can also 
expect to find the prestored utterances ?Hello? and 
?Fine, thank you? near the beginning of that scene. 
This illustrates a memory enhancement feature of 
this system that is not available in other prestored 
text systems ? consistency in placement of mes-
sages from one particular script to another. Over-
all, this underlying organizational structure, which 
we will refer to as a deep structure, represents a 
significant change in the way that utterance-based 
systems in AAC have been designed. With respect 
to appearance, or surface structure, some current 
systems may have, for example, a restaurant 
?page? consisting of a grid of small rectangular 
boxes forming rows and columns across the com-
puter screen. Although each box would contain a 
prestored message appropriate for use in a restau-
rant, there is no deep structure specifying how the 
messages on that page should be organized 
(grouped and ordered) nor how the messages might 
be related (the notion of consistency) to those 
stored on other pages. The only organizing prin-
ciple is that these messages are ?things I can say in 
a restaurant.? If the messages are not ordered (ei-
ther by row or column) in a way that steps the user 
through a scripted sequence of events for a given 
situation, the user must search through a set of 
messages, some of which are unlikely to occur at 
that stage in the interaction. This search process, 
which is likely to include irrelevant messages, may 
slow down the selection process and negatively 
impact the rate of communication. Even if health 
providers or manufacturers programmed messages 
in these boxes to follow such a sequence, this 
would still remain a surface structure ?fix.? The 
strength of our prototype is the deep structure?the 
machinery?such that the consistent location of the 
messages can be easily remembered and accessed 
in a few keystrokes to enhance communication 
rate. Additionally, the hierarchical advantage of the 
deep structure provides the user with a choice of 
scripts (depending on the specificity of the situa-
tion), and saves the user time and energy in enter-
ing text, making the user more independent in 
meeting individual communication needs. 
 
22
 
Figure 1: Hierarchical Script Representation 
 
23
4 Communicating with the system 
In this section we discuss the user interface and 
what the user does in order to actually communi-
cate using the system which has been our focus to 
date. Future work will investigate issues in enter-
ing prestored text into scripts and adapting the 
scripts to the individual user. In a situation where 
the user anticipates using prestored text, he or she 
will be taken to a window menu where the desired 
script (and scene) can be selected. The user may 
then navigate to the script that best fits the actions 
in which he or she is about to engage.  Upon se-
lecting the script, the user will be taken to a screen 
such as that displayed in Figure 2. 
The large window at the top is the display win-
dow. This is where the words of the utterances se-
lected by the user to be spoken will be displayed. 
There is a clear button to clear the display window 
(on the left of the display) and a speak button (the 
arrow on the right-hand-side of the display) that 
causes the display window contents to be sent to 
the speech synthesizer to be spoken. 
The next area of the display helps users keep 
their place and navigate within the chosen script. 
First is the scene map which is a numerical repre-
sentation of the scenes in the current script. From 
this, for instance, users can see that the script they 
have selected contains seven scenes, and the scene 
they are currently performing is scene number one 
which corresponds to the ?enter? scene. The num-
ber of the current scene is colored differently than 
the rest. Below the scene map is a line of tabs, un-
der which are boxes containing prestored text that 
can be selected by the user. In this case, the text for 
the first five scenes of the script are displayed (or 
partially displayed). These scenes are named ?en-
ter?, ?drinks?, ?appetizer?, ?soup/salad?, and 
?entr?e?.  Under each of these scene-name tabs is 
the list of possible prestored utterances associated 
with the scene. For example, there are three pieces 
of text displayed that would be appropriate for the 
?enter? scene. As is the case with the scene-map, 
the current scene (tab and utterances) is colored 
differently from the others so that it is more salient 
to the user. 
Under the boxes are four tabs which bring up 
overlays with some general prestored text that 
might be needed at any time during the script. Ask-
ing for some assistance, talking with the waiter, 
small talk with the table mate, and quickfires are 
just some examples of the kinds of pages that 
might be accessible. Finally, at the bottom of the 
page are some navigation buttons for navigating in 
the device. Here we see buttons that allow the user 
to go to the device home page, move the script 
backward and forward, and go to a page containing 
a keyboard so a novel utterance can be composed. 
The system is set up in a way that allows users 
to select text that they might need while perform-
 
Figure 2: View of Interface with "Entering Scene" Active 
24
ing an action as they step through a scene. Thus, it 
is assumed that the user would select text in left-to-
right order with the left-most scene being the ac-
tive scene (i.e., the scene the user is currently per-
forming).  The user may select one of the boxes in 
the active scene, and the text would be automati-
cally put up into the display window at the top. 
The speak button (arrow in the upper-right corner) 
is used to actually say the desired text. The user 
could select and speak any number of utterances in 
the active scene without any significant changes in 
the display. If the actions the user is performing 
have progressed to the next scene, then the user 
may navigate to the appropriate text in two dif-
ferent ways. First, the user could click on the 
scene map or displayed tabs to have the context 
shift to the new scene. Once selected that scene 
tab and associated text boxes will be shown on 
the left-hand-side of the device. Second, if the 
utterance that the user wishes to say is currently 
visible on the screen, the user may simply select 
that utterance. In this case, in addition to putting 
the utterance in the display window making it 
ready to be spoken, the screen will automatically 
scroll over to display the scene from which the 
utterance was chosen on the far left (revealing 
subsequent scenes to the right of it on the 
screen). Figure 3 displays an example of this kind 
of movement, resulting from the user selecting the 
?I?ll have the nachos? text from the appetizer scene 
displayed in Figure 2. Notice that the scenes have 
been shifted over--the appetizer scene (scene 3) is 
now the active scene, and the text associated with 
the button is now in the display window. 
 
Figure 3 illustrates another feature of the system 
? slot fillers that are specific to a script or scene. 
Notice that ?nachos? is colored differently than the 
other words in this prestored text. This is an indica-
tion that it is a slot-filler and that other options for 
filling that slot are available. To edit that text, the 
user clicks on the highlighted word in the display 
window, and a window such as that in Figure 4 is 
displayed. The user may then select the filler 
he/she desires, and it will replace ?nachos? in the 
display.  
The system described is currently being imple-
mented. Yet to be integrated is a facility that will 
enable more extensive editing of the text in the 
display window and the specifics of easy access to 
typing via an on-screen keyboard (for instances 
where the user wishes to type an utterance from 
scratch rather than using a prestored utterance). 
5 Planned Evaluation 
Two separate comparative efficacy evaluations 
will be conducted to test both the efficiency and 
effectiveness (Schlosser, 1999) of the prototype 
system in contrast to a differently organized pres-
tored text system. In each evaluation, efficiency 
will involve a comparison of the two systems, in a 
training session, with respect to user learning va-
riables (e.g., which system is learned faster, with 
less instruction time, fewer errors/trials). Effec-
tiveness will involve a comparison, in a virtual 
public setting environment with a service provider 
as the partner, dealing with user behavior changes 
and satisfaction (e.g., which system results in faster 
rates of prestored message selection, goal attain-
ment, more satisfaction) and partner attitude and 
Figure 3: Shifting scenes by selecting text from 
appetizer scene 
 
Figure 4: Editing a slot-filler 
25
behavior changes (e.g., which system leads to more 
positive attitudes toward the user, more effective 
conversational behaviors in meeting user goals).  
In the first efficacy evaluation, typically speak-
ing, nondisabled adults will be the participants, 
eliminating bias due to the fact that they will have 
had no previous experience using AAC systems. A 
randomized controlled trial will be employed whe-
reby participants will be assigned to either the pro-
totype system group or the standard system group. 
Each system will contain the same prestored mes-
sages, and the same virtual public setting will be 
used in each group. Results will be used to refine 
the training phase and modify the prototype soft-
ware if necessary. In the second evaluation, a sin-
gle subject experimental design involving an 
adapted alternating treatment design will be em-
ployed with cognitively intact, literate, adult partic-
ipants who currently use prestored text systems. 
Although such a design would expose each partici-
pant to each system (i.e., the prototype system and 
the standard system), carryover effects are elimi-
nated due to counterbalancing the order of the two 
conditions across participants, ensuring that there 
are two equivalent and functionally independent 
instructional sets for the conditions (Schlosser, 
1999) (in this case, the instructional sets would 
involve two virtual public settings and correspond-
ing prestored messages), and counterbalancing the 
sets between conditions.    
6 Related Work 
Storing and retrieving full utterances has been the 
focus of a long tradition of work; Todman et al 
(2008) contains a nice overview of some of these 
systems. The ScripTalker system (Dye et al 
1998a) is closest in theory to our system wit per-
haps the biggest difference being the variety of 
utterances available (and the fact that their proto-
type seemed more geared toward people with low 
literacy skills. While the overall architecture did 
rely on the notion of scripts, the actual utterances 
stored was one per task the user might want to per-
form. I.e., the scripts themselves were linguistic in 
nature. Similar uses were found in other work from 
that same group, for instance see (Alm et al 1995) 
and (Dye et al 1998). In contrast we target users 
with higher literacy skills and more variety in the 
prestored text they might want to have available. 
The script is used to organize the messages but 
there are many messages available within a partic-
ular scene.  
Other work such as the Talk System (Todman & 
Alm, 1997) is intended for social conversation and 
the organization is quite different. As its intention 
is so different, one would expect the stored content 
to need to be updated very often in order to keep it 
current. This is in contrast to the relatively endur-
ing nature expected in the types of conversations 
we envision. 
Another notable system is the FrameTalker 
Project (Higgenbotham & Lesher, 2005) uses a 
looser notion of communication contexts. Our hy-
pothesis is the structure used there does not impose 
enough organization over the utterances, especially 
in the type of situations we envision for use. The 
Contact system is a system that combines notions 
from both Talk and the FrameTalker projects. 
Finally, Langer & Hickey (1997) describe a 
whole utterance system that retrieved utterances 
related to keywords via a keyword search on a 
large database of utterances. In contrast, our sys-
tem would provide access to presumably a series of 
utterances relevant to the current situation. 
7 Conclusions 
AAC systems that use prestored text have a great 
deal of potential to speed communication rate and 
improve attitudes of unfamiliar speaking partners 
towards AAC users in public goal-oriented situa-
tions. In this work we applied empirical evidence 
summarized in a hierarchy of conversational rule 
violations (Bedrosian et al 2000) to identify im-
portant principles of successful interaction with 
AAC text. We then attempted to match appropriate 
NLP technologies with these principles in order to 
develop a different viewpoint for an AAC system 
that used prestored text. Our design is based on 
schema-theory (Schank & Abelson, 1977) and en-
forces a structure over the prestored text that will 
minimize irrelevant text and constrain the rest of 
the text so as to facilitate remembering what text is 
stored while minimizing keystrokes needed to se-
lect the text. 
Acknowledgments 
We would like to thank Tim Walsh who is respon-
sible for many of the interface design choices and 
for implementing the prototype system. This work 
is supported by NIH grant #5 R01 DC003670-06. 
26
References  
Alm, N., Morrison, A., & Arnott, J.L. (1995). A com-
munication system based on scripts, plans, and goals 
for enabling non-speaking people to conduct tele-
phone conversations. In Proceedings of the IEEE In-
ternational Conference on Systems, Man and 
Cybernetics. 
Alm, N., Neumann, H., & van Balkom, H. (2000). 
Scripts on an AAC system. In Proceedings of CSUN 
2000, Northridge, CA.  
Bedrosian, J. L., Hoag, L. A., & McCoy, K. F. (2003). 
Relevance and speed of message delivery trade-offs 
in augmentative and alternative communication. 
Journal of Speech, Language, and Hearing Research, 
46, 800-817. 
Beukelman, D. R., & Mirenda, P. (2005). Augmentative 
and alternative communication: Supporting children 
and adults with complex communication needs (3rd 
ed.). Baltimore, MD: Paul H. Brookes Pub. Co. 
Dye, R., Alm, N., Arnott, J. L., Murray, I.R., & Harper, 
G. (1998a). SrtipTalker - An AAC System Incorpo-
rating Scripts.In Proceedings of the TIDE Congress 
(Technology for Inclusive Design and Equality). 
Dye, R., Alm, N., Arnott, J. L., Harper, G., & Morrison, 
A. (1998). A script-based AAC system for transac-
tional interaction. Natural Language Engineering, 4, 
57?71.  
Grice, H. P. (1975). Logic and conversation. In P. Cole 
& J. Morgan (Eds.), Syntax and semantics: Vol. 3 
Speech acts (pp.41-58). New York: Academic Press. 
Higginbotham, D.J. & Lesher, G. (2005). The Frame-
talker Project: Building an Utterance-Based Commu-
nication Device. In Proceedings of CSUN 
Technology and Persons with Disabilities Confe-
rence. 
Hoag, L., Bedrosian, J., & McCoy, K. (2007, Novem-
ber). Effects of maxim violation degree on a hie-
rarchy in AAC. Poster presented at the American 
Speech-Language-Hearing Association Convention, 
Boston, MA. 
Hoag, L. A., Bedrosian, J. L., McCoy, K. F., & Johnson, 
D. (2004). Informativeness and speed of message de-
livery trade-offs in augmentative and alternative 
communication. Journal of Speech, Language, and 
Hearing Research, 47, 1270-1285. 
Hoag, L. A., Bedrosian, J. L., McCoy, K. F., & Johnson, 
D. E. (2008). Hierarchy of conversational rule viola-
tions involving utterance-based augmentative and al-
ternative communication systems. Augmentative and 
Alternative Communication, 24, 149-161.  
Langer, S. & Hickey, M. (1997). Automatic Message 
Indexing and Full Text Retrieval for a Communica-
tion Aid. In Proceedings of Natural Language 
Processing For Communication Aids a Workshop 
Associated with ACL 1997, Madrid, Spain. 
McCoy, K. F., Bedrosian, J. L., Hoag, L. A., & Johnson, 
D. (2007). Brevity and speed of message delivery 
trade-offs in augmentative and alternative communi-
cation. Augmentative and Alternative Communica-
tion, 23, 76-88. 
Schank, R. C., & Abelson, R. P. (1977). Scripts, plans, 
goals and understanding: An inquiry into human         
knowledge structures. Hillsdale, NJ: Erlbaum. 
Schlosser, R. W. (1999). Comparative efficacy of inter-
ventions in augmentative and alternative communica-
tion. Augmentative and Alternative Communication, 
15, 56-68. 
Todman, J. (2000). Rate and quality of conversations 
using a text-storage AAC system: A training study. 
Augmentative and Alternative Communication, 16, 
164-179.  
Todman, J., & Alm, N. (1997). TALK Boards for social 
conversation. Communication Matters, 11, 13-15. 
Todman, J., Alm, N., Higginbotham, J., & File, P. 
(2008). Whole utterance approaches in AAC. Aug-
mentative and Alternative Communication, 24, 235-
254. 
Trnka, K., McCaw, J., Yarrington, D., McCoy, K.F. , & 
Pennington, C. (2009) User interaction with word 
prediction: The effects of prediction quality. ACM 
Transactions on Accessible Computing (TACCESS), 
1,17-34. 
Vanderheyden, P. B. (1995). Organization of pre-stored 
text in alternative and augmentative communication 
systems: An interactive schema-based approach. 
Technical Report #AAC9501, Applied Science and 
Engineering Laboratories, Wilmington, DE.  
Vanderheyden, P.B.,  Demasco, P.W., McCoy, K.F., & 
Pennington, C.A. (1996). A preliminary study into 
Schema-based access and organization of reusable 
text in AAC. In Proceedings of RESNA '96 19th An-
nual Conference, June. 
Wobbrock, J. & Myers, B. (2006). From letters to 
words: Efficient stroke-based word completion for 
trackball text entry. In Proceedings of the ACM 
SIGACCESS Conference on Computers and Acces-
sibility (ASSETS), pp. 2 
27
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 98?106,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automated Skimming in Response to Questions for NonVisual Readers 
?
?
Debra Yarrington Kathleen F. McCoy 
Dept. of Computer and Information Science Dept. of Computer and Information Science 
University of Delaware University of Delaware 
Newark, DE, 19716, USA Newark, DE, 19716, USA 
yarringt@eecis.udel.edu mccoy@cis.udel.edu 
 
  
Abstract 
This paper presents factors in designing a sys-
tem for automatically skimming text docu-
ments in response to a question. The system 
will take a potentially complex question and a 
single document and return a Web page con-
taining links to text related to the question. 
The goal is that these text areas be those that 
visual readers would spend the most time on 
when skimming for the answer to a question. 
To identify these areas, we had visual readers 
skim for an answer to a complex question 
while being tracked by an eye-tracking sys-
tem. Analysis of these results indicates that 
text with semantic connections to the question 
are of interest, but these connections are much 
looser than can be identified with traditional 
Question-Answering or Information Retrieval 
techniques. Instead, we are expanding tradi-
tional semantic treatments by using a Web 
search. The goal of this system is to give non-
visual readers information similar to what vis-
ual readers get when skimming through a 
document in response to a question. 
1 Introduction 
This paper describes semantic considerations in 
developing a system for giving nonvisual readers 
information similar to what visual readers glean 
when skimming through a document in response to 
a question. Our eventual system will be unique in 
that it takes both simple and complex questions, 
will work in an unrestricted domain, will locate 
answers within a single document, and will return 
not just an answer to a question, but the informa-
tion visual skimmers acquire when skimming 
through a document.  
1.1 Goals 
Production of our skimming system will require 
the attainment of three major goals: 
1. Achieving an understanding of what information 
in the document visual skimmers pay attention 
to when skimming in response to a question 
2. Developing Natural Language Processing (NLP) 
techniques to automatically identify areas of text 
visual readers focus on as determined in 1. 
3. Developing a user interface to be used in con-
junction with screen reading software to deliver 
the visual skimming experience. 
In this paper we focus on the first two of these 
goals. Section 2 will discuss experiments analyzing 
visual skimmers skimming for answers to ques-
tions. Section 3 will discuss developing NLP tech-
niques to replicate the results of Section 2. Section 
4 will discuss future work. 
1.2 Impetus 
The impetus for this system was work done by the 
author with college students with visual impair-
ments who took significantly longer to complete 
homework problems than their visually reading 
counterparts. Students used both ScreenReaders, 
which read electronic text aloud, and screen mag-
nifiers, which increase the size of text on a screen. 
While these students were comfortable listening to 
the screenreader reading at rates of up to 500 
words per minute, their experience was quite dif-
ferent from their visual-reading peers. Even after 
listening to an entire chapter, when they wanted to 
return to areas of text that contained text relevant 
to the answer, they had to start listening from the 
beginning and traverse the document again. Doing 
98
homework was a tedious, time-consuming task 
which placed these students at a serious disadvan-
tage. It is clear that individuals with visual im-
pairments struggle in terms of education. By 
developing a system that levels the playing field in 
at least one area, we may make it easier for at least 
some individuals to succeed. 
2 Visual Skimming  
If our intention is to convey to nonvisual readers 
information similar to what visual readers acquire 
when skimming for answers to questions, we first 
must determine what information visual readers get 
when skimming. For our purposes, we were inter-
ested in what text readers focused on in connection 
to a question. While many systems exist that focus 
on answering simple, fact-based questions, we 
were more interested in more complex questions in 
which the answer could not be found using pattern 
matching and in which the answer would require at 
least a few sentences, not necessarily contiguous 
within a document. From an NLP standpoint, lo-
cating longer answers with relevant information 
occuring in more than one place that may or may 
not have words or word sequences in common with 
the question poses an interesting and difficult prob-
lem. The problem becomes making semantic con-
nections within any domain that are more loosely 
associated than the synonyms, hypernyms, hypo-
nyms, etc. provided by WordNet (Felbaum, 1998). 
Indeed, the questions that students had the most 
difficulty with were more complex in nature. Thus 
we needed to find out whether visual skimmers 
were able to locate text in documents relevant to 
complex questions and, if so, what connections 
visual skimmers are making in terms of the text 
they choose to focus on.  
2.1 Task Description 
To identify how visual readers skim documents to 
answer questions, we collected 14 questions ob-
tained from students? homework assignments, 
along with an accompanying document per ques-
tion from which the answer could be obtained. The 
questions chosen were on a wide variety of topics 
and were complex in nature. An example of a typi-
cal question is, ?According to Piaget, what tech-
niques do children use to adjust to their 
environment as they grow?? Documents largely 
consisted of plain text, although each had a title on 
the first page. They held no images and few sub-
titles or other areas users might find visually inter-
esting. Twelve of the documents were two pages in 
length, one was eight pages in length, and one was 
nine pages long. In each case, the answer to the 
question was judged by the researchers to be found 
within a single paragraph in the document.  
Forty-three visual reading subjects skimmed for 
the answer to between 6 ? 13 questions. The sub-
jects sat in front of a computer screen to which the 
Eye Tracker 1750 by Tobii Technologies was in-
stalled. The questions and accompanying docu-
ments were displayed on the computer screen and, 
after being calibrated, subjects were tracked as 
they skimmed for the answer. For the two-page 
documents, the question appeared at the top of the 
first page. For the longer documents, the question 
appeared at the top of each page. Subjects had no 
time limit for skimming and switched pages by 
pressing the space bar. When done skimming each 
document, subjects were asked to select a best an-
swer in multiple choice form (to give them a rea-
son to take the skimming task seriously).  
2.2 Results 
Results showed that subjects were reliably able to 
correctly answer the multiple choice question after 
skimming the document. Of the 510 questions, 423 
(about 86%) were answered correctly. The two 
questions from longer documents were the least 
likely to be answered correctly (one had 10 correct 
answers of 21 total answers, and the other had 10 
incorrect answers and only one correct answer).  
Clearly for the shorter documents, subjects were 
able to get appropriate information out of the doc-
ument to successfully answer the question. With 
that established, we were interested in analyzing 
the eye tracking data to see if there was a connec-
tion between where subjects spent the most time in 
the document and the question. If there was an un-
derstandable connection, the goal then became to 
automatically replicate those connections and thus 
automatically locate places in the text where sub-
jects were most likely to spend the most time. 
The Tobii Eye Tracking System tracks the path 
and length of time a subject gazes at a particular 
point as a subject skims through a document. The 
system allows us to define Areas of Interest (AOIs) 
and then track the number of prolonged gaze points 
99
within those areas of interest. For our analysis, we 
defined areas of interest as being individual para-
graphs. While we purposely chose documents that 
were predominantly text, each had a title as well. 
Titles and the few subtitles and lists that occurred 
in the documents were also defined as separate 
AOIs. For each skimming activity, the eye tracking 
system gave us a gaze plot showing the order in 
which individuals focused on particular areas, and 
a hot spot image showing the gaze points, with 
duration indicated with color intensity, that oc-
curred in each AOI (see Figure 1).  
In looking at the hot spot images, we found that 
subjects used three techniques to peruse a docu-
ment. One technique subjects used was to move 
their gaze slowly throughout the entire document, 
indicating that they were most likely reading the 
document. A second technique used was to move 
randomly and quickly from top to bottom of the 
document (described as ?fixations distributed in a 
rough zig-zag down the page? by McLaughlin in 
reference to speed reading (1969)), without ever 
focusing on one particular area for a longer period 
of time. This technique was the least useful to us 
because it gave very little information A third 
technique was a combination of the first two, in 
which the subject?s gaze darted quickly and ran-
domly around the page, and then appeared to focus 
on a particular area for an extended period of time. 
Figure 1 is a good example of this technique. The 
data from this group was clearly relevant to our 
task since their fixation points clearly showed what 
areas subjects found most interesting while skim-
ming for an answer to a question.  
2.3 Analysis of Skimming Data 
To determine exactly which AOIs subjects focused 
on most frequently, we counted the number of gaze 
points (or focus points) in each AOI (defined as 
paragraphs, titles, subtitles) across all subjects. In 
looking at what information individuals focused on 
while skimming, we found that individuals did fo-
cus on the title and subtitles that occurred in the 
documents. Subjects frequently focused on the first 
paragraph or paragraphs of a document. There was 
less of a tendency, but still a trend for focusing on 
the first paragraph on each page. Interestingly, al-
though a few subjects focused on the first line of 
each paragraph, this was not a common practice. 
This is significant because it is a technique availa-
ble to users of screenreaders, yet it clearly does not 
give these users the same information that visual 
skimmers get when skimming through a document.  
We also wanted to look at AOIs that did not 
have physical features that may have attracted at-
tention. Our conjecture was that these AOIs were 
focused on by subjects because of their semantic 
?
Figure 1. Hot spot image results of skimming for the answer to the question, ?What are two dietary factors 
thought to raise and lower cholesterol?? using the Tobii Eye Tracking System 
100
relationship to the question. Indeed, we did find 
evidence of this. Results indicated that subjects did 
focus on the areas of text containing the answer to 
the question. As an example, one of the questions 
used in the study was, 
?How do people catch the West Nile Vi-
rus?? 
The paragraph with the most gaze points for the 
most subjects was: 
?In the United States, wild birds, especial-
ly crows and jays, are the main reservoir 
of West Nile virus, but the virus is actually 
spread by certain species of mosquitoes. 
Transmission happens when a mosquito 
bites a bird infected with the West Nile vi-
rus and the virus enters the mosquito's 
bloodstream. It circulates for a few days 
before settling in the salivary glands. Then 
the infected mosquito bites an animal or a 
human and the virus enters the host's 
bloodstream, where it may cause serious 
illness. The virus then probably multiplies 
and moves on to the brain, crossing the 
blood-brain barrier. Once the virus 
crosses that barrier and infects the brain 
or its linings, the brain tissue becomes in-
flamed and symptoms arise.? 
This paragraph contains the answer to the ques-
tion, yet it has very few words in common with the 
question. The word it does have in common with 
the question, ?West Nile Virus?, is the topic of the 
document and occurs fairly frequently throughout 
the document, and thus cannot account for sub-
jects' focusing on this particular paragraph. 
 The subjects must have made semantic connec-
tions between the question and the answer that 
cannot be explained by simple word matching or 
even synonyms, hypernyms and hyponyms. In the 
above example, the ability of the user to locate the 
answer hinged on their ability to make a connec-
tion between the word ?catch? in the question and 
its meaning ?to be infected by?. Clearly simple 
keyword matching won?t suffice in this case, yet 
equally clearly subjects successfully identified this 
paragraph as being relevant to the question. This 
suggests that when skimming subjects were able to 
make the semantic connections necessary to locate 
question answers, even when the answer was of a 
very different lexical form than the question. 
Other areas of text focused on also appear to 
have a semantic relationship with the question. For 
example, with the question, 
?Why was Monet?s work criticized by the 
public?? 
the second most frequently focused on paragraph 
was: 
?In 1874, Manet, Degas, Cezanne, Renoir, 
Pissarro, Sisley and Monet put together an 
exhibition, which resulted in a large finan-
cial loss for Monet and his friends and 
marked a return to financial insecurity for 
Monet. It was only through the help of 
Manet that Monet was able to remain in 
Argenteuil. In an attempt to recoup some 
of his losses, Monet tried to sell some of 
his paintings at the Hotel Drouot. This, 
too, was a failure. Despite the financial 
uncertainty, Monet?s paintings never be-
came morose or even all that sombre. In-
stead, Monet immersed himself in the task 
of perfecting a style which still had not 
been accepted by the world at large. Mo-
net?s compositions from this time were ex-
tremely loosely structured, with color 
applied in strong, distinct strokes as if no 
reworking of the pigment had been at-
tempted. This technique was calculated to 
suggest that the artist had indeed captured 
a spontaneous impression of nature.? 
Of the 30 subjects who skimmed this document, 
15 focused on this paragraph, making it the second 
most focused on AOI in the document, second only 
to the paragraph that contained the answer (fo-
cused on by 21 of the subjects). The above para-
graph occurred within the middle of the second 
page of the document, with no notable physical 
attributes that would have attracted attention. Upon 
closer inspection of the paragraph, there are refer-
ences to ?financial loss,? ?financial insecurity,? 
?losses,? ?failure,? and ?financial uncertainty.? 
The paragraph also includes ?morose? and ?somb-
er? and even ?had not been accepted by the world 
at large.? Subjects appeared to be making a con-
nection between the question topic, Monet?s work 
being criticized by the public, and the above terms. 
Intuitively, we do seem to make this connection. 
Yet the connection being made is not straightfor-
ward and cannot be replicated using the direct se-
101
mantic connections that are available via WordNet. 
Indeed, the relationships made are more similar to 
Hovy and Lin?s (1997) Concept Signatures created 
by clustering words in articles with the same edi-
tor-defined classification from the Wall Street 
Journal. Our system must be able to replicate these 
connections automatically. 
 Upon further examination, we found other pa-
ragraphs that were focused on by subjects for rea-
sons other than their physical appearance or 
location, yet their semantic connection to the ques-
tion was even more tenuous. For instance, when 
skimming for the answer to the question, 
?How does marijuana affect the brain?? 
the second most frequently focused on paragraph 
(second to the paragraph with the answer) was,  
?The main active chemical in marijuana is 
THC (delta-9-tetrahydrocannabinol). The 
protein receptors in the membranes of cer-
tain cells bind to THC. Once securely in 
place, THC kicks off a series of cellular 
reactions that ultimately lead to the high 
that users experience when they smoke 
marijuana.? 
While this paragraph does appear to have loose 
semantic connections with the question, the con-
nections are less obvious than paragraphs that fol-
low it, yet it was this paragraph that subjects chose 
to focus on. The paragraph is the third to last para-
graph on the first page, so its physical location 
could not explain its attraction to subjects. Howev-
er, when we looked more closely at the previous 
paragraphs, we saw that the first paragraph deals 
with definitions and alternate names for marijuana 
(with no semantic links to the question), and the 
second and third paragraph deal with statistics on 
people who use marijuana (again, with no semantic 
connection to the question). The fourth paragraph, 
the one focused on, represents a dramatic semantic 
shift towards the topic of the question. Intuitively it 
makes sense that individuals skimming through the 
document would pay more attention to this para-
graph because it seems to represent the start of the 
area that may contain the answer, not to mention 
conveying topological information about the layout 
of the document and general content information 
as well.  
Data collected from these experiments suggest 
that subjects do make and skim for semantic con-
nections. Subjects not only glean information that 
directly answers the question, but also on content 
within the document that is semantically related to 
the question. While physical attributes of text do 
attract the attention of skimmers, and thus we must 
include methods for accessing this data as well, it 
is clear that in order to create a successful skim-
ming device that conveys information similar to 
what visual skimmers get when skimming for the 
answer to a question, we must come up with a me-
thod for automatically generating loose semantic 
connections and then using those semantic connec-
tions to locate text skimmers considered relevant 
within the document. 
3 NLP Techniques  
In order to automatically generate the semantic 
connections identified above as being those visual 
skimmers make, we want to explore Natural Lan-
guage Processing (NLP) techniques. 
3.1 Related Research 
Potentially relevant methodologies may be found 
in Open Domain Question Answering Systems. 
Open Domain Question Answering Systems in-
volve connecting questions within any domain and 
potential answers. These systems usually do not 
rely on external knowledge sources and are limited 
in the amount of ontological information that can 
be included in the system. The questions are usual-
ly fact-based in form (e.g., ?How tall is Mt. Ever-
est??). These systems take a question and query a 
potentially large set of documents (e.g., the World 
Wide Web) to find the answer. A common tech-
nique is to determine a question type (e.g., ?How 
many ??? would be classified as ?numerical?, 
whereas ?Who was ??? would be classified as 
?person?, etc.) and then locate answers of the cor-
rect type (Abney et al, 2000; Kwok et al, 2001; 
Srihari and Li, 2000; Galea, 2003). Questions are 
also frequently reformulated for pattern matching 
(e.g., ?Who was the first American Astronaut in 
space?? becomes, ?The first American Astronaut 
in space was? (Kwok et al, 2001; Brill et al, 
2002)). Many systems submit multiple queries to a 
document corpus, relying on redundancy of the 
answer to handle incorrect answers, poorly con-
structed answers or documents that don?t contain 
the answer (e.g., Brill et al, 2002; Kwok et al, 
102
2001). For these queries, systems often include 
synonyms, hypernyms, hyponyms, etc. in the query 
terms used for document and text retrieval (Hovy 
et al,2000; Katz et al, 2005). In an attempt to an-
swer more complex relational queries, Banko et al 
(2007) parsed training data into relational tuples 
for use in classifying text tagged for part of speech, 
chunked into noun phrases, and then tagged the 
relations for probability. Soricut and Brill (2006) 
trained data on FAQ knowledge bases from the 
World Wide Web, resulting in approximately 1 
million question-answer pairs. This system related 
potential answers to questions using probability 
models computed using the FAQ knowledge base. 
Another area of research that may lend useful 
techniques for connecting and retrieving relevant 
text to a question is query-biased text summariza-
tion. With many summarization schemes, a good 
deal of effort has been placed on identifying the 
main topic or topics of the document. In query bi-
ased text summarization, however, the topic is 
identified a priori, and the task is to locate relevant 
text within a document or set of documents. In 
multidocument summarization systems, redundan-
cy may be indicative of relevance, but should be 
eliminated from the resulting summary. Thus a 
concern is measuring relevance versus redundancy 
(Carbonell and Goldstein, 1998; Hovy et al, 2005; 
Otterbacher et al, 2006). Like Question Answering 
systems, many summarization systems simply 
match the query terms, expanded to include syn-
onyms, hypernyms, hyponyms, etc., to text in the 
document or documents (Varadarajan and Hristi-
dis, 2006; Chali, 2002)  
Our system is unique in that it has as its goal not 
just to answer a question or create a summary, but 
to return information visual skimmers glean while 
skimming through a document. Questions posed to 
the system will range from simple to complex in 
nature, and the answer must be found within a sin-
gle document, regardless of the form the answer 
takes. Questions can be on any topic. With com-
plex questions, it is rarely possible to categorize 
the type of question (and thus the expected answer 
type). Intuitively, it appears equally useless to at-
tempt reformulation of the query for pattern match-
ing. This intuition is born out by Soricut and Brill 
(2006) who stated that in their study reformulating 
complex questions more often hurt performance 
than improved it. Answering complex questions 
within a single document when the answer may not 
be straightforward in nature poses a challenging 
problem. 
3.2 Baseline Processing 
Our baseline system attempted to identify areas of 
interest by matching against the query in the tradi-
tion of Open Domain Question Answering. For our 
baseline, we used the nonfunction words in each 
question as our query terms. The terms were 
weighted with a variant of TF/IDF (Salton and 
Buckley, 1988) in which terms were weighted by 
the inverse of the number of paragraphs they oc-
curred in within the document. This weighting 
scheme was designed to give lower weight to 
words associated with the document topic and thus 
conveying less information about relevance to the 
question. Each query term was matched to text in 
each paragraph, and paragraphs were ranked for 
matching using the summation of, for each query 
term, the number of times it occurred in the para-
graph multiplied by its weight.  
Results of this baseline ranking were poor. In 
none of the 14 documents did this method connect 
the question to the text relevant to the answer. This 
was expected. This original set of questions was 
purposely chosen because of the complex relation-
ship between the question and answer text. 
Next we expanded the set of query terms to in-
clude synonyms, hypernyms, and hyponyms as 
defined in WordNet (Felbaum, 1998). We included 
all senses of each word (query term). Irrelevant 
senses resulted in the inclusion of terms that were 
no more likely to occur frequently than any other 
random word, and thus had no effect on the result-
ing ranking of paragraphs. Again, each of the 
words in the expanded set of query terms was 
weighted as described above, and paragraphs were 
ranked accordingly. 
Again, results were poor. Paragraphs ranked 
highly were no more likely to contain the answer, 
nor were they likely to be areas focused on by the 
visual skimmers in our collected skimming data.  
Clearly, for complex questions, we need to ex-
pand on these basic techniques to replicate the se-
mantic connections individuals make when 
skimming. As our system must work across a vast 
array of domains, our system must make these 
connections ?on the fly? without relying on pre-
viously defined ontological or other general know-
ledge. And our system must work quickly: asking 
103
individuals to wait long periods of time while the 
system creates semantic connections and locates 
appropriate areas of text would defeat the purpose 
of a system designed to save its users time. 
3.3 Semantically-Related Word Clusters 
Our solution is to use the World Wide Web to form 
clusters of topically-related words, with the topic 
being the question. The cluster of words will be 
used as query terms and matched to paragraphs as 
described above for ranking relevant text.  
Using the World Wide Web as our corpus has a 
number of advantages. Because of the vast number 
of documents that make up the World Wide Web, 
we can rely on the redundancy that has proved so 
useful for Question Answering and Text Summari-
zation systems. By creating the word clusters from 
documents returned from a search using question 
words, the words that occur most frequently in the 
related document text will most likely be related in 
some way to the question words. Even relatively 
infrequently occurring word correlations can most 
likely be found in some document existing on the 
Web, and thus strangely-phrased questions or 
questions with odd terms will still most likely 
bring up some documents that can be used to form 
a cluster. The Web covers virtually all domains. 
Somewhere on the Web there is almost certainly an 
answer to questions on even the most obscure top-
ics. Thus questions containing words unique to 
uncommon domains or questions containing un-
usual word senses will return documents with ap-
propriate cluster words. Finally, the Web is 
constantly being updated. Terms that might not 
have existed even a year ago will now be found on 
the Web. 
Our approach is to use the nonstop words in a 
question as query terms for a Web search. The 
search engine we are using is Google 
(www.google.com). For each search engine query, 
Google returns an ranked list of URLs it considers 
relevant, along with a snippet of text it considers 
most relevant to the query (usually because of 
words in the snippet that exactly match the query 
terms). To create the cluster of words related se-
mantically to the question, we are taking the top 50 
URLs, going to their correlating Web page, locat-
ing the snippet of text within the page, and creating 
a cluster of words using a 100-word window sur-
rounding the snippet. We are using only nonstop 
words in the cluster, and weighting the words 
based on their total number of occurrences in the 
windows. These word clusters, along with the ex-
panded baseline words, are used to locate and rank 
paragraphs in our question document. 
Our approach is similar in spirit to other re-
searchers using the Web to identify semantic rela-
tions. Matsuo et al (2006) looked at the number of 
hits of each of two words as a single keyword ver-
sus the number of hits using both words as key-
words to rate the semantic similarity of two words. 
Chen et al (2006) used a similar approach to de-
termine the semantic similarity between two 
words: with a Web search using word P as the 
query term, they counted the number of times word 
Q occurred in the snippet of text returned, and vice 
versa. Bollegala et al (2007) determined semantic 
relationships by extracting lexico-syntactic patterns 
from the snippets returned from a search on two 
keywords (e.g.,??x? is a ?y??) and extracting the 
relationship of the two words based on the pattern. 
Sahami and Heilman (2006) used the snippets from 
a word search to form a set of words weighted us-
ing TF/IDF, and then determined the semantic si-
milarity of two keywords by the similarity of two 
word sets returned in those snippets.  
Preliminary results from our approach have been 
encouraging. For example, with the question, 
?How does Marijuana affect the brain??, the ex-
panded set of keywords included, ?hippocampus, 
receptors, THC, memory, neuron?. These words 
were present in both the paragraph containing the 
answer and the second-most commonly focused on 
paragraph in our study. While neither our baseline 
nor our expanded baseline identified either para-
graph as an area of interest, the semantically-
related word clusters did. 
4 Future Work 
This system is a work in progress. There are many 
facets still under development, including a finer 
analysis of visual skimming data, a refinement of 
the ranking system for locating areas of interest 
within a document, and the development of the 
system?s user interface. 
4.1 Skimming Data Analysis 
For our initial analysis, we focused on the length of 
time users spent gazing at text areas. In future 
104
analysis, we will look at the order of the gaze 
points to determine exactly where the subjects first 
gazed before choosing to focus on a particular 
area. This may give us even more information 
about the type of semantic connection subjects 
made before choosing to focus on a particular area. 
In addition, in our initial analysis, we defined AOIs 
to be paragraphs. We may want to look at smaller 
AOIs. For example, with longer paragraphs, the 
text that actually caught the subject?s eye may have 
occurred only in one portion of the paragraph, yet 
as the analysis stands now the entire content of the 
paragraph is considered relevant and thus we are 
trying to generate semantic relationships between 
the question and potentially unrelated text. While 
the system only allows us to define AOIs as rec-
tangular areas (and thus we can?t do a sentence-by-
sentence analysis), we may wish to define AOIs as 
small as 2 lines of text to narrow in on exactly 
where subjects chose to focus.  
4.2 Ranking System Refinement 
It is worth mentioning that, while a good deal of 
research has been done on evaluating the goodness 
of automatically generated text summaries (Mani 
et al,2002; Lin and Hovy, 2003; Santos et al, 
2004) our system is intended to mimic the actions 
of skimmers when answering questions, and thus 
our measure of goodness will be our system?s 
ability to recreate the retrieval of text focused on 
by our visual skimmers. This gives us a distinct 
advantage over other systems in measuring good-
ness, as defining a measure of goodness can prove 
difficult. In future work, we will be exploring dif-
ferent methods of ranking text such that the system 
returns results most similar to the results obtained 
from the visual skimming studies. The system will 
then be used on other questions and documents and 
compared to data to be collected of visual skim-
mers skimming for answers to those questions. 
Many variations on the ranking system are poss-
ible. These will be explored to find the best 
matches with our collected visual skimming data. 
Possibilities include weighting keywords different-
ly according to where they came from (e.g., direct-
ly from the question, from the text in retrieved 
Web pages, from text from a Web page ranked 
high on the returned URL list or lower, etc.), or 
considering how a diversity of documents might 
affect results. For instance, if keywords include 
?falcon? and ?hawk? the highest ranking URLs will 
most likely be related to birds. However, in G.I. 
Joe, there are two characters, Lieutenant Falcon 
and General Hawk. To get the less common con-
nection between falcon and hawk and G.I. Joe, one 
may have to look for diversity in the topics of the 
returned URLs. Another area to be explored will 
be the effect of varying the window size surround-
ing the snippet of text to form the bag of words.  
4.3 User Interface 
The user interface for our system poses some inter-
esting questions. It is important that the output of 
the system provide the user with information about 
(1) document topology, (2) document semantics, 
and (3) information most relevant to answering the 
question. At the same time, it is important that us-
ing the output be relatively fast. The output of the 
system is envisioned as a Web page with ranked 
links at the top pointing to sections of the text like-
ly to be relevant to answering the question.  
An important issue that must be explored in 
depth with potential users of the system is the ex-
act form of the output web page. We need to ex-
plore the best method for indicating text areas of 
interest and the overall topology. The goal is that 
reading the links simulate what a visual skimmer 
gets from lightly skimming. The user would actual-
ly follow the links that appeared to be ?worth read-
ing? in more detail in the same way that skimmers 
focus in on particular text segments that appear 
worth reading.  
5 Conclusion 
This system attempts to correlate NLP techniques 
for creating semantic connections with the seman-
tic connections individuals make. Using the World 
Wide Web, we may be able to make those seman-
tic connections across any topic in a reasonable 
amount of time without any previously defined 
knowledge. We have ascertained that people can 
and do make semantic links when skimming for 
answers to questions, and we are currently explor-
ing the best use of the World Wide Web in repli-
cating those connections. In the long run, we 
envision a system that is user-friendly to nonvisual 
and low vision readers that will give them an intel-
ligent way to skim through documents for answers 
to questions.  
105
References  
S. Abney, M. Collins, and A. Singhal. 2000. Answer 
Extraction. In Proceedings of ANLP 2000, 296-301. 
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open information extraction 
from the Web. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence, 
2670-2676. 
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using Web 
search engines. In Proceedings of WWW 2007. 757-
766. 
Brill, E., Lin, J., Banko, M., Domais, S. and Ng, A. 
2001. Data-Intensive Question Answering. In Pro-
ceedings of the TREC-10 Conference, NIST, Gai-
thersburg, MD, 183-189. 
J. Carbonell and J. Goldstein. 1998. The use of MMR, 
diversity-based reranking for reordering documents 
and producing summaries. In Proceedings of SIGIR 
?98, New York, NY, USA, 335-336. 
Y. Chali. 2002. Generic and query-based text summari-
zation using lexical cohesion, In Proceedings of the 
Fifteenth Canadian Conference on Artificial Intelli-
gence, Calgary, May, 293-303. 
H. Chen, M. Lin, and Y. Wei. 2006. Novel association 
measures using web search with double checking. In 
Proceedings of the COLING/ACL 2006. 1009-1016. 
C. Felbaum. 1998. WordNet an Electronic Database, 
Boston/Cambridge: MIT Press. 
A. Galea.2003. Open-domain Surface-Based Question 
Answering System. In Proceedings of the Computer 
Science Annual Workshop (CSAW), University of 
Malta. 
E. H. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C.-
Y. Lin. 2000. Question Answering in Webclopedia. 
In Proceedings of the TREC-9 Conference. NIST, 
Gaithersburg, MD. November 2000. 655-664. 
E. Hovy and C.Y. Lin. 1997. Automated Text Summa-
rization in SUMMARIST. In Proceedings of the 
Workshop on Intelligent Scalable Text Summariza-
tion, Madrid, Spain, 18-24. 
Boris Katz, Gregory Marton, Gary Borchardt, Alexis 
Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-
Rosenberg, Ben Lu, Federico Mora, Stephan Stiller, 
Ozlem Uzuner, and Angela Wilcox. 2005. External 
Knowledge Sources for Question Answering Pro-
ceedings of the 14th Annual Text REtrieval Confe-
rence (TREC2005), November 2005, Gaithersburg, 
MD. 
C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling 
Question Answering to the Web. In Proceedings of 
the 10th World Wide Web Conference, Hong Kong, 
150-161. 
M. Sahami and T. Heilman. 2006. A Web-based kernel 
function for measuring the similarity of short text 
snippets. In Proceedings of 15th International World 
Wide Web Conference. 377-386. 
Chin-Yew Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence Sta-
tistics, In Proceedings of HLT-NAACL, 71?78. 
I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, 
and B. Sundheim. 2002. SUMMAC: a text summari-
zation evaluation, Natural Language Engineering, 8 
(1):43-68. 
Y, Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka. 
2006. Graph-based word clustering using Web search 
engine. In Proceedings of EMNLP 2006, 542-550. 
G. Harry McLaughlin. 1969. Reading at ?Impossible? 
Speeds. Journal of Reading, 12(6):449-454,502-510. 
Radu Soricut and Eric Brill. 2006. Automatic question 
answering using the web: Beyond the factoid. Jour-
nal of Information Retrieval - Special Issue on Web 
Information Retrieval, 9:191?206. 
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing & Management, 24, 5, 513-523. 
E. J. Santos, A. A. Mohamed, and Q. Zhao. 2004. "Au-
tomatic Evaluation of Summaries Using Document 
Graphs," Text Summarization Branches Out. Pro-
ceedings of the ACL-04 Workshop, Barcelona, Spain, 
66-73. 
R. Srihari and W.A. Li. 2000. Question Answering Sys-
tem Supported by Information Extraction. In Pro-
ceedings of the 1st Meeting of the North American 
Chapter of the Association for Computational Lin-
guistics (NAACL-00), 166-172. 
R. Varadarajan and V. Hristidis. 2006. A system for 
query-specific document summarization, ACM 15th 
Conference on Information and Knowledge Man-
agement (CIKM), Arlington, VA, 622-631. 
 
106
A Discourse-Aware Graph-Based Content-Selection Framework
Seniz Demir Sandra Carberry Kathleen F. McCoy
Department of Computer Science
University of Delaware
Newark, DE 19716
{demir,carberry,mccoy}@cis.udel.edu
Abstract
This paper presents an easy-to-adapt,
discourse-aware framework that can be
utilized as the content selection compo-
nent of a generation system whose goal is
to deliver descriptive texts in several turns.
Our framework involves a novel use of a
graph-based ranking algorithm, to itera-
tively determine what content to convey to
a given request while taking into account
various considerations such as capturing a
priori importance of information, convey-
ing related information, avoiding redun-
dancy, and incorporating the effects of dis-
course history. We illustrate and evaluate
this framework in an accessibility system
for sight-impaired individuals.
1 Introduction
Content selection is the task responsible for deter-
mining what to convey in the output of a gener-
ation system at the current exchange (Reiter and
Dale, 1997). This very domain dependent task
is extremely important from the perspective of
users (Sripada et al, 2001) who have been ob-
served to be tolerant of realization problems as
long as the appropriate content is expressed. The
NLG community has proposed various content
selection approaches since early systems (Moore
and Paris, 1993; McKeown, 1985) which placed
emphasis on text structure and adapted planning
techniques or schemas to meet discourse goals.
This paper proposes a domain-independent
framework which can be incorporated as a content
selection component in a system whose goal is to
deliver descriptive or explanatory texts, such as the
ILEX (O?Donnell et al, 2001), KNIGHT (Lester
and Porter, 1997), and POLIBOX (Chiarcos and
Stede, 2004) systems. At the core of our frame-
work lies a novel use of a graph-based ranking al-
gorithm, which exploits discourse related consid-
erations in determining what content to convey in
response to a request for information. This frame-
work provides the ability to generate successive
history-aware texts and the flexibility to generate
different texts with different parameter settings.
One discourse consideration is the tenet that the
propositions selected for inclusion in a text should
be in some way related to one another. Thus,
the selection process should be influenced by the
relevance of information to what has already been
selected for inclusion. Moreover, we argue that
if the information given in a proposition can be
deduced from the information provided by any
other proposition in the text, this would introduce
redundancy and should be avoided.
Many systems (such as MATCH (Walker et al,
2004) and GEA (Carenini and Moore, 2006)) con-
tain a user model which is employed to adapt con-
tent selection to the user?s preferences (Reiter and
Dale, 1997). Our framework provides a facility
to model a stereotypical user by incorporating the
a priori importance of propositions. This facility
can also be used to capture the preferences of a
particular user.
In a dialogue system, utterances that are gen-
erated without exploiting the previous discourse
seem awkward and unnatural (Moore, 1993). Our
framework takes the previous discourse into ac-
count so as to omit recently communicated propo-
sitions and to determine when repetition of a pre-
viously communicated proposition is appropriate.
To our knowledge, our work is the first effort
utilizing a graph-based ranking algorithm for con-
tent selection, while taking into account what in-
formation preferably should and shouldn?t be con-
veyed together, the a priori importance of infor-
mation, and the discourse history. Our framework
is a domain-independent methodology containing
domain-dependent features that must be instanti-
ated when applying the methodology to a domain.
Section 2 describes our domain-independent
methodology for determining the content of a re-
sponse. Section 3 illustrates its application in an
accessibility system for sight-impaired individuals
and shows the generation flexibility provided by
this framework. Finally, Section 4 discusses the
results of user studies conducted to evaluate the
effectiveness of our methodology.
2 A Graph-based Content Selection
Framework
Our domain-independent framework can be ap-
plied to any domain where there is a set of proposi-
tions that might be conveyed and where a bottom-
up strategy for content selection is appropriate. It
is particularly useful when the set of propositions
should be delivered a little at a time. For exam-
ple, the ILEX system (O?Donnell et al, 2001) uses
multiple descriptions to convey the available infor-
mation about a museum artifact, since the length
of the text that can be displayed on a page is lim-
ited. In order to use our framework, an application
developer should identify the set of propositions
that might be conveyed in the domain, specify the
relations between these propositions, and option-
ally assess a priori importance of the propositions.
Our framework uses a weighted undirected
graph (relation graph), where the propositions
are captured as vertices of the graph and the
edges represent relations between these proposi-
tions. While the number and kinds of relations
represented is up to the developer, the frame-
work does require the use of one specific rela-
tion (Redundancy Relation) that is generalizable
to any descriptive domain. Redundancy Relation
must be specified between two propositions if they
provide similar kinds of information or the infor-
mation provided by one of the propositions can
be deduced from the information provided by the
other. For example, consider applying the frame-
work to the ILEX domain. Since the proposition
that ?this jewelry is produced by a single crafts-
man? can be deduced from the proposition that
?this jewelry is made by a British designer?, these
propositions should be connected with a Redun-
dancy Relation in the relation graph.
There is at most one edge between any two ver-
tices and the weight of that edge represents how
important it is to convey the corresponding propo-
sitions in the same text (which we refer to as
the strength of the relation between these proposi-
tions). For example, suppose that once a museum
artifact is introduced in ILEX, it is more impor-
tant to convey its design style in the same descrip-
tion as opposed to where it is produced. In this
case, the weight of the edge between the proposi-
tions introducing the artifact and its style should
be higher than the weight of the edge between the
propositions introducing the artifact and its pro-
duction place.
The framework incorporates a stereotyp-
ical user model via an additional vertex
(priority vertex) in the relation graph. The
priority vertex is connected to all other vertices
in the graph. The weight of the edge between
a vertex and the priority vertex represents the a
priori importance of that vertex, which in turn
specifies the importance of the corresponding
proposition. For example, suppose that in the
ILEX domain an artifact has two features that
are connected to the proposition introducing the
artifact by the ?feature-of? relation. The a priori
importance of one of these features over the
other can be specified by giving a higher weight
to the edge connecting this proposition to the
priority vertex than is given to the edge between
the other feature and the priority vertex. This
captures a priori importance and makes it more
likely that the important feature will be included
in the artifact?s description.
2.1 Our Ranking Algorithm
With this graph-based setting, the most important
thing to say is the proposition which is most cen-
tral. Several centrality algorithms have been pro-
posed in the literature (Freeman, 1979; Navigli
and Lapata, 2007) for calculating the importance
scores of vertices in a graph. The well-known
PageRank centrality (Brin and Page, 1998) calcu-
lates the importance of a vertex by taking into ac-
count the importance of all other vertices and the
relation of vertices to one another. This metric has
been applied to various tasks such as word sense
disambiguation (Sinha and Mihalcea, 2007) and
text summarization (Erkan and Radev, 2004). We
adopted the weighted PageRank metric (Sinha and
Mihalcea, 2007) for our framework and therefore
compute the importance score of a vertex (Vx) as:
PR(V x) = (1? d) + d ?
?
(V x,V y)?E
wyx
?
wyz
(V z ,V y)?E
PR(V y)
where wxy is the weight associated with the edge
between vertices (Vx) and (Vy), E is the set of all
edges, and d is the damping factor, set to 0.85,
which is its usual setting.
Once the propositions in a domain are captured
in a relation graph with weights assigned to the
edges between them, the straightforward way of
identifying the propositions to be conveyed in the
generated text would be to calculate the impor-
tance of each vertex via the formula above and
then select the k vertices with the highest scores.
However, this straightforward application would
fail to address the discourse issues cited earlier.
Thus we select propositions incrementally, where
with each proposition selected, weights in the
graph are adjusted causing related propositions to
be highlighted and redundant information to be re-
pelled. Because our responses are delivered over
several turns, we also adjust weights between re-
sponses to reflect that discourse situation.
Our algorithm, shown in Figure 1, is run each
time a response text is to be generated. For each
new response, the algorithm begins by adjusting
the importance of the priority vertex (making it
high) and clearing the list of selected propositions.
Step 2 is the heart of the algorithm for generating a
single response. It incrementally selects proposi-
tions to include in the current response, and ad-
justs weights to reflect what has been selected.
In particular, in order to select a proposition, im-
portance scores are computed using the weighted
PageRank metric for all vertices corresponding to
propositions that have not yet been selected for in-
clusion in this response (Step 2-a), and only the
proposition that receives the highest score is se-
lected (Step 2-b). Then, adjustments are made to
achieve four goals toward taking discourse infor-
mation into account (Steps 2-c thru 2-g) before the
PageRank algorithm is run again to select the next
proposition. Steps 3 and 4 adjust weights to reflect
the completed response and to prepare for gener-
ating the next response.
Our first goal is to reflect the a priori impor-
tance of propositions in the selection process. For
this purpose, we always assign the highest (or
one of the highest) importance scores to the pri-
ority vertex among the other vertices (Steps 1 and
2-g). This will make the priority vertex as influen-
tial as any other neighbor of a vertex when calcu-
lating its importance.
Our second goal is to select propositions that are
relevant to previously selected propositions, or in
terms of the graph-based notation, to attract the
selection of vertices that are connected to the se-
lected vertices. To achieve this, we increase the
importance of the vertices corresponding to se-
lected propositions so that the propositions related
to them have a higher probability of being chosen
as the next proposition to include (Step 2-g).
Our third goal is to avoid selecting propositions
that preferably shouldn?t be communicated with
previously selected propositions if other related
propositions are available. To accomplish this, we
introduce the term repellers to refer to the kinds
of relations between propositions that are dispre-
ferred over other relations once one of the propo-
sitions is selected for inclusion. Once a proposi-
tion is selected, we penalize the weights on the
edges between the corresponding vertex and other
vertices that are connected by a repeller (Step 2-
d). We don?t provide any general repellers in the
framework, but rather this is left for the developer
familiar with the domain; any number (zero or
more) and kinds of relations could be identified as
repellers for a particular application domain. For
example, suppose that in the ILEX domain, some
artifacts (such as necklaces) have as features both
a set of design characteristics and the person who
found the artifact. Once the artifact is introduced,
it becomes more important to present the design
characteristics rather than the person who found
that artifact. This preference might be captured by
classifying the relation connecting the proposition
conveying the person who found it to the proposi-
tion introducing the artifact as arepeller.
Our fourth goal is to avoid redundancy by dis-
couraging the selection of propositions connected
by a Redundancy Relation to previously selected
propositions. Once a proposition is selected, we
identify the vertices (redundant to selected ver-
tices) which are connected to the selected ver-
tex by the Redundancy Relation (Step 2-e). For
each redundant to selected vertex, we penalize the
weights on the edges of the vertex except the edge
connected to the priority vertex (Step 2-f) and
hence decrease the probability of that vertex being
chosen for inclusion in the same response.
We have so far described how the content of a
single response is constructed in our framework.
To capture a situation where the system is engaged
in a dialogue with the user and must generate addi-
tional responses for each subsequent user request,
we need to ensure that discourse flows naturally.
Thus, the ranking algorithm must take the previ-
Figure 1: Our Ranking Algorithm for Content Selection.
ous discourse into account in order to identify and
preferably select propositions that have not been
conveyed before and to determine when repetition
of a previously communicated proposition is ap-
propriate. So once a proposition is included in a
response, we have to reduce its ability to compete
for inclusion in subsequent responses. Thus once a
proposition is conveyed in a response, the weight
of the edge connecting the corresponding vertex
to the priority vertex is reduced (Step 2-c in Fig-
ure 1). Once a response is completed, we penal-
ize the weights of the edges of each vertex that
has been selected for inclusion in the current re-
sponse via a penalty factor (if they aren?t already
adjusted) (Step 3 in Figure 1). We use the same
penalty factor (which is used in Step 2-d in Fig-
ure 1) on each edge so that all edges connected to
a selected vertex are penalized equally. However,
it isn?t enough just to penalize the edges of the ver-
tices corresponding to the communicated proposi-
tions. Even after the penalties are applied, a propo-
sition that has just been communicated might re-
ceive a higher importance score than an uncommu-
nicated proposition1. In order to allow all propo-
sitions to become important enough to be said at
some point, the algorithm increases the weights
of the edges of all other vertices in the graph if
they haven?t already been decreased (Step 4 in Fig-
ure 1), thereby increasing their ability to compete
in subsequent responses. In the current implemen-
tation, the weight of an edge is increased via a
boost factor after a response if it is not connected
to a proposition included in that response. The
1We observed that it might happen if a vertex is connected
only to the priority vertex.
boost factor ensures that all propositions will even-
tually become important enough for inclusion.
3 Application in a Particular Domain
This section illustrates the application of our
framework to a particular domain and how our
framework facilitates flexible content selection.
Our example is content selection in the SIGHT
system (Elzer et al, 2007), whose goal is to pro-
vide visually impaired users with the knowledge
that one would gain from viewing information
graphics (such as bar charts) that appear in popu-
lar media. In the current implementation, SIGHT
constructs a brief initial summary (Demir et al,
2008) that conveys the primary message of a bar
chart along with its salient features. We enhanced
the current SIGHT system to respond to user?s
follow-up requests for more information about the
graphic, where the request does not specify the
kind of information that is desired.
The first step in using our framework is deter-
mining the set of propositions that might be con-
veyed in this domain. In our earlier work (Demir
et al, 2008), we identified a set of propositions
that capture information that could be determined
by looking at a bar chart, and for each message
type defined in SIGHT, specified a subset of these
propositions that are related to this message type.
In our example, we use these propositions as can-
didates for inclusion in follow-up responses. Fig-
ure 2 presents a portion of the relation graph,
where some of the identified propositions are rep-
resented as vertices.
The second step is optionally assessing the a
priori importance of each proposition. In user
Figure 2: Subgraph of the Relation graph for Increasing and Decreasing Trend Message Types.
studies (Demir et al, 2008), we asked subjects to
classify the propositions given for a message type
into one of three classes according to their impor-
tance for inclusion in the initial summary: essen-
tial, possible, and not important. We leverage
this information as the a priori importance of ver-
tices in our graph representation. We define three
priority classes. For the propositions that were not
selected as essential by any participant, we clas-
sify the edges connecting these propositions to the
priority vertex into Possible class. For the propo-
sitions which were selected as essential by a single
participant, we classify the edges connecting them
to the priority vertex into Important class. The
edges of the remaining propositions are classified
into Highly Important class. In this example in-
stantiation, we assigned different numeric scores
to these classes where Highly Important and Pos-
sible received the highest and lowest scores re-
spectively.
The third step requires specifying the relations
between every pair of related propositions and de-
termining the weights associated with these re-
lations in the relation graph. First, we identi-
fied propositions which we decided should be
connected by the Redundancy Relation (such as
the propositions conveying ?the overall amount of
change in the trend? and ?the range of the trend?).
Next, we had to determine other relations and as-
sign relative weights. Instead of defining a unique
relation for each related pair, we defined three re-
lation classes, and assigned the relations between
related propositions to one of these classes:
? Period Relation: expresses a relation be-
tween two propositions that span the same
time period
? Entity Relation: expresses a relation be-
tween two propositions if the entities in-
volved in the propositions overlap
? Contrast Relation: expresses a relation be-
tween two propositions if the information
provided by one of the propositions contrasts
with the information provided by the other
We determined that it was very common in
this domain to deliver contrasting propositions to-
gether (similar to other domains (Marcu, 1998))
and therefore we assigned the highest score to the
Contrast Relation class. For local focusing pur-
poses, it is desirable that propositions involving
common entities be delivered in the same response
and thus the Entity Relation class was given the
second highest score. On the other hand, two
propositions which only share the same period are
not very related and conveying such propositions
in the same response could cause the text to appear
?choppy?. We thus identified the Period Relation
class as a repeller and assigned the second low-
est score to relations in that class. Since we don?t
want redundancy in the generated text, the lowest
score was assigned to the Redundancy Relation
class. The next section shows how associating
particular weights with the priority and relation
classes changes the behavior of the framework.
In the domain of graphics, a collection of de-
scriptions of the targeted kind which would facil-
itate a learning based model isn?t available. How-
ever, the accessibility of a corpus in a new domain
would allow the identification of the propositions
along with their relations to each other and the de-
termination of what weighting scheme and adjust-
ment policy will produce the corpus within reason-
able bounds.
3.1 Generating Flexible Responses
The behavior of our framework is dependent on a
number of design parameters such as the weights
associated with various relations, the identification
of repellers, the a priori importance of informa-
tion (if applicable), and the extent to which con-
veying redundant information should be avoided.
The framework allows the application developer
to adjust these factors resulting in the selection of
different content and the generation of different re-
sponses. For instance, in a very straightforward
setting where the same numeric score is assigned
to all relations, the a priori importance of infor-
mation would be the major determining factor in
the selection process. In this section, we will il-
lustrate our framework?s behavior in SIGHT with
three different scenarios. In each case, the user is
assumed to post two consecutive requests for ad-
ditional information about the graphic in Figure 3
after receiving its initial summary.
In our first scenario (which we refer to as ?base-
setting?), the following values have been given to
various design parameters that must be specified in
order to run the ranking algorithm. 1) The weights
of the relations are set to the numeric scores shown
in the text labelled Edges at the bottom (right side)
of Figure 2. 2) The stopping criteria which speci-
fies the number of propositions selected for inclu-
sion in a follow-up response (Step 2 in Figure 1)
is set to four. 3) The amount of decrease in the
weight of the edge between the priority vertex and
the vertex selected for inclusion (Step 2-c in Fig-
ure 1) is set to that edge?s original weight. Thus,
in our example, the weight of that edge is set to 0
once a proposition has been selected for inclusion.
4) The penalty and the redundancy penalty factors
which are used to penalize the edges of a selected
vertex and the vertices redundant to the selected
vertex (Steps 2-d and 3, and 2-f in Figure 1) are
set to the quotient of the highest numeric score
initially assigned to a relation class divided by the
lowest numeric score initially assigned to a rela-
tion class. A penalized score for a relation class
is computed by dividing its initial score by the
penalty factor. The edges of a vertex are penalized
by assigning the penalized scores to these edges
based on the relations that they represent. This set-
ting guarantees that the weight of an edge which
represents the strongest relation cannot be penal-
ized to be lower than the score initially assigned
to the weakest relation. 5) The boost factor which
is used to favor the selection of previously uncon-
veyed propositions for inclusion in subsequent re-
sponses (Step 4 in Figure 1) is set to the square
root of the penalty factor. Thus, the weights of
the edges connected to vertices of previously com-
municated propositions are restored to their initial
scores slowly.
Since in our example, the initial summary has
already been presented, we treat the propositions
conveyed in that summary (P1 and P5 in Figure 2)
as if they had been conveyed in a follow-up re-
sponse and penalize the edges of their correspond-
ing vertices (Steps 2-c and 3 in Figure 1). Thus,
before we invoke the algorithm to construct the
first follow-up response, the weights of edges of
the graph are as shown in Figure 2-A. Within this
base-setting, SIGHT generates the set of follow-up
responses shown in Figure 3A.
In our first scenario (base-setting), we assumed
that the user is capable of making mathematical
deductions such as inferring ?the overall amount
of change in the trend? from ?the range of the
trend?; thus we identified such propositions as
sharing a Redundancy Relation. Young read-
ers (such as fourth graders) might not find these
propositions as redundant because they are lack-
ing in mathematical skills. In our second sce-
nario, we address this issue by setting the re-
dundancy penalty factor to 1 (Step 2-f in Fig-
ure 1) and thus eliminate the penalty on the Re-
dundancy Relation. Now, for the same graphic,
SIGHT generates, in turn, the second alternative
set of responses shown in Figure 3B. The re-
sponses for the two scenarios differ in the second
follow-up response. In the first scenario, a descrip-
tion of the smallest drop was included. However,
in the second scenario, this proposition is replaced
with the overall amount of change in the trend.
This proposition was excluded in the first sce-
nario because the redundancy penalty factor made
it drop in importance.
Our third scenario shows how altering the
weights assigned to relations may change the re-
sponses. Consider a situation where the Con-
trast Relation is given even higher importance by
doubling its score; this might occur in a univer-
sity course domain where courses on the same
general topic are contrasted. SIGHT would then
generate the third alternative set of follow-up re-
sponses shown in Figure 3C. The algorithm is
more strongly forced to group propositions that
Figure 3: Initial Summary and Follow-up Responses.
are in a contrast relation (shown in bold), which
changes the ranking of these propositions.
4 Evaluation
To determine whether our framework selects ap-
propriate content within the context of an applica-
tion, and to assess the contribution of the discourse
related considerations to the selected content and
their impact on readers? satisfaction, we conducted
two user studies. In both studies, the partici-
pants were told that the initial summary should
include the most important information about the
graphic and that the remaining pieces of informa-
tion should be conveyed via follow-up responses.
The participants were also told that the informa-
tion in the first response should be more important
than the information in subsequent responses.
Our goal in the first study was to evaluate the
effectiveness of our framework (base-setting) in
determining the content of follow-up responses in
SIGHT. To our knowledge, no one else has gener-
ated high-level descriptions of information graph-
ics, and therefore evaluation using implementa-
tions of existing content selection modules in the
domain of graphics as a baseline is not feasible.
Thus, we evaluated our framework by comparing
the content that it selects for inclusion in a follow-
up response for a particular graphic with the con-
tent chosen by human subjects for the same re-
sponse. Twenty one university students partici-
pated in the first study and each participant was
presented with the same four graphics. For each
graphic, the participants were first presented with
its initial summary and the set of propositions (18
different propositions) that were used to construct
the relation graph in our framework. The partic-
ipants were then asked to select the four propo-
sitions that they thought were most important to
convey in the first follow-up response.
For each graphic, we ranked the propositions
with respect to the number of times that they were
selected by the participants and determined the po-
sition of each proposition selected by our frame-
work for inclusion in the first follow-up response
with respect to this ranking. The propositions se-
lected by our framework were ranked by the par-
ticipants as the 1st, 2nd, 3rd, and 5th in the first
graphic, as the 1st, 3rd, 4th, and 5th in the sec-
ond graphic, as the 1st, 2nd, 3rd, and 6th in the
third graphic, and as the 2nd, 3rd, 4th, and 6th
in the fourth graphic. Thus for every graph, three
of the four propositions selected by our frame-
work were also in the top four highly-rated propo-
sitions selected by the participants. Therefore,
this study demonstrated that our content selection
framework selects the most important information
for inclusion in a response at the current exchange.
We argued that simply running PageRank to se-
lect the highly-rated propositions is likely to lead
to text that does not cohere because it may con-
tain unrelated or redundant propositions, or fail
to communicate related propositions. Thus, our
approach iteratively runs PageRank and includes
discourse related factors in order to allow what
has been selected to influence the future selections
and consequently improve text coherence. To ver-
ify this argument, we conducted a second study
with four graphics and two different sets of follow-
up responses (each consisting of two consecutive
responses) generated for each graphic. We con-
structed the first set of responses (baseline) by
running PageRank to completion and selecting the
top eight highly-rated propositions, where the top
four propositions form the first response. The con-
tent of the second set of responses was identified
by our approach. Twelve university students (who
did not participate in the first study) were pre-
sented with these four graphics along with their
initial summaries. Each participant was also pre-
sented with the set of responses generated by our
approach in two graphics and the set of responses
generated by the baseline in other cases; the par-
ticipants were unaware of how the follow-up re-
sponses were generated. Overall, each set of re-
sponses was presented to six participants.
We asked the participants to evaluate the set
of responses in terms of their quality in convey-
ing additional information (from 1 to 5 with 5 be-
ing the best). We also asked each participant to
choose which set of responses (from among the
four sets of responses presented to them) best pro-
vides further information about the correspond-
ing graphic. The participants gave the set of re-
sponses generated by our approach an average rat-
ing of 4.33. The average participant rating for
the set of responses generated by the baseline was
3.96. In addition, the lowest score given to the
set of responses generated by our approach was
3, whereas the lowest score that the baseline re-
ceived was 2. We also observed that the set of re-
sponses generated by our approach was selected
as the best set by eight of the twelve participants.
Three of the remaining four participants selected
the set of responses generated by the baseline as
best (although they gave the same score to a set
of responses generated by our approach). In these
cases, the participants emphasized the wording
of the responses as the reason for their selection.
Thus this study demonstrated that the inclusion of
discourse related factors in our approach, in addi-
tion to the use of PageRank (which utilizes the a
priori importance of the propositions and their re-
lations to each other), contributes to text coherence
and improves readers? satisfaction.
5 Conclusion
This paper has presented our implemented
domain-independent content selection framework,
which contains domain-dependent features that
must be instantiated when applying it to a particu-
lar domain. To our knowledge, our work is the first
to select appropriate content by using an incre-
mental graph-based ranking algorithm that takes
into account the tendency for some information to
seem related or redundant to other information, the
a priori importance of information, and what has
already been said in the previous discourse. Al-
though our framework requires a knowledge engi-
neering phase to port it to a new domain, it handles
discourse issues without requiring that the devel-
oper write code to address them. We have demon-
strated how our framework was incorporated in
an accessibility system whose goal is the genera-
tion of texts to describe information graphics. The
evaluation studies of our framework within that
accessibility system show its effectiveness in de-
termining the content of follow-up responses.
6 Acknowledgements
The authors would like to thank Debra Yarrington
and the members of the NLP-AI Lab at UD for
their help throughout the evaluation of this work.
This material is based upon work supported by the
National Institute on Disability and Rehabilitation
Research under Grant No. H133G080047.
References
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual Web search engine. Computer
Networks and ISDN Systems, 30(1-7):107?117.
G. Carenini and J. Moore. 2006. Generating and eval-
uating evaluative arguments. Artificial Intelligence,
170(11):925?452.
C. Chiarcos and M. Stede. 2004. Salience-Driven Text
Planning. In Proc. of INLG?04.
S. Demir, S. Carberry, and K. F. McCoy. 2008. Gener-
ating Textual Summaries of Bar Charts. In Proc. of
INLG?08.
S. Elzer, E. Schwartz, S. Carberry, D. Chester,
S. Demir, and P. Wu. 2007. A browser extension
for providing visually impaired users access to the
content of bar charts on the web. In Proc. of WE-
BIST?2007.
G. Erkan and D. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
L. C. Freeman. 1979. Centrality in Social Networks: I.
Conceptual Clarification. Social Networks, 1:215?
239.
J. Lester and B. Porter. 1997. Developing and empir-
ically evaluating robust explanation generators: the
KNIGHT experiments. Computational Linguistics,
23(1):65?101.
D. Marcu. 1998. The rhetorical parsing, summariza-
tion, and generation of natural language texts. PhD.
Thesis, Department of Computer Science, University
of Toronto.
K. McKeown. 1985. Discourse strategies for gener-
ating natural-language text. Artificial Intelligence,
27(1):1?41.
J. Moore and C. Paris. 1993. Planning text for advisory
dialogues: capturing intentional and rhetorical infor-
mation. Computational Linguistics, 19(4):651?694.
J. Moore. 1993. Indexing and exploiting a discourse
history to generate context-sensitive explanations.
In Proc. of HLT?93, 165?170.
R. Navigli and M. Lapata. 2007. Graph Connectiv-
ity Measures for Unsupervised Word Sense Disam-
biguation. In Proc. of IJCAI?07, 1683?1688.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: an architecture for a dynamic hypertext
generation system. In Natural Language Engineer-
ing, 7(3):225?250.
E. Reiter and R. Dale. 1997. Building applied natural
language generation systems. In Natural Language
Engineering, 3(1):57?87.
R. Sinha and R. Mihalcea. 2007. Unsupervised Graph-
based Word Sense Disambiguation Using Measures
of Word Semantic Similarity. In Proc. of ICSC?07.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2001. A
Two-Stage Model for Content Determination. In
Proc. of ENLGW?01.
M. Walker, S.J. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
Generation and evaluation of user tailored responses
in multimodal dialogue. In Cognitive Science,
28(5):811?840.
UDel: Refining a Method of Named Entity Generation
Charles F. Greenbacker, Nicole L. Sparks, Kathleen F. McCoy, and Che-Yu Kuo
Department of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[charlieg|sparks|mccoy|kuo]@cis.udel.edu
Abstract
This report describes the methods and re-
sults of a system developed for the GREC
Named Entity Challenge 2010. We de-
tail the refinements made to our 2009 sub-
mission and present the output of the self-
evaluation on the development data set.
1 Introduction
The GREC Named Entity Challenge 2010 (NEG)
is an NLG shared task whereby submitted systems
must select a referring expression from a list of
options for each mention of each person in a text.
The corpus is a collection of 2,000 introductory
sections from Wikipedia articles about individual
people in which all mentions of person entities
have been annotated. An in-depth description of
the task, along with the evaluation results from the
previous year, is provided by Belz et al (2009).
Our 2009 submission (Greenbacker and Mc-
Coy, 2009a) was an extension of the system we
developed for the GREC Main Subject Refer-
ence Generation Challenge (MSR) (Greenbacker
and McCoy, 2009b). Although our system per-
formed reasonably-well in predicting REG08-
Type in the NEG task, our string accuracy scores
were disappointingly-low, especially when com-
pared to the other competing systems and our own
performance in the MSR task. As suggested by the
evaluators (Belz et al, 2009), this was due in large
part to our reliance on the list of REs being in a
particular order, which had changed for the NEG
task.
2 Method
The first improvement we made to our existing
methods related to the manner by which we se-
lected the specific RE to employ. In 2009, we
trained a series of decision trees to predict REG08-
Type based on our psycholinguistically-inspired
feature set (described in (Greenbacker and Mc-
Coy, 2009c)), and then simply chose the first op-
tion in the list of REs matching the predicted type.
For 2010, we incorporated the case of each RE
into our target attribute so that the decision tree
classifier would predict both the type and case for
the given reference. Then, we applied a series
of rules governing the length of initial and sub-
sequent REs involving a person?s name (following
Nenkova and McKeown (2003)), as well as ?back-
offs? if the predicted type or case were not avail-
able.
Another improvement we made involved our
method of determining whether the use of a pro-
noun would introduce ambiguity in a given con-
text. Previously, we searched for references to
other people entities since the most recent mention
of the entity at hand, and if any were found, we
assumed these would cause the use of a pronoun
to be ambiguous. However, this failed to account
for the fact that personal pronouns in English are
gender-specific (ie. the mention of a male individ-
ual would not make the use of ?she? ambiguous).
So, we refined this by determining the gender of
each named entity (by seeing which personal pro-
nouns were associated with it in the list of REs),
and only noting ambiguity when the current entity
and candidate interfering antecedent were of the
same gender.
Other small changes from 2009 include an ex-
panded abbreviation set in the sentence segmenter,
separate decision trees for the main subject and
other entities, and fixing how we handled embed-
ded REF elements with unspecified mention IDs.
3 Results
Scores for REG08-Type precision & recall, string
accuracy, and string-edit distance are presented in
Figure 1. These were computed on the entire de-
velopment set, as well as the three subsets, us-
ing the geval.pl self-evaluation tool provided in the
NEG participants? pack.
While we were able to achieve an improvement
of nearly 50% over our 2009 scores in string ac-
curacy, we saw less than a 1% gain in overall
REG08-Type performance.
Metric Score
Type Precision/Recall 0.757995735607676
String Accuracy 0.650496141124587
Mean Edit Distance 0.875413450937156
Normalized Distance 0.319266300067796
(a) Scores on the entire development set.
Metric Score
Type Precision/Recall 0.735294117647059
String Accuracy 0.623287671232877
Mean Edit Distance 0.839041095890411
Normalized Distance 0.345490867579909
(b) Scores on the ?Chefs? subset.
Metric Score
Type Precision/Recall 0.790769230769231
String Accuracy 0.683544303797468
Mean Edit Distance 0.882911392405063
Normalized Distance 0.279837251356239
(c) Scores on the ?Composers? subset.
Metric Score
Type Precision/Recall 0.745928338762215
String Accuracy 0.642140468227425
Mean Edit Distance 0.903010033444816
Normalized Distance 0.335326519731057
(d) Scores on the ?Inventors? subset.
Figure 1: Scores on the development set obtained
via the geval.pl self-evaluation tool. REG08-Type
precision and recall were equal in all four sets.
4 Conclusions
The fact that our string accuracy scores improved
over our 2009 submission far more than REG08-
Type prediction is hardly surprising. Our efforts
during this iteration of the NEG task were primar-
ily focused on enhancing our methods of choosing
the best RE once the reference type was selected.
We remain several points below the best-
performing team from 2009 (ICSI-Berkeley), pos-
sibly due to the inclusion of additional items in
their feature set, or the use of Conditional Ran-
dom Fields as their learning technique (Favre and
Bohnet, 2009).
5 Future Work
Moving forward, we hope to expand our feature
set by including the morphology of words immedi-
ately surrounding the reference, as well as a more
extensive reference history, as suggested by (Favre
and Bohnet, 2009). We suspect that these features
may play a significant role in determining the type
of referenced used, the prediction of which acts as
a ?bottleneck? in generating exact REs.
We would also like to compare the efficacy of
several different machine learning techiques as ap-
plied to our feature set and the NEG task.
References
Anja Belz, Eric Kow, and Jette Viethen. 2009. The
GREC named entity generation challenge 2009:
Overview and evaluation results. In Proceedings
of the 2009 Workshop on Language Generation and
Summarisation (UCNLG+Sum 2009), pages 88?98,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Benoit Favre and Bernd Bohnet. 2009. ICSI-
CRF: The generation of references to the main
subject and named entities using conditional ran-
dom fields. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 99?100, Suntec, Singapore,
August. Association for Computational Linguistics.
Charles Greenbacker and Kathleen McCoy. 2009a.
UDel: Extending reference generation to multiple
entities. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 105?106, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Charles Greenbacker and Kathleen McCoy. 2009b.
UDel: Generating referring expressions guided by
psycholinguistc findings. In Proceedings of the
2009 Workshop on Language Generation and Sum-
marisation (UCNLG+Sum 2009), pages 101?102,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Charles F. Greenbacker and Kathleen F. McCoy.
2009c. Feature selection for reference generation as
informed by psycholinguistic research. In Proceed-
ings of the CogSci 2009 Workshop on Production of
Referring Expressions (PRE-Cogsci 2009), Amster-
dam, July.
Ani Nenkova and Kathleen McKeown. 2003. Improv-
ing the coherence of multi-document summaries:
a corpus study for modeling the syntactic realiza-
tion of entities. Technical Report CUCS-001-03,
Columbia University, Computer Science Depart-
ment.
UDel: Named Entity Recognition and Reference Regeneration
from Surface Text
Nicole L. Sparks, Charles F. Greenbacker, Kathleen F. McCoy, and Che-Yu Kuo
Department of Computer and Information Sciences
University of Delaware
Newark, Delaware, USA
[sparks|charlieg|mccoy|kuo]@cis.udel.edu
Abstract
This report describes the methods and re-
sults of a system developed for the GREC
Named Entity Recognition and GREC
Named Entity Regeneration Challenges
2010. We explain our process of automat-
ically annotating surface text, as well as
how we use this output to select improved
referring expressions for named entities.
1 Introduction
Generation of References in Context (GREC) is a
set of shared task challenges in NLG involving a
corpus of introductory sentences from Wikipedia
articles. The Named Entity Recognition (GREC-
NER) task requires participants to recognize all
mentions of people in a document and indicate
which mentions corefer. In the Named Entity Re-
generation (GREC-Full) task, submitted systems
attempt to improve the clarity and fluency of a
text by generating improved referring expressions
(REs) for all references to people. Participants are
encouraged to use the output from GREC-NER as
input for the GREC-Full task. To provide ample
opportunities for improvement, a certain portion
of REs in the corpus have been replaced by more-
specified named references. Ideally, the GREC-
Full output will be more fluent and have greater
referential clarity than the GREC-NER input.
2 Method
The first step in our process to complete the
GREC-NER task is to prepare the corpus for in-
put into the parser by stripping all XML tags and
segmenting the text into sentences. This is accom-
plished with a simple script based on common ab-
breviations and sentence-final punctuation.
Next, the files are run through the Stanford
Parser (The Stanford Natural Language Process-
ing Group, 2010), providing a typed dependency
representation of the input text from which we ex-
tract the syntactic functions (SYNFUNC) of, and
relationships between, words in the sentence.
The unmarked segmented text is also used
as input for the Stanford Named Entity Recog-
nizer (The Stanford Natural Language Processing
Group, 2009). We eliminate named entity tags for
locations and organizations, leaving only person
entities behind. We find the pronouns and com-
mon nouns (e.g. ?grandmother?) referring to per-
son entities that the NER tool does not tag. We
also identify the REG08-Type and case for each
RE. Entities found by the NER tool are marked
as names, and the additional REs we identified
are marked as either pronouns or common nouns.
Case values are determined by analyzing the as-
signed type and any type dependency representa-
tion (provided by the parser) involving the entity.
At this stage we also note the gender of each pro-
noun and common noun, the plurality of each ref-
erence, and begin to deal with embedded entities.
The next step identifies which tagged mentions
corefer. We implemented a coreference resolu-
tion tool using a shallow rule-based approach in-
spired by Lappin and Leass (1994) and Bontcheva
et al (2002). Each mention is compared to all
previously-seen entities on the basis of case, gen-
der, SYNFUNC, plurality, and type. Each en-
tity is then evaluated in order of appearance and
compared to all previous entities starting with the
most recent and working back to the first in the
text. We apply rules to each of these pairs based
on the REG08-Type attribute of the current en-
tity. Names and common nouns are analyzed us-
ing string and word token matching. We collected
extensive, cross-cultural lists of male and female
first names to help identify the gender of named
entities, which we use together with SYNFUNC
values for pronoun resolution. Separate rules gov-
ern gender-neutral pronouns such as ?who.? By
the end of this stage, we have all of the resources
MUC-6 CEAF B-CUBED
Corpus F prec. recall F prec. recall F prec. recall
Entire Set 71.984 69.657 74.471 68.893 68.893 68.893 72.882 74.309 71.509
Chefs 71.094 65.942 77.119 65.722 65.722 65.722 71.245 69.352 73.244
Composers 68.866 66.800 71.064 68.672 68.672 68.672 71.929 73.490 70.433
Inventors 76.170 77.155 75.210 72.650 72.650 72.650 75.443 80.721 70.812
Table 1: Self-evaluation scores for GREC-NER.
necessary to complete the GREC-NER task.
As a post-processing step, we remove all extra
(non-GREC) tags used in previous steps, re-order
the remaining attributes in the proper sequence,
add the list of REs (ALT-REFEX), and write the
final output following the GREC format. At this
point, the GREC-NER task is concluded and its
output is used as input for the GREC-Full task.
To improve the fluency and clarity of the text
by regenerating the referring expressions, we rely
on the system we developed for the GREC Named
Entity Challenge 2010 (NEG), a refined version
of our 2009 submission (Greenbacker and Mc-
Coy, 2009a). This system trains decision trees
on a psycholinguistically-inspired feature set (de-
scribed by Greenbacker and McCoy (2009b)) ex-
tracted from a training corpus. It predicts the most
appropriate reference type and case for the given
context, and selects the best match from the list of
available REs. For the GREC-Full task, however,
instead of using the files annotated by the GREC
organizers as input, we use the files we annotated
automatically in the GREC-NER task. By keep-
ing the GREC-NER output in the GREC format,
our NEG system was able to successfully run un-
modified and generate our output for GREC-Full.
3 Results
Scores calculated by the GREC self-evaluation
tools are provided in Table 1 for GREC-NER and
in Table 2 for GREC-Full.
Corpus NIST BLEU-4
Entire Set 8.1500 0.7953
Chefs 7.5937 0.7895
Composers 7.5381 0.8026
Inventors 7.5722 0.7936
Table 2: Self-evaluation scores for GREC-Full.
4 Conclusions
Until we compare our results with others teams or
an oracle, it is difficult to gauge our performance.
However, at this first iteration of these tasks, we?re
pleased just to have end-to-end RE regeneration
working to completion with meaningful output.
5 Future Work
Future improvements to our coreference resolu-
tion approach involve analyzing adjacent text, uti-
lizing more of the parser output, and applying ma-
chine learning to our GREC-NER methods.
References
Kalina Bontcheva, Marin Dimitrov, Diana Maynard,
Valentin Tablan, and Hamish Cunningham. 2002.
Shallow Methods for Named Entity Coreference
Resolution. In Cha??nes de re?fe?rences et re?solveurs
d?anaphores, workshop TALN 2002, Nancy, France.
Charles Greenbacker and Kathleen McCoy. 2009a.
UDel: Extending reference generation to multiple
entities. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 105?106, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Charles F. Greenbacker and Kathleen F. McCoy.
2009b. Feature selection for reference generation as
informed by psycholinguistic research. In Proceed-
ings of the CogSci 2009 Workshop on Production of
Referring Expressions (PRE-Cogsci 2009), Amster-
dam, July.
Shalom Lappin and Herbert J. Leass. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 20(4):535?561.
The Stanford Natural Language Processing Group.
2009. Stanford Named Entity Recognizer.
http://nlp.stanford.edu/software/
CRF-NER.shtml.
The Stanford Natural Language Processing Group.
2010. The Stanford Parser: A statistical parser.
http://nlp.stanford.edu/software/
lex-parser.shtml.
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 41?48,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Abstractive Summarization of Line Graphs from Popular Media
Charles F. Greenbacker Peng Wu
Sandra Carberry Kathleen F. McCoy Stephanie Elzer*
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
[charlieg|pwu|carberry|mccoy]@cis.udel.edu
*Department of Computer Science
Millersville University, Millersville, Pennsylvania, USA
elzer@cs.millersville.edu
Abstract
Information graphics (bar charts, line graphs,
etc.) in popular media generally have a dis-
course goal that contributes to achieving the
communicative intent of a multimodal docu-
ment. This paper presents our work on ab-
stractive summarization of line graphs. Our
methodology involves hypothesizing the in-
tended message of a line graph and using it
as the core of a summary of the graphic. This
core is then augmented with salient proposi-
tions that elaborate on the intended message.
1 Introduction
Summarization research has focused primarily on
summarizing textual documents, and until recently,
other kinds of communicative vehicles have been
largely ignored. As noted by Clark (1996), language
is more than just words ? it is any signal that is
intended to convey a message. Information graph-
ics (non-pictorial graphics such as bar charts, line
graphs, etc.) in popular media such as Newsweek,
Businessweek, or newspapers, generally have a com-
municative goal or intended message. For exam-
ple, the graphic in Figure 1 is intended to convey
a changing trend in sea levels ? relatively flat from
1900 to 1930 and then rising from 1930 to 2003.
Thus, using Clark?s view of language, information
graphics are a means of communication.
Research has shown that the content of informa-
tion graphics in popular media is usually not re-
peated in the text of the accompanying article (Car-
berry et al, 2006). The captions of such graphics
are also often uninformative or convey little of the
graphic?s high-level message (Elzer et al, 2005).
This contrasts with scientific documents in which
graphics are often used to visualize data, with ex-
plicit references to the graphic being used to explain
their content (e.g., ?As shown in Fig. A...?). Infor-
mation graphics in popular media contribute to the
overall communicative goal of a multimodal docu-
ment and should not be ignored.
Our work is concerned with the summarization
of information graphics from popular media. Such
summaries have several major applications: 1) they
can be integrated with the summary of a multimodal
document?s text, thereby producing a richer sum-
mary of the overall document?s content; 2) they can
be stored in a digital library along with the graphic
itself and used to retrieve appropriate graphics in re-
sponse to user queries; and 3) for individuals with
sight impairments, they can be used along with a
screen reader to convey not only the text of a docu-
ment, but also the content of the document?s graph-
ics. In this paper we present our work on summariz-
ing line graphs. This builds on our previous efforts
into summarizing bar charts (Demir et al, 2008;
Elzer et al, 2011); however, line graphs have dif-
ferent messages and communicative signals than bar
charts and their continuous nature requires different
processing. In addition, a very different set of visual
features must be taken into account in deciding the
importance of including a proposition in a summary.
2 Methodology
Most summarization research has focused on ex-
tractive techniques by which segments of text are
extracted and put together to form the summary.
41
?
102468 1900?1
0?20
?50?60
?70?80
?90
?03
?30?40
2000
10
8.9
1.979 inches over
 the past ce
ntury. Ann
ual differen
ce from Se
attle?s
In the seatt
le area, for
 example, t
he Pacific 
Ocean has 
risen nearly
 
they are ris
ing about 0
.04?0.09 o
f an inch ea
ch year.
Sea levels 
fluctuate ar
ound the gl
obe, but oc
eanographe
rs believe
Ocean leve
ls rising
1899 sea le
vel, in inch
es:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
However, the Holy Grail of summarization work is
abstractive summarization in which the document?s
content is understood and the important concepts are
integrated into a coherent summary. For informa-
tion graphics, extractive summarization might mean
treating the text in the graphic (e.g., the caption) as if
it were document text. One could imagine perhaps
expanding this view to include selecting particular
data points or segments and constructing sentences
that convey them. Abstractive summarization, on
the other hand, requires that the high-level content
of the graphic be identified and conveyed in the sum-
mary. The goal of our work is abstractive summa-
rization. The main issues are identifying the knowl-
edge conveyed by a graphic, selecting the concepts
that should be conveyed in a summary, and integrat-
ing them into coherent natural language sentences.
As noted in the Introduction, information graphics
in popular media generally have a high-level mes-
sage that they are intended to convey. This mes-
sage constitutes the primary communicative or dis-
course goal (Grosz and Sidner, 1986) of the graphic
and captures its main contribution to the overall dis-
course goal of the entire document. However, the
graphic also includes salient features that are impor-
tant components of the graphic?s content. For exam-
ple, the graphic in Figure 1 is very jagged with sharp
fluctuations, indicating that short-term changes have
been inconsistent. Since the graphic?s intended mes-
sage represents its primary discourse goal, we con-
tend that this message should form the core or fo-
cus of the graphic?s summary. The salient features
should be used to augment the summary of the graph
and elaborate on its intended message. Thus, our
methodology consists of the following steps: 1) hy-
pothesize the graphic?s primary discourse or com-
municative goal (i.e., its intended message), 2) iden-
tify additional propositions that are salient in the
graphic, and 3) construct a natural language sum-
mary that integrates the intended message and the
additional salient propositions into a coherent text.
Section 3 presents our methodology for hypothe-
sizing a line graph?s intended message or discourse
goal. It starts with an XML representation of the
graphic that specifies the x-y coordinates of the sam-
pled pixels along the data series in the line graph, the
axes with tick marks and labels, the caption, etc.;
constructing the XML representation is the respon-
sibility of a Visual Extraction Module similar to the
one for bar charts described by Chester and Elzer
(2005). Section 4 presents our work on identifying
the additional propositions that elaborate on the in-
tended message and should be included in the sum-
mary. Section 5 discusses future work on realizing
the propositions in a natural language summary, and
Section 6 reviews related work in multimodal and
abstractive summarization.
3 Identifying a Line Graph?s Message
Research has shown that human subjects have a
strong tendency to use line graphs to portray trend
relationships, as well as a strong tendency to de-
scribe line graphs in terms of trends (Zacks and
Tversky, 1999). We analyzed a corpus of sim-
ple line graphs collected from various popular me-
dia including USA Today, Businessweek, and The
(Wilmington) News Journal, and identified a set of
10 high-level message categories that capture the
kinds of messages that are conveyed by a simple
line graph. Table 1 defines four of them. The com-
plete list can be found in (Wu et al, 2010b). Each
of these messages requires recognizing the visual
trend(s) in the depicted data. We use a support vec-
tor machine (SVM) to first segment the line graph
into a sequence of visually-distinguishable trends;
this sequence is then input into a Bayesian net-
work that reasons with evidence from the graphic
42
Intention Category Description
RT: Rising-trend There is a rising trend from <param1> to <param2>.
CT: Change-trend There is a <direction2> trend from <param2> to <param3> that is signifi-
cantly different from the <direction1> trend from <param1> to <param2>.
CTR:
Change-trend-return
There is a <direction1> trend from <param3> to <param4> that is different
from the <direction2> trend between <param2> and <param3> and reflects
a return to the kind of <direction1> trend from <param1> to <param2>.
BJ: Big-jump There was a very significant sudden jump in value between <param1> and
<param2> which may or may not be sustained.
Table 1: Four categories of High Level Messages for Line Graphs
in order to recognize the graphic?s intended mes-
sage. The next two subsections outline these
steps. (Our corpus of line graphs can be found at
www.cis.udel.edu/?carberry/Graphs/viewallgraphs.php)
3.1 Segmenting a Line Graph
A line graph can consist of many short, jagged
line segments, although a viewer of the graphic ab-
stracts from it a sequence of visually-distinguishable
trends. For example, the line graph in Figure 1 con-
sists of two trends: a relatively stable trend from
1900 to 1930 and a longer, increasing trend from
1930 to 2003. Our Graph Segmentation Module
(GSM) takes a top-down approach (Keogh et al,
2001) to generalize the line graph into sequences of
rising, falling, and stable segments, where a segment
is a series of connected data points. The GSM starts
with the entire line graph as a single segment and
uses a learned model to recursively decide whether
each segment should be split into two subsegments;
if the decision is to split, the division is made at the
point being the greatest distance from a straight line
between the two end points of the original segment.
This process is repeated on each subsegment until
no further splits are identified. The GSM returns a
sequence of straight lines representing a linear re-
gression of the points in each subsegment, where
each straight line is presumed to capture a visually-
distinguishable trend in the original graphic.
We used Sequential Minimal Optimization (Platt,
1999) in training an SVM to make segment split-
ting decisions. We chose to use an SVM because it
works well with high-dimensional data and a rela-
tively small training set, and lessens the chance of
overfitting by using the maximum margin separat-
ing hyperplane which minimizes the worst-case gen-
eralization errors (Tan et al, 2005). 18 attributes,
falling into two categories, were used in building
the data model (Wu et al, 2010a). The first cat-
egory captures statistical tests computed from the
sampled data points in the XML representation of
the graphic; these tests estimate how different the
segment is from a linear regression (i.e., a straight
line). The second category of attributes captures
global features of the graphic. For example, one
such attribute relates the segment size to the size of
the entire graphic, based on the hypothesis that seg-
ments comprising more of the total graph may be
stronger candidates for splitting than segments that
comprise only a small portion of the graph.
Our Graph Segmentation Module was trained
on a set of 649 instances that required a split/no-
split decision. Using leave-one-out cross validation,
in which one instance is used for testing and the
other 648 instances are used for training, our model
achieved an overall accuracy rate of 88.29%.
3.2 A Bayesian Recognition System
Once the line graph has been converted into
a sequence of visually-distinguishable trends, a
Bayesian network is built that captures the possible
intended messages for the graphic and the evidence
for or against each message. We adopted a Bayesian
network because it weighs different pieces of evi-
dence and assigns a probability to each candidate
intended message. The next subsections briefly out-
line the Bayesian network and its evaluation; details
can be found in (Wu et al, 2010b).
Structure of the Bayesian Network Figure 2
shows a portion of the Bayesian network constructed
for Figure 1. The top-level node in our Bayesian net-
work represents all of the high-level message cat-
43
Intended Message
... ...
... ...
...
CT?Suggestion?1
CT IntentionRT Intention
EvidenceOtherPointsAnnotated
Have SuggestionEvidence
Portion of GraphicEvidence EndpointsAnnotatedEvidence EvidenceSplittingPointsAnnotated
Adjective in CaptionEvidence
Verb in CaptionEvidence
Figure 2: A portion of the Bayesian network
egories. Each of these possible non-parameterized
message categories is repeated as a child of the
top-level node; this is purely for ease of repre-
sentation. Up to this point, the Bayesian net-
work is a static structure with conditional proba-
bility tables capturing the a priori probability of
each category of intended message. When given
a line graph to analyze, an extension of this net-
work is built dynamically according to the partic-
ulars of the graph itself. Candidate (concrete) in-
tended messages, having actual instantiated param-
eters, appear beneath the high-level message cat-
egory nodes. These candidates are introduced by
a Suggestion Generation Module; it dynamically
constructs all possible intended messages with con-
crete parameters using the visually-distinguishable
trends (rising, falling, or stable) identified by the
Graph Segmentation Module. For example, for each
visually-distinguishable trend, a Rising, Falling, or
Stable trend message is suggested; similary, for each
sequence of two visually-distinguishable trends, a
Change-trend message is suggested. For the graphic
in Figure 1, six candidate messages will be gener-
ated, including RT(1930, 2003), CT(1900, stable,
1930, rise, 2003) and BJ(1930, 2003) (see Table 1).
Entering Evidence into the Bayesian Network
Just as listeners use evidence to identify the intended
meaning of a speaker?s utterance, so also must a
viewer use evidence to recognize a graphic?s in-
tended message. The evidence for or against each
of the candidate intended messages must be entered
into the Bayesian network. We identified three kinds
of evidence that are used in line graphs: attention-
getting devices explicitly added by the graphic de-
signer (e.g., the annotation of a point with its value),
aspects of a graphic that are perceptually-salient
(e.g., the slope of a segment), and clues that sug-
gest the general message category (e.g., a verb [or
noun derived from a verb such as rebound] in the
caption which might indicate a Change-trend mes-
sage). The first two kinds of evidence are attached
to the Bayesian network as children of each candi-
date message node, such as the child nodes of ?CT-
Suggestion-1? in Figure 2. The third kind of evi-
dence is attached to the top level node as child nodes
named ?Verb in Caption Evidence? and ?Adjective
in Caption Evidence? in Figure 2.
Bayesian Network Inference We evaluated the
performance of our system for recognizing a line
graph?s intended message on a corpus of 215 line
graphs using leave-one-out cross validation in which
one graph is held out as a test graph and the con-
ditional probability tables for the Bayesian network
are computed from the other 214 graphs. Our sys-
tem recognized the correct intended message with
the correct parameters for 157 line graphs, resulting
in a 73.36% overall accuracy rate.
4 Identifying Elaborative Propositions
Once the intended message has been determined,
the next step is to identify additional important
informational propositions1 conveyed by the line
graph which should be included in the summary.
To accomplish this, we collected data to determine
what kinds of propositions in what situations were
deemed most important by human subjects, and de-
veloped rules designed to make similar assessments
based on the graphic?s intended message and visual
features present in the graphic.
4.1 Collecting Data from Human Subjects
Participants in our study were given 23 different line
graphs. With each graph, the subjects were provided
1We define a ?proposition? as a logical representation de-
scribing a relationship between one or more concepts, while a
?sentence? is a surface form realizing one or more propositions.
44
Figure 3: From ?This Cable Outfit Is Getting Tuned In?
in Businessweek magazine, Oct 4, 1999.
with an initial sentence describing the overall in-
tended message of the graphic. The subjects were
asked to add additional sentences so that the com-
pleted summary captured the most important infor-
mation conveyed by the graphic. The graphs were
presented to the subjects in different orders, and the
subjects completed as many graphs as they wanted
during the one hour study session. The set covered
the eight most prevalent of our intended message
categories and a variety of visual features. Roughly
half of the graphs were real-world examples from
the corpus used to train the Bayesian network in
Section 3.2, (e.g., Figure 3), with the others created
specifically to fill a gap in the coverage of intended
messages and visual features.
We collected a total of 998 summaries written by
69 human subjects for the 23 different line graphs.
The number of summaries we received per graph
ranged from 37 to 50. Most of the summaries were
between one and four sentences long, in addition to
the initial sentence (capturing the graphic?s intended
message) that was provided for each graph. A rep-
resentative sample summary collected for the line
graph shown in Figure 3 is as follows, with the initial
sentence provided to the study participants in italics:
This line graph shows a big jump in Blon-
der Tongue Laboratories stock price in
August ?99. The graph has many peaks
and valleys between March 26th 1999 to
August ?99 but maintains an average stock
price of around 6 dollars. However, in Au-
gust ?99 the stock price jumps sharply to
around 10 dollars before dropping quickly
to around 9 dollars by September 21st.
4.2 Extracting & Weighting Propositions
The data collected during the study was analyzed by
a human annotator who manually coded the propo-
sitions that appeared in each individual summary in
order to determine, for each graphic, which proposi-
tions were used and how often. For example, the set
of propositions coded in the sample summary from
Section 4.1 were:
? volatile(26Mar99, Aug99)
? average val(26Mar99, Aug99, $6)
? jump 1(Aug99, $10)
? steep(jump 1)
? decrease 1(Aug99, $10, 21Sep99, $9)
? steep(decrease 1)
From this information, we formulated a set of
rules governing the use of each proposition accord-
ing to the intended message category and various
visual features. Our intuition was that by finding
and exploiting a correlation between the intended
message category and/or certain visual features and
the propositions appearing most often in the human-
written summaries, our system could use these in-
dicators to determine which propositions are most
salient in new graphs. Our rules assign a weight
to each proposition in the situation captured by the
rule; these weights are based on the relative fre-
quency of the proposition being used in summaries
reflecting similar situations in our corpus study. The
rules are organized into three types:
1. Message Category-only (M):
IF M = m THEN select P with weight w1
2. Visual Feature-only (V):
IF V = v THEN select P with weight w2
3. Message Category + Visual Feature:
IF M = m and V = v
THEN select P with weight w2
We constructed type 1 (Message Category-only)
rules when a plurality of human-written summaries
45
in our corpus for all line graphs belonging to a
given message category contain the proposition. A
weight was assigned according to the frequency with
which the proposition was included. This weighting,
shown in Equation 1, is based on the proportion of
summaries for each line graph in the corpus having
intended message m and containing proposition P.
w1 =
n?
i=1
Pi
Si
(1)
In this equation, n is the number of line graphs in
this intended message category, Si is the total num-
ber of summaries for a particular line graph with this
intended message category, and Pi is the number of
these summaries that contain the proposition.
Intuitively, a proposition appearing in all sum-
maries for all graphs in a given message category
will have a weight of 1.0, while a proposition which
never appears will have a weight of zero. How-
ever, a proposition appearing in all summaries for
half of the graphs in a category, and rarely for the
other half of the graphs in that category, will have a
much lower weight than one which appears in half
of the summaries for all the graphs in that category,
even though the overall frequencies could be equal
for both. In this case, the message category is an
insufficient signal, and it is likely that the former
proposition is more highly correlated to some par-
ticular visual feature than to the message category.
Weights for type 2 and type 3 rules (Visual
Feature-only and Message Category + Visual Fea-
ture) are slightly more complicated in that they in-
volve a measure of degree for the associated visual
feature rather than simply its presence. The defini-
tion of this measure varies depending on the nature
of the visual feature (e.g., steepness of a trend line,
volatility), but all such measures range from zero to
one. Additionally, since the impact of a visual fea-
ture is a matter of degree, the weighting cannot rely
on a simple proportion of summaries containing the
proposition as in type 1 rules. Instead, it is neces-
sary to find the covariance between the magnitude of
the visual feature (|v|) and how frequently the corre-
sponding proposition is used (PS ) in the corpus sum-
maries for the n graphs having this visual feature, as
shown in Equation 2.
Cov(|v|,
P
S
) =
[(?n
i=1 |vi|
n
?n
i=1
Pi
Si
n
)
?
?n
i=1 |vi|
Pi
Si
n
] (2)
Then for a particular graphic whose magnitude for
this feature is |v|, we compute the weight w2 for the
proposition P as shown in Equation 3.
w2 = |v| ? Cov(|v|,
P
S
) (3)
This way, the stronger a certain visual feature is in a
given line graph, the higher the weight for the asso-
ciated proposition.
Type 3 rules (Message Category + Visual Fea-
ture) differ only from type 2 rules in that they are
restricted to a particular intended message category,
rather than any line graph having the visual feature
in question. For example, a proposition compar-
ing the slope of two trends may be appropriate for
a graph in the Change-trend message category, but
does not make sense for a line graph with only a sin-
gle trend (e.g., Rising-trend).
Once all propositions have been extracted and
ranked, these weights are passed along to a graph-
based content selection framework (Demir et al,
2010) that iteratively selects for inclusion in the ini-
tial summary those propositions which provide the
best coverage of the highest-ranked information.
4.3 Sample Rule Application
Figures 1 and 4 consist of two different line graphs
with the same intended message category: Change-
trend. Figure 1 shows a stable trend in annual sea
level difference from 1900 to 1930, followed by a
rising trend through 2003, while Figure 4 shows a
rising trend in Durango sales from 1997 to 1999,
followed by a falling trend through 2006. Proposi-
tions associated with type 1 rules will have the same
weights for both graphs, but propositions related to
visual features may have different weights. For ex-
ample, the graph in Figure 1 is far more volatile than
the graph in Figure 4. Thus, the type 2 rule associ-
ated with volatility will have a very high weight for
the graph in Figure 1 and will almost certainly be in-
cluded in the initial summary of that line graph (e.g.,
46
20062005200420032002
19971998
1999
20012000
200,000 150,000199
9: 189,840
70,6062006:
50,000100,000Declining Du
rango sales
0
Figure 4: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
?The values vary a lot...?, ?The trend is unstable...?),
possibly displacing a type 1 proposition that would
still appear in the summary for the graph in Figure 4.
5 Future Work
Once the propositions that should be included in the
summary have been selected, they must be coher-
ently organized and realized as natural language sen-
tences. We anticipate using the FUF/SURGE sur-
face realizer (Elhadad and Robin, 1996); our col-
lected corpus of line graph summaries provides a
large set of real-world expressions to draw from
when crafting the surface realization forms our sys-
tem will produce for the final-output summaries.
Our summarization methodology must also be eval-
uated. In particular, we must evaluate the rules for
identifying the additional informational propositions
that are used to elaborate the overall intended mes-
sage, and the quality of the summaries both in terms
of content and coherence.
6 Related Work
Image summarization has focused on constructing a
smaller image that contains the important content of
a larger image (Shi et al, 2009), selecting a set of
representative images that summarize a collection
of images (Baratis et al, 2008), or constructing a
new diagram that summarizes one or more diagrams
(Futrelle, 1999). However, all of these efforts pro-
duce an image as the end product, not a textual sum-
mary of the content of the image(s).
Ferres et al (2007) developed a system for con-
veying graphs to blind users, but it generates the
same basic information for each instance of a graph
type (e.g., line graphs) regardless of the individual
graph?s specific characteristics. Efforts toward sum-
marizing multimodal documents containing graph-
ics have included na??ve approaches relying on cap-
tions and direct references to the image in the text
(Bhatia et al, 2009), while content-based image
analysis and NLP techniques are being combined for
multimodal document indexing and retrieval in the
medical domain (Ne?ve?ol et al, 2009).
Jing and McKeown (1999) approached abstrac-
tive summarization as a text-to-text generation task,
modifying sentences from the original document via
editing and rewriting. There have been some at-
tempts to do abstractive summarization from seman-
tic models, but most of it has focused on text docu-
ments (Rau et al, 1989; Reimer and Hahn, 1988),
though Alexandersson (2003) used abstraction and
semantic modeling for speech-to-speech translation
and multilingual summary generation.
7 Discussion
Information graphics play an important communica-
tive role in popular media and cannot be ignored.
We have presented our methodology for construct-
ing a summary of a line graph. Our method is ab-
stractive, in that we identify the important high-level
knowledge conveyed by a graphic and capture it in
propositions to be realized in novel, coherent natu-
ral language sentences. The resulting summary can
be integrated with a summary of the document?s text
to produce a rich summary of the entire multimodal
document. In addition, the graphic?s summary can
be used along with a screen reader to provide sight-
impaired users with full access to the knowledge
conveyed by multimodal documents.
Acknowledgments
This work was supported in part by the National In-
stitute on Disability and Rehabilitation Research un-
der Grant No. H133G080047.
References
Jan Alexandersson. 2003. Hybrid Discourse Modeling
and Summarization for a Speech-to-Speech Transla-
tion System. Ph.D. thesis, Saarland University.
Evdoxios Baratis, Euripides Petrakis, and Evangelos Mil-
ios. 2008. Automatic web site summarization by im-
age content: A case study with logo and trademark
47
images. IEEE Transactions on Knowledge and Data
Engineering, 20(9):1195?1204.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proc. of the 29th Annual Int?l ACM
SIGIR Conf. on Research & Development in Informa-
tion Retrieval, SIGIR ?06, pages 581?588, Seattle, Au-
gust. ACM.
Daniel Chester and Stephanie Elzer. 2005. Getting com-
puters to see information graphics so users do not have
to. In Proceedings of the 15th International Sympo-
sium on Methodologies for Intelligent Systems (LNAI
3488), ISMIS 2005, pages 660?668, Saratoga Springs,
NY, June. Springer-Verlag.
Herbert Clark. 1996. Using Language. Cambridge Uni-
versity Press.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters & Demos), Sussex, UK, June. ACL.
Stephanie Elzer, Sandra Carberry, Daniel Chester, Seniz
Demir, Nancy Green, Ingrid Zukerman, and Keith
Trnka. 2005. Exploring and exploiting the limited
utility of captions in recognizing intention in infor-
mation graphics. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics, pages 223?230, Ann Arbor, June. ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proc. of the 9th Int?l ACM
SIGACCESS Conf. on Computers & Accessibility, AS-
SETS ?07, pages 67?74, Tempe, October. ACM.
Robert P. Futrelle. 1999. Summarization of diagrams in
documents. In I. Mani and M. Maybury, editors, Ad-
vances in Automatic Text Summarization. MIT Press.
Barbara Grosz and Candace Sidner. 1986. Attention,
Intentions, and the Structure of Discourse. Computa-
tional Linguistics, 12(3):175?204.
Hongyan Jing and Kathleen R. McKeown. 1999. The
decomposition of human-written summary sentences.
In Proc. of the 22nd Annual Int?l ACM SIGIR Conf.
on Research & Development in Information Retrieval,
SIGIR ?99, pages 129?136, Berkeley, August. ACM.
Eamonn J. Keogh, Selina Chu, David Hart, and
Michael J. Pazzani. 2001. An online algorithm
for segmenting time series. In Proceedings of the
2001 IEEE International Conference on Data Mining,
ICDM ?01, pages 289?296, Washington, DC. IEEE.
Aure?lie Ne?ve?ol, Thomas M. Deserno, Ste?fan J. Darmoni,
Mark Oliver Gu?ld, and Alan R. Aronson. 2009. Nat-
ural language processing versus content-based image
analysis for medical document retrieval. Journal of the
American Society for Information Science and Tech-
nology, 60(1):123?134.
John C. Platt. 1999. Fast training of support vector
machines using sequential minimal optimization. In
B. Scho?lkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in kernel methods: support vector learning,
pages 185?208. MIT Press, Cambridge, MA, USA.
Lisa F. Rau, Paul S. Jacobs, and Uri Zernik. 1989. In-
formation extraction and text summarization using lin-
guistic knowledge acquisition. Information Process-
ing & Management, 25(4):419 ? 428.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Liang Shi, Jinqiao Wang, Lei Xu, Hanqing Lu, and
Changsheng Xu. 2009. Context saliency based im-
age summarization. In Proceedings of the 2009 IEEE
international conference on Multimedia and Expo,
ICME ?09, pages 270?273, New York. IEEE.
Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.
2005. Introduction to Data Mining. Addison Wesley.
Peng Wu, Sandra Carberry, and Stephanie Elzer. 2010a.
Segmenting line graphs into trends. In Proceedings of
the 2010 International Conference on Artificial Intel-
ligence, ICAI ?10, pages 697?703, Las Vegas, July.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel
Chester. 2010b. Recognizing the intended message
of line graphs. In Proc. of the 6th Int?l Conf. on Dia-
grammatic Representation & Inference, Diagrams ?10,
pages 220?234, Portland. Springer-Verlag.
Jeff Zacks and Barbara Tversky. 1999. Bars and lines:
A study of graphic communication. Memory & Cog-
nition, 27:1073?1079.
48
Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 52?62,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Improving the Accessibility of Line Graphs in Multimodal Documents
Charles F. Greenbacker Peng Wu Sandra Carberry Kathleen F. McCoy
Stephanie Elzer* David D. McDonald? Daniel Chester Seniz Demir?
Dept. of Computer & Information Sciences, University of Delaware, USA
[charlieg|pwu|carberry|mccoy|chester]@cis.udel.edu
*Dept. of Computer Science, Millersville University, USA elzer@cs.millersville.edu
?SIFT LLC., Boston, Massachusetts, USA dmcdonald@sift.info
?TU?BI?TAK BI?LGEM, Gebze, Kocaeli, Turkey senizd@uekae.tubitak.gov.tr
Abstract
This paper describes our work on improv-
ing access to the content of multimodal docu-
ments containing line graphs in popular media
for people with visual impairments. We pro-
vide an overview of our implemented system,
including our method for recognizing and con-
veying the intended message of a line graph.
The textual description of the graphic gener-
ated by our system is presented at the most rel-
evant point in the document. We also describe
ongoing work into obtaining additional propo-
sitions that elaborate on the intended message,
and examine the potential benefits of analyz-
ing the text and graphical content together in
order to extend our system to produce sum-
maries of entire multimodal documents.
1 Introduction
Individuals with visual impairments have difficulty
accessing the information contained in multimodal
documents. Although screen-reading software can
render the text of the document as speech, the graph-
ical content is largely inaccessible. Here we con-
sider information graphics (e.g., bar charts, line
graphs) often found in popular media sources such
as Time magazine, Businessweek, and USA Today.
These graphics are typically intended to convey a
message that is an important part of the overall story,
yet this message is generally not repeated in the ar-
ticle text (Carberry et al, 2006). People who are
unable to see and assimilate the graphical material
will be left with only partial information.
While some work has addressed the accessibility
of scientific graphics through alternative means like
touch or sound (see Section 7), such graphs are de-
signed for an audience of experts trained to use them
for data visualization. In contrast, graphs in popular
media are constructed to make a point which should
be obvious without complicated scientific reasoning.
We are thus interested in generating a textual pre-
sentation of the content of graphs in popular media.
Other research has focused on textual descriptions
(e.g., Ferres et al (2007)); however in that work the
same information is included in the textual summary
for each instance of a graph type (i.e., all summaries
of line graphs contain the same sorts of informa-
tion), and the summary does not attempt to present
the overall intended message of the graph.
SIGHT (Demir et al, 2008; Elzer et al, 2011) is
a natural language system whose overall goal is pro-
viding blind users with interactive access to multi-
modal documents from electronically-available pop-
ular media sources. To date, the SIGHT project
has concentrated on simple bar charts. Its user in-
terface is implemented as a browser helper object
within Internet Explorer that works with the JAWS
screen reader. When the system detects a bar chart
in a document being read by the user, it prompts the
user to use keystrokes to request a brief summary of
the graphic capturing its primary contribution to the
overall communicative goal of the document. The
summary text can either be read to the user with
JAWS or read by the user with a screen magnifier
tool. The interface also enables the user to request
further information about the graphic, if desired.
However, SIGHT is limited to bar charts only.
In this work, we follow the methodology put forth
by SIGHT, but investigate producing a summary of
52
?
102468 1900?10
?20
?50?60
?70?80
?90?
03
?30?40
2000
10
8.9
1.979 inches over t
he past centu
ry. Annual d
ifference fro
m Seattle?s
In the seattl
e area, for e
xample, the
 Pacific Oce
an has risen
 nearly 
they are risi
ng about 0.0
4?0.09 of an
 inch each y
ear.
Sea levels fl
uctuate arou
nd the globe
, but oceano
graphers bel
ieve
Ocean level
s rising
1899 sea lev
el, in inches
:
Figure 1: From ?Worry flows from Arctic ice to tropical
waters? in USA Today, May 31, 2006.
line graphs. Line graphs have different discourse
goals and communicative signals than bar charts,1
and thus require significantly different processing.
In addition, our work addresses the issue of coher-
ent placement of a graphic?s summary when reading
the text to the user and considers the summarization
of entire documents ? not just their graphics.
2 Message Recognition for Line Graphs
This section provides an overview of our imple-
mented method for identifying the intended message
of a line graph. In processing a line graph, a vi-
sual extraction module first analyzes the image file
and produces an XML representation which fully
specifies the graphic (including the beginning and
ending points of each segment, any annotations on
points, axis labels, the caption, etc.). To identify
the intended message of a line graph consisting of
many short, jagged segments, we must generalize
it into a sequence of visually-distinguishable trends.
This is performed by a graph segmentation module
which uses a support vector machine and a variety
of attributes (including statistical tests) to produce a
model that transforms the graphic into a sequence of
straight lines representing visually-distinguishable
trends. For example, the line graph in Figure 1 is
divided into a stable trend from 1900 to 1930 and a
rising trend from 1930 to 2003. Similarly, the line
graph in Figure 2 is divided into a rising trend from
1Bar charts present data as discrete bars and are often used
to compare entities, while line graphs contain continuous data
series and are designed to portray longer trend relationships.
20062005200420032002
19971998
1999
20012000
200,000 150,0001999
: 189,840
70,6062006:
50,000100,000Declining Dur
ango sales
0
Figure 2: From ?Chrysler: Plant had $800 million im-
pact? in The (Wilmington) News Journal, Feb 15, 2007.
1997 to 1999 and a falling trend from 1999 to 2006.
In analyzing a corpus of around 100 line graphs
collected from several popular media sources, we
identified 10 intended message categories (includ-
ing rising-trend, change-trend, change-trend-return,
and big-jump, etc.), that seem to capture the kinds
of high-level messages conveyed by line graphs. A
suggestion generation module uses the sequence of
trends identified in the line graph to construct all
of its possible candidate messages in these message
categories. For example, if a graph contains three
trends, several candidate messages are constructed,
including two change-trend messages (one for each
adjacent pair of trends), a change-trend-return mes-
sage if the first and third trends are of the same type
(rising, falling, or stable), as well as a rising, falling,
or stable trend message for each individual trend.
Next, various communicative signals are ex-
tracted from the graphic, including visual features
(such as a point annotated with its value) that draw
attention to a particular part of the line graph, and
linguistic clues (such as the presence of certain
words in the caption) that suggest a particular in-
tended message category. Figure 2 contains several
such signals, including two annotated points and the
word declining in its caption. Next, a Bayesian net-
work is built to estimate the probability of the can-
didate messages; the extracted communicative sig-
nals serve as evidence for or against each candidate
message. For Figure 2, our system produces change-
trend(1997, rise, 1999, fall, 2006) as the logical rep-
resentation of the most probable intended message.
Since the dependent axis is often not explicitly la-
beled, a series of heuristics are used to identify an
appropriate referent, which we term the measure-
ment axis descriptor. In Figure 2, the measurement
axis descriptor is identified as durango sales. The
53
intended message and measurement axis descriptor
are then passed to a realization component which
uses FUF/SURGE (Elhadad and Robin, 1996) to
generate the following initial description:
This graphic conveys a changing trend in
durango sales, rising from 1997 to 1999
and then falling to 2006.
3 Identifying a Relevant Paragraph
In presenting a multimodal document to a user via a
screen reader, if the author does not specify a read-
ing order in the accessibility preferences, it is not
entirely clear where the description of the graph-
ical content should be given. The text of scien-
tific articles normally makes explicit references to
any graphs contained in the document; in this case,
it makes sense to insert the graphical description
alongside the first such reference. However, popular
media articles rarely contain explicit references to
graphics. We hypothesize that describing the graphi-
cal content together with the most relevant portion of
the article text will result in a more coherent presen-
tation. Results of an experiment described in Sec-
tion 3.3 suggest the paragraph which is geograph-
ically closest to the graphic is very often not rele-
vant. Thus, our task becomes identifying the portion
of the text that is most relevant to the graph.
We have developed a method for identifying the
most relevant paragraph by measuring the similarity
between the graphic?s textual components and the
content of each individual paragraph in the docu-
ment. An information graphic?s textual components
may consist of a title, caption, and any additional
descriptions it contains (e.g., the five lines of text in
Figure 1 beneath the caption Ocean levels rising).
An initial method (P-KL) based on KL divergence
measures the similarity between a paragraph and the
graphic?s textual component; a second method (P-
KLA) is an extension of the first that incorporates
an augmented version of the textual component.
3.1 Method P-KL: KL Divergence
Kullback-Leibler (KL) divergence (Kullback, 1968)
is widely used to measure the similarity between two
language models. It can be expressed as:
DKL(p||q) =
?
i?V
p(i)log
p(i)
q(i)
where i is the index of a word in vocabulary V , and
p and q are two distributions of words. Liu et al
(Liu and Croft, 2002) applied KL divergence to text
passages in order to improve the accuracy of docu-
ment retrieval. For our task, p is a smoothed word
distribution built from the line graph?s textual com-
ponent, and q is another smoothed word distribution
built from a paragraph in the article text. Smoothing
addresses the problem of zero occurrences of a word
in the distributions. We rank the paragraphs by their
KL divergence scores from lowest to highest, since
lower scores indicate a higher similarity.
3.2 Method P-KLA: Using Augmented Text
In analyzing paragraphs relevant to the graphics, we
realized that they included words that were germane
to describing information graphics in general, but
not related to the domains of individual graphs. This
led us to build a set of ?expansion words? that tend to
appear in paragraphs relevant to information graph-
ics. If we could identify domain-independent terms
that were correlated with information graphics in
general, these expansion words could then be added
to the textual component of a graphic when measur-
ing its similarity to a paragraph in the article text.
We constructed the expansion word set using an
iterative process. The first step is to use P-KL to
identify m pseudo-relevant paragraphs in the cor-
responding document for each graphic in the train-
ing set (the current implementation uses m = 3).
This is similar to pseudo-relevance feedback used in
IR (Zhai, 2008), except only a single query is used
in the IR application, whereas we consider many
pairs of graphics and documents to obtain an ex-
pansion set applicable to any subsequent informa-
tion graphic. Given n graphics in the training set,
we identify (up to) m ? n relevant paragraphs.
The second step is to extract a set of words re-
lated to information graphics from these m ?n para-
graphs. We assume the collection of pseudo-relevant
paragraphs was generated by two models, one pro-
ducing words relevant to the information graphics
and another producing words relevant to the topics
of the individual documents. Let Wg represent the
word frequency vector yielding words relevant to
the graphics, Wa represent the word frequency vec-
tor yielding words relevant to the document topics,
and Wp represent the word frequency vector of the
54
pseudo-relevant paragraphs. We compute Wp from
the pseudo-relevant paragraphs themselves, and we
estimate Wa using the word frequencies from the
article text in the documents. Finally, we compute
Wg by filtering-out the components ofWa fromWp.
This process is related to the work by Widdows
(2003) on orthogonal negation of vector spaces.
The task can be formulated as follows:
1. Wp = ?Wa + ?Wg where ? > 0 and ? > 0,
which means the word frequency vector for
the pseudo-relevant paragraphs is a linear com-
bination of the background (topic) word fre-
quency vector and the graphic word vector.
2. < Wa,Wg >= 0 which means the background
word vector is orthogonal to the graph descrip-
tion word vector, under the assumption that the
graph description word vector is independent of
the background word vector and that these two
share minimal information.
3. Wg is assumed to be a unit vector, since we are
only interested in the relative rank of the word
frequencies, not their actual values.
Solving the above equations, we obtain:
? =
< Wp,Wa >
< Wa,Wa >
Wg = normalized
(
Wp ?
< Wp,Wa >
< Wa,Wa >
?Wa
)
After computing Wg, we use WordNet to filter-
out words having a predominant sense other than
verb or adjective, under the assumption that nouns
will be mainly relevant to the domains or topics
of the graphs (and are thus ?noise?) whereas we
want a general set of words (e.g., ?increasing?)
that are typically used when describing the data in
any graph. As a rough estimate of whether a word
is predominantly a verb or adjective, we determine
whether there are more verb and adjective senses of
the word in WordNet than there are noun senses.
Next, we rank the words in the filteredWg accord-
ing to frequency and select the k most frequent as
our expansion word list (we used k = 25 in our ex-
periments). The two steps (identifyingm?n pseudo-
relevant paragraphs and then extracting a word list of
size k to expand the graphics? textual components)
are applied iteratively until convergence occurs or
minimal changes are observed between iterations.
In addition, parameters of the intended message
that represent points on the x-axis capture domain-
specific content of the graphic?s communicative
goal. For example, the intended message of the line
graph in Figure 1 conveys a changing trend from
1900 to 2003 with the change occurring in 1930. To
help identify relevant paragraphs mentioning these
years, we also add these parameters of the intended
message to the augmented word list.
The result of this process is the final expansion
word list used in method P-KLA. Because the tex-
tual component may be even shorter than the expan-
sion word list, we do not add a word from the expan-
sion word list to the textual component unless the
paragraph being compared also contains this word.
3.3 Results of P-KL and P-KLA
334 training graphs with their accompanying articles
were used to build the expansion word set. A sepa-
rate set of 66 test graphs and articles was analyzed
by two human annotators who identified the para-
graphs in each document that were most relevant to
its associated information graphic, ranking them in
terms of relevance. On average, annotator 1 selected
2.00 paragraphs and annotator 2 selected 1.71 para-
graphs. The annotators agreed on the top ranked
paragraph for only 63.6% of the graphs. Consid-
ering the agreement by chance, we can calculate the
kappa statistic as 0.594. This fact shows that the
most relevant paragraph is not necessarily obvious
and multiple plausible options may exist.
We applied both P-KL and P-KLA to the test set,
with each method producing a list of the paragraphs
ranked by relevance. Since our goal is to provide
the summary of the graphic at a suitable point in the
article text, two evaluation criteria are appropriate:
1. TOP: the method?s success rate in selecting
the most relevant paragraph, measured as how
often it chooses the paragraph ranked highest
by either of the annotators
2. COVERED: the method?s success rate in se-
lecting a relevant paragraph, measured as how
often it chooses one of the relevant paragraphs
identified by the annotators
Table 1 provides the success rates of both of our
methods for the TOP and COVERED criteria, along
with a simple baseline that selected the paragraph
55
geographically-closest to the graphic. These results
show that both methods outperform the baseline,
and that P-KLA further improves on P-KL. P-KLA
selects the best paragraph in 60.6% of test cases,
and selects a relevant paragraph in 71.2% of the
cases. For both TOP and COVERED, P-KLA nearly
doubles the baseline success rate. The improve-
ment of P-KLA over P-KL suggests that our expan-
sion set successfully adds salient words to the tex-
tual component. A one-sided Z-test for proportion
based on binomial distribution is shown in Table 1
and indicates that the improvements of P-KL over
the baseline and P-KLA over P-KL are statistically-
significant at the 0.05 level across both criteria. The
Z-test is calculated as:
p? p0
?
p0(1?p0)
n
where p0 is the lower result and p is the improved
result. The null hypothesis is H0 : p = p0 and the
alternative hypothesis is H1 : p > p0.
3.4 Using relevant paragraph identification to
improve the accessibility of line graphs
Our system improves on SIGHT by using method
P-KLA to identify the paragraph that is most rele-
vant to an information graphic. When this paragraph
is encountered, the user is asked whether he or she
would like to access the content of the graphic. For
example, our system identifies the following para-
graph as most relevant to Figure 2:
Doing so likely would require the com-
pany to bring in a new model. Sales of
the Durango and other gas-guzzling SUVs
have slumped in recent years as prices at
the pump spiked.
In contrast, the geographically-closest paragraph has
little relevance to the graphic:
?We have three years to prove to them
we need to stay open,? said Sam Latham,
president of the AFL-CIO in Delaware,
who retired from Chrysler after 39 years.
4 Identifying Additional Propositions
After the intended message has been identified, the
system next looks to identify elaborative informa-
tional propositions that are salient in the graphic.
These additional propositions expand on the initial
description of the graph by filling-in details about
the knowledge being conveyed (e.g., noteworthy
points, properties of trends, visual features) in order
to round-out a summary of the graphic.
We collected a corpus of 965 human-written sum-
maries for 23 different line graphs to discover which
propositions were deemed most salient under varied
conditions.2 Subjects received an initial description
of the graph?s intended message, and were asked to
write additional sentences capturing the most impor-
tant information conveyed by the graph. The propo-
sitions appearing in each summary were manually
coded by an annotator to determine which were most
prevalent. From this data, we developed rules to
identify important propositions in new graphs. The
rules assign weights to propositions indicating their
importance, and the weights can be compared to de-
cide which propositions to include in a summary.
Three types of rules were built. Type-1 (message
category-only) rules were created when a plurality
of summaries for all graphs having a given intended
message contained the same proposition (e.g., pro-
vide the final value for all rising-trend and falling-
trend graphs). Weights for type-1 rules were based
on the frequency with which the proposition ap-
peared in summaries for graphs in this category.
Type-2 (visual feature-only) rules were built when
there was a correlation between a visual feature and
the use of a proposition describing that feature, re-
gardless of the graph?s message category (e.g., men-
tion whether the graph is highly volatile). Type-2
rule weights are a function of the covariance be-
tween the magnitude of the visual feature (e.g., de-
gree of volatility) and the proportion of summaries
mentioning this proposition for each graph.
For propositions associated with visual features
linked to a particular message category (e.g., de-
scribe the trend immediately following a big-jump
or big-fall when it terminates prior to the end of the
graph), we constructed Type-3 (message category
+ visual feature) rules. Type-3 weights were cal-
culated just like Type-2 weights, except the graphs
were limited to the given category.
As an example of identifying additional proposi-
2This corpus is described in greater detail by Greenbacker et
al. (2011) and is available at www.cis.udel.edu/~mccoy/corpora
56
closest P-KL significance level over closest P-KLA significance level over P-KL
TOP 0.272 0.469 (z = 3.5966, p < 0.01) 0.606 (z = 2.2303, p < 0.025)
COVERED 0.378 0.606 (z = 3.8200, p < 0.01) 0.712 (z = 1.7624, p < 0.05)
Table 1: Success rates for baseline method (?closest?), P-KL, and P-KLA using the TOP and COVERED criteria.
tions, consider Figures 1 and 2. Both line graphs
belong to the same intended message category:
change-trend. However, the graph in Figure 1 is far
more volatile than Figure 2, and thus it is likely that
we would want to mention this proposition (i.e., ?the
graph shows a high degree of volatility...?) in a sum-
mary of Figure 1. By finding the covariance between
the visual feature (i.e., volatility) and the frequency
with which a corresponding proposition was anno-
tated in the corpus summaries, a Type-2 rule assigns
a weight to this proposition based on the magnitude
of the visual feature. Thus, the volatility proposi-
tion will be weighted strongly for Figure 1, and will
likely be selected to appear in the initial summary,
while the weight for Figure 2 will be very low.
5 Integrating Text and Graphics
Until now, our system has only produced summaries
for the graphical content of multimodal documents.
However, a user might prefer a summary of the en-
tire document. Possible use cases include examining
this summary to decide whether to invest the time re-
quired to read a lengthy article with a screen reader,
or simply addressing the common problem of having
too much material to review in too little time (i.e.,
information overload). We are developing a system
extension that will allow users to request summaries
of arbitrary length that cover both the text and graph-
ical content of a multimodal document.
Graphics in popular media convey a message that
is generally not repeated in the article text. For ex-
ample, the March 3, 2003 issue of Newsweek con-
tained an article entitled, ?The Black Gender Gap,?
which described the professional achievements of
black women. It included a line graph (Figure 3)
showing that the historical gap in income equality
between white women and black women had been
closed, yet this important message appears nowhere
in the article text. Other work in multimodal doc-
ument summarization has relied on image captions
and direct references to the graphic in the text (Bha-
tia et al, 2009); however, these textual elements do
Figure 3: From ?The Black Gender Gap? in Newsweek,
Mar 3, 2003.
not necessarily capture the message conveyed by in-
formation graphics in popular media. Thus, the user
may miss out on an essential component of the over-
all communicative goal of the document if the sum-
mary covers only material presented in the text.
One approach to producing a summary of the en-
tire multimodal document might be to ?concatenate?
a traditional extraction-based summary of the text
(Kupiec et al, 1995; Witbrock and Mittal, 1999)
with the description generated for the graphics by
our existing system. The summary of the graphi-
cal content could be simply inserted wherever it is
deemed most relevant in the text summary. How-
ever, such an approach would overlook the relation-
ships and interactions between the text and graphical
content. The information graphics may make certain
concepts mentioned in the text more salient, and vice
versa. Unless we consider the contributions of both
the text and graphics together during the content se-
lection phase, the most important information might
not appear in the summary of the document.
Instead, we must produce a summary that inte-
grates the content conveyed by the text and graphics.
We contend that this integration must occur at the se-
mantic level if it is to take into account the influence
of the graphic?s content on the salience of concepts
in the text and vice versa. Our tack is to first build
a single semantic model of the concepts expressed
in both the article text and information graphics, and
then use this model as the basis for generating an
abstractive summary of the multimodal document.
57
Drawing from a model of the semantic content of the
document, we select as many or as few concepts as
we wish, at any level of detail, to produce summaries
of arbitrary length. This will permit the user to re-
quest a quick overview in order to decide whether to
read the original document, or a more comprehen-
sive synopsis to obtain the most important content
without having to read the entire article.
5.1 Semantic Modeling of Multimodal
Documents
Content gathered from the article text by a seman-
tic parser and from the information graphics by
our graph understanding system is combined into
a single semantic model based on typed, struc-
tured objects organized under a foundational ontol-
ogy (McDonald, 2000a). For the semantic pars-
ing of text, we use Sparser (McDonald, 1992), a
bottom-up, phrase-structure-based chart parser, op-
timized for semantic grammars and partial parsing.3
Using a built-in model of core English grammar
plus domain-specific grammars, Sparser extracts in-
formation from the text and produces categorized
objects as a semantic representation (McDonald,
2000b). The intended message and salient additional
propositions identified by our system for the infor-
mation graphics are decomposed and added to the
model constructed by Sparser.4
Model entries contain slots for attributes in the
concept category?s ontology definition (fillable by
other concepts or symbols), the original phrasings
mentioning this concept in the text (represented as
parameterized synchronous TAG derivation trees),
and markers recording document structure (i.e.,
where in the text [including title, headings, etc.] or
graphic the concept appeared). Figure 4 shows some
of the information contained in a small portion of
the semantic model built for an article entitled ?Will
Medtronic?s Pulse Quicken?? from the May 29,
2006 edition of Businessweek magazine5, which in-
cluded a line graph. Nodes correspond to concepts
3https://github.com/charlieg/Sparser
4Although the framework is general enough to accommo-
date any modality (e.g., images, video) given suitable seman-
tic analysis tools, our prototype implementation focuses on bar
charts and line graphs analyzed by SIGHT.
5http://www.businessweek.com/magazine/
content/06_22/b3986120.htm
and edges denote relationships between concepts;
dashed lines indicate links to concepts not shown in
this figure. Nodes are labelled with the name of the
conceptual category they instantiate, and a number
to distinguish between individuals. The middle of
each box displays the attributes of the concept, while
the bottom portion shows some of the original text
phrasings. Angle brackets (<>) note references to
other concepts, and hash marks (#) indicate a sym-
bol that has not been instantiated as a concept.
P1S1: "medical device
    giant Medtronic"
P1S5: "Medtronic"
Name: "Medtronic"
Stock: "MDT"
Industry: (#pacemakers,
    #defibrillators,
    #medical devices)
Company1
P1S4: "Joanne
    Wuensch"
P1S7: "Wuensch"
FirstName: "Joanne"
LastName: "Wuensch"
Person1
P1S4: "a 12-month
    target of 62"
Person: <Person 1>
Company: <Company 1>
Price: $62.00
Horizon: #12_months
TargetStockPrice1
Figure 4: Detail of model for Businessweek article.
5.2 Rating Content in Semantic Models
The model is then rated to determine which items are
most salient. The concepts conveying the most in-
formation and having the most connections to other
important concepts in the model are the ones that
should be chosen for the summary. The importance
of each concept is rated according to a measure of
information density (ID) involving several factors:6
Saturation Level Completeness of attributes in
model entry: a concept?s filled-in slots (f ) vs. its
total slots (s), and the importance of the concepts
(ci) filling those slots:
f
s ? log(s) ?
?f
i=1 ID(ci)
Connectedness Number of connections (n) with
other concepts (cj), and the importance of these con-
nected concepts:
?n
j=1 ID(cj)
Frequency Number of observed phrasings (e) re-
alizing the concept in text of the current document
Prominence in Text Prominence based on docu-
ment structure (WD) and rhetorical devices (WR)
Graph Salience Salience assessed by the graph
understanding system (WG) ? only applies to con-
cepts appearing in the graphics
6The first three factors are similar to the dominant slot
fillers, connectivity patterns, and frequency criteria described
by Reimer and Hahn (1988).
58
Saturation corresponds to the completeness of the
concept in the model. The more attribute slots that
are filled, the more we know about a particular con-
cept instance. However, this measure is highly sen-
sitive to the degree of detail provided in the seman-
tic grammar and ontology class definition (whether
created by hand or automatically). A concept having
two slots, both of which are filled-out, is not neces-
sarily more important than a concept with only 12
of its 15 slots filled. The more important a concept
category is in a given domain, the more detailed its
ontology class definition will likely be. Thus, we
can assume that a concept definition having a dozen
or more slots is, broadly speaking, more important
in the domain than a less well-defined concept hav-
ing only one or two slots. This insight is the basis of
a normalization factor (log(s)) used in ID.
Saturation differs somewhat from repetition in
that it attempts to measure the amount of informa-
tion associated with a concept, rather than simply
the number of times a concept is mentioned in the
text. For example, a news article about a proposed
law might mention ?Washington? several times, but
the fact that the debate took place in Washington,
D.C. is unlikely to be an important part of the article.
However, the key provisions of the bill, which may
individually be mentioned only once, are likely more
important as a greater amount of detail is provided
concerning them. Simple repetition is not necessar-
ily indicative of the importance of a concept, but if a
large amount of information is provided for a given
concept, it is safe to assume the concept is important
in the context of that document.
Document structure (WD) is another important
clue in determining which elements of a text are
important enough to include in a summary (Marcu,
1997). If a concept is featured prominently in the
title, or appears in the first or final paragraphs, it is
likely more important than a concept buried in the
middle of the document. Importance is also affected
by certain rhetorical devices (WR) which serve to
highlight particular concepts. Being used in an id-
iom, or compared to another concept by means of
juxtaposition suggests that a given concept may hold
special significance. Finally, the weights assigned
by our graph understanding system for the additional
propositions identified in the graphics are incorpo-
rated into the ID of the concepts involved as WG.
5.3 Selecting Content for a Summary
To select concepts for inclusion in the summary,
the model will then be passed to a discourse-aware
graph-based content selection framework (Demir et
al., 2010), which selects concepts one at a time
and iteratively re-weights the remaining items so
as to include related concepts and avoid redun-
dancy. This algorithm incorporates PageRank (Page
et al, 1999), but with several modifications. In ad-
dition to centrality assessment based on relation-
ships between concepts, it includes apriori impor-
tance nodes enabling us to incorporate concept com-
pleteness, number of expressions, document struc-
ture, and rhetorical devices. More importantly from
a summary generation perspective, the algorithm it-
eratively picks concepts one at a time, and re-ranks
the remaining entries by increasing the weight of re-
lated items and discounting redundant ones. This
allows us to select concepts that complement each
other while simultaneously avoiding redundancy.
6 Generating an Abstractive Summary of
a Multimodal Document
Figure 4 shows the two most important concepts
(Company1 & Person1) selected from the Medtronic
article in Section 5.1. Following McDonald and
Greenbacker (2010), we use the phrasings observed
by the parser as the ?raw material? for expressing
these selected concepts. Reusing the original phras-
ings reduces the reliance on built-in or ?canned?
constructions, and allows the summary to reflect the
style of the original text. The derivation trees stored
in the model to realize a particular concept may use
different syntactic constituents (e.g., noun phrases,
verb phrases). Multiple trees are often available for
each concept, and we must select particular trees that
fit together to form a complete sentence.
The semantic model also contains concepts rep-
resenting propositions extracted from the graphics,
as well as relationships connecting these graphical
concepts with those derived from the text, and there
are no existing phrasings in the original document
that can be reused to convey this graphical content.
However, the set of proposition types that can be ex-
tracted from the graphics is finite. To ensure that we
have realizations for every concept in our model, we
create TAG derivation trees for each type of graphi-
59
cal proposition. As long as realizations are supplied
for every proposition that can be decomposed in the
model, our system will never be stuck with a concept
without the means to express it.
The set of expressions is augmented by many
built-in realizations for common semantic relation-
ships (e.g., ?is-a,? ?has-a?), as well as expressions
inherited from other conceptual categories in the hi-
erarchy. If the observed expressions are retained as
the system analyzes multiple documents over time,
making these realizations available for later use by
concepts in the same category, the variety of utter-
ances we can generate is increased greatly.
By using synchronous TAG trees, we know that
the syntactic realizations of two semantically-related
concepts will fit together syntactically (via substitu-
tion or adjunction). However, the concepts selected
for the summary of the Medtronic article (Com-
pany1 & Person1), are not directly connected in the
model. To produce a single summary sentence for
these two concepts, we must find a way of express-
ing them together with the available phrasings. This
can be accomplished by using an intermediary con-
cept that connects both of the selected items in the
semantic model, in order to ?bridge the gap? be-
tween them. In this example, a reasonable option
would be TargetStockPrice1, one of the many con-
cepts linking Company1 and Person1. Combining
original phrasings from all three concepts (via sub-
stitution and adjunction operations on the underly-
ing TAG trees), along with a ?built-in? realization
inherited by the TargetStockPrice category (a sub-
type of Expectation), yields this surface form:
Wuensch expects a 12-month target of 62
for medical device giant Medtronic.
7 Related Work
Research into providing alternative access to graph-
ics has taken both verbal and non-verbal approaches.
Kurze (1995) presented a verbal description of the
properties (e.g., diagram style, number of data sets,
range and labels of axes) of business graphics. Fer-
res et al (2007) produced short descriptions of the
information in graphs using template-driven genera-
tion based on the graph type. The SIGHT project
(Demir et al, 2008; Elzer et al, 2011) generated
summaries of the high-level message content con-
veyed by simple bar charts. Other modalities, like
sound (Meijer, 1992; Alty and Rigas, 1998; Choi
and Walker, 2010) and touch (Ina, 1996; Krufka et
al., 2007), have been used to impart graphics via a
substitute medium. Yu et al (2002) and Abu Doush
et al (2010) combined haptic and aural feedback,
enabling users to navigate and explore a chart.
8 Discussion
This paper presented our system for providing ac-
cess to the full content of multimodal documents
with line graphs in popular media. Such graph-
ics generally have a high-level communicative goal
which should constitute the core of a graphic?s sum-
mary. Rather than providing this summary at the
point where the graphic is first encountered, our sys-
tem identifies the most relevant paragraph in the
article and relays the graphic?s summary at this
point, thus increasing the presentation?s coherence.
System extensions currently in development will
provide a more integrative and accessible way for
visually-impaired readers to experience multimodal
documents. By producing abstractive summaries of
the entire document, we reduce the amount of time
and effort required to assimiliate the information
conveyed by such documents in popular media.
Several tasks remain as future work. The intended
message descriptions generated by our system need
to be evaluated by both sighted and non-sighted hu-
man subjects for clarity and accuracy. We intend
to test our hypothesis that graphics ought to be de-
scribed alongside the most relevant part of the text
by performing an experiment designed to determine
the presentation order preferred by people who are
blind. The rules developed to identify elaborative
propositions also must be validated by a corpus or
user study. Finally, once the system is fully imple-
mented, the abstractive summaries generated for en-
tire multimodal documents will need to be evaluated
by both sighted and sight-impaired judges.
Acknowledgments
This work was supported in part by the by the Na-
tional Institute on Disability and Rehabilitation Re-
search under grant H133G080047 and by the Na-
tional Science Foundation under grant IIS-0534948.
60
References
Iyad Abu Doush, Enrico Pontelli, Tran Cao Son, Dominic
Simon, and Ou Ma. 2010. Multimodal presenta-
tion of two-dimensional charts: An investigation using
Open Office XML and Microsoft Excel. ACM Trans-
actions on Accessible Computing (TACCESS), 3:8:1?
8:50, November.
James L. Alty and Dimitrios I. Rigas. 1998. Communi-
cating graphical information to blind users using mu-
sic: the role of context. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems,
CHI ?98, pages 574?581, Los Angeles, April. ACM.
Sumit Bhatia, Shibamouli Lahiri, and Prasenjit Mitra.
2009. Generating synopses for document-element
search. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management, CIKM
?09, pages 2003?2006, Hong Kong, November. ACM.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
Stephen H. Choi and Bruce N. Walker. 2010. Digitizer
auditory graph: making graphs accessible to the visu-
ally impaired. In Proceedings of the 28th International
Conference on Human Factors in Computing Systems,
CHI ?10, pages 3445?3450, Atlanta, April. ACM.
Seniz Demir, Sandra Carberry, and Kathleen F. McCoy.
2008. Generating textual summaries of bar charts.
In Proceedings of the 5th International Natural Lan-
guage Generation Conference, INLG 2008, pages 7?
15, Salt Fork, Ohio, June. ACL.
Seniz Demir, Sandra Carberry, and Kathleen F. Mc-
Coy. 2010. A discourse-aware graph-based content-
selection framework. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 17?26, Trim, Ireland, July. ACL.
Michael Elhadad and Jacques Robin. 1996. An overview
of SURGE: a re-usable comprehensive syntactic re-
alization component. In Proceedings of the 8th In-
ternational Natural Language Generation Workshop
(Posters and Demonstrations), Sussex, UK, June.
ACL.
Stephanie Elzer, Sandra Carberry, and Ingrid Zukerman.
2011. The automated understanding of simple bar
charts. Artificial Intelligence, 175:526?555, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
Charles F. Greenbacker, Sandra Carberry, and Kathleen F.
McCoy. 2011. A corpus of human-written summaries
of line graphs. In Proceedings of the EMNLP 2011
Workshop on Language Generation and Evaluation,
UCNLG+Eval, Edinburgh, July. ACL. (to appear).
Satoshi Ina. 1996. Computer graphics for the blind. SIG-
CAPH Newsletter on Computers and the Physically
Handicapped, pages 16?23, June. Issue 55.
Stephen E. Krufka, Kenneth E. Barner, and Tuncer Can
Aysal. 2007. Visual to tactile conversion of vector
graphics. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 15(2):310?321, June.
Solomon Kullback. 1968. Information Theory and
Statistics. Dover, revised 2nd edition.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In Proceedings
of the 18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, SIGIR ?95, pages 68?73, Seattle, July. ACM.
Martin Kurze. 1995. Giving blind people access
to graphics (example: Business graphics). In Pro-
ceedings of the Software-Ergonomie ?95 Workshop
on Nicht-visuelle graphische Benutzungsoberfla?chen
(Non-visual Graphical User Interfaces), Darmstadt,
Germany, February.
Xiaoyong Liu and W. Bruce Croft. 2002. Passage re-
trieval based on language models. In Proceedings of
the eleventh international conference on Information
and knowledge management, CIKM ?02, pages 375?
382.
Daniel C. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, University of Toronto, December.
David D. McDonald and Charles F. Greenbacker. 2010.
?If you?ve heard it, you can say it? - towards an ac-
count of expressibility. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
INLG 2010, pages 185?190, Trim, Ireland, July. ACL.
David D. McDonald. 1992. An efficient chart-based
algorithm for partial-parsing of unrestricted texts. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing, pages 193?200, Trento, March.
ACL.
David D. McDonald. 2000a. Issues in the repre-
sentation of real texts: the design of KRISP. In
Lucja M. Iwan?ska and Stuart C. Shapiro, editors, Nat-
ural Language Processing and Knowledge Represen-
tation, pages 77?110. MIT Press, Cambridge, MA.
David D. McDonald. 2000b. Partially saturated refer-
ents as a source of complexity in semantic interpreta-
tion. In Proceedings of the NAACL-ANLP 2000 Work-
shop on Syntactic and Semantic Complexity in Natural
61
Language Processing Systems, pages 51?58, Seattle,
April. ACL.
Peter B.L. Meijer. 1992. An experimental system for
auditory image representations. IEEE Transactions on
Biomedical Engineering, 39(2):112?121, February.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web. Technical Report 1999-
66, Stanford InfoLab, November. Previous number:
SIDL-WP-1999-0120.
Ulrich Reimer and Udo Hahn. 1988. Text condensation
as knowledge base abstraction. In Proceedings of the
4th Conference on Artificial Intelligence Applications,
CAIA ?88, pages 338?344, San Diego, March. IEEE.
Dominic Widdows. 2003. Orthogonal negation in vector
spaces for modelling word-meanings and document
retrieval. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics - Volume
1, ACL ?03, pages 136?143, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael J. Witbrock and Vibhu O. Mittal. 1999. Ultra-
summarization: a statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, SIGIR ?99, pages 315?316, Berkeley,
August. ACM.
Wai Yu, Douglas Reid, and Stephen Brewster. 2002.
Web-based multimodal graphs for visually impaired
people. In Proceedings of the 1st Cambridge Work-
shop on Universal Access and Assistive Technology,
CWUAAT ?02, pages 97?108, Cambridge, March.
Chengxiang Zhai. 2008. Statistical Language Models
for Information Retrieval. Morgan and Claypool Pub-
lishers, December.
62
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 23?27,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
A Corpus of Human-written Summaries of Line Graphs
Charles F. Greenbacker, Sandra Carberry, and Kathleen F. McCoy
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
[charlieg|carberry|mccoy]@cis.udel.edu
Abstract
We describe a corpus of human-written En-
glish language summaries of line graphs. This
corpus is intended to help develop a system
to automatically generate summaries captur-
ing the most salient information conveyed by
line graphs in popular media, as well as to
evaluate the output of such a system.
1 Motivation
We are developing a system designed to automati-
cally generate summaries of the high-level knowl-
edge conveyed by line graphs found in multimodal
documents from popular media sources (e.g., mag-
azines, newspapers). Intended applications include
making these graphics more accessible for people
with visual impairments and indexing their infor-
mational content for digital libraries. Information
graphics like line graphs are generally included in
a multimodal document in order to make a point
supporting the overall communicative intent of the
document. Our goal is to produce summaries that
convey the knowledge gleaned by humans when in-
formally viewing the graphic, focusing on the ?take-
away? message rather than the raw data points.1
Studies have shown (Carberry et al, 2006) that
the captions of information graphics in popular me-
dia often do not repeat the message conveyed by the
graphic itself; such captions are thus not appropriate
for use as a summary. Furthermore, while scientific
graphs are designed for experts trained in their use
1Users generally prefer conceptual image descriptions over
perceptual descriptions (Jo?rgensen, 1998; Hollink et al, 2004).
for data visualization, information graphics in pop-
ular media are meant to be understood by all read-
ers, including those with only a primary school ed-
ucation. Accordingly, summaries for these graphics
should be tailored for the same general audience.
Research into information graphics by Wu et al
(2010) has identified a limited number of intended
message categories conveyed by line graphs in pop-
ular media. Their efforts included the creation of a
corpus2 of line graphs marked with the overall in-
tended message identified by human annotators.
However, we hypothesize that an effective sum-
mary should present the graph?s intended message
plus additional informational propositions that elab-
orate on this message. McCoy et al (2001) observed
that the intended message was consistently included
in line graph summaries written by human subjects.
Furthermore, participants in that study augmented
the intended message with descriptions of salient vi-
sual features of the graphic (e.g., steepness of a trend
line, volatility of data values). As part of the pro-
cess of building a system to identify which visual
features are salient and to describe them using nat-
ural language expressions, we collected a corpus of
human-written summaries of line graphs.
2 Building the Corpus
We selected 23 different line graphs for use in build-
ing our corpus. This set covered the eight most-
common intended message categories from the Wu
corpus; only Point Correlation and Stable Trend
were omitted. Table 1 shows the distribution of
2www.cis.udel.edu/~carberry/Graphs/viewallgraphs.php
23
Message Category No. (graphs)
Big Fall (BF) 4 (20?23)
Big Jump (BJ) 2 (18, 19)
Changing Trend (CT) 4 (8?11)
Change Trend Return (CTR) 2 (12, 13)
Contrast Trend with
Last Segment (CTLS)
2 (14, 15)
Contrast Segment with
Changing Trend (CSCT)
2 (16, 17)
Rising Trend (RT) 4 (1?4)
Falling Trend (FT) 3 (5?7)
Total 23 (1?23)
Table 1: Distribution of overall intended message cate-
gories in the set of line graphs used to build the corpus.
graphs across message categories.3 Ten of the line
graphs were real world examples in popular media
taken from the Wu corpus (e.g., Figure 1). Another
ten graphs were adapted from items in the Wu cor-
pus ? modified in order to isolate visual features so
that their individual effects could be analyzed (e.g.,
Figure 2). The remaining three line graphs were cre-
ated specifically to fill a gap in the coverage of in-
tended messages and visual features for which no
good example was available (e.g., Figure 3). Our
goal was to include as many different combinations
of message category and visual features as possible
(e.g., for graphs containing a dramatic change in val-
ues because of a big jump or fall, we included ex-
amples which sustained the change as well as others
that did not sustain the change).
69 subjects participated in our study. All were
native English speakers, 18 years of age or older,
without major sight impairments, and enrolled in an
introductory computer science course at a university
in the US. They received a small amount of extra
credit in their course for participating in this study.
Each participant was given the full set of 23 line
graphs in differing orders. With each graph, the sub-
jects were presented with an initial summary sen-
tence describing the overall intended message of the
graphic, as identified by a human annotator. The
captions for Figures 1, 2, and 3 each contain the cor-
responding initial summary sentence that was pro-
vided to the participants. Participants were tasked
with writing additional sentences so that the com-
3Category descriptions can be found in (Wu et al, 2010).
Figure 1: From ?This Cable Outfit Is Getting Tuned In?
in Businessweek magazine, Oct 4, 1999. (Initial sentence:
?This line graph shows a big jump in Blonder Tongue
Laboratories stock price in August ?99.?)
pleted summary of each line graph captured the most
important information conveyed by the graphic, fin-
ishing as many or as few of the 23 graphs as they
wished during a single one-hour session.
Participants were told that we were developing a
system to convey an initial summary of an informa-
tion graphic from popular media (as opposed to text-
books or scientific articles) to blind users via speech.
We indicated that the summaries they write should
be brief (though we did not specify any length re-
quirements), but ought to include all essential infor-
mation provided by the graphic. Subjects were only
given the graphics and did not receive the original ar-
ticle text (if any existed) that accompanied the real-
world graphs. Finally, the participants were told that
a person able to see the graphics should not think
that the summaries they wrote were misleading.
3 Corpus Characteristics
A total of 965 summaries were collected, ranging
from 37 to 49 summaries for each individual line
graph. Table 2 offers some descriptive statistics for
the corpus as a whole, while Table 3 lists the ten
most commonly-occurring content words.
Sample summary 1 (18-4.txt) was written for Fig-
ure 1, summary 2 (7-40.txt) for Figure 2, and sum-
maries 3 (9-2.txt) and 4 (9-5.txt) both for Figure 3:
24
!Figure 2: Adapted from original in ?Dell goes with a few
AMD chips,? USA Today, Oct 19, 2006. (Initial sentence:
?This line graph shows a falling trend in Dell stock from
May ?05 to May ?06.?)
From March 26, 1999 the graph rises and de-
clines up until August 1999 where it rises at
about a 90-degree angle then declines again.
(1)
The graph peaked in July ?05 but then sharply
decreased after that. It had several sharp in-
clines and declines and ended with a shaper
decline from March ?06 to May ?06.
(2)
February has a much larger amount of jackets
sold than the other months shown. From dec-
ember to january, there was a slight drop in
the amount of jackets sold and then a large
spike from january to february.
(3)
The values in November and May are pretty
close, with both being around 37 or 38
jackets. At its peak (February), around 47
jackets were sold.
(4)
4 Potential Usage
To our knowledge, this is the first and only publicly-
available corpus of line graph summaries. It has sev-
eral possible applications in both natural language
generation and evaluation tasks. By finding and ex-
amining patterns in the summaries, we can discover
which propositions are found to be most salient for
certain kinds of graphs. We are currently analyzing
the collected corpus for this very purpose ? to iden-
tify relationships between visual features, intended
messages, and the relative importance of includ-
ing corresponding propositions in a summary (e.g.,
volatility is more salient in Figure 2 than Figure 3).
!Figure 3: Sample line graph created for this study. (Ini-
tial sentence: ?This line graph shows a rising trend in
Boscov?s jacket sales from November to February fol-
lowed by a falling trend through May.?)
Metric Value
total characters 213,261
total words (w) 45,217
total sentences 2,184
characters per word 4.72
words per sentence 20.70
sentences per summary 2.26
unique words (u) 1,831
lexical diversity (w/u) 24.70
hapax legomena 699
pct. of unique words 38.18%
pct. of total words 1.55%
Table 2: Various descriptive statistics for the corpus.
Not only does this corpus offer insight into what
humans perceive to be the most important informa-
tion conveyed by line graphs, it provides a large set
of real-world expressions from which to draw when
crafting the surface realization forms for summaries
of line graphs. From a generation perspective, this
collection of summaries offers copious examples of
the expressions human use to describe characteris-
tics of information graphics. The corpus could also
be used to determine the proper structural character-
istics of a line graph summary (e.g., when multiple
information is included, how propositions are aggre-
gated into sentences, which details come first).
The evaluation of graph understanding systems
will also benefit from the use of this corpus. It will
enable comparisons between system and human-
25
Word Count Word Count
graph 715 stock 287
price 349 increase 280
august 305 may 279
dollars 300 decrease 192
around 299 trend 183
Table 3: The ten most frequently occurring words in the
corpus (omitting stopwords and punctuation).
generated descriptions at the propositional (content)
level, as well as judgments involving clarity and co-
herence. The set of summaries for each graph may
be used as a ?gold standard? against which to com-
pare automatically-generated summaries in prefer-
ence judgment experiments involving human judges.
We are currently developing rules for identifying
the most salient information conveyed by a given
line graph based on an analysis of this corpus, and
will also use the expressions in the collected sum-
maries as examples for surface realization during the
summary generation process. Additionally, we are
planning to use the corpus during part of the evalu-
ation phase of our project, by asking human judges
to compare these human-written summaries against
our system?s output across multiple dimensions of
preference. It may also be useful to perform some
additional human subjects experiments to determine
which summaries in the corpus are found to be most
helpful and understandable.
5 Related Work
Prior to this study, we performed an initial investi-
gation based on a questionnaire similar to the one
used by Demir (2010) for bar charts. A group of
human subjects was asked to review several line
graphs and indicate how important it would be to
include various propositions in an initial summary
of each graphic. Although this method was effec-
tive with bar charts, it proved to be far too cumber-
some to work with line graphs. Bar charts are some-
what simpler, propositionally-speaking, as there are
fewer informational propositions that can be ex-
tracted from data represented as discrete bars rather
than as a continuous data series in a line graph.
It required far more effort for subjects to evaluate
the relative importance of each individual proposi-
tion than to simply provide (in the form of a writ-
ten summary) the set of propositions they consid-
ered to be most important. In the end, the summary-
based approach allowed for a more direct exami-
nation of salience judgments without subjects be-
ing constrained or influenced by the questions and
structure of the questionnaire-based approach, with
the added bonus of producing a reusable corpus of
human-written summaries of line graphs.
McCoy et al (2001) performed a study in which
participants were asked to write brief summaries for
a series of line graphs. While they did not release
a corpus for distribution, their analysis did suggest
that a graph?s visual features could be used to help
select salient propositions to include in a summary.
Although several corpora exist for general im-
age descriptions, we are unaware of any other cor-
pora of human-written summaries for information
graphics. Jo?rgensen (1998) collected unconstrained
descriptions of pictorial images, while Hollink et
al. (2004) analyzed descriptions of mental images
formed by subjects to illustrate a given text pas-
sage. Aker and Gaizauskas (2010) built a corpus of
human-generated captions for location-related im-
ages. Large collections of general image captions
have been assembled for information retrieval tasks
(Smeaton and Quigley, 1996; Tribble, 2010). Roy
(2002) evaluated automatically-generated descrip-
tions of visual scenes against human-generated de-
scriptions. The developers of the iGraph-Lite system
(Ferres et al, 2007) released a corpus of descrip-
tions for over 500 graphs collected from Statistics
Canada, but these descriptions were generated auto-
matically by their system and not written by human
authors. Additionally, the descriptions contained in
their corpus focus on the quantitative data presented
in the graphics rather than the high-level message,
and tend to vary only slightly between graphs.4
Since using corpus texts as a ?gold standard? in
generation and evaluation can be tricky (Reiter and
Sripada, 2002), we tried to mitigate some of the
common problems, including giving participants as
much time as they wanted for each summary to
avoid ?hurried writing.? However, as we intend to
use this corpus to understand which propositions hu-
mans find salient for line graphs, as well as generat-
4The iGraph-Lite system provides the same information for
each instance of a graph type (i.e., all summaries of line graphs
contain the same sorts of information).
26
ing and evaluating new summaries, a larger collec-
tion of examples written by many authors for several
different graphics was more desirable than a smaller
corpus of higher-quality texts from fewer authors.
6 Availability
The corpus is freely available for download5 without
restrictions under an open source license.
The structure of the corpus is as follows. The
?summaries? directory consists of a series of subdi-
rectories numbered 1-23 containing the summaries
for all 23 line graphs, with each summary stored in
a separate file (encoded as ASCII text). The files
are named according to the graph they are associ-
ated with and their position in that graph?s collec-
tion (e.g., 8-10.txt is the 10th summary for the 8th
line graph, and is located in the directory named 8).
The root of the distribution package contains a
directory of original image files for the line graphs
(named ?line graphs?), the initial sentences describ-
ing each graph?s intended message (which was pro-
vided to the participants) in sentences.txt, and a
README file describing the corpus layout.
The corpus is easily loaded with NLTK (Loper
and Bird, 2002) using these Python commands:
from nltk.corpus import PlaintextCorpusReader
LGSroot = './LGSummaryCorpus/summaries'
corpus = PlaintextCorpusReader(LGSroot, '.*')
Acknowledgments
This work was supported in part by the National In-
stitute on Disability and Rehabilitation Research un-
der Grant No. H133G080047.
References
Ahmet Aker and Robert Gaizauskas. 2010. Model sum-
maries for location-related images. In Proceedings
of the Seventh Conference on International Language
Resources and Evaluation, LREC ?10, pages 3119?
3124, Malta, May. ELRA.
Sandra Carberry, Stephanie Elzer, and Seniz Demir.
2006. Information graphics: an untapped resource for
digital libraries. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ?06,
pages 581?588, Seattle, August. ACM.
5www.cis.udel.edu/~mccoy/corpora
Seniz Demir. 2010. SIGHT for Visually Impaired Users:
Summarizing Information Graphics Textually. Ph.D.
thesis, University of Delaware, February.
Leo Ferres, Petro Verkhogliad, Gitte Lindgaard, Louis
Boucher, Antoine Chretien, and Martin Lachance.
2007. Improving accessibility to statistical graphs: the
iGraph-Lite system. In Proceedings of the 9th Inter-
national ACM SIGACCESS Conference on Computers
and Accessibility, ASSETS ?07, pages 67?74, Tempe,
October. ACM.
L. Hollink, A. Th. Schreiber, B. J. Wielinga, and M. Wor-
ring. 2004. Classification of user image descriptions.
International Journal of Human-Computer Studies,
61(5):601?626, November.
Corinne Jo?rgensen. 1998. Attributes of images in de-
scribing tasks. Information Processing and Manage-
ment, 34:161?174, March?May.
Edward Loper and Steven Bird. 2002. NLTK: The nat-
ural language toolkit. In Proceedings of the ACL-02
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics, pages 63?70, Philadelphia, July.
ACL.
Kathleen F. McCoy, M. Sandra Carberry, Tom Roper,
and Nancy Green. 2001. Towards generating textual
summaries of graphs. In Proceedings of the 1st Inter-
national Conference on Universal Access in Human-
Computer Interaction, UAHCI 2001, pages 695?699,
New Orleans, August. Lawrence Erlbaum.
Ehud Reiter and Somayajulu Sripada. 2002. Should cor-
pora texts be gold standards for NLG? In Proceed-
ings of the Second International Conference on Natu-
ral Language Generation, INLG 2002, pages 97?104,
Harriman, New York, July. ACL.
Deb K. Roy. 2002. Learning visually grounded words
and syntax for a scene description task. Computer
Speech & Language, 16(3?4):353?385, July?October.
Alan F. Smeaton and Ian Quigley. 1996. Experiments on
using semantic distances between words in image cap-
tion retrieval. In Proceedings of the 19th Annual Inter-
national ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR ?96, pages
174?180, Zurich, August. ACM.
Alicia Tribble. 2010. Textual Inference for Retrieving
Labeled Object Descriptions. Ph.D. thesis, Carnegie
Mellon University, April.
Peng Wu, Sandra Carberry, Stephanie Elzer, and Daniel
Chester. 2010. Recognizing the intended message
of line graphs. In Proceedings of the Sixth Interna-
tional Conference on the Theory and Application of
Diagrams, Diagrams 2010, pages 220?234, Portland,
Oregon, August. Springer-Verlag.
27
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, page 1,
Utica, May 2012. c?2012 Association for Computational Linguistics
Invited Speaker
Dr. Kathleen F. McCoy
University of Delaware
Natural Language Generation and Assistive Technologies
Abstract
Some people with disabilities find it difficult to access some forms of language. Assistive Technology is a 
term used to describe a class of technologies/interventions designed to enable people with disabilities to  
do things that their disabilitie currently make difficult. A large amount of work on Assistive Technology 
has focused on enabling access to language and communication; this class of interventions could greatly 
benefit from Natural Language Generation technologies.
This  talk  will  briefly  survey  some  Assistive  Technology  applications  that  have  employed  Natural  
Language Generation technologies ? highlighting some of the needs in this application area along with 
the opportunities that it provides for investigating hard problems in Natural Language Generation. It will  
then highlight a project, called the SIGHT System, intended to provide access to information graphics 
(e.g., bar charts, line graphs) found in popular media to people who have visual impairments. This system 
employs  Natural  Language Generation technologies to generate appropriate  textual  summaries  of the  
information  graphics.  As  such,  it  makes  contributions  to  several  areas  within  the  field  of  Natural  
Language Generation while  also enabling access to the information in these graphics to people  who  
cannot access  it with visual means.
Biography
Dr. Kathleen F. McCoy is a professor in the Department of Computer and Information Sciences at the 
University of  Delaware.  She  received  her  PhD from the University of  Pennsylvania  in  1985 with  a 
dissertation in the area of Natural Language Generation, and has been at the University of Delaware ever 
since then. Shortly after joining Delaware, she began working in applying Natural Language Processing 
to  Assistive  technologies  at  the  Center  for  Applied Science and Engineering in  Rehabilitation at  the 
University of Delaware and the DuPont Hospital for Children. She served as the Center?s director from 
2000-2009.  She received a University of Delaware Excellence in Teaching Award in 1997, a University 
of Delaware Excellence in Advising Award in 2001,  and a College of Arts and Science Outstanding 
Advisor  Award  in  2003.  From 1995-2008  she  served  on  the  ACL Executive  committee  in  various 
capacities including 10 years as Treasurer.  She is the founding President of the ACL Special Interest 
Group on Speech and Language Processing for Assistive Technologies (2011). She has been an organizer  
of several workshops on that area associated with various ACL conferences. She was program co-chair of  
the  User  Modeling  Conference  in  2007,  the  ACM  SIGACCESS  Conference  on  Computers  and 
Accessibility in 2009, and the General Chair of that same conference in 2011. She is a Senior Member of  
the ACM.
1
Proceedings of the 8th International Natural Language Generation Conference, pages 74?82,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Experimental Design to Improve Topic Analysis Based Summarization
John E. Miller
Computer & Information Sciences
University of Delaware
Newark, DE 19711
jmiller@udel.edu
Kathleen F. McCoy
Computer & Information Sciences
University of Delaware
Newark, DE 19711
mccoy@udel.edu
Abstract
We use efficient screening experiments
to investigate and improve topic analysis
based multi-document extractive summa-
rization. In our summarization process,
topic analysis determines the weighted
topic content vectors that characterize the
corpora, and then Jensen-Shannon diver-
gence extracts sentences that best match
the weighted content vectors to assemble
the summaries. We use screening experi-
ments to investigate several control param-
eters in this process, gaining better under-
standing of and improving the topic anal-
ysis based summarization process.
1 Introduction
We use efficient experimental design to investi-
gate and improve topic analysis based multiple
document extractive summarization. Our process
proceeds in two steps: Latent Dirichlet Anal-
ysis (LDA) topic analysis determines the top-
ics that characterize the multi-document corpus,
and Jensen-Shannon divergence selects sentences
from the corpus. This process offers many poten-
tial control settings for understanding and improv-
ing the summarization process.
Figure 1 shows topic analysis with corpus input,
control settings, and product outputs of topics and
probability estimates of topic compositions and
document mixtures. There are controls for doc-
ument preparation (headlines) and analysis (num-
ber of topics, initial ? and ?, number of iterations,
and whether to optimize ? and ? in process).
Figure 2 shows summarization with corpus and
topic inputs, control settings, and the text summa-
rization product. There are controls for extraction
of sentences (Extract ? and JSD Divisor) and for
composing the summary (Order policy).
Topic analysis has become a popular choice for
text summarization as seen in Text Analysis Con-
CorpusAnalyze 
Topics
# Iterations!
Optimize !, "
# Topics!
Initial !, "
Topics
#
$
Headlines
Figure 1: Topic Analysis
CorpusAnalyze 
Topics
Order policy
Extract !!
JSD divisor
Summary
Topics
Figure 2: Text Summarization
ferences (TAC, 2010; TAC, 2011) with individual
team reports (Delort and Alfonseca, 2011; Lui et
al., 2011; Mason and Charniak, 2011). Nenkova
and McKeown (2012; 2011) included topic anal-
ysis among standard methods in their surveys of
text summarization methodologies. Haghighi and
Vanderwende (2009) explored extensions of LDA
topic analysis for use in multiple document sum-
marization tasks. Yet there are many control set-
tings that can affect summarization that have not
been explicitly studied or documented, and that
are important for reproducing research results.
In this text summarization pilot study, we exper-
iment with several control settings. As in Mason
and Charniak (2011) we do a general rather than
guided summarization. Our primary contribution
is illustrating the use of efficient experimental de-
sign on control settings to help understand and im-
prove the text summarization process. We enjoy
some success in this endeavor even as we are sur-
prised by some of our results.
74
2 Technical Background
2.1 LDA Topic Analysis
LDA topic analysis uses a per document bag of
words approach to determine topic compositions
of words and document mixtures of topics. Anal-
ysis constructs topic compositions and document
mixtures by assigning words to topics within doc-
uments. Weighted topic compositions can then be
used as a basis for selecting the most informative
text to include in summarizations.
LDA topic analysis is based on a generative
probabilistic model. Document mixtures of top-
ics are generated by a multinomial distribution,
?, and topic compositions of words are gener-
ated by a multinomial distribution, ?. Both ? and
? in turn are generated by Dirichlet distributions
with parameters ? and ? respectively. Figure 3
(Steyvers and Griffiths, 2007) shows a corpus ex-
plained as the product of topic word compositions
(?) and document topic mixtures (?).
Corpus
w
o
r
d
s
documents
w
o
r
d
s
topics documents
t
o
p
i
c
s
=
x
!
"
Figure 3: Topic Model
The joint distribution of words and topics (Grif-
fiths and Steyvers (2004)) is given by P (w, z) =
P (w|z)P (z) where in generating a document the
topics are generated with probability P (z) and the
words given the topics are generated with proba-
bility P (w|z). Here
P (w|z) =
(
? (??)
? (?)V
)Z Z?
z=1
?
v ? (nzv + ?)
? (nz? + ??)
,
(1)
where nzv is the number of times word v occurs in
topic z, nz? is the number of times topic z occurs,
?? is the sum of the ? scalar over all word types,
and ? ( ) is the gamma function (Knuth, 2004),
and
P (z) =
(
? (??)
? (?)Z
)D D?
d=1
?
z ? (nzd + ?)
? (n?d + ??)
, (2)
where nzd is the number of times topic z occurs in
document d, n?d is the number of times document
d occurs, and ?? is the sum of ?s over topics.
Analysis reverses the generative model. Given
a corpus, topic analysis identifies weighted topic
word compositions and document topic mixtures
from the corpus. We assign topics to words in
the training corpus using Gibbs sampling (Gel-
man et al., 2004) where each word is considered in
turn in making the topic assignment. We monitor
training progress by logP (w, z) where a greater
logP (w, z) indicates better fit. After sufficient it-
erations through the corpus the logP (w, z) typi-
cally converges to steady state.
Analysis products are topic determinations for
the corpus as well as weighted estimates of topic
word compositions ? and document topic mix-
tures ?. The ? and ? priors are optimized (re-
estimated) during training and the asymmetric ?
which varies by topic can be used as a measure of
topic importance in our summarization step.
The topic analysis implementation used in this
pilot study borrows from the UMass Mallet topic
analysis (McCallum, 2002).
2.2 Jensen-Shannon Divergence
From the topic word compositions and optimized
?s, we form a weighted aggregate vector of the
prominent topics, and select sentences from the
corpus that have minimal divergence from the ag-
gregate topic. The operating assumption is that the
aggregate topic vector adequately represents the
content of an ideal summary. So the closer to zero
divergence from the aggregate topic, the closer we
are to the ideal summary.
We seek to minimize the Jensen-Shannon Di-
vergence, JSD(C||T ), a symmetric Kullback-
Liebler (KL) divergence, between the extractive
summary content, C, and the aggregate topic, T,
using a greedy search method of adding at each
pass through the corpus the sentence that most
reduces the divergence. Haghighi and Vander-
wende (2009) made similar use of KL divergence
in their Topic Sum method.
In preliminary studies, this minimize JSD cri-
terion seemed to give overly long sentences be-
cause the greedy method favored the greatest re-
duction in JSD regardless of the length of the sen-
tence. This affected readability and rapidly used
up all available target document size. Therefore
we modified the greedy search method to consider
sentence length as well.1
1Global optimization of JSD(C||T ) could address both
of these issues; we will investigate this option in a future ef-
fort.
75
In selecting each new sentence we seek to maxi-
mize the reduction in divergence corrected for sen-
tence length
(JSD(Ct?1||T )? JSD(St, Ct?1||T ))
function(length(St))
, (3)
where St is the sentence under consideration and
Ct?1 is the content from the previously completed
iterations, and the function of length of St, is ei-
ther the constant 1 (i.e. no correction for sentence
length) or?length(St).
3 Pilot Study Using TAC 2010 Samples
Our goal is to investigate and optimize factors that
impact multi-document extractive summarization.
We hope to subsequently extend our findings and
experience to abstractive summarization as well.
For our pilot, we?ve chosen summarization of
the 2010 Text Analysis Conference (2010) sam-
ple themes, which are conveniently available and
of a manageable size. The three sample themes
are from different summarization categories out of
a total of 46 news themes over five different cat-
egories, with 10 original and 10 follow-up news
reports each. In the original TAC 2010 task, par-
ticipants were asked to do focused queries varying
with the summarization category. In our pilot we
perform an undirected summarization of the orig-
inal news reports.
NIST provides 4 model summaries for each
news theme annotated for the focused summary,
and we use these model summaries in scoring our
extractive summarizations.2 We also include a
measure of fluency in our assessment.
Our document summarization task is then: mul-
tiple document extractive summarization using 10
documents of less than 250 words each to con-
struct summaries of 100 words.
3.1 Preliminary Results of Topic Analysis
Topic analysis is such a complex methodology that
it makes sense to fix some parameters before using
it in the summarization process.
We use the commonly accepted initial ? value
of 1 for each topic giving a sum of ? values equal
to the number of topics. Later, we experiment with
a single individual topic initial ? value, but we al-
ways maintain an initial ? sum equal to the num-
ber of topics. Likewise we use the scalar ? value
2Comparison of our summarization results versus the
TAC 2010 task will necessarily be imprecise given the dif-
ferences in focus of our pilot study from TAC 2010.
0.1 typical of a modest number of word types (less
than 1000 in this study).
In prior studies, we found that re-estimating ?
and ? frequently adds little cost to topic analysis
and drives better and more rapid convergence. We
optimize ? and ? every 5 iterations, starting at it-
eration 50.
How Many Topics to Use
The number of topics depends on the problem it-
self. The problem of size of ? 2000 words per
news theme would indicate a number of topics be-
tween 3 and 20 as adequate to explain document
word use where the log(|Corpus|) is the mini-
mum and?|Corpus| is the maximum number of
topics to use (Meila?, 2007).
A common way to select the correct number of
topics is to optimize logP (w) on held-out doc-
uments, where greater log likelihoods indicate a
better number of topics. While it would be im-
practical to do such a study for each news theme
or each document summary, it is reasonable to do
so on a few sample themes and then generalize to
similar corpora. We look at log likelihood for 3, 5,
and 10 topics using the TAC 2010 sample themes.
As there are only 10 documents for each theme,
we use the TAC 2010 update documents as held-
out documents for calculating the log likelihoods.
Topic word distributions, ?, from training are
used to infer document mixtures, ?, on the held-
out data, and the log P (w) is calculated (Teh et
al., 2007) as:
P (w) =
?
d,i
(?
z
nzwi + ?
nz? + ??
nzd + ?
n?d + ??
)
, (4)
where the sum is over all possible topics for a
given word and the product is over all documents
and words.
Table 1 shows mean log likelihoods for the news
themes at 3, 5 and 10 topics each. There is lit-
tle practical difference between the log likelihood
measures even though the 3 topic model has a sig-
nificantly lower log likelihood (p < 0.05) than
the 5 and 10 topic models. We assess topic quality
more directly to see which model is better.
3 Topics 5 Topics 10 Topics
-6.00 -5.97 -5.96
Table 1: Held-out Log Likelihood Number Topics.
76
Useful topic quality measures are:
Importance measured by number of documents
(or optimized ?s). Low importance topics,
with very few documents related to a topic,
indicate that we have more topics than neces-
sary. While not a fatal flaw, the topic model
may be over fit.
Coherence measured as a log sum of co-
occurrence proportions of each topic?s high
frequency words across multiple docu-
ments (Mimno et al., 2011). The more neg-
ative the coherence measure, the poorer the
coherence. A few poor coherence topics is
not fatal, but the topic model may be over fit.
Similarity to other topics measured by cosine
distance between topic vectors is undesirable.
The more similar the topics, the more diffi-
cult it is to distinguish between them. Many
similar topics makes it difficult to discrimi-
nate among topics over the corpus.
Reviewing the document quality for 3, 5 and 10
topics we find:
? More low importance topics in 10 versus 5
and 3 topic models,
? Somewhat better topic coherence in 3 and 5
topic models,
? Undesirable greater topic similarity for the 3
versus 5 versus 10 topic models.
We choose the 10 topic model giving higher pri-
ority to the problem of undesirable topic similar-
ity, recognizing that we may get some unimportant
or less coherent topics. As our summarization pro-
cess only uses the most important topics for the ag-
gregate topic, the occasional unimportant and less
coherent topic should not matter.
Document Preparation
Document cleaning removed all HTML, as well
as all header information not related to the articles
themselves; document dates, references, and head-
lines were saved for use in the document summa-
rization step. Document headlines were optionally
folded into the document text. Stop words were re-
moved and remaining words lemmatized for topic
analysis.
4 Design of Experiments
As our information about the various controls in
the process and the expected results is fairly rudi-
mentary, we use efficient screening experimental
designs to evaluate several factors at the same time
with a minimum number of trials. We define the
factors (control parameters) in our experiment, the
dependent variables we will measure, and finally
select the screening design itself.
Most of the process of topic analysis will re-
main fixed such as the use of 10 topics, initial ?
sum of 10, initial scalar ? of 0.1, optimization of
? and ? every 5 iterations and 500 total iterations
before saving the final topic vector weights and
corresponding topic alphas.
From our experimentation we hope to find:
? Factors impacting dependent variables,
? Gross magnitude of impact on dependent
variables,
? Factors to followup with in more detail.
4.1 Experimental Factors
In screening experiments, we chose factors about
which we have crude information, and which we
think could impact intermediate or final product
results. To learn as much as possible about factor
effects, we choose to vary them between default
and extreme settings or between two extremes
where we hope to see some positive impact.
Our experimental factors are:
Save headline text as part of document prepara-
tion (Yes, No). Headlines often contain im-
portant summary information. We test to see
if such information improves summaries.
Single fixed ? proportion of the ? sum (*, 0.5).
Topic analysis typically selects (weights) a
few important topic vectors with substantial
proportions of the ? sum. We want to see if
biasing selection of a single important vec-
tor at a 0.5 proportion of the ? sum improves
summaries versus unbiased ? weighting (*).
Aggregate topic policy as a proportion of the ?
sum for selecting the topic aggregate used in
summarization (0.5, 0.75). We order topics
based on the optimized (re-estimated) ?s and
aggregate topics summing and weighting by
the ?s until we reach the aggregate topic pol-
icy proportion. We want to see which policy
(0.5 or 0.75) proportion of the ? sum results
in better summaries.
JSD divisor to use with iterative greedy search
for sentences (ONE, SQRT). Prior work
shows the JSD Divisor impacts the length of
77
sentences selected. We test the impact on the
summaries themselves.
Order policy for constructing the summary
from selected sentences (DATE-DOC,
SALIENCE-DOC). Ordering sentences by
news report date or by salience as measured
by reduction in JSD should impact the
fluency of summaries.
4.2 Dependent Variables
We want readable and informative text that sum-
marizes content of the input documents in the al-
lowable space. We measure several intermedi-
ate process variables as well as evaluate the sum-
maries themselves.
Intermediate measures include:
? Initial selected sentence Jensen-Shannon di-
vergence from the aggregate topic. The first
sentence selected should substantially reduce
divergence.
? Final selected sentence Jensen-Shannon di-
vergence from the aggregate topic. Diver-
gence close to zero would indicate broad cov-
erage of the aggregate topic; it may be related
to summary content.
? Number of topics in the aggregate topic.
? Average sentence length. This should be im-
pacted by the JSD divisor; it may be related
to summary fluency.
ROUGE (Lin, 2011) is a package for auto-
matic evaluation of summaries that compares sys-
tem produced summaries to model (gold stan-
dard) summaries and reports statistics such as R-
2, bi-gram co-occurrence statistics between sys-
tem and model summaries, and SU4, skip bi-gram
co-occurrence statistics where word pairs no more
than 4 words apart may also be counted as bi-
grams. The R-2 and SU4 are automated content
measures reported for TAC 2010, and the gold
standard summaries are readily available for the
samples topics. We use ROUGE R-2 and SU4 as
reliable dependent measures and for comparison
to TAC 2010 results.
We add a simple measure of fluency focused on
across sentence issues. The fluency score starts
at a value of 5 and then subtracts: 1 for each
non sequitur or obvious out of order sentence, 1/2
for each missing co-reference, non-informative,
ungrammatical, or redundant sentence. For sen-
tences of less than 20 words, when more than one
penalty applies only the most severe penalty is ap-
plied, so as not to penalize the same short phrase
multiple times. Scoring is done by one of the au-
thors without knowing the combination of experi-
mental factors of the summary (blind scoring).
Summary measures thus include: ROUGE R-2,
ROUGE SU4, and Fluency.
4.3 Select Experimental Design
Screening designs focus on detecting and assess-
ing main effects and optionally low order inter-
action effects. When all experimental factors are
continuous, center points may also be included in
some designs. In subsequent stages of experimen-
tation, when factors have been reduced to a min-
imum, one can use more fine grained factor set-
tings to better map the response surface for those
factors. Two common families of screening de-
signs (Montgomery, 1997) are:
Two level fractional factorial Uses a power of
1/2 fraction of a full two level factorial design.
For example, instead of running all possible
combinations of 5 factors (i.e. 32 trials), you
could choose a 1/2 or even 1/4 fraction of the
design, based on how many experiments you
can run and how much confounding you are
willing to accept between main effects and
various interaction effects. The 1/2 fraction of
a 5 factor design would result in 16 trials be-
ing run with the main effects estimated clear
of any 2-way or 3-way interactions.
Plackett-Burman These screening designs are
available in multiples of 4 trials and can have
as many factors as the number of trials less
one. Main effects are confounded with all
other effects in the Plackett-Burman design
and so not estimable, but the confounding is
spread evenly among all main effects rather
than concentrated in specific interactions as
in the fractional factorial.
We?ve chosen the 12 run Plackett-Burman de-
sign with 5 factors and 6 degrees of freedom from
the unassigned (dummy) factors available to esti-
mate error. Assuming sparsity of effects (or equiv-
alently invoking the Pareto principal), there will
likely only be a few critical factors explaining
much of the variation in dependent variables.
Table 2 shows the resulting Plackett-Burman
design excluding dummy factors.
78
Run Fixed Aggr JSD Order Head
Alpha Topic Div Line
1 .5 .75 ONE SAL YES
2 * .75 SQRT DATE YES
3 .5 .5 SQRT SAL NO
4 * .75 ONE SAL YES
5 * .5 SQRT DATE YES
6 * .5 ONE SAL NO
7 .5 .5 ONE DATE YES
8 .5 .75 ONE DATE NO
9 .5 .75 SQRT DATE NO
10 * .75 SQRT SAL NO
11 .5 .5 SQRT SAL YES
12 * .5 ONE DATE NO
Table 2: Plackett-Burman 12 DOE.
5 Experimental Results
We analyze our experiment using conventional
analysis of variance (ANOVA) and show tables of
means for the various experimental conditions. As
this is a screening experiment, we treat a p-value<
0.20 as informative and consider the correspond-
ing factor worth further consideration. To save
space, only significant p-values are reported rather
than the full ANOVAs.
5.1 Intermediate Measures
Number of topics in the aggregate topic is directly
impacted by the AggrTopic setting; we simply re-
port the mean number of topics selected by Aggr-
Topic value (Table 3). The 1.0 average number of
topics for AggrTopic set to 0.5 indicates that only
one topic was ever selected for the aggregate topic
at this setting. This implies that the most impor-
tant topic always had an ? proportion > 0.50 of
the ? sum even when the FixedAlpha setting was
* (for unbiased ? weighting). This is unexpected
in that we thought the most important topic ? de-
termined by topic analysis would be more variable
and show some ? values with proportions less than
0.5 of the ? sum.
Aggr Number
Topic Topics
0.50 1.00
0.75 4.55
Table 3: Average Number Topics.
Average sentence length in the summary may be
affected by any of the independent variables ex-
cept sentence order policy. JSD Divisor has a dra-
matic impact (p < 0.0001) and AggrTopic a mod-
est impact (p < 0.01) on average sentence length.
Using a divisor of ONE in the JSD based sentence
selection results in much longer sentences while
using AggrTopic of 0.5 results in shorter sentences
(Table 4).
Aggr Sentence JSD Sentence
Topic Length Divisor Length
0.5 20.3 ONE 26.8
0.75 23.9 SQRT 17.4
Standard Error of the mean = 0.78
Table 4: Average Sentence Length.
Initial selected sentence Jensen-Shannon diver-
gence (JSD) should be affected directly by JSD
Divisor in iterative sentence selection, but may
also be affected by any of the other independent
variables except for sentence order policy. Aggr-
Topic and JSD Divisor strongly impact initial sen-
tence JSD (p < 0.00005).
The table of JSD initial sentence means by Ag-
grTopic and JSD Divisor is revealing (Table 5).
The JSD for the initial sentence selected is lower
for AggrTopic of 0.5. We observed above that only
one topic is selected for the aggregate topic when
AggrTopic is 0.5. Thus we achieve a lower di-
vergence of the initial sentence from the aggregate
topic when the aggregate is composed of only one
topic. For initial sentence JSD, aggregating topics
seems ineffective.
Similarly a JSD Divisor of ONE gives a lower
initial divergence than using the SQRT as the di-
visor. The interpretation is problematic here in
that a divisor of ONE seems to give lower ini-
tial divergence because it selects longer sentences,
which means that less space remains in the sum-
mary to select other sentences minimizing total di-
vergence.
Aggr JSD JSD JSD
Topic Initial Divisor Initial
0.5 0.665 ONE 0.658
0.75 0.735 SQRT 0.742
Standard Error of the mean = 0.0056
Table 5: Average Initial JSD.
Table 6 shows the impact of AggrTopic and JSD
Divisor together on the JSD for the initial sen-
tence. There is still the issue of whether using a
JSD Divisor of ONE is appropriate given the ef-
fect on the remaining summary size, but the effects
appear additive.
79
Aggr JSD JSD
Topic Divisor initial
0.50 ONE 0.627
0.50 SQRT 0.703
0.75 ONE 0.690
0.75 SQRT 0.780
Standard Error of the mean = 0.0080
Table 6: Average Initial JSD.
Final sentence Jensen-Shannon Divergence
(JSD) may be affected by any but the sentence or-
der policy variable. AggrTopic (p < 0.00001) and
JSD Divisor (p < 0.001) strongly impact the final
sentence JSD; there is also a possible effect from
including headlines in the summary (p < 0.1).
The effect of the JSD Divisor has reversed from
the initial JSD; using a divisor of ONE results here
in a less desirable higher divergence for the final
sentence. The AggrTopic effect is about the same
as for initial JSD divergence; a single dominant
topic seems more effective than using an aggre-
gate topic.
Aggr JSD JSD JSD
Topic Final Divisor Final
0.5 0.422 ONE 0.487
0.75 0.513 SQRT 0.448
Standard Error of the mean = 0.0047
Table 7: Average Initial JSD.
Impact of AggrTopic and JSD Divisor together
on the JSD for the initial sentence (Table 8) seems
additive.
Aggr JSD JSD
Topic Divisor final
0.50 ONE 0.437
0.50 SQRT 0.407
0.75 ONE 0.537
0.75 SQRT 0.490
Standard Error of the mean = 0.0066
Table 8: Average Final JSD.
5.2 Product Measures
Based on the analysis of intermediate measures, it
would seem that using a JSD Divisor of the SQRT
and selecting only the dominant topic gives less
divergence from the aggregate topic. However,
we have to be careful here in drawing conclusions
based on intermediate variables; selecting only the
dominant topic may result in reduced divergence,
but this does not necessarily mean that the domi-
nant topic is representative of good summaries.
We examine product variables to provide direct
support in our study, and so we ask how ROUGE
R-2 and SU4, and fluency evaluations vary with
the experimental factors. This pilot studies un-
guided summarization of initial stories from the 3
sample news themes from 3 separate categories.
While results are not directly comparable with
those of the full TAC 2010 test corpus, we will use
the TAC 2010 results as a reference point versus
our own results. The average of all experiments
are reported along with the TAC 2010 results (Ta-
ble 9). Our ROUGE R-2 and SU4 performance
seems reasonable showing results better than the
baseline but not as good as the best system.
Reference System R-2 SU4
Baseline - Lead sentences 5.4 8.6
Baseline - MEAD? 5.9 9.1
Best System 9.6 13.0
Pilot Average 6.7 10.1
Pilot Minimum 5.6 8.7
Pilot Maximum 8.1 11.9
?Text summarization system (Radevet al., 2004)
Table 9: TAC 2010 ROUGE Scores.
ROUGE R-2 results show no significant impact
from our experimental factors. This is disappoint-
ing as it gives us no handle on how to improve
performance.
ROUGE SU4 shows a modest impact for Aggr-
Topic (p < 0.025) and the possible impact of JSD
Divisor (p < 0.20). Note that we dropped Or-
der and FixedAlpha factors from the model; Or-
der because it can only effect sentence order and
FixedAlpha because the most important ? deter-
mined automatically by topic analysis did not vary
much from the 0.5 FixedAlpha. A benefit of drop-
ping terms from the model is that we have more
dummy factors to estimate error.
The ROUGE SU4 means (Table 10) show the
same pattern as for the JSD final sentence, but the
differences are not as clear cut. Box and whiskers
plots for AggrTopic and JSD Divisor (Figures 4
and 5) offer more insight into the AggrTopic and
JSD Divisor effects.
There is a clear distinction between AggrTopic
80
Aggr ROUGE JSD ROUGE
Topic SU4 Divisor SU4
0.5 10.75 ONE 9.70
0.75 9.48 SQRT 10.53
Standard Error of the mean = 0.32
Table 10: Average ROUGE SU4.
levels 0.5 and 0.75 with better results at the 0.5
level, except for an outlier value of 9.1. Investiga-
tion shows no data coding error and nothing spe-
cial about the experimental conditions other than
if uses a JSD Divisor of ONE which also gives
lower SU4 scores. The box and whiskers plots for
JSD Divisor effects also suggest a positive effect
for JSD Divisor of SQRT, but the whiskers over-
lap the boxes indicating no strong effect.
l
0.5 0.75
9.0
9.5
10.0
10.5
11.0
11.5
12.0
Alpha Extract Proportion
ROUGE 
SU4
Figure 4: ROUGE
SU4 by Aggr Topic
ONE SQRT
9.0
9.5
10.0
10.5
11.0
11.5
12.0
JSD Divisor
ROUGE 
SU4
Figure 5: ROUGE
SU4 by JSD Divisor
We had speculated that the final sentence diver-
gence might be related to some of the end prod-
uct measures. Indeed, we find that JSD final sen-
tence is strongly inversely related to ROUGE SU4
as shown by regression analysis (Table 11). While
the residual error of 0.73 indicates that we can
only reliably predict ROUGE SU4 within 1.5 units
(for averages of 3 trials), this is still important.
A 0.1 reduction in final sentence divergence cor-
responds on the average to a 1.4 unit increase in
ROUGE SU4.
Estimate StdErr t Pr(>|t|)
Intercept 16.865 1.934 8.721 ?0.0
JSDfinal -14.435 4.112 -3.510 0.006
Residual standard error: 0.73 on 10 degrees of freedom
F-statistic: 12.32 on 1 and 10 DF, p-value: 0.0056
Table 11: Regression - ROUGE SU4.
We thought Simple Fluency would show an ef-
fect for sentence order policy and maybe other fac-
tors. Analysis shows an effect for JSD Divisor
(p < 0.05) and possible effects of Order policy
and Head lines (p < 0.20).
Fluency means (Table 12) show that fluency is
better for JSD Divisor ONE. From our experience
of scoring Fluency, this would seem to be because
the fewer and longer sentences with JSD Divisor
of ONE offer fewer chances for disfluencies. The
better Fluency with DATE ordering likely comes
from fewer out of order or non sequitur sentences,
and the better Fluency with NO headlines likely
results from fewer short ungrammatical headlines
as part of the text.
JSD Flu- Order Flu- Head Flu-
Div ency ency Lines ency
ONE 3.95 DATE 3.80 NO 3.80
SQRT 3.33 SAL 3.47 YES 3.47
Standard Error of the mean = 0.16
Table 12: Average Fluency.
6 Summary and Discussion
Our pilot studied topic analysis based multi-
document extractive summarization using the
2010 TAC sample topics. Our experimental design
process identified control factors with their default
and extreme settings, defined intermediate and fi-
nal product dependent measures, designed the ex-
periment, ran, and analyzed the experiment.
We identified an intermediate variable, final se-
lected sentence divergence, that could be used as a
stand-in for the product content measure, ROUGE
SU4. We found that using a single dominant topic,
instead of an aggregate topic, and using a divisor
of the square root of sentence length in sentence
selection, improved final sentence divergence and
ROUGE SU4. However, using a divisor of one in
sentence selection improved fluency of summaries
which is at odds with the benefit of using square
root of sentence length to improve content.
Our planned experimentation has made obvious
and objective the process of describing and im-
proving our extractive summarization process. It
is an extremely useful process and furthermore a
process that when documented permits sharing of
results and even duplicating of results by others
working in this area.
81
References
Jean-Yves Delort and Enrique Alfonseca. 2011. De-
scription of the Google Update Summarizer. 2011
TAC Proceedings.
Andrew Gelman, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2004. Bayesian Data Analysis.
Chapman and Hall/CRC, New York, USA.
Tom L. Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. PNAS, 101(Suppl. 1):5228-5235.
Aria Haghighi and Lucy Vanderwalde. 2009.
Exploring Content Models for Multi-Document
Summarization. 2009 NACL Conference, HLT
Proceedings:362-370.
Donald E. Knuth. 1997. The Art of Computer Pro-
gramming, Volume 1 (Fundamental Algorithms).
Addison Wesley, New York, USA.
Chin-Yew Lin. 204. ROUGE: A Package for Auto-
matic Evaluation of Summaries. ACL 2004 Proceed-
ings of Workshop: Text Summarization Branches
Out.
Hongyan Liu, Pingan Liu, Wei Heng, and Lei Li.
2011. The CIST Summarization System at TAC
2011. 2011 TAC Proceedings.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi,
Stanko Dimitrov, Elliott Drabek, Ali Hakim,
Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi,
Horacio Saggion, Simone Teufel, Michael Topper,
Adam Winkel, and Zhu Zhang. 2004. MEAD
? A platform for multidocument multilingual
text summarization. Conference on Language
Resources and Evaluation LREC, Lisbon, Portugal,
(May 2004).
Rebecca Mason and Eugene Charniak. 2011. BLLIP
at TAC 2011: A General Summarization System for
a Guided Summarization Task. 2011 TAC Proceed-
ings.
Andres K. McCallum. 2002. MALLET: A
Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Marina Meila?. 2007. Comparing Clusterings ? an in-
formation based distance. J. Multivariate Analysis,
98(5):873-895.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing Semantic Coherence in Topic Models.
2011 EMNLP Conference, Proceedings:262-272.
Douglas C. Montgomery. 1997. Design and Analysis
of Experiments. John Wiley and Sons, New York,
USA.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic Summarization. Foundations and Trends in
Information Retrieval, 5(2-3):1003-233.
Ani Nenkova and Kathleen McKeown. 2012. A Sur-
vey of Text Summarization Techniques. Mining Text
Data. In Charu C. Aggarwal and ChengXiang Zhai
(eds.) Springer.
Mark Steyvers and Tom Griffiths. 2007. Probabilisitic
Topic Models. Latent Semantic Analysis: A road to
Meaning. In T. Landauer, S. D. McNamara & W.
Kintsch (eds.) Laurence Erlbaum.
Task Analysis Conference 2010 ?
Summarization Track. 2010.
http://www.nist.gov/tac/2010/Summarization/.
Task Analysis Conference 2011 ?
Summarization Track. 2011.
http://www.nist.gov/tac/2011/Summarization/.
Yee Whye Teh, Dave Newman, and Max Welling.
2007. Collapsed Variational Inference for HDP.
Advances in Neural Information Processing
Systems:1481-1488.
82
