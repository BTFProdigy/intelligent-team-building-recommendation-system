Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1135?1145, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
PATTY: A Taxonomy of Relational Patterns with Semantic Types
Ndapandula Nakashole, Gerhard Weikum, Fabian Suchanek
Max Planck Institute for Informatics
Saarbru?cken, Germany
{nnakasho,weikum,suchanek}@mpi-inf.mpg.de
Abstract
This paper presents PATTY: a large resource
for textual patterns that denote binary relations
between entities. The patterns are semanti-
cally typed and organized into a subsumption
taxonomy. The PATTY system is based on ef-
ficient algorithms for frequent itemset mining
and can process Web-scale corpora. It har-
nesses the rich type system and entity popu-
lation of large knowledge bases. The PATTY
taxonomy comprises 350,569 pattern synsets.
Random-sampling-based evaluation shows a
pattern accuracy of 84.7%. PATTY has 8,162
subsumptions, with a random-sampling-based
precision of 75%. The PATTY resource
is freely available for interactive access and
download.
1 Introduction
Motivation. WordNet (Fellbaum 1998) is one of the
most widely used lexical resources in computer sci-
ence. It groups nouns, verbs, and adjectives into sets
of synonyms, and arranges these synonyms in a tax-
onomy of hypernyms. WordNet is limited to single
words. It does not contain entire phrases or pat-
terns. For example, WordNet does not contain the
pattern X is romantically involved with Y. Just like
words, patterns can be synonymous, and they can
subsume each other. The pattern X is romantically
involved with Y is synonymous with the pattern X is
dating Y. Both are subsumed by X knows Y. Patterns
for relations are a vital ingredient for many appli-
cations, including information extraction and ques-
tion answering. If a large-scale resource of relational
patterns were available, this could boost progress in
NLP and AI tasks.
Yet, existing large-scale knowledge bases are
mostly limited to abstract binary relationships be-
tween entities, such as ?bornIn? (Auer 2007; Bol-
lacker 2008; Nastase 2010; Suchanek 2007). These
do not correspond to real text phrases. Only the Re-
Verb system (Fader 2011) yields a larger number of
relational textual patterns. However, no attempt is
made to organize these patterns into synonymous
patterns, let ale into a taxonomy. Thus, the pat-
terns themselves do not exhibit semantics.
Goal. Our goal in this paper is to systematically
compile relational patterns from a corpus, and to im-
pose a semantically typed structure on them. The
result we aim at is a WordNet-style taxonomy of
binary relations. In particular, we aim at patterns
that contain semantic types, such as ?singer? sings
?song?. We also want to automatically generalize
syntactic variations such as sings her ?song? and
sings his ?song?, into a more general pattern sings
[prp] ?song? with POS tag [prp]. Analogously but
more demandingly, we want to automatically infer
that the above patterns are semantically subsumed
by the pattern ?musician? performs on ?musical
composition? with more general types for the entity
arguments in the pattern.
Compiling and organizing such patterns is chal-
lenging for the following reasons. 1) The number
of possible patterns increases exponentially with the
length of the patterns. For example, the string ?Amy
sings ?Rehab?? can give rise to the patterns ?singer?
sings ?song?, ?person? sings ?artifact?, ?person?
[vbz] ?entity?, etc. If wildcards for multiple words
are allowed (such as in ?person? sings * ?song?), the
number of possible patterns explodes. 2) A pattern
1135
can be semantically more general than another pat-
tern (when one relation is implied by the other re-
lation), and it can also be syntactically more gen-
eral than another pattern (by the use of placehold-
ers such as [vbz]). These two subsumption orders
have a non-obvious interplay, and none can be ana-
lyzed without the other. 3) We have to handle pattern
sparseness and coincidental matches. If the corpus
is small, e.g., the patterns ?singer? later disliked her
song ?song? and ?singer? sang ?song?, may apply to
the same set of entity pairs in the corpus. Still, the
patterns are not synonymous. 4) Computing mutual
subsumptions on a large set of patterns may be pro-
hibitively slow. Moreover, due to noise and vague
semantics, patterns may even not form a crisp tax-
onomy, but require a hierarchy in which subsump-
tion relations have to be weighted by statistical con-
fidence measures.
Contributions. In this paper, we present PATTY, a
large resource of relational patterns that are arranged
in a semantically meaningful taxonomy, along with
entity-pair instances. More precisely, our contribu-
tions are as follows:
1) SOL patterns: We define an expressive fam-
ily of relational patterns, which combines syntac-
tic features (S), ontological type signatures (O), and
lexical features (L). The crucial novelty is the addi-
tion of the ontological, semantic dimension to pat-
terns. When compared to a state-of-the-art pattern
language, we found that SOL patterns yield higher
recall while achieving similar precision.
2) Mining algorithms: We present efficient and
scalable algorithms that can infer SOL patterns and
subsumptions at scale, based on instance-level over-
laps and an ontological type hierarchy.
3) A large Lexical resource:. On the Wikipe-
dia corpus, we obtained 350,569 pattern synsets
with 84.7% precision. We make our pat-
tern taxonomy available for further research at
www.mpi-inf.mpg.de/yago-naga/patty/ .
The paper is structured as follows. Section 2 dis-
cusses related work. Section 3 outlines the basic
machinery for pattern extraction. Section 4 intro-
duces our SOL pattern model. Sections 5 and 6
present the syntactic and semantic generalization of
patterns. Section 7 explains how to arrange the pat-
terns into a taxonomy. Section 8 reports our experi-
mental findings.
2 Related Work
A wealth of taxonomic knowledge bases (KBs)
about entities and their semantic classes have be-
come available. These are very rich in terms of
unary predicates (semantic classes) and their entity
instances. However, the number of binary relations
(i.e., relation types, not instances) in these KBs is
usually small: Freebase (Bollacker 2008) has a few
thousand hand-crafted relations. WikiNet (Nastase
2010) has automatically extracted ca. 500 relations
from Wikipedia category names. DBpedia (Auer
2007) has automatically compiled ca. 8000 names
of properties from Wikipedia infoboxes, but these
include many involuntary semantic duplicates such
as surname and lastname. In all of these projects,
the resource contains the relation names, but not the
natural language patterns for them. The same is true
for other projects along these lines (Navigli 2010;
Philpot 2008; Ponzetto 2007; Suchanek 2007).
In contrast, knowledge base projects that auto-
matically populate relations from Web pages also
learn surface patterns for the relations: examples
are TextRunner/ReVerb (Banko 2007; Fader 2011),
NELL (Carlson 2010; Mohamed11), Probase (Wu
2011), the dynamic lexicon approach by (Hoffmann
2010; Wu 2008), the LDA-style clustering approach
by (Yao 2011), and projects on Web tables (Li-
maye 2010; Venetis 2011). Of these, only TextRun-
ner/ReVerb and NELL have made large pattern col-
lections publicly available.
ReVerb (Fader 2011) constrains patterns to verbs
or verb phrases that end with prepositions, while
PATTY can learn arbitrary patterns. More impor-
tantly, all methods in the TextRunner/ReVerb family
are blind to the ontological dimension of the enti-
ties in the patterns. Therefore, there is no notion of
semantic typing for relation phrases as in PATTY.
NELL (Carlson 2010) is based on a fixed set
of prespecified relations with type signatures, (e.g.,
personHasCitizenship: ?person? ? ?country?), and
learns to extract suitable noun-phrase pairs from a
large Web corpus. In contrast, PATTY discovers pat-
terns for relations that are a priori unknown.
1136
In OntExt (Mohamed11), the NELL architecture
was extended to automatically compute new re-
lation types (beyond the prespecified ones) for a
given type signature of arguments, based on a clus-
tering technique. For example, the relation mu-
sicianPlaysInstrument is found by clustering pat-
tern co-occurrences for the noun-phrase pairs that
fall into the specific type signature ?musician? ?
?musicinstrument?. This technique works for one
type signature at a time, and does not scale up to
mining a large corpus. Also, the technique is not
suitable for inferring semantic subsumptions. In
contrast, PATTY efficiently acquires patterns from
large-scale corpora and organizes them into a sub-
sumption hierarchy.
Class-based attribute discovery is a special case
of mining relational patterns (e.g., (Alfonseca 2010;
Pasca 2007; Pasca 2008; Reisinger 2009)). Given a
semantic class, such as movies or musicians, the task
is to determine relevant attributes, such as cast and
budget for movies, or albums and biography for mu-
sicians, along with their instances. Unlike PATTY?s
patterns, the attributes are not typed. They come
with a prespecified type for the domain, but without
any type for the range of the underlying relation.
There are further relation-centric tasks in NLP
and text mining that have commonalities with our
endeavor, but differ in fundamental ways. The
SemEval-2010 task on classification of semantic re-
lations between noun-phrase pairs (Hendrickx 2010)
aimed at predicting the relation for a given sentence
and pair of nominals, but used a fixed set of prespec-
ified relations. Another task in this research avenue
is to characterize and predict the argument types for
a given relation or pattern (Kozareva 2010; Nakov
2008). This is closer to KB population and less re-
lated to our task of discovering relational patterns
and systematically organizing them.
From a linguistic perspective, there is ample
work on patterns for unary predicates of the form
class(entity). This includes work on entailment of
classes, i.e., on is-a and subclassOf relationships.
Entailment among binary predicates of the form re-
lation(entity1, entity2) has received less attention
(Lin 2001; Chklovski 2004; Hashimoto 2009; Be-
rant 2011). These works focus solely on verbs, while
PATTY learns arbitrary phrases for patterns.
Several lexical resources capture verb categories
and entailment: WordNet 3.0 (Fellbaum 1998) con-
tains about 13,000 verb senses, with troponymy and
entailment relations; VerbNet (Kipper 2008) is a hi-
erarchical lexicon with more than 5,000 verb senses
in ca. 300 classes, including selectional preferences.
Again, all of these resources focus solely on verbs.
ConceptNet 5.0 (Havasi 2007) is a thesaurus of
commonsense knowledge built as a crowdsourcing
endeavor. PATTY, in contrast, is constructed fully
automatically from large corpora. Automatic learn-
ing of paraphrases and textual entailment has re-
ceived much attention (see the survey of (Androut-
sopoulos 2010)), but does not consider fine-grained
typing for binary relations, as PATTY does.
3 Pattern Extraction
This section explains how we obtain basic textual
patterns from the input corpus. We first apply the
Stanford Parser (Marneffe 2006) to the individual
sentences of the corpus to obtain dependency paths.
The dependency paths form a directed graph, with
words being nodes and dependencies being edges.
For example, the sentence ?Winehouse effortlessly
performed her song Rehab.? yields the following de-
pendency paths:
nsubj(performed-3, Winehouse-1)
advmod(performed-3, effortlessly-2)
poss(Rehab-6, her-4)
nn(Rehab-6, song-5)
dobj(performed-3, Rehab-6)
While our method also works with patterns obtained
from shallow features such as POS tags, we found
that dependency paths improve pattern extraction
precision especially on long sentences.
We then detect mentions of named entities in the
parsed corpus. For this purpose, we use a dictio-
nary of entities. This can be any resource that con-
tains named entities with their surface names and se-
mantic types (Auer 2007; Suchanek 2007; Hoffart
2011; Bollacker 2008). In our experiments, we used
the YAGO2 knowledge base (Hoffart 2011). We
match noun phrases that contain at least one proper
noun against the dictionary. For disamiguation, we
1137
use a simple context-similarity prior, as described
in (Suchanek 2009). We empirically found that this
technique has accuracy well above 80% (and higher
for prominent and thus frequently occurring enti-
ties). In our example, the entity detection yields the
entities Amy Winehouse and Rehab (song).
Whenever two named entities appear in the same
sentence, we extract a textual pattern. For this pur-
pose, we traverse the dependency graph to get the
shortest path that connects the two entities. In the
example, the shortest path between ?Winehouse?
and ?Rehab? is: Winehouse nsubj performed dobj
Rehab. In order to capture only relations that refer
to subject-relation-object triples, we only consider
shortest paths that start with subject-like dependen-
cies, such as nsubj, rcmod and partmod. To re-
flect the full meaning of the patterns, we expand the
shortest path with adverbial and adjectival modifiers,
for example the advmod dependency. The sequence
of words on the expanded shortest path becomes our
final textual pattern. In the example, the textual pat-
tern is Amy Winehouse effortlessly performed Rehab
(song).
4 SOL Pattern Model
Textual patterns are tied to the particular surface
form of the text. Therefore, we transform the textual
patterns into a new type of patterns, called syntactic-
ontologic-lexical patterns (SOL patterns). SOL pat-
terns extend lexico-syntactic patterns by ontological
type signatures for entities. The SOL pattern lan-
guage is expressive enough to capture fine-grained
relational patterns, yet simple enough to be dealt
with by efficient mining algorithms at Web scale.
A SOL pattern is an abstraction of a textual pat-
tern that connects two entities of interest. It is a
sequence of words, POS-tags, wildcards, and onto-
logical types. A POS-tag stands for a word of the
part-of-speech class. We introduce the special POS-
tag [word], which stands for any word of any POS
class. A wildcard, denoted ?, stands for any (pos-
sibly empty) sequence of words. Wildcards are es-
sential to avoid overfitting of patterns to the corpus.
An ontological type is a semantic class name (such
as ?singer?) that stands for an instance of that class.
Every pattern contains at least two types, and these
are designated as entity placeholders.
A string and a pattern match, if there is an order-
preserving bijection from sequences of words in the
string to items in the pattern, so that each item can
stand for the respective sequence of words. For ex-
ample, the pattern ?person??s [adj] voice * ?song?
matches the strings ?Amy Winehouse?s soft voice
in ?Rehab?? and ?Elvis Presley?s solid voice in his
song ?All shook up??. The type signature of a pat-
tern is the pair of the entity placeholders. In the ex-
ample, the type signature is person ? song. The
support set of a pattern is the set of pairs of entities
that appear in the place of the entity placeholders
in all strings in the corpus that match the pattern.
In the example, the support set of the pattern could
be {(Amy,Rehab), (Elvis, AllShookUp)}. Each
pair is called a support pair of the pattern.
Pattern B is syntactically more general than pat-
tern A if every string that matches A also matches
B. Pattern B is semantically more general than A
if the support set of B is a superset of the support
set of A. If A is semantically more general than B
and B is semantically more general than A, the pat-
terns are called synonymous. A set of synonymous
patterns is called a pattern synset. Two patterns, of
which neither is semantically more general than the
other, are called semantically different.
To generate SOL patterns from the textual pat-
terns, we decompose the textual patterns into n-
grams (n consecutive words). A SOL pattern con-
tains only the n-grams that appear frequently in the
corpus and the remaining word sequences are re-
placed by wildcards. For example, in the sentence
?was the first female to run for the governor of?
might give rise to the pattern * the first female * gov-
ernor of, if ?the first female? and ?governor of? are
frequent in the corpus.
To find the frequent n-grams efficiently, we apply
the technique of frequent itemset mining (Agrawal
1993; Srikant 1996): each sentence is viewed as a
?shopping transaction? with a ?purchase? of several
n-grams, and the mining algorithm computes the n-
gram combinations with large co-occurrence sup-
port1. These n-grams allow us to break down a sen-
1Our implementation restricts n-grams to length 3 and uses
up to 4 n-grams per sentence
1138
tence into wildcard-separated subsequences, which
yields an SOL pattern. We generate multiple pat-
terns with different types, one for each combination
of types that the detected entities have in the under-
lying ontology.
We quantify the statistical strength of a pattern by
means of its support set. For a given pattern p with
type signature t1 ? t2, the support of p is the size
of its support set. For confidence, we compare the
support-set sizes of p and an untyped variant pu of
p, in which the types ?t1? and ?t2? are replaced by
the generic type ?entity?. We define the confidence
of p as the ratio of the support-set sizes of p and pu.
5 Syntactic Pattern Generalization
Almost every pattern can be generalized into a syn-
tactically more general pattern in several ways: by
replacing words by POS-tags, by introducing wild-
cards (combining more n-grams), or by generaliz-
ing the types in the pattern. It is not obvious which
generalizations will be reasonable and useful. We
observe, however, that generalizing a pattern may
create a pattern that subsumes two semantically dif-
ferent patterns. For example, the generalization
?person? [vb] ?person? subsumes the two semanti-
cally different patterns ?person? loves ?person? and
?person? hates ?person?. This means that the pattern
is semantically meaningless.
Therefore, we proceed as follows. For every pat-
tern, we generate all possible generalizations. If a
generalization subsumes multiple patterns with dis-
joint support sets, we abandon the generalized pat-
tern. Otherwise, we add it to our set of patterns.
6 Semantic Pattern Generalization
The main difficulty in generating semantic subsump-
tions is that the support sets may contain spurious
pairs or be incomplete, thus destroying crisp set in-
clusions. To overcome this problem, we designed
a notion of a soft set inclusion, in which one set S
can be a subset of another set B to a certain degree.
One possible measure for this degree is the confi-
dence, i.e., the ratio of elements in S that are in B,
deg(S ? B) = |S ? B|/|S|. However, if a support
set S has only few elements due to sparsity, it may
become a subset of another support setB, even if the
two patterns are semantically different. Therefore,
one has to take into account also the support, i.e., the
size of the set S. Traditionally, this is done through a
weighted trade-off between confidence and support.
To avoid the weight tuning, we instead devised
a probabilistic model. We interpret S as a random
sample from the ?true? support set S? that the pattern
would have on an infinitely large corpus. We want
to estimate the ratio of elements of S? that are in
B. This ratio is a Bernoulli parameter that can be
estimated from the ratio of elements of the sample S
that are in B. We compute the Wilson score interval
[c ? d, c + d] (Brown 2001) for the sample. This
interval guarantees that with a given probability (set
a priori, usually to ? = 95%), the true ratio falls into
the interval [c ? d, c + d]. If the sample is small, d
is large and c is close to 0.5. If the sample is large,
d decreases and c approaches the naive estimation
|S ? B|/|S|. Thereby, the Wilson interval center
naturally balances the trade-off between confidence
and the support. Hence we define deg(S ? B) = c.
This estimator may degrade when the sample size
is too small We can alternatively use a conservative
estimator deg(S ? B) = c?d, i.e., the lower bound
of the Wilson score interval. This gives a low score
to the case where S ? B if we have few samples (S
is small).
7 Taxonomy Construction
We now have to arrange the patterns in a semantic
taxonomy. A baseline solution would compare ev-
ery pattern support set to every other pattern support
set in order to determine inclusion, mutual inclusion,
or independence. This would be prohibitively slow.
For this reason, we make use of a prefix-tree for fre-
quent patterns (Han 2005). The prefix-tree stores
support sets of patterns. We then developed an algo-
rithm for obtaining set intersections from the prefix-
tree.
7.1 Prefix-Tree Construction
Suppose we have pattern synsets and their support
sets as shown in Table 1. An entity pair in a support
set is denoted by a letter. For example, in the sup-
port set for the pattern ?Politican? was governor
of ?State?, the entry ?A,80? may denote the entity
1139
ID Pattern Synset & Support Sets
P1 ?Politician? was governor of ?State?
A,80 B,75 C,70
P2 ?Politician? politician from ?State?
A,80 B,75 C,70 D,66 E,64
P3 ?Person? daughter of ?Person?
F,78 G,75 H,66
P4 ?Person? child of ?Person?
I,88 J,87 F,78 G,75 K,64
Table 1: Pattern Synsets and their Support Sets
Root 
A p1,p2 
B 
C 
D 
p1,p2 
p1,p2 
p2 
E p2 
F 
G 
H 
p3 I 
J 
F 
p4 
G p4 
K p4 
p4 
p4 p3 p3 
Figure 1: Prefix-Tree for the Synsets in Table 1.
pair Arnold Schwarzenegger, California, with an oc-
currence frequency 80. The contents of the support
sets are used to construct a prefix-tree, where nodes
are entity pairs. If synsets have entity pairs in com-
mon, they share a common prefix; thus the shared
parts can be represented by one prefix-path in the
tree. This enables subsumptions to be directly ?read
off? from the tree, while representing the tree in a
compact manner. To increase the chance of shared
prefixes, entity pairs are inserted into the tree in de-
creasing order of occurrence frequency.
The prefix-tree of support sets is a prefix-tree aug-
mented with synset information stored at the nodes.
Each node (entity pair) stores the identifiers of the
pattern sysnets whose support sets contain that en-
tity pair. In addition, each node stores a link to the
next node with the same entity pair.
Figure 1 shows the tree for the pattern synsets
in Table 1. The left-most path contains synsets P1
and P2. The two patterns have a prefix in common,
thus they share the same path. This is reflected by
the synsets stored in the nodes in the path. Synsets
P2 and P3 belong to two different paths due to dis-
similar prefixes although they have common nodes.
Instead, their common nodes are connected by the
same-entity-pair links shown as dotted lines in Fig-
ure 1. These links are created whenever the entity
pair already exists in the tree but with a prefix differ-
ent from the prefix of the synset being added to the
tree. The size of the tree is at most the total num-
ber of entity pairs making up the supports sets of the
synsets. The height of the tree is at most the size of
the the largest support set.
7.2 Mining Subsumptions from the Prefix-Tree
To efficiently mine subsumptions from the prefix-
tree, we have to avoid comparing every path to every
other path as this introduces the same inefficiencies
that the baseline approach suffers from.
From the construction of the tree it follows that
for any node Ni in the tree, all paths containing Ni
can be found by following node Ni?s links includ-
ing the same-entity-pair links. By traversing the en-
tire path of a synset Pi, we can reach all the pattern
synsets sharing common nodes with Pi. This leads
to our main insight: if we start traversing the tree
bottom up, starting at the last node in P ?is support
set, we can determine exactly which paths are sub-
sumed by Pi. Traversing the tree this way for all
patterns gives us the sizes of the support set intersec-
tion. The determined intersection sizes can then be
used in the Wilson estimator to determine the degree
of semantic subsumption and semantic equivalence
of patterns.
7.3 DAG Construction
Once we have generated subsumptions between re-
lational patterns, there might be cycles in the graph
we generate. We ideally want to remove the minimal
total number of subsumptions whose removal results
in an a directed acyclic graph (DAG). This task is
related to the minimum feedback-arc-set problem:
given a directed graph, we want to remove the small-
est set of edges whose removal makes the remaining
graph acyclic. This is a well known NP-hard prob-
lem (Kann 1992). We use a greedy algorithm for
1140
removing cycles and eliminating redundancy in the
subsumptions, thus effectively constructing a DAG.
Starting with a list of subsumption edges ordered by
decreasing weights, we construct the DAG bottom-
up by adding the highest-weight subsumption edge.
This step is repeated for all subsumptions, where we
add a subsumption to the DAG only if it does not
introduce cycles or redundancy. Redundancy occurs
when there already exists a path, by transitivity of
subsumptions, between pattern synsets linked by the
subsumption. This process finally yields a DAG of
pattern synsets ? the PATTY taxonony.
8 Experimental Evaluation
8.1 Setup
The PATTY extraction and mining algorithms were
run on two different input corpora: the New York
Times archive (NYT) which includes about 1.8 Mil-
lion newspaper articles from the years 1987 to 2007,
and the English edition of Wikipedia (WKP), which
contains about 3.8 Million articles (as of June 21,
2011). Experiments were carried out, for each cor-
pus, with two different type systems: a) the type sys-
tem of YAGO2, which consists of about 350,000 se-
mantic classes from WordNet and the Wikipedia cat-
egory system, and b) the two-level domain/type hier-
archy of Freebase which consists of 85 domains and
a total of about 2000 types within these domains.
All relational patterns and their respective entity
pairs are stored in a MongoDB database. We evalu-
ated PATTY along four dimensions: quality of pat-
terns, quality of subsumptions, coverage, and de-
sign alternatives. These dimensions are discussed
in the following four subsections. We also per-
formed an extrinsic study to demonstrate the use-
fulness of PATTY for paraphrasing the relations
of DBpedia and YAGO2. In terms of runtimes,
he most expensive part is the pattern extraction,
where we identify pattern candidates through de-
pendency parsing and perform entity recognition
on the entire corpus. This phase runs about a
day for Wikipedia a cluster. All other phases of
the PATTY system take less than an hour. All
experimental data is available on our Web site at
www.mpi-inf.mpg.de/yago-naga/patty/.
8.2 Precision of Relational Patterns
To assess the precision of the automatically mined
patterns (patterns in this section always mean pattern
synsets), we sampled the PATTY taxonomy for each
combination of input corpus and type system. We
ranked the patterns by their statistical strength (Sec-
tion 4), and evaluated the precision of the top 100
pattern synsets. Several human judges were shown
a sampled pattern synset, its type signature, and a
few example instances, and then stated whether the
pattern synset indicates a valid relation or not. Eval-
uators checked the correctness of the type signature,
whether the majority of patterns in the synset is rea-
sonable, and whether the instances seem plausible.
If so, the synset was flagged as meaningful. The re-
sults of this evaluation are shown in column four of
Table 2, with a 0.9-confidence Wilson score inter-
val (Brown 2001). In addition, the same assessment
procedure was applied to randomly sampled synsets,
to evaluate the quality in the long tail of patterns.
The results are shown in column five of Table 2. For
the top 100 patterns, we achieve above 90% preci-
sion for Wikipedia, and above 80% for 100 random
samples.
Corpus Types Patterns Top 100 Random
NYT
YAGO2 86,982 0.89?0.06 0.72?0.09
Freebase 809,091 0.87 ?0.06 0.71?0.09
WKP
YAGO2 350,569 0.95?0.04 0.85?0.07
Freebase 1,631,531 0.93?0.05 0.80?0.08
Table 2: Precision of Relational Patterns
From the results we make two observations. First,
Wikipedia patterns have higher precision than those
from the New York Times corpus. This is because
some the language in the news corpus does not ex-
press relational information; especially the news on
stock markets produced noisy patterns picked up by
PATTY. However, we still manage to have a preci-
sion of close to 90% for the top 100 patterns and
around 72% for random sample on the NYT cor-
pus. The second observation is that the YAGO2
type system generally led to higher precision than
the Freebase type system. This is because YAGO2
has finer grained, ontologically clean types, whereas
Freebase has broader categories with a more liberal
1141
assignment of entities to categories.
8.3 Precision of Subsumptions
We evaluated the quality of the subsumptions by
assessing 100 top-ranked as well as 100 randomly
selected subsumptions. As shown in Table 3, a
large number of the subsumptions are correct. The
Wikipedia-based PATTY taxonomy has a random-
sampling-based precision of 75%.
Corpus Types # Edges Top 100 Random
NYT
YAGO2 12,601 0.86?0.07 0.68?0.09
Freebase 80,296 0.89?0.06 0.41?0.09
WKP
YAGO2 8,162 0.83?0.07 0.75?0.07
Freebase 20,339 0.85?0.07 0.62?0.09
Table 3: Quality of Subsumptions
Example subsumptions from Wikipedia are:
? ?person? nominated for ?award? =
?person? winner of ?award?
? ?person? ? s wife ?person? =
?person? ?s widow ?person?
8.4 Coverage
To evaluate the coverage of PATTY, we would need
a complete ground-truth resource that contains all
possible binary relations between entities. Unfor-
tunately, there is no such resource2. We tried to
approximate such a resource by manually compil-
ing all binary relations between entities that ap-
pear in Wikipedia articles of a certain domain. We
chose the domain of popular music, because it offers
a plethora of non-trivial relations (such as addict-
edTo(person,drug), coveredBy(musician,musician),
dedicatedSongTo(musician,entity))). We considered
the Wikipedia articles of five musicians (Amy Wine-
house, Bob Dylan, Neil Young, John Coltrane, Nina
Simone). For each page, two annotators hand-
extracted all relationship types that they would spot
in the respective articles. The annotators limited
themselves to relations where at least one argument
type is ?musician?. Then we formed the intersection
of the two annotators? outputs (i.e., their agreement)
2Lexical resources such as WordNet contain only verbs, but
not binary relations such as is the president of. Other resources
are likely incomplete.
as a reasonable gold standard for relations identifi-
able by skilled humans. In total, the gold-standard
set contains 163 relations.
We then compared our relational patterns to the
relations included in four major knowledge bases,
namely, YAGO2, DBpedia (DBP), Freebase (FB),
and NELL, limited to the specific domain of music.
Table 4 shows the absolute number of relations cov-
ered by each resource. For PATTY, the patterns were
derived from the Wikipedia corpus with the YAGO2
type system.
gold standard PATTY YAGO2 DBP FB NELL
163 126 31 39 69 13
Table 4: Coverage of Music Relations
PATTY covered 126 of the 163 gold-standard re-
lations. This is more than what can be found in large
semi-curated knowledge bases such as Freebase,
and twice as much as Wikipedia-infobox-based re-
sources such as DBpedia or YAGO offer. Some
PATTY examples that do not appear in the other re-
sources at all are:
? ?musician? PRP idol ?musician? for the relation
hasMusicalIdol
? ?person? criticized by ?organization? for
critizedByMedia
? ?person? headliner ?artifact? for headlinerAt
? ?person? successfully sued ?person? for suedBy
? ?musician? wrote hits for ?musician? for wrote-
HitsFor,
This shows (albeit anecdotically) that PATTY?s pat-
terns contribute added value beyond today?s knowl-
edge bases.
8.5 Pattern Language Alternatives
We also investigated various design alternatives to
the PATTY pattern language. We looked at three
main alternatives: the first is verb-phrase-centric
patterns advocated by ReVerb (Fader 2011), the sec-
ond is the PATTY language without type signatures
(just using sets of n-grams with syntactic general-
izations), and the third one is the full PATTY lan-
guage. The results for the Wikipedia corpus and the
1142
Reverb-style patterns PATTY without types PATTY full
# Patterns 5,996 184,629 350,569
Patterns Precision 0.96?0.03 0.74?0.08 0.95?0.04
# Subsumptions 74 15,347 8,162
Subsumptions Precision 0.79 ?0.09 0.58?0.09 0.83?0.07
# Facts 192,144 6,384,684 3,890,075
Facts Precision. 0.86 ?0.07 0.64?0.09 0.88 ?0.06
Table 5: Results for Different Pattern Language Alternatives
Relation Paraphrases Precision Sample Paraphrases
DBPedia/artist 83 0.96?0.03 [adj] studio album of, [det] song by . . .
DBPedia/associatedBand 386 0.74?0.11 joined band along, plays in . . .
DBPedia/doctoralAdvisor 36 0.558?0.15 [det] student of, under * supervision . . .
DBPedia/recordLabel 113 0.86?0.09 [adj] artist signed to, [adj] record label . . .
DBPedia/riverMouth 31 0.83?0.12 drains into, [adj] tributary of . . .
DBPedia/team 1,108 0.91?0.07 be * traded to, [prp] debut for . . .
YAGO/actedIn 330 0.88?0.08 starred in * film, [adj] role for . . .
YAGO/created 466 0.79?0.10 founded, ?s book . . .
YAGO/isLeaderOf 40 0.53?0.14 elected by, governor of . . .
YAGO/holdsPoliticalPosition 72 0.73?0.10 [prp] tenure as, oath as . . .
Table 6: Sample Results for Relation Paraphrasing
YAGO2 type system are shown in Table 5; preci-
sion figures are based on the respective top 100 pat-
terns or subsumption edges. We observe from these
results that the type signatures are crucial for pre-
cision. Moreover, the number of patterns, subsump-
tions and facts found by verb-phrase-centric patterns
(ReVerb (Fader 2011)), are limited in recall. Gen-
eral pattern synsets with type signatures, as newly
pursued in this paper, substantially outperform the
verb-phrase-centric alternative in terms of pattern
and subsumption recall while yielding high preci-
sion.
8.6 Extrinsic Study: Relation Paraphrasing
To further evaluate the usefulness of PATTY, we per-
formed a study on relation paraphrasing: given a re-
lation from a knowledge base, identify patterns that
can be used to express that relation. Paraphrasing
relations with high-quality patterns is important for
populating knowledge bases and counters the prob-
lem of semantic drifting caused by ambiguous and
noisy patterns.
We considered relations from two knowledge
bases, DBpedia and YAGO2, focusing on relations
that hold between entities and do not include literals.
PATTY paraphrased 225 DBpedia relations with a
total of 127,811 patterns, and 25 YAGO2 relations
with a total of 43,124 patterns. Among these we
evaluated a random sample of 1,000 relation para-
phrases. Table 6 shows precision figures for some
selected relations, along anecdotic example patterns.
Some relations are hard to capture precisely. For
DBPedia/doctoralAdvisor, e.g., PATTY picked up
patterns like ?worked with? as paraphrases. These
are not entirely wrong, but we evaluated them as
false because they are too general to indicate the
more specific doctoral advisor relation.
Overall, however, the paraphrasing precision is
high. Our evaluation showed an average precision
of 0.76?0.03 across all relations.
9 Conclusion and Future Directions
This paper presented PATTY, a large resource of text
patterns. Different from existing resources, PATTY
organizes patterns into synsets and a taxonomy, sim-
ilar in spirit to WordNet. Our evaluation shows
that PATTY?s patterns are semantically meaning-
ful, and that they cover large parts of the relations
of other knowledge bases. The Wikipedia-based
version of PATTY contains 350,569 pattern synsets
at a precision of 84.7%, with 8,162 subsumptions,
at a precision of 75%. The PATTY resource is
1143
freely available for interactive access and download
at www.mpi-inf.mpg.de/yago-naga/patty/.
Our approach harnesses existing knowledge bases
for entity-type information. However, PATTY is not
tied to a particular choice for this purpose. In fact,
it would be straightforward to adjust PATTY to us-
ing surface-form noun phrases rather than disam-
biguated entities, as long as we have means to infer
at least coarse-grained types (e.g., person, organiza-
tion, location). An interesting future direction is to
study this generalized setting. We would also like
to investigate the enhanced interplay of information
extraction and pattern extraction, and possible appli-
cations for question answering.
References
Ion Androutsopoulos, Prodromos Malakasiotis: A Sur-
vey of Paraphrasing and Textual Entailment Methods.
Journal of Artificial Intelligence Research 38: 135?
187, 2010
Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami:
Mining Association Rules between Sets of Items in
Large Databases. SIGMOD Conference 1993
Enrique Alfonseca, Marius Pasca, Enrique Robledo-
Arnuncio: Acquisition of instance attributes via la-
beled and related instances. SIGIR 2010
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, Zachary G. Ives: DBpe-
dia: A Nucleus for a Web of Open Data. ISWC 2007,
data at http://dbpedia.org
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, Oren Etzioni: Open Information
Extraction from the Web. IJCAI 2007
Jonathan Berant, Ido Dagan, Jacob Goldberger: Global
Learning of Typed Entailment Rules. ACL 2011
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, Jamie Taylor: Freebase: a collaboratively
created graph database for structuring human knowl-
edge. SIGMOD Conference 2008, data at http://
freebase.com
Lawrence D. Brown, T.Tony Cai, Anirban Dasgupta: In-
terval Estimation for a Binomial Proportion. Statistical
Science 16: 101?133, 2001
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., Tom M. Mitchell.
Toward an Architecture for Never-Ending Language
Learning. AAAI 2010, data at http://rtw.ml.
cmu.edu/rtw/
Timothy Chklovski, Patrick Pantel: VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Relations.
EMNLP 2004; data available at http://demo.
patrickpantel.com/demos/verbocean/
Anthony Fader, Stephen Soderland, Oren Etzioni:
Identifying Relations for Open Information Extrac-
tion. EMNLP 2011, data at http://reverb.cs.
washington.edu
Christiane Fellbaum (Editor): WordNet: An Electronic
Lexical Database. MIT Press, 1998
Jiawei Han, Jian Pei , Yiwen Yin : Mining frequent pat-
terns without candidate generation. SIGMOD 2000.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, Jun?ichi Kazama:
Large-Scale Verb Entailment Acquisition from the
Web. EMNLP 2009
Catherine Havasi, Robert Speer, and Jason Alonso. Con-
ceptNet 3: a Flexible, Multilingual Semantic Net-
work for Common Sense Knowledge, RANLP 2007;
data available at http://conceptnet5.media.
mit.edu/
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O Seaghdha, Sebastian Pado, Marco
Pennacchiotti, Lorenza Romano, Stan Szpakowicz:
SemEval-2010 Task 8: Multi-Way Classification of
Semantic Relations Between Pairs of Nominals, 5th
ACL International Workshop on Semantic Evaluation,
2010; data available at http://www.isi.edu/
?kozareva/downloads.html
Johannes Hoffart, Fabian Suchanek, Klaus Berberich,
Edwin Lewis-Kelham, Gerard de Melo, Ger-
hard Weikum: YAGO2: Exploring and Query-
ing World Knowledge in Time, Space, Con-
text, and Many Languages. WWW 2011, data at
http://yago-knowledge.org
Raphael Hoffmann, Congle Zhang, Daniel S. Weld:
Learning 5000 Relational Extractors. ACL 2010
Vigo Kann: On the approximability of NP-complete opti-
mization problems. PhD thesis, Department of Numer-
ical Analysis and Computing Science, Royal Institute
of Technology, Stockholm. 1992.
Karin Kipper, Anna Korhonen, Neville Ryant,
Martha Palmer, A Large-scale Classification of
English Verbs, Language Resources and Evalua-
tion Journal, 42(1): 21-40, 2008, data available at
http://verbs.colorado.edu/?mpalmer/
projects/verbnet/downloads.html
Zornitsa Kozareva, Eduard H. Hovy: Learning Argu-
ments and Supertypes of Semantic Relations Using
Recursive Patterns. ACL 2010
Girija Limaye, Sunita Sarawagi, Soumen Chakrabarti:
Annotating and Searching Web Tables Using Entities,
Types and Relationships. PVLDB 3(1): 1338-1347
(2010)
Dekang Lin, Patrick Pantel: DIRT: discovery of inference
rules from text. KDD 2001
1144
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. Generating Typed Depen-
dency Parses from Phrase Structure Parses. LREC
2006.
Thahir Mohamed, Estevam R. Hruschka Jr., Tom M.
Mitchell: Discovering Relations between Noun Cat-
egories. EMNLP 2011
Ndapandula Nakashole, Martin Theobald, Gerhard
Weikum: Scalable Knowledge Harvesting with High
Precision and High Recall. WSDM 2011
Preslav Nakov, Marti A. Hearst: Solving Relational Simi-
larity Problems Using the Web as a Corpus. ACL 2008
Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Ca?cilia Zirn, Anas Elghafari: WikiNet: A Very Large
Scale Multi-Lingual Concept Network. LREC 2010,
data at http://www.h-its.org/english/
research/nlp/download/wikinet.php
Roberto Navigli, Simone Paolo Ponzetto: BabelNet:
Building a Very Large Multilingual Semantic Net-
work. ACL 2010 data at http://lcl.uniroma1.
it/babelnet/
Marius Pasca, Benjamin Van Durme: What You Seek Is
What You Get: Extraction of Class Attributes from
Query Logs. IJCAI 2007
Marius Pasca, Benjamin Van Durme: Weakly-Supervised
Acquisition of Open-Domain Classes and Class At-
tributes from Web Documents and Query Logs. ACL
2008
Andrew Philpot, Eduard Hovy, Patrick Pantel: The
Omega Ontology, in: Ontology and the Lexicon,
Cambridge University Press, 2008, data at http:
//omega.isi.edu/
Simone Paolo Ponzetto, Michael Strube: Deriving a
Large-Scale Taxonomy from Wikipedia. AAAI 2007,
data at http://www.h-its.org/english/
research/nlp/download/wikitaxonomy.
php
Joseph Reisinger, Marius Pasca: Latent Variable Models
of Concept-Attribute Attachment. ACL/AFNLP 2009
Ramakrishnan Srikant, Rakesh Agrawal: Mining Se-
quential Patterns: Generalizations and Performance
Improvements. EDBT 1996
Fabian M. Suchanek, Gjergji Kasneci, Gerhard Weikum:
YAGO: a Core of Semantic Knowledge. WWW 2007
Fabian M. Suchanek, Mauro Sozio, Gerhard Weikum:
SOFIE: a self-organizing framework for information
extraction. WWW 2009
Lin Sun, Anna Korhonen: Hierarchical Verb Clustering
Using Graph Factorization. EMNLP 2011
Petros Venetis, Alon Y. Halevy, Jayant Madhavan, Marius
Pasca, Warren Shen, Fei Wu, Gengxin Miao, Chung
Wu: Recovering Semantics of Tables on the Web.
PVLDB 4(9): 528-538, 2011
Tom White: Hadoop: The Definitive Guide, 2nd Edition.
O?Reilly, 2010.
Fei Wu, Daniel S. Weld: Automatically refining the wiki-
pedia infobox ontology. WWW 2008
Wentao Wu, Hongsong Li, Haixun Wang, Kenny Q. Zhu:
Towards a Probabilistic Taxonomy of Many Concepts.
Technical Report MSR-TR-2011-25, Microsoft Re-
search, 2011
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew
McCallum: Structured Relation Discovery using Gen-
erative Models. EMNLP 2011
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, Ji-Rong
Wen: StatSnowball: a statistical approach to extracting
entity relationships. WWW 2009
1145
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1930?1936,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
CTPs: Contextual Temporal Profiles for Time Scoping Facts using
State Change Detection
Derry Tanti Wijaya
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
dwijaya@cs.cmu.edu
Ndapandula Nakashole
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
ndapa@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
tom.mitchell@cs.cmu.edu
Abstract
Temporal scope adds a time dimension to
facts in Knowledge Bases (KBs). These
time scopes specify the time periods when
a given fact was valid in real life. With-
out temporal scope, many facts are under-
specified, reducing the usefulness of the
data for upper level applications such as
Question Answering. Existing methods
for temporal scope inference and extrac-
tion still suffer from low accuracy. In this
paper, we present a new method that lever-
ages temporal profiles augmented with
context? Contextual Temporal Profiles
(CTPs) of entities. Through change pat-
terns in an entity?s CTP, we model the en-
tity?s state change brought about by real
world events that happen to the entity (e.g,
hired, fired, divorced, etc.). This leads to
a new formulation of the temporal scoping
problem as a state change detection prob-
lem. Our experiments show that this for-
mulation of the problem, and the resulting
solution are highly effective for inferring
temporal scope of facts.
1 Introduction
Recent years have seen the emergence of large
Knowledge Bases (KBs) of facts (Carlson 2010;
Auer 2007; Bollacker 2008; Suchanek 2007).
While the wealth of accumulated facts is huge,
most KBs are still sparsely populated in terms of
temporal scope. Time information is an important
dimension in KBs because knowledge is not static,
it changes over time: people get divorced; coun-
tries elect new leaders; and athletes change teams.
This means that facts are not always indefinitely
true. Therefore, temporal scope has crucial impli-
cations for KB accuracy.
Figure 1: Behavior patterns of context uni-grams
for the US presidency state change as seen in the
Google Books N-grams corpus: the rise of ?elect?,
immediately followed by the rise of ?administra-
tion? and ?president?.
Towards bridging the time gap in KBs, we
propose a new method for temporal scope infer-
ence. Our method is based on leveraging aggre-
gate statistics from a time-stamped corpus. First
we generate Contextual Temporal Profiles (CTPs)
of entities from contexts surrounding mentions of
these entities in the corpus. We then detect change
patterns in the CTPs. We then use these changes
to determine when a given entity undergoes a spe-
cific state change caused by real world events. Our
main insight is as follows: events that happen to
an entity change the entity?s state and therefore its
facts. Thus by learning when a given entity under-
goes a specific state change, we can directly infer
the time scopes of its facts. For example, in the di-
vorce event, the person?s state changes from ?mar-
ried? to ?divorced? hence the hasSpouse relation
no longer applies to it, signaling the end time of
its current hasSpouse value. In a country election
event, the country?s state changes and it obtains a
new value for its hasPresident relation.
1930
Our method involves learning context units
(uni-grams and bi-grams surrounding mentions of
an entity) that are relevant to a given state change.
For this we use a few seed examples of entities that
have gone through the state change. For example,
for the US presidency state change denoting the
beginning of a US presidency, given seed exam-
ples such as (Richard Nixon, 1969) and (Jimmy
Carter, 1977), relevant context units include uni-
grams such as ?administration? and ?elect?, which
are common to both CTPs in 1969 and 1977 re-
spectively. Secondly, we learn the mention behav-
ior of these context units for an entity undergoing
a given state change (section 3 has more details).
Figure 1 shows a motivating example, we see the
behavior patterns of context uni-grams for the US
presidency state change: the rise of ?elect? at the
beginning of presidencies, immediately followed
by the rise of ?administration? and ?president? in
the context of the entities, Nixon and Carter.
2 Related work
Prior work mainly falls into two categories: i)
methods that extract temporal scope from text,
at the time of fact extraction; ii) methods that
infer temporal scope from aggregate statistics in
large Web corpora. Early methods mostly fall
under category i); Timely YAGO (Wang 2010),
TIE (Ling 2010), and PRAVDA (Wang 2011) are
three such methods. Timely YAGO applies regu-
lar expressions to Wikipedia infoboxes to extract
time scopes. It is therefore not applicable to any
other corpora but Wikipedia. The TIE (Ling 2010)
system produces a maximal set of events and their
temporal relations based on the text of a given sen-
tence. PRAVDA uses textual patterns along with
a graph-based re-ranking method. Methods falling
under category i) have the downside that it is un-
clear how they can be applied to facts that are al-
ready in the knowledge base. Only one other ap-
proach learned time scopes from aggregate cor-
pus statistics, a recent system called CoTS (Taluk-
dar 2012b). CoTS uses temporal profiles of facts
and how the mentions of such facts rise and fall
over time. However, CoTS is based on frequency
counts of fact mentions and does not take into ac-
count state change inducing context. For exam-
ple, to find the time scope of Nixon presidency,
CoTS uses the rise and fall of the mention ?nixon?
and ?president? over time. To improve accuracy,
CoTS combined this frequency signal with manu-
ally supplied constraints such as the functionality
of the US presidency relation to scope the begin-
ning and end of Nixon presidency. In contrast, the
proposed system does not require constraints as in-
put.
There have also been tools and competitions
developed to facilitate temporal scope extraction.
TARSQI (Verhagen 2005) is a tool for automat-
ically annotating time expressions in text. The
TempEval (Verhagen 2007) challenge has led to
a number of works on temporal relation extrac-
tion (Puscasu 2007; Yoshikawa 2009; Bethard
2007).
3 Method
Given an entity and its Contextual Temporal Pro-
file (CTP), we can learn when such an entity un-
dergoes a specific state change. We can then di-
rectly infer the begin or end time of the fact asso-
ciated with the state change.
The CTP of an entity at a given time point t con-
tains the context within which the entity is men-
tioned at that time. Our method is based on two
related insights: i) the context of the entity at time
t reflects the events happening to the entity and
the state of the entity at time t. ii) the differ-
ence in context before, at time t ? 1, and after, at
time t, reflect the associated state change at time
t. However an entity can undergo a multiplicity of
changes at the same time. Thus both the contexts
and the differences in contexts can contain infor-
mation pertaining to several state changes. We
therefore need a way of determining which part
of the context is relevant to a given state change
sc
i
. To this end, we generate what we refer to as
an aggregate state vector, V s(e, sc
i
) for a hypo-
thetical average entity e undergoing state change
sc
i
. We generate V s(e, sc
i
) from the CTPs of a
seed set of entities at the time they undergo state
change sc
i
.
3.1 Learning State and State Change Vectors
To build CTPs for entities, we use two time-
stamped corpora: the Google Books Ngram cor-
pus (Michel 2011); and the English Gigaword
(Graff 2003) corpus. The Google Books Ngram
corpus contains n-grams for n = 1?5; along with
occurrence statistics from over about 5 million
digitized books. The English Gigaword (Graff
1931
2003) corpus contains newswire text from 1994-
2008. From these corpora, we use the time granu-
larity of a year as it is the finest granularity com-
mon to both corpora.
Definition 1 (Contextual Temporal Profile)
The Contextual Temporal Profile (CTP) of an
entity e at time t, C
e
(t), consists of the context
within which e is mentioned. Specifically C
e
(t)
consists of uni-grams and bi-grams generated
from the 5-grams(Google Books Ngram) or
sentences (Gigaword) that mention e at time t.
Notice that the CTPs can contain context units
(bi-grams or uni-grams) that are simply noise. To
filter the noise, we compute tf-idf statistics for
each contextual unit and only retain the top k rank-
ing units in C
e
(t). In our experiments, we used
k = 100. We compute tf-idf by treating each time
unit t as a document containing words that occur
in the context of e (Wijaya 2011).
Furthermore, CTPs may contain context units
attributed to several state changes. We therefore
tease apart the CTPs to isolate contexts specific
to a given state change. For this, our method
takes as input a small set of seed entities, S(sc
i
),
for each type of state change. Thus for the US
presidency state change that denotes the begin-
ning of a US presidency, we would have seeds as
follows: (Richard Nixon, 1969), (Jimmy Carter,
1977). From the CTPs of the seeds for state
change sc
i
, we generate an aggregate state vector,
V s(e, sc
i
). To obtain the few dozen seeds required
by our method, one can leverage semi-structured
sources such as Wikipedia infoboxes, where rela-
tions e.g., spouse often have time information.
Definition 2 ( Aggregate State Vector for e)
The aggregate state vector of a mean entity
e for state change sc
i
, V s(e, sc
i
), is made
up of the contextual units from the CTPs of
entities in the seed set S(sc
i
) that undergo
state change sc
i
. Thus, we have: V s(e, sc
i
) =
1
|S(sc
i
)|
?
e,t:(e,t)?S(sc
i
)
C
e
(t).
Thus, the state vector V s(e, sc
i
) reflects events
happening to e and the state of e at the time it
undergoes the state change sc
i
. Additionally, we
compute another type of aggregate vector, aggre-
gate change vector 4V s(e, sc
i
) to capture the
change patterns in the context units of e. Recall
that context units rise or fall due to state change,
as seen earlier in Figure 1.
Definition 3 ( Aggregate Change Vector for e)
The aggregate change vector of a mean entity e
for state change sc
i
, 4V s(e, sc
i
), is made up of
the change in the contextual units of the CTPs
of entities in the seed set S(sc
i
) that undergo
state change sc
i
. Thus, we have: 4V s(e, sc
i
) =
1
|S(sc
i
)|
?
e,t:(e,t)?S(sc
i
)
C
e
(t)? C
e
(t? 1).
The aggregate state vector V s(e, sc
i
) and the
aggregate change vector 4V s(e, sc
i
) are then
used to detect state changes.
3.2 Detecting State Changes
To detect state changes in a previously unseen en-
tity e
new
, we generate its state vector, C
e
new
(t),
and its change vector, 4C
e
new
(t) = C
e
new
(t) -
C
e
new
(t ? 1), for every time point t. We consider
every time point t in the CTP of the new entity to
be a candidate for a given state change sc
i
, which
we seek to determine whether e
new
goes through
and at which time point. We then compare the
state vector and change vector of every candidate
time point t to the aggregate state and aggregate
change vector of state change sc
i
. We use cosine
similarity to measure similarities between the state
vector and the aggregate state vector and between
the change vector and the aggregate change vector.
To combine these two vector similarities, we sum
the state vector and change vector similarities. In
future we can explore cross validation and a sepa-
rate development set to define a weighted sum for
combining these two similarities.
The highest ranking candidate time point (most
similar to the aggregate state and aggregate change
vector) is then considered to be the start of state
change sc
i
for the new entity e
new
.
4 Experiments
We carried out experiments to answer the fol-
lowing questions: Is treating temporal scoping
as state change detection in Contextual Temporal
Profiles(CTPs) effective? Do CTPs help improve
temporal scope extraction over context-unaware
temporal profiles?
4.1 Methods under Comparison
We answer these questions by comparing to the
following methods.
1. CoTS a state-of-the-art temporal scoping
system (Talukdar 2012b)
1932
2. MaxEnt a baseline to which CoTS was com-
pared. It is a Maximum Entropy classifier
trained separately for each relation using nor-
malized counts and gradients of facts as fea-
tures. An Integer Linear Program (ILP) is
used to predict which facts are active at which
times. This is done based on the output of
the MAXENT classifier together with tem-
poral intra-relation constraints that regulate
the temporal scoping of one or more fac-
sts from a single relation (e.g., FUNCTIONAL
constraints on US President relation that reg-
ulate that at most one fact from the relation
can be true at any given time i.e., there is only
one US President at any given time).
3. MaxEnt + Intra Relation Constraints
MaxEnt with cross relation constraints
added: constraints that couple facts from
multiple relations e.g., a constraint that Al
Gore?s vice presidency is aligned exactly
with Bill Clinton?s presidency.
We evaluate on the same set of facts as CoTS
and its baselines: facts from the US Administra-
tion domain ( US President, US Vice President,
and US Secretary of State); and facts from the
Academy Awards domain (Best Director and Best
Picture). The number of facts per relation are as
follows: US President, 9; US Vice President, 12;
US Secretary of State, 13; Best Director, 14; and
Best Picture, 14. Our method however is not spe-
cific to these relations from these two domains.
Since our method does not depend on temporal
constraints, the method can work a very different
domain, for example one where many facts can ex-
ist for any time span without being superseded by
another, as long as the entities involved experience
a change of state. Thus, it can be applied to re-
lations like spouse, even though many people are
married in a year as these people change state from
single or engaged to married.
Similar to CoTS, the datasets from which the
CTPs were generated are as follows: The Google
Books Ngram (1960-2008) dataset (Michel 2011)
for the US Administration domain and the En-
glish Gigaword (1994-2008) dataset (Graff 2003)
for Academy Award domain.
Figure 2: Precision @ k using Contextual Tempo-
ral Profiles.
Figure 3: Comparison of F1 scores with CoTS and
other baselines.
4.2 CTPs Begin time precision
To compute precision we used cross validation,
in particular, leave-one-out cross validation due to
the small number of facts per relation.We predict
the begin time of each fact, the time the fact starts
to be valid. True begin times were determined by
a human annotator. This human annotated data
formed the gold-standard which we used to deter-
mine Precision (P), Recall (R), and the F1 mea-
sure. All evaluations were performed at the year
level, the finest granularity common to the two
time-stamped datasets.
For our first experiment, we report the aver-
age precision@k, where k=1 to n, where n=47 is
the number of years between 1960 to 2008 to se-
lect from. As can be seen in Figure 2, precision
quickly reaches 1 for most relations. The true be-
gin time is usually found within top k=5 results.
1933
4.3 Comparison to baselines
For our second experiment, we compared to the F1
scores of CoTS and other baselines in (Talukdar
2012b). As can be seen in Figure 3, our CTPs ap-
proach gives comparable or better F1 (@k=1) than
systems that use only plain temporal profiles, even
when these systems are supplemented with many
carefully crafted, hand-specified constraints.
We note that the performance on the US Secre-
tary of State relation is low in both CoTS (Taluk-
dar 2012b) and in our approach. We found that this
was due to few documents mentioning the ?sec-
retary of state? in Google Books Ngram dataset.
This leads to weak signals for predicting the tem-
poral scope of secretary of state appointments.
We also observe that the uni-grams and bi-
grams in the train CTPs and change vectors reflect
meaningful events and state changes happening to
the entities (Table 1). For example, after ?becom-
ing president? and ?taking office?, US presidents
often see a drop in mentions of their previous (job
title state) such as ?senator?, ?governor? or ?vice
president? as they gain the?president? state.
4.4 Discussion
Overall, our results show that our method is
promising for detecting begin time of facts. In its
current state, our method performs poorly on in-
ferring end times as contexts relevant to a fact of-
ten still mentioned with the entity even after the
fact ceases to be valid. For example, the entity
Al Gore is still mentioned a lot with the bi-gram
?vice president? even after he is no longer a vice
president. Prior work, CoTS, inferred end times
by leveraging manually specified constraints, e.g.,
that there can only be one vice president at a time:
the beginning of one signals the end of another
(Talukdar 2012b). However such methods do not
scale due to the amount of constraints that must be
hand-specified. In future, we would like to inves-
tigate how to better detect the end times of facts.
5 Conclusion
This paper presented a new approach for inferring
temporal scopes of facts. Our approach is to re-
formulate temporal scoping as a state change de-
tection problem. To this end, we introduced Con-
textual Temporal Profiles (CTPs) which are entity
temporal profiles enriched with relevant context.
Relation CTP State
Context
Unigrams and Bigrams
in CTP Change Vectors
US President was
elected,
took office,
became
president
vice president (-), by
president (+), adminis-
tration (+), senator (-),
governor (-), candidate
(-)
Best Picture nominated
for, to
win, won
the, was
nominated
best picture (+), hour
minute (-), academy
award (+), oscar (+),
nominated (+), won (+),
star (-), best actress (+),
best actor (+), best sup-
porting (+)
Table 1: Example behavior of various contex-
tual units (unigrams and bigrams) automatically
learned in the train CTPs and change vector. The
(+) and (-) signs indicate rise and fall in mention
frequency, respectively.
From the CTPs, we learned change vectors that re-
flect change patterns in context units of CTPs. Our
experiments showed that the change patterns are
highly relevant for detecting state change, which
is an effective way of identifying begin times of
facts. For future work, we would like to investi-
gate how our method can be improved to dp better
at detecting fact end times. We also would like to
investigate time-stamped corpora of finer-grained
granularity such as day. This information can be
obtained by subscribing to daily newsfeeds of spe-
cific entities.
Acknowledgments
We thank members of the NELL team at CMU
for their helpful comments. This research was
supported by DARPA under contract number
FA8750-13-2-0005 and in part by Fulbright and
Google Anita Borg Memorial Scholarship.
References
A. Angel, N. Koudas, N. Sarkas, D. Srivastava:
Dense Subgraph Maintenance under Streaming
Edge Weight Updates for Real-time Story Identi-
fication. In Proceedings of the VLDB Endowment,
PVLDB 5(10):574?585, 2012.
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722?735,
Busan, Korea, 2007.
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
1934
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670?2676, Hyderabad, India, 2007.
S. Bethard and J.H. Martin. Cu-tmp: Temporal relation
classification using syntactic and semantic features.
In In SemEval-2007, 2007.
K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101?110, New
York, NY, USA, 2010.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R.
Hruschka Jr., T. M. Mitchell: Toward an Architec-
ture for Never-Ending Language Learning. In Pro-
ceedings of the Twenty-Fourth AAAI Conference on
Artificial Intelligence (AAAI) 2010.
L. Del Corro, R. Gemulla: ClausIE: clause-based
open information extraction. In Proceedings of the
22nd International Conference on World Wide Web
(WWW), pages 355-366. 2013.
A. Das Sarma, A. Jain, C. Yu: Dynamic Relationship
and Event Discovery. In Proceedings of the Forth
International Conference on Web Search and Web
Data Mining (WSDM), pages 207?216, Hong Kong,
China, 2011.
A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535?1545, Edinburgh, UK, 2011.
D. Graff, J. Kong, K. Chen, and K. Maeda. English gi-
gaword. Linguistic Data Consortium, Philadelphia,
2003.
C. Havasi, R. Speer, J. Alonso. ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.
J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229?232, Hyderabad, In-
dia. 2011.
X. Ling and D.S. Weld. Temporal information extrac-
tion. In Proceedings of AAAI, 2010.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The Google
Books Team, Joseph P. Pickett, Dale Holberg, Dan
Clancy, Peter Norvig, Jon Orwant, Steven Pinker,
Martin A. Nowak, Erez Lieberman Aiden: Quantita-
tive Analysis of Culture Using Millions of Digitized
Books. Science, 331(6014):176182.
N. Nakashole, M. Theobald, G. Weikum: Scalable
Knowledge Harvesting with High Precision and
High Recall. In Proceedings of the 4th International
Conference on Web Search and Web Data Mining
(WSDM), pages 227?326, Hong Kong, China, 2011.
N. Nakashole, T. Tylenda, G. Weikum: Fine-grained
Semantic Typing of Emerging Entities. In Proceed-
ings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp. 1488-
1497, 2013.
N.Nakahsole, T. M. Mitchell: Language-Aware Truth
Assessment of Fact Candidates In Proceedings of
the 52nd Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 1009-1019,
2014.
G. Puscasu. Wvali: Temporal relation identification by
syntactico-semantic analysis. In Proceedings of the
4th International Workshop on SemEval, 2007.
J. Pustejovsky, J. Castano, R. Ingria, R. Sauri, R
Gaizauskas, A. Setzer, G. Katz, and D. Radev.
Timeml: Robust specification of event and temporal
expressions in text. In Fifth International Workshop
on Computational Semantics, 2003.
P. P. Talukdar, D. T. Wijaya, Tom M. Mitchell: Acquir-
ing temporal constraints between relations. In Pro-
ceeding of the 21st ACM International Conference
on Information and Knowledge Management, pages
992-1001, CIKM 2012.
P. P. Talukdar, D. T. Wijaya, T. Mitchell: Coupled
temporal scoping of relational facts. In Proceedings
of the fifth ACM international conference on Web
search and data mining. ACM, 2012.
M. Verhagen, I. Mani, R. Sauri, R. Knippen, S.B. Jang,
J. Littman, A. Rumshisky, J. Phillips, and J. Puste-
jovsky. Automating temporal annotation with tarsqi.
In Proceedings of the ACL Session on Interactive
poster and demonstration sessions, 2005.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. Semeval-2007 task 15:
Tempeval temporal relation identi
cation. In Proceedings of the 4th International Work-
shop on Semantic Evaluations, 2007.
D. T. Wijaya, and R. Yeniterzi: Understanding seman-
tic change of words over centuries. In Proceedings
of the 2011 international workshop on DETecting
and Exploiting Cultural diversiTy on the social web.
ACM, 2011.
1935
F. M. Suchanek, G. Kasneci, G. Weikum: Yago: a
Core of Semantic Knowledge. In Proceedings of the
16th International Conference on World Wide Web
(WWW) pages, 697-706, Banff, Alberta, Canada,
2007.
Y. Wang, M. Zhu, L. Qu, M. Spaniol, and G. Weikum:
Timely yago: harvesting, querying, and visualizing
temporal knowledge from wikipedia. In Proceedings
of the 13th International Conference on Extending-
Database Technology, 2010.
W. Wu, H. Li, H. Wang, K. Zhu: Probase: A
Probabilistic Taxonomy for Text Understanding. In
Proceedings of the International Conference on
Management of Data (SIGMOD), pages 481?492,
Scottsdale, AZ, USA, 2012.
Y. Wang, B. Yang, L. Qu, M. Spaniol, and G. Weikum:
Harvesting facts from textual web sources by con-
strained label propagation. In Proceedings of CIKM,
2011.
K. Yoshikawa, S. Riedel, M. Asahara, and Y. Mat-
sumoto. Jointly identifying temporal relations with
markov logic. In Proceedings of ACL, 2009.
1936
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1488?1497,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fine-grained Semantic Typing of Emerging Entities
Ndapandula Nakashole, Tomasz Tylenda, Gerhard Weikum
Max Planck Institute for Informatics
Saarbru?cken, Germany
{nnakasho,ttylenda,weikum}@mpi-inf.mpg.de
Abstract
Methods for information extraction (IE)
and knowledge base (KB) construction
have been intensively studied. However, a
largely under-explored case is tapping into
highly dynamic sources like news streams
and social media, where new entities are
continuously emerging. In this paper, we
present a method for discovering and se-
mantically typing newly emerging out-of-
KB entities, thus improving the freshness
and recall of ontology-based IE and im-
proving the precision and semantic rigor
of open IE. Our method is based on a prob-
abilistic model that feeds weights into in-
teger linear programs that leverage type
signatures of relational phrases and type
correlation or disjointness constraints. Our
experimental evaluation, based on crowd-
sourced user studies, show our method
performing significantly better than prior
work.
1 Introduction
A large number of knowledge base (KB) con-
struction projects have recently emerged. Promi-
nent examples include Freebase (Bollacker 2008)
which powers the Google Knowledge Graph, Con-
ceptNet (Havasi 2007), YAGO (Suchanek 2007),
and others. These KBs contain many millions of
entities, organized in hundreds to hundred thou-
sands of semantic classes, and hundred millions
of relational facts between entities. However, de-
spite these impressive advances, there are still ma-
jor limitations regarding coverage and freshness.
Most KB projects focus on entities that appear in
Wikipedia (or other reference collections such as
IMDB), and very few have tried to gather entities
?in the long tail? beyond prominent sources. Vir-
tually all projects miss out on newly emerging en-
tities that appear only in the latest news or social
media. For example, the Greenlandic singer Nive
Nielsen has gained attention only recently and is
not included in any KB (a former Wikipedia article
was removed because it ?does not indicate the im-
portance or significance of the subject?), and the
resignation of BBC director Entwistle is a recently
new entity (of type event).
Goal. Our goal in this paper is to discover emerg-
ing entities of this kind on the fly as they become
noteworthy in news and social-media streams. A
similar theme is pursued in research on open infor-
mation extraction (open IE) (Banko 2007; Fader
2011; Talukdar 2010; Venetis 2011; Wu 2012),
which yields higher recall compared to ontology-
style KB construction with canonicalized and se-
mantically typed entities organized in prespecified
classes. However, state-of-the-art open IE meth-
ods extract all noun phrases that are likely to de-
note entities. These phrases are not canonical-
ized, so the same entity may appear under many
different names, e.g., ?Mr. Entwistle?, ?George
Entwistle?, ?the BBC director?, ?BBC head En-
twistle?, and so on. This is a problem because
names and titles are ambiguous, and this hampers
precise search and concise results.
Our aim is for all recognized and newly dis-
covered entities to be semantically interpretable
by having fine-grained types that connect them
to KB classes. The expectation is that this will
boost the disambiguation of known entity names
and the grouping of new entities, and will also
strengthen the extraction of relational facts about
entities. For informative knowledge, new entities
must be typed in a fine-grained manner (e.g., gui-
tar player, blues band, concert, as opposed to crude
types like person, organization, event).
Strictly speaking, the new entities that we cap-
1488
ture are typed noun phrases. We do not attempt
any cross-document co-reference resolution, as
this would hardly work with the long-tail na-
ture and sparse observations of emerging entities.
Therefore, our setting resembles the established
task of fine-grained typing for noun phrases (Fleis-
chmann 2002), with the difference being that we
disregard common nouns and phrases for promi-
nent in-KB entities and instead exclusively focus
on the difficult case of phrases that likely denote
new entities. The baselines to which we compare
our method are state-of-the-art methods for noun-
phrase typing (Lin 2012; Yosef 2012).
Contribution. The solution presented in this
paper, called PEARL, leverages a repository of
relational patterns that are organized in a type-
signature taxonomy. More specifically, we har-
ness the PATTY collection consisting of more
than 300,000 typed paraphrases (Nakashole 2012).
An example of PATTY?s expressive phrases is:
?musician? * cover * ?song? for a musician per-
forming someone else?s song. When extract-
ing noun phrases, PEARL also collects the co-
occurring PATTY phrases. The type signatures of
the relational phrases are cues for the type of the
entity denoted by the noun phrase. For example,
an entity named Snoop Dogg that frequently co-
occurs with the ?singer? * distinctive voice in *
?song? pattern is likely to be a singer. Moreover,
if one entity in a relational triple is in the KB and
can be properly disambiguated (e.g., a singer), we
can use a partially bound pattern to infer the type
of the other entity (e.g., a song) with higher confi-
dence.
In this line of reasoning, we also leverage the
common situation that many input sentences con-
tain one entity registered in the KB and one novel
or unknown entity. Known entities are recognized
and mapped to the KB using a recent tool for
named entity disambiguation (Hoffart 2011). For
cleaning out false hypotheses among the type can-
didates for a new entity, we devised probabilistic
models and an integer linear program that consid-
ers incompatibilities and correlations among entity
types.
In summary, our contribution in this paper is
a model for discovering and ontologically typ-
ing out-of-KB entities, using a fine-grained type
system and harnessing relational paraphrases with
type signatures for probabilistic weight computa-
tion. Crowdsourced quality assessments demon-
strate the accuracy of our model.
2 Detection of New Entities
To detect noun phrases that potentially refer to en-
tities, we apply a part-of-speech tagger to the in-
put text. For a given noun phrase, there are four
possibilities: a) The noun phrase refers to a gen-
eral concept (a class or abstract concept), not an
individual entity. b) The noun phrase is a known
entity that can be directly mapped to the knowl-
edge base. c) The noun phrase is a new name for
a known entity. d) The noun phrase is a new entity
not known to the knowledge base at all. In this pa-
per, our focus is on case d); all other cases are out
of the scope of this paper.
We use an extensive dictionary of surface forms
for in-KB entities (Hoffart 2012), to determine if
a name or phrase refers to a known entity. If a
phrase does not have any match in the dictionary,
we assume that it refers to a new entity. To decide
if a noun phrase is a true entity (i.e., an individ-
ual entity that is a member of one or more lexi-
cal classes) or a non-entity (i.e., a common noun
phrase that denotes a class or a general concept),
we base the decision on the following hypothesis
(inspired by and generalizing (Bunescu 2006): A
given noun phrase, not known to the knowledge
base, is a true entity if its headword is singular
and is consistently capitalized (i.e., always spelled
with the first letter in upper case).
3 Typing Emerging Entities
To deduce types for new entities we propose to
align new entities along the type signatures of pat-
terns they occur with. In this manner we use the
patterns to suggest types for the entities they occur
with. In particular, we infer entity types from pat-
tern type signatures. Our approach builds on the
following hypothesis:
Hypothesis 3.1 (Type Alignment Hypothesis)
For a given pattern such as ?actor??s character
in ?movie?, we assume that an entity pair (x, y)
frequently occurring with the pattern in text
implies that x and y are of the types ?actor? and
?movie?, respectively.
Challenges and Objective. While the type align-
ment hypothesis works as a starting point, it in-
troduces false positives. Such false positives stem
1489
from the challenges of polysemy, fuzzy pattern
matches, and incorrect paths between entities.
With polysemy, the same lexico-syntactic pattern
can have different type signatures. For example,
the following are three different patterns: ?singer?
released ?album?, ?music band? released ?album?,
?company? released ?product?. For an entity pair
(x, y) occurring with the pattern ?released?, x can
be one of three different types.
We cannot expect that the phrases we extract in
text will be exact matches of the typed relational
patterns learned by PATTY. Therefore, for better
recall, we must accept fuzzy matches. Quite often
however, the extracted phrase matches multiple re-
lational patterns to various degrees. Each of the
matched relational patterns has its own type sig-
nature. The type signatures of the various matched
patterns can be incompatible with one another.
The problem of incorrect paths between entities
emerges when a pair of entities occurring in the
same sentence do not stand in a true subject-object
relation. Dependency parsing does not adequately
solve the issue. Web sources contain a plethora
of sentences that are not well-formed. Such sen-
tences mislead the dependency parser to extract
wrong dependencies.
Our solution takes into account polysemy, fuzzy
matches, as well as issues stemming from poten-
tial incorrect-path limitations. We define and solve
the following optimization problem:
Definition 1 (Type Inference Optimization)
Given all the candidate types for x, find the
best types or ?strongly supported? types for x.
The final solution must satisfy type disjointness
constraints. Type disjointness constraints are
constraints that indicate that, semantically, a pair
of types cannot apply to the same entity at the
same time. For example, a ?university? cannot be
a ?person?.
We also study a relaxation of type disjointness
constraints through the use of type correlation con-
straints. Our task is therefore twofold: first, gen-
erate candidate types for new entities; second, find
the best types for each new entity among its can-
didate types.
4 Candidate Types for Entities
For a given entity, candidate types are types that
can potentially be assigned to that entity, based on
the entity?s co-occurrences with typed relational
patterns.
Definition 2 (Candidate Type) Given a new en-
tity x which occurs with a number of patterns
p1, p2, ..., pn, where each pattern pi has a type sig-
nature with a domain and a range: if x occurs on
the left of pi, we pick the domain of pi as a candi-
date type for x; if x occurs on the right of pi, we
pick the range of pi as a candidate type for x.
For each candidate type, we compute confi-
dence weights. Ideally, if an entity occurs with
a pattern which is highly specific to a given type
then the candidate type should have high con-
fidence. For example ?is married to? is more
specific to people then ?expelled from?. A per-
son can be expelled from an organization but a
country can also be expelled from an organization
such as NATO. There are various ways to com-
pute weights for candidate types. We first intro-
duce a uniform weight approach and then present a
method for computing more informative weights.
4.1 Uniform Weights
We are given a new entity x which occurs with
phrases (x phrase1 y1), (x phrase2 y2), ..., (x
phrasen yn). Suppose these occurrences lead
to the facts (x, p1, y1), (x, p2, y2),..., (x, pn, yn).
The pis are the typed relational patterns extracted
by PATTY. The facts are generated by matching
phrases to relational patterns with type signa-
tures. The type signature of a pattern is denoted
by:
sig(pi) = (domain(pi), range(pi))
We allow fuzzy matches, hence each fact comes
with a match score. This is the similarity degree
between the phrase observed in text and the typed
relational pattern.
Definition 3 (Fuzzy Match Score) Suppose we
observe the surface string: (x phrase y) which
leads to the fact: x, pi, y. The fuzzy match similar-
ity score is: sim(phrase, pi), where similarity is
the n-gram Jaccard similarity between the phrase
and the typed pattern.
The confidence that x is of type domain is de-
fined as follows:
Definition 4 (Candidate Type Confidence)
For a given observation (x phrase y), where
1490
phrase matches patterns p1, ..., pn, with domains
d1, ..., db which are possibly the same:
typeConf(x, phrase, d) =
?
{pi:domain(pi)=d}
(
sim(phrase, pi)
)
Observe that this sums up over all patterns that
match the phrase.
To compute the final confidence for
typeConf(x, domain), we aggregate the
confidences over all phrases occurring with x.
Definition 5 (Aggregate Confidence) For
a set of observations (x, phrase1, y1),
(x, phrase2, y2), ..., (x, phrasen, yn), the
aggregate candidate type confidence is given by:
aggTypeConf(x, d) =
?
phrasei
typeConf(x, phrasei, d)
=
?
phrasei
?
{pj :domain(pj)=d}
(sim(phrasei, pj))
The confidence for the range
typeConf(x, range) is computed analogously.
All confidence weights are normalized to values
in [0, 1].
The limitation of the uniform weight approach
is that each pattern is considered equally good for
suggesting candidate types. Thus this approach
does not take into account the intuition that an en-
tity occurring with a pattern which is highly spe-
cific to a given type is a stronger signal that the
entity is of the type suggested. Our next approach
addresses this limitation.
4.2 Co-occurrence Likelihood Weight
Computation
We devise a likelihood model for computing
weights for entity candidate types. Central to this
model is the estimation of the likelihood of a given
type occurring with a given pattern.
Suppose using PATTY methods we mined a
typed relational pattern ?t1? p ?t2?. Suppose that
we now encounter a new entity pair (x, y) occur-
ring with a phrase that matches p. We can com-
pute the likelihood of x and y being of types t1
and t2, respectively, from the likelihood of p co-
occurring with entities of types t1, t2. Therefore
we are interested in the type-pattern likelihood,
defined as follows:
Definition 6 (Type-Pattern Likelihood) The
likelihood of p co-occurring with an entity pair
(x, y) of the types (t1, t2) is given by:
P [t1, t2|p] (1)
where t1 and t2 are the types of the arguments ob-
served with p from a corpus such as Wikipedia.
P [t1, t2|p] is expanded as follows:
P [t1, t2|p] =
P [t1, t2, p]
P [p] . (2)
The expressions on the right-hand side of Equa-
tion 2 can be directly estimated from a corpus.
We use Wikipedia (English), for corpus-based es-
timations. P [t1, t2, p] is the relative occurrence
frequency of the typed pattern among all entity-
pattern-entity triples in a corpus (e.g., the frac-
tion of ?musican? plays ?song? among all triples).
P[p] is the relative occurrence frequency of the un-
typed pattern (e.g., plays) regardless of the argu-
ment types. For example, this sums up over both
?musican? plays ?song? occurrences and ?actor?
plays ?fictional character?. If we observe a fact
where one argument name can be easily disam-
biguated to a knowledge-base entity so that its type
is known, and the other argument is considered to
be an out-of-knowledge-base entity, we condition
the joint probability of t1, p, and t2 in a different
way:
Definition 7 (Conditional Type-PatternLikelihood)
The likelihood of an entity of type t1 occurring
with a pattern p and an entity of type t2 is given
by:
P [t1|t2, p] =
P [t1, t2, p]
P [p, t2]
(3)
where the P [p, t2] is the relative occurrence fre-
quency of a partial triple, for example, ?*? plays
?song?.
Observe that all numbers refer to occurrence
frequencies. For example, P [t1, p, t2] is a frac-
tion of the total number of triples in a corpus.
Multiple patterns can suggest the same type for
an entity. Therefore, the weight of the assertion
that y is of type t, is the total support strength from
all phrases that suggest type t for y.
Definition 8 (Aggregate Likelihood) The aggre-
gate likelihood candidate type confidence is given
1491
by:
typeConf(x, domain)) =
?
phrasei
?
pj
(
sim(phrasei, pj) ??
)
Where ? = P [t1, t2|p] or P [t1|t2, p] or P [t2|t1, p]
The confidence weights are normalized to values
in [0, 1]. So far we have presented a way of gener-
ating a number of weighted candidate types for x.
In the next step we pick the best types for an entity
among all its candidate types.
4.3 Integer Linear Program Formulation
Given a set of weighted candidate types, our goal
is to pick a compatible subset of types for x. The
additional asset that we leverage here is the com-
patibility of types: how likely is it that an entity
belongs to both type ti and type tj . Some types
are mutually exclusive, for example, the type loca-
tion rules out person and, at finer levels, city rules
out river and building, and so on. Our approach
harnesses these kinds of constraints. Our solution
is formalized as an Integer Linear Program (ILP).
We have candidate types for x: t1, .., tn. First, we
define a decision variable Ti for each candidate
type i = 1, . . . , n. These are binary variables:
Ti = 1 means type ti is selected to be included
in the set of types for x, Ti = 0 means we discard
type ti for x.
In the following we develop two variants of this
approach: a ?hard? ILP with rigorous disjointness
constraints, and a ?soft? ILP which considers type
correlations.
?Hard? ILP with Type Disjointness Con-
straints. We infer type disjointness constraints
from the YAGO2 knowledge base using occur-
rence statistics. Types with no overlap in entities
or insignificant overlap below a specified thresh-
old are considered disjoint. Notice that this intro-
duces hard constraints whereby selecting one type
of a disjoint pair rules out the second type. We de-
fine type disjointness constraints Ti + Tj ? 1 for
all disjoint pairs ti, tj (e.g. person-artifact, movie-
book, city-country, etc.). The ILP is defined as
follows:
objective
max?i Ti ? wi
type disjointness constraint
?(ti, tj)disjoint Ti + Tj ? 1
The weights wi are the aggregrated likelihoods
as specified in Definition 8.
?Soft? ILP with Type Correlations. In many
cases, two types are not really mutually exclusive
in the strict sense, but the likelihood that an en-
tity belongs to both types is very low. For exam-
ple, few drummers are also singers. Conversely,
certain type combinations are boosted if they are
strongly correlated. An example is guitar players
and electric guitar players. Our second ILP con-
siders such soft constraints. To this end, we pre-
compute Pearson correlation coefficients for all
type pairs (ti, tj) based on co-occurrences of types
for the same entities. These values vij ? [?1, 1]
are used as weights in the objective function of
the ILP. We additionally introduce pair-wise deci-
sion variables Yij , set to 1 if the entity at hand be-
longs to both types ti and tj , and 0 otherwise. This
coupling between the Yij variables and the Ti, Tj
variables is enforced by specific constraints. For
the objective function, we choose a linear combi-
nation of per-type evidence, using weights wi as
before, and the type-compatibility measure, using
weights vij . The ILP with correlations is defined
as follows:
objective
max ??i Ti ? wi + (1? ?)
?
ij Yij ? vij
type correlation constraints
?i,j Yij + 1 ? Ti + Tj
?i,j Yij ? Ti
?i,j Yij ? Tj
Note that both ILP variants need to be solved
per entity, not over all entities together. The ?soft?
ILP has a size quadratic in the number of candidate
types, but this is still a tractable input for modern
solvers. We use the Gurobi software package to
compute the solutions for the ILP?s. With this de-
sign, PEARL can efficiently handle a typical news
article in less than a second, and is well geared for
keeping up with high-rate content streams in real
time. For both the ?hard? and ?soft? variants of
the ILP, the solution is the best types for entity x
satisfying the constraints.
1492
5 Evaluation
To define a suitable corpus of test data, we ob-
tained a stream of news documents by subscrib-
ing to Google News RSS feeds for a few topics
over a six-month period (April 2012 ? Septem-
ber 2012). This produced 318, 434 documents.
The topics we subscribed to are: Angela Merkel,
Barack Obama, Business, Entertainment, Hillary
Clinton, Joe Biden, Mitt Romney, Newt Gingrich,
Rick Santorum, SciTech and Top News. All our ex-
periments were carried out on this data. The type
system used is that of YAGO2, which is derived
from WordNet. Human evaluations were carried
out on Amazon Mechanical Turk (MTurk), which
is a platform for crowd-sourcing tasks that require
human input. Tasks on MTurk are small question-
naires consisting of a description and a set of ques-
tions.
Baselines. We compared PEARL against two
state-of-the-art baselines: i). NNPLB (No Noun
Phrase Left Behind), is the method presented in
(Lin 2012), based on the propagation of types
for known entities through salient patterns occur-
ring with both known and unknown entities. We
implemented the algorithm in (Lin 2012) in our
framework, using the relational patterns of PATTY
(Nakashole 2012) for comparability. For assess-
ment we sampled from the top-5 highest ranked
types for each entity. In our experiments, our im-
plementation of NNPLB achieved precision values
comparable to those reported in (Lin 2012). ii).
HYENA (Hierarchical tYpe classification for En-
tity NAmes), the method of (Yosef 2012), based
on a feature-rich classifier for fine-grained, hierar-
chical type tagging. This is a state-of-the-art rep-
resentative of similar methods such as (Rahman
2010; Ling 2012).
Evaluation Task. To evaluate the quality of types
assigned to emerging entities, we presented turk-
ers with sentences from the news tagged with out-
of-KB entities and the types inferred by the meth-
ods under test. The turkers task was to assess the
correctness of types assigned to an entity mention.
To make it easy to understand the task for the turk-
ers, we combined the extracted entity and type into
a sentence. For example if PEARL inferred that
Brussels Summit is an political event, we generate
and present the sentence: Brussels Summit is an
event. We allowed four possible assessment val-
ues: a) Very good output corresponds to a perfect
result. b) Good output exhibits minor errors. For
instance, the description G20 Summit is an orga-
nization is wrong, because the summit is an event,
but G20 is indeed an organization. The problem in
this example is incorrect segmentation of a named
entity. c) Wrong for incorrect types (e.g., Brussels
Summit is a politician). d) Not sure / do not know
for other cases.
Comparing PEARL to Baselines. Per method,
turkers evaluated 105 entity-type pair test sam-
ples. We first sampled among out-of-KB entities
that were mentioned frequently in the news cor-
pus: in at least 20 different news articles. Each
test sample was given to 3 different turkers for as-
sessment. Since the turkers did not always agree
if the type for a sample is good or not, we ag-
gregate their answers. We use voting to decide
whether the type was assigned correctly to an en-
tity. We consider the following voting variants:
i) majority ?very good? or ?good?, a conservative
notion of precision: precisionlower. ii) at least
one ?very good? or ?good?, a liberal notion of
precision: precisionupper. Table 1 shows preci-
sion for PEARL-hard, PEARL-soft, NNPLB, and
HYENA, with a 0.9-confidence Wilson score in-
terval (Brown 2001). PEARL-hard outperformed
PEARL-soft and also both baselines. HYENA?s
relatively poor performance can be attributed to
the fact that its features are mainly syntactic such
as bi-grams and part-of-speech tags. Web data is
challenging, it has a lot of variations in syntac-
tic formulations. This introduces a fair amount
of ambiguity which can easily mislead syntactic
features. Leveraging semantic features as done
by PEARL could improve HYENA?s performance.
While the NNPLB method performs better than
HYENA, in comparison to PEARL-hard, there is
room for improvement. Like HYENA, NNPLB
assigns negatively correlated types to the same en-
tity. This limitation could be addressed by apply-
ing PEARL?s ILPs and probabilistic weights to the
candidate types suggested by NNPLB.
To compute inter-judge agreement we calcu-
lated Fleiss? kappa and Cohen?s kappa ?, which
are standard measures. The usual assumption for
Fleiss?? is that labels are categorical, so that each
disagreement counts the same. This is not the case
in our settings, where different labels may indicate
partial agreement (?good?, ?very good?). There-
1493
Precisionlower Precisionupper
PEARL-hard 0.77?0.08 0.88?0.06
PEARL-soft 0.53?0.09 0.77?0.09
HYENA 0.26?0.08 0.56?0.09
NNPLB 0.46?0.09 0.68?0.09
Table 1: Comparison of PEARL to baselines.
? F leiss Cohen
0.34 0.45
Table 2: Lower bound estimations for inter-judge
agreement kappa: Fleiss? ? & adapted Cohen?s ?.
fore the ? values in Table 2 are lower-bound esti-
mates of agreement in our experiments; the ?true
agreement? seems higher. Nevertheless, the ob-
served Fleiss ? values show that the task was fairly
clear to the turkers; values > 0.2 are generally
considered as acceptable (Landis 1977). Cohen?s
? is also not directly applicable to our setting. We
approximated it by finding pairs of judges who as-
sessed a significant number of the same entity-type
pairs.
Precisionlower Precisionupper
Freq. mentions 0.77?0.08 0.88?0.06
All mentions 0.65?0.09 0.77?0.08
Table 3: PEARL-hard performance on a sample of
frequent entities (mention frequency? 20) and on
a sample of entities of all mention frequencies.
Mention Frequencies. We also studied PEARL-
hard?s performance on entities of different men-
tion frequencies. The results are shown in Ta-
ble 3. Frequently mentioned entities provide
PEARL with more evidence as they potentially oc-
cur with more patterns. Therefore, as expected,
precision when sampling over all entities drops
a bit. For such infrequent entities, PEARL does
not have enough evidence for reliable type assign-
ments.
Variations of PEARL. To quantify how various
aspects of our approach affect performance, we
studied a few variations. The first method is the
full PEARL-hard. The second method is PEARL
with no ILP (denoted No ILP), only using the
probabilistic model. The third variation is PEARL
without probabilistic weights (denoted Uniform
Figure 1: Variations of the PEARL method.
Weights). From Figure 1, it is clear that both the
ILP and the weighting model contribute signifi-
cantly to PEARL?s ability to make precise type as-
signments. Sample results from PEARL-hard are
shown in Table 4.
NDCG. For a given entity mention e, an entity-
typing system returns a ranked list of types
{t1, t2, ..., tn}. We evaluated ranking quality us-
ing the top-5 ranks for each method. These assess-
ments were aggregated into the normalized dis-
counted cumulative gain (NDCG), a widely used
measure for ranking quality. The NDCG values
obtained are 0.53, 0.16, and 0.16, for PEARL-
hard, HYENA, and NNPLB, respectively. PEARL
clearly outperforms the baselines on ranking qual-
ity, too.
6 Related Work
Tagging mentions of named entities with lexical
types has been pursued in previous work. Most
well-known is the Stanford named entity recog-
nition (NER) tagger (Finkel 2005) which assigns
coarse-grained types like person, organization, lo-
cation, and other to noun phrases that are likely to
denote entities. There is fairly little work on fine-
grained typing, notable results being (Fleischmann
2002; Rahman 2010; Ling 2012; Yosef 2012).
These methods consider type taxonomies similar
to the one used for PEARL, consisting of several
hundreds of fine-grained types. All methods use
trained classifiers over a variety of linguistic fea-
tures, most importantly, words and bigrams with
part-of-speech tags in a mention and in the textual
context preceding and following the mention. In
addition, the method of (Yosef 2012) (HYENA)
utilizes a big gazetteer of per-type words that oc-
cur in Wikipedia anchor texts. This method out-
performs earlier techniques on a variety of test
1494
Entity Inferred Type Sample Source Sentence (s)
Lochte medalist Lochte won America?s lone gold ...
Malick director ... the red carpet in Cannes for Malick?s 2011 movie ...
Bonamassa musician Bonamassa recorded Driving Towards the Daylight in Las Vegas ...
... Bonamassa opened for B.B. King in Rochester , N.Y.
Analog Man album Analog Man is Joe Walsh?s first solo album in 20 years.
Melinda Liu journalist ... in a telephone interview with journalist Melinda Liu of the Daily Beast.
RealtyTrac publication Earlier this month, RealtyTrac reported that ...
Table 4: Sample types inferred by PEARL.
cases; hence it served as one of our baselines.
Closely related to our work is the recent ap-
proach of (Lin 2012) (NNPLB) for predicting
types for out-of-KB entities. Noun phrases in the
subject role in a large collection of fact triples
are heuristically linked to Freebase entities. This
yields type information for the linked mentions.
For unlinkable entities the NNPLB method (in-
spired by (Kozareva 2011)) picks types based on
co-occurrence with salient relational patterns by
propagating types of linked entities to unlinkable
entities that occur with the same patterns. Unlike
PEARL, NNPLB does not attempt to resolve in-
consistencies among the predicted types. In con-
trast, PEARL uses an ILP with type disjointness
and correlation constraints to solve and penalize
such inconsistencies. NNPLB uses untyped pat-
terns, whereas PEARL harnesses patterns with
type signatures. Furthermore, PEARL computes
weights for candidate types based on patterns and
type signatures. Weight computations in NNPLB
are only based on patterns. NNPLB only assigns
types to entities that appear in the subject role of
a pattern. This means that entities in the object
role are not typed at all. In contrast, PEARL in-
fers types for entities in both the subject and object
role.
Type disjointness constraints have been studied
for other tasks in information extraction (Carlson
2010; Suchanek 2009), but using different formu-
lations.
7 Conclusion
This paper addressed the problem of detecting and
semantically typing newly emerging entities, to
support the life-cycle of large knowledge bases.
Our solution, PEARL, draws on a collection of
semantically typed patterns for binary relations.
PEARL feeds probabilistic evidence derived from
occurrences of such patterns into two kinds of
ILPs, considering type disjointness or type corre-
lations. This leads to highly accurate type predic-
tions, significantly better than previous methods,
as our crowdsourcing-based evaluation showed.
References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722?735,
Busan, Korea, 2007.
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670?2676, Hyderabad, India, 2007.
K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.
Lawrence D. Brown, T.Tony Cai, Anirban Dasgupta:
Interval Estimation for a Binomial Proportion. Sta-
tistical Science 16: pages 101?133, 2001.
R. C. Bunescu, M. Pasca: Using Encyclopedic Knowl-
edge for Named entity Disambiguation. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), Trento, Italy, 2006.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101?110, New
York, NY, USA, 2010.
S. Cucerzan: Large-Scale Named Entity Disambigua-
tion Based on Wikipedia Data. In Proceedings of
the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
1495
CoNLL), pages 708?716, Prague, Czech Republic,
2007.
A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535?1545, Edinburgh, UK, 2011.
J.R. Finkel, T. Grenager, C. Manning. 2005. Incorpo-
rating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 363?370,
Ann Arbor, Michigan, 2005.
Michael Fleischman, Eduard H. Hovy: Fine Grained
Classification of Named Entities. In Proceedings
the International Conference on Computational Lin-
guistics, COLING 2002.
X. Han, J. Zhao: Named Entity Disambiguation by
Leveraging Wikipedia Semantic Knowledge. In Pro-
ceedings of 18th ACM Conference on Information
and Knowledge Management (CIKM), pages 215 ?
224,Hong Kong, China, 2009.
C. Havasi, R. Speer, J. Alonso. ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.
Sebastian Hellmann, Claus Stadler, Jens Lehmann,
Sren Auer: DBpedia Live Extraction. OTM Confer-
ences (2) 2009: 1209-1223.
J. Hoffart, M. A. Yosef, I.Bordino and H. Fuerstenau,
M. Pinkal, M. Spaniol, B.Taneva, S.Thater, Gerhard
Weikum: Robust Disambiguation of Named Entities
in Text. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 782?792, Edinburgh, UK, 2011.
J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229?232, Hyderabad, In-
dia. 2011.
J. Hoffart, F. Suchanek, K. Berberich, G. Weikum:
YAGO2: A Spatially and Temporally Enhanced
Knowledge Base from Wikipedia. Artificial Intelli-
gence 2012.
Z. Kozareva, L. Voevodski, S.-H.Teng: Class Label
Enhancement via Related Instances. EMNLP 2011:
118-128
J. R. Landis, G. G. Koch: The measurement of observer
agreement for categorical data in Biometrics. Vol.
33, pp. 159174, 1977.
C. Lee, Y-G. Hwang, M.-G. Jang: Fine-grained
Named Entity Recognition and Relation Extraction
for Question Answering. In Proceedings of the 30th
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR), pages 799?800, Amsterdam, The Nether-
lands, 2007.
T. Lin, Mausam , O. Etzioni: No Noun Phrase Left
Behind: Detecting and Typing Unlinkable Entities.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 893?903, Jeju, South Ko-
rea, 2012.
Xiao Ling, Daniel S. Weld: Fine-Grained Entity
Recognition. In Proceedings of the Conference on
Artificial Intelligence (AAAI), 2012
D. N. Milne, I. H. Witten: Learning to Link with Wi-
kipedia. In Proceedings of 17th ACM Conference on
Information and Knowledge Management (CIKM),
pages 509-518, Napa Valley, California, USA, 2008.
N. Nakashole, G. Weikum, F. Suchanek: PATTY:
A Taxonomy of Relational Patterns with Seman-
tic Types. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1135 -
1145, Jeju, South Korea, 2012.
V. Nastase, M. Strube, B. Boerschinger, Ca?cilia Zirn,
Anas Elghafari: WikiNet: A Very Large Scale
Multi-Lingual Concept Network. In Proceedings of
the 7th International Conference on Language Re-
sources and Evaluation(LREC), Malta, 2010.
H. T. Nguyen, T. H. Cao: Named Entity Disambigua-
tion on an Ontology Enriched by Wikipedia. In Pro-
ceedings of the IEEE International Conference on
Research, Innovation and Vision for the Future in
Computing & Communication Technologies (RIVF),
pages 247?254, Ho Chi Minh City, Vietnam, 2008.
Feng Niu, Ce Zhang, Christopher Re, Jude W. Shav-
lik: DeepDive: Web-scale Knowledge-base Con-
struction using Statistical Learning and Inference. In
the VLDS Workshop, pages 25-28, 2012.
A. Rahman, Vincent Ng: Inducing Fine-Grained Se-
mantic Classes via Hierarchical and Collective Clas-
sification. In Proceedings the International Con-
ference on Computational Linguistics (COLING),
pages 931-939, 2010.
F. M. Suchanek, G. Kasneci, G. Weikum: Yago: a
Core of Semantic Knowledge. In Proceedings of the
16th International Conference on World Wide Web
(WWW) pages, 697-706, Banff, Alberta, Canada,
2007.
1496
F. M. Suchanek, M. Sozio, G. Weikum: SOFIE: A
Self-organizing Framework for Information Extrac-
tion. InProceedings of the 18th International Con-
ference on World Wide Web (WWW), pages 631?640,
Madrid, Spain, 2009.
P.P. Talukdar, F. Pereira: Experiments in Graph-Based
Semi-Supervised Learning Methods for Class-
Instance Acquisition. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 1473-1481, 2010.
P. Venetis, A. Halevy, J. Madhavan, M. Pasca, W. Shen,
F. Wu, G. Miao, C. Wu: Recovering Semantics of
Tables on the Web. In Proceedings of the VLDB En-
dowment, PVLDB 4(9), pages, 528?538. 2011.
W. Wu, H. Li, H. Wang, K. Zhu: Probase: A
Probabilistic Taxonomy for Text Understanding. In
Proceedings of the International Conference on
Management of Data (SIGMOD), pages 481?492,
Scottsdale, AZ, USA, 2012.
M. A. Yosef, S. Bauer, J. Hoffart, M. Spaniol, G.
Weikum: HYENA: Hierarchical Type Classifica-
tion for Entity Names. In Proceedings the In-
ternational Conference on Computational Linguis-
tics(COLING), to appear, 2012.
1497
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1009?1019,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Language-Aware Truth Assessment of Fact Candidates
Ndapandula Nakashole
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
ndapa@cs.cmu.edu
Tom M. Mitchell
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA, 15213
tom.mitchell@cs.cmu.edu
Abstract
This paper introduces FactChecker,
language-aware approach to truth-finding.
FactChecker differs from prior approaches
in that it does not rely on iterative peer
voting, instead it leverages language to
infer believability of fact candidates. In
particular, FactChecker makes use of lin-
guistic features to detect if a given source
objectively states facts or is speculative
and opinionated. To ensure that fact
candidates mentioned in similar sources
have similar believability, FactChecker
augments objectivity with a co-mention
score to compute the overall believability
score of a fact candidate. Our experiments
on various datasets show that FactChecker
yields higher accuracy than existing
approaches.
1 Introduction
Truth-finding algorithms aim to separate true
statements (facts) from false information. More
specifically, given a set of statements whose truth-
fulness is unknown (fact candidates), the key goal
of truth-finding algorithms is to generate a ranking
such that true statements are ranked ahead of false
ones. Truth-finders have the potential to address a
major obstacle on the Web: the problem of sources
spreading inaccurate and conflicting information.
This problem continues to grow with the develop-
ment of tools for easy Web authorship. Blogs, fo-
rums and social networking websites are not sub-
ject to traditional journalistic standards. Conse-
quently, the accuracy of information reported by
these sources is often unclear. Even more estab-
lished newspapers and websites may sometimes
report false information as they race to break sto-
ries. Therefore, truth-finding is becoming an in-
creasingly important problem. Information extrac-
tion projects aim to distill relational facts from nat-
ural language text (Auer et al, 2007; Bollacker et
al., 2008; Carlson et al, 2010; Fader et al, 2011;
Nakashole et al, 2011; Del Corro and Gemulla,
2013). These projects have produced knowledge
bases containing many millions of relational facts
between entities. However, despite these impres-
sive advances, there are still major limitations re-
garding precision. Within the context of informa-
tion extraction, fact extractors assign confidence
scores to extracted facts. However, such scores
are often tied to the extractor?s ability to read and
understand natural language text. This is differ-
ent from a score that indicates the degree to which
a given fact candidate is believable. Such a be-
lievability score is sometimes also referred to as
a credibility score or truthfulness score. The be-
lievability score reflects the likelihood that a given
statement is true. Truth-finding algorithms aim to
compute this score for each fact candidate.
Prior truth-finding methods are mostly based on
iterative voting, where votes are propagated from
sources to fact candidates and then back to sources
(Yin et al, 2007; Galland et al, 2010; Paster-
nack and Roth, 2010; Li et al, 2011; Yin and
Tan, 2011). At the core of iterative voting is the
assumption that candidates mentioned by many
sources are more likely to be true. However, ad-
ditional aspects of a source influence its trustwor-
thiness, besides external votes.
Our goal is to accurately assess truthfulness of
fact candidates by taking into account the lan-
guage of sources that mention them. A Mechan-
ical Turk study we carried out revealed that there
is a significant correlation between objectivity of
language and trustworthiness of sources. Objec-
tivity of language refers to the use of neutral,
impartial language, which is not personal, judg-
mental, or emotional. Trustworthiness refers to
1009
a source of information being reliable and truth-
ful. We use linguistics features to detect if a given
source objectively states facts or is speculative
and opinionated. Additionally, in order to ensure
that fact candidates mentioned in similar sources
have similar believability scores, our believability
computation model incorporates influence of co-
mentions. However, we must avoid falsely boost-
ing co-mentioned fact candidates. Our model ad-
dresses potential false boosts in two ways: first, by
ensuring that co-mention influence is only propa-
gated to related fact candidates; second, by ensur-
ing that the degree of co-mention influence is de-
termined by the trustworthiness of the sources in
which co-mentions occur.
The contribution of this paper is a language-
aware truth-finding approach. More precisely,
we make the following contributions: (1) Al-
ternative Fact Candidates: Truth-finders rank a
given fact candidate with respect to its alter-
natives. For example, alternative places where
Barack Obama could have been born. Virtually
all existing truth-finders assume that the alterna-
tives are provided. In contrast, we developed a
method for generating alternative fact candidates.
(2) Objectivity-Trustworthiness Correlation: We
hypothesize that objectivity of language and trust-
worthiness of sources are positively correlated. To
test this hypothesis, we designed a Mechanical
Turk study. The study showed that this correlation
does in fact hold. (3) Objectivity Classifier: Us-
ing labeled data from the Mechanical Turk study,
we developed and trained an objectivity classifier
which performed better than prior proposed lexi-
cons from literature. (4) Believability Computa-
tion: We developed FactChecker, a truth-finding
method that linearly combines objectivity and co-
mention influence. Our experiments showed that
FactChecker outperforms prior methods.
2 Fact Candidates
In this section, we formally define what constitutes
a fact candidate and describe how we go about
understanding semantics of fact candidates. We
then present our approach for generating alterna-
tive fact candidates.
2.1 Representation
The triple format is the most common representa-
tion of facts in knowledge bases. A formal specifi-
cation of the triple format is presented in the RDF
primer
1
. In RDF, data is represented as subject-
predicate-object (SPO) triples. In this work, we
restrict predicates to verbs (or verbal phrases such
as ?plays for?, ?graduated from?, etc.). Litera-
ture on automatic relation discovery (Fader et al,
2011) has shown that verbal phrases uncover a
large fraction of binary predicates while reducing
the amount of noisy phrases that do not denote any
relations. Therefore, we define a fact candidate as
follows:
Definition 1 (Fact Candidate) A fact candidate
f
i
is an ?S? V ?O? triple; where S is the subject,
V is a verbal phrase, and O is the object. We aim
to compute the truthfulness of f
i
, ?(f
i
) ? {T, F},
where T and F stand for true and false, respec-
tively.
Note that in this paper we are interested in cases
where ?(f
i
) is either T or F . That is, we assess
truthfulness of factual statements and not opinions
whose truthfulness is often both T and F to some
degree. For example, the triples: ?Obama? born in
?Kenya? and ?Obama? graduated from ?Harvard?
are valid fact candidates. However, the triple:
?Obama? deserves ?Nobel Peace Prize? is not.
2.2 Semantics
Based on the SVO triple, the meaning of a fact
candidate can be unclear and ambiguous. There-
fore, we first determine the semantics of a fact can-
didate before computing its truthfulness.
Entity Types. We first determine the expected
types of the subject and object in the SVO. For ex-
ample, for the SVO ?Einstein? died in ?Princeton?,
the expected types are person ? location. We de-
termine this by first computing the types of en-
tities that are valid for each verb (verbal phrase)
in a large SVO collection of 114m SVO triples
(Talukdar et al, 2012). Typing verbal phrases
is a once-off computation. Our phrase typing
method is similar to prior work on typing rela-
tional phrases (Nakashole et al, 2012). Exam-
ples of typed phrases are: ?person? died in ?year?,
?person? died in ?location?, and ?athlete? plays for
?team?. Given a triple, we look up the types for the
subject and the object and then determine which
of the typed phrases are compatible with the cur-
rent triple. We look up entity types in a knowledge
1
http://www.w3.org/TR/rdf-primer/
1010
base containing entities and their types. In partic-
ular, we use the NELL entity typing API (Carlson
et al, 2010). NELL?s entity typing method has
high recall because when entities are not in the
knowledge base, it performs on-the-fly type infer-
ence using the Web. This is not the case for other
options such as (Auer et al, 2007; Bollacker et al,
2008; Hoffart et al, 2011).
Relation Cardinality. Next, we learn cardinali-
ties of verbal phrases. Cardinality refers to how
arguments of a given relation relate to one another
numerically. We define the relation cardinality of a
verb Card(V ), as the average number of expected
arguments per given subject. For example, for the
relation ?died in?, 1 location is expected for each
subject. For other relations, the expected number
of arguments can be greater than 1 but less than
n : n ? R, n > 1. We approximate n using
statistics from the 114m SVO corpus based on the
average number of arguments per given first argu-
ment. In a once-off computation, we generate car-
dinality approximations per typed verbal phrase V
and its inverse V
?1
. For example, we generate
the cardinality estimates for both: ?person? died in
?location? and for ?location? INVERSE-OF(died
in) ?person?.
Synonymous Relations. Natural language is di-
verse. Semantically similar phrases can be syntac-
tically different. Therefore, we learn other verbs
that can be used to substitute V in SVO. We
pre-compute synonymous phrases from the 114m
SVO corpus using distributional semantics in the
same spirit as (Lin and Pantel, 2001; Nakashole et
al., 2012).
Synonymous verbs, relation cardinalities, and
entity types enable us to generate alternative fact
candidates.
2.3 Alternative Fact Candidates
Truth-finding methods rank f
i
relative to alter-
native candidates. While prior methods assume
the alternatives are known apriori, we developed
a method for generating alternative fact candi-
dates. For a given f
i
, we first identify the fixed
argument. The fixed argument is the argument of
the SVO which when fixed, requires finding the
fewest number of alternative candidates. For ex-
ample, for ?Einstein? died in ?Princeton?, the so-
lution is to fix the subject. This is because the car-
dinality of ?person? died in ?location? is one (1).
On the other hand, the cardinality of ?INVERSE-
OF(died in)? is many(n). In other words, the num-
ber of places where a person can be born (one)
is much fewer than the number of people that
can die in a place (many). In our example, al-
ternatives are possible places, other than Prince-
ton, where Einstein could have died. For example:
?Einstein? died in ?Germany? or ?Einstein? died in
?Switzerland?. More generally, the fixed argument
of fact candidate f
i
, is defined as follows:
Definition 2 (Fixed Argument) Let Card(V) be
the cardinality of V and Card(V
?1
) be the car-
dinality of the inverse of V , if Card(V ) <
Card(V
?1
), then the fixed argument is the sub-
ject, Arg
fixed
(f
i
) = S, else it is the object, O. If
Card(V ) == Card(V
?1
), then both arguments
are fixed, one at a time.
We use the fixed argument to define a topic as the
fixed argument plus the verb. Therefore, for the
SVO ?X? died in ?Y?, the topic ?places where X
died?, (Arg
fixed
= S), is not the same as the topic
?people who died in Y? (Arg
fixed
= O).
To locate alternatives, we use the topic
(Arg
fixed
+ V ) as a query. We search three
sources to either locate relevant documents or rele-
vant triples: the Google Web search API, the 114m
SVO collection, and the NELL KB. The SVO col-
lection and the KB return triples, however, the
Web search API returns documents. Therefore,
we apply a triple extractor to the retrieved docu-
ments. For all potential alternative triples, we per-
form type checking to ensure that the arguments
of the triples are type-compatible with f
i
. Further-
more, we generate an additional query for every
synonymous verb sV
i
, replacing V with sV
i
. Ex-
ample queries are: ?Einstein died in?, ?Einstein
passed in?, etc.
3 Objectivity and Trustworthiness
The principle of objective journalism, which is
a significant part of journalistic ethics, aims to
promote factual and fair reporting, undistorted by
emotion or personal bias (Schudson, 1978; Ka-
plan, 2002). Objectivity is also required in refer-
ence sources such as encyclopedias, scientific pub-
lications, and textbooks. For example, Wikipedia
enforces a neutral point-of-view policy (NPOV)
2
.
Articles violating the NPOV policy are marked
2
http://en.wikipedia.org/wiki/Wikipedia:Neutral point of view
1011
to indicate potential bias. While opinions, emo-
tions, and speculations can also be expressed us-
ing objective language, they are often stated using
subjective language (Turney et al, 2002; Riloff
and Wiebe, 2003; Yu and Hatzivassiloglou, 2003;
Wiebe et al, 2004; Liu et al, 2005; Recasens et
al., 2013). For example, consider the following
pieces of text:
(S) Well, I think Obama was born in Kenya
because his grandma who lives in Kenya said
he was born there.
(O) Theories allege that Obama?s published
birth certificate is a forgery, that his actual
birthplace is not Hawaii but Kenya.
Text S is a snippet from Yahoo Answers and
text O is a snippet from the Wikipedia page ti-
tled: ?Barack Obama Citizenship Conspiracy The-
ories?. S is subjective, expressing the opinion of
the author. On the other hand, O is objective, stat-
ing only what has been alleged. Literature on sen-
timent analysis (Turney et al, 2002; Liu et al,
2005), subjectivity detection (Riloff and Wiebe,
2003; Wiebe et al, 2004), and bias detection (Yu
and Hatzivassiloglou, 2003; Recasens et al, 2013)
has developed lexicons for identifying subjective
language. Due to the principle of objective jour-
nalism and the requirement of objectivity placed
on reference sources, we hypothesize a link be-
tween objectivity and trustworthiness as follows.
Hypothesis 1 Objective sources are more trust-
worthy than subjective sources. Therefore, we
can assume that fact candidates stated in objec-
tive sources are more likely to be true than those
stated in subjective sources.
To test the validity of the hypothesis, we carried
out a study where we solicited human input.
3.1 Mechanical Turk Study
We deployed an annotation study on Amazon Me-
chanical Turk (MTurk)
3
, a crowd-sourcing plat-
form for tasks requiring human input. Tasks on
MTurk are small questionnaires consisting of a de-
scription and a set of questions. Our study con-
sisted of two independent tasks. The first task was
titled ?Trustworthiness of News Articles?, where
annotators were given a link to a news article and
3
http://www.mturk.com
Figure 1: Summary of the results of the annotation
study on objectivity and trustworthiness.
asked to judge if they thought it was trustworthy
or not. The second task was titled ?Objectivity
of News Articles?. For this task, annotators were
asked to judge if a given article is objective or sub-
jective. For both tasks a third option of ?not sure?
was provided. We randomly selected 500 news ar-
ticles from a corpus of about 300,000 news articles
obtained from Google News from the topics of
Top News, Business, Entertainment, and SciTech.
For each task, every article was judged by three
annotators. This produced a total of 3000 annota-
tions. When we analyzed the output, we accepted
a label as valid for a given article if the label was
selected by the majority of the judges. Based on
this criteria, we obtained a set of 420 articles that
were both labeled for trustworthiness and objec-
tivity.
A summary of the outcome of the study is
shown in Figure 1; 74% of the untrustworthy
articles were independently labeled as subjec-
tive. On the other hand, 64% of trustworthy
articles were independently labeled as objective.
These results indicate a non-trivial positive cor-
relation between objectivity and trustworthiness.
We leverage this correlation in our believability
computation model. To incorporate objectivity in
FactChecker, we require for a given source docu-
ment, an objectivity score ? [0, 1], where 0 means
the source is subjective and 1 means it is objec-
tive. Next, describe our method for automatically
determining objectivity of sources.
3.2 Automatic Objectivity Detection
We trained a logistic regression classifier to pre-
dict the objectivity of a document. For training
and testing data, we used the labeled data from
the Mechanical Turk study. We additionally used
labeled text from prior work on subjectivity de-
tection (Pang and Lee, 2004). This resulted in a
total of 4, 600 documents, half subjective and the
other half objective. We used 4000 documents for
1012
# Feature
1 Subjectivity lexicon of strong and weak
subjective words (Riloff and Wiebe,
2003).
2 Sentiment lexicon of positive and negative
words (Liu et al, 2005).
3 Wikipedia-derived bias lexicon (Recasens
et al, 2013).
4 Part-of-speech (POS) tags
5 Frequent bi-grams
Table 1: Features used for the objectivity detector.
training, 2000 per label. The rest of the documents
were split into a development set (380) and a test
set (220).
A summary of the features we used is shown
in Table 1. Features 1-3 refer to lexicons devel-
oped by prior methods on subjectivity (Wiebe et
al., 2004), sentiment analysis (Liu et al, 2005) and
bias detection (Recasens et al, 2013). Feature 4
refers to part-of-speech tags of the terms found in
the document that are also in the lexicons. Feature
5 refers to bi-grams that frequently occur (men-
tion frequency of > 10) in the 4, 600 documents.
The most contributing features were the lexicons,
features (1-3) and the frequent bi-grams, feature
5. We discovered that using frequent bi-gram fea-
tures instead of uni-grams or bi-grams resulted in
higher precision. The classifier was able to de-
termine that for example bi-grams such as ?think
that?, ?so funny? and ?you thought? are negative
features for objectivity. Evaluation results of our
objectivity detector vs. baselines are shown in Ta-
ble 2. FactChecker?s objectivity detector has pre-
cision of 0.7814 ? 0.0539, with a 0.9-confidence
Wilson score interval (Brown et al, 2001) and this
outperforms the baselines. Next, we describe how
we leverage objectivity into FactChecker?s truth-
fulness model.
4 Believability Computation Model
FactChecker computes the believability score of a
fact candidate from its: i) objectivity score and
(ii) co-mention score. In this section we define
each of these scores.
The objectivity score reflects the trustworthi-
ness of sources where a fact candidate is men-
tioned. Given a fact candidate f
i
, mentioned in
a set of documents D
i
, where each document d ?
Approach Accuracy
Sentiment Lexicon 0.65?0.06
Wikipedia bias Lexicon 0.69?0.06
Subjectivity Lexicon 0.70?0.06
FC-Objectivity Detector 0.78?0.05
Table 2: Accuracy of the objectivity detector.
D
i
has objectivity O(d), f
i
?s objectivity score is
defined as follows:
Definition 3 (Objectivity Score)
O(f
i
) = log|D
i
|.
?
d
k
?D
i
O(d
k
)
|D
i
|
(1)
We do not use the sum of objectivity of sources
as the objectivity score because this enables fact
candidates mentioned in many low objectivity
sources to have high aggregate objectivity. Sim-
ilarly, we avoid using average objectivity of the
sources as it overestimates objectivity of candi-
dates stated in few sources. A candidate men-
tioned in 10 sources with 0.9 objectivity should
have higher objectivity than a candidate stated in
1 source of 0.9 objectivity. In Equation 1, log|D
i
|
addresses this issue.
The co-mention score aims to ensure that fact
candidates mentioned in similar sources have sim-
ilar believability scores. Suppose candidate f
i
is
mentioned in many highly objective sources, an-
other candidate f
j
is stated in only one highly ob-
jective source d
k
where f
i
is also mentioned. Then
the believability of f
j
should be boosted by it be-
ing co-mentioned with f
i
. If on the other hand f
i
and f
j
were co-mentioned in a subjective source,
f
j
should receive less boost from f
i
. This leads us
to the co-mention score ?(f
i
) of a candidate.
Definition 4 (Co-Mention Score)
?(f
i
) = ?(f
i
) +
?
f
j
?F
w
ij
?(f
j
) (2)
Where ?(f
i
) is the normalized mention fre-
quency of f
i
. The propagation weight w
ij
controls
how much boost is obtained from a co-mentioned
candidate. We define propagation weight, w
ij
, as
the average of the objectivity of the sources that
mention both candidates.
w
ij
= average O(d
k
) : d
k
? (D
i
?D
j
) (3)
1013
where O(d
k
) is the objectivity of document d
k
,
D
i
and D
j
are the sets of documents that mention
f
i
and f
j
, respectively. Notice that we could boost
co-mentioned but not related candidates, thereby
causing false boosts. To remedy this, we only al-
low w
ij
to be greater than zero if the fact can-
didates f
i
and f
j
are on the same topic. Recall
that the topic is determined by the fixed argument
(Definition 2) and the verb. Allowing only fact
candidates on the same topic to influence each
other is important considering that many trivial
facts are often repeated in sources of diverse qual-
ity.
To leverage the inter-dependencies among re-
lated co-mentioned fact candidates, we model the
solution with a graph ranking method. Each fact
candidate is a node and there is an edge between
each pair of related fact candidate nodes f
i
and
f
j
, with w
ij
as the edge weight. Thus, equation 2
can be reformulated as ? = M?, where ? is the
co-mention score vector and M is a Markov ma-
trix which is stochastic, irreducible and aperiodic.
Thus, a power method will converge to a solution
in a similar manner to PageRank. Implementation
consists of iteratively applying Equation 2 until
the change in the score is less than a threshold .
The solution is the final co-mention scores of fact
candidates.
Finally, to compute the believability score of a
fact candidate, we linearly combine its objectivity
score with its co-mention as follows:
Definition 5 (Believability Score)
?(f
i
) = ?O(f
i
) + (1? ?)?(f
i
) (4)
Where ? is a weighting parameter ? [0, 1]
which controls the relative importance of the two
aspects of FactChecker. As we show in our exper-
iments, ? can be robustly chosen within the range
of 0.2 to 0.6. In our experiments we used ? = 0.6.
The entire procedure of FactChecker is summa-
rized in Algorithm 1.
5 Evaluation
We evaluated FactChecker for accuracy. We de-
fine accuracy as the probability of a true fact can-
didate having a higher believability score than a
false candidate. Let ?(f
i
) ? {T, F} be the truth-
fulness of a fact candidate f
i
, accuracy is defined
as:
Algorithm 1 FactChecker
Input: A set F of fact candidates
Input: KB K, SVO corpus C, WebW
Output: A set L of rankings ?f
i
? F
L = ?
while F 6= ? do
pick f
i
from F
A= getAlternatives(f
i
,K,C,W)
PriorityQueue L
i
= ?
for all alternative fact candidates f
?
j
? A do
?(f
?
j
) = getBelievabilityScore(f
?
j
)
L
i
.insert(f
?
j
, ?(f
?
j
))
end for
?(f
i
) = getBelievabilityScore(f
i
)
L
i
.insert(f
i
, ?(f
i
))
L ? L
i
Remove f
i
from F
end while
return L
Acc =
?
(?(f
i
)=T :?(f
j
)=F )
(?(f
i
) > ?(f
j
))
|{?(f
i
, f
j
) : ?(f
i
) = T ? ?(f
j
) = F}|
Datasets. We evaluated FactChecker on three
datasets: i) KB Fact Candidates: The first dataset
consists of fact candidates taken from the fact ex-
traction pipeline of a state-of-the-art knowledge
base, NELL (Carlson et al, 2010). The fact candi-
dates span four different relation types: company
acquisitions, book authors, movie directors and
athlete teams. For each fact candidate, we applied
our alternative candidate generation method. We
only considered fact candidates with non-trivial
alternative candidate sets; where the alternative
candidate set is greater than zero. Since all of
the baselines we compared against assume alter-
natives are provided, we apply all methods to the
same set of alternative fact candidates discovered
by our method. Details of this dataset are shown
as rows starting with ?KB-? in Table 3.
ii) Wikipedia Fact Candidates: For the sec-
ond dataset, we did not restrict the fact candidates
to specific topics from a knowledge base, instead
we aimed to evaluate all fact candidates about a
given entity. We selected entities from Wikipe-
dia. For this, we chose US politicians: all current
state senators, all current state governors, and all
44 presidents. First, we extracted fact candidates
1014
#Candidates #Alternatives
KB-Acquisitions 50 241
KB-Authors 50 295
KB-Directors 50 228
KB-Teams 40 162
WKP Politicians 54 219
GK Quiz 18 72
Table 3: Fact candidate datasets.
from the infoboxes of the Wikipedia pages of the
entities. Second, we applied our alternative can-
didate generation method to discover alternatives
from the Web, SVO corpus, and NELL. Details of
the resulting dataset are shown in the row ?WKP
Politicians? in Table 3.
iii) General Knowledge Quiz: The third
dataset consists of questions from a general
knowledge quiz
4
. We selected questions from
the inventions category. Questions are multiple
choice, with 4 options per question. Thus, from
each question, we created one fact candidate and
3 alternative candidates. Details of the resulting
dataset are shown in the row ?KWP Quiz? in Ta-
ble 3.
Baselines. We compared FactChecker against five
baselines: i) Vote counts the number of sources
that mention the fact candidate. ii) TruthFinder is
an iterative voting approach where votes are prop-
agated from sources to fact candidates and then
back to sources. Implemented as described in (Yin
et al, 2007). iii) Investment is also based on tran-
sitive voting, however scores are updated differ-
ently. A source gets a vote of trust from each
candidate it ?invests? in, but the vote is weighted
by the proportion of trust the source previously
?invested? in the candidate relative to other in-
vestors. Implemented as described in (Pasternack
and Roth, 2010). iv) PooledInvest is a variation
of investment, we report both because in their pa-
per, there was no clear winner among the two vari-
ations. v) 2-Estimates is a probabilistic model
which approximates error rates of sources and fact
candidates (Galland et al, 2010).
5.1 Accuracy on KB Fact Candidates
Figure 2 shows accuracy on KB fact candidates.
FactChecker achieves accuracy between 70% and
88% and is significantly more accurate than the
4
http://www.indiabix.com/general-knowledge/
Figure 2: Accuracy of KB fact candidates.
Figure 3: FactChecker variations.
other approaches on all relations except com-
pany acquisitions. On book authors, movie di-
rectors, and athlete teams, FactChecker outper-
forms all other approaches by at least 10%, 9%,
and 8% respectively. On company acquisitions,
the different methods achieve similar accuracy,
with TruthFinder being the most accurate and
FactChecker is 4% behind. Company acquisitions
also yield the lowest difference between Vote and
the highest performing method, of 6%. For book
authors, movie directors, and athlete teams, the
difference between majority Vote and the highest
performing method (FactChecker in this case) is
13%, 12%, and 13% respectively.
5.2 Accuracy of FactChecker Variations
To quantify how various aspects of our approach
affect overall performance, we studied two varia-
tions. The first variation is FC-Objectivity which
only uses objectivity to compute believability.
Thus, ? = 1 in Definition 5. The second varia-
tion is FC-CoMention which only uses co-mention
scores to compute believability, ? = 0. The
1015
Approach WKP Politicians GK Quiz
Vote 0.85?0.09 0.82?0.15
TruthFinder 0.85?0.09 0.82?0.15
2-Estimates 0.85?0.09 0.82?0.15
Investment 0.86?0.08 0.82?0.15
PooledInvest 0.85?0.09 0.82?0.15
FC-Objectivity 0.88?0.08 0.87?0.12
FC-CoMention 0.85?0.09 0.72?0.18
FactChecker 0.90?0.07 0.87?0.12
Table 4: Accuracy on politicians and quiz data sets
last variation is the full FactChecker method us-
ing both objectivity and co-mentions with ? = 0.6
From Figure 3, it is clear that both the objectiv-
ity of sources and the influence of co-mentions
contribute to the overall accuracy of FactChecker.
Full-fledged FactChecker performs better than
both variations. In most cases, FC-Objectivity per-
forms better than FC-CoMention.
5.3 Accuracy on Wikipedia Fact Candidates
Table 4, column ?WKP Politicians?, shows ac-
curacy on Wikipedia fact candidates, with a 0.9-
confidence Wilson score interval (Brown et al,
2001). For this dataset we again see FactChecker
outperforming the other methods under compari-
son. On this dataset, FactChecker has a accuracy
of 0.9 ? 0.07 and a 5% accuracy advantage over
the other methods. The second best performance
comes from the FC-Objectivity variation, with ac-
curacy of 0.88? 0.08.
5.4 Accuracy on General Knowledge Quiz
Table 4, column ?GK Quiz ?, shows accuracy on
the general knowledge quiz fact candidates. On
this dataset, FactChecker and its objectivity-only
variation (FC-objectivity) have the highest accu-
racy of 87%. Notice that this dataset was the only
one where we did not generate the alternative fact
candidates. Instead, we took the options of the
multiple choice questions as alternatives. Since
the quiz is meant to be taken by humans, the alter-
natives are often very close, plausible answers. Yet
even in this difficult setting, we see FactChecker
outperforming the baselines.
Sample fact candidates, with ranked alternatives
from all three datasets are shown in Table 5.
Figure 4: Effect of ? of FactChecker.
5.5 Parameter Sensitivity
We analyzed the effect of the selection of lambda
? (see Definition 5) on FactChecker?s perfor-
mance. The result of this analysis is shown in Fig-
ure 4. FactChecker is insensitive to this parame-
ter when ? is varied from 0.2 to 0.6. Therefore,
lambda can be robustly chosen within this range.
5.6 Discussion
Overall, from these results we make the follow-
ing observations: i) Majority vote is a competitive
baseline; ii) Iterative voting-based methods pro-
vide slight improvements on majority vote. This
is due to the fact that at the core of iterative vot-
ing is still the assumption that fact candidates
mentioned in many sources are more likely to be
true. Therefore, for both majority vote and it-
erative voting, when mention frequencies of var-
ious alternatives are the same, accuracy suffers.
Based on these observations, it is clear that truth-
finding solutions need to incorporate fine-grained
content-aware features outside of external votes.
FactChecker takes a step in this direction by incor-
porating the document-level feature of objectivity.
6 Related Work
There is a fairly small body of work on truth-
finding (Yin et al, 2007; Galland et al, 2010;
Pasternack and Roth, 2010; Li et al, 2011; Yin and
Tan, 2011; Zhao et al, 2012; Pasternack and Roth,
2013). The method underlying most truth-finding
algorithms is iterative transitive voting (Yin et al,
2007; Galland et al, 2010; Pasternack and Roth,
2010; Li et al, 2011). Fact candidates are ini-
tialized with a score. Trustworthiness of sources
is then computed from the believability of the fact
candidates they mention. In return, believability of
candidates is recomputed based on the trustworthi-
1016
Dataset Fact Candidate Alternatives & Ranking
WKP ?George W. Bush? lived in ?Midland,TX? 1.Midland,TX
2.Compton,CA
3.Washington D.C.
4.Venezuela*
KB ?Dirk Kuyt? plays for ?Liverpool? 1. Liverpool
2.Cardiff City*
3.Netherlands
4.Hungary*
Quiz ?Bifocals? invented by ?Benjamin Franklin? 1. Benjamin Franklin
2. Rudolf Diesel*
3.Thomas Alva Edison*
4.Alfred B. Nobel*
Table 5: Sample rankings by FactChecker, alternatives marked (*) are false. The ranking of the candidate
from the ?KB? dataset is not completely accurate.
ness of their sources. This process is repeated over
several iterations until convergence. (Yin et al,
2007) was the first to implement this idea, subse-
quent work improved upon iterative voting in sev-
eral directions. (Dong et al, 2009) incorporates
copying-detection; giving high trust to sources
that are independently authored. (Galland et al,
2010) approximates error rates of sources and fact
candidates. (Pasternack and Roth, 2010) intro-
duces prior knowledge in the form of linear pro-
gramming constraints in order to ensure that the
truth discovered is consistent with what is already
known. (Yin and Tan, 2011) introduces supervi-
sion by using ground truth facts so that sources
that disagree with the ground truth are penalized.
(Li et al, 2011) uses search engine APIs to gather
additional evidence for believability of fact can-
didates. WikiTrust (Adler and Alfaro, 2007) is
a content-aware but domain-specific method. It
computes trustworthiness of wiki authors based
on the revision history of the articles they have
authored. Motivated by interpretability of prob-
abilistic scores, two recent papers addressed the
truth-finding problem as a probabilistic inference
problem over the sources and the fact candidates
(Zhao et al, 2012; Pasternack and Roth, 2013).
Truth-finders based on textual entailment such as
TruthTeller (Lotan et al, 2013) determine if a sen-
tence states something or not. The focus is on un-
derstanding natural language, including the use of
negation. This is similar to the goal of fact ex-
traction (Banko et al, 2007; Carlson et al, 2010;
Fader et al, 2011; Nakashole et al, 2011; Del
Corro and Gemulla, 2013).
In a departure from prior work, our method
leverages language of sources in its believability
computation model. Furthermore, we introduced
a co-mention score which is designed to avoid po-
tential false boots among fact candidates. Addi-
tionally, we developed a method for generating al-
ternative fact candidates. Prior methods assume
these are readily available. Only (Li et al, 2011)
uses the Web to identify alternatives, however, this
is only done after manually specifying the fixed ar-
gument. In contrast, we introduced a method for
identifying the fixed argument based on relation
cardinalities learned from SVO statistics.
7 Conclusion
In this paper, we presented FactChecker, a
language-aware approach to truth-finding. In con-
trast to prior approaches, which rely on external
votes, FactChecker includes objectivity of sources
in its believability computation model.
FactChecker can be seen as a first step to-
wards language-aware truth-finding. Future di-
rections include using more sentence-level fea-
tures such the use of hedges, assertive verbs, and
factive verbs. These types of words fall into a
class of words used to express certainties, spec-
ulations or doubts ? these are important cues that
FactChecker can leverage.
Acknowledgments
We thank members of the NELL team at CMU
for their helpful comments. This research was
supported by DARPA under contract number
FA8750-13-2-0005.
1017
References
S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyga-
niak, Z.G. Ives: DBpedia: A Nucleus for a Web of
Open Data. In Proceedings of the 6th International
Semantic Web Conference (ISWC), pages 722?735,
Busan, Korea, 2007.
B. T. Adler, L. de Alfaro: A content-driven reputa-
tion system for the wikipedia. In Proceedings of the
16th International Conference on World Wide Web
(WWW), pages 261-270, 2007.
M. Banko, M. J. Cafarella, S. Soderland, M. Broad-
head, O. Etzioni: Open Information Extraction from
the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 2670?2676, Hyderabad, India, 2007.
K. D. Bollacker, C. Evans, P. Paritosh, T. Sturge, J.
Taylor: Freebase: a Collaboratively Created Graph
Database for Structuring Human Knowledge. In
Proceedings of the ACM SIGMOD International
Conference on Management of Data (SIGMOD),
pages, 1247-1250, Vancouver, BC, Canada, 2008.
L. D. Brown, T.T. Cai, A. Dasgupta: Interval Estima-
tion for a Binomial Proportion. Statistical Science
16: pages 101?133, 2001.
E. Cabrio, S. Villata: Combining Textual Entailment
and Argumentation Theory for Supporting Online
Debates Interaction. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pp. 208-212, 2012.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka,
T.M. Mitchell: Coupled Semi-supervised Learning
for Information Extraction. In Proceedings of the
Third International Conference on Web Search and
Web Data Mining (WSDM), pages 101?110, New
York, NY, USA, 2010.
L. Del Corro, R. Gemulla: ClausIE: clause-based
open information extraction. In Proceedings of the
22nd International Conference on World Wide Web
(WWW), pages 355-366. 2013.
X. Dong, L. Berti-Equille, D. Srivastava: Truth discov-
ery and copying detection in a dynamic world. In
Proceedings of the VLDB Endowment PVLDB, 2(1),
pp. 562-573, 2009.
A. Fader, S. Soderland, O. Etzioni: Identifying Rela-
tions for Open Information Extraction. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1535?1545, Edinburgh, UK, 2011.
A. Galland, S. Abiteboul, A. Marian, P. Senellart: Cor-
roborating information from disagreeing views. In
Proceedings of the 3rd International Conference on
Web Search and Web Data Mining (WSDM), pages
131-140, 2010.
C. Havasi, R. Speer, J. Alonso: ConceptNet 3: a Flex-
ible, Multilingual Semantic Network for Common
Sense Knowledge. In Proceedings of the Recent Ad-
vances in Natural Language Processing (RANLP),
Borovets, Bulgaria, 2007.
J. Hoffart, F. Suchanek, K. Berberich, E. Lewis-
Kelham, G. de Melo, G. Weikum: YAGO2: Ex-
ploring and Querying World Knowledge in Time,
Space, Context, and Many Languages. In Proceed-
ings of the 20th International Conference on World
Wide Web (WWW), pages 229?232, Hyderabad, In-
dia. 2011.
R. Kaplan: Politics and the American Press: The Rise
of Objectivity, pages 1865-1920, New York, Cam-
bridge University Press, 2002.
X. Li and W. Meng, C. T. Yu: T-verifier: Verifying
truthfulness of fact statements. In Proceedings of
the International Conference on Data Engineering
(ICDE), pp. 63-74, 2011.
D. Lin, P. Pantel: DIRT: discovery of inference rules
from text. KDD 2001
B. Liu, M. Hu, J. Cheng: Opinion Observer: analyzing
and comparing opinions on the Web. InProceedings
of the 14th International Conference on World Wide
Web (WWW), pages 342351, 2005.
A. Lotan, A. Stern, I. Dagan TruthTeller: Annotating
Predicate Truth. In Proceedings of Human Language
Technologies: Conference of the North American
Chapter of the Association of Computational Lin-
guistics (HLT-NAACL), pp. 752-757, 2013.
N. Nakashole, M. Theobald, G. Weikum: Scalable
Knowledge Harvesting with High Precision and
High Recall. In Proceedings of the 4th International
Conference on Web Search and Web Data Mining
(WSDM), pages 227?326, Hong Kong, China, 2011.
N. Nakashole, T. Tylenda, G. Weikum: Fine-grained
Semantic Typing of Emerging Entities. In Proceed-
ings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pp. 1488-
1497, 2013.
N. Nakashole, G. Weikum, F. Suchanek: PATTY:
A Taxonomy of Relational Patterns with Seman-
tic Types. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1135 -
1145, Jeju, South Korea, 2012.
V. Nastase, M. Strube, B. Boerschinger, C. Zirn, A.
Elghafari: WikiNet: A Very Large Scale Multi-
Lingual Concept Network. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation(LREC), Malta, 2010.
B. Pang, L. Lee: A Sentimental Education: Sentiment
Analysis Using Subjectivity Summarization Based
1018
on Minimum Cuts. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL), 271-278, 2004.
J. Pasternack, D. Roth: Knowing What to Believe. In
Proceedings the International Conference on Com-
putational Linguistics (COLING), pp. 877-885, Bei-
jing, China. 2010.
J. Pasternack, D. Roth: Latent credibility analysis. In
Proceedings of the 22nd International Conference
on World Wide Web (WWW), pp. 1009-1020, 2013.
E. Riloff, J. Wiebe: Learning Learning extraction pat-
terns for subjective expressions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 105112,
2013.
M. Recasens, C. Danescu-Niculescu-Mizil, D. Juraf-
sky: Linguistic Models for Analyzing and Detecting
Biased Language. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pp. 1650-1659, 2013.
F. Niu, C. Zhang, C. Re, J. W. Shavlik: DeepDive:
Web-scale Knowledge-base Construction using Sta-
tistical Learning and Inference. In the VLDS Work-
shop, pages 25-28, 2012.
M. Schudson: Discovering the News: A Social History
of American Newspapers. New York: Basic Books.
1978.
F. M. Suchanek, M. Sozio, G. Weikum: SOFIE: A
Self-organizing Framework for Information Extrac-
tion. InProceedings of the 18th International Con-
ference on World Wide Web (WWW), pages 631?640,
Madrid, Spain, 2009.
P. P. Talukdar, D. T. Wijaya, T.M. Mitchell: Acquir-
ing temporal constraints between relations. In Pro-
ceeding of the 21st ACM International Conference
on Information and Knowledge Management, pages
992-1001, CIKM 2012.
P. D. Turney: Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 417424. 2002.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, M. Martin:
Learning subjective language. Computational Lin-
guistics, 30(3):277308. 2004.
X. Yin, J. Han, P. S. Yu: Truth Discovery with
Multiple Conflicting Information Providers on the
Web. In Proceedings of the International Confer-
ence on Knowledge Discovery in Databases (KDD)
, pages1048-1052. 2007.
X. Yin, W. Tan: Semi-supervised truth discover. In
Proceedings of the 19th International Conference on
World Wide Web (WWW), pp. 217-226, 2011.
H. Yu, V. Hatzivassiloglou: Towards Answering Opin-
ion Questions: Separating Facts from Opinions and
Identifying the Polarity of Opinion Sentences. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages. 129-
136, 2003.
B. Zhao, B. I. P. Rubinstein, J. Gemmell, J. Han: A
Bayesian approach to discovering truth from con-
flicting sources for data integration. In Proceedings
of the VLDB Endowment (PVLDB), 5(6):550-561,
2012.
1019
