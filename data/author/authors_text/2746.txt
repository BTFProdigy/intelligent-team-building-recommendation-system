Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 265?272
Manchester, August 2008
Instance-Based Ontology Population
Exploiting Named-Entity Substitution
Claudio Giuliano
Fondazione Bruno Kessler
Trento, Italy
giuliano@fbk.eu
Alfio Gliozzo
Laboratory for Applied Ontology
Italian National Research Council
Rome, Italy
alfio.gliozzo@cnr.istc.it
Abstract
We present an approach to ontology popu-
lation based on a lexical substitution tech-
nique. It consists in estimating the plausi-
bility of sentences where the named entity
to be classified is substituted with the ones
contained in the training data, in our case,
a partially populated ontology. Plausibility
is estimated by using Web data, while the
classification algorithm is instance-based.
We evaluated our method on two different
ontology population tasks. Experiments
show that our solution is effective, out-
performing existing methods, and it can
be applied to practical ontology population
problems.
1 Introduction
Semantic Web and knowledge management appli-
cations require to populate the concepts of their
domain ontologies with individuals and find their
relationships from various data sources, including
databases and natural language texts. As the ex-
tensional part of an ontology (the ABox) is often
manually populated, this activity can be very time-
consuming, requiring considerable human effort.
The development of automatic techniques for on-
tology population is then a crucial research area.
Natural language processing techniques are natu-
ral candidates to solve this problem as most of the
data contained in the Web and in the companies?
intranets is free text. Information extraction (IE)
is commonly employed to (semi-) automate such a
task.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Current state-of-the-art IE systems are mostly
based on general purpose supervised machine
learning techniques (e.g., kernel methods). How-
ever, supervised systems achieve acceptable accu-
racy only if they are supplied with a sufficiently
large amount of training data, usually consisting of
manually annotated texts. Consequently, they can
be only used to populate top-level concepts of on-
tologies (e.g., people, locations, organizations). In
fact, when the number of subclasses increases the
number of annotated documents required to find
sufficient positive examples for all subclasses be-
comes too large to be practical. As domain ontolo-
gies usually contain hundreds of concepts arranged
in deep class/subclass hierarchies, alternative tech-
niques have to be found to recognize fine-grained
distinctions (e.g., to categorize people as scientists
and scientists as physicists, mathematicians, biol-
ogists, etc.).
In this paper, we present an approach to the clas-
sification of named entities into fine-grained onto-
logical categories based on a method successfully
employed in lexical substitution.
1
In particular, we
predict the fine-grained category of a named en-
tity, previously recognized, by simply estimating
the plausibility of sentences where the entity to be
classified is substituted with the ones contained in
the training data, in our case, a partially populated
ontology.
In most of the cases, ontologies are partially
populated during the development phase and af-
ter that the annotation cost is practically negligi-
ble, making this method highly attractive in many
applicative domains. This allows us to define an
instance-based learning approach for fine-grained
1
Lexical substitution consists in identifying the most
likely alternatives (substitutes) of a target word given its con-
text (McCarthy, 2002).
265
entity categorization that exploits the Web to col-
lect evidence of the new entities and does not re-
quire any labeled text for supervision, only a par-
tially populated ontology. Therefore, it can be used
in different domains and languages to enrich an
existing ontology with new entities extracted from
texts by a named-entity recognition system and/or
databases.
We evaluated our method on the benchmark pro-
posed by Tanev and Magnini (2006) to provide a
fair comparison with other approaches, and on a
general purpose ontology of people derived from
WordNet (Fellbaum, 1998) to perform a more ex-
tensive evaluation. Specifically, the experiments
were designed to investigate the effectiveness of
our approach at different levels of generality and
with different amounts of training data. The results
show that it significantly outperforms the base-
line methods and, where a comparison is possible,
other approaches and achieves a good performance
with a small number of examples per category. Er-
ror analysis shows that most of the misclassifica-
tion errors are due to the finer-grained distinctions
between instances of the same super-class.
2 Lexical Substitutability and Ontology
Population
Our approach is based on the assumption that en-
tities that occur in similar contexts belong to the
same concept(s). This can be seen as a special
case of the distributional hypothesis, that is, terms
that occur in the same contexts tend to have similar
meanings (Harris, 1954).
If our assumption is correct, then given an in-
stance in different contexts one can substitute it
with another of the same ontological type (i.e.,
of the same category) and probably generate true
statements. In fact, most of the predicates that
can be asserted for an instance of a particular cate-
gory can also be asserted for other instances of the
same category. For instance, the sentence ?Ayr-
ton Senna is a F1 Legend? preserves its truthful-
ness when Ayrton Senna is replaced with Michael
Schumacher, while it is false when Ayrton Senna
is replaced with the MotoGP champion Valentino
Rossi.
For our purposes, the Web provides a simple and
effective solution to the problem of determining
whether a statement is true or false. Due to the high
redundancy of the Web, the high frequency of a
statement generated by a substitution usually pro-
vides sufficient evidence for its truth, allowing us
to easily implement an automatic method for fine-
grained entity classification. Following this intu-
ition, we developed an ontology population tech-
nique adopting pre-classified entities as training
data (i.e., a partially populated ontology) to clas-
sify new ones.
When a new instance has to be classified, we
first collect snippets containing it from the Web.
Then, for each snippet, we substitute the new in-
stance with each of the training instances. The
snippets play a crucial role in our approach be-
cause we expect that they provide the features that
characterize the category to which the entity be-
longs. Thus, it is important to collect a sufficiently
large number of snippets to capture the features
that allow a fine-grained classification.
To estimate the correctness of each substitution,
we calculate a plausibility score using a modified
version of the lexical substitution algorithm intro-
duced in Giuliano et al (2007), that assigns higher
scores to the substitutions that generate highly fre-
quent sentences on the Web. In particular, this
technique ranks a given list of synonyms accord-
ing to a similarity metric based on the occur-
rences in the Web 1T 5-gram corpus,
2
which spec-
ify n-grams frequencies in a large Web sample.
This technique achieved the state-of-the-art perfor-
mance on the English Lexical Substitution task at
SemEval 2007 (McCarthy and Navigli, 2007).
Finally, on the basis of these plausibility scores,
the algorithm assigns the new instance to the cat-
egory whose individuals show a closer linguistic
behavior (i.e., they can be substituted generating
plausible statements).
3 The IBOP algorithm
In this section, we describe the algorithmic and
mathematical details of our approach. The
instance-based ontology population (IBOP) algo-
rithm is an instance-based supervised machine
learning approach.
3
The proposed algorithm is
summarized as follows:
Step 1 For each candidate instance i, we collect
the first N snippets containing i from the Web.
For instance, 3 snippets for the candidate instance
Ayrton Senna are ?The death of Ayrton Senna at
2
http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13.
3
An analogy between instance-based learning methods
and our approach is left to future work.
266
the 1994 San Marino GP?, ?triple world cham-
pion Ayrton Senna?, and ?about F1 legend Ayrton
Senna?.
Step 2 Then, for each retrieved snippet q
k
(1 6
k 6 N ), we derive a list of hypothesis phrases
by replacing i with each training instance j from
the given ontology. For instance, from the snippet
?about F1 legend Ayrton Senna?, we derive ?about
F1 legend Michael Schumacher? and ?about F1
legend Valentino Rossi?, assuming to have the for-
mer classified as F1 driver and the latter as Mo-
toGP driver.
Step 3 For each hypothesis phrase h
j
, we calcu-
late the plausibility score s
j
using a variant of the
scoring procedure defined in Giuliano et al (2007).
In our case, s
j
is given by the sum of the point-
wise mutual information (PMI) of all the n-grams
(1 < n 6 5) that contain j divided by the self-
information of the right and left contexts.
4
Divid-
ing by the self-information allows us to penalize
the hypotheses that have contexts with a low infor-
mation content, such as sequences of stop words.
The frequency of the n-grams is estimated from
the Web 1T 5-gram corpus. For instance, from
the hypothesis phrase ?about F1 legend Michael
Schumacher?, we generate and score the following
n-grams: ?legend Michael Schumacher?, ?F1 leg-
end Michael Schumacher?, and ?about F1 legend
Michael Schumacher?.
Step 4 To obtain an overall score s
c
for the cat-
egory c, we sum the scores obtained from each
training instance of category c for all snippets, as
defined in Equation 1.
s
c
=
N
X
k=1
M
X
l=1
s
l
, (1)
where M is the number of training instances for
the category c.
5
Step 5 Finally, the instance i is categorized with
that concept having the maximum score:
c
?
=
(
argmax
c
s
c
if s
c
> ?;
? otherwise.
(2)
4
The pointwise mutual information is defined as the log of
the deviation between the observed frequency of a n-gram and
the probability of that n-gram if it were independent and the
self-information is a measure of the information content of a
n-gram (? log p, where p is the probability of the n-gram).
5
Experiments using the sum of average or argmax score
yield worst results.
Where a higher value of the parameter ? increases
precision but degrades recall.
4 Benchmarks
For evaluating the proposed algorithm and com-
paring it with other algorithms, we adopted the two
benchmarks described below.
4.1 Tanev and Magnini Benchmark
Tanev and Magnini (2006) proposed a benchmark
ontology that consists of two high-level named
entity categories (i.e., person and location) both
having five fine-grained subclasses (i.e., mountain,
lake, river, city, and country as subtypes of loca-
tion; statesman, writer, athlete, actor, and inventor
are subtypes of person). WordNet and Wikipedia
were used as primary data sources for populating
the evaluation ontology. In total, the ontology is
populated with 280 instances which were not am-
biguous (with respect to the ontology). We ex-
tracted the training set from WordNet, collecting
20 examples per sub-category, of course, not al-
ready contained in the test set.
4.2 People Ontology
The benchmark described in the previous section
is clearly a toy problem, and it does not allow us
to evaluate the effectiveness of our method, in par-
ticular the ability to perform fine-grained classifi-
cations. To address this problem, we developed
a larger ontology of people (called People Ontol-
ogy), characterized by a complex taxonomy hav-
ing multiple layers and containing thousands of in-
stances. This ontology has been extracted from
WordNet, that we adapted to our purpose after a
re-engineering phase. In fact, we need a formal
specification of the conceptualizations that are ex-
pressed by means of WordNet?s synsets, and, in
particular, we need a clear distinction between in-
dividuals and categories, as well as a robust cate-
gorization mechanism to assign individuals to gen-
eral concepts.
This result can be achieved by following the di-
rectives defined by Gangemi et al (2003) for On-
toWordNet, in which the informal WordNet se-
mantics is re-engineered in terms of a description
logic. We follow an analogous approach. Firstly,
any possible instance in WordNet 1.7.1 has been
identified by looking for all those synsets contain-
ing at least one word starting with a capital letter.
The result is a set of instances I . All the remaining
267
Figure 1: The taxonomy of the People Ontology extracted from WordNet 1.7.1. Numbers in brackets are
the total numbers of individuals per category. Concepts that have less than 40 instances were removed.
synsets are then regarded as concepts, collected in
the set C. Then, is a relations between synsets are
converted into one of the following standard OWL-
DL constructs:
X subclass of Y if X is a Y and X ? C and Y ? C
X instance of Y if X is a Y and X ? I and Y ? C
The formal semantics of both subclass of
and instance of is formally defined in OWL-
DL. subclass of is a transitive relation (i.e.,
Xsubclass ofY and Y subclass ofZ implies
Xsubclass ofZ) and the instance of relation
has the following property: Xinstance ofY and
Y subclass ofZ implies Xinstance ofZ.
To define the People Ontology, we selected
the sub-hierarchy of WordNet representing peo-
ple, identifying the corresponding top-level synset
X = {person, individual, someone, somebody,
mortal, soul}, and collecting all the classes Y such
that Y is a subclass of X and all the instances I
such that I is an instance of Y . We discovered
that many concepts in the derived hierarchy were
empty or scarcely populated. As we need a suffi-
cient amount data to obtain statistically significant
results, we eliminated the classes that contain less
than 40 instances from the ontology. The derived
ontology contains 1627 instances structured in 21
sub-categories (Figure 1). Finally, we randomly
split its individuals into two equally sized subsets.
The results reported in the following section were
evaluated using two-fold cross-validation on these
two subsets.
5 Evaluation
In this section, we present the performance of the
IBOP algorithm on the evaluation benchmarks de-
scribed in the previous section.
5.1 Experimental Setting
For each individual, we collected 100 entity men-
tions in their context by querying Google
TM
. As
most of them are names of celebrities, the Web
provided sufficient data.
6
We approached the population task as a stan-
dard categorization problem, trying to assign new
instances to the most specific category. We mea-
sured standard precision/recall figures. In addition,
we evaluated the classifier accuracy at the most ab-
stract level, by inheriting the predictions from sub-
concepts to super-concepts. For example, when
an instance is assigned to a specific category (e.g.,
Musician), it is also (implicitly) assigned to all its
super-classes (e.g., Artist and Creator). This op-
eration is performed according to the extensional
semantics of the description logic, as described in
the previous section. Following this approach, we
are able to evaluate the effectiveness of our algo-
rithm at any level of generality. The micro- and
macro-averaged F
1
have been evaluated by taking
into account both specific and generic classes at
the same time. In this way, we tend to penalize the
6
A study of how the number of snippets N would impact
the performance of the IBOP algorithm has been deferred to
future work.
268
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 10  20  30  40  50
Mic
ro-
F 1
Number of examples
Figure 2: Learning curve on the People Ontology.
gross misclassification errors (e.g., Biologist vs.
Poet), while minor errors (e.g., Poet vs. Drama-
tist) are less relevant. This approach is similar to
the one proposed by Melamed and Resnik (2000)
for a similar hierarchical categorization task.
5.2 Accuracy
Table 1 shows micro- and macro-averaged results
of the proposed method obtained on the Tanev
and Magnini (2006) benchmark and compares
them with the class-example (Tanev and Magnini,
2006), IBLE (Giuliano and Gliozzo, 2007), and
class-word (Cimiano and V?olker, 2005) methods,
respectively. Table 2 shows micro- and macro-
averaged results of the proposed method obtained
on the People Ontology and compares them with
the random and most frequent baseline methods.
7
In both experiments, the IBOP algorithm was
trained on 20 examples per category and setting
the parameter ? = 0 in Equation 2.
For the People Ontology, we performed a dis-
aggregated evaluation, whose results are shown in
Table 3, while Figure 2 shows the learning curve.
The experiment was conducted setting the param-
eter ? = 0.
System Micro-F
1
Macro-F
1
IBOP 73 71
Class-Example 68 62
IBLE 57 47
Class-Word 42 33
Table 1: Comparison of different ontology popula-
tion techniques on the Tanev and Magnini (2006)
benchmark.
7
The most frequent category has been estimated on the
training data.
System Micro-F
1
Macro-F
1
IBOP 70.1 62.3
Random 15.4 15.5
Most Frequent 20.7 3.3
Table 2: Comparison between the IBOP algorithm
and the baseline methods on the People Ontology.
Class Prec Recall F
1
Scientist 84.4 73.3 78.4
Physicist 63.0 39.3 48.4
Mathematician 25.0 67.5 36.5
Chemist 44.2 52.0 47.7
Biologist 62.5 13.2 21.7
Social scientist 43.1 30.1 35.5
Performer 76.5 66.9 71.4
Actor 67.5 67.9 67.7
Musician 68.1 48.9 56.9
Creator 70.6 84.5 76.9
Film Maker 52.9 68.7 59.7
Artist 72.8 85.5 78.6
Painter 74.4 86.1 79.8
Musician 68.9 81.6 74.7
Comunicator 76.4 83.1 79.6
Writer 78.6 76.6 77.6
Poet 67.4 61.2 64.1
Dramatist 65.0 70.7 67.7
Representative 84.8 76.7 80.6
Business man 47.2 40.5 43.6
Health professional 29.3 25.0 27.0
micro 69.6 70.7 70.1
macro 62.3 70.7 62.3
Table 3: Results for each category of the People
Ontology.
5.3 Confusion Matrix
Table 4 shows the confusion matrix for the People
Ontology task, in which the rows are ground truth
classes and the columns are predictions. The ex-
periment was conducted using 20 training exam-
ples per category and setting the parameter ? =
0. The matrix has been calculated for the finer-
grained categories and, then, grouped according to
their top-level concepts.
5.4 Precision/Recall Tradeoff
Figure 3 shows the precision/recall curve for the
People Ontology task obtained varying the param-
eter ? in Equation 2. The experiment was con-
ducted using 20 training examples per category.
5.5 Discussion
The results obtained are undoubtedly satisfactory.
Table 1 shows that our approach outperforms the
other three methods on the Tanev and Magnini
(2006) benchmark. Note that the Class-Example
approach has been trained on 1194 named enti-
269
Scientist Performer Creator Communicator Business Health
Phy Mat Che Bio Soc Act Mus Fil Pai Mus Poe Dra Rep man prof
Phy 68 40 25 3 11 2 0 0 3 1 7 1 7 2 3
Mat 3 27 1 0 0 0 0 1 0 0 4 0 2 1 1
Che 12 10 53 2 7 3 1 2 2 0 1 0 4 4 1
Bio 4 12 13 10 3 3 0 1 5 2 4 1 11 2 5
Soc 6 3 4 1 22 4 0 2 2 3 4 1 12 0 9
Act 3 1 2 0 0 106 6 20 0 3 2 4 7 1 1
Mus 1 1 2 0 0 16 64 5 2 28 2 2 7 0 1
Fil 0 0 0 0 0 7 0 46 0 4 1 1 4 3 1
Pai 2 1 0 0 1 1 1 2 93 3 1 0 2 1 0
Mus 1 0 0 0 0 1 16 2 3 142 1 3 2 1 2
Poe 1 2 1 0 1 2 3 3 6 12 93 20 6 1 1
Dra 0 2 1 0 0 3 0 2 2 3 9 65 1 2 2
Rep 0 6 7 0 3 6 1 0 3 2 5 0 189 1 0
Bus 3 3 6 0 0 0 1 1 0 1 2 0 6 17 2
Hea 4 0 5 0 3 3 1 0 4 2 2 2 10 0 12
Table 4: Confusion matrix for the finer-grained categories grouped according to their top-level concepts
of the People Ontology.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pre
cis
ion
Recall
terminal conceptsall concepts
Figure 3: Precision/recall curve on the People On-
tology.
ties, almost 60 examples per category, whereas we
achieved the same result with around 10 examples
per category. On the other hand, as Table 2 shows,
the IBOP algorithm is effective in populating a
more complex ontology and significantly outper-
forms the random and most frequent baselines.
An important characteristic of the algorithm is
the small number of examples required per cate-
gory. This affects both the prediction accuracy and
the computation time (this is generally a common
property of instance-based algorithms). Therefore,
finding an optimal tradeoff between the training
size and the performance is crucial. The learn-
ing curve (Figure 2) shows that the algorithm out-
performs the baselines with only 1 example per
category and achieves a good accuracy (F
1
?
67%) with only 10 examples per category, while
it reaches a plateau around 20 examples (F
1
?
70%), but leaving a little room for improvement.
Table 4 shows that misclassification errors are
largely distributed among categories belonging to
the same super-class (i.e., the blocks on the main
diagonal are more densely populated than others).
As expected, the algorithm is much more accurate
for the top-level concepts (i.e., Scientist, Commu-
nicator, etc.), where the category distinctions are
clearer, while a further fine-grained classification,
in some cases, is even difficult for human anno-
tators. In particular, results are higher for fine-
grained categories densely populated and with a
small number of sibling categories (i.e., Painter
and Musician). We have observed that the results
on sparse categories can be made more precise by
increasing the training size, generally at the ex-
pense of a lower recall.
We tried to maximize precision by varying the
parameter ? in Equation 2, that is, avoiding all
assignments where the plausibility score is lower
than a given threshold. Figure 3 shows that the
precision can be significantly enhanced (? 90%)
at the expense of poor recall (? 20%), while the
algorithm achieves 80% precision at around 50%
recall.
Finally, we performed some preliminary error
analysis, investigating the misclassifications in the
categories Scientists and Musicians. Several errors
are due to lack of information in WordNet, For ex-
ample, Leonhard Euler was a mathematician and
physicist, however, in WordNet, he is classified as
physicist, and our system classifies him as math-
ematician. On the other hand, for simplicity, the
algorithm returns a single category per instance,
270
however, the test set contains many entities that are
classified in more than one category. For instance,
Bertolt Brecht is both poet and dramatist and the
system classified him as dramatist. Another inter-
esting case is the presence of two categories Musi-
cian, one is subclass of Performer and the other of
Artist, in which, for instance, Ringo Starr is a per-
former while John Lennon is an artist, while the
system classified both as performers.
6 Related work
Brin (1998) defined a methodology to extract in-
formation from the Web starting from a small set
of seed examples, then alternately learning extrac-
tion patterns from seeds, and further seeds from
patterns. Despite the fact that the evaluation was
on relation extraction the method is general and
might be applied to entity extraction and catego-
rization. The approach was further extended by
Agichtein and Gravano (2000). Our approach dif-
fers from theirs in that we do not learn patterns.
Thus, we do not require ad hoc strategies for gen-
erating patterns and estimating their reliability, a
crucial issue in these approaches as ?bad? patterns
may extract wrong seeds instances that in turn may
generate even more inaccurate patterns in the fol-
lowing iteration.
Fleischman and Hovy (2002) approached the
ontology population problem as a supervised clas-
sification task. They compare different machine
learning algorithms, providing instances in their
context as training examples as well as more global
semantic information derived from topic signature
and WordNet.
Alfonseca and Manandhar (2002) and Cimiano
and V?olker (2005) present similar approaches re-
lying on the Harris? distributional hypothesis and
the vector-space model. They assign a particu-
lar instance represented by a certain context vec-
tor to the concept corresponding to the most simi-
lar vector. Contexts are represented using lexical-
syntactic features.
KnowItAll (Etzioni et al, 2005) uses a search
engine and semantic patterns (similar to those de-
fined by Hearst (1992)) to classify named entities
on the Web. The approach uses simple techniques
from the ontology learning field to perform extrac-
tion and then annotation. It also is able to perform
very simple pattern induction, consisting of look-
ing at n words before and n words after the occur-
rence of an example in the document. With pat-
tern learning, KnowItAll becomes a bootstrapped
learning system, where rules are used to learn new
seeds, which in turn are used to learn new rules.
A similar approach is used in C-PANKOW (Cimi-
ano et al, 2005). Compared to KnowItAll and
C-PANKOW, our approach does not need hand-
crafted patterns as input. They are implicitly found
by substituting the training instances in the con-
texts of the input entities. Another key difference
is that concepts in the ontology do not need to be
lexicalized.
Tanev and Magnini (2006) proposed a weakly-
supervised method that requires as training data a
list of terms without context for each category un-
der consideration. Given a generic syntactically
parsed corpus containing at least each training en-
tity twice, the algorithm learns, for each category, a
feature vector describing the contexts where those
entities occur. Then, it compares the new (un-
known) entity with the so obtained feature vec-
tors, assigning it to the most similar category. Even
though we used a significantly smaller number of
training instances, we obtained better results on
their benchmark.
More recently, Giuliano and Gliozzo (2007)
proposed an unsupervised approach based on lexi-
cal entailment, consisting in assigning an entity to
the category whose lexicalization can be replaced
with its occurrences in a corpus preserving the
meaning. A disadvantage is that the concepts in the
ontology have to be lexicalized, as they are used
as training examples. Our approach is based on a
similar idea, but with the main difference that an
instance is substituted with other instances rather
than with their category names. Considering that,
in most of the cases, ontologies are partially popu-
lated during the development phase, and hence the
annotation cost is marginal, our approach is a re-
alistic alternative for practical ontology population
problems.
7 Conclusions and Future Work
We have described an instance-based algorithm
for automatic fine-grained categorization of named
entities, previously identified by an entity recogni-
tion system or already present in a database. This
method is meant to provide an effective solution to
the ontology population problem. It exploits the
Web or a domain corpus to collect evidence of the
new instances and does not require labeled texts
for supervision, but a partially populated ontology.
271
The experimental results show that, where a com-
parison is possible, our method outperforms previ-
ous methods and it can be applied to different do-
mains and languages to (semi-) automatically en-
rich an existing ontology.
Future work will address the definition of a hi-
erarchical categorization strategy where instances
are classified in a top-down manner, in order to ef-
ficiently populate very large ontologies, since we
plan to apply this method to extract structured in-
formation from Wikipedia. Furthermore, we will
investigate how co-reference resolution might well
benefit from our ontology classification. Finally,
we plan to exploit the IBOP algorithm for ontol-
ogy mapping and multilingual alignment of lexical
resources.
Acknowledgments
Claudio Giuliano is supported by the X-Media
project (http://www.x-media-project.
org), sponsored by the European Commission as
part of the Information Society Technologies (IST)
program under EC grant number IST-FP6-026978.
References
Agichtein, Eugene and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In DL ?00: Proceedings of the fifth ACM conference
on Digital libraries, pages 85?94, New York, NY,
USA. ACM.
Alfonseca, Enrique and Suresh Manandhar. 2002. Ex-
tending a lexical ontology by a combination of dis-
tributional semantics signatures. In EKAW ?02: Pro-
ceedings of the 13th International Conference on
Knowledge Engineering and Knowledge Manage-
ment. Ontologies and the Semantic Web, pages 1?7,
London, UK. Springer-Verlag.
Brin, Sergey. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, EDBT?98.
Cimiano, Philipp and Johanna V?olker. 2005. Towards
large-scale, open-domain and ontology-based named
entity classification. In Proceedings of RANLP?05,
pages 66? 166?172, Borovets, Bulgaria.
Cimiano, Philipp, G?unter Ladwig, and Steffen Staab.
2005. Gimme the context: Context-driven automatic
semantic annotation with C-PANKOW. In Ellis, Al-
lan and Tatsuya Hagino, editors, Proceedings of the
14th World Wide Web Conference, pages 332 ? 341,
Chiba, Japan, MAY. ACM Press.
Etzioni, Oren, Michael Cafarella, Doug Downey,
Ana M. Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):191?134.
Fellbaum, Christiane. 1998. WordNet. An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Fleischman, Michael and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, Taipei, Taiwan.
Gangemi, Aldo, Roberto Navigli, and Paola Velardi.
2003. Axiomatizing WordNet glosses in the On-
toWordNet project. In Proocedings of the Workshop
on Human Language Technology for the Semantic
Web and Web Services at ISWC 2003, Sanibel Island,
Florida.
Giuliano, Claudio and Alfio Gliozzo. 2007. In-
stance based lexical entailment for ontology popu-
lation. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 248?256.
Giuliano, Claudio, Alfio Gliozzo, and Carlo Strappar-
ava. 2007. Fbk-irst: Lexical substitution task ex-
ploiting domain and syntagmatic coherence. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 145?
148, Prague, Czech Republic, June.
Harris, Zellig. 1954. Distributional structure. WORD,
10:146?162.
Hearst, Marti A. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Compu-
tational Linguistics, Nantes, France, July.
McCarthy, Diana and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of the Fourth International Workshop
on Semantic Evaluations (SemEval-2007), pages 48?
53, Prague, Czech Republic, June.
McCarthy, Diana. 2002. Lexical substitution as a task
for WSD evaluation. In Proceedings of the ACL-
02 workshop on Word Sense Disambiguation, pages
109?115, Morristown, NJ, USA.
Melamed, I. Dan and Philip Resnik. 2000. Tagger eval-
uation given hierarchical tag sets. Computers and
the Humanities, pages 79?84.
Tanev, Hristo and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-2006), Trento, Italy.
272
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 276?285,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Wikipedia as Frame Information Repository
Sara Tonelli
FBK-irst
I-38100, Trento, Italy
satonelli@fbk.eu
Claudio Giuliano
FBK-irst
I-38100, Trento, Italy
giuliano@fbk.eu
Abstract
In this paper, we address the issue of au-
tomatic extending lexical resources by ex-
ploiting existing knowledge repositories.
In particular, we deal with the new task
of linking FrameNet and Wikipedia us-
ing a word sense disambiguation system
that, for a given pair frame ? lexical unit
(F, l), finds the Wikipage that best ex-
presses the the meaning of l. The mapping
can be exploited to straightforwardly ac-
quire new example sentences and new lex-
ical units, both for English and for all lan-
guages available in Wikipedia. In this way,
it is possible to easily acquire good-quality
data as a starting point for the creation of
FrameNet in new languages. The evalua-
tion reported both for the monolingual and
the multilingual expansion of FrameNet
shows that the approach is promising.
1 Introduction
Many applications in the context of natural lan-
guage processing or information retrieval have
proved to convey significant improvement by ex-
ploiting lexical databases with high-quality anno-
tation such as FrameNet (Fillmore et al, 2003)
and WordNet (Fellbaum, 1998). Nevertheless, the
practical use of similar resources is often biased
by their limited coverage because manual anno-
tation is time-consuming and requires a relevant
financial effort. For this reason, some research ac-
tivities have focused on the automatic enrichment
of such resources with annotated information in
(near) manual quality. The main strategy proposed
was the mapping between resources in order to
reciprocally enrich different lexical databases by
linking their information layers. This has proved
to be useful in several tasks, from verb classifica-
tion (Chow and Webster, 2007) to semantic role
labeling (Giuglea and Moschitti, 2006), open text
semantic parsing (Shi and Mihalcea, 2004) and
textual entailment (Burchardt and Frank, 2006).
In this work, we focus on the automatic enrich-
ment of the FrameNet database for English and we
propose a new framework to extend this procedure
to new languages. While similar works in the past
have mainly proposed to automatically extend the
FrameNet database by mapping frames and Word-
Net synsets (Shi and Mihalcea (2005), Johans-
son and Nugues (2007), and Tonelli and Pighin
(2009)), we present an explorative approach that
for the first time exploits Wikipedia to this pur-
pose. In particular, given a lexical unit l belong-
ing to a frame F , we devise a strategy to link
l to the Wikipedia article that best captures the
sense of l in F . This is basically a word disam-
biguation (WSD) problem (Erk, 2004) and to this
purpose we employ a state-of-the-art WSD sys-
tem (Gliozzo et al, 2005). The mapping between
(F, l) pairs and Wikipedia pages could then be ex-
ploited for three further subtasks: (a) automati-
cally extract from Wikipedia all sentences point-
ing to the Wikipage mapped with (F, l) and assign
them to F ; (b) automatically expand the lexical
units sets in the English FrameNet by exploiting
the redirecting and linking strategy of Wikipedia;
and (c) since Wikipedia is available in 260 lan-
guages, use the English Wikipedia article linked to
(F, l) as a bridge to carry out sentence and lexical
unit retrieval in other languages. The set of auto-
matically collected data would represent the start-
ing point for the creation of FrameNet in new lan-
guages. In fact, having a repository of sentences
extracted from Wikipedia which have already been
divided by sense would significantly speed up the
annotation process. In this way, the annotators
would not need to extract all sentences in a cor-
pus containing l and classify them by sense. In-
stead, they should simply validate the given sen-
tences and assign the correct frame elements.
276
In the following, we start by providing a brief
overview of FrameNet and Wikipedia and we
present their structure and organization. Next, we
describe the algorithm for mapping lexical units
and Wikipages and the word sense disambigua-
tion algorithm employed by the system. In Sec-
tion 5 we describe the dataset used in the first ex-
periment and report evaluation results of the map-
ping between (F, l) pairs and Wikipedia senses. In
Section 6 we describe an application of the map-
ping, i.e. the automatic enrichment of English
FrameNet. We describe the data extraction pro-
cess and evaluate the quality of the data. In Section
7 we describe and evaluate another application of
the mapping, i.e. the acquisition of data for the
automatic creation of Italian FrameNet using the
Italian Wikipedia. Finally, we draw conclusions
and present future research directions.
2 FrameNet and Wikipedia
FrameNet (Fillmore et al, 2003) is a lexical re-
source for English based on corpus evidence,
whose conceptual model comprises a set of proto-
typical situations called frames, the frame-evoking
words or expressions called lexical units (LUs)
and the roles or participants involved in these situ-
ations, called frame elements. All lexical units be-
longing to the same frame have similar semantics
but, differently from WordNet synsets, they can
belong to different categories and present differ-
ent parts of speech. For example, the KILLING
frame is described in the FrameNet database
1
as ?A Killer or Cause causes the death of the
Victim?. The elements in capitals are the se-
mantic roles (frame elements) typically involved
in the KILLING situation. The frame definition
comes also with the list of frame-evoking lexical
units, namely annihilate.v, annihilation.n, butch-
ery.n, carnage.n, crucify.v, deadly.a, etc. Since
FrameNet is a corpus-based resource, every lexi-
cal unit should be instantiated by a set of exam-
ple sentences, where the frame elements are anno-
tated as well. Instead, FrameNet is still an ongoing
project and in the latest release (v. 1.3) there are
about 3,380 lexical units out of 10,195 that come
with no example sentences. In this work we focus
on these lexical units and propose how to automat-
ically collect the missing sentences. Anyhow, the
algorithm we propose is suitable also for expand-
ing sentence sets already present in FrameNet.
1
http://framenet.icsi.berkeley.edu
Wikipedia
2
is one of the largest online reposito-
ries of encyclopedic knowledge, with millions of
articles available for a large number of languages
(>2,800,000 for English). The article (or page)
is the basic entry in Wikipedia. Every article has
an unique reference, i.e., one or more words that
identify the page and are present in its URL. For
example, Ball (dance) identifies the page that de-
scribes several types of ball intended as formal
dance, while Dance (musical form) describes the
dance as musical genre. Every Wikipedia article
is linked to others, and in the body of every page
there are plenty of links to connect the most rel-
evant terms to other pages. Another important
attribute is the presence of about 3,000,000 redi-
rection pages, that given an identifier that is not
present in Wikipedia, automatically display the
page with the most semantically similar identi-
fier (for example Killing is redirected to the Mur-
der page). Wikipedia contains also more than
100,000 disambiguation pages listing all senses
(pages) for an ambiguous entity. For example,
Book has 9 senses, which correspond to 9 dif-
ferent articles. Wikipedia structure and quality
make this resource particularly suitable for infor-
mation extraction and word sense disambiguation
tasks (Csomai and Mihalcea (2008) and Milne and
Witten (2008)). In fact, page references can be
seen as senses and Wikipedia as a large sense in-
ventory. From this point of view, also linking
a lexical unit to the correct Wikipedia page is a
word sense disambiguation issue because it im-
plies recognizing what meaning the lexical unit
has in the given frame. For example, dance.n
in the SOCIAL EVENT frame should be linked to
Ball (dance) and not to Dance (musical form).
3 The Mapping Algorithm
In this section, we describe how to map a frame
? lexical unit pair (F, l) into the Wikipedia arti-
cle that best captures the sense of l as defined in
F . The mapping problem is casted as a supervised
WSD problem, in which l must be disambiguated
using F to provide the context and Wikipedia to
provide the sense inventory and the training data.
Even if the idea of using Wikipedia links for dis-
ambiguation is not novel (Cucerzan, 2007), it is
applied for the first time to FrameNet lexical units,
considering a frame as a sense definition. The pro-
posed algorithm is summarized as follows:
2
http://en.wikipedia.org
277
Step 1 For each lexical unit l, we collect from
the English Wikipedia dump
3
all contexts
4
where l
is the anchor of an internal link (wiki link). The set
of targets represents the senses of l in Wikipedia
and the contexts are used as labelled training ex-
amples. For example, the lexical unit building.n in
the frame Buildings is an anchor in 708 different
contexts that point to 42 different Wikipedia pages
(senses).
Step 2 The set of contexts with their correspond-
ing senses is then used to train the WSD system
described in Section 4. For example, the context
?The building, which date from the mid-to-late
19th century, were built in a variety of High Victo-
rian architectural styles.? is a training example for
the sense defined by the Wikipedia page Building.
Step 3 Finally, the disambiguation model
learned in the previous step is used to map a pair
(F, l) to a Wikipedia article. (F, l) is represented
as a fictitious-context derived by aggregating the
frame definition and all lexical units associated to
F . We used the term ?fictitious-context? to re-
mark the slight difference in structure compared
with the training contexts (i.e., the Wikipedia
paragraphs). For example, ?. . . structures form-
ing an enclosure and providing protection from
the elements . . . acropolis arena auditorium bar
building . . . ? is the fictitious-context built for
the pair (Buildings, building.n). The sense, i.e.,
the Wikipedia article, assigned to the fictitious-
context by the disambiguation algorithm uniquely
defines the mapping. The previous example is as-
signed to the Wikipedia page Building.
4 The WSD Algorithm
Gliozzo et al (2005) proposed an elegant approach
to WSD based on kernel methods. The algorithm
proved effective at Senseval-3 (Mihalcea and Ed-
monds, 2004) and, nowadays, it still represents
the state-of-the-art in WSD (Pradhan et al, 2007).
Specifically, they addressed these issues: (i) inde-
pendently modeling domain and syntagmatic as-
pects of sense distinction to improve feature rep-
resentativeness; and (ii) exploiting external knowl-
edge acquired from unlabeled data, with the pur-
pose of drastically reducing the amount of labeled
3
http://download.wikimedia.org/enwiki/
20090306
4
A context corresponds to a line of text in the Wikipedia
dump and it is represented as a paragraph in a Wikipedia ar-
ticle.
training data. The first direction is based on the
linguistic assumption that syntagmatic and domain
(associative) relations are crucial for representing
sense distictions, but they are originated by differ-
ent phenomena. Regarding the second direction, it
is possible to obtain a more accurate prediction by
taking into account unlabeled data relevant for the
learning problem (Chapelle et al, 2006).
On the other hand, kernel methods are theoret-
ically well founded in statistical learning theory
and shown good empirical results in many appli-
cations (Shawe-Taylor and Cristianini, 2004). The
strategy adopted by kernel methods consists of
splitting the learning problem into two parts. They
first embed the input data in a suitable feature
space, and then use a linear algorithm (e.g., sup-
port vector machines) to discover nonlinear pat-
terns in the input space. The kernel function is
the only task-specific component of the learning
algorithm. Thus, to develop a WSD system, one
only needs to define appropriate kernel functions
to represent the domain and syntagmatic aspects
of sense distinction and to exploit the properties
of kernel functions in order to define a composite
kernel that combines and extends individual ker-
nels.
The WSD system described in the following
consists of a composite kernel (Section 4.3) that
combines the domain and syntagmatic kernels.
The former (Section 4.1) models the domain as-
pects of sense distinction, the latter (Section 4.2)
represents the syntagmatic aspects of sense dis-
tinction.
4.1 Domain Kernel
It is been shown that domain information is fun-
damental for WSD (Magnini et al, 2002). For in-
stance, the (domain) polysemy between the com-
puter science and the medicine senses of the word
?virus? can be solved by considering the domain
of the context in which it appears.
In the context of kernel methods, domain infor-
mation can be exploited by defining a kernel func-
tion that estimates the domain similarity between
the contexts of the word to be disambiguated. The
simplest method to estimate the domain similarity
between two texts is to compute the cosine simi-
larity of their vector representations in the vector
space model (VSM). The VSM is a k-dimensional
space R
k
, in which the text t
j
is represented by
a vector
~
t
j
, where the i
th
component is the term
278
frequency of the term w
i
in t
j
. However, such an
approach does not deal well with lexical variabil-
ity and ambiguity. For instance, despite the fact
that the sentences ?he is affected by AIDS? and
?HIV is a virus? express concepts closely related,
their similarity is zero in the VSM because they
have no words in common (they are represented
by orthogonal vectors). On the other hand, due
to the ambiguity of the word ?virus? , the simi-
larity between the sentences ?the laptop has been
infected by a virus? and ?HIV is a virus? is greater
than zero, even though they convey very different
messages.
To overcome this problem, Gliozzo et al (2005)
introduced the domain model (DM) and show how
to define a domain VSM in which texts and terms
are represented in a uniform way. A DM is com-
posed of soft clusters of terms. Each cluster rep-
resents a semantic domain, that is, a set of terms
that often co-occur in texts having similar topics.
A DM is represented by a k ? k
?
rectangular ma-
trix D, containing the degree of association among
terms and domains.
The matrix D is used to define a function D :
R
k
? R
k
?
, that maps the vector
~
t
j
represented in
the standard VSM, into the vector
~
t
?
j
in the domain
VSM. D is defined by
D(
~
t
j
) =
~
t
j
(I
IDF
D) =
~
t
?
j
, (1)
where
~
t
j
is represented as a row vector, I
IDF
is a
k?k diagonal matrix such that i
IDF
i,i
= IDF (w
i
),
and IDF (w
i
) is the inverse document frequency
of w
i
.
In the domain space, the similarity is esti-
mated by taking into account second order rela-
tions among terms. For example, the similarity of
the two sentences ?He is affected by AIDS? and
?HIV is a virus? is very high, because the terms
AIDS, HIV and virus are strongly associated with
the domain medicine.
Singular valued decomposition (SVD) is used to
acquire in a unsupervised way the DM from a cor-
pus represented by its term-by-document matrix
T. SVD decomposes the term-by-document ma-
trix T into three matrixes T ' V?
k
?
U
T
, where
V and U are orthogonal matrices (i.e., V
T
V = I
and U
T
U = I) whose columns are the eigenvec-
tors of TT
T
and T
T
T respectively, and ?
k
?
is
the diagonal k ? k matrix containing the highest
k
?
 k eigenvalues of T, and all the remaining
elements set to 0. The parameter k
?
is the dimen-
sionality of the domain VSM and can be fixed in
advance. Under this setting, the domain matrix D
is defined by
D = I
N
V
p
?
k
?
(2)
where I
N
is a diagonal matrix such that i
N
i,i
=
1
q
?
~
w
?
i
,
~
w
?
i
?
,
~
w
?
i
is the i
th
row of the matrix V
?
?
k
?
.
The domain kernel is explicitly defined by
K
D
(t
i
, t
j
) = ?D(t
i
),D(t
j
)?, (3)
where D is the domain mapping defined in Equa-
tion 1. Finally, the domain kernel is further ex-
tended to include the standard bag-of-word kernel.
4.2 Syntagmatic Kernel
Kernel functions are not restricted to operate on
vectorial objects ~x ? R
k
. In principle, kernels
can be defined for any kind of object representa-
tion, such as strings and trees. As syntagmatic re-
lations hold among words collocated in a partic-
ular temporal order, they can be modeled by ana-
lyzing sequences of words. Therefore, the string
kernel (Shawe-Taylor and Cristianini, 2004) is a
valid tool to represent such relations. It counts
how many times a (non-contiguous) subsequence
of symbols u of length n occurs in the input string
s, and penalizes non-contiguous occurrences ac-
cording to the number of gaps they contain. For-
mally, let V be the vocabulary, the feature space
associated with the string kernel of length n is in-
dexed by a set I of subsequences over V of length
n. The (explicit) mapping function is defined by
?
n
u
(s) =
X
i:u=s(i)
?
l(i)
, u ? V
n
, (4)
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and ? ?]0, 1] is the decay factor used to pe-
nalize non-contiguous subsequences.
The associated string kernel is defined by
K
n
(s
i
, s
j
) = ??
n
(s
i
), ?
n
(s
j
)? =
X
u?V
n
?
n
(s
i
)?
n
(s
j
)
(5)
Gliozzo et al (2005) modified the generic def-
inition of the string kernel in order to take into
account (sparse) collocations. Specifically, they
defined syntagmatic kernels as a combination of
string kernels applied to sequences of words in a
fixed-size window centered on the word to be dis-
ambiguated. This formulation allows estimating
the number of common (sparse) subsequences of
279
words (i.e., collocations) between two examples,
in order to capture syntagmatic similarity. The
syntagmatic kernel is defined by
K
S
(s
i
, s
j
) =
p
X
n=1
K
n
(s
i
, s
j
), (6)
where K
n
is the string kernel defined in Equation
5 and the parameter n represents the length of the
subsequences analyzed when estimating the sim-
ilarity between contexts. Notice that the syntag-
matic kernel is only effective for those fictitious
contexts in which the lexical units do occur in
meaningful sentences, however this is not guaran-
teed for the lexical units without examples.
4.3 Composite Kernel
Finally, to combine domain and syntagmatic infor-
mation, the composite kernel is defined by
K
WSD
(t
i
, t
j
) =
?
K
D
(t
i
, t
j
) +
?
K
S
(t
i
, t
j
), (7)
where
?
K
D
and
?
K
S
are normalized kernels defined
in Equation 3 and 6, respectively.
5
It follows di-
rectly from the explicit construction of the feature
space and from closure properties of kernels that
it is a valid kernel.
5 Mapping task
In this section we report the first experiment,
namely the mapping between (F, l) pairs and a
Wikipedia pages. We describe the experimental
setup and then present the corresponding evalua-
tion.
5.1 Experimental setup
We applied our algorithm to all lexical units that
do not have any example sentence in the FrameNet
database. In principle, the proposed approach can
be applied to every lexical unit, and we expect the
algorithm performance to improve if some exam-
ple sentences are already available because they
could be added to the fictitious-context used to
represent (F, l) in the system. Nevertheless, in this
explorative study we wanted to focus on the harder
cases, even if results are likely to be worse than on
the whole FrameNet database.
In FrameNet, 3,305 (F, l) pairs have no exam-
ple sentences (536 pairs with adjectival LU, 1313
verbal LU, 1456 nominal LU). Since Wikipedia is
basically a resource organized by concepts, which
5
?
K(x
i
, x
j
) =
K(x
i
,x
j
)
?
K(x
j
,x
j
)K(x
i
,x
i
)
are generally expressed by nouns, we decided to
restrict our experiment to nominal lexical units.
Besides, many verbal and adjectival concepts in
Wikipedia are redirected to nominal identifiers.
So, we randomly selected 900 pairs with nominal
lexical units. For the moment, we decided to dis-
card lexical units expressed by multiwords (about
150), which will be taken into account in a future
version of our system. The average ambiguity of
the 900 LUs considered is 1.24 in FrameNet. In-
stead, every LU corresponds to about 35 candidate
senses in Wikipedia.
In order to perform WSD, we built the domain
model from the 200,000 most visited Wikipedia
articles. After removing terms that occur less than
5 times, the resulting dictionaries contain about
300,000 terms. We used the SVDLIBC pack-
age
6
to compute the SVD, truncated to 100 di-
mensions. The experiments were performed using
the SVM package LIBSVM (Chang and Lin, 2001)
customized to embed the kernels described in Sec-
tion 4.
5.2 Evaluation
In this first evaluation step, we focus on the quality
of the mapping between (F, l) pairs and Wikipedia
articles. In order to evaluate the system output,
we created a gold standard where 250 (F, l) pairs
randomly extracted from the nominal subset de-
scribed above have been manually linked to the
Wikipedia page (if available) that best corresponds
to the meaning of l in F . The pairs have been cho-
sen in order to maximize the frame variability, i.e.
every pair corresponds to a different frame. Since
our gold standard contains 34% of all frames in
the FrameNet database, we believe that, despite its
limited size, it is well representative of FrameNet
characteristics. Evaluation was carried out com-
paring the system output against the gold stan-
dard. Results are reported in Table 1. The base-
line was computed considering the most frequent
sense of every lexical unit in Wikipedia. This ele-
ment is obtained by taking into account all occur-
rences in Wikipedia where the lexical unit LU we
consider is anchored to a given page. The most
frequent sense for LU is the page to which LU is
most frequently linked in Wikipedia. Since about
14% of the lexical units in the gold standard are
not present in Wikipedia, we also estimated an up-
per bound accuracy of 0.86. This confirms our in-
6
http://tedlab.mit.edu/
?
dr/svdlibc/
280
tuition that FrameNet and Wikipedia are linkable
resources to a large extent and that our task is well-
founded.
Accuracy
Baseline 0.66
System output 0.71
Upper bound 0.86
Table 1: Accuracy evaluation.
Wrong assignments include also problematic
cases that are not directly connected to proper sys-
tem errors. One of the most relevant issues is the
different granularity between FrameNet frames
and Wikipages. For example, the NETWORK
frame is defined as ?a set of entities of the same
or similar types (Nodes) are linked to each other
by Connections to form a Network allowing for
the flow of information, resources, etc.?. Even
if the listed lexical units (network.n and web.n)
and some examples refer to the informatics do-
main, the situation described in the FrameNet
database is more general. Wikipedia instead lists
several pages that may be seen as subdomains
of NETWORK such as Computer network, So-
cial network, Telecommunications network, etc.
In the future, it may be worth modifying the sys-
tem in order to allow multiple assignments of
Wikipages for every frame.
In other cases, frame definitions seem not to
be very consistent and it is very difficult to dis-
criminate between two frames even for a human
annotator. For example, ESTIMATED VALUE and
ESTIMATING include both estimation.n as lexical
unit, but since their frame definitions are almost
the same and the other lexical units in the same
frame are not discriminative, the system links both
(F, l) pairs to the same Wikipedia article.
6 English FrameNet expansion
In the following part of the experiment, we want
to investigate to what extent the FrameNet ?
Wikipedia mapping can be effectively applied to
automatically expand the FrameNet database with
new example sentences, and eventually to acquire
new lexical units. For every (F, l) pair, we con-
sider the linked Wikipedia sense s and extract all
sentences C
s
in Wikipedia with a reference to
s. In this way, we can assume that, if s was
linked to (F, l), C
s
can be included in the exam-
ple sentences of F . This repository of sentences
is already divided by sense and can significantly
speed-up manual annotation. On the other hand,
the extracted sentences could enrich the training
set of machine learning systems for frame annota-
tion to improve the frame identification step. In
fact, this task has raised growing interest in the
NLP community, with a devoted challenge at the
last SemEval campaign (Baker et al, 2007).
This retrieval process allows also to ex-
tract from C
s
all words W
s
that have an
embedded reference to s in the form <a
href=?/wiki/Wiki Sense?...>word</a>. In this
way, W
s
are automatically included in F as new
lexical units. In this phase, redirecting links are
very useful because they automatically connect a
word or expression to its nearest sense in case
there is no specific page for this word. The infor-
mation about redirecting allows also to account for
orthographic variations of the same lexical unit,
for example collectible is redirected to collectable.
We explain the data extraction process in
the light of an example from our dataset.
Our WSD system assigned to the (F, l) pair
(WORD RELATIONS ? homonym.n) the Wikipage
http://en.wikipedia.org/wiki/Homonym.
So, we extracted from the English Wikipedia
dump all sentences where the anchor <a
href=?/wiki/Homonym?... > appears and as-
sumed that the word or multiword expression that
is linked to the Homonym site may be a good can-
didate as lexical unit for the WORD RELATIONS
frame. In this case, the example sentences were
186. Apart from homonym, the candidate lexical
units are homograph, homophone, homophonous,
homonymic, heteronym, same. Among them, only
the latter is not appropriate, even if the sentence
where it occurs is semantically connected to the
WORD RELATIONS frame: ?In Hebrew the word
?thus? has the same triconsonantal root?. Instead,
homonymic and heteronym can be acquired as
new lexical units for WORD RELATIONS, and
homograph, for which no example sentences
are provided in FrameNet, can be automatically
instantiated by a set of examples.
6.1 Experimental setup
We considered 893 frame ? lexical unit pairs as-
signed to Wikipedia pages following the algorithm
described in Section 3. We discarded 7 pairs for
which the system reported an assignment failure,
i.e. the best sense delivered is the disambigua-
281
tion page. Then we extracted a set of sentences
for every (F, l) pair as described in the previous
paragraph. Statistics about the retrieved data is re-
ported in Table 2.
English Wikipedia
(F, l) pairs 893
N. of extracted sents 964,268
Avg. sents per (F, l) 1,080
Table 2: Extracted data from English Wikipedia
6.2 Evaluation
The dimension of the extracted corpus does not
allow to carry out a comprehensive evaluation.
For this reason, we manually evaluated 1,000 sen-
tences, i.e. we considered 20 (F, l) pairs, and for
each of them we evaluated 50 sentences extracted
from our large repository. Both (F, l) pairs and
the assigned sentences were randomly selected.
In particular, the 20 (F, l) pairs do not contain
only correctly assigned pairs, in fact three of them
are wrong. Anyhow, the 20 pairs seem to be
a representative subset of the 893 pairs consid-
ered in our experiment because they include both
monosemic lexical units (gynaecology.n in MED-
ICAL SPECIALTIES) and more ambiguous ones
(club.n in the WEAPON frame).
Our evaluation shows that 78% of the sentences
were correctly linked to (F, l) pairs. This value is
higher that the mapping accuracy between (F, l)
and Wikipages reported in Section 5.2. In fact,
we noticed that even if the Wikipage assigned to
(F, l) is not the article that best corresponds to the
meaning of l in F , some sentences pointing to it
may be appropriate to express l.
As we already mentioned in Section 5.2,
the different granularity of the information en-
coded by frames and Wikipages impacts on
the output quality. For example, conversion.n
in CAUSE CHANGE has a causative meaning,
while it implies a personal process in UN-
DERGO CHANGE. The mapping, instead, links
(CAUSE CHANGE ? conversion.n) to the Reli-
gious conversion page, and all the sentences col-
lected point to religious conversion, regardless of
their causative form or not. Another characteristic
of this approach is that we can acquire new lexi-
cal units regardless of their part-of-speech, even if
we start from nominal lexical units. This proves
that we do not need to apply the initial mapping to
verbal or adjectival LUs to obtain new data for all
parts of speech. For example, we linked (MEDI-
CAL SPECIALTIES ? gynaecology.n) to the Gynae-
cology Wikipage. Consequently, we could include
the adjective gynaecologic, pointing to the Gy-
naecology page, into the MEDICAL SPECIALTIES
frame for sentences like ?Fellowship training in a
gynaecologic subspeciality can range from one to
four years?. However, this advantage can also turn
into a weakness, because gynaecologist is also
redirected to the Gynaecology page, but it belongs
to MEDICAL PROFESSIONALS and should not be
included into MEDICAL SPECIALTIES.
For the 20 (F, l) pairs considered in the given
sentences, it was possible also to retrieve 8 lex-
ical units that are not present in FrameNet, for
example billy-club for the WEAPON frame. Ex-
ploiting redirections and anchoring strategies, our
induction method can account for orthographical
variations, for example it acquires both memorize
and memorise. On the other hand, also misspelled
words may be collected, for instance gynaecolo-
gial instead of gynaecological.
7 Multilingual FrameNet expansion
One of the great advantages of Wikipedia is its
availability in several languages. The English ver-
sion is by far the most extended, but a considerable
repository of pages is available also for other lan-
guages, esp. European ones. In general, articles on
the same object in different languages are edited
independently and do not have to be translations
of one another, but are linked to each other by their
authors. In this way, the multilingual versions of
Wikipedia can be easily exploited to build compa-
rable corpora, with connected Wikipages in differ-
ent languages dealing with the same contents.
In this research step, we focus on this aspect of
Wikipedia and propose a methodology that, using
the English Wikipages as a bridge, automatically
acquires new lexical units and example sentences
also for other languages. This would represent the
starting point towards the creation of FrameNet
for new languages. Indeed, FrameNet structure
comprises a language-independent level of infor-
mation, namely frame and frame element defini-
tions, and a language-dependent one, i.e. the lex-
ical units and the example sentences. This makes
the resource particularly suitable to corpus-based
(semi) automatic creation of FrameNet for new
languages, because the descriptive part can be pre-
282
served and the language-dependent layer can be
populated with new instances in other languages
(Crespo and Buitelaar, 2008).
We apply our extraction algorithm to the Italian
Wikipedia. Since several approaches have been
experimented to (semi) automatically build Italian
FrameNet using WordNet (De Cao et al (2008)
and Tonelli and Pighin (2009)), we believe that
our new proposal to exploit Wikipedia may be of
interest in the research community. Anyhow, the
approach can be exploited in principle for every
language available in Wikipedia.
7.1 Experimental setup
Similarly to the data extraction process described
in Section 6, we consider for every (F, l) pair in
English the linked Wikipedia sense s, in English
as well. Then, we retrieve the Italian Wikipedia
sense s
i
linked to s and extract all sentences C
i
in the Italian Wikipedia dump
7
with a reference to
s
i
. In this way, we can assume that C
i
are exam-
ple sentences of F and that the words or expres-
sions W
i
in C
i
containing an embedded reference
to s
i
are good candidate lexical units of F in the
Italian FrameNet. For example, if we link http:
//en.wikipedia.org/wiki/Court to the JUDI-
CIAL BODY frame, we first retrieve the Italian
version of the site http://it.wikipedia.org/
wiki/Tribunale. Then, with a top-down strat-
egy, we further extract all Italian sentences point-
ing to the Tribunale page and acquire as lexi-
cal units all words with an embedded reference to
this concept, for example tribunale and corte. In
this way, we can include the extracted lexical units
and the sentences where they occur in the JUDI-
CIAL BODY frame for Italian.
Given the 893 (F, l) pairs in English and the
linked Wikipedia senses described in 6.2, we first
extracted the Italian Wikipages that are linked to
the English ones. Then for every linked Wikipage
in Italian, we retrieved all sentences with a refer-
ence pointing to that page in the Italian Wikipedia
dump. Statistics about the extracted data are re-
ported in Table 3.
Since the Italian Wikipedia is about one fifth of
the English one, it was not possible to map ev-
ery English Wikipage with an Italian article. In
fact, only 371 senses out of 893 in English were
linked to an Italian page. Also the average num-
7
http://download.wikimedia.org/itwiki/
20090203
Italian Wikipedia
Linked Wikipages in Italian 371
N. of extracted sents 23,078
Avg. sents per Italian sense 62
Table 3: Extracted data from Italian Wikipedia
ber of sentences extracted for every sense is much
smaller (62 vs. 1,080). Anyhow, this does not rep-
resent a problem because in the English FrameNet,
the lexical units whose annotation is considered
to be complete are usually instantiated by set of
20 annotated sentences on average. So, according
to the FrameNet standard, 60 sentences are more
than enough to represent the meaning of a lexical
unit in a frame.
7.2 Evaluation
In this evaluation part, we took into account 1,000
sentences, in order to have a comparable dataset
w.r.t. the evaluation for English. However, the sets
of Italian sentences extracted for every (F, l), i.e.
for every Wikipedia article, were much smaller,
so we increased the number of randomly chosen
(F, l) pairs to 80. Our evaluation is focused on the
quality of the sentences and aims at assessing if the
given sentences are correctly assigned to the (F, l)
pairs. We report 69% accuracy, which is 9% lower
than for English. Apart from the same errors and
issues reported for English, a decrease in perfor-
mance can be explained by the fact that, since less
articles are present w.r.t. the English version, redi-
rections and internal links tend to be less precise
and fine-grained. For example, the word ?diritti?
in the sense of ?(human) rights? redirects to the ar-
ticle about Diritto, corresponding to Law as a sys-
tem of rules. On the contrary, Law and Rights have
two different pages in English. Besides, the differ-
ent quality of the two resources can also depend
on the smaller number of users that edit and check
the Italian articles. From the 1,000 sentences eval-
uated we extracted 145 new lexical units: since
Italian FrameNet does not exist yet, every lexical
unit in a sentence that is correct can be straightfor-
wardly included in the first version of the resource.
8 Conclusions and Future work
In this work, we have proposed to apply a
word sense disambiguation system to a new
task, namely the linking between FrameNet and
Wikipedia. Results are promising and show that
283
the task is adequately substantiated. The proposed
approach can help enriching FrameNet with new
example sentences and lexical units and provide a
starting point for the creation of FrameNet-like re-
sources in all Wikipedia languages. On the one
hand, the retrieved data could speed up human
annotation, requiring only a manual validation.
On the other hand, the extracted sentences could
provide enough training data to machine learning
systems for frame assignment, since insufficient
frame attestations in the FrameNet database are a
major problem for such systems.
In the next research step, we plan to carry out an
extended evaluation process in order to compute
inter-annotator agreement and eventually point out
validation problems. Then, we want to extend
the mapping and the data extraction process to all
(F, l) pairs in FrameNet (about 10,000). The re-
trieved sentences will be made available as train-
ing or annotation material. Besides, we want
to create an online resource where the links be-
tween (F, l) pairs and Wikipages are made explicit
and where users can browse the retrieved sen-
tences. The resource can be produced and made
available with a reduced effort for every language
in Wikipedia. Anyway, the English version has
proved to be more precise, while the resource for
new languages would require a more accurate re-
vision.
Acknowledgments
Claudio Giuliano is supported by the ITCH
project (http://itch.fbk.eu), sponsored
by the Italian Ministry of University and Re-
search and by the Autonomous Province of
Trento and the X-Media project (http://www.
x-media-project.org), sponsored by the
European Commission as part of the Information
Society Technologies (IST) programme under EC
grant number IST-FP6-026978.
References
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 10: Frame Semantic
Structure Extraction. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 99?104, Prague, CZ, June.
Aljoscha Burchardt and Anette Frank. 2006. Approxi-
mating Textual Entailment with LFG and FrameNet
Frames. In Proceedings of the 2nd PASCAL RTE
Workshop, pages 92?97, Venice, Italy.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining Word Sense
and Usage for modeling Frame Semantics. In Pro-
ceedings of STEP 2008, Venice, Italy.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/
?
cjlin/libsvm.
Olivier Chapelle, Bernhard Sch?olkopf, and Alexander
Zien. 2006. Semi-Supervised Learning. MIT Press,
Cambridge, MA.
Ian Chow and Jonathan Webster. 2007. Integra-
tion of Linguistic Resources for Verb Classification:
FrameNet Frame, WordNet Verb and Suggested Up-
per Merged Ontology. Computational Linguistics
and Intelligent Text Processing, pages 1?11.
Mario Crespo and Paul Buitelaar. 2008. Domain-
specific English-to-Spanish Translation of
FrameNet. In Proc. of LREC 2008, Marrakech.
Andras Csomai and Rada Mihalcea. 2008. Linking
Documents to Encyclopedic Knowledge. IEEE In-
telligent Systems, special issue on ?Natural Lan-
guage Processing for the Web?.
Silviu Cucerzan. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June. Association for Computational Linguistics.
Katrin Erk. 2004. Frame assignment as Word
Sense Disambiguation. In Proceedings of IWCS-6,
Tilburg, NL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
C.J. Fillmore, C.R. Johnson, and M. R. L. Petruck.
2003. Background to FrameNet. International
Journal of Lexicography, 16:235?250, September.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual ACL meeting, pages 929?936, Morris-
town, US.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005.
Domain kernels for word sense disambiguation. In
Proceedings of the 43
rd
annual meeting of the As-
sociation for Computational Linguistics (ACL-05),
pages 403?410, Ann Arbor, Michigan, June.
R. Johansson and P. Nugues. 2007. Using Word-
Net to extend FrameNet coverage. In Proc. of the
Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, at NODAL-
IDA, Tartu.
284
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The Role of Domain Information
in Word Sense Disambiguation. Natural Language
Engineering, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3, Barcelona, Spain, July.
David Milne and Ian H. Witten. 2008. Learning to
link with Wikipedia. In CIKM ?08: Proceedings of
the 17th ACM conference on Information and knowl-
edge management, pages 509?518, NY, USA. ACM.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 Task-17: En-
glish Lexical Sample, SRL and All Words. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 87?
92, Prague, Czech Republic, June. Association for
Computational Linguistics.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge Univer-
sity Press.
Lei Shi and Rada Mihalcea. 2004. Open Text Seman-
tic Parsing Using FrameNet and WordNet. In Pro-
ceedings of HLT-NAACL 2004.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings of
CICLing 2005, pages 100?111. Springer.
Sara Tonelli and Daniele Pighin. 2009. New features
for FrameNet - WordNet Mapping. In Proceedings
of the Thirteenth Conference on Computational Nat-
ural Language Learning, Boulder, CO, USA.
285
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 141?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-IRST: Kernel Methods for Semantic Relation Extraction
Claudio Giuliano and Alberto Lavelli and Daniele Pighin and Lorenza Romano
FBK-IRST, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Povo (TN), ITALY
{giuliano,lavelli,pighin,romano}@itc.it
Abstract
We present an approach for semantic rela-
tion extraction between nominals that com-
bines shallow and deep syntactic processing
and semantic information using kernel meth-
ods. Two information sources are consid-
ered: (i) the whole sentence where the re-
lation appears, and (ii) WordNet synsets and
hypernymy relations of the candidate nom-
inals. Each source of information is rep-
resented by kernel functions. In particu-
lar, five basic kernel functions are linearly
combined and weighted under different con-
ditions. The experiments were carried out
using support vector machines as classifier.
The system achieves an overall F1 of 71.8%
on the Classification of Semantic Relations
between Nominals task at SemEval-2007.
1 Introduction
The starting point of our research is an approach
for identifying relations between named entities ex-
ploiting only shallow linguistic information, such as
tokenization, sentence splitting, part-of-speech tag-
ging and lemmatization (Giuliano et al, 2006). A
combination of kernel functions is used to represent
two distinct information sources: (i) the global con-
text where entities appear and (ii) their local con-
texts. The whole sentence where the entities appear
(global context) is used to discover the presence of
a relation between two entities. Windows of limited
size around the entities (local contexts) provide use-
ful clues to identify the roles played by the entities
within a relation (e.g., agent and target of a gene in-
teraction). In the task of detecting protein-protein
interactions, we obtained state-of-the-art results on
two biomedical data sets. In addition, promising re-
sults have been recently obtained for relations such
as work for and org based in in the news domain1.
In this paper, we investigate the use of the above
approach to discover semantic relations between
nominals. In addition to the original feature rep-
resentation, we have integrated deep syntactic pro-
cessing of the global context and semantic informa-
tion for each candidate nominals using WordNet as
external knowledge source. Each source of informa-
tion is represented by kernel functions. A tree kernel
(Moschitti, 2004) is used to exploit the deep syn-
tactic processing obtained using the Charniak parser
(Charniak, 2000). On the other hand, bag of syn-
onyms and hypernyms is used to enhance the repre-
sentation of the candidate nominals. The final sys-
tem is based on five basic kernel functions (bag-of-
words kernel, global context kernel, tree kernel, su-
persense kernel, bag of synonyms and hypernyms
kernel) linearly combined and weighted under dif-
ferent conditions. The experiments were carried out
using support vector machines (Vapnik, 1998) as
classifier.
We present results on the Classification of Seman-
tic Relations between Nominals task at SemEval-
2007, in which sentences containing ordered pairs
of marked nominals, possibly semantically related,
have to be classified. On this task, we achieve an
overall F1 of 71.8% (B category evaluation), largely
outperforming all the baselines.
1These results appear in a paper currently under revision.
141
2 Kernel Methods for Relation Extraction
In order to implement the approach based on syntac-
tic and semantic information, we employed a linear
weighted combination of kernels, using support vec-
tor machines as classifier. We designed two families
of basic kernels: syntactic kernels and semantic ker-
nels. These basic kernels are combined by exploit-
ing the closure properties of kernels. We define our
composite kernel KC(x1, x2) as follows
n
?
i=1
wi
Ki(x1, x2)
?
Ki(x1, x1)Ki(x2, x2)
, (1)
where each basic kernel Ki is normalized and wi ?
{0, 1} is the kernel weight. The normalization factor
plays an important role in allowing us to integrate in-
formation from heterogeneous knowledge sources.
All basic kernels, but the tree kernel (see Section
2.1.3), are explicitly calculated as follows
Ki(x1, x2) = ??(x1), ?(x2)?, (2)
where ?(?) is the embedding vector. Even though
the resulting feature space has high dimensionality,
an efficient computation of Equation 2 can be carried
out explicitly since the input representations defined
below are extremely sparse.
2.1 Syntactic Kernels
Syntactic kernels are defined over the whole sen-
tence where the candidate nominals appear.
2.1.1 Global Context Kernel
Bunescu and Mooney (2005) and Giuliano et al
(2006) successfully exploited the fact that relations
between named entities are generally expressed us-
ing only words that appear simultaneously in one of
the following three contexts.
Fore-Between Tokens before and between the two
entities, e.g. ?the head of [ORG], Dr. [PER]?.
Between Only tokens between the two entities, e.g.
?[ORG] spokesman [PER]?.
Between-After Tokens between and after the two
entities, e.g. ?[PER], a [ORG] professor?.
Here, we investigate whether this assumption is
also correct for semantic relations between nomi-
nals. Our global context kernel operates on the con-
texts defined above, where each context is repre-
sented using a bag-of-words. More formally, given
a) S1
S
NP
PRP
I
VP
VBD
found
NP
DT
some
NN
candy
PP
IN
in
NP
PRP$
my
NN
underwear
.
.
b) S
VP
VBD
found
NP
NNS
agent
PP
IN
in
NP
NN
target
Figure 1: A content-container relation test sentence
parse tree (a) and the corresponding RT structure (b).
a relation example R, we represent a context C as a
row vector
?C(R) = (tf(t1, C), tf(t2, C), . . . , tf(tl, C)) ? Rl, (3)
where the function tf(ti, C) records how many
times a particular token ti is used in C . Note that
this approach differs from the standard bag-of-words
as punctuation and stop words are included in ?C ,
while the nominals are not. To improve the classi-
fication performance, we have further extended ?C
to embed n-grams of (contiguous) tokens (up to n =
3). By substituting ?C into Equation 2, we obtain
the n-gram kernel Kn, which counts uni-grams, bi-
grams, . . . , n-grams that two patterns have in com-
mon2. The Global Context kernel KGC(R1, R2) is
then defined as
KF B(R1, R2) +KB(R1, R2) +KBA(R1, R2), (4)
where KFB , KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
2.1.2 Bag-of-Words Kernel
The bag-of-words kernel is defined as the previ-
ous kernel but it operates on the whole sentence.
2.1.3 Tree Kernel
Tree kernels can trigger automatic feature selec-
tion and represent a viable alternative to the man-
2In the literature, it is also called n-spectrum kernel.
142
ual design of attribute-value syntactic features (Mos-
chitti, 2004). A tree kernel KT (t1, t2) evaluates
the similarity between two trees t1 and t2 in terms
of the number of fragments they have in common.
Let Nt be the set of nodes of a tree t and F =
{f1, f2, . . . , f|F|} be the fragment space of t1 and
t2. Then
KT (t1, t2) =
P
ni?Nt1
P
nj?Nt2
?(ni, nj) , (5)
where ?(ni, nj) =
?|F|
k=1 Ik(ni) ? IK(nj) and
Ik(n) = 1 if k is rooted in n, 0 otherwise.
For this task, we defined an ad-hoc class of struc-
tured features (Moschitti et al, 2006), the Reduced
Tree (RT), which can be derived from a sentence
parse tree t by the following steps: (1) remove all the
terminal nodes but those labeled as relation entities
and those POS tagged as verbs, auxiliaries, prepo-
sitions, modals or adverbs; (2) remove all the in-
ternal nodes not covering any remaining terminal;
(3) replace the entity words with placeholders that
indicate the direction in which the relation should
hold. Figure 1 shows a parse tree and the resulting
RT structure.
2.2 Semantic Kernels
In (Giuliano et al, 2006), we used the local context
kernel to infer semantic information on the candi-
date entities (i.e., roles played by the entities). As
the task organizers provide the WordNet sense and
role for each nominal, we directly use this informa-
tion to enrich the feature space and do not include
the local context kernel in the combination.
2.2.1 Bag of Synonyms and Hypernyms Kernel
By using the WordNet sense key provided, each
nominal is represented by the bag of its synonyms
and hypernyms (direct and inherited hypernyms).
Formally, given a relation example R, each nominal
N is represented as a row vector
?N(R) = (f(t1, N), f(t2, N), . . . , f(tl, N)) ? Rl, (6)
where the binary function f(ti, N) records if a par-
ticular lemma ti is contained into the bag of syn-
onyms and hypernyms of N. The bag of synonyms
and hypernyms kernel KS&H(R1, R2) is defined as
Ktarget(R1, R2) +Kagent(R1, R2), (7)
where Ktarget and Kagent are defined by substitut-
ing the embedding of the target and agent nominals
into Equation 2 respectively.
2.2.2 Supersense Kernel
WordNet synsets are organized into 45 lexicogra-
pher files, based on syntactic category and logical
groupings. E.g., noun.artifact is for nouns denoting
man-made objects, noun.attribute for nouns denot-
ing attributes for people and objects etc. The super-
sense kernel KSS(R1, R2) is a variant of the previ-
ous kernel that uses the names of the lexicographer
files (i.e., the supersense) to index the feature space.
3 Experimental Setup and Results
Sentences have been tokenized, lemmatized, and
POS tagged with TextPro3. We considered each re-
lation as a different binary classification task, and
each sentence in the data set is a positive or negative
example for the relation. The direction of the rela-
tion is considered labelling the first argument of the
relation as agent and the second as target.
All the experiments were performed using the
SVM package SVMLight-TK4, customized to em-
bed our own kernels. We optimized the linear com-
bination weights wi and regularization parameter c
using 10-fold cross-validation on the training set.
We set the cost-factor j to be the ratio between the
number of negative and positive examples.
Table 1 shows the performance on the test set. We
achieve an overall F1 of 71.8% (B category evalua-
tion), largely outperforming all the baselines, rang-
ing from 48.5% to 57.0%. The average training plus
test running time for a relation is about 10 seconds
on a Intel Pentium M755 2.0 GHz. Figure 2 shows
the learning curves on the test set. For all relations
but theme-tool, accurate classifiers can be learned
using a small fraction of training.
4 Discussion and Conclusion
Experimental results show that our kernel-based ap-
proach is appropriate also to detect semantic rela-
tions between nominals. However, differently from
relation extraction between named entities, there is
not a common kernel setup for all relations. E.g.,
3http://tcc.itc.it/projects/textpro/
4http://ai-nlp.info.uniroma2.it/moschitti/
143
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 30  40  50  60  70  80  90  100
F 1
Percentage of Training
Learning Curve
Cause-Effect
Instrument-Agency
Product-Producer
Origin-Entity
Theme-Tool
Part-Whole
Content-Container
Figure 2: Learning curves on the test set.
Relation P R F1 Acc
Cause-Effect 67.3 90.2 77.1 72.5
Instrument-Agency 76.9 78.9 77.9 78.2
Product-Producer 76.2 77.4 76.8 68.8
Origin-Entity 62.2 63.9 63.0 66.7
Theme-Tool 69.2 62.1 65.5 73.2
Part-Whole 65.5 73.1 69.1 76.4
Content-Container 78.8 68.4 73.2 74.3
Avg 70.9 73.4 71.8 72.9
Table 1: Results on the test set.
for content-container we obtain the best perfor-
mance combining the tree kernel and the bag of syn-
onyms and hypernyms kernel; on the other hand, for
instrument-agency the best performance is obtained
by combining the global kernel and the supersense
kernel. Surprisingly, the supersense kernel alone
works quite well and obtains results comparable to
the bag of synonyms and hypernyms kernel. This
result is particularly interesting as a supersense tag-
ger can easily provide a satisfactory accuracy (Cia-
ramita and Altun, 2006). On the other hand, ob-
taining an acceptable accuracy in word sense disam-
biguation (required for a realistic application of the
bag of synonyms and hypernyms kernel) is imprac-
tical as a sufficient amount of training for at least all
nouns is currently not available. Hence, the super-
sense could play a crucial role to improve the perfor-
mance when approaching this task without the nomi-
nals disambiguated. To model the global context us-
ing the Fore-Between, Between and Between-After
contexts did not produce a significant improvement
with respect to the bag-of-words model. This is
mainly due to the fact that examples have been col-
lected from the Web using heuristic patterns/queries,
most of which implying Between patterns/contexts
(e.g., for the cause-effect relation ?* comes from *?,
?* out of *? etc.).
5 Acknowledgements
Claudio Giuliano, Alberto Lavelli and Lorenza Ro-
mano are supported by the X-Media project (http:
//www.x-media-project.org), sponsored
by the European Commission as part of the Infor-
mation Society Technologies (IST) programme un-
der EC grant number IST-FP6-026978.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Subse-
quence kernels for relation extraction. In Proceedings
of the 19th Conference on Neural Information Pro-
cessing Systems, Vancouver, British Columbia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132?139, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-2006), Trento, Italy, 5-7 April.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
CoNLL-X.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 335?
342, Barcelona, Spain, July.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York, NY.
144
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 201?209,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Fine-Grained Classification of Named Entities
Exploiting Latent Semantic Kernels
Claudio Giuliano
FBK-irst
I-38100, Trento, Italy
giuliano@fbk.eu
Abstract
We present a kernel-based approach for fine-
grained classification of named entities. The
only training data for our algorithm is a few
manually annotated entities for each class. We
defined kernel functions that implicitly map
entities, represented by aggregating all con-
texts in which they occur, into a latent seman-
tic space derived from Wikipedia. Our method
achieves a significant improvement over the
state of the art for the task of populating an
ontology of people, although requiring con-
siderably less training instances than previous
approaches.
1 Introduction
Populating an ontology with relevant entities ex-
tracted from unstructured textual documents is a
crucial step in Semantic Web and knowledge man-
agement systems. As the concepts in an ontology
are generally arranged in deep class/subclass hierar-
chies, the problem of populating ontologies is typi-
cally solved top-down, firstly identifying and classi-
fying entities in the most general concepts, and then
refining the classification process.
Recent advances have made supervised ap-
proaches very successful in entity identification and
classification. However, to achieve satisfactory per-
formance, supervised systems must be supplied with
a sufficiently large amount of training data, usually
consisting of hand tagged texts. As domain specific
ontologies generally contains hundreds of subcate-
gories, such approaches are not directly applicable
for a more fine-grained categorization because the
number of documents required to find sufficient pos-
itive examples for all subclasses becomes too large,
making the manual annotation very expensive.
Consequently, in the literature, supervised ap-
proaches are confined to classify entities into broad
categories, such as persons, locations, and or-
ganizations, while the fine-grained classification
has been approached with minimally supervised
(e.g., Tanev and Magnini (2006) and Giuliano and
Gliozzo (2008)) and unsupervised learning algo-
rithms (e.g., Cimiano and Vo?lker (2005) and Giu-
liano and Gliozzo (2007)).
Following this trend, we present a minimally su-
pervised approach to fine-grained categorization of
named entities previously recognized into coarse-
grained categories, e.g., by a named-entity recog-
nizer. The only training data for our algorithm is a
few manually annotated entities for each class. For
example, Niels Bohr, Albert Einstein, and Enrico
Fermi might be used as examples for the class physi-
cists. In some cases, training entities can be acquired
(semi-) automatically from existing ontologies al-
lowing us to automatically derive training entities
for use with our machine learning algorithm. For
instance, we may easily obtain tens of training en-
tities for very specific classes, such as astronomers,
materials scientists, nuclear physicists, by querying
the Yago ontology (Suchanek et al, 2008).
We represent the entities using features extracted
from the textual contexts in which they occur.
Specifically, we use a search engine to collect such
contexts from the Web. Throughout this paper, we
will refer to such a representation as multi-context
representation, in contrast to the single-context rep-
201
resentation in which an entity is categorized using
solely features extracted from the local context sur-
rounding it, usually a window of a few words around
the entity occurrence. Single-context features are
commonly used in named-entity recognition, how-
ever to assign very specific categories the local con-
text might not provide sufficient information. For
example, in the sentence ?Prof. Enrico Fermi dis-
covered a way to induce artificial radiation in heavy
elements by shooting neutrons into their atomic nu-
clei,? single-context features such as, the prefix Prof.
and the capital letters, provides enough evidence that
Enrico Fermi is a person and a professor. However,
to discover that he is a physicist we need to analyze
a wider context, or alternatively multiple ones. Re-
cently, Ganti et al (2008) has shown that exploiting
multi-context information can greatly improve the
fine-grained classification of named entities, when
compared to methods using single context only.
In order to effectively represent entities? multi-
contexts, we extend the traditional vector space
model (VSM), offering a way to integrate external
semantic information in the classification process by
means of latent semantic kernels (Shawe-Taylor and
Cristianini, 2004). As a result, we obtain a general-
ized similarity function between multi-contexts that
incorporates semantic relations between terms, auto-
matically learned from unlabeled data. In particular,
we use Wikipedia to build the latent semantic space.
The underlying idea is that similar named entities
tend to have a similar description in Wikipedia. As
Wikipedia provides reliable information and it ex-
ceeds all other encyclopedias in coverage, it should
be a valuable resource for the task of populating an
ontology. To validate this hypothesis, we compare
this model with one built from a news corpus.
Our approach achieves a significant improvement
over the state of the art for the task of populating the
People Ontology (Giuliano and Gliozzo, 2008), al-
though requiring considerably less training instances
than previous approaches. The task consists in clas-
sifying person names into a multi-level taxonomy
composed of 21 categories derived from WordNet,
making very fine-grained distinctions (e.g., physi-
cists vs. mathematicians). It provides a more real-
istic and challenging benchmark than the ones pre-
viously available (e.g., Tanev and Magnini (2006)
and Fleischman and Hovy (2002)), that consider a
smaller number of categories arranged in a one-level
taxonomy.
2 Entity Representation
The goal of our research is to determine the fine-
grained categories of named entities requiring a min-
imal amount of human supervision.
Our method is based on the common assump-
tion that named entities co-occurring with the same
(domain-specific) terms are highly probable to refer
to the same categories. For example, quantum me-
chanics, atomic physics, and Nobel Prize in physics
are all terms that bound Niels Bohr and Enrico Fermi
to the concept of physics.
To automatically derive features for the training
and testing entities we proceed as follows. We pair
each entity i with a multi-context mi obtained by
querying a search engine with the entity ?i? and
merging the first M snippets si,j returned (1 6 j 6
M ). A multi-context is therefore a fictitious doc-
ument obtained by aggregating snippets, i.e., sum-
mary texts of the search engine result. Formally,
mi = ?Mj=1si,j , where the operator ? denotes the
concatenation of strings. For example, Figure 1 (a)
and (b) show some snippets retrieved for ?Enrico
Fermi? and ?Albert Einstein,? while s1? s2? s3 and
s4 ? s5 ? s6 represent their multi-contexts, respec-
tively.
The following section describes how entities?
multi-contexts are embedded into the feature space
in order to train a kernel-based classifier.
3 Kernels for Fine-Grained Classification
of Entities
The strategy adopted by kernel methods (Shawe-
Taylor and Cristianini, 2004; Scho?lkopf and Smola,
2002) consists of splitting the learning problem in
two parts. They first embed the input data in a suit-
able feature space, and then use a linear algorithm
(e.g., the perceptron) to discover nonlinear pattern in
the input space. Typically, the mapping is performed
implicitly by a so-called kernel function. The ker-
nel function is a similarity measure between the in-
put data that depends exclusively on the specific data
type and domain. A typical similarity function is the
inner product between feature vectors. Characteriz-
ing the similarity of the inputs plays a crucial role in
202
s1: [Enrico Fermi]PER discovered that many nuclear transformations could be conducted by using neutrons.
s2: [Enrico Fermi]PER led the manhattan project?s effort to create the first man-made and self-sustaining nuclear chain.
s3: [Enrico Fermi]PER was most noted for his work on the development of the first nuclear reactor.
(a)
s4: [Albert Einstein]PER did not directly participate in the invention of the atomic bomb.
s5: [Albert Einstein]PER is one of the most recognized and well-known scientists of the century.
s6: [Albert Einstein]PER was born at Ulm, in Wu?rttemberg, Germany, on March 14, 1879.
(b)
Figure 1: Examples of snippets retrieved for Enrico Fermi (a) and Albert Einstein (b).
determining the success or failure of the learning al-
gorithm, and it is one of the central questions in the
field of machine learning.
Formally, the kernel is a function k : X?X ? R
that takes as input two data objects (e.g., vectors,
texts, parse trees) and outputs a real number charac-
terizing their similarity, with the property that the
function is symmetric and positive semi-definite.
That is, for all xi, xj ? X , it satisfies
k(xi, xj) = ??(xi), ?(xj)? (1)
where ? is an explicit mapping from X to an (inner
product) feature space F .
In the next sections, we define and combine differ-
ent kernel functions that calculate the pairwise sim-
ilarly between multi-contexts. They are the only do-
main specific element of our classification system,
while the learning algorithm is a general purpose
component. Many classifiers can be used with ker-
nels. The most popular ones are perceptron, sup-
port vector machines (SVM), and k-nearest neighbor
(KNN).
3.1 Bag-of-Words Kernel
The simplest method to estimate the similarity be-
tween two multi-contexts is to compute the inner
product of their vector representations in the VSM.
Formally, we define a space of dimensionality N in
which each dimension is associated with one word
from the dictionary, and the multi-context m is rep-
resented by a row vector
?(m) = (f(t1,m), f(t2,m), . . . , f(tN ,m)), (2)
where the function f(ti,m) records whether a par-
ticular token ti is used in m. Using this representa-
tion we define bag-of-words kernel between multi-
contexts as
KBOW (m1,m2) = ??(m1), ?(m2)? (3)
However, the bag-of-words representation does
not deal well with lexical variability. To significantly
reduce the training set size, we need to map contexts
containing semantically equivalent terms into simi-
lar feature vectors. To this aim, in the next section,
we introduce the class of semantic kernels and show
how to define an effective semantic VSM using (un-
labeled) external knowledge.
3.2 Semantic Kernels
It has been shown that semantic information is fun-
damental for improving the accuracy and reducing
the amount of training data in many natural language
tasks, including fine-grained classification of named
entities (Fleischman and Hovy, 2002), question clas-
sification (Li and Roth, 2005), text categorization
(Giozzo and Strapparava, 2005), word sense disam-
biguation (Gliozzo et al, 2005).
In the context of kernel methods, semantic infor-
mation can be integrated considering linear trans-
formations of the type ??(cj) = ?(cj)S, where S
is a N ? k matrix (Shawe-Taylor and Cristianini,
2004). The matrix S can be rewritten as S = WP,
where W is a diagonal matrix determining the word
weights, while P is the word proximity matrix cap-
turing the semantic relations between words. The
proximity matrix P can be defined by setting non-
zero entries between those words whose semantic
relation is inferred from an external source of do-
main knowledge. The semantic kernel takes the gen-
eral form
k?(mi,mj) = ?(mi)SS??(mj)? = ??(mi)??(mj)?. (4)
It follows directly from the explicit construction that
Equation 4 defines a valid kernel.
WordNet and manually constructed lists of se-
mantically related words typically provide a sim-
ple way to introduce semantic information into the
203
kernel. To define a semantic kernel from such re-
sources, we could explicitly construct the proximity
matrix P by setting its entries to reflect the semantic
proximity between the words i and j in the specific
lexical resource. However, we prefer an approach
that exploits unlabeled data to automatically build
the proximity matrix, defining a language and do-
main independent approach.
3.2.1 Latent Semantic Kernel
To define a proximity matrix, we look at co-
occurrence information in a (large) corpus. Two
words are considered semantically related if they
frequently co-occur in the same texts. We use sin-
gular valued decomposition (SVD) to automatically
derive the proximity matrix ? from a corpus, rep-
resented by its term-by-document matrix D, where
the Di,j entry gives the frequency of term ti in doc-
ument dj .1 SVD decomposes the term-by-document
matrix D into three matrixes D = U?V?, where U
and V are orthogonal matrices (i.e., U?U = I and
V?V = I) whose columns are the eigenvectors of
DD? and D?D respectively, and ? is the diagonal
matrix containing the singular values of D.
Under this setting, we define the proximity matrix
? as follows:
? = Uk?k, (5)
where Uk is the matrix containing the first k
columns of U and k is the dimensionality of the la-
tent semantic space and can be fixed in advance. By
using a small number of dimensions, we can define a
very compact representation of the proximity matrix
and, consequently, reduce the memory requirements
while preserving most of the information.
The matrix ? is used to define a linear transfor-
mation pi : RN ? Rk, that maps the vector ?(mj),
represented in the standard VSM, into the vector
??(mj) in the latent semantic space. Formally, pi is
defined as follows
pi(?(mj)) = ?(mj)(W?) = ??(mj), (6)
where ?(mj) is a row vector, W is a N ? N diag-
onal matrix determining the word weights such that
Wi,i = log(idf(wi)), where idf(wi) is the inverse
document frequency of wi.
1SVD has been first applied to perform latent semantic anal-
ysis of terms and latent semantic indexing of documents in large
corpora by Deerwester et al (1990).
Finally, the latent semantic kernel is explicitly de-
fined as follows
KLS(mi,mj) = ?pi(?(mi)), pi(?(mj))?, (7)
where ? is the mapping defined in Equation 2 and
pi is the linear transformation defined in Equation 6.
Note that we have used a series of successive map-
pings each of which adds some further improvement
to the multi-context representation.
3.3 Composite Kernel
Finally, to combine the two representations of multi-
contexts, we define the composite kernel as follows
KBOW (m1,m2) +KLS(m1,m2). (8)
It follows directly from the explicit construction of
the feature space and from closure properties of ker-
nels that it is a valid kernel.
4 Experiments
In this section, we compare performance of different
kernel setups and previous approaches on an ontol-
ogy population task.
4.1 Benchmark
Experiments were carried out on the People Ontol-
ogy (Giuliano and Gliozzo, 2008). An ontology
extracted from WordNet, containing 1,657 distinct
person instances arranged in a multi-level taxonomy
having 21 fine-grained categories (Figure 2). To pro-
vide a formal distinction between classes and in-
stances, required to assign instances to classes, the
authors followed the directives defined by Gangemi
et al (2003) for OntoWordNet, in which the infor-
mal WordNet semantics is re-engineered in terms of
a description logic.
In order to have a fair comparison, we reproduced
the same experimental settings used in Giuliano and
Gliozzo (2008). The population task is cast as a cate-
gorization problem, trying to assign person instances
to the most specific category. For each class, the in-
stances were randomly split into two equally sized
subsets. One is used for training and the other for
test, and vice versa. The reported results are the av-
erage performance over these two subsets. When an
instance is assigned to a sub-class it is also implic-
itly assigned to all its super-classes. For instance,
204
Figure 2: The People Ontology defined by Giuliano and Gliozzo (2008). Numbers in brackets are the total numbers
of person instances per category. Concepts with less than 40 instances were removed.
classifying Salvador Dali as painter we implicitly
classify him as artist and creator. The evaluation
is performed as proposed by Melamed and Resnik
(2000) for a similar hierarchical categorization task.
For instance, classifying John Lennon as painter, we
obtain a false positive for the spurious classification
painter, a false negative for missing class musician,
and two true positives for the correct assignment to
the super-classes artist and creator.
4.2 Experimental Settings
We built two proximity matrices ?W and ?NY T .
The former is derived from the 200,000 most visited
Wikipedia articles, while the latter from 200,000 ar-
ticles published by the New York Times between
June 1, 1998 and January 01, 2000. After remov-
ing terms that occur less than 5 times, the result-
ing dictionaries contain about 300,000 and 150,000
terms respectively. We used the SVDLIBC pack-
age2 to compute the SVD, truncated to 400 dimen-
sions. To derive the multi-context representation, we
collected 100 english snippets for each person in-
stance by querying GoogleTM. To classify each per-
son instance into one of the fine-grained categories,
we used a KNN classifier (K = 1). No parameter
optimization was performed.
2http://tedlab.mit.edu/?dr/svdlibc/
4.3 Results
Table 1 shows micro- and macro-averaged results
for KBOW , KW , KBOW +KW , KNY T , KBOW +
KNY T , the IBOP method (Giuliano and Gliozzo,
2008), the random baseline, and most frequent base-
line.3 Where KW and KNY T are instances of the
latent semantic kernel, KLS , using the proximity
matrices ?W and ?NY T , derived from Wikipedia
and the New York Times corpus, respectively. Ta-
ble 2 shows detailed results for each sub- and super-
category for KBOW +KW . Table 3 shows the con-
fusion matrix of KBOW + KW , in which the rows
are ground truth classes and the columns are predic-
tions. The matrix has been calculated for the finer-
grained categories and, then, grouped according to
their super-class. To be compared with the IBOP
method, all experiments were conducted using only
20 training examples per category. Finally, figure
3 shows the learning curves for KBOW + KW ob-
tained varying the number of snippets (12, 25, 50,
and 100) used to derive the multi-contexts.
4.4 Discussion
On the one hand, the results (Table 2) show that
learning the semantic model from Wikipedia gives
no significant improvement. Therefore, we reject the
hypothesis that encyclopedic knowledge can provide
3The most frequent category has been estimated on the train-
ing data.
205
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 2  4  6  8  10  12  14  16  18  20
Mic
ro F
1
Number of instances
12 snippets
KBOWKWKBOW+KW  0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 2  4  6  8  10  12  14  16  18  20
Mic
ro F
1
Number of instances
25 snippets
KBOWKWKBOW+KW
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 2  4  6  8  10  12  14  16  18  20
Mic
ro F
1
Number of instances
50 snippets
KBOWKWKBOW+KW  0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 2  4  6  8  10  12  14  16  18  20
Mic
ro F
1
Number of instances
100 snippets
KBOWKWKBOW+KW
Figure 3: Learning curves for KBOW +KW obtained varying the number of snippets used to derive the training and
test sets. From top-left to bottom right: 12, 25, 50, and 100.
Method Micro-F1 Macro-F1
KBOW 75.6 70.6
KW 78.1 73.1
KBOW +KW 80.0 75.4
KNY T 77.6 72.9
KBOW + KNY T 79.7 75.1
IBOP 70.1 62.3
Random 15.4 15.5
Most Frequent 20.7 3.3
Table 1: Comparison among the kernel-based ap-
proaches, the IBOP method (Giuliano and Gliozzo,
2008), the random baseline, and most frequent baseline.
more accurate semantic models than general pur-
pose corpora. Moreover, further experiments have
shown that even a larger number of Wikipedia ar-
ticles (600,000) does not help. On the other hand,
the latent semantic kernels outperform all the other
methods, and their composite (KBOW + KW and
KBOW + KNY T ) perform the best on every con-
figuration, demonstrating the effectiveness of la-
tent semantic kernels in fine-grained classification
of named entities. As in text categorization and
word sense disambiguation, they have proven effec-
tive tools to overcome the limitation of the VSM by
introducing semantic similarity among words.
An important characteristic of the approach is the
small number of training examples required per cat-
egory. This affects both the prediction accuracy and
the computation time (this is generally a common
property of instance-based algorithms). The learn-
ing curves (Figure 3) show that the composite ker-
nel (KBOW +KLS) obtained the same performance
of the bag-of-word kernel (KBOW ) using less than
half of the training examples per category. The
difference is much more pronounced when using
less snippets. The composite kernel KBOW + KW
reaches a plateau around 10 examples, and after 20
examples adding more examples does not signifi-
cantly improve the classification performance.
As most of entities in the People Ontol-
ogy are celebrities, all the snippets retrieved by
GoogleTMgenerally refer to them, alleviating the
problem of ambiguity of proper names. However,
person names are highly ambiguous. In a more real-
istic scenario, the result of a search engine for a per-
son name is usually a mix of contexts about different
entities sharing the same name. In this case, our ap-
proach have to be combined with a system that clus-
ters the search engine result, where each cluster is
assumed to contain all (and only those) contexts that
refer to the same entity. The WePS evaluation cam-
paign on disambiguation of person names (Artiles et
al., 2007; Artiles et al, 2009) has shown that the best
clustering systems achieve a precision of about 90%
206
Scientist Performer Creator Communicator Business Health
Phy Mat Che Bio Soc Act Mus Fil Pai Mus Poe Dra Rep man prof
Phy 118 24 10 4 2 0 0 0 0 0 0 0 0 7 2
Mat 2 33 0 0 1 0 0 0 0 0 1 0 0 3 0
Che 13 2 68 9 2 0 0 0 0 0 0 0 0 5 2
Bio 3 0 7 52 0 0 0 0 1 0 0 0 1 6 6
Soc 0 4 1 1 55 0 0 0 0 0 3 1 1 4 2
Act 0 0 0 0 0 98 5 27 0 0 2 14 0 3 0
Mus 0 0 0 0 0 17 67 0 0 32 1 0 1 2 1
Fil 0 0 0 0 0 13 0 45 0 0 1 4 0 2 0
Pai 0 0 0 1 1 2 0 1 100 0 1 0 0 1 0
Mus 0 0 0 0 0 4 29 0 0 139 0 1 0 0 0
Poe 0 2 0 0 0 0 0 0 7 3 98 26 1 2 3
Dra 0 0 0 1 1 9 0 1 0 1 12 61 1 4 1
Rep 0 0 0 0 0 1 1 0 2 0 0 0 197 22 0
Bus 1 0 1 0 1 0 0 1 0 1 0 0 1 36 0
Hea 0 0 0 8 4 0 1 0 0 0 0 1 1 2 31
Table 3: Confusion matrix of KBOW +KW for the more fine-grained categories grouped according to their top-level
concepts of the People Ontology.
Category Prec. Recall F1
Scientist 95.1 90.1 92.6
Physicist 86.1 70.7 77.6
Mathematician 50.8 82.5 62.9
Chemist 78.2 67.3 72.3
Biologist 68.4 68.4 68.4
Social scientist 82.1 76.4 79.1
Performer 75.7 69.3 72.3
Actor 68.1 65.8 66.9
Musician 65.0 55.4 59.8
Creator 78.9 82.6 80.7
Film Maker 60.0 69.2 64.3
Artist 83.6 85.4 84.5
Painter 90.9 93.5 92.2
Musician 79.0 80.3 79.7
Communicator 91.9 86.7 89.2
Representative 96.6 88.3 92.3
Writer 86.8 84.2 85.5
Poet 82.4 69.0 75.1
Dramatist 56.5 66.3 61.0
Business man 36.4 85.7 51.1
Health professional 64.6 64.6 64.6
micro 80.9 79.6 80.2
macro 75.1 76.3 75.7
Table 2: Results for each category using KBOW +KW .
and a recall of about 70% and that, in the major-
ity of the cases, the number of contexts per entity is
less than 20. This shows that latent semantic kernels
are an effective tool for fine-grained classification of
person names.
Finally, table 3 shows that misclassification er-
rors are largely distributed among categories belong-
ing to the same super-class (i.e., the blocks on the
main diagonal are more densely populated than oth-
ers). As expected, the algorithm is much more accu-
rate for the top-level concepts (i.e., Scientist, Com-
municator, etc.), where the category distinctions are
clearer, while a further fine-grained classification, in
some cases, is even difficult for human annotators.
5 Related Work
Fleischman and Hovy (2002) approach the fine-
grained classification of person instances using su-
pervised learning, where the training set is gener-
ated semi-automatically, bootstrapping from a small
training set. They compare different machine learn-
ing algorithms, providing local features as well as
global semantic information derived from topic sig-
nature and WordNet. Person instances were classi-
fied into one of eight categories.
Cimiano and Vo?lker (2005) present an approach
for the fine-grained classification of entities relying
on the Harris? distributional hypothesis and the vec-
tor space model. They assign a particular instance
to the most similar concept representing both with
lexical-syntactic features extracted from the context
of the instance and the lexicalization of the concept,
respectively. Experiments were performed using a
large ontology with 682 concepts (unfortunately not
yet available).
Tanev and Magnini (2006) proposed a weakly-
supervised method that requires as training data a
list of named entities, without context, for each cat-
egory under consideration. Given a generic syntacti-
cally parsed corpus containing at least each training
entity twice, the algorithm learns, for each category,
207
a feature vector describing the contexts where those
entities occur. Then, it compares the new (unknown)
entity with the so obtained feature vectors, assigning
it to the most similar category. Experiments are per-
formed on a benchmark of 5 sub-classes of location
and 5 sub-classes of person.
Giuliano and Gliozzo (2007) propose an unsuper-
vised approach based on lexical entailment, consist-
ing in assigning an entity to the category whose lex-
icalization can be replaced with its occurrences in
a corpus preserving the meaning. Using unsuper-
vised learning, they obtained slightly worst results
than Tanev and Magnini (2006) on the same bench-
mark.
Picca et al (2007) present an approach for on-
tology learning from open domain text collections,
based on the combination of Super Sense Tagging
and Domain Modeling techniques. The system rec-
ognizes terms pertinent to the domain and assigns
them the correct ontological type.
Giuliano and Gliozzo (2008) present an instance-
based learning algorithm for fine-grained named en-
tity classification based on syntactic features (word-
order, case-marking, agreement, verb tenses, etc.).
Their method can handle much finer distinctions
than previous methods, and it is evaluated on a hi-
erarchical taxonomy of 21 ancestors of people that
was induced from WordNet. One contribution is to
create this richer People Ontology. Another is to
make effective use of the Web 1T 5-gram corpus
(Brants and Franz, 2006) to represent syntactic in-
formation. The main difference between the two ap-
proaches lies primarily in the use of syntactic and
semantic information. Our experiments show that
semantic features do provide richer information than
syntactic ones for a more fine-grained classification
of named entities. In fact, the accuracy improve-
ment achieved by our approach is more evident for
the more specific classes. For example, the improve-
ment in accuracy is about 14% for the class scientist,
while it ranges from 25% to 46% for its sub-classes
(physicist, mathematician, etc.).
Kozareva et al (2008) propose an approach for
person name categorization based on the domain
distribution. They use the information provided by
WordNet Domains to generated lists of words rele-
vant for a given domain, by mapping and ranking the
words from the WordNet glosses to their WordNet
Domains. A named entity is then classified accord-
ing the similarity between the word-domain lists and
the global context in which the entity appears. How-
ever, the evaluation was performed only on 6 person
names using two categories.
Ganti et al (2008) present a method that considers
an entity?s context across multiple documents con-
taining it, and exploiting word n-grams and existing
large list of related entities as features. They gener-
ated training and test data using Wikipedia articles
that contain list of instances. They compare their
system with a single-context classifier, showing that
their approach based on aggregate context perform
better.
Finally, Talukdar et al (2008) propose a graph-
based semi-supervised label propagation algorithm
for acquiring open-domain labeled classes and their
instances from a combination of unstructured and
structured text.
6 Conclusions
We presented an approach to automatic fine-grained
categorization of named entities based on kernel
methods. Entities are represented by aggregating all
contexts in which they occur. We employed latent
semantic kernels to extend the bag-of-words repre-
sentation. The latent semantic models were derived
from Wikipedia and a news corpus We evaluated our
approach on the People Ontology, a multi-level on-
tology of people derived from WordNet. Although
this benchmark is still far from being ?large?, it al-
lows drawing more valid conclusions than past ones.
We significantly outperformed the previous results
on both coarse- and fine-grained classification, al-
though requiring much less training instances. From
this preliminary analysis, it appears that semantic in-
formation is much more effective that syntactic one
for this task, and deriving the semantic model from
Wikipedia gives no significant improvement, as well
as, using a larger number of Wikipedia articles.
Acknowledgments
Claudio Giuliano is supported by the X-Media project (http:
//www.x-media-project.org), sponsored by the Euro-
pean Commission as part of the Information Society Technolo-
gies (IST) programme under EC grant number IST-FP6-026978
and the ITCH project (http://itch.fbk.eu), sponsored
by the Italian Ministry of University and Research and by the
Autonomous Province of Trento.
208
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 weps evaluation: Establishing a
benchmark for the web people search task. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 64?69,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2009.
Weps 2 evaluation campaign: overview of the web
people search clustering task. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference, Madrid, Spain, April.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram corpus version 1, Linguistic Data Consortium,
Philadelphia.
Philipp Cimiano and Johanna Vo?lker. 2005. Towards
large-scale, open-domain and ontology-based named
entity classification. In Proceedings of RANLP?05,
pages 66? 166?172, Borovets, Bulgaria.
Scott C. Deerwester, Susan T. Dumais, Thoms K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics, Taipei, Taiwan.
Aldo Gangemi, Roberto Navigli, and Paola Velardi.
2003. Axiomatizing WordNet glosses in the On-
toWordNet project. In Proocedings of the Workshop
on Human Language Technology for the Semantic
Web and Web Services at ISWC 2003, Sanibel Island,
Florida.
Venkatesh Ganti, Arnd C. Ko?nig, and Rares Vernica.
2008. Entity categorization over large document col-
lections. In KDD ?08: Proceeding of the 14th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 274?282, New York,
NY, USA. ACM.
Alfio Giozzo and Carlo Strapparava. 2005. Domain ker-
nels for text categorization. In Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 56?63, Ann Arbor, Michigan, June.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 248?256.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 265?272, Manchester, UK, August.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for word sense
disambiguation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 403?410, Ann Arbor, Michigan,
June.
Zornitsa Kozareva, Sonia Vazquez, and Andres Montoyo.
2008. Domain information for fine-grained person
name categorization. In 9th International Conference
on Intelligent Text Processing and Computational Lin-
guistics (CICLing 2008), pages 311?321, Haifa, Israel,
17-23 February.
Xin Li and Dan Roth. 2005. Learning question classi-
fiers: the role of semantic information. Natural Lan-
guage Engineering, 12(3):229?249.
I. Dan Melamed and Philip Resnik. 2000. Tagger eval-
uation given hierarchical tag sets. Computers and the
Humanities, pages 79?84.
Davide Picca, Alfio Gliozzo, and Massimiliano Cia-
ramita. 2007. Semantic domains and supersense tag-
ging for domain-specific ontology learning. In David
Evans, Sadaoki Furui, and Chantal Soule?-Dupuy, edi-
tors, Recherche d?Information Assiste?e par Ordinateur
(RIAO), Pittsburgh, PA, USA, May.
B. Scho?lkopf and A. Smola. 2002. Learning with Ker-
nels. MIT Press, Cambridge, Massachusetts.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
wikipedia and wordnet. Elsevier Journal of Web Se-
mantics.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the conference on Empirical Methods
in Natural Language Processing (EMNLP), Waikiki,
Honolulu, Hawaii, October 25-27.
Hristo Tanev and Bernardo Magnini. 2006. Weakly su-
pervised approaches for ontology population. In Pro-
ceedings of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-2006), Trento, Italy.
209
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 248?256, Prague, June 2007. c?2007 Association for Computational Linguistics
Instance Based Lexical Entailment for Ontology Population
Claudio Giuliano and Alfio Gliozzo
FBK-irst, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Trento, ITALY
{giuliano,gliozzo}@itc.it
Abstract
In this paper we propose an instance based
method for lexical entailment and apply
it to automatic ontology population from
text. The approach is fully unsupervised and
based on kernel methods. We demonstrate
the effectiveness of our technique largely
surpassing both the random and most fre-
quent baselines and outperforming current
state-of-the-art unsupervised approaches on
a benchmark ontology available in the liter-
ature.
1 Introduction
Textual entailment is formally defined as a relation-
ship between a coherent text T and a language ex-
pression, the hypothesis H . T is said to entail H ,
denoted by T ? H , if the meaning of H can be in-
ferred from the meaning of T (Dagan et al, 2005;
Dagan and Glickman., 2004). Even though this no-
tion has been recently proposed in the computational
linguistics literature, it has already attracted a great
attention due to the very high generality of its set-
tings and to the indubitable usefulness of its (poten-
tial) applications.
In this paper, we concentrate on the problem of
lexical entailment, a textual entailment subtask in
which the system is asked to decide whether the sub-
stitution of a particular word w with the word e in a
coherent text Hw = H lwHr generates a sentence
He = H leHr such that Hw ? He, where H l and
Hr denote the left and the right context of w, re-
spectively. For example, given the word ?weapon? a
system may substitute it with the synonym ?arm?, in
order to identify relevant texts that denote the sought
concept using the latter term. A particular case of
lexical entailment is recognizing synonymy, where
both Hw ? He and He ? Hw hold.
In the literature, slight variations of this problem
are also referred to as sense matching (Dagan et al,
2006), lexical reference (Glickman et al, 2006a)
and lexical substitution (Glickman et al, 2006b).
They have been applied to a wide variety of tasks,
such as semantic matching, subtitle generation and
Word Sense Disambiguation (WSD). Modeling lex-
ical entailment is also a prerequisite to approach the
SemEval-2007 lexical substitution task1, consisting
of finding alternative words that can occur in given
context.
In this paper, we propose to apply an approach for
lexical entailment to the ontology population task.
The basic idea is that if a word entails another one
in a given context then the former is an instance or
a subclass of the latter. This approach is intuitively
appealing because lexical entailment is intrinsically
an unsupervised task, therefore it does not require
lexical resources, seed examples or manually anno-
tated data sets. Unsupervised approaches are partic-
ularly suited for ontology population, whose goal is
to find instances of concepts from corpora, because
both corpus and the ontology sizes can scale up to
millions of documents and thousands of concepts,
preventing us from applying supervised learning. In
addition, the top level part of the ontology (i.e., the
Tbox in the Description Logics terminology) is very
1http://nlp.cs.swarthmore.edu/semeval/
tasks/task10/description.shtml
248
often modified during the ontology engineering life-
cycle, for example by introducing new concepts and
restructuring the subclass of hierarchy according to
the renewed application needs required by the evo-
lution of the application domain. It is evident that
to preserve the consistency between the Tbox and
the Abox (i.e., the set of instances and their rela-
tions) in such a dynamic ontology engineering pro-
cess, supervised approaches are clearly inadequate,
as small changes in the TBox will be reflected into
dramatic annotation effort to keep instances in the
Abox aligned.
The problem of populating a predefined ontol-
ogy of concepts with novel instances implies a WSD
task, as the entities in texts are ambiguous with re-
spect to the domain ontology. For example, the en-
tity Washington is both the name of a state and the
name of a city. In the ontology population settings
traditional WSD approaches cannot be directly ap-
plied since entities are not reported into dictionar-
ies, making the lexical entailment alternative more
viable. In particular, we model the problem of on-
tology population as the problem of recognizing for
each mention of an entity of a particular coarse-
grained type (e.g., location) the fine-grained con-
cept (e.g., lake or mountain) that can be substi-
tuted in texts preserving the meaning. For example,
in the sentence ?the first man to climb the Everest
without oxygen?, ?Everest? can be substituted with
the word mountain preserving the meaning, while
the sentence is meaningless when ?Everest? is re-
placed with the word lake. Following the lexical
entailment approach, the ontology population task
is transformed into the problem of recognizing the
term from a fine-grained set of categories (e.g., city,
country, river, lake and mountain) that can be substi-
tuted in the contexts where the entity is mentioned
(e.g., Everest in the example above).
The main contributions of this paper are summa-
rized as follows. First, we propose a novel approach
to lexical entailment, called Instance Based Lexi-
cal Entailment (IBLE), that allows approaching the
problem as a classification task, in which a given
target word (i.e., the entailing word) in a particu-
lar context is judged to entail a different word taken
from a (pre-defined) set of (possible) candidate en-
tailed words (see Section 3). Second, we exploit the
IBLE approach to model the ontology population
task as follows. Given a set of candidate concepts
belonging to generic ontological types (e.g., peo-
ple or locations), and a set of pre-recognized men-
tions of entities of these types in the corpus (e.g.,
Newton, Ontario), we assign the entity to the class
whose lexicalization is more frequently entailed in
the corpus. In particular, as training set to learn
the fine-grained category models, we use all the oc-
currences of their corresponding expressions in the
same corpus (e.g., we collected all occurrences in
context of the word scientist to describe the concept
scientist). Then, we apply the trained model
to classify the pre-recognized coarse-grained entities
into the fine-grained categories.
Our approach is fully unsupervised as for training
it only requires occurrences of the candidate entailed
words taken in their contexts. Restricted to the on-
tology population task, for each coarse-grained en-
tity (e.g., location), the candidate entailed words are
the terms corresponding to the fine-grained classes
(e.g., lake or mountain) and the entailing words are
mentions of entities (e.g., New York, Ontario) be-
longing to the coarse-grained class, recognized by
an entity tagger.
Experiments show that our method for recog-
nizing lexical entailment is effective for the on-
tology population task, reporting improvements
over a state-of-the-art unsupervised technique based
on contextual similarity measures (Cimiano and
Vo?lker, 2005). In addition, we also compared it to
a supervised approach (Tanev and Magnini, 2006),
that we regarded as an upper bound, obtaining com-
parable results.
2 The Ontology Population Task
Populating concepts of a predefined ontology with
instances found in a corpus is a primary goal of
knowledge management systems. As concepts in
the ontology are generally structured into hierar-
chies belonging to a common ontological type (e.g.,
people or locations), the problem of populating on-
tologies can be solved hierarchically, firstly identi-
fying instances in texts as belonging to the topmost
concepts, and then assigning them to a fine-grained
class. Supervised named entity recognition (NER)
systems can be used for accomplishing the first step.
State-of-the-art NER systems are characterized by
249
high accuracy, but they require a large amount of
training data. However, domain specific ontologies
generally contains many ?fine-grained? categories
(e.g., particular categories of people, such as writ-
ers, scientists, and so on) and, as a consequence, su-
pervised methods cannot be used because the anno-
tation costs would become prohibitive.
Therefore, in the literature, the fine-grained clas-
sification task has been approached by adopting
weakly supervised (Tanev and Magnini, 2006; Fleis-
chman and Hovy, 2002) or unsupervised methods
(Cimiano and Vo?lker, 2005). Tanev and Magnini
(2006) proposed a weakly supervised method that
requires as training data a list of terms without con-
text for each class under consideration. Such list can
be automatically acquired from existing ontologies
or other sources (i.e., database fields, web sites like
Wikipedia, etc.) since the approach imposes virtu-
ally no restrictions on them. Given a generic syntac-
tically parsed corpus containing at least each train-
ing entity twice, the algorithm learns, for each class,
a feature vector describing the contexts where those
entities occur. Then it compares the new (unknown)
entity with the so obtained feature vectors, assigning
it to the most similar class. Fleischman and Hovy
(2002) approached the ontology population problem
as a classification task, providing examples of in-
stances in their context as training examples for their
respective fine-grained categories.
The aforementioned approaches are clearly inad-
equate to recognize such fine-grained distinctions,
as they would require a time consuming and costly
annotation process for each particular class, that
is clearly infeasible when the number of concepts
in the ontology scales up. Therefore, most of the
present research in ontology population is focus-
ing on either unsupervised approaches (Cimiano
and Vo?lker, 2005) or weakly supervised approaches
(Tanev and Magnini, 2006).
Unsupervised approaches are mostly based on
term similarity metrics. Cimiano and Vo?lker (2005)
assign a particular entity to the fine-grained class
such that the contextual similarity is maximal among
the set of fine-grained subclasses of a coarse-grained
category. Contextual similarity has been measured
by adopting lexico-syntactic features provided by a
dependency parser, as proposed in (Lin, 1998).
3 Instance Based Lexical Entailment
Dagan et al (2006) adapted the classical supervised
WSD setting to approach the sense matching prob-
lem (i.e., the binary lexical entailment problem of
deciding whether a word, such as position, entails
a different word, such as job, in a given context)
by defining a one-class learning algorithm based on
support vector machines (SVM). They train a one-
class model for each entailed word (e.g., all the oc-
currences of the word job in the corpus) and, then,
apply it to classify all the occurrences of the entail-
ing words (e.g., the word position), providing a bi-
nary decision criterion2. Similarly to the WSD case,
examples are represented by feature vectors describ-
ing their contexts, and then compared to the feature
vectors describing the context of the target word.
In this paper, we adopt a similar strategy to ap-
proach a multi-class lexical entailment problem.
The basic hypothesis is that if a word w entails
e in a particular context (Hw ? He), then some
of the contexts T je in which e occurs in the train-
ing corpus are similar to Hw. Given a word w
and an (exhaustive) set of candidate entailed words
E = {e1, e2, . . . , en}, to which we refer hereafter
with the expression ?substitution lexica?, our goal is
to select the word ei ? E that can be substituted to
w in the context Hw generating a sentence He such
that Hw ? He. In the multi-class setting, super-
vised learning approaches can be used. In particular,
we can apply a one-versus-all learning methodology,
in which each class ei is trained from both positive
(i.e., all the occurrences of ei in the corpus) and neg-
ative examples (i.e., all the occurrences of the words
in the set {ej |j 6= i}).
Our approach is clearly a simplification of the
more general lexical entailment settings, where
given two generic words w and e, and a context
H = H lwHr, the system is asked to decide whether
w entails e or not. In fact, the latter is a binary
classification problem, while the former is easier as
the system is required to select ?the best? option
among the substitution lexicon. Of course providing
such set could be problematic in many cases (e.g.,
it could be incomplete or simply not available for
2This approach resembles the pseudo-words technique pro-
posed to evaluate WSD algorithms at the earlier stages of the
WSD studies (Gale et al, 1992), when large scale sense tagged
corpora were not available for training supervised algorithms.
250
many languages or rare words). On the other hand,
such a simplification is practically effective. First of
all, it allows us to provide both positive and nega-
tive examples, avoiding the use of one-class classi-
fication algorithms that in practice perform poorly
(Dagan et al, 2006). Second, the large availabil-
ity of manually constructed substitution lexica, such
as WordNet (Fellbaum, 1998), or the use of reposi-
tories based on statistical word similarities, such as
the database constructed by Lin (1998), allows us to
find an adequate substitution lexicon for each target
word in most of the cases.
For example, as shown in Table 1, the word job
has different senses depending on its context, some
of them entailing its direct hyponym position (e.g.,
?looking for permanent job?), others entailing the
word task (e.g., ?the job of repairing?). The prob-
lem of deciding whether a particular instance of job
can be replaced by position, and not by the word
place, can be solved by looking for the most simi-
lar contexts where either position or place occur in
the training data, and then selecting the class (i.e.,
the entailed word) characterized by the most similar
ones, in an instance based style. In the first example
(see row 1), the word job is strongly associated to
the word position, because the contexts of the latter
in the examples 1 and 2 are similar to the context
of the former, and not to the word task, whose con-
texts (4, 5 and 6) are radically different. On the other
hand, the second example (see row 2) of the word
job is similar to the occurrences 4 and 5 of the word
task, allowing its correct substitution.
It is worthwhile to remark that, due to the ambi-
guity of the entailed words (e.g., position could also
entail either perspective or place), not every occur-
rence of them should be taken into account, in order
to avoid misleading predictions caused by the irrele-
vant senses. Therefore, approaches based on a more
classical contextual similarity technique (Lin, 1998;
Dagan, 2000), where words are described ?globally?
by context vectors, are doomed to fail. We will pro-
vide empirical evidence of this in the evaluation sec-
tion.
Choosing an appropriate similarity function for
the contexts of the words to be substituted is a pri-
mary issue. In this work, we exploited similar-
ity functions already defined in the WSD literature,
relying on the analogy between the lexical entail-
ment and the WSD task. The state-of-the-art super-
vised WSD methodology, reporting the best results
in most of the Senseval-3 lexical sample tasks in dif-
ferent languages, is based on a combination of syn-
tagmatic and domain kernels (Gliozzo et al, 2005)
in a SVM classification framework. Therefore, we
adopted exactly the same strategy for our purposes.
A great advantage of this methodology is that it
is totally corpus based, as it does not require nei-
ther the availability of lexical databases, nor the use
of complex preprocessing steps such as parsing or
anaphora resolution, allowing us to apply it on dif-
ferent languages and domains once large corpora are
available for training. Therefore, we exploited ex-
actly the same strategy to implement the IBLE clas-
sifier required for our purposes, defining a kernel
composed by n simple kernels, each representing
a different aspect to be considered when estimating
contextual similarity among word occurrences. In
fact, by using the closure properties of the kernel
functions, it is possible to define the kernel combi-
nation schema as follows3:
KC(xi, xj) =
n?
l=1
Kl(xi, xj)
?
Kl(xj , xj)Kl(xi, xi)
, (1)
where Kl are valid kernel functions, measuring sim-
ilarity between the objects xi and xj from different
perspectives4.
One means to satisfy both the WSD and the lex-
ical entailment requirements is to consider two dif-
ferent aspects of similarity: domain aspects, mainly
related to the topic (i.e., the global context) of the
texts in which the word occurs, and syntagmatic as-
pects, concerning the lexico-syntactic pattern in the
local context. Domain aspects are captured by the
domain kernel, described in Section 3.1, while syn-
tagmatic aspects are taken into account by the syn-
tagmatic kernel, presented in Section 3.2.
3Some recent works (Zhao and Grishman, 2005; Gliozzo
et al, 2005) empirically demostrate the effectiveness of com-
bining kernels in this way, showing that the combined kernel
always improves the performance of the individual ones. In ad-
dition, this formulation allows evaluating the individual contri-
bution of each information source.
4An exhaustive discussion about kernel methods for NLP
can be found in (Shawe-Taylor and Cristianini, 2004).
251
Entailed job Training
position ... looking for permanent academic job in ... 1 ... from entry-level through permanent positions.
2 My academic position ...
3 ... put the lamp in the left position ...
task The job of repairing 4 The task of setting up ...
5 Repairing the engine is an hard task.
6 ... task based evaluation.
Table 1: IBLE example.
3.1 The Domain Kernel
(Magnini et al, 2002) claim that knowing the do-
main of the text in which the word is located is a cru-
cial information forWSD. For example the (domain)
polysemy among the Computer Science and
the Medicine senses of the word virus can be
solved by simply considering the domain of the con-
text in which it is located. Domain aspects are also
crucial in recognizing lexical entailment. For exam-
ple, the term virus entails software agent in
the Computer Science domain (e.g., ?The lap-
top has been infected by a virus?), while it entails
bacterium when located in the Medicine domain
(e.g., ?HIV is a virus?). As argued in (Magnini et
al., 2002), domain aspects can be considered by an-
alyzing the lexicon in a large context of the word
to be disambiguated, regardless of the actual word
order. We refer to (Gliozzo et al, 2005) for a de-
tailed description of the domain kernel. The sim-
plest methodology to estimate the domain similar-
ity among two texts is to represent them by means
of vectors in the Vector Space Model (VSM), and
to exploit the cosine similarity. The VSM is a k-
dimensional space Rk, in which the text tj is rep-
resented by means of the vector ~tj such that the ith
component of ~tj is the term frequency of the term
wi in it. The similarity between two texts in the
VSM is estimated by computing the cosine between
them, providing the kernel function KV SM that can
be used as a basic tool to estimate domain similarity
between texts5.
5In (Gliozzo et al, 2005), in addition to the standard VSM,
a domain kernel, exploiting external information acquired from
unlabeled data, has been also used to reduce the amount of (la-
beled) training data. Here, given that our approach is fully un-
supervised, i.e., we can obtain as many examples as we need,
we do not use the domain kernel.
3.2 The Syntagmatic Kernel
Syntagmatic aspects are probably the most impor-
tant evidence for recognizing lexical entailment. In
general, the strategy adopted to model syntagmatic
relations in WSD is to provide bigrams and trigrams
of collocated words as features to describe local con-
texts (Yarowsky, 1994). The main drawback of this
approach is that non contiguous or shifted colloca-
tions cannot be identified, decreasing the general-
ization power of the learning algorithm. For ex-
ample, suppose that the word job has to be disam-
biguated into the sentence ?. . . permanent academic
job in. . . ?, and that the occurrence ?We offer per-
manent positions. . . ? is provided for training. A
traditional feature mapping would extract the con-
text words w?1:academic, w?2:permanent
to represent the former, and w?1:permanent,
w?2:offer to index the latter. Evidently such fea-
tures will not match, leading the algorithm to a mis-
classification.
The syntagmatic kernel, proposed by Gliozzo et
al. (2005), is an attempt to solve this problem. It
is based on a gap-weighted subsequences kernel
(Shawe-Taylor and Cristianini, 2004). In the spirit
of kernel methods, this kernel is able to compare
sequences directly in the input space, avoiding any
explicit feature mapping. To perform this opera-
tion, it counts how many times a (non-contiguous)
subsequence of symbols u of length n occurs in
the input string s, and penalizes non-contiguous oc-
currences according to the number of the contained
gaps. To define our syntagmatic kernel, we adapted
the generic definition of the sequence kernels to the
problem of recognizing collocations in local word
contexts. We refer to (Giuliano et al, 2006) for a
detailed description of the syntagmatic kernel.
252
4 Lexical Entailment for Ontology
Population
In this section, we apply the IBLE technique, de-
scribed in Section 3, to recognize lexical entailment
for ontology population. To this aim, we cast ontol-
ogy population as a lexical entailment task, where
the fine-grained categories are the candidate entailed
words, and the named entities to be subcategorized
are the entailing words. Below, we present the main
steps of our algorithm in details.
Step 1 By using a state-of-the-art supervised NER
system, we recognize the named entities belonging
to a set of coarse-grained categories (e.g., location
and people) of interest for the domain.
Step 2 For all fine-grained categories belonging to
the same coarse-grained type, we extract from a do-
main corpus all the occurrences of their lexicaliza-
tions in context (e.g., for the category actor, we
extract all contexts where the term actor occurs),
and use them as input to train the IBLE classifier. In
this way, we obtain a multi-class classifier for each
ontological type. Then, we classify all the occur-
rences of the named entities recognized in the first
step. The output of this process is a list of tagged
named entities; where the elements of the list could
have been classified into different fine-grained cat-
egories even though they refer to the same phrase
(e.g., the occurrences of the entity ?Jack London?
could have been classified both as writer and
actor, depending on the contexts where they oc-
cur).
Step 3 A distinct category is finally assigned to the
entities referring to the same phrase in the list. This
is done on the basis of the tags that have been as-
signed to all its occurrences during the previous step.
To this purpose, we implemented a voting mecha-
nism. The basic idea is that an entity belongs to a
specific category if its occurrences entail a particu-
lar superclass ?more often than expected by chance?,
where the expectation is modeled on the basis of the
overall distribution of fine-grained category labels,
assigned during the second step, in the corpus. This
intuition is formalized by applying a statistical reli-
ability measure, that depends on the distribution of
positive assignments for each class, defined by the
following formula:
R(e, c) =
P (c|e)? ?c
?c
, (2)
where P (c|e) is estimated by the relative frequency
of the fine-grained class c among the different oc-
currences of the entity e, ?c and ?c measure the
mean and the standard deviation of the distribution
P (c|E), and E is an (unlabeled) training set of in-
stances of the coarse-grained type classified by the
IBLE algorithm. Finally, each entity is assigned to
the category c? such that
c? = argmax
c
R(e, c). (3)
5 Evaluation
Evaluating a lexical entailment algorithm in itself
is rather complex. Therefore, we performed a task
driven evaluation of our system, measuring its use-
fulness in an ontology population task, for which
evaluation benchmarks are available, allowing us to
compare our technique to existing state-of-the-art
approaches.
As introduced in Section 4, the ontology popu-
lation task can be modeled as a lexical entailment
problem, in which the fine-grained classes are the
entailed words and the named entities belonging to
the coarse-grained ontological type are the entailing
words.
In the following, we first introduce the experimen-
tal settings (Section 5.1). Then we evaluate our tech-
nique by comparing it to state-of-the-art unsuper-
vised approaches for ontology population (Section
5.2).
5.1 Experimental Settings
For all experiments, we adopted the evaluation
benchmark proposed in (Tanev and Magnini, 2006).
It considers two high-level named entity cate-
gories both having five fine-grained sub-classes (i.e.,
mountain, lake, river, city, and country
as subtypes of LOCATION; statesman, writer,
athlete, actor, and inventor are subtypes of
PERSON). The authors usedWordNet andWikipedia
as primary data sources for populating the evaluation
ontology. In total, the ontology is populated with
280 instances which were not ambiguous (with re-
spect to the ontology) and appeared at least twice in
253
the English CLEF corpus6. Even the evaluation task
is rather small and can be perceived as an artificial
experimental setting, it is the best available bench-
mark we can use to compare our system to existing
approaches in the literature, as we are not aware of
other available resources.
To perform NER we used CRFs (Lafferty et al,
2001). We trained a first-order CRF on the MUC
data set to annotate locations and people. In our
experiments, we used the implementation provided
in MALLET (McCallum, 2002). We used a stan-
dard feature set inspired by the literature on text
chunking and NER (Tjong Kim Sang and Buch-
holz, 2000; Tjong Kim Sang and De Meulder, 2003;
Tjong Kim Sang, 2002) to train a first-order CRFs.
Each instance is represented by encoding all the
following families of features, all time-shifted by -
2,-1,0,1,2: (a) the word itself, (b) the PoS tag of
the token, (c) orthographic predicates, such as cap-
italization, upper-case, numeric, single character,
and punctuation, (d) gazetteers of locations, people
names and organizations, (e) character-n-gram pred-
icates for 2 6 n 6 3.
As an (unsupervised) training set for the fine-
grained categories, we exploited all occurrences in
context of their corresponding terms we found in
the CLEF corpus (e.g., for the category actor we
used all the occurrences of the term actor). We did
not use any prior estimation of the class frequency,
adopting a pure unsupervised approach. Table 2
lists the fine-grained concepts and the number of
the training examples found for each of them in the
CLEF corpus.
As a reference for a comparison of the outcomes
of this study, we used the results presented in (Tanev
and Magnini, 2006) for the Class-Word and Class-
Example approaches. The Class-Word approach ex-
ploits a similarity metric between terms and con-
cepts based on the comparison of the contexts where
they appear. Details of this technique can be found
in (Cimiano and Vo?lker, 2005). Tanev and Magnini
(2006) proposed a variant of the Class-Word algo-
rithm, called Class-Example, that relies on syntactic
features extracted from corpus and uses as an addi-
tional input a set of training examples for each class.
Overall, it required 1, 194 examples to accomplish
6http://www.clef-campaign.org
this task.
All experiments were performed using the SVM
package LIBSVM7 customized to embed our own
kernel. In all the experiments, we used the default
parameter setting.
location person
mountain 1681 statesman 119
lake 730 writer 3436
river 1411 athlete 642
city 35000 actor 2356
country 15037 inventor 105
Table 2: Number of training examples for each class.
5.2 Results
Table 4 shows our results compared with two base-
lines (i.e., random and most frequent, estimated
from the test data) and the two alternative ap-
proaches for ontology population described in the
previous section. Our system outperforms both
baselines and largely surpasses the Class-Word un-
supervised method.
It is worthwhile to remark here that, being the
IBLE algorithm fully unsupervised, improving the
most frequent baseline is an excellent result, rarely
achieved in the literature on unsupervised methods
for WSD (McCarthy et al, 2004). In addition, our
system is also competitive when compared to super-
vised approaches, being it only 5 points lower than
the Class-Example method, while it does not require
seed examples and syntactic parsing. This charac-
teristic makes our system flexible and adaptable to
different languages and domains.
System Micro F1 Macro F1
RND Baseline 0.20 0.20
Class-Word 0.42 0.33
MF baseline 0.52 NA
IBLE 0.57 0.47
Class-Example 0.62 0.68
Table 3: Comparison of different ontology popula-
tion techniques.
7http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
254
Finally, we performed a disaggregated evaluation
of our system, assessing the performance for differ-
ent ontological types and different concepts. Re-
sults show that our method performs better on larger
fine-grained classes (i.e., writer and country),
while the results on smaller categories are affected
by low recall, even if the predictions provided by
the system tends to be highly accurate. Taking into
consideration that our system is fully unsupervised,
this behavior is highly desirable because it implies
that it is somehow able to identify the predominant
class. In addition the high precision on the smaller
classes can be explained by our instance based ap-
proach.
Person N Prec Rec F1
Inventor 11 1 0.18 0.31
Statesman 20 1.0 0.05 0.10
Writer 88 0.61 0.89 0.72
Actor 25 0.57 0.68 0.62
Athlete 20 1 0.1 0.18
Micro 164 0.61 0.61 0.61
Macro 5 0.83 0.38 0.52
Table 4: Performance of the IBLE approach on peo-
ple.
Location N Prec Rec F1
City 23 0.35 0.26 0.30
Country 40 0.61 0.70 0.65
River 10 0.8 0.4 0.53
Mountain 5 0.25 0.2 0.22
Lake 4 0.2 0.5 0.29
Micro 82 0.50 0.50 0.50
Macro 5 0.44 0.41 0.42
Table 5: Performance of the IBLE approach on lo-
cations.
6 Conclusions and Future Work
In this paper, we presented a novel unsupervised
technique for recognizing lexical entailment in texts,
namely instance based lexical entailment, and we
exploited it to approach an ontology population task.
The basic assumption is that if a word is entailed
by another in a given context, then some of the
contexts of the entailed word should be similar to
that of the word to be disambiguated. Our tech-
nique is effective, as it largely surpasses both the
random and most frequent baselines. In addition, it
improves over the state-of-the-art for unsupervised
approaches, achieving performances close to the su-
pervised rivaling techniques requiring hundreds of
examples for each class.
Ontology population is only one of the possible
applications of lexical entailment. For the future,
we plan to apply our instance based approach to a
wide variety of tasks, e.g., lexical substitution, word
sense disambiguation and information retrieval. In
addition, we plan to exploit our lexical entailment as
a subcomponent of a more complex system to rec-
ognize textual entailment. Finally, we are going to
explore more elaborated kernel functions to recog-
nize lexical entailment and more efficient learning
strategies to apply our method to web-size corpora.
Acknowledgments
The authors would like to thank Bernardo Magnini
and Hristo Tanev for providing the benchmark,
Ido Dagan for useful discussions and comments
regarding the connections between lexical entail-
ment and ontology population, and Alberto Lavelli
for his thorough review. Claudio Giuliano is sup-
ported by the X-Media project (http://www.
x-media-project.org), sponsored by the Eu-
ropean Commission as part of the Information So-
ciety Technologies (IST) program under EC grant
number IST-FP6-026978. Alfio Gliozzo is sup-
ported by the FIRB-Israel research project N.
RBIN045PXH.
References
Philipp Cimiano and Johanna Vo?lker. 2005. Towards
large-scale, open-domain and ontology-based named
entity classification. In Proceedings of RANLP?05,
pages 66? 166?172, Borovets, Bulgaria.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In Proceedings of the PASCAL Workshop
on LearningMethods for Text Understanding andMin-
ing, Grenoble.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
255
challenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings ACL-2006, pages 449?456, Sydney, Australia,
July.
I. Dagan. 2000. Contextual word similarity. In Rob
Dale, Hermann Moisl, and Harold Somers, editors,
Handbook of Natural Language Processing, chap-
ter 19, pages 459?476. Marcel Dekker Inc.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. MIT Press.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of ACL-2002, pages 1?7, Morristown, NJ, USA.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation. In R. Goldman et al, editor,
Working Notes of the AAAI Fall Symposium on Prob-
abilistic Approaches to Natural Language, pages 54?
60.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2006. Syntagmatic kernels: a word
sense disambiguation case study. In Proceedings of
the EACL-2006 Workshop on Learning Structured In-
formation in Natural Language Applications, Trento,
Italy, 5-7 April.
O. Glickman, E. Shnarch, and I. Dagan. 2006a. Lexical
reference: a semantic matching subtask. In proceed-
ings of EMNLP 2006.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-
gio, and Walter Daelemans. 2006b. Investigating lexi-
cal substitution scoring for subtitle generation. In Pro-
ceedings of CoNLL-2006.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005. Do-
main kernels for word sense disambiguation. In Pro-
ceedings of ACL-2005, pages 403?410, Ann Arbor,
Michigan, June.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of ICML-2002, pages 282?289, Williams
College, MA. Morgan Kaufmann, San Francisco, CA.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of ACL-98, pages 768?
774, Morristown, NJ, USA.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of ACL-2004,
Barcelona, Spain, July.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
Hristo Tanev and Bernardo Magnini. 2006. Weakly su-
pervised approaches for ontology population. In Pro-
ceedings of EACL-2006, Trento, Italy.
Erik Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL-2000 shared task: Chunking.
In Proceedings of CoNLL-2000, Lisbon, Portugal.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003, pages 142?147, Edmonton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to
the CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL-
2002, pages 155?158, Taipei, Taiwan.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in Span-
ish and French. In Proceedings of ACL-94, pages 88?
95, Las Cruces, New Mexico.
Shubin Zhao and Ralph Grishman. 2005. Extracting re-
lations with integrated information using kernel meth-
ods. In Proceedings of ACL 2005, Ann Arbor, Michi-
gan, June.
256
Exploiting Shallow Linguistic Information for
Relation Extraction from Biomedical Literature
Claudio Giuliano and Alberto Lavelli and Lorenza Romano
ITC-irst
Via Sommarive, 18
38050, Povo (TN)
Italy
{giuliano,lavelli,romano}@itc.it
Abstract
We propose an approach for extracting re-
lations between entities from biomedical
literature based solely on shallow linguis-
tic information. We use a combination of
kernel functions to integrate two different
information sources: (i) the whole sen-
tence where the relation appears, and (ii)
the local contexts around the interacting
entities. We performed experiments on ex-
tracting gene and protein interactions from
two different data sets. The results show
that our approach outperforms most of the
previous methods based on syntactic and
semantic information.
1 Introduction
Information Extraction (IE) is the process of find-
ing relevant entities and their relationships within
textual documents. Applications of IE range from
Semantic Web to Bioinformatics. For example,
there is an increasing interest in automatically
extracting relevant information from biomedi-
cal literature. Recent evaluation campaigns on
bio-entity recognition, such as BioCreAtIvE and
JNLPBA 2004 shared task, have shown that sev-
eral systems are able to achieve good performance
(even if it is a bit worse than that reported on news
articles). However, relation identification is more
useful from an applicative perspective but it is still
a considerable challenge for automatic tools.
In this work, we propose a supervised machine
learning approach to relation extraction which is
applicable even when (deep) linguistic process-
ing is not available or reliable. In particular, we
explore a kernel-based approach based solely on
shallow linguistic processing, such as tokeniza-
tion, sentence splitting, Part-of-Speech (PoS) tag-
ging and lemmatization.
Kernel methods (Shawe-Taylor and Cristianini,
2004) show their full potential when an explicit
computation of the feature map becomes compu-
tationally infeasible, due to the high or even infi-
nite dimension of the feature space. For this rea-
son, kernels have been recently used to develop
innovative approaches to relation extraction based
on syntactic information, in which the examples
preserve their original representations (i.e. parse
trees) and are compared by the kernel function
(Zelenko et al, 2003; Culotta and Sorensen, 2004;
Zhao and Grishman, 2005).
Despite the positive results obtained exploiting
syntactic information, we claim that there is still
room for improvement relying exclusively on shal-
low linguistic information for two main reasons.
First of all, previous comparative evaluations put
more stress on the deep linguistic approaches and
did not put as much effort on developing effec-
tive methods based on shallow linguistic informa-
tion. A second reason concerns the fact that syn-
tactic parsing is not always robust enough to deal
with real-world sentences. This may prevent ap-
proaches based on syntactic features from produc-
ing any result. Another related issue concerns the
fact that parsers are available only for few lan-
guages and may not produce reliable results when
used on domain specific texts (as is the case of
the biomedical literature). For example, most of
the participants at the Learning Language in Logic
(LLL) challenge on Genic Interaction Extraction
(see Section 4.2) were unable to successfully ex-
ploit linguistic information provided by parsers. It
is still an open issue whether the use of domain-
specific treebanks (such as the Genia treebank1)
1http://www-tsujii.is.s.u-tokyo.ac.jp/
401
can be successfully exploited to overcome this
problem. Therefore it is essential to better investi-
gate the potential of approaches based exclusively
on simple linguistic features.
In our approach we use a combination of ker-
nel functions to represent two distinct informa-
tion sources: the global context where entities ap-
pear and their local contexts. The whole sentence
where the entities appear (global context) is used
to discover the presence of a relation between two
entities, similarly to what was done by Bunescu
and Mooney (2005b). Windows of limited size
around the entities (local contexts) provide use-
ful clues to identify the roles of the entities within
a relation. The approach has some resemblance
with what was proposed by Roth and Yih (2002).
The main difference is that we perform the extrac-
tion task in a single step via a combined kernel,
while they used two separate classifiers to identify
entities and relations and their output is later com-
bined with a probabilistic global inference.
We evaluated our relation extraction algorithm
on two biomedical data sets (i.e. the AImed cor-
pus and the LLL challenge data set; see Section
4). The motivations for using these benchmarks
derive from the increasing applicative interest in
tools able to extract relations between relevant en-
tities in biomedical texts and, consequently, from
the growing availability of annotated data sets.
The experiments show clearly that our approach
consistently improves previous results. Surpris-
ingly, it outperforms most of the systems based on
syntactic or semantic information, even when this
information is manually annotated (i.e. the LLL
challenge).
2 Problem Formalization
The problem considered here is that of iden-
tifying interactions between genes and proteins
from biomedical literature. More specifically, we
performed experiments on two slightly different
benchmark data sets (see Section 4 for a detailed
description). In the former (AImed) gene/protein
interactions are annotated without distinguishing
the type and roles of the two interacting entities.
The latter (LLL challenge) is more realistic (and
complex) because it also aims at identifying the
roles played by the interacting entities (agent and
target). For example, in Figure 1 three entities
are mentioned and two of the six ordered pairs of
GENIA/topics/Corpus/GTB.html
entities actually interact: (sigma(K), cwlH) and
(gerE, cwlH).
Figure 1: A sentence with two relations, R12 and
R32, between three entities, E1, E2 and E3.
In our approach we cast relation extraction as a
classification problem, in which examples are gen-
erated from sentences as follows.
First of all, we describe the complex case,
namely the protein/gene interactions (LLL chal-
lenge). For this data set entity recognition is per-
formed using a dictionary of protein and gene
names in which the type of the entities is unknown.
We generate examples for all the sentences con-
taining at least two entities. Thus the number of
examples generated for each sentence is given by
the combinations of distinct entities (N ) selected
two at a time, i.e. NC2. For example, as the sen-
tence shown in Figure 1 contains three entities, the
total number of examples generated is 3C2 = 3. In
each example we assign the attribute CANDIDATE
to each of the candidate interacting entities, while
the other entities in the example are assigned the
attribute OTHER, meaning that they do not partici-
pate in the relation. If a relation holds between the
two candidate interacting entities the example is
labeled 1 or 2 (according to the roles of the inter-
acting entities, agent and target, i.e. to the direc-
tion of the relation); 0 otherwise. Figure 2 shows
the examples generated from the sentence in Fig-
ure 1.
Figure 2: The three protein-gene examples gener-
ated from the sentence in Figure 1.
Note that in generating the examples from the
sentence in Figure 1 we did not create three neg-
402
ative examples (there are six potential ordered re-
lations between three entities), thereby implicitly
under-sampling the data set. This allows us to
make the classification task simpler without loos-
ing information. As a matter of fact, generating
examples for each ordered pair of entities would
produce two subsets of the same size containing
similar examples (differing only for the attributes
CANDIDATE and OTHER), but with different clas-
sification labels. Furthermore, under-sampling al-
lows us to halve the data set size and reduce the
data skewness.
For the protein-protein interaction task (AImed)
we use the correct entities provided by the manual
annotation. As said at the beginning of this sec-
tion, this task is simpler than the LLL challenge
because there is no distinction between types (all
entities are proteins) and roles (the relation is sym-
metric). As a consequence, the examples are gen-
erated as described above with the following dif-
ference: an example is labeled 1 if a relation holds
between the two candidate interacting entities; 0
otherwise.
3 Kernel Methods for Relation
Extraction
The basic idea behind kernel methods is to embed
the input data into a suitable feature space F via
a mapping function ? : X ? F , and then use
a linear algorithm for discovering nonlinear pat-
terns. Instead of using the explicit mapping ?, we
can use a kernel function K : X ? X ? R, that
corresponds to the inner product in a feature space
which is, in general, different from the input space.
Kernel methods allow us to design a modular
system, in which the kernel function acts as an
interface between the data and the learning algo-
rithm. Thus the kernel function is the only domain
specific module of the system, while the learning
algorithm is a general purpose component. Po-
tentially any kernel function can work with any
kernel-based algorithm. In our approach we use
Support Vector Machines (Vapnik, 1998).
In order to implement the approach based on
shallow linguistic information we employed a
linear combination of kernels. Different works
(Gliozzo et al, 2005; Zhao and Grishman, 2005;
Culotta and Sorensen, 2004) empirically demon-
strate the effectiveness of combining kernels in
this way, showing that the combined kernel always
improves the performance of the individual ones.
In addition, this formulation allows us to evalu-
ate the individual contribution of each informa-
tion source. We designed two families of kernels:
Global Context kernels and Local Context kernels,
in which each single kernel is explicitly calculated
as follows
K(x1, x2) =
??(x1), ?(x2)?
??(x1)???(x2)?
, (1)
where ?(?) is the embedding vector and ? ? ? is the
2-norm. The kernel is normalized (divided) by the
product of the norms of embedding vectors. The
normalization factor plays an important role in al-
lowing us to integrate information from heteroge-
neous feature spaces. Even though the resulting
feature space has high dimensionality, an efficient
computation of Equation 1 can be carried out ex-
plicitly since the input representations defined be-
low are extremely sparse.
3.1 Global Context Kernel
In (Bunescu and Mooney, 2005b), the authors ob-
served that a relation between two entities is gen-
erally expressed using only words that appear si-
multaneously in one of the following three pat-
terns:
Fore-Between: tokens before and between the
two candidate interacting entities. For in-
stance: binding of [P1] to [P2], interaction in-
volving [P1] and [P2], association of [P1] by
[P2].
Between: only tokens between the two candidate
interacting entities. For instance: [P1] asso-
ciates with [P2], [P1] binding to [P2], [P1],
inhibitor of [P2].
Between-After: tokens between and after the two
candidate interacting entities. For instance:
[P1] - [P2] association, [P1] and [P2] interact,
[P1] has influence on [P2] binding.
Our global context kernels operate on the patterns
above, where each pattern is represented using a
bag-of-words instead of sparse subsequences of
words, PoS tags, entity and chunk types, or Word-
Net synsets as in (Bunescu and Mooney, 2005b).
More formally, given a relation example R, we
represent a pattern P as a row vector
?P (R) = (tf(t1, P ), tf(t2, P ), . . . , tf(tl, P )) ? Rl, (2)
where the function tf(ti, P ) records how many
times a particular token ti is used in P . Note that,
403
this approach differs from the standard bag-of-
words as punctuation and stop words are included
in ?P , while the entities (with attribute CANDI-
DATE and OTHER) are not. To improve the clas-
sification performance, we have further extended
?P to embed n-grams of (contiguous) tokens (up
to n = 3). By substituting ?P into Equation 1, we
obtain the n-gram kernel Kn, which counts com-
mon uni-grams, bi-grams, . . . , n-grams that two
patterns have in common2. The Global Context
kernel KGC(R1, R2) is then defined as
KFB(R1, R2) +KB(R1, R2) +KBA(R1, R2), (3)
where KFB , KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
3.2 Local Context Kernel
The type of the candidate interacting entities can
provide useful clues for detecting the agent and
target of the relation, as well as the presence of the
relation itself. As the type is not known, we use
the information provided by the two local contexts
of the candidate interacting entities, called left and
right local context respectively. As typically done
in entity recognition, we represent each local con-
text by using the following basic features:
Token The token itself.
Lemma The lemma of the token.
PoS The PoS tag of the token.
Orthographic This feature maps each token into
equivalence classes that encode attributes
such as capitalization, punctuation, numerals
and so on.
Formally, given a relation example R, a local con-
text L = t?w, . . . , t?1, t0, t+1, . . . , t+w is repre-
sented as a row vector
?L(R) = (f1(L), f2(L), . . . , fm(L)) ? {0, 1}m, (4)
where fi is a feature function that returns 1 if it is
active in the specified position of L, 0 otherwise3.
The Local Context kernel KLC(R1, R2) is defined
as
Kleft(R1, R2) +Kright(R1, R2), (5)
whereKleft andKright are defined by substituting
the embedding of the left and right local context
into Equation 1 respectively.
2In the literature, it is also called n-spectrum kernel.
3In the reported experiments, we used a context window
of ?2 tokens around the candidate entity.
Notice that KLC differs substantially from
KGC as it considers the ordering of the tokens and
the feature space is enriched with PoS, lemma and
orthographic features.
3.3 Shallow Linguistic Kernel
Finally, the Shallow Linguistic kernel
KSL(R1, R2) is defined as
KGC(R1, R2) +KLC(R1, R2). (6)
It follows directly from the explicit construction
of the feature space and from closure properties of
kernels that KSL is a valid kernel.
4 Data sets
The two data sets used for the experiments concern
the same domain (i.e. gene/protein interactions).
However, they present a crucial difference which
makes it worthwhile to show the experimental re-
sults on both of them. In one case (AImed) in-
teractions are considered symmetric, while in the
other (LLL challenge) agents and targets of genic
interactions have to be identified.
4.1 AImed corpus
The first data set used in the experiments is the
AImed corpus4, previously used for training pro-
tein interaction extraction systems in (Bunescu et
al., 2005; Bunescu and Mooney, 2005b). It con-
sists of 225 Medline abstracts: 200 are known
to describe interactions between human proteins,
while the other 25 do not refer to any interaction.
There are 4,084 protein references and around
1,000 tagged interactions in this data set. In this
data set there is no distinction between genes and
proteins and the relations are symmetric.
4.2 LLL Challenge
This data set was used in the Learning Language
in Logic (LLL) challenge on Genic Interaction
extraction5 (Nede?llec, 2005). The objective of
the challenge was to evaluate the performance of
systems based on machine learning techniques to
identify gene/protein interactions and their roles,
agent or target. The data set was collected by
querying Medline on Bacillus subtilis transcrip-
tion and sporulation. It is divided in a training set
(80 sentences describing 271 interactions) and a
4ftp://ftp.cs.utexas.edu/pub/mooney/
bio-data/interactions.tar.gz
5http://genome.jouy.inra.fr/texte/
LLLchallenge/
404
test set (87 sentences describing 106 interactions).
Differently from the training set, the test set con-
tains sentences without interactions. The data set
is decomposed in two subsets of increasing diffi-
culty. The first subset does not include corefer-
ences, while the second one includes simple cases
of coreference, mainly appositions. Both subsets
are available with different kinds of annotation:
basic and enriched. The former includes word and
sentence segmentation. The latter also includes
manually checked information, such as lemma and
syntactic dependencies. A dictionary of named
entities (including typographical variants and syn-
onyms) is associated to the data set.
5 Experiments
Before describing the results of the experiments,
a note concerning the evaluation methodology.
There are different ways of evaluating perfor-
mance in extracting information, as noted in
(Lavelli et al, 2004) for the extraction of slot
fillers in the Seminar Announcement and the Job
Posting data sets. Adapting the proposed classi-
fication to relation extraction, the following two
cases can be identified:
? One Answer per Occurrence in the Document
? OAOD (each individual occurrence of a
protein interaction has to be extracted from
the document);
? One Answer per Relation in a given Docu-
ment ? OARD (where two occurrences of the
same protein interaction are considered one
correct answer).
Figure 3 shows a fragment of tagged text drawn
from the AImed corpus. It contains three different
interactions between pairs of proteins, for a total
of seven occurrences of interactions. For example,
there are three occurrences of the interaction be-
tween IGF-IR and p52Shc (i.e. number 1, 3 and
7). If we adopt the OAOD methodology, all the
seven occurrences have to be extracted to achieve
the maximum score. On the other hand, if we use
the OARD methodology, only one occurrence for
each interaction has to be extracted to maximize
the score.
On the AImed data set both evaluations were
performed, while on the LLL challenge only the
OAOD evaluation methodology was performed
because this is the only one provided by the eval-
uation server of the challenge.
Figure 3: Fragment of the AImed corpus with all
proteins and their interactions tagged. The pro-
tein names have been highlighted in bold face and
their same subscript numbers indicate interaction
between the proteins.
5.1 Implementation Details
All the experiments were performed using the
SVM package LIBSVM6 customized to embed our
own kernel. For the LLL challenge submission,
we optimized the regularization parameter C by
10-fold cross validation; while we used its default
value for the AImed experiment. In both exper-
iments, we set the cost-factor Wi to be the ratio
between the number of negative and positive ex-
amples.
5.2 Results on AImed
KSL performance was first evaluated on the
AImed data set (Section 4.1). We first give an
evaluation of the kernel combination and then we
compare our results with the Subsequence Ker-
nel for Relation Extraction (ERK) described in
(Bunescu and Mooney, 2005b). All experiments
are conducted using 10-fold cross validation on
the same data splitting used in (Bunescu et al,
2005; Bunescu and Mooney, 2005b).
Table 1 shows the performance of the three ker-
nels defined in Section 3 for protein-protein in-
teractions using the two evaluation methodologies
described above.
We report in Figure 4 the precision-recall curves
of ERK andKSL using OARD evaluation method-
ology (the evaluation performed by Bunescu and
Mooney (2005b)). As in (Bunescu et al, 2005;
Bunescu andMooney, 2005b), the graph points are
obtained by varying the threshold on the classifi-
6http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
405
OAOD
Kernel Precision Recall F1
KGC 57.7 60.1 58.9
KLC 37.3 56.3 44.9
KSL 60.9 57.2 59.0
OARD
Kernel Precision Recall F1
KGC 58.9 66.2 62.2
KLC 44.8 67.8 54.0
KSL 64.5 63.2 63.9
ERK 65.0 46.4 54.2
Table 1: Performance on the AImed data set us-
ing the two evaluation methodologies, OAOD and
OARD.
cation confidence7. The results clearly show that
KSL outperforms ERK, especially in term of re-
call (see Table 1).
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Pr
ec
is
io
n
Recall
KSL vs. ERK
ERK
KSL
Figure 4: Precision-recall curves on the AImed
data set using OARD evaluation methodology.
Finally, Figure 5 shows the learning curve of the
combined kernel KSL using the OARD evaluation
methodology. The curve reaches a plateau with
around 100 Medline abstracts.
5.3 Results on LLL challenge
The system was evaluated on the ?basic? version
of the LLL challenge data set (Section 4.2).
Table 2 shows the results of KSL returned by
the scoring service8 for the three subsets of the
training set (with and without coreferences, and
with their union). Table 3 shows the best results
obtained at the official competition performed in
April 2005. Comparing the results we see that
KSL trained on each subset outperforms the best
7For this purpose the probability estimate output of LIB-
SVM is used.
8http://genome.jouy.inra.fr/texte/
LLLchallenge/scoringService.php
0
0.2
0.4
0.6
0.8
1
0 50 100 150 200
F 1
Number of documents
Figure 5: KSL learning curve on the AImed data
set using OARD evaluation methodology.
Coref. Precision Recall F1
all 56.0 61.4 58.6
with 29.0 31.0 30.0
without 54.8 62.9 58.6
Table 2: KSL performance on the LLL challenge
test set using only the basic linguistic information.
systems of the LLL challenge9. Notice that the
best results at the challenge were obtained by dif-
ferent groups and exploiting the linguistic ?en-
riched? version of the data set. As observed in
(Nede?llec, 2005), the scores obtained using the
training set without coreferences and the whole
training set are similar.
We also report in Table 4 an analysis of the ker-
nel combination. Given that we are interested here
in the contribution of each kernel, we evaluated
the experiments by 10-fold cross-validation on the
whole training set avoiding the submission pro-
cess.
5.4 Discussion of Results
The experimental results show that the combined
kernel KSL outperforms the basic kernels KGC
andKLC on both data sets. In particular, precision
significantly increases at the expense of a lower re-
call. High precision is particularly advantageous
when extracting knowledge from large corpora,
because it avoids overloading end users with too
many false positives.
Although the basic kernels were designed to
model complementary aspects of the task (i.e.
9After the challenge deadline, Reidel and Klein (2005)
achieved a significant improvement, F1 = 68.4% (without
coreferences) and F1 = 64.7% (with and without corefer-
ences).
406
Test set Coref. Precision Recall F1
Enriched all 55.6 53.0 54.3
with 29.0 31.0 24.4
without 60.9 46.2 52.6
Basic all n/a n/a n/a
with 14.0 82.7 24.0
without 50.0 53.8 51.8
Table 3: Best performance on basic and enriched
test sets obtained by participants in the official
competition at the LLL challenge.
Kernel Precision Recall F1
KGC 55.1 66.3 60.2
KLC 44.8 60.1 53.8
KSL 62.1 61.3 61.7
Table 4: Comparison of the performance of kernel
combination on the LLL challenge using 10-fold
cross validation.
presence of the relation and roles of the interact-
ing entities), they perform reasonably well even
when considered separately. In particular, KGC
achieved good performance on both data sets. This
result was not expected on the LLL challenge be-
cause this task requires not only to recognize the
presence of relationships between entities but also
to identify their roles. On the other hand, the out-
comes of KLC on the AImed data set show that
such kernel helps to identify the presence of rela-
tionships as well.
At first glance, it may seem strange that KGC
outperforms ERK on AImed, as the latter ap-
proach exploits a richer representation: sparse
sub-sequences of words, PoS tags, entity and
chunk types, or WordNet synsets. However, an
approach based on n-grams is sufficient to identify
the presence of a relationship. This result sounds
less surprising, if we recall that both approaches
cast the relation extraction problem as a text cate-
gorization task. Approaches to text categorization
based on rich linguistic information have obtained
less accuracy than the traditional bag-of-words ap-
proach (e.g. (Koster and Seutter, 2003)). Shallow
linguistics information seems to be more effective
to model the local context of the entities.
Finally, we obtained worse results performing
dimensionality reduction either based on generic
linguistic assumptions (e.g. by removing words
from stop lists or with certain PoS tags) or using
statistical methods (e.g. tf.idf weighting schema).
This may be explained by the fact that, in tasks like
entity recognition and relation extraction, useful
clues are also provided by high frequency tokens,
such as stop words or punctuation marks, and by
the relative positions in which they appear.
6 Related Work
First of all, the obvious references for our work
are the approaches evaluated on AImed and LLL
challenge data sets.
In (Bunescu and Mooney, 2005b), the authors
present a generalized subsequence kernel that
works with sparse sequences containing combina-
tions of words and PoS tags.
The best results on the LLL challenge were ob-
tained by the group from the University of Ed-
inburgh (Reidel and Klein, 2005), which used
Markov Logic, a framework that combines log-
linear models and First Order Logic, to create a
set of weighted clauses which can classify pairs of
gene named entities as genic interactions. These
clauses are based on chains of syntactic and se-
mantic relations in the parse or Discourse Repre-
sentation Structure (DRS) of a sentence, respec-
tively.
Other relevant approaches include those that
adopt kernel methods to perform relation extrac-
tion. Zelenko et al (2003) describe a relation ex-
traction algorithm that uses a tree kernel defined
over a shallow parse tree representation of sen-
tences. The approach is vulnerable to unrecover-
able parsing errors. Culotta and Sorensen (2004)
describe a slightly generalized version of this ker-
nel based on dependency trees, in which a bag-of-
words kernel is used to compensate for errors in
syntactic analysis. A further extension is proposed
by Zhao and Grishman (2005). They use compos-
ite kernels to integrate information from different
syntactic sources (tokenization, sentence parsing,
and deep dependency analysis) so that process-
ing errors occurring at one level may be overcome
by information from other levels. Bunescu and
Mooney (2005a) present an alternative approach
which uses information concentrated in the short-
est path in the dependency tree between the two
entities.
As mentioned in Section 1, another relevant ap-
proach is presented in (Roth and Yih, 2002). Clas-
sifiers that identify entities and relations among
them are first learned from local information in
the sentence. This information, along with con-
straints induced among entity types and relations,
is used to perform global probabilistic inference
407
that accounts for the mutual dependencies among
the entities.
All the previous approaches have been evalu-
ated on different data sets so that it is not possi-
ble to have a clear idea of which approach is better
than the other.
7 Conclusions and Future Work
The good results obtained using only shallow lin-
guistic features provide a higher baseline against
which it is possible to measure improvements ob-
tained using methods based on deep linguistic pro-
cessing. In the near future, we plan to extend our
work in several ways.
First, we would like to evaluate the contribu-
tion of syntactic information to relation extraction
from biomedical literature. With this aim, we will
integrate the output of a parser (possibly trained on
a domain-specific resource such the Genia Tree-
bank). Second, we plan to test the portability of
our model on ACE and MUC data sets. Third,
we would like to use a named entity recognizer
instead of assuming that entities are already ex-
tracted or given by a dictionary. Our long term
goal is to populate databases and ontologies by
extracting information from large text collections
such as Medline.
8 Acknowledgements
We would like to thank Razvan Bunescu for pro-
viding detailed information about the AImed data
set and the settings of the experiments. Clau-
dio Giuliano and Lorenza Romano have been sup-
ported by the ONTOTEXT project, funded by the
Autonomous Province of Trento under the FUP-
2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, Van-
couver, B.C, October.
Razvan Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In
Proceedings of the 19th Conference on Neural In-
formation Processing Systems, Vancouver, British
Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004), Barcelona,
Spain.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
Cornelis H. A. Koster and Mark Seutter. 2003. Taming
wild phrases. In Advances in Information Retrieval,
25th European Conference on IR Research (ECIR
2003), pages 161?176, Pisa, Italy.
Alberto Lavelli, Mary Elaine Califf, Fabio Ciravegna,
Dayne Freitag, Claudio Giuliano, Nicholas Kushm-
erick, and Lorenza Romano. 2004. IE evaluation:
Criticisms and recommendations. In Proceedings of
the AAAI 2004 Workshop on Adaptive Text Extrac-
tion and Mining (ATEM 2004), San Jose, California.
Claire Nede?llec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the ICML-2005 Workshop on Learning Lan-
guage in Logic (LLL05), pages 31?37, Bonn, Ger-
many, August.
Sebastian Reidel and Ewan Klein. 2005. Genic
interaction extraction with semantic and syntactic
chains. In Proceedings of the ICML-2005 Workshop
on Learning Language in Logic (LLL05), pages 69?
74, Bonn, Germany, August.
D. Roth and W. Yih. 2002. Probabilistic reasoning
for entity & relation recognition. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING-02), Taipei, Taiwan.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, New York, NY, USA.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for information
extraction. Journal of Machine Learning Research,
3:1083?1106.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
408
Proceedings of the 43rd Annual Meeting of the ACL, pages 403?410,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Domain Kernels for Word Sense Disambiguation
Alfio Gliozzo and Claudio Giuliano and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica
I-38050, Trento, ITALY
{gliozzo,giuliano,strappa}@itc.it
Abstract
In this paper we present a supervised
Word Sense Disambiguation methodol-
ogy, that exploits kernel methods to model
sense distinctions. In particular a combi-
nation of kernel functions is adopted to
estimate independently both syntagmatic
and domain similarity. We defined a ker-
nel function, namely the Domain Kernel,
that allowed us to plug ?external knowl-
edge? into the supervised learning pro-
cess. External knowledge is acquired from
unlabeled data in a totally unsupervised
way, and it is represented by means of Do-
main Models. We evaluated our method-
ology on several lexical sample tasks in
different languages, outperforming sig-
nificantly the state-of-the-art for each of
them, while reducing the amount of la-
beled training data required for learning.
1 Introduction
The main limitation of many supervised approaches
for Natural Language Processing (NLP) is the lack
of available annotated training data. This problem is
known as the Knowledge Acquisition Bottleneck.
To reach high accuracy, state-of-the-art systems
for Word Sense Disambiguation (WSD) are de-
signed according to a supervised learning frame-
work, in which the disambiguation of each word
in the lexicon is performed by constructing a dif-
ferent classifier. A large set of sense tagged exam-
ples is then required to train each classifier. This
methodology is called word expert approach (Small,
1980; Yarowsky and Florian, 2002). However this
is clearly unfeasible for all-words WSD tasks, in
which all the words of an open text should be dis-
ambiguated.
On the other hand, the word expert approach
works very well for lexical sample WSD tasks (i.e.
tasks in which it is required to disambiguate only
those words for which enough training data is pro-
vided). As the original rationale of the lexical sam-
ple tasks was to define a clear experimental settings
to enhance the comprehension of WSD, they should
be considered as preceding exercises to all-words
tasks. However this is not the actual case. Algo-
rithms designed for lexical sample WSD are often
based on pure supervision and hence ?data hungry?.
We think that lexical sample WSD should regain
its original explorative role and possibly use a min-
imal amount of training data, exploiting instead ex-
ternal knowledge acquired in an unsupervised way
to reach the actual state-of-the-art performance.
By the way, minimal supervision is the basis
of state-of-the-art systems for all-words tasks (e.g.
(Mihalcea and Faruque, 2004; Decadt et al, 2004)),
that are trained on small sense tagged corpora (e.g.
SemCor), in which few examples for a subset of the
ambiguous words in the lexicon can be found. Thus
improving the performance of WSD systems with
few learning examples is a fundamental step towards
the direction of designing a WSD system that works
well on real texts.
In addition, it is a common opinion that the per-
formance of state-of-the-art WSD systems is not sat-
isfactory from an applicative point of view yet.
403
To achieve these goals we identified two promis-
ing research directions:
1. Modeling independently domain and syntag-
matic aspects of sense distinction, to improve
the feature representation of sense tagged ex-
amples (Gliozzo et al, 2004).
2. Leveraging external knowledge acquired from
unlabeled corpora.
The first direction is motivated by the linguistic
assumption that syntagmatic and domain (associa-
tive) relations are both crucial to represent sense
distictions, while they are basically originated by
very different phenomena. Syntagmatic relations
hold among words that are typically located close
to each other in the same sentence in a given tempo-
ral order, while domain relations hold among words
that are typically used in the same semantic domain
(i.e. in texts having similar topics (Gliozzo et al,
2004)). Their different nature suggests to adopt dif-
ferent learning strategies to detect them.
Regarding the second direction, external knowl-
edge would be required to help WSD algorithms to
better generalize over the data available for train-
ing. On the other hand, most of the state-of-the-art
supervised approaches to WSD are still completely
based on ?internal? information only (i.e. the only
information available to the training algorithm is the
set of manually annotated examples). For exam-
ple, in the Senseval-3 evaluation exercise (Mihal-
cea and Edmonds, 2004) many lexical sample tasks
were provided, beyond the usual labeled training
data, with a large set of unlabeled data. However,
at our knowledge, none of the participants exploited
this unlabeled material. Exploring this direction is
the main focus of this paper. In particular we ac-
quire a Domain Model (DM) for the lexicon (i.e.
a lexical resource representing domain associations
among terms), and we exploit this information in-
side our supervised WSD algorithm. DMs can be
automatically induced from unlabeled corpora, al-
lowing the portability of the methodology among
languages.
We identified kernel methods as a viable frame-
work in which to implement the assumptions above
(Strapparava et al, 2004).
Exploiting the properties of kernels, we have de-
fined independently a set of domain and syntagmatic
kernels and we combined them in order to define a
complete kernel for WSD. The domain kernels esti-
mate the (domain) similarity (Magnini et al, 2002)
among contexts, while the syntagmatic kernels eval-
uate the similarity among collocations.
We will demonstrate that using DMs induced
from unlabeled corpora is a feasible strategy to in-
crease the generalization capability of the WSD al-
gorithm. Our system far outperforms the state-of-
the-art systems in all the tasks in which it has been
tested. Moreover, a comparative analysis of the
learning curves shows that the use of DMs allows
us to remarkably reduce the amount of sense-tagged
examples, opening new scenarios to develop sys-
tems for all-words tasks with minimal supervision.
The paper is structured as follows. Section 2 in-
troduces the notion of Domain Model. In particular
an automatic acquisition technique based on Latent
Semantic Analysis (LSA) is described. In Section 3
we present a WSD system based on a combination
of kernels. In particular we define a Domain Ker-
nel (see Section 3.1) and a Syntagmatic Kernel (see
Section 3.2), to model separately syntagmatic and
domain aspects. In Section 4 our WSD system is
evaluated in the Senseval-3 English, Italian, Spanish
and Catalan lexical sample tasks.
2 Domain Models
The simplest methodology to estimate the similar-
ity among the topics of two texts is to represent
them by means of vectors in the Vector Space Model
(VSM), and to exploit the cosine similarity. More
formally, let C = {t1, t2, . . . , tn} be a corpus, let
V = {w1, w2, . . . , wk} be its vocabulary, let T be
the k ? n term-by-document matrix representing C ,
such that ti,j is the frequency of word wi into the text
tj . The VSM is a k-dimensional space Rk, in which
the text tj ? C is represented by means of the vec-
tor ~tj such that the ith component of ~tj is ti,j. The
similarity among two texts in the VSM is estimated
by computing the cosine among them.
However this approach does not deal well with
lexical variability and ambiguity. For example the
two sentences ?he is affected by AIDS? and ?HIV is
a virus? do not have any words in common. In the
404
VSM their similarity is zero because they have or-
thogonal vectors, even if the concepts they express
are very closely related. On the other hand, the sim-
ilarity between the two sentences ?the laptop has
been infected by a virus? and ?HIV is a virus? would
turn out very high, due to the ambiguity of the word
virus.
To overcome this problem we introduce the notion
of Domain Model (DM), and we show how to use it
in order to define a domain VSM in which texts and
terms are represented in a uniform way.
A DM is composed by soft clusters of terms. Each
cluster represents a semantic domain, i.e. a set of
terms that often co-occur in texts having similar top-
ics. A DM is represented by a k?k? rectangular ma-
trix D, containing the degree of association among
terms and domains, as illustrated in Table 1.
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Matrix
DMs can be used to describe lexical ambiguity
and variability. Lexical ambiguity is represented
by associating one term to more than one domain,
while variability is represented by associating dif-
ferent terms to the same domain. For example the
term virus is associated to both the domain COM-
PUTER SCIENCE and the domain MEDICINE (ambi-
guity) while the domain MEDICINE is associated to
both the terms AIDS and HIV (variability).
More formally, let D = {D1, D2, ..., Dk?} be a
set of domains, such that k?  k. A DM is fully
defined by a k?k? domain matrix D representing in
each cell di,z the domain relevance of term wi with
respect to the domain Dz . The domain matrix D is
used to define a function D : Rk ? Rk? , that maps
the vectors ~tj expressed into the classical VSM, into
the vectors ~t?j in the domain VSM. D is defined by1
D(~tj) = ~tj(IIDFD) = ~t?j (1)
1In (Wong et al, 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM is
a particular instance.
where IIDF is a k ? k diagonal matrix such that
iIDFi,i = IDF (wi), ~tj is represented as a row vector,
and IDF (wi) is the Inverse Document Frequency of
wi.
Vectors in the domain VSM are called Domain
Vectors (DVs). DVs for texts are estimated by ex-
ploiting the formula 1, while the DV ~w?i, correspond-
ing to the word wi ? V is the ith row of the domain
matrix D. To be a valid domain matrix such vectors
should be normalized (i,e. ? ~w?i, ~w?i? = 1).
In the Domain VSM the similarity among DVs is
estimated by taking into account second order rela-
tions among terms. For example the similarity of the
two sentences ?He is affected by AIDS? and ?HIV
is a virus? is very high, because the terms AIDS,
HIV and virus are highly associated to the domain
MEDICINE.
A DM can be estimated from hand made lexical
resources such as WORDNET DOMAINS (Magnini
and Cavaglia`, 2000), or by performing a term clus-
tering process on a large corpus. We think that the
second methodology is more attractive, because it
allows us to automatically acquire DMs for different
languages.
In this work we propose the use of Latent Seman-
tic Analysis (LSA) to induce DMs from corpora.
LSA is an unsupervised technique for estimating the
similarity among texts and terms in a corpus. LSA
is performed by means of a Singular Value Decom-
position (SVD) of the term-by-document matrix T
describing the corpus. The SVD algorithm can be
exploited to acquire a domain matrix D from a large
corpus C in a totally unsupervised way. SVD de-
composes the term-by-document matrix T into three
matrixes T ' V?k?UT where ?k? is the diagonal
k ? k matrix containing the highest k ?  k eigen-
values of T, and all the remaining elements set to
0. The parameter k? is the dimensionality of the Do-
main VSM and can be fixed in advance2 . Under this
setting we define the domain matrix DLSA as
DLSA = INV
?
?k? (2)
where IN is a diagonal matrix such that iNi,i =
1
q
? ~w?i, ~w?i?
, ~w?i is the ith row of the matrix V
?
?k? .3
2It is not clear how to choose the right dimensionality. In
our experiments we used 50 dimensions.
3When DLSA is substituted in Equation 1 the Domain VSM
405
3 Kernel Methods for WSD
In the introduction we discussed two promising di-
rections for improving the performance of a super-
vised disambiguation system. In this section we
show how these requirements can be efficiently im-
plemented in a natural and elegant way by using ker-
nel methods.
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping ?, we can use a kernel
function K : X ? X ? R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Exploiting the properties of the kernel func-
tions, it is possible to define the kernel combination
schema as
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj, xj)Kl(xi, xi)
(3)
Our WSD system is then defined as combination
of n basic kernels. Each kernel adds some addi-
tional dimensions to the feature space. In particular,
we have defined two families of kernels: Domain
and Syntagmatic kernels. The former is composed
by both the Domain Kernel (KD) and the Bag-of-
Words kernel (KBoW ), that captures domain aspects
(see Section 3.1). The latter captures the syntag-
matic aspects of sense distinction and it is composed
by two kernels: the collocation kernel (KColl) and
is equivalent to a Latent Semantic Space (Deerwester et al,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema (Salton and McGill, 1983), widely adopted in Informa-
tion Retrieval.
the Part of Speech kernel (KPoS) (see Section 3.2).
The WSD kernels (K ?WSD and KWSD) are then de-
fined by combining them (see Section 3.3).
3.1 Domain Kernels
In (Magnini et al, 2002), it has been claimed that
knowing the domain of the text in which the word
is located is a crucial information for WSD. For
example the (domain) polysemy among the COM-
PUTER SCIENCE and the MEDICINE senses of the
word virus can be solved by simply considering
the domain of the context in which it is located.
This assumption can be modeled by defining a
kernel that estimates the domain similarity among
the contexts of the words to be disambiguated,
namely the Domain Kernel. The Domain Kernel es-
timates the similarity among the topics (domains) of
two texts, so to capture domain aspects of sense dis-
tinction. It is a variation of the Latent Semantic Ker-
nel (Shawe-Taylor and Cristianini, 2004), in which a
DM (see Section 2) is exploited to define an explicit
mapping D : Rk ? Rk? from the classical VSM into
the Domain VSM. The Domain Kernel is defined by
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(ti),D(tj)??D(ti),D(tj)?
(4)
where D is the Domain Mapping defined in equa-
tion 1. Thus the Domain Kernel requires a Domain
Matrix D. For our experiments we acquire the ma-
trix DLSA, described in equation 2, from a generic
collection of unlabeled documents, as explained in
Section 2.
A more traditional approach to detect topic (do-
main) similarity is to extract Bag-of-Words (BoW)
features from a large window of text around the
word to be disambiguated. The BoW kernel, de-
noted by KBoW , is a particular case of the Domain
Kernel, in which D = I, and I is the identity ma-
trix. The BoW kernel does not require a DM, then it
can be applied to the ?strictly? supervised settings,
in which an external knowledge source is not pro-
vided.
3.2 Syntagmatic kernels
Kernel functions are not restricted to operate on vec-
torial objects ~x ? Rk. In principle kernels can be
defined for any kind of object representation, as for
406
example sequences and trees. As stated in Section 1,
syntagmatic relations hold among words collocated
in a particular temporal order, thus they can be mod-
eled by analyzing sequences of words.
We identified the string kernel (or word se-
quence kernel) (Shawe-Taylor and Cristianini, 2004)
as a valid instrument to model our assumptions.
The string kernel counts how many times a (non-
contiguous) subsequence of symbols u of length
n occurs in the input string s, and penalizes non-
contiguous occurrences according to the number of
gaps they contain (gap-weighted subsequence ker-
nel).
Formally, let V be the vocabulary, the feature
space associated with the gap-weighted subsequence
kernel of length n is indexed by a set I of subse-
quences over V of length n. The (explicit) mapping
function is defined by
?nu(s) =
?
i:u=s(i)
?l(i), u ? V n (5)
where u = s(i) is a subsequence of s in the posi-
tions given by the tuple i, l(i) is the length spanned
by u, and ? ?]0, 1] is the decay factor used to penal-
ize non-contiguous subsequences.
The associate gap-weighted subsequence kernel is
defined by
kn(si, sj) = ??n(si), ?n(sj)? =
X
u?V n
?n(si)?n(sj) (6)
We modified the generic definition of the string
kernel in order to make it able to recognize collo-
cations in a local window of the word to be disam-
biguated. In particular we defined two Syntagmatic
kernels: the n-gram Collocation Kernel and the n-
gram PoS Kernel. The n-gram Collocation ker-
nel KnColl is defined as a gap-weighted subsequence
kernel applied to sequences of lemmata around the
word l0 to be disambiguated (i.e. l?3, l?2, l?1, l0,
l+1, l+2, l+3). This formulation allows us to esti-
mate the number of common (sparse) subsequences
of lemmata (i.e. collocations) between two exam-
ples, in order to capture syntagmatic similarity. In
analogy we defined the PoS kernel KnPoS , by setting
s to the sequence of PoSs p?3, p?2, p?1, p0, p+1,
p+2, p+3, where p0 is the PoS of the word to be dis-
ambiguated.
The definition of the gap-weighted subsequence
kernel, provided by equation 6, depends on the pa-
rameter n, that represents the length of the sub-
sequences analyzed when estimating the similarity
among sequences. For example, K2Coll allows us to
represent the bigrams around the word to be disam-
biguated in a more flexible way (i.e. bigrams can be
sparse). In WSD, typical features are bigrams and
trigrams of lemmata and PoSs around the word to
be disambiguated, then we defined the Collocation
Kernel and the PoS Kernel respectively by equations
7 and 84.
KColl(si, sj) =
p
?
l=1
K lColl(si, sj) (7)
KPoS(si, sj) =
p
?
l=1
K lPoS(si, sj) (8)
3.3 WSD kernels
In order to show the impact of using Domain Models
in the supervised learning process, we defined two
WSD kernels, by applying the kernel combination
schema described by equation 3. Thus the following
WSD kernels are fully specified by the list of the
kernels that compose them.
Kwsd composed by KColl, KPoS and KBoW
K?wsd composed by KColl, KPoS , KBoW and KD
The only difference between the two systems is
that K ?wsd uses Domain Kernel KD. K ?wsd exploits
external knowledge, in contrast to Kwsd, whose only
available information is the labeled training data.
4 Evaluation and Discussion
In this section we present the performance of our
kernel-based algorithms for WSD. The objectives of
these experiments are:
? to study the combination of different kernels,
? to understand the benefits of plugging external
information using domain models,
? to verify the portability of our methodology
among different languages.
4The parameters p and ? are optimized by cross-validation.
The best results are obtained setting p = 2, ? = 0.5 for KColl
and ? ? 0 for KPoS .
407
4.1 WSD tasks
We conducted the experiments on four lexical sam-
ple tasks (English, Catalan, Italian and Spanish)
of the Senseval-3 competition (Mihalcea and Ed-
monds, 2004). Table 2 describes the tasks by re-
porting the number of words to be disambiguated,
the mean polysemy, and the dimension of training,
test and unlabeled corpora. Note that the organiz-
ers of the English task did not provide any unlabeled
material. So for English we used a domain model
built from a portion of BNC corpus, while for Span-
ish, Italian and Catalan we acquired DMs from the
unlabeled corpora made available by the organizers.
#w pol # train # test # unlab
Catalan 27 3.11 4469 2253 23935
English 57 6.47 7860 3944 -
Italian 45 6.30 5145 2439 74788
Spanish 46 3.30 8430 4195 61252
Table 2: Dataset descriptions
4.2 Kernel Combination
In this section we present an experiment to em-
pirically study the kernel combination. The basic
kernels (i.e. KBoW , KD , KColl and KPoS) have
been compared to the combined ones (i.e. Kwsd and
K ?wsd) on the English lexical sample task.
The results are reported in Table 3. The results
show that combining kernels significantly improves
the performance of the system.
KD KBoW KPoS KColl Kwsd K?wsd
F1 65.5 63.7 62.9 66.7 69.7 73.3
Table 3: The performance (F1) of each basic ker-
nel and their combination for English lexical sample
task.
4.3 Portability and Performance
We evaluated the performance of K ?wsd and Kwsd on
the lexical sample tasks described above. The results
are showed in Table 4 and indicate that using DMs
allowed K ?wsd to significantly outperform Kwsd.
In addition, K ?wsd turns out the best systems for
all the tested Senseval-3 tasks.
Finally, the performance of K ?wsd are higher than
the human agreement for the English and Spanish
tasks5.
Note that, in order to guarantee an uniform appli-
cation to any language, we do not use any syntactic
information provided by a parser.
4.4 Learning Curves
The Figures 1, 2, 3 and 4 show the learning curves
evaluated on K ?wsd and Kwsd for all the lexical sam-
ple tasks.
The learning curves indicate that K ?wsd is far su-
perior to Kwsd for all the tasks, even with few ex-
amples. The result is extremely promising, for it
demonstrates that DMs allow to drastically reduce
the amount of sense tagged data required for learn-
ing. It is worth noting, as reported in Table 5, that
K ?wsd achieves the same performance of Kwsd using
about half of the training data.
% of training
English 54
Catalan 46
Italian 51
Spanish 50
Table 5: Percentage of sense tagged examples re-
quired by K ?wsd to achieve the same performance of
Kwsd with full training.
5 Conclusion and Future Works
In this paper we presented a supervised algorithm
for WSD, based on a combination of kernel func-
tions. In particular we modeled domain and syn-
tagmatic aspects of sense distinctions by defining
respectively domain and syntagmatic kernels. The
Domain kernel exploits Domain Models, acquired
from ?external? untagged corpora, to estimate the
similarity among the contexts of the words to be dis-
ambiguated. The syntagmatic kernels evaluate the
similarity between collocations.
We evaluated our algorithm on several Senseval-
3 lexical sample tasks (i.e. English, Spanish, Ital-
ian and Catalan) significantly improving the state-ot-
the-art for all of them. In addition, the performance
5It is not clear if the inter-annotator-agreement can be con-
siderated the upper bound for a WSD system.
408
MF Agreement BEST Kwsd K ?wsd DM+
English 55.2 67.3 72.9 69.7 73.3 3.6
Catalan 66.3 93.1 85.2 85.2 89.0 3.8
Italian 18.0 89.0 53.1 53.1 61.3 8.2
Spanish 67.7 85.3 84.2 84.2 88.2 4.0
Table 4: Comparative evaluation on the lexical sample tasks. Columns report: the Most Frequent baseline,
the inter annotator agreement, the F1 of the best system at Senseval-3, the F1 of Kwsd, the F1 of K ?wsd,
DM+ (the improvement due to DM, i.e. K ?wsd ?Kwsd).
0.5
0.55
0.6
0.65
0.7
0.75
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 1: Learning curves for English lexical sample
task.
0.65
0.7
0.75
0.8
0.85
0.9
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 2: Learning curves for Catalan lexical sample
task.
of our system outperforms the inter annotator agree-
ment in both English and Spanish, achieving the up-
per bound performance.
We demonstrated that using external knowledge
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 3: Learning curves for Italian lexical sample
task.
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 0.2 0.4 0.6 0.8 1
F1
Percentage of training set
K'wsd
K wsd
Figure 4: Learning curves for Spanish lexical sam-
ple task.
inside a supervised framework is a viable method-
ology to reduce the amount of training data required
for learning. In our approach the external knowledge
is represented by means of Domain Models automat-
409
ically acquired from corpora in a totally unsuper-
vised way. Experimental results show that the use
of Domain Models allows us to reduce the amount
of training data, opening an interesting research di-
rection for all those NLP tasks for which the Knowl-
edge Acquisition Bottleneck is a crucial problem. In
particular we plan to apply the same methodology to
Text Categorization, by exploiting the Domain Ker-
nel to estimate the similarity among texts. In this im-
plementation, our WSD system does not exploit syn-
tactic information produced by a parser. For the fu-
ture we plan to integrate such information by adding
a tree kernel (i.e. a kernel function that evaluates the
similarity among parse trees) to the kernel combi-
nation schema presented in this paper. Last but not
least, we are going to apply our approach to develop
supervised systems for all-words tasks, where the
quantity of data available to train each word expert
classifier is very low.
Acknowledgments
Alfio Gliozzo and Carlo Strapparava were partially
supported by the EU project Meaning (IST-2001-
34460). Claudio Giuliano was supported by the EU
project Dot.Kom (IST-2001-34038). We would like
to thank Oier Lopez de Lacalle for useful comments.
References
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
B. Decadt, V. Hoste, W. Daelemens, and A. van den
Bosh. 2004. Gambl, genetic algorithm optimiza-
tion of memory-based wsd. In Proc. of Senseval-3,
Barcelona, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18(3):275?299.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, pages 1413?1418, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceedings
of SENSEVAL-3, Barcelona, Spain, July.
R. Mihalcea and E. Faruque. 2004. Senselearner: Min-
imally supervised WSD for all words in open text. In
Proceedings of SENSEVAL-3, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
S. Small. 1980. Word Expert Parsing: A Theory of Dis-
tributed Word-based Natural Language Understand-
ing. Ph.D. Thesis, Department of Computer Science,
University of Maryland.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation: Irst at senseval-3. In Proc. of
SENSEVAL-3 Third International Workshop on Eval-
uation of Systems for the Semantic Analysis of Text,
pages 229?234, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 8th ACM SIGIR Conference.
D. Yarowsky and R. Florian. 2002. Evaluating sense dis-
ambiguation across diverse parameter space. Natural
Language Engineering, 8(4):293?310.
410
Pattern Abstraction and Term Similarity for Word Sense Disambiguation:
IRST at Senseval-3
Carlo Strapparava and Alfio Gliozzo and Claudio Giuliano
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica, I-38050 Trento, ITALY
{strappa, gliozzo, giuliano}@itc.it
Abstract
This paper summarizes IRST?s participation in
Senseval-3. We participated both in the English all-
words task and in some lexical sample tasks (En-
glish, Basque, Catalan, Italian, Spanish). We fol-
lowed two perspectives. On one hand, for the all-
words task, we tried to refine the Domain Driven
Disambiguation that we presented at Senseval-2.
The refinements consist of both exploiting a new
technique (Domain Relevance Estimation) for do-
main detection in texts, and experimenting with the
use of Latent Semantic Analysis to avoid reliance on
manually annotated domain resources (e.g. WORD-
NET DOMAINS). On the other hand, for the lexical
sample tasks, we explored the direction of pattern
abstraction and we demonstrated the feasibility of
leveraging external knowledge using kernel meth-
ods.
1 Introduction
The starting point for our research in the Word
Sense Disambiguation (WSD) area was to explore
the use of semantic domains in order to solve lex-
ical ambiguity. At the Senseval-2 competition we
proposed a new approach to WSD, namely Domain
Driven Disambiguation (DDD). This approach con-
sists of comparing the estimated domain of the con-
text of the word to be disambiguated with the do-
mains of its senses, exploiting the property of do-
mains to be features of both texts and words. The
domains of the word senses can be either inferred
from the learning data or derived from the informa-
tion in WORDNET DOMAINS.
For Senseval-3, we refined the DDD methodol-
ogy with a fully unsupervised technique - Domain
Relevance Estimation (DRE) - for domain detection
in texts. DRE is performed by an expectation maxi-
mization algorithm for the gaussian mixture model,
which is exploited to differentiate relevant domain
information in texts from noise. This refined DDD
system was presented in the English all-words task.
Originally DDD was developed to assess the use-
fulness of domain information for WSD. Thus it
did not exploit other knowledge sources commonly
used for disambiguation (e.g. syntactic patterns or
collocations). As a consequence the performance of
the DDD system is quite good for precision (it dis-
ambiguates well the ?domain? words), but as far as
recall is concerned it is not competitive compared
with other state of the art techniques. On the other
hand DDD outperforms the state of the art for unsu-
pervised systems, demonstrating the usefulness of
domain information for WSD.
In addition, the DDD approach requires domain
annotations for word senses (for the experiments we
used WORDNET DOMAINS, a lexical resource de-
veloped at IRST). Like all manual annotations, such
an operation is costly (more than two man years
have been spent for labeling the whole WORDNET
DOMAINS structure) and affected by subjectivity.
Thus, one drawback of the DDD methodology was
a lack of portability among languages and among
different sense repositories (unless we have synset-
aligned WordNets).
Besides the improved DDD, our other proposals
for Senseval-3 constitute an attempt to overcome
these previous issues.
To deal with the problem of having a domain-
annotated WORDNET, we experimented with a
novel methodology to automatically acquire domain
information from corpora. For this aim we esti-
mated term similarity from a large scale corpus, ex-
ploiting the assumption that semantic domains are
sets of very closely related terms. In particular we
implemented a variation of Latent Semantic Analy-
sis (LSA) in order to obtain a vector representation
for words, texts and synsets. LSA performs a di-
mensionality reduction in the feature space describ-
ing both texts and words, capturing implicitly the
notion of semantic domains required by DDD. In
order to perform disambiguation, LSA vectors have
been estimated for the synsets in WORDNET. We
participated in the English all-words task also with
a first prototype (DDD-LSA) that exploits LSA in-
stead of WORDNET DOMAINS.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Task Systems
English All-Words DDD DDD-LSA
English Lex-sample Kernels-WSD Ties
Italian Lex-Sample Kernels-WSD Ties
Basque Lex-Sample Kernels-WSD
Catalan Lex-Sample Kernels-WSD
Spanish Lex-Sample Kernels-WSD
Table 1: IRST participation at Senseval-3
As far as lexical sample tasks are concerned, we
participated in the English, Italian, Spanish, Cata-
lan, and Basque tasks. For these tasks, we ex-
plored the direction of pattern abstraction for WSD.
Pattern abstraction is an effective methodology for
WSD (Mihalcea, 2002). Our preliminary experi-
ments have been performed using TIES, a general-
ized Information Extraction environment developed
at IRST that implements the boosted wrapper induc-
tion algorithm (Freitag and Kushmerick, 2000). The
main limitation of such an approach is, once more,
the integration of different knowledge sources. In
particular, paradigmatic information seems hard to
be represented in the TIES framework, motivating
our decision to exploit kernel methods for WSD.
Kernel methods is an area of recent interest in
Machine Learning. Kernels are similarity functions
between instances that allows to integrate different
knowledge sources and to model explicitly linguis-
tic insights inside the powerful framework of sup-
port vector machine classification. For Senseval-3
we implemented the Kernels-WSD system, which
exploits kernel methods to perform the following
operations: (i) pattern abstraction; (ii) combination
of different knowledge sources, in particular domain
information and syntagmatic information; (iii) inte-
gration of unsupervised term proximity estimation
in the supervised framework.
The paper is structured as follows. Section 2 in-
troduces LSA and its relations with semantic do-
mains. Section 3 presents the systems for the En-
glish all-words task (i.e. DDD and DDD-LSA). In
section 4 our supervised approaches are reported.
In particular the TIES system is described in section
4.1, while the approach based on kernel methods is
discussed in section 4.2.
2 Semantic Domains and LSA
Domains are common areas of human discussion,
such as economics, politics, law, science etc., which
are at the basis of lexical coherence. A substantial
part of the lexicon is composed by ?domain words?,
that refer to concepts belonging to specific domains.
In (Magnini et al, 2002) it has been claimed that
domain information provides generalized features at
the paradigmatic level that are useful to discriminate
among word senses.
The WORDNET DOMAINS1 lexical resource
is an extension of WORDNET which provides
such domain labels for all synsets (Magnini and
Cavaglia`, 2000). About 200 domain labels were se-
lected from a number of dictionaries and then struc-
tured in a taxonomy according to the Dewey Deci-
mal Classification (DDC). The annotation method-
ology was mainly manual and took about 2 person
years.
WORDNET DOMAINS has been proven a useful
resource for WSD. However some aspects induced
us to explore further developments. These issues
are: (i) it is difficult to find an objective a-priori
model for domains; (ii) the annotation procedure
followed to develop WORDNET DOMAINS is very
expensive, making hard the replicability of the lexi-
cal resource for other languages or domain specific
sub-languages; (iii) the domain distinctions are rigid
in WORDNET DOMAINS, while a more ?fuzzy? as-
sociation between domains and concepts is often
more appropriate to describe term similarity.
In order to generalize the domain approach and to
overcome these issues, we explored the direction of
unsupervised learning on a large-scale corpus (we
used the BNC corpus for all the experiments de-
scribed in this paper).
In particular, we followed the LSA approach
(Deerwester et al, 1990). In LSA, term co-
occurrences in the documents of the corpus are cap-
tured by means of a dimensionality reduction oper-
ated on the term-by-document matrix. The result-
ing LSA vectors can be exploited to estimate both
term and document similarity. Regarding document
similarity, Latent Semantic Indexing (LSI) is a tech-
nique that allows one to represent a document by
a LSA vector. In particular, we used a variation
of the pseudo-document methodology described in
(Berry, 1992). Each document can be represented in
the LSA space by summing up the normalized LSA
vectors of all the terms contained in it.
By exploiting LSA vectors for terms, it is pos-
sible to estimate domain vectors for the synsets of
WORDNET, in order to obtain similarity values be-
tween concepts that can be used for synset cluster-
ing and WSD. Thus, term and document vectors can
be used instead of WORDNET DOMAINS for WSD
and other applications in which term similarity and
domain relevance estimation is required.
1WORDNET DOMAINS is freely available for research pur-
poses at wndomains.itc.it
3 All-Words systems: DDD and DDD-LSA
DDD with DRE. DDD assignes the right sense of
a word in its context comparing the domain of the
context to the domain of each sense of the word.
This methodology exploits WORDNET DOMAINS
information to estimate both the domain of the tex-
tual context and the domain of the senses of the
word to disambiguate.
The basic idea to estimate domain relevance for
texts is to exploit lexical coherence inside texts. A
simple heuristic approach to this problem, used in
Senseval-2, is counting the occurrences of domain
words for every domain inside the text: the higher
the percentage of domain words for a certain do-
main, the more relevant the domain will be for the
text.
Unfortunately, the simple local frequency count
is not a good domain relevance measure for sev-
eral reasons. Indeed irrelevant senses of ambigu-
ous words contribute to augment the final score of
irrelevant domains, introducing noise. Moreover,
the level of noise is different for different domains
because of their different sizes and possible dif-
ferences in the ambiguity level of their vocabular-
ies. We refined the original Senseval-2 DDD system
with the Domain Relevance Estimation (DRE) tech-
nique. Given a certain domain, DRE distinguishes
between relevant and non-relevant texts by means
of a Gaussian Mixture model that describes the fre-
quency distribution of domain words inside a large-
scale corpus (in particular we used the BNC corpus
also in this case). Then, an Expectation Maximiza-
tion algorithm computes the parameters that maxi-
mize the likelihood of the model on the empirical
data (Gliozzo et al, 2004).
In order to represent domain information we in-
troduced the notion of Domain Vectors (DV), which
are data structures that collect domain information.
These vectors are defined in a multidimensional
space, in which each domain represents a dimen-
sion of the space. We distinguish between two
kinds of DVs: (i) synset vectors, which represent
the relevance of a synset with respect to each con-
sidered domain and (ii) text vectors, which repre-
sent the relevance of a portion of text with respect
to each domain in the considered set. The core of
the DDD algorithm is based on scoring the compar-
ison of these kinds of vectors. The synset vectors
are built considering WORDNET DOMAINS, while
in the calculation of scoring the system takes into
account synset probabilities on SemCor. The sys-
tem makes use of a threshold th-cut, ranging in the
interval [0,1], that allows us to tune the tradeoff be-
tween precision and recall.
th-cut Prec Recall Attempted
0.0 0.583 0.583 99.76
0.9 0.729 0.441 60.51
Table 2: DDD on the English all-words task.
Latent Semantic Domains for DDD. As seen in
Section 2, it is possible to implement a DDD ver-
sion that does not use WORDNET DOMAINS and
instead it exploits LSA term and document vectors
for estimating synset vectors and text vectors, leav-
ing the core of DDD algorithm unchanged. As for
text vectors, we used the psedo-document technique
also for building synset vectors: in this case we con-
sider the synonymous terms contained in the synset
itself.
The system presented at Senseval-3 does not
make use of any statistics on SemCor, and conse-
quently it can be considered fully unsupervised. Re-
sults are reported in table 3 and do not differ much
from the results obtained by DDD in the same task.
th-cut Prec Recall Attempted
0.5 0.661 0.496 75.01
Table 3: DDD-LSA on the English all-words task.
4 Lexical Sample Systems: Pattern
abstraction and Kernel Methods
One of the most discriminative features for lexi-
cal disambiguation is the lexical/syntactic pattern in
which the word appears. A well known issue in the
WSD area is the one sense per collocation claim
(Yarowsky, 1993) stating that the word meanings
are strongly associated with the particular colloca-
tion in which the word is located. Collocations are
sequences of words in the context of the word to
disambiguate, and can be associated to word senses
performing supervised learning.
Another important knowledge source for WSD is
the shallow-syntactic pattern in which a word ap-
pears. Syntactic patterns, like lexical patterns, can
be obtained by exploiting pattern abstraction tech-
niques on POS sequences. In the WSD literature
both lexical and syntactic patterns have been used
as features in a supervised learning schema by rep-
resenting each instance using bigrams and trigrams
in the surrounding context of the word to be ana-
lyzed2.
2More recently deep-syntactic features have been also con-
sidered by several systems, as for example modifiers of nouns
and verbs, object and subject of the sentence, etc. In order to
Representing each instance by a ?bag of features?
presents several disadvantages from the point of
view of both machine learning and computational
linguistics: (1) Sparseness in the learning data: most
of the collocations found in the learning data occur
just once, reducing the generalization power of the
learning algorithm. In addition most of the collo-
cations found in the test data are often unseen in
the training data. (2) Low flexibility for pattern ab-
straction purposes: bigram and trigram extraction
schemata are fixed in advance. (3) Knowledge ac-
quisition bottleneck: the size of the training data is
not large enough to cover each possible collocation
in the language.
To overcome problems 1 and 2 we investigated
some pattern abstraction techniques from the area
of Information Extraction (IE) and we adapted them
to WSD. To overcome problem 3 we developed La-
tent Semantic Kernels, which allow us to integrate
external knowledge provided by unsupervised term
similarity estimation.
4.1 TIES
Our first experiments have been performed exploit-
ing TIES, an environment developed at IRST for IE
that induces patterns from the marked entities in the
training phase, and then applies those patterns in the
test phase in order to assign a category if the pat-
tern is satisfied. For our experiments, we used the
Boosted Wrapper Induction (BWI) algorithm (Fre-
itag and Kushmerick, 2000) that is implemented in
TIES.
For Senseval-3 we used very few features (lemma
and POS). We proposed the system in this configu-
ration as a ?baseline? system for pattern abstraction.
Task Prec Recall Attempted
English LS 0.706 0.505 71.50
English LS (coarse) 0.767 0.548 71.50
Italian LS 0.552 0.309 55.92
Table 4: Performance of the TIES system
Our preliminary experiments with BWI have
shown that pattern abstraction is very attractive for
WSD, allowing us to achieve a very high precision
for a restricted number of words, in which the syn-
tagmatic information is sufficient for disambigua-
tion. However, we still had some restrictions. In
particular, the integration with different knowledge
sources for classification is not trivial.
obtain such features parsing of the data is required. However,
we decided to do not use such information, while we plan to
introduce it in the next future.
4.2 Kernel-WSD
Our choice of exploiting kernel methods for WSD
has been motivated by the observation that pattern-
based approaches for disambiguation are comple-
mentary to the domain based ones: they require dif-
ferent knowledge sources and different techniques
for classification and feature description. Both ap-
proaches have to be simultaneously taken into ac-
count in order to perform accurate disambiguation.
Our aim was to combine them into a common
framework.
Kernel methods, e.g. Support Vector Machines
(SVMs), are state-of-the-art learning algorithms,
and they are successfully adopted in many NLP
tasks.
The idea of SVM (Cristianini and Shawe-Taylor,
2000) is to map the set of training data into a higher-
dimensional feature space F via a mapping func-
tion ? : ? ? F , and construct a separating hy-
perplane with maximum margin (distance between
planes and closest points) in the new space. Gen-
erally, this yields a nonlinear decision boundary in
the input space. Since the feature space is high di-
mensional, performing the transformation has of-
ten a high computational cost. Rather than use the
explicit mapping ?, we can use a kernel function
K : ??? ? < , that corresponds to the inner prod-
uct in a feature space which is, in general, different
from the input space.
Therefore, a kernel function provides a way
to compute (efficiently) the separating hyperplane
without explicitly carrying out the map ? into the
feature space - this is called the kernel trick. In this
way the kernel acts as an interface between the data
and the learning algorithm by defining an implicit
mapping into the feature space. Intuitively, we can
see the kernel as a function that measures the sim-
ilarity between pairs of objects. The learning algo-
rithm, which compares all pairs of data items, ex-
ploits the information encoded in the kernel. An
important characteristic of kernels is that they are
not limited to vector objects but are applicable to
virtually any kind of object representation.
In this work we use kernel methods to combine
heterogeneous sources of information that we found
relevant for WSD. For each of these aspects it is
possible to define kernels independently. Then they
are combined by exploiting the property that the
sum of two kernels is still a kernel (i.e. k(x, y) =
k1(x, y) + k2(x, y)), taking advantage of each sin-
gle contribution in an intuitive way3.
3In order to keep the kernel values comparable for dif-
ferent values and to be independent from the length of the
examples, we considered the normalized version K?(x, y) =
lsa Task Prec Recall Attempted MF-Baseline
? English LS 0.726 0.726 100 0.552
? English LS (coarse) 0.795 0.795 100 0.645
- English LS (no-lsa) 0.704 0.704 100 0.552
- Basque LS 0.655 0.655 100 0.558
- Italian LS 0.531 0.531 100 0.183
- Catalan LS 0.858 0.846 98.62 0.663
- Spanish LS 0.842 0.842 100 0.677
Table 5: Performance of the Kernels-WSD system
The Word Sense Disambiguation Kernel is de-
fined in this way:
KWSD(x, y) = KS(x, y) + KP (x, y) (1)
where KS is the Syntagmatic Kernel and KP is
the Paradigmatic Kernel.
The Syntagmatic Kernel. The syntagmatic ker-
nel generalizes the word-sequence kernels defined
by (Cancedda et al, 2003) to sequences of lem-
mata and POSs. Word sequence kernels are based
on the following idea: two sequences are similar
if they have in common many sequences of words
in a given order. The similarity between two ex-
amples is assessed by the number (possibly non-
contiguous) of the word sequences matching. Non-
contiguous occurrences are penalized according to
the number of gaps they contain. For example the
sequence of words ?I go very quickly to school? is
less similar to ?I go to school? than ?I go quickly to
school?. Different than the bag-of-word approach,
word sequence kernels capture the word order and
allow gaps between words. The word sequence ker-
nels are parametric with respect to the length of the
(sparse) sequences they want to capture.
We have defined the syntagmatic kernel as the
sum of n distinct word-sequence kernels for lem-
mata (i.e. Collocation Kernel - KC ) and sequences
of POSs (i.e. POS Kernel - KPOS), according to the
formula (for our experiments we set n to 2):
KS(x, y) =
n
X
i=1
KCi(x, y) +
n
X
i=1
KPOSi(x, y) (2)
In the above definition of syntagmatic kernel,
only exact lemma/POS matches contribute to the
similarity. One shortcoming of this approach is
that (near-)synonyms will never be considered sim-
ilar. We address this problem by considering soft-
matching of words employing a term similarity
K(x, y)/sqrt(K(x, x)K(y, y))
measure based on LSA4. In particular we consid-
ered equivalent two words having the same POS and
a similarity value higher than an empirical thresh-
old. For example, if we consider as equivalent
the terms Ronaldo and football player the sequence
The football player scored the first goal can be con-
sidered equivalent to the sentence Ronaldo scored
the first goal. The properties of the kernel methods
offer a flexible way to plug additional information,
in this case unsupervised (we could also take this in-
formation from a semantic network such as WORD-
NET).
The Paradigmatic Kernel. The paradigmatic
kernel takes into account the paradigmatic aspect of
sense distinction (i.e. domain aspects) (Gliozzo et
al., 2004). For example the word virus can be dis-
ambiguated by recognizing the domain of the con-
text in which it is placed (e.g. computer science
vs. biology). Usually such an aspect is captured
by ?bag-of-words?, in analogy to the Vector Space
Model, widely used in Text Categorization and In-
formation Retrieval. The main limitation of this
model for WSD is the knowledge acquisition bot-
tleneck (i.e. the lack of sense tagged data). Bag of
words are very sparse data that require a large scale
corpus to be learned. To overcome such a limita-
tion, Latent Semantic Indexing (LSI) can provide a
solution.
Thus we defined a paradigmatic kernel composed
by the sum of a ?traditional? bag of words kernel
and an LSI kernel (Cristianini et al, 2002) as de-
fined by formula 3:
KP (x, y) = KBoW (x, y) + KLSI(x, y) (3)
where KBoW computes the inner product be-
tween the vector space model representations and
KLSI computes the cosine between the LSI vectors
representing the texts.
4For languages other than English, we did not exploit this
soft-matching and the KLSI kernel described below. See the
first column in the table 5.
Table 5 displays the performance of Kernel-
WSD. As a comparison, we also report the figures
on the English task without using LSA. The last col-
umn reports the recall of the most-frequent baseline.
Acknowledgments
Claudio Giuliano is supported by the IST-Dot.Kom
project sponsored by the European Commission
(Framework V grant IST-2001-34038). TIES and
the kernel package have been developed in the con-
text of the Dot.Kom project.
References
M. Berry. 1992. Large-scale sparse singular value
computations. International Journal of Super-
computer Applications, 6(1):13?49.
N. Cancedda, E. Gaussier, C. Goutte, and J.M. Ren-
ders. 2003. Word-sequence kernels. Journal of
Machine Learning Research, 3(6):1059?1082.
N. Cristianini and J. Shawe-Taylor. 2000. Support
Vector Machines. Cambridge University Press.
N. Cristianini, J. Shawe-Taylor, and H. Lodhi.
2002. Latent semantic kernels. Journal of Intel-
ligent Information Systems, 18(2):127?152.
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K.
Landauer, and R. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science, 41(6):391?407.
D. Freitag and N. Kushmerick. 2000. Boosted
wrapper induction. In Proc. of AAAI-00, pages
577?583, Austin, Texas.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004.
Unsupervised and supervised exploitation of se-
mantic domains in lexical disambiguation. Com-
puter Speech and Language, Forthcoming.
B. Magnini and G. Cavaglia`. 2000. Integrating sub-
ject field codes into WordNet. In Proceedings of
LREC-2000, Athens, Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
R. F. Mihalcea. 2002. Word sense disambiguation
with pattern learning and automatic feature selec-
tion. Natural Language Engineering, 8(4):343?
358.
D. Yarowsky. 1993. One sense per collocation. In
Proceedings of ARPA Human Language Technol-
ogy Workshop, pages 266?271, Princeton.
Simple Information Extraction (SIE):
A Portable and Effective IE System
Claudio Giuliano and Alberto Lavelli and Lorenza Romano
ITC-irst
Via Sommarive, 18
38050, Povo (TN)
Italy
{giuliano,lavelli,romano}@itc.it
Abstract
This paper describes SIE (Simple Infor-
mation Extraction), a modular information
extraction system designed with the goal
of being easily and quickly portable across
tasks and domains. SIE is composed by
a general purpose machine learning algo-
rithm (SVM) combined with several cus-
tomizable modules. A crucial role in the
architecture is played by Instance Filter-
ing, which allows to increase efficiency
without reducing effectiveness. The re-
sults obtained by SIE on several standard
data sets, representative of different tasks
and domains, are reported. The experi-
ments show that SIE achieves performance
close to the best systems in all tasks, with-
out using domain-specific knowledge.
1 Introduction
In designing Information Extraction (IE) systems
based on supervised machine learning techniques,
there is usually a tradeoff between carefully tun-
ing the system to specific tasks and domains and
having a ?generic? IE system able to obtain good
(even if not the topmost) performance when ap-
plied to different tasks and domains (requiring a
very reduced porting time). Usually, the former
alternative is chosen and system performance is
often shown only for a very limited number of
tasks (sometimes even only for a single task), af-
ter a careful tuning. For example, in the Bio-entity
Recognition Shared Task at JNLPBA 2004 (Kim
et al, 2004) the best performing system obtained
a considerable performance improvement adopt-
ing domain specific hacks.
A second important issue in designing IE sys-
tems concerns the fact that usually IE data sets are
highly unbalanced (i.e., the number of positive ex-
amples constitutes only a small fraction with re-
spect to the number of negative examples). This
fact has important consequences. In some ma-
chine learning algorithms the unbalanced distri-
bution of examples can yield a significant loss in
classification accuracy. Moreover, very large data
sets can be problematic to process due to the com-
plexity of many supervised learning techniques.
For example, using kernel methods, such as word
sequence and tree kernels, can become prohibitive
due to the difficulty of kernel based algorithms,
such as Support Vector Machines (SVM) (Cortes
and Vapnik, 1995), to scale to large data sets. As
a consequence, reducing the number of instances
without degrading the prediction accuracy is a cru-
cial issue for applying advanced machine learning
techniques in IE, especially in the case of highly
unbalanced data sets.
In this paper, we present SIE (Simple Informa-
tion Extraction), an information extraction system
based on a supervised machine learning approach
for extracting domain-specific entities from docu-
ments. In particular, IE is cast as a classification
problem by applying SVM to train a set of classi-
fiers, based on a simple and general-purpose fea-
ture representation, for detecting the boundaries of
the entities to be extracted.
SIE was designed with the goal of being easily
and quickly portable across tasks and domains. To
support this claim, we conducted a set of exper-
iments on several tasks in different domains and
languages. The results show that SIE is competi-
tive with the state-of-the-art systems, and it often
outperforms systems customized to a specific do-
main.
SIE resembles the ?Level One? of the ELIE
algorithm (Finn and Kushmerick, 2004). How-
9
ever, a key difference between the two algorithms
is the capability of SIE to drastically reduce the
computation time by exploiting Instance Filtering
(Gliozzo et al, 2005a). This characteristic allows
scaling from toy problems to real-world data sets
making SIE attractive in applicative fields, such as
bioinformatics, where very large amounts of data
have to be analyzed.
2 A Simple IE system
SIE has a modular system architecture. It is com-
posed by a general purpose machine learning algo-
rithm combined with several customizable com-
ponents. The system components are combined
in a pipeline, where each module constrains the
data structures provided by the previous ones.
This modular specification brings significant ad-
vantages. Firstly, a modular architecture is sim-
pler to implement. Secondly, it allows to easily
integrate different machine learning algorithms.
Finally, it allows, if necessary, a fine tuning to
a specific task by simply specializing few mod-
ules. Furthermore, it is worth noting that we tested
SIE across different domains using the same basic
configuration without exploiting any domain spe-
cific knowledge, such as gazetteers, and ad-hoc
pre/post-processing.
Instance
Filtering
Feature
Extraction
Learning
Algorithm
Tag
Matcher
Classification
Algorithm
Instance
Filtering
Feature
Extraction
Lexicon
Training Corpus New Documents
Data Model
Tagged
Documents
Filter Model
Extraction
Script
Extraction
Script
Figure 1: The SIE Architecture.
The architecture of the system is shown in Fig-
ure 1. The information extraction task is per-
formed in two phases. SIE learns off-line a set of
data models from a specified labeled corpus, then
the models are applied to tag new documents.
In both phases, the Instance Filtering module
(Section 3) removes certain tokens from the data
set in order to speed-up the whole process, while
Feature Extraction module (Section 4) is used to
extract a pre-defined set of features from the to-
kens. In the training phase, the Learning Mod-
ule (Section 5) learns two distinct models for each
entity, one for the beginning boundary and an-
other for the end boundary (Ciravegna, 2000; Fre-
itag and Kushmerick, 2000). In the recognition
phase, as a consequence, the Classification mod-
ule (Section 5) identifies the entity boundaries as
distinct token classifications. A Tag Matcher mod-
ule (Section 6) is used to match the boundary pre-
dictions made by the Classification module. Tasks
with multiple entities are considered as multiple
independent single-entity extraction tasks (i.e. SIE
only extracts one entity at a time).
3 Instance Filtering
The purpose of the Instance Filtering (IF) mod-
ule is to reduce the data set size and skewness
by discarding harmful and superfluous instances
without degrading the prediction accuracy. This
is a generic module that can be exploited by any
supervised system that casts IE as a classification
problem.
Instance Filtering (Gliozzo et al, 2005a) is
based on the assumption that uninformative words
are not likely to belong to entities to recognize,
being their information content very low. A naive
implementation of this assumption consists in fil-
tering out very frequent words in corpora because
they are less likely to be relevant than rare words.
However, in IE relevant entities can be composed
by more than one token and in some domains a few
of such tokens can be very frequent in the corpus.
For example, in the field of bioinformatics, protein
names often contain parentheses, whose frequency
in the corpus is very high.
To deal with this problem, we exploit a set of In-
stance Filters (called Stop Word Filters), included
in a Java tool called jInFil1. These filters per-
form a ?shallow? supervision to identify frequent
words that are often marked as positive examples.
The resulting filtering algorithm consists of two
stages. First, the set of uninformative tokens is
identified by training the term filtering algorithm
on the training corpus. Second, instances describ-
ing ?uninformative? tokens are removed from both
the training and the test sets. Note that instances
are not really removed from the data set, but just
1http://tcc.itc.it/research/textec/
tools-resources/jinfil/
10
marked as uninformative. In this way the learning
algorithm will not learn from these instances, but
they will still appear in the feature description of
the remaining instances.
A Stop Word Filter is fully specified by a list of
stop words. To identify such a list, different fea-
ture selection methods taken from the text catego-
rization literature can be exploited. In text catego-
rization, feature selection is used to remove non-
informative terms from representations of texts. In
this sense, IF is closely related to feature selection:
in the former non-informative words are removed
from the instance set, while in the latter they are
removed from the feature set. Below, we describe
the different metrics used to collect a stop word
list from the training corpora.
Information Content (IC) The most commonly
used feature selection metric in text categoriza-
tion is based on document frequency (i.e, the num-
ber of documents in which a term occurs). The
basic assumption is that very frequent terms are
non-informative for document indexing. The fre-
quency of a term in the corpus is a good indica-
tor of its generality, rather than of its information
content. From this point of view, IF consists of
removing all tokens with a very low information
content2.
Correlation Coefficient (CC) In text catego-
rization the ?2 statistic is used to measure the lack
of independence between a term and a category
(Yang and Pedersen, 1997). The correlation coef-
ficient CC2 = ?2 of a term with the negative class
can be used to find those terms that are less likely
to express relevant information in texts.
Odds Ratio (OR) Odds ratio measures the ra-
tio between the odds of a term occurring in the
positive class, and the odds of a term occurring in
the negative class. In text categorization the idea
is that the distribution of the features on the rel-
evant documents is different from the distribution
on non-relevant documents (Raskutti and Kowal-
czyk, 2004). Following this assumption, a term
is non-informative when its probability of being a
negative example is sensibly higher than its prob-
ability of being a positive example (Gliozzo et al,
2005b).
2The information content of a word w can be measured
by estimating its probability from a corpus by the equation
I(w) = ?p(w) log p(w).
An Instance Filter is evaluated by using two
metrics: the Filtering Rate (?), the total percent-
age of filtered tokens in the data set, and the Pos-
itive Filtering Rate (?+), the percentage of pos-
itive tokens (wrongly) removed. A filter is opti-
mized by maximizing ? and minimizing ?+; this
allows us to reduce as much as possible the data
set size preserving most of the positive instances.
We fixed the accepted level of tolerance () on ?+
and found the maximum ? by performing 5-fold
cross-validation on the training set.
4 Feature Extraction
The Feature Extraction module is used to extract
for each input token a pre-defined set of features.
As said above, we consider each token an instance
to be classified as a specific entity boundary or
not. To perform Feature Extraction an applica-
tion called jFex3 was implemented. jFex gener-
ates the features specified by a feature extraction
script, indexes them, and returns the example set,
as well as the mapping between the features and
their indices (lexicon). If specified, it only ex-
tracts features for the instances not marked as ?un-
informative? by instance filtering. jFex is strongly
inspired by FEX (Cumby and Yih, 2003), but it
introduces several improvements. First of all, it
provides an enriched feature extraction language.
Secondly, it makes possible to further extend this
language through a Java API, providing a flexi-
ble tool to define task specific features. Finally,
jFex can output the example set in formats di-
rectly usable by LIBSVM (Chang and Lin, 2001),
SVMlight (Joachims, 1998) and SNoW (Carlson
et al, 1999).
4.1 Corpus Format
The corpus must be prepared in IOBE notation, a
extension of the IOB notation. Both notations do
not allow nested and overlapping entities. Tokens
outside entities are tagged with O, while the first
token of an entity is tagged with B-entity-type, the
last token is tagged E-entity-type, and all the to-
kens inside the entity boundaries are tagged with
I-entity-type, where entity-type is the type of the
marked entity (e.g. protein, person).
Beside the tokens and their types, the nota-
tion allows to represent general purpose and task-
specific annotations defining new columns. Blank
3http://tcc.itc.it/research/textec/
tools-resources/jfex.html.
11
lines can be used to specify sentence or document
boundaries. Table 1 shows an example of a pre-
pared corpus. The columns are: the entity-type,
the PoS tag, the actual token, the token index, and
the output of the instance filter (the ?uninforma-
tive? tokens are marked with 0) respectively.
O TO To 2.12 0
O VB investigate 2.13 0
O IN whether 2.14 0
O DT the 2.15 0
B-cell type NN tumor 2.16 1
O NN expression 2.17 1
O IN of 2.18 0
B-protein NN Beta-2-Microglobulin 2.19 1
O ( ( 2.20 1
B-protein NN Beta 2.21 1
I-protein NN 2 2.22 1
I-protein NN - 2.22 1
E-protein NN M 2.22 1
O ) ) 2.23 1
Table 1: A corpus fragment represented in IOBE
notation.
4.2 Extraction Language
As input to the begin and end classifiers, we use
a bit-vector representation. Each instance is rep-
resented encoding all the following basic features
for the actual token and for all the tokens in a con-
text window of fixed size (in the reported experi-
ments, 3 words before and 3 words after the actual
token):
Token The actual token.
POS The Part of Speech (PoS) of the token.
Token Shapes This feature maps each token into
equivalence classes that encode attributes
such as capitalization, numerals, single char-
acter, and so on.
Bigrams of tokens and PoS tags.
The Feature Extraction language allows to
formally encode the above problem description
through a script. Table 2 provides the extraction
script used in all the tasks4. More details about the
Extraction Language are provided in (Cumby and
Yih, 2003; Giuliano et al, 2005).
4In JNLPBA shared task we added some orthographic fea-
tures borrowed from the bioinformatics literature.
-1 inc loc: w [-3, 3]
-1 inc loc: coloc(w,w) [-3, 3]
-1 inc loc: t [-3, 3]
-1 inc loc: coloc(t,t) [-3, 3]
-1 inc loc: sh [-3, 3]
Table 2: The extraction script used in all tasks.
5 Learning and Classification Modules
As already said, we approach IE as a classifica-
tion problem, assigning an appropriate classifica-
tion label to each token in the data set except for
the tokens marked as irrelevant by the instance fil-
ter. As learning algorithm we use SVM-light5. In
particular, we identify the boundaries that indi-
cate the beginning and the end of each entity as
two distinct classification tasks, following the ap-
proach adopted in (Ciravegna, 2000; Freitag and
Kushmerick, 2000). All tokens that begin(end) an
entity are considered positive instances for the be-
gin(end) classifier, while all the remaining tokens
are negative instances. In this way, two distinct
models are learned, one for the beginning bound-
ary and another for the end boundary. All the pre-
dictions produced by the begin and end classifiers
are then paired by the Tag Matcher module.
When we have to deal with more than one en-
tity (i.e., with a multi-class problem) we train 2n
binary classifiers (where n is the number of entity-
types for the task). Again, all the predictions are
paired by the Tag Matcher module.
6 Tag Matcher
All the positive predictions produced by the begin
and end classifiers are paired by the Tag Matcher
module. If nested or overlapping entities occur,
even if they are of different types, the entity with
the highest score is selected. The score of each
entity is proportional to the entity length probabil-
ity (i.e., the probability that an entity has a certain
length) and the scores assigned by the classifiers to
the boundary predictions. Normalizing the scores
makes it possible to consider the score function as
a probability distribution. The entity length distri-
bution is estimated from the training set.
For example, in the corpus fragment of Table 3
the begin and end classifiers have identified four
possible entity boundaries for the speaker of a
seminar. In the table, the left column shows the
5http://svmlight.joachims.org/
12
Table 3: A corpus fragment with multiple predic-
tions.
O The
O speaker
O will
O be
B-speaker Mr. B-speaker (0.23)
I-speaker John B-speaker (0.1), E-speaker (0.12)
E-speaker Smith E-speaker (0.34)
O .
Table 4: The length distribution for the entity
speaker.
entity len 1 2 3 4 5 ...
P(entity len) 0.10 0.33 0.28 0.02 0.01 ...
actual label, while the right column shows the pre-
dictions and their normalized scores. The match-
ing algorithm has to choose among three mutu-
ally exclusive candidates: ?Mr. John?, ?Mr. John
Smith? and ?John Smith?, with scores 0.23 ?
0.12 ? 0.33 = 0.009108, 0.23 ? 0.34 ? 0.28 =
0.021896 and 0.1 ? 0.34 ? 0.33 = 0.01122, re-
spectively. The length distribution for the entity
speaker is shown in Table 4. In this example, the
matcher, choosing the candidate that maximizes
the score function, namely the second one, extracts
the actual entity.
7 Evaluation
In order to demonstrate that SIE is domain and
language independent we tested it on several tasks
using exactly the same configuration. The tasks
and the experimental settings are described in Sec-
tion 7.1. The results (Section 7.2) show that the
adopted filtering technique decreases drastically
the computation time while preserving (and some-
times improving) the overall accuracy of the sys-
tem.
7.1 The Tasks
SIE was tested on the following IE benchmarks:
JNLPBA Shared Task This shared task (Kim
et al, 2004) is an open challenge task proposed
at the ?International Joint Workshop on Natural
Language Processing in Biomedicine and its Ap-
plications?6. The data set consists of 2, 404 MED-
LINE abstracts from the GENIA project (Kim et
6http://research.nii.ac.jp/?collier/
workshops/JNLPBA04st.htm.
al., 2003), annotated with five entity types: DNA,
RNA, protein, cell-line, and cell-type. The GE-
NIA corpus is split into two partitions: training
(492,551 tokens), and test (101,039 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set varies
from 0.2% to 6%.
CoNLL 2002 & 2003 Shared Tasks These
shared tasks (Tjong Kim Sang, 2002; Tjong
Kim Sang and De Meulder, 2003)7 concern
language-independent named entity recognition.
Four types of named entities are considered:
persons (PER), locations (LOC), organizations
(ORG) and names of miscellaneous (MISC) en-
tities that do not belong to the previous three
groups. SIE was applied to the Dutch and English
data sets. The Dutch corpus is divided into three
partitions: training and validation (on the whole
258, 214 tokens), and test (73, 866 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set varies
from 1.1% to 2%. The English corpus is divided
into three partitions: training and validation (on
the whole 274, 585 tokens), and test (50, 425 to-
kens). The fraction of positive examples with re-
spect to the total number of tokens in the training
set varies from 1.6% to 3.3%.
TERN 2004 The TERN (Time Expression
Recognition and Normalization) 2004 Evaluation8
requires systems to detect and normalize temporal
expressions occurring in English text (SIE did not
address the normalization part of the task). The
TERN corpus is divided into two partitions: train-
ing (249,295 tokens) and test (72,667 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set is about
2.1%.
Seminar Announcements The Seminar An-
nouncements (SA) collection (Freitag, 1998) con-
sists of 485 electronic bulletin board postings. The
purpose of each document in the collection is to
announce or relate details of an upcoming talk or
seminar. The documents were annotated for four
entities: speaker, location, stime, and etime. The
corpus is composed by 156, 540 tokens. The frac-
tion of positive examples varies from about 1% to
7http://www.cnts.ua.ac.be/conll2002/
ner/, http://www.cnts.ua.ac.be/conll2003/
ner/.
8http://timex2.mitre.org/tern.html.
13
Metric  ?train/test R P F1 T
0 66.4 67.0 66.7 615
CC 1 64.1/62.3 67.5 67.3 67.4 420
2.5 80.1/78.0 66.6 69.1 67.8 226
5 88.9/86.4 64.8 68.1 66.4 109
OR 1 70.7/68.9 68.3 67.3 67.8 308
2.5 81.0/79.1 67.5 68.3 67.9 193
5 87.8/85.6 65.4 68.2 66.8 114
IC 1 37.3/36.9 58.5 65.7 61.9 570
2.5 38.4/38.0 56.9 65.4 60.9 558
5 39.5/38.9 55.6 65.5 60.1 552
Zhou and Su (2004) 76.0 69.4 72.6
baseline 52.6 43.6 47.7
Table 5: Filtering Rate, Micro-averaged Recall,
Precision, F1 and Time for JNLPBA.
Metric  ?train/test R P F1 T
0 73.6 78.7 76.1 134
CC 1 64.4/64.4 71.6 79.9 75.5 70
2.5 75.1/73.3 72.8 80.3 76.4 50
5 88.6/84.2 66.6 64.7 65.6 24
OR 1 71.5/71.6 72.0 78.3 75.0 61
2.5 82.1/80.7 73.6 78.9 76.2 39
5 90.5/86.1 66.8 64.5 65.6 19
IC 1 47.3/47.5 67.0 79.2 72.6 101
2.5 51.3/51.5 65.9 79.3 72.0 95
5 55.7/56.0 63.8 78.9 70.5 89
Carreras et al (2002) 76.3 77.8 77.1
baseline 45.4 81.3 58.3
Table 6: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
CoNLL-2002 (Dutch).
about 2%. The entire document collection is ran-
domly partitioned five times into two sets of equal
size, training and test (Lavelli et al, 2004). For
each partition, learning is performed on the train-
ing set and performance is measured on the corre-
sponding test set. The resulting figures are aver-
aged over the five test partitions.
7.2 Results
The experimental results in terms of filtering rate,
recall, precision, F1, and computation time for
JNLPBA, CoNLL-2002, CoNLL-2003, TERN and
SA are given in Tables 5, 6, 7, 8 and 9 respectively.
To show the differences among filtering strategies
for JNLPBA, CoNLL-2002, TERN 2004 we used
CC, OR and IC filters, while the results for SA
and CoNLL-2003 are reported only for OR filter
(which usually produces the best performance).
For all filters we report results obtained by set-
ting four different values for parameter , the max-
imum value allowed for the Filtering Rate of pos-
itive examples.  = 0 means that no filter is used.
Metric  ?train/test R P F1 T
0 76.7 90.5 83.1 228
OR 1 70.4/83.9 78.2 88.1 82.8 74
2.5 83.6/95.6 76.4 62.6 68.8 33
5 90.5/97.2 75.3 66.5 70.7 14
Florian et al (2003) 88.5 89.0 88.8
baseline 50.9 71.9 59.6
Table 7: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
CoNLL-2003 (English).
Metric  ?train/test R P F1 T
0 77.9 89.8 83.4 82
CC 1 41.8/41.2 76.6 90.7 83.1 57
2.5 64.5/62.8 60.3 88.6 71.7 41
5 86.9/81.7 59.7 76.0 66.9 14
OR 1 56.4/54.6 77.5 91.1 83.8 48
2.5 69.4/66.7 59.8 88.1 71.2 36
5 82.9/79.0 59.5 88.6 71.2 20
IC 1 17.8/17.4 74.9 91.2 82.3 48
2.5 24.0/23.3 74.8 91.5 82.3 36
5 27.6/27.1 75.0 91.5 82.5 20
Table 8: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
TERN.
The results indicate that both CC and OR do ex-
hibit good performance and are far better than IC
in all the tasks. For example, in the JNLPBA data
set, OR allows to remove more than 70% of the in-
stances, losing less than 1% of the positive exam-
ples. These results pinpoint the importance of us-
ing a supervised metric to collect stop words. The
results also highlight that both CC and OR are ro-
bust against overfitting, because the difference be-
tween the filtering rates in the training and test sets
is minimal. We also report a significant reduction
of the data skewness. Table 10 shows that all the IF
techniques reduce sensibly the skewness ratio, the
ratio between the number of negative and positive
examples, on the JNLPBA data set9. As expected,
both CC and OR consistently outperform IC.
The computation time10 reported includes the
time to perform the overall process of training and
testing the boundary classifiers for each entity11.
The results indicate that both CC and OR are far
superior to IC, allowing a drastic reduction of the
time. Supervised IF techniques are then particu-
9We only report results for this data set as it exhibits the
highest skewness ratios.
10All the experiments have been performed using a dual
1.66 GHz Power Mac G5.
11Execution time for filter optimization is not reported be-
cause it is negligible.
14
Metric  ?train/test R P F1 T
0 81.3 92.5 86.6 179
OR 1 53.6/86.2 81.5 92.1 86.5 91
2.5 69.1/90.8 81.6 90.5 85.9 44
5 74.7/90.8 81.0 85.0 83.0 31
Table 9: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for SA.
entity  CC OR IC
protein 0 17.1 17.1 17.1
1 7.5 3.8 9.6
2.5 3.0 2.5 9.0
5 1.5 1.4 8.8
DNA 0 59.3 59.3 59.3
1 26.4 18.5 33.2
2.5 14.7 12.6 31.7
5 8.3 8.6 32.4
RNA 0 596.2 596.2 596.2
1 250.7 253.1 288.4
2.5 170.4 170.1 274.5
5 92.4 111.1 280.7
cell type 0 72.9 72.9 72.9
1 13.8 13.4 43.2
2.5 6.3 6.5 43.9
5 3.4 4.4 44.5
cell line 0 146.4 146.4 146.4
1 40.4 41.6 87.7
2.5 24.2 25.9 87.5
5 13.6 14.6 89.6
Table 10: Skewness ratio of each entity for
JNLPBA.
larly convenient when dealing with large data sets.
For example, using the CC metric the time re-
quired by SIE to perform the JNLPBA task is re-
duced from 615 to 109 minutes (see Table 5).
Both OR and CC allow to drastically reduce
the computation time and maintain the prediction
accuracy12 with small values of . Using OR,
for example, with  = 2.5% on JNLPBA, F1 in-
creases from 66.7% to 67.9%. On the contrary,
for CoNLL-2002 and TERN, for  > 2.5% and
 > 1% respectively, the performance of all the
filters rapidly declines. The explanation for this
behavior is that, for the last two tasks, the differ-
ence between the filtering rates on the training and
test sets becomes much larger for  > 2.5% and
 > 1%, respectively. That is, the data skewness
changes significantly from the training to the test
set. It is not surprising that an extremely aggres-
sive filtering step reduces too much the informa-
tion available to the classifiers, leading the overall
12For JNLPBA, CoNLL 2002 & 2003 and Tern 2004, re-
sults are obtained using the official evaluation software made
available by the organizers of the tasks.
performance to decrease.
SIE achieves results close to the best systems in
all tasks13. It is worth noting that state-of-the-art
IE systems often exploit external, domain-specific
information (e.g. gazetteers (Carreras et al, 2002)
and lexical resources (Zhou and Su, 2004)) while
SIE adopts exactly the same feature set and does
not use any external or task dependent knowledge
source.
8 Conclusion and Future Work
The portability, the language independence and
the efficiency of SIE suggest its applicability in
practical problems (e.g. semantic web, infor-
mation extraction from biological data) in which
huge collections of texts have to be processed ef-
ficiently. In this perspective we are pursuing the
recognition of bio-entities from several thousands
of MEDLINE abstracts. In addition, the effective-
ness of instance filtering will allow us to experi-
ment with complex kernel methods. For the fu-
ture, we plan to implement more aggressive in-
stance filtering schemata for Entity Recognition,
by performing a deeper semantic analysis of the
texts.
Acknowledgments
SIE was developed in the context of the IST-
Dot.Kom project (http://www.dot-kom.
org), sponsored by the European Commission as
part of the Framework V (grant IST-2001-34038).
Claudio Giuliano and Lorenza Romano have been
supported by the ONTOTEXT project, funded by
the Autonomous Province of Trento under the
FUP-2004 research program.
References
Andrew J. Carlson, ChadM. Cumby, Jeff L. Rosen, and
Dan Roth. 1999. SNoW user?s guide. Technical
Report UIUCDCS-DCS-R-99-210, Department of
Computer Science, University of Illinois at Urbana-
Champaign, April.
Xavier Carreras, Llu??s Ma?rques, and Llu??s Padro?.
2002. Named entity extraction using adaboost. In
Proceedings of CoNLL-2002, Taipei, Taiwan.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
13Note that the TERN results cannot be disclosed, so no di-
rect comparison can be provided. For the reasons mentioned
in (Lavelli et al, 2004), direct comparison cannot be provided
for Seminar Announcements as well.
15
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Fabio Ciravegna. 2000. Learning to tag for infor-
mation extraction. In F. Ciravegna, R. Basili, and
R. Gaizauskas, editors, Proceedings of the ECAI
workshop on Machine Learning for Information Ex-
traction, Berlin.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?
297.
Chad Cumby and W. Yih. 2003. FEX user guide.
Technical report, Department of Computer Science,
University of Illinois at Urbana-Champaign, April.
Aidan Finn and Nicholas Kushmerick. 2004. Multi-
level boundary classification for information extrac-
tion. In Proceedings of the 15th European Confer-
ence on Machine Learning, Pisa, Italy.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Walter Daele-
mans and Miles Osborne, editors, Proceedings of
CoNLL-2003, pages 168?171. Edmonton, Canada.
Dayne Freitag and Nicholas Kushmerick. 2000.
Boosted wrapper induction. In Proceedings of the
17th National Conference on Artificial Intelligence
(AAAI 2000), pages 577?583.
Dayne Freitag. 1998. Machine Learning for Informa-
tion Extraction in Informal Domains. Ph.D. thesis,
Carnegie Mellon University.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2005. Simple information extraction (SIE).
Technical report, ITC-irst.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Raffaella Rinaldi. 2005a. Instance filtering for en-
tity recognition. SIGKDD Explorations (special is-
sue on Text Mining and Natural Language Process-
ing), 7(1):11?18, June.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Raffaella Rinaldi. 2005b. Instance pruning by fil-
tering uninformative words: an Information Extrac-
tion case study. In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2005), Mexico
City, Mexico, 13-19 February.
T. Joachims. 1998. Making large-scale support
vector machine learning practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
J. Kim, T. Ohta, Y. Tateishi, and J. Tsujii. 2003. Ge-
nia corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl.1):180?182.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recog-
nition task at JNLPBA. In N. Collier, P. Ruch,
and A. Nazarenko, editors, Proceedings of the In-
ternational Joint Workshop on Natural Language
Processing in Biomedicine and its Applications
(JNLPBA-2004), pages 70?75, Geneva, Switzer-
land, August 28?29.
A. Lavelli, M. Califf, F. Ciravegna, D. Freitag, C. Giu-
liano, N. Kushmerick, and L. Romano. 2004. IE
evaluation: Criticisms and recommendations. In
AAAI-04 Workshop on Adaptive Text Extraction and
Mining (ATEM-2004), San Jose, California.
Bhavani Raskutti and Adam Kowalczyk. 2004.
Extreme re-balancing for SVMs: a case study.
SIGKDD Explor. Newsl., 6(1):60?69.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmon-
ton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of
CoNLL-2002, pages 155?158. Taipei, Taiwan.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Douglas H. Fisher, editor, Proceedings of the
14th International Conference on Machine Learning
(ICML-97), pages 412?420, Nashville, US. Morgan
Kaufmann Publishers, San Francisco, US.
Guo Dong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recogni-
tion. In Proceedings of 2004 Joint Workshop on Nat-
ural Processing in Biomedicine and its Applications,
Geneva, Switzerland.
16
Syntagmatic Kernels:
a Word Sense Disambiguation Case Study
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Trento, ITALY
{giuliano,gliozzo,strappa}@itc.it
Abstract
In this paper we present a family of ker-
nel functions, named Syntagmatic Ker-
nels, which can be used to model syn-
tagmatic relations. Syntagmatic relations
hold among words that are typically collo-
cated in a sequential order, and thus they
can be acquired by analyzing word se-
quences. In particular, Syntagmatic Ker-
nels are defined by applying a Word Se-
quence Kernel to the local contexts of the
words to be analyzed. In addition, this
approach allows us to define a semi su-
pervised learning schema where external
lexical knowledge is plugged into the su-
pervised learning process. Lexical knowl-
edge is acquired from both unlabeled data
and hand-made lexical resources, such as
WordNet. We evaluated the syntagmatic
kernel on two standard Word Sense Dis-
ambiguation tasks (i.e. English and Ital-
ian lexical-sample tasks of Senseval-3),
where the syntagmatic information plays
a crucial role. We compared the Syntag-
matic Kernel with the standard approach,
showing promising improvements in per-
formance.
1 Introduction
In computational linguistics, it is usual to deal with
sequences: words are sequences of letters and syn-
tagmatic relations are established by sequences of
words. Sequences are analyzed to measure morpho-
logical similarity, to detect multiwords, to represent
syntagmatic relations, and so on. Hence modeling
syntagmatic relations is crucial for a wide variety
of NLP tasks, such as Named Entity Recognition
(Gliozzo et al, 2005a) and Word Sense Disambigua-
tion (WSD) (Strapparava et al, 2004).
In general, the strategy adopted to model syntag-
matic relations is to provide bigrams and trigrams of
collocated words as features to describe local con-
texts (Yarowsky, 1994), and each word is regarded
as a different instance to classify. For instance, oc-
currences of a given class of named entities (such
as names of persons) can be discriminated in texts
by recognizing word patterns in their local contexts.
For example the token Rossi, whenever is preceded
by the token Prof., often represents the name of a
person. Another task that can benefit from modeling
this kind of relations is WSD. To solve ambiguity it
is necessary to analyze syntagmatic relations in the
local context of the word to be disambiguated. In
this paper we propose a kernel function that can be
used to model such relations, the Syntagmatic Ker-
nel, and we apply it to two (English and Italian)
lexical-sample WSD tasks of the Senseval-3 com-
petition (Mihalcea and Edmonds, 2004).
In a lexical-sample WSD task, training data are
provided as a set of texts, in which for each text
a given target word is manually annotated with a
sense from a predetermined set of possibilities. To
model syntagmatic relations, the typical supervised
learning framework adopts as features bigrams and
trigrams in a local context. The main drawback of
this approach is that non contiguous or shifted col-
57
locations cannot be identified, decreasing the gener-
alization power of the learning algorithm. For ex-
ample, suppose that the verb to score has to be dis-
ambiguated into the sentence ?Ronaldo scored the
goal?, and that the sense tagged example ?the foot-
ball player scores#1 the first goal? is provided for
training. A traditional feature mapping would ex-
tract the bigram w+1 w+2:the goal to represent the
former, and the bigram w+1 w+2:the first to index
the latter. Evidently such features will not match,
leading the algorithm to a misclassification.
In the present paper we propose the Syntagmatic
Kernel as an attempt to solve this problem. The
Syntagmatic Kernel is based on a Gap-Weighted
Subsequences Kernel (Shawe-Taylor and Cristian-
ini, 2004). In the spirit of Kernel Methods, this
kernel is able to compare sequences directly in the
input space, avoiding any explicit feature mapping.
To perform this operation, it counts how many times
a (non-contiguous) subsequence of symbols u of
length n occurs in the input string s, and penalizes
non-contiguous occurrences according to the num-
ber of the contained gaps. To define our Syntag-
matic Kernel, we adapted the generic definition of
the Sequence Kernels to the problem of recognizing
collocations in local word contexts.
In the above definition of Syntagmatic Kernel,
only exact word-matches contribute to the similar-
ity. One shortcoming of this approach is that (near-
)synonyms will never be considered similar, lead-
ing to a very low generalization power of the learn-
ing algorithm, that requires a huge amount of data
to converge to an accurate prediction. To solve this
problem we provided external lexical knowledge to
the supervised learning algorithm, in order to define
a ?soft-matching? schema for the kernel function.
For example, if we consider as equivalent the terms
Ronaldo and football player, the proposition ?The
football player scored the first goal? is equivalent to
the sentence ?Ronaldo scored the first goal?, pro-
viding a strong evidence to disambiguate the latter
occurrence of the verb.
We propose two alternative soft-matching criteria
exploiting two different knowledge sources: (i) hand
made resources and (ii) unsupervised term similar-
ity measures. The first approach performs a soft-
matching among all those synonyms words in Word-
Net, while the second exploits domain relations, ac-
quired from unlabeled data, for the same purpose.
Our experiments, performed on two standard
WSD benchmarks, show the superiority of the Syn-
tagmatic Kernel with respect to a classical flat vector
representation of bigrams and trigrams.
The paper is structured as follows. Section 2 in-
troduces the Sequence Kernels. In Section 3 the
Syntagmatic Kernel is defined. Section 4 explains
how soft-matching can be exploited by the Collo-
cation Kernel, describing two alternative criteria:
WordNet Synonymy and Domain Proximity. Sec-
tion 5 gives a brief sketch of the complete WSD
system, composed by the combination of different
kernels, dealing with syntagmatic and paradigmatic
aspects. Section 6 evaluates the Syntagmatic Kernel,
and finally Section 7 concludes the paper.
2 Sequence Kernels
The basic idea behind kernel methods is to embed
the data into a suitable feature space F via a map-
ping function ? : X ? F , and then use a linear al-
gorithm for discovering nonlinear patterns. Instead
of using the explicit mapping ?, we can use a kernel
function K : X ? X ? R, that corresponds to the
inner product in a feature space which is, in general,
different from the input space.
Kernel methods allow us to build a modular sys-
tem, as the kernel function acts as an interface be-
tween the data and the learning algorithm. Thus
the kernel function becomes the only domain spe-
cific module of the system, while the learning algo-
rithm is a general purpose component. Potentially
any kernel function can work with any kernel-based
algorithm. In our system we use Support Vector Ma-
chines (Cristianini and Shawe-Taylor, 2000).
Sequence Kernels (or String Kernels) are a fam-
ily of kernel functions developed to compute the
inner product among images of strings in high-
dimensional feature space using dynamic program-
ming techniques (Shawe-Taylor and Cristianini,
2004). The Gap-Weighted Subsequences Kernel is
the most general Sequence Kernel. Roughly speak-
ing, it compares two strings by means of the num-
ber of contiguous and non-contiguous substrings of
a given length they have in common. Non contigu-
ous occurrences are penalized according to the num-
ber of gaps they contain.
58
Formally, let ? be an alphabet of |?| symbols,
and s = s1s2 . . . s|s| a finite sequence over ? (i.e.
si ? ?, 1 6 i 6 |s|). Let i = [i1, i2, . . . , in], with
1 6 i1 < i2 < . . . < in 6 |s|, be a subset of the
indices in s: we will denote as s[i] ? ?n the sub-
sequence si1si2 . . . sin . Note that s[i] does not nec-
essarily form a contiguous subsequence of s. For
example, if s is the sequence ?Ronaldo scored the
goal? and i = [2, 4], then s[i] is ?scored goal?. The
length spanned by s[i] in s is l(i) = in ? i1 + 1.
The feature space associated with the Gap-Weighted
Subsequences Kernel of length n is indexed by I =
?n, with the embedding given by
?nu(s) =
X
i:u=s[i]
?l(i), u ? ?n, (1)
where ? ?]0, 1] is the decay factor used to penalize
non-contiguous subsequences1 . The associate ker-
nel is defined as
Kn(s, t) = ??n(s), ?n(t)? =
X
u??n
?nu(s)?nu(t). (2)
An explicit computation of Equation 2 is unfea-
sible even for small values of n. To evaluate more
efficiently Kn, we use the recursive formulation pro-
posed in (Lodhi et al, 2002; Saunders et al, 2002;
Cancedda et al, 2003) based on a dynamic program-
ming implementation. It is reported in the following
equations:
K?0(s, t) = 1, ?s, t, (3)
K?i(s, t) = 0, if min(|s|, |t|) < i, (4)
K??i (s, t) = 0, if min(|s|, |t|) < i, (5)
K??i (sx, ty) =
(
?K??i (sx, t), if x 6= y;
?K??i (sx, t) + ?2K?i?1(s, t), otherwise.
(6)
K?i(sx, t) = ?K?i(s, t) + K??i (sx, t), (7)
Kn(s, t) = 0, if min(|s|, |t|) < n, (8)
Kn(sx, t) = Kn(s, t) +
X
j:tj=x
?2K?n?1(s, t[1 : j ? 1]),
(9)
K ?n and K ??n are auxiliary functions with a sim-
ilar definition as Kn used to facilitate the compu-
tation. Based on all definitions above, Kn can be
1Notice that by choosing ? = 1 sparse subsequences are
not penalized. On the other hand, the kernel does not take into
account sparse subsequences with ? ? 0.
computed in O(n|s||t|). Using the above recursive
definition, it turns out that computing all kernel val-
ues for subsequences of lengths up to n is not signif-
icantly more costly than computing the kernel for n
only.
In the rest of the paper we will use the normalised
version of the kernel (Equation 10) to keep the val-
ues comparable for different values of n and to be
independent from the length of the sequences.
K?(s, t) = K(s, t)p
K(s, s)K(t, t)
. (10)
3 The Syntagmatic Kernel
As stated in Section 1, syntagmatic relations hold
among words arranged in a particular temporal or-
der, hence they can be modeled by Sequence Ker-
nels. The Syntagmatic Kernel is defined as a linear
combination of Gap-Weighted Subsequences Ker-
nels that operate at word and PoS tag level. In partic-
ular, following the approach proposed by Cancedda
et al (2003), it is possible to adapt sequence kernels
to operate at word level by instancing the alphabet ?
with the vocabulary V = {w1, w2, . . . , wk}. More-
over, we restricted the generic definition of the Gap-
Weighted Subsequences Kernel to recognize collo-
cations in the local context of a specified word. The
resulting kernel, called n-gram Collocation Kernel
(KnColl), operates on sequences of lemmata around a
specified word l0 (i.e. l?3, l?2, l?1, l0, l+1, l+2, l+3).
This formulation allows us to estimate the number of
common (sparse) subsequences of lemmata (i.e. col-
locations) between two examples, in order to capture
syntagmatic similarity.
Analogously, we defined the PoS Kernel (KnPoS)
to operate on sequences of PoS tags p?3, p?2, p?1,
p0, p+1, p+2, p+3, where p0 is the PoS tag of l0.
The Collocation Kernel and the PoS Kernel are
defined by Equations 11 and 12, respectively.
KColl(s, t) =
n
?
l=1
K lColl(s, t) (11)
and
KPoS(s, t) =
n
?
l=1
K lP oS(s, t). (12)
Both kernels depend on the parameter n, the length
of the non-contiguous subsequences, and ?, the de-
59
cay factor. For example, K2Coll allows us to repre-
sent all (sparse) bi-grams in the local context of a
word.
Finally, the Syntagmatic Kernel is defined as
KSynt(s, t) = KColl(s, t) + KPoS(s, t). (13)
We will show that in WSD, the Syntagmatic Ker-
nel is more effective than standard bigrams and tri-
grams of lemmata and PoS tags typically used as
features.
4 Soft-Matching Criteria
In the definition of the Syntagmatic Kernel only ex-
act word matches contribute to the similarity. To
overcome this problem, we further extended the def-
inition of the Gap-Weigthed Subsequences Kernel
given in Section 2 to allow soft-matching between
words. In order to develop soft-matching criteria,
we follow the idea that two words can be substi-
tuted preserving the meaning of the whole sentence
if they are paradigmatically related (e.g. synomyns,
hyponyms or domain related words). If the meaning
of the proposition as a whole is preserved, the mean-
ing of the lexical constituents of the sentence will
necessarily remain unchanged too, providing a vi-
able criterion to define a soft-matching schema. This
can be implemented by ?plugging? external paradig-
matic information into the Collocation kernel.
Following the approach proposed by (Shawe-
Taylor and Cristianini, 2004), the soft-matching
Gap-Weighted Subsequences Kernel is now calcu-
lated recursively using Equations 3 to 5, 7 and 8,
replacing Equation 6 by the equation:
K??i (sx, ty) = ?K??i (sx, t) + ?2axyK?i?1(s, t),?x, y, (14)
and modifying Equation 9 to:
Kn(sx, t) = Kn(s, t) +
|t|
X
j
?2axtjK
?
n?1(s, t[1 : j ? 1]).
(15)
where axy are entries in a similarity matrix A be-
tween symbols (words). In order to ensure that the
resulting kernel is valid, A must be positive semi-
definite.
In the following subsections, we describe two al-
ternative soft-matching criteria based on WordNet
Synonymy and Domain Proximity. In both cases, to
show that the similarity matrices are a positive semi-
definite we use the following result:
Proposition 1 A matrix A is positive semi-definite
if and only if A = BTB for some real matrix B.
The proof is given in (Shawe-Taylor and Cristianini,
2004).
4.1 WordNet Synonymy
The first solution we have experimented exploits a
lexical resource representing paradigmatic relations
among terms, i.e. WordNet. In particular, we used
WordNet-1.7.1 for English and the Italian part of
MultiWordNet2.
In order to find a similarity matrix between terms,
we defined a vector space where terms are repre-
sented by the WordNet synsets in which such terms
appear. Hence, we can view a term as vector in
which each dimension is associated with one synset.
The term-by-synset matrix S is then the matrix
whose rows are indexed by the synsets. The en-
try xij of S is 1 if the synset sj contains the term
wi, and 0 otherwise. The term-by-synset matrix S
gives rise to the similarity matrix A = SST be-
tween terms. Since A can be rewritten as A =
(ST )TST = BTB, it follows directly by Proposi-
tion 1 that it is positive semi-definite.
It is straightforward to extend the soft-matching
criterion to include hyponym relation, but we
achieved worse results. In the evaluation section we
will not report such results.
4.2 Domain Proximity
The approach described above requires a large scale
lexical resource. Unfortunately, for many languages,
such a resource is not available. Another possibility
for implementing soft-matching is introducing the
notion of Semantic Domains.
Semantic Domains are groups of strongly
paradigmatically related words, and can be acquired
automatically from corpora in a totally unsuper-
vised way (Gliozzo, 2005). Our proposal is to ex-
ploit a Domain Proximity relation to define a soft-
matching criterion on the basis of an unsupervised
similarity metric defined in a Domain Space. The
Domain Space can be determined once a Domain
2http://multiwordnet.itc.it
60
Model (DM) is available. This solution is evidently
cheaper, because large collections of unlabeled texts
can be easily found for every language.
A DM is represented by a k ? k? rectangular ma-
trix D, containing the domain relevance for each
term with respect to each domain, as illustrated in
Table 1. DMs can be acquired from texts by exploit-
MEDICINE COMPUTER SCIENCE
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
Table 1: Example of Domain Model.
ing a lexical coherence assumption (Gliozzo, 2005).
To this aim, Term Clustering algorithms can be used:
a different domain is defined for each cluster, and
the degree of association between terms and clusters,
estimated by the unsupervised learning algorithm,
provides a domain relevance function. As a clus-
tering technique we exploit Latent Semantic Analy-
sis (LSA), following the methodology described in
(Gliozzo et al, 2005b). This operation is done off-
line, and can be efficiently performed on large cor-
pora.
LSA is performed by means of SVD of the term-
by-document matrixT representing the corpus. The
SVD algorithm can be exploited to acquire a domain
matrix D from a large corpus in a totally unsuper-
vised way. SVD decomposes the term-by-document
matrix T into three matrices T = V?kUT where
?k is the diagonal k ? k matrix containing the k
singular values of T. D = V?k? where k?  k.
Once a DM has been defined by the matrixD, the
Domain Space is a k? dimensional space, in which
both texts and terms are represented by means of
Domain Vectors (DVs), i.e. vectors representing the
domain relevances among the linguistic object and
each domain. The DV ~w?i for the term wi ? V is the
ith row of D, where V = {w1, w2, . . . , wk} is the
vocabulary of the corpus.
The term-by-domain matrix D gives rise to the
term-by-term similarity matrix A = DDT among
terms. It follows from Proposition 1 that A is posi-
tive semi-definite.
5 Kernel Combination for WSD
To improve the performance of a WSD system, it
is possible to combine different kernels. Indeed,
we followed this approach in the participation to
Senseval-3 competition, reaching the state-of-the-
art in many lexical-sample tasks (Strapparava et al,
2004). While this paper is focused on Syntagmatic
Kernels, in this section we would like to spend some
words on another important component for a com-
plete WSD system: the Domain Kernel, used to
model domain relations.
Syntagmatic information alone is not sufficient to
define a full kernel for WSD. In fact, in (Magnini
et al, 2002), it has been claimed that knowing the
domain of the text in which the word is located is a
crucial information for WSD. For example the (do-
main) polysemy among the COMPUTER SCIENCE
and the MEDICINE senses of the word virus can
be solved by simply considering the domain of the
context in which it is located.
This fundamental aspect of lexical polysemy can
be modeled by defining a kernel function to esti-
mate the domain similarity among the contexts of
the words to be disambiguated, namely the Domain
Kernel. The Domain Kernel measures the similarity
among the topics (domains) of two texts, so to cap-
ture domain aspects of sense distinction. It is a vari-
ation of the Latent Semantic Kernel (Shawe-Taylor
and Cristianini, 2004), in which a DM is exploited
to define an explicit mapping D : Rk ? Rk? from
the Vector Space Model (Salton and McGill, 1983)
into the Domain Space (see Section 4), defined by
the following mapping:
D(~tj) = ~tj(IIDFD) = ~t?j (16)
where IIDF is a k ? k diagonal matrix such that
iIDFi,i = IDF (wi), ~tj is represented as a row vector,
and IDF (wi) is the Inverse Document Frequency of
wi. The Domain Kernel is then defined by:
KD(ti, tj) =
?D(ti),D(tj)?
?
?D(tj),D(tj)??D(ti),D(ti)?
(17)
The final system for WSD results from a com-
bination of kernels that deal with syntagmatic and
paradigmatic aspects (i.e. PoS, collocations, bag of
words, domains), according to the following kernel
61
combination schema:
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj , xj)Kl(xi, xi)
(18)
6 Evaluation
In this section we evaluate the Syntagmatic Kernel,
showing that it improves over the standard feature
extraction technique based on bigrams and trigrams
of words and PoS tags.
6.1 Experimental settings
We conducted the experiments on two lexical sam-
ple tasks (English and Italian) of the Senseval-3
competition (Mihalcea and Edmonds, 2004). In
lexical-sample WSD, after selecting some target
words, training data is provided as a set of texts.
For each text a given target word is manually anno-
tated with a sense from a predetermined set of pos-
sibilities. Table 2 describes the tasks by reporting
the number of words to be disambiguated, the mean
polysemy, and the dimension of training, test and
unlabeled corpora. Note that the organizers of the
English task did not provide any unlabeled material.
So for English we used a domain model built from
the training partition of the task (obviously skipping
the sense annotation), while for Italian we acquired
the DM from the unlabeled corpus made available
by the organizers.
#w pol # train # test # unlab
English 57 6.47 7860 3944 7860
Italian 45 6.30 5145 2439 74788
Table 2: Dataset descriptions.
6.2 Performance of the Syntagmatic Kernel
Table 3 shows the performance of the Syntagmatic
Kernel on both data sets. As baseline, we report
the result of a standard approach consisting on ex-
plicit bigrams and trigrams of words and PoS tags
around the words to be disambiguated (Yarowsky,
1994). The results show that the Syntagmatic Ker-
nel outperforms the baseline in any configuration
(hard/soft-matching). The soft-matching criteria
further improve the classification performance. It
is interesting to note that the Domain Proximity
methodology obtained better results than WordNet
Standard approach
English Italian
Bigrams and trigrams 67.3 51.0
Syntagmatic Kernel
Hard matching 67.7 51.9
Soft matching (WordNet) 67.3 51.3
Soft matching (Domain proximity) 68.5 54.0
Table 3: Performance (F1) of the Syntagmatic Ker-
nel.
Synonymy. The different results observed between
Italian and English using the Domain Proximity
soft-matching criterion are probably due to the small
size of the unlabeled English corpus.
In these experiments, the parameters n and ? are
optimized by cross-validation. For KnColl, we ob-
tained the best results with n = 2 and ? = 0.5. For
KnPoS , n = 3 and ? ? 0. The domain cardinality k?
was set to 50.
Finally, the global performance (F1) of the full
WSD system (see Section 5) on English and Italian
lexical sample tasks is 73.3 for English and 61.3 for
Italian. To our knowledge, these figures represent
the current state-of-the-art on these tasks.
7 Conclusion and Future Work
In this paper we presented the Syntagmatic Kernels,
i.e. a set of kernel functions that can be used to
model syntagmatic relations for a wide variety of
Natural Language Processing tasks. In addition, we
proposed two soft-matching criteria for the sequence
analysis, which can be easily modeled by relax-
ing the constraints in a Gap-Weighted Subsequences
Kernel applied to local contexts of the word to be
analyzed. Experiments, performed on two lexical
sample Word Sense Disambiguation benchmarks,
show that our approach further improves the stan-
dard techniques usually adopted to deal with syntag-
matic relations. In addition, the Domain Proximity
soft-matching criterion allows us to define a semi-
supervised learning schema, improving the overall
results.
For the future, we plan to exploit the Syntagmatic
Kernel for a wide variety of Natural Language Pro-
cessing tasks, such as Entity Recognition and Re-
lation Extraction. In addition we are applying the
soft matching criteria here defined to Tree Kernels,
62
in order to take into account lexical variability in
parse trees. Finally, we are going to further improve
the soft-matching criteria here proposed by explor-
ing the use of entailment criteria for substitutability.
Acknowledgments
The authors were partially supported by the Onto-
Text Project, funded by the Autonomous Province
of Trento under the FUP-2004 research program.
References
N. Cancedda, E. Gaussier, C. Goutte, and J.M. Renders.
2003. Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059?1082.
N. Cristianini and J. Shawe-Taylor. 2000. An introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
A. Gliozzo, C. Giuliano, and R. Rinaldi. 2005a. Instance
filtering for entity recognition. ACM SIGKDD Explo-
rations, special Issue on Natural Language Processing
and Text Mining, 7(1):11?18, June.
A. Gliozzo, C. Giuliano, and C. Strapparava. 2005b. Do-
main kernels for word sense disambiguation. In Pro-
ceedings of the 43rd annual meeting of the Association
for Computational Linguistics (ACL-05), pages 403?
410, Ann Arbor, Michigan, June.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
H. Lodhi, J. Shawe-Taylor, N. Cristianini, and
C. Watkins. 2002. Text classification using string
kernels. Journal of Machine Learning Research,
2(3):419?444.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
R. Mihalcea and P. Edmonds, editors. 2004. Proceed-
ings of SENSEVAL-3: Third International Workshop
on the Evaluation of Systems for the Semantic Analy-
sis of Text, Barcelona, Spain, July.
G. Salton and M.H. McGill. 1983. Introduction to mod-
ern information retrieval. McGraw-Hill, New York.
C. Saunders, H. Tschach, and J. Shawe-Taylor. 2002.
Syllables and other string kernel extensions. In Pro-
ceedings of 19th International Conference onMachine
Learning (ICML02).
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis. Cambridge University Press.
C. Strapparava, C. Giuliano, and A. Gliozzo. 2004. Pat-
tern abstraction and term similarity for word sense dis-
ambiguation: IRST at Senseval-3. In Proceedings of
SENSEVAL-3: Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of
Text, Barcelona, Spain, July.
D. Yarowsky. 1994. Decision lists for lexical ambiguity
resolution: Application to accent restoration in span-
ish and french. In Proceedings of the 32nd Annual
Meeting of the ACL, pages 88?95, Las Cruces, New
Mexico.
63
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 145?148,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-irst: Lexical Substitution Task Exploiting
Domain and Syntagmatic Coherence
Claudio Giuliano and Alfio Gliozzo and Carlo Strapparava
FBK-irst, I-38050, Povo, Trento, ITALY
{giuliano, gliozzo, strappa}@itc.it
Abstract
This paper summarizes FBK-irst participa-
tion at the lexical substitution task of the
SEMEVAL competition. We submitted two
different systems, both exploiting synonym
lists extracted from dictionaries. For each
word to be substituted, the systems rank the
associated synonym list according to a simi-
larity metric based on Latent Semantic Anal-
ysis and to the occurrences in the Web 1T
5-gram corpus, respectively. In particular,
the latter system achieves the state-of-the-art
performance, largely surpassing the baseline
proposed by the organizers.
1 Introduction
The lexical substitution (Glickman et al, 2006a) can
be regarded as a subtask of the lexical entailment,
in which for a given word in context the system is
asked to select an alternative word that can be re-
placed in that context preserving the meaning. Lex-
ical Entailment, and in particular lexical reference
(Glickman et al, 2006b)1 , is in turn a subtask of tex-
tual entailment, which is formally defined as a rela-
tionship between a coherent text T and a language
expression, the hypothesis H . T is said to entail H ,
denoted by T ? H , if the meaning of H can be in-
ferred from the meaning of T (Dagan et al, 2005;
Dagan and Glickman., 2004). Even though this no-
tion has been only recently proposed in the computa-
tional linguistics literature, it attracts more and more
attention due to the high generality of its settings and
to the usefulness of its (potential) applications.
1In the literature, slight variations of this problem have been
also referred to as sense matching (Dagan et al, 2006).
With respect to lexical entailment, the lexical sub-
stitution task has a more restrictive criterion. In
fact, two words can be substituted when meaning is
preserved, while the criterion for lexical entailment
is that the meaning of the thesis is implied by the
meaning of the hypothesis. The latter condition is in
general ensured by substituting either hyperonyms
or synonyms, while the former is more rigid because
only synonyms are in principle accepted.
Formally, in a lexical entailment task a system is
asked to decide whether the substitution of a par-
ticular term w with the term e in a coherent text
Hw = H lwHr generates a sentence He = H leHr
such that Hw ? He, where H l and Hr denote the
left and the right context of w, respectively. For
example, given the source word ?weapon? a system
may substitute it with the target synonym ?arm?, in
order to identify relevant texts that denote the sought
concept using the latter term.
A particular case of lexical entailment is recog-
nizing synonymy, where both Hw ? He and He ?
Hw hold. The lexical substitution task at SEMEVAL
addresses exactly this problem. The task is not easy
since lists of candidate entailed words are not pro-
vided by the organizers. Therefore the system is
asked first to identify a set of candidate words, and
then to select only those words that fit in a particu-
lar context. To promote unsupervised methods, the
organizers did not provide neither labeled data for
training nor dictionaries or list of synonyms explain-
ing the meanings of the entailing words.
In this paper, we describe our approach to the
Lexical Substitution task at SEMEVAL 2007. We
developed two different systems (named IRST1-lsa
and IRST2-syn in the official task ranking), both ex-
ploiting a common lists of synonyms extracted from
dictionaries (i.e. WordNet and the Oxford Dictio-
145
nary) and ranking them according to two different
criteria:
Domain Proximity: the similarity between each
candidate entailed word and the context of the
entailing word is estimated by means of a co-
sine between their corresponding vectors in the
LSA space.
Syntagmatic Coherence: querying a large corpus,
the system finds all occurrences of the target
sentence, in which the entailing word is substi-
tuted with each synonym, and it assigns scores
proportional to the occurrence frequencies.
Results show that both methods are effective. In
particular, the second method achieved the best per-
formance in the competition, defining the state-of-
the-art for the lexical substitution task.
2 Lexical Substitution Systems
The lexical substitution task is a textual entailment
subtask in which the system is asked to provide one
or more terms e ? E ? syn(w) that can be sub-
stituted to w in a particular context Hw = H lwHr
generating a sentence He = H leHr such that both
Hw ? He and He ? Hw hold, where syn(w) is the
set of synonyms lemmata obtained from all synset in
which w appears in WordNet and H l and Hr denote
the left and the right context of w, respectively.
The first step, common to both systems, consists
of determining the set of synonyms syn(w) for each
entailing word (see Section 2.1). Then, each system
ranks the extracted lists according to the criteria de-
scribed in Section 2.2 and 2.3.
2.1 Used Lexical Resources
For selecting the synonym candidates we used two
lexical repositories: WordNet 2.0 and the Oxford
American Writer Thesaurus (1st Edition). For each
target word, we simply collect all the synonyms for
all the word senses in both these resources.
We exploited two corpora for our systems: the
British National Corpus for acquiring the LSA space
for ranking with domain proximity measure (Sec-
tion 2.2) and the Web 1T 5-gram Version 1 corpus
from Google (distributed by Linguistic Data Consor-
tium)2 for ranking the proposed synonyms accord-
ing to syntagmatic coherence (Section 2.3).
2Available from http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13.
No other resources were used and the sense rank-
ing in WordNet was not considered at all. Therefore
our system is fully unsupervised.
2.2 Domain Proximity
Semantic Domains are common areas of human dis-
cussion, such as Economics, Politics, Law (Magnini
et al, 2002). Semantic Domains can be described
by DMs (Gliozzo, 2005), by defining a set of term
clusters, each representing a Semantic Domain, i.e.
a set of terms having similar topics. A DM is repre-
sented by a k ? k? rectangular matrix D, containing
the domain relevance for each term with respect to
each domain.
DMs can be acquired from texts by exploiting
term clustering algorithms. The degree of associ-
ation among terms and clusters, estimated by the
learning algorithm, provides a domain relevance
function. For our experiments we adopted a clus-
tering strategy based on Latent Semantic Analy-
sis (LSA) (Deerwester et al, 1990), following the
methodology described in (Gliozzo, 2005).
The input of the LSA process is a Term by Docu-
ment matrix T of the frequencies in the whole cor-
pus for each term. In this work we indexed all lem-
matized terms. The so obtained matrix is then de-
composed by means of a Singular Value Decompo-
sition, identifying the principal components of T.
Once a DM has been defined by the matrix D, the
Domain Space is a k? dimensional space, in which
both texts and terms are associated to Domain Vec-
tors (DVs), i.e. vectors representing their domain
relevance with respect to each domain. The DV ~t?i
for the term ti ? V is the ith row of D, where
V = {t1, t2, . . . , tk} is the vocabulary of the cor-
pus. The DVs for texts are obtained by mapping the
document vectors ~dj , represented in the vector space
model, into the vectors ~d?j in the Domain Space, de-fined by
D(~dj) = ~dj(IIDFD) = ~d?j (1)
where IIDF is a diagonal matrix such that iIDFi,i =
IDF (wi) and IDF (wi) is the Inverse Document
Frequency of wi. The similarity among both texts
and terms in the Domain Space is then estimated by
the cosine operation.
To implement our lexical substitution criterion we
ranked the candidate entailed words according to
their domain proximity, following the intuition that
if two words can be substituted in a particular con-
text, then the entailed word should belong to the
146
same semantic domain of the context in which the
entailing word is located.
The intuition above can be modeled by estimating
the similarity in the LSA space between the pseudo
document, estimated by Equation 1, formed by all
the words in the context of the entailing word (i.e.
the union of H l and Hr), and each candidate en-
tailed word in syn(w).
2.3 Syntagmatic Coherence
The syntagmatic coherence criterion is based on the
following observation. If the entailing word w in
its context Hw = H lwHr is actually entailed by
a word e, then there exist some occurrences on the
WEB of the expression He = H leHr, obtained
by replacing the entailing word with the candidate
entailed word. This intuition can be easily imple-
mented by looking for occurrences of He in the Web
1T 5-gram Version 1 corpus.
Figure 1 presents pseudo-code for the synonym
scoring procedure. The procedure takes as input the
set of candidate entailed words E = syn(w) for the
entailing word w, the context Hw in which w oc-
curs, the length of the n-gram (2 6 n 6 5) and the
target word itself. For each candidate entailed word
ei, the procedure ngrams(Hw, w, ei, n) is invoked
to substitute w with ei in Hw, obtaining Hei , and re-turns the set Q of all n-grams containing ei. For ex-
ample, all 3-grams obtained replacing ?bright? with
the synonym ?intelligent? in the sentence ?He was
bright and independent and proud.? are ?He was in-
telligent?, ?was intelligent and? and ?intelligent and
independent?. The maximum number of n-grams
generated is ?5n=2 n. Each candidate synonym isthen assigned a score by summing all the frequen-
cies in the Web 1T corpus of the so generated n-
grams3. The set of synonyms is ranked according
the so obtained scores. However, candidates which
appear in longer n-grams are preferred to candidates
appearing in shorter ones. Therefore, the ranked list
contains first the candidate entailed words appearing
in 5-grams, if any, then those appearing in 4-grams,
and so on. For example, a candidate e1 that appears
only once in 5-grams is preferred to a candidate e2
that appears 1000 times in 4-grams. Note that this
strategy could lead to an output list with repetitions.
3Note that n-grams with frequency lower than 40 are not
present in the corpus.
1: Given E, the set of candidate synonyms
2: Given H , the context in which w occurs
3: Given n, the length of the n-gram
4: Given w, the word to be substituted
5: E? ? ?
6: for each ei in E do
7: Q? ngrams(H,w, ei, n)
8: scorei ? 0
9: for each qj in Q do
10: Get the frequency fj of qj
11: scorei ? scorei + fj
12: end for
13: if scorei > 0 then add the pair {scorei, ei}
in E?
14: end for
15: Return E?
Figure 1: The synonym scoring procedure
3 Evaluation
There are basically two scoring methodologies: (i)
BEST, which scores the best substitute for a given
item, and (ii) OOT, which scores for the best 10 sub-
stitutes for a given item, and systems do not benefit
from providing less responses4 .
BEST. Table 1 and 2 report the performance for the
domain proximity and syntagmatic coherence rank-
ing. Please note that in Table 2 we report both the
official score and a score that takes into account just
the first proposal of the systems, as the usual in-
terpretation of BEST score methodology would sug-
gest5.
OOT. Table 4 and 5 report the performance for the
domain proximity and syntagmatic coherence rank-
ing, scoring for the 10 best substitutes. The results
are quite good especially in the case of syntagmatic
coherence ranking.
Baselines. Table 3 displays the baselines respec-
tively for the BEST and OOT using WordNet 2.1
as calculated by the task organizers. They pro-
pose many baseline measures, but we report only the
4The task proposed a third scoring measure MW that scores
precision and recall for detection and identification of multi-
words in the input sentences. However our systems were not
designed for this functionality. For the details of all scoring
methodologies please refer to the task description documents.
5We misinterpreted that the official scorer divides anyway
the figures by the number of proposals. So for the competition
we submitted the oot result file without cutting the words after
the first one.
147
P R Mode P Mode R
all 8.06 8.06 13.09 13.09
Table 1: BEST results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 12.93 12.91 20.33 20.33
all (official) 6.95 6.94 20.33 20.33
Table 2: BEST results for Syntagmatic ranking
(IRST2-syn)
WordNet one, as it is the higher scoring baseline. We
can observe that globally our systems perform quite
good with respect to the baselines.
4 Conclusion
In this paper we reported a detailed description of
the FBK-irst systems submitted to the Lexical En-
tailment task at the SEMEVAL 2007 evaluation cam-
paign. Our techniques are totally unsupervised, as
they do not require neither the availability of sense
tagged data nor an estimation of sense priors, not
considering the WordNet sense order information.
Results are quite good, as in general they signifi-
cantly outperform all the baselines proposed by the
organizers. In addition, the method based on syn-
tagmatic coherence estimated on the WEB outper-
forms, to our knowledge, the other systems sub-
mitted to the competition. For the future, we plan
to avoid the use of dictionaries by adopting term
similarity techniques to select the candidate entailed
words and to exploit this methodology in some spe-
cific applications such as taxonomy induction and
ontology population.
Acknowledgments
Claudio Giuliano is supported by the X-Media
project (http://www.x-media-project.
org), sponsored by the European Commission
as part of the Information Society Technologies
(IST) programme under EC grant number IST-FP6-
026978. Alfio Gliozzo is supported by FIRB-Israel
P R Mode P Mode R
WN BEST 9.95 9.95 15.28 15.28
WN OOT 29.70 29.35 40.57 40.57
Table 3: WordNet Baselines
P R Mode P Mode R
all 41.23 41.20 55.28 55.28
Table 4: OOT results for LSA ranking (IRST1-lsa)
P R Mode P Mode R
all 69.03 68.90 58.54 58.54
Table 5: OOT results for Syntagmatic ranking
(IRST2-syn)
research project N. RBIN045PXH.
References
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. In proceedings of the PASCAL Workshop
on Learning Methods for Text Understanding and Min-
ing, Grenoble.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pas-
cal recognising textual entailment challenge. Proceed-
ings of the PASCAL Challenges Workshop on Recog-
nising Textual Entailment.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings ACL-2006,
pages 449?456, Sydney, Australia, July.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society of Information
Science.
O. Glickman, I. Dagan, M. Keller, S. Bengio, and
W. Daelemans. 2006a. Investigating lexical substi-
tution scoring for subtitle generation tenth conference
on computational natural language learning. In Pro-
ceedings of CoNLL-2006.
O. Glickman, E. Shnarch, and I. Dagan. 2006b. Lexical
reference: a semantic matching subtask. In proceed-
ings of EMNLP 2006.
A. Gliozzo. 2005. Semantic Domains in Computa-
tional Linguistics. Ph.D. thesis, ITC-irst/University of
Trento.
B. Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.
2002. The role of domain information in word
sense disambiguation. Natural Language Engineer-
ing, 8(4):359?373.
148
Kernel Methods for Minimally Supervised WSD
Claudio Giuliano?
Fondazione Bruno Kessler ? IRST
Alfio Massimiliano Gliozzo??
Fondazione Bruno Kessler ? IRST
Carlo Strapparava?
Fondazione Bruno Kessler ? IRST
We present a semi-supervised technique for word sense disambiguation that exploits external
knowledge acquired in an unsupervised manner. In particular, we use a combination of basic
kernel functions to independently estimate syntagmatic and domain similarity, building a set of
word-expert classifiers that share a common domain model acquired from a large corpus of un-
labeled data. The results show that the proposed approach achieves state-of-the-art performance
on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although
it uses a considerably smaller number of training examples than other methods.
1. Introduction
A significant challenge in many natural language processing tasks is to reduce the need
for labeled training data while maintaining an acceptable performance. This is espe-
cially true for word sense disambiguation (WSD) because when moving from the some-
what artificial lexical-sample task to the more realistic all-words task it is practically
impossible to collect a large number of training examples for each word sense. Thus,
many supervised approaches, explicitly designed for the lexical-sample task, cannot be
applied to the all-words task, even though they exhibit excellent performance. This has
led to the somewhat paradoxical situation in which completely different methods have
been developed for the two tasks, although they represent two sides of the same coin.
To address this problem, in recent work we presented a semi-supervised approach
based on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo,
Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006). In particular,
we explored the following research directions: (1) independently modeling domain and
syntagmatic aspects of sense distinction to improve feature representativeness; and
(2) exploiting external knowledge acquired from unlabeled data, with the purpose of
drastically reducing the amount of labeled training data. The first direction is based on
the linguistic assumption that syntagmatic and domain (associative) relations are crucial
for representing sense distinctions, but they are originated by different phenomena.
Regarding the second direction, one can hope to obtain a more accurate prediction
? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: giuliano@fbk.eu.
?? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: gliozzo@fbk.eu.
? FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy. E-mail: strappa@fbk.eu.
Submission received: 23 December 2006; revised submission received: 28 February 2008; accepted for
publication: 17 April 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
by taking into account unlabeled data relevant to the learning problem (Chapelle,
Scho?lkopf, and Zien 2006). As a matter of fact, to test this hypothesis, most of the lexical
sample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a large
amount of unlabeled training data, as well as the usual labeled training data. However,
at that time, we were the only team to use the unlabeled data (Strapparava, Gliozzo,
and Giuliano 2004).
In this article, we review our technique that combines domain and syntagmatic
information in order to define a complete kernel for WSD. The rest of the article is
organized as follows. In Section 2, we provide a general introduction to the kernel
methods, in which we give the basis for understanding our approach. Exploiting kernel
methods, we can define and combine individual kernels representing information from
different sources in a principled way. After this introductory section, in Section 3 we
present the kernels that we developed for WSD. This includes a detailed description
of the individual kernels and the way we define the composite ones. We present our
experiments in Section 4. The results obtained on a range of lexical-sample tasks and on
the English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that our
approach achieves state-of-the-art performance. Finally, in Section 5, we offer conclu-
sions and some directions for future research.
2. Kernel Methods
Kernel methods are a popular machine learning approach within the natural lan-
guage processing community. They are theoretically well founded in statistical learn-
ing theory and have shown good empirical results in many applications (Vapnik 1999;
Cristianini and Shawe-Taylor 2000; Scho?lkopf and Smola 2002; Shawe-Taylor and
Cristianini 2004).
The strategy adopted by kernel methods consists of splitting the learning problem
into two parts. They first embed the input data in a suitable feature space, and then
use a linear algorithm to discover nonlinear patterns in the input space. Typically, the
mapping is performed implicitly by a so-called kernel function. The kernel function
is a similarity measure between the input data that depends exclusively on the specific
data type and domain. A typical similarity function is the inner product between feature
vectors. Characterizing the similarity of the inputs plays a crucial role in determining
the success or failure of the learning algorithm, and it is one of the central questions in
the field of machine learning.
Formally, the kernel is a function K : X? X ? R that takes as input two data objects
(e.g., vectors, texts, or parse trees) and outputs a real number characterizing their
similarity, with the property that the function is symmetric and positive semi-definite.
That is, for all xi, xj ? X satisfies
K(xi, xj) = ??(xi),?(xj)? (1)
where ? is an (implicit) mapping from X to an (inner product) feature space F .
Kernels are used inside learning algorithms such as support vector machines (SVM)
or kernel perceptrons as the interface between the algorithm and the data. The kernel
function is then the only domain specific element of the system, while the learning
algorithm is a general purpose component.
The idea behind the SVM (one of the best known kernel-based learning algorithms)
is to map the set of training data into a high-dimensional feature space F via a mapping
function? : X ? F , and construct a separating hyperplane with maximummargin (i.e.,
514
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
the minimum distance between the hyperplane and data points) in that space. The use
of an appropriate non-linear transformation ? of the input yields a nonlinear decision
boundary in the input space. Kernel functions make possible the use of feature spaces
with an exponential or even infinite number of dimensions. Instead of performing the
explicit feature mapping ?, one can use a kernel function, which permits the (efficient)
computation of inner products in high-dimensional feature spaces without explicitly
carrying out the mapping ?. This is called the kernel trick in the machine learning
literature (Boser, Guyon, and Vapnik 1992).
Finally, we point out the theoretical tools required to create new kernels, and com-
bine individual kernels to form composite ones. Of course, not every similarity function
is a valid kernel because, by definition, kernels should be equivalent to some inner
product in a feature space. The function K : X? X ? R is a valid kernel provided that
its kernel matrices1 are positive semi-definite2 for all training sets S = {x1, ..., xl}, the
so-called finitely positive semi-definite property. Note that defining similarity measures
by means of kernels may be more intuitive than performing the explicit mapping in the
feature space. Furthermore, this formulation does not require the set X to be a vector
space: for example, we shall define kernels that take strings as input.
This result is not only useful because it opens new perspectives to define kernel
functions that only implicitly correspond to a feature mapping ?. Another consequence
is that it can be used to prove a set of rules for combining basic kernels to obtain compos-
ite ones. This will allow us to integrate heterogeneous sources of information in a simple
and effective way.We shall use the following properties of kernels to define our compos-
ite kernels. Let k1 and k2 be kernels over X? X; then the following functions are kernels:
 k(xi, xj) = k1(xi, xj)+ k2(xi, xj)
 k(xi, xj) = c ? k1(xi, xj), c ? R+
 k(xi, xj) =
k1(xi,xj )?
k1(xi,xi )?k1(xj,xj )
(normalization)
In summary, we can define a kernel function by following different strategies: (1)
providing an explicit feature mapping ? : X ? Rn; (2) defining a similarity function
that is symmetric and positive semi-definite; and (3) composing different valid kernels,
using the closure properties of kernels. This forms the basis for the approach described
in the following section.
3. Kernel Methods for WSD
Our approach toWSD consists of representing linguistic phenomena independently and
then defining a combinationmethod to integrate them. As described in the previous sec-
tion, the kernel function is the only task-specific component of the learning algorithm.
Thus, to develop a WSD system, we only need to define appropriate kernel functions to
represent the domain and syntagmatic aspects of sense distinction and, second, exploit
the properties of kernel functions to define a composite kernel to combine and extend
the individual kernels.
The resulting WSD system consists of two families of kernels: the domain and the
syntagmatic kernels. The former family, described in Section 3.1, models the domain
1 Given a set of vectors S = {x1, ..., xl}, the kernel matrix K is defined as the l? lmatrix Kwhose entries
are Kij = k(xi, xj ) = ??(xi ),?(xj )?, where k is a kernel function that evaluates the inner products in a
feature space with feature map ?.
2 A symmetric matrix is positive semi-definite if its eigenvalues are all non-negative. Actually, as we will
see in Section 3.2 using Proposition 1, it is quite easy to verify this property.
515
Computational Linguistics Volume 35, Number 4
Table 1
An example of a domain matrix.
Medicine Computer Science
HIV 1 0
AIDS 1 0
virus 0.5 0.5
laptop 0 1
aspects of sense distinction; it is composed of the domain kernel (KD) and the bag-of-
words kernel (KBoW). The latter, described in Section 3.2, represents the syntagmatic
aspects of sense distinction; it is composed of the collocation kernel (KColl) and the part-
of-speech kernel (KPoS). Finally, Section 3.3 describes the composite kernel for WSD.
3.1 Domain Kernels
It has been shown that domain information is fundamental for WSD (Magnini et al
2002). For instance, the (domain) polysemy between the computer science and the
medicine senses of the word virus can be solved by considering the domain of the
context in which it appears. Gliozzo, Strapparava, and Dagan (2004) proposed a WSD
method that exploits only domain information.
In the context of kernel methods, domain information can be exploited by defining
a kernel function that estimates the domain similarity between the contexts of the
words to be disambiguated. The simplest method to estimate the domain similarity
between two texts is to compute the cosine similarity of their vector representations
in the vector space model (VSM). The VSM is a k-dimensional space Rk, in which the
text tj is represented by a vector tj, where the i
th component is the term frequency of
the term wi in tj. However, such an approach does not deal well with lexical variability
and ambiguity. For instance, despite the fact that the sentences He is affected by AIDS
and HIV is a virus express closely-related concepts, their similarity is zero in the VSM
because they have no words in common (they are represented by orthogonal vectors).
On the other hand, due to the ambiguity of the word virus, the similarity between the
sentences The laptop has been infected by a virus and HIV is a virus is greater than zero,
even though they convey very different messages.
To overcome this problem, we introduce the domain model (DM) and show how to
use it to define a domain VSM in which texts and terms are represented in a uniform
way. A DM is composed of soft clusters of terms. Each cluster represents a semantic
domain, that is, a set of terms that often co-occur in texts having similar topics. A DM
is represented by a k? k? rectangular matrix D, containing the degree of association
among terms and domains, as illustrated in Table 1.
The matrix D is used to define a function D : Rk ? Rk
?
, that maps the vector tj
represented in the standard VSM into the vector t?j in the domain VSM. D is defined
as follows:3
D(tj) = tj(I
IDFD) = t?j (2)
3 In Wong, Ziarko, and Wong (1985), Equation (2) is used to define a generalized vector space model, of
which the domain VSM is a particular instance.
516
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
where tj is represented as a row vector, I
IDF is a k? k diagonal matrix such that iIDFi,i =
IDF(wi), and IDF(wi) is the inverse document frequency of wi.
In the domain space, the similarity is estimated by taking into account second order
relations among terms. For example, the similarity of the two sentences He is affected
by AIDS and HIV is a virus is very high, because the terms AIDS, HIV, and virus are
strongly associated with the medicine domain.
A DM can be estimated frommanually constructed lexical resources, such as Word-
Net Domains (Magnini and Cavaglia` 2000), or by performing a term-clustering process
on a (large) corpus. However, the second approach is more attractive because it allows
us to automatically acquire DMs for different languages and domains.
In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposi-
tion (SVD) to acquire DMs from a corpus represented by its term-by-document matrix
T, in a unsupervised way.4 SVD decomposes the term-by-document matrix T into
three matrixes T  V?k?UT, where V and U are orthogonal matrices (i.e., VTV = I and
UTU = I) whose columns are the eigenvectors of TTT and TTT, respectively, and ?k?
is the diagonal k? k matrix containing the highest k?  k eigenvalues of T, and all the
remaining elements set to 0. The parameter k? is the dimensionality of the domain VSM
and can be fixed in advance. Under this setting, we define the domain matrix D as
follows:
D = INV
?
?k? (3)
where IN is a diagonal matrix such that iNi,i =
1
?
? w?i ,
w?i ?
, w?i is the i
th row of the matrix
V
?
?k? .
5
Note that in this case, with respect to Table 1, the domains are represented by the
columns of the matrix D and they do not have an explicit name. By using a small
number of domains, we can define a very compact representation of the DM and, con-
sequently, reduce the memory requirements while preserving most of the information.
There exist very efficient algorithms to perform the SVD process on sparse matrices,
allowing us to perform this operation on large corpora in a very limited time and with
reduced memory requirements.6
Therefore, we can define the domain kernel to estimate the domain similarity
between the contexts of the words to be disambiguated. It is a variant of the latent
semantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited to
define an explicit mapping D : Rk ? Rk
?
from the classical VSM into the domain VSM.
The domain kernel is explicitly defined as follows:
KD(ti, tj) = ?D(ti),D(tj)? (4)
where D is the domain mapping defined in Equation (2).
4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semantic
indexing of documents in large corpora (Deerwester et al 1990).
5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space
(Deerwester et al 1990). The only difference in our formulation is that the vectors representing the terms
in the domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by
matrix IIDF. Note the analogy with the tf-idf term weighting schema (Salton and McGill 1983), widely
used in information retrieval.
6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allows
us to perform this step in a few minutes even for large corpora. It can be downloaded from
http://tedlab.mit.edu/?dr/SVDLIBC/.
517
Computational Linguistics Volume 35, Number 4
A standard approach for detecting topic (domain) similarity is to extract bag-of-
words features from a wide window of text around the words to be disambiguated.
Based on this representation, we define a linear kernel called the bag-of-words kernel
(KBoW). KBoW is a particular case of the domain kernel in which D = I in Equation (2),
where I is the identity matrix. The BoW kernel does not require a DM; therefore, it
can be applied to the strictly supervised settings, in which external knowledge is not
available.
To summarize, the domain kernel allows us to plug external knowledge into the
supervised learning process; it will be compared and combined with the standard bag-
of-words approach in Section 4. In the following section, we shall see that domain
models are also useful for defining soft-matching collocation kernels.
3.2 Syntagmatic Kernels
Collocations (such as bigrams and trigrams) extracted from the local context of the word
to be disambiguated are typically used to capture syntagmatic relations (Yarowsky
1994). However, traditional approaches to WSD fail to represent non-contiguous or
shifted collocations, and fail to consider lexical variability. For example, suppose we
have to disambiguate the verb to score in the sentence Ronaldo scored the first goal, given
the labeled example The football player scored two goals in the second half as training. A
traditional approach has no clues to return the right answer because the two sentences
have no features in common.
The use of kernels on strings allows us to overcome the aforementioned problems
by representing (non-contiguous) collocations and exploiting external lexical knowl-
edge sources to define non-zero measures of similarity between words (soft-matching
criteria). In this formulation, words taken in their context are compared by kernels that
sum the number of common (non-contiguous) collocations of words, considering lexical
variability, and part-of-speech tags, avoiding an explicit feature mapping that would
lead to an exponential number of features.
String kernels (or sequence kernels) are a family of kernel functions developed
to compute the inner product among images of strings in high-dimensional feature
space using dynamic programming techniques. The gap-weighted subsequences kernel
is one of the most general types of kernel based on sequences. Roughly speaking,
it compares two strings by means of the number of contiguous and non-contiguous
substrings of a given length they have in common. Non-contiguous occurrences are
penalized according to the number of gaps they contain. Formally, let ? be an al-
phabet of |?| symbols, and s = s1s2 . . . s|s| be a finite sequence over ? (i.e., si ? ?, 1 
i  |s|). Let i = [i1, i2, . . . , in], with 1  i1 < i2 < . . . < in  |s|, be a subset of the indices
in s; we will denote as s[i] ? ?n the subsequence si1si2 . . . sin . Note that s[i] does not
necessarily form a contiguous subsequence of s; for example, if s is the sequence
?Ronaldo scored the first goal? and i = [2, 5], then s[i] is ?scored goal?. The length
spanned by s[i] in s is l(i) = in ? i1 + 1. The feature space associated with the gap-
weighted subsequences kernel of length n is indexed by I = ?n, with the embedding
given by
?nu(s) =
?
i:u=s[i]
?l(i),u ? ?n (5)
518
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
where 0<? 1 is the decay factor used to penalize non-contiguous subsequences.7 The
associate kernel is defined as
Kn(s, t) = ??n(s),?n(t)? =
?
u??n
?nu(s)?
n
u(t) (6)
An explicit computation of Equation (6) is unfeasible even for small values of n.
To evaluate Kn more efficiently, we use the recursive formulation based on a dynamic
programming implementation (Lodhi et al 2002; Saunders, Tschach, and Shawe-Taylor
2002; Cancedda et al 2003). It is defined in the following equations:
K?0(s, t) = 1,?s, t (7)
K?i (s, t) = 0, if min(|s|, |t|) < i (8)
K??i (s, t) = 0, if min(|s|, |t|) < i (9)
K??i (sx, ty) =
{
?K??i (sx, t) if x = y;
?K??i (sx, t)+ ?
2K?i?1(s, t) otherwise.
(10)
K?i (sx, t) = ?K
?
i (s, t)+ K
??
i (sx, t) (11)
Kn(s, t) = 0, if min(|s|, |t|) < n (12)
Kn(sx, t) = Kn(s, t)+
?
j:tj=x
?2K?n?1(s, t[1 : j? 1]) (13)
where K?n and K
??
n are auxiliary functions with a similar definition to Kn used to facilitate
the computation. Based on these definitions, Kn can be computed in O(n|s||t|). Using
this recursive definition, it turns out that computing all kernel values for subsequences
of lengths up to n is not significantly more costly than computing the kernel for n only.
The syntagmatic kernel is defined as a sum of gap-weighted subsequences kernels
that operate at word and part-of-speech tag level. In particular, following the approach
proposed by Cancedda et al (2003), it is possible to adapt sequence kernels to operate
at word level by instancing the alphabet ? with the vocabulary V = {w1,w2, . . . ,wk}.
Moreover, we restrict the generic definition of the gap-weighted subsequences kernel
to recognize collocations in the local context of a specified word. The resulting kernel,
called the n-gram collocation kernel (KnColl), operates on sequences of lemmata around
a specified word l0 (i.e., l?3, l?2, l?1, l0, l+1, l+2, l+3). This formulation allows us to
estimate the number of common (sparse) subsequences of lemmata (i.e., collocations)
between two examples, in order to capture syntagmatic similarity. Analogously, we
define the part-of-speech kernel (KnPoS) to operate on sequences of part-of-speech tags
p?3, p?2, p?1, p0, p+1, p+2, p+3, where p0 is the part-of-speech tag of l0.
The collocation kernel and the part-of-speech kernel are defined by Equations (14)
and (15), respectively.
KColl(s, t) =
n
?
l=1
KlColl(s, t) (14)
7 Notice that by choosing ? = 1, sparse subsequences are not penalized. On the other hand, the kernel does
not take into account sparse subsequences with ? ? 0.
519
Computational Linguistics Volume 35, Number 4
KPoS(s, t) =
n
?
l=1
KlPoS(s, t) (15)
Both kernels depend on the parameter n, the length of the non-contiguous subse-
quences, and ?, the decay factor. For example, K2Coll allows us to represent all (sparse)
bigrams in the local context of a word. Finally, the syntagmatic kernel is defined as
KSynt(s, t) = KColl(s, t)+ KPoS(s, t) (16)
In the preceding definition, only exact word-matches contribute to the similarity.
To solve this problem, external lexical knowledge is fed into the supervised learning
process, allowing us to define the soft-matching collocation kernel. In particular, we de-
fine two alternative soft-matching criteria by exploiting synonymy relations inWordNet
and DMs acquired from corpora. Both criteria are based on the assumption that every
word in a sentence can be substituted by another preserving the original meaning, if
these words are paradigmatically related (e.g., synonyms, hyponyms, or domain related
words). For example, if we consider as equivalent the terms Ronaldo and football player,
then the sentence The football player scores the first goal is equivalent to Ronaldo scores the
first goal, providing a strong evidence to disambiguate the verb to score in the second
sentence.
Following the approach proposed by Shawe-Taylor and Cristianini (2004), the soft-
matching gap-weighted subsequences kernel is now calculated recursively using Equa-
tions (7)?(9), (11), and (12), replacing Equation (10) by the equation:
K??i (sx, ty) = ?K
??
i (sx, t)+ ?
2axyK
?
i?1(s, t),?x, y (17)
and modifying Equation (13) to:
Kn(sx, t) = Kn(s, t)+
|t|
?
j
?2axtjK
?
n?1(s, t[1 : j? 1]) (18)
where axy are entries in a similarity matrix A between terms. In order to ensure that the
resulting kernel is still valid, Amust be positive semi-definite.
In the following sections, we describe the two alternative soft-matching criteria
based on WordNet Synonymy and Domain Proximity, respectively. To show that the
similarity matrices are positive semi-definite, we use the following result.
Proposition 1
A matrix A is positive semi-definite if and only if A = BTB for some real matrix B.
The proof is given in Shawe-Taylor and Cristianini (2004).
WordNet Synonymy. The first soft-matching criterion is based on WordNet8 to define
a similarity matrix between words. In particular, we substitute two words if they are
synonyms. To this end, a word is represented as vector whose dimensions are associated
8 We used WordNet 1.7.1 and MultiWordNet for English and Italian experiments, respectively.
520
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
with the synsets. Formally, we define the term-by-synset matrix S as the matrix whose
rows are indexed by the terms and whose columns are indexed by the synsets. The
(i, j)th entry of S is 1 if the synset sj contains the term wi; 0 otherwise. The matrix
S gives rise to the similarity matrix A = SST between terms. Because A can be re-
written as A = (ST )TST = BTB, it follows directly from Proposition 1 that it is positive
semi-definite.
Domain Proximity. The second soft-matching criterion exploits the domain models intro-
duced in Section 3.1 to define a similarity matrix between words. Once a DM has been
defined by the matrixD, the domain space is a k? dimensional space, in which both texts
and terms are represented by means of domain vectors, that is, vectors representing the
domain relevances among the linguistic object and each domain. The domain vector w?i
for the term wi ? V is the ith row of D, where V = {w1,w2, . . . ,wk} is the vocabulary of
the corpus. The term-by-domain matrix D gives rise to the similarity matrix A = DDT
between terms. It follows by Proposition 1 that A is positive semi-definite.
We shall show that the syntagmatic kernel is more effective than standard bigrams
and trigrams of lemmata and part-of-speech tags typically used as features in WSD.
3.3 Composite Kernel
Having defined all the individual kernels representing syntagmatic and domain aspects
of sense distinction, we can define the composite kernel to combine and extend the
individual kernels. The closure properties of the kernel functions allows us to define
the composite kernel as
KC(xi, xj) =
n
?
l=1
Kl(xi, xj)
?
Kl(xj, xj)Kl(xi, xi)
(19)
where Kl is a valid individual kernel. The individual kernels are normalized?this plays
an important role in allowing us to integrate information from heterogeneous feature
spaces.
Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and
Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effec-
tiveness of combining kernels in this way: The composite kernel consistently improves
the performance of the individual ones. In addition, this formulation allows us to
evaluate the individual contribution of each information source.
In order to show the effectiveness of the proposed domain model in supervised
learning, we defined twoWSD kernels, Kwsd and K
?
wsd. They are completely specified by
the n individual kernels that compose them in Equation (19).
Kwsd is composed by KColl, KPoS, and KBoW ;
K?wsd is composed by KColl, KPoS, KBoW , and KD.
The only difference between the two is that K?wsd uses the domain kernel KD to exploit
external knowledge while Kwsd only uses the labeled training data.
521
Computational Linguistics Volume 35, Number 4
4. Evaluation
Experiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds
2004). First of all, we conducted a preliminary set of experiments on the Catalan,
English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.
Second, in order to show the general applicability of the proposed method, we evalu-
ated the system on the English all-words task; the results are presented in Section 4.2.
All the experiments were performed using the SVM package (Chang and Lin 2001)
customized to embed our own kernels. The parameters were optimized by five-fold
cross-validation on the training set.
4.1 Lexical-Sample Tasks
In this section, we report the evaluation of our method on the Catalan, English, Italian,
and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004). Table 2
describes the tasks we have considered. For each task, it summarizes the number of
words to be disambiguated, the mean polysemy, the size of the labeled training set,
the size of the test set, and the size of the unlabeled training set, respectively. For the
Catalan, Italian, and Spanish tasks, we acquired the DMs from the unlabeled corpora
made available by the task organizers. For the English task, we used a DM acquired
from the British National Corpus (BNC) as the task organizers have not provided
any unlabeled training data. The objectives of these experiments are to (a) estimate
the impact of different knowledge sources in WSD; (b) study the effectiveness of the
kernel combination; (c) understand the benefits of plugging external information in a
supervised framework; and (d) verify the portability of our methodology to different
languages.
4.1.1 Results. Table 3 reports the results of the individual kernels KBoW , KD, KColl, and
KPoS and their combinations Kwsd and K
?
wsd (the baselines for the tasks are reported in
Table 5). In our experiments, the parameters n and ? (see Equation (5)) are optimized
by five-fold cross-validation. For KnColl, we obtained the best results with n = 2 and
? = 0.5. For KnPoS, n = 3 and ? ? 0. The domain cardinality k
? was set to 50. Table 4
shows the performance of the syntagmatic kernel in different configurations: hard and
soft matching. As a baseline, we report the result of a standard approach consisting of
explicit bigrams and trigrams of words and part-of-speech tags around the words to
be disambiguated (Yarowsky 1994). We evaluated the impact of the domain kernel on
the overall performance by comparing the learning curves of K?wsd and Kwsd on the four
lexical-sample tasks. Figure 1 shows the results of our experiments. The points of the
learning curves are obtained by sampling the same percentage of training examples for
Table 2
Description of the lexical-sample tasks of Senseval-3.
Task #w mean polysemy #train #test #unlab
Catalan 27 3.11 4,469 2,253 23,935
English 57 6.47 7,860 3,944 -
Italian 45 6.30 5,145 2,439 74,788
Spanish 46 3.30 8,430 4,195 61,252
522
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
Table 3
The performance (F1) of the basic and composite kernels on the Catalan, English, Italian, and
Spanish lexical-sample tasks of Semeval-3.
Kernel Catalan English Italian Spanish
KBoW 81.3 63.7 43.3 78.2
KD 85.2 65.5 44.5 84.4
KColl 84.2 68.5 54.0 83.6
KPoS 79.6 64.0 44.4 79.5
Kwsd 85.2 69.7 53.1 84.2
K?wsd 89.0 73.3 61.3 88.2
Table 4
Performance (F1) of the syntagmatic kernel for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
Method Catalan English Italian Spanish
Bigrams and trigrams 82.6 67.3 51.0 81.9
Hard matching 83.8 67.7 51.9 82.9
Soft matching (WordNet) - 67.3 51.3 -
Soft matching (Domain proximity) 84.2 68.5 54.0 83.6
each word. Finally, Table 5 summarizes the results we obtained, providing a comparison
with the state of the art.
4.1.2 Discussion. Table 3 shows that domain information and syntagmatic information
are crucial for WSD, and their combination significantly outperforms the individual
kernels, showing the effectiveness of the kernel combination method.
In addition, the domain kernel KD outperforms the bag-of-words kernel KBoW ,
and the composite kernel K?wsd that makes use of domain information outperforms the
one Kwsd based only on the labeled training data, demonstrating our assumption (see
Section 3).
Table 4 shows that the syntagmatic kernel outperforms the baseline (bigrams and
trigrams) in any configuration (hard-/soft-matching). The soft-matching criteria further
improve the classification performance. It is interesting to note that the domain proxim-
ity obtained better results thanWordNet synonymy (note that we do not have a Catalan
or a Spanish WordNet). The different results observed for Italian and English using
the domain proximity soft-matching criterion are probably due to the small size of the
unlabeled English corpus.
Figure 1 shows that K?wsd outperforms Kwsd on all lexical sample tasks, even with a
small number of examples. It is worth noting, as reported in Table 5, that K?wsd achieves
the same performance as Kwsd using about half of the labeled training data. This result
shows that the proposed semi-supervised learning approach consisting of acquiring
domain models from unlabeled corpora is effective, as it allows us to drastically reduce
the amount of labeled training data and provide a viable solution for the knowledge
acquisition bottleneck problem in WSD.
To the best of our knowledge, K?wsd turns out to be the best system for all the tested
tasks of Senseval-3, further improving the state of the art by 0.4% to 8.2% for English
and Italian, respectively. Finally, we have demonstrated the language independency
523
Computational Linguistics Volume 35, Number 4
Figure 1
From left to right, top to bottom, learning curves for the Catalan, English, Italian, and Spanish
lexical-sample tasks of Semeval-3.
of our approach. The DMs have been acquired for different languages from different
unlabeled corpora by adopting exactly the same methodology, without requiring any
external lexical resource or ad hoc rule.
4.2 All-Words Task
Encouraged by the excellent results obtained on the lexical-sample tasks, we evaluated
our approach on the all-words task, in which a very small amount of labeled training
Table 5
Comparative evaluation on the lexical sample tasks.
Task MF Agreement BEST Kwsd K
?
wsd DM+ % of training
Catalan 66.3 93.1 85.2 85.2 89.0 3.8 46
English 55.2 67.3 72.9 69.7 73.3 3.6 54
Italian 18.0 89.0 53.1 53.1 61.3 8.2 51
Spanish 67.7 85.3 84.2 84.2 88.2 4.0 50
Columns report: theMost Frequent baseline, the inter-annotator agreement, the F1 of the best system
at Senseval-3, the F1 of Kwsd, the F1 of K
?
wsd, DM+ (the improvement due to DM, i.e., K
?
wsd ? Kwsd),
and the percentage of sense-tagged examples required by K?wsd to achieve the same performance
as Kwsd with full training.
524
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
Table 6
The performance (F1) of the basic kernels and composite kernels on the English all-words task of
Senseval-3.
basic kernels composite kernels
KbncD K
sem
D KBoW KPoS KColl Kwsd K?
bnc
wsd K?
sem
wsd
F1 63.0 63.2 63.2 63.4 64.0 64.4 65.0 65.2
data is typically available. We performed the evaluation on the English all-words task
of Senseval-3 (Snyder and Palmer 2004). The test set was extracted from twoWall Street
Journal articles and one text from the Brown Corpus. The test set consists of 945 words
(2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses. The inter-
annotator agreement rate in the preparation of the corpus was approximately 72.5%.
Themost frequent (MF) baseline using the firstWordNet sense heuristic obtained 60.9%.
We have trained and tested the system exploiting the following resources: (1) Word-
Net 1.7.1 as sense repository; (2) SemCor,9 considering only those words appearing
in the Senseval-3 all-words data set?we extracted about 61,700 tagged examples that
constitute the only labeled training set exploited by the system; and (3) the BNC, from
which we extracted the unlabeled training data.
4.2.1 Results.We trained 734 word-expert classifiers on the SemCor corpus. The labeled
examples for each classifier range from a minimum of one example to a maximum
of 2,275 examples. We return a random sense for those words that have no training
examples in SemCor.10 We have acquired two DMs, one from the BNC (i.e., K?bncD ; the
same we used in the lexical-sample task) and one from SemCor (i.e., K?semD ), obtaining a
slightly better performance with the latter.
Table 6 shows the performance of the individual kernels KBoW , KD, KColl, and KPoS,
and their composite kernels Kwsd, K?
bnc
D and K?
sem
D .
Since for 210 words in the test set we have no training examples, to better under-
stand the results obtained, we performed an evaluation on the subset of the test set for
which at least one training example is available in SemCor. Evaluating only on these
words the performance increases from 65.2% to 70.0%, and the most frequent baseline
becomes 65.7%. Tables 7 and 8 present a more detailed analysis that considers results
grouped according to the amount of training available and the mean polysemy of the
words in the test set, excluding from the data set the monosemous words. Table 7 shows
the results (F1) of K?semwsd at different ranges of polysemy. Table 8 presents the results (F1)
of K?semwsd on those words that have a given number of training examples. This evaluation
is limited to the best composite kernel K?semwsd.
4.2.2 Discussion. We compared our approach with the three best systems that par-
ticipated in the English all-words task of Senseval-3. The best system (Decadt et al
2004) has comparable performance (65.2) to ours; however, it uses a larger training set
composed of 563,129 sense-tagged words. The training corpus was built by merging
9 Texts semantically annotated with WordNet 1.6 senses (created at Princeton University), and
automatically mapped to WordNet 1.7, WordNet 1.7.1, and WordNet 2.0. Downloadable from
http://www.cs.unt.edu/?rada/downloads.html.
10 Note that for these words the WordNet first sense is not necessarily the most frequent sense.
525
Computational Linguistics Volume 35, Number 4
Table 7
The performance (F1) of K?semwsd at different ranges of polysemy. Most Frequent baseline (MF) is
also reported.
Range of polysemy
2?5 6?10 11?15 16?20 21?25 26?30 31+
K?semwsd 73.2 61.4 59.1 33.8 55.2 50.2 37.3
MF 70.0 53.4 56.4 25.7 47.2 39.0 21.7
Table 8
The performance (F1) of K?semwsd on words with a given number of training examples. Most
Frequent baseline (MF) and mean polysemy for each partition are also reported.
Range of training examples
1?10 11?50 51?100 101?200 201+
K?semwsd 76.1 70.8 54.2 67.4 60.0
MF 73.5 66.4 49.4 63.2 53.0
Mean polysemy 3 5 7 9 15
SemCor, and English lexical-sample and all-words data sets taken from all the previous
editions of Senseval. The system proposed by Mihalcea and Faruque (2004) scored sec-
ond (64.6). The dimension of their training set is comparable to ours; however, they also
use additional information drawn from WordNet to derive semantic generalizations
using syntactic dependencies. Finally, the third system (Yuret 2004) obtained 64.1 using
a larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMind
Word Expert, Senseval-2, and Senseval-3 lexical-sample tasks).
The small difference between the two domain models seems to indicate that a
limited amount of unlabeled data is sufficient to improve the overall performance,
and the use of unlabeled data taken from the training set helps to slightly improve
the overall performance. However, the domain model can be acquired from a different
corpus (e.g., the BNC) without significantly affecting the overall performance.
Finally, the results reported in Tables 7 and 8 show that our approach is able to dis-
ambiguate with good accuracy (F1 = 76%) words with a number of training examples
that ranges from 1 to 10, outperforming the most frequent baseline by 3%. This is an
interesting result given the extremely small number of training examples available. On
the other hand, the more training is available for a given word, the more polysemous
that word is. Nevertheless, the algorithm always outperforms the baseline and has a
more significant difference for increasing values of the mean polysemy (from 3% to
16%). These results, together with the ones obtained in the lexical sample tasks, show
that the domain kernel is able to boost the overall performance when little training data
are available, as well as with enough training data. The benefit is evenmore pronounced
for the latter case, even though the disambiguation task is more complex due to the high
polysemy of highly frequent words.
5. Conclusions
This article summarizes the results of a word expert semi-supervised algorithm for
WSD based on a combination of kernel functions. First, we evaluated our methodology
526
Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSD
on four lexical-sample tasks of Senseval-3, significantly improving the state of the art
for all of them. In particular, we demonstrated that using external knowledge inside a
supervised framework is a viable methodology to reduce the amount of training data
required for learning. In our approach, the external knowledge is represented by means
of domain models automatically acquired from corpora in a totally unsupervised way.
Then, we applied the method so defined to the English all-words task of Senseval-
3, achieving state-of-the-art performance while requiring less labeled training data
compared to the other systems we have found in the literature.
Some slight improvement may be possible by exploiting syntactic information pro-
duced by a parser. In the framework of kernel methods, this expansion can be done by
adding a tree kernel (i.e., a kernel function that evaluates the similarity among parse
trees) to our composite kernel. However, the performance achieved is close to the upper
bound, if we consider the inter-annotator agreement as an indication of the upper-
bound performance.
Finally, we think that our semi-supervised approach is at the moment an effective
solution for developing a sense-tagging system. Indeed, we tested the system on the
English lexical-sample task of SemEval 2007, still obtaining state-of-the-art performance
(Pradhan et al 2007). Therefore, we plan to make available an optimized version of
our system, and to exploit it for ontology learning, textual entailment, and information
retrieval.
Acknowledgments
Claudio Giuliano was supported by the
X-Media project (www.x-media-project.org),
sponsored by the European Commission as
part of the Information Society Technologies
(IST) programme under EC grant IST-FP6-
026978. Alfio Massimiliano Gliozzo and
Carlo Strapparava were supported by the
ONTOTEXT project, sponsored by the
Autonomous Province of Trento under the
FUP-2004 research program.
References
Boser, Bernhard, Isabelle Guyon, and
Vladimir Vapnik. 1992. A training
algorithm for optimal margin classifier.
In Proceedings of the 5th Annual ACM
Workshop on Computational Learning
Theory, pages 144?152, Pittsburgh, PA.
Cancedda, Nicola, Eric Gaussier, Cyril
Goutte, and Jean-Michel Renders. 2003.
Word-sequence kernels. Journal of Machine
Learning Research, 32(6):1059?1082.
Chang, Chih-Chung and Chih-Jen Lin,
2001. LIBSVM: A library for support vector
machines. Software available at www.csie.
ntu.edu.tw/?cjlin/libsvm.
Chapelle, Olivier, Bernhard Scho?lkopf, and
Alexander Zien. 2006. Semi-Supervised
Learning. MIT Press, Cambridge, MA.
Cristianini, Nello and John Shawe-Taylor.
2000. An Introduction to Support Vector
Machines. Cambridge University Press.
Decadt, Bart, Veronique Hoste, Walter
Daelemans, and Antal van den Bosch.
2004. GAMBL, genetic algorithm
optimization of memory-based WSD. In
Proceedings of Senseval-3, pages 108?112,
Barcelona.
Deerwester, Scott, Susan Dumais, George
Furnas, Thomas Landauer, and Richard
Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American
Society of Information Science, 41:391?407.
Giuliano, Claudio, Alfio Gliozzo, and Carlo
Strapparava. 2006. Syntagmatic kernels: A
word sense disambiguation case study. In
In Proceedings of the EACL-06 Workshop on
Learning Structured Information in Natural
Language Applications, pages 57?63, Trento.
Giuliano, Claudio, Alberto Lavelli, and
Lorenza Romano. 2006. Exploiting shallow
linguistic information for relation
extraction from biomedical literature. In
Proceedings of the Eleventh Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-2006),
pages 401?408, Trento.
Gliozzo, Alfio, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for
word sense disambiguation. In Proceedings
of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL-05),
pages 403?410, Ann Arbor, MI.
Gliozzo, Alfio, Carlo Strapparava, and
Ido Dagan. 2004. Unsupervised and
supervised exploitation of semantic
527
Computational Linguistics Volume 35, Number 4
domains in lexical disambiguation.
Computer Speech and Language,
18(3):275?299.
Lodhi, Huma, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text
classification using string kernels. Journal
of Machine Learning Research, 2(3):419?444.
Magnini, Bernardo and Gabriela Cavaglia`.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413?1418, Athens.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings of Senseval-3: Third
International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text.
Barcelona.
Mihalcea, Rada and Ehsanul Faruque. 2004.
SenseLearner: Minimally supervised WSD
for all words in open text. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 155?158, Barcelona.
Moschitti, Alessandro. 2004. A study on
convolution kernels for shallow statistic
parsing. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 335?342,
Barcelona.
Pradhan, Sameer, Edward Loper, Dmitriy
Dligach, and Martha Palmer. 2007.
Semeval-2007 task-17: English lexical
sample, SRL and all words. In Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
pages 87?92, Prague.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Saunders, Craig, Hauke Tschach, and John
Shawe-Taylor. 2002. Syllables and other
string kernel extensions. In Proceedings of
19th International Conference on Machine
Learning (ICML02), pages 530?537, Sydney.
Scho?lkopf, Bernhard and Alexander Smola.
2002. Learning with Kernels. MIT Press,
Cambridge, MA.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern Analysis.
Cambridge University Press.
Snyder, Benjamin and Martha Palmer. 2004.
The English all-words task. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 41?43, Barcelona.
Strapparava, Carlo, Alfio Gliozzo, and
Claudio Giuliano. 2004. Pattern abstraction
and term similarity for word sense
disambiguation: Irst at senseval-3. In
Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic
Analysis of Text, pages 229?234, Barcelona.
Vapnik, Vladimir N. 1999. The Nature of
Statistical Learning Theory (Information
Science and Statistics). Springer, Berlin.
Wong, S. K. M., Wojciech Ziarko, and
Patrick C. N. Wong. 1985. Generalized
vector space model in information
retrieval. In Proceedings of the 8th ACM
SIGIR Conference, pages 18?25, Montreal.
Yarowsky, David. 1994. Decision lists for
lexical ambiguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics (ACL 1994), pages 88?95,
Las Cruces, NM.
Yuret, Deniz. 2004. Some experiments with a
naive Bayes WSD system. In Senseval-3:
Third International Workshop on the
Evaluation of Systems for the Semantic
Analysis of Text, pages 265?268, Barcelona.
Zhao, Shubin and Ralph Grishman. 2005.
Extracting relations with integrated
information using kernel methods. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL 2005), pages 419?426, Ann Arbor, MI.
528
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Outsourcing FrameNet to the Crowd
Marco Fossati, Claudio Giuliano, and Sara Tonelli
Fondazione Bruno Kessler
Trento, Italy
{fossati,giuliano,satonelli}@fbk.eu
Abstract
We present the first attempt to perform full
FrameNet annotation with crowdsourcing
techniques. We compare two approaches:
the first one is the standard annotation
methodology of lexical units and frame
elements in two steps, while the second
is a novel approach aimed at acquiring
frames in a bottom-up fashion, starting
from frame element annotation. We show
that our methodology, relying on a single
annotation step and on simplified role defi-
nitions, outperforms the standard one both
in terms of accuracy and time.
1 Introduction
Annotating frame information is a complex task,
usually modeled in two steps: first annotators are
asked to choose the situation (or frame) evoked by
a given predicate (the lexical unit, LU) in a sen-
tence, and then they assign the semantic roles (or
frame elements, FEs) that describe the participants
typically involved in the chosen frame. Existing
frame annotation tools, such as Salto (Burchardt
et al, 2006) and the Berkeley system (Fillmore et
al., 2002) foresee this two-step approach, in which
annotators first select a frame from a large reposi-
tory of possible frames (1,162 frames are currently
listed in the online version of the resource), and
then assign the FE labels constrained by the cho-
sen frame to LU dependents.
In this paper, we argue that such workflow
shows some redundancy which can be addressed
by radically changing the annotation methodology
and performing it in one single step. Our novel an-
notation approach is also more compliant with the
definition of frames proposed in Fillmore (1976):
in his seminal work, Fillmore postulated that the
meanings of words can be understood on the basis
of a semantic frame, i.e. a description of a type
of event or entity and the participants in it. This
implies that frames can be distinguished one from
another on the basis of the participants involved,
thus it seems more cognitively plausible to start
from the FE annotation to identify the frame ex-
pressed in a sentence, and not the contrary.
The goal of our methodology is to provide full
frame annotation in a single step and in a bottom-
up fashion. Instead of choosing the frame first, we
focus on FEs and let the frame emerge based on
the chosen FEs. We believe this approach com-
plies better with the cognitive activity performed
by annotators, while the 2-step methodology is
more artificial and introduces some redundancy
because part of the annotators? choices are repli-
cated in the two steps (i.e. in order to assign a
frame, annotators implicitly identify the partici-
pants also in the first step, even if they are anno-
tated later).
Another issue we investigate in this work is how
semantic roles should be annotated in a crowd-
sourcing framework. This task is particularly
complex, therefore it is usually performed by ex-
pert annotators under the supervision of linguis-
tic experts and lexicographers, as in the case of
FrameNet. In NLP, different annotation efforts
for encoding semantic roles have been carried out,
each applying its own methodology and annota-
tion guidelines (see for instance Ruppenhofer et
al. (2006) for FrameNet and Palmer et al (2005)
for PropBank). In this work, we present a pilot
study in which we assess to what extent role de-
scriptions meant for ?linguistics experts? are also
suitable for annotators from the crowd. Moreover,
we show how a simplified version of these descrip-
tions, less bounded to a specific linguistic theory,
improve the annotation quality.
2 Related work
The construction of annotation datasets for NLP
tasks via non-expert contributors has been ap-
742
proached in different ways, the most prominent
being games with a purpose (GWAP) and micro-
tasks. Verbosity (Von Ahn et al, 2006) was one
of the first attempts in gathering annotations with
a GWAP. Phrase Detectives (Chamberlain et al,
2008; Chamberlain et al, 2009) was meant to
gather a corpus with coreference resolution an-
notations. Snow et al (2008) described design
and evaluation guidelines for five natural language
micro-tasks. However, they explicitly chose a set
of tasks that could be easily understood by non-
expert contributors, thus leaving the recruitment
and training issues open. Negri et al (2011) built
a multilingual textual entailment dataset for statis-
tical machine translation systems.
The semantic role labeling problem has been re-
cently addressed via crowdsourcing by Hong and
Baker (2011). Furthermore, Baker (2012) high-
lighted the crucial role of recruiting people from
the crowd in order to bypass the need for linguis-
tics expert annotations. Nevertheless, Hong and
Baker (2011) focused on the frame discrimination
task, namely selecting the correct frame evoked by
a given lemma. Such task is comparable to the
word sense disambiguation one as per (Snow et
al., 2008), although the complexity increased, due
to lower inter-annotator agreement values.
3 Experiments
In this section, we describe the anatomy and dis-
cuss the results of the tasks we outsourced to the
crowd via the CrowdFlower1 platform.
Golden data Quality control of the collected
judgements is a key factor for the success of
the experiments. Cheating risk is minimized by
adding gold units, namely data for which the re-
quester already knows the answer. If a worker
misses too many gold answers within a given
threshold, he or she will be flagged as untrusted
and his or her judgments will be automatically dis-
carded.
Worker switching effect Depending on their
accuracy in providing answers to gold units, work-
ers may switch from a trusted to an untrusted sta-
tus and vice versa. In practice, a worker submits
his or her responses via a web page. Each page
contains one gold unit and a variable number of
regular units that can be set by the requester dur-
ing the calibration phase. If a worker becomes un-
1https://crowdflower.com
trusted, the platform collects another judgment to
fill the gap. If a worker moves back to the trusted
status, his or her previous contribution is added
to the results as free extra judgments. Such phe-
nomenon typically occurs when the complexity of
gold units is high enough to induce low agree-
ment in workers? answers. Thus, the requester is
constrained to review gold units and to eventually
forgive workers who missed them. This has mas-
sively happened in our experiments and is one of
the main causes of the overall cost decrease and
time increase.
Cost calibration The total cost of a generic
crowdsourcing task is naturally bound to a data
unit. This represents an issue in most of our ex-
periments, as the number of questions per unit
(i.e. a sentence) varies according to the number
of frames and FEs evoked by the LU contained in
a sentence. In order to enable cost comparison, for
each experiment we need to use the average num-
ber of questions per sentence as a multiplier to a
constant cost per sentence. We set the payment
per working page to 5 $ cents and the number of
sentences per page to 3, resulting in 1.83 $ cent
per sentence.
3.1 Assessing task reproducibility and
worker behavior change
Since our overall goal is to compare the perfor-
mance of FrameNet annotation using our novel
workflow to the performance of the standard, 2-
step approach, we first take into account past re-
lated works and try to reproduce them.
To our knowledge, the only attempt to annotate
frame information through crowdsourcing is the
one presented in Hong and Baker (2011), which
however did not include FE annotation.
Modeling The task is designed as follows. (a)
Workers are invited to read a sentence where a
LU is bolded. (b) The question Which is the
correct sense? is combined with the set of
frames evoked by the given LU, as well as the
None choice. Finally, (c) workers must select the
correct frame. A set of example sentences corre-
sponding to each possible frame is provided in the
instructions to facilitate workers.
As a preliminary study, we wanted to assess
to what extent the proposed task could be repro-
duced and if workers reacted in a comparable way
over time. Hong and Baker (2011) did not pub-
lish the input datasets, thus we ignore which sen-
743
LU
2013 2011Sentences Accuracy Accuracy(Gold)
high.a 68 (9) 91.8 92
history.n 72 (9) 84.6 86
range.n 65 (8) 95 93
rip.v 88 (12) 81.9 92
thirst.n 29 (4) 90.4 95
top.a 36 (5) 98.7 96
Table 1: Comparison of the reproduced frame dis-
crimination task as per (Hong and Baker, 2011)
tences were used. Besides, the authors computed
accuracy values directly from the results upon a
majority vote ground truth. Therefore, we de-
cided to consider the same LUs used in Hong
and Baker?s experiments, i.e. high.a, history.n,
range.n, rip.v, thirst.n and top.a, but we lever-
aged the complete sets of FrameNet 1.5 expert-
annotated sentences as gold-standard data for im-
mediate accuracy computation.
Discussion Table 1 displays the results we
achieved, jointly with the experiments by Hong
and Baker (2011). For the latter, we only show ac-
curacy values, as the number of sentences was set
to a constant value of 18, 2 of which were gold.
If we assume that the crowd-based ground truth in
2011 experiments is approximately equivalent to
the expert one, workers seem to have reacted in
a similar manner compared to Hong and Baker?s
values, except for rip.v.
3.2 General task setting
We randomly chose the following LUs among
the set of all verbal LUs in FrameNet evoking 2
frames each: disappear.v [CEASING TO BE, DE-
PARTING], guide.v [COTHEME, INFLUENCE OF -
EVENT ON COGNIZER], heap.v [FILLING, PLAC-
ING], throw.v [BODY MOVEMENT, CAUSE MO-
TION]. We considered verbal LUs as they usually
have more overt arguments in a sentence, so that
we were sure to provide workers with enough can-
didate FEs to annotate. Linguistic tasks in crowd-
sourcing frameworks are usually decomposed to
make them accessible to the crowd. Hence, we
set the polysemy of LUs to 2 to ensure that all
experiments are executed using the smallest-scale
subtask. More frames can then be handled by just
replicating the experiments.
3.3 2-step approach
After observing that we were able to achieve sim-
ilar results on the frame discrimination task as in
previous work, we focused on the comparison be-
tween the 2-step and the 1-step frame annotation
approaches.
We first set up experiments that emulate the for-
mer approach both in frame discrimination and
FEs annotation. This will serve as the baseline
against our methodology. Given the pipeline na-
ture of the approach, errors in the frame discrim-
ination step will affect FE recognition, thus im-
pacting on the final accuracy. The magnitude of
such effect strictly depends on the number of FEs
associated with the wrongly detected frame.
3.3.1 Frame discrimination
Frame discrimination is the first phase of the 2-
step annotation procedure. Hence, we need to
leverage its output as the input for the next step.
Modeling The task is modeled as per Sec-
tion 3.1.
Discussion Table 2 gives an insight into the re-
sults, which confirm the overall good accuracy as
per the experiments discussed in Section 3.1.
3.3.2 Frame elements recognition
We consider all sentences annotated in the previ-
ous subtask with the frame assigned by the work-
ers, even if it is not correct.
Modeling The task is presented as follows. (a)
Workers are invited to read a sentence where a LU
is bolded and the frame that was identified in the
first step is provided as a title. (b) A list of FE def-
initions is then shown together with the FEs text
chunks. Finally, (c) workers must match each def-
inition with the proper FE.
Simplification Since FEs annotation is a very
challenging task, and FE definitions are usually
meant for experts in linguistics, we experimented
with three different types of FE definitions: the
original ones from FrameNet, a manually simpli-
fied version, and an automatically simplified one,
using the tool by Heilman and Smith (2010). The
latter simplifies complex sentences at the syntactic
level and generates a question for each of the ex-
tracted clauses. As an example, we report below
three versions obtained for the Agent definition in
the DAMAGING frame:
744
Approach 2-STEP 1-STEP
Task FD FER
Accuracy .900 .687 .792
Answers 100 160 416
Trusted 100 100 84
Untrusted 21 36 217
Time (h) 102 69 130
Cost/question 1.83 2.74 8.41($ cents)
Table 2: Overview of the experimental results.
FD stands for Frame Discrimination, FER for FEs
Recognition
Original: The conscious entity, generally a per-
son, that performs the intentional action that re-
sults in the damage to the Patient.
Manually simplified: This element describes the
person that performs the intentional action result-
ing in the damage to another person or object.
Automatic system: What that performs the in-
tentional action that results in the damage to the
Patient?
Simplification was performed by a linguistic ex-
pert, and followed a set of straightforward guide-
lines, which can be summarized as follows:
? When the semantic type associated with the
FE is a common concept (e.g. Location),
replace the FE name with the semantic type.
? Make syntactically complex definitions as
simple as possible.
? Avoid variability in FE definitions, try to
make them homogeneous (e.g. they should
all start with ?This element describes...? or
similar).
? Replace technical concepts such as
Artifact or Sentient with com-
mon words such as Object and Person
respectively.
Although these changes (especially the last
item) may make FE definitions less precise from
a lexicographic point of view (for instance, sen-
tient entities are not necessarily persons), annota-
tion became more intuitive and had a positive im-
pact on the overall quality.
After few pilot annotations with the three types
of FE definitions, we noticed that the simplified
one achieved a better accuracy and a lower num-
ber of untrusted annotators compared to the oth-
ers. Therefore, we use the simplified definitions
in both the 2-step and the 1-step approach (Sec-
tion 3.4).
Discussion Table 2 provides an overview of the
results we gathered. The total number of answers
differs from the total number of trusted judgments,
since the average value of questions per sentence
amounts to 1.5.2 First of all, we notice an increase
in the number of untrusted judgments. This is
caused by a generally low inter-worker agreement
on gold sentences due to FE definitions, which still
present a certain degree of complexity, even af-
ter simplification. We inspected the full reports
sentence by sentence and observed a propagation
of incorrect judgments when a sentence involves
an unclear FE definition. As FE definitions may
mutually include mentions of other FEs from the
same frame, we believe this circularity generated
confusion.
3.4 1-step approach
Having set the LU polysemy to 2, in our case a
sentence S always contains a LU with 2 possible
frames (f1, f2), but only conveys one, e.g. f1. We
formulate the approach as follows. S is replicated
in 2 data units (Sa, Sb). Then, Sa is associated to
the set E1 of f1 FE definitions, namely the correct
ones for that sentence. Instead, Sb is associated to
the set E2 of f2 FE definitions. We call Sb a cross-
frame unit. Furthermore, we allow workers to se-
lect the None answer. In practice, we ask a total
amount of |E1 ? E2| + 2 questions per sentence
S. In this way, we let the frame directly emerge
from the FEs. If workers correctly answer None
to a FE definition d ? E2, the probability that S
evokes f1 increases.
Modeling Figure 1 displays a screenshot of
the worker interface. The task is designed as per
Section 3.3.2, but with major differences with
respect to its content. This is better described
by an example. The sentence Karen threw
her arms round my neck, spilling
champagne everywhere contains the LU
throw.v evoking the frame BODY MOVEMENT.
However, throw.v is ambiguous and may also
evoke CAUSE MOTION. We ask to annotate both
the BODY MOVEMENT and the CAUSE MOTION
2Cf. Section 3 for more details
745
Figure 1: 1-step approach worker interface
core FEs, respectively as regular and cross-frame
units.
Discussion We do not interpret the None choice
as an abstention from judgment, since it is a cor-
rect answer for cross-frame units. Instead of pre-
cision and recall, we are thus able to directly com-
pute workers? accuracy upon a majority vote. We
envision an improvement with respect to the 2-
step methodology, as we avoid the proven risk of
error propagation originating from wrongly anno-
tated frames in the first step. Table 2 illustrates
the results we collected. As expected, accuracy
reached a consistent enhancement. This demon-
strates the hypothesis we stated in Section 1 on
the cognitive plausibility of a bottom-up approach
for frame annotation. Furthermore, the execu-
tion time decreases compared to the sum of the
2 steps, namely 130 hours against 171. Neverthe-
less, the cost is sensibly higher due to the higher
number of questions that need to be addressed, in
average 4.6 against 1.5. Untrusted judgments se-
riously grow, mainly because of the cross-frame
gold complexity. Workers seem puzzled by the
presence of None, which is a required answer for
such units. If we consider the English FrameNet
annotation agreement values between experts re-
ported by Pado? and Lapata (2009) as the upper
bound (i.e., .897 for frame discrimination and .949
for FEs recognition), we believe our experimental
setting can be reused as a valid alternative.
4 Conclusion
In this work, we presented an approach to perform
frame annotation with crowdsourcing techniques,
based on a single annotation step and on manu-
ally simplified FE definitions. Since the results
seem promising, we are currently running larger
scale experiments with the full set of FrameNet 1.5
annotated sentences. Input data, interface screen-
shots and full results are available and regularly
updated at http://db.tt/gu2Mj98i.
Future work will include the investigation of a
frame assignment strategy. In fact, we do not take
into account the case of conflicting FE annotations
in cross-frame units. Hence, we need a confidence
score to determine which frame emerges if work-
ers selected contradictory answers in a subset of
cross-frame FE definitions.
Acknowledgements
The research leading to this paper was partially
supported by the European Union?s 7th Frame-
work Programme via the NewsReader Project
(ICT-316404).
746
References
[Baker2012] Collin F Baker. 2012. Framenet, cur-
rent collaborations and future goals. Language Re-
sources and Evaluation, pages 1?18.
[Burchardt et al2006] Aljoscha Burchardt, Katrin Erk,
Anette Frank, Andrea Kowalski, Sebastian Pado,
and Manfred Pinkal. 2006. Salto?a versatile multi-
level annotation tool. In Proceedings of LREC 2006,
pages 517?520. Citeseer.
[Chamberlain et al2008] Jon Chamberlain, Massimo
Poesio, and Udo Kruschwitz. 2008. Phrase detec-
tives: A web-based collaborative annotation game.
Proceedings of I-Semantics, Graz.
[Chamberlain et al2009] Jon Chamberlain, Udo Kr-
uschwitz, and Massimo Poesio. 2009. Constructing
an anaphorically annotated corpus with non-experts:
Assessing the quality of collaborative annotations.
In Proceedings of the 2009 Workshop on The Peo-
ple?s Web Meets NLP: Collaboratively Constructed
Semantic Resources, pages 57?62. Association for
Computational Linguistics.
[Fillmore et al2002] Charles J. Fillmore, Collin F.
Baker, and Hiroaki Sato. 2002. The FrameNet
Database and Software Tools. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC 2002), pages 1157?
1160, Las Palmas, Spain.
[Fillmore1976] Charles J. Fillmore. 1976. Frame Se-
mantics and the nature of language. In Annals of the
New York Academy of Sciences: Conference on the
Origin and Development of Language, pages 20?32.
Blackwell Publishing.
[Heilman and Smith2010] Michael Heilman and
Noah A. Smith. 2010. Extracting Simplified
Statements for Factual Question Generation. In
Proceedings of QG2010: The Third Workshop on
Question Generation, Pittsburgh, PA, USA.
[Hong and Baker2011] Jisup Hong and Collin F Baker.
2011. How good is the crowd at ?real? wsd? ACL
HLT 2011, page 30.
[Negri et al2011] Matteo Negri, Luisa Bentivogli,
Yashar Mehdad, Danilo Giampiccolo, and Alessan-
dro Marchetti. 2011. Divide and conquer: crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 670?679, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Pado? and Lapata2009] Sebastian Pado? and Mirella La-
pata. 2009. Cross-lingual annotation projection for
semantic roles. Journal of Artificial Intelligence Re-
search, 36(1):307?340.
[Palmer et al2005] Martha Palmer, Dan Gildea, and
Paul Kingsbury. 2005. The Proposition Bank: A
Corpus Annotated with Semantic Roles. Computa-
tional Linguistics, 31(1).
[Ruppenhofer et al2006] Josef Ruppenhofer, Michael
Ellsworth, Miriam R.L. Petruck, Christopher R.
Johnson, and Jan Scheffczyk. 2006. FrameNet
II: Extended Theory and Practice. Available at
http://framenet.icsi.berkeley.edu/book/book.html.
[Snow et al2008] Rion Snow, Brendan O?Connor,
Daniel Jurafsky, and Andrew Y Ng. 2008. Cheap
and fast?but is it good?: evaluating non-expert an-
notations for natural language tasks. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 254?263. Association
for Computational Linguistics.
[Von Ahn et al2006] Luis Von Ahn, Mihir Kedia, and
Manuel Blum. 2006. Verbosity: a game for col-
lecting common-sense facts. In Proceedings of the
SIGCHI conference on Human Factors in computing
systems, pages 75?78. ACM.
747
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 214?217,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
FBK-IRST: Semantic Relation Extraction using Cyc
Kateryna Tymoshenko and Claudio Giuliano
FBK-IRST
I-38050, Povo (TN), Italy
tymoshenko@fbk.eu, giuliano@fbk.eu
Abstract
We present an approach for semantic re-
lation extraction between nominals that
combines semantic information with shal-
low syntactic processing. We propose to
use the ResearchCyc knowledge base as
a source of semantic information about
nominals. Each source of information
is represented by a specific kernel func-
tion. The experiments were carried out
using support vector machines as a clas-
sifier. The system achieves an overall F
1
of 77.62% on the ?Multi-Way Classifica-
tion of Semantic Relations Between Pairs
of Nominals? task at SemEval-2010.
1 Introduction
The SemEval-2010 Task 8 ?Multi-Way Classifi-
cation of Semantic Relations Between Pairs of
Nominals? consists in identifying which seman-
tic relation holds between two nominals in a sen-
tence (Hendrickx et al, 2010). The set of rela-
tions is composed of nine mutually exclusive se-
mantic relations and the Other relation. Specifi-
cally, the task requires to return the most informa-
tive relation between the specified pair of nomi-
nals e
1
and e
2
taking into account their order. An-
notation guidelines show that semantic knowledge
about e
1
and e
2
plays a very important role in dis-
tinguishing among different relations. For exam-
ple, relations Cause-Effect and Product-Producer
are closely related. One of the restrictions which
might help to distinguish between them is that
products must be concrete physical entities, while
effects must not.
Recently, there has emerged a large number of
freely available large-scale knowledge bases. The
ground idea of our research is to use them as
source of semantic information. Among such re-
sources there are DBpedia,
1
YAGO,
2
and Open-
Cyc.
3
On the one hand, DBpedia and YAGO have
been automatically extracted from Wikipedia.
They have a good coverage of named entities, but
their coverage of common nouns is poorer. They
seem to be more suitable for relation extraction be-
tween named entities. On the other hand, Cyc is
a manually designed knowledge base, which de-
scribes actions and entities both in common life
and in specific domains (Lenat, 1995). Cyc has
a good coverage of common nouns, making it in-
teresting for our task. The full version of Cyc is
freely available to the research community as Re-
searchCyc.
4
We approached the task using the system intro-
duced by Giuliano et al (2007) as a basis. They
exploited two information sources: the whole sen-
tence where the relation appears, and WordNet
synonymy and hyperonymy information. In this
paper, we (i) investigate usage of Cyc as a source
of semantic knowledge and (ii) linguistic infor-
mation, which give useful clues to semantic re-
lation extraction. From Cyc, we obtain informa-
tion about super-classes (in the Cyc terminology
generalizations) of the classes which correspond
to nominals in a sentence. The sentence itself
provides linguistic information, such as local con-
texts of entities, bag of verbs and distance between
nominals in the context.
The different sources of information are rep-
resented by kernel functions. The final system
is based on four kernels (i.e., local context ker-
nel, distance kernel, verbs kernel and generaliza-
tion kernel). The experiments were carried out us-
ing support vector machines (Vapnik, 1998) as a
classifier. The system achieves an overall F
1
of
1
http://dbpedia.org/
2
http://www.mpi-inf.mpg.de/yago-naga/
yago/
3
http://www.cyc.com/opencyc
4
http://research.cyc.com/
214
77.62%.
2 Kernel Methods for Relation
Extraction
In order to implement the approach based on shal-
low syntactic and semantic information, we em-
ployed a linear combination of kernels, using the
support vector machines as a classifier. We de-
veloped two types of basic kernels: syntactic and
semantic kernels. They were combined by exploit-
ing the closure properties of kernels. We define the
composite kernelK
C
(x
1
, x
2
) as follows.
n
?
i=1
K
i
(x
1
, x
2
)
?
K
i
(x
1
, x
1
)K
i
(x
2
, x
2
)
. (1)
Each basic kernelK
i
is normalized.
All the basic kernels are explicitly calculated as
follows
K
i
(x
1
, x
2
) = ??(x
1
), ?(x
2
)? , (2)
where ?(?) is the embedding vector. The resulting
feature space has high dimensionality. However,
Equation 2 can be efficiently computed explicitly
because the representations of input are extremely
sparse.
2.1 Local context kernel
Local context is represented by terms, lemmata,
PoS tags, and orthographic features extracted
from a window around the nominals considering
the token order. Formally, given a relation ex-
ample R, we represent a local context LC =
t
?w
, ..., t
?1
, t
0
, t
+1
, ..., t
+w
as a row vector
?
LC
(R) = (tf
1
(LC), tf
2
(LC), ..., tf
m
(LC) ) ? {0, 1}
m
,
(3)
where tf
i
is a feature function which returns 1
if the feature is active in the specified position
of LC; 0 otherwise. The local context kernel
K
LC
(R
1
, R
2
) is defined as
K
LC e1
(R
1
, R
2
) +K
LC e2
(R
1
, R
2
), (4)
where K
LC e1
and K
LC e2
are defined by substi-
tuting the embedding of the local contexts of e
1
and e
2
into Equation 2, respectively.
2.2 Verb kernel
The verb kernel operates on the verbs present in
the sentence,
5
representing it as a bag-of-verbs.
5
On average there are 2.65 verbs per sentence
More formally, given a relation example R, we
represent the verbs from it as a row vector
?
V
(R) = (vf(v
1
, R), ..., vf(v
l
, R)) ? {0, 1}
l
, (5)
where the binary function vf(v
i
, R) shows if a
particular verb is used in R. By substituting
?
V
(R) into Equation 2 we obtain the bag-of-verbs
kernelK
V
.
2.3 Distance kernel
Given a relation example R(e
1
, e
2
), we repre-
sent the distance between the nominals as a one-
dimensional vector
?
D
(R) =
1
dist(e
1
, e
2
)
? <
1
, (6)
where dist(e
1
, e
2
) is number of tokens between
the nominals e
1
and e
2
in a sentence. By substitut-
ing ?
D
(R) into Equation 2 we obtain the distance
kernelK
D
.
2.4 Cyc-based kernel
Cyc is a comprehensive, manually-build knowl-
edge base developed since 1984 by CycCorp. Ac-
cording to Lenat (1995) it can be considered as
an expert system with domain spanning all ev-
eryday actions and entities, like Fish live in wa-
ter. The open-source version of Cyc named Open-
Cyc, which contains the full Cyc ontology and re-
stricted number of assertions, is freely available
on the web. Also the full power of Cyc has been
made available to the research community via Re-
searchCyc. Cyc knowledge base contains more
than 500,000 concepts and more than 5 million as-
sertions about them. They may refer both to com-
mon human knowledge like food or drinks and to
specialized knowledge in domains like physics or
chemistry. The knowledge base has been formu-
lated using CycL language. A Cyc constant repre-
sents a thing or a concept in the world. It may be
an individual, e.g. BarackObama, or a collection,
e.g. Gun, Screaming.
2.4.1 Generalization kernel
Given a nominal e, we map it to a set of Cyc
constants EC = {c
i
}, using the Cyc function
denotation-mapper. Nominals in Cyc usually de-
note constants-collections. Notice that we do not
performword sense disambiguation. For each c
i
?
EC, we query Cyc for collections which general-
ize it. In Cyc collection X generalizes collection
215
Y if each element of Y is also an element of col-
lectionX . For instance, collection Gun is general-
ized by Weapon, ConventionalWeapon, Mechani-
calDevice and others.
The semantic kernel incorporates the data from
Cyc described above. More formally, given a rela-
tion example R each nominal e is represented as
?
EC
(R) = (fc(c
1
, e), ..., fc(c
k
, e)) ? {0, 1}
k
, (7)
where the binary function fc(c
i
, e) shows if a par-
ticular Cyc collection c
i
is a generalization of e.
The bag-of-generalizations kernel
K
genls
(R
1
, R
2
) is defined as
K
genls e1
(R
1
, R
2
) +K
genls e2
(R
1
, R
2
) , (8)
whereK
genls e1
andK
genls e2
are defined by sub-
stituting the embedding of generalizations e
1
and
e
2
into Equation 2 respectively.
3 Experimental setup and Results
Sentences have been tokenized, lemmatized and
PoS tagged with TextPro.
6
Information for gener-
alization kernel has been obtained from Research-
Cyc. All the experiments were performed using
jSRE customized to embed our kernels.
7
jSRE
uses the SVM package LIBSVM (Chang and Lin,
2001). The task is casted as multi-class classifica-
tion problem with 19 classes (2 classes for each
relation to encode the directionality and 1 class
to encode Other). The multiple classification task
is handled with One-Against-One technique. The
SVM parameters have been set as follows. The
cost-factor W
i
for a given class i is set to be the
ratio between the number of negative and positive
examples. We used two values of regularization
parameter C: (i) C
def
=
1
?
K(x,x)
where x are
all examples from the training set, (ii) optimized
C
grid
value obtained by brute-force grid search
method. The default value is used for the other
parameters.
Table 1 shows the performance of different ker-
nel combinations, trained on 8000 training exam-
ples, on the test set. The system achieves the
best overall macro-average F
1
of 77.62% using
K
LC
+K
V
+K
D
+K
genls
. Figure 1 shows the
learning curves on the test set. Our experimen-
tal study has shown that the size of the training
6
http://textpro.fbk.eu/
7
jSRE is a Java tool for relation extraction avail-
able at http://tcc.itc.it/research/textec/
tools-resources/jsre.html.
1000 2000 4000 80000.40
0.450.50
0.550.60
0.650.70
0.750.80
0.850.90
Cause-EffectComponent-Whole Content-ContainerEntity-Destination Entity-Origin Instrument-Agency Member-CollectionMessage-TopicProduct-ProducerAll
Number of training examples
F1
Figure 1: Learning curves on the test set per rela-
tion
Kernels P R F
1
K
LC
+K
V
+K
D
+K
genls
74.98 80.69 77.62
K
LC
+K
V
+K
D
+K
genls
* 78.51 76.03 77.11
K
LC
+K
D
+K
genls
* 78.14 75.93 76.91
K
LC
+K
genls
* 78.19 75.70 76.81
K
LC
+K
D
+K
genls
72.98 80.28 76.39
K
LC
+K
genls
73.05 79.98 76.28
Table 1: Performance on the test set. Combina-
tions marked with * were run with C
grid
, others
with C
def
.
set influences the performance of the system. We
observe that when the system is trained on 8000
examples the overall F
1
increases for 14.01% as
compared to the case of 1000 examples.
4 Discussion and error analysis
The experiments have shown thatK
LC
is the core
kernel of our approach. It has good performance
on its own. For instance, it achieves precision of
66.16%, recall 72.67% and F
1
of 69.13% evalu-
ated using 10-fold cross-validation on the training
set.
Relation K
LC
K
LC
+K
genls
?F
1
Cause-Effect 74.29 76.41 2.12
Component-Whole 61.24 66.13 4.89
Content-Container 76.36 79.12 2.76
Entity-Destination 82.85 83.95 1.10
Entity-Origin 72.09 74.13 2.04
Instrument-Agency 57.71 65.51 7.80
Member-Collection 81.30 83.40 2.10
Message-Topic 60.41 69.09 8.68
Product-Producer 55.95 63.52 7.57
Table 2: The contribution of Cyc evaluated on the
training set.
216
Generalization kernel combined with local con-
text kernel gives precision of 70.38%, recall of
76.96%, and F
1
73.47% with the same exper-
imental setting. The increase of F
1
per re-
lation is shown in the Table 2 in the col-
umn ?F
1
. The largest F
1
increase is ob-
served for Instrument-Agency (+7.80%),Message-
Topic (+8.68%) and Product-Producer (+7.57%).
K
genls
reduces the number of misclassifications
between the two directions of the same rela-
tion, like Product-Producer(artist,design). It
also captures the differences among relations,
specified in the annotation guidelines. For in-
stance, the system based only on K
LC
misclass-
fied ?The<e1>species</e1>makes a squelching
<e2>noise</e2>? as Product-Producer(e2,e1).
Generalizations for <e2>noise</e2> provided
by Cyc include Event, MovementEvent, Sound.
According to the annotation guidelines a product
must not be an event. A system based on the com-
bination of K
LC
and K
genls
correctly labels this
example as Cause-Effect(e1,e2).
K
genls
improves the performance in general.
However, in some cases using Cyc as a source of
semantic information is a source of errors. Firstly,
sometimes the set of constants for a given nom-
inal is empty (e.g., disassembler, babel) or does
not include the correct one (noun surge is mapped
to the constant IncreaseEvent). In other cases,
an ambiguous nominal is mapped to many con-
stants at once. For instance, notes is mapped
to a set of constants, which includes Musical-
Note, Note-Document and InformationRecording-
Process. Word sense disambiguation should help
to solve this problem. Other knowledge bases like
DBpedia and FreeBase
8
can be used to overcome
the problem of lack of coverage.
Bag-of-word kernel with all words from the
sentence did not impact the final result.
9
However,
the information about verbs present in the sentence
represented by K
V
helped to improve the perfor-
mance. A preliminary error analysis shows that a
deeper syntactic analysis could help to further im-
prove the performance.
For comparison purposes, we also exploited
WordNet information by means of the supersense
kernel K
SS
(Giuliano et al, 2007). In all exper-
iments, K
SS
was outperformed by K
genls
. For
instance, K
LC
+ K
SS
gives overall F
1
measure
8
http://www.freebase.com/
9
This kernel has been evaluated only on the training data.
of 70.29% with the same experimental setting as
described in the beginning of this section.
5 Conclusion
The paper describes a system for semantic rela-
tions extraction, based on the usage of semantic
information provided by ResearchCyc and shal-
low syntactic features. The experiments have
shown that the external knowledge, encoded as
super-class information from ResearchCyc with-
out any word sense disambiguation, significantly
contributes to improve overall performance of the
system. The problem of the lack of coverage may
be overcome by the usage of other large-scale
knowledge bases, such as DBpedia. For future
work, we will try to use the Cyc inference en-
gine to obtain implicit information about nominals
in addition to the information about their super-
classes and perform word sense disambiguation.
Acknowledgments
The research leading to these results has received funding
from the ITCH project (http://itch.fbk.eu), spon-
sored by the Italian Ministry of University and Research and
by the Autonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint Research
Project under Future Internet - Internet of Content program
of the Information Technology Center, Fondazione Bruno
Kessler.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Claudio Giuliano, Alberto Lavelli, Daniele Pighin, and
Lorenza Romano. 2007. Fbk-irst: Kernel methods
for semantic relation extraction. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid
?
O S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.
2010. Semeval-2010 task 8: Multi-way classification of
semantic relations between pairs of nominals. In Proceed-
ings of the 5th SIGLEXWorkshop on Semantic Evaluation,
Uppsala, Sweden.
Douglas B. Lenat. 1995. CYC: A large-scale investment in
knowledge infrastructure. Communications of the ACM,
38(11):33?38.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.
217
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 19?27,
Beijing, August 2010
Extending English ACE 2005 Corpus Annotation with Ground-truth
Links to Wikipedia
Luisa Bentivogli
FBK-Irst
bentivo@fbk.eu
Pamela Forner
CELCT
forner@celct.it
Claudio Giuliano
FBK-Irst
giuliano@fbk.eu
Alessandro Marchetti
CELCT
amarchetti@celct.it
Emanuele Pianta
FBK-Irst
pianta@fbk.eu
Kateryna Tymoshenko
FBK-Irst
tymoshenko@fbk.eu
Abstract
This paper describes an on-going annota-
tion effort which aims at adding a man-
ual annotation layer connecting an exist-
ing annotated corpus such as the English
ACE-2005 Corpus to Wikipedia. The an-
notation layer is intended for the evalua-
tion of accuracy of linking to Wikipedia in
the framework of a coreference resolution
system.
1 Introduction
Collaboratively Constructed Resources (CCR)
such as Wikipedia are starting to be used for a
number of semantic processing tasks that up to
few years ago could only rely on few manually
constructed resources such as WordNet and Sem-
Cor (Fellbaum, 1998). The impact of the new re-
sources can be multiplied by connecting them to
other existing datasets, e.g. reference corpora. In
this paper we will illustrate an on-going annota-
tion effort which aims at adding a manual anno-
tation layer connecting an existing annotated cor-
pus such as the English ACE-2005 dataset1 to a
CCR such as Wikipedia. This effort will produce
a new integrated resource which can be useful for
the coreference resolution task.
Coreference resolution is the task of identify-
ing which mentions, i.e. individual textual de-
scriptions usually realized as noun phrases or pro-
nouns, refer to the same entity. To solve this
task, especially in the case of non-pronominal co-
reference, researchers have recently started to ex-
ploit semantic knowledge, e.g. trying to calculate
1http://projects.ldc.upenn.edu/ace/
the semantic similarity of mentions (Ponzetto and
Strube, 2006) or their semantic classes (Ng, 2007;
Soon et al, 2001). Up to now, WordNet has been
one of the most frequently used sources of se-
mantic knowledge for the coreference resolution
task (Soon et al, 2001; Ng and Cardie, 2002). Re-
searchers have shown, however, that WordNet has
some limits. On one hand, although WordNet has
a big coverage of the English language in terms
of common nouns, it still has a limited coverage
of proper nouns (e.g. Barack Obama is not avail-
able in the on-line version) and entity descrip-
tions (e.g. president of India). On the other hand
WordNet sense inventory is considered too fine-
grained (Ponzetto and Strube, 2006; Mihalcea and
Moldovan, 2001). In alternative, it has been re-
cently shown that Wikipedia can be a promising
source of semantic knowledge for coreference res-
olution between nominals (Ponzetto and Strube,
2006).
Consider some possible uses of Wikipedia.
For example, knowing that the entity men-
tion ?Obama? is described on the Wikipedia
page Barack_Obama2, one can benefit from
the Wikipedia category structure. Categories as-
signed to the Barack_Obama page can be used
as semantic classes, e.g. ?21st-century presidents
of the United States?. Another example of a
useful Wikipedia feature are the links between
Wikipedia pages. For instance, some Wikipedia
pages contain links to the Barack_Obama page.
Anchor texts of these links can provide alterna-
2The links to Wikipedia pages are given displaying only
the last part of the link which corresponds to the title of the
page. The complete link can be obtained adding this part to
http://en.wikipedia.org/wiki/.
19
tive names of this entity, e.g. ?Barack Hussein
Obama? or ?Barack Obama Junior?.
Naturally, in order to obtain semantic knowl-
edge about an entity mention from Wikipedia
one should link this mention to an appropriate
Wikipedia page, i.e. to disambiguate it using
Wikipedia as a sense inventory. The accuracy
of linking entity mentions to Wikipedia is a very
important issue. For example, such linking is a
step of the approach to coreference resolution de-
scribed in (Bryl et al, 2010). In order to evaluate
this accuracy in the framework of a coreference
resolution system, a corpus of documents, where
entity mentions are annotated with ground-truth
links to Wikipedia, is required.
The possible solution of this problem is to ex-
tend the annotation of entity mentions in a corefer-
ence resolution corpus. In the recent years, coref-
erence resolution systems have been evaluated on
various versions of the English Automatic Content
Extraction (ACE) corpus (Ponzetto and Strube,
2006; Versley et al, 2008; Ng, 2007; Culotta et
al., 2007; Bryl et al, 2010). The latest publicly
available version is ACE 20053.
In this paper we present an extension of ACE
2005 non-pronominal entity mention annotations
with ground-truth links to Wikipedia. This exten-
sion is intended for evaluation of accuracy of link-
ing entity mentions to Wikipedia pages. The an-
notation is currently in progress. At the moment
of writing this paper we have completed around
55% of the work. The extension can be exploited
by coreference resolution systems, which already
use ACE 2005 corpus for development and testing
purposes, e.g. (Bryl et al, 2010). Moreover, En-
glish ACE 2005 corpus is multi-purpose and can
be used in other information extraction (IE) tasks
as well, e.g. relation extraction. Therefore, we
believe that our extension might also be useful for
other IE tasks, which exploit semantic knowledge.
In the following we start by providing a brief
overview of the existing corpora annotated with
links to Wikipedia. In Section 3 we describe some
characteristics of the English ACE 2005 corpus,
which are relevant to the creation of the extension.
Next, we describe the general annotation princi-
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
ples and the procedure adopted to carry out the
annotation. In Section 4 we present some anal-
yses of the annotation and statistics about Inter-
Annotator Agreement.
2 Related work
Recent approaches to linking terms to Wikipedia
pages (Cucerzan, 2007; Csomai and Mihalcea,
2008; Milne and Witten, 2008; Kulkarni et al,
2009) have used two kinds of corpora for eval-
uation of accuracy: (i) sets of Wikipedia pages
and (ii) manually annotated corpora. In Wikipedia
pages links are added to terms ?only where
they are relevant to the context?4. Therefore,
Wikipedia pages do not contain the full annotation
of all entity mentions. This observation applies
equally to the corpus used by (Milne and Wit-
ten, 2008), which includes 50 documents from the
AQUAINT corpus annotated following the same
strategy5. The corpus created by (Cucerzan, 2007)
contains annotation of named entities only6. It
contains 756 annotations, therefore for our pur-
poses it is limited in terms of size.
Kulkarni et al (2009) have annotated 109 doc-
uments collected from homepages of various sites
with as many links as possible7. Their annotation
is too extensive for our purposes, since they do not
limit annotation to the entity mentions. To tackle
this issue, one can use an automatic entity mention
detector, however it is likely to introduce noise.
3 Creating the extension
The task consists of manually annotating the
non-pronominal mentions contained in the En-
glish ACE 2005 corpus with links to appropriate
Wikipedia articles. The objective of the work is
to create an extension of ACE 2005, where all the
mentions contained in the ACE 2005 corpus are
disambiguated using Wikipedia as a sense reposi-
tory to point to. The extension is intended for the
4http://en.wikipedia.org/wiki/
Wikipedia:Manual_of_Style
5http://www.nzdl.org/wikification/
docs.html
6http://research.microsoft.com/en-us/
um/people/silviu/WebAssistant/TestData/
7http://soumen.cse.iitb.ac.in/?soumen/
doc/CSAW/
20
evaluation of accuracy of linking to Wikipedia in
the framework of a coreference resolution system.
3.1 The English ACE 2005 Corpus
The English ACE 2005 corpus is composed of
599 articles assembled from a variety of sources
selected from broadcast news programs, newspa-
pers, newswire reports, internet sources and from
transcribed audio. It contains the annotation of a
series of entities (person, location, organization)
for a total of 15,382 different entities and 43,624
mentions of these entities. A mention is an in-
stance of a textual reference to an object, which
can be either named (e.g. Barack Obama), nom-
inal (e.g. the president), or pronominal (e.g. he,
his, it). An entity is an aggregate of all the men-
tions which refer to one conceptual entity. Beyond
the annotation of entities and mentions, ACE 05
contains also the annotation of local co-reference
for the entities; this means that mentions which
refer to the same entity in a document have been
marked with the same ID.
3.2 Annotating ACE 05 with Wikipedia
Pages
For the purpose of our task, not all the
ACE 05 mentions are annotated, but only the
named (henceforth NAM) and nominal (hence-
forth NOM) mentions. The resulting additional
annotation layer will contain a total of 29,300
mentions linked to Wikipedia pages. As specif-
ically regards the annotation of NAM mentions,
information about local coreference contained in
ACE 05 has been exploited in order to speed up
the annotation process. In fact, only the first
occurrence of the NAM mentions in each doc-
ument has been annotated and the annotation is
then propagated to all the other co-referring NAM
mentions in the document.
Finally, it must be noted that in ACE 05, given
a complex entity description, both the full ex-
tent of the mention (e.g. president of the United
States) and its syntactic head (e.g. ?president?)
are marked. In our Wikipedia extension only the
head of the mention is annotated, while the full ex-
tent of the mention is available from the original
ACE 05 corpus.
3.3 General Annotation Principles
Depending on the mention type to be annotated,
i.e. NAM or NOM, a different annotation strategy
has been followed. Each mention of type NAM
is annotated with a link to a Wikipedia page de-
scribing the referred entity. For instance, ?George
Bush? is annotated with a link to the Wikipedia
page George_W._Bush.
NOM mentions are annotated with a link to the
Wikipedia page which provides a description of
its appropriate sense. For instance, in the exam-
ple ?I was driving Northwest of Baghdad and I
bumped into these guys going around the capi-
tal? the mention ?capital? is linked to the page
which provides a description of its meaning, i.e.
Capital_(political). Note that the object
of linking is the textual description of an entity,
and not the entity itself. In the example, even
though from the context it is clear that the mention
?capital? refers to Baghdad, we provide a link to
the concept of capital and not to the entity Bagdad.
As a term can have both a more generic sense
and a more specific one, depending on the context
in which it occurs, mentions of type NOM can of-
ten be linked to more than one Wikipedia page.
Whenever possible, the NOM mentions are anno-
tated with a list of links to appropriate Wikipedia
pages in the given context. In such cases, links
are sorted in order of relevance, where the first
link corresponds to the most specific sense for that
term in its context, and therefore is regarded as the
best choice. For instance, for the NOM mention
head ?President? which in the context identifies
the United States President George Bush the an-
notation?s purpose is to provide a description of
the item ?President?, so the following links are
selected as appropriate: President_of_the_
United_States and President.
The correct interpretation of the term is strictly
related to the context in which the term occurs.
While performing the annotation, the context of
the entire document has always been exploited in
order to correctly identify the specific sense of the
mention.
3.4 Annotation Procedure
The annotation procedure requires that the men-
tion string is searched in Wikipedia in order to
21
find the appropriate page(s) to be used for anno-
tating the mention. In the annotation exercise, the
annotators have always taken into consideration
the context where a mention occurs, searching for
both the generic and the most specific sense of the
mention disambiguated in the context. In fact, in
the example provided above, not only ?President?,
but also ?President of the United States? has been
queried in Wikipedia as required by the context.
Not only the context, but also some features of
Wikipedia must be mentioned as they affect the
annotation procedure:
a. One element which contributes to the choice
of the appropriate Wikipedia page(s) for
one mention is the list of links proposed in
Wikipedia?s Disambiguation pages. Disam-
biguation pages are non-article pages which
are intended to allow the user to choose from
a list of Wikipedia articles defining different
meanings of a term, when the term is am-
biguous. Disambiguation pages cannot be
used as links for the annotation as they are
not suitable for the purposes of this task. In
fact, the annotator?s task is to disambiguate
the meaning of the mention, so one link,
pointing to a specific sense, is to be cho-
sen. Disambiguation pages should always be
checked as they provide useful suggestions
in order to reach the appropriate link(s).
b. In the same way as Disambiguation pages,
Wikitionary cannot be used as linking page,
as it provides a list of possible senses for a
term and not only one specific sense which is
necessary to disambiguate the mention.
c. In Wikipedia, terms may be redirected to
other terms which are related in terms of
morphological derivation; i.e. searching for
the term ?Senator? you are automatically
redirected to ?Senate?; or querying ?citizen?
you are automatically redirected to ?citizen-
ship?. Redirections have always been con-
sidered appropriate links for the term.
Some particular rules have been followed in order
to deal with specific cases in the annotation, which
are described below:
1. As explained before in Section 3.2, as a gen-
eral rule the head of the ACE 05 mention
is annotated with Wikipedia links. In those
cases where the syntactic head of the men-
tion is a multiword lexical unit, the ACE 05
practice is to mark as head only the rightmost
item of the multiword. For instance, in the
case of the multiword ?flight attendant? only
?attendant? is marked as head of the men-
tion, although ?flight attendant? is clearly a
multiword lexical unit that should be anno-
tated as one semantic whole. In our anno-
tation we take into account the meaning of
the whole lexical unit; so, in the above exam-
ple, the generic sense of ?attendant? has not
been given, whereas Flight_attendant
is considered as the appropriate link.
2. In some cases, in ACE 2005 pronouns like
?somebody?, ?anybody?, ?anyone?, ?one?,
?others?, were incorrectly marked as NOM
(instead of PRO). Such cases, which amount
to 117, have been marked with the tag ?No
Annotation?.
3. When a page exists in Wikipedia for a given
mention but not for the specific sense in that
context the ?Missing sense? annotation has
been used. One example of ?Missing sense?
is for instance the term ?heart? which has 29
links proposed in the ?Disambiguation page?
touching different categories (sport, science,
anthropology, gaming, etc.), but there is no
link pointing to the sense of ?center or core of
something?; so, when referring to the heart
of a city, the term has been marked as ?Miss-
ing sense?.
4. When no article exists in Wikipedia for a
given mention, the tag ?No page? has been
adopted.
5. Nicknames, i.e. descriptive names used
in place of or in addition to the official
name(s) of a person, have been treated as
NAM. Thus, even if nicknames look like de-
scriptions of individuals (and their reference
should not be solved, following the general
rule), they are actually used and annotated as
22
Number of annotated mentions 16310
Number of single link mentions 13774
Number of multi-link mentions 1458
Number of ?No Page? annotations 481
Number of ?Missing Sense? 480
annotations
Number of ?No Annotation? 117
annotations
Total number of links 16851
Total number of links in multi-link 3077
mentions
Table 1: Annotation data
proper names aliases. For example, given the
mention ?Butcher of Baghdad?, whose head
?Butcher? is to be annotated, the appropriate
Wikipedia link is Saddam_Hussein, auto-
matically redirected from the searched string
?Butcher of Baghdad?. The link Butcher
is not appropriate as it provides a description
of the mention. It is interesting the fact that
Wikipedia itself redirects to the page of Sad-
dam Hussein.
4 The ACE05-WIKI Extension
Up to now, the 55% of the markable men-
tions have been annotated by one annotator,
amounting to 16,310 mentions. This annotation
has been carried out by CELCT in a period
of two months from February 22 to April 30,
2010, using the on-line version of Wikipedia,
while the remaining 45% of the ACE mentions
will be annotated during August 2010. The
complete annotation will be freely available
at: http://www.celct.it/resources.
php?id_page=acewiki2010, while the
ACE 2005 corpus is distributed by LDC8.
4.1 Annotation Data Analysis
Table 1 gives some statistics about the overall
annotation. In the following sections, mentions
annotated with one link are called ?single link?,
whereas, mentions annotated with more than one
link are named ?multi-link?.
These data refer to the annotation of each sin-
gle mention. It is not possible to give statis-
tics at the entity level, as mentions have differ-
8http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
Annotation Mention Type
NAM NOM
Single link mentions 6589 7185
Multi-link mentions 79 1379
Missing sense 96 384
No Page 440 41
Table 2: Distinction of NAM and NOM in the an-
notation
ent ID depending on the documents they belong
to, and the information about the cross-document
co-reference is not available. Moreover, mentions
of type NOM are annotated with different links
depending on their disambiguated sense, making
thus impossible to group them together.
Most mentions have been annotated with only
one link; if we consider multi-link mentions, we
can say that each mention has been assigned an
average of 2,11 links (3,077/1,458).
Data about ?Missing sense? and ?No page?
are important as they provide useful information
about the coverage of Wikipedia as sense in-
ventory. Considering both ?Missing sense? and
?No page? annotations, the total number of men-
tions which have not been linked to a Wikipedia
page amounts to 6%, equally distributed between
?Missing sense? and ?No page? annotations. This
fact proves that, regarded as a sense inventory,
Wikipedia has a broad coverage. As Table 2
shows, the mentions for which more than one link
was deemed appropriate are mostly of type NOM,
while NAM mentions have been almost exclu-
sively annotated with one link only. The very few
cases in which a NAM mention is linked to more
than one Wikipedia page are primarily due to (i)
mistakes in the ACE 05 annotation (for example,
the mention ?President? was erroneously marked
as a NAM); (ii) or to cases where nouns marked
as NAM could also be considered as NOMs (see
for instance the mention ?Marine?, to mean the
Marine Corps).
Table 2 provides also statistics about the ?Miss-
ing sense? and ?No page? cases provided on men-
tions divided among the NAM and NOM type.
The ?missing sense? annotation concerns mostly
the NOM category, whereas the NAM category
is hardly affected. This attests the fact that per-
sons, locations and organizations are well repre-
23
sented in Wikipedia. This is mainly due to the
encyclopedic nature of Wikipedia where an arti-
cle may be about a person, a concept, a place,
an event, a thing etc.; instead, information about
nouns (NOM) is more likely to be found in a
dictionary, where information about the meanings
and usage of a term is provided.
4.2 Inter-Annotator Agreement
About 3,100 mentions, representing more than
10% of the mentions to be annotated, have been
annotated by two annotators in order to calculate
Inter-Annotator Agreement.
Once the annotations were completed, the
two annotators carried out a reconciliation phase
where they compared the two sets of links pro-
duced. Discrepancies in the annotation were
checked with the aim of removing only the more
rough errors and oversights. No changes have
been made in the cases of substantial disagree-
ment, which has been maintained.
In order to measure Inter-Annotator Agree-
ment, two metrics were used: (i) the Dice coeffi-
cient to measure the agreements on the set of links
used in the annotation9 and (ii) two measures of
agreement calculated at the mention level, i.e. on
the group of links associated to each mention.
The Dice coefficient is computed as follows:
Dice = 2C/(A + B)
where C is the number of common links chosen by
the two annotators, while A and B are respectively
the total number of links selected by the first and
the second annotator. Table 3 shows the results
obtained both before and after the reconciliation
9The Dice coefficient is a typical measure used to com-
pare sets in IR and is also used to calculate inter-annotator
agreement in a number of tasks where an assessor is allowed
to select a set of labels to apply to each observation. In fact,
in these cases measures such as the widely used K are not
good to calculate agreement. This is because K only offers
a dichotomous distinction between agreement and disagree-
ment, whereas what is needed is a coefficient that also allows
for partial disagreement between judgments. In fact, in our
case we often have a partial agreement on the set of links
given for each mention. Also considering only the mentions
for which a single link has been chosen, it is not possible
to calculate K statistics in a straightforward way as the cate-
gories (i.e. the possible Wikipedia pages) in some cases can-
not be determined a priori and are different for each mention.
Due to these factors chance agreement cannot be calculated
in an appropriate way.
BEFORE AFTER
reconciliation reconciliation
DICE 0.85 0.94
Table 3: Statistics about Dice coefficient
BEFORE AFTER
reconciliation reconciliation
Complete 77.98% 91.82%
On first link 84.41% 95.58%
Table 4: Agreement at the mention level
process. Agreement before reconciliation is satis-
factory and shows the feasibility of the annotation
task and the reliability of the annotation scheme.
Two measures of agreement at the mention
level are also calculated. To this purpose, we
count the number of mentions where annotators
agree, as opposed to considering the agreement on
each link separately. Mention-level agreement is
calculated as follows:
Number of mentions with annotation in agreement
Total number of annotated mentions
We calculate both ?complete? agreement and
agreement on the first link. As regards the first
measure, a mention is considered in complete
agreement if (i) it has been annotated with the
same link(s) and (ii) in the case of multi-link men-
tions, links are given in the same order. As for the
second measure, there is agreement on a mention
if both the annotators chose the same first link (i.e.
the one judged as the most appropriate), regard-
less of other possible links assigned to that men-
tion. Table 4 provides data about both complete
agreement and first link agreement, calculated be-
fore and after the annotators reconciliation.
4.3 Disagreement Analysis
Considering the 3,144 double-annotated men-
tions, the cases of disagreements amount to 692
(22,02%) before the reconciliation while they are
reduced to 257 (8,18%) after that process. It is in-
teresting to point out that the disagreements affect
the mentions of type NOM in most of the cases,
whereas mentions of type NAM are involved only
in 3,8% of the cases.
Examining the two annotations after the recon-
ciliation, it is possible to distinguish three kinds
of disagreement which are shown in Table 5 to-
24
Number of
Disagreement type Disagreements
1) No matching in the link(s)
proposed
105 (40,85%)
2) No matching on the first link,
but at least one of the other links
is the same
14 (5,45%)
3) Matching on the first link and
mismatch on the number of ad-
ditional links
138 (53,70%)
Total Disagreements 257
Table 5: Types of disagreements
gether with the data about their distribution. An
example of disagreement of type (1) is the anno-
tation of the mention ?crossing?, in the following
context: ?Marines from the 1st division have se-
cured a key Tigris River Crossing?. Searching for
the word ?river crossing? in the Wikipedia search-
box, the Disambiguation Page is opened and a
list of possible links referring to more specific
senses of the term are offered, while the generic
?river crossing? sense is missing. The annota-
tors are required to choose just one of the possi-
ble senses provided and they chose two different
links pointing to pages of more specific senses:
{Ford_%28river%29} and {Bridge}.
Another example is represented by the annota-
tion of the mention ?area? in the context : ?Both
aircraft fly at 125 miles per hour gingerly over en-
emy area?. In Wikipedia no page exists for the
specific sense of ?area? appropriate in the con-
text. Searching for ?area? in Wikipedia, the page
obtained is not suitable, and the Disambiguation
page offers a list of various possible links to either
more specific or more general senses of the term.
One annotator judged the more general Wikipedia
page Area_(subnational_entity) as ap-
propriate to annotate the mention, while the sec-
ond annotator deemed the page not suitable and
thus used the ?Missing sense? annotation.
Disagreement of type (2) refers to cases where
at least one of the links proposed by the annota-
tors is the same, but the first (i.e. the one judged
as the most suitable) is different. Given the fol-
lowing context: ?Tom, You know what Liber-
als want?, the two annotation sets provided for
the mention ?Liberal? are: {Liberalism} and
{Liberal_Party, Modern_liberalism_
in_the_United_States, Liberalism}.
The first annotator provided only one link for
the mention ?liberal?, which is different from the
first link provided by second annotator. However,
the second annotator provided also other links,
among which there is the link provided by the first
annotator.
Another example is represented by the annota-
tion of the mention ?killer?. Given the context:
?He?d be the 11th killer put to death in Texas?, the
two annotators provided the following link sets:
{Assassination, Murder} and {Murder}.
Starting from the Wikipedia disambiguation page,
the two annotators agreed on the choice of one of
the links but not on the first one.
Disagreement of type (3) refers to cases where
both annotators agree on the first link, correspond-
ing to the most specific sense, but one of them
also added link(s) considered appropriate to an-
notate the mention. Given the context: ?7th Cav-
alry has just taken three Iraqi prisoners?, the an-
notations provided for the term ?prisoners? are:
{Prisoner_of_war} and {Prisoner_of_
war, Incarceration}. This happens when
more than one Wikipedia pages are appropriate to
describe the mention.
As regards the causes of disagreement, we see
that the cases of disagreement mentioned above
are due to two main reasons:
a. The lack of the appropriate sense in
Wikipedia for the given mention
b. The different interpretation of the context in
which the mention occurs.
In cases of type (a) the annotators adopted differ-
ent strategies to perform their task, that is:
i. they selected a more general sense (i.e.
?area? which has been annotated with
Area_(subnational_entity)),
ii. they selected a more specific sense (see for
example the annotations of the mentions
?river crossing?).
iii. they selected the related senses proposed by
the Wikipedia Disambiguation page (as in
the annotation of ?killer? in the example
above).
25
Disagreement Reas. a Reas. b Tot
type (see above)
1) No match 95 10 105
2) No match on 4 10 14
first link
3) Mismatch on 138 138
additional links
Total 99 158 257
(38,5%) (61,5%)
Table 6: Distribution of disagreements according
to their cause
iv. they used the tag ?Missing sense?.
As Wikipedia is constantly evolving, adding
new pages and consequently new senses, it is
reasonable to think that the considered elements
might find the appropriate specific/general link as
time goes by.
Case (b) happens when the context is ambigu-
ous and the information provided in the text al-
lows different possible readings of the mention
to be annotated, making thus difficult to disam-
biguate its sense. These cases are independent
from Wikipedia sense repository but are related to
the subjectivity of the annotators and to the inher-
ent ambiguity of text.
Table 6 shows the distribution of disagreements
according to their cause. Disagreements of type 1
and 2 can be due to both a and b reasons, while
disagreements of type 3 are only due to b.
The overall number of disagreements shows
that the cases where the two annotators did not
agree are quite limited, amounting only to 8%.
The analyses of the disagreements show some
characteristics of Wikipedia considered as sense
repository. As reported in Table 8, in the 61,5%
of the cases of disagreement, the different anno-
tations are caused by the diverse interpretation
of the context and not by the lack of senses in
Wikipedia. It is clear that Wikipedia has a good
coverage and it proves to be a good sense disam-
biguation tool. In some cases it reveals to be too
fine-grained and in other cases it remains at a more
general level.
5 Conclusion
This paper has presented an annotation work
which connects an existing annotated corpus such
as the English ACE 2005 dataset to a Collabo-
ratively Constructed Semantic Resource such as
Wikipedia. Thanks to this connection Wikipedia
becomes an essential semantic resource for the
task of coreference resolution. On one hand, by
taking advantage of the already existing annota-
tions, with a relatively limited additional effort,
we enriched an existing corpus and made it useful
for a new NLP task which was not planned when
the corpus was created. On the other hand, our
work allowed us to explore and better understand
certain characteristics of the Wikipedia resource.
For example we were able to demonstrate in quan-
titative terms that Wikipedia has a very good cov-
erage, at least as far as the kind of entity men-
tions which are contained in the ACE 2005 dataset
(newswire) is concerned.
Acknowledgments
The research leading to these results has re-
ceived funding from the ITCH project (http://
itch.fbk.eu), sponsored by the Italian Min-
istry of University and Research and by the Au-
tonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint
Research Project under Future Internet - Internet
of Content program of the Information Technol-
ogy Center, Fondazione Bruno Kessler.
We thank Giovanni Moretti from CELCT for
technical assistance.
References
Bryl, Volha, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In
Proceedings of the 19th European Conference on
Artificial Intelligence (ECAI 2010), August.
Csomai, Andras and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Cucerzan, Silviu. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June. Association for Computational Linguistics.
26
Culotta, Aron, Michael L. Wick, and Andrew McCal-
lum. 2007. First-order probabilistic models for
coreference resolution. In Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 81?88.
Fellbaum, Christiane, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Kulkarni, Sayali, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of wikipedia entities in web text. In KDD
?09: Proceedings of the 15th ACM SIGKDD inter-
national conference on Knowledge discovery and
data mining, pages 457?466, New York, NY, USA.
ACM.
Mihalcea, Rada and Dan I. Moldovan. 2001.
Ez.wordnet: Principles for automatic generation of
a coarse grained wordnet. In Russell, Ingrid and
John F. Kolen, editors, FLAIRS Conference, pages
454?458. AAAI Press.
Milne, David and Ian H. Witten. 2008. Learning
to link with wikipedia. In CIKM ?08: Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 509?518, New York,
NY, USA. ACM.
Ng, Vincent and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104?111.
Ng, Vincent. 2007. Semantic class induction and
coreference resolution. In ACL 2007, Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, June 23-30, 2007,
Prague, Czech Republic, pages 536?543.
Ponzetto, S. P. and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 192?
199.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistic, 27(4):521?544.
Versley, Yannick, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
Bart: a modular toolkit for coreference resolution.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies, pages 9?12.
27
Proceedings of the 4th International Workshop on Computational Terminology, pages 22?31,
Dublin, Ireland, August 23 2014.
Identification of Bilingual Terms from Monolingual Documents for
Statistical Machine Translation
Mihael Arcan1 Claudio Giuliano2 Marco Turchi2 Paul Buitelaar1
1 Unit for Natural Language Processing, Insight @ NUI Galway, Ireland
{mihael.arcan , paul.buitelaar}@insight-centre.org
2 FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
{giuliano, turchi}@fbk.eu
Abstract
The automatic translation of domain-specific documents is often a hard task for generic Sta-
tistical Machine Translation (SMT) systems, which are not able to correctly translate the large
number of terms encountered in the text. In this paper, we address the problems of automatic
identification of bilingual terminology using Wikipedia as a lexical resource, and its integration
into an SMT system. The correct translation equivalent of the disambiguated term identified in
the monolingual text is obtained by taking advantage of the multilingual versions of Wikipedia.
This approach is compared to the bilingual terminology provided by the Terminology as a Ser-
vice (TaaS) platform. The small amount of high quality domain-specific terms is passed to the
SMT system using the XML markup and the Fill-Up model methods, which produced a relative
translation improvement up to 13% BLEU score points
1 Introduction
Translation tasks often need to deal with domain-specific terms in technical documents, which require
specific lexical knowledge of the domain. Nowadays, SMT systems are suitable to translate very frequent
expressions but fail in translating domain-specific terms. This mostly depends on a lack of domain-
specific parallel data from which the SMT systems can learn. Translation tools such as Google Translate
or open source phrase-based SMT systems, trained on generic data, are the most common solutions and
they are often used to translate manuals or very specific texts, resulting in unsatisfactory translations.
This problem is particular relevant for professional translators that work with documents coming from
different domains and are supported by generic SMT systems. A valuable solution to help them in han-
dling domain-specific terms is represented by online terminology resources, e.g. IATE - Inter-Active
Terminology for Europe,1 which are continuously updated and can be easily queried. However, the man-
ual use of these services can be very time demanding. For this reason, the identification and embedding
of domain-specific terms in an SMT system is a crucial step towards increasing translator productivity
and translation quality in highly specific domains.
In this paper, we propose an approach to automatically detect monolingual domain-specific terms from
a source language document and identify their equivalents using Wikipedia cross-lingual links. For this
purpose we extend The Wiki Machine API,2 a tool for linking terms in text to Wikipedia pages, adding
two more components able to first identify domain-specific terms, and to find their translations in a target
language. The identified bilingual terms are then compared with those obtained by TaaS (Skadins? et al.,
2013). The embedding of the domain-specific terms into an SMT system is performed by use of the
XML markup approach, which uses the terms as preferred translation candidates at run time, and the
Fill-Up model (Bisazza et al., 2011), which emphasizes phrase pairs extracted from the bilingual terms.
Our results show that the performance of our technique and TaaS are comparable in the identification
of monolingual and bilingual domain-specific terms. From the machine translation point of view, our
experiments highlight the benefit of integrating bilingual terms into the SMT system, and the relative
improvement in BLEU score of the Fill-Up model over the baseline and the XML markup approach.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1 http://iate.europa.eu/ 2 https://bitbucket.org/fbk/thewikimachine/
Terminology questions in texts authored by patients
Noemie Elhadad
Department of Biomedical Informatics
Columbia University, USA
noemie@dbmi.columbia.edu
This work is c er a Creative Com ons Attribution 4.0 Internatio l License. Page numb rs and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
22
2 Methodology
Given a source document, it is processed by our pipeline that: (i) with the help of The Wiki Machine, it
identifies, disambiguates and links all terms in the document to the Wikipedia pages; (ii) the terms and
their links are used to identify the domain of the document and filter out the terms that are not domain-
specific; (iii) the translation of such terms is obtained following the Wikipedia cross-lingual links; (iv)
the bilingual domain-specific terms are embedded into the SMT system using different strategies. In the
rest of this section, each step is described in detail.
2.1 Bilingual Term Identification
Term Detection and Linking The Wiki Machine is a tool for linking terms in text to Wikipedia pages
and enriching them with information extracted from Wikipedia and Linked Open Data (LOD) resources
such as DBPedia or Freebase. The Wiki Machine has been preferred among other approaches because it
achieves the best performance in term disambiguation and linking (Mendes et al., 2011), and facilitates
the extraction of structured information from Wikipedia.
The annotation process consists of a three-step pipeline based on statistical and machine learning
methods that exclusively uses Wikipedia to train the models. No linguistic processing, such as stemming,
morphology analysis, POS tagging, or parsing, is performed. This choice facilitates the portability of the
system as the only requirement is the existence of a Wikipedia version with a sufficient coverage for the
specific language and domain. The first step identifies and ranks the terms by relevance using a simple
statistical approach based on tf-idf weighting, where all the n-grams, for n from 1 to 10, are generated
and the idf is directly calculated on Wikipedia pages. The second step links the terms to Wikipedia pages.
The linking problem is cast as a supervised word sense disambiguation problem, in which the terms must
be disambiguated using Wikipedia to provide the sense inventory and the training data (for each sense,
a list of phrases where the term appears) as first introduced in (Mihalcea, 2007). The application uses
an ensemble of word-expert classifiers that are implemented using the kernel-based approach (Giuliano
et al., 2009). Specifically, domain and syntagmatic aspects of sense distinction are modelled by means
of a combination of the latent semantic and string kernels (Shawe-Taylor and Cristianini, 2004). The
third step enriches the linked terms using information extracted from Wikipedia and LOD resources.
The additional information relative to the pair term/Wikipedia page consists of alternative terms (i.e.,
orthographical and morphological variants, synonyms, and related terms), images, topic, type, cross
language links, etc. For example, in the text ?click right mouse key to pop up menu and Gnome panel?,
The Wiki Machine identifies the terms mouse, key, pop up menu and Gnome panel. For the ambiguous
term mouse, the linking algorithm returns the Wikipedia page ?Mouse (computing)?, and the other terms
used to link that page in Wikipedia with their frequency, i.e., computer mouse, mice, and Mouse.
In the context of the experiments reported here, we were specifically interested in the identification of
domain-specific bilingual terminology to be embedded into the SMT system. For this reason, we extend
The Wiki Machine adding the functionality of filtering out terms that do not belong to the document
domain, and of automatically retrieving term translations.
Domain Detection To identify specific terms, we assign a domain to each linked term in a text, after
that we obtain the most frequent domain and filter out the terms that are out of scope. In the example
above, the term mouse is accepted because it belongs to the domain computer science, as the majority of
terms (mouse, pop up menu and Gnome panel), while the term key in the domain music is rejected.
The large number of languages and domains to cover prevents us from using standard text classification
techniques to categorize the document. For this reason, we implemented an approach based on the
mapping of the Wikipedia categories into the WordNet domains (Bentivogli et al., 2004). The Wikipedia
categories are created and assigned by different human editors, and are therefore less rigorous, coherent
and consistent than usual ontologies. In addition, the Wikipedia?s category hierarchy forms a cyclic graph
(Zesch and Gurevych, 2007) that limits its usability. Instead, the WordNet domains are organized in a
hierarchy that contains only 164 items with a degree of granularity that makes them suitable for Natural
Language Processing tasks. The approach we are proposing overcomes the Wikipedia category sparsity,
allows us reducing the number of domains to few tens instead of some hundred thousands (800,000
23
categories in the English Wikipedia) and does not require any language-specific training data. Wikipedia
categories that contain more pages (?1,000) have been manually mapped to WordNet domains. The
domain for a term is obtained as follows. First, for each term, we extract its set of categories, C, from
the Wikipedia page linked to it. Second, by means of a recursive procedure, all possible outgoing paths
(usually in a large number) from each category in C are followed in the graph of Wikipedia categories.
When one of the mapped categories to a WordNet domain is found, the approach stops and associates the
relative WordNet domain to the term. In this way, more and more domains are assigned to a single term.
Third, to isolate the most relevant one, these domains are ranked according the number of times they have
been found following all the paths. The most frequent domain is assigned to the terms. Although this
process needs the human intervention for the manual mapping, it is done once and it is less demanding
than annotating large amounts of training documents for text classification, because it does not require
the reading of the document for topic identification.
Bilingual Term Extraction The last phase consists in finding the translation of the domain terminol-
ogy. We exploit the Wikipedia cross-language links, which, however, provide an alignment at page level
not at term level. To deal with this issue we introduced the following procedure. If the term is equal to
the source page title (ignoring case) we return the target page; otherwise, we return the most frequent al-
ternative form of the term in the target language. From the previous example, the system is able to return
the Italian page Mouse and all terms used in the Italian Wikipedia to express this concept of Mouse in
computer science. Using this information, the term mouse is paired with its translation into Italian.
2.2 Integration of Bilingual Terms into SMT
A straightforward approach for adding bilingual terms to the SMT system consists of concatenating the
training data and the terms. Although it has been shown to perform better than more complex techniques
(Bouamor et al., 2012), it is still affected by major disadvantages that limits its use in real applications.
In particular, when small amounts of bilingual terms are concatenated with a large training dataset, terms
with ambiguous translations are penalised, because the most frequent and general translations often
receive the highest probability, which drives the SMT system to ignore specific translations.
In this paper, we focus on two techniques that give more priority to specific translations than generic
ones: the Fill-Up model and the XML markup approach. The Fill-Up model has been developed to
address a common scenario where a large generic background model exists, and only a small quantity
of in-domain data can be used to build an in-domain model. Its goal is to leverage the large coverage
of the background model, while preserving the domain-specific knowledge coming from the in-domain
data. Given the generic and the in-domain phrase tables, they are merged. For those phrase pairs that
appear in both tables, only one instance is reported in the Fill-Up model with the largest probabilities
according to the tables. To keep track of a phrase pair?s provenance, a binary feature that penalises if
the phrase pair comes from the background table is added. The same strategy is used for reordering
tables. In our experiments, we use the bilingual terms identified from the source data as in-domain
data. Word alignments are computed on the concatenation of the data. Phrase extraction and scoring
are carried out separately on each corpus. The XML markup approach makes it possible to directly pass
external knowledge to the decoder, specifying translations for particular spans of the source sentence. In
our scenario, the source term is used to identify a span in the source sentence, while the target term is
directly passed to the decoder. With the setting exclusive, the decoder uses only the specified translations
ignoring other possible translations in the translation model.
3 Experimental Setting
In our experiments, we used different English-Italian and Italian-English test sets from two domains: (i)
a small subset of the GNOME project data3 (4,3K tokens) and KDE4 Data4 (9,5K) for the IT domain
and (ii) a subset of the EMEA corpus (11K) for the medical domain.
In order to assess the quality of the monolingual and bilingual terms, we create a terminological gold
standard. Two annotators with a linguistic background and English and Italian proficiency were asked
3 https://l10n.gnome.org/ 4 http://i18n.kde.org/
24
to mark all domain-specific terms in a set of 66 English and Italian documents of the GNOME corpus,
and a set of 100 paragraphs (4,3K tokens) from the KDE4 corpus.5 Domain-specificity was defined as
all (multi-)words that are typically used in the IT domain and that may have different Italian translations
in other domains. The average Cohen?s Kappa of GNOME and KDE anno computed at token level was
0.66 for English and 0.53 for Italian. Following Landis and Koch (1977), this corresponds to a substantial
and moderate agreement between the annotators.
Finally the gold standard dataset was generated by the intersection of the annotations of the two an-
notators. In detail, for the GNOME dataset the annotators marked 93 single-word and 134 multi-word
expressions (MWEs), resulting 227 terms in overall. For the KDE anno dataset, 321 monolingual terms
for the GNOME dataset were annotated, whereby 192 of them were multi-word expressions. This results
in 190 unique bilingual terms for the GNOME corpus and 355 for the KDE anno dataset.
We compare the monolingual and bilingual terms identified by our approach to the terms obtained
by the online service TaaS,6 which is a cloud-based platform for terminology services based on the
state-of-the-art terminology extraction and bilingual terminology alignment methods. TaaS provides
several options in term identification, of which we selected TWSC, Tilde wrapper system for CollTerm,
(Pinnis et al., 2012). TWSC is based on linguistic analysis, i.e. part of speech tagging and morpho-
syntactic patterns, enriched with statistical features. TaaS allows for lookup in several manually and
automatically built monolingual and bilingual terminological resources and for our experiment we use
EuroTermBank (ETB), Taus Data and Web Data. Accessing several resources, TaaS may provide several
translations for a unique source term, but not an indicator of their translation quality. To avoid assigning
the same probability to all the translations of the same source term, we prioritise a translation by the
resource it was provided. In our case, we favour first the translation provided by ETB. If no translation
is available, we use the translation provided by Taus Data or eventually from Web Data. Before starting
the term extraction approach, TaaS requires manual specification of the source and target languages, the
domain, and the source document. Since we focused on the IT and medical domains we set the options
to ?Information and communication technology? and ?Medicine and pharmacy?, respectively.
For each translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where
the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The IRSTLM toolkit
(Federico et al., 2008) was used to build the 5-gram language model. For a broader domain coverage,
we merged parts of the following parallel resources: JRC-Acquis (Steinberger et al., 2006), Europarl
(Koehn, 2005) and OpenSubtitles2013 (Tiedemann, 2012), this results in a generic training corpus of
?37M tokens and a development set of ?10K tokens.
In our experiments, an instance of Moses trained on the generic parallel dataset was used in three
different scenarios: (i) as baseline SMT system without embedded terminology; (ii) in the XML markup
approach for translating remaining parts that were not covered by the embedded terminology; (iii) in the
Fill-Up method as background translation model.
4 Evaluation
In this Section, we report the performance of the different term identification tools and term embedding
methods for the two domains: IT and the medical domain. For evaluating the extracted monolingual
and bilingual terms, we calculate precision, recall and f-measure using the manually labelled KDE anno
and GNOME datasets. In addition, we perform a manual inspection of a subset of the bilingual identi-
fied terms. The BLEU metric (Papineni et al., 2002) was used to automatically evaluate the quality of
the translations. The metric calculates the overlap of n-grams between the SMT system output and a
reference translation, provided by a professional translator.
4.1 Monolingual Term Identification
In Table 1, the column ?Ident.? represents the number of identified terms for each tool, whereby we
observed TaaS always extracts more terms than The Wiki Machine. While extracting Italian terms,
TaaS extracts twice as more terms as The Wiki Machine, which can be explained by the overall lower
5 In the rest of the paper, we refer to the annotated part of KDE4 as KDE anno
6 https://demo.taas-project.eu/
25
English Italian
KDE anno Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 431 144 287 0.442 0.594 0.507 518 147 371 0.326 0.511 0.398
The Wiki Machine 327 247 80 0.400 0.406 0.403 207 184 23 0.429 0.268 0.330
GNOME Ident. unigram MWE Precision Recall F1 Ident. unigram MWE Precision Recall F1
TaaS 311 119 192 0.260 0.355 0.301 359 110 249 0.272 0.415 0.329
The Wiki Machine 275 199 76 0.303 0.364 0.330 196 167 29 0.331 0.275 0.301
Table 1: Evaluation of monolingual term identification for the KDE anno and GNOME dataset.
amount of Italian pages in Wikipedia compared to the English version. Focusing on the amount of
identified single-word and multi-word expressions, it is interesting to notice that TaaS, independently of
the language, extracts around twice as more MWEs than single words. Differently, The Wiki Machine
identifies mostly single-word terms, whereby they represent around three-fourth of all identified terms
for English and around 12% for Italian.
For the KDE anno dataset, TaaS in most cases (except in precision for the Italian KDE anno dataset)
outperforms The Wiki Machine approach in all metrics. Especially we observed a higher recall produced
by the TaaS approach, which can be deduced from the higher number of extracted MWEs compared to
The Wiki Machine approach. On the English GNOME dataset, The Wiki Machine performs comparable
results to TaaS, with a slightly higher recall and F1. On the Italian side, The Wiki Machine identifies less
MWEs than TaaS, which results in a low recall and F1.
In summary, we observe that TaaS performs best on the KDE anno dataset, whereas The Wiki Machine
and TaaS perform comparable results on the GNOME dataset. Analysing the overall results, we notice
that precision, recall and F1 are generally better in English than in Italian. This is due to the fact that
Italian tends to use more words to express the same concept compared to English.
4.2 Bilingual Term Identification
Table 2 reports the performance of The Wiki Machine and TaaS in the identification of bilingual terms
evaluated against the manually produced list of terms. In both language pairs and datasets, TaaS and The
Wiki Machine mostly identify similar amounts of bilingual terms (column ?Ident.?) and match with the
gold standard (column ?Mat.?). Only for KDE anno, It?En, TaaS identifies almost 50% more bilingual
terms than The Wiki Machine.
It is worth noticing that, although TaaS is accessing high quality manually-produced termbases, e.g.
ETB in our results, there is no evidence that it works significantly better than The Wiki Machine access-
ing Wikipedia. In fact, in terms of F1, The Wiki Machine performs best on the GNOME annotated test
set, while it is outperformed by TaaS on KDE anno. In both cases, differences in performance are mini-
mal. According to the precision measure, The Wiki Machine seems to be able to produce more accurate
bilingual terms.
The automatic evaluation shows difficulties (low F1 scores) for The Wiki Machine and TaaS in iden-
tifying bilingual terms that perfectly match the gold standard. To better understand the quality of term
translations, we asked one of the annotators involved in the creation of the gold standard to perform a
manual evaluation of a subset of fifty bilingual terms randomly selected from each list. We used the
four error categories proposed in (Aker et al., 2013): 1) The terms are exact translations of each other
in the domain; 2) Inclusion: Not an exact translation, but an exact translation of one term is entirely
contained within the term in the other language; 3) Overlap: Not category 1 or 2, but the terms share at
least one translated word; 4) Unrelated: No word in either term is a translation of a word in the other.
The percentages of bilingual terms assigned to each class are shown in Table 3.
In terms of comparison between the two tools, the manual evaluation confirms that there is no evidence
that a tool produces better term translations than the other in all the test sets. In fact, except for KDE anno
En?It where TaaS outperforms The Wiki Machine, the percentage of bilingual terms assigned to class
1 for both the tools is almost similar. In terms of absolute scores, the manual evaluation shows that
the quality of the identified bilingual terms is relatively high (merging the terms assigned to classes 1
26
GNOME En?It Ident. Mat. Precision Recall F1
TaaS 145 20 0.138 0.105 0.119
The Wiki Machine 156 25 0.160 0.130 0.144
GNOME It?En Ident. Mat. Precision Recall F1
TaaS 139 21 0.151 0.110 0.127
The Wiki Machine 140 23 0.164 0.121 0.139
KDE anno En?It Ident. Mat. Precision Recall F1
TaaS 249 65 0.261 0.183 0.215
The Wiki Machine 229 49 0.202 0.138 0.164
KDE anno It?En Ident. Mat. Precision Recall F1
TaaS 228 58 0.254 0.163 0.199
The Wiki Machine 155 48 0.292 0.135 0.185
Table 2: Automatic evaluation of bilingual terms ex-
tracted from GNOME and KDE anno.
GNOME En?It 1 2 3 4
TaaS 0.66 0.08 0.00 0.26
The Wiki Machine 0.70 0.08 0.06 0.16
GNOME It?En 1 2 3 4
TaaS 0.78 0.08 0.02 0.12
The Wiki Machine 0.68 0.12 0.04 0.16
KDE anno En?It 1 2 3 4
TaaS 0.90 0.00 0.06 0.04
The Wiki Machine 0.70 0.10 0.06 0.14
KDE anno It?En 1 2 3 4
TaaS 0.70 0.10 0.10 0.10
The Wiki Machine 0.64 0.22 0.08 0.06
Table 3: Manual evaluation of bilingual terms
based on four error categories (1-4).
and 2, we reach a score, in most of the cases, larger than 80%). This is in contrast with the automatic
evaluation, which reports limited performances (F1 ? 0.2) for both methods. The main reason is that
the automatic evaluation requires a perfect match between the identified and the gold standard bilingual
terms to measure an improvement in F1, while the manual evaluation can reward bilingual terms that do
not perfectly match any gold standard terms but are correct translations of each other. An example is
the multi-word bilingual term ?settings of the network connection? impostazioni della connessione di
rete? that is present in the gold standard as a single multi-word term, while it is identified by The Wiki
Machine as two distinct bilingual terms, i.e. ?network connection? connessione di rete? and ?settings
? impostazioni?. From the translation point of view, both the distinct terms are correct and they are
assigned to class 1 during the manual evaluation, but they are ignored by the automatic evaluation.
The analysis of terms assigned to error class four shows that both methods are affected by similar
problems. The main source of error is the correct detection of the source term domain, which results in
a translated term that does not belong to the correct domain. For instance, in the bilingual term ?stringhe
? shoe and boot laces?, the term ?stringhe? (?strings? in the IT domain) is translated into ?laces?. Simi-
larly, the English term ?launchers? (?lanciatori? in Italian in the IT domain) is translated into ?lanciarazzi
multiplo? (?multiple rocket launchers? in English), which is clearly not an IT term. Furthermore, The
Wiki Machine seems to have more problems in identifying the right morphological variation, e.g. ?in-
dirizzi ip? ip address?, where ?indirizzi? is a plural noun and needs to be translated into ?addresses?.
This is expected because page titles in Wikipedia are not always inflected. An interesting example high-
lighted by the annotator in the TaaS translations is: ?percorso di ricerca? ? ?how do i access refresh
grid texture??, where the Italian term (?search path? in English) is translated with a completely wrong
translation. In the next Section we evaluate whether the automatic identified bilingual terms can improve
the performance of an SMT system and if it is robust to the aforementioned errors.
4.3 Embedding Terminology into SMT
Our further experiments focused on the automatic evaluation of the translation quality of the EMEA,
GNOME and KDE test sets (Table 4). The obtained bilingual terminology from TaaS and The Wiki Ma-
chine was embedded through the Fill-Up and XML markup approaches. The approximate randomization
approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances
are statistically significant with a p-value < 0.05. The parameters of the baseline method and the Fill-Up
models were optimized on the development set.
Injecting the obtained TaaS bilingual terms improves the BLEU score in several cases. XML markup
outperforms the general baseline approach in three (out of eight) datasets, whereby three of them are
statistically significant (GNOME En?It, KDE anno En?It). Embedding the same bilingual terminol-
ogy into the Fill-Up model helped to outperform the baseline approach for all test sets, whereby only the
result for EMEA En?It is not statistically significant.
27
GNOME KDE anno EMEA KDE4
En?It It?En En?It It?En En?It It?En En?It It?En
general baseline 15.39 21.62 15.58 22.64 25.88 25.75 19.22 23.54
XML Mark-up (TaaS) 15.87 22.45* 17.62* 23.88* 25.84 25.74 18.97 24.27*
Fill-Up Model (TaaS) 16.22* 22.73* 17.61* 23.45* 25.95 26.02* 19.69* 24.56*
XML Mark-up (The Wiki Machine) 15.49 20.57 17.19* 23.44* 25.59 24.97 17.74 22.16
Fill-Up Model (The Wiki Machine) 15.82 21.70 16.48* 23.28* 26.35* 26.44* 19.61* 24.14*
Table 4: Automatic BLEU Evaluation on GNOME, KDE and EMEA datasets with different term em-
bedding strategies (bold results = best performance ; * statistically significant compared to baseline).
Finally, we investigate the impact of embedding the identified terms provided by The Wiki Machine.
When we suggest translation candidates with the XML markup, it only slightly outperforms the baseline
approach for GNOME En?It, but statistically significant improves the translations for the KDE anno
test set for both language directions. Similarly to previous observations, the Fill-Up model improves
further the translations, i.e. the translations are statistically significant better than the baseline for both
language pairs of both KDE test sets as well as for EMEA.
To better understand our translation results, we manually inspected the EMEA En?It sentences, which
have the best translation performance. For each of the source sentence and the translation method,
we analyse the translated sentences and the bilingual terms that match at least one word in the source
sentence. Both translation strategies tried to encapsulate the bilingual terms, but there is clear evidence
that the Fill-Up model better embeds the target terms in the context of the translation. For instance in
the following example, the target sentence produced by the XML markup (XML trg) does not contain
the article ?la?, uses a wrong conjunction (?di? instead of ?per?) and wrongly orders the adjective with
the noun (?adulti pazienti? instead of ?pazienti adulti?). All these issues are correctly addressed by the
Fill-Up model (Fill-Up trg) which produces a smoother translation.
source sentence: adult patients receive therapy for tumours
reference sentence: pazienti adulti ricevono la terapia per i tumori
bilingual terms: therapy? terapia, patients? pazienti, adult? adulti
XML trg: adulti pazienti ricevono terapia di tumori
Fill-Up trg: pazienti adulti ricevono la terapia per i tumori
Analysing the number of suggested bilingual terms per sentence, we notice that The Wiki Machine
tends to propose more terms than TaaS (on average, The Wiki Machine 3.1, TaaS 2.5 per sentence).
Of these terms, TaaS provides on average more translations for each unique source term than The Wiki
Machine (on average, TaaS 1.51, The Wiki Machine 1).
In addition to evaluating the performance of TaaS and The Wiki Machine separately, for the EMEA
dataset we concatenate the terminological lists provided by the tools and supply it to the XML markup
and the Fill-Up approach. Embedding the combined terminology with the XML markup produces a
BLEU score of 25.59 for En?It and 24.92 for It?En. This performance is similar to the scores obtained
using the terminology provided by The Wiki Machine, but worse compared to TaaS. Passing the whole
terminology to the Fill-Up model, the BLEU score increases up to 26.57 for En?It and 27.02 for It?En,
which are the best BLEU scores for the EMEA test set. This experiment shows the complementarity of
the two term identification methods and suggests a novel research direction.
5 Related Work
The main focus of our research is on bilingual term identification and the embedding of this knowledge
into an SMT system. Since previous research (Wu et al. (2008); Haddow and Koehn (2012)) showed that
an SMT system built by using a large general resource cannot be used to translate domain-specific terms,
we have to provide the system domain-specific lexical knowledge.
Wikipedia with its rich lexical and semantic knowledge was used as a resource for bilingual term
identification in the context of SMT. Tyers and Pieanaar (2008) describe method for extracting bilingual
dictionary entries from Wikipedia to support the machine translation system. Based on exact string
28
matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual
dictionary. Besides the interwiki link system, Erdmann et al. (2009) enhances their bilingual dictionary
by using redirection page titles and anchor text within Wikipedia. To filter out incorrect term translation
pairs, the authors use the backward link information to prove if a redirect page title or an anchor text
represents a synonymous expression. Niehues and Waibel (2011) analyse different methods to integrate
the extracted Wikipedia titles into their system, whereby they explore methods to disambiguate between
different translations by using the text in the articles. In addition, the authors use morphological forms
of terms to enhance the extracted bilingual dictionary. The results show that the number of out-of-
vocabulary words could be reduced by 50% on computer science lectures, which improved the translation
quality by more than 1 BLEU point. Arcan et al. (2013) restrict term identification to the observed
domain by using the frequency information of Wikipedia categories. Different from these approaches
we focus on domain-specific dictionary generation, ignoring identified terms which do not belong to the
domain to be observed. Furthermore, we take advantage of the Wikipedia category graph representation
and its linking to WordNet domain, which allowed us to identify the domain we were interested in.
Furthermore, research has been done on the integration of domain-specific parallel data into SMT,
either by retraining small domain-specific and large general resources as one concatenated parallel data
(Koehn and Schroeder, 2007), adding new phrase pairs directly into the phrase table (Langlais, 2002;
Ren et al., 2009; Haddow and Koehn, 2012) or assigning adequate weights to the in- and out-of-domain
translation models (Foster and Kuhn (2007); La?ubli et al. (2013)). Bouamor et al. (2012) address the
problem of finding the best approach to integrate new obtained knowledge in an SMT system, and show
that they should be used as additional parallel sentences to train the translation model. In our approach,
we use the XML markup and the Fill-Up approach, which handles the in-domain parallel data equally
to the out-domain data. Furthermore, Okita and Way (2010) investigate the effect of integrating bilin-
gual terminology in the training step of an SMT system, and analyse in particular the performance and
sensitivity of the word aligner. As opposed to their approach, we do not have prior knowledge about the
bilingual terminology, since we extract it from the document to be translated.
6 Conclusion
In this paper we presented an approach to identify bilingual domain-specific terms starting from a mono-
lingual text and to integrate these into an SMT system. With the help of terminological and lexical
resources, we are able to discover a small amount (?200) of high-quality domain-specific terms and
enhanced the performance of an SMT system trained on large amounts (1.8M) of parallel sentences.
Monolingual and bilingual term evaluation showed no evidence that one of the tested tools (The Wiki
Machine or TaaS) produces better terms than the other in all the test sets. Depending on the manual map-
ping between the Wikipedia categories and WordNet domains and the existence of a Wikipedia version,
our approach is language and domain independent, does not need training data and is able to overcome
the sparseness and coherence problems of the Wikipedia categories. Evaluation of the two systems on
different language directions and domains shows significant improvements over the baseline in terms
of two BLEU scores (up to 13%) and confirms the applicability of such techniques in a real scenario.
It is interesting to notice that the Fill-Up technique regularly outperforms the XML markup approach,
taking advantage of all terms and not only the overlapping terms in the text to be translated. Our contri-
bution shows a different context of using Fill-Up and extends the usability of it in terms of embedding
terminological knowledge into SMT. In future work, we plan to focus on exploiting morphological term
variations taking advantage of the alternative terms (i.e., orthographical and morphological variants,
synonyms, and related terms) provided by The Wiki Machine. This will make it possible to increase the
coverage adding new terms and the accuracy of the proposed method for bilingual term identification.
Acknowledgments
This publication has emanated from research supported in part by a research grant from Science Founda-
tion Ireland (SFI) under Grant Number SFI/12/RC/2289 and by the European Union supported projects
EuroSentiment (Grant No. 296277), LIDER (Grant No. 610782) and MateCat (ICT-2011.4.2-287688).
29
References
Ahmet Aker, Monica Paramita, and Robert Gaizauskas. 2013. Extracting bilingual terminologies from comparable
corpora. In Proceedings of ACL, Sofia, Bulgaria.
Mihael Arcan, Susan Marie Thomas, Derek De Brandt, and Paul Buitelaar. 2013. Translating the FINREP taxon-
omy using a domain-specific corpus. In Machine Translation Summit XIV, pages 199?206.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, and Emanuele Pianta. 2004. Revising the wordnet domains
hierarchy: semantics, coverage and balancing. In Proceedings of the Workshop on Multilingual Linguistic
Ressources, pages 101?108. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based
SMT Adaptation. In Proceedings of IWSLT.
Dhouha Bouamor, Nasredine Semmar, and Pierre Zweigenbaum. 2012. Identifying bilingual multi-word expres-
sions for statistical machine translation. In Proceedings of the Eight International Conference on Language Re-
sources and Evaluation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instability . In Proceedings of the Association for Computational
Lingustics.
Maike Erdmann, Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio. 2009. Improving the extraction of bilin-
gual terminology from wikipedia. ACM Trans. Multimedia Comput. Commun. Appl., 5(4):31:1?31:17, Novem-
ber.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages 1618?1621. ISCA.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo Strapparava. 2009. Kernel methods for minimally
supervised wsd. Computational Linguistics, 35(4):513?528.
Barry Haddow and Philipp Koehn. 2012. Analysing the Effect of Out-of-Domain Data on SMT Systems. In
Proceedings of the Seventh Workshop on Statistical Machine Translation, Montre?al, Canada. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In
Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-
burg, PA, USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual
Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ?07, pages 177?180, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages 79?86. AAMT.
J. Richard Landis and Gary G. Koch. 1977. Measurement of Observer Agreement for Categorical Data. In
Biometrics, volume 33, pages 159?174.
Philippe Langlais. 2002. Improving a general-purpose statistical translation engine by terminological lexicons. In
Proceedings of the 2nd International Workshop on Computational Terminology (COMPUTERM) ?2002, Taipei,
Taiwan, pages 1?7.
Samuel La?ubli, Mark Fishel, Martin Volk, and Manuela Weibel. 2013. Combining statistical machine translation
and translation memories with domain adaptation. In Stephan Oepen, Kristin Hagen, and Janne Bondi Johan-
nesse, editors, Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013),
May 22?24, 2013, Oslo University, Norway, Linko?ping Electronic Conference Proceedings, pages 331?341,
Oslo, May. Linko?pings universitet Electronic Press.
30
Pablo N Mendes, Max Jakob, Andre?s Garc??a-Silva, and Christian Bizer. 2011. Dbpedia spotlight: shedding light
on the web of documents. In Proceedings of the 7th International Conference on Semantic Systems, pages 1?8.
ACM.
Rada Mihalcea. 2007. Using Wikipedia for Automatic Word Sense Disambiguation. In Proceedings of NAACL-
HLT, pages 196?203.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia to Translate Domain-specific Terms in SMT. In nterna-
tional Workshop on Spoken Language Translation, San Francisco, CA, USA.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29.
Tsuyoshi Okita and Andy Way. 2010. Statistical Machine Translation with Terminology. In Proceedings of the
First Symposium on Patent Information Processing (SPIP), Tokyo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 311?318.
Ma?rcis Pinnis, Nikola Ljubes?ic?, Dan S?tefa?nescu, Inguna Skadin?a, Marko Tadic?, and Tatiana Gornostay. 2012.
Term extraction, tagging, and mapping tools for under-resourced languages. In Proceedings of the Terminology
and Knowledge Engineering (TKE2012) Conference.
Zhixiang Ren, Yajuan Lu?, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation
using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions:
Identification, Interpretation, Disambiguation and Applications, MWE ?09, pages 47?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Raivis Skadins?, Marcis Pinnis, Tatiana Gornostay, and Andrejs Vasiljevs. 2013. Application of online terminology
services in statistical machine translation. In Proceedings of the XIV Machine Translation Summit, Nice, France.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel Varga.
2006. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th
International Conference on Language Resources and Evaluation (LREC?2006).
Jo?rg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mariani, Jan Odijk, and
Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Francis M. Tyers and Jacques A. Pieanaar. 2008. Extracting bilingual word pairs from wikipedia. In Collabo-
ration: interoperability between people in the creation of language resources for less-resourced languages (A
SALTMIL workshop).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08, pages 993?1000.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for nlp applications. In
Proceedings of the TextGraphs-2 Workshop (NAACL-HLT), pages 1?8, Rochester, April. Association for Com-
putational Linguistics.
31
