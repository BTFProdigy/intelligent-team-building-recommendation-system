Proceedings of NAACL HLT 2009: Short Papers, pages 265?268,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Anchored Speech Recognition for Question Answering
Sibel Yaman1, Gokhan Tur2, Dimitra Vergyri2, Dilek Hakkani-Tur1,
Mary Harper3 and Wen Wang2
1 International Computer Science Institute
2 SRI International
3 Hopkins HLT Center of Excellence, University of Maryland
{sibel,dilek}@icsi.berkeley.edu,{gokhan,dverg,wwang}@speech.sri.com,mharper@casl.umd.edu
Abstract
In this paper, we propose a novel question
answering system that searches for responses
from spoken documents such as broadcast
news stories and conversations. We propose a
novel two-step approach, which we refer to as
anchored speech recognition, to improve the
speech recognition of the sentence that sup-
ports the answer. In the first step, the sen-
tence that is highly likely to contain the an-
swer is retrieved among the spoken data that
has been transcribed using a generic automatic
speech recognition (ASR) system. This candi-
date sentence is then re-recognized in the sec-
ond step by constraining the ASR search space
using the lexical information in the question.
Our analysis showed that ASR errors caused
a 35% degradation in the performance of the
question answering system. Experiments with
the proposed anchored recognition approach
indicated a significant improvement in the per-
formance of the question answering module,
recovering 30% of the answers erroneous due
to ASR.
1 Introduction
In this paper, we focus on finding answers to user
questions from spoken documents, such as broad-
cast news stories and conversations. In a typical
question answering system, the user query is first
processed by an information retrieval (IR) system,
which finds out the most relevant documents among
massive document collections. Each sentence in
these relevant documents is processed to determine
whether or not it answers user questions. Once a
candidate sentence is determined, it is further pro-
cessed to extract the exact answer.
Answering factoid questions (i.e., questions like
?What is the capital of France??) using web makes
use of the redundancy of information (Whittaker et
al., 2006). However, when the document collection
is not large and when the queries are complex, as
in the task we focus on in this paper, more sophis-
ticated syntactic, semantic, and contextual process-
ing of documents and queries is performed to ex-
tract or construct the answer. Although much of the
work on question answering has been focused on
written texts, many emerging systems also enable
either spoken queries or spoken document collec-
tions (Lamel et al, 2008). The work we describe
in this paper also uses spoken data collections to
answer user questions but our focus is on improv-
ing speech recognition quality of the documents by
making use of the wording in the queries. Consider
the following example:
Manual transcription: We understand from Greek of-
ficials here that it was a Russian-made rocket which is
available in many countries but certainly not a weapon
used by the Greek military
ASR transcription: to stand firm greek officials here that
he was a a russian made rocket uh which is available in
many countries but certainly not a weapon used by he
great moments
Question: What is certainly not a weapon used by the
Greek military?
Answer: a Russian-made rocket
Answering such questions requires as good ASR
transcriptions as possible. In many cases, though,
there is one generic ASR system and a generic lan-
guage model to use. The approach proposed in this
paper attempts to improve the ASR performance by
re-recognizing the candidate sentence using lexical
information from the given question. The motiva-
265
tion is that the question and the candidate sentence
should share some common words, and therefore
the words of the answer sentence can be estimated
from the given question. For example, given a fac-
toid question such as: ?What is the tallest build-
ing in the world??, the sentence containing its an-
swer is highly likely to include word sequences such
as: ?The tallest building in the world is NAME?
or ?NAME, the highest building in the world, ...?,
where NAME is the exact answer.
Once the sentence supporting the answer is lo-
cated, it is re-recognized such that the candidate an-
swer is constrained to include parts of the question
word sequence. To achieve this, a word network is
formed to match the answer sentence to the given
question. Since the question words are taken as a ba-
sis to re-recognize the best-candidate sentence, the
question acts as an anchor, and therefore, we call
this approach anchored recognition.
In this work, we restrict out attention to questions
about the subject, the object and the locative, tempo-
ral, and causative arguments. For instance, the fol-
lowings are the questions of interest for the sentence
Obama invited Clinton to the White House to discuss
the recent developments:
Who invited Clinton to the White House?
Who did Obama invite to the White House?
Why did Obama invite Clinton to the White House?
2 Sentence Extraction
The goal in sentence extraction is determining the
sentence that is most likely to contain the answer
to the given question. Our sentence extractor relies
on non-stop word n-gram match between the ques-
tion and the candidate sentence, and returns the sen-
tence with the largest weighted average. Since not
all word n-grams have the same importance (e.g.
function vs. content words), we perform a weighted
sum as typically done in the IR literature, i.e., the
matching n-grams are weighted with respect to their
inverse document frequency (IDF) and length.
A major concern for accurate sentence extraction
is the robustness to speech recognition errors. An-
other concern is dealing with alternative word se-
quences for expressing the same meaning. To tackle
the second challenge, one can also include syn-
onyms, and compare paraphrases of the question and
the candidate answer. Since our main focus is on ro-
Predicate
Sentence ExtractionSemantic Roles
Answering Sentence
Answer Extraction Anchored Recognition
Document
Speech Recognition
Question
Searched Argument
Baseline
Proposed
Figure 1: Conceptual scheme of the baseline and pro-
posed information distillation system.
bustness to speech recognition errors, our data set
is limited to those questions that are worded very
similarly to the candidate answers. However, the
approach is more general, and can be extended to
tackle both challenges.
3 Answer Extraction
When the answer is to be extracted from ASR out-
put, the exact answers can be erroneous because (1)
the exact answer phrase might be misrecognized, (2)
other parts of the sentence might be misrecognized,
so the exact answer cannot be extracted either be-
cause parser fails or because the sentence cannot
match the query.
The question in the example in the Introduction
section is concerned with the object of the predicate
?is? rather than of the other predicates ?understand?
or ?was?. Therefore, a pre-processing step is needed
to correctly identify the object (in this example) that
is being asked, which is described next.
Once the best candidate sentence is estimated, a
syntactic parser (Harper and Huang, ) that also out-
puts function tags is used to parse both the ques-
tion and candidate answering sentence. The parser
is trained on Fisher, Switchboard, and speechified
Broadcast News, Brown, and Wall Street Journal
treebanks without punctuation and case to match in-
put the evaluation conditions.
An example of such a syntactic parse is given in
Figure 2. As shown there, the ?SBJ? marks the sur-
face subject of a given predicate, and the ?TMP? tag
marks the temporal argument. There are also the
?DIR? and ?LOC? tags indicating the locative ar-
gument and the ?PRP? tag indicating the causal ar-
gument. Such parses not only provide a mechanism
to extract information relating to the subject of the
predicate of interest, but also to extract the part of
266
Figure 2: The function tags assist in finding the subject,
object, and arguments of a given predicate.
the sentence that the question is about, in this ex-
ample ?a Russian-made rocket [which] is certainly
not a weapon used by the Greek military?. The ex-
traction of the relevant part is achieved by matching
the predicate of the question to the predicates of the
subsentences in the best candidate sentence. Once
such syntactic parses are obtained for the part of the
best-candidate sentence that matches the question, a
set of rules are used to extract the argument that can
answer the question.
4 Anchored Speech Recognition
In this study we employed a state-of-the-art broad-
cast news and conversations speech recognition
system (Stolcke et al, 2006). The recognizer
performs a total of seven decoding passes with
alternating acoustic front-ends: one based on
Mel frequency cepstral coefficients (MFCCs) aug-
mented with discriminatively estimated multilayer-
perceptron (MLP) features, and one based on per-
ceptual linear prediction (PLP) features. Acoustic
models are cross-adapted during recognition to out-
put from previous recognition stages, and the output
of the three final decoding steps are combined via
confusion networks.
Given a question whose answer we expect to find
in a given sentence, we construct a re-decoding net-
work to match that question. We call this process an-
chored speech recognition, where the anchor is the
question text. Note that this is different than forced
alignment, which enforces the recognition of an au-
dio stream to align with some given sentence. It is
used for detecting the start times of individual words
or for language learning applications to exploit the
acoustic model scores, since there is no need for a
language model.
Our approach is also different than the so-called
flexible alignment (Finke and Waibel, 1997), which
is basically forced alignment that allows skipping
any part of the given sentence, replacing it with a re-
ject token, or inserting hesitations in between words.
In our task, we require all the words in the ques-
tion to be in the best-candidate sentence without any
skips or insertions. If we allow flexible alignment,
then any part of the question could be deleted. In the
proposed anchored speech recognition scheme, we
allow only pauses and rejects between words, but do
not allow any deletions or skips.
The algorithm for extracting anchored recognition
hypotheses is as follows: (i) Construct new recogni-
tion and rescoring language models (LMs) by inter-
polating the baseline LMs with those trained from
only the question sentences and use the new LM
to generate lattices - this aims to bias the recogni-
tion towards word phrases that are included in the
questions. (ii) Construct for each question an ?an-
chored? word network that matches the word se-
quence of the question, allowing any other word se-
quence around it. For example if the question is
WHAT did Bruce Gordon say?, we construct a word
network to match Bruce Gordon said ANYTHING
where ?ANYTHING? is a filler that allows any word
(a word loop). (iii) Intersect the recognition lat-
tices from step (i) with the anchored network for
each question in (ii), thus extracting from the lattice
only the paths that match as answers to the ques-
tion. Then rescore that new lattice with higher order
LM and cross-word adapted acoustic models to get
the best path. (iv) If the intersection part in (iii) fails
then we use a more constrained recognition network:
Starting with the anchored network in (ii) we first
limit the vocabulary in the ANYTHING word-loop
sub-network to only the words that were included in
the recognition lattice from step (i). Then we com-
pose this network with the bigram LM (from step (i))
to add bigram probabilities to the network. Vocab-
ulary limitation is done for efficiency reasons. We
also allow optional filler words and pauses to this
network to allow for hesitations, non-speech events
and pauses within the utterance we are trying to
match. This may limit somewhat the potential im-
provement from this approach and we are working
267
Question Type ASR Output Manual Trans.
Subject 85% 98%
Object 75% 90%
Locative Arg. 81% 93%
Temporal Arg. 94% 98%
Reason 86% 100%
Total 83% 95%
Table 1: Performance figures for the sentence extraction
system using automatic and manual transcriptions.
Question ASR Manual Anchored
Type Output Trans. Output
Subject 51% 77% 61%
Object 41% 73% 51%
Locative Arg. 18% 22% 22%
Temporal Arg. 55% 73% 63%
Reason 26% 47% 26%
Total 44% 68% 52%
Table 2: Performance figures for the answer extraction
system using automatic and manual transcriptions com-
pared with anchored recognition outputs.
towards enhancing the vocabulary with more candi-
date words that could contain the spoken words in
the region. (v) Then we perform recognition with
the new anchored network and extract the best path
through it. Thus we enforce partial alignment of
the audio with the question given, while the regu-
lar recognition LM is still used for the parts outside
the question.
5 Experiments and Results
We performed experiments using a set of questions
and broadcast audio documents released by LDC for
the DARPA-funded GALE project Phase 3. In this
dataset we have 482 questions (177 subject, 160 ob-
ject, 73 temporal argument, 49 locative argument,
and 23 reason) from 90 documents. The ASR word
error rate (WER) for the sentences from which the
questions are constructed is 37% with respect to
noisy closed captions. To factor out IR noise we as-
sumed that the target document is given.
Table 1 presents the performance of the sentence
extraction system using manual and automatic tran-
scriptions. As seen, the system is almost perfect
when there is no noise, however performance de-
grades about 12% with the ASR output.
The next set of experiments demonstrate the per-
formance of the answer extraction system when the
correct sentence is given using both automatic and
manual transcriptions. As seen from Table 2, the
answer extraction performance degrades by about
35% relative using the ASR output. However, using
the anchored recognition approach, this improves to
23%, reducing the effect of the ASR noise signifi-
cantly1 by more than 30% relative. This is shown
in the last column of this table, demonstrating the
use of the proposed approach. We observe that the
WER of the sentences for which we now get cor-
rected answers is reduced from 45% to 28% with
this approach, a reduction of 37% relative.
6 Conclusions
We have presented a question answering system
for querying spoken documents with a novel an-
chored speech recognition approach, which aims to
re-decode an utterance given the question. The pro-
posed approach significantly lowers the error rate for
answer extraction. Our future work involves han-
dling audio in foreign languages, that is robust to
both ASR and machine translation noise.
Acknowledgments: This work was funded by DARPA un-
der contract No. HR0011-06-C-0023. Any conclusions or rec-
ommendations are those of the authors and do not necessarily
reflect the views of DARPA.
References
M. Finke and A. Waibel. 1997. Flexible transcription
alignment. In Proceedings of the IEEE ASRU Work-
shop, Santa Barbara, CA.
M. Harper and Z. Huang. Chinese Statistical Parsing,
chapter To appear.
L. Lamel, S. Rosset, C. Ayache, D. Mostefa, J. Turmo,
and P. Comas. 2008. Question answering on speech
transcriptions: the qast evaluation in clef. In Proceed-
ings of the LREC, Marrakech, Morocco.
A. Stolcke, B. Chen, H. Franco, V. R. R. Gadde,
M. Graciarena, M.-Y. Hwang, K. Kirchhoff, N. Mor-
gan, X. Lin, T. Ng, M. Ostendorf, K. Sonmez,
A. Venkataraman, D. Vergyri, W. Wang, J. Zheng,
and Q. Zhu. 2006. Recent innovations in speech-
to-text transcription at SRI-ICSI-UW. IEEE Trans-
actions on Audio, Speech, and Language Processing,
14(5):1729?1744, September.
E. W. D. Whittaker, J. Mrozinski, and S. Furui. 2006.
Factoid question answering with web, mobile and
speech interfaces. In Proceedings of the NAACL/HLT,
Morristown, NJ.
1according to the Z-test with 0.95 confidence interval
268
The Effectiveness of Corpus-Induced Dependency Grammars for 
Post-processing Speech* 
M.  P. Harper ,  C.  M .  Whi te ,  W.  Wang,  M.  T .  Johnson ,  and  R .  A.  He lzerman 
School of Electrical and Computer  Engineer ing 
Purdue University 
West Lafayette,  IN 47907-1285 
{ harper, robot, wang28, m johnson, helz} @ecn.purdue. du 
Abst rac t  
This paper investigates the impact of Constraint 
Dependency Grammars (CDG) on the accuracy of 
an integrated speech recognition and CDG pars- 
ing system. We compare a conventional CDG with 
CDGs that are induced from annotated sentences 
and template-expanded s ntences. The grammars 
are evaluated on parsing speed, precision/coverage, 
and improvement of word and sentence accuracy of 
the integrated system. Sentence-derived CDGs sig- 
nificantly improve recognition accuracy over the con- 
ventional CDG but are less general. Expanding the 
sentences with templates provides us with a mech- 
anism for increasing the coverage of the grammar 
with only minor reductions in recognition accuracy. 
1 Background 
The question of when and how to integrate language 
models with speech recognition systems i gaining in 
importance as recognition tasks investigated by the 
speech community become increasingly more chal- 
lenging and as speech recognizers are used in hu- 
man/computer interfaces and dialog systems (Block, 
1997; Pieraccini and Levin, 1992; Schmid, 1994; 
Wright et al, 1994; Zue et al, 1991). Many sys- 
tems tightly integrate N-gram stochastic language 
models, with a power limited to a regular grammar, 
into the recognizer (Jeanrenaud et al, 1995; Ney et 
al., 1994; Placeway et al, 1993) to build more ac- 
curate speech recognizers. However, in order to act 
based on the spoken interaction with the user, the 
speech signal must be mapped to an internal repre- 
sentation. Obtaining a syntactic representation for 
the spoken utterance has a high degree of utility for 
mapping to a semantic representation. Without a 
structural analysis of an input, it is difficult o guar- 
antee the correctness ofthe mapping from a sentence 
to its interpretation (e.g., mathematical expressions 
to internal calculations). We believe that significant 
additional improvement in accuracy can be gained 
in specific domains by using a more complex lan- 
* This research was supported by grants from Intel, Purdue 
Research Foundation, and National Science Foundation IRI 
97-04358, CDA 96-17388, and ~9980054-BCS. 
guage model that combines syntactic, semantic, and 
domain knowledge. 
A language processing module that is more pow- 
erful than a regular grammar can be loosely, mod- 
erately, or tightly integrated with the spoken lan- 
guage system, and there are advantages and dis- 
advantages associated with each choice (Harper et 
al., 1994). To tightly integrate a language model 
with the power of a context-free grammar with the 
acoustic module requires that the power of the two 
modules be matched, making the integrated system 
fairly intractable and difficult to train. By separat- 
ing the language model from the acoustic model, it 
becomes possible to use a more powerful anguage 
model without increasing computational costs or the 
amount of acoustic training data required by the rec- 
ognizer. Furthermore, a loosely-integrated language 
model can be developed independently of the speech 
recognition component, which is clearly an advan- 
tage. Decoupling the acoustic and language mod- 
els also adds flexibility: a wide variety of language 
models can be tried with a single acoustic model. 
Systems that utilize a language model that operates 
as a post-processor to a speech recognizer include 
(Block, 1997; Seneff, 1992; Zue et al, 1991). 
The goal of this research is to construct and ex- 
perimentally evaluate a prototype of a spoken lan- 
guage system that loosely integrates a speech recog- 
nition component with an NLP component that uses 
syntactic, semantic, and domain-specific knowledge 
to more accurately select the sentence uttered by a 
speaker. First we describe the system we have built. 
Then we describe the mechanism used to rapidly de- 
velop a domain-specific grammar that improves ac- 
curacy of our speech recognizer. 
2 Our  Sys tem 
We have developed the prototype spoken language 
system depicted in Figure 1 that integrates a speech 
recognition component based on HMMs with a pow- 
erful grammar model based on Constraint Depen- 
dency Grammar (CDG). The speech recognizer is 
implemented as a multiple-mixture triphone HMM 
with a simple integrated word co-occurrence gram- 
102 
mar (Ent, 1997; Young et al, 1997). Mel-scale cep- 
stral coefficients, energy, and each of their their first 
and second order differences are used as the under- 
lying feature vector for each speech frame. Model 
training is done using standard Baum-Welch Max- 
imum Likelihood parameter re-estimation on diag- 
onal covariance Gaussian Mixture Model (GMM) 
feature distributions. The speech recognizer em- 
ploys a token-passing version of the Viterbi algo- 
rithm (Young et al, 1989) and pruning settings to 
produce a pruned recognition lattice. This pruned 
lattice contains the most likely alternative sentences 
that account for the sounds present in an utterance 
as well as their probabilities. Without any loss of in- 
formation, this lattice is then compressed into a word 
graph (Harper et al, 1999b; Johnson and Harper, 
1999), which acts as the interface between the rec- 
ognizer and the CDG parser. The word graph algo- 
rithm begins with the recognition lattice and elim- 
inates identical subgraphs by iteratively combining 
word nodes that have exactly the same preceding 
or following nodes (as well as edge probabilities), 
pushing excess probability to adjacent nodes when- 
ever possible. The resulting word graph represents 
all possible word-level paths without eliminating or 
adding any paths or modifying their probabilities. 
Word graphs increase the bandwidth of useful acous- 
tic information passed from the HMM to the CDG 
parser compared to most current speech recognition 
systems. 
The CDG parser parses the word graph to identify 
the best sentence consistent with both the acoustics 
of the utterance and its own additional knowledge. 
The loose coupling of the parser with the HMM 
allows us to construct a more powerful combined 
system without increasing the amount of training 
data for the HMM or the computational complex- 
ity of either of the component modules. Our NLP 
component is implemented using a CDG parser 
(Harper and Helzerman, 1995; Maruyama, 1990a; 
Maruyama, 1990b) because of its power and flexibil- 
ity, in particular: 
? It supports the use of syntactic, semantic, and 
domain-specific knowledge in a uniform frame- 
work. 
? Our CDG parser supports efficient simultaneous 
parsing of alternative sentence hypotheses in a 
word graph (Harper and Helzerman, 1995; Helz- 
erman and Harper, 1996). 
* Because CDG is a dependency grammar, it can 
better model free-order languages. Hence, CDG 
can be used in processing a wider variety of human 
languages than other grammar paradigms. 
? It is capable of representing and using context- 
dependent information unlike traditional gram- 
mar approaches, thus providing a finer degree of 
control over the syntactic analysis of a sentence. 
? A CDG can be extracted irectly from sentences 
annotated with dependency information (i.e., fea- 
ture and syntactic relationships). 
We hypothesize that the accuracy of the combined 
HMM/CDG system should benefit from the ability 
to create a grammar that covers the domain as pre- 
cisely as possible and that does not consider sen- 
tences that would not make sense given the domain. 
A corpus-based grammar is likely to have this degree 
of control. In the next section we describe how we 
construct a CDG from corpora. 
Figure 1: Block diagram of the loosely-coupled spo- 
ken language system. 
3 Learn ing  CDG Ru les  
In this section, we introduce CDG and then describe 
how CDG constraints can be learned from sentences 
annotated with grammatical information. 
3.1 In t roduct ion  to  CDG 
Constraint Dependency Grammar (CDG), first 
introduced by Maruyama (Maruyama, 1990a; 
Maruyama, 1990b), uses constraints to determine 
the grammatical dependencies for a sentence. The 
parsing algorithm is framed as a constraint satis- 
faction problem: the rules are the constraints and 
the solutions are the parses. A CDG is defined as 
a five-tuple, (2E, R, L, C, T), where ~ = {a l , . . . ,  c%} 
is a finite set of lexical categories (e.g., determiner), 
R = {r l , . . . , rp}  is a finite set of uniquely named 
roles or role ids (e.g., governor, needl, need2), L = 
{ l l , . . . , lq}  is a finite set of labels (e.g., subject), 
C is a constraint formula, and T is a table that 
specifies allowable category-role-label combinations. 
A sentence s - WlW2W3. . .wn has length n and 
is an element of ~*. For each word wi E ~ of a 
sentence s, there are up to p different roles (with 
most words needing only one or two (Harper et al, 
1999a)), yielding a maximum of n * p roles for the 
entire sentence. A role is a variable that is assigned 
a role value, an element of the set L ? (1, 2 , . . . ,  n}. 
Role values are denoted as l-m, where l E L and 
m E (1, 2 , . . . ,  n} is called the modifiee. Maruyama 
originally used a modifiee of NIL to indicate that a 
role value does not require a modifiee, but it is more 
parsimonious to indicate that there is no dependent 
by setting the modifiee to the position of its word. 
Role values are assigned to roles to record the syn- 
tactic dependencies between words in the sentence. 
103 
The governor ole is assigned role values such that 
the modifiee of the word indicates the position of the 
word's governor or head (e.g., DET-3, when assigned 
to the governor ole of a determiner, indicates its 
function and the position of its head). Every word 
in a sentence has a governor ole. Need roles are 
used to ensure the requirements of a word are met. 
For example, an object is required by a verb that 
subcategorizes for one, unless it has passive voice. 
The required object is accounted for by requiring 
the verb's need role to be assigned a role value with 
a modifiee that points at the object. Words can 
have more than one need role, depending on the lex- 
ical category of the word. The table T indicates the 
roles that a word with a particular lexical category 
must support. 
A sentence s is said to be generated  by the gram- 
mar G if there exists an assignment A that maps a 
role value to each of the roles for s such that C is 
satisfied. There may be more than one assignment 
of role values to the roles of a sentence that satisfies 
C, in which case there is ambiguity. C is a first- 
order predicate calculus formula over all roles that 
requires that an assignment of role values to roles be 
consistent with the formula; those role values incon- 
sistent with C can be eliminated. A subformula P~ 
of C is a predicate involving =, <, or >, or predi- 
cates joined by the logical connectives and, or, i f ,  
or not. A subformula is a unary constraint if it con- 
tains only a single variable (by convention, we use 
zl) and a binary constraint if it contains two vari- 
ables (by convention zl and z2). An example of 
a unary and binary constraint appears in Figure 2. 
A CDG has an arity parameter a, which indicates 
the maximum number of variables in the subformu- 
las of C, and a degree parameter d, which is the 
number of roles in the grammar. An arity of two 
suffices to represent a grammar at least as power- 
ful as a context-free grammar (Maruyama, 1990a; 
Maruyama, 1990b). In (Harper et al, 1999a), we 
developed a way to write constraints concerning the 
category and feature values of a modifiee of a role 
value (or role value pair). These constraints loosely 
capture binary constraint information in unary con- 
straints (or beyond binary for binary constraints) 
and results in more efficient parsing. 
A u, liwy ?~nst\]llnt requiring that ? role vmluo IlmlgnlKI to the vernot role of I determiner 
have the label D~ lind ? modlflee pointing to ? lub4NIqtNl~t wocd* 
(if (and (= (category x 1) determiner) 
(= (rid x 1 ) G)) 
(and (= (label x 1 ) DET) 
(> (rood x 1 ) (pos Xl))) ) 
A binary oonatrllnt requiring that ? role vllue 
with the libel S Illlgned to ? ne~dl role of one 
word pOklt It Imother word whole governor 
role I= mml~gnened ? role veltm with the libel 8UBJ 
and ? rnodlflee that point? beck at the flrat word. 
(if (and (= (label x I ) S) 
(= (rid Xl) N1) 
(= (rnod Xl) (pos x2)) 
(= (rid x2) G)) 
(and (= (label x2) SUBJ) 
(= (rood x2) (pos xl)))) 
Figure 2: A Unary and binary constraint for CDG. 
The white box in Figure 3 depicts a parse for 
the sentence Clear the screen from the Resource 
Management corpus (Price et al, 1988) (the ARV 
and ARVP in the gray box will be discussed later), 
which is a corpus we will use to evaluate our speech 
processing system. We have constructed a conven- 
tional CDG with around 1,500 unary and binary 
constraints (i.e., its arity is 2) that were designed 
to parse the sentences in the corpus. This CDG 
covers a wide variety of grammar constructs (includ- 
ing conjunctions and wh-movement) and has a fairly 
rich semantics. It uses 16 lexical categories, 4 roles 
(so its degree is 4), 24 labels, and 13 lexical fea- 
ture types (subcat,  agr, case, vtype (e.g., progres- 
sive), mood, gap, inverted, voice, behavior (e.g., 
mass), type (e.g., interrogative, relative), semtype, 
takesdet ,  and conj type) .  The parse in Figure 3 is 
an assignment of role values to roles that is consis- 
tent with the unary and binary constraints. A role 
value, when assigned to a role, has access to not only 
the label and modifiee of its role value, but also the 
role name of the role to which it is assigned, informa- 
tion specific to the word (i.e., the word's position in 
the sentence, its lexical category, and feature values 
for each feature), and information about the lexical 
class and feature values of its modifiee. Our unary 
and binary constraints use this information to elim- 
inate ungrammatical ssignments. 
Parse for "Clear the screen" 
I 1 th2e-~ 3 Clear ~ n  
?a~=comlnon 
t vtype=lnf =ulxa~t3= I behav=count 
Nmt/~ram 
-G=root-1 G=de~3 
N2=S~3 N3=S-1 
{'~t l=det erm~ner, type1 ==definite, subcat l=count3s, ddl=G ?bell=de (< pOSXl) mod Xl) ) 
J"~tl---determiner, typel=definite, subcat 1--cour~3s, ddl=G, 
~ ~,bell=det, (< (pos Xl) (rood Xl)), Cat2=noun, c~se2=common, I 1 
t b e.hav2=count, type2=none, semty~2=display, gl2=3s, J\] 
\] rid2=G, label2==obi, (< (rood x2) (POs x2)), 
(rood x2) (pos Xl)), (= (rood xl) (pos x2)) J = 
Figure 3: A CDG parse (see white box) is repre- 
sented by the assignment of role values to roles as- 
sociated with a word with a specific lexical category 
and one feature value per feature. ARVs and ARVPs 
(see gray box) represent grammatical relations that 
can be extracted from a sentence's parse. 
3.2 Learn ing  CDG Const ra in ts  
The grammaticality of a sentence in a language de- 
fined by a CDG was originally determined by apply- 
ing the constraints of the grammar to the possible 
104 
role value assignments. If the set of all possible role 
values assigned to the roles of a sentence of length n 
is denotedS1 =Y;.x RxPOSxLxMODx Ftx  
.. .  x Fk, where k is the number of feature types, 
Fi represents the set of feature values for that type, 
POS = {1, 2 , . . . ,  n} is the set of possible positions, 
MOD = {1, 2 , . . . ,  n} is the set of possible modi- 
flees, and n is sentence length (which can be any 
arbitrary natural number), then unary constraints 
partition $1 into grammatical and ungrammatical 
role values. Similarly, binary constraints partition 
the set $2 = $1 x $1 = S~ into compatible and in- 
compatible pairs. Building upon this concept of role 
value partitioning, it is possible to construct another 
way of representing unary and binary constraints 
because CDG constraints do not need to reference 
the exact position of a word or a modifiee in the 
sentence to parse sentences (Harper and Helzerman, 
1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 
1994; Menzel, 1995). 
To represent he relative, rather than the abso- 
lute, position information for the role values in a 
grammatical sentence, it is only necessary to repre- 
sent the positional relations between the modifiees 
and the positions of the role values. To support an 
arity of 2, these relations involve either equality or 
less-than relations over the modifiees and positions 
of role values assigned to the roles zl and x2. Since 
unary constraints operate over role values assigned 
to a single role, the only relative position relations 
that can be tested are between the role value's posi- 
tion (denoted as Pzl)  and its modifiee (denoted as 
Mzl); one and only one of the following three re- 
lations must be true: (P~:I < Mzl), (Mzl < Pzl) ,  
or (Pzl = Mzl). Since binary constraints operate 
over role values assigned to pairs of roles, zl and z2, 
the only possible relative position relations that can 
be tested are between Pzl  and Mxt, P:e2 and Mx2, 
Pz l  and Mz~, Pz2 and Mxt, Pzt  and Px2, Mxl and 
Mz2. Note that each of the six has three positional 
relations (as in the case of unary relations on Pzl  
and Mzt) such that one and only one of them is 
simultaneously true. 
The unary and binary positional relations provide 
the necessary mechanism to develop an alternative 
view of the unary and binary constraints. First, we 
develop the concept of an abstract role value (ARV), 
which is a finite characterization f all possible role 
values using relative, rather than absolute, position 
relations. Formally, an ARV for a particular gram- 
mar G = (~,, R, L, C, T, F t , . . . ,  Fk) is an element of 
the set: .dl = ExR? L xFt  ?. . .xFkxUC,  where UC 
encodes the three possible positional relations be- 
tween Pxl and Mxl. The gray box of Figure 3 shows 
an example of an ARV obtained from the parsed sen- 
tence. Note that .At is a finite set representing the 
space of all possible ARVs for the grammar1; hence, 
the set provides an alternative characterization of
the unary constraints for the grammar, which can 
be partitioned into positive (grammatical) and neg- 
ative (ungrammatical) ARVs. During parsing, if a 
role value does not match one of the elements in the 
positive ARV space, then it should be disallowed. 
Positive ARVs can be obtained directly from the 
parses of sentences: for each role value in a parse for 
a sentence, simply extract its category, feature, role, 
and label information, and then determine the po- 
sitional relation that holds between the role value's 
position and modifiee. 
Similarly the set of legal abstract role value pairs 
(ARVPs), A2 = \ ]ExRxLxFtx . . .xFkx~xRxLx  
F1 x . . .  x Fk x BC, where BC encodes the positional 
relations among Pxl,  Mxt, Px2, and Mx2, provides 
an alternative definition for the binary constraints 2. 
The gray box of Figure 3 shows an example of an 
ARVP obtained from the parsed sentence. Positive 
ARVPs can be obtained directly from the parses of 
sentences. For each pair of role values assigned to 
different roles, simply extract heir category, feature, 
role, and label information, and then determine the 
positional relations that hold between the positions 
and modifiees. 
An enumeration of the positive ARV/ARVPs can 
be used to represent he CDG constraints, C, and 
ARV/ARVPs are PAC-learnable from positive ex- 
amples, as can be shown using the techniques of 
(Natarajan, 1989; Valiant, 1984). ARV/ARVP con- 
straints can be enforced by using a fast table lookup 
method to see if a role value (or role value pair) is 
allowed (rather than propagating thousands of con- 
straints), thus speeding up the parser. 
4 Eva luat ion  Us ing  the  Nava l  
Resource  Management  Domain  
An experiment was conducted to determine the 
plausibility and the benefits of extracting CDG con- 
straints from a domain-specific corpus of sentences. 
For our speech application, the ideal CDG should be 
general enough to cover sentences imilar to those 
that appear in the corpus while being restrictive 
enough to eliminate sentences that are implausible 
given the observed sentences. Hence, we investigate 
whether a grammar extracted from annotated sen- 
tences in a corpus achieves this precision of cover- 
age. We also examine whether a learned grammar 
has the ability to filter out incorrect sentence hy- 
potheses produced by the HMM component of our 
system in Figure 1. To investigate these issues, we 
have performed an experiment using the standard 
1,fit 1 can also include information about  the possible lexical 
categories and feature values of the modifiee of Xl. 
2.A2 can also include information about  the possible lexical 
categories and feature values of the modifiees of Xl and x2. 
105 
Resource Management (RM) (Price et al, 1988) and 
Extended Resource Management (RM2) ((DARPA), 
1990) corpora. These mid-size speech corpora have 
a vocabulary of 991 words and contain utterances of 
sentences derived from sentence templates based on 
interviews with naval personnel familiar with naval 
resource management tasks. They were chosen for 
several reasons: they are two existing speech corpora 
from the same domain; their manageable sizes make 
them a good platform for the development of tech- 
niques that require extensive xperimentation; and 
the sentences have both syntactic variety and rea- 
sonably rich semantics. RM contains 5,190 separate 
utterances (3,990 testing, 1,200 training) of 2,845 
distinct sentences (2,245 training, 600 testing). We 
have extracted several types of CDGs from annota- 
tions of the RM sentences and tested their generality 
using the 7,396 sentences in RM2 (out of the 8,173) 
that are in the resource management domain but are 
distinct from the RM sentences. We compare these 
CDGs to each other and to the conventional CDG 
described previously. 
The corpus-based CDGs were created by extract- 
ing the allowable grammar elationships from the 
RM sentences that were annotated by language x- 
perts using the SENATOR annotation tool, a CGI 
(Common Gateway Interace) HTML script written 
in GNU C++ version 2.8.1 (White, 2000). We 
tested two major CDG variations: those derived di- 
rectly from the RM sentences (Sentence CDGs) and 
those derived from simple template-expanded RM
sentences (Template CDGs). For example, "List 
MIDPAC's deployments during (date)" is a sentence 
containing a date template which allows any date 
representations. For these experiments, we focused 
on templates for dates, years, times, numbers, and 
latitude and longitude coordinates. Each template 
name identifies a sub-grammar which was produced 
by annotating the appropriate strings. We then an- 
notated sentences containing the template names as 
if they were regular sentences. Approximately 25% 
of the 2,845 RM sentences were expanded with one 
or more templates. 
Although annotating a corpus of sentences can be 
a labor intensive task, we used an iterative approach 
that is based on parsing using grammars with vary- 
ing degrees of restrictiveness. A grammar can be 
made less restrictive by ignoring: 
* lexical information associated with a role value's 
modifiee in the ARVPs, 
o feature information of two role values in an ARVP 
not directly related based on their modifiee rela- 
tions, 
. syntactic information provided by two role values 
that are not directly related, 
? specific feature information (e.g., semantics or 
subcategorization). 
Initially, we bootstrapped the grammar by annotat- 
ing a 200 sentence subset of the RM corpus and ex- 
tracting a fairly general grammar from the annota- 
tions. Then using increasingly restrictive grammars 
at each iteration, we used the current grammar to 
identify sentences that required annotation and ver- 
ified the parse information for sentences that suc- 
ceeded. This iterative technique reduced the time 
required to build a CDG from about one year for the 
conventional CDG to around two months (White, 
2000). 
Several methods of extracting an ARV/ARVP 
grammar from sentences or template-extended sen- 
tences were investigated. The ARVPs are extracted 
differently for each method; whereas, the ARVs 
are extracted in the same manner egardless of the 
method. Recall that ARVs represent the set of ob- 
served role value assignments. In our implementa- 
tion, each ARV includes: the label of the role value, 
the role to which the role value was assigned, the 
lexical category and feature values of the word con- 
taining the role, the relative position of the word and 
the role value's modifiee, and the modifiee's lexical 
category and feature values (modifiee constraints). 
We use modifiee constraints for ARVs regardless of 
extraction method because their use does not change 
the coverage of the extracted grammar and not using 
the information would significantly slow the parser 
(Harper et al, 1999a). Because the ARVP space is 
larger than the ARV space, we investigate six varia- 
tions for extracting the pairs: 
1. Ful l  Mod:  contains all grammar and feature 
value information for all pairs of role values from 
annotated sentences, as well as modifiee con- 
straints. For a role value pair in a sentence to be 
considered valid during parsing with this gram- 
mar, it must match an ARVP extracted from the 
annotated sentences. 
2. Full: like Ful l  Mod except it does not impose 
modifiee constraints on a pair of role values during 
parsing. 
3. Feature  Mod:  contains all grammar elations 
between all pairs of role values, but it consid- 
ers feature and modifiee constraints only for pairs 
that are directly related by a modifiee link. Dur- 
ing parsing, if a role value pair is related by a 
modifiee link, then a corresponding ARVP with 
full feature and modifiee information must appear 
in the grammar for it to be allowed. If the pair 
is not directly related, then an ARVP must be 
stored for the grammar elations, ignoring feature 
and modifiee constraint information. 
4. Feature :  like Feature  Mod except it does not 
impose modifiee constraints on a pair of role val- 
ues during parsing. 
5. D i rect  Mod:  stores only the grammar, feature, 
and modifiee information for those pairs of role 
106 
Table 1: Number of ARVs and ARVPs extracted for 
each RM grammar. 
ARVP Sentence I Template Percent 
Variation CDG \[ CDG Increase 
Full Mod 270,034 408,912 
Full 165,480 200,792 
Feature Mod 
Feature 
Direct Mod 
Direct 
ARVs 
49,468 
36,558 
41,124 
28,214 
4,424 
56,758 
40,308 
47,004 
30,554 
4,648 
51.43% 
21.34% 
14.74% 
10.26% 
14.30% 
8.29% 
5.06% 
Table 2: Number of successfully parsed sentences in 
RM2 using the conventional CDG and CDGs derived 
from sentences only or template-expanded s ntences. 
ARVP ~: Parsed with ~ Parsed with 
Variation Sentence CDG Template CDG 
Full Mod 
Full 
Feature Mod 
Feature 
Direct Mod 
Direct 
Conventional 
3,735 (50.50%) 
4,509 (60.97%) 
5,365 (72.54%) 
5,772 (78.04%) 
5,464 (73.88%) 
5,931 (80.19%) 
7,144 (96.59%) 
4,461 (60.32%) 
5,316 (71.88%) 
5,927 (80.14%) 
6,208 (83.94%) 
5,979 (80.84%) 
6,275 (84.82%) 
not applicable 
values that are directly related by a modifiee link. 
During parsing, if a role value pair is related by 
such a link, then a corresponding ARVP must ap- 
pear in the grammar for it to be allowed. Any 
pair of role values not related by a modifiee link 
is allowed (an open-world assumption). 
6. Di rect :  like D i rect  Mod except it does not im- 
pose modifiee constraints on a pair of role values 
during parsing. 
Grammar sizes for these six grammars, extracted 
either directly from the 2,845 sentences or from the 
2,845 sentences expanded with our sub-grammar 
templates, appear in Table 1. The largest gram- 
mars were derived using the Full Mod extrac- 
tion method, with a fairly dramatic growth result- 
ing from processing template-expanded sentences. 
The Feature  and D i rect  variations are more man- 
ageable in size, even those derived from template- 
expanded sentences. 
Size is not the only important consideration for 
a grammar. Other important issues are grammar 
generality and the impact of the grammar on the 
accuracy of selecting the correct sentence from the 
recognition lattice of a spoken utterance. After 
extracting the CDG grammars from the RM sen- 
tences and template-expanded sentences, we tested 
the generality of the extracted grammars by using 
each grammar to parse the 7,396 RM2 sentences. 
See the results in Table 2. The grammar with the 
greatest generality was the conventional CDG for 
the RM corpus; however, this grammar also has 
the unfortunate attribute of being quite ambigu- 
ous. The most generalizable of extracted grammars 
uses the D i rect  method on template-expanded s n- 
tences. In all cases, the template-expanded sen- 
tence grammars gave better coverage than their cor- 
responding sentence-only grammars. 
We have also used the extracted grammars to 
post-process word graphs created by the word graph 
compression algorithm of (Johnson and Harper, 
1999) for the test utterances in the RM corpus. As 
was reported in (Johnson and Harper, 1999), the 
word-error rate of our HMM recognizer with an em- 
bedded word pair language model on the RM test set 
of 1200 utterances was 5.0%, the 1-best sentence ac- 
curacy was 72.1%, and the word graph coverage ac- 
curacy was 95.1%. Also, the average uncompressed 
word graph size was 75.15 nodes, and our compres- 
sion algorithm resulted in a average word graph size 
of 28.62 word nodes. When parsing the word graph, 
the probability associated with a word node can ei- 
ther represent its acoustic score or a combination 
of its acoustic and stochastic grammar score. We 
use the acoustic score because (Johnson and Harper, 
1999) showed that by using a word node's acoustic 
score alone when extracting the top sentence candi- 
date after parsing gave a 4% higher sentence accu- 
racy. 
For the parsing experiments, we processed the 
1,080 word graphs produced for the RM test set 
that contained 50 or fewer word nodes after com- 
pression (out of 1,200 total) in order to efficiently 
compare the 12 ARV/ARVP CDG grammars and 
the conventional CDG (the larger word graphs re- 
quire significant ime and space to parse using the 
conventional CDG). These 1,080 word graphs con- 
tain 24.95 word nodes on average with a standard 
deviation (SD) of 10.80, and result in 1-best sen- 
tence accuracy was 75% before parsing. The num- 
ber of role values prior to binary constraint propa- 
gation differ across the grammars with an average 
(and SD) for the conventional grammar of 504.99 
(442.00), for the sentence-only grammars of 133.37 
(119.48), and for the template-expanded grammars 
of 157.87 (145.16). Table 3 shows the word graph 
parsing speed and the path, node, and role value 
(RV) ambiguity after parsing; Table 4 shows the 
sentence accuracy and the accuracy and percent cor- 
rect for words. Note that percent correct words is 
calculated using N-D-S  and word accuracy using N 
N-D-S- I  where N is the number of words, D is N 
the number of deletions, S is the number of substi- 
tutions, and I is the number of insertions. 
The most selective RM sentence grammar, Full 
Mod,  achieves the highest sentence accuracy, but 
at a cost of a greater average parsing time than 
the other RM sentence grammars. Higher accu- 
107 
ARVP Variation Parse Time (sec.) 
Full Mod 33.89 (41.12) 
Template Full Mod 41.85 (51.75) 
Full 29.73 (36.68) 
Template Full 36.80 (46.90) 
Feature Mod 11.46 (14.46) 
Template Feature Mod 13.80 (18.47) 
Feature 11.60 (14.97) 
Template Feature 14.24 (19.63) 
Direct Mod 13.93 (19.73) 
Template Direct Mod 17.28 (26.56) 
Direct 19.95 (36.89) 
Template Direct 28.02 (69.50) 
Coventional 83.48 (167.51) 
No. Paths 
2.21 (1.74) 
2.78 (3.75) 
2.83 (2.92) 
3.40 (5.19) 
3.9 (5.97) 
4.22 (6.93) 
5.19 (8.36) 
6.86 (14.83) 
No. Nodes 
10.59 (3.44) 
10.76 (3.64) 
10.87 (3.54) 
11.03 (3.74) 
11.20 (3.94) 
11.28 (4.06) 
11.72 (4.22) 
11.94 (4.52) 
4.25 (6.49) 11.46 (4.27) 
4.62 (8.61) 11.45 (4.28) 
808 (18.52) 12.81 (5.73) 
9.98 (25.52) 12.95 (5.95) 
51.33 (132.43) 17.14 (8.02) 
No. RVs 
19.51 (8.32) 
19.93 (8.76) 
20.32 (8.86) 
20.77 (9.47) 
21.43 (10.49) 
21.81 (11.17) 
23.41 (12.72) 
24.47 (14.41) 
22.79 (13.44) 
22.95 (13.34) 
32.85 (34.65) 
33.36 (35.66) 
77.19 (76.26) 
Table 3: Average parse times (SD), number of paths (SD), number of nodes (SD), and number of role values 
(SD) remaining after parsing the 1,080 word graphs of 50 or fewer word nodes produced for the RM test set 
using the 13 CDGs. 
ARVP Variation Sentence Accuracy ~o Correct Words Word Accuracy 
Full Mod 
Template Full Mod 
Full 
Template Full 
Feature Mod 
Template Feature Mod 
Feature 
Template Feature 
Direct Mod 
Template Direct Mod 
Direct 
Template Direct 
Conventional 
91.94% 
91.57% 
91.57% 
91.20% 
90.56% 
90.19% 
90.28% 
89.91% 
90.46% 
90.09% 
89.91% 
89.44% 
81.20% 
98.55% 
98.50% 
98.49% 
98.45% 
98.38% 
98.34% 
98,35% 
98.29% 
98.37% 
98.32% 
98.30% 
98.25% 
97.11% 
98.19% 
98.14% 
98.11% 
98.05% 
97.95% 
97.90% 
97.91% 
97.85% 
97.91% 
97.86% 
97.82% 
97.75% 
96.10% 
Table 4: The sentence accuracy, percent correct words, and word accuracy from parsing 1,080 word graphs 
of 50 or fewer word nodes produced for the RM test set using the 13 CDGs. 
racy appears to be correlated with the ability of the 
constraints to eliminate word nodes from the word 
graph during parsing. The least restrictive sentence 
grammar, D i rect ,  is less accurate than the other 
sentence grammars and offers an intermediate speed 
of parsing, most likely due to the increased ambigu- 
ity in the parsing space. The fastest grammar was 
the Feature -Mod grammar, which also offers an 
intermediate l vel of accuracy. Its size (even with 
templates), restrictiveness, and speed make it very 
attractive. The template versions of each grammar 
showed a slight increase in average parse times (from 
processing a larger number of role values) and a 
slight decrease in parsing accuracy. The conven- 
tional grammar was the least competitive of the 
grammars both in speed and in accuracy. 
5 Conclus ion and Future  D i rect ions  
ity to improve sentence accuracy of our speech sys- 
tem. To achieve balance between precision and cov- 
erage of our corpus-induced grammars, we have ex- 
panded the RM sentences with templates for expres- 
sions like dates and times. The grammars extracted 
from these expanded sentences gave increased RM2 
coverage without sacrificing even 1% of the sentence 
accuracy. We are currently expanding the number of 
templates in our grammar in an attempt o obtain 
full coverage of the RM2 corpus using only template- 
expanded RM sentences. We have recently added 
ten semantic templates to the grammar and have 
improved the coverage by 9.19% without losing any 
sentence accuracy. We are also developing a stochas- 
tic version of CDG that uses a statistical ARV, which 
is similar to a supertag (Srinivas, 1996). 
References  
The ability to extract ARV/ARVP grammars with 
varying degrees of specificity provides us with the 
ability to rapidly develop a grammar with the abil- 
H. U. Block. 1997. Language components in VERB- 
MOBIL. In Proc. of the Int. Conf. of Acoustics, 
Speech, and Signal Proc., pages 79-82. 
108 
Defense Advanced Research Projects Agency 
(DARPA). 1990. Extended resource manage- 
ment: Continuous speech speaker-dependent 
corpus (RM2). CD-ROM. NIST Speech Discs 
3-1.2 and 3-2.2. 
Entropic Cambridge Research Laboratory, Ltd., 
1997. HTK: Hidden Markov Model Toolkit V2.1. 
M. P. Harper and R. A. Helzerman. 1995. Exten- 
sions to constraint dependency parsing for spoken 
language processing. Computer Speech and Lan- 
guage, 9:187-234. 
M. P. Harper, L. H. Jamieson, C. D. Mitchell, 
G. Ying, S. Potisuk, P. N. Srinivasan, R. Chen, 
C. B. Zoltowski, L. L. McPheters, B. Pellom, 
and R. A. Helzerman. 1994. Integrating language 
models with speech recognition. In Proc. of the 
AAAI Workshop on the Integration of Natural 
Language and Speech Processing, pages 139-146. 
M. P. Harper, S. A. Hockema, and C. M. White. 
1999a. Enhanced constraint dependency grammar 
parsers. In Proc. of the IASTED Int. Conf. on 
Artificial Intelligence and Soft Computing. 
M. P. Harper, M. T. Johnson, L. H. Jamieson, and 
C. M. White. 1999b. Interfacing a CDG parser 
with an HMM word recognizer using word graphs. 
In Proc. of the Int. Conf. of Acoustics, Speech, and 
Signal Proc. 
R. A. Helzerman and M. P. Harper. 1996. MUSE 
CSP: An extension to the constraint satisfaction 
problem. Journal of Artificial Intelligence Re- 
search, 5:239-288. 
P. Jeanrenaud, E. Eide, U. Chaudhari, J. Mc- 
Donough, K. Ng, M. Siu, and H. Gish. 1995. Re- 
ducing word error rate on conversational speech 
from the Switchboard corpus. In Proc. of the 
Int. Conf. of Acoustics, Speech, and Signal Proc., 
pages 53-56. 
M. T. Johnson and M. P. Harper. 1999. Near min- 
imal weighted word graphs for post-processing 
speech. In 1999 Int. Workshop on Automatic 
Speech Recognition and Understanding. 
H. Maruyama. 1990a. Constraint Dependency 
Grammar and its weak generative capacity. Com- 
puter Software. 
H. Maruyama. 1990b. Structural disambiguation 
with constraint propagation. In Proc. of the An- 
nual Meeting of Association for Computational 
Linguistics, pages 31-38. 
W. Menzel. 1994. Parsing of spoken language un- 
der time constraints. In 11th European Conf. on 
Artificial Intelligence, pages 560-564. 
W. Menzel. 1995. Robust processing of natural an- 
guage. In Proc. of the 19th Annual German Conf. 
on Artificial Intelligence. 
B. Natarajan. 1989. On learning sets and functions. 
Machine Learning, 4(1). 
H. Ney, U. Essen, and R. Kneser. 1994. On struc- 
turing probabilistic dependences in stochastic lan- 
guage modelling. Computer Speech and Language, 
8:1-38. 
R. Pieraccini and E. Levin. 1992. Stochastic repre- 
sentation of semantic structure for speech under- 
standing. Speech Communication, 11:283-288. 
P. Placeway, R. Schwartz, P. Fung, and L. Nguyen. 
1993. The estimation of powerful anguage mod- 
els from small and large corpora. In Proc. of the 
Int. Conf. of Acoustics, Speech, and Signal Proc., 
pages 33-36. 
P. J. Price, W. Fischer, J. Bernstein, and D. Pallett. 
1988. A database for continuous peech recog- 
nition in a 1000-word omain. In Proc. of the 
Int. Conf. of Acoustics, Speech, and Signal Proc., 
pages 651-654. 
L. A. Schmid. 1994. Parsing word graphs using a lin- 
guistic grammar and a statistical language model. 
In Proc. of the Int. Conf. of Acoustics, Speech, 
and Signal Proc., pages 41-44. 
S. Seneff. 1992. TINA: A natural language system 
for spoken language applications. American Jour- 
nal of Computational Linguistics, 18:61-86. 
B. Srinivas. 1996. 'Almost parsing' technique for 
language modeling. In Proc. of the Int. Conf. on 
Spoken Language Processing, pages 1173-1176. 
L. G. Valiant. 1984. A theory of the learnable. Com- 
munications of the ACM, 27(11):1134-1142. 
C. M. White. 2000. Rapid Grammar Development 
and Parsing Using Constraint Dependency Gram- 
mars with Abstract Role Values. Ph.D. thesis, 
Purdue University, School of Electrical and Com- 
puter Engineering, West Lafayette, IN. 
J. H. Wright, G. J. F. Jones, and H. Lloyd-Thomas. 
1994. Robust language model incorporating a
substring parser and extended N-grams. In Proc. 
of the Int. Conf. of Acoustics, Speech, and Signal 
Proc., pages 361-364. 
S. J. Young, N. H. Russell, and J. H. S. Thornton. 
1989. Token passing : a simple conceptual model 
for connected speech recognition systems. Tech- 
nical Report TR38, Cambridge University, Cam- 
bridge, England. 
S. J. Young, J. Odell, D. Ollason, V. Valtchev, and 
P. Woodland, 1997. The HTK Book. Entropic 
Cambridge Research Laboratory Ltd., 2.1 edition. 
V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, 
J. Polifroni, and S. Seneff. 1991. Integration of 
speech recognition and natural anguage process- 
ing in the MIT Voyager system. In Proe. of the 
Int. Conf. of Acoustics, Speech, and Signal Proe., 
pages 713-716. 
109 
Book Reviews
Introducing Speech and Language Processing
John Coleman
(University of Oxford)
Cambridge University Press (Cambridge introductions to language and
linguistics), 2005, xi+301 pp; hardbound, ISBN 0-521-82365-X, $90.00; paperbound,
ISBN 0-521-53069-5, $39.99
Reviewed by
Mary Harper
Purdue University
In October 2003, a group of multidisciplinary researchers convened at the Symposium
on Next Generation Automatic Speech Recognition (ASR) to consider new directions in
building ASR systems (Lee 2003). Although the workshop?s goal of ?integrating multi-
disciplinary sources of knowledge, from acoustics, speech, linguistics, cognitive science,
signal processing, human computer interaction, and computer science, into every stage
of ASR component and system design? is an important goal, there remains a divide
among these communities that can only be addressed through the educational process.
The book Introducing Speech and Language Processing by John Coleman represents a bold
effort to educate students in speech science about some of the important methods used
in speech and natural language processing (NLP). This book represents an important
first step for forging effective collaborations with the speech and language processing
communities.
Coleman states in chapter 1 of his book that ?This is a first, basic, elementary and
short textbook in speech and natural language processing for beginners with little or no
previous experience of computer programming? (page 2). Coleman targets the book at
students in a variety of disciplines, including arts, humanities, linguistics, psychology,
and speech science, as well as early science and engineering students who want a
glimpse into natural language and speech processing. However, since it assumes prior
knowledge of basic linguistics, the text is likely to be less accessible to traditional
science and engineering students. Coleman?s motivation for writing this book is that
the currently available textbooks in NLP and speech require knowledge that students
from more of a humanities background would not have (e.g., programming, signal
processing). The author also astutely points out that there tends to be a divide between
the areas of signal processing and computational linguistics, although in recent years
with ubiquity of statistical modeling and machine learning techniques in both areas,
this divide is becoming much smaller. The author?s motivation for this book is excellent:
?a refusal to let the old sociological divide between arts and sciences stand in the way
of a new wave of spoken language researchers with a foot in both camps? (page 4).
The textbook covers a variety of techniques in speech and natural language process-
ing, along with computer programs implementing many of them in either C or Prolog,
and it capitalizes on Coleman?s insights from courses offered to graduate linguistics
students. It comes with a companion CD containing software needed to compile and/or
execute the programs in the book, as well as source code for all of the described
implementations. The readme file on the CD contains helpful installation notes, while
the text describes how to compile and use each of the programs. Chapter 1 contains
Computational Linguistics Volume 32, Number 1
a comprehensive list of topics that are covered from ?first principles,? provides de-
tails about the computational environment that is needed to compile and execute the
programs provided on the CD, and a listing of computer skills one would need to
get started. Coleman encourages the reader/student (I will use student henceforth) not
just to run the programs but to also to ?tinker? with them in order to gain a deeper
understanding of the way they work. Chapter 1 also lays out the structure of the text
graphically in order to depict the dependencies among the chapters. In addition to the
book chapters, there is an appendix on ASCII characters, a helpful glossary, a list of
references, and a comprehensive index. Importantly, there is also a companion website
with errata, solutions to selected exercises, bug reports, software updates, additional
programs, links to third-party software, and some nice bibliography links. Presumably,
this page will be updated over time.
The overall chapter organization of the book is quite nice. Each chapter begins with
a preview and a list of key terms (allowing the student an opportunity to look up the
definitions prior to beginning to read the chapter content) and ends with a chapter
summary, a set of exercises that are helpful for developing a deeper understanding of
the materials discussed in the chapter, suggestions for further reading, and suggestions
for readings to prepare for the next chapter. I will discuss chapters 2 through 9 in turn.
Chapter 2 discusses issues related to the digital representation of a signal with a
focus on the composition of a sound file and how such a file can be loaded into a sound-
editing program for audio display. The chapter starts off by guiding the student through
the process of listening to a cosine waveform and then viewing the same file using a
sound editing program such as Cool Edit 2000. The student is asked to fill in a worksheet
with values for a cosine function and then plot the values. Coleman then presents
important information on the digital representation of sound and on sampling theory.
Given this knowledge, the student is walked through the process of generating and
playing a cosine wave. The chapter contains a just-in-time introduction to C sufficient
for a student to read and comprehend the cosine wave generation program coswave.c.
Various computing terms (e.g., bit, compilation, machine code) are defined, followed
by a discussion of C numeric data types and differences in representation across ar-
chitecture. The C code presented in this chapter makes concrete Coleman?s discussion
of loops, arrays, calculation of mathematical expressions, overall program layout, and
file output. The chapter ends with several helpful exercises. The first provides a very
detailed set of instructions for compiling and executing the coswave program and then
playing the generated output signal in Cool Edit 2000. It should be noted that Cool Edit
2000 is not a public-domain package and is no longer available through the original
developers. Alternatives mentioned on the text?s Web site (e.g., wavesurfer, Praat) can
be used instead, although no details are offered about using them for the exercises.
Students may face some challenges in opening and playing raw data files with these
alternatives.
Chapter 3 introduces methods for modifying the digital representations of sound; in
particular, the concept of filtering is introduced, followed by a very brief discussion of
how filters are employed in a Klatt formant synthesizer. The chapter first discusses how
operations can be applied to number sequences in C to set the stage for discussion of
several speech-processing applications. RMS energy is then defined and a correspond-
ing C program is discussed in detail. Next, a moving-average program is presented as
an example of a low-pass filter. The concept of recursion is next introduced in order to
pave the way for a discussion of IIR (Infinite Impulse Response) filters. High-, low-, and
band-pass filters are defined and tables of coefficients for various filters are provided.
An implementation of an IIR filter is discussed quite briefly; here the author relies on the
138
Book Reviews
fact that there is similarity to the earlier moving-average program. Finally, after the basic
introduction to filters, the Klatt synthesizer is discussed and a schematic diagram for the
system is presented together with a brief discussion of the control parameters that are
used to synthesize sound. IIR filters are tied in because they are used for modeling the
glottal wave and filter-specific frequency components in order to obtain the resonant
frequencies of the vocal tract required for the sound to be synthesized. A consonant?
vowel example is used to demonstrate the synthesizer in action. There is a series of
three exercises at the end of the chapter that should help the student get a better sense
of filters and the type of sound generated by the Klatt synthesizer. The synthesizer
exercises have a cookbook feel to them and give only a glimpse of what is needed to
actually synthesize speech. At the end of the chapter, no further readings on filters are
provided, although readings are recommended for the Klatt synthesizer and methods
for estimating its parameters.
Chapter 4 discusses several programs to extract acoustic parameters from a speech
signal. First up is the fast Fourier transform (FFT), for which a C implementation is
presented and described in detail. The student is asked to apply the compiled code
to an example speech file in order to generate its spectrum, which is then plotted in
Excel or Matlab for comparison to the spectral analysis obtained using Cool Edit. Given
this example, there is a discussion of the types of peaks found in the spectrum, the
resonances of the vocal tract, and the harmonics, as a prelude to the discussion of
cepstral analysis. Coleman first provides a high-level discussion of cepstral analysis,
which employs an inverse FFT, followed by the discussion of its C implementation and
an example using the executable. Cepstral analysis is then used to build a rudimentary
pitch tracker, which is applied to a speech example. This leads to the discussion of
a voicing detection algorithm. Next, the autocorrelation method for pitch tracking is
presented together with its C implementation. Finally, the chapter discusses linear
predictive coding (LPC) and various applications. The chapter ends with exercises to
compare the cepstral and autocorrelation pitch trackers, to modify the output of the LPC
program, and to analyze the vowels in a speech sample and use the LPC spectrum to
estimate their formants. Additional readings are provided on the algorithms presented
in this chapter.
Chapter 5 offers a change of pace as the book introduces finite-state machines
(FSMs) with a focus initially on symbolic language applications. There is a shift from
C to Prolog, although it would have been perhaps more coherent to stick with C. The
discussion of the peculiarities of Prolog could be distracting to a novice programmer.
Furthermore, the representation of an FSM in Prolog is tedious to read, and it may
be difficult for the uninitiated to observe correspondences between the Prolog code
and depictions of corresponding models. Simple examples are used to introduce the
concept of, rather than a formal definition of, FSMs. Issues of coverage, over-generation,
determinism, and nondeterminism of an FSM are discussed briefly. Although Coleman
makes clear that backtracking is an issue for a nondeterministic FSM and notes that
there are methods for converting such an FSM to a representationally equivalent deter-
ministic form, existing tools that could be used for carrying out this conversion (e.g., the
AT&T FSM library) are not discussed. Coleman next presents a Prolog implementation
of an interesting English-like monosyllable FSM. A box is used to introduce a collection
of facts about Prolog, and then there is a walk-through of the code. A nice set of exercises
follows in which the student loads the FSM program and executes it in various ways,
followed by a discussion of some examples of using the FSM to generate strings with
particular constraints. A more formal presentation of FSMs is then provided togeth-
er with a discussion of the state-transition-table representation. The chapter ends by
139
Computational Linguistics Volume 32, Number 1
introducing the concept of finite-state transducers and providing several examples from
various levels of processing, including phonetics, phonology, orthography, and syntax.
Exercises at the end of the chapter build nicely upon the Prolog code already discussed.
The suggested additional readings are appropriate, but perhaps too broad, as many are
textbooks.
Chapter 6 turns to the topic of automatic speech recognition. Coleman provides
a general discussion of knowledge-based and pattern-recognition approaches to ASR
without a historical perspective. The knowledge-based method with its focus on fea-
ture extraction and knowledge integration is described at a very high level without
the benefit of any hands-on exercises. Coleman uses dynamic time warping (DTW)
to exemplify the pattern-matching approach, as it is a fairly straightforward dynamic
programming algorithm of which the student can gain some understanding by filling
in tables of distances. The chapter also contains a nice discussion on the sources of
variability in speech, although no insights are offered on how they would be addressed
by the two approaches to ASR. Only two exercises are found in this chapter, one to fill
in matrices used by the dynamic time-warping algorithm and one asking the student to
think about pitfalls of the pattern-matching approach. The chapter does not discuss the
implementation of any of the methods discussed, although I believe a C implementation
of DTW could have been added to good effect. There are some helpful recommended
readings on ASR techniques, many of which are textbooks or edited books of papers.
Chapter 7 introduces probabilistic finite-state approaches, bringing together acous-
tic analysis with finite-state models. The chapter begins with a discussion of Markov
models and the use of probabilistic methods for coping with uncertainty. Part-of-speech
n-gram models are introduced together with a very brief discussion of probabilities
and Markov models (along with a few simple exercises). Coleman then provides an
informal discussion of the hidden Markov model (HMM), followed by a discussion of
trigram models, data incompleteness, and backoff. Finally, the three basic problems for
HMMs (Rabiner 1989; Rabiner and Juang 1993) are discussed, providing the student
with a clearer understanding of the kinds of questions that can be addressed with them.
There are two very short sections on using HMMs for part-of-speech tagging and speech
recognition, but there are no code or exercises associated with them. The chapter ends
with a discussion of Chomsky?s objections to Markov models and a response to each.
The only exercises appearing in this chapter concern probability and Markov models.
The chapter does not discuss implementations of any of the approaches discussed, and
yet it would seem that the student would gain a deeper understanding of many of the
topics presented in this chapter by playing, for example, with a simple part-of-speech
tagger. There are many publicly available resources that could be used to fill in this
hole.
Chapter 8 moves on to parsing, building upon the knowledge of Prolog gained
in chapter 5. A simple definite-clause grammar is introduced, followed by an intuitive
discussion of parsing and recursive descent parsers. A second grammar, dog grammar.pl,
is then discussed together with difference lists in Prolog so that the grammar can be
updated to produce a tree structure. Coleman then provides an example grammar
that breaks phoneme sequences into syllables. The chapter ends with a very brief
introduction to various parsing algorithms, chart parsing, issues of search, deterministic
parsing, and parallel parsing. The chapter would have been improved by the addition
of exercises; however, the student could load the grammars discussed in the chapter
into Prolog and play with them. Several textbooks are recommended for additional
reading, although the novice might gain a richer perspective by consulting the chapters
on parsing of Allen (1994).
140
Book Reviews
Chapter 9, the final chapter in the book, discusses the incorporation of probability
into a context-free parsing algorithm. Coleman begins with a discussion about why a
probabilistic approach is useful in computational linguistics, ranging from the fact that
human judgments of grammaticality are gradient and at times uncertain of providing a
good mechanism to account for collocations and the learnability of grammars. A simple
probabilistic context-free grammar (CFG) is then presented, along with a discussion of
how to obtain the grammar rules and estimate their probabilities. The chapter ends by
discussing limitations of probabilistic CFGs and briefly introducing two alternative ap-
proaches, tree-adjoining grammars and data-oriented parsing. This chapter contains no
exercises for the student. However, it does provide a list of materials to assist the student
in learning more about C programming, digital signal processing, the Klatt synthesizer,
speech recognition, Prolog, computational linguistics, and probabilistic grammars.
Overall, Coleman has written a textbook that more than adequately fulfills his goal
of introducing the uninitiated to a variety of techniques in speech and language process-
ing. Due to its broad coverage, the text is unable to delve deeply into many of the details,
although this is mitigated in part by the fact that he provides additional readings for
students with an interest in a particular topic. The reading list on the companion website
would be improved by including more modern sources, pointers to current conferences
and journals in speech and natural language processing (e.g., Bird 2005), and links to
helpful resources available on the Internet (e.g., DISC 1999; Hunt 1997; Jamieson 2002;
Kita 2000; Krauwer 2005; Manning 2005; Picone 2005). Additionally, although the book
is not aimed at students with a strong background in mathematics or computer science,
they would benefit from additional readings in these areas. The book would benefit
from additional editing, as it contains errors that could easily confuse a novice, as well
as from the addition of more hands-on exercises, particularly in Chapters 6 through 9.
Quibbles aside, if the book builds bridges between the communities Coleman desires,
it will have a broad impact that could be felt for years to come. I believe education is an
important first step to building multidisciplinary solutions to some of the most pressing
problems in speech and natural language processing. It would be wonderful to see more
books with Coleman?s vision.
References
Allen, James. 1994. Natural Language
Understanding. The Benjamin/Cummings
Publishing Company Inc., Redwood
City, CA.
Bird, Steven, editor. 2005. ACL Anthology:
A digital archive of research papers in
computational linguistics.
acl.ldc.upenn.edu/.
DISC. 1999. A survey of existing methods
and tools for developing and
evaluation of speech synthesis and
of commercial speech synthesis
systems. www.disc2.dk/tools/SGsurvey.
html.
Hunt, Andrew. 1997. CompS?peech
frequently asked questions.
fife.speech.cs.cmu.edu/comp.speech/.
Jamieson, Leah H. 2002. Notes for EE649
lectures: Speech processing by
computer. shay.ecn.purdue.edu/ee649/
notes/.
Kita, Kenji. 2000. Speech and language Web
resources. www-a2k.is.tokushima-u.ac.
jp/member/kita/NLP/.
Krauwer, Steven. 2005. Tools for NLP and
speech. www.elsnet.org/toolslist.html.
Lee, Chin-Hui. 2003. NSF Symposium on
Next Generation ASR.
users.ece.gatech.edu/chl/ngasr03/.
Manning, Christopher. 2005. Linguistics,
natural language, and computational
linguistics meta-index. www-nlp.stanford.
edu/ links/ linguistics.html.
Picone, Joseph. 2005. Automatic speech
recognition. www.cavs.msstate.edu/hse/ies/
projects/speech/.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition.
Proceedings of the IEEE, 77(2):257?286.
Rabiner, Lawrence and Biing-Hwang Juang.
1993. Fundamentals of Speech Recognition.
Prentice-Hall, Inc., Upper Saddle River, NJ.
141
Computational Linguistics Volume 32, Number 1
Mary Harper joined the faculty of Purdue University in 1989, where she holds the rank of Professor
in the School of Electrical and Computer Engineering. She recently finished a term of slightly
more than three years as the Program Director for the Human Language and Communication
Program at the National Science Foundation with the goal of advancing research in speech, nat-
ural language, and multimodal processing. Her research focuses on computer modeling of human
communication with a focus on methods for incorporating multiple types of knowledge sources,
including lexical, syntactic, prosodic, and, most recently, visual sources. Harper is currently at the
Center for Advanced Study of Language, University of Maryland, College Park, MD 20742-0025;
e-mail: harper@purdue.edu, mharper@casl.umd.edu; URL: yara.ecn.purdue.edu/?harper.
142
Proceedings of the 43rd Annual Meeting of the ACL, pages 451?458,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Using Conditional Random Fields For Sentence Boundary Detection In
Speech
Yang Liu
ICSI, Berkeley
yangl@icsi.berkeley.edu
Andreas Stolcke Elizabeth Shriberg
SRI and ICSI
stolcke,ees@speech.sri.com
Mary Harper
Purdue University
harper@ecn.purdue.edu
Abstract
Sentence boundary detection in speech is
important for enriching speech recogni-
tion output, making it easier for humans to
read and downstream modules to process.
In previous work, we have developed hid-
den Markov model (HMM) and maximum
entropy (Maxent) classifiers that integrate
textual and prosodic knowledge sources
for detecting sentence boundaries. In this
paper, we evaluate the use of a condi-
tional random field (CRF) for this task
and relate results with this model to our
prior work. We evaluate across two cor-
pora (conversational telephone speech and
broadcast news speech) on both human
transcriptions and speech recognition out-
put. In general, our CRF model yields a
lower error rate than the HMM and Max-
ent models on the NIST sentence bound-
ary detection task in speech, although it
is interesting to note that the best results
are achieved by three-way voting among
the classifiers. This probably occurs be-
cause each model has different strengths
and weaknesses for modeling the knowl-
edge sources.
1 Introduction
Standard speech recognizers output an unstructured
stream of words, in which the important structural
features such as sentence boundaries are missing.
Sentence segmentation information is crucial and as-
sumed in most of the further processing steps that
one would want to apply to such output: tagging
and parsing, information extraction, summarization,
among others.
1.1 Sentence Segmentation Using HMM
Most prior work on sentence segmentation (Shriberg
et al, 2000; Gotoh and Renals, 2000; Christensen
et al, 2001; Kim and Woodland, 2001; NIST-
RT03F, 2003) have used an HMM approach, in
which the word/tag sequences are modeled by N-
gram language models (LMs) (Stolcke and Shriberg,
1996). Additional features (mostly related to speech
prosody) are modeled as observation likelihoods at-
tached to the N-gram states of the HMM (Shriberg
et al, 2000). Figure 1 shows the graphical model
representation of the variables involved in the HMM
for this task. Note that the words appear in both
the states1 and the observations, such that the
word stream constrains the possible hidden states
to matching words; the ambiguity in the task stems
entirely from the choice of events. This architec-
ture differs from the one typically used for sequence
tagging (e.g., part-of-speech tagging), in which the
?hidden? states represent only the events or tags.
Empirical investigations have shown that omitting
words in the states significantly degrades system
performance for sentence boundary detection (Liu,
2004). The observation probabilities in the HMM,
implemented using a decision tree classifier, capture
the probabilities of generating the prosodic features
1In this sense, the states are only partially ?hidden?.
451
P (F
i
jE
i
;W
i
).
2 An N-gram LM is used to calculate
the transition probabilities:
P (W
i
E
i
jW
1
E
1
: : :W
i 1
E
i 1
) =
P (W
i
jW
1
E
1
: : :W
i 1
E
i 1
)
P (E
i
jW
1
E
1
: : :W
i 1
E
i 1
E
i
)
In the HMM, the forward-backward algorithm is
used to determine the event with the highest poste-
rior probability for each interword boundary:
^
E
i
= argmax
E
i
P (E
i
jW;F ) (1)
The HMM is a generative modeling approach since
it describes a stochastic process with hidden vari-
ables (sentence boundary) that produces the observ-
able data. This HMM approach has two main draw-
backs. First, standard training methods maximize
the joint probability of observed and hidden events,
as opposed to the posterior probability of the correct
hidden variable assignment given the observations,
which would be a criterion more closely related to
classification performance. Second, the N-gram LM
underlying the HMM transition model makes it dif-
ficult to use features that are highly correlated (such
as words and POS labels) without greatly increas-
ing the number of model parameters, which in turn
would make robust estimation difficult. More details
about using textual information in the HMM system
are provided in Section 3.
1.2 Sentence Segmentation Using Maxent
A maximum entropy (Maxent) posterior classifica-
tion method has been evaluated in an attempt to
overcome some of the shortcomings of the HMM
approach (Liu et al, 2004; Huang and Zweig, 2002).
For a boundary position i, the Maxent model takes
the exponential form:
P (E
i
jT
i
; F
i
) =
1
Z

(T
i
; F
i
)
e
P
k

k
g
k
(E
i
;T
i
;F
i
) (2)
where Z

(T
i
; F
i
) is a normalization term and T
i
represents textual information. The indicator func-
tions g
k
(E
i
; T
i
; F
i
) correspond to features defined
over events, words, and prosody. The parameters in
2In the prosody model implementation, we ignore the word
identity in the conditions, only using the timing or word align-
ment information.
Wi Ei
Fi
Oi
Wi+1 Ei+1
Oi+1
Wi Fi+1Wi+1
Figure 1: A graphical model of HMM for the
sentence boundary detection problem. Only one
word+event pair is depicted in each state, but in
a model based on N-grams, the previous N   1
tokens would condition the transition to the next
state. O are observations consisting of words W and
prosodic features F , and E are sentence boundary
events.
Maxent are chosen to maximize the conditional like-
lihood
Q
i
P (E
i
jT
i
; F
i
) over the training data, bet-
ter matching the classification accuracy metric. The
Maxent framework provides a more principled way
to combine the largely correlated textual features, as
confirmed by the results of (Liu et al, 2004); how-
ever, it does not model the state sequence.
A simple combination of the results from the
Maxent and HMM was found to improve upon the
performance of either model alone (Liu et al, 2004)
because of the complementary strengths and weak-
nesses of the two models. An HMM is a generative
model, yet it is able to model the sequence via the
forward-backward algorithm. Maxent is a discrimi-
native model; however, it attempts to make decisions
locally, without using sequential information.
A conditional random field (CRF) model (Laf-
ferty et al, 2001) combines the benefits of the HMM
and Maxent approaches. Hence, in this paper we
will evaluate the performance of the CRF model and
relate the results to those using the HMM and Max-
ent approaches on the sentence boundary detection
task. The rest of the paper is organized as follows.
Section 2 describes the CRF model and discusses
how it differs from the HMM and Maxent models.
Section 3 describes the data and features used in the
models to be compared. Section 4 summarizes the
experimental results for the sentence boundary de-
tection task. Conclusions and future work appear in
Section 5.
452
2 CRF Model Description
A CRF is a random field that is globally conditioned
on an observation sequence O. CRFs have been suc-
cessfully used for a variety of text processing tasks
(Lafferty et al, 2001; Sha and Pereira, 2003; McCal-
lum and Li, 2003), but they have not been widely ap-
plied to a speech-related task with both acoustic and
textual knowledge sources. The top graph in Figure
2 is a general CRF model. The states of the model
correspond to event labels E. The observations O
are composed of the textual features, as well as the
prosodic features. The most likely event sequence ^E
for the given input sequence (observations) O is
^
E = argmax
E
e
P
k

k
G
k
(E;O)
Z

(O)
(3)
where the functions G are potential functions over
the events and the observations, and Z

is the nor-
malization term:
Z

(O) =
X
E
e
P
k

k
G
k
(E;O) (4)
Even though a CRF itself has no restriction on
the potential functions G
k
(E;O), to simplify the
model (considering computational cost and the lim-
ited training set size), we use a first-order CRF in
this investigation, as at the bottom of Figure 2. In
this model, an observation O
i
(consisting of textual
features T
i
and prosodic features F
i
) is associated
with a state E
i
.
The model is trained to maximize the conditional
log-likelihood of a given training set. Similar to the
Maxent model, the conditional likelihood is closely
related to the individual event posteriors used for
classification, enabling this type of model to explic-
itly optimize discrimination of correct from incor-
rect labels. The most likely sequence is found using
the Viterbi algorithm.3
A CRF differs from an HMM with respect to its
training objective function (joint versus conditional
likelihood) and its handling of dependent word fea-
tures. Traditional HMM training does not maxi-
mize the posterior probabilities of the correct la-
bels; whereas, the CRF directly estimates posterior
3The forward-backward algorithm would most likely be bet-
ter here, but it is not implemented in the software we used (Mc-
Callum, 2002).
E 1 E 2 E i E N
O
E i
Oi
E i-1
O i-1
E i+1
O i+1
Figure 2: Graphical representations of a general
CRF and the first-order CRF used for the sentence
boundary detection problem. E represent the state
tags (i.e., sentence boundary or not). O are observa-
tions consisting of words W or derived textual fea-
tures T and prosodic features F .
boundary label probabilities P (EjO). The under-
lying N-gram sequence model of an HMM does
not cope well with multiple representations (fea-
tures) of the word sequence (e.g., words, POS), es-
pecially when the training set is small; however, the
CRF model supports simultaneous correlated fea-
tures, and therefore gives greater freedom for incor-
porating a variety of knowledge sources. A CRF
differs from the Maxent method with respect to its
ability to model sequence information. The primary
advantage of the CRF over the Maxent approach is
that the model is optimized globally over the entire
sequence; whereas, the Maxent model makes a local
decision, as shown in Equation (2), without utilizing
any state dependency information.
We use the Mallet package (McCallum, 2002) to
implement the CRF model. To avoid overfitting, we
employ a Gaussian prior with a zero mean on the
parameters (Chen and Rosenfeld, 1999), similar to
what is used for training Maxent models (Liu et al,
2004).
3 Experimental Setup
3.1 Data and Task Description
The sentence-like units in speech are different from
those in written text. In conversational speech,
these units can be well-formed sentences, phrases,
or even a single word. These units are called SUs
in the DARPA EARS program. SU boundaries, as
453
well as other structural metadata events, were an-
notated by LDC according to an annotation guide-
line (Strassel, 2003). Both the transcription and the
recorded speech were used by the annotators when
labeling the boundaries.
The SU detection task is conducted on two cor-
pora: Broadcast News (BN) and Conversational
Telephone Speech (CTS). BN and CTS differ in
genre and speaking style. The average length of SUs
is longer in BN than in CTS, that is, 12.35 words
(standard deviation 8.42) in BN compared to 7.37
words (standard deviation 8.72) in CTS. This dif-
ference is reflected in the frequency of SU bound-
aries: about 14% of interword boundaries are SUs in
CTS compared to roughly 8% in BN. Training and
test data for the SU detection task are those used in
the NIST Rich Transcription 2003 Fall evaluation.
We use both the development set and the evalua-
tion set as the test set in this paper in order to ob-
tain more meaningful results. For CTS, there are
about 40 hours of conversational data (around 480K
words) from the Switchboard corpus for training
and 6 hours (72 conversations) for testing. The BN
data has about 20 hours of Broadcast News shows
(about 178K words) in the training set and 3 hours
(6 shows) in the test set. Note that the SU-annotated
training data is only a subset of the data used for
the speech recognition task because more effort is
required to annotate the boundaries.
For testing, the system determines the locations
of sentence boundaries given the word sequence W
and the speech. The SU detection task is evaluated
on both the reference human transcriptions (REF)
and speech recognition outputs (STT). Evaluation
across transcription types allows us to obtain the per-
formance for the best-case scenario when the tran-
scriptions are correct; thus factoring out the con-
founding effect of speech recognition errors on the
SU detection task. We use the speech recognition
output obtained from the SRI recognizer (Stolcke et
al., 2003).
System performance is evaluated using the offi-
cial NIST evaluation tools.4 System output is scored
by first finding a minimum edit distance alignment
between the hypothesized word string and the refer-
4See http://www.nist.gov/speech/tests/rt/rt2003/fall/ for
more details about scoring.
ence transcriptions, and then comparing the aligned
event labels. The SU error rate is defined as the total
number of deleted or inserted SU boundary events,
divided by the number of true SU boundaries. In
addition to this NIST SU error metric, we use the
total number of interword boundaries as the denomi-
nator, and thus obtain results for the per-boundary-
based metric.
3.2 Feature Extraction and Modeling
To obtain a good-quality estimation of the condi-
tional probability of the event tag given the obser-
vations P (E
i
jO
i
), the observations should be based
on features that are discriminative of the two events
(SU versus not). As in (Liu et al, 2004), we utilize
both textual and prosodic information.
We extract prosodic features that capture duration,
pitch, and energy patterns associated with the word
boundaries (Shriberg et al, 2000). For all the model-
ing methods, we adopt a modular approach to model
the prosodic features, that is, a decision tree classi-
fier is used to model them. During testing, the de-
cision tree prosody model estimates posterior prob-
abilities of the events given the associated prosodic
features for a word boundary. The posterior prob-
ability estimates are then used in various modeling
approaches in different ways as described later.
Since words and sentence boundaries are mu-
tually constraining, the word identities themselves
(from automatic recognition or human transcrip-
tions) constitute a primary knowledge source for
sentence segmentation. We also make use of vari-
ous automatic taggers that map the word sequence to
other representations. Tagged versions of the word
stream are provided to support various generaliza-
tions of the words and to smooth out possibly un-
dertrained word-based probability estimates. These
tags include part-of-speech tags, syntactic chunk
tags, and automatically induced word classes. In ad-
dition, we use extra text corpora, which were not an-
notated according to the guideline used for the train-
ing and test data (Strassel, 2003). For BN, we use
the training corpus for the LM for speech recogni-
tion. For CTS, we use the Penn Treebank Switch-
board data. There is punctuation information in
both, which we use to approximate SUs as defined
in the annotation guideline (Strassel, 2003).
As explained in Section 1, the prosody model and
454
Table 1: Knowledge sources and their representations in different modeling approaches: HMM, Maxent,
and CRF.
HMM Maxent CRF
generative model conditional approach
Sequence information yes no yes
LDC data set (words or tags) LM N-grams as indicator functions
Probability from prosody model real-valued cumulatively binned
Additional text corpus N-gram LM binned posteriors
Speaker turn change in prosodic features a separate feature,
in addition to being in the prosodic feature set
Compound feature no POS tags and decisions from prosody model
the N-gram LM can be integrated in an HMM. When
various textual information is used, jointly modeling
words and tags may be an effective way to model the
richer feature set; however, a joint model requires
more parameters. Since the training set for the SU
detection task in the EARS program is quite limited,
we use a loosely coupled approach:
 Linearly combine three LMs: the word-based
LM from the LDC training data, the automatic-
class-based LMs, and the word-based LM
trained from the additional corpus.
 These interpolated LMs are then combined
with the prosody model via the HMM. The
posterior probabilities of events at each bound-
ary are obtained from this step, denoted as
P
HMM
(E
i
jW;C;F ).
 Apply the POS-based LM alone to the POS
sequence (obtained by running the POS tag-
ger on the word sequence W ) and generate the
posterior probabilities for each word boundary
P
posLM
(E
i
jPOS), which are then combined
from the posteriors from the previous step,
i.e., P
final
(E
i
jT; F ) = P
HMM
(E
i
jW;C;F )+
P
posLM
(E
i
jP ).
The features used for the CRF are the same as
those used for the Maxent model devised for the SU
detection task (Liu et al, 2004), briefly listed below.
 N-grams of words or various tags (POS tags,
automatically induced classes). Different Ns
and different position information are used (N
varies from one through four).
 The cumulative binned posterior probabilities
from the decision tree prosody model.
 The N-gram LM trained from the extra cor-
pus is used to estimate posterior event proba-
bilities for the LDC-annotated training and test
sets, and these posteriors are then thresholded
to yield binary features.
 Other features: speaker or turn change, and
compound features of POS tags and decisions
from the prosody model.
Table 1 summarizes the features and their repre-
sentations used in the three modeling approaches.
The same knowledge sources are used in these ap-
proaches, but with different representations. The
goal of this paper is to evaluate the ability of these
three modeling approaches to combine prosodic and
textual knowledge sources, not in a rigidly parallel
fashion, but by exploiting the inherent capabilities
of each approach. We attempt to compare the mod-
els in as parallel a fashion as possible; however, it
should be noted that the two discriminative methods
better model the textual sources and the HMM bet-
ter models prosody given its representation in this
study.
4 Experimental Results and Discussion
SU detection results using the CRF, HMM, and
Maxent approaches individually, on the reference
transcriptions or speech recognition output, are
shown in Tables 2 and 3 for CTS and BN data, re-
spectively. We present results when different knowl-
edge sources are used: word N-gram only, word N-
gram and prosodic information, and using all the
455
Table 2: Conversational telephone speech SU detection results reported using the NIST SU error rate (%)
and the boundary-based error rate (% in parentheses) using the HMM, Maxent, and CRF individually and in
combination. Note that the ?all features? condition uses all the knowledge sources described in Section 3.2.
?Vote? is the result of the majority vote over the three modeling approaches, each of which uses all the
features. The baseline error rate when assuming there is no SU boundary at each word boundary is 100%
for the NIST SU error rate and 15.7% for the boundary-based metric.
Conversational Telephone Speech
HMM Maxent CRF
word N-gram 42.02 (6.56) 43.70 (6.82) 37.71 (5.88)
REF word N-gram + prosody 33.72 (5.26) 35.09 (5.47) 30.88 (4.82)
all features 31.51 (4.92) 30.66 (4.78) 29.47 (4.60)
Vote: 29.30 (4.57)
word N-gram 53.25 (8.31) 53.92 (8.41) 50.20 (7.83)
STT word N-gram + prosody 44.93 (7.01) 45.50 (7.10) 43.12 (6.73)
all features 43.05 (6.72) 43.02 (6.71) 42.00 (6.55)
Vote: 41.88 (6.53)
features described in Section 3.2. The word N-
grams are from the LDC training data and the extra
text corpora. ?All the features? means adding textual
information based on tags, and the ?other features? in
the Maxent and CRF models as well. The detection
error rate is reported using the NIST SU error rate,
as well as the per-boundary-based classification er-
ror rate (in parentheses in the table) in order to factor
out the effect of the different SU priors. Also shown
in the tables are the majority vote results over the
three modeling approaches when all the features are
used.
4.1 CTS Results
For CTS, we find from Table 2 that the CRF is supe-
rior to both the HMM and the Maxent model across
all conditions (the differences are significant at p <
0:05). When using only the word N-gram informa-
tion, the gain of the CRF is the greatest, with the dif-
ferences among the models diminishing as more fea-
tures are added. This may be due to the impact of the
sparse data problem on the CRF or simply due to the
fact that differences among modeling approaches are
less when features become stronger, that is, the good
features compensate for the weaknesses in models.
Notice that with fewer knowledge sources (e.g., us-
ing only word N-gram and prosodic information),
the CRF is able to achieve performance similar to or
even better than other methods using all the knowl-
edges sources. This may be useful when feature ex-
traction is computationally expensive.
We observe from Table 2 that there is a large
increase in error rate when evaluating on speech
recognition output. This happens in part because
word information is inaccurate in the recognition
output, thus impacting the effectiveness of the LMs
and lexical features. The prosody model is also af-
fected, since the alignment of incorrect words to the
speech is imperfect, thereby degrading prosodic fea-
ture extraction. However, the prosody model is more
robust to recognition errors than textual knowledge,
because of its lesser dependence on word identity.
The results show that the CRF suffers most from the
recognition errors. By focusing on the results when
only word N-gram information is used, we can see
the effect of word errors on the models. The SU
detection error rate increases more in the STT con-
dition for the CRF model than for the other models,
suggesting that the discriminative CRF model suf-
fers more from the mismatch between the training
(using the reference transcription) and the test con-
dition (features obtained from the errorful words).
We also notice from the CTS results that when
only word N-gram information is used (with or
without combining with prosodic information), the
HMM is superior to the Maxent; only when various
additional textual features are included in the fea-
ture set does Maxent show its strength compared to
456
Table 3: Broadcast news SU detection results reported using the NIST SU error rate (%) and the boundary-
based error rate (% in parentheses) using the HMM, Maxent, and CRF individually and in combination. The
baseline error rate is 100% for the NIST SU error rate and 7.2% for the boundary-based metric.
Broadcast News
HMM Maxent CRF
word N-gram 80.44 (5.83) 81.30 (5.89) 74.99 (5.43)
REF word N-gram + prosody 59.81 (4.33) 59.69 (4.33) 54.92 (3.98)
all features 48.72 (3.53) 48.61 (3.52) 47.92 (3.47)
Vote: 46.28 (3.35)
word N-gram 84.71 (6.14) 86.13 (6.24) 80.50 (5.83)
STT word N-gram + prosody 64.58 (4.68) 63.16 (4.58) 59.52 (4.31)
all features 55.37 (4.01) 56.51 (4.10) 55.37 (4.01)
Vote: 54.29 (3.93)
the HMM, highlighting the benefit of Maxent?s han-
dling of the textual features.
The combined result (using majority vote) of the
three approaches in Table 2 is superior to any model
alone (the improvement is not significant though).
Previously, it was found that the Maxent and HMM
posteriors combine well because the two approaches
have different error patterns (Liu et al, 2004). For
example, Maxent yields fewer insertion errors than
HMM because of its reliance on different knowledge
sources. The toolkit we use for the implementation
of the CRF does not generate a posterior probabil-
ity for a sequence; therefore, we do not combine
the system output via posterior probability interpola-
tion, which is expected to yield better performance.
4.2 BN Results
Table 3 shows the SU detection results for BN. Sim-
ilar to the patterns found for the CTS data, the CRF
consistently outperforms the HMM and Maxent, ex-
cept on the STT condition when all the features are
used. The CRF yields relatively less gain over the
other approaches on BN than on CTS. One possible
reason for this difference is that there is more train-
ing data for the CTS task, and both the CRF and
Maxent approaches require a relatively larger train-
ing set than the HMM. Overall the degradation on
the STT condition for BN is smaller than on CTS.
This can be easily explained by the difference in
word error rates, 22.9% on CTS and 12.1% on BN.
Finally, the vote among the three approaches outper-
forms any model on both the REF and STT condi-
tions, and the gain from voting is larger for BN than
CTS.
Comparing Table 2 and Table 3, we find that the
NIST SU error rate on BN is generally higher than
on CTS. This is partly because the NIST error rate
is measured as the percentage of errors per refer-
ence SU, and the number of SUs in CTS is much
larger than for BN, giving a large denominator and
a relatively lower error rate for the same number of
boundary detection errors. Another reason is that the
training set is smaller for BN than for CTS. Finally,
the two genres differ significantly: CTS has the ad-
vantage of the frequent backchannels and first per-
son pronouns that provide good cues for SU detec-
tion. When the boundary-based classification metric
is used (results in parentheses), the SU error rate is
lower on BN than on CTS; however, it should also
be noted that the baseline error rate (i.e., the priors
of the SUs) is lower on BN than CTS.
5 Conclusion and Future Work
Finding sentence boundaries in speech transcrip-
tions is important for improving readability and aid-
ing downstream language processing modules. In
this paper, prosodic and textual knowledge sources
are integrated for detecting sentence boundaries in
speech. We have shown that a discriminatively
trained CRF model is a competitive approach for
the sentence boundary detection task. The CRF
combines the advantages of being discriminatively
trained and able to model the entire sequence, and
so it outperforms the HMM and Maxent approaches
457
consistently across various testing conditions. The
CRF takes longer to train than the HMM and Max-
ent models, especially when the number of features
becomes large; the HMM requires the least training
time of all approaches. We also find that as more fea-
tures are used, the differences among the modeling
approaches decrease. We have explored different ap-
proaches to modeling various knowledge sources in
an attempt to achieve good performance for sentence
boundary detection. Note that we have not fully op-
timized each modeling approach. For example, for
the HMM, using discriminative training methods is
likely to improve system performance, but possibly
at a cost of reducing the accuracy of the combined
system.
In future work, we will examine the effect of
Viterbi decoding versus forward-backward decoding
for the CRF approach, since the latter better matches
the classification accuracy metric. To improve SU
detection results on the STT condition, we plan to
investigate approaches that model recognition un-
certainty in order to mitigate the effect of word er-
rors. Another future direction is to investigate how
to effectively incorporate prosodic features more di-
rectly in the Maxent or CRF framework, rather than
using a separate prosody model and then binning the
resulting posterior probabilities.
Important ongoing work includes investigating
the impact of SU detection on downstream language
processing modules, such as parsing. For these ap-
plications, generating probabilistic SU decisions is
crucial since that information can be more effec-
tively used by subsequent modules.
6 Acknowledgments
The authors thank the anonymous reviewers for their valu-
able comments, and Andrew McCallum and Aron Culotta at
the University of Massachusetts and Fernando Pereira at the
University of Pennsylvania for their assistance with their CRF
toolkit. This work has been supported by DARPA under
contract MDA972-02-C-0038, NSF-STIMULATE under IRI-
9619921, NSF KDI BCS-9980054, and ARDA under contract
MDA904-03-C-1788. Distribution is unlimited. Any opinions
expressed in this paper are those of the authors and do not reflect
the funding agencies. Part of the work was carried out while the
last author was on leave from Purdue University and at NSF.
References
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for smooth-
ing maximum entropy models. Technical report, Carnegie
Mellon University.
H. Christensen, Y. Gotoh, and S. Renal. 2001. Punctuation an-
notation using statistical prosody models. In ISCA Workshop
on Prosody in Speech Recognition and Understanding.
Y. Gotoh and S. Renals. 2000. Sentence boundary detection in
broadcast speech transcripts. In Proceedings of ISCA Work-
shop: Automatic Speech Recognition: Challenges for the
New Millennium ASR-2000, pages 228?235.
J. Huang and G. Zweig. 2002. Maximum entropy model for
punctuation annotation from speech. In Proceedings of the
International Conference on Spoken Language Processing,
pages 917?920.
J. Kim and P. C. Woodland. 2001. The use of prosody in a com-
bined system for punctuation generation and speech recogni-
tion. In Proceedings of the European Conference on Speech
Communication and Technology, pages 2757?2760.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random field: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2004. Com-
paring and combining generative and posterior probability
models: Some advances in sentence boundary detection in
speech. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich Transcription
of Speech. Ph.D. thesis, Purdue University.
A. McCallum and W. Li. 2003. Early results for named en-
tity recognition with conditional random fields. In Proceed-
ings of the Conference on Computational Natural Language
Learning.
A. McCallum. 2002. Mallet: A machine learning for language
toolkit. http://mallet.cs.umass.edu.
NIST-RT03F. 2003. RT-03F workshop agenda and
presentations. http://www.nist.gov/speech/tests/rt/rt2003/
fall/presentations/, November.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proceedings of Human Language Technol-
ogy Conference / North American Chapter of the Association
for Computational Linguistics annual meeting.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur. 2000.
Prosody-based automatic segmentation of speech into sen-
tences and topics. Speech Communication, pages 127?154.
A. Stolcke and E. Shriberg. 1996. Automatic linguistic seg-
mentation of conversational speech. In Proceedings of the
International Conference on Spoken Language Processing,
pages 1005?1008.
A. Stolcke, H. Franco, R. Gadde, M. Graciarena, K. Pre-
coda, A. Venkataraman, D. Vergyri, W. Wang, and
J. Zheng. 2003. Speech-to-text research at SRI-
ICSI-UW. http://www.nist.gov/speech/tests/rt/rt2003/
spring/presentations/index.htm.
S. Strassel, 2003. Simple Metadata Annotation Specification
V5.0. Linguistic Data Consortium.
458
A Question Answering System Developed as a Project in a 
Natural Language Processing Course* 
W. Wang, J. Auer, R. Parasuraman, I. Zubarev, D. Brandyberry, and M. P. Harpm 
Purdue University 
West Lafayette, IN 47907 
{wang28,jauer,pram,dbrandyb,harper}@ecn.purdue.edu an  zubarevi@cs.purdue.edu 
Abstract 
This paper describes the Question Answering Sys- 
tem constructed uring a one semester graduate- 
level course on Natural Language Processing (NLP). 
We hypothesized that by using a combination ofsyn- 
tactic and semantic features and machine learning 
techniques, we could improve the accuracy of ques- 
tion answering on the test set of the Remedia corpus 
over the reported levels. The approach, although 
novel, was not entirely successful in the time frame 
of the course. 
1 Introduction 
This paper describes a preliminary reading com- 
prehension system constructed as a semester-long 
project for a natural language processing course. 
This was the first exposure to this material for 
all but one student, and so much of the semester 
was spent learning about and constructing the tools 
that would be needed to attack this comprehen- 
sive problem. The course was structured around 
the project of building a question answering system 
following the HumSent evaluation as used by the 
Deep Read system (Hirschman eta\]., 1999). The 
Deep Read reading comprehension prototype system 
(Hirschman et al, 1999) achieves a level of 36% of 
the answers correct using a bag-of-words approach 
together with limited linguistic processing. Since the 
average number of sentences per passage is 19.41, 
this performance is much better than chance (i.e., 
5%). We hypothesized that by using a combina- 
tion of syntactic and semantic features and machine 
learning techniques, we could improve the accuracy 
of question answering on the test set of the Remedia 
corpus over these reported levels. 
2 System Description 
The overall architecture of our system is depicted 
in Figure 1. The story sentences and its five ques- 
tions (who, what, where, when, and why) are first 
preprocessed and tagged by the Brill part-of-speech 
* We wou ld  l ike to  thank  the  Deep Read group for giving us 
" access  to  the i r  test bed. 
P la in  Text  ( Story and Questions ) 
)i . . . . .  8__~'!P?_s__!,g_ ~L  . . . . . . .  
Tagged Text  
2 i Name Identification 
Propernoun Identified 
3 Tagged Text 
J r _  . . . . . . . . . . . . . . . . . . . . . . . . .  T . . . . . . . . . . .  
Word Lex\]cat Lexica$ and Ro le  ? In fonmat io r  L 
. . . . . . . .  Labe l ln fo rmal J~1 ~ . . . .  
. . . .  ~ Lex icon  . . . . . . .  ~ . . . . . . . . . . . . . .  
. . . . . . . .  L . . . . . . . . . . . . . . . . . .  
Wordnel . . . . . . . . . . .  Grammar Par t ia l  ~ : P ronoun 
Reso lu t ion  . . . . . . .  - . . . . . . . . . . . .  Rules Parser 4- - -  . . . . . . . . . . . . . . . .  - 
Gramnrmr  . . . . . .  ~ Pronouns  . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Resolved 
Sentence-to-Question 
. I, Compar i son  i 
. - -~=: :  . . . . .  ~__  . .  ~ . . . . . . .  ~ 
" Ru le -Based  " Neuro -  : Neura l  Genet ic  
Classifier .' Fuzzy  Net .  Network Algorithm . '  
? ? .ANS ANS ANS ~ - -  ~"  ANS 
Vot ing  
. . . . . . . . . . . . . . . . . . . . . . . . . .  ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Answer with i 
Highest Scores  i 
Figure 1: The architecture for our question answer- 
ing system. 
(POS) tagger distributed with the Deep Read sys- 
tem. This tagged text is then passed to the Name 
Identification Module, which updates the tags of 
named entities with semantic information and gen- 
der when appropriate." The Partial Parser Mod- 
ule then takes this updated text and breaks it into 
phrases while attempting to \]exically disambiguate 
the text. The Pronoun Resolution Module is con- 
sulted by the parser in order to resolve pronouns be- 
fore passing partially parsed sentences and questions 
to the Sentence-to-Question Comparison Module. 
The Comparison Module determines how strongly 
the phrases of a sentence are related to those of a 
question, and this information is passed to several 
28 
modules which attempt to learn which features of 
the comparison are the most important for identify- 
ing whether a sentence is a strong answer candidate. 
We intended to set up a voting scheme among vari- 
ous modules; however, this part of the work has not 
been completed (as indicated by the dashed lines). 
Our system, like Deep Read, uses as the develop- 
ment set 28 stories from grade 2 and 27 from grade 
5, each with five short answer questions (who, what, 
when, where, and why), and 60 stories with ques- 
tions from grades 3 and 4 for testing 1. We will refer 
to the development and testing data as the Remedia 
corpus. The following example shows the informa- 
tion added to a plain text sentence as it progresses 
through each module of the system we have created. 
Each module-is described in more detail in the fol- 
lowing sections. 
2.1 Name Ident i f i ca t ion  Modu le  
The Name Identification Module expects as in- 
put a file that has been tagged by the Brill 
tagger distributed with the Deep Read system. 
The most important named entities in the Re- 
media corpus are the names of people and the 
names of places. To distinguish between these 
two types, we created dictionaries for names of 
people and names of places. The first and last 
name dictionaries were derived from the files 
at http ://www. census, gov/genealogy/names/. 
First names had an associated gender feature; 
names that were either male or female included 
gender frequency. Place names were extracted from 
atlases and other references, and included names 
of countries, major cities and capital cities, major 
attractions and parks, continents, etc. WordNet 
was also consulted because of its coverage of place 
names. There are 5,165 first name entries, 88,798 
last name entries, and 1,086 place name entries in 
the dictionaries used by this module. 
The module looks up possible names to decide 
whether a word is a person's name or a location. If it 
cannot find the word in the dictionaries, it then looks 
at the POS tags provided in the input file to deter- 
mine whether or not it is a propernoun. Heuristics 
(e.g., looking at titles like Mr. or word endings like 
rifle) are then applied to decide the semantic type 
of the propernoun, and if the type cannot be deter- 
mined, the module returns both person and location 
as its type. The accuracy of the Name Identification 
Module on the testing set was 79.6%. The accuracy 
adjusted to take into account incorrect tagging was 
83.6%. 
There were differences between the Deep Read electronic 
version of the passages and the Remedia  published passages. 
We used the electronic passages. 
29 
Rm3-5  (... The club is fo r  boys who are under 12 years Bid.) They are called Cub ScButs. 
(Answer to Question 1 ) 
: POS tagging 
They are called i Cub Scouts 
PO_S _-.~_P.R. p_: P_O_ S_=_~V_B P" ' P_OS='__V_BN'_~ i_POS='N_N_ P21 POS_--. :NN__S" 
Name Identification 
They are ~ i called Cub Scouts' 
Pos.'t~s" : 
POS="PRP"  POS='VBP"  " POS='VBN"  PROP~PE. '~t  
They 
TYPE=NP 
ID=I 
LABEL=subject 
BASE=they 
AGR=3p 
GENDER=male 
SEM_TYPE=per~ .z~, 
are  
TYPE=VP 
ID=I 
LABEL=auK 
BASE=be 
AGR~,~p 
TENSE=present 
VOtCE=ac~,'e 
SEM TYPE=tan  
They are 
"P#PE=NP TYP~=VP 
11:>-i io=1 
IL'~'BE L=subjec~ LABEL~ 
~',SElthey BAS.~Mce 
AGR=3p , AGR.~3 p 
GENDER=nut  ~1 : TENBE=pQI=~I~ 
BEM TYPE=per  ' VOI CE,=active 
PRONRE F=boys ' = _S E_M TYF~i=b~-pu~.E 
They  are  
TYPE=NP TYPE.V'P 
ID=I IB=~ 
LABEL=subject LABEL=am 
BASE-be 
BASE=boy AGR=3p 
AGFt=3g TE NSIE.pms4mt 
GENDER=male voICE~ctive 
? SEM_ ~PE=~.~ers~ __. SE M "i'~f P _Ept?e-p~_ 
Who 
: ~ 'V~?-  . . . . . . . . .  
ID=I 
, LABEL=subject 
= BABE=who 
: AGR=3p 
: GENDER=male 
Initial Partial Parsing 
called \] 
TYPE.VP ! 
ID..2 
Lt~,B EL .  rnvb 
BASE.,call ! 
AGR.3p 
TENSE=pastp i 
VOICE=gas=ire 
SEM_TYPEiequat el 
Pronoun Resolution 
Y 
cal led 
TYPE=VP 
~O.2 
LABEL.nwb 
BASE=call 
AGFI=3p 
TENSE=pastp 
VOICE=pass)re 
L SEM TYPE=equate 
Updating Features 
called ! 
TYPE=VP 
1\[:),=2 
LABEL=mvb i 
BASE~an 
AGRB3p 
TENSE~p~tp  
VOICE=passive 
_ SEU='r~'Pe=~q2~:  
Bracketed Sentence 
are  
TypE~.VP 
JD.I 
LABEL~vb ! 
BABE.be 
AGR,,3p i 
TENSE=,preserd 
YOICE~ct i~ , 
? s E_M , .TY~,~ 
Bracketed Question 1 
Cub Scouts 
TYPE=NP 
1\[:)=2 
LABEL=object 
BASE =CubScouts 
AGR=3p 
GENDER=male 
SEM ~__PE-~_erson 
Cub Scouts 
TYPE=NP 
ID=2 
LAeEL,=objoct 
BASE=CubScouts 
AGR=3p ! 
GENDER.mate , 
_, SEM-_UPE=~__~ 
Cub Scouts ' 
TYPE=NP 
ID=2 
LABEL=object 
BASE=CubSco~t= ' 
AGR=3p 
GENDER=mak~ ; 
Cub  Scouts 
TYPE=NP 
ID=2 
LABEL=object 
8ABE=CubScout= i 
AGR=3p 
GENDER=male 
. . . .  SE_M= D'~,E:--p. ~. 
Figure 2: Processing an example sentence for match- 
ing with a question in our system. 
2.2 Par t ia l  Parser  Modu le  
The Partial Parser Module follows sequentially af- 
ter the Name Identification Module. The input is 
the set of story sentences and questions, such that 
the words in each are tagged with POS tags and 
the names are marked with type and gender infor- 
mation. Initially pronouns have not been resolved; 
the partial parser provides segmented text with rich 
lexical information and role labels directly to the 
Pronoun Resolution Modffle. After pronoun reso- 
lution, the segmented text with resolved pronouns 
is returned to the partial parser for the parser to 
update the feature values corresponding to the pro- 
nouns. Finally, the partial parser provides bracketed 
text to the Comparison Module, which extracts fea- 
tures that will be used to construct modules for an- 
swering questions. 
The Partial Parser Module utilizes information 
in a lexicon and a grammar to provide the partial 
parses. The lexicon and the parser will be detailed 
in the next two subsections. 
2.2.1 The  Lex icon 
There were two methods we used to construct he 
lexicon: open lexicon, which includes all words 
from the development set alng with all determiners, 
pronouns, prepositions, particles, and conjunctions 
(these words are essential to achieving ood sentence 
segmentation), and closed lexicon,  which includes 
all of the development and testing words 2. We con- 
structed the closed lexicon with the benefit of the 
development corpus only (i.e., we did not consult the 
test materials to design the entries). To improve cov- 
erage in the case of the open lexicon, we constructed 
a module for obtaining features fbr words that do 
not appear in the development set (unknown words) 
that interfaces with WordNet to determine a word's 
base/stem, semantic type, and synonyms. When an 
unknown word has multiple senses, we have opted to 
choose the first sense because WordNet orders senses 
by frequency of use. Ignoring numbers, there are 
1,999 unique words in the development set of the 
Remedia corpus, and 2,067 in the testing data, of 
which 1,008 do not appear in the development set. 
Overall, there are 3,007 unique words across both 
training and testing. 
One of our hypotheses was that by creating a lex- 
icon with a rich set of features, we would improve 
the accuracy of question answering. The entries in 
the lexicon were constructed using the conventions 
adopted for the Parsec parser (Harper and Helzer- 
man, 1995; Harper et al, 1995; Harper et al, 2000). 
Each word entry contains information about its root 
word (if there is one), its lexical category (or cate- 
gories) along with a corresponding set of allowable 
features and their corresponding values. Lexical cat- 
egories include noun, verb, pronoun, propernoun, 
adjective, adverb, preposition, particle, conjunction, 
determiner, cardinal, ordinal, predeterminer, noun 
modifier, and month. Feature types used in the 
lexicon include subcat,  gender, agr, case, vtype 
(e.g., progressive), mood, gap, inver ted ,  voice,  
behav ior  (e.g., mass), type (e.g., interrogative, rel- 
ative), semtype, and con j type  (e.g., noun-type, 
verb-type, etc.). We hypothesized that semtype 
should play a significant role in improving question 
answering performance, but the choice of semantic 
granularity is a difficult problem. We chose to keep 
the number of semantic values relatively small. By 
using the lexicographers' files in WordNet to group 
the semantic values, we selected 25 possible seman- 
tic values for the nouns and 15 for the verbs. A 
2Initially, we created the closed lexicon because this list 
of words was in the Deep Read materials. Once we spotted 
that the list contained words not in the development material, 
we kept it as an alternative to see how important full lexical 
knowledge would be for answering questions. 
30 
script was created to semi-automate he construc- 
tion of the lexicon from information extracted from 
previously existing dictionaries and from WordNet. 
2.2.2 The  Par t ia l  Parser  
The parser segments each sentence into either a noun 
phrase (NP), a verb phrase (VP), or a prepositional 
phrase (PP), each with various feature sets. NPs 
have the feature types: Base (the root word of the 
head word of the NP), AGR (number/person i for- 
mation), SemType (the semtype of the root form in 
the lexicon, e.g., person, object, event, artifact, or- 
ganization), Label (the role type of the word in the 
sentence, e.g., subject), and Gender. Verb phrases 
(VPs) have the feature types: Base, AGR, SemType 
(the semtype of the root form in the lexicon, e.g., 
contact, act, possession), Tense (e.g., present, past), 
and Voice. Prepositional phrases (PPs) have the 
feature types: Prep (the root form of the preposition 
word), SemType (the semtype of the root form in the 
lexicon, e.g., at-loc, at-time), Need (the object of the 
preposition), and NeedSemType (the semtype of the 
object of the preposition). Feature values are as- 
signed using the lexicon, Pronoun Resolution Mod- 
ule, and grammar ules. 
We implemented a bottom-up partial parser to 
segment each sentence into syntactic subparts. The 
grammar used in the bottom-up arser is shown be- 
low: 
1. NP -+ DET ADJ+ NOUN+ 
2. NP ~ DET NOUN 
3. NP ~ ADJ PROPERNOUN+ 
4. VP ~ (AUX-VERB) MAIN-VERB 
5. PP --~ ADV 
6. PP ~ ADJ (PRED) 
7. PP ~ PREP NP 
At the outset, the parser checks whether there are 
any punctuation marks in the sentence, with corn- 
mass and periods being the most helpful. A comma 
is used in two ways in the Remedia corpus: it acts 
as a signal for the conjunction of a group of nouns 
or propernouns, or it acts as punctuation signalling 
an auxiliary phrase (usually a PP) or sentence. In 
the NP conjunction case, the parser groups the con- 
joined nouns or propernouns together as a plural NP. 
In the second case, the sentence is partially parsed. 
The partial parser operates in a bottom-up fashion 
taking as input a POS:tagged and name-identified 
sentence and matching it to the right-hand side of 
the grammar ules. Starting from the beginning of 
the sentence or auxiliary phrase (or sentence), the 
parser looks for the POS tags of the words, trans- 
forming the POS tags into corresponding lexical cat- 
egories and tries to match the RHS of the rules. 
Phrases are maintained on an agenda until they are 
finalized. 
NPs often require merging sincesome consecutive 
NPs form a single multi-word token (i.e., multi-word 
names and conjunctions). An NP that results from 
merging two tokens into a single multi-word token 
has its Base as the rootword of the combined token, 
and AGR and SemType features are updated according 
to the information retrieved from the lexicon based 
on the multi-word token. In the case of an NP con- 
junction, the Base is the union of the Base of each 
NP, AGR is set to 3p, and SemType is assigned as that 
of the head word of the merged NP. The rule for find- 
ing the head word of an NP is: find the F IRST  con- 
secutive noun (propernoun) group in the NP, then 
the LAST  noun (propernoun) in this group is de- 
fined as the head word of the NP. 
The partial parser performs word-sense disam- 
biguation as it parses. Words such as Washington 
have multiple_semtype values in the lexicon for one 
lexical category. The following are rules for word- 
sense disambiguation used by the parser: 
? NP  plus VP  rules for word-sense disambiguation: 
If there are verbs such as name, call, or be, 
which have the semtype of equate, then the NPs  
that precede and follow the VP  have the same 
semtype. 
If a noun is the object of a verb, then the subcat  
feature value of the verb can be used to disam- 
biguate its word sense (e.g., take generally has 
the subcat  of obj+time). 
? PP  rules for word-sense disambiguation: 
For some nouns (propernouns) which are the 
object of a preposition, the intersection of the 
semtype value sets of the preposition word and 
its object determines their semtype. 
? NPs  in the date line of each passage are all ei- 
ther dates or places with the typical order be- 
ing place then time. For example, in (WASH- 
INGTON, June, 1989), Washington is assigned 
semtype of location rather than person. 
To process unknown words (the 1,008 words in the 
testing set that don't appear in the development set) 
in the case of the open lexicon, WordNet is used to 
assign the semtype feature for nouns and verbs, the 
AGR feature for verbs can be obtained in part from 
the POS tag, and AGR for unknown noun words can 
be determined when they are used as the subject of 
a sentence. For the closed lexicon, the only unknown 
words are numbers. If a number is a four-digit num- 
ber starting with 16 to 19 or is followed by A.D or 
B.C. then generally it is a year, so its semtype is de- 
fined as time. Other numbers tend to be modifiers 
or predicates and have the semtype of num. 
2.3 P ronoun Reso lu t ion  Modu le  
A pronoun resolution module was developed using 
the rules given in Allen's text (Allen, 1995) along 
with other rules described in the work of Hobbs 
(Hobbs, 1979). The module takes as input the 
feature-augmented and segmented text provided by 
the partial parser. Hence, the words are marked 
31 
with lexical (including gender) and semantic feature 
information, and the phrase structure is also avail- 
able. After the input file is provided by the Par- 
tial Parser Module, the Pronoun Resolution Module 
searches for the pronouns by looking through the 
NPs identified by the partial parser. Candidate an- 
tecedents are identified and a comparison of the fea- 
tures is made between the pronoun and the possible 
antecedent. The phrase that passes the most rule 
filters is chosen as the antecedent. First and second 
person pronouns are handled by using default val- 
ues (i.e., writer and reader). If the system fails to 
arrive at an antecedent, he pronoun is marked as 
non-referential, which is often the case for pronouns 
like it or they. Some of the most useful rules are 
listed below: 
? Reflexives must refer to an antecedent in the same 
sentence. For simplicity, we chose the closest 
noun preceding the pronoun in the sentence with 
matching Gender, AGR, and SemType. 
? Two NPs that co-refer must agree in AGR, Gender, 
and SemType (e.g., person, location). Since, in 
many cases the gender cannot be determined, this 
information was used only when available. 
? A subject was preferred over the object when the 
pronoun occurred as the subject in a sentence. 
? When it occurs in the beginning of a paragraph, 
it is considered non-referential. 
? We prefer a global entity (the first named entity in 
a paragraph) when there is a feature match. In the 
absence of such, we prefer the closest propernoun 
preceding the pronoun with a feature match. If 
that fails, we prefer the closest preceding noun or 
pronoun with a feature match. 
The accuracy of our pronoun resolution module 
on the training corpus was 79.5% for grade 2 and 
79.4% for grade 5. On testing, it was 81.33% for 
grade 3 and 80.39% for grade 4. The overall accu- 
racy of this module on both the testing and train- 
ing corpus was 80.17%. This was an improvement 
over the baseline Deep Read coreference module 
which achieved a 51.61% accuracy on training and 
a 50.91% accuracy on testing, giving an overall ac- 
curacy of 51.26%. This accuracy was determined 
based on Professor Harper's manual pronoun reso- 
lution of both the training and testing set (the per- 
fect coreference information was not included in the 
distribution of the Deep Read system). 
2.4 Sentence- to -Quest ion  Compar i son  
Modu le  
The Sentence-to-Question Comparison Module 
takes as input a set of tagged stories, for which 
phrase types and features have been identified. 
The semantic and syntactic information is coded as 
shown in Figure 2 (using XML tags). A mechanism 
to quantify a qualitative comparison of questions 
and sentences has been developed. The comparison 
:.'.' 
provides data about how questions compare to their 
answers and how questions compare to non-answers. 
The classification of answers and non-answers i im- 
plemented by using feature comparison vectors of 
phrase-to-phrase comparisons in questions and po- 
tential answer sentences. 
A comparison is made using phrase-to-phrase 
comparisons between each sentence and each ques- 
tion in a passage. In particular, NP-to-NP, VP-to- 
VP, PP-to-PP, and NP-to-PP comparisons are made 
between each sentence and each of the five questions. 
These comparisons are stored for each sentence in 
the following arrays. Note that in these arrays Q 
varies from 1 to 5, signifying the question that the 
sentence matches. F varies over the features for the 
phrase match. 
CN\[Q\]\[F\] Comparison of NP features (F = I{Base, 
h6R, and SemType}D between question 
Q and the sentence. 
CV\[Q\]\[F\] Comparison of VP features (F = i{Base, 
AGR, SemType, Tense}\[) between 
question Q and the sentence. 
CP\[Q\]\[F\] Comparison of PP features (F = \[{NeedBase, 
Prep, PPSemType, NeedSemType}\[) 
between question Q and the sentence. 
CPN\[Q\]\[F\] Comparison of PP features in sentence 
to NP features in question Q. Here F=2,  
comparing lWeedBase and Base, and 
NeedSemType and SemType. 
Values for these comparison matrices were calcu- 
lated for each sentence by comparing the features of 
each phrase type in the sentence to features of the 
indicated phrase types in each of the five questions. 
The individual matrix values describe the compari- 
son of the best match between a sentence and a ques- 
tion for NP-to-NP (the three feature match scores 
for the best matching NP pair of the sentence and 
question Q are stored in CN\[Q\]), VP-to-VP (stored 
in CV\[Q\]), PP-to-PP (stored in CP\[Q\]), and PP-to- 
NP (store in CPN\[Q\]). Selecting the phrase compar- 
ison vector for a phrase type that best matches a 
sentence phrase to a question phrase was chosen as 
a heuristic to avoid placing more importance on a 
sentence only because it contains more information. 
Comparisons between features were calculated us- 
ing the following equations. The first is used when 
comparing features such as Base, NeedBase, and 
Prep, where a partial match must be quantified. 
The second is used when comparing features uch as 
SemType, AGR, and Tense where only exact matches 
make sense. 
1 if Strl = Str2 
rain len~th(Strl,Str2) length(Sth)  # length(Str2) 
c = max length(Strl,Str2) A(Strl 6 Str2 V Str2 6 Strl ) 
0 if Strl -~ Str2 
1 if Strl = Str2 
c = 0 if Strl ~- Str2 
32 
The matrices for the development set were pro- 
vided to the algorithms in the Answer Module for 
training the component answer classifiers. The ma- 
trices for the testing set were also passed to the al- 
gorithms for testing. Additionally, specific informa- 
tion about the feature values for each sentence was 
passed to the Answer Module. 
2.5 Answer  Modu les  
Several methods were developed in parallel in an 
attempt o learn the features that were central to 
identifying the sentence from a story that correctly 
answer a question. These methods are described in 
the following subsections. Due to time constraints, 
the evaluations of these Answer Modules were car- 
ried out with a closed lexicon and perfect pronoun 
resolution. 
2.5.1 A Neuro--Fuzzy Network  Classif ier 
An Adaptive Network-based Fuzzy Inference System 
(ANFIS) (Jang, 1993) from the Matlab Fuzzy Logic 
Toolbox was used as one method to resolve the story 
questions. A separate network was trained for each 
question type in an attempt o make the networks 
learn relationships between phrases that classify an- 
swer sentences and non-answer sentences differently. 
ANFIS has the ability to learn complex relationships 
between its input variables. It was expected that 
by learning the relationships in the training set, the 
resolution of questions could be performed on the 
testing set. 
For ANFIS, the set of sentence-question pairs was 
divided into five groups according to question type. 
Currently the implementation of ANFIS on Matlab 
is restricted to 4 inputs. Hence, we needed to devise 
a way to aggregate the feature comparison informa- 
tion for each comparison vector. The comparison 
vectors for each phrase-to-phrase comparison were 
reduced to a single number for each comparison pair 
(i.e., NP-NP, VP-VP, PP-PP, NP-PP). This reduc- 
tion was performed by multiplying the vector values 
by a normalized weighting constant for the feature 
values (e.g., NP-comparison = (Base weight)*(Base 
comparison value) + (AGR weight)*(AGR compari- 
son value) + (SemType weight)*(SemType compari- 
son value), with the weights umming to 1). In most 
cases that a match is found, the comparison values 
are 1 (exact match). So weights were chosen that al- 
lowed the ANFIS to tell'something about the match 
characteristics (e.g., if the AGR weight is 0.15 and 
the SemType weight is 0.1, and the NP-comparison 
value was 0.25, it can be concluded that the NP 
that matched best between in the sentence-question 
pair had the same AGR and SemType features). The 
aggregation weights were chosen so that all com- 
binations of exact matches on features would have 
unique values and the magnitude of the weights were 
chosen based on the belief that the higher weighted 
features contribute more useful information. The 
weights, ordered to correspond to the features in the 
table on the previous page are: (.55, .15, .3) for CN, 
(.55, .1, .22, .13) for CV, (.55, .15, .2, .1) for CP, and 
(.55, .45) for CPN. 
ANFIS was trained using the update on the de- 
velopment set provided by the Sentence-to-Question 
Comparison Module as described above. During 
testing, the data, provided by the Comparison Mod- 
ule and updated as described above, is used as input 
to ANFIS. The output is a confidence value that de- 
scribes the likelihood of a sentence being a answer. 
Every sentence is compared with every question in 
ANFIS, and then within question, the sentences are 
ranked by the likelihood that they are a question's 
answer. 
The accuracy of the best classifier produced with 
ANFIS was quite poor. In the grade 3 set, we 
achieved an accuracy of 13.33% on who questions, 
6.67% on what questions, 0% on where questions, 
6.67% on when questions, and 3.33% on why ques- 
tions. In the grade 4 set, we achieved an accuracy of 
3.54% on who questions, 10.34% on what questions, 
10.34% on where questions, 0% on when questions, 
and 6.9% on why questions. Although the best rank- 
ing sentence produced poor accuracy results on the 
testing set, with some additional knowledge the top- 
ranking incorrect answers may be able to be elimi- 
nated. The plots in Figure 3 display the number of 
times the answer sentence was assigned a particular 
rank by ANFIS. The rank of the correct sentence 
tends to be in the top 10 fairly often for most ques- 
tion types. This rank tendency is most noticeable 
for who, what and when questions, but it is also 
present for where questions. The rank distribution 
for why questions appears to be random, which is 
consistent with our belief that they require a deeper 
analysis than would be possible with simple feature 
comparisons. 
2.5.2 A Neura l  Network  Classi f ier  
Like ANFIS, this module uses a neural network, but 
it has a different opology and uses an extended fea- 
ture set. The nn (Neureka) neural network sim- 
ulation system (Mat, 1998) was used to create a 
multi-layer (one hidden layer) back-propagation net- 
work. A single training/testing instance was gener- 
ated from each story sentence. The network contains 
an input layer with two groups of features. The sen- 
tence/question feature vectors that compare a sen- 
tence to each of the five story questions comprise the 
first group. Sentence features that are independent 
of the questions, i.e., contains a location, contains a 
time/date, and contains a human, comprise the sec- 
ond group. The hidden layer contains a number of 
nodes that was experimentally varied to achieve best 
performance. The output layer contains five nodes, 
each of which has a binary outpht value which indi- 
cates whether or not the sentence is the answer to 
the corresponding question (i.e., question 1 through 
5). 
Several training trials were performed to deter- 
mine the opt imum parameters for the network. We 
trained using various subsets of the full input fea- 
ture set since some features could be detrimental to 
creating a good classifier. However, in the end, the 
full set of features performed better than or equiva- 
lently to the various subsets. Increasing the number 
of hidden nodes can often improve the accuracy of 
the network because it can learn more complex re- 
lationships; however, this did not help much in the 
current domain, and so the number of hidden nodes 
was set to 16. For this domain, there are many more 
sentences that are not the answer to a question than 
that are. An effort was made to artificially change 
this distribution by replicating the answer sentences 
in the training set; however, no additional accuracy 
was gained by this experimentation. Finally, we cre- 
ated a neural network for each question type as in 
ANFIS; however, these small networks had lower ac- 
curacy than the single network approach. 
The overall test set accuracy of the best neural 
network classifier was 14%. In the grade 3 set, we 
achieved an accuracy of 30% on who questions, 0% 
on what questions, 23.3% on when questions, 13.3% 
on where questions, and 3.3% on why questions. In 
the grade 4 set, we achieved an accuracy of 17.2% 
on who questions, 10.3% on what questions, 23.6% 
on when questions, 10.3% on where questions, and 
3.4% on why questions. 
2.5.3 A Ru le -based  Classi f ier  based  on C5.0 
We attempted to learn rules for filtering out sen- 
tences that are not good candidates as answers to 
questions using C5.0 (Rul, 1999). First we ex- 
tracted information from the sentence-to-question 
correspondence data ignoring the comparison values 
to make the input C5.0-compatible, and produced 
five different files (one for each question type). These 
files were then fed to C5.0; however, the program did 
not produce a useful tree. The problem may have 
been that most sentences in the passages are nega- 
tive instances of answers to questions. 
2.5.4 GAS 
GAS (Jelasity and Dombi, 1998) is a steady genetic 
algorithm with subpopulation support. It is capa- 
ble of optimizing functions with a high number of 
local optima. The initial parameters were set theo- 
retically. In the current matching problem, because 
the number of local opt ima can be high due to the 
coarse level of sentence information (there can be 
several sentence candidates with very close scores), 
this algorithm is preferred over other common ge- 
netic algorithms. This algorithm was trained on the 
training set, but due to the high noise level in the 
33 
Who Questions 
~2 '~t <-~ :~ ~.  ~:~" ; ~ .~ ~ .~ ~~ ~. ,~! -  :-::~ i~',~ . '  
. . . . . . .  , t . . . ,~  ~ :~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ,  " - :  . . . . . . . . . . . .  :~ ....... 
N-best order of sentences 
What  Questions 
 t2  
N-best order of sentences 
When Questions 
18 
F~ 6.  
~- ~ ~ 
g 4 ~, . . . . . . . . . . . . . . . . . . . . . .  ~ ~ ,~ . . . . . . . . . .  :~  ~ 
N-best order of sentences 
Where Questions 
 10 \ [ ;p~? ,~z~, : : : : : : :~ ,~, :~: :~.~:~- :~:~:~: ,~.~? . : :~f f : , :~  
N-best order of sentences 
Why Questions 
4.5 ~i~:~:~ 
0 
N-best order of sentences 
Figure 3: Correct answers ordered by ANFIS preference. 
training data, the algorithm fails to produce a win- 
ning population based on the mean square minimiza- 
tion function. 
3 A Closer Look at the Features 
After observing the question answer accuracy results 
of the above classifiers, we concluded that the fea- 
tures we extracted for the classifiers are affected by 
noise. The fact that we take into consideration only 
the top matching phrase-to-phrase matches on a spe- 
cific set of features may have contributed to this 
noisiness. To analyze the noise source of features, 
given that SemType was hypothesized to be essen- 
tial for answer candidate discrimination, we exam- 
ined those SemType values that occurred most fre- 
quently and calculated statistics on how often the 
values occurred in story sentences that are answers 
versus non-answers to the questions. We observed 
the following phenomena: 
1. For who questions, the SemType value person 
plays an important role in ?identifying answer 
sentences, since 83.64% answers have person as 
its NP SemType value, and 21.82% have it as 
its PP NeedSemType value. However, 66.83% of 
the non-answer sentences also have person as its 
NP SemType and 15.85% as its PP NeedSemType. 
Phrases with person SemType appear in most sen- 
tences, whether they are answers or not, and this 
34 
weakens its ability to act as an effective filter. 
2. For what questions, the SemType value person ap- 
pears as the NP SemType of most answer and non- 
answer sentences. The next most dominant fea- 
ture is the SemType value object, which appears 
in the NP for 29.41% of the answer sentences and 
PP NeedSemType for 15.68% of the answer sen- 
tences. Most of the other SemType values such as 
time contribute trivially to distinguishing answers 
from non-answers, as might be expected. 
3. For when questions, person appears dominant 
among NP SemType values; however, time fea- 
tures appear to be second most dominant since 
19.30% of the answer sentences have time as their 
NP SemType, and 26.32% have at-time as their PP 
SemType. N_o.te that the PP NeedSeraType and VP 
SemType appear to be less capable of guiding the 
selection of the correct answer. 
4. For where questions, location features are impor- 
tant with 24.07% answer sentences having loca- 
tion as their NP SemType value, and 20.37% hav- 
ing at-loc as their PP SemType. However, the 
distribution of values for VP SemType and PP 
NeedSemType shows no interesting patterns. 
The current raining strategy weights the NP-NP, 
VP-VP, PP-PP, and NP-PP comparisons equiva- 
lently. The above observations suggest hat training 
classifiers based on these equally weighted compar- 
isons may have prevented the detection of a clear 
class boundary, resulting in poor classification per- 
formance. Since different phrase types do not appear 
to contribute in the same way across different ques- 
tion types, it may be better to generate a rule base 
as a prefitter to assign more weight to certain phrases 
or discard others before inputting the feature vector 
into the classifier for training. 
4 Future  D i rec t ions  
As a next step, we will try to tame our feature set. 
One possibility is to use a rule-based classifier that 
is less impacted by the serious imbalance between 
negative and positive instances than C5.0 in order 
to learn more effective feature sets for answer candi- 
date discrimination corresponding to different ques- 
tion types. We could then use the classifier as a pre- 
processing filter to discard those less relevant com- 
parison vector elements before inputting them into 
the classifiers, instead of inputting comparison re- 
sults based on the complete feature sets. This should 
help to reduce noise generated by irrelevant features. 
Also, we will perform additional data analysis on the 
classification results to gain further insight into the 
noise sources. 
The classifiers we developed covered a wide range 
of approaches. To optimize the classification perfor- 
mance, we would like to implement a voting mod- 
ule to process the answer candidates from different 
classifiers. The confidence rankings of the classifiers 
would be determined f rom their corresponding an- 
swer selection accuracy in the training set, and will 
be used horizontally over the classifiers to provide 
a weighted confidence measure for each sentence, 
giving a final ordered list, where the head of the 
list is the proposed answer sentence. We propose 
to use a voting neural network to train the confi- 
dence weights on different classifiers based on differ- 
ent question types, since we also want to explore the 
relationship of classifier performance with question 
types. We believe this voting scheme will optimize 
the bagging of different classifiers and improve the 
hypothesis accuracy. 
References  
J. Allen. 1995. Natural Language Understanding. 
The Benjamin/Cummings Publishing Company, 
Menlo Park, CA. 
M. P. Harper and R. A. Helzerman. 1995. Man- 
aging multiple knowledge sources in constraint- 
based parsing spoken language. Fundamenta In- 
formaticae, 23(2,3,4):303-353. 
M. P. Harper, R. A. Helzerman, C. B. Zoltowski, 
B. L. Yeo, Y. Chan, T. Stewart, and B. L. Pellom. 
1995. Implementation issues in the development 
of the parsec parser. SOFTWARE - Practice and 
Experience, 25:831-862. 
M. P. Harper, C. M. White, W. Wang, M. T. John- 
son, and R. A. Helzerman. 2000. Effectiveness 
of corpus-induced dependency grammars for post- 
processing speech. In Proceedings of the 1st An- 
nual Meeting of the North American Association 
for Computational Linguistics. 
L. Hirschman, M. Light, E. Breck, and J.D. Burger. 
1999. Deep Read: A reading comprehension sys- 
tem. In Proceedings of the 37th Annual Meeting 
of the Association for Computational Linguistics, 
pages 325-332. 
J. R. Hobbs. 1979. Coherence and coreference. Cog- 
nitive Science, 1:67-90. 
J-SR Jang. 1993. ANFIS: Adaptive-Network-based 
Fuzzy Inference System. IEEE Transactions on 
System, Man, and Cybernetics, 23(3):665-685. 
M. Jelasity and J. Dombi. 1998. GAS, a concept on 
modeling species in genetic algorithms. Artificial 
Intelligence, 99 (1) :1-19. 
The MathWorks, Inc., 1998. Neural Network Tool- 
box, v3. O. 1. 
Rulequest Research, 1999. Data 
Mining Tools See5 and C5. O. 
http ://www. rulequest, com/s eeS-in~o, html. 
35 
The SuperARV Language Model: Investigating the Eectiveness
of Tightly Integrating Multiple Knowledge Sources
Wen Wang and Mary P. Harper
School of Electrical and Computer Engineering
Purdue University
1285 The Electrical Engineering Building
West Lafayette, IN 47907-1285
fwang28,harperg@ecn.purdue.edu
Abstract
A new almost-parsing language model incorporat-
ing multiple knowledge sources that is based upon
the concept of Constraint Dependency Grammars is
presented in this paper. Lexical features and syn-
tactic constraints are tightly integrated into a uni-
form linguistic structure called a SuperARV that is
associated with a word in the lexicon. The Super-
ARV language model reduces perplexity and word er-
ror rate compared to trigram, part-of-speech-based,
and parser-based language models. The relative con-
tributions of the various knowledge sources to the
strength of our model are also investigated by using
constraint relaxation at the level of the knowledge
sources. We have found that although each knowl-
edge source contributes to language model quality,
lexical features are an outstanding contributor when
they are tightly integrated with word identity and
syntactic constraints. Our investigation also suggests
possible reasons for the reported poor performance
of several probabilistic dependency grammar models
in the literature.
1 Introduction
The purpose of a language model (LM) is to de-
termine the a priori probability of a word sequence
w1; : : : ; wn, P (w1; : : : ; wn). Language modeling is es-
sential in a wide variety of applications; we focus on
speech recognition in our research. Although word-
based LMs (with bigram and trigram being the most
common) remain the mainstay in many continuous
speech recognition systems, recent eorts have ex-
plored a variety of ways to improve LM performance
(Niesler and Woodland, 1996; Chelba et al, 1997;
Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosen-
feld, 2000; Goodman, 2001; Roark, 2001; Charniak,
2001).
Class-based LMs attempt to deal with data sparse-
ness and generalize better to unseen word sequences
by rst grouping words into classes and then using
these classes to compute n-gram probabilities. Part-
of-Speech (POS) tags were initially used as classes
by Jelinek (1990) in a conditional probabilistic
model (which predicts the tag sequence for a word
sequence rst and then uses it to predict the word
sequence):
Pr(wN1 ) 
X
t1;t2;:::;tN
N
Y
i=1
Pr(tijti?11 )Pr(wijti) (1)
However, Jelinek?s POS LM is less eective at pre-
dicting word candidates than an n-gram word-based
LM because it deletes important lexical information
for predicting the next word. Heeman?s (1998) POS
LM achieves a perplexity reduction compared to a
trigram LM by instead redening the speech recog-
nition problem as determining:
W; T = argmax
W;T
P (W;T jA)
= argmax
W;T
P (W;T)P (AjW;T)
 argmax
W;T
P (W;T)P (AjW )
where T is the POS sequence tN1 associated with the
word sequence W = wN1 given the speech utterance
A. The LM P (W;T ) is a joint probabilistic model
that accounts for both the sequence of words wN1
and their tag assignments tN1 by estimating the joint
probabilities of words and tags:
P (wN1 ; t
N
1 ) =
N
Y
i=1
P (wi; tijwi?11 ; ti?11 ) (2)
Johnson (2001) and Laerty et al (2001) provide
insight into why a joint model is superior to a con-
ditional model.
Recently, there has been good progress in devel-
oping structured models (Chelba, 2000; Charniak,
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 238-247.
                         Proceedings of the Conference on Empirical Methods in Natural
2001; Roark, 2001) that incorporate syntactic infor-
mation. These LMs capture the hierarchical char-
acteristics of a language rather than specic infor-
mation about words and their lexical features (e.g.,
case, number). In an attempt to incorporate even
more knowledge into a structured LM, Goodman
(1997) has developed a probabilistic feature gram-
mar (PFG) that conditions not only on structure
but also on a small set of grammatical features (e.g.,
number) and has achieved parse accuracy improve-
ment. Goodman?s work suggests that integrating
lexical features with word identity and syntax would
benet LM predictiveness. PFG uses only a small set
of lexical features because it integrates those features
at the level of the production rules, causing a signif-
icant increase in grammar size and a concomitant
data sparsity problem that preclude the addition of
richer features. This sparseness problem can be ad-
dressed by associating lexical features directly with
words.
We hypothesize that high levels of word predic-
tion capability can be achieved by tightly integrat-
ing structural constraints and lexical features at the
word level. Hence, we develop a new dependency-
grammar almost-parsing LM, SuperARV LM, which
uses enriched tags called SuperARVs. In Section 2,
we introduce our SuperARV LM. Section 3 compares
the performance of the SuperARV LM to other LMs.
Section 4 investigates the knowledge source contribu-
tions by constraint relaxation. Conclusions appear in
Section 5.
2 SuperARV Language Model
The SuperARV LM is a highly lexicalized probabilis-
tic LM based on the Constraint Dependency Gram-
mar (CDG) (Harper and Helzerman, 1995). CDG
represents a parse as assignments of dependency re-
lations to functional variables (denoted roles) asso-
ciated with each word in a sentence. Consider the
parse for What did you learn depicted in the white
box of Figure 1. Each word in the parse has a lexi-
cal category and a set of feature values. Also, each
word has a governor role (denoted G) which is as-
signed a role value, comprised of a label as well as a
modiee, which indicates the position of the word?s
governor or head. For example, the role value as-
signed to the governor role of did is vp-1, where its
label vp indicates its grammatical function and its
modiee 1 is the position of its head what. The need
roles (denoted N1, N2, and N3) are used to ensure
the grammatical requirements (e.g., subcategoriza-
tion) of a word are met, as in the case of the verb
did, which needs a subject and a base form verb (but
since the word takes no other complements, the mod-
pronoun
case=common
behavior=nominal
type=interrogative
semtype=inanimate
agr=3s
G=np-4
verb
subcat=base
verbtype=past
voice=active
inverted=yes
type=none
gapp=yes
mood=whquestion
semtype=auxiliary
agr=all
G=vp-1
Need1=S-3
Need2=S-4
Need3=S-2
pronoun
case=common
behavior=nominal
type=personal
semtype=human
agr=2s
G=subj-2
          1
        what
          2
        did
          
 3
        you
The SuperARV of the word "did":
 Category: Verb
           4
        learn
verb
subcat=obj
vtype=infinitive
voice=active
inverted=no
type=none
gapp=yes
mood=whquestion
semtype=behavior
agr=none
G=vp-2
Need1=S-4
Need2=S-1
Need3=S-4
 Features: {verbtype=past, voice=active, inverted=yes, type=none, 
 gapp=yes,mood=whquestion,agr=all}
 Role=G,         Label=vp, PX>MX,                (ModifieeCategory=pronoun)
 Role=Need1, Label=S,   PX<MX,                (ModifieeCategory=pronoun)
 Role=Need2, Label=S,   PX<MX,                (ModifieeCategory=verb)
 Role=Need3, Label=S,   PX=MX,                (ModifieeCategory=verb)
 Dependent Positional Constraints:
 MX[G] < PX = MX[Need3] < MX[Need1] 
 < MX[Need2] MC
}
n
ee
d 
ro
le
 
co
n
st
ra
in
ts}}}CF }
}
(R,L,UC,MC)+
DC
ARV for vp-1 assigned to G for did:
cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, 
gapp=yes, mood=whquestion, semtype=auxiliary, agr=all,
role1=G, label1=vp, (PX1>MX1)
ARVP for vp-1 assigned to G for did and subj-2 assigned to G for you:
cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, gapp=yes,
 mood=whquestion, semtype=auxiliary, agr=all,
role1=G, label1=vp, (PX1>MX1)
cat2=pronoun, case=common, behavior=nominal, type=personal, semtype=human, agr=2s,
role2=G, label2=subj, (PX2>MX2)
(PX1<PX2), (MX1<MX2),(PX1=MX2),(MX1<PX2) 
Figure 1: An example of a CDG parse, an ARV and
ARVP, and the SuperARV of the word did in the sentence
what did you learn. Note: G represents the governor
role; the need roles, Need1, Need2, and Need3, are used
to ensure that the grammatical requirements of the word
are met. PX and MX([R]) represent the position of a
word and its modiee (for role R), respectively.
iee of the role value assigned to N3 is set equal to
its own position). Including need roles also provides
a mechanism for using non-headword dependencies
to constrain parse structures, which Bod (2001) has
shown contributes to improved parsing accuracy.
During parsing, the grammaticality of a sentence
in a language dened by a CDG is determined by
applying a set of constraints to the possible role
value assignments (Harper and Helzerman, 1995;
Maruyama, 1990). Originally, the constraints were
comprised of a set of hand-written rules specifying
which role values (unary constraints) and pairs of
role values (binary constraints) were grammatical
(Maruyama, 1990). In order to derive the constraints
directly from CDG annotated sentences, we have de-
veloped an algorithm to extract grammar relations
using information derived directly from annotated
sentences (Harper et al, 2000; Harper and Wang,
2001). Using the relationship between a role value?s
position and its modiee?s position, unary and bi-
nary constraints can be represented as a nite set of
abstract role values (ARVs) and abstract role value
pairs (ARVPs), respectively. The light gray box of
Figure 1 shows an example of an ARV and an ARVP.
The ARV for the governor role value of did indicates
its lexical category, lexical features, role, label, and
positional relation information. (PX1 > MX1) in-
dicates that did is governed by a word that precedes
it. Note that the constraints of a CDG can be ex-
tracted from a corpus of parsed sentences.
A super abstract role value (SuperARV) is an ab-
straction of the joint assignment of dependencies for
a word, which provides a mechanism for lexicaliz-
ing CDG parse rules. The dark gray box of Figure 1
presents an example of a SuperARV for the word did.
The SuperARV structure provides an explicit way to
organize information concerning one consistent set of
dependency links for a word that can be directly de-
rived from its parse assignments. SuperARVs encode
lexical information as well as syntactic and semantic
constraints in a uniform representation that is much
more ne-grained than POS. A SuperARV can be
thought of as providing admissibility constraints on
syntactic and lexical environments in which a word
may be used.
A SuperARV is formally dened as a four-tuple
for a word, hC; F , (R;L; UC;MC)+; DCi, where C
is the lexical category of the word, F = fFname1
= Fvalue1, : : : ; FNamef = FV aluefg is a fea-
ture vector (where Fnamei is the name of a feature
and Fvaluei is its corresponding value), (R, L, UC,
MC)+ is a list of one or more four-tuples, each rep-
resenting an abstraction of a role value assignment,
where R is a role variable, L is a functionality la-
bel, UC represents the relative position relation of
a word and its dependent, MC is the lexical cat-
egory of the modiee for this dependency relation,
and DC represents the relative ordering of the po-
sitions of a word and all of its modiees. The fol-
lowing features are used in our SuperARV LM: agr,
case, vtype (e.g., progressive), mood, gapp (e.g.,
gap or not), inverted, voice, behavior (e.g., mass,
count), type (e.g., interrogative, relative). These
lexical features constitute a much richer set than the
features used by the parser-based LMs in Section 1.
Since Harper et al (1999) found that enforcing mod-
iee constraints (e.g., the lexical categories of modi-
ees) in parsing results in ecient pruning, we also
include the modiee lexical category (MC) in our Su-
perARV structure to impose modiee constraints.
Words typically have more than one SuperARV to
indicate dierent types of word usage. The average
number of SuperARVs for words of dierent lexical
categories vary, with verbs having the greatest Su-
perARV ambiguity. This is mostly due to the vari-
ety of feature combinations and variations on com-
plement types and positions. We have observed in
several experiments that the number of SuperARVs
does not grow signicantly as training set size in-
creases; the moderate-sized Resource Management
corpus (Price et al, 1988) with 25,168 words pro-
duces 328 SuperARVs, compared to 538 SuperARVs
for the 1 million word Wall Street Journal (WSJ)
Penn Treebank set (Marcus et al, 1993), and 791 for
the 37 million word training set of the WSJ contin-
uous speech recognition task.
SuperARVs can be accumulated from a corpus an-
notated with CDG relations and stored directly with
words in a lexicon, so we can learn their frequency
of occurrence for the corresponding word. A Super-
ARV can then be selected from the lexicon and used
to generate role values that meet their constraints.
Since there are no large benchmark corpora anno-
tated with CDG information1, we have developed a
methodology to automatically transform constituent
bracketing found in available treebanks into CDG
annotations. In addition to generating dependency
structures by headword percolation (Chelba, 2000),
our transformer also utilizes a rule-based method to
determine lexical features and need role values for
words, as described by Wang et al (2001).
Our SuperARV LM estimates the joint probability
of words wN1 and their SuperARV tags t
N
1 :
Pr(wN1 t
N
1 ) =
N
Y
i=1
Pr(witijwi?11 ti?11 )
=
N
Y
i=1
Pr(tijwi?11 ti?11 )  Pr(wijwi?11 ti1)

N
Y
i=1
Pr(tijwi?1i?2ti?1i?2)  Pr(wijwi?1i?2tii?2) (3)
Notice we use a joint probabilistic model to enable
the joint prediction of words and their SuperARVs so
that word form information is tightly integrated at
the model level. Our SuperARV LM does not encode
the word identity directly at the data structure level
as was done in (Galescu and Ringger, 1999) since
this could cause serious data sparsity problems.
To estimate the probability distributions in Equa-
tion (3) from training data, we use recursive lin-
ear interpolation among probability estimations of
dierent orders. Representing each multiplicand
in Equation (3) as the conditional probability
P^ (xjy1; y2; : : : ; yn) where y1; y2; : : : ; yn belong to a
mixed set of words and SuperARVs, the recursive
linear interpolation is calculated as follows:
1We have annotated a moderate-sized corpus,
DARPA Naval Resource Management (Price et al, 1988),
with CDG parse relations as reported in (Harper et al,
2000; Harper and Wang, 2001).
P^n(xjy1; y2; : : : ; yn)
= (x; y1; y2; : : : ; yn)  Pn(xjy1; y2; : : : ; yn)
+(1? (x; y1; y2; : : : ; yn))  P^n?1(xjy1; y2; : : : ; yn?1)
where:
 y1; y2; : : : ; yn is the context of order n-gram to
predict x;
 Pn(xjy1; y2; : : : ; yn) is the order n-gram maximum
likelihood estimation.
Table 1 enumerates the n-grams and their order for
the interpolation smoothing of the two distributions
in Equation (3). The ordering was based on our hy-
pothesis that n-grams with more ne-grained history
information should be ranked higher in the n-gram
list since that information should be more helpful
for discerning word and SuperARVs based on their
history. The SuperARV LM hypothesizes categories
for out-of-vocabulary words using the leave-one-out
technique (Niesler and Woodland, 1996).
Table 1: The enumeration and order of n-grams for
smoothing the distributions in Equation (3).
n-grams P^ (tijwi?1i?2ti?1i?2) P^ (wijwi?1i?2tii?2)
highest P^ (tijwi?1i?2ti?1i?2) P^ (wijwi?1i?2tii?2)
P^ (tijwi?1ti?1i?2) P^ (wijwi?1i?2tii?1)
P^ (tijwi?1i?2ti?1) P^ (wijwi?1tii?2)
P^ (tijti?1i?2) P^ (wijwi?1tii?1)
P^ (tijwi?1ti?1) P^ (wijwi?1ti)
P^ (tijti?1) P^ (wijtii?1)
lowest P^ (ti) P^ (wijti)
In preliminary experiments, we compared several
algorithms for smoothing the probability estima-
tions for our SuperARV LM. The best performance
was achieved by using the modied Kneser-Ney
smoothing algorithm initially introduced in (Chen
and Goodman, 1998) and adapting it by employing
a heldout data set to optimize parameters, includ-
ing cutos for rare n-grams, by using Powell?s search
(Press et al, 1988). Parameters are chosen to opti-
mize the perplexity on a heldout set.
In order to compare our SuperARV LM with a
word-based LM, we must use the following equation
to calculate the word perplexity (PPL):
PPL = 2En (4)
En  ? 1
N
N
X
i=1
log2 P^ (wijw
i?1
i?2)
 ? 1
N
N
X
i=1
log2
P
ti?2;i
P^ (witijwi?1i?2ti?1i?2)P^(wi?1i?2ti?1i?2)
P
ti?2;i?1
P^ (wi?1i?2t
i?1
i?2)
Equation (4) is used by class-based LMs to calculate
word perplexity (Heeman, 1998). Parser-based LMs
use a similar procedure that sums over parses.
The SuperARV LM is most closely related to
the almost-parsing-based LM developed by Srinivas
(1997). Srinivas? LM, based on the notion of a su-
pertag , the elementary structure of Lexicalized Tree-
Adjoining Grammar, achieved a perplexity reduction
compared to a conditional POS n-gram LM (Niesler
and Woodland, 1996). By comparison, our LM in-
corporates dependencies directly on words instead
of through nonterminals, uses more lexical features
than the supertag LM, uses joint instead of con-
ditional probability estimations, and uses modied
Kneser-Ney rather than Katz smoothing.
3 Evaluating the SuperARV
Language Model
Traditionally, the LM quality in speech recognition is
evaluated on two metrics: perplexity and WER, with
the former commonly selected as a less computation-
ally expensive alternative. We carried out two exper-
iments, one using the Wall Street Journal Penn Tree-
bank (WSJ PTB), a text corpus on which perplexity
can be measured and compared to other LMs, and
the Wall Street Journal Continuous Speech Recog-
nition (WSJ CSR) task, a speech corpus on which
both perplexity and WER can be evaluated after LM
rescoring. These two experiments compare our Su-
perARV LM to a baseline trigram, a POS LM that
was implemented using Equation (3) (where for this
model t represents POS tags instead of SuperARV
tags) and modied Kneser-Ney smoothing (as used
in the SuperARV LM), and one or more parser-based
LMs. Additionally, we evaluate the performance of a
conditional probability SuperARV LM (denoted cSu-
perARV) implemented following Equation (1) rather
than Equation (3) to evaluate the importance of us-
ing joint probability estimations.
For the WSJ PTB task, we compare the Super-
ARV LMs to the parser LMs developed by Chelba
(2000), Roark (2001), and Charniak (2001). Al-
though Srinivas (1997) developed an almost-parsing
supertag-based LM, we cannot compare his LM with
the other LMs because he used a small non-standard
subset of the WSJ PTB2 and a trainable supertag
LM is unavailable. Because none of the parser LMs
has been fully trained for the WSJ CSR task, it is
essential that we retrain them for comparison. The
availability of a trainable version of Chelba?s model
enables us to train and test on the CSR task; how-
ever, because we do not have access to a trainable
version of Charniak?s or Roark?s LMs, they are not
considered in the CSR task. Note that for lattice
rescoring, however, Roark found that Chelba?s model
achieves a greater reduction on WER than his LM
(Roark, 2001).
3.1 Evaluating on the WSJ PTB
To evaluate the perplexity of the LMs on the WSJ
PTB task, we adopted the conventions of Chelba
(2000), Roark (2001), and Charniak (2001) for pre-
processing the data. The vocabulary is limited to
the most common 10K words, with all words outside
this vocabulary mapped to hUNKi. All punctuation
is removed and no case information is retained. All
symbols and digits are replaced by the symbol N.
Sections 0-20 (929,564 words) are used as the train-
ing set for collecting counts, sections 21-22 (73,760
words) as the development set for tuning parameters,
and sections 23-24 (82,430 words) for testing.
The baseline trigram uses Katz back-o model
with Good-Turing discounting for smoothing. The
POS, cSuperARV, and SuperARV LMs were imple-
mented as described previously. The results for the
parser-based LMs were initially taken from the lit-
erature. The perplexity on the test set using each
LM and their interpolation with the corresponding
trigram (and the interpolation weight) are shown in
the top six rows of Table 2.
As can be seen in Table 2, the SuperARV LM ob-
tains the lowest perplexity of all of the LMs (and
so it is depicted in bold face). The SuperARV LM
achieves the greatest perplexity reduction of 29.19%
compared to the trigram, with Charniak?s interpo-
lated trihead LM a close second at 24.91%. The cSu-
perARV LM is clearly inferior to the SuperARV LM,
even after interpolation. This result highlights the
value of tight coupling of word, lexical feature, and
syntactic knowledge both at the data structure level
(which is the same for the SuperARV and cSuper-
ARV LMs) and at the probability model level (which
is dierent).
Notice that the cSuperARV, Chelba?s, Roark?s,
and Charniak?s LMs obtain an improvement in per-
formance when interpolated with a trigram; whereas,
2Using the same 180,000 word training and 20,000
word test set as (Srinivas, 1997), our SuperARV LM ob-
tains a perplexity of 92.76, compared to a perplexity of
101 obtained by the supertag LM.
the POS LM and the SuperARV LM do not benet
from trigram interpolation3. To gain more insight
into why a trigram is eectively interpolated with
some, but not all, of the LMs, we calculate the cor-
relation of the trigram with each LM. A standard
correlation is calculated between the probabilities as-
signed to each test set sentence by the trigram LM
and the LM in question. This technique has been
used in (Wang et al, 2002) to identify whether two
LMs can be eectively interpolated.
Since we have access to an executable ver-
sion of Charniak?s LM trained on the WSJ PTB
(ftp.cs.brown.edu/pub/nlparser) and a trainable ver-
sion of Chelba?s LM, we are able to calculate their
correlations with our trigram LM. Chelba?s LM was
retrained using more parameter reestimation itera-
tions than in (Chelba, 2000) to optimize the per-
formance. Table 2 shows the correlation between
each of the executable LMs and the trigram LM.
The POS LM has the highest correlation with the
trigram, closely followed by the SuperARV LM. Be-
cause these two LMs tightly integrate the word infor-
mation jointly with the tag distribution, the trigram
information is already represented. In contrast, the
cSuperARV LM and Chelba?s and Charniak?s parser-
based LMs have much lower correlations, indicating
they have much lower overlap with the trigram. Be-
cause the cSuperARV LM only uses weak word dis-
tribution information in probability estimations, it
leaves room for the trigram LM to compensate for
the lack of word knowledge. The correlations for the
parser-based LMs suggest that they capture dierent
aspects of the words? distributions in the language
than the words themselves.
3.2 Evaluating on the WSJ CSR Task
Next we compare the eectiveness of using the tri-
gram word-based, POS, cSuperARV, SuperARV,
and Chelba?s LMs in rescoring hypotheses generated
by a speech recognizer. The training set of the WSJ
CSR task is composed of the 1987-1989 les con-
taining 37,243,300 words. The speech data for the
training set is used for building the acoustic model;
whereas, the parse trees for the training set are gen-
erated following the policy that if the context-free
grammar constituent bracketing can be found in the
WSJ PTB, it becomes the parse tree for the training
sentence; otherwise, we use the corresponding tree in
the BLLIP treebank (Charniak et al, 2000). Since
WSJ CSR is a speech corpus, there is no punctua-
tion or case information. All words outside the pro-
vided vocabulary are mapped to hUNKi. Note that
3In the remaining experiments, the POS LM and the
SuperARV LM are not interpolated with a trigram.
Perplexity
LM 3gram Model Intp (Weight) r
POS 167.14 142.55 142.55 (1.0) 0.95
SuperARV 167.14 118.35 118.35 (1.0) 0.92
cSuperARV 167.14 150.01 143.83 (0.65) 0.68
Chelba (2000) 167.14 158.28 148.90 (0.64) N/A
Roark (2001) 167.02 152.26 137.26 (0.64) N/A
Charniak (2001) 167.89 130.20 126.07 (0.64) N/A
Chelba 167.14 153.76 147.70 (0.64) 0.73
Charniak 167.14 130.20 126.03 (0.64) 0.69
Table 2: Comparing perplexity results for each LM on the WSJ PTB test set. 3gram represents the word-based
trigram LM, Intp (weight) the LM interpolated with a trigram (and the interpolation weight), and r the correlation
value. N/A means not available.
the word-level tokenization of treebank texts diers
from that used in the speech recognition task with
the major dierences being: numbers (e.g., \1.2%"
versus \one point two percent"), dates (e.g., \Dec.
20, 2001" versus \December twentieth, two thou-
sand one") , currencies (e.g., \$10.25" versus \ten
dollars and twenty ve cents"), common abbrevia-
tions (e.g., \Inc." versus \Incorporated"), acronyms
(e.g., \I.B.M." versus \I. B. M."), hyphenated and
period-delimited phrases (e.g., \red-carpet" versus
\red carpet"), and contractions and possessives (e.g.,
\do n?t" versus \don?t"). The POS, parser-based,
and SuperARV LMs are all trained using the text-
based tokenization from the treebank. Hence, during
testing, a transformation converts the output of the
recognizer to a form compatible with the text-based
tokenization (Roark, 2001) for rescoring.
For testing the LMs, we use the four available
WSJ CSR evaluation sets: 1992 5K closed vocab-
ulary (denoted 92-5k) with 330 utterances and 5,353
words, 1993 5K closed vocabulary (93-5k) with 215
utterances and 3,849 words, 1992 20K open vocabu-
lary (92-20k) with 333 utterances and 5,643 words,
and 1993 20K (93-20k) with 213 utterances and
3,446 words. We also employ a development set for
each vocabulary size: 93-5k-dt (513 utterances and
8,635 words) and 93-20k-dt (252 utterances and 4,062
words).
The trigram provided by LDC for the CSR task
was used due to its high quality. Before evaluation,
all the other LMs (i.e., the POS LM, the cSuperARV
and SuperARV LMs, and Chelba?s LM) are retrained
on the training set trees for the CSR task. Parameter
tuning for the LMs on each task uses the correspond-
ing development set4.
Perplexity Results Table 3 shows the perplexity
results for each test set with the best result for each
4The interpolation weight for cSuperARV for lattice
rescoring was 0.63 on the 5k tasks and 0.60 on the 20k
tasks, and 0.68 and 0.65 for Chelba?s LM, respectively.
in bold face. The SuperARV LM yields the lowest
perplexity, with Chelba?s LM a close second. The
perplexity reductions for the SuperARV LM over
the trigram across the test sets are 53.19%, 53.63%,
34.33%, and 32.05%, which is even higher than on
the WSJ PTB task. This is probably due to the fact
that more training data was used for the CSR task
(37 million words versus 1 million words).
LM 92-5k 93-5k 92-20k 93-20k
3gram 45.61 50.51 106.52 109.22
POS 44.21 30.26 98.79 96.64
cSuperARV 36.53 28.50 86.83 89.12
SuperARV 21.35 23.42 69.95 74.22
Chelba 23.92 25.07 77.16 79.37
Table 3: Comparing perplexity results for each LM on
the WSJ CSR test sets.
Rescoring Lattices Next using the same LMs, we
rescored the lattices generated by an acoustic recog-
nizer built using HTK (Ent, 1997). For each test
set sentence, we generated a word lattice. We tuned
the parameters of the LMs using the lattices on the
corresponding development sets to minimize WER.
Lattices were rescored using a Viterbi search for each
LM.
Table 4 shows the WER and sentence accuracy
(SAC) after rescoring lattices using each LM, with
the lowest WER and highest SAC for each test set
presented in bold face. We also give the lattice
WER/SAC which denes the best accuracy possible
given perfect knowledge. As can be seen from Table
4, the SuperARV LM produces the best reduction
in WER with Chelba?s LM the second best. When
rescoring lattices on the 92-5k, 93-5k, 92-20k, and
93-20k test sets, the SuperARV LM yields a relative
WER reduction of 13.54%, 9.70%, 8.64%, and 3.12%
compared to the trigram, respectively. SAC results
are similar: the SuperARV LM achieves an absolute
increase on SAC of 4.24%, 6.97%, 2.7%, and 3.75%,
compared to the trigram. Note that Chelba?s LM
tied once with the SuperARV LM on 93-20k SAC,
but always obtained higher WER across the four test
sets. Because Chelba?s LM focuses on developing the
complete parse structure for a word sequence, it en-
forces more strict pruning based on the entire sen-
tence. As can be seen in Table 4, the cSuperARV
LM, even when interpolated with a trigram LM, ob-
tains a lower accuracy than our SuperARV LM. This
result is consistent with the hypothesis that a con-
ditional model suers from label bias (Laerty et al,
2001).
The WER reported by Chelba (2000) on the 93-
20k test set was 13.0%. This WER is lower than
what we obtained for Chelba?s retrained LM on the
same task. This disparity is due to the fact that a
higher quality acoustic decoder was used in (Chelba,
2000), which is not available to us. We further com-
pare the LMs on Dr. Chelba?s 93-20K lattices kindly
provided by him, with the rescoring results shown
in the last column of Table 4. We observe that
Chelba?s retrained LM improves his original result,
but the SuperARV LM still obtains a greater accu-
racy. Sign tests show that the dierences between
the accuracies achieved by the SuperARV LM and
the trigram, POS, and cSuperARV LMs are statis-
tically signicant. Although there is no signicant
dierence between the SuperARV LM and Chelba?s
LM, the SuperARV LM has a much lower complexity
than Chelba?s LM.
4 Investigating the Knowledge
Source Contributions
Next, we attempt to explain the contrast between
the encouraging results from our SuperARV LM and
the reported poor performance of several probabilis-
tic dependency grammar models, i.e., the traditional
probabilistic dependency grammar (PDG) LM, the
probabilistic link grammar (PLG) (Laerty et al,
1992) LM, and Zeman?s probabilistic dependency
grammar model (ZPDG) (Hajic et al, 1998). ZPDG
was evaluated on the Prague Dependency Treebank
(Hajic, 1998) during the 1998 Johns Hopkins sum-
mer workshop (Hajic et al, 1998) and produced
a much lower parsing accuracy (under 60%) than
Collins? probabilistic context-free grammar parser
(80%) (Collins, 1996). Fong et al (1995) evalu-
ated the probabilistic link grammar LM described
in (Laerty et al, 1992) on small articial corpora
and found that the LM has a greater perplexity than
a standard bigram. Additionally, only a modest im-
provement on the bigram was achieved after Fong
and Wu (1995) revised the model to make grammar
rule learning feasible.
One possible reason for their poor performance, es-
pecially in the light of our SuperARV LM results, is
that these probabilistic dependency grammar mod-
els do not utilize sucient knowledge to achieve a
high level of accuracy. The knowledge sources the
SuperARV LM uses, represented as components of
the structure shown in Figure 1, include: lexical
category (denoted c), lexical features (denoted f),
role label or link type information (denoted L), a
governor role dependency relation constraint (R, L,
UC) (denoted g), a set of need role dependency rela-
tion constraints (R;L; UC)+ (denoted n), and mod-
iee constraints represented as the lexical category
of the modiee for each role (denoted m). Table
5 summarizes the knowledge sources that each of
the probabilistic dependency grammar models uses.
To determine whether the poor performance of the
three probabilistic dependency grammar models re-
sults from our hypothesis that they utilize insucient
knowledge, we will evaluate our SuperARV LM af-
ter eliminating those knowledge sources that are not
used by each of these models. Additionally, we will
evaluate the contribution of each of the knowledge
sources to the predictiveness of our SuperARV LM.
We use the methodology of selectively ignoring dif-
ferent types of knowledge as constraints to evaluate
the knowledge source contributions to our SuperARV
LM, as well as to approximate the performance of
the other probabilistic dependency grammar models.
The framework of CDG, on which our SuperARV LM
is built, allows constraints to be tightened by adding
more knowledge sources or loosened by ignoring cer-
tain knowledge. The SuperARV structure inherits
this capability from CDG; selective constraint re-
laxation is implemented by eliminating one or more
knowledge source in K = fc; f; L; g; n;mg from the
SuperARV structure. We have constructed nine dif-
ferent LMs based on reduced SuperARV structures
denoted SARV-k (i.e., a SuperARV structure after
removing k with k  K), where ?k represents the
deletion of a subset of knowledge types (e.g., f , mn,
cgmn). Each model is described next.
Modiee constraints potentially hamper grammar
generality, and so we consider their impact by delet-
ing them from the LM by using the SARV-m struc-
ture. Need roles are important for capturing the
structural requirements of dierent types of words
(e.g., subcategorization), and we investigate their ef-
fects by using the SARV-n structure. The model
based on SARV-L is built to investigate the im-
portance of link type information. We can investi-
gate the contribution of the combination of m and
n, fundamental to the enforcement of valency con-
straints, by using the SARV-mn structure. The
model based on SARV-f is used to evaluate whether
Our HTK Lattices Chelba?s
92-5k 93-5k 92-20k 93-20k 93-20k lattices
LM WER(SAC) WER(SAC) WER(SAC) WER(SAC) WER(SAC)
3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52) 13.72(36.18)
POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39) 13.51(37.96)
cSuperARV 3.89(65.15) 6.42(48.84) 10.51(38.44) 14.45(32.86) 13.32 (38.22)
SuperARV 3.83(65.76) 6.24(50.23) 10.15(39.64) 14.28(34.27) 12.87(42.02)
Chelba 3.85(65.45) 6.26(49.77) 10.19(39.34) 14.36(34.27) 12.93(40.48)
lattice 1.79(79.40) 2.16(73.95) 4.93(59.46) 6.65(52.11) 3.41 (68.86)
accuracy
Table 4: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets.
knowledge PDG PLG ZPDG SARV
source
word identity X X lemma X
lexical X X X
category (c)
lexical morpho- X
features (f) logical
features
link type (L) X X X
link direction X X X X
(UC)
valency (n) X
modiee X X
constraints (m)
Table 5: Knowledge sources used by the three prob-
abilistic dependency grammar models compared to our
SuperARV LM. Note link type is dened as L and link
direction is dened as UC in the SuperARV structure.
lexical features improve or degrade LM quality. The
model based on SARV-fmn is very similar to the
standard probabilistic dependency grammar LM, in
which only word, POS, link type, and link di-
rection information is used for probability estima-
tions. The model based on SARV-gmn uses a fea-
ture augmentation of POS, and the model based on
SARV-cgmn uses lexical features only. Addition-
ally, we built the model ZPDG-SARV to approxi-
mate ZPDG. Zeman?s PDG (Hajic et al, 1998) dif-
fers signicantly from our original SuperARV LM in
that it ignores label information L and some lexi-
cal feature information (the morphological tags do
not include some lexical features having influence
on syntax, denoted syntactic lexical features, i.e.,
gapp, inverted, mood, type, case, voice), and does
not enforce valency constraints (instead, the model
only counts the number of links associated with a
word without discriminating whether the links repre-
sent governing or linguistic structural requirements).
Also, word identity information is not used, instead,
the model uses a loose integration of a word?s lemma
and its morphological tag. Given this analysis, we
built the model ZPDG-SARV based on a structure
including lexical category, morphological features,
LM 92-5k 93-5k 92-20k 93-20k
3gram 45.61 50.51 106.52 109.22
SARV-cgmn 45.58 48.37 102.00 104.59
ZPDG-SARV 45.50 47.98 101.89 104.21
POS 44.21 30.26 98.79 96.64
SARV-gmn 43.16 27.75 96.69 93.25
SARV-fmn 45.01 27.42 96.23 93.16
SARV-f 42.33 27.06 94.87 90.20
SARV-mn 40.38 26.96 90.23 89.54
SARV-n 35.02 26.08 87.32 88.04
SARV-L 28.76 25.71 82.45 84.82
SARV-m 26.86 25.58 80.24 83.12
SARV 21.35 23.42 69.95 74.22
Table 6: Comparing perplexity results for each LM on
the WSJ CSR test sets. The LMs appear in decreasing
order of perplexity.
and (G;UC;MC).
Table 6 shows the perplexity results on the WSJ
CSR test sets ordered from highest to lowest for
each test set, with the best result for each in bold
face. The full SuperARV LM yields the lowest per-
plexity. We found that ignoring modiee constraints
(SARV-m) increases perplexity the least, and ignor-
ing link type information (SARV-L) and need role
constraints (SARV-n) are a little worse than that.
Ignoring both knowledge sources (SARV-mn) should
result in even greater degradation, which is veried
by the results. However, ignoring lexical features
(SARV-f) produces an even greater increase in per-
plexity than relaxing both m and n. The SARV-
fmn, which is closest to the traditional probabilistic
dependency grammar LM, shows fairly poor qual-
ity, not much better than the POS LM. One might
hypothesize that lexical features individually con-
tribute the most to the overall performance of the Su-
perARV LM. However, using this knowledge source
by itself (SARV-cgmn) results in dramatic degrada-
tion on perplexity, in fact even worse than that of the
POS LM, but still slightly better than the baseline
trigram. However, as demonstrated by SARV-gmn,
the constraints from lexical features are strength-
ened by combining them with POS. Given the de-
scriptions in Table 5, we can approximate PLG by
a model based on a SuperARV structure eliminat-
ing f and m (which should have a quality between
SARV-f and SARV-fmn). It is noticeable that with-
out word identity information, syntactic lexical fea-
tures, and valency constraints, the ZPDG-SARV LM
performs worse than the POS-based LM and only
slightly better than the LM based on SARV-cgmn.
This suggests that ZPDG can be strengthened by
incorporating more knowledge.
The same ranking of the performance of the LMs
was obtained for WER/SAC after rescoring the lat-
tices using each LM, as shown in Table 7. Our exper-
iments with relaxed SuperARV LMs suggest likely
methods for improving PDG, PLG, and ZPDG mod-
els. The tight integration of word identity, lexical
category, lexical features, and structural dependency
constraints is likely to improve their performance.
Clearly the investigated knowledge sources are quite
synergistic, and their tight integration achieves the
greatest improvement on both perplexity and WER.
5 Conclusions
We have compared our SuperARV LM to a variety of
LMs and found that it achieves both perplexity and
WER reductions compared to a trigram, and despite
the fact that it is an almost-parsing LM, it outper-
forms (or performs comparably to) the more com-
plex parser-based LMs on both perplexity and rescor-
ing accuracy. Additional experiments reveal that se-
lecting a joint instead of a conditional probabilistic
model is an important factor in the performance of
our SuperARV LM. The SuperARV structure pro-
vides a flexible framework that tightly couples a va-
riety of knowledge sources without combinatorial ex-
plosion. We found that although each knowledge
source contributes to the performance of the LM, it
is the tight integration of the word level knowledge
sources (word identity, POS, and lexical features) to-
gether with the structural information of governor
and subcategorization dependencies that produces
the best level of LM performance. We are currently
extending the almost-parsing SuperARV LM to a full
parser-based LM.
6 Acknowledgments
This research was supported by Intel, Purdue Re-
search Foundation, and National Science Founda-
tion under Grant No. IRI 97-04358, CDA 96-17388,
and BCS-9980054. We would like to thank the
anonymous reviewers for their comments and sug-
gestions. We would also like to thank Dr. Charniak,
Dr. Chelba, and Dr. Srinivas for their help with this
research eort. Finally, we would like to thank Yang
Liu (Purdue University) for providing us with the
WSJ CSR test set lattices.
References
R. Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Pro-
ceedings of ACL?2001.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and
M. Johnson. 2000. BLLIP WSJ Corpus. CD-
ROM. Linguistics Data Consortium.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings of ACL?2001.
C. Chelba, F. Jelinek, and S. Khudanpur. 1997.
Structure and performance of a dependency lan-
guage model. In Proceedings of Eurospeech, vol-
ume 5, pages 2775{2778.
C. Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns
Hopkins University.
S. F. Chen and J. T. Goodman. 1998. An empir-
ical study of smoothing techniques for language
modeling. Technical report, Harvard University,
Computer Science Group.
M. J. Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
ACL?1996, pages 184{191.
Entropic Cambridge Research Laboratory, Ltd.,
1997. HTK: Hidden Markov Model Toolkit V2.1.
E. W. Fong and D. Wu. 1995. Learning restricted
probabilistic link grammars. Technical Report
HKUST-CS95-27, University of Science and Tech-
nology, Clear Water Bay, Hong Kong.
L. Galescu and E. K. Ringger. 1999. Augmenting
words with linguistic information for n-gram lan-
guage models. In Proceedings of Eurospeech.
J. Goodman. 1997. Probabilistic feature grammars.
In Proceedings of the Fourth international work-
shop on parsing technologies.
J. Goodman. 2001. A bit of progress in language
modeling, extended version. Technical Report
MSR-TR-2001-72, Microsoft Research, Redmond,
WA.
J. Hajic, E. Brill, M. Collins, B. Hladka, D. Jones,
C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann,
and D. Zeman. 1998. Core natural language
processing technology applicable to multiple lan-
guages { Workshop ?98. Technical report, Johns
Hopkins Univ.
J. Hajic. 1998. Building a syntactically anno-
tated corpus: The Prague Dependency Treekbank.
In Issues of Valency and Meaning ( Festschrift
for Jarmila Panevova), pages 106{132. Carolina,
Charles University, Prague.
92-5k 93-5k 92-20k 93-20k
LM WER(SAC) WER(SAC) WER(SAC) WER(SAC)
3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52)
SARV-cgmn 4.11(62.12) 6.78(45.12) 10.92(37.24) 14.63(31.46)
ZPDG-SARV 4.11(62.44) 6.71(46.02) 10.92(37.24) 14.63(31.65)
POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39)
SARV-gmn 3.92(64.85) 6.52(47.91) 10.56(38.14) 14.51(32.39)
SARV-fmn 3.92(64.85) 6.50(48.37) 10.53(38.14) 14.51(32.39)
SARV-f 3.92(64.85) 6.47(48.37) 10.49(38.14) 14.45(32.86)
SARV-mn 3.92(64.85) 6.44(48.37) 10.47(38.44) 14.42(32.86)
SARV-n 3.89(65.15) 6.39(48.37) 10.40(38.74) 14.39(33.33)
SARV-L 3.85(65.15) 6.29(48.92) 10.32(39.04) 14.39(33.33)
SARV-m 3.85(65.15) 6.29(49.77) 10.24(39.34) 14.35(33.80)
SARV 3.83(65.76) 6.24(50.23) 10.15(39.34) 14.28(34.27)
Table 7: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets. The
LMs appear in decreasing order of WER.
M. P. Harper and R. A. Helzerman. 1995. Exten-
sions to constraint dependency parsing for spoken
language processing. Computer Speech and Lan-
guage, 9:187{234.
M. P. Harper and W. Wang. 2001. Approaches for
learning constraint dependency grammar from cor-
pora. In Proceedings of the Grammar and Nat-
ural Language Processing Conference, Montreal,
Canada.
M. P. Harper, S. A. Hockema, and C. M. White.
1999. Enhanced constraint dependency grammar
parsers. In Proceedings of the IASTED Interna-
tional Conference on Articial Intelligence and
Soft Computing.
M. P. Harper, C. M. White, W. Wang, M. T.
Johnson, and R. A. Helzerman. 2000. Eec-
tiveness of corpus-induced dependency grammars
for post-processing speech. In Proceedings of
NAACL?2000.
P. A. Heeman. 1998. POS tagging versus classes
in language modeling. In Proceedings of the 6th
Workshop on Very Large Corpora, Montreal.
F. Jelinek. 1990. Self-organized language modeling
for speech recognition. In Alex Waibel and Kai-Fu
Lee, editors, Readings in Speech Recognition. Mor-
gan Kaufman Publishers, Inc., San Mateo, CA.
M. Johnson. 2001. Joint and conditional estimation
of tagging and parsing models. In Proceedings of
ACL?2001.
J. D. Laerty, D. Sleator, and D. Temperley. 1992.
Grammatical trigrams: A probabilistic model of
link grammar. In Proc. of AAAI Fall Symp. Prob-
abilistic Approaches to Natural Language, Cam-
bridge, MA.
J. Laerty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for
segmenting and labeling sequence data. In Pro-
ceedings of ICML?2001.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313{330.
H. Maruyama. 1990. Structural disambiguation
with constraint propagation. In The Proceedings
of ACL?1990, pages 31{38.
T. R. Niesler and P. C. Woodland. 1996. A variable-
length category-based N-gram language model. In
Proceedings of ICASSP, volume 1, pages 164{167.
W. H. Press, B. P. Flannery, S. A. Teukolsky, and
W. T. Vetterling. 1988. Numerical Recipes in C.
Cambridge University Press, Cambridge.
P. J. Price, W. Fischer, J. Bernstein, and D. Pallett.
1988. A database for continuous speech recogni-
tion in a 1000-word domain. In Proceedings of
ICASSP?1988, pages 651{654.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249{276.
R. Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88:1270{1278.
B. Srinivas. 1997. Complexity of lexical descriptions
and its relevance to partial parsing. Ph.D. thesis,
University of Pennsylvania.
W. Wang and M. P. Harper. 2001. Investigating
probabilistic constraint dependency grammars in
language modeling. Technical Report TR-ECE-
01-4, Purdue University, School of Electrical En-
gineering.
W. Wang, Y. Liu, and M. P. Harper. 2002. Rescor-
ing eectiveness of language models using dierent
levels of knowledge and their integration. In Pro-
ceedings of ICASSP?2002.
A Statistical Constraint Dependency Grammar (CDG) Parser
Wen Wang
Speech Technology and Research Lab
SRI International
Menlo Park, CA 94025,
U.S.A.,
wwang@speech.sri.com
Mary P. Harper
Electrical and Computer Engineering
Purdue University
West Lafayette, IN 47907-1285,
U.S.A.,
harper@ecn.purdue.edu
Abstract
CDG represents a sentence?s grammatical structure
as assignments of dependency relations to func-
tional variables associated with each word in the
sentence. In this paper, we describe a statistical
CDG (SCDG) parser that performs parsing incre-
mentally and evaluate it on the Wall Street Jour-
nal Penn Treebank. Using a tight integration of
multiple knowledge sources, together with distance
modeling and synergistic dependencies, this parser
achieves a parsing accuracy comparable to several
state-of-the-art context-free grammar (CFG) based
statistical parsers using a dependency-based eval-
uation metric. Factors contributing to the SCDG
parser?s performance are analyzed.
1 Introduction
Statistical parsing has been an important focus of
recent research (Magerman, 1995; Eisner, 1996;
Charniak, 1997; Collins, 1999; Ratnaparkhi, 1999;
Charniak, 2000). Several of these parsers gen-
erate constituents by conditioning probabilities on
non-terminal labels, part-of-speech (POS) tags, and
some headword information (Collins, 1999; Rat-
naparkhi, 1999; Charniak, 2000). They utilize
non-terminals that go beyond the level of a sin-
gle word and do not explicitly use lexical fea-
tures. Collins? Model 2 parser (1999) learns the
distinction between complements and adjuncts by
using heuristics during training, distinguishes com-
plement and adjunct non-terminals, and includes
a probabilistic choice of left and right subcate-
gorization frames, while his Model 3 parser uses
gap features to model wh-movement. Charniak
(Charniak, 2000) developed a state-of-the-art sta-
tistical CFG parser and then built an effective lan-
guage model based on it (Charniak, 2001). But
his parser and language model were originally de-
signed to analyze complete sentences. Among the
statistical dependency grammar parsers, Eisner?s
(1996) best probabilistic dependency model used
unlabeled links between words and their heads, as
well as between words and their complements and
adjuncts. However, the parser does not distinguish
between complements and adjuncts or model wh-
movement. Collins? bilexical dependency grammar
parser (1999) used head-modifier relations between
pairs of words much as in a dependency grammar,
but they are limited to relationships between words
in reduced sentences with base NPs.
Our research interest focuses on building a high
quality statistical parser for language modeling. We
chose CDG as the underlying grammar for several
reasons. Since CDGs can be lexicalized at the word-
level, a CDG parser-based language model is an
important alternative to CFG parser-based models,
which must model both non-terminals and termi-
nals. Furthermore, the lexicalization of CDG parse
rules is able to include not only lexical category in-
formation, but also a rich set of lexical features to
model subcategorization and wh-movement. By us-
ing CDG, our statistical model is able to distinguish
between adjuncts and complements. Additionally,
CDG is more powerful than CFG and is able to
model languages with crossing dependencies and
free word ordering.
In this paper, we describe and evaluate a statisti-
cal CDG parser for which the probabilities of parse
prefix hypotheses are incrementally updated when
the next input word is available, i.e., it parses in-
crementally. Section 2 describes how CDG repre-
sents a sentence?s parse and then defines a Super-
ARV, which is a lexicalization of CDG parse rules
used in our parsing model. Section 3 presents the
parsing model, while Section 4 motivates the eval-
uation metric used to evaluate our parser. Section 5
presents and discusses the experimental results.
2 CDG Parsing
CDG (Harper and Helzerman, 1995) represents syn-
tactic structures using labeled dependencies be-
tween words. Consider an example CDG parse for
the sentence What did you learn depicted in the
white box of Figure 1. Each word in the parse has a
lexical category, a set of feature values, and a set of
roles that are assigned role values, each comprised
of a label indicating the grammatical role of the
word and its modifiee (i.e., the position of the word
it is modifying when it takes on that role). Consider
the role value assigned to the governor role (denoted
G) of you, np-2. The label np indicates the gram-
matical function of you when it is governed by its
head in position 2. Every word in a sentence must
have a governor role with an assigned role value.
Need roles are used to ensure that the grammatical
requirements of a word are met (e.g., subcategoriza-
tion).
pronoun
case=common
behavior=nominal
type=interrogative
agr=3s
G=np-4
verb
subcat=base
verbtype=past
voice=active
inverted=yes
type=none
gapp=yes
mood=whquestion
agr=all
G=vp-1
Need1=S-3
Need2=S-4
Need3=S-2
pronoun
case=common
behavior=nominal
type=personal
agr=2s
G=np-2
          1
        what
          2
        did
           3
        you
The SuperARV of the word "did":
 Category: Verb
           4
        learn
verb
subcat=obj
vtype=infinitive
voice=active
inverted=no
type=none
gapp=yes
mood=whquestion
agr=none
G=vp-2
Need1=S-4
Need2=S-1
Need3=S-4
 Features: {verbtype=past, voice=active, inverted=yes, 
 gapp=yes,mood=whquestion,agr=all}
 Role=G,         Label=vp, PX>MX,                (ModifieeCategory=pronoun)
 Role=Need1, Label=S,   PX<MX,                (ModifieeCategory=pronoun)
 Role=Need2, Label=S,   PX<MX,                (ModifieeCategory=verb)
 Role=Need3, Label=S,   PX=MX,                (ModifieeCategory=verb)
 Dependent Positional Constraints:
 MX[G] < PX = MX[Need3] < MX[Need1] 
 < MX[Need2] MC
}
n
ee
d 
ro
le
 
co
n
st
ra
in
ts}}}CF }
}
(R,L,UC,MC)+
DC
Figure 1: An example of a CDG parse and the Super-
ARV of the word did in the sentence what did you learn.
PX and MX([R]) represent the position of a word and its
modifiee (for role R), respectively.
Note that CDG parse information can be easily
lexicalized at the word level. This lexicalization is
able to include not only lexical category and syn-
tactic constraints, but also a rich set of lexical fea-
tures to model subcategorization and wh-movement
without a combinatorial explosion of the parametric
space (Wang and Harper, 2002). CDG can distin-
guish between adjuncts and complements due to the
use of need roles (Harper and Helzerman, 1995),
is more powerful than CFG, and has the ability to
model languages with crossing dependencies and
free word ordering (hence, this research could be
applicable to a wide variety of languages).
An almost-parsing LM based on CDG has been
developed in (Wang and Harper, 2002). The un-
derlying hidden event of this LM is a SuperARV.
A SuperARV is formally defined as a four-tuple for
a word, ?C, F , (R, L, UC, MC)+, DC?, where C
is the lexical category of the word, F = {Fname1
= Fvalue1, . . . , FNamef = FV aluef} is a fea-
ture vector (where Fnamei is the name of a feature
and Fvaluei is its corresponding value), DC repre-
sents the relative ordering of the positions of a word
and all of its modifiees, (R, L, UC, MC)+ is a list
of one or more four-tuples, each representing an ab-
straction of a role value assignment, where R is a
role variable, L is a functionality label, UC repre-
sents the relative position relation of a word and its
dependent, and MC encodes some modifiee con-
straints, namely, the lexical category of the modifiee
for this dependency relation. The gray box of Figure
1 presents an example of a SuperARV for the word
did. From this example, it is easy to see that a Su-
perARV is a join on the role value assignments of a
word, with explicit position information replaced by
a relation that expresses whether the modifiee points
to the current word, a previous word, or a subse-
quent word. The SuperARV structure provides an
explicit way to organize information concerning one
consistent set of dependency links for a word that
can be directly derived from a CDG parse. Super-
ARVs encode lexical information as well as syntac-
tic and semantic constraints in a uniform represen-
tation that is much more fine-grained than part-of-
speech (POS). A sentence tagged with SuperARVs
is an almost-parse since all that remains is to spec-
ify the precise position of each modifiee. SuperARV
LMs have been effective at reducing word error rate
(WER) on wide variety of continuous speech recog-
nition (CSR) tasks, including Wall Street Journal
(Wang and Harper, 2002), Broadcast News (Wang
et al, 2003), and Switchboard tasks (Wang et al,
2004).
3 SCDG Parser
3.1 The Basic Parsing Algorithm
Our SCDG parser is a probabilistic generative
model. It can be viewed as consisting of two com-
ponents: SuperARV tagging and modifiee determi-
nation. These two steps can be either loosely or
tightly integrated. To simplify discussion, we de-
scribe the loosely integrated version, but we imple-
ment and evaluate both strategies. The basic parsing
algorithm for the loosely integrated case is summa-
rized in Figure 2, with the algorithm?s symbols de-
fined in Table 1. In the first step, the top N-best
SuperARV assignments are generated for an input
sentence w1, . . . , wn using token-passing (Young
et al, 1997) on a Hidden Markov Model with tri-
gram probabilistic estimations for both transition
and emission probabilities. Each SuperARV se-
quence for the sentence is represented as a sequence
of tuples: ?w1, s1?, . . . , ?wn, sn?, where ?wk, sk?
represents the word wk and its SuperARV assign-
ment sk. These assignments are stored in a stack
ranked in non-increasing order by tag assignment
probability.
During the second step, the modifiees are statis-
tically specified in a left-to-right manner. Note that
the algorithm utilizes modifiee lexical category con-
straints to filter out candidates with mismatched lex-
ical categories. When processing the word wk, k =
1, . . . , n, the algorithm attempts to determine the
left dependents of wk from the closest to the far-
thest. The dependency assignment probability when
choosing the (c + 1)th left dependent (with its posi-
tion denoted dep(k,?(c + 1))) is defined as:
Pr(link(sdep(k,?(c+1)), sk,?(c + 1)|syn,H))
where H = ?w, s?k, ?w, s?dep(k,?(c+1)), ?w, s?dep(k,?c)dep(k,?1).
The dependency assignment probability is con-
ditioned on the word identity and SuperARV
assignment of wk and wdep(k,?(c+1)) as well as
all of the c previously chosen left dependents
?w, s?dep(k,?c)dep(k,?1) for wk. A Boolean random variable
syn is used to model the synergistic relationship
between certain role pairs. This mechanism allows
us to elevate, for example, the probability that the
subject of a sentence wi is governed by a tensed
verb wj when the need role value of wj points to
wi as its subject. The syn value for a dependency
relation is determined heuristically based on the
lexical category, role name, and label information
of the two dependent words. After the algorithm
statistically specifies the left dependents for wk,
it must also determine whether wk could be the
(d+1)th right dependent of a previously seen word
wp, p = 1, . . . , k ? 1 (where d denotes the number
of already assigned right dependents of wp), as
shown in Figure 2.
After processing word wk in each partial parse on
the stack, the partial parses are re-ranked according
to their updated probabilities. This procedure is it-
erated until the top parse in the stack covers the en-
tire sentence. For the tightly coupled parser, the Su-
perARV assignment to a word and specification of
its modifiees are integrated into a single step. The
parsing procedure, which is completely incremen-
tal, is implemented as a simple best-first stack-based
search. To control time and memory complexity, we
used two pruning thresholds: maximum stack depth
and maximum difference between the log proba-
bilities of the top and bottom partial parses in the
stack. These pruning thresholds are tuned based on
the tradeoff of time/memory complexity and pars-
ing accuracy on a heldout set, and they both have
hard limits.
Note the maximum likelihood estimation of de-
pendency assignment probabilities in the basic
loosely coupled parsing algorithm presented in Fig-
ure 2 is likely to suffer from data sparsity, and the
estimates for the tightly coupled algorithm are likely
to suffer even more so. Hence, we smooth the prob-
abilities using Jelinek-Mercer smoothing (Jelinek,
1997), as described in (Wang and Harper, 2003;
Wang, 2003).
3.2 Additions to the Basic Model
Some additional features are added to the basic
model because of their potential to improve SCDG
parsing accuracy. Their efficacy is evaluated in Sec-
tion 5.
Modeling crossing dependencies: The basic pars-
ing algorithm was implemented to preclude cross-
ing dependencies; however, it is important to allow
them in order to model wh-movement in some cases
(e.g., wh-PPs).
Distance and barriers between dependents: Be-
cause distance between two dependent words is
an important factor in determining the modifiees
of a word, we evaluate an alternative model that
adds distance, ?dep(k,?(c+1)),k to H in Figure 2.
Note that ?dep(k,?(c+1)),k represents the distance
between position dep(k,?(c + 1)) and k. To avoid
data sparsity problems, distance is bucketed and a
discrete random variable is used to model it. We
also model punctuation and verbs based on prior
work. Like (Collins, 1999), we also found that
verbs appear to act as barriers that impact modifiee
links. Hence, a Boolean random variable that rep-
resents whether there is a verb between the depen-
dencies is added to condition the probability esti-
mations. Punctuation is treated similarly to coordi-
nation constructions with punctuation governed by
the headword of the following phrase, and heuris-
tic questions on punctuation were used to provide
additional constraints on dependency assignments
(Wang, 2003).
Modifiee lexical features: The SuperARV struc-
ture employed in the SuperARV LM (Wang and
Harper, 2002) uses only lexical categories of mod-
ifiees as modifiee constraints. In previous work
(Harper et al, 2001), modifiee lexical features were
central to increasing the selectivity of a CDG.
Hence, we have developed methods to add ad-
ditional relevant lexical features to modifiee con-
straints of a SuperARV structure (Wang, 2003).
4 Parsing Evaluation Metric
To evaluate our parser, which generates CDG anal-
yses rather than CFG constituent bracketing, we
Table 1: Definitions of symbols used in the basic parsing algorithm.
Term Denotes
L(sk), R(sk) all dependents of sk to the left and right of wk, respectively
N(L(sk)), N(R(sk)) the number of left and right dependents of sk, respectively
dep(k,?c), dep(k, c) cth left dependent and right dependent of sk, respectively
dep(k,?1), dep(k, 1) the position of the closest left dependent and right dependent of sk, respectively
dep(k,?N(L(sk))), dep(k, N(L(sk))) the position of the farthest left dependent and right dependent of sk, respectively
Cat(sk) the lexical category of sk
ModCat(sk,?c), ModCat(sk, c) the lexical category of sk?s cth left and right dependent (encoded in the SuperARV
structure), respectively
link(si, sj , k) the dependency relation between SuperARV si and sj with wi assigned as the kth
dependent of sj , e.g., link(sdep(k,?(c+1)), sk,?(c + 1)) indicates that
wdep(k,?(c+1)) is the (c + 1)th left dependent of sk.
D(L(sk)), D(R(sk))) the number of left and right dependents of sk already assigned, respectively
?w, s?dep(k,?c)dep(k,?1) words and SuperARVs of sk?s closest left dependent up to its cth left dependent
?w, s?dep(k,c)dep(k,1) words and SuperARVs of sk?s closest right dependent up to its cth right dependent
syn a random variable denoting the synergistic relation between some dependents
can either convert the CDG parses to CFG brack-
eting and then use PARSEVAL, or convert the CFG
bracketing generated from the gold standard CFG
parses to CDG parses and then use a metric based on
dependency links. Since our parser is trained using
a CFG-to-CDG transformer (Wang, 2003), which
maps a CFG parse tree to a unique CDG parse,
it is sensible to evaluate our parser?s accuracy us-
ing gold standard CDG parse relations. Further-
more, in the 1998 Johns Hopkins Summer work-
shop final report (Hajic et al, 1998), Collins et al
pointed out that in general the mapping from de-
pendencies to tree structures is one-to-many: there
are many possible trees that can be generated for
a given dependency structure since, although gen-
erally trees in the Penn Treebank corpus are quite
flat, they are not consistently ?flat.? This variability
adds a non-deterministic aspect to the mapping from
CDG dependencies to CFG parse trees that could
cause spurious PARSEVAL scoring errors. Addi-
tionally, when there are crossing dependencies, then
no tree can be generated for that set of dependen-
cies. Consequently, we have opted to use a trans-
former to convert CFG trees to CDG parses and de-
fine a new dependency-based metric adapted from
(Eisner, 1996). We define role value labeled pre-
cision (RLP) and role value labeled recall (RLR)
on dependency links as follows:
RLP = correct modifiee assignments
number of modifiees our parser found
RLR = correct modifiee assignments
number of modifiess in the gold test set parses
where a correct modifiee assignment for a word
wi in a sentence means that a three-tuple
?role id, role label, modifiee word position? (i.e.,
a role value) for wi is the same as the three-tuple
role value for the corresponding role id of wi in the
gold test parse. This differs from Eisner?s (1996)
precision and recall metrics which use no label in-
formation and score only parent (governor) assign-
ments, as in traditional dependency grammars. We
will evaluate role value labeled precision and recall
on all roles of the parse, as well as the governor-
only portion of a parse. Eisner (Eisner, 1996) and
Lin (Lin, 1995) argued that dependency link eval-
uation metrics are valuable for comparing parsers
since they are less sensitive than PARSEVAL to sin-
gle misattachment errors that may cause significant
error propagation to other constituents. This, to-
gether with the fact that we must train our parser
using CDG parses generated in a lossy manner from
a CFG treebank, we chose to use RLP and RLR to
compare our parsing accuracy with several state-of-
the-art parsers.
5 Evaluation and Discussion
All of the evaluations were performed on the Wall
Street Journal Penn Treebank task. Following the
traditional data setup, sections 02-21 are used for
training our parser, section 23 is used for testing,
and section 24 is used as the development set for pa-
rameter tuning and debugging. As in (Ratnaparkhi,
1999; Charniak, 2000; Collins, 1999), we evaluate
on all sentences with length ? 40 words (2,245 sen-
tences) and length ? 100 words (2,416 sentences).
For training our probabilistic CDG parser on this
task, the CFG bracketing of the training set is trans-
BASIC PARSING ALGORITHM
1. Using SuperARV tagging on word sequence w1, . . . , wn, obtain a set of N-best SuperARV sequences with each
element consisting of n (word, SuperARV) tuples, denoted ?w1, s1?, . . . , ?wn, sn?, which we will call an assignment.
2. For each SuperARV assignment, initialize the stack of parse prefixes with this assignment:
/? From left-to-right, process each ?word, tag? of the assignment and generate parse prefixes ?/
for k : = 1, n do
/? Step a: ?/
/* decide left dependents of ?wk, sk? from the nearest to the farthest */
for c from 0 to N(L(sk)) ? 1 do
/? Choose a position for the (c + 1)th left dependent of ?wk, sk? from the set of possible positions
C = {1, . . . , dep(k,?c) ? 1}. The position choice is denoted dep(k,?(c + 1)) ? /
/? In the following equations, different left dependent assignments will generate
different parse prefixes, each of which is stored in the stack ? /
for each dep(k,?(c + 1)) from positions C = {1, . . . , dep(k,?c) ? 1}
/? Check whether the lexical category of the choice matches the modifiee lexical
category of the (c + 1)th left dependent of ?wk, sk? ? /
if Cat(sdep(k,?(c+1))) == ModCat(sk,?(c + 1)) then
Pr(T ) : = Pr(T ) ? Pr(link(sdep(k,?(c+1)), sk,?(c + 1)|syn,H))
where H = ?w, s?k, ?w, s?dep(k,?(c+1)), ?w, s?dep(k,?c)dep(k,?1)
/? End of choosing left dependents of ?wk, sk? for this parse prefix ?/
/? Step b: ?/
/? For the word/tag pair ?wk, sk?, check whether it could be a right dependent of any previously
seen word within a parse prefix of ?w1, s1?, . . . , ?wk?1, sk?1? ?/
for p : = 1, k ? 1 do
/? If ?wp, sp? still has right dependents left unspecified, then try out?wk, sk? as a right dependent */
if D(R(sp)) 6= N(R(sp)) then
d : = D(R(sp))
/? If the lexical category of ?wk, sk? matches the modifiee lexical category of the(d + 1)th right
dependent of ?wp, sp?, then sk might be ?wp, sp??s (d + 1)th right dependent ? /
if Cat(sk) == ModCat(sp, d + 1) then
Pr(T ) : = Pr(T ) ? Pr(link(sk, sp, d + 1)|syn,H), where H = ?w, s?p, ?w, s?k, ?w, s?dep(p,d)dep(p,1)
Sort the parse prefixes in the stack according to logPr(T ) and apply pruning using the thresholds.
3. After processing w1, . . . , wn, pick the parse with the highest logPr(T ) in the stack as the parse for that sentence.
Figure 2: The basic loosely coupled parsing algorithm. Note the algorithm updates the probabilities of parse
prefix hypotheses incrementally when processing each input word.
formed into CDG annotations using a CFG-to-CDG
transformer (Wang, 2003). Note that the sound-
ness of the CFG-to-CDG transformer was evaluated
by examining the CDG parses generated from the
transformer on the Penn Treebank development set
to ensure that they were correct given our grammar
definition.
5.1 Contribution of Model Factors
First, we investigate the contribution of the model
additions described in Section 3 to parse accuracy.
Since these factors are independent of the coupling
between the SuperARV tagger and modifiee spec-
ification, we investigate their impact on a loosely
integrated SCDG parser by comparing four models:
(1) the basic loosely integrated model; (2) the ba-
sic model with crossing dependencies; (3) model 2
with distance and barrier information; (4) model 3
with SuperARVs augmented with additional modi-
fiee lexical feature constraints. Each model uses a
trigram SuperARV tagger to generate 40-best Su-
perARV sequences prior to modifiee specification.
Table 2 shows the results for each of the four models
including SuperARV tagging accuracy (%) and role
value labeled precision and recall (%). Allowing
crossing dependencies improves the overall parsing
accuracy, but using distance information with verb
barrier and punctuation heuristics produces an even
greater improvement especially on the longer sen-
tences. The accuracy is further improved by the ad-
ditional modifiee lexical feature constraints added to
the SuperARVs. Note that RLR is lower than RLP
in these investigations possibly due to SuperARV
tagging errors and the use of a tight stack pruning
threshold.
Next, we evaluate the impact of increasing the
context of the SuperARV tagger to a 4-gram while
increasing the size of the N-best list passed from
the tagger to the modifiee specification step of the
parser. For this evaluation, we use model (4)
Table 2: Results on Section 23 of the WSJ Penn Tree-
bank for four loosely-coupled model variations. The
evaluation metrics, RLR and RLP, are our dependency-
based role value labeled precision and recall. Note:
Model (1) denotes the basic model, Model (2) de-
notes (1)+crossing dependencies, Model (3) denotes
(2)+distance (punctuation) model, and Model (4) denotes
(3)+modifiee lexical features.
Models ? 40 words (2,245 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
(1) 94.7 90.6 90.3 86.8 86.2
(2) 95.0 90.7 90.5 87.0 86.5
(3) 95.7 91.1 90.9 87.4 87.0
(4) 96.2 91.5 91.2 88.0 87.4
Models ? 100 words (2,416 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
(1) 94.0 89.7 89.3 86.0 85.5
(2) 94.2 89.9 89.6 86.2 85.8
(3) 94.7 90.4 90.2 86.8 86.3
(4) 95.4 90.9 90.5 87.5 86.8
from Table 2, the most accurate model so far. We
also evaluate whether a tight integration of left-
to-right SuperARV tagging and modifiee specifica-
tion produces a greater parsing accuracy than the
best loosely coupled counterpart. Table 3 shows
the SuperARV tagging accuracy (%) and role value
labeled precision and recall (%) for each model.
Consistent with our intuition, a stronger SuperARV
tagger and a larger search space of SuperARV se-
quences produces greater parse accuracy. However,
tightly integrating SuperARV prediction with mod-
ifiee specification achieves the greatest overall ac-
curacy. Note that SuperARV tagging accuracy and
parse accuracy improve in tandem, as can be seen
in Tables 2 and 3. These results are consistent
with the observations of (Collins, 1999) and (Eis-
ner, 1996). It is important to note that each of the
factors contributing to improved parse accuracy in
these two experiments also improved the word pre-
diction capability of the corresponding parser-based
LM (Wang and Harper, 2003).
5.2 Comparing to Other Parsers
Charniak?s state-of-the-art PCFG parser (Charniak,
2000) has achieved the highest PARSEVAL LP/LR
when compared to Collins? Model 2 and Model
3 (Collins, 1999), Roark?s (Roark, 2001), Ratna-
parkhi?s (Ratnaparkhi, 1999), and Xu & Chelba?s
(Xu et al, 2002) parsers. Hence, we will com-
pare our best loosely integrated and tightly inte-
grated SCDG parsers to Charniak?s parser. Ad-
ditionally, we will compare with Collins? Model
Table 3: Results on Section 23 of the WSJ Penn Tree-
bank comparing models that utilize different SuperARV
taggers and N-best sizes with the tightly coupled imple-
mentation. Note L denotes Loose coupling and T de-
notes Tight coupling. Also (a) denotes trigram, 40-best;
(b) denotes trigram, 100-best; (c) denotes 4-gram, 40-
best; (d) denotes 4-gram, 100-best.
Models ? 40 words (2,245 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
L (a) 96.2 91.5 91.2 88.0 87.4
(b) 96.7 91.9 91.5 88.3 87.7
(c) 96.9 92.2 91.7 88.6 88.1
(d) 97.2 92.4 92.3 89.1 88.6
T 97.4 93.2 92.9 89.8 89.2
Models ? 100 words (2,416 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
L (a) 95.4 90.9 90.5 87.5 86.8
(b) 95.8 91.3 90.8 87.7 87.0
(c) 96.0 91.7 91.2 88.0 87.4
(d) 96.3 91.8 91.5 88.5 87.8
T 96.6 92.6 92.2 89.1 88.5
2 since it makes the complement/adjunct distinc-
tion and Model 3 since it handles wh-movement
(Collins, 1999). Charniak?s parser does not explic-
itly model these phenomena.
Among the statistical CFG parsers to be com-
pared, only Collins? Model 3 produces trees with
information about wh-movement. Since the trans-
former uses empty node information to transform
the CFG parse trees to CDG parses, the accuracy
of Charniak?s parser and Collins? Model 2 may be
slightly reduced for sentences with empty nodes.
Hence, we compare results on two test sets: one that
omits all sentences with traces and one that does not.
As can be seen in Table 4, our tightly coupled parser
consistently produces an accuracy that equals or ex-
ceeds the accuracies of the other parsers, with one
exception (Collins? Model 3), regardless of whether
the test set contains sentences with traces.
Using our evaluation metrics, Collins? Model 3
achieves a better precision/recall than Model 2 and
Charniak?s parser. Since trace information is used
by the CFG-to-CDG transformer to generate cer-
tain lexical features (Wang, 2003), the output from
Model 3 is likely to be mapped to more accu-
rate CDG parses. Although Charniak?s maximum-
entropy inspired parser achieved the highest PAR-
SEVAL results, Collins? Model 3 is more accu-
rate using our dependency metric, possibly be-
cause it makes the complement/adjunct distinction
and models wh-movement. Since the statistical
Table 4: Evaluation of five models on Section 23 sentences with and without traces: L denotes the best loosely
coupled CDG parser and T the tightly coupled CDG parser.
Models ? 40 words (2,245 sentences)
Without TRACE All
(1,903 sentences) (2,245 sentences)
governor only all roles governor only all roles
RLP RLR RLP RLR RLP RLR RLP RLR
L 92.4 92.4 89.5 88.7 92.4 92.3 89.1 88.6
T 93.2 92.9 89.9 89.3 93.2 92.9 89.8 89.2
Charniak (Charniak, 2000) 92.6 92.5 89.4 88.9 92.5 92.3 88.9 88.7
Collins, Model 2 (Collins, 1999) 92.5 92.3 89.1 88.5 92.2 92.1 89.0 88.5
Collins, Model 3 (Collins, 1999) 92.8 92.7 89.9 89.4 92.7 92.4 89.3 89.1
Models ? 100 words (2,416 sentences)
Without TRACE All
(1,979 sentences) (2,416 sentences)
governor only all roles governor only all roles
RLP RLR RLP RLR RLP RLR RLP RLR
L 91.9 91.6 88.8 88.1 91.8 91.5 88.5 87.8
T 92.7 92.3 89.4 88.7 92.6 92.2 89.1 88.5
Charniak (Charniak, 2000) 92.0 91.8 88.8 88.2 91.9 91.6 88.4 87.9
Collins, Model 2 (Collins, 1999) 91.8 91.6 88.6 88.0 91.7 91.5 88.2 87.9
Collins, Model 3 (Collins, 1999) 92.2 92.1 89.4 88.8 92.1 91.9 88.8 88.5
CFG parsers may loose accuracy from the CFG-to-
CDG transformation, similarly to Collins? experi-
ment reported in (Hajic et al, 1998), we also trans-
formed our CDG parses to Penn Treebank style
CFG parse trees and scored them using PARSE-
VAL. On the WSJ PTB test set, Charniak?s parser
achieved 89.6% LR and 89.5% LP, Collins? Model 2
and 3 obtained 88.1% LR and 88.3% LP and 88.0%
LR and 88.3% LP, while the tightly coupled CDG
parser obtains 85.8% LR and 86.4% LP. It is im-
portant to remember that this score is impacted by
two lossy conversions, one for training and one for
testing.
We have conducted a non-parametric Monte
Carlo test to determine the significance of the differ-
ences between the parsing accuracy results in Table
3 and Table 4. We found that the difference between
the tightly and loosely coupled SCDG parsers is sta-
tistically significant, as well as the difference be-
tween the SCDG parser and Charniak?s parser and
Collins? Model 2. Although the difference between
our parser and Collins? Model 3 is not statistically
significant, our parser represents a first attempt to
build a high quality SCDG parser, and there is still
room for improvement, e.g., better handling of bar-
riers (including punctuation) and employing more
sophisticated search and pruning strategies.
This paper has presented a statistical implemen-
tation of a CDG parser, which is both genera-
tive and highly lexicalized. With a framework
of tightly integrated, multiple knowledge sources,
model distance, and synergistic dependencies, we
have achieved a parsing accuracy comparable to the
state-of-the-art statistical parsers trained on the Wall
Street Journal Penn Treebank corpus. However,
more work must be done to build a parser model
capable of coping with speech disfluencies present
in spontaneous speech. We also intend to investi-
gate a hybrid parser that combines the generality of
a CFG with the specificity of a CDG.
References
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the Fourteenth National Conference on Artificial In-
telligence.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the First Annual Meeting
of the North American Association for Computational
Linguistics.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings of ACL?2001.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. M. Eisner. 1996. An empirical comparison of prob-
ability models for dependency grammar. Technical
report, University of Pennsylvania, CIS Department,
Philadelphia PA 19104-6389.
J. Hajic, E. Brill, M. Collins, B. Hladka, D. Jones,
C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann, and
D. Zeman. 1998. Core natural language processing
technology applicable to multiple languages ? Work-
shop ?98. Technical report, Johns Hopkins Univ.
M. P. Harper and R. A. Helzerman. 1995. Extensions
to constraint dependency parsing for spoken language
processing. Computer Speech and Language.
M. P. Harper, W. Wang, and C. M. White. 2001. Ap-
proaches for learning constraint dependency grammar
from corpora. In Proceedings of the Grammar and
Natural Language Processing Conference, Montreal,
Canada.
F. Jelinek. 1997. Statistical Methods For Speech Recog-
nition. The MIT Press.
D. Lin. 1995. A dependency-based method for evaluat-
ing broad-coverage parsers. In Proceedings of the In-
ternational Joint Conference on Artificial Intelligence,
pages 1420?1427.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 276?283.
A. Ratnaparkhi. 1999. Learning to parse natural lan-
guage with maximum entropy models. Machine
Learning, 34:151?175.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
W. Wang and M. P. Harper. 2002. The SuperARV lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proceed-
ings of Conference of Empirical Methods in Natural
Language Processing.
W. Wang and M. P. Harper. 2003. Language model-
ing using a statistical dependency grammar parser. In
Proceedings of International Workshop on Automatic
Speech Recognition and Understanding.
W. Wang, M. P. Harper, and A. Stolcke. 2003. The ro-
bustness of an almost-parsing language model given
errorful training data. In ICASSP 2003.
W. Wang, A. Stolcke, and M. P. Harper. 2004. The use
of a linguistically motivated language model in con-
versational speech recognition. In ICASSP 2004.
W. Wang. 2003. Statistical Parsing and Language Mod-
eling based on Constraint Dependency Grammar.
Ph.D. thesis, Purdue University.
P. Xu, C. Chelba, and F. Jelinek. 2002. A study on richer
syntactic dependencies for structured language mod-
eling. In Proceedings of ACL 2002.
S. J. Young, J. Odell, D. Ollason, V. Valtchev, and P. C.
Woodland, 1997. The HTK Book. Entropic Cam-
bridge Research Laboratory, Ltd.
Comparing and Combining Generative and Posterior Probability Models:
Some Advances in Sentence Boundary Detection in Speech
Yang Liu
ICSI and Purdue University
yangl@icsi.berkeley.edu
Andreas Stolcke Elizabeth Shriberg
SRI and ICSI
stolcke,ees@speech.sri.com
Mary Harper
Purdue University
harper@ecn.purdue.edu
Abstract
We compare and contrast two different models for
detecting sentence-like units in continuous speech.
The first approach uses hidden Markov sequence
models based on N-grams and maximum likeli-
hood estimation, and employs model interpolation
to combine different representations of the data.
The second approach models the posterior proba-
bilities of the target classes; it is discriminative and
integrates multiple knowledge sources in the max-
imum entropy (maxent) framework. Both models
combine lexical, syntactic, and prosodic informa-
tion. We develop a technique for integrating pre-
trained probability models into the maxent frame-
work, and show that this approach can improve
on an HMM-based state-of-the-art system for the
sentence-boundary detection task. An even more
substantial improvement is obtained by combining
the posterior probabilities of the two systems.
1 Introduction
Sentence boundary detection is a problem that has
received limited attention in the text-based com-
putational linguistics community (Schmid, 2000;
Palmer and Hearst, 1994; Reynar and Ratnaparkhi,
1997), but which has recently acquired renewed im-
portance through an effort by the DARPA EARS
program (DARPA Information Processing Technol-
ogy Office, 2003) to improve automatic speech tran-
scription technology. Since standard speech recog-
nizers output an unstructured stream of words, im-
proving transcription means not only that word ac-
curacy must be improved, but also that commonly
used structural features such as sentence boundaries
need to be recognized. The task is thus fundamen-
tally based on both acoustic and textual (via auto-
matic word recognition) information. From a com-
putational linguistics point of view, sentence units
are crucial and assumed in most of the further pro-
cessing steps that one would want to apply to such
output: tagging and parsing, information extraction,
and summarization, among others.
Sentence segmentation from speech is a difficult
problem. The best systems benchmarked in a re-
cent government-administered evaluation yield er-
ror rates between 30% and 50%, depending on the
genre of speech processed (measured as the num-
ber of missed and inserted sentence boundaries as
a percentage of true sentence boundaries). Because
of the difficulty of the task, which leaves plenty of
room for improvement, its relevance to real-world
applications, and the range of potential knowledge
sources to be modeled (acoustics and text-based,
lower- and higher-level), this is an interesting chal-
lenge problem for statistical and computational ap-
proaches.
All of the systems participating in the recent
DARPA RT-03F Metadata Extraction evaluation
(National Institute of Standards and Technology,
2003) were based on a hidden Markov model frame-
work, in which word/tag sequences are modeled by
N-gram language models (LMs). Additional fea-
tures (mostly reflecting speech prosody) are mod-
eled as observation likelihoods attached to the N-
gram states of the HMM (Shriberg et al, 2000). The
HMM is a generative modeling approach, since it
describes a stochastic process with hidden variables
(the locations of sentence boundaries) that produces
the observable data. The segmentation is inferred
by comparing the likelihoods of different boundary
hypotheses.
While the HMM approach is computationally ef-
ficient and (as described later) provides a convenient
way for modularizing the knowledge sources, it has
two main drawbacks: First, the standard training
methods for HMMs maximize the joint probability
of observed and hidden events, as opposed to the
posterior probability of the correct hidden variable
assignment given the observations. The latter is a
criterion more closely related to classification error.
Second, the N-gram LM underlying the HMM tran-
sition model makes it difficult to use features that
are highly correlated (such as word and POS labels)
without greatly increasing the number of model pa-
rameters; this in turn would make robust estimation
channelword string
prosody
idea
syntax, semantics,
word selection,
puntuation
impose prosody
signal
prosodic feature
extraction
prosodic
features
textual feature
speech
recognizer
word string
word, POS,
classes
fusion of
knowledge
souces
sentence boundary
hypothesis
Figure 1: Diagram of the sentence segmentation task.
difficult.
In this paper, we describe our effort to overcome
these shortcomings by 1) replacing the generative
model with one that estimates the posterior proba-
bilities directly, and 2) using the maximum entropy
(maxent) framework to estimate conditional distri-
butions, giving us a more principled way to com-
bine a large number of overlapping features. Both
techniques have been used previously for traditional
NLP tasks, but they are not straightforward to ap-
ply in our case because of the diverse nature of the
knowledge sources used in sentence segmentation.
We describe the techniques we developed to work
around these difficulties, and compare classification
accuracy of the old and new approach on different
genres of speech. We also investigate how word
recognition error affects that comparison. Finally,
we show that a simple combination of the two ap-
proaches turns out to be highly effective in improv-
ing the best previous results obtained on a bench-
mark task.
2 The Sentence Segmentation Task
The sentence boundary detection problem is de-
picted in Figure 1 in the source-channel framework.
The speaker intends to say something, chooses the
word string, and imposes prosodic cues (duration,
emphasis, intonation, etc). This signal goes through
the speech production channel to generate an acous-
tic signal. A speech recognizer determines the most
likely word string given this signal. To detect pos-
sible sentence boundaries in the recognized word
string, prosodic features are extracted from the sig-
nal, and combined with textual cues obtained from
the word string. At issue in this paper is the final
box in the diagram: how to model and combine the
available knowledge sources to find the most accu-
rate hypotheses.
Note that this problem differs from the sen-
tence boundary detection problem for written text in
the natural language processing literature (Schmid,
2000; Palmer and Hearst, 1994; Reynar and Rat-
naparkhi, 1997). Here we are dealing with spo-
ken language, therefore there is no punctuation in-
formation, the words are not capitalized, and the
transcripts from the recognition output are errorful.
This lack of textual cues is partly compensated by
prosodic information (timing, pitch, and energy pat-
terns) conveyed by speech. Also note that in spon-
taneous conversational speech ?sentence? is not al-
ways a straightforward notion. For our purposes we
use the definition of a ?sentence-like unit?, or SU,
as defined by the LDC for labeling and evaluation
purposes (Strassel, 2003).
The training data has SU boundaries marked by
annotators, based on both the recorded speech and
its transcription. In testing, a system has to recover
both the words and the locations of sentence bound-
aries, denoted by (W;E) = w
1
e
1
w
2
: : : w
i
e
i
: : : w
n
where W represents the strings of word tokens and
E the inter-word boundary events (sentence bound-
ary or no boundary).
The system output is scored by first finding a min-
imum edit distance alignment between the hypothe-
sized word string and the reference, and then com-
paring the aligned event labels. The SU error rate is
defined as the total number of deleted or inserted SU
boundary events, divided by the number of true SU
boundaries.1 For diagnostic purposes a secondary
evaluation condition allows use of the correct word
transcripts. This condition allows us to study the
segmentation task without the confounding effect of
speech recognition errors, using perfect lexical in-
formation.
3 Features and Knowledge Sources
Words and sentence boundaries are mutually con-
strained via syntactic structure. Therefore, the word
identities themselves (from automatic recognition
or human transcripts) constitute a primary knowl-
edge source for the sentence segmentation task. We
also make use of various automatic taggers that map
the word sequence to other representations. The
TnT tagger (Brants, 2000) is used to obtain part-of-
speech (POS) tags. A TBL chunker trained on Wall
Street Journal corpus (Ngai and Florian, 2001) maps
each word to an associated chunk tag, encoding
chunk type and relative word position (beginning of
an NP, inside a VP, etc.). The tagged versions of
the word stream are provided to allow generaliza-
tions based on syntactic structure and to smooth out
possibly undertrained word-based probability esti-
1This is the same as simple per-event classification accu-
racy, except that the denominator counts only the ?marked?
events, thereby yielding error rates that are much higher than
if one uses all potential boundary locations.
mates. For the same reasons we also generate word
class labels that are automatically induced from bi-
gram word distributions (Brown et al, 1992).
To model the prosodic structure of sentence
boundaries, we extract several hundred features
around each word boundary. These are based on the
acoustic alignments produced by a speech recog-
nizer (or forced alignments of the true words when
given). The features capture duration, pitch, and
energy patterns associated with the word bound-
aries. Informative features include the pause du-
ration at the boundary, the difference in pitch be-
fore and after the boundary, and so on. A cru-
cial aspect of many of these features is that they
are highly correlated (e.g., by being derived from
the same raw measurements via different normaliza-
tions), real-valued (not discrete), and possibly unde-
fined (e.g., unvoiced speech regions have no pitch).
These properties make prosodic features difficult to
model directly in either of the approaches we are ex-
amining in the paper. Hence, we have resorted to a
modular approach: the information from prosodic
features is modeled separately by a decision tree
classifier that outputs posterior probability estimates
P (e
i
jf
i
), where e
i
is the boundary event after w
i
,
and f
i
is the prosodic feature vector associated with
the word boundary. Conveniently, this approach
also permits us to include some non-prosodic fea-
tures that are highly relevant for the task, but not
otherwise represented, such as whether a speaker
(turn) change occurred at the location in question.2
A practical issue that greatly influences model de-
sign is that not all information sources are avail-
able uniformly for all training data. For example,
prosodic modeling assumes acoustic data; whereas,
word-based models can be trained on text-only data,
which is usually available in much larger quantities.
This poses a problem for approaches that model all
relevant information jointly and is another strong
motivation for modular approaches.
4 The Models
4.1 Hidden Markov Model for Segmentation
Our baseline model, and the one that forms the ba-
sis of much of the prior work on acoustic sentence
segmentation (Shriberg et al, 2000; Gotoh and Re-
nals, 2000; Christensen, 2001; Kim and Woodland,
2001), is a hidden Markov model. The states of
the model correspond to words w
i
and following
2Here we are glossing over some details on prosodic mod-
eling that are orthogonal to the discussion in this paper. For
example, instead of simple decision trees we actually use en-
semble bagging to reduce the variance of the classifier (Liu et
al., 2004).
Wi Ei
Fi
Oi
Wi+1 Ei+1
Oi+1
Wi Fi+1Wi+1
Figure 2: The graphical model for the SU detection
problem. Only one word+event is depicted in each state,
but in a model based on N-grams the previous N   1
tokens would condition the transition to the next state.
event labels e
i
. The observations associated with
the states are the words, as well as other (mainly
prosodic) features f
i
. Figure 2 shows a graphi-
cal model representation of the variables involved.
Note that the words appear in both the states and the
observations, such that the word stream constrains
the possible hidden states to matching words; the
ambiguity in the task stems entirely from the choice
of events.
4.1.1 Classification
Standard algorithms are available to extract the most
probable state (and thus event) sequence given a set
of observations. The error metric is based on clas-
sification of individual word boundaries. Therefore,
rather than finding the highest probability sequence
of events, we identify the events with highest poste-
rior individually at each boundary i:
e^
i
= arg max
e
i
P (e
i
jW;F ) (1)
where W and F are the words and features for
the entire test sequence, respectively. The individ-
ual event posteriors are obtained by applying the
forward-backward algorithm for HMMs (Rabiner
and Juang, 1986).
4.1.2 Model Estimation
Training of the HMM is supervised since event-
labeled data is available. There are two sets of pa-
rameters to estimate. The state transition proba-
bilities are estimated using a hidden event N-gram
LM (Stolcke and Shriberg, 1996). The LM is
obtained with standard N-gram estimation meth-
ods from data that contains the word+event tags in
sequence: w
1
; e
1
; w
2
; : : : e
n 1
; w
n
. The resulting
LM can then compute the required HMM transition
probabilities as3
P (w
i
e
i
jw
1
e
1
: : : w
i 1
e
i 1
) =
P (w
i
jw
1
e
1
: : : w
i 1
e
i 1
) 
P (e
i
jw
1
e
1
: : : w
i 1
e
i 1
w
i
)
The N-gram estimator maximizes the joint
word+event sequence likelihood P (W;E) on the
training data (modulo smoothing), and does not
guarantee that the correct event posteriors needed
for classification according to Equation (1) are
maximized.
The second set of HMM parameters are the ob-
servation likelihoods P (f
i
je
i
; w
i
). Instead of train-
ing a likelihood model we make use of the prosodic
classifiers described in Section 3. We have at our
disposal decision trees that estimate P (e
i
jf
i
). If
we further assume that prosodic features are inde-
pendent of words given the event type (a reasonable
simplification if features are chosen appropriately),
observation likelihoods may be obtained by
P (f
i
jw
i
; e
i
) =
P (e
i
jf
i
)
P (e
i
)
P (f
i
) (2)
Since P (f
i
) is constant we can ignore it when car-
rying out the maximization (1).
4.1.3 Knowledge Combination
The HMM structure makes strong independence as-
sumptions: (1) that features depend only on the cur-
rent state (and in practice, as we saw, only on the
event label) and (2) that each word+event label de-
pends only on the last N   1 tokens. In return, we
get a computationally efficient structure that allows
information from the entire sequence W;F to in-
form the posterior probabilities needed for classifi-
cation, via the forward-backward algorithm.
More problematic in practice is the integration
of multiple word-level features, such as POS tags
and chunker output. Theoretically, all tags could
simply be included in the hidden state representa-
tion to allow joint modeling of words, tags, and
events. However, this would drastically increase the
size of the state space, making robust model estima-
tion with standard N-gram techniques difficult. A
method that works well in practice is linear inter-
polation, whereby the conditional probability esti-
mates of various models are simply averaged, thus
reducing variance. In our case, we obtain good re-
sults by interpolating a word-N-gram model with
3To utilize word+event contexts of length greater than one
we have to employ HMMs of order 2 or greater, or equivalently,
make the entire word+event N-gram be the state.
one based on automatically induced word classes
(Brown et al, 1992).
Similarly, we can interpolate LMs trained from
different corpora. This is usually more effective
than pooling the training data because it allows con-
trol over the contributions of the different sources.
For example, we have a small corpus of training data
labeled precisely to the LDC?s SU specifications,
but a much larger (130M word) corpus of standard
broadcast new transcripts with punctuation, from
which an approximate version of SUs could be in-
ferred. The larger corpus should get a larger weight
on account of its size, but a lower weight given the
mismatch of the SU labels. By tuning the interpola-
tion weight of the two LMs empirically (using held-
out data) the right compromise was found.
4.2 Maxent Posterior Probability Model
As observed, HMM training does not maximize the
posterior probabilities of the correct labels. This
mismatch between training and use of the model
as a classifier would not arise if the model directly
estimated the posterior boundary label probabilities
P (e
i
jW;F ). A second problem with HMMs is that
the underlying N-gram sequence model does not
cope well with multiple representations (features) of
the word sequence (words, POS, etc.) short of build-
ing a joint model of all variables. This type of sit-
uation is well-suited to a maximum entropy formu-
lation (Berger et al, 1996), which allows condition-
ing features to apply simultaneously, and therefore
gives greater freedom in choosing representations.
Another desirable characteristic of maxent models
is that they do not split the data recursively to condi-
tion their probability estimates, which makes them
more robust than decision trees when training data
is limited.
4.2.1 Model Formulation and Training
We built a posterior probability model for sentence
boundary classification in the maxent framework.
Such a model takes the familiar exponential form4
P (ejW;F ) =
1
Z

(W;F )
e
P
k

k
g
k
(e;W;F ) (3)
where Z

(W;F ) is the normalization term:
Z

(W;F ) =
X
e
0
e
P
k

k
g
k
(e
0
;W;F ) (4)
The functions g
k
(e;W;F ) are indicator functions
corresponding to (complex) features defined over
4We omit the index i from e here since the ?current? event
is meant in all cases.
events, words, and prosodic features. For example,
one such feature function might be:
g(e;W;F ) =
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 423?431,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Who, What, When, Where, Why?  
Comparing Multiple Approaches to the Cross-Lingual 5W Task 
Kristen Parton*, Kathleen R. McKeown*, Bob Coyne*, Mona T. Diab*,  
Ralph Grishman?, Dilek Hakkani-T?r?, Mary Harper?, Heng Ji?, Wei Yun Ma*,  
Adam Meyers?, Sara Stolbach*, Ang Sun?, Gokhan Tur?, Wei Xu? and Sibel Yaman? 
 
*Columbia University 
New York, NY, USA 
{kristen, kathy, 
coyne, mdiab, ma, 
sara}@cs.columbia.edu 
 
?New York University 
New York, NY, USA 
{grishman, meyers, 
asun, xuwei} 
@cs.nyu.edu 
?International Computer 
Science Institute 
Berkeley, CA, USA 
{dilek, sibel} 
@icsi.berkeley.edu 
 
?Human Lang. Tech. Ctr. of 
Excellence, Johns Hopkins 
and U. of Maryland, 
College Park  
mharper@umd.edu 
?City University of  
New York 
New York, NY, USA 
hengji@cs.qc.cuny.edu 
 
 
?SRI International 
Palo Alto, CA, USA 
gokhan@speech.sri.com 
 
 
  
Abstract 
Cross-lingual tasks are especially difficult 
due to the compounding effect of errors in 
language processing and errors in machine 
translation (MT). In this paper, we present an 
error analysis of a new cross-lingual task: the 
5W task, a sentence-level understanding task 
which seeks to return the English 5W's (Who, 
What, When, Where and Why) corresponding 
to a Chinese sentence. We analyze systems 
that we developed, identifying specific prob-
lems in language processing and MT that 
cause errors. The best cross-lingual 5W sys-
tem was still 19% worse than the best mono-
lingual 5W system, which shows that MT 
significantly degrades sentence-level under-
standing. Neither source-language nor target-
language analysis was able to circumvent 
problems in MT, although each approach had 
advantages relative to the other. A detailed 
error analysis across multiple systems sug-
gests directions for future research on the 
problem. 
1 Introduction 
In our increasingly global world, it is ever more 
likely for a mono-lingual speaker to require in-
formation that is only available in a foreign lan-
guage document. Cross-lingual applications ad-
dress this need by presenting information in the 
speaker?s language even when it originally ap-
peared in some other language, using machine 
translation (MT) in the process. In this paper, we 
present an evaluation and error analysis of a 
cross-lingual application that we developed for a 
government-sponsored evaluation, the 5W task. 
The 5W task seeks to summarize the informa-
tion in a natural language sentence by distilling it 
into the answers to the 5W questions: Who, 
What, When, Where and Why. To solve this 
problem, a number of different problems in NLP 
must be addressed: predicate identification, ar-
gument extraction, attachment disambiguation, 
location and time expression recognition, and 
(partial) semantic role labeling. In this paper, we 
address the cross-lingual 5W task: given a 
source-language sentence, return the 5W?s trans-
lated (comprehensibly) into the target language. 
Success in this task requires a synergy of suc-
cessful MT and answer selection.  
The questions we address in this paper are: 
? How much does machine translation (MT) 
degrade the performance of cross-lingual 
5W systems, as compared to monolingual 
performance? 
? Is it better to do source-language analysis 
and then translate, or do target-language 
analysis on MT? 
? Which specific problems in language 
processing and/or MT cause errors in 5W 
answers?  
In this evaluation, we compare several differ-
ent approaches to the cross-lingual 5W task, two 
that work on the target language (English) and 
one that works in the source language (Chinese). 
423
A central question for many cross-lingual appli-
cations is whether to process in the source lan-
guage and then translate the result, or translate 
documents first and then process the translation. 
Depending on how errorful the translation is, 
results may be more accurate if models are de-
veloped for the source language. However, if 
there are more resources in the target language, 
then the translate-then-process approach may be 
more appropriate. We present a detailed analysis, 
both quantitative and qualitative, of how the ap-
proaches differ in performance.  
We also compare system performance on hu-
man translation (which we term reference trans-
lations) and MT of the same data in order to de-
termine how much MT degrades system per-
formance. Finally, we do an in-depth analysis of 
the errors in our 5W approaches, both on the 
NLP side and the MT side. Our results provide 
explanations for why different approaches suc-
ceed, along with indications of where future ef-
fort should be spent. 
2 Prior Work 
The cross-lingual 5W task is closely related to 
cross-lingual information retrieval and cross-
lingual question answering (Wang and Oard 
2006; Mitamura et al 2008). In these tasks, a 
system is presented a query or question in the 
target language and asked to return documents or 
answers from a corpus in the source language. 
Although MT may be used in solving this task, it 
is only used by the algorithms ? the final evalua-
tion is done in the source language. However, in 
many real-life situations, such as global business, 
international tourism, or intelligence work, users 
may not be able to read the source language. In 
these cases, users must rely on MT to understand 
the system response. (Parton et al 2008) exam-
ine the case of ?translingual? information re-
trieval, where evaluation is done on translated 
results in the target language. In cross-lingual 
information extraction (Sudo et al 2004) the 
evaluation is also done on MT, but the goal is to 
learn knowledge from a large corpus, rather than 
analyzing individual sentences.  
The 5W task is also closely related to Seman-
tic Role Labeling (SRL), which aims to effi-
ciently and effectively derive semantic informa-
tion from text. SRL identifies predicates and 
their arguments in a sentence, and assigns roles 
to each argument. For example, in the sentence 
?I baked a cake yesterday.?, the predicate 
?baked? has three arguments. ?I? is the subject of 
the predicate, ?a cake? is the object and ?yester-
day? is a temporal argument.  
Since the release of large data resources anno-
tated with relevant levels of semantic informa-
tion, such as the FrameNet (Baker et al, 1998) 
and PropBank corpora (Kingsbury and Palmer, 
2003), efficient approaches to SRL have been 
developed (Carreras and Marquez, 2005). Most 
approaches to the problem of SRL follow the 
Gildea and Jurafsky (2002) model. First, for a 
given predicate, the SRL system identifies its 
arguments' boundaries. Second, the Argument 
types are classified depending on an adopted 
lexical resource such as PropBank or FrameNet. 
Both steps are based on supervised learning over 
labeled gold standard data. A final step uses heu-
ristics to resolve inconsistencies when applying 
both steps simultaneously to the test data.  
Since many of the SRL resources are English, 
most of the SRL systems to date have been for 
English. There has been work in other languages 
such as German and Chinese (Erk 2006; Sun 
2004; Xue and Palmer 2005). The systems for 
the other languages follow the successful models 
devised for English, e.g. (Gildea and Palmer, 
2002; Chen and Rambow, 2003; Moschitti, 2004; 
Xue and Palmer, 2004; Haghighi et al, 2005). 
3 The Chinese-English 5W Task 
3.1 5W Task Description 
We participated in the 5W task as part of the 
DARPA GALE (Global Autonomous Language 
Exploitation) project. The goal is to identify the 
5W?s (Who, What, When, Where and Why) for a 
complete sentence. The motivation for the 5W 
task is that, as their origin in journalism suggests, 
the 5W?s cover the key information nuggets in a 
sentence. If a system can isolate these pieces of 
information successfully, then it can produce a 
pr?cis of the basic meaning of the sentence. Note 
that this task differs from QA tasks, where 
?Who? and ?What? usually refer to definition 
type questions. In this task, the 5W?s refer to se-
mantic roles within a sentence, as defined in Ta-
ble 1.  
In order to get al 5W?s for a sentence correct, 
a system must identify a top-level predicate, ex-
tract the correct arguments, and resolve attach-
ment ambiguity. In the case of multiple top-level 
predicates, any of the top-level predicates may be 
chosen. In the case of passive verbs, the Who is 
the agent (often expressed as a ?by clause?, or 
not stated), and the What should include the syn-
tactic subject.  
424
Answers are judged Correct1 if they identify a 
correct null argument or correctly extract an ar-
gument that is present in the sentence. Answers 
are not penalized for including extra text, such as 
prepositional phrases or subordinate clauses, 
unless the extra text includes text from another 
answer or text from another top-level predicate. 
In sentence 2a in Table 2, returning ?bought and 
cooked? for the What would be Incorrect. Simi-
larly, returning ?bought the fish at the market? 
for the What would also be Incorrect, since it 
contains the Where. Answers may also be judged 
Partial, meaning that only part of the answer was 
returned. For example, if the What contains the 
predicate but not the logical object, it is Partial.  
Since each sentence may have multiple correct 
sets of 5W?s, it is not straightforward to produce 
a gold-standard corpus for automatic evaluation. 
One would have to specify answers for each pos-
sible top-level predicate, as well as which parts 
of the sentence are optional and which are not 
allowed. This also makes creating training data 
for system development problematic. For exam-
ple, in Table 2, the sentence in 2a and 2b is the 
same, but there are two possible sets of correct 
answers. Since we could not rely on a gold-
standard corpus, we used manual annotation to 
judge our 5W system, described in section 5. 
3.2 The Cross-Lingual 5W Task 
In the cross-lingual 5W task, a system is given a 
sentence in the source language and asked to 
produce the 5W?s in the target language. In this 
task, both machine translation (MT) and 5W ex-
traction must succeed in order to produce correct 
answers. One motivation behind the cross-lingual 
5W task is MT evaluation. Unlike word- or 
phrase-overlap measures such as BLEU, the 5W 
evaluation takes into account ?concept? or ?nug-
get? translation. Of course, only the top-level 
predicate and arguments are evaluated, so it is 
not a complete evaluation. But it seeks to get at 
the understandability of the MT output, rather 
than just n-gram overlap. 
Translation exacerbates the problem of auto-
matically evaluating 5W systems. Since transla-
tion introduces paraphrase, rewording and sen-
tence restructuring, the 5W?s may change from 
one translation of a sentence to another transla-
tion of the same sentence. In some cases, roles 
may swap. For example, in Table 2, sentences 1a 
and 1b could be valid translations of the same 
                                                 
1
 The specific guidelines for determining correctness 
were formulated by BAE.  
Chinese sentence. They contain the same infor-
mation, but the 5W answers are different. Also, 
translations may produce answers that are textu-
ally similar to correct answers, but actually differ 
in meaning. These differences complicate proc-
essing in the source followed by translation. 
 
Example: On Tuesday, President Obama met with 
French President Sarkozy in Paris to discuss the 
economic crisis. 
W Definition Example  
answer 
WHO Logical subject of the 
top-level predicate in 
WHAT, or null. 
President 
Obama 
WHAT One of the top-level 
predicates in the sen-
tence, and the predi-
cate?s logical object. 
met with 
French Presi-
dent Sarkozy 
WHEN ARGM-TMP of the 
top-level predicate in 
WHAT, or null. 
On Tuesday 
WHERE ARGM-LOC of the 
top-level predicate in 
WHAT, or null. 
in Paris 
WHY ARGM-CAU of the 
top-level predicate in 
WHAT, or null. 
to discuss the 
economic crisis 
Table 1. Definition of the 5W task, and 5W answers 
from the example sentence above. 
4 5W System 
We developed a 5W combination system that 
was based on five other 5W systems. We se-
lected four of these different systems for evalua-
tion: the final combined system (which was our 
submission for the official evaluation), two sys-
tems that did analysis in the target-language 
(English), and one system that did analysis in the 
source language (Chinese). In this section, we 
describe the individual systems that we evalu-
ated, the combination strategy, the parsers that 
we tuned for the task, and the MT systems.  
 Sentence WHO WHAT 
1a Mary bought a cake 
from Peter. 
Mary bought a 
cake 
1b Peter sold Mary a 
cake. 
Peter sold Mary 
2a I bought the fish at 
the market yesterday 
and cooked it today. 
I bought the 
fish 
[WHEN: 
yesterday] 
2b I bought the fish at 
the market yesterday 
and cooked it today. 
I cooked it 
[WHEN: 
today] 
Table 2. Example 5W answers. 
425
4.1 Latent Annotation Parser 
For this work, we have re-implemented and en-
hanced the Berkeley parser (Petrov and Klein 
2007) in several ways: (1) developed a new 
method to handle rare words in English and Chi-
nese; (2) developed a new model of unknown 
Chinese words based on characters in the word; 
(3) increased robustness by adding adaptive 
modification of pruning thresholds and smooth-
ing of word emission probabilities. While the 
enhancements to the parser are important for ro-
bustness and accuracy, it is even more important 
to train grammars matched to the conditions of 
use. For example, parsing a Chinese sentence 
containing full-width punctuation with a parser 
trained on half-width punctuation reduces accu-
racy by over 9% absolute F. In English, parsing 
accuracy is seriously compromised by training a 
grammar with punctuation and case to process 
sentences without them.  
We developed grammars for English and Chi-
nese trained specifically for each genre by sub-
sampling from available treebanks (for English, 
WSJ, BN, Brown, Fisher, and Switchboard; for 
Chinese, CTB5) and transforming them for a 
particular genre (e.g., for informal speech, we 
replaced symbolic expressions with verbal forms 
and remove punctuation and case) and by utiliz-
ing a large amount of genre-matched self-labeled 
training parses. Given these genre-specific 
parses, we extracted chunks and POS tags by 
script. We also trained grammars with a subset of 
function tags annotated in the treebank that indi-
cate case role information (e.g., SBJ, OBJ, LOC, 
MNR) in order to produce function tags.   
4.2 Individual 5W Systems 
The English systems were developed for the 
monolingual 5W task and not modified to handle 
MT. They used hand-crafted rules on the output 
of the latent annotation parser to extract the 5Ws.  
English-function used the function tags from 
the parser to map parser constituents to the 5Ws. 
First the Who, When, Where and Why were ex-
tracted, and then the remaining pieces of the sen-
tence were returned as the What. The goal was to 
make sure to return a complete What answer and 
avoid missing the object. 
English-LF, on the other hand, used a system 
developed over a period of eight years (Meyers 
et al 2001) to map from the parser?s syntactic 
constituents into logical grammatical relations 
(GLARF), and then extracted the 5Ws from the 
logical form. As a back-up, it also extracted 
GLARF relations from another English-treebank 
trained parser, the Charniak parser (Charniak 
2001). After the parses were both converted to 
the 5Ws, they were then merged, favoring the 
system that: recognized the passive, filled more 
5W slots or produced shorter 5W slots (provid-
ing that the WHAT slot consisted of more than 
just the verb). A third back-up method extracted 
5Ws from part-of-speech tag patterns. Unlike 
English-function, English-LF explicitly tried to 
extract the shortest What possible, provided there 
was a verb and a possible object, in order to 
avoid multiple predicates or other 5W answers.  
Chinese-align uses the latent annotation 
parser (trained for Chinese) to parse the Chinese 
sentences. A dependency tree converter (Johans-
son and Nuges 2007) was applied to the constitu-
ent-based parse trees to obtain the dependency 
relations and determine top-level predicates. A 
set of hand-crafted dependency rules based on 
observation of Chinese OntoNotes were used to 
map from the Chinese function tags into Chinese 
5Ws.  Finally, Chinese-align used the alignments 
of three separate MT systems to translate the 
5Ws: a phrase-based system, a hierarchical 
phrase-based system, and a syntax augmented 
hierarchical phrase-based system. Chinese-align 
faced a number of problems in using the align-
ments, including the fact that the best MT did not 
always have the best alignment. Since the predi-
cate is essential, it tried to detect when verbs 
were deleted in MT, and back-off to a different 
MT system. It also used strategies for finding 
and correcting noisy alignments, and for filtering 
When/Where answers from Who and What.  
4.3 Hybrid System 
A merging algorithm was learned based on a de-
velopment test set. The algorithm selected all 
5W?s from a single system, rather than trying to 
merge W?s from different systems, since the 
predicates may vary across systems. For each 
document genre (described in section 5.4), we 
ranked the systems by performance on the devel-
opment data. We also experimented with a vari-
ety of features (for instance, does ?What? include 
a verb). The best-performing features were used 
in combination with the ranked list of priority 
systems to create a rule-based merger. 
4.4 MT Systems 
The MT Combination system used by both of the 
English 5W systems combined up to nine sepa-
rate MT systems. System weights for combina-
tion were optimized together with the language 
426
model score and word penalty for a combination 
of BLEU and TER (2*(1-BLEU) + TER). Res-
coring was applied after system combination us-
ing large language models and lexical trigger 
models. Of the nine systems, six were phrased-
based systems (one of these used chunk-level 
reordering of the Chinese, one used word sense 
disambiguation, and one used unsupervised Chi-
nese word segmentation), two were hierarchical 
phrase-based systems, one was a string-to-
dependency system, one was syntax-augmented, 
and one was a combination of two other systems. 
Bleu scores on the government supplied test set 
in December 2008 were 35.2 for formal text, 
29.2 for informal text, 33.2 for formal speech, 
and 27.6 for informal speech. More details may 
be found in (Matusov et al 2009). 
5 Methods 
5.1 5W Systems 
For the purposes of this evaluation2, we com-
pared the output of 4 systems: English-Function, 
English-LF, Chinese-align, and the combined 
system. Each English system was also run on 
reference translations of the Chinese sentence. 
So for each sentence in the evaluation corpus, 
there were 6 systems that each provided 5Ws. 
5.2 5W Answer Annotation 
For each 5W output, annotators were presented 
with the reference translation, the MT version, 
and the 5W answers. The 5W system names 
were hidden from the annotators. Annotators had 
to select ?Correct?, ?Partial? or ?Incorrect? for 
each W. For answers that were Partial or Incor-
rect, annotators had to further specify the source 
of the error based on several categories (de-
scribed in section 6). All three annotators were 
native English speakers who were not system 
developers for any of the 5W systems that were 
being evaluated (to avoid biased grading, or as-
signing more blame to the MT system). None of 
the annotators knew Chinese, so all of the judg-
ments were based on the reference translations. 
After one round of annotation, we measured 
inter-annotator agreement on the Correct, Partial, 
or Incorrect judgment only. The kappa value was 
0.42, which was lower than we expected. An-
other surprise was that the agreement was lower 
                                                 
2
 Note that an official evaluation was also performed by 
DARPA and BAE. This evaluation provides more fine-
grained detail on error types and gives results for the differ-
ent approaches. 
for When, Where and Why (?=0.31) than for 
Who or What (?=0.48). We found that, in cases 
where a system would get both Who and What 
wrong, it was often ambiguous how the remain-
ing W?s should be graded. Consider the sentence: 
?He went to the store yesterday and cooked lasa-
gna today.? A system might return erroneous 
Who and What answers, and return Where as ?to 
the store? and When as ?today.? Since Where 
and When apply to different predicates, they 
cannot both be correct. In order to be consistent, 
if a system returned erroneous Who and What 
answers, we decided to mark the When, Where 
and Why answers Incorrect by default. We added 
clarifications to the guidelines and discussed ar-
eas of confusion, and then the annotators re-
viewed and updated their judgments.  
After this round of annotating, ?=0.83 on the 
Correct, Partial, Incorrect judgments. The re-
maining disagreements were genuinely ambigu-
ous cases, where a sentence could be interpreted 
multiple ways, or the MT could be understood in 
various ways. There was higher agreement on 
5W?s answers from the reference text compared 
to MT text, since MT is inherently harder to 
judge and some annotators were more flexible 
than others in grading garbled MT. 
5.3 5W Error Annotation 
In addition to judging the system answers by the 
task guidelines, annotators were asked to provide 
reason(s) an answer was wrong by selecting from 
a list of predefined errors. Annotators were asked 
to use their best judgment to ?assign blame? to 
the 5W system, the MT, or both. There were six 
types of system errors and four types of MT er-
rors, and the annotator could select any number 
of errors. (Errors are described further in section 
6.) For instance, if the translation was correct, 
but the 5W system still failed, the blame would 
be assigned to the system. If the 5W system 
picked an incorrectly translated argument (e.g., 
?baked a moon? instead of ?baked a cake?), then 
the error would be assigned to the MT system. 
Annotators could also assign blame to both sys-
tems, to indicate that they both made mistakes.  
Since this annotation task was a 10-way selec-
tion, with multiple selections possible, there were 
some disagreements. However, if categorized 
broadly into 5W System errors only, MT errors 
only, and both 5W System and MT errors, then 
the annotators had a substantial level of agree-
ment (?=0.75 for error type, on sentences where 
both annotators indicated an error).  
427
5.4 5 W Corpus 
The full evaluation corpus is 350 documents, 
roughly evenly divided between four genres: 
formal text (newswire), informal text (blogs and 
newsgroups), formal speech (broadcast news) 
and informal speech (broadcast conversation). 
For this analysis, we randomly sampled docu-
ments to judge from each of the genres. There 
were 50 documents (249 sentences) that were 
judged by a single annotator. A subset of that set, 
with 22 documents and 103 sentences, was 
judged by two annotators. In comparing the re-
sults from one annotator to the results from both 
annotators, we found substantial agreement. 
Therefore, we present results from the single an-
notator so we can do a more in-depth analysis. 
Since each sentence had 5W?s, and there were 6 
systems that were compared, there were 7,500 
single-annotator judgments over 249 sentences. 
6 Results 
Figure 1 shows the cross-lingual performance 
(on MT) of all the systems for each 5W. The best 
monolingual performance (on human transla-
tions) is shown as a dashed line (% Correct 
only). If a system returned Incorrect answers for 
Who and What, then the other answers were 
marked Incorrect (as explained in section 5.2). 
For the last 3W?s, the majority of errors were due 
to this (details in Figure 1), so our error analysis 
focuses on the Who and What questions. 
6.1 Monolingual 5W Performance 
To establish a monolingual baseline, the Eng-
lish 5W system was run on reference (human) 
translations of the Chinese text. For each partial 
or incorrect answer, annotators could select one 
or more of these reasons: 
? Wrong predicate or multiple predicates. 
? Answer contained another 5W answer. 
? Passive handled wrong (WHO/WHAT). 
? Answer missed. 
? Argument attached to wrong predicate. 
Figure 1 shows the performance of the best 
monolingual system for each 5W as a dashed 
line. The What question was the hardest, since it 
requires two pieces of information (the predicate 
and object). The When, Where and Why ques-
tions were easier, since they were null most of 
the time. (In English OntoNotes 2.0, 38% of sen-
tences have a When, 15% of sentences have a 
Where, and only 2.6% of sentences have a Why.) 
The most common monolingual system error on 
these three questions was a missed answer, ac-
counting for all of the Where errors, all but one 
Why error and 71% of the When errors. The re-
maining When errors usually occurred when the 
system assumed the wrong sense for adverbs 
(such as ?then? or ?just?). 
 Missing Other 
5W 
Wrong/Multiple 
Predicates 
Wrong 
REF-func 37 29 22 7 
REF-LF 54 20 17 13 
MT-func 18 18 18 8 
MT-LF 26 19 10 11 
Chinese 23 17 14 8 
Hybrid 13 17 15 12 
Table 3. Percentages of Who/What errors attributed to 
each system error type. 
The top half of Table 3 shows the reasons at-
tributed to the Who/What errors for the reference 
corpus. Since English-LF preferred shorter an-
swers, it frequently missed answers or parts of 
Figure 1. System performance on each 5W. ?Partial? indicates that part of the answer was missing. Dashed lines 
show the performance of the best monolingual system (% Correct on human translations). For the last 3W?s, the 
percent of answers that were Incorrect ?by default? were: 30%, 24%, 27% and 22%, respectively, and 8% for the 
best monolingual system 
60 60 56 66
36 40 38 42
56 59 59 64 63
70 66 73 68 75 71 78
19201914
0
10
20
30
40
50
60
70
80
90
100
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
En
g-
fu
nc
En
g-
LF
Ch
in
es
e
Hy
br
id
WHO WHAT WHEN WHERE WHY
Partia l
Correct
90
75 81
83 90
Best 
mono-
lingual
428
answers. English-LF also had more Partial an-
swers on the What question: 66% Correct and 
12% Partial, versus 75% Correct and 1% Partial 
for English-function. On the other hand, English-
function was more likely to return answers that 
contained incorrect extra information, such as 
another 5W or a second predicate. 
6.2 Effect of MT on 5W Performance 
The cross-lingual 5W task requires that systems 
return intelligible responses that are semantically 
equivalent to the source sentence (or, in the case 
of this evaluation, equivalent to the reference).  
As can be seen in Figure 1, MT degrades the 
performance of the 5W systems significantly, for 
all question types, and for all systems. Averaged 
over all questions, the best monolingual system 
does 19% better than the best cross-lingual sys-
tem. Surprisingly, even though English-function 
outperformed English-LF on the reference data, 
English-LF does consistently better on MT. This 
is likely due to its use of multiple back-off meth-
ods when the parser failed.  
6.3 Source-Language vs. Target-Language 
The Chinese system did slightly worse than ei-
ther English system overall, but in the formal 
text genre, it outperformed both English systems.  
Although the accuracies for the Chinese and 
English systems are similar, the answers vary a 
lot. Nearly half (48%) of the answers can be an-
swered correctly by both the English system and 
the Chinese system. But 22% of the time, the 
English system returned the correct answer when 
the Chinese system did not. Conversely, 10% of 
the answers were returned correctly by the Chi-
nese system and not the English systems. The 
hybrid system described in section 4.2 attempts 
to exploit these complementary advantages. 
After running the hybrid system, 61% of the 
answers were from English-LF, 25% from Eng-
lish-function, 7% from Chinese-align, and the 
remaining 7% were from the other Chinese 
methods (not evaluated here). The hybrid did 
better than its parent systems on all 5Ws, and the 
numbers above indicate that further improvement 
is possible with a better combination strategy.  
6.4 Cross-Lingual 5W Error Analysis 
For each Partial or Incorrect answer, annotators 
were asked to select system errors, translation 
errors, or both. (Further analysis is necessary to 
distinguish between ASR errors and MT errors.) 
The translation errors considered were: 
? Word/phrase deleted. 
? Word/phrase mistranslated. 
? Word order mixed up. 
? MT unreadable. 
Table 4 shows the translation reasons attrib-
uted to the Who/What errors. For all systems, the 
errors were almost evenly divided between sys-
tem-only, MT-only and both, although the Chi-
nese system had a higher percentage of system-
only errors. The hybrid system was able to over-
come many system errors (for example, in Table 
2, only 13% of the errors are due to missing an-
swers), but still suffered from MT errors. 
Table 4. Percentages of Who/What errors by each 
system attributed to each translation error type. 
Mistranslation was the biggest translation 
problem for all the systems. Consider the first 
example in Figure 3. Both English systems cor-
rectly extracted the Who and the When, but for 
Mistrans-
lation 
Deletion Word 
Order 
Unreadable 
MT-func 34 18 24 18 
MT-LF 29 22 21 14 
Chinese 32 17 9 13 
Hybrid 35 19 27 18 
MT: After several rounds of reminded, I was a little bit 
Ref: After several hints, it began to come back to me. 
 ??????,?????????? 
MT: The Guizhou province, within a certain bank robber, under the watchful eyes of a weak woman, and, with a 
knife stabbed the woman. 
Ref: I saw that in a bank in Guizhou Province, robbers seized a vulnerable young woman in front of a group of 
onlookers and stabbed the woman with a knife. 
 ?????????,?????????,???????,??,???????? 
MT: Woke up after it was discovered that the property is not more than eleven people do not even said that the 
memory of the receipt of the country into the country. 
Ref: Well, after waking up, he found everything was completely changed. Apart from having additional eleven 
grandchildren, even the motherland as he recalled has changed from a socialist country to a capitalist country. 
 ?????????????,?????????,????????????????????????? 
Figure 3 Example sentences that presented problems for the 5W systems. 
 
429
What they returned ?was a little bit.? This is the 
correct predicate for the sentence, but it does not 
match the meaning of the reference. The Chinese 
5W system was able to select a better translation, 
and instead returned ?remember a little bit.? 
Garbled word order was chosen for 21-24% of 
the target-language system Who/What errors, but 
only 9% of the source-language system 
Who/What errors. The source-language word 
order problems tended to be local, within-phrase 
errors (e.g., ?the dispute over frozen funds? was 
translated as ?the freezing of disputes?). The tar-
get-language system word order problems were 
often long-distance problems. For example, the 
second sentence in Figure 3 has many phrases in 
common with the reference translation, but the 
overall sentence makes no sense. The watchful 
eyes actually belong to a ?group of onlookers? 
(deleted). Ideally, the robber would have 
?stabbed the woman? ?with a knife,? rather than 
vice versa. Long-distance phrase movement is a 
common problem in Chinese-English MT, and 
many MT systems try to handle it (e.g., Wang et 
al. 2007). By doing analysis in the source lan-
guage, the Chinese 5W system is often able to 
avoid this problem ? for example, it successfully 
returned ?robbers? ?grabbed a weak woman? for 
the Who/What of this sentence. 
Although we expected that the Chinese system 
would have fewer problems with MT deletion, 
since it could choose from three different MT 
versions, MT deletion was a problem for all sys-
tems. In looking more closely at the deletions, 
we noticed that over half of deletions were verbs 
that were completely missing from the translated 
sentence. Since MT systems are tuned for word-
based overlap measures (such as BLEU), verb 
deletion is penalized equally as, for example, 
determiner deletion. Intuitively, a verb deletion 
destroys the central meaning of a sentence, while 
a determiner is rarely necessary for comprehen-
sion. Other kinds of deletions included noun 
phrases, pronouns, named entities, negations and 
longer connecting phrases.  
Deletion also affected When and Where. De-
leting particles such as ?in? and ?when? that in-
dicate a location or temporal argument caused 
the English systems to miss the argument. Word 
order problems in MT also caused attachment 
ambiguity in When and Where. 
The ?unreadable? category was an option of 
last resort for very difficult MT sentences. The 
third sentence in Figure 3 is an example where 
ASR and MT errors compounded to create an 
unparseable sentence.  
7 Conclusions 
In our evaluation of various 5W systems, we dis-
covered several characteristics of the task. The 
What answer was the hardest for all systems, 
since it is difficult to include enough information 
to cover the top-level predicate and object, with-
out getting penalized for including too much. 
The challenge in the When, Where and Why 
questions is due to sparsity ? these responses 
occur in much fewer sentences than Who and 
What, so systems most often missed these an-
swers. Since this was a new task, this first 
evaluation showed clear issues on the language 
analysis side that can be improved in the future. 
The best cross-lingual 5W system was still 
19% worse than the best monolingual 5W sys-
tem, which shows that MT significantly degrades 
sentence-level understanding. A serious problem 
in MT for systems was deletion. Chinese con-
stituents that were never translated caused seri-
ous problems, even when individual systems had 
strategies to recover. When the verb was deleted, 
no top level predicate could be found and then all 
5Ws were wrong.  
One of our main research questions was 
whether to extract or translate first. We hypothe-
sized that doing source-language analysis would 
be more accurate, given the noise in Chinese 
MT, but the systems performed about the same. 
This is probably because the English tools (logi-
cal form extraction and parser) were more ma-
ture and accurate than the Chinese tools.  
Although neither source-language nor target-
language analysis was able to circumvent prob-
lems in MT, each approach had advantages rela-
tive to the other, since they did well on different 
sets of sentences. For example, Chinese-align 
had fewer problems with word order, and most 
of those were due to local word-order problems.  
Since the source-language and target-language 
systems made different kinds of mistakes, we 
were able to build a hybrid system that used the 
relative advantages of each system to outperform 
all systems. The different types of mistakes made 
by each system suggest features that can be used 
to improve the combination system in the future. 
Acknowledgments 
This work was supported in part by the Defense 
Advanced Research Projects Agency (DARPA) 
under contract number HR0011-06-C-0023. Any 
opinions, findings and conclusions or recom-
mendations expressed in this material are the 
authors' and do not necessarily reflect those of 
the sponsors. 
430
References  
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In 
COLING-ACL '98: Proceedings of the Conference, 
held at the University of Montr?al, pages 86?90. 
Xavier Carreras and Llu?s M?rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role 
labeling. In Proceedings of the Ninth Conference 
on Computational Natural Language Learning 
(CoNLL-2005), pages 152?164. 
Eugene Charniak. 2001. Immediate-head parsing for 
language models. In Proceedings of the 39th An-
nual Meeting on Association For Computational 
Linguistics (Toulouse, France, July 06 - 11, 2001).   
John Chen and Owen Rambow. 2003. Use of deep 
linguistic features for the recognition and labeling 
of semantic arguments. In Proceedings of the 2003 
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan. 
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? 
a toolchain for shallow semantic parsing. Proceed-
ings of LREC. 
Daniel Gildea and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. 
In Proceedings of the 40th Annual Conference of 
the Association for Computational Linguistics 
(ACL-02), Philadelphia, PA, USA. 
Mary Harper and Zhongqiang Huang. 2009. Chinese 
Statistical Parsing, chapter to appear. 
Aria Haghighi, Kristina Toutanova, and Christopher 
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of the Ninth Conference on 
Computational Natural Language Learning 
(CoNLL-2005), pages 173?176.  
Paul Kingsbury and Martha Palmer. 2003. Propbank: 
the next level of treebank. In Proceedings of Tree-
banks and Lexical Theories. 
Evgeny Matusov, Gregor Leusch, & Hermann Ney: 
Learning to combine machine translation systems.  
In: Cyril Goutte, Nicola Cancedda, Marc Dymet-
man, & George Foster (eds.) Learning machine 
translation. (Cambridge, Mass.: The MIT Press, 
2009); pp.257-276. 
Adam Meyers, Ralph Grishman, Michiko Kosaka and 
Shubin Zhao.  2001. Covering Treebanks with 
GLARF. In Proceedings of the ACL 2001 Work-
shop on Sharing Tools and Resources. Annual 
Meeting of the ACL. Association for Computa-
tional Linguistics, Morristown, NJ, 51-58. 
Teruko Mitamura, Eric Nyberg, Hideki Shima, 
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, 
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, 
Donghong Ji, and Noriko Kando. 2008. Overview 
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proceedings of the 
Seventh NTCIR Workshop Meeting. 
Alessandro Moschitti, Silvia Quarteroni, Roberto 
Basili, and Suresh Manandhar. 2007. Exploiting 
syntactic and shallow semantic kernels for question 
answer classification. In Proceedings of the 45th 
Annual Meeting of the Association of Computa-
tional Linguistics, pages 776?783.  
Kristen Parton, Kathleen R. McKeown, James Allan, 
and Enrique Henestroza. Simultaneous multilingual 
search for translingual information retrieval. In 
Proceedings of ACM 17th Conference on Informa-
tion and Knowledge Management (CIKM), Napa 
Valley, CA, 2008. 
Slav Petrov and Dan Klein. 2007. Improved Inference 
for Unlexicalized Parsing. North American Chapter 
of the Association for Computational Linguistics 
(HLT-NAACL 2007). 
Sudo, K., Sekine, S., and Grishman, R. 2004. Cross-
lingual information extraction system evaluation. 
In Proceedings of the 20th international Confer-
ence on Computational Linguistics. 
Honglin Sun and Daniel Jurafsky. 2004. Shallow Se-
mantic Parsing of Chinese. In Proceedings of 
NAACL-HLT. 
Cynthia A. Thompson, Roger Levy, and Christopher 
Manning. 2003. A generative model for semantic 
role labeling. In 14th European Conference on Ma-
chine Learning. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Dekang Lin 
and Dekai Wu, editors, Proceedings of EMNLP 
2004, pages 88?94, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics. 
Xue, Nianwen and Martha Palmer. 2005. Automatic 
semantic role labeling for Chinese verbs. InPro-
ceedings of the Nineteenth International Joint Con-
ference on Artificial Intelligence, pages 1160-1165.  
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese Syntactic Reordering for Statistical 
Machine Translation. Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), 737-745. 
Jianqiang Wang and Douglas W. Oard, 2006. "Com-
bining Bidirectional Translation and Synonymy for 
Cross-Language Information Retrieval," in 29th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Re-
trieval, pp. 202-209. 
431
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 116?120,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Transducing Logical Relations from Automatic and Manual GLARF
Adam Meyers?, Michiko Kosaka?, Heng Ji?, Nianwen Xue?,
Mary Harper?, Ang Sun?, Wei Xu? and Shasha Liao?
? New York Univ., ?Monmouth Univ., ?Brandeis Univ,, ?City Univ. of New York, ?Johns
Hopkins Human Lang. Tech. Ctr. of Excellence & U. of Maryland, College Park
Abstract
GLARF relations are generated from tree-
bank and parses for English, Chinese and
Japanese. Our evaluation of system out-
put for these input types requires consid-
eration of multiple correct answers.1
1 Introduction
Systems, such as treebank-based parsers (Char-
niak, 2001; Collins, 1999) and semantic role la-
belers (Gildea and Jurafsky, 2002; Xue, 2008), are
trained and tested on hand-annotated data. Evalu-
ation is based on differences between system out-
put and test data. Other systems use these pro-
grams to perform tasks unrelated to the original
annotation. For example, participating systems in
CONLL (Surdeanu et al, 2008; Hajic? et al, 2009),
ACE and GALE tasks merged the results of sev-
eral processors (parsers, named entity recognizers,
etc.) not initially designed for the task at hand.
This paper discusses differences between hand-
annotated data and automatically generated data
with respect to our GLARFers, systems for gen-
erating Grammatical and Logical Representation
Framework (GLARF) for English, Chinese and
Japanese sentences. The paper describes GLARF
(Meyers et al, 2001; Meyers et al, 2009) and
GLARFers and compares GLARF produced from
treebank and parses.
2 GLARF
Figure 1 includes simplified GLARF analyses for
English, Chinese and Japanese sentences. For
each sentence, a GLARFer constructs both a Fea-
ture Structure (FS) representing a constituency
analysis and a set of 31-tuples, each representing
1Support includes: NSF IIS-0534700 & IIS-0534325
Structure Alignment-based MT; DARPA HR0011-06-C-
0023 & HR0011-06-C-0023; CUNY REP & GRTI Program.
This work does not necessarily reflect views of sponsors.
up to three dependency relations between pairs of
words. Due to space limitations, we will focus on
the 6 fields of the 31-tuple represented in Figure 1.
These include: (1) a functor (func); (2) the de-
pending argument (Arg); (3) a surface (Surf) la-
bel based on the position in the parse tree with no
regularizations; (4) a logic1 label (L
?
1) for a re-
lation that reflects grammar-based regularizations
of the surface level. This marks relations for fill-
ing gaps in relative clauses or missing infinitival
subjects, represents passives as paraphrases as ac-
tives, etc. While the general framework supports
many regularizations, the relations actually repre-
sented depends on the implemented grammar, e.g.,
our current grammar of English regularizes across
passives and relative clauses, but our grammars
of Japanese and Chinese do not currently.; (5) a
logic2 label (L2) for Chinese and English, which
represents PropBank, NomBank and Penn Dis-
course Treebank relations; and (6) Asterisks (*)
indicate transparent relations, relations where the
functor inherits semantic properties of certain spe-
cial arguments (*CONJ, *OBJ, *PRD, *COMP).
Figure 1 contains several transparent relations.
The interpretation of the *CONJ relations in the
Japanese example, include not only that the nouns
[zaisan] (assets) and [seimei] (lives) are con-
joined, but also that these two nouns, together
form the object of the Japanese verb [mamoru]
(protect). Thus, for example, semantic selection
patterns should treat these nouns as possible ob-
jects for this verb. Transparent relations may serve
to neutralize some of the problematic cases of at-
tachment ambiguity. For example, in the English
sentence A number of phrases with modifiers are
not ambiguous, there is a transparent *COMP re-
lation between numbers and of and a transpar-
ent *OBJ relation between of and phrases. Thus,
high attachment of the PP with modifiers, would
have the same interpretation as low attachment
since phrases is the underlying head of number of
116
Figure 1: GLARF 5-tuples for 3 languages
phrases. In this same example, the adverb not can
be attached to either the copula are or the pred-
icative adjective, with no discernible difference in
meaning?this factor is indicated by the transparent
designation of the relations where the copula is a
functor. Transparent features also provide us with
a simple way of handling certain function words,
such as the Chinese word De which inherits the
function of its underlying head, connecting a vari-
ety of such modifiers to head nouns (an adjective
in the Chinese example.). For conjunction cases,
the number of underlying relations would multi-
ply, e.g., Mary and John bought and sold stock
would (underlyingly) have four subject relations
derived by pairing each of the underlying subject
nouns Mary and John with each of the underlying
main predicate verbs bought and sold.
3 Automatic vs. Manual Annotation
Apart from accuracy, there are several other ways
that automatic and manual annotation differs. For
Penn-treebank (PTB) parsing, for example, most
parsers (not all) leave out function tags and empty
categories. Consistency is an important goal for
manual annotation for many reasons including: (1)
in the absence of a clear correct answer, consis-
tency helps clarify measures of annotation quality
(inter-annotator agreement scores); and (2) consis-
tent annotation is better training data for machine
learning. Thus, annotation specifications use de-
faults to ensure the consistent handling of spurious
ambiguity. For example, given a sentence like I
bought three acres of land in California, the PP in
California can be attached to either acres or land
with no difference in meaning. While annotation
guidelines may direct a human annotator to prefer,
for example, high attachment, systems output may
have other preferences, e.g., the probability that
land is modified by a PP (headed by in) versus the
probability that acres can be so modified.
Even if the manual annotation for a particular
corpus is consistent when it comes to other factors
such as tokenization or part of speech, developers
of parsers sometimes change these guidelines to
suit their needs. For example, users of the Char-
niak parser (Charniak, 2001) should add the AUX
category to the PTB parts of speech and adjust
their systems to account for the conversion of the
word ain?t into the tokens IS and n?t. Similarly, to-
kenization decisions with respect to hyphens vary
among different versions of the Penn Treebank, as
well as different parsers based on these treebanks.
Thus if a system uses multiple parsers, such differ-
ences must be accounted for. Differences that are
not important for a particular application should
be ignored (e.g., by merging alternative analyses).
For example, in the case of spurious attachment
ambiguity, a system may need to either accept both
as right answers or derive a common representa-
tion for both. Of course, many of the particular
problems that result from spurious ambiguity can
be accounted for in hind sight. Nevertheless, it
is precisely this lack of a controlled environment
which adds elements of spurious ambiguity. Us-
ing new processors or training on new treebanks
can bring new instances of spurious ambiguity.
4 Experiments and Evaluation
We ran GLARFers on both manually created tree-
banks and automatically produced parses for En-
glish, Chinese and Japanese. For each corpus, we
created one or more answer keys by correcting
117
system output. For this paper, we evaluate solely
on the logic1 relations (the second column in fig-
ure 1.) Figure 2 lists our results for all three lan-
guages, based on treebank and parser input.
As in (Meyers et al, 2009), we generated 4-
tuples consisting of the following for each depen-
dency: (A) the logic1 label (SBJ, OBJ, etc.), (B)
its transparency (True or False), (C) The functor (a
single word or a named entity); and (D) the argu-
ment (a single word or a named entity). In the case
of conjunction where there was no lexical con-
junction word, we used either punctuation (com-
mas or semi-colons) or the placeholder *NULL*.
We then corrected these results by hand to produce
the answer key?an answer was correct if all four
members of the tuple were correct and incorrect
otherwise. Table 2 provides the Precision, Recall
and F-scores for our output. The F-T columns
indicates a modified F-score derived by ignoring
the +/-Transparent distinction (resulting changes
in precision, recall and F-score are the same).
For English and Japanese, an expert native
speaking linguist corrected the output. For Chi-
nese, several native speaking computational lin-
guists shared the task. By checking compatibil-
ity of the answer keys with outputs derived from
different sources (parser, treebank), we could de-
tect errors and inconsistencies. We processed the
following corpora. English: 86 sentence article
(wsj 2300) from the Wall Street Journal PTB test
corpus (WSJ); 46 sentence letter from Good Will
(LET), the first 100 sentences of a switchboard
telephone transcript (TEL) and the first 100 sen-
tences of a narrative from the Charlotte Narra-
tive and Conversation (NAR). These samples are
taken from the PTB WSJ Corpus and the SIGANN
shared subcorpus of the OANC. The filenames are:
110CYL067, NapierDianne and sw2014. Chi-
nese: a 20 sentence sample of text from the
Penn Chinese Treebank (CTB) (Xue et al, 2005).
Japanese: 20 sentences from the Kyoto Corpus
(KYO) (Kurohashi and Nagao, 1998)
5 Running the GLARFer Programs
We use Charniak, UMD and KNP parsers (Char-
niak, 2001; Huang and Harper, 2009; Kurohashi
and Nagao, 1998), JET Named Entity tagger (Gr-
ishman et al, 2005; Ji and Grishman, 2006)
and other resources in conjunction with language-
specific GLARFers that incorporate hand-written
rules to convert output of these processors into
a final representation, including logic1 struc-
ture, the focus of this paper. English GLAR-
Fer rules use Comlex (Macleod et al, 1998a)
and the various NomBank lexicons (http://
nlp.cs.nyu.edu/meyers/nombank/) for
lexical lookup. The GLARF rules implemented
vary by language as follows. English: cor-
recting/standardizing phrase boundaries and part
of speech (POS); recognizing multiword expres-
sions; marking subconstituents; labeling rela-
tions; incorporating NEs; regularizing infiniti-
val, passives, relatives, VP deletion, predica-
tive and numerous other constructions. Chi-
nese: correcting/standardizing phrase boundaries
and POS, marking subconstituents, labeling rela-
tions; regularizing copula constructions; incorpo-
rating NEs; recognizing dates and number expres-
sions. Japanese: converting to PTB format; cor-
recting/standardizing phrase boundaries and POS;
labeling relations; processing NEs, double quote
constructions, number phrases, common idioms,
light verbs and copula constructions.
6 Discussion
Naturally, the treebank-based system out-
performed parse-based system. The Charniak
parser for English was trained on the Wall Street
Journal corpus and can achieve about 90% accu-
racy on similar corpora, but lower accuracy on
other genres. Differences between treebank and
parser results for English were higher for LET and
NAR genres than for the TEL because the system
is not currently designed to handle TEL-specific
features like disfluencies. All processors were
trained on or initially designed for news corpora.
Thus corpora out of this domain usually produce
lower results. LET was easier as it consisted
mainly of short simple sentences. In (Meyers et
al., 2009), we evaluated our results on 40 Japanese
sentences from the JENAAD corpus (Utiyama
and Isahara, 2003) and achieved a higher F-score
(90.6%) relative to the Kyoto corpus, as JENAAD
tends to have fewer long complex sentences.
By using our answer key for multiple inputs, we
discovered errors and consequently improved the
quality of the answer keys. However, at times we
were also compelled to fork the answer keys?given
multiple correct answers, we needed to allow dif-
ferent answer keys corresponding to different in-
puts. For English, these items represent approxi-
mately 2% of the answer keys (there were a total
118
Treebank Parser
ID % Prec % Rec F F-T % Prec % Rec F F-T
WSJ 12381491 = 83.0
1238
1471 = 84.2 83.6 87.1
1164
1452 = 80.2
1164
1475 = 78.9 79.5 81.8
LET 419451 = 92.9
419
454 = 92.3 92.6 93.3
390
434 = 89.9
390
454 = 85.9 87.8 87.8
TEL 478627 = 76.2
478
589 = 81.2 78.6 82.2
439
587 = 74.8
439
589 = 74.5 74.7 77.4
NAR 8171013 = 80.7
817
973 =84.0 82.3 84.1
724
957 = 75.7
724
969 = 74.7 75.2 76.1
CTB 351400 = 87.8
351
394 = 89.1 88.4 88.7
352
403 = 87.3
352
438 = 80.4 83.7 83.7
KYO 525575 = 91.3
525
577 = 91.0 91.1 91.1
493
581 = 84.9
493
572 = 86.2 85.5 87.8
Figure 2: Logic1 Scores
Figure 3: Examples of Answer Key Divergences
of 74 4-tuples out of a total of 3487). Figure 3 lists
examples of answer key divergences that we have
found: (1) alternative tokenizations; (2) spurious
differences in attachment and conjunction scope;
and (3) ambiguities specific to our framework.
Examples 1 and 2 reflect different treatments of
hyphenation and contractions in treebank specifi-
cations over time. Parsers trained on different tree-
banks will either keep hyphenated words together
or separate more words at hyphens. The Treebank
treatment of can?t regularizes so that (can need
not be differentiated from ca), whereas the parser
treatment makes maintaining character offsets eas-
ier. In example 3, the Japanese parser recognizes
a single word whereas the treebank divides it into
a prefix plus stem. Example 4 is a case of differ-
ences in character encoding (zero).
Example 5 is a common case of spurious attach-
ment ambiguity for English, where a transparent
noun takes an of PP complement?nouns such as
form, variety and thousands bear the feature trans-
parent in the NOMLEX-PLUS dictionary (a Nom-
Bank dictionary based on NOMLEX (Macleod et
al., 1998b)). The relative clause attaches either
to the noun thousands or people and, therefore,
the subject gap of the relative is filled by either
thousands or people. This ambiguity is spurious
since there is no meaningful distinction between
these two attachments. Example 6 is a case of
attachment ambiguity due to a support construc-
tion (Meyers et al, 2004). The recipient of the
gift will be Goodwill regardless of whether the
PP is attached to give or gift. Thus there is not
much sense in marking one attachment more cor-
rect than the other. Example 7 is a case of conjunc-
tion ambiguity?the context does not make it clear
whether or not the pearls are part of a necklace or
just the beads are. The distinction is of little con-
sequence to the understanding of the narrative.
Example 8 is a case in which our grammar han-
dles a case ambiguously: the prenominal adjective
can be analyzed either as a simple noun plus ad-
jective phrase meaning various businesses or as a
noun plus relative clause meaning businesses that
are varied. Example 9 is a common case in Chi-
nese where the verb/noun distinction, while un-
clear, is not crucial to the meaning of the phrase ?
under either interpretation, 5 billion was exported.
7 Concluding Remarks
We have discussed challenges of automatic an-
notation when transducers of other annotation
schemata are used as input. Models underly-
ing different transducers approximate the origi-
nal annotation in different ways, as do transduc-
ers trained on different corpora. We have found
it necessary to allow for multiple correct answers,
due to such differences, as well as, genuine and
spurious ambiguities. In the future, we intend to
investigate automatic ways of identifying and han-
dling spurious ambiguities which are predictable,
including examples like 5,6 and 7 in figure 3 in-
volving transparent functors.
119
References
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL 2001, pages 116?123.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28:245?288.
R. Grishman, D. Westbrook, and A. Meyers. 2005.
Nyu?s english ace 2005 system description. In ACE
2005 Evaluation Workshop.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,
M. A. Mart??, L. Ma`rquez, A. Meyers, J. Nivre,
S. Pado?, J. ?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,
N. Xue, and Y. Zhang. 2009. The CoNLL-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In CoNLL-2009, Boulder, Col-
orado, USA.
Z. Huang and M. Harper. 2009. Self-training PCFG
Grammars with Latent Annotations across Lan-
guages. In EMNLP 2009.
H. Ji and R. Grishman. 2006. Analysis and Repair of
Name Tagger Errors. In COLING/ACL 2006, Syd-
ney, Australia.
S. Kurohashi and M. Nagao. 1998. Building a
Japanese parsed corpus while improving the pars-
ing system. In Proceedings of The 1st International
Conference on Language Resources & Evaluation,
pages 719?724.
C. Macleod, R. Grishman, and A. Meyers. 1998a.
COMLEX Syntax. Computers and the Humanities,
31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and
R. Reeves. 1998b. Nomlex: A lexicon of nominal-
izations. In Proceedings of Euralex98.
A. Meyers, M. Kosaka, S. Sekine, R. Grishman, and
S. Zhao. 2001. Parsing and GLARFing. In Pro-
ceedings of RANLP-2001, Tzigov Chark, Bulgaria.
A. Meyers, R. Reeves, and Catherine Macleod. 2004.
NP-External Arguments: A Study of Argument
Sharing in English. In The ACL 2004 Workshop
on Multiword Expressions: Integrating Processing,
Barcelona, Spain.
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. Liao,
and W. Xu. 2009. Automatic Recognition of Log-
ical Relations for English, Chinese and Japanese in
the GLARF Framework. In SEW-2009 at NAACL-
HLT-2009.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma?rquez,
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proceedings of the CoNLL-2008 Shared
Task, Manchester, GB.
M. Utiyama and H. Isahara. 2003. Reliable Mea-
sures for Aligning Japanese-English News Articles
and Sentences. In ACL-2003, pages 72?79.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase Structure Annota-
tion of a Large Corpus. Natural Language Engi-
neering.
N. Xue. 2008. Labeling Chinese Predicates with Se-
mantic roles. Computational Linguistics, 34:225?
255.
120
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1093?1102, Prague, June 2007. c?2007 Association for Computational Linguistics
Mandarin Part-of-Speech Tagging and Discriminative Reranking
Zhongqiang Huang1
1Purdue University
West Lafayette, IN 47907
zqhuang@purdue.edu
Mary P. Harper1,2
2University of Maryland
College Park, MD 20742
mharper@casl.umd.edu
Wen Wang
SRI International
Menlo Park, CA 94025
wwang@speech.sri.com
Abstract
We present in this paper methods to improve
HMM-based part-of-speech (POS) tagging
of Mandarin. We model the emission prob-
ability of an unknown word using all the
characters in the word, and enrich the stan-
dard left-to-right trigram estimation of word
emission probabilities with a right-to-left
prediction of the word by making use of the
current and next tags. In addition, we utilize
the RankBoost-based reranking algorithm
to rerank the N-best outputs of the HMM-
based tagger using various n-gram, mor-
phological, and dependency features. Two
methods are proposed to improve the gen-
eralization performance of the reranking al-
gorithm. Our reranking model achieves an
accuracy of 94.68% using n-gram and mor-
phological features on the Penn Chinese
Treebank 5.2, and is able to further improve
the accuracy to 95.11% with the addition of
dependency features.
1 Introduction
Part-of-speech (POS) tagging is potentially help-
ful for many advanced natural language processing
tasks, for example, named entity recognition, pars-
ing, and sentence boundary detection. Much re-
search has been done to improve tagging perfor-
mance for a variety of languages. The state-of-the-
art systems have achieved an accuracy of 97% for
English on the Wall Street Journal (WSJ) corpus
(which contains 4.5M words) using various mod-
els (Brants, 2000; Ratnaparkhi, 1996; Thede and
Harper, 1999). Lower accuracies have been reported
in the literature for Mandarin POS tagging (Tseng et
al., 2005; Xue et al, 2002). This is, in part, due to
the relatively small size and the different annotation
guidelines (e.g., granularity of the tag set) for the an-
notated corpus of Mandarin. Xue at el. (2002) and
Tseng at el. (2005) reported accuracies of 93% and
93.74% on CTB-I (Xue et al, 2002) (100K words)
and CTB 5.0 (500K words), respectively, each us-
ing a Maximum Entropy approach. The character-
istics of Mandarin make it harder to tag than En-
glish. Chinese words tend to have greater POS tag
ambiguity than English. Tseng at el. (2005) reported
that 29.9% of the words in CTB have more than one
POS assignment compared to 19.8% of the English
words in WSJ. Moreover, the morphological prop-
erties of Chinese words complicate the prediction of
POS type for unknown words.
These challenges for Mandarin POS tagging
suggest the need to develop more sophisticated
methods. In this paper, we investigate the use
of a discriminative reranking approach to in-
crease Mandarin tagging accuracy. Reranking ap-
proaches (Charniak and Johnson, 2005; Chen et al,
2002; Collins and Koo, 2005; Ji et al, 2006; Roark
et al, 2006) have been successfully applied to many
NLP applications, including parsing, named entity
recognition, sentence boundary detection, etc. To
the best of our knowledge, reranking approaches
have not been used for POS tagging, possibly due
to the already high levels of accuracy for English,
which leave little room for further improvement.
However, the relatively poorer performance of ex-
isting methods on Mandarin POS tagging makes
reranking a much more compelling technique to
evaluate. In this paper, we use reranking to improve
tagging performance of an HMM tagger adapted to
1093
Mandarin. Hidden Markov models are simple and
effective, but unlike discriminative models, such as
Maximum Entropy models (Ratnaparkhi, 1996) and
Conditional Random Fields (John Lafferty, 2001),
they have more difficulty utilizing a rich set of con-
ditionally dependent features. This limitation can be
overcome by utilizing reranking approaches, which
are able to make use of the features extracted from
the tagging hypotheses produced by the HMM tag-
ger. Reranking also has advantages over MaxEnt
and CRF models. It is able to use any features
extracted from entire labeled sentences, including
those that cannot be incorporated into MaxEnt and
CRF models due to inference difficulties. In addi-
tion, reranking methods are able to utilize the infor-
mation provided by N-best lists. Finally, the decod-
ing phase of reranking is much simpler.
The rest of the paper is organized as follows. We
describe the HMM tagger in Section 2. We discuss
the modifications to better handle unknown words in
Mandarin and to enrich the word emission probabil-
ities through the combination of bi-directional esti-
mations. In Section 3, we first describe the reranking
algorithm and then propose two methods to improve
its performance. We also describe the features that
will be used for Mandarin POS reranking in Sec-
tion 3. Experimental results are given in Section 4.
Conclusions and future work appear in Section 5.
2 The HMM Model
2.1 Porting English Tagger to Mandarin
The HMM tagger used in this work is a second-
order HMM tagger initially developed for English
by Thede and Harper (1999). This state-of-the-art
second-order HMM tagger uses trigram transition
probability estimations, P (ti|ti?2ti?1), and trigram
emission probability estimations, P (wi|ti?1ti). Let
ti1 denote the tag sequence t1, ? ? ? , ti, and w
i
1 denote
the word sequencew1, ? ? ? , wi. The tagging problem
can be formally defined as finding the best tag se-
quence ?(wN1 ) for the word sequence w
N
1 of length
N as follows1:
?(wN1 ) = arg max
tN1
P (tN1 |w
N
1 ) = arg max
tN1
P (tN1 w
N
1 )
P (wN1 )
= arg max
tN1
P (tN1 w
N
1 ) (1)
= arg max
tN1
?
i
P (ti|t
i?1
1 w
i?1
1 )P (wi|t
i
1w
i?1
1 )
1We assume that symbols exist implicitly for boundary con-
ditions.
? arg max
tN1
?
i
P (ti|ti?2ti?1)P (wi|ti?1ti) (2)
The best tag sequence ?(wN1 ) can be determined ef-
ficiently using the Viterbi algorithm.
For estimating emission probabilities of unknown
words (i.e., words that do not appear in the train-
ing data) in English (and similarly for other inflected
languages), a weighted sum of P (ski |ti?1ti) (with
k up to four) was used as an approximation, where
ski is the suffix of length k of word wi (e.g., s
1
i is
the last character of word wi). The suffix informa-
tion and three binary features (i.e., whether the word
is capitalized, whether the word is hyphenated, and
whether the word contains numbers) are combined
to estimate the emission probabilities of unknown
words.
The interpolation weights for smoothing tran-
sition, emission, and suffix probabilities were
estimated using the log-based Thede smoothing
method (Thede and Harper, 1999) as follows:
PThede(n-gram)
= ?(n-gram)PML(n-gram) +
(1? ?(n-gram))PThede((n-1)-gram)
where:
PML(n-gram) = the ML estimation
?(n-gram) = f(n-gram count)
f(x) =
loga(x+ 1) + b
loga(x+ 1) + (b+ 1)
While porting the HMM-based English POS tag-
ger to Mandarin is fairly straightforward for words
seen in the training data, some thought is required to
handle unknown words due to the morphology dif-
ferences between the two languages. First, in Man-
darin, there is no capitalization and no hyphenation.
Second, although Chinese has morphology, it is not
the same as in English; words tend to contain far
fewer characters than inflected words in English, so
word endings will tend to be short, say one or two
characters long. Hence, in our baseline model (de-
noted HMM baseline), we simply utilize word end-
ings of up to two characters in length along with a
binary feature of whether the word contains num-
bers or not. In the next two subsections, we describe
two ways in which we enhance this simple HMM
baseline model.
1094
2.2 Improving the Mandarin Unknown Word
Model
Chinese words are quite different from English
words, and the word formation process for Chinese
words can be quite complex (Packard, 2000). In-
deed, the last characters in a Chinese word are, in
some cases, most informative of the POS type, while
for others, it is the characters at the beginning. Fur-
thermore, it is not uncommon for a character in the
middle of a word to provide some evidence for the
POS type of the word. Hence, we chose to employ
a rather simple but effective method to estimate the
emission probability, P (wi|ti?1, ti), of an unknown
word, wi. We use the geometric average2 of the
emission probability of the characters in the word,
i.e., P (ck|ti?1, ti) with ck being the k-th character
in the word. Since some of the characters in wi may
not have appeared in any word tagged as ti in that
context in the training data, only characters that are
observed in this context are used in the computation
of the geometric average, as shown below:
P (wi|ti?1, ti) = n
? ?
ck?wi,P (ck|ti?1,ti)6=0
P (ck|ti?1, ti) (3)
where
n = |{ck ? wi|P (ck|ti?1, ti) 6= 0}|
2.3 Bi-directional Word Probability Estimation
In Equation 2, the word emission probability
P (wi|ti?1ti) is a left-to-right prediction that de-
pends on the current tag ti associated with wi, as
well as its previous tag ti?1. Although the interac-
tion between wi and the next tag ti+1 is captured to
some extent when ti+1 is generated by the model,
this implicit interaction may not be as effective as
adding the information more directly to the model.
Hence, we chose to apply the constraint explicitly in
our HMM framework by replacing P (wi|ti?1ti) in
Equation 2 with P ?(wi|ti?1ti)P 1??(wi|titi+1) for
both known and unknown words, with ?(wN1 ) deter-
mined by:
?(wN1 ) = arg max
tN1
?
i
(P (ti|ti?2ti?1)?
P?(wi|ti?1ti)P
1??(wi|titi+1)) (4)
2Based on preliminary testing, the geometric average pro-
vided greater tag accuracy than the arithmetic average.
This corresponds to a mixture model of two genera-
tion paths, one from the left and one from the right,
to approximate ?(wN1 ) in Equation 1 in a different
way.
?(wN1 ) = arg max
tN1
P (tN1 w
N
1 )
= arg max
tN1
P (tN1 )P (w
N
1 |t
N
1 )
P (tN1 ) ?
?
i
P (ti|ti?1ti?2)
P (wN1 |t
N
1 ) = P
?(wN1 |t
N
1 )P
1??(wN1 |t
N
1 )
?
?
i
P?(wi|ti?1ti)P
1??(wi|titi+1)
In this case, the decoding process involves the
computation of three local probabilities, i.e.,
P (ti|ti?2ti?1), P (wi|ti?1ti), and P (wi|titi+1).
By using a simple manipulation that shifts the
time index of P (wi|titi+1) in Equation 4 by two
time slices3 (i.e., by replacing P (wi|titi+1) with
P (wi?2|ti?2ti?1)), we are able to compute ?(wN1 )
in Equation 4 with the same asymptotic time com-
plexity of decoding as in Equation 2.
3 Discriminative Reranking
In this section, we describe our use of the
RankBoost-based (Freund and Schapire, 1997; Fre-
und et al, 1998) discriminative reranking approach
that was originally developed by Collins and Koo
(2005) for parsing. It provides an additional avenue
for improving tagging accuracy, and also allows us
to investigate the impact of various features on Man-
darin tagging performance. The reranking algorithm
takes as input a list of candidates produced by some
probabilistic model, in our case the HMM tagger,
and reranks these candidates based on a set of fea-
tures. We first introduce Collins? reranking algo-
rithm in Subsection 3.1, and then describe two mod-
ifications in Subsections 3.2 and 3.3 that were de-
signed to improve the generalization performance of
the reranking algorithm for our POS tagging task.
The reranking features that are used for POS tagging
are then described in Subsection 3.4.
3.1 Collins? Reranking Algorithm
For training the reranker for the POS tagging task,
there are n sentences {si : i = 1, ? ? ? , n} each with
ni candidates {xi,j : j = 1, ? ? ? , ni} along with
3Replacing P (wi|titi+1) with P (wi?1|ti?1ti) also gives
the same solution.
1095
the log-probability L(xi,j) produced by the HMM
tagger. Each tagging candidate xi,j in the training
data has a ?goodness? score Score(xi,j) that mea-
sures the similarity between the candidate and the
gold reference. For tagging, we use tag accuracy
as the similarity measure. Without loss of general-
ity, we assume that xi,1 has the highest score, i.e.,
Score(xi,1) ? Score(xi,j) for j = 2, ? ? ? , ni. To
summarize, the training data consists of a set of ex-
amples {xi,j : i = 1, ? ? ? , n; j = 1, ? ? ? , ni}, each
along with a ?goodness? score Score(xi,j) and a
log-probability L(xi,j).
A set of indicator functions {hk : k = 1, ? ? ? ,m}
are used to extract binary features {hk(xi,j) : k =
1, ? ? ? ,m} on each example xi,j . An example of an
indicator function for POS tagging is given below:
h2143(x) = 1 ifx contains n-gram ?go/VV to?
0 otherwise
Each indicator function hk is associated with a
weight parameter ?k which is real valued. In ad-
dition, a weight parameter ?0 is associated with
the log-probability L(xi,j). The ranking func-
tion of candidate xi,j is defined as ?0L(xi,j) +
m?
k=1
?khk(xi,j).
The objective of the training process is to set the
parameters ?? = {?0, ?1, ? ? ? , ?m} to minimize the
following loss function Loss(??) (which is an upper
bound on the training error):
Loss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??)
where Si,j is the weight function that gives the im-
portance of each example, and Mi,j(??) is the mar-
gin:
Si,j = Score(xi,1)? Score(xi,j)
Mi,j(??) = ?0(L(xi,1)? L(xi,j)) +
m?
k=1
?k(hk(xi,1)? hk(xi,j))
All of the ?i?s are initially set to zero. The value
of ?0 is determined first to minimize the loss func-
tion and is kept fixed afterwards. Then a greedy se-
quential 4 optimization method is used in each itera-
tion (i.e., a boosting round) to select the feature that
4Parallel optimization algorithms exist and have comparable
performance according to (Collins et al, 2002).
has the most impact on reducing the loss function
and then update its weight parameter accordingly.
For each k ? {1, ? ? ? ,m}, (hk(xi,1)? hk(xi,j)) can
only take one of the three values: +1, -1, or 0. Thus
the training examples can be divided into three sub-
sets with respect to k:
A+k = {(i, j) : (hk(xi,1)? hk(xi,j)) = +1}
A?k = {(i, j) : (hk(xi,1)? hk(xi,j)) = ?1}
A0k = {(i, j) : (hk(xi,1)? hk(xi,j)) = 0}
The new loss after adding the update parameter ?
to the parameter ?k is shown below:
Loss(??, k, ?) =
?
(i,j)?A+
k
Si,je
?Mi,j(??)?? +
?
(i,j)?A?
k
Si,je
?Mi,j(??)+? +
?
(i,j)?A0
k
Si,je
?Mi,j(??)
= e??W+k + e
?W?k +W
0
k
The best feature/update pair (k?, ??) that minimizes
Loss(??, k, ?) is determined using the following for-
mulas:
k? = arg max
k
?
?
?
?
?
W+k ?
?
W?k
?
?
?
? (5)
?? =
1
2
log
W+k?
W?k?
(6)
The update formula in Equation 6 is problematic
when either W+k? or W
?
k? is zero. W
+
k is zero if hk
never takes on a value 1 for any xi,1 with value 0 on
a corresponding xi,j for j = 2, ? ? ? , ni (and similarly
for W?k ). Collins introduced a smoothing parameter
 to address this problem, resulting in a slight modi-
fication to the update formula:
?? =
1
2
log
W+k? + Z
W?k? + Z
(7)
The value of  plays an important role in this for-
mula. If  is set too small, the smoothing factor Z
would not prevent setting ?? to a potentially overly
large absolute value, resulting in over-fitting. If  is
set too large, then the opposite condition of under-
training could result. The value of  is determined
based on a development set.
1096
3.2 Update Once
Collins? method allows multiple updates to the
weight of a feature based on Equations 5 and 7. We
found that for those features for which either W+k or
W?k equals zero, the update formula in Equation 7
can only increase their weight (in absolute value) in
one direction. Although these features are strong
and useful, setting their weights too large can be un-
desirable in that it limits the use of other features for
reducing the loss.
Based on this analysis, we have developed and
evaluated an update-once method, in which we use
the update formula in Equation 7 but limit weight
updates so that once a feature is selected on a cer-
tain iteration and its weight parameter is updated,
it cannot be updated again. Using this method, the
weights of the strong features are not allowed to pre-
vent additional features from being considered dur-
ing the training phase.
3.3 Regularized Reranking
Although the update-once method may attenuate
over-fitting to some extent, it also prevents adjust-
ing the value of any weight parameter that is initially
set too high or too low in an earlier boosting round.
In order to design a more sophisticated weight up-
date method that allows multiple updates in both di-
rections while penalizing overly large weights, we
have also investigated the addition of a regulariza-
tion term R(??), an exponential function of ??, to the
loss function:
RegLoss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??) +R(??)
R(??) =
m?
k=1
pk ? (e
??k + e?k ? 2)
where pk is the penalty weight of parameter ?k. The
reason that we chose this form of regularization is
that (e??k +e?k?2) is a symmetric, monotonically
decreasing function of |?k|, and more importantly it
provides a closed analytical expression of the weight
update formula similar to Equations 5 and 6. Hence,
the best feature/update pair for the regularized loss
function is defined as follows:
k? = arg max
k
?
?
?
?
?
W+k + pke
??k ?
?
W?k + pke
+?k
?
?
?
?
?? =
1
2
log
W+k? + pk?e
??k?
W?k? + pk?e
+?k?
There are many ways of choosing pk, the penalty
weight of ?k. In this paper, we use the values of
? ?(W+k +W
?
k ) at the beginning of the first iteration
(after ?0 is determined) for pk, where ? is a weight-
ing parameter to be tuned on the development set.
The regularized weight update formula has many ad-
vantages. It is always well defined no matter what
value W+k and W
?
k take, in contrast to Equation 6.
For all features, even in the case when either W+k or
W?k equals zero, the regularized update formula al-
lows weight updates in two directions. If the weight
is small, W+k and W
?
k have more impact on deter-
mining the weight update direction, however, when
the weight becomes large, the regularization factors
pke?? and pke+? favor reducing the weight.
3.4 Reranking Features
A reranking model has the flexibility of incorporat-
ing any type of feature extracted from N-best can-
didates. For the work presented in this paper, we
examine three types of features. For each window
of three word/tag pairs, we extract all the n-grams,
except those that are comprised of only one word/tag
pair, or only tags, or only words, or do not include
either the word or tag in the center word/tag pair.
These constitute the n-gram feature set.
In order to better handle unknown words, we also
extract the two most important types of morpho-
logical features5 that were utilized in (Tseng et al,
2005) for those words that appear no more than
seven times (following their convention) in the train-
ing set:
Affixation features: we use character n-gram pre-
fixes and suffixes for n up to 4. For example,
for word/tag pair D??/NN (Information-
Bag, i.e., folder), we add the following fea-
tures: (prefix1, D, NN), (prefix2, D?, NN),
(prefix3, D??, NN), (suffix1, ?, NN), (suf-
fix2,??, NN), (suffix3,D??, NN).
AffixPOS features6: we used the training set to
build a prefix/POS and suffix/POS dictionary
associating possible tags with each prefix and
5Tseng at el. also used other morphological features that
require additional resources to which we do not have access.
6AffixPOS features are somewhat different from the CTB-
Morph features used in (Tseng et al, 2005), where a mor-
pheme/POS dictionary with the possible tags for all morphemes
in the training set was used instead of two separate dictionaries
for prefix and suffix. AffixPOS features perform slightly better
in our task than the CTB-morph features.
1097
suffix in the training set. The AffixPOS fea-
tures indicate the set of tags a given affix could
have. For the same example D??/NN, D
occurred as prefix in both NN and VV words in
the training data. So we add the following fea-
tures based on the prefix D: (prefix, D, NN,
1, NN), (prefix, D, VV, 1, NN), and (prefix,
D, X, 0, NN) for every tag X not in {NN, VV},
where 1 and 0 are indicator values. Features are
extracted in the similar way for the suffix?.
The n-gram and morphological features are easy
to compute, however, they have difficulty in captur-
ing the long distance information related to syntac-
tic relationships that might help POS tagging ac-
curacy. In order to examine the effectiveness of
utilizing syntactic information in tagging, we have
also experimented with dependency features that are
extracted based on automatic parse trees. First a
bracketing parser (the Charniak parser (Charniak,
2000) in our case) is used to generate the parse
tree of a sentence, then the const2dep tool devel-
oped by Hwa was utilized to convert the bracket-
ing tree to a dependency tree based on the head
percolation table developed by the second author.
The dependency tree is comprised of a set of de-
pendency relations among word pairs. A depen-
dency relation is a triple ?word-a, word-b, relation?,
in which word-a is governed by word-b with gram-
matical relation denoted as relation. For example,
in the sentence ??(Tibet) ?N(economy) ?
?(construction) ??(achieves) >W(significant)
?(accomplishments)?, one example dependency
relation is ???, ?, mod?. Given these depen-
dency relations, we then extract dependency features
(in total 36 features for each relation) by examining
the POS tags of the words for each tagging candi-
date of a sentence. The relative positions of the word
pairs are also taken into account for some features.
For example, if?? and? in the above sentence
are tagged as VV and NN respectively in one can-
didate, then two example dependency features are
(dep-1, ??, VV, ?, NN, mod), (dep-14, ??,
VV, NN, right, mod), in which dep-1 and dep-14 are
feature types and right indicates that word-b (??)
is to the right of word-a (?).
4 Experiments
4.1 Data
The most recently released Penn Chinese Treebank
5.2 (denoted CTB, released by LDC) is used in our
experiments. It contains 500K words, 800K char-
acters, 18K sentences, and 900 data files, includ-
ing articles from the Xinhua news agency (China-
Mainland), Information Services Department of
HKSAR (Hongkong), and Sinorama magazine (Tai-
wan). Its format is similar to the English WSJ Penn
Treebank, and it was carefully annotated. There are
33 POS tags used, to which we add tags to discrim-
inate among punctuation types. The original POS
tag for punctuation was PU; we created new POS
tags for each distinct punctuation type (e.g., PU-?).
The CTB corpus was collected during different
time periods from different sources with a diversity
of articles. In order to obtain a representative split
of training, development, and test sets, we divide
the whole corpus into blocks of 10 files by sorted
order. For each block, the first file is used for de-
velopment, the second file is used for test, and the
remaining 8 files are used for training. Table 1 gives
the basic statistics on the data. The development
set is used to determine the parameter ? in Equa-
tion 4, the smoothing parameter  in Equation 7, the
weight parameter ? described in Section 3.3, and the
number of boosting rounds in the reranking model.
In order to train the reranking model, the method
in (Collins and Koo, 2005) is used to prepare the
N-best training examples. We divided the training
set into 20 chunks, with each chunk N-best tagged
by the HMM model trained on the combination of
the other 19 chunks. The development set is N-best
tagged by the HMM model trained on the training
set, and the test set is N-best tagged by the HMM
model trained on the combination of the training set
and the development set.
Train Dev Test
#Sentences 14925 1904 1975
#Words 404844 51243 52900
Table 1: The basic statistics on the data.
In the following subsections, we will first exam-
ine the HMM models alone to determine the best
HMM configuration to use to generate the N-best
candidates, and then evaluate the reranking mod-
els. Finally, we compare our performance with pre-
vious work. In this paper, we use the sign test
with p ? 0.01 to evaluate the statistical significance
of the difference between the performances of two
models.
1098
4.2 Results of the HMM taggers
The baseline HMM model ported directly from the
English tagger, as described in Subsection 2.1, has
an overall tag accuracy of 93.12% on the test set,
which is fairly low compared to the 97% accuracy
of many state-of-the-art taggers on WSJ for English.
By approximating the unknown word emission
probability using the characters in the word as in
Equation 3, the performance of the HMM tagger im-
proves significantly to 93.43%, suggesting that char-
acters in different positions of a Chinese word help
to disambiguate the word class of the entire word, in
contrast to English for which suffixes are most help-
ful.
Figure 1 depicts the impact of combining the left-
to-right and right-to-left word emission models us-
ing different weighting values (i.e., ?) on the devel-
opment set. Note that emission probabilities of un-
known words are estimated based on characters us-
ing the same ? for combination. When ? = 1.0, the
model uses only the standard left-to-right prediction
of words, while when ? = 0 it uses only the right-to-
left estimation. It is interesting to note that the right-
to-left estimation results in greater accuracy than the
left-to-right estimation. This might be because there
is stronger interaction between a word and its next
tag. Also as shown in Figure 1, the estimations in
the two directions are complementary to each other,
with ? = 0.5 performing best. The performance of
the HMM taggers on the test set is given in Table 2
for the best operating point, as well as the two other
extreme operating points to compare the left-to-right
and right-to-left constraints. Our best HMM tagger
further improves the tag accuracy significantly from
93.43% (? = 1.0) to 94.01% (? = 0.5).
Figure 1: The accuracy of the HMM tagger on the
development set with various ? values for combin-
ing the word emission probabilities.
Overall Known Unknown
HMM baseline 93.12% 94.65% 69.08%
HMM, ?=1.0 93.43% 94.71% 73.41%
HMM, ?=0.0 93.65% 94.88% 74.23%
HMM, ?=0.5 94.01% 95.21% 75.15%
Table 2: The performance of various HMM taggers
on the test set.
4.3 Results of the Reranking Models
The HMM tagger with the best accuracy (i.e., the
one with ? = 0.5 in Table 2) is used to generate
the N-Best tagging candidates, with a maximum of
100 candidates. As shown in Table 3, a maximum of
100-Best provides a reasonable margin for improve-
ment in the reranking task.
We first test the performance of the reranking
methods using only the n-gram feature set, which
contains around 18 million features. Later, we
will investigate the addition of morphological fea-
tures and dependency features. The smoothing
parameter  (for Collins? method and the update-
once method) and the weight parameter ? (for
the regularization method) both have great im-
pact on reranking performance. We trained vari-
ous reranking models with  values of 0.0001 ?
{1, 2.5, 5, 7.5, 10, 25, 50, 75, 100}, and ? values of
{0.1, 0.25, 0.5, 0.75, 1}. For all these parameter val-
ues, 600,000 rounds of iterations were executed on
the training set. The development set was used to
determine the early stopping point in training. If
not mentioned explicitly, all the results reported are
based on the best parameters tuned on the develop-
ment set.
1-Best 50-Best 100-Best
train 93.48% 96.96% 97.13%
dev 93.75% 97.68% 97.84%
test 93.19% 97.19% 97.35%
Table 3: The oracle tag accuracies of the 1-Best, 50-
Best, and 100-Best candidates in the training, devel-
opment, and test sets for the reranking experiments.
Note that the tagging candidates are prepared using
the method described in Subsection 4.1.
Table 4 reports the performance of the best HMM
tagger and the three reranking taggers on the test set.
All three reranking methods improve the HMM tag-
ger significantly. Also, the update-once and regu-
larization methods both outperform Collins? original
training method significantly.
1099
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.38% 95.56% 75.85%
Update-once 94.50% 95.67% 76.13%
Regularized 94.54% 95.70% 76.48%
Table 4: The performance on the test set of the
HMM tagger, and the reranking methods using the
n-gram features.
We observed that no matter which value the
smoothing parameter  takes, there are only about
10,000 non-zero features finally selected by Collins?
original method. In contrast, the two new methods
select substantially more features, as shown in Ta-
ble 5. As mentioned before, there are some strong
features that only appear in positive or negative sam-
ples, i.e., either W+k or W
?
k equals zero. Although
introducing the smoothing parameter  in Equation 7
prevents infinite weight values, the update to the
feature weights is no longer optimal (in terms of
minimizing the error function). Since the update
is not optimal, subsequent iterations may still fo-
cus on these features (and thus ignore other weaker
but informative features) and always increase their
weights in one direction, leading to biased training.
The update-once method at each iteration selects
a new feature that has the most impact in reduc-
ing the training loss function. It has the advantage
of preventing increasingly large weights from being
assigned to the strong features, enabling the update
of other features. The regularization method allows
multiple updates and also penalizes large weights.
Once a feature is selected and has its weight updated,
no matter how strong the feature is, the weight value
is optimal in terms of the current weights of other
features, so that the training algorithm would choose
another feature to update. A previously selected fea-
ture may be selected again if it becomes suboptimal
due to a change in the weights of other features.
#iterations #features percent
Collins 115400 10020 8.68%
Update-once 545100 545100 100%
Regularized 92500 70131 75.82%
Table 5: The number of iterations (for the best
performance), the number of selected features, and
the percentage of selected features, by Collins?
method, the update-once method, and the regular-
ization method on the development set.
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.44% 95.55% 77.05%
Update-once 94.68% 95.68% 78.91%
Regularized 94.64% 95.71% 77.84%
Table 6: The performance on the test set of the
HMM tagger and the reranking methods using n-
gram and morphological features.
We next add morphological features to the n-gram
features selected by the reranking methods7. As
can be seen by comparing Table 6 to Table 4, mor-
phological features improve the tagging accuracy of
unknown words. It should be noted that the im-
provement made by both update-one and regulariza-
tion methods is statistically significant over using n-
gram features alone; however, the improvement by
Collins? original method is not significant. This sug-
gests that the two new methods are able to utilize a
greater variety of features than the original method.
We trained several Charniak parsers using the
same method for the HMM taggers to generate auto-
matic parse trees for training, development, and test
data. The update-once method is used to evaluate
the effectiveness of dependency features for rerank-
ing, as shown in Table 7. The parser has an overall
tagging accuracy that is greater than that of the best
HMM tagger, but worse than that of the reranking
models using n-gram and morphological features. It
is interesting to note that reranking with the depen-
dency features alone improves the tagging accuracy
significantly, outperforming reranking models using
n-gram and morphological features. This suggests
that the long distance features based on the syntactic
structure of the sentence are very beneficial for POS
tagging of Mandarin. Moreover, n-gram and mor-
phological features are complementary to the depen-
dency features, with their combination performing
the best. The n-gram features improve the accuracy
on known words, while the morphological features
improve the accuracy on unknown words. The best
accuracy of 95.11% is an 18% relative reduction in
error compared to the best HMM tagger.
7Because the size of the combined feature set of all n-gram
features and morphological features is too large to be handled
by our server, we chose to add morphological features to the
n-gram features selected by the reranking methods, and then
retrain the reranking model.
1100
Overall Known Unknown
Parser 94.31% 95.57% 74.52%
dep 94.93% 96.01% 77.87%
dep+ngram 95.00% 96.11% 77.49%
dep+morph 94.98% 96.01% 78.79%
dep+ngram+morph 95.11% 96.12% 79.32%
Table 7: The tagging performance of the parser
and the update-once reranking models with depen-
dency features and their combination with n-gram
and morphological features.
4.4 Comparison to Previous Work
So how is our performance compared to previous
work? When working on the same training/test data
(CTB5.0 with the same pre-processing procedures)
as in (Tseng et al, 2005), our HMM model ob-
tained an accuracy of 93.72%, as compared to their
93.74% accuracy. Our reranking model8 using n-
gram and morphological features improves the ac-
curacy to 94.16%. Note that we did not use all the
morphological features as in (Tseng et al, 2005),
which would probably provide additional improve-
ment. The dependency features are expected to fur-
ther improve the performance, although they are not
included here in order to provide a relatively fair
comparison.
5 Conclusions and Future Work
We have shown that the characters in a word are
informative of the POS type of the entire word in
Mandarin, reflecting the fact that the individual Chi-
nese characters carry POS information to some de-
gree. The syntactic relationship among characters
may provide further information, which we leave
as future work. We have also shown that the ad-
ditional right-to-left estimation of word emission
probabilities is useful for HMM tagging of Man-
darin. This suggests that explicit modeling of bi-
directional interactions captures more sequential in-
formation. This could possibly help in other sequen-
tial modeling tasks.
We have also investigated using the reranking al-
gorithm in (Collins and Koo, 2005) for the Man-
darin POS tagging task, and found it quite effective
8Tseng at el.?s training/test split uses up the entire CTB cor-
pus, leaving no development data for tuning parameters. In
order to roughly measure reranking performance, we use the
update-once method to train the reranking model for 600,000
rounds with the other parameters tuned in Section 4. This sac-
rifices performance to some extent.
in improving tagging accuracy. The original algo-
rithm has a tendency to focus on a small subset of
strong features and ignore some of the other useful
features. We were able to improve the performance
of the reranking algorithm by utilizing two different
methods that make better use of more features. Both
are simple and yet effective. The effectiveness of de-
pendency features suggests that syntax-based long
distance features are important for improving part-
of-speech tagging performance in Mandarin. Al-
though parsing is computationally more demanding
than tagging, we hope to identify related features
that can be extracted more efficiently.
In future efforts, we plan to extract additional
reranking features utilizing more explicitly the char-
acteristics of Mandarin. We also plan to extend our
work to speech transcripts for Broadcast News and
Broadcast Conversation corpora, and explore semi-
supervised training methods for reranking.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA. We gratefully acknowledge the com-
ments from the anonymous reviewers.
References
Thorsten Brants. 2000. TnT a statistical part-of-speech
tagger. In ANLP, pages 224?231.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram supertag-
ger. In the Sixth International Workshop on Tree Ad-
joining Grammars and Related Frameworks.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
1101
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, adaboost and bregman dis-
tances. Machine Learning, 48(1):253?285.
Yoav Freund and Robert E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. Journal of Computer and System
Sciences, 1(55):119?139.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In the Fifteenth International
Conference on Machine Learning.
Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006. Re-
ranking algorithms for name tagging. In HLT/NAACL
06 Workshop on Computationally Hard Problems and
Joint Inference in Speech and Language Processing.
Fernando Pereira John Lafferty, Andrew McCallum.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Jerome Packard. 2000. The Morphology of Chinese.
Cambridge University Press.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In ACL, pages 175?182.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing.
Nianwen Xue, Fu dong Chiou, and Martha Palmer. 2002.
Building a large-scale annotated chinese corpus. In
COLING.
1102
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832?841,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Self-Training PCFG Grammars with Latent Annotations
Across Languages
Zhongqiang Huang
1
1
Laboratory for Computational Linguistics
and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
zqhuang@umiacs.umd.edu
Mary Harper
1,2
2
Human Language Technology
Center of Excellence
Johns Hopkins University
mharper@umiacs.umd.edu
Abstract
We investigate the effectiveness of self-
training PCFG grammars with latent anno-
tations (PCFG-LA) for parsing languages
with different amounts of labeled training
data. Compared to Charniak?s lexicalized
parser, the PCFG-LA parser was more ef-
fectively adapted to a language for which
parsing has been less well developed (i.e.,
Chinese) and benefited more from self-
training. We show for the first time that
self-training is able to significantly im-
prove the performance of the PCFG-LA
parser, a single generative parser, on both
small and large amounts of labeled train-
ing data. Our approach achieves state-
of-the-art parsing accuracies for a single
parser on both English (91.5%) and Chi-
nese (85.2%).
1 Introduction
There is an extensive research literature on build-
ing high quality parsers for English (Collins, 1999;
Charniak, 2000; Charniak and Johnson, 2005;
Petrov et al, 2006), however, models for parsing
other languages are less well developed. Take Chi-
nese for example; there have been several attempts
to develop accurate parsers for Chinese (Bikel and
Chiang, 2000; Levy and Manning, 2003; Petrov
and Klein, 2007), but the state-of-the-art perfor-
mance, around 83% F measure on Penn Chinese
Treebank (achieved by the Berkeley parser (Petrov
and Klein, 2007)) falls far short of performance
on English (?90-92%). As pointed out in (Levy
and Manning, 2003), there are many linguistic dif-
ferences between Chinese and English, as well as
structural differences between their corresponding
treebanks, and some of these make it a harder task
to parse Chinese. Additionally, the fact that the
available treebanked Chinese materials are more
limited than for English also increases the chal-
lenge of building high quality Chinese parsers.
Many of these differences would also tend to apply
to other less well investigated languages.
In this paper, we focus on English and Chinese
because the former is a language for which ex-
tensive parsing research has been conducted while
the latter is a language that has been less exten-
sively studied. We adapt and improve the Berke-
ley parser, which learns PCFG grammars with la-
tent annotations, and show through comparative
studies that this parser significantly outperforms
Charniak?s parser, which was initially developed
for English and subsequently ported to Chinese.
We focus on answering two questions: how well
does a parser perform across languages and how
much does it benefit from self-training?
The first question is of special interest when
choosing a parser that is designed for one language
and adapting it to another less studied language.
We improve the PCFG-LA parser by adding a
language-independent method for handling rare
words and adapt it to another language, Chinese,
by creating a method to better model Chinese un-
known words. Our results show that the PCFG-
LA parser performs significantly better than Char-
niak?s parser on Chinese, and is also somewhat
more accurate on English, although both parsers
have high accuracy.
The second question is important because la-
beled training data is often quite limited, espe-
cially for less well investigated languages, while
unlabeled data is ubiquitous. Early investigations
on self-training for parsing have had mixed re-
sults. Charniak (1997) reported no improvements
from self-training a PCFG parser on the standard
WSJ training set. Steedman et al (2003) re-
ported some degradation using a lexicalized tree
adjoining grammar parser and minor improve-
ment using Collins lexicalized PCFG parser; how-
ever, this gain was obtained only when the parser
832
was trained on a small labeled set. Reichart and
Rappoport (2007) obtained significant gains us-
ing Collins lexicalized parser with a different self-
training protocol, but again they only looked at
small labeled sets. McClosky et al (2006) effec-
tively utilized unlabeled data to improve parsing
accuracy on the standard WSJ training set, but they
used a two-stage parser comprised of Charniak?s
lexicalized probabilistic parser with n-best pars-
ing and a discriminative reranking parser (Char-
niak and Johnson, 2005), and thus it would be bet-
ter categorized as ?co-training? (McClosky et al,
2008). It is worth noting that their attempts at self-
training Charniak?s lexicalized parser directly re-
sulted in no improvement. There are other suc-
cessful semi-supervised training approaches for
dependency parsing, such as (Koo et al, 2008;
Wang et al, 2008), and it would be interesting
to investigate how they could be applied to con-
stituency parsing.
We show in this paper, for the first time, that
self-training is able to significantly improve the
performance of the PCFG-LA parser, a single gen-
erative parser, on both small and large amounts of
labeled training data, for both English and Chi-
nese. With self-training, a fraction of the WSJ
or CTB6 treebank training data is sufficient to
train a PCFG-LA parser that is able to achieve or
even beat the accuracies obtained using a single
parser trained on the entire treebank without self-
training. We conjecture based on our comparison
of the PCFG-LA parser to Charniak?s parser that
the addition of self-training data helps the former
parser learn more fine-grained latent annotations
without over-fitting.
The rest of this paper is organized as follows.
We describe the PCFG-LA parser and several en-
hancements in Section 2, and discuss self-training
in Section 3. We then outline the experimental
setup in Section 4, describe the results in Sec-
tion 5, and present a detailed analysis in Section 6.
The last section draws conclusions and describes
future work.
2 Parsing Model
The Berkeley parser (Petrov et al, 2006; Petrov
and Klein, 2007) is an efficient and effective parser
that introduces latent annotations (Matsuzaki et
al., 2005) to refine syntactic categories to learn
better PCFG grammars. In the example parse tree
in Figure 1(a), each syntactic category is split into
multiple latent subcategories, and accordingly the
original parse tree is decomposed into many parse
trees with latent annotations. Figure 1(b) depicts
one of such trees. The grammar and lexical rules
are split accordingly, e.g., NP?PRP is split into
different NP-i?PRP-j rules. The expansion prob-
abilities of these split rules are the parameters of a
PCFG-LA grammar.
S
She
PRP
NP VP
VBD
heard DT
NP
NN
the noise
.
.
.
NP?2
VBD?5PRP?3
She heard DT?2
the noise
NN?6
NP?6
.?1
S?1
VP?4
(a) (b)
Figure 1: (a) original treebank tree, (b) after latent
annotation.
The objective of training is to learn a grammar
with latent annotations that maximizes the like-
lihood of the training trees, i.e., the sum of the
likelihood of all parse trees with latent annota-
tions. Since the latent annotations are not avail-
able in the treebank, a variant of the EM algo-
rithm is utilized to learn the rule probabilities for
them. The Berkeley parser employs a hierarchi-
cal split-merge method that gradually increases the
number of latent annotations and adaptively allo-
cates them to different treebank categories to best
model the training data. In this paper, we call a
grammar trained after n split-merge steps an n-
th order grammar. The order of a grammar is a
step (not continuous) function of the number of la-
tent annotations because the split-merge algorithm
first splits each latent annotation into two and then
merges some of the splits back based on their abil-
ity to increase training likelihood.
For this paper, we implemented
1
our own ver-
sion of Berkeley parser. Updates include better
handling of rare words across languages, as well
as unknown Chinese words. The parser is able
to process difficult sentences robustly using adap-
tive beam expansion. The training algorithm was
updated to support a wide range of self-training
experiments (e.g., posterior-weighted unlabeled
data, introducing self-training in later iterations)
and to make use of multiple processors to paral-
lelize EM training. The parallelization is crucial
1
A major motivation for this implementation was to sup-
port some algorithms we are developing. Most of our en-
hancements will be merged with a future release of the Berke-
ley parser.
833
for training a model with large volumes of data in
a reasonable amount of time
2
.
We next describe the language-independent
method to handle rare words, which is impor-
tant for training better PCFG-LA grammars es-
pecially when the training data is limited in size,
and our unknown Chinese word handling method,
highlighting the importance of utilizing language-
specific features to enhance parsing performance.
As we will see later, both of these methods signif-
icantly improve parsing performance.
2.1 Rare Word Handling
Whereas rule expansions are frequently observed
in the treebank, word-tag co-occurrences are
sparser and more likely to suffer from over-fitting.
Although the lexicon smoothing method in the
Berkeley parser is able to make the word emis-
sion probabilities of different latent states of a
POS tag more alike, the EM training algorithm
still strongly discriminates among word identities.
Suppose word tag pairs ?w
1
, t? and ?w
2
, t? both
appear the same number of times in the training
data. In a PCFG grammar without latent annota-
tions, the probabilities of emitting these two words
given tag t would be the same, i.e., p(w
1
|t) =
p(w
2
|t). After introducing latent annotation x to
tag t, the emission probabilities of these two words
given a latent state t
x
may no longer be the same
because p(w
1
|t
x
) and p(w
2
|t
x
) are two indepen-
dent parameters that the EM algorithm optimizes
on. It is beneficial to learn subcategories of POS
tags to model different types of words, especially
for frequent words; however, it is not desirable to
strongly discriminate among rare words because it
could distract the model from learning about com-
mon phenomena.
To handle this problem, the probability of a la-
tent state t
x
generating a rare word w is forced
to be proportional to the emission probability of
word w given the surface tag t. This is achieved
by mapping all words with frequency less than
threshold
3
? to the unk symbol, and for each la-
tent state t
x
of a POS tag t, accumulating the word
tag statistics of these rare words to c
r
(t
x
, unk) =
?
w:c(w)<?
c(t
x
, w), and then redistributing them
among the rare words to estimate their emission
2
The parallel version is able to train our largest grammar
on a 8-core machine within a week, while the non-parallel
version is not able to finish even after 3 weeks.
3
The value of ? is tuned on the development set.
probabilities:
c(t
x
, w) = c
r
(t
x
, unk) ?
c(t, w)
c
r
(t, unk)
p(w|t
x
) = c(t
x
, w)/
?
w
c(t
x
, w)
2.2 Chinese Unknown Word Handling
The Berkeley parser utilizes statistics associated
with rare words (e.g., suffix, capitalization) to esti-
mate the emission probabilities of unknown words
at decoding time. This is adequate for for English,
however, only a limited number of classes of un-
known words, such as digits and dates, are handled
for Chinese. In this paper, we develop a character-
based unknown word model inspired by (Huang
et al, 2007) that reflects the fact that characters in
any position (prefix, infix, or suffix) can be predic-
tive of the part-of-speech (POS) type for Chinese
words. In our model, the word emission proba-
bility, p(w|t
x
), of an unknown word w given the
latent state t
x
of POS tag t is estimated by the ge-
ometric average of the emission probability of the
characters c
k
in the word:
P (w|t
x
) =
n
?
?
c
k
?w,P (c
k
|t)6=0
P (c
k
|t)
where n = |{c
k
? w|P (c
k
|t) 6= 0}|. Characters
not seen in the training data are ignored in the
computation of the geometric average. We back
off to use the rare word statistics regardless of
word identity when the above equation cannot be
used to compute the emission probability.
3 Parser Self-Training
Our hypothesis is that combining automatically la-
beled parses with treebank trees will help the EM
training of the PCFG-LA parser to make more in-
formed decisions about latent annotations and thus
generate more effective grammars. In this section,
we discuss how self-training is applied to train a
PCFG-LA parser.
There are several ways to automatically label
the data. A fairly standard method is to parse the
unlabeled sentences with a parser trained on la-
beled training data, and then combine the result-
ing parses with the treebank training data to re-
train the parser. This is the approach we chose
for self-training. An alternative approach is to run
EM directly on the labeled treebank trees and the
unlabeled sentences, without explicit parse trees
for the unlabeled sentences. However, because the
834
brackets would need to be determined for the un-
labeled sentences together with the latent annota-
tions, this would increase the running time from
linear in the number of expansion rules to cubic in
the length of the sentence.
Another important decision is how to weight
the gold standard and automatically labeled data
when training a new parser model. Errors in the
automatically labeled data could limit the accu-
racy of the self-trained model, especially when
there is a much greater quantity of automatically
labeled data than the gold standard training data.
To balance the gold standard and automatically
labeled data, one could duplicate the treebank
data to match the size of the automatically la-
beled data; however, the training of the PCFG-
LA parser would result in redundant applications
of EM computations over the same data, increas-
ing the cost of training. Instead we weight the
posterior probabilities computed for the gold and
automatically labeled data, so that they contribute
equally to the resulting grammar. Our preliminary
experiments show that balanced weighting is ef-
fective, especially for Chinese (about 0.4% abso-
lute improvement) where the automatic parse trees
have a relatively lower accuracy.
The training procedure of the PCFG-LA parser
gradually introduces more latent annotations dur-
ing each split-merge stage, and the self-labeled
data can be introduced at any of these stages. In-
troduction of the self-labeled data in later stages,
after some important annotations are learned from
the treebank, could result in more effective learn-
ing. We have found that a middle stage introduc-
tion (after 3 split-merge iterations) of the automat-
ically labeled data has an effect similar to balanc-
ing the weights of the gold and automatically la-
beled trees, possibly due to the fact that both meth-
ods place greater trust in the former than the latter.
In this study, we introduce the automatically la-
beled data at the outset and weight it equally with
the gold treebank training data in order to focus
our experiments to support a deeper analysis.
4 Experimental Setup
For the English experiments, sections from the
WSJ Penn Treebank are used as labeled training
data: section 2-19 for training, section 22 for de-
velopment, and section 23 as the test set. We also
used 210k
4
sentences of unlabeled news articles in
the BLLIP corpus for English self-training.
For the Chinese experiments, the Penn Chinese
Treebank 6.0 (CTB6) (Xue et al, 2005) is used
as labeled data. CTB6 includes both news articles
and transcripts of broadcast news. We partitioned
the news articles into train/development/test sets
following Huang et al (2007). The broadcast news
section is added to the training data because it
shares many of the characteristics of newswire text
(e.g., fully punctuated, contains nonverbal expres-
sions such as numbers and symbols). In addi-
tion, 210k sentences of unlabeled Chinese news
articles are used for self-training. Since the Chi-
nese parsers in our experiments require word-
segmented sentences as input, the unlabeled sen-
tences need to be word-segmented first. As shown
in (Harper and Huang, 2009), the accuracy of au-
tomatic word segmentation has a great impact on
Chinese parsing performance. We chose to use
the Stanford segmenter (Chang et al, 2008) in
our experiments because it is consistent with the
treebank segmentation and provides the best per-
formance among the segmenters that were tested.
To minimize the discrepancy between the self-
training data and the treebank data, we normalize
both CTB6 and the self-training data using UW
Decatur (Zhang and Kahn, 2008) text normaliza-
tion.
Table 1 summarizes the data set sizes used
in our experiments. We used slightly modi-
fied versions of the treebanks; empty nodes and
nonterminal-yield unary rules
5
, e.g., NP?VP, are
deleted using tsurgeon (Levy and Andrew, 2006).
Train Dev Test Unlabeled
English
39.8k 1.7k 2.4k 210k
(950.0k) (40.1k) (56.7k) (5,082.1k)
Chinese
24.4k 1.9k 2.0k 210k
(678.8k) (51.2k) (52.9k) (6,254.9k)
Table 1: The number of sentences (and words in
parentheses) in our experiments.
We trained parsers on 20%, 40%, 60%, 80%,
and 100% of the treebank training data to evaluate
4
This amount was constrained based on both CPU and
memory. We plan to investigate cloud computing to exploit
more unlabeled data.
5
As nonterminal-yield unary rules are less likely to be
posited by a statistical parser, it is common for parsers trained
on the standard Chinese treebank to have substantially higher
precision than recall. This gap between bracket recall and
precision is alleviated without loss of parse accuracy by delet-
ing the nonterminal-yield unary rules. This modification sim-
ilarly benefits both parsers we study here.
835
the effect of the amount of labeled training data on
parsing performance. We also compare how self-
training impacts the models trained with different
amounts of gold-standard training data. This al-
lows us to simulate scenarios where the language
has limited human-labeled resources.
We compare models trained only on the gold
labeled training data with those that utilize ad-
ditional unlabeled data. Self-training (PCFG-LA
or Charniak) proceeds in two steps. In the first
step, the parser is first trained on the allocated la-
beled training data (e.g., 40%) and is then used
to parse the unlabeled data. In the second step,
a new parser is trained on the weighted combina-
tion
6
of the allocated labeled training data and the
additional automatically labeled data. The devel-
opment set is used in each step to select the gram-
mar order with the best accuracy for the PCFG-LA
parser and to tune the smoothing parameters for
Charniak?s parser.
5 Results
In this section, we first present the effect of un-
known and rare word handling for the PCFG-LA
parser, and then compare and discuss the perfor-
mance of the PCFG-LA parser and Charniak?s
parser across languages with different amounts
of labeled training, either with or without self-
training.
5.1 Rare and Unknown Word Handling
Table 2 reports the effect of unknown and rare
word handing for the PCFG-LA parser trained on
100%
7
of the labeled training data. The rare word
handling improves the English parser by 0.68%
and the Chinese parser by 0.56% over the Berke-
ley parser. The Chinese unknown word handling
method alone improves the Chinese parser by
0.47%. The rare and unknown handling methods
together improve the Chinese parser by 0.92%. All
the improvements are statistically significant
8
.
We found that the rare word handling method
becomes more effective as the number of latent an-
notations increases, especially when there is not a
6
We balance the size of manually and automatically la-
beled data by posterior weighting for the PCFG-LA parsers
and by duplication for Charniak?s parser.
7
Greater improvements are obtained using smaller
amounts of labeled training data.
8
We use Bikel?s randomized parsing evaluation compara-
tor to determine the significance (p < 0.05) of difference
between two parsers? output.
English Chinese
PCFG-LA 89.95 83.23
+R 90.63 83.79
+U N/A 83.70
+R+U N/A 84.15
Table 2: Effects of rare word handling (+R) and
Chinese unknown handling (+U) on the test set.
sufficient amount of labeled training data. Shar-
ing statistics of the rare words during training re-
sults in more robust grammars with better pars-
ing performance. The unknown word handling
method also gives greater improvements on gram-
mars trained on smaller amounts of training data,
suggesting that it is quite helpful for modeling un-
seen words at decoding time. However, it tends to
be less effective when the number of latent anno-
tations increases, probably because the probability
estimation of unseen words based on surface tags
is less reliable for finer-gained latent annotations.
5.2 Labeled Data Only
When comparing the two parsers on both lan-
guages in Figure 2 with treebank training, it is
clear that they perform much better on English
than Chinese. While this is probably due in part
to the years of research on English, Chinese still
appears to be more challenging than English. The
comparison between the two parsing approaches
provides two interesting conclusions.
First, the PCFG-LA parser always performs sig-
nificantly better than Charniak?s parser on Chi-
nese, although both model English well. Ad-
mittedly Charniak?s parser has not been opti-
mized
9
on Chinese, but neither has the PCFG-
LA parser
10
. The lexicalized model in Charniak?s
parser was first optimized for English and required
sophisticated smoothing to deal with sparseness;
however, the lexicalized model developed for Chi-
nese works less well. In contrast, the PCFG-LA
parser learns the latent annotations from the data,
without any specification of what precisely should
be modeled and how it should be modeled. This
flexibility may help it better model new languages.
Second, while both parsers benefit from in-
creased amounts of gold standard training data,
the PCFG-LA parser gains more. The PCFG-LA
parser is initially poorer than Charniak?s parser
9
The Chinese port includes modification of the head table,
implementation of a Chinese punctuation model, etc.
10
The PCFG-LA parser without the unknown word han-
dling method still outperforms Charniak?s parser on Chinese.
836
 87
 88
 89
 90
 91
 92
 0.2  0.4  0.6  0.8  1
F s
co
re
Number of Labeled WSJ Training Trees
x 39,832
PCFG-LAPCFG-LA.ST CharniakCharniak.ST
(a) English
 76
 78
 80
 82
 84
 86
 0.2  0.4  0.6  0.8  1
F s
co
re
Number of Labeled CTB Training Trees
x 24,416
(b) Chinese
Figure 2: The performance of the PCFG-LA
parser and Charniak?s parser evaluated on the test
set, trained with different amounts of labeled train-
ing data, with and without self-training (ST).
when trained on 20% WSJ training data, proba-
bly because the training data is too small for it to
learn fine-grained annotations without over-fitting.
As more labeled training data becomes avail-
able, the performance of the PCFG-LA parser im-
proves quickly and finally outperforms Charniak?s
parser significantly. Moreover, performance of the
PCFG-LA parser continues to grow when more la-
beled training data is available, while the perfor-
mance of Charniak?s parser levels out at around
80% of the labeled data. The PCFG-LA parser im-
proves by 3.5% when moving from 20% to 100%
training data, compared to a 2.21% gain for Char-
niak?s parser. Similarly for Chinese, the PCFG-
LA parser also gains more (4.48% vs 3.63%).
5.3 Labeled + Self-Labeled
The PCFG-LA parser is also able to benefit more
from self-training than Charniak?s parser. On the
WSJ data set, Charniak?s parser benefits from self-
training initially when there is little labeled train-
ing data, but the improvement levels out quickly
as more labeled training trees become available.
In contrast, the PCFG-LA parser benefits consis-
tently from self-training
11
, even when using 100%
11
One may notice that the self-trained PCFG-LA parser
with 100% labeled WSJ data has a slightly lower test accu-
of the labeled training set. Similar trends are also
found for Chinese.
It should be noted that the PCFG-LA parser
trained on a fraction of the treebank training data
plus a large amount of self-labeled training data,
which comes with little or no cost, performs com-
parably or even better than grammars trained with
additional labeled training data. For example, the
self-trained PCFG-LA parser with 60% labeled
data is able to outperform the grammar trained
with 100% labeled training data alone for both En-
glish and Chinese. With self-training, even 40%
labeled WSJ training data is sufficient to train a
PCFG-LA parser that is comparable to the model
trained on the entire WSJ training data alone. This
is of significant importance, especially for lan-
guages with limited human-labeled resources.
One might conjecture that the PCFG-LA parser
benefits more from self-training than Charniak?s
parser because its self-labeled data has higher ac-
curacy. However, this is not true. As shown in Fig-
ure 2 (a), the PCFG-LA parser trained with 40%
of the WSJ training set alne has a much lower
performance (88.57% vs 89.96%) than Charniak?s
parser trained on the full WSJ training set. With
the same amount of self-training data (labeled by
each parser), the resulting PCFG-LA parser ob-
tains a much higher F score than the self-trained
Charniak?s parser (90.52% vs 90.18%). Similar
patterns can also be found for Chinese.
English Chinese
PCFG-LA 90.63 84.15
+ Self-training 91.46 85.18
Table 3: Final results on the test set.
Table 3 reports the final results on the test set
when trained on the entire WSJ or CTB6 training
set. For English, self-training contributes 0.83%
absolute improvement to the PCFG-LA parser,
which is comparable to the improvement obtained
from using semi-supervised training with the two-
stage parser in (McClosky et al, 2006). Note that
their improvement is achieved with the addition
of 2,000k unlabeled sentences using the combi-
nation of a generative parser and a discriminative
reranker, compared to using only 210k unlabeled
sentences with a single generative parser in our
approach. For Chinese, self-training results in a
racy than the self-trained PCFG-LA parser with 80% labeled
WSJ data. This is due to the variance in parser performance
when initialized with different seeds and the fact that the de-
velopment set is used to pick the best model for evaluation.
837
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 98
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled WSJ Training Trees
x 39,832
TestTest.ST TrainTrain.ST
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  1  2  3  4  5  6  7 103
104
105
106
107
F sc
ore
Num
ber 
of R
ules
 (log 
scale
)
Split-Merge Roundsfewer latent states more latent states
TestTest.ST TrainTrain.ST RulesRules.ST
 87
 88
 89
 90
 91
 92
 93
 94
 95
 96
 97
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled WSJ Training Trees
x 39,832
TestTest.ST TrainTrain.ST
5
6 6 6 6
6 7 7 7 7
(a) Charniak (b) PCFG-LA (20% WSJ) (c) PCFG-LA
Figure 3: (a) The training/test accuracy of Charniak?s parser trained on varying amounts of labeled
WSJ training data, with and without self-training (ST). (b) The training/test accuracy and the number
of nonzero rules of the PCFG-LA grammars trained on 20% of the labeled WSJ training data, w/ and
w/o ST. (c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled WSJ
training data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars.
 76
 78
 80
 82
 84
 86
 88
 90
 92
 94
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled CTB Training Trees
x 24,416
TestTest.ST TrainTrain.ST
 55
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  1  2  3  4  5  6  7 103
104
105
106
107
F sc
ore
Non
zero
 Ru
les 
(log s
cale)
Split-Merge Roundsfewer latent states more latent states
TestTest.ST TrainTrain.ST RulesRules.ST
 78
 80
 82
 84
 86
 88
 90
 92
 94
 0.2  0.4  0.6  0.8  1
F s
cor
e
Number of Labeled CTB Training Trees
Train/Test Performance of the PCFG-LA Parser (CTB)
x 24,416
TestTest.ST TrainTrain.ST
4
5 5
6 6
6
6
6
7 7
(a) Charniak (b) PCFG-LA (20% CTB) (c) PCFG-LA
Figure 4: (a) The training/test accuracy of Charniak?s parser trained on varying amounts of labeled
CTB training data, with and without self-training (ST). (b) The training/test accuracy and the number
of nonzero rules of the PCFG-LA grammars trained on 20% of the labeled CTB training data, w/ and
w/o ST. (c) The training/test accuracy of the PCFG-LA parser trained on varying amount of labeled CTB
training data, w/ and w/o ST; the numbers along the training curves indicate the order of the grammars.
state-of-the-art parsing model with 85.18% accu-
racy (1.03% absolute improvement) on a represen-
tative test set. Both improvements are statistically
significant.
6 Analysis
In this section, we perform a series of analyses,
focusing on English (refer to Figure 3), to investi-
gate why the PCFG-LA parser benefits more from
additional data, most particularly automatically la-
beled data, when compared to Charniak?s parser.
Similar analyses have been done for Chinese with
similar results (refer to Figure 4).
Charniak?s parser is a lexicalized PCFG parser
that models lexicalized dependencies explicitly
observable in the training data and relies on
smoothing to avoid over-fitting. Although it is
able to benefit from more training data because of
broader lexicon and rule coverage and more robust
estimation of parameters, its ability to benefit from
the additional data is limited in the sense that it is
not able to generate additional predictive features
that are supported by this data. As shown in fig-
ure 3(a), the parsing accuracy of Charniak?s parser
on the test set improves as the amount of labeled
training data increases; however, the training accu-
racy
12
degrades as more data is added. Note that
the training accuracy
13
of Charniak?s parser also
12
The parser is tested on the treebank labeled set that the
parser is trained on.
13
The self-training data is combined with the labeled tree-
bank trees in a weighted manner; otherwise, the training ac-
curacy would be even lower.
838
decreases after the addition of self-training data.
This is expected for models like Charniak?s parser
with fixed model parameters; it is harder to model
more data with greater diversity. The addition of
self-labeled data helps on the test set initially but it
provides little gain when the labeled training data
becomes relatively large.
In contrast, the PCFG-LA grammar is able to
model the training data with different granulari-
ties. Fewer latent annotations are employed when
the training set is small. As the size of the train-
ing data increases, it is able to allocate more latent
annotations to better model the data. As shown in
Figure 3 (b), for a fixed amount (20%) of labeled
training data, the accuracy of the model on train-
ing data continues to improve as the number of la-
tent annotation increases. Although it is important
to limit the number of latent annotations to avoid
over-fitting, the ability to model training data ac-
curately given sufficient latent annotations is desir-
able when more training data is available. When
trained on the labeled data (20%) alone, the 5-th
order grammar achieves its optimal generalization
performance (based on the development set) and
begins to degrade afterwords. With the addition of
self-training data, the 5-th order grammar achieves
an even greater accuracy on the test set and its per-
formance continues to increase
14
when moving to
the 6-th or even 7-th order grammar.
Figure 3 (c) plots the training and test curves
of the English PCFG-LA parser with varying
amounts of labeled training data, with and with-
out self-training. This figure differs substantially
from Figure 3 (a). First, as mentioned earlier, the
PCFG-LA parser benefits much more from self-
training than Charniak?s parser with moderate to
large amounts of labeled training data. Second, in
contrast to Charniak?s parser for which training ac-
curacy degrades consistently as the amount of la-
beled training data increases, the training accuracy
of the PCFG-LA parser sometimes improves when
trained on more labeled training data (e.g., the best
model (at order 6) trained on 40%
15
labeled train-
14
Although the 20% self-trained grammar has a higher test
accuracy at the 7-th round than the 6-th round, the develop-
ment accuracy was better at the 6-th round, and thus we report
the test accuracy of the 6-th round grammar in Figure 3 (c).
15
For models trained with greater amounts of labeled train-
ing data, although their training accuracy becomes lower (due
to greater diversity) for the grammars (all at order 6) selected
by the development set, their 7-th order grammars (not re-
ported in the figure) actually have both higher training and
test accuracies than the 6-th order grammar trained on less
training data.
ing data alone has a higher training accuracy than
the best model (at order 5) trained on 20% labeled
training data). Third, the addition of self-labeled
data supports more accurate PCFG-LA grammars
with higher orders than those trained without self-
training, as evidenced by scores on both the train-
ing and test data. This suggests that the self-
trained grammars are able to utilize more latent
annotations to learn deeper dependencies.
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 22
 0  2  4  6  8  10  12  14  16 0
 1.2
 2.4
 3.6
 4.8
 6
Re
lati
ve 
red
uct
ion
 of 
F e
rro
r (%
)
Nu
mb
er 
of b
rac
ket
s
Span Length
x 1e+4
+Labeled +Unlabled #Brackets
Figure 5: The relative reduction of bracketing er-
rors for different span lengths, evaluated on the
test set. The baseline model is the PCFG-LA
parser trained on 20% of the WSJ training data.
The +Unlabeled curve corresponds to the parser
trained with the additional automatically labeled
data and the +Labeled curve corresponds to the
parser trained with additional 20% labeled training
data. The counts of the brackets are computed on
the gold reference. Span length ?0? is designated
for the effect on preterminal POS tags to differ-
entiate it from the non-terminal brackets spanning
only one word.
Figure 5 compares the effect of additional tree-
bank labeled and automatically labeled data on the
relative reduction of bracketing errors for different
span lengths. It is clear from the figure that the im-
provement in parsing accuracy from self-training
is the result of better bracketing across all span
lengths
16
. However, even though the automati-
cally labeled training data provides more improve-
ment than the additional treebank labeled data in
terms of parsing accuracy, this data is less effective
at improving tagging accuracy than the additional
treebank labeled training data.
So, how could self-training improve rule esti-
mation when training the PCFG-LA parser with
more latent annotations? One possibility is that the
automatically labeled data smooths the parameter
16
There is a slight degradation in bracketing accuracy for
some spans longer than 16 words, but the effect is negligible
due to their low counts.
839
estimates in the EM algorithm, enabling effective
training of models with more parameters to learn
deeper dependencies. Let p(a ? b|e, t) be the
posterior probability of expanding subcategories a
to b given the event e, which is a rule expansion
on a treebank parse tree t. T
l
and T
u
are the sets
of gold and automatically labeled parse trees, re-
spectively. The update of the rule expansion prob-
ability p(a ? b) in self-training (with weighting
parameter ?) can be expressed as:
P
t?T
l
P
e?t
p(a? b|e, t) + ?
P
t?T
u
P
e?t
p(a? b|e, t)
P
b
(
P
t?T
l
P
e?t
p(a? b|e, t) + ?
P
t?T
u
P
e?t
p(a? b|e, t))
Since the unlabeled data is parsed by a lower
order grammar (with fewer latent annotations),
the expected counts from the automatically la-
beled data can be thought of as counts from a
lower-order grammar
17
that smooth the higher-
order (with more latent annotations) grammar.
We observe that many of the rule parameters of
the grammar trained on WSJ training data alone
have zero probabilities (rules with extremely low
probabilities are also filtered to zero), as was also
pointed out in (Petrov et al, 2006). On the one
hand, this is what we want because the grammar
should learn to avoid impossible rule expansions.
On the other hand, this might also be a sign of
over-fitting of the labeled training data. As shown
in Figure 3 (b), the grammar obtained with the ad-
dition of automatically labeled data contains many
more non-zero rules, and its performance contin-
ues to improve with more latent annotations. Sim-
ilar patterns also appear when using self-training
for other amounts of labeled training data. As is
partially reflected by the zero probability rules, the
addition of the automatically labeled data enables
the exploration of a broader parameter space with
less danger of over-fitting the data. Also note that
the benefit of the automatically labeled data is less
clear in the early training stages (i.e., when there
are fewer latent annotations), as can be seen in Fig-
ure 3 (b). This is probably because there is a small
number of free parameters and the treebank data is
sufficiently large for robust parameter estimation.
17
We also trained models using only the automatically la-
beled data without combining it with human-labeled training
data, but they were no more accurate than those trained on
the human-labeled training data alone without self-training.
7 Conclusion
In this paper, we showed that PCFG-LA parsers
can be more effectively applied to languages
where parsing is less well developed and that they
are able to benefit more from self-training than
lexicalized generative parsers. We show for the
first time that self-training is able to significantly
improve the performance of a PCFG-LA parser, a
single generative parser, on both small and large
amounts of labeled training data.
We conjecture based on our analysis that the
EM training algorithm is able to exploit the in-
formation available in both gold and automati-
cally labeled data with more complex grammars
while being less affected by over-fitting. Bet-
ter results would be expected by combining the
PCFG-LA parser with discriminative reranking
approaches (Charniak and Johnson, 2005; Huang,
2008) for self training. Self-training should also
benefit other discriminatively trained parsers with
latent annotations (Petrov and Klein, 2008), al-
though training would be much slower compared
to using generative models, as in our case.
In future work, we plan to scale up the training
process with more unlabeled training data (e.g.,
gigaword) and investigate automatic selection of
materials that are most suitable for self-training.
We also plan to investigate domain adaptation and
apply the model to other languages with modest
treebank resources. Finally, it is also important to
explore other ways to exploit the use of unlabeled
data.
Acknowledgments
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023 and NSF IIS-0703859. Any opinions,
findings and/or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the funding agencies or the
institutions where the work was completed.
References
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the chinese tree-
bank. In Proceedings of the Second Chinese Lan-
guage Processing Workshop.
Pi-Chuan Chang, Michel Gally, and Christopher Man-
ning. 2008. Optimizing chinese word segmentation
840
for machine translation performance. In ACL 2008
Third Workshop on Statistical Machine Translation.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In ACL.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In ICAI.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In ACL.
Michael John Collins. 1999. Head-driven statistical
models for natural language parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Mary Harper and Zhongqiang Huang. 2009. Chinese
statistical parsing. To appear in The Gale Book.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In EMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Terry Koo, Xavier Carrera, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In
ACL.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: Tools for querying and manipulating tree data
structures. In LREC.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse chinese, or the chinese treebank. In
ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2008. When is self-training effective for pars-
ing? In COLING.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov and Dan Klein. 2008. Sparse multi-scale
grammars for discriminative latent variable parsing.
In EMNLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL.
Qin Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency
parsing. In ACL.
Nianwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering.
Bin Zhang and Jeremy G. Kahn. 2008. Evaluation of
decatur text normalizer for language model training.
Technical report, University of Washington.
841
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1114?1123,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Joint Language Model With Fine-grain Syntactic Tags
Denis Filimonov1
1Laboratory for Computational Linguistics
and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
den@cs.umd.edu
Mary Harper1,2
2Human Language Technology
Center of Excellence
Johns Hopkins University
mharper@umiacs.umd.edu
Abstract
We present a scalable joint language
model designed to utilize fine-grain syn-
tactic tags. We discuss challenges such
a design faces and describe our solutions
that scale well to large tagsets and cor-
pora. We advocate the use of relatively
simple tags that do not require deep lin-
guistic knowledge of the language but pro-
vide more structural information than POS
tags and can be derived from automati-
cally generated parse trees ? a combina-
tion of properties that allows easy adop-
tion of this model for new languages. We
propose two fine-grain tagsets and evalu-
ate our model using these tags, as well as
POS tags and SuperARV tags in a speech
recognition task and discuss future direc-
tions.
1 Introduction
In a number of language processing tasks, particu-
larly automatic speech recognition (ASR) and ma-
chine translation (MT), there is the problem of se-
lecting the best sequence of words from multiple
hypotheses. This problem stems from the noisy
channel approach to these applications. The noisy
channel model states that the observed data, e.g.,
the acoustic signal, is the result of some input
translated by some unknown stochastic process.
Then the problem of finding the best sequence of
words given the acoustic input, not approachable
directly, is transformed into two separate models:
argmax
w
n
1
p(w
n
1
|A) = argmax
w
n
1
p(A|w
n
1
) ? p(w
n
1
)
(1)
where A is the acoustic signal and wn
1
is a se-
quence of n words. p(A|wn
1
) is called an acoustic
model and p(wn
1
) is the language model1.
Typically, these applications use language mod-
els that compute the probability of a sequence in a
generative way:
p(w
n
1
) =
n
?
i=1
p(w
i
|w
i?1
1
)
Approximation is required to keep the parameter
space tractable. Most commonly the context is re-
duced to just a few immediately preceding words.
This type of model is called an ngram model:
p(w
i
|w
i?1
1
) ? p(w
i
|w
i?1
i?n+1
)
Even with limited context, the parameter space can
be quite sparse and requires sophisticated tech-
niques for reliable probability estimation (Chen
and Goodman, 1996). While the ngram models
perform fairly well, they are only capable of cap-
turing very shallow knowledge of the language.
There is extensive literature on a variety of
methods that have been used to imbue models
with syntactic and semantic information in differ-
ent ways. These methods can be broadly catego-
rized into two types:
? The first method uses surface words within
its context, sometimes organizing them into
deterministic classes. Models of this type in-
clude: (Brown et al, 1992; Zitouni, 2007),
which use semantic word clustering, and
(Bahl et al, 1990), which uses variable-
length context.
? The other method adds stochastic variables
to express the ambiguous nature of surface
words2. To obtain the probability of the next
1Real applications use argmax
w
n
1
p(A|w
n
1
)?p(w
n
1
)
?
?n
?
instead of Eq. 1, where ? and ? are set to optimize a heldout
set.
2These variables have to be predicted by the model.
1114
word we need to sum over all assignments of
the stochastic variables, as in Eq. 2.
p(w
i
|w
i?1
1
) =
?
t
1
...t
i
p(w
i
t
i
|w
i?1
1
t
i?1
1
) (2)
=
?
t
1
...t
i
p(w
i
t
i
|w
i?1
1
t
i?1
1
)p(w
i?1
1
t
i?1
1
)
?
t
1
...t
i?1
p(w
i?1
1
t
i?1
1
)
Models of this type, which we call joint
models since they essentially predict joint
events of words and some random vari-
able(s), include (Chelba and Jelinek, 2000)
which used POS tags in combination with
?parser instructions? for constructing a full
parse tree in a left-to-right manner; (Wang
et al, 2003) used SuperARVs (complex tu-
ples of dependency information) without re-
solving the dependencies, thus called almost
parsing; (Niesler and Woodland, 1996; Hee-
man, 1999) utilize part of speech (POS) tags.
Note that some models reduce the context by
making the following approximation:
p(w
i
t
i
|w
i?1
1
t
i?1
1
) ? p(w
i
|t
i
)?p(t
i
|t
i?1
1
) (3)
thus, transforming the problem into a stan-
dard HMM application. However, these
models perform poorly and have only been
able to improve over the ngram model when
interpolated with it (Niesler and Woodland,
1996).
Although joint models have the potential
to better express variability in word usage
through the introduction of additional latent
variables, they do not necessarily perform
better because the increased dimensionality
of the context substantially increases the al-
ready complex problem of parameter estima-
tion. The complexity of the space also makes
computation of the probability a challenge
because of space and time constraints. This
makes the choice of the random variables a
matter of utmost importance.
The model presented in this paper has some el-
ements borrowed from prior work, notably (Hee-
man, 1999; Xu and Jelinek, 2004), while others
are novel.
1.1 Paper Outline
The message we aim to deliver in this paper can
be summarized in two theses:
? Use fine-grain syntactic tags in a joint LM.
We propose a joint language model that can
be used with a variety of tagsets. In Section
2, we describe those that we used in our ex-
periments. Rather than tailoring our model to
these tagsets, we aim for flexibility and pro-
pose an information theoretic framework for
quick evaluation for tagsets, thus simplifying
the creation of new tagsets. We show that
our model with fine-grain tagsets outperform
the coarser POS model, as well as the ngram
baseline, in Section 5.
? Address the challenges that arise in a joint
language model with fine-grain tags. While
the idea of using joint language modeling is
not novel (Chelba and Jelinek, 2000; Hee-
man, 1999), nor is the idea of using fine-grain
tags (Bangalore, 1996; Wang et al, 2003),
none of prior papers focus on the issues that
arise from the combination of joint language
modeling with fine-grain tags, both in terms
of reliable parameter estimation and scalabil-
ity in the face of the increased computational
complexity. We dedicate Sections 3 and 4 to
this problem.
In Section 6, we summarize conclusions and lay
out directions for future work.
2 Structural Information
As we have mentioned, the selection of the ran-
dom variable in Eq. 2 is extremely important for
the performance of the model. On one hand, we
would like for this variable to provide maximum
information. On the other hand, as the number of
parameters grow, we must address reliable param-
eter estimation in the face of sparsity, as well as
increased computational complexity. In the fol-
lowing section we will compare the use of Super-
ARVs, POS tags, and other structural tags derived
from parse trees.
2.1 POS Tags
Part-of-speech tags can be easily obtained for
unannotated data using off-the-shelf POS taggers
or PCFG parsers. However, the amount of infor-
mation these tags typically provide is very limited,
1115
Figure 1: A parse tree example
e.g., while it is helpful to know whether fly is a
verb or a noun, knowing that you is a personal pro-
noun does not carry the information whether it is
a subject or an object (given the Penn Tree Bank
tagset), which would certainly help to predict the
following word.
2.2 SuperARV
The SuperARV essentially organizes information
concerning one consistent set of dependency links
for a word that can be directly derived from its
syntactic parse. SuperARVs encode lexical in-
formation as well as syntactic and semantic con-
straints in a uniform representation that is much
more fine-grained than POS. It is a four-tuple
(C;F ;R+;D), where C is the lexical category
of the word, F is a vector of lexical features for
the word, R+ is a set of governor and need labels
that indicate the function of the word in the sen-
tence and the types of words it needs, and D rep-
resents the relative position of the word and its de-
pendents. We refer the reader to the literature for
further details on SuperARVs (Wang and Harper,
2002; Wang et al, 2003).
SuperARVs can be produced from parse trees
by applying deterministic rules. In this work we
use SuperARVs as individual tags and do not clus-
ter them based of their structure. While Super-
ARVs are very attractive for language modeling,
developing such a rich set of annotations for a new
language would require a large amount of human
effort.
We propose two other types of tags which have
not been applied to this task, although similar in-
formation has been used in parsing.
2.3 Modifee Tag
This tag is a combination of the word?s POS
tag and the POS tag of its governor role. We
designed it to resemble dependency parse struc-
ture. For example, the sentence in Figure 1 would
be tagged: the/DT-NN black/JJ-NN cat/NN-VBD
sat/VBD-root. Henceforth, we will refer to this
kind of tag as head.
2.4 Parent Constituent
This tag is a combination of the word?s POS tag
with its immediate parent in the parse tree, along
with the POS tag?s relative position among its sib-
lings. We refer to this type of tags as parent. The
example in Figure 1 will be tagged: the/DT-NP-
start black/JJ-NP-mid cat/NN-NP-end sat/VB-VP-
single. This tagset is designed to represent con-
stituency information.
Note that the head and parent tagsets are more
language-independent (all they require is a tree-
bank) than the SuperARVs which, not only uti-
lized the treebank, but were explicitly designed by
a linguist for English only.
2.5 Information Theoretic Comparison of
Tags
As we have mentioned in Section 1, the choice of
the tagset is very important to the performance of
the model. There are two conflicting intuitions for
tags: on one hand they should be specific enough
to be helpful in the language model?s task; on the
other hand, they should be easy for the LM to pre-
dict.
Of course, in order to argue which tags are more
suitable, we need some quantifiable metrics. We
propose an information theoretic approach:
? To quantify how hard it is to predict a tag, we
compute the conditional entropy:
H
p
(t
i
|w
i
) = H
p
(t
i
w
i
)?H
p
(w
i
)
=
?
w
i
t
i
p(t
i
w
i
) log p(t
i
|w
i
)
? To measure how helpful a tagset is in the LM
task, we compute the reduction of the condi-
tional cross entropy:
H
p?,q
(w
i
|w
i?1
t
i?1
) ?H
p?,q
(w
i
|w
i?1
) =
?
?
w
i
i?1
t
i?1
p?(w
i
i?1
t
i?1
) log q(w
i
|w
i?1
t
i?1
)
+
?
w
i
i?1
p?(w
i
i?1
) log q(w
i
|w
i?1
)
= ?
?
w
i
i?1
t
i?1
p?(w
i
i?1
t
i?1
) log
q(w
i
|w
i?1
t
i?1
)
q(w
i
|w
i?1
)
1116
Note that in this case we use conditional
cross entropy because conditional entropy
has the tendency to overfit the data as we se-
lect more and more fine-grain tags. Indeed,
H
p
(w
i
|w
i?1
t
i?1
) can be reduced to zero if
the tags are specific enough, which would
never happen in reality. This is not a prob-
lem for the former metric because the con-
text there, w
i
, is fixed. For this metric, we
use a smoothed distribution p? computed on
the training set3 and the test distribution q.
 
B
it
s
0
0.5
1
1.5
2
2.5
3
Tags
POS SuperARV parent head
Figure 2: Changes in entropy for different tagsets
The results of these measurements are presented
in Figure 2. POS tags, albeit easy to predict, pro-
vide very little additional information about the
following word, and therefore we would not ex-
pect them to perform very well. The parent tagset
seems to perform somewhat better than Super-
ARVs ? it provides 0.13 bits more information
while being only 0.09 bits harder to predict based
on the word. The head tagset is interesting: it pro-
vides 0.2 bits more information about the follow-
ing word (which would correspond to 15% per-
plexity reduction if we had perfect tags), but on
the other hand the model is less likely to predict
these tags accurately.
This approach is only a crude estimate (it uses
only unigram and bigram context) but it is very
useful for designing tagsets, e.g., for a new lan-
guage, because it allows us to assess relative per-
formance of tagsets without having to train a full
model.
3We used one-count smoothing (Chen and Goodman,
1996).
3 Language Model Structure
The size and sparsity of the parameter space of the
joint model necessitate the use of dimensionality
reduction measures in order to make the model
computationally tractable and to allow for accu-
rate estimation of the model?s parameters. We also
want the model to be able to easily accommodate
additional sources of information such as morpho-
logical features, prosody, etc. In the rest of this
section, we discuss avenues we have taken to ad-
dress these problems.
3.1 Decision Tree Clustering
Binary decision tree clustering has been shown to
be effective for reducing the parameter space in
language modeling (Bahl et al, 1990; Heeman,
1999) and other language processing applications,
e.g., (Magerman, 1994). Like any clustering algo-
rithm, it can be represented by a function H that
maps the space of histories to a set of equivalence
classes.
p(w
i
t
i
|w
i?1
i?n+1
t
i?1
i?n+1
) ? p(w
i
t
i
|H(w
i?1
i?n+1
t
i?1
i?n+1
))
(4)
While the tree construction algorithm is fairly
standard ? to recursively select binary questions
about the history optimizing some function ? there
are important decisions to make in terms of which
questions to ask and which function to optimize.
In the remainder of this section, we discuss the de-
cisions we made regarding these issues.
3.2 Factors
The Factored Language Model (FLM) (Bilmes
and Kirchhoff, 2003) offers a convenient view of
the input data: it represents every word in a sen-
tence as a tuple of factors. This allows us to extend
the language model with additional parameters. In
an FLM, however, all factors have to be determin-
istically computed in a joint model; whereas, we
need to distinguish between the factors that are
given or computed and the factors that the model
must predict stochastically. We call these types
of factors overt and hidden, respectively. Exam-
ples of overt factors include surface words, mor-
phological features such as suffixes, case informa-
tion when available, etc., and the hidden factors
are POS, SuperARVs, or other tags.
Henceforth, we will use word to represent the
set of overt factors and tag to represent the set of
hidden factors.
1117
3.3 Hidden Factors Tree
Similarly to (Heeman, 1999), we construct a bi-
nary tree where each tag is a leaf; we will refer
to this tree as the Hidden Factors Tree (HFT). We
use Minimum Discriminative Information (MDI)
algorithm (Zitouni, 2007) to build the tree. The
HFT represents a hierarchical clustering of the tag
space. One of the reasons for doing this is to allow
questions about subsets of tags rather than individ-
ual tags alone4.
Unlike (Heeman, 1999), where the tree of tags
was only used to create questions, this representa-
tion of the tag space is, in addition, a key feature
of our decoding optimizations, which we discuss
in Section 4.
3.4 Questions
The context space is partitioned by means of bi-
nary questions. We use different types of ques-
tions for hidden and overt factors.
? Questions about surface words are con-
structed using the Exchange algorithm (Mar-
tin et al, 1998). This algorithm takes the set
of words that appear at a certain position in
the training data associated with the current
node in the history tree and divides the set
into two complementary subsets greedily op-
timizing some target function (we use the av-
erage entropy of the marginalized word dis-
tribution, the same as for question selection).
Note that since the algorithm only operates
on the words that appear in the training data,
we need to do something more to account for
the unseen words. Thus, to represent this type
of question, we create the history tree struc-
ture depicted in Fig. 4.
For other overt factors with smaller vocabu-
laries, such as suffixes, we use equality ques-
tions.
? As we mentioned in Section 3.3, we use the
Hidden Factors Tree to create questions about
hidden factors. Note that every node in a bi-
nary tree can be represented by a binary path
from the root with all nodes under an inner
node sharing the same prefix. Thus, a ques-
tion about whether a tag belongs to a subset
4Trying all possible subsets of tags is not feasible since
there are 2|T | of them. The tree allows us to reduce the num-
ber to O(T ) of the most meaningful (as per the clustering
algorithm) subsets.
Figure 3: Recursive smoothing: p?
n
= ?
n
p
n
+
(1? ?
n
)p?
n
?
of tags dominated by a node can be expressed
as whether the tag?s path matches the binary
prefix.
3.5 Optimization Criterion and Stopping
Rule
To select questions we use the average entropy of
the marginalized word distribution. We found that
this criterion significantly outperforms the entropy
of the distribution of joint events. This is proba-
bly due to the increased sparsity of the joint distri-
bution and the fact that our ultimate metrics, i.e.,
WER and word perplexity, involve only words.
3.6 Distribution Representation
In a cluster H
x
, we factor the joint distribution as
follows:
p(w
i
t
i
|H
x
) = p(w
i
|H
x
) ? p(t
i
|w
i
, H
x
)
where p(t
i
|w
i
, H
x
) is represented in the form of
an HFT, in which each leaf has the probability of a
tag and each internal node contains the sum of the
probabilities of the tags it dominates. This repre-
sentation is designed to assist the decoding process
described in Section 4.
3.7 Smoothing
In order to estimate probability distributions at the
leaves of the history tree, we use the following re-
cursive formula:
p?
n
(w
i
t
i
) = ?
n
p
n
(w
i
t
i
) + (1? ?
n
)p?
n
?
(w
i
t
i
) (5)
where n? is the n-th node?s parent, p
n
(w
i
t
i
) is
the distribution at node n (see Figure 3). The
1118
root of the tree is interpolated with the distribu-
tion p
unif
(w
i
t
i
) =
1
|V |
p
ML
(t
i
|w
i
)
5
. To estimate
interpolation parameters ?
n
, we use the EM algo-
rithm described in (Magerman, 1994); however,
rather than setting aside a separate development
set of optimizing ?
n
, we use 4-fold cross valida-
tion and take the geometric mean of the resulting
coefficients6. We chose this approach because a
small development set often does not overlap with
the training set for low-count nodes, leading the
EM algorithm to set ?
n
= 0 for those nodes.
Let us consider one leaf of the history tree in
isolation. Its context can be represented by the
path to the root, i.e., the sequence of questions and
answers q
1
, . . . q
(n
?
)
?q
n
? (with q
1
being the answer
to the topmost question):
p?
n
(w
i
t
i
) = p?(w
i
t
i
|q
1
. . . q
(n
?
)
?q
n
?
)
Represented this way, Eq. 5 is a variant of Jelinek-
Mercer smoothing:
p?(w
i
t
i
|q
1
. . . q
n
?
) = ?
n
p(w
i
t
i
|q
1
. . . q
n
?
) +
(1? ?
n
)p?(w
i
t
i
|q
1
. . . q
(n
?
)
?)
For backoff nodes (see Fig. 4), we use a lower
order model7 interpolated with the distribution at
the backoff node?s grandparent (see node A in Fig.
4):
p?
B
(w
i
t
i
|w
i?1
i?n+1
t
i?1
i?n+1
) =
?
A
p?
bo
(w
i
t
i
|w
i?1
i?n+2
t
i?1
i?n+2
) + (1 ? ?
A
)p?
A
(w
i
t
i
)
How to compute ?
A
is an open question. For this
study, we use a simple heuristic based on obser-
vation that the further node A is from the root
the more reliable the distribution p?
A
(w
i
t
i
) is, and
hence ?
A
is lower. The formula we use is as fol-
lows:
?
A
=
1
?
1 + distanceToRoot(A)
5We use this distribution rather than uniform joint distri-
bution 1
|V ||T |
because we do not want to allow word-tag pairs
that have never been observed. The idea is similar to (Thede
and Harper, 1999).
6To avoid a large number of zeros due to the product, we
set a minimum for ? to be 10?7.
7The lower order model is constructed by the same algo-
rithm, although with smaller context. Note that the lower or-
der model can back off on words or tags, or both. In this paper
we backoff both on words and tags, i.e., p(w
i
t
i
|w
i?1
i?2
t
i?1
i?2
)
backs off to p(w
i
t
i
|w
i?1
t
i?1
), which in turn backs off to the
unigram p(w
i
t
i
).
Figure 4: A fragment of the decision tree with a
backoff node. S ? ?S is the set of words observed
in the training data at the node A. To account for
unseen words, we add the backoff node B.
4 Decoding
As in HMM decoding, in order to compute prob-
abilities for i-th step, we need to sum over |T |n?1
possible combinations of tags in the history, where
T is the set of tags and n is the order of the
model. With |T | predictions for the i-th step, we
have O(|T |n) computational complexity per word.
Straightforward computation of these probabili-
ties is problematic even for a trigram model with
POS tags, i.e., n = 3, |T | ? 40. A standard ap-
proach to limit computational requirements is to
use beam search where only N most likely paths
are retained. However, with fine-grain tags where
|T | ? 1, 500, a tractable beam size would only
cover a small fraction of the whole space, leading
to search errors such as pruning good paths.
Note that we have a history clustering function
(Eq. 4) represented by the decision tree, and we
should be able to exploit this clustering to elimi-
nate unnecessary computations involving equiva-
lent histories. Note that words in the history are
known exactly, thus we can create a projection of
the clustering function H in Eq. 4 to the plane
w
i?1
i?n+1
= const, i.e., where words in the context
are fixed to be whatever is observed in the history:
H(w
i?1
i?n+1
t
i?1
i?n+1
) ?
?
H
w
i?1
i?n+1
=const
(t
i?1
i?n+1
)
(6)
The number of distinct clusters in the projection
?
H depends on the decision tree configuration and
can vary greatly for different words wi?1
i?n+1
in the
history, but generally it is relatively small:
|
?
H
w
i?1
i?n+1
=const
(t
i?1
i?n+1
)| ? |T
n?1
| (7)
1119
Figure 5: Questions about hidden factors split
states (see Figure 6) in the decoding lattice rep-
resented by HFTs.
thus, the number of probabilities that we need to
compute is | ?H
w
i?1
i?n+1
=const
| ? |T |.
Our decoding algorithm works similarly to
HMM decoding with the exception that the set of
hidden states is not predetermined. Let us illus-
trate how it works in the case of a bigram model.
Recall that the set of tags T is represented as a
binary tree (HFT) and the only type of questions
about tags is about matching a binary prefix in the
HFT. Such a question dissects the HFT into two
parts as depicted in Figure 5. The cost of this op-
eration is O(log |T |).
We represent states in the decoding lattice as
shown in the Figure 6, where pS
in
is the probability
of reaching the state S:
p
S
in
=
?
S
?
?IN
S
?
?
p
S
?
in
p(w
i?2
|H
S
?
)
?
t?T
S
?
p(t|w
i?2
H
S
?
)
?
?
where IN
S
is the set of incoming links to the
state S from the previous time index, and T
S
? is
the set of tags generated from the state S? repre-
sented as a fragment of the HFT. Note, that since
we maintain the property that the probability as-
signed to an inner node of the HFT is the sum
of probabilities of the tags it dominates, the sum
?
t?T
S
?
p(t|w
i?2
H
S
?
) is located at the root of T
S
? ,
and therefore this is an O(1) operation.
Now given the state S at time i ? 1, in order to
generate tag predictions for i-th word, we apply
questions from the history clustering tree, start-
ing from the top. Questions about overt factors
Figure 6: A state S in the decoding lattice. pS
in
is
the probability of reaching the state S through the
set of links IN
S
. The probabilities of generating
the tags p(t
i?1
|w
i?1
, H
s
), (t
i?1
? T
S
) are repre-
sented in the form of the HFT.
always follow either a true or false branch, implic-
itly computing the projection in Eq. 6. Questions
about hidden factors, can split the state S into two
states S
true
and S
false
, each retaining a part of T
S
as shown in the Figure 5.
The process continues until each fragment of
each state at the time i ? 1 reaches the bottom of
the history tree, at which point new states for time
i are generated from the clusters associated with
leaves. The states at i? 1 that generate the cluster
H
?
S
become the incoming links to the state ?S.
Higher order models work similarly, except that
at each time we consider a state S at time i ? 1
along with one of its incoming links (to some
depth according to the size of the context).
5 Experimental Setup
To evaluate the impact of fine-grain tags on lan-
guage modeling, we trained our model with five
settings: In the first model, questions were re-
stricted to be about overt factors only, thus making
it a tree-based word model. In the second model,
we used POS tags. To evaluate the effect of fine-
grain tags, we train two models: head and parent
described in Section 2.3 and Section 2.4 respec-
tively. Since our joint model can be used with
any kind of tags, we also trained it with Super-
ARV tags (Wang et al, 2003). The SuperARVs
were created from the same parse trees that were
used to produce POS and fine-grain tags. All our
models, including SuperARV, use trigram context.
We include standard trigram, four-gram, and five-
1120
gram models for reference. The ngram models
were trained using SRILM toolkit with interpo-
lated modified Kneser-Ney smoothing.
We evaluate our model with an nbest rescoring
task using 100-best lists from the DARPA WSJ?93
and WSJ?92 20k open vocabulary data sets. The
details on the acoustic model used to produce the
nbest lists can be found in (Wang and Harper,
2002). Since the data sets are small, we com-
bined the 93et and 93dt sets for evaluation and
used 92et for the optimization8. We transformed
the nbest lists to match PTB tokenization, namely
separating possessives from nouns, n?t from auxil-
iary verbs in contractions, as well as contractions
from personal pronouns.
All language models were trained on the NYT
1994-1995 section of the English Gigaword cor-
pus (approximately 70M words). Since the New
York Times covers a wider range of topics than
the Wall Street Journal, we eliminated the most ir-
relevant stories based on their trigram coverage by
sections 00-22 of WSJ. We also eliminated sen-
tences over 120 words, because the parser?s per-
formance drops significantly on long sentences.
After parsing the corpus, we deleted sentences that
were assigned a very low probability by the parser.
Overall we removed only a few percent of the data;
however, we believe that such a rigorous approach
to data cleaning is important for building discrim-
inating models.
Parse trees were produced by an extended ver-
sion of the Berkeley parser (Huang and Harper,
2009). We trained the parser on a combination of
the BN and WSJ treebanks, preprocessed to make
them more consistent with each other. We also
modified the trees for the speech recognition task
by replacing numbers and abbreviations with their
verbalized forms. We pre-processed the NYT cor-
pus in the same way, and parsed it. After that, we
removed punctuation and downcased words. For
the ngram model, we used text processed in the
same way.
In head and parent models, tag vocabularies
contain approximately 1,500 tags each, while the
SuperARV model has approximately 1,400 dis-
tinct SuperARVs, most of which represent verbs
(1,200).
In these experiments we did not use overt fac-
tors other than the surface word because they split
8We optimized the LM weight and computed WER with
scripts in the SRILM and NIST SCTK toolkits.
Models WER
trigram (baseline) 17.5
four-gram 17.7
five-gram 17.8
Word Tree 17.3
POS Tags 17.0
Head Tags 16.8
Parent Tags 16.7
SuperARV 16.9
Table 1: WER results, optimized on 92et set, eval-
uated on combined 93et and 93dt set. The Oracle
WER is 9.5%.
<unk>, effectively changing the vocabulary thus
making perplexity incomparable to models with-
out these factors, without improving WER notice-
ably. However, we do plan to use more overt
factors in Machine Translation experiments where
a language model faces a wider range of OOV
phenomena, such as abbreviations, foreign words,
numbers, dates, time, etc.
Table 1 summarizes performance of the LMs on
the rescoring task. The parent tags model outper-
forms the trigram baseline model by 0.8% WER.
Note that four- and five-gram models fail to out-
perform the trigram baseline. We believe this is
due to the sparsity as well as relatively short sen-
tences in the test set (16 words on average).
Interestingly, whereas the improvement of the
POS model over the baseline is not statistically
significant (p < 0.10)9, the fine-grain models out-
perform the baseline much more reliably: p <
0.03 (SuperARV) and p < 0.007 (parent).
We present perplexity evaluations in Table 2.
The perplexity was computed on Section 23 of
WSJ PTB, preprocessed as the rest of the data we
used. The head model has the lowest perplexity
outperforming the baseline by 9%. Note, it even
outperforms the five-gram model, although by a
small 2% margin.
Although the improvements by the fine-grain
tagsets over POS are not significant (due to the
small size of the test set), the reductions in per-
plexity suggest that the improvements are not ran-
dom.
9For statistical significance, we used SCTK implementa-
tion of the mapsswe test.
1121
Models PPL
trigram (baseline) 162
four-gram 152
five-gram 150
Word Tree 160
POS Tags 154
Head Tags 147
Parent Tags 150
SuperARV 150
Table 2: Perplexity results on Section 23 WSJ
PTB
6 Conclusion and Future Work
In this paper, we presented a joint language mod-
eling framework. Unlike any prior work known
to us, it was not tailored for any specific tag set,
rather it was designed to accommodate any set
of tags, especially large sets (? 1, 000), which
present challenges one does not encounter with
smaller tag sets, such at POS tags. We discussed
these challenges and our solutions to them. Some
of the solutions proposed are novel, particularly
the decoding algorithm.
We also proposed two simple fine-grain tagsets,
which, when applied in language modeling, per-
form comparably to highly sophisticated tag sets
(SuperARV). We would like to stress that, while
our fine-grain tags did not significantly outperform
SuperARVs, the former use much less linguistic
knowledge and can be automatically induced for
any language with a treebank.
Because a joint language model inherently pre-
dicts hidden events (tags), it can also be used to
generate the best sequence of those events, i.e.,
tagging. We evaluated our model in the POS tag-
ging task and observed similar results: the fine-
grain models outperform the POS model, while
both outperform the state-of-the-art HMM POS
taggers. We refer to (Filimonov and Harper, 2009)
for details on these experiments.
We plan to investigate how parser accuracy and
data selection strategies, e.g., based on parser con-
fidence scores, impact the performance of our
model. We also plan on evaluating the model?s
performance on other genres of speech, as well as
in other tasks such as Machine Translation. We
are also working on scaling our model further to
accommodate amounts of data typical for mod-
ern large-scale ngram models. Finally, we plan to
apply the technique to other languages with tree-
banks, such as Chinese and Arabic.
We intend to release the source code of our
model within several months of this publication.
7 Acknowledgments
This material is based upon work supported in
part by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-
06-C-0023 and NSF IIS-0703859. Any opinions,
findings and/or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the funding agencies or the
institutions where the work was completed.
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical
language model for natural language speech recog-
nition. Readings in speech recognition, pages 507?
514.
Srinivas Bangalore. 1996. ?Almost parsing? technique
for language modeling. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, volume 2, pages 1173?1176.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff.
In Proceedings of HLT/NACCL, 2003, pages 4?6.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jennifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling for speech recognition.
CoRR.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310?318, Morristown, NJ, USA. Association
for Computational Linguistics.
Denis Filimonov and Mary Harper. 2009. Measuring
tagging performance of a joint language model. In
Proceedings of the Interspeech 2009.
Peter A. Heeman. 1999. POS tags and decision trees
for language modeling. In In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 129?137.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP
2009.
1122
David M. Magerman. 1994. Natural language pars-
ing as statistical pattern recognition. Ph.D. thesis,
Stanford, CA, USA.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 1253?1256.
Thomas R. Niesler and Phil C. Woodland. 1996.
A variable-length category-based n-gram language
model. Proceedings of the IEEE International Con-
ference on Acoustics, Speech, and Signal Process-
ing, 1:164?167 vol. 1, May.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden markov model for part-of-speech tag-
ging. In Proceedings of the 37th Annual Meeting of
the ACL, pages 175?182.
Wen Wang and Mary P. Harper. 2002. The SuperARV
language model: investigating the effectiveness of
tightly integrating multiple knowledge sources. In
EMNLP ?02: Proceedings of the ACL-02 conference
on Empirical methods in natural language process-
ing, pages 238?247, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Wen Wang, Mary P. Harper, and Andreas Stolcke.
2003. The robustness of an almost-parsing language
model given errorful training data. In Proceedings
of the IEEE International Conference on Acoustics,
Speech, and Signal Processing.
Peng Xu and Frederick Jelinek. 2004. Random forests
in language modeling. In in Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model un-
seen events in speech recognition. Computer Speech
& Language, 21(1):88?104.
1123
Proceedings of NAACL HLT 2009: Short Papers, pages 213?216,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving A Simple Bigram HMM Part-of-Speech Tagger by Latent
Annotation and Self-Training
Zhongqiang Huang?, Vladimir Eidelman?, Mary Harper??
?Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
?Human Language Technology Center of Excellence
Johns Hopkins University
{zqhuang,vlad,mharper}@umiacs.umd.edu
Abstract
In this paper, we describe and evaluate a bi-
gram part-of-speech (POS) tagger that uses
latent annotations and then investigate using
additional genre-matched unlabeled data for
self-training the tagger. The use of latent
annotations substantially improves the per-
formance of a baseline HMM bigram tag-
ger, outperforming a trigram HMM tagger
with sophisticated smoothing. The perfor-
mance of the latent tagger is further enhanced
by self-training with a large set of unlabeled
data, even in situations where standard bigram
or trigram taggers do not benefit from self-
training when trained on greater amounts of
labeled training data. Our best model obtains
a state-of-the-art Chinese tagging accuracy of
94.78% when evaluated on a representative
test set of the Penn Chinese Treebank 6.0.
1 Introduction
Part-of-speech (POS) tagging, the process of as-
signing every word in a sentence with a POS tag
(e.g., NN (normal noun) or JJ (adjective)), is pre-
requisite for many advanced natural language pro-
cessing tasks. Building upon the large body of re-
search to improve tagging performance for various
languages using various models (e.g., (Thede and
Harper, 1999; Brants, 2000; Tseng et al, 2005b;
Huang et al, 2007)) and the recent work on PCFG
grammars with latent annotations (Matsuzaki et al,
2005; Petrov et al, 2006), we will investigate the use
of fine-grained latent annotations for Chinese POS
tagging. While state-of-the-art tagging systems have
achieved accuracies above 97% in English, Chinese
POS tagging (Tseng et al, 2005b; Huang et al,
2007) has proven to be more challenging, and it is
the focus of this study.
The value of the latent variable approach for tag-
ging is that it can learn more fine grained tags to bet-
ter model the training data. Liang and Klein (2008)
analyzed the errors of unsupervised learning using
EM and found that both estimation and optimiza-
tion errors decrease as the amount of unlabeled data
increases. In our case, the learning of latent anno-
tations through EM may also benefit from a large
set of automatically labeled data to improve tagging
performance. Semi-supervised, self-labeled data has
been effectively used to train acoustic models for
speech recognition (Ma and Schwartz, 2008); how-
ever, early investigations of self-training on POS
tagging have mixed outcomes. Clark et al (2003)
reported positive results with little labeled training
data but negative results when the amount of labeled
training data increases. Wang et al (2007) reported
that self-training improves a trigram tagger?s accu-
racy, but this tagger was trained with only a small
amount of in-domain labeled data.
In this paper, we will investigate whether the
performance of a simple bigram HMM tagger can
be improved by introducing latent annotations and
whether self-training can further improve its perfor-
mance. To the best of our knowledge, this is the first
attempt to use latent annotations with self-training
to enhance the performance of a POS tagger.
2 Model
POS tagging using a hidden Markov model can be
considered as an instance of Bayesian inference,
213
wherein we observe a sequence of words and need
to assign them the most likely sequence of POS tags.
If ti1 denotes the tag sequence t1, ? ? ? , ti, and wi1denotes the word sequence w1, ? ? ? , wi, given the
first-order Markov assumption of a bigram tagger,
the best tag sequence ?(wn1 ) for sentence wn1 can be
computed efficiently as1:
?(wn1 ) = argmaxtn1 p(tn1 |wn1 )
? argmaxtn1
?
i
p(ti|ti?1)p(wi|ti)
with a set of transition parameters {p(b|a)}, for tran-
siting to tag b from tag a, and a set of emission
parameters {p(w|a)}, for generating word w from
tag a. A simple HMM tagger is trained by pulling
counts from labeled data and normalizing to get the
conditional probabilities.
It is well know that the independence assumption
of a bigram tagger is too strong in many cases. A
common practice for weakening the independence
assumption is to use a second-order Markov as-
sumption, i.e., a trigram tagger. This is similar to
explicitly annotating each POS tag with the preced-
ing tag. Rather than explicit annotation, we could
use latent annotations to split the POS tags, sim-
ilarly to the introduction of latent annotations to
PCFG grammars (Matsuzaki et al, 2005; Petrov
et al, 2006). For example, the NR tag may be
split into NR-1 and NR-2, and correspondingly the
POS tag sequence of ?Mr./NR Smith/NR saw/VV
Ms./NR Smith/NR? could be refined as: ?Mr./NR-2
Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1?.
The objective of training a bigram tagger with la-
tent annotations is to find the transition and emission
probabilities associated with the latent tags such that
the likelihood of the training data is maximized. Un-
like training a standard bigram tagger where the POS
tags are observed, in the latent case, the latent tags
are not observable, and so a variant of EM algorithm
is used to estimate the parameters.
Given a sentence wn1 and its tag sequence tn1 , con-sider the i-th word wi and its latent tag ax ? a = ti
(which means ax is a latent tag of tag a, the i-th tag
in the sequence) and the (i + 1)-th word wi+1 and
its latent tag by ? b = ti+1, the forward, ?i+1(by) =
p(wi+11 , by), and backward, ?i(ax) = p(wni+1|ax),probabilities can be computed recursively:
?i+1(by) =
?
x
?i(ax)p(by|ax)p(wi+1|by)
1We assume that symbols exist implicitly for boundary con-
ditions.
?i(ax) =
?
y
p(by|ax)p(wi+1|by)?j+1(by)
In the E step, the posterior probabilities of co-
occurrence events can be computed as:
p(ax, by|w) ? ?i(ax)p(by|ax)?i+1(by)
p(ax, wi|w) ? ?i(ax)?i(ax)
In the M step, the above posterior probabilities are
used as weighted observations to update the transi-
tion and emission probabilities2:
p(by|ax) = c(ax, by)/
?
by
c(ax, by)
p(w|ax) = c(ax, w)/
?
w
c(ax, w)
A hierarchical split-and-merge method, similar
to (Petrov et al, 2006), is used to gradually increase
the number of latent annotations while allocating
them adaptively to places where they would pro-
duce the greatest increase in training likelihood (e.g.,
we observe heavy splitting in categories such as NN
(normal noun) and VV (verb), that cover a wide vari-
ety of words, but only minimal splitting in categories
like IJ (interjection) and ON (onomatopoeia)).
Whereas tag transition occurrences are frequent,
allowing extensive optimization using EM, word-tag
co-occurrences are sparser and more likely to suf-
fer from over-fitting. To handle this problem, we
map all words with frequency less than threshold3
? to symbol unk and for each latent tag accumu-
late the word tag statistics of these rare words to
cr(ax, unk) = ?w:c(w)<? c(ax, w). These statistics
are redistributed among the rare words (w : c(w) <
?) to compute their emission probabilities:
c(ax, w) = cr(ax, unk) ? c(a,w)/cr(a, unk)
p(w|ax) = c(ax, w)/
?
w
c(ax, w)
The impact of this rare word handling method will
be investigated in Section 3.
A character-based unknown word model, similar
to the one described in (Huang et al, 2007), is used
to handle unknown Chinese words during tagging.
A decoding method similar to the max-rule-product
method in (Petrov and Klein, 2007) is used to tag
sentences using our model.
3 Experiments
The Penn Chinese Treebank 6.0 (CTB6) (Xue et al,
2005) is used as the labeled data in our study. CTB6
2c(?) represents the count of the event.
3The value of ? is tuned on the development set.
214
contains news articles, which are used as the primary
source of labeled data in our experiments, as well as
broadcast news transcriptions. Since the news ar-
ticles were collected during different time periods
from different sources with a diversity of topics, in
order to obtain a representative split of train-test-
development sets, we divide them into blocks of 10
files in sorted order and for each block use the first
file for development, the second for test, and the re-
maining for training. The broadcast news data ex-
hibits many of the characteristics of newswire text
(it contains many nonverbal expressions, e.g., num-
bers and symbols, and is fully punctuated) and so is
also included in the training data set. We also uti-
lize a greater number of unlabeled sentences in the
self-training experiments. They are selected from
similar sources to the newswire articles, and are
normalized (Zhang and Kahn, 2008) and word seg-
mented (Tseng et al, 2005a). See Table 1 for a sum-
mary of the data used.
Train Dev Test Unlabeled
sentences 24,416 1904 1975 210,000
words 678,811 51,229 52,861 6,254,947
Table 1: The number of sentences and words in the data.
50 100 150 200 250 300 350 40091.5
92
92.5
93
93.5
94
94.5
Number of latent annotations
Tok
en 
acc
ura
cy (%
)
Bigram+LA:1
Bigram+LA:2
Trigram
Figure 1: The learning curves of the bigram tagger with
latent annotations on the development set.
Figure 1 plots the learning curves of two bigram
taggers with latent annotations (Bigram+LA:2 has
the special handling of rare words as described in
Section 2 while Bigram+LA:1 does not) and com-
pares its performance with a state-of-the-art trigram
HMM tagger (Huang et al, 2007) that uses trigram
transition and emission models together with bidi-
rectional decoding. Both bigram taggers initially
have much lower tagging accuracy than the trigram
tagger, due to its strong but invalid independence as-
sumption. As the number of latent annotations in-
creases, the bigram taggers are able to learn more
from the context based on the latent annotations,
and their performance improves significantly, out-
performing the trigram tagger. The performance
gap between the two bigram taggers suggests that
over-fitting occurs in the word emission model when
more latent annotations are available for optimiza-
tion; sharing the statistics among rare words alle-
viates some of the sparseness while supporting the
modeling of deeper dependencies among more fre-
quent events. In the later experiments, we use Bi-
gram+LA to denote the Bigram+LA:2 tagger.
Figure 2 compares the self-training capability of
three models (the bigram tagger w/ or w/o latent
annotations, and the aforementioned trigram tagger)
using different sizes of labeled training data and the
full set of unlabeled data. For each model, a tag-
ger is first trained on the allocated labeled training
data and is then used to tag the unlabeled data. A
new tagger is then trained on the combination4 of
the allocated labeled training data and the newly au-
tomatically labeled data.
0.1 0.2 0.4 0.6 0.8 189
90
91
92
93
94
95
Fraction of CTB6 training data
Tok
en 
acc
ura
cy (%
)
Bigram+LA+STBigram+LATrigram+STTrigramBigram+STBigram
Figure 2: The performance of three taggers evaluated on
the development set, before and after self-training with
different sizes of labeled training data.
There are two interesting observations that distin-
guish the bigram tagger with latent annotations from
the other two taggers. First, although all of the tag-
gers improve as more labeled training data is avail-
able, the performance gap between the bigram tag-
ger with latent annotations and the other two taggers
also increases. This is because more latent annota-
tions can be used to take advantage of the additional
training data to learn deeper dependencies.
Second, the bigram tagger with latent annotations
benefits much more from self-training, although it
4We always balance the size of manually and automatically
labeled data through duplication (for the trigram tagger) or pos-
terior weighting (for the bigram tagger w/ or w/o latent annota-
tions), as this provides superior performance.
215
already has the highest performance among the three
taggers before self-training. The bigram tagger
without latent annotations benefits little from self-
training. Except for a slight improvement when
there is a small amount of labeled training, self-
training slightly hurts tagging performance as the
amount of labeled data increases. The trigram tag-
ger benefits from self-training initially but eventu-
ally has a similar pattern to the bigram tagger when
trained on the full labeled set. The performance
of the latent bigram tagger improves consistently
with self-training. Although the gain decreases for
models trained on larger training sets, since stronger
models are harder to improve, self-training still con-
tributes significantly to model accuracy.
The final tagging performance on the test set is
reported in Table 2. All of the improvements are
statistically significant (p < 0.005).
Tagger Token Accuracy (%)
Bigram 92.25
Trigram 93.99
Bigram+LA 94.53
Bigram+LA+ST 94.78
Table 2: The performance of the taggers on the test set.
It is worth mentioning that we initially added la-
tent annotations to a trigram tagger, rather than a bi-
gram tagger, to build from a stronger starting point;
however, this did not work well. A trigram tagger re-
quires sophisticated smoothing to handle data spar-
sity, and introducing latent annotations exacerbates
the sparsity problem, especially for trigram word
emissions. The uniform extension of a bigram tag-
ger to a trigram tagger ignores whether the use of ad-
ditional context is helpful and supported by enough
data, nor is it able to use a longer context. In con-
trast, the bigram tagger with latent annotations is
able to learn different granularities for tags based on
the training data.
4 Conclusion
In this paper, we showed that the accuracy of a sim-
ple bigram HMM tagger can be substantially im-
proved by introducing latent annotations together
with proper handling of rare words. We also showed
that this tagger is able to benefit from self-training,
despite the fact that other models, such as bigram or
trigram HMM taggers, do not.
In the future work, we will investigate automatic
data selection methods to choose materials that are
most suitable for self-training and evaluate the effect
of the amount of automatically labeled data.
Acknowledgments
This work was supported by NSF IIS-0703859
and DARPA HR0011-06-C-0023 and HR0011-06-
2-001. Any opinions, findings and/or recommenda-
tions expressed in this paper are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
References
T. Brants. 2000. TnT a statistical part-of-speech tagger.
In ANLP.
S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrap-
ping pos taggers using unlabelled data. In CoNLL.
Z. Huang, M. Harper, and W. Wang. 2007. Mandarin
part-of-speech tagging and discriminative reranking.
EMNLP.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In ACL.
J. Ma and R. Schwartz. 2008. Factors that affect unsu-
pervised training of acoustic models. In Interspeech.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL. Association
for Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL.
S. M. Thede and M. P. Harper. 1999. A second-order
hidden markov model for part-of-speech tagging. In
ACL.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005a. A conditional random field word seg-
menter. In SIGHAN Workshop on Chinese Language
Processing.
H. Tseng, D. Jurafsky, and C. Manning. 2005b. Morpho-
logical features help pos tagging of unknown words
across language varieties. In SIGHAN Workshop on
Chinese Language Processing.
W. Wang, Z. Huang, and M. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of Man-
darin transcribed speech. In ICASSP.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
penn chinese treebank: Phrase structure annotation of
a large corpus. Natural Language Engineering.
B. Zhang and J. G. Kahn. 2008. Evaluation of decatur
text normalizer for language model training. Technical
report, University of Washington.
216
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 620?629, Prague, June 2007. c?2007 Association for Computational Linguistics
Recovery of Empty Nodes in Parse Structures
Denis Filimonov1
1University of Maryland
College Park, MD 20742
den@cs.umd.edu
Mary P. Harper1,2
2Purdue University
West Lafayette, IN 47907
mharper@casl.umd.edu
Abstract
In this paper, we describe a new algorithm
for recovering WH-trace empty nodes. Our
approach combines a set of hand-written
patterns together with a probabilistic model.
Because the patterns heavily utilize regu-
lar expressions, the pertinent tree structures
are covered using a limited number of pat-
terns. The probabilistic model is essen-
tially a probabilistic context-free grammar
(PCFG) approach with the patterns acting as
the terminals in production rules. We eval-
uate the algorithm?s performance on gold
trees and parser output using three differ-
ent metrics. Our method compares favorably
with state-of-the-art algorithms that recover
WH-traces.
1 Introduction
In this paper, we describe a new algorithm for re-
covering WH-trace empty nodes in gold parse trees
in the Penn Treebank and, more importantly, in
automatically generated parses. This problem has
only been investigated by a handful of researchers
and yet it is important for a variety of applications,
e.g., mapping parse trees to logical representations
and structured representations for language mod-
eling. For example, SuperARV language models
(LMs) (Wang and Harper, 2002; Wang et al, 2003),
which tightly integrate lexical features and syntactic
constraints, have been found to significantly reduce
word error in English speech recognition tasks. In
order to generate SuperARV LM training, a state-of-
the-art parser is used to parse training material and
then a rule-based transformer converts the parses to
the SuperARV representation. The transformer is
quite accurate when operating on treebank parses;
however, trees produced by the parser lack one im-
portant type of information ? gaps, particularly WH-
traces, which are important for more accurate ex-
traction of the SuperARVs.
Approaches applied to the problem of empty
node recovery fall into three categories. Dienes
and Dubey (2003) recover empty nodes as a pre-
processing step and pass strings with gaps to their
parser. Their performance was comparable to
(Johnson, 2002); however, they did not evaluate
the impact of the gaps on parser performance.
Collins (1999) directly incorporated wh-traces into
his Model 3 parser, but he did not evaluate gap in-
sertion accuracy directly. Most of the research be-
longs to the third category, i.e., post-processing of
parser output. Johnson (2002) used corpus-induced
patterns to insert gaps into both gold standard trees
and parser output. Campbell (2004) developed a
set of linguistically motivated hand-written rules for
gap insertion. Machine learning methods were em-
ployed by (Higgins, 2003; Levy and Manning, 2004;
Gabbard et al, 2006).
In this paper, we develop a probabilistic model
that uses a set of patterns and tree matching to guide
the insertion of WH-traces. We only insert traces of
non-null WH-phrases, as they are most relevant for
our goals. Our effort differs from the previous ap-
proaches in that we have developed an algorithm for
the insertion of gaps that combines a small set of ex-
pressive patterns with a probabilistic grammar-based
model.
620
2 The Model
We have developed a set of tree-matching patterns
that are applied to propagate a gap down a path in
a parse tree. Pattern examples appear in Figure 1.
Each pattern is designed to match a subtree (a root
and one or more levels below that root) and used to
guide the propagation of the trace into one or more
nodes at the terminal level of the pattern (indicated
using directed edges). Since tree-matching patterns
are applied in a top-down fashion, multiple patterns
can match the same subtree and allow alternative
ways to propagate a gap. Hence, we have developed
a probabilistic model to select among the alterna-
tive paths. We have created 24 patterns for WHNP
traces, 16 for WHADVP, 18 for WHPP, and 11 for
WHADJP.
Figure 1: Examples of tree-matching patterns
Before describing our model, we first introduce
some notation.
? TNij is a tree dominating the string of words be-
tween positions i and j with N being the label of
the root. We assume there are no unary chains like
N?X? ...?Y ?N (which could be collapsed to
a single node N ) in the tree, so that TNij uniquely
describes the subtree.
? A gap location gab,Ncd is represented as a tuple
(gaptype, ancstr(a, b,N), c, d), where gaptype
is the type of the gap, (e.g., whnp for a WHNP
trace), ancstr(a, b,N) is the gap?s nearest ances-
tor, with a and b being its span and N being its
label, and c and d indicating where the gap can
be inserted. Note that a gap?s location is specified
precisely when c = d. If the gap is yet to be in-
serted into its final location but will be inserted
somewhere inside ancstr(a, b,N), then we set
c = a and d = b.
? ancstr(a, b,N) in the tuple for gab,Nxy is the tree
TNab .
? p(gab,Nxy |gaptype, TNij ) is the probability that a
gap of gaptype is located between x and y, with a
and b being the span of its ancestor, and i ? a ?
x ? y ? b ? j.
Given this notation, our model is tasked to identify
the best location for the gap in a parse tree among
the alternatives, i.e.,
argmax
x,a,b,N
Pr(gab,Nxx |T, gaptype)
where gab,Nxx represents a gap location in a tree, and
T = TNij is the subtree of the parse tree whose
root node is the nearest ancestor node dominating
the WH-phrase, excluding the WH-node itself, and
gaptype is the type of the gap. In order to simplify
the notation, we will omit the root labels N in TNij
and gab,Nxy , implying that they match where appropri-
ate.
To guide this model, we utilize tree-matching pat-
terns (see Figure 1), which are formally defined as
functions:
ptrn : T ? G ? ? ? {none}
where T is the space of parse trees, G is the space
of gap types, and ? is the space of gaps gabcd ,
and none is a special value representing failure to
match1. The application of a pattern is defined as:
app(ptrn, ?, gaptype) = ptrn(?, gaptype), where
? ? T and gaptype ? G. We define application of
patterns as follows:
app(ptrn, Tij , gaptype) ? gabxy : i ? a ? x < y ? b ? j
app(ptrn, Tij , gaptype) ? gabxx : i ? a ? x ? b ? j
app(ptrn, Tij , gaptype) ? none
Because patterns are uniquely associated with spe-
cific gap types, we will omit gaptype to simplify the
notation. Application is a function defined for every
pair (ptrn, Tij) with fixed gaptype. Patterns are ap-
plied to the root of Tij , not to an arbitrary subtree.
Consider an example of pattern application shown
in Figure 2. The tree contains a relative clause such
that the WHNP-phrase that was moved from some
location inside the subtree of its sister node S.
2
viewers
3
will
4
tune
5
in
6
to
7
see
8
1Modeling conjunction requires an alternative definition for
patterns: ptrn : T ? G ? Powerset(?) ? {none}. For the
sake of simplicity, we ignore conjunctions in the following dis-
cussion, except for in the few places where it matters, since this
has little impact on the development of our model.
621
Figure 2: A pattern application example
Now suppose there is a pattern P1 that matches
the tree T28 indicating that the gap is some-
where in its subtree T38 (will tune in to see), i.e.,
app(P1, T28) ? g3838 . The process of applying pat-
terns continues until the pattern P4 proposes an ex-
act location for the gap: app(P4, T78) = g7888 .
Figure 3: Another pattern application example
Suppose that, in addition to the pattern applica-
tions shown in Figure 2, there is one more, namely:
app(P5, T48) ? g4866 . The sequence of patterns
P1, P2, P5 proposes an alternative grammatically
plausible location for the gap, as shown in Figure
3. Notice that the combination of the two sequences
produces a tree of patterns, as shown in Figure 4,
and this pattern tree covers much of the structure of
the T28 subtree.
2.1 Tree Classes
The number of unique subtrees that contain WH-
phrases is essentially infinite; hence, modeling them
directly is infeasible. However, trees with varying
details, e.g., optional adverbials, often can be char-
P1
P2
P3C
D P4,$
E
A
B
F
P5,$
Figure 4: Pattern tree
acterized by the same tree of patterns. Hence, we
can represent the space of trees by utilizing a rela-
tively small set of classes of trees that are determined
by their tree of pattern applications.
Let ? be the set of all patterns. We define the set
of patterns matching tree Tij as follows:
M(Tij) = {P | P ? ? ? app(P, Tij) 6= none}
To enable recursive application:
app(ptrn, gabxy) =
{ app(ptrn, Tab) if x < y
none if x = y
A Pattern Chain PC is a sequence of pairs
of patterns and sets of pattern sets, terminated by
$, i.e., ( p1M1 ,
p2
M2 , ...
pn
Mn , $), where ?i pi ? Mi ?
?. Mi = M(Tab), where Tab is the result of
consequent application of the first i ? 1 patterns:
app(pi?1, app(pi?2, ..., app(p1, T??))) = gabxy, and
where T?? is the subtree we started with, (T28 in the
example above). We define the application of a pat-
tern chain PC = ( p1M1 ,
p2
M2 , ...
pn
Mn , $) to a tree Tij
as:
app(PC, Tij) = app(pn, ...app(p2, app(p1, Tij)))
It is important to also define a function to map
a tree to the set of pattern chains applicable to a
particular tree. The pseudocode for this function
called FindPCs appears in Figure 52. When ap-
plied to Tij , this function returns the set of all pat-
tern chains, applications of which would result in
concrete gap locations. The algorithm is guaranteed
to terminate as long as trees are of finite depth and
each pattern moves the gap location down at least
one level in the tree at each iteration. Using this
function, we define Tree Class (TC) of a tree Tij
as TC(Tij) = FindPCs(Tij).
2list ? element means ?append element to list?.
622
function FindPCs?(Tij , PC, allPCs) {
Mij ? {P | P ? ? ? app(P, Tij) 6= none}
forall P ? Mij
gabxy ? app(P, Tij)
PC ? PC ? PMij
if x = y then // gabxy is a concrete location
allPCs ? allPCs ? {PC ? $}
else
allPCs ? FindPCs?(Tab, PC, allPCs)
return allPCs }
function FindPCs(Tij) { return FindPCs?(Tij , [ ], ?) }
Figure 5: Pseudocode for FindPCs
In the case of a conjunction, the function Find-
PCs is slightly more complex. Recall that in this
case app(P, Tij) produces a set of gaps or none. The
pseudocode for this case appears in Figure 6.
2.2 A Gap Automaton
The set of pattern chains constructed by the function
FindPCs can be represented as a pattern tree with
patterns being the edges. For example, the pattern
tree in Figure 4 corresponds to the tree displayed in
Figures 2 and 3.
This pattern tree captures the history of gap prop-
agations beginning at A. Assuming at that point only
pattern P1 is applicable, subtree B is produced. If P2
yields subtree C, and at that point patterns P3 and
P5 can be applied, this yields subtree D and exact
location F (which is expressed by the termination
symbol $), respectively. Finally, pattern P4 matches
subtree D and proposes exact gap location E. It is
important to note that this pattern tree can be thought
of as an automaton, with A,B,C,D,E, and F be-
ing the states and the pattern applications being the
transitions.
Now, let us assign meaning of the states
A,B,C, and D to be the set of matching patterns,
i.e., A = {P1}, B = {P2}, C = {P3, P5}, D = {P4}, and
E = F = ?. Given this representation, the pattern
chains for the insertion of the gaps in our example
would be as follows:
({P1}) P1? ({P2}) P2? ({P3, P5}) P3? ({P4}) P4,$?? (?)
({P1}) P1? ({P2}) P2? ({P3, P5}) P5,$?? (?)
With this representation, we can create a regular
grammar using patterns as the terminals and their
function CrossProd(PC1, PC2) {
prod ? ?
forall pci ? PC1
forall pcj ? PC2 : prod ? prod?{pci?pcj}
return prod }
function FindPCs(Tij) {
Mij ? {P | P ? ? ? app(P, Tij) 6= none}
newPCs ? ?
forall P ? Mij
PCs ? {[ ]}
forall gabxy ? app(P, Tij)
if x = y then
forall pc ? PCs : pc ? pc ? $
else
PCs ? CrossProd(PCs,FindPCs(Tab))
forall pc ? PCs : pc ? PMij ? pc
newPCs ? newPCs ? PCs
return newPCs }
The set app(P, Tij) must be ordered, so that
branches of conjunction are concatenated in a well de-
fined order.
Figure 6: Pseudocode for FindPCs in the case of
conjunction
powerset as the non-terminals (adding a few more
details like the start symbol) and production rules
such as {P2} ? P2 {P3, P5}. However, for our exam-
ple the chain of patterns applied P1, P2, P3, P4, $ could
generate a pattern tree that is incompatible with the
original tree. For example:
({P1}) P1? ({P2}) P2? ({P3, P5}) P3? ({P3, P4}) P4,$?? (?)
which might correspond to something like ?that
viewers will tune in to expect to see.? Note that this
pattern chain belongs to a different tree class, which
incidentally would have inserted the gap at a differ-
ent location (VP see gap).
To overcome this problem we add additional con-
straints to the grammar to ensure that all parses the
grammar generates belong to the same tree class.
One way to do this is to include the start state of
a transition as an element of the terminal, e.g., P2{P2} ,
P3
{P3,P5} . That is, we extend the terminals to include
the left-hand side of the productions they are emitted
from, e.g.,
{P2} ? P2{P2} {P3, P5}
623
{P3, P5} ? P3{P3, P5} {P4}
and the sequence of terminals becomes:
P1
{P1}
P2
{P2}
P3
{P3,P5}
P4
{P4} $.
Note that the grammar is unambiguous. For such
a grammar, the question ?what is the probability of a
parse tree given a string and grammar? doesn?t make
sense; however, the question ?what is the probability
of a string given the grammar? is still valid, and this
is essentially what we require to develop a genera-
tive model for gap insertion.
2.3 The Pattern Grammar
Let us define the pattern grammar more rigorously.
Let ? be the set of patterns, and ?? ? ? be the set
of terminal patterns3. Let pset(P ) be the set of all
subsets of patterns which include the pattern P , i.e.,
pset(P ) = {? ? {P} | ? ? powerset(?)}
? Let T = { Ppset(P ) | P ? ?}
?{$} be the set of
terminals, where $ is a special symbol4.
? Let N = {S}? powerset(?) be the set of non-
terminals with S being the start symbol.
? Let P be the set of productions, defined as the
union of the following sets:
1. {S ? ? | ? ? powerset(?)}.
2. {? ? P? ? | P ? ???? , ? ? pset(P ) and ? ?powerset(?)}. These are nonterminal transi-
tions, note that they emit only non-terminal pat-
terns.
3. {? ? P? $ | P ? ?? and ? ? pset(P )}. These
are the terminal transitions, they emit a termi-
nal pattern and the symbol $.
4. {? ? P? ?1 . . . ?n | P ? ? ? ?? , ? ?pset(P ) and ?i?[1..n] ?i ? powerset(?)}.
This rule models conjunction with n branches.
2.4 Our Gap Model
Given the grammar defined in the previous subsec-
tion, we will define a probabilistic model for gap in-
sertion. Recall that our goal is to find:
argmax
x,a,b
Pr(gabxx|T )
Just like the probability of a sentence is obtained by
summing up the probabilities of its parses, the prob-
ability of the gap being at gabxx is the sum of proba-
bilities of all pattern chains that yield gabxx.
3Patterns that generate exact position for a gap.
4Symbol $ helps to separate branches in strings with con-
junction.
Pr(gabxx|T ) =
?
pci??
Pr(pci|T )
where ? = {pc | app(pc, T ) = gabxx}. Note that
pci ? TC(T ) by definition.
For our model, we use two approximations. First,
we collapse a tree T into its Tree Class TC(T ), ef-
fectively ignoring details irrelevant to gap insertion:
Pr(pci|T ) ? Pr(pci|TC(T ))
Figure 7: A pattern tree with the pattern chain
ABDGM marked using bold lines
Consider the pattern tree shown in Figure 7. The
probability of the pattern chain ABDGM given the
pattern tree can be computed as:
Pr(ABDGM |TC(T )) = Pr(ABDGM,TC(T ))Pr(TC(T ))
= NR(ABDGM,TC(T ))
NR(TC(T ))
where NR(TC(T )) is the number of occurrences
of the tree class TC(T ) in the training corpus and
NR(ABDGM,TC(T )) is the number cases when
the pattern chain ABDGM leads to a correct gap in
trees corresponding to the tree class TC(T ). For
many tree classes, NR(TC(T )) may be a small
number or even zero, thus this direct approach can-
not be applied to the estimation of Pr(pci|TC(T )).
Further approximation is required to tackle the spar-
sity issue.
In the following discussion, XY will denote
an edge (pattern) between vertices X and Y in
624
the pattern tree shown in Figure 7. Note that
Pr(ABDGM |TC(T )) can be represented as:
Pr(AB|TC(T ), A)? Pr(BD|TC(T ), AB)?
?Pr(DG|TC(T ), ABD)? Pr(GM |TC(T ), ABDG)
We make an independence assumption, specifi-
cally, that Pr(BD|TC(T ), AB) depends only on
states B, D, and the edge between them, not on
the whole pattern tree or the edges above B, i.e.,
Pr(BD|TC(T ), AB) ? Pr(BD,D|B). Note that
this probability is equivalent to the probability of a
production Pr(B BD? D) of a PCFG.
Recall that the meaning assigned to a state
in pattern grammar in Section 2.2 is the set of
patterns matching at that state. Thus, accord-
ing to that semantics, only the edges displayed
bold in Figure 8 are involved in computation of
Pr(B BD? D). Written in the style we used for
our grammar, the production is {BD,BE,BF} ?
BD
{BD,BE,BF}{DG,DH}.
Figure 8: The context considered for estimation of
the probability of transition from B to D
Pattern trees are fairly shallow (partly because
many patterns cover several layers in a parse tree
as can be seen in Figures 1 and 2); therefore, the
context associated with a production covers a good
part of a pattern tree. Another important observa-
tion is that the local configuration of a node, which
is described by the set of matching patterns, is the
most relevant to the decision of where the gap is to
be propagated5. This is the reason why the states are
represented this way.
Formally, the second approximation we make is
5We have evaluated a model that only uses
Pr(BD|{BD,BE,BF}) for the probability of taking
BD and found it performs only slightly worse than the model
presented here.
as follows:
Pr(pci|TC(T )) ? Pr(pci|G)
where G is a PCFG model based on the grammar
described above.
Pr(pci|G) =
?
prodj?P(pci)
Pr(prodj |G)
where P(pci) is the parse of the pattern chain pci
which is a string of terminals of G. Combining the
formulae:
Pr(gabxx|T ) ?
?
pci??
Pr(pci|G)
Finally, since Pr(TC(T )|G) is a constant for T ,
argmax
x,a,b
Pr(gabxx|T ) ? argmaxx,a,b
?
pci??
Pr(pci|G)
To handle conjunction, we must express the fact
that pattern chains yield sets of gaps. Thus, the goal
becomes:
argmax
(x1,a1,b1),...,(xn,an,bn)
Pr({ga1b1x1x1 , . . . , ganbnxnxn}|T )
Pr({ga1b1x1x1 , . . . , ganbnxnxn}|T ) =
?
pci??
Pr(pci|T )
where ? = {pc | app(pc, T ) =
{ga1b1x1x1 , . . . , ganbnxnxn}}. The remaining equations
are unaffected.
2.5 Smoothing
Even for the relatively small number of patterns,
the number of non-terminals in the grammar can
potentially be large (2|?|). This does not happen
in practice since most patterns are mutually exclu-
sive. Nonetheless, productions, unseen in the train-
ing data, do occur and their probabilities have to be
estimated. Rewriting the probability of a transition
Pr(A ? aA B) as P(A, a,B), we use the following in-
terpolation:
P?(A, a,B) = ?1P(A, a,B) + ?2P(A, a)
+?3P(A,B) + ?4P(a,B) + ?5P(a)
We estimate the parameters on the held out data
(section 24 of WSJ) using a hill-climbing algorithm.
625
3 Evaluation
3.1 Setup
We compare our algorithm under a variety of condi-
tions to the work of (Johnson, 2002) and (Gabbard
et al, 2006). We selected these two approaches be-
cause of their availability6. In addition, (Gabbard et
al., 2006) provides state-of-the-art results. Since we
only model the insertion of WH-traces, all metrics
include co-indexation with the correct WH phrases
identified by their type and word span.
We evaluate on three metrics. The first metric,
which was introduced by Johnson (2002), has been
widely reported by researchers investigating gap in-
sertion. A gap is scored as correct only when it has
the correct type and string position. The metric has
the shortcoming that it does not require correct at-
tachment into the tree.
The second metric, which was developed by
Campbell (2004), scores a gap as correct only when
it has the correct gap type and its mother node has
the correct nonterminal label and word span. As
Campbell points out, this metric does not restrict the
position of the gap among its siblings, which in most
cases is desirable; however, in some cases (e.g., dou-
ble object constructions), it does not correctly detect
errors in object order. This metric is also adversely
affected by incorrect attachments of optional con-
stituents, such as PPs, due to the span requirement.
To overcome the latter issue with Campbell?s met-
ric, we propose to use a third metric that evaluates
gaps with respect to correctness of their lexical head,
type of the mother node, and the type of the co-
indexed wh-phrase. This metric differs from that
used by Levy and Manning (2004) in that it counts
only the dependencies involving gaps, and so it rep-
resents performance of the gap insertion algorithm
more directly.
We evaluate gap insertion on gold trees from sec-
tion 23 of the Wall Street Journal Penn Treebank
(WSJ) and parse trees automatically produced using
the Charniak (2000) and Bikel (2004) parsers. These
parsers were trained using sections 00 through 22 of
the WSJ with section 24 as the development set.
Because our algorithm inserts only traces of non-
empty WH phrases, to fairly compare to Johnson?s
and Gabbard?s performance on WH-traces alone, we
6Johnson?s source code is publicly available, and Ryan Gab-
bard kindly provided us with output trees produced by his sys-
tem.
remove the other gap types from both the gold trees
and the output of their algorithms. Note that Gab-
bard et al?s algorithm requires the use of function
tags, which are produced using a modified version
of the Bikel parser (Gabbard et al, 2006) and a sep-
arate software tool (Blaheta, 2003) for the Charniak
parser output.
For our algorithm, we do not utilize function tags,
but we automatically replace the tags of auxiliary
verbs in tensed constructions with AUX prior to in-
serting gaps using tree surgeon (Levy and Andrew,
2006). We found that Johnson?s algorithm more
accurately inserts gaps when operating on auxified
trees, and so we evaluate his algorithm using these
modified trees.
In order to assess robustness of our algorithm, we
evaluate it on a corpus of a different genre ? Broad-
cast News Penn Treebank (BN), and compare the re-
sult with Johnson?s and Gabbard?s algorithms. The
BN corpus uses a modified version of annotation
guidelines, with some of the modifications affecting
gap placement.
Treebank 2 guidelines (WSJ style):
(SBAR (WHNP-2 (WP whom))
(S (NP-SBJ (PRP they))
(VP (VBD called)
(S (NP-SBJ (-NONE- *T*-2))
(NP-PRD (NNS exploiters))))))
Treebank 2a guidelines (BN style):
(SBAR-NOM (WHNP-1 (WP what))
(S (NP-SBJ (PRP they))
(VP (VBP call)
(NP-2 (-NONE- *T*-1))
(S-CLR (NP-SBJ (-NONE- *PRO*-2))
(NP-PRD (DT an) (NN epidemic))))))
Since our algorithms were trained on WSJ, we ap-
ply tree transformations to the BN corpus to convert
these trees to WSJ style. We also auxify the trees as
described previously.
3.2 Results
Table 1 presents gap insertion F measure for John-
son?s (2002) (denoted J), Gabbard?s (2006) (denoted
G), and our (denoted Pres) algorithms on section 23
gold trees, as well as on parses generated by the
Charniak and Bikel parsers. In addition to WHNP
and WHADVP results that are reported in the liter-
ature, we also present results for WHPP gaps even
though there is a small number of them in section
23 (i.e., 22 gaps total). Since there are only 3 non-
empty WHADJP phrases in section 23, we omit
them in our evaluation.
626
Gold Trees Charniak Parser Bikel Parser
Metric J G Pres J G Pres J G Pres
WHNP Johnson 94.8 90.7 97.9 89.8 86.3 91.5 90.2 86.8 92.6
Campbell 94.8 97.0 99.1 81.9 83.8 83.5 80.7 81.5 82.2
Head dep 94.8 97.0 99.1 88.8 90.6 91.0 89.1 91.4 92.3
WHADVP Johnson 75.5 91.4 96.5 61.4 78.0 80.0 61.0 77.9 77.2
Campbell 74.5 89.1 95.0 61.4 71.7 78.4 60.0 71.5 74.8
Head dep 75.5 89.8 95.8 64.4 78.0 84.7 63.0 77.1 80.3
WHPP Johnson 58.1 N/R 72.7 35.7 N/R 55.0 42.9 N/R 53.7
Campbell 51.6 N/R 86.4 28.6 N/R 60.0 35.7 N/R 63.4
Head dep 51.6 N/R 86.4 35.7 N/R 70.0 35.7 N/R 73.2
Table 1: F1 performance on section 23 of WSJ (N/R indicates not reported)
Compared to Johnson?s and Gabbard?s algorithm,
our algorithm significantly reduces the error on
gold trees (table 1). Operating on automatically
parsed trees, our system compares favorably on
all WH traces, using all metrics, except for two
instances: Gabbard?s algorithm has better perfor-
mance on WHNP, using Cambpell?s metric and trees
generated by the Charniak parser by 0.3% and on
WHADVP, using Johnson?s metric and trees pro-
duces by the Bikel parser by 0.7%. However, we
believe that the dependency metric is more appropri-
ate for evaluation on automatically parsed trees be-
cause it enforces the most important aspects of tree
structure for evaluating gap insertion. The relatively
poor performance of Johnson?s and our algorithms
on WHPP gaps compared that on WHADVP gaps
is probably due, at least in part, to the significantly
smaller number of WHPP gaps in the training corpus
and the relatively wider range of possible attachment
sites for the prepositional phrases.
Table 2 displays how well the algorithms trained
on WSJ perform on BN. A large number of the er-
rors are due to FRAGs which are far more com-
mon in the speech corpus than in WSJ. WHPP and
WHADJP, although more rare than the other types,
are presented for reference.
3.3 Error Analysis
It is clear from the contrast between the results based
on gold standard trees and the automatically pro-
duced parses in Table 1 that parse error is a major
source of error. Parse error impacts all of the met-
rics, but the patterns of errors are different. For WH-
NPs, Campbell?s metric is lower than the other two
across all three algorithms, suggesting that this met-
ric is adversely affected by factors that do not im-
pact the other metrics (most likely the span of the
gap?s mother node). For WHADVPs, the metrics
show a similar degradation due to parse error across
the board. We are reluctant to draw conclusions for
the metrics on WHPPs; however, it should be noted
that the position of the PP should be less critical for
evaluating these gaps than their correct attachment,
suggesting that the head dependency metric would
more accurately reflect the performance of the sys-
tem for these gaps.
Campbell?s metric has an interesting property: in
parse trees, we can compute the upper bound on re-
call by simply checking whether the correct WH-
phrase and gap?s mother node exist in the parse tree.
We present recall results and upper bounds in Table
3. Clearly the algorithms are performing close to the
upper bound for WHNPs when we take into account
the impact of parse errors on this metric. Clearly
there is room for improvement for the WHPPs.
Metric J G Pres
WHNP Johnson 88.0 90.3 92.0
Campbell 88.2 94.0 95.3
Head dep 88.3 94.0 95.3
WHADVP Johnson 76.4 92.0 94.3
Campbell 76.3 88.2 92.4
Head dep 76.3 88.5 92.5
WHPP Johnson 56.6 N/R 75.7
Campbell 60.4 N/R 91.9
Head dep 60.4 N/R 91.9
WHADJP Johnson N/R N/R 89.8
Campbell N/R N/R 85.7
Head dep N/R N/R 85.7
Table 2: F1 performance on gold trees of BN
In addition to parser errors, which naturally have
the most profound impact on the performance, we
found the following sources of errors to have impact
on our results:
? Annotation errors and inconsistency in PTB,
which impact not only the training of our system,
but also its evaluation.
627
Charniak Parser J G Pres UB
WHNP 81.9 82.8 83.5 84.0
WHADVP 61.4 71.7 78.4 81.1
WHPP 28.6 N/R 60.0 86.4
Bikel Parser J G Pres UB
WHNP 77.0 80.5 81.5 82.0
WHADVP 47.2 70.1 74.8 78.0
WHPP 22.7 N/R 59.1 81.8
Table 3: Recall on trees produced by the Charniak
and Bikel parsers and their upper bounds (UB)
1. There are some POS labeling errors that con-
fuse our patterns, e.g.,
(SBAR (WHNP-3 (IN that))
(S (NP-SBJ (NNP Canada))
(VP (NNS exports)
(NP (-NONE- *T*-3))
(PP ...))))
2. Some WHADVPs have gaps attached in the
wrong places or do not have gaps at all, e.g.,
(SBAR (WHADVP (WRB when))
(S (NP (PRP he))
(VP (VBD arrived)
(PP (IN at)
(NP ...))
(ADVP (NP (CD two)
(NNS days))
(JJ later)))))
3. PTB annotation guidelines leave it to annota-
tors to decide whether the gap should be at-
tached at the conjunction level or inside its
branches (Bies et al, 1995) leading to incon-
sistency in attachment decisions for adverbial
gaps.
? Lack of coverage: Even though the patterns we
use are very expressive, due to their small number
some rare cases are left uncovered.
? Model errors: Sometimes despite one of the appli-
cable pattern chains proposes the correct gap, the
probabilistic model chooses otherwise. We be-
lieve that a lexicalized model can eliminate most
of these errors.
4 Conclusions and Future Work
The main contribution of this paper is the de-
velopment of a generative probabilistic model for
gap insertion that operates on subtree structures.
Our model achieves state-of-the-art performance,
demonstrating results very close to the upper bound
on WHNP using Campbell?s metric. Performance
for WHADVPs and especially WHPPs, however,
has room for improvement.
We believe that lexicalizing the model by adding
information about lexical heads of the gaps may re-
solve some of the errors. For example:
(SBAR (WHADVP-3 (WRB when))
(S (NP (NNP Congress))
(VP (VBD wanted)
(S (VP (TO to)
(VP (VB know) ...)))
(ADVP (-NONE- *T*-3)))))
(SBAR (WHADVP-1 (WRB when))
(S (NP (PRP it))
(VP (AUX is)
(VP (VBN expected)
(S (VP (TO to)
(VP (VB deliver) ...
(ADVP (-NONE- *T*-1)))))))))
These sentences have very similar structure, with
two potential places to insert gaps (ignoring re-
ordering with siblings). The current model inserts
the gaps as follows: when Congress (VP wanted (S
to know) gap) and when it is (VP expected (S to
deliver) gap), making an error in the second case
(partly due to the bias towards shorter pattern chains,
typical for a PCFG). However, deliver is more likely
to take a temporal modifier than know.
In future work, we will investigate methods for
adding lexical information to our model in order to
improve the performance on WHADVPs and WH-
PPs. In addition, we will investigate methods for
automatically inferring patterns from a treebank cor-
pus to support fast porting of our approach to other
languages with treebanks.
5 Acknowledgements
We would like to thank Ryan Gabbard for provid-
ing us output from his algorithm for evaluation. We
would also like to thank the anonymous reviewers
for invaluable comments. This material is based
upon work supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of DARPA.
References
A. Bies, M. Ferguson, K. Katz, and R. MacIntyre. 1995.
Bracketing guidelines for treebank II style Penn Tree-
bank project. Technical report.
D. M. Bikel. 2004. On the Parameter Space of Gen-
628
erative Lexicalized Statistical Parsing Models. Ph.D.
thesis, University of Pennsylvania.
D. Blaheta. 2003. Function Tagging. Ph.D. thesis,
Brown University.
R. Campbell. 2004. Using linguistic principles to re-
cover empty categories. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics.
M. Collins. 1999. Head-driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
P. Dienes and A. Dubey. 2003. Antecedent recovery:
Experiments with a trace tagger. In Proceedings of
the 2003 Conference on Empirical Methods in Natural
Language Processing.
R. Gabbard, S. Kulick, and M. Marcus. 2006. Fully pars-
ing the Penn Treebank. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
D. Higgins. 2003. A machine-learning approach to the
identification of WH gaps. In Proceedings of the An-
nual Meeting of the European Chapter of the Associa-
tion for Computational Linguistics.
M. Johnson. 2002. A simple pattern-matching algorithm
for recovering empty nodes and their antecedents. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
R. Levy and G Andrew. 2006. Tregex and Tsurgeon:
Tools for querying and manipulating tree data struc-
tures. In Proceedings of LREC.
R. Levy and C. Manning. 2004. Deep dependencies
from context-free statistical parsers: Correcting the
surface dependency approximation. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
W. Wang and M. P. Harper. 2002. The SuperARV lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources in language
modeling. In Proceedings of the Empirical Methods
in Natural Language Processing.
W. Wang, M. P. Harper, and A. Stolcke. 2003. The ro-
bustness of an almost-parsing language model given
errorful training data. In Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing.
629
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
page 1, Dublin, Ireland, August 23-29 2014.
Learning from 26 Languages: Program Management and Science in the
Babel Program
Mary Harper
Incisive Analysis Office
Intelligence Advanced Research Projects Activity
Office of the Director of National Intelligence
USA
mary.harper@iarpa.gov
Invited Speaker Abstract
This presentation will illustrate how program management and science can cooperate to increase our
understanding of human languages and algorithms for processing them. In this presentation, I will use
the IARPA Babel program as an example. The goal of the Babel Program is to rapidly develop speech
recognition capability for keyword search in new languages, working with speech recorded in a variety
of conditions and with limited amounts of transcription. The speech data is recorded in native countries
and contains variability in speaker demographics and recording conditions. The Program will ultimately
address a broad set of languages with a variety of phonotactic, phonological, tonal, morphological, and
syntactic characteristics. I will discuss the data resources collected to support the research, the challenges
that performers have faced when working with a variety of languages collected in realistic environments,
the lessons learned, and future directions.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 12?22,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Self-training with Products of Latent Variable Grammars
Zhongqiang Huang?
?UMIACS
University of Maryland
College Park, MD
zqhuang@umd.edu
Mary Harper??
?HLT Center of Excellence
Johns Hopkins University
Baltimore, MD
mharper@umd.edu
Slav Petrov?
?Google Research
76 Ninth Avenue
New York, NY
slav@google.com
Abstract
We study self-training with products of latent
variable grammars in this paper. We show
that increasing the quality of the automatically
parsed data used for self-training gives higher
accuracy self-trained grammars. Our genera-
tive self-trained grammars reach F scores of
91.6 on the WSJ test set and surpass even
discriminative reranking systems without self-
training. Additionally, we show that multi-
ple self-trained grammars can be combined in
a product model to achieve even higher ac-
curacy. The product model is most effective
when the individual underlying grammars are
most diverse. Combining multiple grammars
that were self-trained on disjoint sets of un-
labeled data results in a final test accuracy of
92.5% on the WSJ test set and 89.6% on our
Broadcast News test set.
1 Introduction
The latent variable approach of Petrov et al (2006)
is capable of learning high accuracy context-free
grammars directly from a raw treebank. It starts
from a coarse treebank grammar (Charniak, 1997),
and uses latent variables to refine the context-free
assumptions encoded in the grammar. A hierarchi-
cal split-and-merge algorithm introduces grammar
complexity gradually, iteratively splitting (and po-
tentially merging back) each observed treebank cat-
egory into a number of increasingly refined latent
subcategories. The Expectation Maximization (EM)
algorithm is used to train the model, guaranteeing
that each EM iteration will increase the training like-
lihood. However, because the latent variable gram-
mars are not explicitly regularized, EM keeps fit-
ting the training data and eventually begins over-
fitting (Liang et al, 2007). Moreover, EM is a lo-
cal method, making no promises regarding the final
point of convergence when initialized from different
random seeds. Recently, Petrov (2010) showed that
substantial differences between the learned gram-
mars remain, even if the hierarchical splitting re-
duces the variance across independent runs of EM.
In order to counteract the overfitting behavior,
Petrov et al (2006) introduced a linear smoothing
procedure that allows training grammars for 6 split-
merge (SM) rounds without overfitting. The in-
creased expressiveness of the model, combined with
the more robust parameter estimates provided by the
smoothing, results in a nice increase in parsing ac-
curacy on a held-out set. However, as reported by
Petrov (2009) and Huang and Harper (2009), an ad-
ditional 7th SM round actually hurts performance.
Huang and Harper (2009) addressed the issue of
data sparsity and overfitting from a different angle.
They showed that self-training latent variable gram-
mars on their own output can mitigate data spar-
sity issues and improve parsing accuracy. Because
the capacity of the model can grow with the size
of the training data, latent variable grammars are
able to benefit from the additional training data, even
though it is not perfectly labeled. Consequently,
they also found that a 7th round of SM training was
beneficial in the presence of large amounts of train-
ing data. However, variation still remains in their
self-trained grammars and they had to use a held-out
set for model selection.
The observation of variation is not surprising;
EM?s tendency to get stuck in local maxima has been
studied extensively in the literature, resulting in vari-
ous proposals for model selection methods (e.g., see
12
Burnham and Anderson (2002)). What is perhaps
more surprising is that the different latent variable
grammars seem to capture complementary aspects
of the data. Petrov (2010) showed that a simple ran-
domization scheme produces widely varying gram-
mars. Quite serendipitously, these grammars can
be combined into an unweighted product model that
substantially outperforms the individual grammars.
In this paper, we combine the ideas of self-
training and product models and show that both
techniques provide complementary effects. We hy-
pothesize that the main factors contributing to the
final accuracy of the product model of self-trained
grammars are (i) the accuracy of the grammar used
to parse the unlabeled data for retraining (single
grammar versus product of grammars) and (ii) the
diversity of the grammars that are being combined
(self-trained grammars trained using the same auto-
matically labeled subset or different subsets). We
conduct a series of analyses to develop an under-
standing of these factors, and conclude that both di-
mensions are important for obtaining significant im-
provements over the standard product models.
2 Experimental Setup
2.1 Data
We conducted experiments in two genres: newswire
text and broadcast news transcripts. For the
newswire studies, we used the standard setup (sec-
tions 02-21 for training, 22 for development, and 23
for final test) of the WSJ Penn Treebank (Marcus et
al., 1999) for supervised training. The BLLIP cor-
pus (Charniak et al, 2000) was used as a source of
unlabeled data for self-training the WSJ grammars.
We ignored the parse trees contained in the BLLIP
corpus and retained only the sentences, which are
already segmented and tokenized for parsing (e.g.,
contractions are split into two tokens and punctua-
tion is separated from the words). We partitioned
the 1,769,055 BLLIP sentences into 10 equally sized
subsets1.
For broadcast news (BN), we utilized the Broad-
1We corrected some of the most egregious sentence segmen-
tation problems in this corpus, and so the number of sentences is
different than if one simply pulled the fringe of the trees. It was
not uncommon for a sentence split to occur on abbreviations,
such as Adm.
cast News treebank from Ontonotes (Weischedel et
al., 2008) together with the WSJ Penn Treebank for
supervised training, because their combination re-
sults in better parser models compared to using the
limited-sized BN corpus alone (86.7 F vs. 85.2 F).
The files in the Broadcast News treebank represent
news stories collected during different time periods
with a diversity of topics. In order to obtain a rep-
resentative split of train-test-development sets, we
divided them into blocks of 10 files sorted by alpha-
betical filename order. We used the first file in each
block for development, the second for test, and the
remaining files for training. This training set was
then combined with the entire WSJ treebank. We
also used 10 equally sized subsets from the Hub4
CSR 1996 utterances (Garofolo et al, 1996) for self-
training. The Hub 4 transcripts are markedly noisier
than the BLLIP corpus is, in part because it is harder
to sentence segment, but also because it was pro-
duced by human transcription of spoken language.
The treebanks were pre-processed differently for
the two genres. For newswire, we used a slightly
modified version of the WSJ treebank: empty
nodes and function labels were deleted and auxiliary
verbs were replaced with AUXB, AUXG, AUXZ,
AUXD, or AUXN to represent infinitive, progres-
sive, present, past, or past participle auxiliaries2.
The targeted use of the broadcast models is for pars-
ing broadcast news transcripts for language mod-
els in speech recognition systems. Therefore, in
addition to applying the transformations used for
newswire, we also replaced symbolic expressions
with verbal forms (e.g., $5 was replaced with five
dollars) and removed punctuation and case. The
Hub4 data was segmented into utterances, punctua-
tion was removed, words were down-cased, and con-
tractions were tokenized for parsing. Table 1 sum-
marizes the data set sizes used in our experiments,
together with average sentence length and standard
deviation.
2.2 Scoring
Parses from all models are compared with respective
gold standard parses using SParseval bracket scor-
ing (Roark et al, 2006). This scoring tool pro-
2Parsing accuracy is marginally affected. The average over
10 SM6 grammars with the transformation is 90.5 compared to
90.4 F without it, a 0.1% average improvement.
13
Genre Statistics Train Dev Test Unlabeled
Newswire
# sentences 39.8k 1.7k 2.4k 1,769.1k
# words 950.0k 40.1k 56.7k 43,057.0k
length Avg./Std. 28.9/11.2 25.1/11.8 25.1/12.0 24.3/10.9
Broadcast News
# sentences 59.0k 1.0k 1.1k 4,386.5k
# words 1,281.1k 17.1k 19.4k 77,687.9k
length Avg./Std. 17.3/11.3 17.4/11.3 17.7/11.4 17.7/12.8
Table 1: The number of words and sentences, together with average (Avg.) sentence length and its standard deviation
(Std.), for the data sets used in our experiments.
duces scores that are identical to those produced
by EVALB for WSJ. For Broadcast News, SParse-
val applies Charniak and Johnson?s (Charniak and
Johnson, 2001) scoring method for EDITED nodes3.
Using this method, BN scores were slightly (.05-.1)
lower than if EDITED constituents were treated like
any other, as in EVALB.
2.3 Latent Variable Grammars
We use the latent variable grammar (Matsuzaki et
al., 2005; Petrov et al, 2006) implementation of
Huang and Harper (2009) in this work. Latent vari-
able grammars augment the observed parse trees in
the treebank with a latent variable at each tree node.
This effectively splits each observed category into
a set of latent subcategories. An EM-algorithm is
used to fit the model by maximizing the joint like-
lihood of parse trees and sentences. To allocate the
grammar complexity only where needed, a simple
split-and-merge procedure is applied. In every split-
merge (SM) round, each latent variable is first split
in two and the model is re-estimated. A likelihood
criterion is used to merge back the least useful splits
(50% merge rate for these experiments). This itera-
tive refinement proceeds for 7 rounds, at which point
parsing performance on a held-out set levels off and
training becomes prohibitively slow.
Since EM is a local method, different initial-
izations will result in different grammars. In
fact, Petrov (2010) recently showed that this EM-
algorithm is very unstable and converges to widely
varying local maxima. These local maxima corre-
3Non-terminal subconstituents of EDITED nodes are re-
moved so that the terminal constituents become immediate chil-
dren of a single EDITED node, adjacent EDITED nodes are
merged, and they are ignored for span calculations of the other
constituents.
spond to different high quality latent variable gram-
mars that have captured different types of patterns in
the data. Because the individual models? mistakes
are independent to some extent, multiple grammars
can be effectively combined into an unweighted
product model of much higher accuracy. We build
upon this line of work and investigate methods to
exploit products of latent variable grammars in the
context of self-training.
3 Self-training Methodology
Different types of parser self-training have been pro-
posed in the literature over the years. All of them
involve parsing a set of unlabeled sentences with a
baseline parser and then estimating a new parser by
combining this automatically parsed data with the
original training data. McClosky et al (2006) pre-
sented a very effective method for self-training a
two-stage parsing system consisting of a first-stage
generative lexicalized parser and a second-stage dis-
criminative reranker. In their approach, a large
amount of unlabeled text is parsed by the two-stage
system and the parameters of the first-stage lexical-
ized parser are then re-estimated taking the counts
from the automatically parsed data into considera-
tion.
More recently Huang and Harper (2009) pre-
sented a self-training procedure based on an EM-
algorithm. They showed that the EM-algorithm that
is typically used to fit a latent variable grammar
(Matsuzaki et al, 2005; Petrov et al, 2006) to a tree-
bank can also be used for self-training on automati-
cally parsed sentences. In this paper, we investigate
self-training with products of latent variable gram-
mars. We consider three training scenarios:
ST-Reg Training Use the best single grammar to
14
Regular Best Average Product
SM6 90.8 90.5 92.0
SM7 90.4 90.1 92.2
Table 2: Performance of the regular grammars and their
products on the WSJ development set.
parse a single subset of the unlabeled data and
train 10 self-trained grammars using this single
set.
ST-Prod Training Use the product model to parse
a single subset of the unlabeled data and train
10 self-trained grammars using this single set.
ST-Prod-Mult Training Use the product model to
parse all 10 subsets of the unlabeled data and
train 10 self-trained grammars, each using a
different subset.
The resulting grammars can be either used individu-
ally or combined in a product model.
These three conditions provide different insights.
The first experiment allows us to investigate the
effectiveness of product models for standard self-
trained grammars. The second experiment enables
us to quantify how important the accuracy of the
baseline parser is for self-training. Finally, the third
experiment investigates a method for injecting some
additional diversity into the individual grammars to
determine whether a product model is most success-
ful when there is more variance among the individ-
ual models.
Our initial experiments and analysis will focus on
the development set of WSJ. We will then follow
up with an analysis of broadcast news (BN) to de-
termine whether the findings generalize to a second,
less structured type of data. It is important to con-
struct grammars capable of parsing this type of data
accurately and consistently in order to support struc-
tured language modeling (e.g., (Wang and Harper,
2002; Filimonov and Harper, 2009)).
4 Newswire Experiments
In this section, we compare single grammars and
their products that are trained in the standard way
with gold WSJ training data, as well as the three
self-training scenarios discussed in Section 3. We
ST-Reg Best Average Product
SM6 91.5 91.2 92.0
SM7 91.6 91.5 92.4
Table 3: Performance of the ST-Reg grammars and their
products on the WSJ development set.
report the F scores of both SM6 and SM7 grammars
on the development set in order to evaluate the ef-
fect of model complexity on the performance of the
self-trained and product models. Note that we use
6th round grammars to produce the automatic parse
trees for the self-training experiments. Parsing with
the product of the 7th round grammars is slow and
requires a large amount of memory (32GB). Since
we had limited access to such machines, it was in-
feasible for us to parse all of the unlabeled data with
the SM7 product grammars.
4.1 Regular Training
We begin by training ten latent variable models ini-
tialized with different random seeds using the gold
WSJ training set. Results are presented in Table 2.
The best F score attained by the individual SM6
grammars on the development set is 90.8, with an
average score of 90.5. The product of grammars
achieves a significantly improved accuracy at 92.04.
Notice that the individual SM7 grammars perform
worse on average (90.1 vs. 90.5) due to overfitting,
but their product achieves higher accuracy than the
product of the SM6 grammars (92.2 vs. 92.0). We
will further investigate the causes for this effect in
Section 5.
4.2 ST-Reg Training
Given the ten SM6 grammars from the previous sub-
section, we can investigate the three self-training
methods. In the first regime (ST-Reg), we use the
best single grammar (90.8 F) to parse a single subset
of the BLLIP data. We then train ten grammars from
different random seeds, using an equally weighted
combination of the WSJ training set with this sin-
gle set. These self-trained grammars are then com-
bined into a product model. As reported in Table 3,
4We use Dan Bikel?s randomized parsing evaluation com-
parator to determine the significance (p < 0.05) of the differ-
ence between two parsers? outputs.
15
ST-Prod Best Average Product
SM6 91.7 91.4 92.2
SM7 91.9 91.7 92.4
Table 4: Performance of the ST-Prod grammars and their
products on the WSJ development set.
thanks to the use of additional automatically labeled
training data, the individual SM6 ST-Reg grammars
perform significantly better than the individual SM6
grammars (91.2 vs. 90.5 on average), and the indi-
vidual SM7 ST-Reg grammars perform even better,
achieving a high F score of 91.5 on average.
The product of ST-Reg grammars achieves signif-
icantly better performance over the individual gram-
mars, however, the improvement is much smaller
than that obtained by the product of regular gram-
mars. In fact, the product of ST-Reg grammars per-
forms quite similarly to the product of regular gram-
mars despite the higher average accuracy of the in-
dividual grammars. This may be caused by the fact
that self-training on the same data tends to reduce
the variation among the self-trained grammars. We
will show in Section 5 that the diversity among the
individual grammars is as important as average ac-
curacy for the performance attained by the product
model.
4.3 ST-Prod Training
Since products of latent variable grammars perform
significantly better than individual latent variable
grammars, it is natural to try using the product
model for parsing the unlabeled data. To investi-
gate whether the higher accuracy of the automati-
cally labeled data translates into a higher accuracy
of the self-trained grammars, we used the product of
6th round grammars to parse the same subset of the
unlabeled data as in the previous experiment. We
then trained ten self-trained grammars, which we
call ST-Prod grammars. As can be seen in Table 4,
using the product of the regular grammars for label-
ing the self-training data results in improved individ-
ual ST-Prod grammars when compared with the ST-
Reg grammars, with 0.2 and 0.3 improvements for
the best SM6 and SM7 grammars, respectively. In-
terestingly, the best individual SM7 ST-Prod gram-
mar (91.9 F) performs comparably to the product of
ST-Prod-Mult Best Average Product
SM6 91.7 91.4 92.5
SM7 91.8 91.7 92.8
Table 5: Performance of the ST-Prod-Mult grammars and
their products on the WSJ development set.
the regular grammars (92.0 F) that was used to label
the BLLIP subset used for self-training. This is very
useful for practical reasons because a single gram-
mar is faster to parse with and requires less memory
than the product model.
The product of the SM6 ST-Prod grammars also
achieves a 0.2 higher F score compared to the prod-
uct of the SM6 ST-Reg grammars, but the product
of the SM7 ST-Prod grammars has the same perfor-
mance as the product of the SM7 ST-Reg grammars.
This could be due to the fact that the ST-Prod gram-
mars are no more diverse than the ST-Reg grammars,
as we will show in Section 5.
4.4 ST-Prod-Mult Training
When creating a product model of regular gram-
mars, Petrov (2010) used a different random seed for
each model and conjectured that the effectiveness of
the product grammars stems from the resulting di-
versity of the individual grammars. Two ways to
systematically introduce bias into individual mod-
els are to either modify the feature sets (Baldridge
and Osborne, 2008; Smith and Osborne, 2007) or
to change the training distributions of the individual
models (Breiman, 1996). Petrov (2010) attempted to
use the second method to train individual grammars
on either disjoint or overlapping subsets of the tree-
bank, but observed a performance drop in individ-
ual grammars resulting from training on less data,
as well as in the performance of the product model.
Rather than reducing the amount of gold training
data (or having treebank experts annotate more data
to support the diversity), we employ the self-training
paradigm to train models using a combination of the
same gold training data with different sets of the
self-labeled training data. This approach also allows
us to utilize a much larger amount of low-cost self-
labeled data than can be used to train one model by
partitioning the data into ten subsets and then train-
ing ten models with a different subset. Hence, in
16
-3
-2
-1
0
1
2
3
Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJP
D
i
f
f
e
r
e
n
c
e
 
i
n
 
F
G0
G1
G2
G3
G4
G5
G6
G7
G8
G9
(a) Difference in F score between the product and the individual SM6 regular grammars.
-3
-2
-1
0
1
2
3
Total VP QP NP SBAR PP ADVP_PRT S WHNP ADJP
D
i
f
f
e
r
e
n
c
e
 
i
n
 
F
G0
G1
G2
G3
G4
G5
G6
G7
G8
G9
(b) Difference in F score between the product of SM6 regular grammars and the individual SM7 ST-Prod-Mult
grammars.
Figure 1: Difference in F scores between various individual grammars and representative product grammars.
the third self-training experiment, we use the prod-
uct of the regular grammars to parse all ten subsets
of the unlabeled data and train ten grammars, which
we call ST-Prod-Mult grammars, each using a dif-
ferent subset.
As shown in Table 5, the individual ST-Prod-Mult
grammars perform similarly to the individual ST-
Prod grammars. However, the product of the ST-
Prod-Mult grammars achieves significantly higher
accuracies than the product of the ST-Prod gram-
mars, with 0.3 and 0.4 improvements for SM6 and
SM7 grammars, respectively, suggesting that the use
of multiple self-training subsets plays an important
role in model combination.
5 Analysis
We conducted a series of analyses to develop an un-
derstanding of the factors affecting the effectiveness
of combining self-training with product models.
5.1 What Has Improved?
Figure 1(a) depicts the difference between the prod-
uct and the individual SM6 regular grammars on
overall F score, as well as individual constituent F
scores. As can be observed, there are significant
variations among the individual grammars, and the
product of the regular grammars improves almost all
categories, with a few exceptions (some individual
grammars do better on QP and WHNP constituents).
Figure 1(b) shows the difference between the
product of the SM6 regular grammars and the indi-
vidual SM7 ST-Prod-Mult grammars. Self-training
dramatically improves the quality of single gram-
mars. In most of the categories, some individ-
ual ST-Prod-Mult grammars perform comparably or
slightly better than the product of SM6 regular gram-
mars used to automatically label the unlabeled train-
ing set.
5.2 Overfitting vs. Smoothing
Figure 2(a) and 2(b) depict the learning curves of
the regular and the ST-Prod-Mult grammars. As
more latent variables are introduced through the iter-
ative SM training algorithm, the modeling capacity
of the grammars increases, leading to improved per-
formance. However, the performance of the regular
grammars drops after 6 SM rounds, as also previ-
ously observed in (Huang and Harper, 2009; Petrov,
2009), suggesting that the regular SM7 grammars
have overfit the relatively small-sized gold training
17
data. In contrast, the performance of the self-trained
grammars continues to improve in the 7th SM round.
Huang and Harper (2009) argued that the additional
self-labeled training data adds a smoothing effect to
the grammars, supporting an increase in model com-
plexity without overfitting.
Although the performance of the individual gram-
mars, both regular and self-trained, varies signif-
icantly and the product model consistently helps,
there is a non-negligible difference between the im-
provement achieved by the two product models over
their component grammars. The regular product
model improves upon its individual grammars more
than the ST-Prod-Mult product does in the later SM
rounds, as illustrated by the relative error reduction
curves in figures 2(a) and (b). In particular, the prod-
uct of the SM7 regular grammars gains a remarkable
2.1% absolute improvement over the average perfor-
mance of the individual regular SM7 grammars and
0.2% absolute over the product of the regular SM6
grammars, despite the fact that the individual regular
SM7 grammars perform worse than the SM6 gram-
mars. This suggests that the product model is able
to effectively exploit less smooth, overfit grammars.
We will examine this issue further in the next sub-
section.
5.3 Diversity
From the perspective of Products of Experts (Hin-
ton, 1999) or Logarithmic Opinion Pools (Smith et
al., 2005), each individual expert learns complemen-
tary aspects of the training data and the veto power
of product models enforces that the joint prediction
of their product has to be licensed by all individual
experts. One possible explanation of the observa-
tion in the previous subsection is that with the ad-
dition of more latent variables, the individual gram-
mars become more deeply specialized on certain as-
pects of the training data. This specialization leads
to greater diversity in their prediction preferences,
especially in the presence of a small training set.
On the other hand, the self-labeled training set size
is much larger, and so the specialization process is
therefore slowed down.
Petrov (2010) showed that the individually
learned grammars are indeed very diverse by look-
ing at the distribution of latent annotations across the
treebank categories, as well as the variation in over-
all and individual category F scores (see Figure 1).
However, these measures do not directly relate to the
diversity of the prediction preferences of the gram-
mars, as we observed similar patterns in the regular
and self-trained models.
Given a sentence s and a set of grammars G =
{G1, ? ? ? , Gn}, recall that the decoding algorithm of
the product model (Petrov, 2010) searches for the
best tree T such that the following objective function
is maximized:
?
r?T
?
G?G
log p(r|s,G)
where log p(r|s,G) is the log posterior probability
of rule r given sentence s and grammar G. The
power of the product model comes directly from the
diversity in log p(r|s,G) among individual gram-
mars. If there is little diversity, the individual
grammars would make similar predictions and there
would be little or no benefit from using a product
model. We use the average empirical variance of
the log posterior probabilities of the rules among the
learned grammars over a held-out set S as a proxy
of the diversity among the grammars:
?
s?S
?
G?G
?
r?R(G,s)
p(r|s,G)VAR(log(p(r|s,G)))
?
s?S
?
G?G
?
r?R(G,s)
p(r|s,G)
where R(G, s) represents the set of rules extracted
from the chart when parsing sentence s with gram-
mar G, and VAR(log(p(r|s,G))) is the variance of
log(p(r|s,G)) among all grammars G ? G.
Note that the average empirical variance is only
an approximation of the diversity among grammars.
In particular, this measure tends to be biased to pro-
duce larger numbers when the posterior probabili-
ties of rules tend to be small, because small differ-
ences in probability produce large changes in the log
scale. This happens for coarser grammars produced
in early SM stages when there is more uncertainty
about what rules to apply, with the rules remaining
in the parsing chart having low probabilities overall.
As shown in Figure 2(c), the average variances
all start at a high value and then drop, probably due
to the aforementioned bias. However, as the SM
iteration continues, the average variances increase
despite the bias. More interestingly, the variance
18
83
85
87
89
91
93
2 3 4 5 6 7
5%
9%
13%
17%
21%
25%
Regular Grammars
F
(a) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
83
85
87
89
91
93
2 3 4 5 6 7
5%
9%
13%
17%
21%
25%
ST-Prod-Mult Grammars
F
(b) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
Product Mean Error Reduction
0.1
0.2
0.3
0.4
0.5
2 3 4 5 6 7
Test
A
v
e
r
a
g
e
 
V
a
r
i
a
n
c
e
(c) SM Rounds
Regular
ST-Prod-Mult
ST-Prod
ST-Reg
Figure 2: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, with
minimum and maximum values indicated by bars) and their products before and after self-training on the WSJ de-
velopment set. The relative error reductions of the products are also reported. (c) The measured average empirical
variance among the grammars trained on WSJ.
among the regular grammars grows at a much faster
speed and is consistently greater when compared to
the self-trained grammars. This suggests that there
is more diversity among the regular grammars than
among the self-trained grammars, and explains the
greater improvement obtained by the regular product
model. It is also important to note that there is more
variance among the ST-Prod-Mult grammars, which
were trained on disjoint self-labeled training data,
and a greater improvement in their product model
relative to the ST-Reg and ST-Prod grammars, fur-
ther supporting the diversity hypothesis. Last but not
the least, the trend seems to indicate that the vari-
ance of the self-trained grammars would continue
increasing if EM training was extended by a few
more SM rounds, potentially resulting in even bet-
ter product models. It is currently impractical to test
this due to the dramatic increase in computational
requirements for an SM8 product model, and so we
leave it for future work.
5.4 Generalization to Broadcast News
We conducted the same set of experiments on the
broadcast news data set. While the development set
results in Table 6 show similar trends to the WSJ
results, the benefits from the combination of self-
training and product models appear even more pro-
nounced here. The best single ST-Prod-Mult gram-
mar (89.2 F) alone is able to outperform the product
of SM7 regular grammars (88.9 F), and their prod-
uct achieves another 0.7 absolute improvement, re-
sulting in a significantly better accuracy at 89.9 F.
Model Rounds Best Product
Regular
SM6 87.1 88.6
SM7 87.1 88.9
ST-Prod
SM6 88.5 89.0
SM7 89.0 89.6
ST-Prod-Mult
SM6 88.8 89.5
SM7 89.2 89.9
Table 6: F-score for various models on the BN develop-
ment set.
Figure 3 shows again that the benefits of self-
training and product models are complementary and
can be stacked. As can be observed, the self-
trained grammars have increasing F scores as the
split-merge rounds increase, while the regular gram-
mars have a slight decrease in F score after round 6.
In contrast to the newswire models, it appears that
the individual ST-Prod-Mult grammars trained on
broadcast news always perform comparably to the
product of the regular grammars at all SM rounds,
including the product of SM7 regular grammars.
This is noteworthy, given that the ST-Prod-Mult
grammars are trained on the output of the worse per-
forming product of the SM6 regular grammars. One
19
79
81
83
85
87
89
91
2 3 4 5 6 7
3%
6%
9%
12%
15%
Regular Gramamrs
F
(a) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
79
81
83
85
87
89
91
2 3 4 5 6 7
3%
6%
9%
12%
15%
ST-Prod-Mult Gramamrs
F
(b) SM Rounds
R
e
l
a
t
i
v
e
 
E
r
r
o
r
 
R
e
d
u
c
t
i
o
n
Product Mean Error Reduction
0.1
0.2
0.3
0.4
0.5
2 3 4 5 6 7
Test
A
v
e
r
a
g
e
 
V
a
r
i
a
n
c
e
(c) SM Rounds
Regular
ST-Prod-Mult
ST-Prod
Figure 3: Learning curves of the individual regular (a) and ST-Prod-Mult (b) grammars (average performance, with
minimum and maximum values indicated by bars) and their products before and after self-training on the BN develop-
ment set. The relative error reductions of the products are also reported. (c) The measured average empirical variance
among the grammars trained on BN.
possible explanation is that we used more unlabeled
data for self-training the broadcast news grammars
than for the newswire grammars. The product of the
ST-Prod-Mult grammars provides further and signif-
icant improvement in F score.
6 Final Results
We evaluated the best single self-trained gram-
mar (SM7 ST-Prod), as well as the product of
the SM7 ST-Prod-Mult grammars on the WSJ test
set. Table 7 compares these two grammars to
a large body of related work grouped into sin-
gle parsers (SINGLE), discriminative reranking ap-
proaches (RE), self-training (SELF), and system
combinations (COMBO).
Our best single grammar achieves an accuracy
that is only slightly worse (91.6 vs. 91.8 in F score)
than the product model in Petrov (2010). This is
made possible by self-training on the output of a
high quality product model. The higher quality of
the automatically parsed data results in a 0.3 point
higher final F score (91.6 vs. 91.3) over the self-
training results in Huang and Harper (2009), which
used a single grammar for parsing the unlabeled
data. The product of the self-trained ST-Prod-Mult
grammars achieves significantly higher accuracies
with an F score of 92.5, a 0.7 improvement over the
product model in Petrov (2010).
8Our ST-Reg grammars are trained in the same way as in
Type Parser LP LR EX
S
IN
G
L
E Charniak (2000) 89.9 89.5 37.2
Petrov and Klein (2007) 90.2 90.1 36.7
Carreras et al (2008) 91.4 90.7 -
R
E Charniak and Johnson (2005) 91.8 91.2 44.8
Huang (2008) 92.2 91.2 43.5
S
E
L
F Huang and Harper (2009)8 91.6 91.1 40.4
McClosky et al (2006) 92.5 92.1 45.3
C
O
M
B
O Petrov (2010) 92.0 91.7 41.9
Sagae and Lavie (2006) 93.2 91.0 -
Fossum and Knight (2009) 93.2 91.7 -
Zhang et al (2009) 93.3 92.0 -
This Paper
Best Single 91.8 91.4 40.3
Best Product 92.7 92.2 43.1
Table 7: Final test set accuracies on WSJ.
Although our models are based on purely gen-
erative PCFG grammars, our best product model
performs competitively to the self-trained two-step
discriminative reranking parser of McClosky et al
(2006), which makes use of many non-local rerank-
ing features. Our parser also performs comparably
to other system combination approaches (Sagae and
Lavie, 2006; Fossum and Knight, 2009; Zhang et
al., 2009) with higher recall and lower precision,
Huang and Harper (2009) except that we keep all unary rules.
The reported numbers are from the best single ST-Reg grammar
in this work.
20
but again without using a discriminative reranking
step. We expect that replacing the first-step genera-
tive parsing model in McClosky et al (2006) with a
product of latent variable grammars would give even
higher parsing accuracies.
On the Broadcast News test set, our best perform-
ing single and product grammars (bolded in Table 6)
obtained F scores of 88.7 and 89.6, respectively.
While there is no prior work using our setup, we ex-
pect these numbers to set a high baseline.
7 Conclusions and Future Work
We evaluated methods for self-training high accu-
racy products of latent variable grammars with large
amounts of genre-matched data. We demonstrated
empirically on newswire and broadcast news genres
that very high accuracies can be achieved by training
grammars on disjoint sets of automatically labeled
data. Two primary factors appear to be determin-
ing the efficacy of our self-training approach. First,
the accuracy of the model used for parsing the unla-
beled data is important for the accuracy of the result-
ing single self-trained grammars. Second, the diver-
sity of the individual grammars controls the gains
that can be obtained by combining multiple gram-
mars into a product model. Our most accurate sin-
gle grammar achieves an F score of 91.6 on the WSJ
test set, rivaling discriminative reranking approaches
(Charniak and Johnson, 2005) and products of latent
variable grammars (Petrov, 2010), despite being a
single generative PCFG. Our most accurate product
model achieves an F score of 92.5 without the use of
discriminative reranking and comes close to the best
known numbers on this test set (Zhang et al, 2009).
In future work, we plan to investigate additional
methods for increasing the diversity of our self-
trained models. One possibility would be to utilize
more unlabeled data or to identify additional ways to
bias the models. It would also be interesting to deter-
mine whether further increasing the accuracy of the
model used for automatically labeling the unlabeled
data can enhance performance even more. A simple
but computationally expensive way to do this would
be to parse the data with an SM7 product model.
Finally, for this work, we always used products
of 10 grammars, but we sometimes observed that
subsets of these grammars produce even better re-
sults on the development set. Finding a way to se-
lect grammars from a grammar pool to achieve high
performance products is an interesting area of future
study.
8 Acknowledgments
This research was supported in part by NSF IIS-
0703859. Opinions, findings, and recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the views of the funding
agency or the institutions where the work was com-
pleted.
References
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for HPSG parse se-
lection. Natural Language Engineering.
Leo Breiman. 1996. Bagging predictors. Machine
Learning.
Kenneth P. Burnham and David R. Anderson. 2002.
Model Selection and Multimodel Inference: A Prac-
tical Information-Theoretic Approach. New York:
Springer-Verlag.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
Tag, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In CoNLL, pages 9?16.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In NAACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson, 2000. BLLIP 1987-89
WSJ Corpus Release 1. Linguistic Data Consortium,
Philadelphia.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In ICAI.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ACL.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP, pages 1114?1123, Singapore, August.
Victoria Fossum and Kevin Knight. 2009. Combining
constituent parsers. In NAACL, pages 253?256.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Geoffrey E. Hinton. 1999. Products of experts. In
ICANN.
21
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
Dirichlet processes. In EMNLP.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor, 1999. Treebank-3.
Linguistic Data Consortium, Philadelphia.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In HLT-
NAACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley.
Slav Petrov. 2010. Products of random latent variable
grammars. In HLT-NAACL.
Brian Roark, Mary Harper, Yang Liu, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie J. Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. SParseval: Evaluation metrics for pars-
ing speech. In LREC.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In NAACL, pages 129?132.
Andrew Smith and Miles Osborne. 2007. Diversity
in logarithmic opinion pools. Lingvisticae Investiga-
tiones.
Andrew Smith, Trevor Cohn, and Miles Osborne. 2005.
Logarithmic opinion pools for conditional random
fields. In ACL.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources. In EMNLP,
pages 238?247, Philadelphia, July.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In EMNLP, pages 1552?1560.
22
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 821?831,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Lessons Learned in Part-of-Speech Tagging of Conversational Speech
Vladimir Eidelman?, Zhongqiang Huang?, and Mary Harper??
?Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park, MD
?Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD
{vlad,zhuang,mharper}@umiacs.umd.edu
Abstract
This paper examines tagging models for spon-
taneous English speech transcripts. We ana-
lyze the performance of state-of-the-art tag-
ging models, either generative or discrimi-
native, left-to-right or bidirectional, with or
without latent annotations, together with the
use of ToBI break indexes and several meth-
ods for segmenting the speech transcripts (i.e.,
conversation side, speaker turn, or human-
annotated sentence). Based on these studies,
we observe that: (1) bidirectional models tend
to achieve better accuracy levels than left-to-
right models, (2) generative models seem to
perform somewhat better than discriminative
models on this task, and (3) prosody improves
tagging performance of models on conversa-
tion sides, but has much less impact on smaller
segments. We conclude that, although the use
of break indexes can indeed significantly im-
prove performance over baseline models with-
out them on conversation sides, tagging ac-
curacy improves more by using smaller seg-
ments, for which the impact of the break in-
dexes is marginal.
1 Introduction
Natural language processing technologies, such as
parsing and tagging, often require reconfiguration
when they are applied to challenging domains that
differ significantly from newswire, e.g., blogs, twit-
ter text (Foster, 2010), or speech. In contrast to
text, conversational speech represents a significant
challenge because the transcripts are not segmented
into sentences. Furthermore, the transcripts are of-
ten disfluent and lack punctuation and case informa-
tion. On the other hand, speech provides additional
information, beyond simply the sequence of words,
which could be exploited to more accurately assign
each word in the transcript a part-of-speech (POS)
tag. One potentially beneficial type of information
is prosody (Cutler et al, 1997).
Prosody provides cues for lexical disambigua-
tion, sentence segmentation and classification,
phrase structure and attachment, discourse struc-
ture, speaker affect, etc. Prosody has been found
to play an important role in speech synthesis sys-
tems (Batliner et al, 2001; Taylor and Black, 1998),
as well as in speech recognition (Gallwitz et al,
2002; Hasegawa-Johnson et al, 2005; Ostendorf et
al., 2003). Additionally, prosodic features such as
pause length, duration of words and phones, pitch
contours, energy contours, and their normalized val-
ues have been used for speech processing tasks like
sentence boundary detection (Liu et al, 2005).
Linguistic encoding schemes like ToBI (Silver-
man et al, 1992) have also been used for sentence
boundary detection (Roark et al, 2006; Harper et al,
2005), as well as for parsing (Dreyer and Shafran,
2007; Gregory et al, 2004; Kahn et al, 2005). In
the ToBI scheme, aspects of prosody such as tone,
prominence, and degree of juncture between words
are represented symbolically. For instance, Dreyer
and Shafran (2007) use three classes of automati-
cally detected ToBI break indexes, indicating major
intonational breaks with a 4, hesitation with a p, and
all other breaks with a 1.
Recently, Huang and Harper (2010) found that
they could effectively integrate prosodic informa-
821
tion in the form of this simplified three class ToBI
encoding when parsing spontaneous speech by us-
ing a prosodically enriched PCFG model with latent
annotations (PCFG-LA) (Matsuzaki et al, 2005;
Petrov and Klein, 2007) to rescore n-best parses
produced by a baseline PCFG-LA model without
prosodic enrichment. However, the prosodically en-
riched models by themselves did not perform sig-
nificantly better than the baseline PCFG-LA model
without enrichment, due to the negative effect that
misalignments between automatic prosodic breaks
and true phrase boundaries have on the model.
This paper investigates methods for using state-
of-the-art taggers on conversational speech tran-
scriptions and the effect that prosody has on tagging
accuracy. Improving POS tagging performance of
speech transcriptions has implications for improving
downstream applications that rely on accurate POS
tags, including sentence boundary detection (Liu
et al, 2005), automatic punctuation (Hillard et al,
2006), information extraction from speech, parsing,
and syntactic language modeling (Heeman, 1999;
Filimonov and Harper, 2009). While there have
been several attempts to integrate prosodic informa-
tion to improve parse accuracy of speech transcripts,
to the best of our knowledge there has been little
work on using this type of information for POS tag-
ging. Furthermore, most of the parsing work has
involved generative models and rescoring/reranking
of hypotheses from the generative models. In this
work, we will analyze several factors related to ef-
fective POS tagging of conversational speech:
? discriminative versus generative POS tagging
models (Section 2)
? prosodic features in the form of simplified ToBI
break indexes (Section 4)
? type of speech segmentation (Section 5)
2 Models
In order to fully evaluate the difficulties inherent in
tagging conversational speech, as well as the possi-
ble benefits of prosodic information, we conducted
experiments with six different POS tagging mod-
els. The models can be broadly separated into two
classes: generative and discriminative. As the first
of our generative models, we used a Hidden Markov
Model (HMM) trigram tagger (Thede and Harper,
1999), which serves to establish a baseline and to
gauge the difficulty of the task at hand. Our sec-
ond model, HMM-LA, was the latent variable bi-
gram HMM tagger of Huang et al (2009), which
achieved state-of-the-art tagging performance by in-
troducing latent tags to weaken the stringent Markov
independence assumptions that generally hinder tag-
ging performance in generative models.
For the third model, we implemented a bidirec-
tional variant of the HMM-LA (HMM-LA-Bidir)
that combines evidence from two HMM-LA tag-
gers, one trained left-to-right and the other right-to-
left. For decoding, we use a product model (Petrov,
2010). The intuition is that the context information
from the left and the right of the current position
is complementary for predicting the current tag and
thus, the combination should serve to improve per-
formance over the HMM-LA tagger.
Since prior work on parsing speech with prosody
has relied on generative models, it was necessary
to modify equations of the model in order to incor-
porate the prosodic information, and then perform
rescoring in order to achieve gains. However, it is
far simpler to directly integrate prosody as features
into the model by using a discriminative approach.
Hence, we also investigate several log-linear mod-
els, which allow us to easily include an arbitrary
number and varying kinds of possibly overlapping
and non-independent features.
First, we implemented a Conditional Random
Field (CRF) tagger, which is an attractive choice due
to its ability to learn the globally optimal labeling
for a sequence and proven excellent performance on
sequence labeling tasks (Lafferty et al, 2001). In
contrast to an HMM which optimizes the joint like-
lihood of the word sequence and tags, a CRF opti-
mizes the conditional likelihood, given by:
p?(t|w) =
exp
?
j ?jFj(t, w)
?
t exp
?
j ?jFj(t, w)
(1)
where the ??s are the parameters of the model to es-
timate and F indicates the feature functions used.
The denominator in (1) is Z?(x), the normalization
factor, with:
Fj(t, w) =
?
i
fj(t, w, i)
822
Class Model Name Latent Variable Bidirectional N-best-Extraction Markov Order
Generative
Trigram HMM
?
2nd
HMM-LA
? ?
1st
HMM-LA-Bidir
? ?
1st
Discriminative
Stanford Bidir
?
2nd
Stanford Left5 2nd
CRF 2nd
Table 1: Description of tagging models
The objective we need to maximize then becomes :
L =
?
n
?
?
?
j
?jFj(tn, wn)? logZ?(xn)
?
??
???2
2?2
where we use a spherical Gaussian prior to pre-
vent overfitting of the model (Chen and Rosen-
feld, 1999) and the wide-spread quasi-Newtonian
L-BFGS method to optimize the model parame-
ters (Liu and Nocedal, 1989). Decoding is per-
formed with the Viterbi algorithm.
We also evaluate state-of-the-art Maximum En-
tropy taggers: the Stanford Left5 tagger (Toutanova
and Manning, 2000) and the Stanford bidirectional
tagger (Toutanova et al, 2003), with the former us-
ing only left context and the latter bidirectional de-
pendencies.
Table 1 summarizes the major differences be-
tween the models along several dimensions: (1) gen-
erative versus discriminative, (2) directionality of
decoding, (3) the presence or absence of latent anno-
tations, (4) the availability of n-best extraction, and
(5) the model order.
In order to assess the quality of our models, we
evaluate them on the section 23 test set of the stan-
dard newswire WSJ tagging task after training all
models on sections 0-22. Results appear in Ta-
ble 2. Clearly, all the models have high accuracy
on newswire data, but the Stanford bidirectional tag-
ger significantly outperforms the other models with
the exception of the HMM-LA-Bidir model on this
task.1
1Statistically significant improvements are calculated using
the sign test (p < 0.05).
Model Accuracy
Trigram HMM 96.58
HMM-LA 97.05
HMM-LA-Bidir 97.16
Stanford Bidir 97.28
Stanford Left5 97.07
CRF 96.81
Table 2: Tagging accuracy on WSJ
3 Experimental Setup
In the rest of this paper, we evaluate the tag-
ging models described in Section 2 on conver-
sational speech. We chose to utilize the Penn
Switchboard (Godfrey et al, 1992) and Fisher tree-
banks (Harper et al, 2005; Bies et al, 2006) because
they provide gold standard tags for conversational
speech and we have access to corresponding auto-
matically generated ToBI break indexes provided by
(Dreyer and Shafran, 2007; Harper et al, 2005)2.
We utilized the Fisher dev1 and dev2 sets contain-
ing 16,519 sentences (112,717 words) as the primary
training data and the entire Penn Switchboard tree-
bank containing 110,504 sentences (837,863 words)
as an additional training source3. The treebanks
were preprocessed as follows: the tags of auxiliary
verbs were replaced with the AUX tag, empty nodes
2A small fraction of words in the Switchboard treebank do
not align with the break indexes because they were produced
based on a later refinement of the transcripts used to produce
the treebank. For these cases, we heuristically added break *1*
to words in the middle of a sentence and *4* to words that end
a sentence.
3Preliminary experiments evaluating the effect of training
data size on performance indicated using the additional Switch-
board data leads to more accurate models, and so we use the
combined training set.
823
and function tags were removed, words were down-
cased, punctuation was deleted, and the words and
their tags were extracted. Because the Fisher tree-
bank was developed using the lessons learned when
developing Switchboard, we chose to use its eval
portion for development (the first 1,020 tagged sen-
tences containing 7,184 words) and evaluation (the
remaining 3,917 sentences with 29,173 words).
We utilize the development set differently for the
generative and discriminative models. Since the EM
algorithm used for estimating the parameters in the
latent variable models introduces a lot of variabil-
ity, we train five models with a different seed and
then choose the best one based on dev set perfor-
mance. For the discriminative models, we tuned
their respective regularization parameters on the dev
set. All results reported in the rest of this paper are
on the test set.
4 Integration of Prosodic Information
In this work, we use three classes of automatically
generated ToBI break indexes to represent prosodic
information (Kahn et al, 2005; Dreyer and Shafran,
2007; Huang and Harper, 2010): 4, 1, and p.
Consider the following speech transcription exam-
ple, which is enriched with ToBI break indexes in
parentheses and tags: i(1)/PRP did(1)/VBD
n?t(1)/RB you(1)/PRP know(4)/VBP
i(1)/PRP did(1)/AUX n?t(1)/RB...
The speaker begins an utterance, and then restarts
the utterance. The automatically predicted break 4
associated with know in the utterance compellingly
indicates an intonational phrase boundary and could
provide useful information for tagging if we can
model it appropriately.
To integrate prosody into our generative models,
we utilize the method from (Dreyer and Shafran,
2007) to add prosodic breaks. As Figure 1 shows,
ToBI breaks provide a secondary sequence of ob-
servations that is parallel to the sequence of words
that comprise the sentence. Each break bi in the sec-
ondary sequence is generated by the same tag ti as
that which generates the corresponding wordwi, and
so it is conditionally independent of its correspond-
ing word given the tag:
P (w, b|t) = P (w|t)P (b|t)
PRP
i
1
VBD
did
1
RB
n?t
1
VBP
know
4
Figure 1: Parallel generation of words and breaks for the
HMM models
The HMM-LA taggers are then able to split tags to
capture implicit higher order interactions among the
sequence of tags, words, and breaks.
The discriminative models are able to utilize
prosodic features directly, enabling the use of con-
textual interactions with other features to further im-
prove tagging accuracy. Specifically, in addition to
the standard set of features used in the tagging lit-
erature, we use the feature templates presented in
Table 3, where each feature associates the break bi,
word wi, or some combination of the two with the
current tag ti4.
Break and/or word values Tag value
bi=B ti = T
bi=B & bi?1=C ti = T
wi=W & bi=B ti = T
wi+1=W & bi=B ti = T
wi+2=W & bi=B ti = T
wi?1=W & bi=B ti = T
wi?2=W & bi=B ti = T
wi=W & bi=B & bi?1=C ti = T
Table 3: Prosodic feature templates
5 Experiments
5.1 Conversation side segmentation
When working with raw speech transcripts, we ini-
tially have a long stream of unpunctuated words,
which is called a conversation side. As the average
length of conversation side segments in our data is
approximately 630 words, it poses quite a challeng-
ing tagging task. Thus, we hypothesize that it is on
these large segments that we should achieve the most
4We modified the Stanford taggers to handle these prosodic
features.
824
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak OracleBreak+Sent OracleSent OracleBreak-Sent Rescoring
Figure 2: Tagging accuracy on conversation sides
improvement from the addition of prosodic informa-
tion.
In fact, as the baseline results in Figure 2 show,
the accuracies achieved on this task are much lower
than those on the newswire task. The trigram HMM
tagger accuracy drops to 92.43%, while all the other
models fall to within the range of 93.3%-94.12%,
a significant departure from the 96-97.3% range on
newswire sentences. Note that the Stanford bidi-
rectional and HMM-LA tagger perform very simi-
larly, although the HMM-LA-Bidir tagger performs
significantly better than both. In contrast to the
newswire task on which the Stanford bidirectional
tagger performed the best, on this genre, it is slightly
worse than the HMM-LA tagger, albeit the differ-
ence is not statistically significant.
With the direct integration of prosody into the
generative models (see Figure 2), there is a slight but
statistically insignificant shift in performance. How-
ever, integrating prosody directly into the discrimi-
native models leads to significant improvements in
the CRF and Stanford Left5 taggers. The gain in
the Stanford bidirectional tagger is not statistically
significant, however, which suggests that the left-
to-right models benefit more from the addition of
prosody than bidirectional models.
5.2 Human-annotated sentences
Given the lack-luster performance of the tagging
models on conversation side segments, even with the
direct addition of prosody, we chose to determine the
performance levels that could be achieved on this
task using human-annotated sentences, which we
will refer to as sentence segmentation. Figure 3 re-
ports the baseline tagging accuracy on sentence seg-
ments, and we see significant improvements across
all models. The HMM Trigram tagger performance
increases to 93.00%, while the increase in accuracy
for the other models ranges from around 0.2-0.3%.
The HMM-LA taggers once again achieve the best
performance, with the Stanford bidirectional close
behind. Although the addition of prosody has very
little impact on either the generative or discrimina-
tive models when applied to sentences, the base-
line tagging models (i.e., not prosodically enriched)
significantly outperform all of the prosodically en-
riched models operating on conversation sides.
At this point, it would be apt to suggest us-
ing automatic sentence boundary detection to cre-
ate shorter segments. Table 4 presents the results
of using baseline models without prosodic enrich-
ment trained on the human-annotated sentences to
tag automatically segmented speech5. As can be
seen, the results are quite similar to the conversation
side segmentation performances, and thus signifi-
cantly lower than when tagging human-annotated
sentences. A caveat to consider here is that we break
the standard assumption that the training and test set
be drawn from the same distribution, since the train-
ing data is human-annotated and the test is automat-
ically segmented. However, it can be quite challeng-
ing to create a corpus to train on that represents the
biases of the systems that perform automatic sen-
tence segmentation. Instead, we will examine an-
5We used the Baseline Structural Metadata System de-
scribed in Harper et al (2005) to predict sentence boundaries.
825
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody OracleBreak Rescoring
Figure 3: Tagging accuracy on human-annotated segments
other segmentation method to shorten the segments
automatically, i.e., by training and testing on speaker
turns, which preserves the train-test match, in Sec-
tion 5.5.
Model Accuracy
HMM-LA 93.95
HMM-LA-Bidir 94.07
Stanford Bidir 93.77
Stanford Left5 93.35
CRF 93.29
Table 4: Baseline tagging accuracy on automatically de-
tected sentence boundaries
5.3 Oracle Break Insertion
As we believe one of the major roles that prosodic
cues serve for tagging conversation sides is as a
proxy for sentence boundaries, perhaps the efficacy
of the prosodic breaks can, at least partially, be at-
tributed to errors in the automatically induced break
indexes themselves, as they can misalign with syn-
tactic phrase boundaries, as discussed in Huang and
Harper (2010). This may degrade the performance
of our models more than the improvement achieved
from correctly placed breaks. Hence, we conduct
a series of experiments in which we systematically
eliminate noisy phrase and disfluency breaks and
show that under these improved conditions, prosodi-
cally enriched models can indeed be more effective.
To investigate to what extent noisy breaks are im-
peding the possible improvements from prosodically
enriched models, we replaced all 4 and p breaks in
the training and evaluation sets that did not align
to the correct phrase boundaries as indicated by the
treebank with break 1 for both the conversation sides
and human-annotated sentences. The results from
using Oracle Breaks on conversation sides can be
seen in Figure 2. All models except Stanford Left5
and HMM-LA-Bidir significantly improve in accu-
racy when trained and tested on the Oracle Break
modified data. On human-annotated sentences, Fig-
ure 3 shows improvements in accuracies across all
models, however, they are statistically insignificant.
To further analyze why prosodically enriched
models achieve more improvement on conversation
sides than on sentences, we conducted three more
Oracle experiments on conversation sides. For the
first, OracleBreak-Sent, we further modified the data
such that all breaks corresponding to a sentence
ending in the human-annotated segments were con-
verted to break 1, thus effectively only leaving in-
side sentence phrasal boundaries. This modification
results in a significant drop in performance, as can
be seen in Figure 2.
For the second, OracleSent, we converted all
the breaks corresponding to a sentence end in the
human-annotated segmentations to break 4, and all
the others to break 1, thus effectively only leaving
sentence boundary breaks. This performed largely
on par with OracleBreak, suggesting that the phrase-
aligned prosodic breaks seem to be a stand-in for
sentence boundaries.
Finally, in the last condition, OracleBreak+Sent,
we modified the OracleBreak data such that all
breaks corresponding to a sentence ending in the
human-annotated sentences were converted to break
826
93
93.3
93.6
93.9
94.2
94.5
HMM-LA HMM-LA Bidir Stanford Bidir Stanford Left5 CRF
Baseline Prosody Rescoring
Figure 4: Tagging accuracy on speaker turns
4 (essentially combining OracleBreak and Oracle-
Sent). As Figure 2 indicates, this modification re-
sults in the best tagging accuracies for all the mod-
els. All models were able to match or even improve
upon the baseline accuracies achieved on the human
segmented data. This suggests that when we have
breaks that align with phrasal and sentence bound-
aries, prosodically enriched models are highly effec-
tive.
5.4 N-best Rescoring
Based on the findings in the previous section and the
findings of (Huang and Harper, 2010), we next ap-
ply a rescoring strategy in which the search space
of the prosodically enriched generative models is re-
stricted to the n-best list generated from the base-
line model (without prosodic enrichment). In this
manner, the prosodically enriched model can avoid
poor tag sequences produced due to the misaligned
break indexes. As Figure 2 shows, using the base-
line conversation side model to produce an n-best
list for the prosodically enriched model to rescore
results in significant improvements in performance
for the HMM-LA model, similar to the parsing re-
sults of (Huang and Harper, 2010). The size of the
n-best list directly impacts performance, as reducing
to n = 1 is akin to tagging with the baseline model,
and increasing n ? ? amounts to tagging with the
prosodically enriched model. We experimented with
a number of different sizes for n and chose the best
one using the dev set. Figure 3 presents the results
for this method applied to human-annotated sen-
tences, where it produces only marginal improve-
ments6.
5.5 Speaker turn segmentation
The results presented thus far indicate that if we
have access to close to perfect break indexes, we
can use them effectively, but this is not likely to be
true in practice. We have also observed that tagging
accuracy on shorter conversation sides is greater
than longer conversation sides, suggesting that post-
processing the conversation sides to produce shorter
segments would be desirable.
We thus devised a scheme by which we could
automatically extract shorter speaker turn segments
from conversation sides. For this study, speaker
turns, which effectively indicate speaker alterna-
tions, were obtained by using the metadata in the
treebank to split the sentences into chunks based on
speaker change. Every time a speaker begins talk-
ing after the other speaker was talking, we start a
new segment for that speaker. In practice, this would
need to be done based on audio cues and automatic
transcriptions, so these results represent an upper
bound.
Figure 4 presents tagging results on speaker turn
segments. For most models, the difference in accu-
racy achieved on these segments and that of human-
annotated sentences is statistically insignificant. The
only exception is the Stanford bidirectional tagger,
6Rescoring using the CRF model was also performed, but
led to a performance degradation. We believe this is due to
the fact that the prosodically enriched CRF model was able to
directly use the break index information, and so restricting it to
the baseline CRF model search space limits the performance to
that of the baseline model.
827
0100
200
300
400
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
Number
 o
f Er
ro
rs
 Conv Baseline Conv Rescore Conv OracleBreak Sent Baseline
(a) Number of errors by part of speech category for the HMM-LA model with and without prosody
0
100
200
300
400
NNP RP AUX JJ PRP RB WDT VBP VBZ UH XX VB NN DT VBD IN
Number
 o
f Er
ro
rs
 Conv Baseline Conv Prosody Conv OracleBreak Sent Baseline
(b) Number of errors by part of speech category for the CRF model with and without prosody
Figure 5: Error reduction for prosodically enriched HMM-LA (a) and CRF (b) models
which performs worse on these slightly longer seg-
ments. With the addition of break indexes, we see
marginal changes in most of the models; only the
CRF tagger receives a significant boost. Thus, mod-
els achieve performance gains from tagging shorter
segments, but at the cost of limited usefulness of the
prosodic breaks. Overall, speaker turn segmenta-
tion is an attractive compromise between the original
conversation sides and human-annotated sentences.
6 Discussion
Across the different models, we have found that tag-
gers applied to shorter segments, either sentences or
speaker turns, do not tend to benefit significantly
from prosodic enrichment, in contrast to conversa-
tion sides. To analyze this further we broke down
the results by part of speech for the two models
for which break indexes improved performance the
most: the CRF and HMM-LA rescoring models,
which achieved an overall error reduction of 2.8%
and 2.1%, respectively. We present those categories
that obtained the greatest benefit from prosody in
Figure 5 (a) and (b). For both models, the UH cate-
gory had a dramatic improvement from the addition
of prosody, achieving up to a 10% reduction in error.
For the CRF model, other categories that saw im-
pressive error reductions were NN and VB, with
10% and 5%, respectively. Table 5 lists the prosodic
features that received the highest weight in the CRF
model. These are quite intuitive, as they seem to rep-
resent places where the prosody indicates sentence
or clausal boundaries. For the HMM-LA model,
the VB and DT tags had major reductions in error
of 13% and 10%, respectively. For almost all cat-
egories, the number of errors is reduced by the ad-
dition of breaks, and further reduced by using the
OracleBreak processing described above.
Weight Feature
2.2212 wi=um & bi=4 & t=UH
1.9464 wi=uh & bi=4 & t=UH
1.7965 wi=yes & bi=4 & t=UH
1.7751 wi=and & bi=4 & t=CC
1.7554 wi=so & bi=4 & t=RB
1.7373 wi=but & bi=4 & t=CC
Table 5: Top break 4 prosody features in CRF prosody
model
To determine more precisely the effect that the
segment size has on tagging accuracy, we extracted
the oracle tag sequences from the HMM-LA and
CRF baseline and prosodically enriched models
across conversation sides, sentences, and speaker
turn segments. As the plot in Figure 6 shows, as
we increase the n-best list size to 500, the ora-
cle accuracy of the models trained on sentences in-
828
92
94
96
98
100
2 5 10 20 50 100 200 500
Ac
cur
ac
y
 
N -Best size  
Sentences 
Speaker  tuns  
Conversation sides  
Figure 6: Oracle comparison: solid lines for sentences,
dashed lines for speaker turns, and dotted lines for con-
versation sides
creases rapidly to 99%; whereas, the oracle accu-
racy of models on conversation sides grow slowly
to between 94% and 95%. The speaker turn trained
models, however, behave closely to those using sen-
tences, climbing rapidly to accuracies of around
98%. This difference is directly attributable to the
length of the segments. As can be seen in Table 6,
the speaker turn segments are more comparable in
length to sentences.
Train Eval
Conv 627.87 ? 281.57 502.98 ? 151.22
Sent 7.52? 7.86 7.45 ? 8.29
Speaker 15.60? 29.66 15.27? 21.01
Table 6: Length statistics of different data segmentations
Next, we return to the large performance degrada-
tion when tagging speech rather than newswire text
to examine the major differences among the mod-
els. Using two of our best performing models, the
Stanford bidirectional and HMM-LA, in Figure 7
we present the categories for which performance
degradation was the greatest when comparing per-
formance of a tagger trained on WSJ to a tagger
trained on spoken sentences and conversation sides.
The performance decrease is quite similar across
both models, with the greatest degradation on the
NNP, RP, VBN, and RBS categories.
Unsurprisingly, both the discriminative and gen-
erative bidirectional models achieve the most im-
pressive results. However, the generative HMM-
LA and HMM-LA-Bidir models achieved the best
results across all three segmentations, and the best
overall result, of 94.35%, on prosodically enriched
sentence-segmented data. Since the Stanford bidi-
rectional model incorporates all of the features that
produced its state-of-the-art performance on WSJ,
we believe the fact that the HMM-LA outperforms
it, despite the discriminative model?s more expres-
sive feature set, is indicative of the HMM-LA?s abil-
ity to more effectively adapt to novel domains during
training. Another challenge for the discriminative
models is the need for regularization tuning, requir-
ing additional time and effort to train several mod-
els and select the most appropriate parameter each
time the domain changes. Whereas for the HMM-
LA models, although we also train several models,
they can be combined into a product model, such as
that described by Petrov (2010), in order to further
improve performance.
Since the prosodic breaks are noisier features than
the others incorporated in the discriminative models,
it may be useful to set their regularization param-
eter separately from the rest of the features, how-
ever, we have not explored this alternative. Our ex-
periments used human transcriptions of the conver-
sational speech; however, realistically our models
would be applied to speech recognition transcripts.
In such a case, word error will introduce noise in ad-
dition to the prosodic breaks. In future work, we will
evaluate the use of break indexes for tagging when
there is lexical error. We would also apply the n-
best rescoring method to exploit break indexes in the
HMM-LA bidirectional model, as this would likely
produce further improvements.
7 Conclusion
In this work, we have evaluated factors that are im-
portant for developing accurate tagging models for
speech. Given that prosodic breaks were effective
knowledge sources for parsing, an important goal
of this work was to evaluate their impact on vari-
ous tagging model configurations. Specifically, we
have examined the use of prosodic information for
tagging conversational speech with several different
discriminative and generative models across three
different speech transcript segmentations. Our find-
829
0%
10%
20%
30%
40%
50%
NNP VBN WP CD RP EX WRB WDT JJR POS JJS RBS
Err
or
 R
at
e
 
WSJ (Stanford-Bidir) WSJ (HMM-LA)
Sent (Stanford-Bidir) Sent (HMM-LA)
Conv (Stanford-Bidir) Conv (HMM-LA)
Figure 7: Comparison of error rates between the Standford Bidir and HMM-LA models trained on WSJ, sentences,
and conversation sides
ings suggest that generative models with latent an-
notations achieve the best performance in this chal-
lenging domain. In terms of transcript segmenta-
tion, if sentences are available, it is preferable to use
them. In the case that no such annotation is avail-
able, then using automatic sentence boundary detec-
tion does not serve as an appropriate replacement,
but if automatic speaker turn segments can be ob-
tained, then this is a good alternative, despite the fact
that prosodic enrichment is less effective.
Our investigation also shows that in the event that
conversation sides must be used, prosodic enrich-
ment of the discriminative and generative models
produces significant improvements in tagging accu-
racy (by direct integration of prosody features for
the former and by restricting the search space and
rescoring with the latter). For tagging, the most im-
portant role of the break indexes appears to be as a
stand in for sentence boundaries. The oracle break
experiments suggest that if the accuracy of the au-
tomatically induced break indexes can be improved,
then the prosodically enriched models will perform
as well, or even better, than their human-annotated
sentence counterparts.
8 Acknowledgments
This research was supported in part by NSF IIS-
0703859 and the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-2-001. Any opinions, findings, and rec-
ommendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of the funding agency or the institutions where the
work was completed.
References
Anton Batliner, Bernd Mo?bius, Gregor Mo?hler, Antje
Schweitzer, and Elmar No?th. 2001. Prosodic models,
automatic speech understanding, and speech synthesis:
toward the common ground. In Eurospeech.
Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki
Maeda, Seth Kulick, Yang Liu, Mary Harper, and
Matthew Lease. 2006. Linguistic resources for speech
parsing. In LREC.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy models.
Technical report, Technical Report CMU-CS-99-108,
Carnegie Mellon University.
Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.
1997. Prosody in comprehension of spoken language:
A literature review. Language and Speech.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Inter-
speech.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Jennifer Foster. 2010. ?cba to check the spelling?: Inves-
tigating parser performance on discussion forum posts.
In NAACL-HLT.
Florian Gallwitz, Heinrich Niemann, Elmar No?th, and
Volker Warnke. 2002. Integrated recognition of words
and prosodic phrase boundaries. Speech Communica-
tion.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In ICASSP.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In NAACL.
Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. 2005 Johns Hopkins Summer Work-
shop Final Report on Parsing and Spoken Structural
830
Event Detection. Technical report, Johns Hopkins
University.
Mark Hasegawa-Johnson, Ken Chen, Jennifer Cole,
Sarah Borys, Sung suk Kim, Aaron Cohen, Tong
Zhang, Jeung yoon Choi, Heejin Kim, Taejin Yoon,
and Ra Chavarria. 2005. Simultaneous recognition
of words and prosody in the boston university radio
speech corpus. speech communication. Speech Com-
munication.
Peter A. Heeman. 1999. POS tags and decision trees for
language modeling. In EMNLP.
Dustin Hillard, Zhongqiang Huang, Heng Ji, Ralph Gr-
ishman, Dilek Hakkani-Tur, Mary Harper, Mari Os-
tendorf, and Wen Wang. 2006. Impact of automatic
comma prediction on POS/name tagging of speech. In
ICASSP.
Zhongqiang Huang and Mary Harper. 2010. Appropri-
ately handled prosodic breaks help PCFG parsing. In
NAACL.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram hmm part-
of-speech tagger by latent annotation and self-training.
In NAACL-HLT.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP-HLT.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
D. C. Liu and Jorge Nocedal. 1989. On the limited mem-
ory BFGS method for large scale optimization. Math-
ematical Programming.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. In Plenary Meeting and Symposium on
Prosody and Speech Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Slav Petrov. 2010. Products of random latent variable
grammars. In HLT-NAACL.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, and Julia Hirshberg. 1992. ToBI: A standard for
labeling English prosody. In ICSLP.
Paul Taylor and Alan W. Black. 1998. Assigning
phrase breaks from part-of-speech sequences. Com-
puter Speech and Language.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden markov model for part-of-speech tag-
ging. In ACL.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the knowledge sources used in a maximum
entropy part-of-speech tagger. In EMNLP.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In NAACL.
831
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 691?699,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Syntactic Decision Tree LMs: Random Selection or Intelligent Design?
Denis Filimonov??
?Human Language Technology
Center of Excellence
Johns Hopkins University
den@cs.umd.edu
Mary Harper?
?Department of Computer Science
University of Maryland, College Park
mharper@umd.edu
Abstract
Decision trees have been applied to a vari-
ety of NLP tasks, including language mod-
eling, for their ability to handle a variety of
attributes and sparse context space. More-
over, forests (collections of decision trees)
have been shown to substantially outperform
individual decision trees. In this work, we in-
vestigate methods for combining trees in a for-
est, as well as methods for diversifying trees
for the task of syntactic language modeling.
We show that our tree interpolation technique
outperforms the standard method used in the
literature, and that, on this particular task, re-
stricting tree contexts in a principled way pro-
duces smaller and better forests, with the best
achieving an 8% relative reduction in Word
Error Rate over an n-gram baseline.
1 Introduction
Language Models (LMs) are an essential part of
NLP applications that require selection of the most
fluent word sequence among multiple hypotheses.
The most prominent applications include Automatic
Speech Recognition (ASR) and Machine Transla-
tion (MT).
Statistical LMs formulate the problem as the
computation of the model?s probability to gener-
ate the word sequence w1, w2, . . . , wm (denoted as
wm1 ), assuming that higher probability corresponds
to more fluent hypotheses. LMs are often repre-
sented in the following generative form:
p(wm1 ) =
m?
i=1
p(wi|wi?11 )
Note the context space for this function, wi?11 is ar-
bitrarily long, necessitating some independence as-
sumption, which usually consists of reducing the rel-
evant context to n?1 immediately preceding tokens:
p(wi|wi?11 ) ? p(wi|wi?1i?n+1) (1)
These distributions are typically estimated from ob-
served counts of n-grams wii?n+1 in the training
data. The context space is still far too large1; there-
fore, the models are recursively smoothed using
lower order distributions. For instance, in a widely
used n-gram LM, the probabilities are estimated as
follows:
p?(wi|wi?1i?n+1) = ?(wi|wi?1i?n+1) + (2)
?(wi?1i?n+1) ? p?(wi|wi?1i?n+2)
where ? is a discounted probability2.
Note that this type of model is a simple Markov
chain lacking any notion of syntax. It is widely
accepted that languages do have some structure.
Moreover, it has been shown that incorporating syn-
tax into a language model can improve its perfor-
mance (Bangalore, 1996; Heeman, 1998; Chelba
and Jelinek, 2000; Filimonov and Harper, 2009). A
straightforward way of incorporating syntax into a
language model is by assigning a tag to each word
and modeling them jointly; then to obtain the proba-
1O(|V |n?1) in n-gram model with typical order n =
3 . . . 5, and a vocabulary size of |V | = 104 . . . 106.
2We refer the reader to (Chen and Goodman, 1996) for a
survey of the discounting methods for n-gram models.
691
bility of a word sequence, the tags must be marginal-
ized out:
p(wm1 ) =
?
t1...tm
p(wm1 tm1 ) =
?
t1...tm
m?
i=1
p(witi|wi?11 ti?11 )
An independence assumption similar to Eq. 1 can be
made:
p(witi|wi?11 ti?11 ) ? p(witi|wi?1i?n+1ti?1i?n+1) (3)
A primary goal of our research is to build strong
syntactic language models and provide effective
methods for constructing them to the research com-
munity. Note that the tags in the context of the joint
model in Eq. 3 exacerbate the already sparse prob-
lem in Eq. 1, which makes the probability estima-
tion particularly challenging. We utilize decision
trees for joint syntactic language models to clus-
ter context because of their strengths (reliance on
information theoretic metrics to cluster context in
the face of extreme sparsity and the ability to in-
corporate attributes of different types3), and at the
same time, unlike log-linear models (Rosenfeld et
al., 1994), computationally expensive probability
normalization does not have to be postponed until
runtime.
In Section 2, we describe the details of the syntac-
tic decision tree LM. Construction of a single-tree
model is difficult due to the inevitable greediness
of the tree construction process and its tendency to
overfit the data. This problem is often addressed by
interpolating with lower order decision trees. In Sec-
tion 3, we point out the inappropriateness of backoff
methods borrowed from n-gram models for decision
tree LMs and briefly describe a generalized interpo-
lation for such models. The generalized interpola-
tion method allows the addition of any number of
trees to the model, and thus raises the question: what
is the best way to create diverse decision trees so that
their combination results in a stronger model, while
at the same time keeping the total number of trees in
the model relatively low for computational practical-
ity. In Section 4, we explore and evaluate a variety
3For example, morphological features can be very helpful
for modeling highly inflectional languages (Bilmes and Kirch-
hoff, 2003).
of methods for creating different trees. To support
our findings, we evaluate several of the models on
an ASR rescoring task in Section 5. Finally, we dis-
cuss our findings in Section 6.
2 Joint Syntactic Decision Tree LM
A decision tree provides us with a clustering func-
tion ?(wi?1i?n+1ti?1i?n+1) ? {?1, . . . ,?N}, where N
is the number of clusters, and clusters ?k are disjoint
subsets of the context space. The probability estima-
tion for a joint decision tree model is approximated
as follows:
p(witi|wi?1i?n+1ti?1i?n+1) ? p(witi|?(wi?1i?n+1ti?1i?n+1))(4)
In the remainder of this section, we briefly describe
the techniques that we use to construct such a deci-
sion tree ? and to estimate the probability distribu-
tion for the joint model in Eq. 4.
2.1 Decision Tree Construction
We use recursive partitioning to grow decision trees.
In this approach, a number of alternative binary
splits of the training data associated with a node are
evaluated using some metric, the best split is chosen,
checked against a stopping rule (which aims at pre-
venting overfitting to the training data and usually
involves a heldout set), and then the two partitions
become the child nodes if the stopping rule does not
apply. Then the algorithm proceeds recursively into
the newly constructed leaves.
Binary splits are often referred to as questions
about the context because a binary partition can
be represented by a binary function that decides
whether an element of context space belongs to one
partition or the other. We utilize univariate questions
where each question partitions the context on one
attribute, e.g., wi?2 or ti?1. The questions about
words and tags are constructed differently:
? The questions q about the words are in the form
q(x) ? wi+x ? S, where x is an integer be-
tween ?n + 1 and ?1, and S ? V is a subset
of the word vocabulary V . To construct the set
S, we take the set of words So observed at the
offset x in the training data associated with the
692
current node and split it into two complemen-
tary subsets S ? S? = So using the Exchange
algorithm (Martin et al, 1998). Because the
algorithm is greedy and depends on the initial-
ization, we construct 4 questions per word po-
sition using different random initializations of
the Exchange algorithm.
Since we need to account for words that were
not observed in the training data, we utilize
the structure depicted in Figure 1. To estimate
the probability at the backoff node (B in Fig-
ure 1), we can either use the probability from its
grandparent nodeA or estimate it using a lower
order tree (see Section 3), or combine the two.
We have observed no noticeable difference be-
tween these methods, which suggests that only
a small fraction of probability is estimated from
these nodes; therefore, for simplicity, we use
the probability estimated at the backoff node?s
grandparent.
? To create questions about tags we create a hi-
erarchical clustering of all tags in the form of
a binary tree. This is done beforehand, using
the Minimum Discriminating Information al-
gorithm (Zitouni, 2007) with the entire train-
ing data set. In this tree, each leaf is an in-
dividual tag and each internal node is associ-
ated with the subset of tags that the node dom-
inates. Questions about tags are constructed in
the form q(x, k) ? ti+x ? Tk, where k is a
node in the tag tree and Tk is the subset of tags
associated with that node. The rationale behind
constructing tag questions in this form is that
it enables a more efficient decoding algorithm
than standard HMM decoding (Filimonov and
Harper, 2009).
Questions are evaluated in two steps. First the
context attribute x is selected using a metric simi-
lar to information gain ratio proposed by (Quinlan,
1986):
M = 1? H(wi)?H(wi|x)H(x) = 1?
I(x;wi)
H(x)
where x is one of the context attributes, e.g., wi?2
or ti?1. Then, among the questions about attribute
wi?2??S
Backoff leaf
yes
yesno
no
A
B
wi?2?S
Figure 1: A fragment of the decision tree with a backoff
node. S ? S? is the set of words observed in the training
data at the node A. To account for unseen words, we add
the backoff node B.
x, we select the question that maximizes the entropy
reduction.
Instead of dedicating an explicit heldout set for
the stopping criterion, we utilize a technique simi-
lar to cross validation: the training data set is par-
titioned into four folds, and the best question is re-
quired to reduce entropy on each of the folds.
Note that the tree induction algorithm can also be
used to construct trees without tags:
p(wi|wi?1i?n+1) ? p(wi|?(wi?1i?n+1))
We refer to this model as the word-tree model. By
comparing syntactic and word-tree models, we are
able to separate the effects of decision tree modeling
and syntactic information on language modeling by
comparing both models to an n-gram baseline.
2.2 In-tree Smoothing
A decision tree offers a hierarchy of clusterings that
can be exploited for smoothing. We can interpo-
late the observed distributions at leaves recursively
with their parents, as in (Bahl et al, 1990; Heeman,
1998):
p?k(witi) = ?kpML(witi) + (1? ?k)p?k?(witi) (5)
where pML is the observed distribution at node k
and k? is the parent of k. The coefficients ?k are
estimated using an EM algorithm.
We can also combine p(witi|?(wi?1i?n+1ti?1i?n+1))
with lower order decision trees, i.e.,
693
p(witi|?(wi?1i?n+2ti?1i?n+2)), and so on up until
p(witi) which is a one-node tree (essentially a
unigram model). Although superficially similar to
backoff in n-gram models, lower order decision
trees differ substantially from lower order n-gram
models and require different interpolation methods.
In the next section, we discuss this difference and
present a generalized interpolation that is more
suitable for combining decision tree models.
3 Interpolation with Backoff Tree Models
In this section, for simplicity of presentation, we fo-
cus on the equations for word models, but the same
equations apply equally to joint models (Eq. 3) with
trivial transformations.
3.1 Backoff Property
Let us rewrite the interpolation Eq. 2 in a more
generic way:
p?(wi|wi?11 ) = ?n(wi|?n(wi?11 )) + (6)
?(?n(wi?11 )) ? p?(wi|BOn?1(wi?11 ))
where, ?n is a discounted distribution, ?n is a clus-
tering function of order n, and ?(?n(wi?11 )) is the
backoff weight chosen to normalize the distribution.
BOn?1 is the backoff clustering function of order
n ? 1, representing a reduction of context size. In
the case of an n-gram model, ?n(wi?11 ) is the set
of word sequences where the last n ? 1 words are
wi?1i?n+1. Similarly, BOn?1(wi?11 ) is the set of se-
quences ending with wi?1i?n+2. In the case of a de-
cision tree model, the same backoff function is typ-
ically used, but the clustering function can be arbi-
trary.
The intuition behind Eq. 6 is that the backoff con-
text BOn?1(wi?11 ) allows for a more robust (but
less informed) probability estimation than the con-
text cluster ?n(wi?11 ). More precisely:
?wi?11 ,W : W ? ?n(w
i?1
1 )?W ? BOn?1(wi?11 )
(7)
that is, every word sequence W that belongs to a
context cluster ?n(wi?11 ), belongs to the same back-
off cluster BOn?1(wi?11 ) (hence has the same back-
off distribution). For n-gram models, Property 7
?nBOn?2 Backofk leyaslknol Asol ? cl??ac?kal?eeoyockl?? c?
?A?l?A??eel?ya?yk?l Ak? e?o? ???l?A?aeel?ya?yk?l??a?Ako?
?ckofkl ?A? ?ackofkl ?A?
Figure 2: Backoff Property
trivially holds since BOn?1(wi?11 ) and ?n(wi?11 )
are defined as sets of sequences ending with wi?1i?n+2
andwi?1i?n+1, with the former clearly being a superset
of the latter. However, when ? can be arbitrary, e.g.,
a decision tree, the property is not necessarily satis-
fied. Figure 2 illustrates cases when the Property 7
is satisfied (a) and violated (b).
Let us consider what happens when we have
two context sequences W and W ? that belong to
the same cluster ?n(W ) = ?n(W ?) but differ-
ent backoff clusters BOn?1(W ) 6= BOn?1(W ?).
For example: suppose we have ?(wi?2wi?1) =
({on}, {may,june}) and two corresponding backoff
clusters: BO? = ({may}) and BO?? = ({june}).
Following on, the word may is likely to be a month
rather than a modal verb, although the latter is
more frequent and will dominate in BO?. There-
fore we have much less faith in p?(wi|BO?) than in
p?(wi|BO??) and would like a much smaller weight
? assigned to BO?. However this would not be pos-
sible in the backoff scheme in Eq. 6, thus we will
have to settle on a compromise value of ?, resulting
in suboptimal performance.
Hence arbitrary clustering (an advantage of deci-
sion trees) leads to a violation of Property 7, which
is likely to produce a degradation in performance if
backoff interpolation Eq. 6 is used.
3.2 Generalized Interpolation
Recursive linear interpolation similar to Jelinek-
Mercer smoothing for n-gram models (Jelinek and
Mercer, 1980) has been applied to decision tree
models:
694
p?n(wi|wi?1i?n+1) = ?n(?n) ? pn(wi|?n) + (8)
(1? ?n(?n)) ? p?n?1(wi|wi?1i?n+2)
where ?n ? ?n(wi?1i?n+1), and ?n(?n) ? [0, 1] are
assigned to each cluster and are optimized on a held-
out set using EM. pn(wi|?n) is the probability dis-
tribution at the cluster ?n in the tree of order n. This
interpolation method is particularly useful as, un-
like count-based discounting methods (e.g., Kneser-
Ney), it can be applied to already smoothed distribu-
tions pn.
In (Filimonov and Harper, 2011), we observed
that because of the violation of Property 7 in deci-
sion tree models, the interpolation method of Eq. 8
is not appropriate for such models. Instead we pro-
posed the following generalized form of linear inter-
polation:
p?n(wi|wi?1i?n+1) =
?n
m=1 ?m(?m) ? pm(wi|?m)?n
m=1 ?m(?m)
(9)
Note that the recursive interpolation of Eq. 8 can
be represented in this form with the additional con-
straint?nm=1 ?m(?m) = 1, which is not required in
the generalized interpolation of Eq. 9; thus, the gen-
eralized interpolation, albeit having the same num-
ber of parameters, has more degrees of freedom. We
also showed that the recursive interpolation Eq. 8 is
a special case of Eq. 9 that occurs when the Prop-
erty 7 holds.
4 From Backoff Trees to Forest
Note that, in Eq. 9, individual trees do not have ex-
plicit higher-lower order relations, they are treated
as a collection of trees, i.e., as a forest. Naturally,
to benefit from the forest model, its trees must differ
in some way. Different trees can be created based
on differences in the training data, differences in the
tree growing algorithm, or some non-determinism in
the way the trees are constructed.
(Xu, 2005) used randomization techniques to pro-
duce a large forest of decision trees that were com-
bined as follows:
p(wi|wi?1i?n+1) =
1
M
M?
m=1
pm(wi|wi?1i?n+1) (10)
whereM is the number of decision trees in the forest
(he proposed M = 100) and pm is the m-th tree
model4. Note that this type of interpolation assumes
that each tree model is ?equal? a priori and therefore
is only appropriate when the tree models are grown
in the same way (particularly, using the same order
of context). Note that Eq. 10 is a special case of
Eq. 9 when all parameters ? are equal.
(Xu, 2005) showed that, although each individual
tree is a fairly weak model, their combination out-
performs the n-gram baseline substantially. How-
ever, we find this approach impractical for online
application of any sizable model: In our experi-
ments, fourgram trees have approximately 1.8 mil-
lion leaves and the tree structure itself (without prob-
abilities) occupies nearly 200MB of disk space af-
ter compression. It would be infeasible to apply a
model consisting of more than a handful of such
trees without distributed computing of some sort.
Therefore, we pose the following question: If we
can afford to have only a handful of trees in the
model, what would be best approach to construct
those trees?
In the remainder of this section, we will describe
the experimental setup, discuss and evaluate differ-
ent ways of building decision tree forests for lan-
guage modeling, and compare combination methods
based on Eq. 9 and Eq. 10 (when Eq. 10 is applica-
ble).
4.1 Experimental Setup
To train our models we use 35M words of WSJ
94-96 from LDC2008T13. The text was converted
into speech-like form, namely numbers and abbrevi-
ations were verbalized, text was downcased, punctu-
ation was removed, and contractions and possessives
were joined with the previous word (i.e., they ?ll be-
comes they?ll). For the syntactic modeling, we used
tags comprised of the POS tags of the word and it?s
head. Parsing of the text for tag extraction occurred
after verbalization of numbers and abbreviations but
4Note that (Xu, 2005) used lower order models to estimate
pm.
695
before any further processing; we used a latent vari-
able PCFG parser as in (Huang and Harper, 2009).
For reference, we include an n-gram model with
modified interpolated KN discounting. All mod-
els use the same vocabulary of approximately 50k
words.
Perplexity numbers reported in Tables 1, 2, 3,
and 4 are computed on WSJ section 23 (tokenized
in the same way)5.
In Table 1, we show results reported in (Filimonov
and Harper, 2011), which we use as the baseline for
further experiments. We constructed two sets of de-
cision trees (a joint syntactic model and a word-tree
model) as described in Section 2. Each set was com-
prised of a fourgram tree with backoff trigram, bi-
gram, and unigram trees. We combined these trees
using either Eq. 8 or Eq. 9. The ? parameters in
Eq. 8 were estimated using EM by maximizing like-
lihood of a heldout set (we utilized 4-way cross-
validation); whereas, the parameters in Eq. 9 were
estimated using L-BFGS because the denominator
in Eq. 9 makes the maximization step problematic.
4.2 Random Forest
(Xu, 2005) evaluated a variety of randomization
techniques that can be used to build trees. He used
a word-only model, with questions constructed us-
ing the Exchange algorithm, similar to our model.
He tried two methods of randomization: selecting
the positions in the history for question construction
by a Bernoulli trials6, and random initialization of
the Exchange algorithm. He found that when the
Exchange algorithm was initialized randomly, the
Bernoulli trial parameter did not matter; however,
when the Exchange algorithm was initialized deter-
ministically; lower values for the Bernoulli trial pa-
rameter r yielded better overall forest performance.
We implemented a similar method, namely, initial-
izing the Exchange algorithm randomly and using
r = 0.1 for Bernoulli trials7.
There is a key difference between the two ran-
5This section was not used for training the parser or for the
LM training.
6In this method, positions in the history are ignored with
probability 1? r, where r is the Bernoulli trials parameter.
7Note that because in the joint model, the question about
tags are deterministic, we use a lower value of r than (Xu, 2005)
to increase randomness.
domization methods. Since we do not have an a
priori preference for choosing initializations for the
Exchange algorithm, by using random initializations
it is hoped that due to the greedy nature of the al-
gorithm, the constructed trees, while being ?unde-
graded,?8 will be sufficiently different so that their
combination improves over an individual tree. By
introducing Bernoulli trials, on the other hand, there
is a choice to purposely degrade the quality of in-
dividual trees in the hope that additional diversity
would enable their combination to compensate for
the loss of quality in individual trees.
Another way of introducing randomness to the
tree construction without apparent degradation of in-
dividual tree quality is through varying the data, e.g.,
using different folds of the training data (see Sec-
tion 2.1).
Let us take a closer look at the effect of differ-
ent types of randomization on individual trees and
their combinations. In the first set of experiments,
we compare the performance of a single undegraded
fourgram tree9 with forests of fourgram trees grown
randomly with Bernoulli trials. Having only same-
order trees in a forest allows us to apply interpola-
tion of Eq. 10 (used in (Xu, 2005)) and compare
with the interpolation method presented in Eq. 9. By
comparing forests of different sizes with the baseline
from Table 1, we are able to evaluate the effect of
randomization in decision tree growing and assess
the importance of the lower order trees.
The results are shown in Table 2. Note that, while
an undegraded syntactic tree is better than the word
tree, the situation is reversed when the trees are
grown randomly. This can be explained by the fact
that the joint model has a much higher dimensional-
ity of the context space, and therefore is much more
sensitive to the clustering method.
As we increase the number of random trees in the
forest, the perplexity decreases as expected, with the
interpolation method of Eq. 9 showing improvement
of a few percentile points over Eq. 10. Note that
in the case of the word-tree model, it takes 4 ran-
dom decision trees to reach the performance of a sin-
gle undegraded tree, while in the joint model, even
8Here and henceforth, by ?undegraded? we mean ?accord-
ing to the algorithm described in Section 2.?
9Since each tree has a smooth distribution based on Eq. 5,
lower order trees are not strictly required.
696
Eq. 8 Eq. 9 (generalized)
order n-gram word-tree syntactic word-tree syntactic
2-gram 261.0 257.8 214.3 258.1 214.6
3-gram 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)
4-gram 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)
Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of
perplexity relative to the lower order model of the same type.
word-tree syntactic
Eq. 10 Eq. 9 Eq. 10 Eq. 9
1 ? undgr 204.9 189.1
1 ? rnd 250.2 289.9
2 ? rnd 229.5 221.5 244.6 240.9
3 ? rnd 227.5 214.5 226.2 220.0
4 ? rnd 219.5 205.0 219.5 212.2
5 ? rnd 200.9 184.1 216.5 209.0
baseline N/A 155.7 N/A 147.1
Table 2: Perplexity numbers obtained using fourgram
trees only. Note that ?undgr? and ?rnd? denote unde-
graded and randomly grown trees with Bernoulli trials,
respectively, and the number indicates the number of
trees in the forest. Also ?baseline? refers to the fourgram
models with lower order trees (from Table 1, Eq. 9).
5 trees are much worse than a single decision tree
constructed without randomization. Finally, com-
pare the performance of single undegraded fourgram
trees in Table 2 with fourgram models in Table 1,
which are constructed with lower order trees: both
word-tree and joint models in Table 1 have over
20% lower perplexity compared to the correspond-
ing models consisting of a single fourgram tree.
In Table 3, we evaluate forests of fourgram trees
produced using randomizations without degrading
the tree construction algorithm. That is, we use ran-
dom initializations of the Exchange algorithm and,
additionally, variations in the training data fold. All
forests in this table use the interpolation method
of Eq. 9. Note that, while these perplexity num-
bers are substantially better than trees produced with
Bernoulli trials in Table 2, they are still significantly
worse than the baseline model from Table 1.
These results suggest that, while it is beneficial
to combine different decision trees, we should in-
troduce differences to the tree construction process
word-tree syntactic
# trees Exchng. +data Exchng. +data
1 204.9 189.1
2 185.9 186.5 174.5 173.7
3 179.5 179.9 168.8 167.2
4 176.2 176.4 165.1 164.0
5 173.7 172.0 163.0 162.0
baseline 155.7 147.1
Table 3: Perplexity numbers obtained using fourgram
trees produced using random initialization of the Ex-
change algorithm (Exchng. columns) and, additionally,
variations in training data folds (+data columns). Note
that ?baseline? refers to the fourgram models with lower
order trees (from Table 1). All models use the interpola-
tion method of Eq. 9.
without degrading the trees when introducing ran-
domness, especially for joint models. In addition,
lower order trees seem to play an important role for
high quality model combination.
4.3 Context-Restricted Forest
As we have mentioned above, combining higher and
lower order decision trees produces much better re-
sults. A lower order decision tree is grown from
a lower order context space, i.e., the context space
where we purposely ignore some attributes. Note
that in this case, rather than randomly ignoring con-
texts via Bernoulli trials at every node in the decision
tree, we discard some context attributes upfront in
a principled manner (i.e., most distant context) and
then grow the decision tree without degradation.
Since the joint model, having more context at-
tributes, affords a larger variety of different contexts,
we use this model in the remaining experiments.
In Table 4, we present the perplexity numbers for
our standard model with additional trees. We de-
note context-restricted trees by their Markovian or-
697
Model size PPL
1w1t + 2w2t + 3w3t + 4w4t (*) 294MB 147.1
(*) + 4w3t + 3w2t 579MB 143.5
(*) + 4w3t + 3w4t 587MB 144.9
(*) + 4w3t + 3w4t + 3w2t + 2w3t 699MB 140.7
(*) + 1 ? bernoulli-rnd 464MB 149.7
(*) + 2 ? bernoulli-rnd 632MB 150.4
(*) + 3 ? bernoulli-rnd 804MB 151.1
(*) + 1 ? data-rnd 484MB 147.0
(*) + 2 ? data-rnd 673MB 145.0
(*) + 3 ? data-rnd 864MB 145.2
Table 4: Perplexity results using the standard syntactic
model with additional trees. ?bernoulli-rnd? and ?data-
rnd? indicate fourgram trees randomized using Bernoulli
trials and varying training data, respectively. The second
column shows the combined size of decision trees in the
forest.
ders (words w and tags t independently), so 3w2t
indicates a decision tree implementing the probabil-
ity function: p(witi|wi?1wi?2ti?1). The fourgram
joint model presented in Table 1 has four trees and
is labeled with the formula ?1w1t + 2w2t + 3w3t +
4w4t? in Table 4. The randomly grown trees (de-
noted ?bernoulli-rnd?) are grown utilizing the full
context 4w4t using the methods described in Sec-
tion 4.2. All models utilize the generalized interpo-
lation method described in Section 3.2.
As can be seen in Table 4, adding undegraded
trees consistently improves the performance of an
already strong baseline, while adding random trees
only increases the perplexity because their quality
is worse than undegraded trees?. Trees produced
by data randomization (denoted ?data-rnd?) also im-
prove the performance of the model; however, the
improvement is not greater than that of additional
lower order trees, which are considerably smaller in
size.
5 ASR Rescoring Results
In order to verify that the improvements in perplex-
ity that we observe in Tables 1 and 4 are sufficient
for an impact on a task, we measure Word Error
Rate (WER) of our models on an Automatic Speech
Recognition (ASR) rescoring task using the Wall
Street Journal corpus (WSJ) for evaluation. The test
set consists of 4,088 utterances of WSJ0. We opti-
Model PPL WER
n-gram 161.7 7.81%
1w1t + 2w2t + 3w3t + 4w4t (Eq.8) 156.5 7.57%
1w1t + 2w2t + 3w3t + 4w4t (*) 147.1 7.32%
(*) + 4w3t + 3w4t + 3w2t + 2w3t 140.7 7.20%
Table 5: Perplexity and WER results. Note that the last
two rows are syntactic models using the interpolation
method of Eq. 9.
mized the weights for the combination of acoustic
and language model scores on a separate develop-
ment set comprised of 1,243 utterances from Hub2
5k closed vocabulary and the WSJ1 5k open vocab-
ulary sets.
The ASR system used to produce lattices is based
on the 2007 IBM Speech transcription system for the
GALE Distillation Go/No-go Evaluation (Chen et
al., 2006). The acoustic models are state-of-the-art
discriminatively trained models which are trained on
Broadcast News (BN) Hub4 acoustic training data.
Lattices were produced using a trigram LM trained
on the same data as the models we evaluate, then
1,000 best unique hypotheses were extracted from
the lattices. WER of the 1-best hypothesis on the
test set is 8.07% and the oracle WER is 3.54%.
In Table 5, we present WER results along with
the corresponding perplexity numbers from Ta-
bles 1 and 4 for our lowest perplexity syntactic
model, as well as the baselines (modified KN n-gram
model and standard decision tree models using in-
terpolation methods of Eq. 8 and Eq. 9). The in-
terpolation method of Eq. 9 substantially improves
performance over the interpolation method of Eq. 8,
reducing WER by 0.25% absolute (p < 10?5).
Adding four trees utilizing context restricted in dif-
ferent ways further reduces WER by 0.12%, which
is also a statistically significant (p < 0.025) im-
provement over the baseline models labeled (*). Al-
together, the improvements over the n-gram baseline
add up to 0.61% absolute (8% relative) WER reduc-
tion.
6 Conclusion
In this paper, we investigate various aspects of com-
bining multiple decision trees in a single language
model. We observe that the generalized interpola-
698
tion (Eq. 9) for decision tree models proposed in
(Filimonov and Harper, 2011) is in fact a forest in-
terpolation method rather than a backoff interpola-
tion because, in Eq. 9, models do not have explicit
higher-lower order relation as they do in backoff in-
terpolation (Eq. 6). Thus, in this paper we investi-
gate the question of how to construct decision trees
so that their combination results in improved per-
formance (under the assumption that computational
tractability allows only a handful of decision trees
in a forest). We compare various techniques for
producing forests of trees and observe that methods
that diversify trees by introducing random degrada-
tion of the tree construction algorithm perform more
poorly (especially with joint models) than methods
in which the trees are constructed without degrada-
tion and with variability being introduced via param-
eters that are inherently arbitrary (e.g., training data
fold differences or initializations of greedy search
algorithms). Additionally, we observe that simply
restricting the context used to construct trees in dif-
ferent ways, not only produces smaller trees (be-
cause of the context reduction), but the resulting
variations in trees also produce forests that are at
least as good as forests of larger trees.
7 Acknowledgments
We would like to thank Ariya Rastrow for providing
word lattices for the ASR rescoring experiments.
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical lan-
guage model for natural language speech recognition.
Readings in speech recognition, pages 507?514.
Srinivas Bangalore. 1996. ?Almost parsing? technique
for language modeling. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
volume 2, pages 1173?1176.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proceedings of HLT/NAACL, pages 4?6.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling for speech recognition. CoRR.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proceedings of the 34th annual meeting on As-
sociation for Computational Linguistics, pages 310?
318.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596?1608.
Denis Filimonov and Mary Harper. 2009. A joint lan-
guage model with fine-grain syntactic tags. In Pro-
ceedings of the EMNLP 2009.
Denis Filimonov and Mary Harper. 2011. Generalized
interpolation in decision tree LM. In Proceedings of
the 49st Annual Meeting of the Association for Com-
putational Linguistics.
Peter Heeman. 1998. POS tagging versus classes in lan-
guage modeling. In Sixth Workshop on Very Large
Corpora.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP 2009.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering. In
Speech Communication, pages 1253?1256.
J. R. Quinlan. 1986. Induction of decision trees. Ma-
chine Learning, 1(1):81?106.
Ronald Rosenfeld, Jaime Carbonell, and Alexander Rud-
nicky. 1994. Adaptive statistical language modeling:
A maximum entropy approach. Technical report.
Peng Xu. 2005. Random Forests and Data Sparseness
Problem in Language Modeling. Ph.D. thesis, Balti-
more, Maryland, April.
Imed Zitouni. 2007. Backoff hierarchical class n-
gram language models: effectiveness to model unseen
events in speech recognition. Computer Speech &
Language, 21(1):88?104.
699
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 37?45,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Appropriately Handled Prosodic Breaks Help PCFG Parsing
Zhongqiang Huang1, Mary Harper1,2
1Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park, MD USA
2Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
{zqhuang,mharper}@umiacs.umd.edu
Abstract
This paper investigates using prosodic infor-
mation in the form of ToBI break indexes for
parsing spontaneous speech. We revisit two
previously studied approaches, one that hurt
parsing performance and one that achieved
minor improvements, and propose a new
method that aims to better integrate prosodic
breaks into parsing. Although these ap-
proaches can improve the performance of ba-
sic probabilistic context free grammar (PCFG)
parsers, they all fail to produce fine-grained
PCFG models with latent annotations (PCFG-
LA) (Matsuzaki et al, 2005; Petrov and Klein,
2007) that perform significantly better than the
baseline PCFG-LA model that does not use
break indexes, partially due to mis-alignments
between automatic prosodic breaks and true
phrase boundaries. We propose two alterna-
tive ways to restrict the search space of the
prosodically enriched parser models to the n-
best parses from the baseline PCFG-LA parser
to avoid egregious parses caused by incor-
rect breaks. Our experiments show that all
of the prosodically enriched parser models can
then achieve significant improvement over the
baseline PCFG-LA parser.
1 Introduction
Speech conveys more than a sequence of words to
a listener. An important additional type of informa-
tion that phoneticians investigate is called prosody,
which includes phenomena such as pauses, pitch,
energy, duration, grouping, and emphasis. For a
review of the role of prosody in processing spo-
ken language, see (Cutler et al, 1997). Prosody
can help with the disambiguation of lexical meaning
(via accents and tones) and sentence type (e.g., yes-
no question versus statement), provide discourse-
level information like focus, prominence, and dis-
course segment, and help a listener to discern a
speaker?s emotion or hesitancy, etc. Prosody often
draws a listener?s attention to important information
through contrastive pitch or duration patterns associ-
ated words or phrases. In addition, prosodic cues can
help one to segment speech into chunks that are hy-
pothesized to have a hierarchical structure, although
not necessarily identical to that of syntax. This sug-
gests that prosodic cues may help in the parsing of
speech inputs, the topic of this paper.
Prosodic information such as pause length, du-
ration of words and phones, pitch contours, en-
ergy contours, and their normalized values have
been used in speech processing tasks like sentence
boundary detection (Liu et al, 2005). In contrast,
other researchers use linguistic encoding schemes
like ToBI (Silverman et al, 1992), which encodes
tones, the degree of juncture between words, and
prominence symbolically. For example, a simplified
ToBI encoding scheme uses the symbol 4 for ma-
jor intonational breaks, p for hesitation, and 1 for all
other breaks (Dreyer and Shafran, 2007). In the lit-
erature, there have been several attempts to integrate
prosodic information to improve parse accuracy of
speech transcripts. These studies have used either
quantized acoustic measurements of prosody or au-
tomatically detected break indexes.
Gregory et al (2004) attempted to integrate quan-
tized prosodic features as additional tokens in the
same manner that punctuation marks are added
into text. Although punctuation marks can signif-
icantly improve parse accuracy of newswire text,
the quantized prosodic tokens were found harm-
37
ful to parse accuracy when inserted into human-
generated speech transcripts of the Switchboard cor-
pus. The authors hypothesized that the inserted
pseudo-punctuation break n-gram dependencies in
the parser model, leading to lower accuracies. How-
ever, another possible cause is that the prosody has
not been effectively utilized due to the fact that
it is overloaded; it not only provides information
about phrases, but also about the state of the speaker
and his/her sentence planning process. Hence, the
prosodic information may at times be more harmful
than helpful to parsing performance.
In a follow-on experiment, Kahn et al (2005), in-
stead of using raw quantized prosodic features, used
three classes of automatically detected ToBI break
indexes (1, 4, or p) and their posteriors. Rather than
directly incorporating the breaks into the parse trees,
they used the breaks to generate additional features
for re-ranking the n-best parse trees from a gener-
ative parsing model trained without prosody. They
were able to obtain a significant 0.6% improvement
on Switchboard over the generative parser, and a
more modest 0.1% to 0.2% improvement over the
reranking model that also utilizes syntactic features.
Dreyer and Shafran (2007) added prosodic breaks
into a generative parsing model with latent vari-
ables. They utilized three classes of ToBI break in-
dexes (1, 4, and p), automatically predicted by the
approach described in (Dreyer and Shafran, 2007;
Harper et al, 2005). Breaks were modeled as a se-
quence of observations parallel to the sentence and
each break was generated by the preterminal of the
preceding word, assuming that the observation of a
break, b, was conditionally independent of its pre-
ceding word, w, given preterminal X:
P (w, b|X) = P (w|X)P (b|X) (1)
Their approach has advantages over (Gregory et al,
2004) in that it does not break n-gram dependencies
in parse modeling. It also has disadvantages in that
the breaks are modeled by preterminals rather than
higher level nonterminals, and thus cannot directly
affect phrasing in a basic PCFG grammar. How-
ever, they addressed this independence drawback by
splitting each nonterminal into latent tags so that the
impact of prosodic breaks could be percolated into
the phrasing process through the interaction of la-
tent tags. They achieved a minor 0.2% improvement
over their baseline model without prosodic cues and
also found that prosodic breaks can be used to build
more compact grammars.
In this paper, we re-investigate the models of
(Gregory et al, 2004) and (Dreyer and Shafran,
2007), and propose a new way of modeling that
can potentially address the shortcomings of the two
previous approaches. We also attribute part of the
failure or ineffectiveness of the previously investi-
gated approaches to errors in the quantized prosodic
tokens or automatic break indexes, which are pre-
dicted based only on acoustic cues and could mis-
align with phrase boundaries. We illustrate that
these prosodically enriched models are in fact highly
effective if we systematically eliminate bad phrase
and hesitation breaks given their projection onto the
reference parse trees. Inspired by this, we pro-
pose two alternative rescoring methods to restrict
the search space of the prosodically enriched parser
models to the n-best parses from the baseline PCFG-
LA parser to avoid egregious parse trees. The effec-
tiveness of our rescoring method suggests that the
reranking approach of (Kahn et al, 2005) was suc-
cessful not only because of their prosodic feature de-
sign, but also because they restrict the search space
for reranking to n-best lists generated by a syntactic
model alone.
2 Experimental Setup
Due to our goal of investigating the effect of
prosodic information on the accuracy of state of the
art parsing of conversational speech, we utilize both
Penn Switchboard (Godfrey et al, 1992) and Fisher
treebanks (Harper et al, 2005; Bies et al, 2006), for
which we also had automatically generated break in-
dexes from (Dreyer and Shafran, 2007; Harper et al,
2005)1. The Fisher treebank is a higher quality pars-
ing resource than Switchboard due to its greater use
of audio and refined specifications for sentence seg-
mentation and disfluency markups, and so we utilize
its eval set for our parser evaluation; the first 1,020
trees (7,184 words) were used for development and
the remaining 3,917 trees (29,173 words) for eval-
uation. We utilized the Fisher dev1 and dev2 sets
containing 16,519 trees (112,717 words) as the main
training data source and used the Penn Switchboard
1A small fraction of words in the Switchboard treebank
could not be aligned with the break indexes that were produced
based on a later refinement of the transcription. We chose not
to alter the Switchboard treebank, so in cases of missing break
values, we heuristically added break *1* to words in the middle
of a sentence and *4* to words that end a sentence.
38
treebank containing 110,504 trees (837,863 words)
as an additional training source to evaluate the ef-
fect of training data size on parsing performance.
The treebank trees are normalized by downcasing
all terminal strings and deleting punctuation, empty
nodes, and nonterminal-yield unary rules that are not
related to edits.
We will compare2 three prosodically enriched
PCFG models described in the next section, with a
baseline PCFG parser. We will also utilize a state
of the art PCFG-LA parser (Petrov and Klein, 2007;
Huang and Harper, 2009) to examine the effect of
prosodic enrichment3. Unlike (Kahn et al, 2005),
we do not remove EDITED regions prior to parsing
because parsing of EDITED regions is likely to ben-
efit from prosodic information. Also, parses from all
models are compared with the gold standard parses
in the Fisher evaluation set using SParseval bracket
scoring (Harper et al, 2005; Roark et al, 2006)
without flattening the EDITED constituents.
3 Methods of Integrating Breaks
Rather than using quantized raw acoustic features as
in (Gregory et al, 2004), we use automatically gen-
erated ToBI break indexes as in (Dreyer and Shafran,
2007; Kahn et al, 2005) as the prosodic cues, and
investigate three alternative methods of modeling
prosodic breaks. Figure 1 shows parse trees for the
four models for processing the spontaneous speech
transcription she?s she would do, where the speaker
hesitated after saying she?s and then resumed with
another utterance she would do. Each word input
into the parser has an associated break index repre-
sented by the symbol 1, 4, or p enclosed in asterisks
indicating the break after the word. The automat-
ically detected break *4* after the contraction is a
strong indicator of an intonational phrase boundary
that might provide helpful information for parsing if
modeled appropriately. Figure 1 (a) shows the ref-
erence parse tree (thus the name REGULAR) where
the break indexes are not utilized.
The first method to incorporate break indexes,
BRKINSERT, shown in Figure 1 (b), treats the *p*
and *4* breaks as tokens, placing them under the
2We use Bikel?s randomized parsing evaluation comparator
to determine the significance (p < 0.005) of the difference be-
tween two parsers? outputs.
3Due to the randomness of parameter initialization in the
learning of PCFG-LA models with increasing numbers of latent
tags, we train each latent variable grammar with 10 different
seeds and report the average F score on the evaluation set.
would
MD
do
VB
VP
VP
she
PRP
NP
?s
VBZ
VP
she
PRP
NP
S
EDITED
S
*4* *4**1* *1* *1*
MD VB
VP
VP
PRP
NP
VBZ
VP
PRP
NP
S
EDITED
S
BREAK BREAK
would doshe?sshe *4* *4**1* *1* *1*
(a) REGULAR (b) BRKINSERT
would
MD
do
VB
VP
VP
she
PRP
NP
?s
VBZ
VP
she
PRP
NP
S
EDITED
S
*4* *4**1* *1* *1* would
MD
do
VB
VP
VP
she
PRP
NP
?s
VBZ
VP
she
PRP
NP
S
EDITED
S
*4* *4**1* *1* *1*
(c) BRKPOS (d) BRKPHRASE
Figure 1: Modeling Methods
highest nonterminal nodes so that the order of words
and breaks remain unchanged in the terminals. This
is similar to (Gregory et al, 2004), except that auto-
matically generated ToBI breaks are used rather than
quantized raw prosodic tokens.
The second method, BRKPOS, shown in Fig-
ure 1 (c), treats breaks as a sequence of observa-
tions parallel to the words in the sentence as in
(Dreyer and Shafran, 2007). The dotted edges in
Figure 1 (c) represent the relation between pretermi-
nals and prosodic breaks, and we call them prosodic
rewrites, with analogy to grammar rewrites and lex-
ical rewrites. The generation of words and prosodic
breaks is assumed to be conditionally independent
given the preterminal, as in Equation 1.
The third new method, BRKPHRASE, shown in
Figure 1 (d), also treats breaks as a sequence of ob-
servations parallel to the sentence; however, rather
than associating the prosodic breaks with the preter-
minals, each is generated by the highest nonterminal
(including preterminal) in the parse tree that covers
the preceding word as the right-most terminal. The
observation of break, b, is assumed to be condition-
ally independent of grammar or lexical rewrite, r,
given the nonterminal X:
P (r, b|X) = P (r|X)P (b|X) (2)
The relation is indicated by the dotted edges in Fig-
ure 1 (d), and it is also called a prosodic rewrite.
The potential advantage of BRKPHRASE is that it
does not break or fragment n-gram dependencies of
the grammar rewrites, as in the BRKINSERT method,
and it directly models the dependency between
breaks and phrases, which the BRKPOS method ex-
plicitly lacks.
4 Model Training
Since automatically generated prosodic breaks are
incorporated into the parse trees deterministi-
39
cally for all of the three enrichment methods
(BRKINSERT, BRKPOS, and BRKPHRASE), train-
ing a basic PCFG is straightforward; we simply pull
the counts of grammar rules, lexical rewrites, or
prosodic rewrites from the treebank and normalize
them to obtain their probabilities.
As is well known in the parsing community, the
basic PCFG does not provide state-of-the-art per-
formance due to its strong independence assump-
tions. We can relax these assumptions by explicitly
incorporating more information into the conditional
history, as in Charniak?s parser (Charniak, 2000);
however, this would require sophisticated engineer-
ing efforts to decide what to include in the history
and how to smooth probabilities appropriately due
to data sparsity. In this paper, we utilize PCFG-LA
models (Matsuzaki et al, 2005; Petrov and Klein,
2007) that split each nonterminal into a set of latent
tags and learn complex dependencies among the la-
tent tags automatically during training. The result-
ing model is still a PCFG, but it is probabilistically
context free on the latent tags, and the interaction
among the latent tags is able to implicitly capture
higher order dependencies among the original non-
terminals and observations. We follow the approach
in (Huang and Harper, 2009) to train the PCFG-LA
models.
5 Parsing
In a basic PCFG without latent variables, the goal
of maximum probability parsing is to find the most
likely parse tree given a sentence based on the gram-
mar. Suppose our grammar is binarized (so it con-
tains only unary and binary grammar rules). Given
an input sentence wn1 = w1, w2, ? ? ? , wn, the inside
probability, P (i, j, X), of the most likely sub-tree
that is rooted at nonterminal X and generates sub-
sequence wji can be computed recursively by:
P (i, j, X) = max(max
Y
P (i, j, Y )P (X ? Y ),
max
i<k<j,Y,Z
P (i, k, Y )P (k + 1, j, Z)P (X ? Y Z)) (3)
Backtracing the search process then returns the most
likely parse tree for the REGULAR grammar.
The same parsing algorithm can be directly ap-
plied to the BRKINSERT grammar given that the
break indexes are inserted appropriately into the in-
put sentence as additional tokens. Minor modifica-
tion is needed to extend the same parsing algorithm
to the BRKPOS grammar. The only difference is that
the inside probability of a preterminal is set accord-
ing to Equation 1. The rest of the algorithm proceeds
as in Equation 3.
However, parsing with the BRKPHRASE grammar
is more complicated because whether a nonterminal
generates a break or not is determined by whether
it is the highest nonterminal that covers the preced-
ing word as its right-most terminal. In this case,
the input observation also contains a sequence of
break indexes bn1 = b1, b2, ? ? ? , bn that is parallel
to the input sentence wn1 = w1, w2, ? ? ? , wn. Let
P (i, j, X, 0) be the probability of the most likely
sub-tree rooted at nonterminal X over span (i, j)
that generates word sequence wji , as well as break
index sequence bj?1i , excluding bj . According to
the independence assumption in Equation 2, with
the addition of prosodic edge X ? bj , the same
sub-tree also has the highest probability, denoted by
P (i, j, X, 1), of generating word sequence wji to-
gether with the break index sequence bji . Thus we
have:
P (i, j, X, 1) = P (i, j, X, 0)P (bj |X) (4)
The structural constraint that a break index is only
generated by the highest nonterminal that covers
the preceding word as the right-most terminal en-
ables a dynamic programming algorithm to compute
P (i, j, X, 0) and thus P (i, j, X, 1) efficiently. If the
sub-tree (without the prosodic edge that generates
bj) over span (i, j) is constructed from a unary rule
rewrite X ? Y , then the root nonterminal Y of
some best sub-tree over the same span (i, j) can not
generate break bj because it has a higher nontermi-
nal X that also covers word wj as its right-most ter-
minal. If the sub-tree is constructed from a binary
rule rewrite X ? Y Z, then the root nonterminal Y
of some best sub-tree over some span (i, k) will gen-
erate break bk because Y is the highest nonterminal
that covers word wk as the right-most terminal4. In
contrast, the root nonterminal Z of some best sub-
tree over some span (k+1, j) can not generate break
bj because Z has a higher nonterminal X that also
covers word wj as its right-most terminal. Hence,
4Use of left-branching is required for the BRKPHRASE
method to ensure that the prosodic breaks are associated with
the original nonterminals, not intermediate nonterminals in-
troduced by binarization. Binarization is needed for efficient
parametrization of PCFG-LA models and left- versus right-
branching binarization does not significantly affect model per-
formance; hence, we use left-branching for all models.
40
P (i, j, X, 1) and P (i, j, X, 0) can be computed re-
cursively by Equation 4 above and Equation 5 be-
low:
P (i, j, X, 0) = max(max
Y
P (i, j, Y, 0)P (X ? Y ),
max
i<k<j,Y,Z
P (i, k, Y, 1)P (k + 1, j, Z, 0)P (X ? Y Z)) (5)
Although dynamic programming algorithms exist
for maximum probability decoding of basic PCFGs
without latent annotations for all four methods, it is
an NP hard problem to find the most likely parse tree
using PCFG-LA models. Several alternative decod-
ing algorithms have been proposed in the literature
for parsing with latent variable grammars. We use
the best performing max-rule-product decoding al-
gorithm, which searches for the best parse tree that
maximizes the product of the posterior rule (either
grammar, lexical, or prosodic) probabilities, as de-
scribed in (Petrov and Klein, 2007) for our models
with latent annotations and extend the dynamic pars-
ing algorithm described in Equation 5 for the BRK-
PHRASE grammar with latent annotations.
6 Results on the Fisher Corpus
6.1 Prosodically Enriched Models
Table 1 reports the parsing accuracy of the four basic
PCFGs without latent annotations when trained on
the Fisher training data. All of the grammars have a
low F score of around 65% due to the overly strong
and incorrect independence assumptions. We ob-
serve that the BRKPHRASE grammar benefits most
from breaks, significantly improving the baseline
accuracy from 64.9% to 67.2%, followed by the
BRKINSERT grammar, which at 66.2% achieves a
smaller improvement. The BRKPOS grammar ben-
efits the least among the three because breaks are
attached to the preterminals and thus have less im-
pact on phrasing due to the independence assump-
tions in the basic PCFG. In contrast, both the BRK-
PHRASE and BRKINSERT methods directly model
the relationship between breaks and phrase bound-
aries through governing nonterminals; however, the
BRKPHRASE method does not directly change any
of the grammar rules in contrast to the BRKINSERT
method that more or less breaks n-gram dependen-
cies and fragments rule probabilities.
The bars labeled DIRECT in Figure 2 report the
parsing performance of the four PCFG-LA models
trained on Fisher. The introduction of latent anno-
tations significantly boosts parsing accuracies, pro-
viding relative improvements ranging from 16.8%
REGULAR BRKINSERT BRKPOS BRKPHRASE
64.9 66.2 65.2 67.2
Table 1: Fisher evaluation parsing results for the basic
PCFGs without latent annotations trained on the Fisher
training set.
up to 19.0% when trained on Fisher training data
due to the fact that the PCFG-LA models are able
to automatically learn more complex dependencies
not captured by basic PCFGs.
 82.5
 83.5
 84.5
 85.5
Regular BrkInsert BrkPos BrkPhrase
83.9
83.2
84.2 84.2
84.4
84.0
85.0
84.5
84.7
84.0
85.1
84.7 84.8
Direct
Oracle
OracleRescore
DirectRescore
Figure 2: Parsing results on the Fisher evaluation set
of the PCFG-LA models trained on the Fisher training
data. The DIRECT bars represent direct parsing results for
models trained and evaluated on the original data, ORA-
CLE bars for models trained and evaluated on the modi-
fied oracle data (see Subsection 6.2), and the ORACLE-
RESCORE and DIRECTRESCORE bars for results of the
two rescoring approaches (described in Subsection 6.3)
on the original evaluation data.
However, the prosodically enriched methods do
not significantly improve upon the REGULAR base-
line after the introduction of latent annotations. The
BRKPHRASE method only achieves a minor in-
significant 0.1% improvement over the REGULAR
baseline; whereas, the BRKINSERT method is a sig-
nificant 0.7% worse than the baseline. Similar re-
sults for BRKINSERT were reported in (Gregory et
al., 2004), where they attributed the degradation to
the fact that the insertion of the prosodic ?punctua-
tion? breaks the n-gram dependencies. Another pos-
sible cause is that the insertion of ?bad? breaks that
do not align with true phrase boundaries hurts per-
formance more than the benefits gained from ?good?
breaks due to the tightly integrated relationship be-
tween phrases and breaks. For the BRKPOS method,
the impact of break indexes is implicitly percolated
to the nonterminals through the interaction among
latent tags, as discussed in (Dreyer and Shafran,
2007), and its performance may thus be less affected
by the ?bad? breaks. With latent annotations (in con-
trast to the basic PCFG), the model is now signif-
icantly better than BRKINSERT and is on par with
BRKPHRASE.
41
6.2 Models with Oracle Breaks
In order to determine whether ?bad? breaks limit
the improvements in parsing performance from
prosodic enrichment, we conducted a simple ora-
cle experiment where all *p* and *4* breaks that
did not align with phrase boundaries in the tree-
bank were systematically converted to *1* breaks5.
When trained and evaluated on this modified ora-
cle data, all three prosodically enriched latent vari-
able models improve by about 1% and were then
able to achieve significant improvements over the
REGULAR PCFG-LA baseline, as shown by the bars
labeled ORACLE in Figure 2. It should be noted,
however, that the BRKINSERT method is much less
effective than the other two methods in the oracle
experiment, suggesting that broken n-gram depen-
dencies affect the model in addition to the erroneous
breaks.
6.3 N-Best Re-Scoring
As mentioned previously, prosody does not only
provide information about phrases, but also about
the state of the speaker and his/her sentence plan-
ning process. Given that our break detector uti-
lizes only acoustic knowledge to predict breaks, the
recognized *p* and *4* breaks may not correctly
reflect hesitations and phrase boundaries. Incor-
rectly recognized breaks could hurt parsing more
than the benefit brought from the correctly recog-
nized breaks, as demonstrated by superior perfor-
mance of the prosodically enhanced models in the
oracle experiment. We next describe two alternative
methods to make better use of automatic breaks.
In the first approach, which is called ORACLE-
RESCORE, we train the prosodically enhanced
grammars on cleaned-up break-annotated training
data, where misclassified *p* and *4* breaks are
converted to *1* breaks (as in the oracle experi-
ment). If these grammars were used to directly parse
the test sentences with automatically detected (un-
modified) breaks, the results would be quite poor
due to mismatch between the training and testing
conditions. However, we can automatically bias
against potentially misclassified *p* and *4* breaks
if we utilize information provided by n-best parses
from the baseline REGULAR PCFG-LA grammar.
5Other sources of errors include misclassification of *p*
breaks as *1* or *4* and misclassification of *4* breaks as *1*
or *p*. Although these errors are not repaired in the oracle ex-
periment, fixing them could potentially provide greater gains.
For each hypothesized parse tree in the n-best list,
the *p* and *4* breaks that do not align with the
phrase boundaries of the hypothesized parse tree are
converted to *1* breaks, and then a new score is
computed using the product of posterior rule proba-
bilities6, as in the max-rule-product criterion, for the
hypothesized parse tree using the grammars trained
on the cleaned-up training data. In this approach,
we convert the posterior probability, P (T |W, B),
of parse tree T given words W and breaks B
to P (B?|W, B)P (T |W, B?), where B? is the new
break sequence constrained by T , and simplify it to
P (T |W, B?), assuming that conversions to a new se-
quence of breaks as constrained by a hypothesized
parse tree are equally probable given the original se-
quence of breaks. We consider this to be a reason-
able assumption for a small n-best (n = 50) list with
reasonably good quality.
In the second approach, called DIRECTRESCORE,
we train the prosodically enhanced PCFG-LA mod-
els using unmodified, automatic breaks, and then
use them to rescore the n-best lists produced by
the REGULAR PCFG-LA model to avoid the poorer
parse trees caused by fully trusting automatic break
indexes. The size of the n-best list should not be too
small or too large, or the results would be like di-
rectly parsing with REGULAR when n = 1 or with
the prosodically enriched model when n ? ?.
The ORACLERESCORE and DIRECTRESCORE
bars in Figure 2 report the performance of the
prosodically enriched models with the correspond-
ing rescoring method. Both methods use the same
50-best lists produced by the baseline REGULAR
PCFG-LA model using the max-rule-product cri-
terion. Both rescoring methods produce signifi-
cant improvements in the performance of all three
prosodically enriched PCFG-LA models. The pre-
viously ineffective (0.7% worse than REGULAR)
BRKINSERT PCFG-LA model is now 0.3% and
0.5% better than the REGULAR baseline using
the ORACLERESCORE and DIRECTRESCORE ap-
proaches, respectively. The best performing BRK-
POS and BRKPHRASE rescoring models are 0.6-
0.9% better than the REGULAR baseline. It is in-
teresting to note that rescoring with models trained
on cleaned up prosodic breaks is somewhat poorer
6The product of posterior rule probabilities of a parse tree
is more suitable for rescoring than the joint probability of the
parse tree and the observables (words and breaks) because the
breaks are possibly different for different trees.
42
than models trained using all automatic breaks.
7 Models with Augmented Training Data
Figure 3 reports the evaluation results for mod-
els that are trained on the combination of Fisher
and Switchboard training data. With the additional
Switchboard training data, the nonterminals can be
split into more fine-grained latent tags, enabling the
learning of deeper dependencies without over-fitting
the limited sized Fisher training data. This improved
all models by at least 2.6% absolute. Note also that
the patterns observed for models trained using the
larger training set are quite similar to those from us-
ing the smaller training set in Figure 2. The prosod-
ically enriched models all benefit significantly from
the oracle breaks and from the rescoring methods.
The BRKPOS and BRKPHRASE methods, with the
additional training data, also achieve significant im-
provements over the REGULAR baseline without
rescoring.
 85
 86
 87
 88
Regular BrkInsert BrkPos BrkPhrase
86.5
86.3
87.4
86.8
87.1
86.8
87.7
87.2 87.3
86.8
87.5
87.2 87.3
Direct
Oracle
OracleRescore
DirectRescore
Figure 3: Parsing results on the Fisher evaluation set of
the PCFG-LA models trained on the Fisher+Switchboard
training data.
8 Error Analysis
In this section, we compare the errors of the
BRKPHRASE PCFG-LA model and the DIRECT-
RESCORE approach for that model to each other and
to the baseline PCFG-LA model without prosodic
breaks. All models are trained and tested on Fisher
as in Section 6. The results using other prosodically
enhanced PCFG-LA models and their rescoring al-
ternatives show similar patterns.
Figure 4 depicts the difference in F scores be-
tween BRKPHRASE and REGULAR and between
BRKPHRASE+DIRECTRESCORE and REGULAR on
a tree-by-tree basis in a 2D plot. Each quad-
rant also contains +/? signs roughly describing how
much BRKPHRASE+DIRECTRESCORE is better (+)
or worse (?) than BRKPHRASE and a pair of num-
bers (a, b), in which a represents the percentage of
sentences in that quadrant containing *p* or *4*
-20
-15
-10
-5
 0
 5
 10
 15
 20
-20 -15 -10 -5  0  5  10  15  20
F(
B
rk
P
h
ra
se
+
D
ir
ec
tR
es
co
re
)-
F(
R
eg
u
la
r)
F(BrkPhrase)-F(Regular)
-
(47.2%, 25.3%)
+++
(70.2%, 30.0%)
++
(48.2%, 27.6%)
---
(66.7%, 28.1%)
Figure 4: 2D plot of the difference in F scores be-
tween BRKPHRASE and REGULAR and between BRK-
PHRASE+DIRECTRESCORE and REGULAR, on a tree-
by-tree basis, where each dot represents a test sentence.
Each quadrant also contains +/? signs roughly describ-
ing how much BRKPHRASE+DIRECTRESCORE is better
(+) or worse (?) than BRKPHRASE and a pair of numbers
(a, b), in which a represents the percentage of sentences
in that quadrant containing *p* or *4* breaks that do not
align with true phrase boundaries, and b represents the
percentage of such *p* and *4* breaks among the total
number of *p* and *4* breaks in that quadrant.
breaks that do not align with true phrase bound-
aries, and b represents the percentage of such *p*
and *4* breaks among the total number of *p* and
*4* breaks in that quadrant.
Each dot in the top-right quadrant represents a
test sentence for which both BRKPHRASE and BRK-
PHRASE+DIRECTRESCORE produce better trees
than the baseline REGULAR PCFG-LA model. The
BRKPHRASE+DIRECTRESCORE approach is on av-
erage slightly worse than the BRKPHRASE method
(hence the single minus sign), although it also often
produces better parses than BRKPHRASE alone. In
contrast, the BRKPHRASE+DIRECTRESCORE ap-
proach on average makes many fewer errors than
BRKPHRASE (hence + +) as can be observed in the
bottom-left quadrant, where both approaches pro-
duce worse parse trees than the REGULAR base-
line. The most interesting quadrant is on the top-left
where the BRKPHRASE approach always produces
worse parses than the REGULAR baseline while the
BRKPHRASE+DIRECTRESCORE approach is able
to avoid these errors while producing better parses
than the baseline (hence + + +). Although the BRK-
PHRASE+DIRECTRESCORE approach can also pro-
duce worse parses than REGULAR, as in the bottom-
right quadrant (hence ? ? ?), altogether the quad-
rants suggest that, by restricting the search space
43
to the n-best lists produced by the baseline REG-
ULAR parser, the BRKPHRASE+DIRECTRESCORE
approach is able to avoid many bad parses trees
at the expense of somewhat poorer parses in cases
when BRKPHRASE is able to benefit from the full
search space.
The reader should note that the top-left quadrant
of Figure 4 has the highest percentage (70.2%) of
sentences with ?bad? *p* and *4* breaks and the
highest percentage (30.0%) of such ?bad? breaks
among all breaks. This evidence supports our argu-
ment that ?bad? breaks are harmful to parsing per-
formance and some parse errors caused by mislead-
ing breaks can be resolved by limiting the search
space of the prosodically enriched models to the
n-best lists produced by the baseline REGULAR
parser. However, the significant presence of ?bad?
breaks in the top-right quadrant also suggests that
the prosodically enriched models are able to pro-
duce better parses than the baseline despite the pres-
ence of ?bad? breaks, probably because the models
are trained on the mixture of both ?good? and ?bad?
breaks and are able to somehow learn to use ?good?
breaks while avoiding being misled by ?bad? breaks.
BRKPHRASE
REGULAR BRKPHRASE +DIRECTRESCORE
NP 90.4 90.4 90.9
VP 84.7 84.7 85.6
S 84.4 84.3 85.2
INTJ 93.0 93.4 93.4
PP 76.5 76.7 77.9
EDITED 60.4 62.2 63.3
SBAR 67.2 67.0 68.8
Table 2: F scores of the seven most frequent non-
terminals of the REGULAR, BRKPHRASE, and BRK-
PHRASE+DIRECTRESCORE models.
Table 2 reports the F scores of the seven most fre-
quent phrases for the REGULAR, BRKPHRASE, and
BRKPHRASE+DIRECTRESCORE methods trained
on Fisher. When comparing the BRKPHRASE
method to REGULAR, the break indexes help to im-
prove the score for edits most, followed by inter-
jections and prepositional phrases; however, they do
not improve the accuracy of any of the other phrases.
The BRKPHRASE+DIRECTRESCORE approach ob-
tains improvements on all of the major phrases.
Figure 5 (a) shows a reference parse tree of a
test sentence. The REGULAR approach correctly
parses the first half (omitted) of the sentence but
it fails to correctly interpret the second half (as
shown). The BRKPHRASE approach, in contrast,
is misguided by the incorrectly classified inter-
ruption point *p* after word ?has?, and so pro-
duces an incorrect parse early in the sentence. The
BRKPHRASE+DIRECTRESCORE approach is able
to provide the correct tree given the n-best list pro-
duced by the REGULAR approach, despite the break
index errors.
(a) Reference, BRKPHRASE+DIRECTRESCORE
(b) REGULAR (c) BRKPHRASE
Figure 5: Parses for like?1? has?p? anything?1? like?1?
affected?1? you?4? personally?4? or?1? anything?4?
9 Conclusions
We have investigated using prosodic information in
the form of automatically detected ToBI break in-
dexes for parsing spontaneous speech by compar-
ing three prosodic enrichment methods. Although
prosodic enrichment improves the basic PCFGs, that
performance gain disappears when latent variables
are used, partly due to the impact of misclassified
(?bad?) breaks that are assigned to words that do not
occur at phrase boundaries. However, we find that
by simply restricting the search space of the three
prosodically enriched latent variable parser models
to the n-best parses from the baseline PCFG-LA
parser, all of them attain significant improvements.
Our analysis more fully explains the positive results
achieved by (Kahn et al, 2005) from reranking with
prosodic features and suggests that the hypothesis
that inserted prosodic punctuation breaks n-gram de-
pendencies only partially explains the negative re-
sults of (Gregory et al, 2004). Our findings from
the oracle experiment suggest that integrating ToBI
classification with syntactic parsing should increase
the accuracy of both tasks.
Acknowledgments
We would like to thank Izhak Shafran for providing
break indexes for Fisher and Switchboard and for
44
comments on an earlier draft of this paper. This re-
search was supported in part by NSF IIS-0703859.
Opinions, findings, and recommendations expressed
in this paper are those of the authors and do not nec-
essarily reflect the views of the funding agency or
the institutions where the work was completed.
References
Ann Bies, Stephanie Strassel, Haejoong Lee, Kazuaki
Maeda, Seth Kulick, Yang Liu, Mary Harper, and
Matthew Lease. 2006. Linguistic resources for speech
parsing. In LREC.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In ACL.
Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.
1997. Prosody in comprehension of spoken language:
A literature review. Language and Speech.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Inter-
speech.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In ICASSP.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In NAACL.
Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,
Izhak Shafran, Matthew Lease, Yang Liu, Matthew
Snover, Lisa Yung, Anna Krasnyanskaya, and Robin
Stewart. 2005. 2005 Johns Hopkins Summer Work-
shop Final Report on Parsing and Spoken Structural
Event Detection. Technical report, Johns Hopkins
University.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In EMNLP.
Jeremy G. Kahn, Matthew Lease, Eugene Charniak,
Mark Johnson, and Mari Ostendorf. 2005. Effective
use of prosody in parsing conversational speech. In
EMNLP-HLT.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In HLT-NAACL.
Brian Roark, Mary Harper, Yang Liu, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie J. Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Sparseval: Evaluation metrics for pars-
ing speech. In LREC.
Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-
tendorf, Colin Wightman, Patti Price, Janet Pierrehum-
bert, and Julia Hirshberg. 1992. ToBI: A standard for
labeling English prosody. In ICSLP.
45
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 620?624,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Generalized Interpolation in Decision Tree LM
Denis Filimonov??
?Human Language Technology
Center of Excellence
Johns Hopkins University
den@cs.umd.edu
Mary Harper?
?Department of Computer Science
University of Maryland, College Park
mharper@umd.edu
Abstract
In the face of sparsity, statistical models are
often interpolated with lower order (backoff)
models, particularly in Language Modeling.
In this paper, we argue that there is a rela-
tion between the higher order and the backoff
model that must be satisfied in order for the
interpolation to be effective. We show that in
n-gram models, the relation is trivially held,
but in models that allow arbitrary clustering
of context (such as decision tree models), this
relation is generally not satisfied. Based on
this insight, we also propose a generalization
of linear interpolation which significantly im-
proves the performance of a decision tree lan-
guage model.
1 Introduction
A prominent use case for Language Models (LMs)
in NLP applications such as Automatic Speech
Recognition (ASR) and Machine Translation (MT)
is selection of the most fluent word sequence among
multiple hypotheses. Statistical LMs formulate the
problem as the computation of the model?s proba-
bility to generate the word sequencew1w2 . . . wm ?
wm1 , assuming that higher probability corresponds to
more fluent hypotheses. LMs are often represented
in the following generative form:
p(wm1 ) =
m?
i=1
p(wi|w
i?1
1 )
In the following discussion, we will refer to the func-
tion p(wi|w
i?1
1 ) as a language model.
Note the context space for this function, wi?11
is arbitrarily long, necessitating some independence
assumption, which usually consists of reducing the
relevant context to n? 1 immediately preceding to-
kens:
p(wi|w
i?1
1 ) ? p(wi|w
i?1
i?n+1)
These distributions are typically estimated from ob-
served counts of n-grams wii?n+1 in the training
data. The context space is still far too large; there-
fore, the models are recursively smoothed using
lower order distributions. For instance, in a widely
used n-gram LM, the probabilities are estimated as
follows:
p?(wi|w
i?1
i?n+1) = ?(wi|w
i?1
i?n+1) + (1)
?(wi?1i?n+1) ? p?(wi|w
i?1
i?n+2)
where ? is a discounted probability1.
In addition to n-gram models, there are many
other ways to estimate probability distributions
p(wi|w
i?1
i?n+1); in this work, we are particularly in-
terested in models involving decision trees (DTs).
As in n-gram models, DT models also often uti-
lize interpolation with lower order models; however,
there are issues concerning the interpolation which
arise from the fact that decision trees permit arbi-
trary clustering of context, and these issues are the
main subject of this paper.
1We refer the reader to (Chen and Goodman, 1999) for a
survey of the discounting methods for n-gram models.
620
2 Decision Trees
The vast context space in a language model man-
dates the use of context clustering in some form. In
n-gram models, the clustering can be represented as
a k-ary decision tree of depth n ? 1, where k is the
size of the vocabulary. Note that this is a very con-
strained form of a decision tree, and is probably sub-
optimal. Indeed, it is likely that some of the clusters
predict very similar distributions of words, and the
model would benefit from merging them. Therefore,
it is reasonable to believe that arbitrary (i.e., uncon-
strained) context clustering such as a decision tree
should be able to outperform the n-gram model.
A decision tree provides us with a clustering func-
tion ?(wi?1i?n+1) ? {?
1, . . . ,?N}, where N is the
number of clusters (leaves in the DT), and clusters
?k are disjoint subsets of the context space; the
probability estimation is approximated as follows:
p(wi|w
i?1
i?n+1) ? p(wi|?(w
i?1
i?n+1)) (2)
Methods of DT construction and probability estima-
tion used in this work are based on (Filimonov and
Harper, 2009); therefore, we refer the reader to that
paper for details.
Another advantage of using decision trees is the
ease of adding parameters such as syntactic tags:
p(wm1 ) =
X
t1...tm
p(wm1 t
m
1 ) =
X
t1...tm
mY
i=1
p(witi|w
i?1
1 t
i?1
1 )
?
X
t1...tm
mY
i=1
p(witi|?(w
i?1
i?n+1t
i?1
i?n+1)) (3)
In this case, the decision tree would cluster the con-
text space wi?1i?n+1t
i?1
i?n+1 based on information the-
oretic metrics, without utilizing heuristics for which
order the context attributes are to be backed off (cf.
Eq. 1). In subsequent discussion, we will write
equations for word models (Eq. 2), but they are
equally applicable to joint models (Eq. 3) with trivial
transformations.
3 Backoff Property
Let us rewrite the interpolation Eq. 1 in a more
generic way:
p?(wi|w
i?1
1 ) = ?n(wi|?n(w
i?1
1 )) + (4)
?(?n(w
i?1
1 )) ? p?(wi|BOn?1(w
i?1
1 ))
where, ?n is a discounted distribution, ?n is a clus-
tering function of order n, and ?(?n(w
i?1
1 )) is the
backoff weight chosen to normalize the distribution.
BOn?1 is the backoff clustering function of order
n ? 1, representing a reduction of context size. In
the case of an n-gram model, ?n(w
i?1
1 ) is the set
of word sequences where the last n ? 1 words are
wi?1i?n+1, similarly, BOn?1(w
i?1
1 ) is the set of se-
quences ending with wi?1i?n+2. In the case of a de-
cision tree model, the same backoff function is typ-
ically used, but the clustering function can be arbi-
trary.
The intuition behind Eq. 4 is that the backoff con-
text BOn?1(w
i?1
1 ) allows for more robust (but less
informed) probability estimation than the context
cluster ?n(w
i?1
1 ). More precisely:
?wi?11 ,W
: W ? ?n(w
i?1
1 )?W ? BOn?1(w
i?1
1 )
(5)
that is, every word sequence W that belongs to a
context cluster ?n(w
i?1
1 ), belongs to the same back-
off cluster BOn?1(w
i?1
1 ) (hence has the same back-
off distribution). For n-gram models, Property 5
trivially holds since BOn?1(w
i?1
1 ) and ?n(w
i?1
1 )
are defined as sets of sequences ending with wi?1i?n+2
and wi?1i?n+1 with the former clearly being a superset
of the latter. However, when ? can be arbitrary, e.g.,
a decision tree, that is not necessarily so.
Let us consider what happens when we have
two context sequences W and W ? that belong to
the same cluster ?n(W ) = ?n(W ?) but differ-
ent backoff clusters BOn?1(W ) 6= BOn?1(W ?).
For example: suppose we have ?(wi?2wi?1) =
({on}, {may,june}) and two corresponding backoff
clusters: BO? = ({may}) and BO?? = ({june}).
Following on, the word may is likely to be a month
rather than a modal verb, although the latter is
more frequent and will dominate in BO?. There-
fore we have much less faith in p?(wi|BO?) than in
p?(wi|BO??) and would like a much smaller weight ?
assigned to BO?, but it is not possible in the back-
off scheme in Eq. 4, thus we will have to settle on a
compromise value of ?, resulting in suboptimal per-
formance.
We would expect this effect to be more pro-
nounced in higher order models, because viola-
621
tions of Property 5 are less frequent in lower or-
der models. Indeed, in a 2-gram model, the
property is never violated since its backoff, un-
igram, contains the entire context in one clus-
ter. The 3-gram example above, ?(wi?2wi?1) =
({on}, {may,june}), although illustrative, is not
likely to occur because may in wi?1 position will
likely be split from june very early on, since it is
very informative about the following word. How-
ever, in a 4-gram model, ?(wi?3wi?2wi?1) =
({on}, {may,june}, {<unk>}) is quite plausible.
Thus, arbitrary clustering (an advantage of DTs)
leads to violation of Property 5, which, we argue,
may lead to a degradation of performance if back-
off interpolation Eq. 4 is used. In the next section,
we generalize the interpolation scheme which, as we
show in Section 6, allows us to find a better solution
in the face of the violation of Property 5.
4 Linear Interpolation
We use linear interpolation as the baseline, rep-
resented recursively, which is similar to Jelinek-
Mercer smoothing for n-gram models (Jelinek and
Mercer, 1980):
p?n(wi|w
i?1
i?n+1) = ?n(?n) ? pn(wi|?n) + (6)
(1? ?n(?n)) ? p?n?1(wi|w
i?1
i?n+2)
where ?n ? ?n(w
i?1
i?n+1), and ?n(?n) ? [0, 1] are
assigned to each cluster and are optimized on a held-
out set using EM. pn(wi|?n) is the probability dis-
tribution at the cluster ?n in the tree of order n. This
interpolation method is particularly useful as, un-
like count-based discounting methods (e.g., Kneser-
Ney), it can be applied to already smooth distribu-
tions pn2.
5 Generalized Interpolation
We can unwind the recursion in Eq. 6 and make sub-
stitutions:
?n(?n) ? ??n(?n)
(1? ?n(?n)) ? ?n?1(?n?1) ? ??n?1(?n?1)
...
2In decision trees, the distribution at a cluster (leaf) is often
recursively interpolated with its parent node, e.g. (Bahl et al,
1990; Heeman, 1999; Filimonov and Harper, 2009).
p?n(wi|w
i?1
i?n+1) =
n?
m=1
??m(?m) ? pm(wi|?m) (7)
n?
m=1
??m(?m) = 1
Note that in this parameterization, the weight as-
signed to pn?1(wi|?n?1) is limited by (1??n(?n)),
i.e., the weight assigned to the higher order model.
Ideally we should be able to assign a different set
of interpolation weights for every eligible combina-
tion of clusters ?n, ?n?1, . . . , ?1. However, not only
is the number of such combinations extremely large,
but many of them will not be observed in the train-
ing data, making parameter estimation cumbersome.
Therefore, we propose the following parameteriza-
tion for the interpolation of decision tree models:
p?n(wi|w
i?1
i?n+1) =
?n
m=1 ?m(?m) ? pm(wi|?m)?n
m=1 ?m(?m)
(8)
Note that this parameterization has the same num-
ber of parameters as in Eq. 7 (one per cluster in ev-
ery tree), but the number of degrees of freedom is
larger because the the parameters are not constrained
to sum to 1, hence the denominator.
In Eq. 8, there is no explicit distinction between
higher order and backoff models. Indeed, it ac-
knowledges that lower order models are not backoff
models when Property 5 is not satisfied. However,
it can be shown that Eq. 8 reduces to Eq. 6 if Prop-
erty 5 holds. Therefore, the new parameterization
can be thought of as a generalization of linear inter-
polation. Indeed, suppose we have the parameteri-
zation in Eq. 8 and Property 5. Let us transform this
parameterization into Eq. 7 by induction. We define:
?m ?
m?
k=1
?k ; ?m = ?m + ?m?1
where, due to space limitation, we redefine ?m ?
?m(?m) and ?m ? ?m(?m); ?m ? ?m(w
i?1
1 ),
i.e., the cluster of model order m, to which the se-
quence wi?11 belongs. The lowest order distribution
p1 is not interpolated with anything, hence:
?1p?1(wi|?1) = ?1p1(wi|?1)
Now the induction step. From Property 5, it follows
that ?m ? ?m?1, thus, for all sequences in ?wn1 ?
622
n-gram DT: Eq. 6 (baseline) DT: Eq. 8 (generalized)
order Jelinek-Mercer Mod KN word-tree syntactic word-tree syntactic
2-gram 270.2 261.0 257.8 214.3 258.1 214.6
3-gram 186.5 (31.0%) 174.3 (33.2%) 168.7 (34.6%) 156.8 (26.8%) 168.4 (34.8%) 155.3 (27.6%)
4-gram 177.1 (5.0%) 161.7 (7.2%) 164.0 (2.8%) 156.5 (0.2%) 155.7 (7.5%) 147.1 (5.3%)
Table 1: Perplexity results on PTB WSJ section 23. Percentage numbers in parentheses denote the reduction of per-
plexity relative to the lower order model of the same type. ?Word-tree? and ?syntactic? refer to DT models estimated
using words only (Eq. 2) and words and tags jointly (Eq. 3).
?m, we have the same distribution:
?mpm(wi|?m) + ?m?1p?m?1(wi|?m?1) =
= ?m
(
?m
?m
pm(wi|?m) +
?m?1
?m
p?m?1(wi|?m?1)
)
= ?m
(
??mpm(wi|?m) + (1? ??m)p?m?1(wi|?m?1)
)
= ?mp?m(wi|?m) ; ??m ?
?m
?m
Note that the last transformation is because ?m ?
?m?1; had it not been the case, p?m would depend on
the combination of ?m and ?m?1 and require multi-
ple parameters to be represented on its entire domain
wn1 ? ?m. After n iterations, we have:
n?
m=1
?m(?m)pm(wi|?m) = ?np?n(wi|?n); (cf. Eq. 8)
Thus, we have constructed p?n(wi|?n) using the
same recursive representation as in Eq. 6, which
proves that the standard linear interpolation is a spe-
cial case of the new interpolation scheme, which oc-
curs when the backoff Property 5 holds.
6 Results and Discussion
Models are trained on 35M words of WSJ 94-96
from LDC2008T13. The text was converted into
speech-like form, namely numbers and abbrevia-
tions were verbalized, text was downcased, punc-
tuation was removed, and contractions and posses-
sives were joined with the previous word (i.e., they
?ll becomes they?ll). For syntactic modeling, we
used tags comprised of POS tags of the word and its
head, as in (Filimonov and Harper, 2009). Parsing
of the text for tag extraction occurred after verbal-
ization of numbers and abbreviations but before any
further processing; we used an appropriately trained
latent variable PCFG parser (Huang and Harper,
2009). For reference, we include n-gram models
with Jelinek-Mercer and modified interpolated KN
discounting. All models use the same vocabulary of
approximately 50k words.
We implemented four decision tree models3: two
using the interpolation method of (Eq. 6) and two
based on the generalized interpolation (Eq. 8). Pa-
rameters ? were estimated using the L-BFGS to
minimize the entropy on a heldout set. In order to
eliminate the influence of all factors other than the
interpolation, we used the same decision trees. The
perplexity results on WSJ section 23 are presented in
Table 1. As we have predicted, the effect of the new
interpolation becomes apparent at the 4-gram order,
when Property 5 is most frequently violated. Note
that we observe similar patterns for both word-tree
and syntactic models, with syntactic models outper-
forming their word-tree counterparts.
We believe that (Xu and Jelinek, 2004) also suf-
fers from violation of Property 5, however, since
they use a heuristic method4 to set backoff weights,
it is difficult to ascertain the extent.
7 Conclusion
The main contribution of this paper is the insight
that in the standard recursive backoff there is an im-
plied relation between the backoff and the higher or-
der models, which is essential for adequate perfor-
mance. When this relation is not satisfied other in-
terpolation methods should be employed; hence, we
propose a generalization of linear interpolation that
significantly outperforms the standard form in such
a scenario.
3We refer the reader to (Filimonov and Harper, 2009) for
details on the tree construction algorithm.
4The higher order model was discounted according to KN
discounting, while the lower order model could be either a lower
order DT (forest) model, or a standard n-gram model, with the
former performing slightly better.
623
References
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1990. A tree-based statistical lan-
guage model for natural language speech recognition.
Readings in speech recognition, pages 507?514.
Stanley F. Chen and Joshua Goodman. 1999. An empir-
ical study of smoothing techniques for language mod-
eling. Computer Speech & Language, 13(4):359?393.
Denis Filimonov and Mary Harper. 2009. A joint lan-
guage model with fine-grain syntactic tags. In Pro-
ceedings of the EMNLP.
Peter A. Heeman. 1999. POS tags and decision trees
for language modeling. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
129?137.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the EMNLP 2009.
Frederick Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of markov source parameters from
sparse data. In Proceedings of the Workshop on Pat-
tern Recognition in Practice, pages 381?397.
Peng Xu and Frederick Jelinek. 2004. Random forests in
language modeling. In Proceedings of the EMNLP.
624
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 204?207,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Non-Expert Correction of Automatically Generated Relation Annotations
Matthew R. Gormley?? and Adam Gerber?? and Mary Harper?? and Mark Dredze ??
?Human Language Technology Center of Excellence
?Center for Language and Speech Processing
Johns Hopkins University, Baltimore, MD 21211, USA
?Laboratory for Computational Linguistics and Information Processing
University of Maryland, College Park, MD 20742 USA
mrg@cs.jhu.edu,adam.gerber@jhu.edu,mharper@umd.edu,mdredze@cs.jhu.edu
Abstract
We explore a new way to collect human an-
notated relations in text using Amazon Me-
chanical Turk. Given a knowledge base of
relations and a corpus, we identify sentences
which mention both an entity and an attribute
that have some relation in the knowledge base.
Each noisy sentence/relation pair is presented
to multiple turkers, who are asked whether the
sentence expresses the relation. We describe
a design which encourages user efficiency and
aids discovery of cheating. We also present
results on inter-annotator agreement.
1 Introduction
Relation extraction (RE) is the task of determining
the existence and type of relation between two tex-
tual entity mentions. Slot filling, a general form of
relation extraction, includes relations between non-
entities, such as a person and an occupation, age, or
cause of death (McNamee and Dang, 2009).
RE annotated data, such as ACE (2008), is expen-
sive to produce so systems take different approaches
to minimizing data needs. For example, tree kernels
can reduce feature sparsity and generalize across
many examples (GuoDong et al, 2007; Zhou et
al., 2009). Distant supervision automatically gen-
erates noisy training examples from a knowledge
base (KB) without needing annotations (Bunescu
and Mooney, 2007; Mintz et al, 2009). While
this method can quickly generate training data, it
also generates many false examples. We reduce the
noise in such examples by using Amazon Mechani-
cal Turk (MTurk), which has been shown to produce
high quality annotations for a variety of natural lan-
guage processing tasks (Snow et al, 2008).
We use MTurk for annotation of textual relations
to establish an inexpensive and rapid method of cre-
ating data for slot filling. We present a two step an-
notation process: (1) automatic creation of noisy ex-
amples, and (2) human validation of examples.
2 Method
2.1 Automatic generation of noisy examples
To create noisy examples we use a similar approach
to Mintz et al (2009). We extract relations from a
KB in the form of tuples, (e, r, v), where e is an
entity, v is a value, and r is a relation that holds
between them; for example (J.R.R. Tolkien, occu-
pation, author). Our KB is Freebase1, an online
database of structured information, and our corpus
is from the TAC KBP task (McNamee and Dang,
2009)2. For each tuple, we find sentences in a cor-
pus that contain both an exact mention of the entity
e and of the value v. Of course, such sentences may
not attest to the relation r, so the process produces
many incorrect examples.
2.2 Human Intelligence Tasks
A Human Intelligence Task (HIT) is a short paid task
on MTurk. In our HITs, we present the turker with
ten relation examples as sentence/relation pairs. For
each example, the user is asked to select from three
annotation options: the sentence (1) expresses the
relation, (2) does not express the relation, or (3) the
1http://www.freebase.com
2http://projects.ldc.upenn.edu/kbp/
204
1. The sentence expresses the relation.
Sentence: For the past eleven years, James has
lived in Tucson.
Relation: ?Tucson? is the residence of ?James?
2. The sentence does not express the relation.
Sentence: Samuel first met Divya in 1990, while
she was still a student.
Relation: ?Divya? is a spouse of ?Samuel?
3. The relation does not make sense.
Sentence: Soojin was born in January.
Relation: ?January? is the birth place of ?Soojin?
Figure 1: The three annotation options with examples.
relation does not make sense (figure 1.)
Of the ten examples that comprise each HIT,
seven are automatically generated by the method
above. The correct answer is known for the three re-
maining examples; these are included for quality as-
surance (control examples.) The three control exam-
ples are a positive example (expresses the relation,) a
negative example (contradicts the true relation,) and
a nonsense example (relation is nonsensical.)
All control examples derive from a subset of the
automatically generated person examples. Positive
examples were randomly sampled and hand anno-
tated. Negative examples are familial relations in
which we change the relation type so that it would
not be expressed in the sentence. For example,
the relation ?Barack Obama is the parent of Malia
Obama? would be changed to ?Barack Obama is a
sibling of Malia Obama.? To generate nonsense ex-
amples we employ the same method for a different
mapping of relations, which produces relations like
?New Zealand is the gender of John Key.?
2.3 HIT Design
MTurk is a marketplace so users have total freedom
in choosing which HITs to complete. As such, HIT
design should maximize its appeal. We assume that
users find appealing those HITs through which they
may maximize their own monetary gain, while mini-
mizing moment-to-moment frustrations. We empha-
sized clarity and ease of use.
The layout consists of three sections (figure 2).
The leftmost section is a progress list, which shows
the user?s answers and current position; the middle
section contains the current relation example and an-
notation options; the rightmost section (not pictured)
# HITs Cost Time (hours)
Trial 50 $2.75 27
Batch 1 500 $27.50 34
Batch 2 765 $42.08 25
Batch 3 500 $27.50 22
Total 1815 $99.83 108
Table 1: Size, cost and time to complete each HITs batch.
contains instructions. All sections and all UI ele-
ments remain visible and in the same position for the
duration of the HIT, with only the text of the sen-
tence and relation changing according to question
number. Because only a single question is displayed
at a time, we are able to minimize user actions such
as scrolling, clicking small targets, or making large
mouse movements. Additionally, we can monitor
how much time a user spends on each question.
At all times the user is able to consult the instruc-
tions for the task, which include examples of each
annotation option. The user is also reminded of the
technical requirements for the HIT and expectations
for honesty and accuracy. A comment box provides
users with the opportunity to ask questions, make
suggestions, or clarify their responses.
3 Results
We submitted a trial run and three full batches of
HITs. Table 1 summarizes the costs and completion
times for all HITs. The HITs were labeled rapidly
and for a low cost ($0.05 per HIT, i.e., .5? per anno-
tation). Each HIT was assigned to five unique work-
ers. We found that 50% of the 352 different workers
completed 2 or more HITs (figure 3.) Our results
exclude a trial run of 50 hits. Across the 17,650 ex-
amples the mean time spent was 20.77 seconds, with
a standard deviation of 99.96 seconds. The median
time per example was 10.0 seconds.
3.1 Analysis
To evaluate the annotations, two of the authors an-
notated a random sample of 247 (10%) of the 2471
noisy examples. In addition, we analyzed the work-
ers agreement with the control examples.
We used two metrics to assess agreement. The
first metric is pairwise percent agreement (Pair-
wise): the average of the example agreement scores,
where the example agreement score is the percent of
205
Figure 2: An example HIT with instructions excluded.
0 
20 
40 
60 
80 
100 
120 
140 
0 50 100
 
150
 
200
 
250
 
300
 
350
 
# H
ITs 
Workers 
Figure 3: The number of HITs per worker, with columns
sorted left to right.
pairs of annotators that agreed for a particular exam-
ple. The second metric is the exact kappa coefficient
(Exact-?) (Conger, 1980), which takes into account
that agreement can occur by chance. The number of
annotators (R) varies with the test scenario.
Table 2 presents the inter-annotator agreement
scores for various subsets of the examples and com-
binations of annotators. On a sample of examples,
we evaluated agreement between the first and sec-
ond expert annotators (E1/E2) and also the agree-
ment between each expert and the majority vote of
the workers (E1/M and E2/M). The agreement be-
tween the two experts is substantially higher than
their individual agreements with the majority. Yet,
we achieve our goal of reducing noise.
We also analyzed the agreement between the
known control answer and the majority vote of the
workers (C/M). This high level of agreement sup-
ports our belief that the automatically generated neg-
ative and nonsense examples were easier to identify
# Ex. R Exact-? Pairwise
E1/E2 247 2 0.64 0.81
E1/M 247 2 0.29 0.60
E2/M 247 2 0.39 0.70
C/M 1059 2 0.90 0.93
T(sample) 247 5 0.31 0.69
T(control) 1059 5 0.52 0.68
T(all) 3530 5 0.45 0.68
Table 2: Inter-annotator agreement
than noisy negative and nonsense examples. Finally,
we evaluated the agreement between the five work-
ers for different subsets of the data: the sample of
noisy examples (T(sample)), the control examples
only (T(control)), and all examples (T(all)). Table 3
lists the number of examples collected and the agree-
ment scores for all workers for each relation type.
Table 4 shows the divergence of the workers? an-
notations from those of an expert. The high level
of confusability for those examples which the expert
annotated as Not Expressed suggests their inherent
difficulty. The workers labeled more examples as
Expressed than the expert, but both labeled few ex-
amples as Nonsense.
4 Quality Control
We identify spurious responses and unreliable users
in two ways. First, worker responses are compared
to control examples; greater agreement with controls
should indicate greater confidence in the user. We
filtered any worker whose agreement with the con-
trols was less than 0.85 (Control Filtered). The sec-
ond approach uses behavioral data. Because only a
single example is visible at any time, we can mea-
206
Relation # Ex. Exact-? Pairwise
siblings 13 0.67 0.82
children 12 0.57 0.83
gender 80 0.46 0.70
place of death 40 0.43 0.68
parent 12 0.40 0.64
spouse 54 0.37 0.65
title 71 0.30 0.78
residences 228 0.29 0.60
ethnicity 38 0.28 0.54
occupation 551 0.26 0.77
activism 4 0.26 0.55
religion 22 0.23 0.55
place of birth 160 0.20 0.64
nationality 1044 0.19 0.67
schools attended 8 0.16 0.55
employee of 132 0.16 0.70
charges 2 0.14 0.70
Total 2471 0.35 0.69
Table 3: Inter-annotator agreement across relation type.
# Ex. is the number of noisy examples. Exact-? and Pair-
wise agreement are among the five workers.
Worker
E NE Nn Total
E
xp
er
t-
1 E 561 89 20 670
NE 284 248 28 560
Nn 1 1 3 5
Total 846 338 51 1235
Table 4: Confusion matrix of expert-1 and user?s anno-
tations on the sample of noisy examples, for the choices
Expressed (E), Not Expressed (NE), and Nonsense (Nn)
sure how much time a user spends on each exam-
ple. The UI is designed to allow for the extremely
rapid completion of examples and of the HIT in gen-
eral. Thus, a user could complete the HIT in only a
few seconds without even reading any of the exam-
ples. Still other users spend only a moment on all-
but-one question, and then several minutes on the
remaining question. Here, we filter a user answering
three or more questions each in under three seconds
(Time Filtered). We combine these two approaches
(Control and Time), which yields the highest expert-
agreement levels (table 5.)
5 Conclusion
Using non-expert annotators from Amazon Mechan-
ical Turk for the correction of noisy, automatically
E1/M E2/M
Unfiltered 0.28 0.38
Time Filtered 0.32 0.43
Control Filtered 0.34 0.47
Control and Time 0.37 0.48
Table 5: Exact-? scores for three levels of quality control
and a baseline, between each expert and the majority vote
on 231 sampled examples. For a fair comparison, we re-
duced the sample size to include only examples for which
each level of quality control had at least one worker an-
notation remaining.
generated examples is inexpensive and fast. We
achieve good inter-annotator agreement using qual-
ity assurance measures to detect cheating. The result
is thousands of new annotated slot filling example
sentences for 17 person relations.
Acknowledgments
We would like to thank the 352 turkers who made
this work possible.
References
ACE. 2008. Automatic content extraction.
http://projects.ldc.upenn.edu/ace/.
R. Bunescu and R. Mooney. 2007. Learning to extract
relations from the web using minimal supervision. In
Association for Computational Linguistics (ACL).
A.J. Conger. 1980. Integration and generalization of
kappas for multiple raters. Psychological Bulletin,
88(2):322?328.
Z. GuoDong, M. Zhang, D. H Ji, and Z. H. U. QiaoM-
ing. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In
Empirical Methods in Natural Language Processing
(EMNLP).
Paul McNamee and Hoa Dang. 2009. Overview of the
TAC 2009 knowledge base population track. In Text
Analysis Conference (TAC).
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Association for Computational Linguistics
(ACL).
R. Snow, B. O?Connor, D. Jurafsky, and A.Y. Ng. 2008.
Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Empirical
Methods in Natural Language Processing (EMNLP).
G. Zhou, L. Qian, and J. Fan. 2009. Tree kernel-based
semantic relation extraction with rich syntactic and se-
mantic information. Information Sciences.
207
