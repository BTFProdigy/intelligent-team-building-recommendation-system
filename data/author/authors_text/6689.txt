 ABSTRACT
Spoken user interfaces are conventionally either dialogue-
based or menu-based. In this paper we propose a third
approach, in which the task of invoking responses from the
system is treated as one of retrieval from the set of all possi-
ble responses. Unlike conventional spoken user interfaces
that return a unique response to the user, the proposed inter-
face returns a shortlist of possible responses, from which
the user must make the final selection. We refer to such
interfaces as Speech-In List-Out or SILO interfaces. Exper-
iments show that SILO interfaces can be very effective, are
highly robust to degraded speech recognition performance,
and can impose significantly lower cognitive load on the
user as compared to menu-based interfaces.
Keywords
Speech interfaces, information retrieval, spoken query
1. INTRODUCTION
Spoken input based user interfaces can be broadly cate-
gorized as dialogue-based interfaces and menu-selection
interfaces. In dialogue-based interfaces, the system engages
in a dialogue with the user in an attempt to determine the
user?s intention. In menu-based interfaces users traverse a
tree of menus, each node of which presents a list of possible
choices for the user. In both kinds of interfaces, the speech
recognizer must typically convert the user?s speech to an
unambiguous text string, which is then used by the UI to
determine the action it must take next. Both kinds of inter-
faces eventually respond to the user with a unique output.
In this paper we advocate a third, and different approach
to spoken user interfaces. We note that in a majority of
applications for which speech interfaces may be used, the
goal of the interaction between the user and the system is to
evoke a specific response from a limited set of possible
responses. In our approach, we view the set of possible
responses as documents in an index, and the task of obtain-
ing a specific response as that of retrieval from the set. Spo-
ken input from the user is treated as a query, which is used
to retrieve a list of potentially valid responses that are dis-
played to the user. The user must then make the final selec-
tion from the returned list. We call spoken user interfaces
based on this approach ?Speech-In List-Out? or SILO inter-
faces.
While much has been written on text-based retrieval of
spoken or multimedia documents, the topic of information
retrieval (IR) using spoken queries has not been addressed
much. The usual approach to spoken query based IR has
been to use the recognizer as a speech-to-text convertor that
generates a text string (Chang et. al, 2002; Chen et. al.,
2000) or a phoneme sequence (Kupiec et. al. 1994) which is
used to query the index. This approach is critically depen-
dent on accurate recognition of the spoken query.
In our system, however, we do not require direct con-
version of the spoken input to an unambiguous text string.
Instead, the spoken query is converted to a probabilistically
scored ?bag of words? derived from the entire hypothesis
space of the recognizer, that serves as the query to the
index. This system is able to perform effectively even when
the actual text string output by the recognizer is erroneous.
The proposed SILO approach has several advantages
over dialogue-based or menu-based interfaces. The latter
require users to know or guess what to say at each stage in
an interaction. Interactions typically follow a sequence of
steps, and the allowed responses from the user vary with
the state of the interaction. On the other hand, since the
SILO interface essentially performs information retrieval
based on the query, restrictions on the allowed language are
few, if any. Additionally, the SILO interface responds in a
single step to the user, without requiring repeated refine-
ment of the request. This simplicity of operation makes the
SILO interface measurably simpler to use than menu-based
interfaces.
Speech recognition systems often make mistakes, espe-
cially under noisy recording conditions. Recognition errors
can result in incorrect responses from user interfaces. To
improve the robustness of the UI to recognition errors, dia-
logue and menu-based systems use various techniques such
as rejection, confirmatory responses and error-correcting
dialogues. SILO interfaces do not use such techniques, and
are more reliant on getting the responses right in the first
place. This is possible in SILO interfaces because the spo-
ken query based IR technique used in them is inherently
robust to recognition errors.
The rest of this paper is arranged as follows: in Section
2 we describe the basic operation of the SILO interface. In
Section 3 we describe the spoken query based IR algorithm
used by the SILO interface. In Section 4 we describe some
example applications that use the SILO interface and
present experiments evidence of the effectiveness of SILO.
Finally in Section 5 we present our conclusions.
2. THE OPERATION OF THE SILO INTERFACE
Figure 1. demonstrates the difference between the con-
ventional spoken user interfaces and the SILO interface.
Figure 1a. shows the typical operation of a conventional
dialog or menu-based interface. The user initiates the inter-
action typically using a push-to-talk or press-to-talk button.
Thereafter the system goes through a cycle of processing
A Speech-In List-Out Approach to Spoken User Interfaces
Vijay Divi1, Clifton Forlines2, Jan Van Gemert2, Bhiksha Raj2, Bent Schmidt-Nielsen2, Kent Wittenburg2, Joseph 
Woelfel2, Peter Wolf2, Fang-Fang Zhang2
1. Massachussetts Institute of Tehcnology, Cambridge, MA, USA
2. Mitsubishi Electric Research Laboratories, Cambridge, MA, USA
vdivi@mit.edu, {forlines, gemert, bent, wittenburg, woelfel, wolf, fzhang}@merl.com
the user?s spoken input, and responding to it in some man-
ner, either with a menu, or some intermediate response of a
dialogue. After a number of cycles through this loop, the
system eventually responds with the desired action.
Conventional interfaces are predicated on the theory
that it is advantageous to obtain information about the
desired final response in an incremental manner, through
controlled exchanges in which the number of possible inter-
pretations of the user?s input is limited. In addition, dia-
logue-based interfaces also attempt to make the interaction
between the user and the system conversational.
The design of the SILO interface, on the other hand, is
based on the following premises: a) when the set of possi-
ble responses by the system is limited and can be enumer-
ated, simple information retrieval techniques are sufficient
to shortlist the possible final responses to the user. The
shortlist may bear no resemblance to the structured lists that
are derived by a menu or dialogue-based system, but will
nevertheless contain the desired response provided the IR
technique used is sufficiently robust. b) It is best to push the
final choice of response from such a list back to the user.
Figure 1b. shows the operation of the SILO interface.
The user initiates the interaction using a push-to-talk but-
ton. The system then records the user?s spoken input and
returns a ranked list of plausible responses based on the
input. The user finally selects the desired response through
a secondary selection, which may be performed using but-
tons, or even by voice. The entire operation is performed in
two steps, one in which the system retrieves a list of
choices, and the other in which the user selects from the
list.
3. INFORMATION RETRIEVAL IN SILO USING SPOKEN 
QUERIES
At the heart of the SILO interface is the MERL Spoken-
Query information retrieval engine (Wolf and Raj, 2002).
The standard approach to spoken query based IR
requires the unambiguous conversion of the spoken input to
text by the recognizer, and is likely to fail if the recognizer
makes errors, especially in recognizing keywords that iden-
tify the desired documents. The SpokenQuery IR engine
used in the SILO interface, however, does not require
unambiguous speech-to-text conversion. Spoken queries
Process
Spoken Input
Respond to
Input
Initiate
Interaction
Desired
Action
Spoken query
based Information
Retrieval
Initiate
Interaction
speech
speech
(a) Menu or dialog
based spoken user
interface
(b) SILO
Desired
ActionList
Selection
Figure  1. Operation of spoken user interfaces. (a) Conven-
tional dialog or menu-based interfaces. (b) SILO.
need not conform to any grammar and are permitted to be
free form.
Textual descriptions of the responses required from the
system are stored as documents in a document index. The
descriptions must uniquely identify the response. For exam-
ple, in a digital music retrieval system, the documents
might be the descriptive meta data associated with the
music. For a command and control system, they might be a
text description of possible actions to be taken by the appli-
cation.
Documents are represented in this index as word-count
vectors. Each entry in the vector represents the frequency of
occurrence of a specific word in the document. This repre-
sentation ignores the order in which words occur in the doc-
uments, retaining information only about their frequency of
occurrence. This particular representation is related to the
absence of linguistic constraints applied to spoken queries.
Since queries are permitted to be free form, word order can-
not be a consideration in the formation of the query, and
consequently in the document index.
Spoken queries are converted to a data structure that is
similar to the structures used to represent documents in the
index. The query structure is a vector of normalized
expected word counts in the spoken query. The expected
count for a word is simply the statistical expectation of the
number of times the word occurred in the spoken query and
is computed from the entire set of word sequence hypothe-
ses considered by the recognizer, not just the single text
string finally output by the recognizer. 
The recognizer represents the set of all considered
hypotheses as a graph that is commonly known as a word
lattice. Nodes in the graph represent words and arcs repre-
sent valid transitions between words. The scores associated
with nodes and arcs are the probabilities of the correspond-
ing words and transitions. The best-choice transcript gener-
ated by the recognizer is the most likely path through this
lattice (i.e. the path with the best score). 
The a posteriori probability of any node in the word lat-
tice is the ratio of the total probability scores of all paths
through the lattice that pass through that node, to the total
probability of all paths through the lattice and can be com-
puted through a forward-backward algorithm. Multiple
nodes in the word lattice may represent the same word. The
normalized expected count for a word is simply the total of
the a posteriori probabilities of all instances of that word in
the lattice. We call these ?normalized? counts since the
expected counts of all words sum to 1.0.
A useful feature of speech recognition systems is that
the actual words spoken by the user are usually included in
the recognition lattice for the utterance, often with high
probability, even when they are not included in the recog-
nizer?s final output. As a result, the true query word usually
have non-zero, and often relatively high, expected word
counts, regardless of their inclusion in the final recognizer
output.
For retrieval, the dot product of the document word-
count vectors and the expected word count vectors derived
from the spoken query is computed. The output of the
retrieval system is a list of the documents sorted by this
value.
Figure 2 shows the overall system architecture for the
SpokenQuery information retrieval system in SILO. 
The vocabulary of the speech recognizer used for query
construction minimally includes all keywords in the docu-
ments. In addition to these, the recognizer may include all
the rest of the words in the documents as well, or may
replace them with a smaller set of garbage words. Since
queries may be free form, the recognizer cannot use con-
strained grammars that impose strict restrictions on word
order. Instead, it uses an N-gram language model that high-
lights the keywords and key phrases, but does not strictly
disallow any particular sequence of words.
4. EXAMPLE SILO APPLICATIONS AND EXPERIMENTS
In this section we describe some applications for which
we have implemented SILO interfaces, and report three
experiments that evaluate different aspects of the SILO
interface. The targeted aspects are: a) the effectiveness of
the spoken input based IR technique used in the SILO inter-
face, b) the effectiveness of the SILO interface and c)
whether the SILO interface provides any advantage over
conventional UIs.
Document retrieval with spoken queries
Since the main component of the SILO interface is the
proposed SpokenQuery IR engine, our first application
demonstrates the effectiveness of the proposed IR method
on a conventional document retrieval task. This application
is exactly the same as normal IR (e.g. Google, AltaVista,
etc.) except that the user speaks instead of types. As with
normal IR, the user may say any word that he/she judges to
specify the information. There is no grammar to memorize.
It is intended that the returned list of documents contain
appropriate documents near the top, however, as with any
IR engine, there is no guarantee that all returned documents
will be pertinent to the query.
For the experimental evaluation, a database of 262 tech-
nical reports from our laboratory formed the document
index. A total of 75 spoken queries were recorded from a
number of users. In order to establish ground truth, the true
relevance of each of the 262 documents to each of the que-
ries was judged by a team of humans. For each query, docu-
ments were assigned a relevance score of 0 (irrelevant), 1
(somewhat relevant), or 2 (definitely relevant). All queries
were evaluated by every evaluator and their average score,
scaled to lie between 1 and 10, was deemed to be the
ground truth.
Figure 3 shows the average relevance of the first 30
documents retrieved, for retrieval using a text transcription
of the queries, with the recognizer?s best output, and the
SpokenQuery IR engine respectively. The text transcription
is not affected by noise. For retrieval based on spoken
input, however, SpokenQuery is found to be significantly
better than retrieval based on the recognizer?s single best
output. While performance on noisy speech is generally
poor, the proposed method is observed to result in an equiv-
alent of 5dB improvement in noise level over retrieval
based on the recognizer?s output.
MediaFinder: retrieving music with spoken queries
We now evaluate the effectiveness of SILO as a user
interface. A UI is effective if the user is able to obtain the
desired response from the system in a small number of
steps. Since the SILO based UI returns lists of possible
responses, the interaction may be considered successful, if
the returned list contains the desired response. Further, we
deem it more effective if the desired response is ranked
higher in the list, since the user has to spend less time scan-
ning the list. If the returned list does not contain the desired
response, the user must repeat the interaction, and the
exchange is considered a failure.
The MediaFinder application is a spoken interface for
retrieving music from digital collections, and represents a
good example of an application where SILO can make a
significant difference in the effectiveness of the UI. Hand-
held mp3 players can hold thousands of songs, yet the inter-
face provided on these devices is usually minimal,
consisting of a small screen and some buttons. Users must
access music by navigating a hierarchy of menus with the
buttons. An effective spoken user interface can improve the
usability of such devices greatly.
MP3 files contain extensive metadata (ID3) that
describe their contents. In the case of music files, these
The Internet 
provides 
worldwide 
access to a 
huge number 
The Internet 
provides huge 
number of 
publicly 
available 
multi-media 
Compute
word
certainties
Spoken query
Recognition
Lattice
Speech
recognizer
Compute 
Word 
Counts
Document 
Word-Count Vector
Search by
comparing
vectors
Query Word-Probabilty 
Vector
Document index
Documents
Sorted 
List
Figure  2. A block diagram representation of the Spoken-
Query IR engine. The document indexer computes word
count vectors for documents and stores them in an index.
For retrieval, spoken queries are converted to a recognition
lattice by a recognizer. Normalized expected word count
vectors are computed from the lattice. Documents are
ranked by the dot product of their word count vectors and
the query vector, and retrieved in the order of their ranking.
0
50
100
150
200
250
0 5 10 15
Txt
BP
SQ
O
ve
ra
ll 
Pe
rf
or
m
an
ce
SNR(dB)
Figure  3. Average ranking of documents retrieved using i)
text transcriptions of spoken queries, ii) the text output of a
recognizer and iii) the proposed spoken query IR method.
would include the title of the album, the name of the singer
or composer, and often other details such as the musical
genre. This meta-data text is used to index MP3 files. To
search for a song, the user speaks a description of the
desired music. The description may include a combination
of words from the name of the song, the artist, the album, or
the genre in any order. A list of songs that match the spoken
query most closely are displayed on the screen. Using up,
down and select buttons, the user scrolls through the
returned list, and selects the desired song. If the requested
song is not in the displayed list, the speaker must repeat the
query, perhaps trying different words.
We conducted two different experiments on Medi-
aFinder: one to evaluate the ability of users to successfully
obtain the desired response from the application - in this
case the playback of a desired piece of music, and a second
to determine if there is any advantage to the SILO interface
over conventional interfaces.
In the first experiment users attempted to retrieve a
desired piece of music from collections of different sizes.
MediaFinder returns a list of up to 10 songs in response to a
query. A score of 100 is given to the interaction if the
desired song is at the top of the list. The score decreases lin-
early if the required response is lower in the list. If the
required response is not in the returned list, the score for the
interaction is 0. Figure 4 shows the average score for an
interaction, as a function of the size of the collection from
which songs are to be retrieved. Each point in the plot rep-
resents an average score across 100 queries from two users.
We note that the average score is greater than 90 in all
cases, indicating that the desired music is not only
retrieved, but is typically near the very top of the list in a
majority of the cases. This shows that the SILO interface
can indeed be used effectively for such tasks, and may even
effectively substitute other more complex interfaces.
In the second experiment we compared the cognitive
load imposed by the MediaFinder SILO interface to that
imposed by a conventional graphical menu-based interface
for the same task - selection of music from a digital collec-
tion in an automobile. We note here that digital music play-
ers are becoming increasingly common in automobiles, and
having an effective UI that imposes minimal cognitive load
is of tantamount importance in such devices.
Experiments were conducted using a simple driving
simulator that mimicked two important facets of driving:
steering and braking. Subjects steered, braked, and con-
trolled the searching interfaces with a steering wheel and its
gas and brake pedals. Steering was measured with a pursuit
tracking task in which the subject used the wheel to closely
frame a moving target. Braking was measured by recording
subjects? reaction time to circles that appeared on screen at
random intervals. The tests were conducted on fourteen
subjects (8 male, 6 female, 18-37 years old). Four were
non-native speakers and all but one were regular automo-
bile drivers.
The study shows that a) subjects made an average of
20.7% less steering error when using the SILO interface,
and b) Subjects took an average of 28.6% less time to
retrieve songs using the SILO interface. Both interfaces had
the same effect on braking response. The results indicate
that the SILO interface does indeed impose a significantly
lower cognitive load on the user, at least for such tasks.
It must be mentioned, however, that subjects were much
better at both steering and braking when they did not
attempt to retrieve music at all (suggesting, perhaps, that
when driving an automobile, people must simply just
drive).
5. DISCUSSIONS AND CONCLUSION
The SILO interface presents a different approach to user
interfaces, that treats the problem of obtaining a particular
final response from a system as one of retrieving one of the
elements from the set of all possible responses from the
system. The experiments demonstrate that the SILO inter-
face can be effectively used in applications where the
responses of the system can be enumerated, and textually
described, and that it can actually be easier to use in some
situations. 
REFERENCES
1. Chang, E., Seide, F., Meng, H., Chen, Z. Shi, Y., Li, Y.
A system for spoken query information retrieval on
mobile devices. IEEE Trans. on Speech and Audio Pro-
cessing, 10:8, pp. 531-541. November 2002.
2. Chen, B., Want, H.M., Lee, L.S. Retrieval of broadcast
news speech in Mandarin Chinese collected in Taiwan
using syllable-level statistical characteristics. Proc.
IEEE Intl. Conf. on Acoustics Speech and Signal Pro-
cessing (ICASSP2000). Istanbul, Turkey. 2000.
3. Kupiec, J., Kimber, D., Balasubramanian, V. Speech-
based retrieval using semantic co-occurrence filtering,
Proc. Human Language Technology Conf. 1994.
4. Wolf, P. and Raj, B. The MERL SpokenQuery Informa-
tion Retrieval System: A System for Retrieving Perti-
nent Documents from a Spoken Query. Proc. IEEE
Conference and Multimedia Expo (ICME2003). Lau-
sanne, Switzerland. August 2002.
90
91
92
93
94
95
96
97
98
99
100
6828 3414 1707 854 427 214 107 54
Index Size
Av
er
ag
e 
sc
or
e
Text Only
SILO
Figure  4. Average ranking of the correct response in a list
of responses returned by SILO, as a function of the size of
the index. A score of 0 indicates that the desired response
was not returned. 100 indicates that it was returned at the
top of the list.
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 787?797,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
An Unsupervised Dynamic Bayesian Network Approach to Measuring
Speech Style Accommodation
Mahaveer Jain1, John McDonough1, Gahgene Gweon2, Bhiksha Raj1, Carolyn Penstein Rose?1,2
1. Language Technologies Institute; 2. Human Computer Interaction Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{mmahavee,johnmcd,ggweon,bhiksha,cprose}@cs.cmu.edu
Abstract
Speech style accommodation refers to
shifts in style that are used to achieve strate-
gic goals within interactions. Models of
stylistic shift that focus on specific fea-
tures are limited in terms of the contexts
to which they can be applied if the goal of
the analysis is to model socially motivated
speech style accommodation. In this pa-
per, we present an unsupervised Dynamic
Bayesian Model that allows us to model
stylistic style accommodation in a way that
is agnostic to which specific speech style
features will shift in a way that resem-
bles socially motivated stylistic variation.
This greatly expands the applicability of the
model across contexts. Our hypothesis is
that stylistic shifts that occur as a result of
social processes are likely to display some
consistency over time, and if we leverage
this insight in our model,we will achieve
a model that better captures inherent struc-
ture within speech.
1 Introduction
Sociolinguistic research on speech style and its
resulting social interpretation has frequently fo-
cused on the ways in which shifts in style are
used to achieve strategic goals within interac-
tions, for example the ways in which speakers
may adapt their speaking style to suppress differ-
ences and accentuate similarities between them-
selves and their interlocutors in order to build
solidarity (Coupland, 2007; Eckert & Rickford,
2001; Sanders, 1987). We refer to this stylis-
tic convergence as speech style accommodation.
In the language technologies community, one tar-
geted practical benefit of such modeling has been
the achievement of more natural interactions with
speech dialogue systems (Levitan et al 2011).
Monitoring social processes from speech or
language data has other practical benefits as well,
such as enabling monitoring how beneficial an in-
teraction is for group learning (Ward & Litman,
2007; Gweon, 2011), how equal participation is
within a group (DiMicco et al 2004), or how
conducive an environment is for fostering a sense
of belonging and identification with a community
(Wang et al 2011).
Typical work on computational models of
speech style accommodation have focused on spe-
cific aspects of style that may be accommodated,
such as the frequency or timing of pauses or
backchannels (i.e., words that show attention like
?Un huh? or ?ok?), pitch, or speaking rate (Ed-
lund et al 2009; Levitan & Hirschberg, 2011). In
this paper, we present an unsupervised Dynamic
Bayesian Model that allows us to model speech
style accommodation in a way that does not re-
quire us to specify which linguistic features we
are targeting. We explore a space of models de-
fined by two independent factors, namely the di-
rect influence of one speaker?s style on another
speaker?s style and the influence of the relational
gestalt between the two speakers that motivates
the stylistic accommodation, and thus may keep
the accommodation moving consistently, with the
same momentum. Prior work has explored the in-
fluence of the first factor. However, because ac-
commodation reflects social processes that extend
over time within an interaction, one may expect a
certain consistency of motion within the stylistic
shift. Furthermore, we can leverage this consis-
tency of style shift to identify socially meaningful
variation without specifying ahead of time which
787
particular stylistic elements we are focusing on.
Our evaluation provides support for this hypothe-
sis.
When stylistic shifts are focused on specific
linguistic features, then measuring the extent of
the stylistic accommodation is simple since a
speaker?s style may be represented on a one or two
dimensional space, and movement can then be
measured precisely within this space using sim-
ple linear functions. However, the rich sociolin-
guistic literature on speech style accommodation
highlights a much greater variety of speech style
characteristics that may be associated with social
status within an interaction and may thus be bene-
ficial to monitor for stylistic shifts. Unfortunately,
within any given context, the linguistic features
that have these status associations, which we re-
fer to as indexicality, are only a small subset of
the linguistic features that are being used in some
way. Furthermore, which features carry this in-
dexicality are specific to a context. Thus, separat-
ing the socially meaningful variation from varia-
tion in linguistic features occurring for other rea-
sons is akin to searching for the proverbial needle
in a haystack. It is this technical challenge that we
address in this paper.
In the remainder of the paper we review the lit-
erature on speech style accommodation both from
a sociolinguistic perspective and from a techno-
logical perspective in order to motivate our hy-
pothesis and proposed model. We then describe
the technical details of our model. Next, we
present an experiment in which we test our hy-
pothesis about the nature of speech style accom-
modation and find statistically significant con-
firming evidence. We conclude with a discussion
of the limitations of our model and directions for
ongoing research.
2 Theoretical Framework
Our research goal is to model the structure of
speech in a way that allows us to monitor so-
cial processes through speech. One common goal
of prior work on modeling speech dynamics has
been for the purpose of informing the design of
more natural spoken dialogue systems (Levitan et
al., 2011). The practical goal of our work is to
measure the social processes themselves, for ex-
ample in order to estimate the extent to which
group discussions show signs of productive con-
sensus building processes (Gweon, 2011). Much
prior work on modeling emotional speech has
sought to identify features that themselves have
a social interpretation, such as features that pre-
dict emotional states like uncertainty (Liscombe
et al 2005), or surprise (Ang et al 2002), or
social strategies like flirting (Ranganath et al
2009). However, our goal is to monitor social pro-
cesses that evolve over time and are reflected in
the change in speech dynamics. Examples include
fostering trust, forming attachments, or building
solidarity.
2.1 Defining Speech Style Accommmodation
The concept of what we refer to as Speech
Style Accommodation has its roots in the field
of the Social Psychology of Language, where
the many ways in which social processes are re-
flected through language, and conversely, how
language influences social processes, are the ob-
jects of investigation (Giles & Coupland, 1991).
As a first step towards leveraging this broad range
of language processes, we refer to one very spe-
cific topic, which has been referred to as entrain-
ment, priming, accommodation, or adaptation in
other computational work (Levitan & Hirschberg,
2011). Specifically we refer to the finding that
conversational partners may shift their speaking
style within the interaction, either becoming more
similar or less similar to one another.
Our usage of the term accommodation specifi-
cally refers to the process of speech style conver-
gence within an interaction. Stylistic shifts may
occur at a variety of levels of speech or language
representation. For example, much of the early
work on speech style accommodation focused on
regional dialect variation, and specifically on as-
pects of pronunciation, such as the occurrence of
post-vocalic ?r? in New York City, that reflected
differences in age, regional identification, and so-
cioeconomic status (Labov, 2010a,b). Distribu-
tion of backchannels and pauses have also been
the target of prior work on accommodation (Lev-
itan & Hirschberg, 2011). These effects may be
moderated by other social factors. For example,
Bilous & Krauss (1988) found that females ac-
commodated to their male partners in conversa-
tion in terms of average number of words uttered
per turn. For example, Hecht et al(1989) re-
ported that extroverts are more listener adaptive
than introverts and hence extroverts converged
more in their data.
788
Accommodation could be measured either
from textual or speech content of a conversation.
The former relates to ?what? people say whereas
the latter to ?how? they say it. We are only inter-
ested in measuring accommodation from speech
in this work. There has been work on convergence
in text such as syntactic adaptation (Reitter et al
2006) and language similarity in online commu-
nities (Huffaker et al 2006).
2.2 Social Interpretation of Speech Style
Accommodation
It has long been established that while some
speech style shifts are subconscious, speakers
may also choose to adapt their way of speaking
in order to achieve social effects within an in-
teraction (Sanders, 1987). One of the main mo-
tives for accommodation is to decrease social dis-
tance. On a variety of levels, speech style accom-
modation has been found to affect the impression
that speakers give within an interaction. For ex-
ample, Welkowitz & Feldstein (1970) found that
when speakers become more similar to their part-
ners, they are liked more by partners. Another
study by Putman & Street Jr (1984) demonstrated
that interviewees who converge to the speaking
rate and response latency of their interviewers are
rated more favorably by the interviewers. Giles et
al. (1987) found that more accommodating speak-
ers were rated as more intelligent and supportive
by their partners. Conversely, social factors in
an interaction affect the extent to which speak-
ers engage in, and some times chose not to en-
gage in, accommodation. For example, Purcell
(1984) found that Hawaiian children exhibit more
convergence in interactions with peer groups that
they like more. Bourhis & Giles (1977) found that
Welsh speakers while answering to an English
surveyor broadened their Welsh accent when their
ethnic identity was challenged. Scotton (1985)
found that few people hesitated to repeat lexi-
cal patterns of their partners to maintain integrity.
Nenkova et al(2008) found that accommodation
on high frequency words correlates with natural-
ness, task success, and coordinated turn-taking
behavior.
2.3 Computational models of speech style
accommodation
Prior research has attempted to quantify accom-
modation computationally by measuring similar-
ity of speech and lexical features either over full
conversations or by comparing the similarity in
the first half and the second half of the conver-
sation. For example, Edlund et al(2009) mea-
sure accommodation in pause and gap length us-
ing measures such as synchrony and convergence.
Levitan & Hirschberg (2011) found that accom-
modation is also found in special social behaviors
within conversation such as backchannels. They
show that speakers in conversation tend to use
similar kinds of speech cues such as high pitch at
the end of utterance to invite a backchannel from
their partner. In order to measure accommodation
on these cues, they compute the correlation be-
tween the numerical values of these cues used by
partners.
In our work we measure accommodation using
Dynamic Bayesian Networks (DBNs). Our mod-
els are learnt in an unsupervised fashion. What
we are specifically interested in is the manner in
which the influence of one partner on the other is
modeled. What is novel in our approach is the
introduction of the concept of an accommodation
state, or relational gestalt variable, which essen-
tially models the momentum of the influence that
one partner is having on the other partner?s speak-
ing style. It allows us to represent structurally the
insight that accommodation occurs over time as a
reflection of a social process, and thus has some
consistency in the nature of the accommodation
within some span of time. The prior work de-
scribed in this section can be thought of as tak-
ing the influence of the partner?s style directly on
the speaker?s style within an instant as the floor
shifts from one speaker to the next. Thus, no con-
sistency in the manner in which the accommoda-
tion is occurring is explicitly encouraged by the
model. The major advantage of consistency of
motion within the style shift over time is that it
provides a sign post for identifying which style
variation within the speech is salient with respect
to social interpretation within a specific interac-
tion so that the model may remain agnostic and
may thus be applied to a variety of interactions
that differ with respect to which stylistic features
are salient in this respect.
3 A Dynamic Bayesian Network Model
for Conversation
Speech stylistic information is reflected in
prosodic features such as pitch, energy, speak-
789
ing rate etc. In this work, we leverage on sev-
eral of these speech features to quantify accom-
modation. We propose a series of models that
can be trained unsupervised from speech features
and can be used for predicting accommodation.
The models attempt to capture the dependence of
speech features on speaking style, as well as the
effect of persistence and accommodation on style.
We use a dynamic Bayesian network (DBN) for-
malism to capture these relationships. Below we
briefly review DBNs, and subsequently describe
the speech features used, and the proposed mod-
els.
3.1 Dynamic Bayesian Networks
The theory of Bayesian networks is well doc-
umented and understood (Jensen, 1996; Pearl,
1988). A Bayesian network is a probabilistic
model that represents statistical relationships be-
tween random variables via a directed acyclic
graph (DAG). Formally, it is a directed acyclic
graph whose nodes represent random variables
(which may be observable quantities, latent unob-
servable variables, or hypotheses to be estimated).
Edges represent conditional dependencies; nodes
which are connected by an edge represent ran-
dom variables that have a direct influence on one
another. The entire network represents the joint
probability of all the variables represented by the
nodes, with appropriate factoring of the condi-
tional dependencies between variables.
Consider, for instance, a joint distribution
over a set of random variables x1, x2, ? ? ? , xn,
modeled by a Bayesian network. Let V =
v1, v2, ? ? ? , vn represent the set of n nodes in
the network, representing the random variables
x1, x2, ? ? ? , xn respectively. Let ?(vi) represent
the set of parent nodes of vi, i.e. nodes in V
that have a directed edge into a node vi. Then,
by the dependencies specified by the network,
P (xi|x1, x2, ? ? ? , xn) = P (xi|xj : vj ? ?(vi)).
In other words, any variable xi is directly depen-
dent only on its parent variables, i.e. the random
variables represented by the nodes in ?(vi), and
is independent of all other variables given these
variables. The joint probability of x1, x2, ? ? ? , xn
is hence given by
p(x1, x2, ..., xn) =
?
i
p(xi|xpii) (1)
Where xpii represents {xj : vj ? ?(vi), i.e. the
Figure 1: An example Dynamic Bayesian Network
(DBN) showing the temporal relationship between
three random variables (A,B and C). A is observered
and dependent on two hidden variables B and C. Di-
rected edges across time (t? 1 ? t) indicate temporal
relationships between variables. In this example, the
variables At and Bt are both dependent on Bt?1 with
the relationship defined through conditional distribu-
tions P (At|Bt?1) and P (Bt|Bt?1).
parents of xi in the network. We note that not
all of these variables need to be observable; of-
ten in such models several of the variables are
unobservable, i.e. they are latent. In order
to obtain the joint distribution of the observable
variables the latent variables must be marginal-
ized out. I.e. if x1, ? ? ? , xm are observable
and xm+1, ? ? ? , xn are latent, P (x1, ? ? ? , xm) =
?
xm+1,??? ,xn P (x1, x2, ? ? ? , xn).
Dynamic Bayesian networks (DBNs) further
represent time-series data through a recurrent for-
mulation of a basic Bayesian network that repre-
sents the relationship between variables. Within
a DBN a set of random variables at each time in-
stance t is represented as a static Bayesian Net-
work with temporal dependencies to variables at
other instants. Namely, the distribution of a vari-
able xi,t at time t is dependent on other variables
at times t ? ? , xj,t?? through conditional prob-
abilities of the form Pr(xi,t|xj,t?? ). An exam-
ple DBN, consisting of three variables (A,B and
C), two of which have temporal dependencies is
shown in Figure 1.
One benefit of the DBN formalism is that in
addition to providing a compact graphical way
of representing statistical relationships between
variables in a process, the constrained, directed
network structure also allows for simplified in-
ference. Moreover, the conditional distributions
associated with the network are often assumed
not to vary over time, i.e. Pr(xi,t|xj,t?? ) =
Pr(xi,t? |xj,t??? ). This allows for a very com-
pact representation of DBNs and allows for ef-
ficient Expectation-Maximization (EM) learning
algorithms to be applied.
790
In the discussion that follows we do not explic-
itly specify the random variables and the form of
the associated probability distributions, but only
present them graphically. The joint distribution of
the variables should nevertheless be obvious from
the figures. We employ EM to learn the param-
eters of the models from training data, and the
junction tree algorithm (Lauritzen & Spiegelhal-
ter, 1988) to perform inference.
3.2 Speech Features
We characterize conversations as a series of spo-
ken turns by the partners. We characterize the
speech in each turn through a vector that cap-
tures several aspects of the signal that are salient
to style. We used the OPENSmile toolkit (opens-
mile, 2011) to compute the features. Specifi-
cally, within each turn the speech was segmented
into analysis windows of 50ms, where adjacent
windows overlapped by 40ms. From each anal-
ysis window a total of 7 features were com-
puted: voice probability, harmonic to noise ratio,
voice quality , three measures of pitch (F0, F raw0 ,
F env0 ), and loudness. A 10-bin histogram of fea-
ture values was computed for each of these fea-
tures, which was then normalized to sum to 1.0.
The normalized histogram effectively represents
both the values and the fluctuation in the features.
For instance, a histogram of loudness values cap-
tures the variation in the loudness of the speaker
within a turn. The logarithms of the normalized
10-bin histograms for the 7 features were concate-
nated to result in a single 70-dimensional obser-
vation vector for the turn. These 70 dimensional
observation vectors for each turn of any speaker
are represented in our model as oit where t is turn
index and i is speaker index.
3.3 Elements of the Models
In this section we formally describe the elements
of our model.
Speaking Style State: These states represent the
speaking styles of the partners in a conversation.
We represent these states as sit, where t represent
turn index and i represents speaker index. These
states are assumed to belong to a finite, discrete
set S = {s1, s2, ? ? ? , sk}, i.e. sit ? S ?(i, t).
Accommodation State: An accommodation state
represents the indirect influence of partners on
each other in a conversation. In our present de-
sign, it can take a value of either 1 or 0. These
Yt-1 Yt+1
O1t-1 O1t+1O1tS1t-1 S1t S1t+1
Figure 2: The basic generative model.
Yt-1 Yt+1
O1t-1 O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 3: ISM: The dynamics of each speaker are in-
dependent of the other speaker.
states are represented as At, where t is turn index.
Observation Vector: The observation vectors are
the feature vectors oit computed for each turn.
3.4 Models for Accommodation
Our models embody two premises. First, a per-
son?s speech in any turn is a function of his/her
speaking style in that turn. Second, a person?s
speaking style at any turn depends not only by
their own personal biases, but also by their ac-
commodation to their partner. We represent these
dependencies as a DBN.
Our basic model to represent the generation of
speech (i.e. speech features) by a speaker in the
absence of other influences is shown in Figure 2.
The speech features oit in any turn depend only on
the speaking style sit in that turn. The style sit in
any turn depends on the style sit?1 in the previ-
ous turn, to capture the speaker-specific patterns
of variation in speaking style. We note that this
is a rather simple model and patterns of variation
in style are captured only through the statistical
dependence between styles in consequent turns.
We now build our models for accommodation
on this basic model.
3.4.1 Style-based models
Our two first models assume that accommo-
dation is demonstrated as a direct dependence
of a person?s speaking sytle on their partner?s
style. Therefore the models only consider speak-
ing styles.
The Independent Speaker Model
Our simplest model for a conversation assumes
791
Yt-1 Yt+1
O1t-1 O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 4: CSDM: A speaker?s style depends on their
partner?s style at the previous turn.
Yt-1 Yt+1
O1t-1
Y Yt-1At At+1
O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 5: SASM: Both partners? styles depend on mu-
tual accommodation to one another.
that each person?s speaking style evolves indepen-
dently, uninfluenced by their partner. The DBN
for this is shown in Figure 3. We refer to this
model as the Independent Speaker Model (ISM).
Note that the set of values that the style states can
take is common for both speakers. The speaking
styles for the two speakers may be said to be con-
fluent in any turn if both of them are in the same
style state at that turn.
The Cross-speaker Dependence Model
Intuitively, in a conversation speakers are influ-
enced by their partners? speaking style in previ-
ous turns. The Cross-Speaker Dependence Model
(CSDM) represents this dependence as shown in
the DBN in Figure 4. In this model a person?s
speaking style depends on both their own and
their partner?s speaking styles in the previous turn.
3.4.2 Accommodation state models
Accommodation state models assume that con-
versations actually have an underlying state of ac-
commodation, and that speakers in fact vary their
speaking styles in response to it. We models this
through a binary-valued accommodation state that
is embedded into the DBN. We posit two types of
accommodation state models.
The Symmetric Accommodation State Model
In the symmetric accommodation state model
Yt-1 Yt+1
O1t
Y Yt-1A2t A1t
O1t+1S1t S1t+1
S2t
O2t O2t+1
S2t+1Yt-1A2t+1
Figure 6: AASM: Accommodation state associated
with every speaker turn
(SASM) we assume that accommodation is a
jointly experienced characteristic of the conversa-
tion at any time, which enjoys some persistence,
but is also affected by the speaking styles exhib-
ited by the speakers at each turn. The accom-
modation at any time in turn affects the speaking
styles of both speakers in the next turn. The DBN
for this model is shown in Figure 5.
The Asymmetric Accommodation State Model
The asymmetric accommodation state model
(AASM) represents accommodation as a speaker-
turn-specific characteristic. In any turn, the ac-
commodation for a speaker depends chiefly on
their partner?s most recent speaking style. The ac-
commodation state can change after each speaker
turn. Figure 6 shows the DBN for this model.
Note that this model captures the asymmetric na-
ture of accommodation, e.g. it may be the case
that only one of the speakers is accommodating.
For instance, if if a1t = 0 and a2t = 1, only
speaker2 is accommodating but not speaker1.
3.4.3 Accommodated style dependence
models
While accommodation state models explicitly
models accommodation, they do not explicitly
represent how it is expressed. In reality, accom-
modation is a process of convergence ? an ac-
commodating speaker?s speaking style may be ex-
pected to converge toward that of their partner. In
other words, the person?s speaking style depends
not only on whether they are accommodating or
not, but also on their partner?s style at the previ-
ous turn. Accommodated style dependence mod-
els explicitly represent this dependence.
The Symmetric Accommodated Style Depen-
dence Model
The Symmetric Accommodated Style Depen-
dence Model (SASDM) extends the SASM, to in-
792
Yt-1 Yt+1
O1t-1
Y Yt-1At At+1
O1t+1O1tS1t-1 S1t S1t+1
S2t-1O2t-1
S2t
O2t O2t+1
S2t+1
Figure 7: SASDM: A speaker?s style depends both on
mutual accommodation and the partner?s style in the
previous turn.
Yt-1 Yt+1
O1t
Y Yt-1A2t A1t
O1t+1S1t S1t+1
S2t
O2t O2t+1
S2t+1Yt-1A2t+1
Figure 8: AASDM: The accommodation state associ-
ated with every speaker and a speaker?s style depends
on the partner?s style.
dicate that a speaker?s style in any turn depends
both on accommodation and on their partner?s
style in the previous turn. Figure 7 shows the
DBN for this model.
Asymmetric Accommodated Style Dependence
Model
The Asymmetric Accommodated Style Depen-
dence Model (AASDM) extends the AASM by
adding a direct dependence between a speaker?s
style and their partner?s style in their most recent
turn. The DBN for this is shown in Figure 8.
3.5 Interpreting the states
We note that we have referred to the states in the
models above as ?style? states. In reality, in all
cases, we learn the parameters of the model in
an unsupervised manner, since the data we use to
train it do not have either speaking style or ac-
commodation indicated (although, if they were la-
beled, the labels could be employed within our
models). Consequently, we have no assurance
that the states learned will actually correspond to
speaking styles. They can only be considered a
proxy for speaking style. Nevertheless, if both
speakers are in the same state, they can both be
expected to be producing similar prosodic fea-
tures, as represented in the observation vectors.
It is hence reasonable to assume that they are both
speaking in similar style. Similarly, the accom-
modation state cannot be expected to actually de-
pict accommodation; nevertheless, it can capture
the dependencies that govern when the two speak-
ers are likely to be in the same state.
4 Evaluation
The model we have just described allows us to in-
vestigate two separate aspects of our concept of
speech style accommodation. The first aspect is
that style accommodation occurs as a local influ-
ence of one speaker?s style on the other speaker?s
style, as depicted by direct links between style
states. The second aspect is that although this is a
local phenomenon, because it is a reflection of a
social process that extends over a period of time,
there will be some persistence of accommodation
over longer periods of time, as characterized by
the accommodation state. We presented two dif-
ferent operationalizations of the accommodation
state above, namely Asymmetric and Symmetric.
Accommodation is a phenomenon that occurs
within interactions between speakers; we can ex-
pect not to observe accommodation occurring be-
tween individuals that have never met and are not
interacting. On average, then, we expect to see
more evidence of speech style accommodation in
pairs of individuals who are interacting (i.e., Real
Pairs) than in pairs of individuals who are not in-
teracting and have never met (i.e., Constructed
Pairs). Thus, we may evaluate the extent to which
our model is sensitive to social dynamics within
pairs by the extent to which it is able to distinguish
between true conversation between Real Pairs of
speaker and synthetic conversation between Con-
structed Pairs. A similar experimental paradigm
has been adopted in prior work on speech style
accommodation (Levitan et al 2011).
Hypothesis: Our hypothesis is that models that
explicitly represent the notion that accommoda-
tion occurs over a span of time with consistency
of momentum will achieve better success at dis-
tinguishing between Real Pairs and Constructed
Pairs than models that do not.
Experimental Manipulation: Thus, using the
model we have just described, we are able to
test our hypothesis using a 2 ? 3 factorial design
in which one factor is the inclusion of direct
links from the style of one speaker to the style
793
of the other speaker, which we refer to as the
DirectInfluence (DI) factor, with values True
(T) and False (F), and the second factor is the
inclusion of links from style states to and from
Accommodation states, which we refer to as the
IndirectInfluence (II) factor, with values False
(F), Asymmetric (A), and Symmetric (S). The
result of this 2 ? 3 factorial design are the 6
different models described in Section 3, namely
ISM (DI=False, II=False), CSDM (DI=True,
II=False), SASM (DI=False, II=Symmetric),
AASM (DI=False, II=Asymmetric), SASDM
(DI=True, II=Symmetric), and AASDM
(DI=True, II= Asymmetric).
Corpus: The success criterion in our experiment
is the extent to which models of speech style
accommodation are able to distinguish between
Real Pairs and Constructed pairs. In order to set
up this comparison, we began with a corpus of de-
bates between students about the reasons for the
fall of the Ottoman Empire. We obtained this cor-
pus from researchers who originally collected it
to investigate issues related to learning from con-
versational interactions (Nokes et al 2010). The
full corpus contains interactions between 76 pairs
of students who interacted for 8 minutes. Within
each pair, one student was assigned the role of ar-
guing that the fall of the Ottoman empire was due
to internal causes, whereas the other student was
assigned the role of arguing that the fall of the Ot-
toman empire was due to external causes. Each
student was given a 4 page packet of supporting
information for their side of the debate to draw
from in the interaction.
The speech from each participant was recorded
on a separate channel. As a first step, we aligned
the speech recordings automatically to their tran-
scriptions at the word and turn level. After align-
ing the corpus at the word level, we identify the
turn interval of each partner in the conversation.
We use 66 of the debates out of the complete set
of 76 for the experiments discussed in this paper.
We had to eliminate 10 dialogues where the seg-
mentation and alignment failed. For each of our
models, we used the same 3 fold cross-validation.
Participants: Participants were all male under-
graduate students between the ages of 18 and 25.
In prior studies, it has been shown that accommo-
dation varies based on gender, age and familiar-
ity between partners. This corpus is particularly
appropriate because it controls for most of these
factors. Furthermore, because the participants did
not know each other before the debate, we can
assume that if accommodation happened, it was
only during the conversation.
Real versus Constructed Pairs: In our analy-
sis below, we compare measured accommodation
between pairs of humans who had a real conver-
sation and a constructed pair in which one per-
son from that conversation is paired with a con-
structed partner, where the partner?s side of the
conversation was constructed from turns that oc-
curred in other conversations. We set up this com-
parison in order to isolate speech style conver-
gence from lexical convergence when we evalu-
ate the performance of our model. The difference
between the measured accommodation between
real and constructed pairs is treated as a weak op-
erationalization of model accuracy at measuring
speech style accommodation.
For each of the 20 Real pairs in the test corpus
we composed one Constructed Pair. Each Con-
structed Pair comprised one student from the cor-
responding Real Pair (i.e., the Real Student) and a
Constructed Partner that resembled the real part-
ner in content but not necessarily style. We did
this by iterating through the real partner?s turns,
replacing each with a turn that matched as well as
possible in terms of lexical content but came from
a different conversation. Lexical content match
was measured in terms of cosine similarity. Turns
were selected from the other Real pairs. Thus, the
Constructed Partner had similar content to the cor-
responding real partner on a turn by turn basis, but
the style of expression could not be influenced by
the Real Student. Thus, ideally we should not see
evidence of speech style accommodation within
the Constructed Pairs.
Experimental Procedure: For each of the four
models we computed an Accommodation Score
for each of the Real Pairs and Constructed Pairs.
In order to obtain a measure that can be used to
compute accommodation for all the models con-
sidered, we compute the accommodation value as
the fraction of turns in a session where partners
exhibited the same speaking style.
Results: In order to test our hypothesis we con-
structed an ANOVA model with Accommodation
Score as the dependent variable and DirectInflu-
ence, IndirectInfluence, RealVsConstructed as in-
dependent variables. Additionally we included
the interaction terms between all pairs of inde-
794
DI II Real Constructed
?(?) ?(?)
SASDM T S .54 (.23) .44 (.29)
SASM F S .54 (.23) .44 (.29)
CSDM T F .6 (.26) .52 (.3)
ISM F F .56 (.25) .51 (.32)
AASM F A .6 (.24) .51 (.3)
AASDM T A .61 (.24) .48 (.3)
Table 1: Accommodation measured using different
models. Legend: ?=mean, ? = standard deviation, DI
= ?Direct Influence?, II = ?Indirect Influence?.
pendent variables. Using this ANOVA model, we
find a highly significant main effect of the Re-
alVsConstructed factor that demonstrates the gen-
eral ability of the models to achieve separation be-
tween Real Pairs and Constructed Pairs; on aver-
age F(1,780) = 18.22, p < .0001.
However, when we look more closely, we find
that although the trend is consistently to find more
evidence of speech style accommodation in Real
Pairs than in Constructed Pairs, we see differen-
tiation among the models in terms of their abil-
ity to achieve this separation. When we exam-
ine the two way interactions between DirectIn-
fluence and RealVsConstructed as well as be-
tween IndirectInfluence and RealVsConstructed,
although we do not find significant interactions,
we do find some suggestive patterns when we
do the student T posthoc analysis. In particular,
when we explore just the interaction between In-
directInfluence links, we find a significant separa-
tion between Real vs Constructed pairs for models
with Accommodation states, but not for the cases
where no Accommodation states are included.
However, when we do the same for the interaction
between DirectInfluence links and RealVsCon-
structed, we find significant separation with or
without those links. This suggests that IndirectIn-
fluence links are more important than DirectInflu-
ence links. At a finer-grained level, when we ex-
amine the models individually, we only find a sig-
nificant separation between Real and Constructed
pairs with the model that includes both Direct-
Influence and Symmetric IndirectInfluence links.
These results suggest that Symmetric IndirectIn-
fluence links may be slightly better than Asym-
metric ones, and that combining DirectInfluence
links and Symmetric IndirectInfluence links may
be the best combination.
Based on this analysis, we find support for our
hypothesis. We find that the model that includes
Symmetric IndirectInfluence links and DirectIn-
fluence links is the best balance between represen-
tational power and simplicity. The support for the
inclusion of DirectInfluence links in the model is
weaker than that of IndirectInfluence links, how-
ever. On a larger dataset, we may have observed
stronger effects of both factors. Even on this small
dataset, we find evidence that adding that struc-
ture improves the performance of the model with-
out leading to overfitting.
5 Conclusions and Current Directions
In this paper we presented an unsupervised dy-
namic Bayesian modeling approach to modeling
speech style accommodation in face-to-face inter-
actions. Our model was motivated by the idea that
because accommodation reflects social processes
that extend over time within an interaction, one
may expect a certain consistency of motion within
the stylistic shift. Our evaluation demonstrated a
statistically significant advantage for the models
that embodied this idea.
An important motivation for our modeling ap-
proach was that it allows us to avoid targeting
specific linguistic style features in our measure
of accommodation. However, in our evaluation,
we only tested our approach on conversations be-
tween male undergraduate students discussing the
fall of the Ottoman Empire. Thus, while our eval-
uation provides evidence that we have taken a first
important step towards our ultimate goal, we can-
not yet claim that we have a model that performs
equally effectively across contexts. In our future
work, we plan to formally test the extent to which
this allows us to accurately measure accommoda-
tion within contexts in which very different stylis-
tic elements carry strategic social value.
Another important direction of our current re-
search is to explore how measures of speech style
accommodation may predict other important mea-
sures such as how positively partners view one an-
other, how successful partners perform tasks to-
gether, or how well students learn together.
6 Acknowledgments
We gratefully acknowledge John Levine and Tim-
othy Nokes for sharing their data with us. This
work was funded by NSF SBE 0836012.
795
References
Ang, J., Dhillon, R., Krupski, A., Shriberg, E., & Stol-
cke, A. (2002). Prosody-based automatic detection
of annoyance and frustration in human-computer di-
alog. In Proc. ICSLP, volume 3, pages 2037?2040.
Citeseer.
Bilous, F. & Krauss, R. (1988). Dominance and
accommodation in the conversational behaviours
of same-and mixed-gender dyads. Language and
Communication, 8(3), 4.
Bourhis, R. & Giles, H. (1977). The language of in-
tergroup distinctiveness. Language, ethnicity and
intergroup relations, 13, 119.
Coupland, N. (2007). Style: Language variation and
identity. Cambridge Univ Pr.
DiMicco, J., Pandolfo, A., & Bender, W. (2004). Influ-
encing group participation with a shared display. In
Proceedings of the 2004 ACM conference on Com-
puter supported cooperative work, pages 614?623.
ACM.
Eckert, P. & Rickford, J. (2001). Style and sociolin-
guistic variation. Cambridge Univ Pr.
Edlund, J., Heldner, M., & Hirschberg, J. (2009).
Pause and gap length in face-to-face interaction. In
Proc. Interspeech.
Giles, H. & Coupland, N. (1991). Language: Contexts
and consequences. Thomson Brooks/Cole Publish-
ing Co.
Giles, H., Mulac, A., Bradac, J., & Johnson, P. (1987).
Speech accommodation theory: The next decade
and beyond. Communication yearbook, 10, 13?48.
Gweon, G. A. P. U. M. R. B. R. C. P. (2011). The
automatic assessment of knowledge integration pro-
cesses in project teams. In Proceedings of Computer
Supported Collaborative Learning.
Hecht, M., Boster, F., & LaMer, S. (1989). The ef-
fect of extroversion and differentiation on listener-
adapted communication. Communication Reports,
2(1), 1?8.
Huffaker, D., Jorgensen, J., Iacobelli, F., Tepper, P., &
Cassell, J. (2006). Computational measures for lan-
guage similarity across time in online communities.
In In ACTS: Proceedings of the HLT-NAACL 2006
Workshop on Analyzing Conversations in Text and
Speech, pages 15?22.
Jensen, F. V. (1996). An introduction to Bayesian net-
works. UCL Press.
Labov, W. (2010a). Principles of linguistic change:
Internal factors, volume 1. Wiley-Blackwell.
Labov, W. (2010b). Principles of linguistic change:
Social factors, volume 2. Wiley-Blackwell.
Lauritzen, S. L. & Spiegelhalter, D. J. (1988). Local
computations with probabilities on graphical struc-
tures and their application to expert systems. Jour-
nal of the Royal Statistical Society, 50, 157?224.
Levitan, R. & Hirschberg, J. (2011). Measuring
acoustic-prosodic entrainment with respect to mul-
tiple levels and dimensions. In Proceedings of In-
terspeech.
Levitan, R., Gravano, A., & Hirschberg, J. (2011).
Entrainment in speech preceding backchannels. In
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human
Language Technologies: short papers-Volume 2,
pages 113?117. Association for Computational Lin-
guistics.
Liscombe, J., Hirschberg, J., & Venditti, J. (2005). De-
tecting certainness in spoken tutorial dialogues. In
Proceedings of INTERSPEECH, pages 1837?1840.
Citeseer.
Nenkova, A., Gravano, A., & Hirschberg, J. (2008).
High frequency word entrainment in spoken dia-
logue. In In Proceedings of ACL-08: HLT. Asso-
ciation for Computational Linguistics.
opensmile (2011). http://opensmile.sourceforge.net/.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
Purcell, A. (1984). Code shifting hawaiian style: chil-
drens accommodation along a decreolizing contin-
uum. International Journal of the Sociology of Lan-
guage, 1984(46), 71?86.
Putman, W. & Street Jr, R. (1984). The conception
and perception of noncontent speech performance:
Implications for speech-accommodation theory. In-
ternational Journal of the Sociology of Language,
1984(46), 97?114.
Ranganath, R., Jurafsky, D., & McFarland, D. (2009).
It?s not you, it?s me: detecting flirting and its mis-
perception in speed-dates. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1-Volume 1, pages
334?342. Association for Computational Linguis-
tics.
Reitter, D., Keller, F., & Moore, J. D. (2006). Com-
putational modelling of structural priming in dia-
logue. In In Proc. Human Language Technology
conference - North American chapter of the Asso-
ciation for Computational Linguistics annual mtg,
pages 121?124.
Sanders, R. (1987). Cognitive foundations of calcu-
lated speech. State University of New York Press.
Scotton, C. (1985). What the heck, sir: Style shifting
and lexical colouring as features of powerful lan-
796
guage. Sequence and pattern in communicative be-
haviour, pages 103?119.
Wang, Y., Kraut, R., & Levine, J. (2011). To stay or
leave? the relationship of emotional and informa-
tional support to commitment in online health sup-
port groups. In Proceedings of the ACM conference
on computer-supported cooperative work. ACM.
Ward, A. & Litman, D. (2007). Automatically measur-
ing lexical and acoustic/prosodic convergence in tu-
torial dialog corpora. In Proceedings of the SLaTE
Workshop on Speech and Language Technology in
Education. Citeseer.
Welkowitz, J. & Feldstein, S. (1970). Relation of ex-
perimentally manipulated interpersonal perception
and psychological differentiation to the temporal
patterning of conversation. In Proceedings of the
78th Annual Convention of the American Psycho-
logical Association, volume 5, pages 387?388.
797
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 30?38,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Comparison of Latent Variable Models For Conversation Analysis
Sourish Chaudhuri and Bhiksha Raj
Language Technologies Institute,
School of Computer Science,
Carnegie Mellon University,
Pittsburgh, PA - 15213.
{sourishc, bhiksha} @ cs.cmu.edu
Abstract
With the evolution of online communication
methods, conversations are increasingly han-
dled via email, internet forums and other such
methods. In this paper, we attempt to model
lexical information in a context sensitive man-
ner, encoding our belief that the use of lan-
guage depends on the participants in the con-
versation. We model the discourse as a com-
bination of the speaker, the addressee and
other participants in the conversation as well
as a context specific language model. In or-
der to do this, we introduce a novel method
based on an HMM with an exponential state
space to capture speaker-addressee context.
We also study the performance of topic model-
ing frameworks in conversational settings. We
evaluate the models on the tasks of identify-
ing the set of people present in any conver-
sation, as well as identifying the speaker for
every utterance in the conversation, and they
show significant improvement over the base-
line models.
1 Introduction
In this paper, we experiment with different methods
of automatically analyzing discourse. We present
and validate hypotheses on how conversations can
be better analyzed using information about the
speakers, as well as other participants in the con-
versation. We present a novel method of modeling
discourse using an exponential state Hidden Markov
Model where states are based on speakers and ad-
dressees. We also cast the problem into the popular
topic modeling frameworks, and compare the vari-
ous approaches.
Consider a small group of people that a person
knows well. Given a transcript of a discussion on
a topic of mutual interest, that person would likely
be able to identify who is likely to have said what,
based on his knowledge of the speakers and their in-
clinations on various topics. We would like to be
able to encode similar intelligence into a system that
could automatically learn about speakers based on
transcripts of prior conversations, and use that infor-
mation to analyze new conversations.
The scenario we consider in this work is as fol-
lows: we have a known set of characters, any subset
of whom could be present in a conversation. Given
the transcript of a conversation only, without speaker
annotations, we would like to : 1. Predict the set of
participants in the conversation from the character-
istics of the entire conversation, and 2. Identify the
individual speakers at each conversation turn.
In order to do this, we model each utterance in
a conversation as dependent on the speaker, the ad-
dressee and the other people present. As we shall
describe, our models encode the belief that people
speak/behave differently depending on other partic-
ipants in the conversation. This has a two-fold ben-
efit: first, it can help us discover social (or even,
professional) relationship structures; second, it can
help us understand how to respond to different peo-
ple, and incorporate that information into automated
conversational agents which can then behave in a
more context sensitive manner. The ability to auto-
matically model discourse as context specific in this
manner is also useful for other tasks such as directed
advertising and duplicity detection.
In Section 2, we describe relevant related work.
30
Section 3 describes the dataset for our experiments,
Section 4 describes the problem, our use of topic
models, and the novel HMM based method, while
Section 5 summarizes the results and we conclude
in Section 6.
2 Related Work
The task of automatically segmenting speech and
then identifying speakers from audio (Reynolds
and Torres-Carrasquillo, 2005) is referred to as di-
arization and has been well-studied (Tranter and
Reynolds, 2006). More recently, approaches have
been developed to fuse information from both the
audio and video modalities (Noulas et al, 2011)
to improve diarization systems when video informa-
tion is available. In this paper, we attempt to under-
stand just how much information is available in the
text alone. Systems that can work with text only can
be used to improve audio-based systems which can
provide speech recognition output to a text-based
system. They can also be used to work with closed
caption streams, or on human-generated transcrip-
tions of meeting recordings.
Research on identifying speakers from text or lex-
ical information is limited in comparison to work
with audio data. However, efforts have been made
to use discourse level information to automatically
identify speakers to calibrate idiolectal differences
between speakers (Doddington, 2001). (Canseco et
al., 2005, ) investigated the use of lexical features
to automatically diarize (but not actually identify)
transcripts to determine if a current speaker contin-
ued or a previous speaker spoke or the next speaker
spoke. Lei and Mirghafori (2 007) attempted to in-
corporate idiolect based speaker information by us-
ing word conditioning of phone N -grams to recog-
nize speakers in dialogs with 2 speakers.
In our work, the models we use to identify speak-
ers are powerful enough to predict the addressee as
well. In this context, we note that several attempts
have been made recently to automatically identify
addressees in dialog settings. These approaches
have used information about the context and con-
tent of the utterance, using dialog acts and informa-
tion about the speaker?s gaze to aid classifier per-
formance (Jovanovic et al, 2006). Den Akker and
Traum (2009) proposed rule-based methods for ad-
dressee classification. Unlike in these works, we
attempt to jointly model both the speaker and the
addressee as one of our proposed approaches. This
is similar to the approach employed by (Otsuka et
al., 2005, ), who proposed a Dynamic Bayesian Net-
work model to understand multiparty conversation
structure using non-verbal cues only? eye gaze, fa-
cial expression, gesticulations and posture.
3 Data
The data for our experiments consists of fan-
sourced transcripts of the episodes of the sitcom
F.R.I.E.N.D.S. The structure of the data is as fol-
lows: we have a set of conversations as training data.
Each conversation contains a sequence of turns, with
each turn annotated with its speaker. We do not
have any information about the addressee from the
dataset. We do, however, have implicit informa-
tion of the set of speakers within a conversation seg-
ment (we make the assumption here that if a char-
acter doesn?t speak in a segment, he is not present).
Annotator notes appear periodically to indicate that
the scene changed or that new characters entered the
scene or that some characters left the scene. We
treat these annotator notes as conversation bound-
aries and the segment of turns between two such
boundaries constitutes one conversation instance.
The set of characters used for our experiments is
finite. The 6 primary characters in the sitcom (Chan-
dler, Joey, Monica, Phoebe, Rachel and Ross) are
retained. In addition to these 6 primary characters,
there are a number of supporting characters who ap-
pear occasionally. We use Other to denote all other
characters, as the amount of data for a number of the
supporting characters is quite small and would not
result in learning useful patterns regarding their be-
havior. As a result, we treat all of these characters
as one character that can be thought of as a univer-
sal supporting character. Hence, we have a total of
7 possible characters. Any subset of these 7 char-
acters could be part of a conversation. Below is an
example of a pair of conversations from our dataset:
[EVENT]
Paul: thank you! thank you so much!
Monica: stop!
Paul: no, i?m telling you last night was like umm,
all my birthdays, both graduations, plus the barn
31
raising scene in witness.
Monica: we?ll talk later.
Paul: yeah. thank you.
[EVENT]
Joey: that wasn?t a real date?! what the hell do
you do on a real date?
Monica: shut up, and put my table back.
All: okayyy!
[EVENT]
The event markers are tags inserted at pre-
processing time, to denote transcriber annotations
such as characters entering or leaving scenes. The
sequence of turns between two event markers are
treated as one conversation. Also, note the character
Paul in the first conversation in the example above
? when training the system, the content of Paul?s ut-
terances are used to train the model for Other, since
Paul is not one of the primary characters that we
track. At test time, the input looks similar to the
above, except that the turns are not annotated by
speaker.
The transcripts used in our experiments are seg-
mented by speaker turns, so that consecutive turns
are uttered by different speakers. The entire set of
230 episodes was split randomly into training, de-
velopment and test splits. Sequential information
for the individual conversations were not used. Each
episode was further divided into conversations based
on the scene boundaries denoted by the transcribers.
For training, overall, we used 195 episodes from
F.R.I.E.N.D.S, with a total of 9,171 conversations
and a total 52,516 turns. The average length in num-
ber of turns for each conversation was 5.73. The
test set consisted of a total of 20 episodes with 855
conversations and 4,981 turns. The average length
of a conversation in the test set was 5.83. The re-
maining 15 episodes were used as development data
to tune hyperparameters ? this set consisted of 529
conversations and 2,984 turns in total. The distribu-
tion of the number of utterances by speakers across
the training, test and development set are shown in
Figure 1. As one can observe, the distribution is not
particularly skewed for any of the speakers across
the splits of the dataset.
Figure 1: Distribution of #utterances for each speaker in
the dataset.
4 Conversation Models
Previous work in analyzing participants in a conver-
sation have used meeting data, with a fixed number
of participants. In our task, the total number of pos-
sible participants is finite, but we do not have in-
formation on how many of them are present at any
particular instant. Thus, our model first attempts to
detect the participants in a segment of conversation,
and then attempts to attribute speaker turns to indi-
viduals.
Our model for discourse structure is based on two
premises. First, we believe that what a person says
will depend on who he or she is speaking to. Intu-
itively, consider a person trying to make the same
point to his boss and (at a different time and place)
to his friend. It is likely that he will be more formal
with his boss than his friend. Second, if the speaker
addresses someone specifically in a group of people,
knowing who he addressed would likely help us pre-
dict better who would speak next. We assume that
the first hypothesis above also holds for groups of
people in conversations, where the topics and their
distribution in discussions (and words that affect the
tone of the discussion) depend on the participants.
As described earlier, we evaluate our models on
two tasks. First, we would like to identify the set of
characters present in any conversation. Given seg-
ments of conversation, we attempt to understand the
distribution of topics for specific subsets of charac-
ters present in that segment. To do this, we cast
this problem into a topic modeling framework ? we
experiment with the Author-Topic model (Rosen-
32
Zvi et al, 2004), described in Section 4.1, for this
task. We use the Author Topic Model to link the co-
occurrence information of characters with the words
in the conversation.
Second, we attempt to attribute speakers to ut-
terances, described in Section 4.2. We introduce a
novel approach using an HMM with an exponen-
tial state space to model speakers and addressees,
described in Section 4.2.1. We also use the Author
Topic Model and the Author-Recipient Topic Model
(McCallum et al, 2007), described in Section 4.2.2
for this task. The key difference between the HMM-
based model and the topic model based approaches
is that the former explicitly takes sequence informa-
tion into account.
4.1 Identifying Character Subset Presence
The premise behind attempting to model subsets of
characters is that the nature of the conversation de-
pends on the group of people participating. For in-
stance, it seems intuitively likely that the content of
a conversation between two friends would be differ-
ent if they were the only ones present than it would
be if their families were also present. To extend this
hypothesis to a general scenario, the content of each
speaker?s turn depends not only on the speaker, but
also on the person being spoken to as well as the
other people present. To model this, we require a
model that captures the distribution of the text for
entire conversation, for each possible subset of char-
acters. In this section, we describe the training of a
generic model for conversations, and use it to pro-
duce features for a discriminative classifier.
Let there be N characters who could participate
in a conversation. We assume a general scenario,
where any subset of these characters may be present.
Thus, there are 2N?1 character subsets that are pos-
sible. We can model this as a multi-class classifica-
tion problem (we will refer to this as subset model-
ing, henceforth).
The generative model for this task is as follows:
Each conversation segment is associated with a set
of utterances, and a set of characters. For each such
set of characters, we associate a distribution over
topics. For each word that is present in the seg-
ment, we select a topic from the subset-specific topic
distribution, and then we select the word from that
topic. Figure 2 shows the graphical model for this in
plate notation.
Figure 2: Graphical representation of the subset model in
plate notation
In the plate notation, the observed variables are
shaded and the latent variables are unshaded. Plates
encapsulate a set of variables which are repeatedly
sampled a fixed number of times, and the number at
the bottom right indicates this fixed number.
Sc represents a subset of the characters who were
present in the conversation segment. We have C
such conversations, and each conversation contains
Nc words. z represents the latent topic variable, and
? represents the multinomial topic distribution for
each subset of characters (there are 2N such sub-
sets). The multinomial distribution of topics has a
prior distribution characterized by ?. Similarly, ev-
ery topic (there are a set of T topics) has a multino-
mial distribution ? over the words in the vocabulary,
and ? has a prior distribution characterized by ?.
For every conversation in the training corpus, the
set of characters present is known. The content of
the conversation is treated as a bag of words. From
the topic distribution for the subset of characters
present, we sample a topic. Based on the word dis-
tributions for this topic, we sample a word. This
process is repeated Nc times corresponding to the
number of words in the conversation. The entire
process of generating a conversation is repeated C
times, corresponding to the number of conversations
in the training corpus.
Depending on the value of N , the number of pos-
33
sible classes may be very high. Training a large
number of models may lead to a data scarcity, es-
pecially given the high dimensionality of language
data. We therefore slightly modify the model, so that
instead of topic distributions for each possible sub-
set, we have a topic distribution for each character,
and the distribution of topics in the conversation is
a mixture of the topic distributions for each charac-
ter. This leads us to a graphical model that has been
well-studied in the past ? the Author-Topic model
(ATM, henceforth) and is shown in Figure 3.
Figure 3: Graphical representation of the simplified sub-
set model in plate notation
Thus, given the set of characters present, we sam-
ple one of them (x) from a uniform distribution.
Then we generate a topic by sampling from the dis-
tribution of topics for that speaker. The rest of the
process remains the same.We use this model to help
us predict which subset of characters was present in
a given conversation.
We learn speaker-specific topic distributions us-
ing the ATM. In order to predict characters present
in a test conversation, we train binary SVM (Shawe-
Taylor and Cristianini, 2000) classifiers for each
speaker in the following manner: we compute the
distribution of the speaker-specific topics in each
conversation, and use these as the features of the
data point. If the speaker was present in the con-
versation, the data point corresponding to the con-
versation has a class label of +1, else -1. A linear
SVM classifier is trained over the data. At test time,
we compute the distribution of the speaker?s topics
in the conversation, and use the SVM to predict if
the speaker was present or not.
4.2 Identifying Speakers From Utterances
In this section, we describe our approach to identi-
fying speakers from the text of the utterance. The
ATM (as described above) treats all the participants
in the conversation as being potential contributors to
each turn. However, we can also use the ATM to
predict speakers directly. In this case, we will use
each turn as analogous to a document. Each such
document has only one author and the author topic
model can be used to learn models for each author.
The plate notation for this would look very similar
to the one in Figure 2, except that instead of a sub-
set of characters being observed, only one would be
observed, and the number of possible topic distribu-
tions would be equal to the number of characters.
The ATM for this task does not take any context
information into account. In the following subsec-
tion, we introduce a novel HMM based approach
that seeks to leverage information from the sequence
of turns.
4.2.1 Exponential State Hidden Markov Model
In this model, we assign a state to each speaker-
addressee combination possible. If our data consists
ofN characters, only one of theN characters will be
speaking at any given point. He/She may be speak-
ing to any combination of the remainingN?1 char-
acters. Thus, the number of states in this model is
N ?2(N?1). Note that the addressee is not observed
directly from the data.
The sequence of turns in a conversation is mod-
eled by a Hidden Markov Model (Rabiner, 1989).
At each time instant, the speaker corresponding to
the state speaks a turn, which is the observed emis-
sion, before transitioning to another state at the next
time instant. The state at the next time instant is con-
strained to have a different speaker.
The model is trained using the standard Baum-
Welch training. The emission probabilities are cap-
tured by a trigram language model, trained using
the SRILM toolkit (Stolcke, 2002). The parameters
of the model are initialized as follows: for emis-
sion probabilities, we take all the utterances by a
speaker and distributing them uniformly among the
34
states that have that speaker, since we do not have
direct information about the addressees. For tran-
sition probabilities, we initialize with a bias instead
of uniformly. Given a conversation, for a state with
speaker A and set of addressees (R, say ? Note
that R may have multiple characters), we give equal
probabilities of transitioning to all states that have
one of the characters in R as the speaker. Now, we
pick the set of speakers (call it M ) that uttered the
next three turns (essentially, we look ahead in the
data stream to see who the next 3 speakers are while
training). We add a bias to every state with A as
the speaker, and every possible combination of the
speakers in M , to encode the hypothesis that the ad-
dressee would be likely to speak pretty soon, if not
directly after.
The large state space in this model makes compu-
tation extremely expensive. However, an examina-
tion of the posterior probabilities show that a number
of states are rarely, or never, entered. We prune away
such states after every 5 iterations in the following
manner ? we use the current parameters of the model
after each iteration to identify the speakers of each
turn on the development set. Decoding of a sequence
of turns at test time is done using the Viterbi algo-
rithm. However, instead of using the best path only,
we keep track of the top 10 best paths. Thus, after
an iteration of training, we test on the development
data, and obtain 10 possible sequences of speakers
for each conversation. Over 5 iterations, we have
the 50 best paths for each conversation. We then
compute the average number of states entered in all
the decoded paths obtained. If the average number
of times a state was entered is ?, then any state that
was entered less than k ? ? times (k = 0.02, for
our experiments), according to the posterior proba-
bilities was pruned out. In order to set the value of
k, the development set was split into 2 halves, with
one half being used to compute the average number
of times a state is entered across the 10 best decodes
for data in that half. For different values of k, accu-
racy of speaker identification on the 1-best decode
was computed on the other half of the development
set, for values of k from 0.005 to 0.1.
The optimal state sequence at test time also con-
tains information about the addressee. For the tasks
we evaluate, this information is not directly used.
However, in other applications, such as those in-
volving automated agents, this information could be
valuable in triggering the agent.
4.2.2 Author-Recipient Topic Model
The Author Recipient Topic Model (McCallum et
al., 2007) (ARTM, henceforth) was used for discov-
ering topics and roles in social networks. It is built
over the Author-Topic Model discussed previously,
with the exception that messages are conditioned on
the sender as well as the receivers. The graphical
model in plate notation is shown in Figure 4.
Figure 4: Graphical representation of the Author-
Recipient Topic model in plate notation
Here, we model each turn as having a set of Nt
words. Each turn has one speaker S, and a set of
addressees At. The generative model works as fol-
lows: For each word in a turn, sample an addressee
a from the set of addressees. Topic distributions are
now conditioned over speaker-addressee pairs, in-
stead of only the speaker as we saw in the ATM.
A topic is now sampled from the speaker-addressee
specific topic distribution. A word is now sampled
from this topic using the topic specific word distri-
butions. The parameters ?, ?, and z have the same
meaning as in the ATM described earlier.
Note that the set of addressees in our setting is
not explicitly observed. We know the participants in
the conversation at training time, and we know the
speaker, but we do not know who was addressed.
Since we do not have information to make a better
choice of addressee, we model the entire set of par-
35
ticipants without the speaker as the set of addressees,
in this model.
For the task of identifying the speaker who uttered
the turn, we employ an approach, similar to the one
used for ATM. We train speaker-addressee-specific
models. The feature set for this task includes fea-
tures not only from the turn itself, but also from
the context. Thus, we have the distribution of the
topics in the turn for every speaker-addressee pair
with the right speaker, the speakers of the previ-
ous two turns, and the distribution of topics of the
speaker of the current turn over the previous two
turns. (Thus, while the model does not explicitly
model sequence, as an HMM does, it utilizes con-
text information in its feature space.) Using these
features, we train a linear SVM to predict whether
or not the speaker uttered the turn. In this case, we
could potentially have multiple speakers (or none of
them) predicted to have uttered the same turn. In
that case, we choose the speaker with the maximum
distance from the margin.
4.3 Baseline Models
In this section, we set up simple baseline models to
evaluate our performance against. We describe how
we set up a random baseline, a Naive Bayes baseline
and an HMM baseline model.
4.3.1 Random Baseline
For the task of identifying the set of charac-
ters present in a conversation, the random baseline
would work as follows: it knows that the number of
characters present in any conversation lies between
1 and N (N = 7, in this case). (Note that monologues,
with only 1 person being present, are possible. Typ-
ically, in our data, they happen at the beginning or
end of scenes.) Thus, it randomly decides if each
of these characters are present or not in any given
conversation.
Suppose that the total number of characters are n
and r of them are actually present in the conversa-
tion. Let us say the random guess system predicts
t of the characters to be present. If we use the uni-
form distribution for picking t, then P (t) = 17 , ?t ?
[1, 2, ..., 7], in this case. For any given t, the proba-
bility that we get k correct is given by:
P (k|t) =
(r
k
)
?
(n?r
t?k
)
(n
t
) (1)
To compute the probability of getting k right, we
marginalize out the number of characters guessed to
be present, t:
P (k) =
?
t
P (k, t) =
?
t
P (k|t).P (t) (2)
Now we can compute the probability of getting k
correct by randomly guessing, for all k from 0 to r.
Using these, we can compute the expected number
of correct guesses, which turns out to be 0.571.r for
an average recall would be 57.1%.
For the task of identifying the characters, every
turn could have been uttered by one of the n charac-
ters (n = 7, for our case). Thus, the average accu-
racy at identifying turns would be 17 or 14.29%.
4.3.2 Naive Bayes Classifier
For the task of predicting the subset of speakers,
we set up a Naive Bayes using words as features.
We build up a term-document matrix, with each con-
versation treated as a document. For each charac-
ter, we train a binary classifier using the training
data- conversations where the character was present
were marked as a positive instance for that charac-
ter, and ones where he was not present were marked
as negative instances. We experimented both with
using priors based on the empirical distribution in
the training data and with using uniform prior (i.e.
P (character) = 0.5). Given a test conversation,
we use individual classifiers for each of the charac-
ters to determine whether he/she was present or not.
For the task of identifying speakers, given an ut-
terance, the Naive Bayes classifier is set up as fol-
lows: Again, we create term-document matrices for
each of the speakers, where a document is a turn ut-
tered by the speaker. Turns uttered by that speaker
are positive instances and those uttered by someone
else are negative instances. For each speaker, we
compute the Naive Bayes probability ratio (odds) of
him uttering the turn and not uttering the turn, in or-
der to decide. If multiple speakers are classified as
having uttered the turn, or no speaker is classified
as having uttered the turn, the speaker with the best
odds of having uttered the turn is selected.
36
System Precision Recall
Author Topic Model 63.22% 74.71%
NB 52.33% 44.19%
NB-prior 68.31% 36.25%
Random Baseline 28.05% 57.1%
Table 1: Results for predicting subset of characters
present
4.3.3 Single Speaker HMM
This model is only used to attribute speakers to
turns. Section 4.2.1 described an HMM model
that captures speaker-addressee information. In the
single- speaker HMM, we have a state for each
speaker. Emission probabilities are given by a tri-
gram language model that is trained on the speaker?s
utterances in the training data. The transition proba-
bilities are initialized as per the empirical transitions
between speakers in the data. This model does not
capture any kind of addressee information.
5 Results
In this section, we present results of our experi-
ments with the models we described earlier, on the
two tasks, identifying the set of speakers in any
given conversation and identifying individual speak-
ers who uttered each turn in a conversation.
For the task of identifying the set of speakers in
any given conversation, we evaluate performance
using precision and recall, which are defined as fol-
lows: If the conversation actually contained r char-
acters, the system predicted that it contained t char-
acters, and got k right, then:
Precision =
k
t
;Recall =
k
r
(3)
The results are summarized in Table 1. In the ta-
ble, NB-prior indicates that the prior for the binary
classifier was determined based on the number of
conversations each character appeared in, while NB
indicates that the prior was uniform (i.e., for each
character, P (present) = P (absent) = 0.5). We
find that the results obtained using the author-topic
model are significantly better than each of the other
three models.
On average, the number of speakers in each con-
versation in the test data was 2.44 (the correspond-
System Accuracy
ESHMM 27.13%
Speaker-LM HMM 25.04%
ARTM 23.64%
Author Topic Model 26.2%
NB 23.41%
NB-prior 21.39%
Random Baseline 14.29%
Table 2: Results for predicting speakers of utterances
ing number in the training data appears to be some-
what higher at 2.65). Our attempts to restrict the set
of characters in a real setting plays a significant role
here as we shall discuss later.
The Naive Bayes classifier with empirical priors
on average predicted that there were 1.3 characters
present per conversation, while the version with uni-
form priors predicted 2.2 characters to be present per
conversation on average. The author-topic model,
on average, over-estimated the number of characters
at 2.86 characters per conversation.
For the task of predicting the speaker, given an ut-
terance, we have two kinds of Hidden Markov Mod-
els, the Exponential State HMM (ESHMM) and and
HMM with emission probabilities based on individ-
ual speaker language models (Speaker LM HMM).
We also have the topic model based systems- the
ARTM and the ATM. Finally, we have the baseline
models- the Naive Bayes with empirical priors and
with uniform priors, and the random baseline. Table
2 summarizes their performance. In this case, we
only report accuracy. Since each turn has only one
speaker, we can constrain each of the models to pro-
duce one speaker, in order to calculate the accuracy.
The HMM and topic based models all incorporate
sequence information in some form. In the case of
the HMM based models, state transitions are condi-
tioned on the previous speaker. In the case of the
topic model based systems, the feature vectors con-
tain context, although the task is modeled as a dis-
criminative classification task. The ESHMM model
worked the best on this dataset. With the exception
of the ATM and the speaker LM HMM (p < 0.10),
the improvements obtained by using the ESHMM
over all other models were statistically significant
(p < 0.05). Surprisingly, the single speaker LM
37
HMM and the ATM both outperform the ARTM on
this task. One of the reasons for this could be that
the ARTM does not suitably capture what we hoped
it would, perhaps because of the fact that the recipi-
ents (addressees) are not observed.
6 Conclusion
In this paper, we presented a set of latent variable
model based approaches to analyzing conversation
structure using the text transcript of the conversa-
tions only. The initial set of experiments show
promising improvements over simple baseline meth-
ods, though the overall results leave considerable
room for improvement. Conversations are a dy-
namic process, with the content varying significantly
with time, and the use of formulations such as dy-
namic topic modeling (Blei and Lafferty, 2006) may
help.
We believe that the concept of modeling speak-
ers and addressees would be a powerful one in mod-
eling conversation structure and useful in applica-
tions such as those involving automated agents, or
in understanding discourse on discussion forums, as
well as understanding development of authority in
such forums. The state sequences predicted by the
ESHMM implicitly predict addressees for each turn.
This is not directly used in our tasks, but could be
useful for automated agents, in understanding appro-
priate moments to take its turn.
The dataset used in this case introduced some
noise. We decided to subsume everyone aside from
the 6 main characters under the moniker other, in or-
der to keep the state space manageable. In reality, it
was a collection of a few dozen characters, some of
whom appeared intermittently through the episodes.
As a result, the emission model for this state was not
a stable one. The system rarely predicted this class,
and had very low accuracy when it did.
Further, development of datasets with annotations
specifying the addressees explicitly would probably
accelerate development of methods that work well
in such settings.
References
Andrew McCallum, Xuerui Wang and Andres Corrada-
Emmanuel. 2007. Topic and Role Discovery in Social
Networks with Experiments on Enron and Academic
Email. In Journal of Artificial Intelligence Research..
Andreas Stolcke. 2002. SRILM an Extensible Language
Modeling Toolkit. In ICSLP.
D. A. Reynolds and P. Torres-Carrasquillo. 2005. Ap-
proaches and applications of audio diarization. In
Proc. of ICASSP.
David M. Blei and John D. Lafferty. 2006. Dynamic
Topic Models. In Proceedings of ICML.
George Doddington. 2001. Speaker Recognition based
on Idiolectal Differences between Speakers. In Eu-
rospeech..
S.E. Tranter and D.A. Reynolds. 2006. An overview of
automatic speaker diarization systems. In IEEE Trans-
actions on Audio, Speech and Language Processing..
Athanasios Noulas, Gwenn Englebienne, Ben J.A. Krse
2011. Multimodal Speaker Diarization. In IEEE
Transactions on Pattern Analysis and Machine Intel-
ligence..
Howard Lei and Nikki Mirghafori. 2007. Word-
Conditioned Phone n-grams For Speaker Recognition.
In Proceedings of ICASSP
John Shawe Taylor and Nello Cristianini. 2000. Sup-
port Vector Machines and other Kernel Based Learn-
ing Methods. Cambridge University Press..
Kazuhiro Otsuka, Yoshinao Takemae and Junji Yam-
ato. 2005. A probabilistic inference of multiparty-
conversation structure based on Markov-switching
models of gaze patterns, head directions, and utter-
ances. In Proceedings of the 7th international con-
ference on Multimodal interfaces.
L.R. Rabiner. 1989. A tutorial on Hidden Markov Mod-
els and selected applications in speech recognition. In
Proceedings of IEEE.
Leonardo Canseco, Lori Lamel and Jean-Luc Gauvain
2005. A Comparative Study using Manual and Auto-
matic Transcriptions for Diarization. In Proceedings
of ASRU.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers and
Padhraic Smyth. 2004. The Author-Topic Model for
Authors and Documents. In 20th Conference on Un-
certainty in Artificial Intelligence.
Natasa Jovanovic, Rieks op den Akker and Anton Nijholt.
2006. Addressee Identification in Face-to-Face Meet-
ings In Proc. of EACL.
Rieks op den Akker and David Traum. 2009. A Compar-
ison of Addressee Detection Methods for Multiparty
Conversations. In Proc. of Diaholmia 2009.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc of UAI..
Xavier Anguera, Chuck Wooters and Javier Hernando
2005. Speaker Diarization for Multi-Party Meetings
using Acoustic Fusion In IEEE Workshop on Auto-
matic Speech Recognition and Understanding..
38
