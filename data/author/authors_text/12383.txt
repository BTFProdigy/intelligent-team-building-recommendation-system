Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880?889,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Polylingual Topic Models
David Mimno Hanna M. Wallach Jason Naradowsky
University of Massachusetts, Amherst
Amherst, MA 01003
{mimno, wallach, narad, dasmith, mccallum}cs.umass.edu
David A. Smith Andrew McCallum
Abstract
Topic models are a useful tool for analyz-
ing large text collections, but have previ-
ously been applied in only monolingual,
or at most bilingual, contexts. Mean-
while, massive collections of interlinked
documents in dozens of languages, such
as Wikipedia, are now widely available,
calling for tools that can characterize con-
tent in many languages. We introduce a
polylingual topic model that discovers top-
ics aligned across multiple languages. We
explore the model?s characteristics using
two large corpora, each with over ten dif-
ferent languages, and demonstrate its use-
fulness in supporting machine translation
and tracking topic trends across languages.
1 Introduction
Statistical topic models have emerged as an in-
creasingly useful analysis tool for large text col-
lections. Topic models have been used for analyz-
ing topic trends in research literature (Mann et al,
2006; Hall et al, 2008), inferring captions for im-
ages (Blei and Jordan, 2003), social network anal-
ysis in email (McCallum et al, 2005), and expand-
ing queries with topically related words in infor-
mation retrieval (Wei and Croft, 2006). Much of
this work, however, has occurred in monolingual
contexts. In an increasingly connected world, the
ability to access documents in many languages has
become both a strategic asset and a personally en-
riching experience. In this paper, we present the
polylingual topic model (PLTM). We demonstrate
its utility and explore its characteristics using two
polylingual corpora: proceedings of the European
parliament (in eleven languages) and a collection
of Wikipedia articles (in twelve languages).
There are many potential applications for
polylingual topic models. Although research liter-
ature is typically written in English, bibliographic
databases often contain substantial quantities of
work in other languages. To perform topic-based
bibliometric analysis on these collections, it is
necessary to have topic models that are aligned
across languages. Such analysis could be sig-
nificant in tracking international research trends,
where language barriers slow the transfer of ideas.
Previous work on bilingual topic modeling
has focused on machine translation applications,
which rely on sentence-aligned parallel transla-
tions. However, the growth of the internet, and
in particular Wikipedia, has made vast corpora
of topically comparable texts?documents that are
topically similar but are not direct translations of
one another?considerably more abundant than
ever before. We argue that topic modeling is
both a useful and appropriate tool for leveraging
correspondences between semantically compara-
ble documents in multiple different languages.
In this paper, we use two polylingual corpora
to answer various critical questions related to
polylingual topic models. We employ a set of di-
rect translations, the EuroParl corpus, to evaluate
whether PLTM can accurately infer topics when
documents genuinely contain the same content.
We also explore how the characteristics of dif-
ferent languages affect topic model performance.
The second corpus, Wikipedia articles in twelve
languages, contains sets of documents that are not
translations of one another, but are very likely to
be about similar concepts. We use this corpus
to explore the ability of the model both to infer
similarities between vocabularies in different lan-
guages, and to detect differences in topic emphasis
between languages. The internet makes it possible
for people all over the world to access documents
from different cultures, but readers will not be flu-
ent in this wide variety of languages. By linking
topics across languages, polylingual topic mod-
els can increase cross-cultural understanding by
providing readers with the ability to characterize
880
the contents of collections in unfamiliar languages
and identify trends in topic prevalence.
2 Related Work
Bilingual topic models for parallel texts with
word-to-word alignments have been studied pre-
viously using the HM-bitam model (Zhao and
Xing, 2007). Tam, Lane and Schultz (Tam et
al., 2007) also show improvements in machine
translation using bilingual topic models. Both
of these translation-focused topic models infer
word-to-word alignments as part of their inference
procedures, which would become exponentially
more complex if additional languages were added.
We take a simpler approach that is more suit-
able for topically similar document tuples (where
documents are not direct translations of one an-
other) in more than two languages. A recent ex-
tended abstract, developed concurrently by Ni et
al. (Ni et al, 2009), discusses a multilingual topic
model similar to the one presented here. How-
ever, they evaluate their model on only two lan-
guages (English and Chinese), and do not use the
model to detect differences between languages.
They also provide little analysis of the differ-
ences between polylingual and single-language
topic models. Outside of the field of topic mod-
eling, Kawaba et al (Kawaba et al, 2008) use
a Wikipedia-based model to perform sentiment
analysis of blog posts. They find, for example,
that English blog posts about the Nintendo Wii of-
ten relate to a hack, which cannot be mentioned in
Japanese posts due to Japanese intellectual prop-
erty law. Similarly, posts about whaling often
use (positive) nationalist language in Japanese and
(negative) environmentalist language in English.
3 Polylingual Topic Model
The polylingual topic model (PLTM) is an exten-
sion of latent Dirichlet alocation (LDA) (Blei et
al., 2003) for modeling polylingual document tu-
ples. Each tuple is a set of documents that are
loosely equivalent to each other, but written in dif-
ferent languages, e.g., corresponding Wikipedia
articles in French, English and German. PLTM as-
sumes that the documents in a tuple share the same
tuple-specific distribution over topics. This is un-
like LDA, in which each document is assumed to
have its own document-specific distribution over
topics. Additionally, PLTM assumes that each
?topic? consists of a set of discrete distributions
D
N
1
T
N
L
...
w
? ?
wz
z
...
?
1
?
L
?
1
?
L
Figure 1: Graphical model for PLTM.
over words?one for each language l = 1, . . . , L.
In other words, rather than using a single set of
topics ? = {?
1
, . . . ,?
T
}, as in LDA, there are L
sets of language-specific topics, ?
1
, . . . ,?
L
, each
of which is drawn from a language-specific sym-
metric Dirichlet with concentration parameter ?
l
.
3.1 Generative Process
A new document tuplew = (w
1
, . . . ,w
L
) is gen-
erated by first drawing a tuple-specific topic dis-
tribution from an asymmetric Dirichlet prior with
concentration parameter ? and base measurem:
? ? Dir (?, ?m). (1)
Then, for each language l, a latent topic assign-
ment is drawn for each token in that language:
z
l
? P (z
l
|?) =
?
n
?
z
l
n
. (2)
Finally, the observed tokens are themselves drawn
using the language-specific topic parameters:
w
l
? P (w
l
| z
l
,?
l
) =
?
n
?
l
w
l
n
|z
l
n
. (3)
The graphical model is shown in figure 1.
3.2 Inference
Given a corpus of training and test document
tuples?W and W
?
, respectively?two possible
inference tasks of interest are: computing the
probability of the test tuples given the training
tuples and inferring latent topic assignments for
test documents. These tasks can either be accom-
plished by averaging over samples of ?
1
, . . . ,?
L
and ?m from P (?
1
, . . . ,?
L
, ?m |W
?
, ?) or by
evaluating a point estimate. We take the lat-
ter approach, and use the MAP estimate for ?m
and the predictive distributions over words for
?
1
, . . . ,?
L
. The probability of held-out docu-
ment tuples W
?
given training tuples W is then
approximated by P (W
?
|?
1
, . . . ,?
L
, ?m).
Topic assignments for a test document tuple
w = (w
1
, . . . ,w
L
) can be inferred using Gibbs
881
sampling. Gibbs sampling involves sequentially
resampling each z
l
n
from its conditional posterior:
P (z
l
n
= t |w, z
\l,n
,?
1
, . . . ,?
L
, ?m)
? ?
l
w
l
n
|t
(N
t
)
\l,n
+ ?m
t
?
t
N
t
? 1 + ?
, (4)
where z
\l,n
is the current set of topic assignments
for all other tokens in the tuple, while (N
t
)
\l,n
is
the number of occurrences of topic t in the tuple,
excluding z
l
n
, the variable being resampled.
4 Results on Parallel Text
Our first set of experiments focuses on document
tuples that are known to consist of direct transla-
tions. In this case, we can be confident that the
topic distribution is genuinely shared across all
languages. Although direct translations in multi-
ple languages are relatively rare (in contrast with
comparable documents), we use direct translations
to explore the characteristics of the model.
4.1 Data Set
The EuroParl corpus consists of parallel texts in
eleven western European languages: Danish, Ger-
man, Greek, English, Spanish, Finnish, French,
Italian, Dutch, Portuguese and Swedish. These
texts consist of roughly a decade of proceedings
of the European parliament. For our purposes we
use alignments at the speech level rather than the
sentence level, as in many translation tasks using
this corpus. We also remove the twenty-five most
frequent word types for efficiency reasons. The
remaining collection consists of over 121 million
words. Details by language are shown in Table 1.
Table 1: Average document length, # documents, and
unique word types per 10,000 tokens in the EuroParl corpus.
Lang. Avg. leng. # docs types/10k
DA 160.153 65245 121.4
DE 178.689 66497 124.5
EL 171.289 46317 124.2
EN 176.450 69522 43.1
ES 170.536 65929 59.5
FI 161.293 60822 336.2
FR 186.742 67430 54.8
IT 187.451 66035 69.5
NL 176.114 66952 80.8
PT 183.410 65718 68.2
SV 154.605 58011 136.1
Models are trained using 1000 iterations of
Gibbs sampling. Each language-specific topic?
word concentration parameter ?
l
is set to 0.01.
centralbank europ?iske ecb s l?n centralbanks 
zentralbank ezb bank europ?ischen investitionsbank darlehen 
??????? ???????? ???????? ??? ????????? ???????? 
bank central ecb banks european monetary 
banco central europeo bce bancos centrales 
keskuspankin ekp n euroopan keskuspankki eip 
banque centrale bce europ?enne banques mon?taire 
banca centrale bce europea banche prestiti 
bank centrale ecb europese banken leningen 
banco central europeu bce bancos empr?stimos 
centralbanken europeiska ecb centralbankens s l?n 
b?rn familie udnyttelse b?rns b?rnene seksuel 
kinder kindern familie ausbeutung familien eltern 
?????? ??????? ?????????? ??????????? ?????? ???????? 
children family child sexual families exploitation 
ni?os familia hijos sexual infantil menores 
lasten lapsia lapset perheen lapsen lapsiin 
enfants famille enfant parents exploitation familles 
bambini famiglia figli minori sessuale sfruttamento 
kinderen kind gezin seksuele ouders familie 
crian?as fam?lia filhos sexual crian?a infantil 
barn barnen familjen sexuellt familj utnyttjande 
m?l n? m?ls?tninger m?let m?ls?tning opn? 
ziel ziele erreichen zielen erreicht zielsetzungen 
??????? ????? ?????? ?????? ?????? ???????? 
objective objectives achieve aim ambitious set 
objetivo objetivos alcanzar conseguir lograr estos 
tavoite tavoitteet tavoitteena tavoitteiden tavoitteita tavoitteen 
objectif objectifs atteindre but cet ambitieux 
obiettivo obiettivi raggiungere degli scopo quello 
doelstellingen doel doelstelling bereiken bereikt doelen 
objectivo objectivos alcan?ar atingir ambicioso conseguir 
m?l m?let uppn? m?len m?ls?ttningar m?ls?ttning 
andre anden side ene andet ?vrige 
anderen andere einen wie andererseits anderer 
????? ???? ???? ????? ?????? ???? 
other one hand others another there 
otros otras otro otra parte dem?s 
muiden toisaalta muita muut muihin muun 
autres autre part c?t? ailleurs m?me 
altri altre altro altra dall parte 
andere anderzijds anderen ander als kant 
outros outras outro lado outra noutros 
andra sidan ? annat ena annan 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
DA
DE
EL
EN
ES
FI
FR
IT
NL
PT
SV
 
Figure 2: EuroParl topics (T=400)
The concentration parameter ? for the prior over
document-specific topic distributions is initialized
to 0.01T , while the base measure m is initialized
to the uniform distribution. Hyperparameters ?m
are re-estimated every 10 Gibbs iterations.
4.2 Analysis of Trained Models
Figure 2 shows the most probable words in all lan-
guages for four example topics, from PLTM with
400 topics. The first topic contains words relating
to the European Central Bank. This topic provides
an illustration of the variation in technical ter-
minology captured by PLTM, including the wide
array of acronyms used by different languages.
The second topic, concerning children, demon-
strates the variability of everyday terminology: al-
though the four Romance languages are closely
882
related, they use etymologically unrelated words
for children. (Interestingly, all languages except
Greek and Finnish use closely related words for
?youth? or ?young? in a separate topic.) The third
topic demonstrates differences in inflectional vari-
ation. English and the Romance languages use
only singular and plural versions of ?objective.?
The other Germanic languages include compound
words, while Greek and Finnish are dominated by
inflected variants of the same lexical item. The fi-
nal topic demonstrates that PLTM effectively clus-
ters ?syntactic? words, as well as more semanti-
cally specific nouns, adjectives and verbs.
Although the topics in figure 2 seem highly fo-
cused, it is interesting to ask whether the model
is genuinely learning mixtures of topics or simply
assigning entire document tuples to single topics.
To answer this question, we compute the posterior
probability of each topic in each tuple under the
trained model. If the model assigns all tokens in
a tuple to a single topic, the maximum posterior
topic probability for that tuple will be near to 1.0.
If the model assigns topics uniformly, the maxi-
mum topic probability will be near 1/T . We com-
pute histograms of these maximum topic prob-
abilities for T ? {50, 100, 200, 400, 800}. For
clarity, rather than overlaying five histograms, fig-
ure 3 shows the histograms converted into smooth
curves using a kernel density estimator.
1
Although
there is a small bump around 1.0 (for extremely
short documents, e.g., ?Applause?), values are
generally closer to, but greater than, 1/T .
0.0 0.2 0.4 0.6 0.8 1.0
0
2
4
6
8
10
12
Smoothed histograms of max(P(t))
Maximum topic probability in document
Den
sity 800 topics400 topics200 topics100 topics50 topics
Figure 3: Smoothed histograms of the probability of the
most probable topic in a document tuple.
Although the posterior distribution over topics
for each tuple is not concentrated on one topic,
it is worth checking that this is not simply be-
cause the model is assigning a single topic to the
1
We use the R density function.
tokens in each of the languages. Although the
model does not distinguish between topic assign-
ment variables within a given document tuple (so
it is technically incorrect to speak of different pos-
terior distributions over topics for different docu-
ments in a given tuple), we can nevertheless divide
topic assignment variables between languages and
use them to estimate a Dirichlet-multinomial pos-
terior distribution for each language in each tuple.
For each tuple we can then calculate the Jensen-
Shannon divergence (the average of the KL di-
vergences between each distribution and a mean
distribution) between these distributions. Figure 4
shows the density of these divergences for differ-
ent numbers of topics. As with the previous fig-
ure, there are a small number of documents that
contain only one topic in all languages, and thus
have zero divergence. These tend to be very short,
formulaic parliamentary responses, however. The
vast majority of divergences are relatively low (1.0
indicates no overlap in topics between languages
in a given document tuple) indicating that, for each
tuple, the model is not simply assigning all tokens
in a particular language to a single topic. As the
number of topics increases, greater variability in
topic distributions causes divergence to increase.
0.0 0.1 0.2 0.3 0.4 0.5
0
5
10
15
20
Smoothed histograms of inter?language JS divergence
Jensen?Shannon Divergence
Den
sity
800 topics400 topics200 topics100 topics50 topics
Figure 4: Smoothed histograms of the Jensen-Shannon
divergences between the posterior probability of topics be-
tween languages.
4.3 Language Model Evaluation
A topic model specifies a probability distribution
over documents, or in the case of PLTM, docu-
ment tuples. Given a set of training document tu-
ples, PLTM can be used to obtain posterior esti-
mates of ?
1
, . . . ,?
L
and ?m. The probability of
previously unseen held-out document tuples given
these estimates can then be computed. The higher
the probability of the held-out document tuples,
the better the generalization ability of the model.
883
Analytically calculating the probability of a set
of held-out document tuples given ?
1
, . . . ,?
L
and
?m is intractable, due to the summation over an
exponential number of topic assignments for these
held-out documents. However, recently developed
methods provide efficient, accurate estimates of
this probability. We use the ?left-to-right? method
of (Wallach et al, 2009). We perform five esti-
mation runs for each document and then calculate
standard errors using a bootstrap method.
Table 2 shows the log probability of held-out
data in nats per word for PLTM and LDA, both
trained with 200 topics. There is substantial varia-
tion between languages. Additionally, the predic-
tive ability of PLTM is consistently slightly worse
than that of (monolingual) LDA. It is important to
note, however, that these results do not imply that
LDA should be preferred over PLTM?that choice
depends upon the needs of the modeler. Rather,
these results are intended as a quantitative analy-
sis of the difference between the two models.
Table 2: Held-out log probability in nats/word. (Smaller
magnitude implies better language modeling performance.)
PLTM does slightly worse than monolingual LDA models,
but the variation between languages is much larger.
Lang PLTM sd LDA sd
DA -8.11 0.00067 -8.02 0.00066
DE -8.17 0.00057 -8.08 0.00072
EL -8.44 0.00079 -8.36 0.00087
EN -7.51 0.00064 -7.42 0.00069
ES -7.98 0.00073 -7.87 0.00070
FI -9.25 0.00089 -9.21 0.00065
FR -8.26 0.00072 -8.19 0.00058
IT -8.11 0.00071 -8.02 0.00058
NL -7.84 0.00067 -7.75 0.00099
PT -7.87 0.00085 -7.80 0.00060
SV -8.25 0.00091 -8.16 0.00086
As the number of topics is increased, the word
counts per topic become very sparse in mono-
lingual LDA models, proportional to the size of
the vocabulary. Figure 5 shows the proportion
of all tokens in English and Finnish assigned to
each topic under LDA and PLTM with 800 topics.
More than 350 topics in the Finnish LDA model
have zero tokens assigned to them, and almost all
tokens are assigned to the largest 200 topics. En-
glish has a larger tail, with non-zero counts in all
but 16 topics. In contrast, PLTM assigns a sig-
nificant number of tokens to almost all 800 top-
ics, in very similar proportions in both languages.
PLTM topics therefore have a higher granularity ?
i.e., they are more specific. This result is impor-
tant: informally, we have found that increasing the
granularity of topics correlates strongly with user
perceptions of the utility of a topic model.
0 200 400 600 800
0.00
0.01
0.02
0.03
0.04
Sorted topic rank
Perc
enta
ge o
f tok
ens
Figure 5: Topics sorted by number of words assigned.
Finnish is in black, English is in red; LDA is solid, PLTM is
dashed. LDA in Finnish essentially learns a 200 topic model
when given 800 topics, while PLTM uses all 800 topics.
4.4 Partly Comparable Corpora
An important application for polylingual topic
modeling is to use small numbers of comparable
document tuples to link topics in larger collections
of distinct, non-comparable documents in multiple
languages. For example, a journal might publish
papers in English, French, German and Italian. No
paper is exactly comparable to any other paper, but
they are all roughly topically similar. If we wish
to perform topic-based bibliometric analysis, it is
vital to be able to track the same topics across all
languages. One simple way to achieve this topic
alignment is to add a small set of comparable doc-
ument tuples that provide sufficient ?glue? to bind
the topics together. Continuing with the exam-
ple above, one might extract a set of connected
Wikipedia articles related to the focus of the jour-
nal and then train PLTM on a joint corpus consist-
ing of journal papers and Wikipedia articles.
In order to simulate this scenario we create a
set of variations of the EuroParl corpus by treat-
ing some documents as if they have no paral-
lel/comparable texts ? i.e., we put each of these
documents in a single-document tuple. To do this,
we divide the corpusW into two sets of document
tuples: a ?glue? set G and a ?separate? set S such
that |G| / |W| = p. In other words, the proportion
of tuples in the corpus that are treated as ?glue?
(i.e., placed in G) is p. For every tuple in S, we
assign each document in that tuple to a new single-
document tuple. By doing this, every document in
S has its own distribution over topics, independent
of any other documents. Ideally, the ?glue? doc-
884
uments in G will be sufficient to align the topics
across languages, and will cause comparable doc-
uments in S to have similar distributions over top-
ics even though they are modeled independently.
Table 3: The effect of the proportion p of ?glue? tuples on
mean Jensen-Shannon divergence in estimated topic distribu-
tions for pairs of documents in S that were originally part of
a document tuple. Lower divergence means the topic distri-
butions distributions are more similar to each other.
p Mean JS # of pairs Std. Err.
0.01 0.83755 487670 0.00018
0.05 0.79144 467288 0.00021
0.1 0.70228 443753 0.00026
0.25 0.38480 369608 0.00029
0.5 0.29712 246380 0.00030
Table 4: Topics are meaningful within languages but di-
verge between languages when only 1% of tuples are treated
as ?glue? tuples. With 25% ?glue? tuples, topics are aligned.
lang Topics at p = 0.01
DE ru?land russland russischen tschetschenien sicherheit
EN china rights human country s burma
FR russie tch?etch?enie union avec russe r?egion
IT ho presidente mi perch?e relazione votato
lang Topics at p = 0.25
DE ru?land russland russischen tschetschenien ukraine
EN russia russian chechnya cooperation region belarus
FR russie tch?etch?enie avec russe russes situation
IT russia unione cooperazione cecenia regione russa
We train PLTM with 100 topics on corpora with
p ? {0.01, 0.05, 0.1, 0.25, 0.5}. We use 1000 it-
erations of Gibbs sampling with ? = 0.01. Hy-
perparameters ?m are re-estimated every 10 it-
erations. We calculate the Jensen-Shannon diver-
gence between the topic distributions for each pair
of individual documents in S that were originally
part of the same tuple prior to separation. The
lower the divergence, the more similar the distri-
butions are to each other. From the results in fig-
ure 4, we know that leaving all document tuples
intact should result in a mean JS divergence of
less than 0.1. Table 3 shows mean JS divergences
for each value of p. As expected, JS divergence is
greater than that obtained when all tuples are left
intact. Divergence drops significantly when the
proportion of ?glue? tuples increases from 0.01 to
0.25. Example topics for p = 0.01 and p = 0.25
are shown in table 4. At p = 0.01 (1% ?glue? doc-
uments), German and French both include words
relating to Russia, while the English and Italian
word distributions appear locally consistent but
unrelated to Russia. At p = 0.25, the top words
for all four languages are related to Russia.
These results demonstrate that PLTM is appro-
priate for aligning topics in corpora that have only
a small subset of comparable documents. One area
for future work is to explore whether initializa-
tion techniques or better representations of topic
co-occurrence might result in alignment of topics
with a smaller proportion of comparable texts.
4.5 Machine Translation
Although the PLTM is clearly not a substitute for
a machine translation system?it has no way to
represent syntax or even multi-word phrases?it is
clear from the examples in figure 2 that the sets of
high probability words in different languages for a
given topic are likely to include translations. We
therefore evaluate the ability of the PLTM to gen-
erate bilingual lexica, similar to other work in un-
supervised translation modeling (Haghighi et al,
2008). In the early statistical translation model
work at IBM, these representations were called
?cepts,? short for concepts (Brown et al, 1993).
We evaluate sets of high-probability words in
each topic and multilingual ?synsets? by compar-
ing them to entries in human-constructed bilingual
dictionaries, as done by Haghighi et al (2008).
Unlike previous work (Koehn and Knight, 2002),
we evaluate all words, not just nouns. We col-
lected bilingual lexica mapping English words to
German, Greek, Spanish, French, Italian, Dutch
and Swedish. Each lexicon is a set of pairs con-
sisting of an English word and a translated word,
{w
e
, w
`
}. We do not consider multi-word terms.
We expect that simple analysis of topic assign-
ments for sequential words would yield such col-
locations, but we leave this for future work.
For every topic t we select a small number K
of the most probable words in English (e) and
in each ?translation? language (`): W
te
and W
t`
,
respectively. We then add the Cartesian product
of these sets for every topic to a set of candidate
translations C. We report the number of elements
of C that appear in the reference lexica. Results
for K = 1, that is, considering only the single
most probable word for each language, are shown
in figure 6. Precision at this level is relatively
high, above 50% for Spanish, French and Italian
with T = 400 and 800. Many of the candidate
pairs that were not in the bilingual lexica were
valid translations (e.g. EN ?comitology? and IT
885
?comitalogia?) that simply were not in the lexica.
We also do not count morphological variants: the
model finds EN ?rules? and DE ?vorschriften,? but
the lexicon contains only ?rule? and ?vorschrift.?
Results remain strong as we increase K. With
K = 3, T = 800, 1349 of the 7200 candidate
pairs for Spanish appeared in the lexicon.
l l
l
l
l
200 400 600 800
0
100
200
300
400
500
Translation pairs at K=1
Topics
Corr
ect t
rans
lation
s
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ESFRITDESVEL
Figure 6: Are the single most probable words for a given
topic in different languages translations of each other? The
number of such pairs that appear in bilingual lexica is shown
on the y-axis. For T = 800, the top English and Spanish
words in 448 topics were exact translations of one another.
4.6 Finding Translations
In addition to enhancing lexicons by aligning
topic-specific vocabulary, PLTM may also be use-
ful for adapting machine translation systems to
new domains by finding translations or near trans-
lations in an unstructured corpus. These aligned
document pairs could then be fed into standard
machine translation systems as training data. To
evaluate this scenario, we train PLTM on a set of
document tuples from EuroParl, infer topic distri-
butions for a set of held-out documents, and then
measure our ability to align documents in one lan-
guage with their translations in another language.
It is not necessarily clear that PLTM will be ef-
fective at identifying translations. In finding a low-
dimensional semantic representation, topic mod-
els deliberately smooth over much of the varia-
tion present in language. We are therefore inter-
ested in determining whether the information in
the document-specific topic distributions is suffi-
cient to identify semantically identical documents.
We begin by dividing the data into a training
set of 69,550 document tuples and a test set of
17,435 document tuples. In order to make the task
more difficult, we train a relatively coarse-grained
PLTM with 50 topics on the training set. We then
use this model to infer topic distributions for each
40
50
60
70
80
90
100
Min query doc length
% o
f tra
nsl 
at r
ank
0 50 100 200
Rank 1
Rank 5
Rank 10
Rank 20
Figure 7: Percent of query language documents for which
the target language translation is ranked at or above 1, 5, 10
or 20 by JS divergence, averaged over all language pairs.
of the 11 documents in each of the held-out doc-
ument tuples using a method similar to that used
to calculate held-out probabilities (Wallach et al,
2009). Finally, for each pair of languages (?query?
and ?target?) we calculate the difference between
the topic distribution for each held-out document
in the query language and the topic distribution for
each held-out document in the target language. We
use both Jensen-Shannon divergence and cosine
distance. For each document in the query language
we rank all documents in the target language and
record the rank of the actual translation.
Results averaged over all query/target language
pairs are shown in figure 7 for Jensen-Shannon
divergence. Cosine-based rankings are signifi-
cantly worse. It is important to note that the
length of documents matters. As noted before,
many of the documents in the EuroParl collection
consist of short, formulaic sentences. Restrict-
ing the query/target pairs to only those with query
and target documents that are both longer than 50
words results in significant improvement and re-
duced variance: the average proportion of query
documents for which the true translation is ranked
highest goes from 53.9% to 72.7%. Performance
continues to improve with longer documents, most
likely due to better topic inference. Results vary
by language. Table 5 shows results for all tar-
get languages with English as a query language.
Again, English generally performs better with Ro-
mance languages than Germanic languages.
5 Results on Comparable Texts
Directly parallel translations are rare in many lan-
guages and can be extremely expensive to pro-
duce. However, the growth of the web, and in par-
ticular Wikipedia, has made comparable text cor-
886
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 8: Squares represent the proportion of tokens in each language assigned to a topic. The left topic, world ski km won,
centers around Nordic counties. The center topic, actor role television actress, is relatively uniform. The right topic, ottoman
empire khan byzantine, is popular in all languages but especially in regions near Istanbul.
Table 5: Percent of English query documents for which the
translation was in the top n ? {1, 5, 10, 20} documents by JS
divergence between topic distributions. To reduce the effect
of short documents we consider only document pairs where
the query and target documents are longer than 100 words.
Lang 1 5 10 20
DA 78.0 90.7 93.8 95.8
DE 76.6 90.0 93.4 95.5
EL 77.1 90.4 93.3 95.2
ES 81.2 92.3 94.8 96.7
FI 76.7 91.0 94.0 96.3
FR 80.1 91.7 94.3 96.2
IT 79.1 91.2 94.1 96.2
NL 76.6 90.1 93.4 95.5
PT 80.8 92.0 94.7 96.5
SV 80.4 92.1 94.9 96.5
pora ? documents that are topically similar but are
not direct translations of one another ? consider-
ably more abundant than true parallel corpora.
In this section, we explore two questions re-
lating to comparable text corpora and polylingual
topic modeling. First, we explore whether com-
parable document tuples support the alignment of
fine-grained topics, as demonstrated earlier using
parallel documents. This property is useful for
building machine translation systems as well as
for human readers who are either learning new
languages or analyzing texts in languages they do
not know. Second, because comparable texts may
not use exactly the same topics, it becomes cru-
cially important to be able to characterize differ-
ences in topic prevalence at the document level (do
different languages have different perspectives on
the same article?) and at the language-wide level
(which topics do particular languages focus on?).
5.1 Data Set
We downloaded XML copies of all Wikipedia ar-
ticles in twelve different languages: Welsh, Ger-
man, Greek, English, Farsi, Finnish, French, He-
brew, Italian, Polish, Russian and Turkish. These
versions of Wikipedia were selected to provide a
diverse range of language families, geographic ar-
eas, and quantities of text. We preprocessed the
data by removing tables, references, images and
info-boxes. We dropped all articles in non-English
languages that did not link to an English article. In
the English version of Wikipedia we dropped all
articles that were not linked to by any other lan-
guage in our set. For efficiency, we truncated each
article to the nearest word after 1000 characters
and dropped the 50 most common word types in
each language. Even with these restrictions, the
size of the corpus is 148.5 million words.
We present results for a PLTM with 400 topics.
1000 Gibbs sampling iterations took roughly four
days on one CPU with current hardware.
5.2 Which Languages Have High Topic
Divergence?
As with EuroParl, we can calculate the Jensen-
Shannon divergence between pairs of documents
within a comparable document tuple. We can then
average over all such document-document diver-
gences for each pair of languages to get an over-
all ?disagreement? score between languages. In-
terestingly, we find that almost all languages in
our corpus, including several pairs that have his-
torically been in conflict, show average JS diver-
gences of between approximately 0.08 and 0.12
for T = 400, consistent with our findings for
EuroParl translations. Subtle differences of sen-
timent may be below the granularity of the model.
887
sadwrn blaned gallair at lloeren mytholeg 
space nasa sojus flug mission 
?????????? sts nasa ???? small 
space mission launch satellite nasa spacecraft 
??????? ??????? ???? ???? ??????? ?????  
sojuz nasa apollo ensimm?inen space lento 
spatiale mission orbite mars satellite spatial 
?????? ? ???? ??? ???? ????  
spaziale missione programma space sojuz stazione 
misja kosmicznej stacji misji space nasa 
??????????? ???? ???????????? ??????? ???????
uzay soyuz ay uzaya salyut sovyetler 
sbaen madrid el la jos? sbaeneg 
de spanischer spanischen spanien madrid la 
???????? ??????? de ??????? ??? ??????? 
de spanish spain la madrid y 
?????? ???? ????????? ???????  de ????  
espanja de espanjan madrid la real 
espagnol espagne madrid espagnole juan y 
???? ??????? ????? ?? ?????? ????  
de spagna spagnolo spagnola madrid el 
de hiszpa?ski hiszpanii la juan y 
?? ?????? ??????? ??????? ????????? de 
ispanya ispanyol madrid la k?ba real 
bardd gerddi iaith beirdd fardd gymraeg 
dichter schriftsteller literatur gedichte gedicht werk 
??????? ?????? ?????? ???? ??????? ???????? 
poet poetry literature literary poems poem 
???? ???? ????? ?????? ??? ????  
runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi 
po?te ?crivain litt?rature po?sie litt?raire ses 
?????? ????? ???? ???? ????? ?????
poeta letteratura poesia opere versi poema 
poeta literatury poezji pisarz in jego 
???? ??? ???????? ?????????? ?????? ????????? 
?air edebiyat ?iir yazar edebiyat? adl? 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
 
CY
DE
EL
EN
FA
FI
FR
HE
IT
PL
RU
TR
Figure 9: Wikipedia topics (T=400).
Overall, these scores indicate that although indi-
vidual pages may show disagreement, Wikipedia
is on average consistent between languages.
5.3 Are Topics Emphasized Differently
Between Languages?
Although we find that if Wikipedia contains an ar-
ticle on a particular subject in some language, the
article will tend to be topically similar to the arti-
cles about that subject in other languages, we also
find that across the whole collection different lan-
guages emphasize topics to different extents. To
demonstrate the wide variation in topics, we cal-
culated the proportion of tokens in each language
assigned to each topic. Figure 8 represents the es-
timated probabilities of topics given a specific lan-
guage. Competitive cross-country skiing (left) ac-
counts for a significant proportion of the text in
Finnish, but barely exists in Welsh and the lan-
guages in the Southeastern region. Meanwhile,
interest in actors and actresses (center) is consis-
tent across all languages. Finally, historical topics,
such as the Byzantine and Ottoman empires (right)
are strong in all languages, but show geographical
variation: interest centers around the empires.
6 Conclusions
We introduced a polylingual topic model (PLTM)
that discovers topics aligned across multiple lan-
guages. We analyzed the characteristics of PLTM
in comparison to monolingual LDA, and demon-
strated that it is possible to discover aligned top-
ics. We also demonstrated that relatively small
numbers of topically comparable document tu-
ples are sufficient to align topics between lan-
guages in non-comparable corpora. Additionally,
PLTM can support the creation of bilingual lexica
for low resource language pairs, providing candi-
date translations for more computationally intense
alignment processes without the sentence-aligned
translations typically used in such tasks. When
applied to comparable document collections such
as Wikipedia, PLTM supports data-driven analysis
of differences and similarities across all languages
for readers who understand any one language.
7 Acknowledgments
The authors thank Limin Yao, who was involved
in early stages of this project. This work was
supported in part by the Center for Intelligent In-
formation Retrieval, in part by The Central In-
telligence Agency, the National Security Agency
and National Science Foundation under NSF grant
number IIS-0326249, and in part by Army prime
contract number W911NF-07-1-0216 and Uni-
versity of Pennsylvania subaward number 103-
548106, and in part by National Science Founda-
tion under NSF grant #CNS-0619337. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this material are the authors?
and do not necessarily reflect those of the sponsor.
References
David Blei and Michael Jordan. 2003. Modeling an-
notated data. In SIGIR.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. JMLR.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. CL, 19(2):263?311.
888
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771?779.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using
topic models. In EMNLP.
Mariko Kawaba, Hiroyuki Nakasaki, Takehito Utsuro,
and Tomohiro Fukuhara. 2008. Cross-lingual blog
analysis based on multilingual blog distillation from
multilingual Wikipedia entries. In ICWSM.
Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.
Gideon Mann, David Mimno, and Andrew McCal-
lum. 2006. Bibliometric impact measures leverag-
ing topic analysis. In JCDL.
Andrew McCallum, Andr?es Corrada-Emmanuel, and
Xuerui Wang. 2005. Topic and role discovery in
social networks. In IJCAI.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from Wikipedia.
In WWW.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 28:187?
207.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluation methods for
topic models. In ICML.
Xing Wei and Bruce Croft. 2006. LDA-based docu-
ment models for ad-hoc retrieval. In SIGIR.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual topic exploration, word alignment, and transla-
tion. In NIPS.
889
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 262?272,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Optimizing Semantic Coherence in Topic Models
David Mimno
Princeton University
Princeton, NJ 08540
mimno@cs.princeton.edu
Hanna M. Wallach
University of Massachusetts, Amherst
Amherst, MA 01003
wallach@cs.umass.edu
Edmund Talley Miriam Leenders
National Institutes of Health
Bethesda, MD 20892
{talleye,leenderm}@ninds.nih.gov
Andrew McCallum
University of Massachusetts, Amherst
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
Latent variable models have the potential
to add value to large document collections
by discovering interpretable, low-dimensional
subspaces. In order for people to use such
models, however, they must trust them. Un-
fortunately, typical dimensionality reduction
methods for text, such as latent Dirichlet al
location, often produce low-dimensional sub-
spaces (topics) that are obviously flawed to
human domain experts. The contributions of
this paper are threefold: (1) An analysis of the
ways in which topics can be flawed; (2) an au-
tomated evaluation metric for identifying such
topics that does not rely on human annotators
or reference collections outside the training
data; (3) a novel statistical topic model based
on this metric that significantly improves topic
quality in a large-scale document collection
from the National Institutes of Health (NIH).
1 Introduction
Statistical topic models such as latent Dirichlet al
location (LDA) (Blei et al, 2003) provide a pow-
erful framework for representing and summarizing
the contents of large document collections. In our
experience, however, the primary obstacle to accep-
tance of statistical topic models by users the outside
machine learning community is the presence of poor
quality topics. Topics that mix unrelated or loosely-
related concepts substantially reduce users? confi-
dence in the utility of such automated systems.
In general, users prefer models with larger num-
bers of topics because such models have greater res-
olution and are able to support finer-grained distinc-
tions. Unfortunately, we have observed that there
is a strong relationship between the size of topics
and the probability of topics being nonsensical as
judged by domain experts: as the number of topics
increases, the smallest topics (number of word to-
kens assigned to each topic) are almost always poor
quality. The common practice of displaying only a
small number of example topics hides the fact that as
many as 10% of topics may be so bad that they can-
not be shown without reducing users? confidence.
The evaluation of statistical topic models has tra-
ditionally been dominated by either extrinsic meth-
ods (i.e., using the inferred topics to perform some
external task such as information retrieval (Wei
and Croft, 2006)) or quantitative intrinsic methods,
such as computing the probability of held-out doc-
uments (Wallach et al, 2009). Recent work has
focused on evaluation of topics as semantically-
coherent concepts. For example, Chang et al (2009)
found that the probability of held-out documents is
not always a good predictor of human judgments.
Newman et al (2010) showed that an automated
evaluation metric based on word co-occurrence
statistics gathered from Wikipedia could predict hu-
man evaluations of topic quality. AlSumait et al
(2009) used differences between topic-specific dis-
tributions over words and the corpus-wide distribu-
tion over words to identify overly-general ?vacuous?
topics. Finally, Andrzejewski et al (2009) devel-
oped semi-supervised methods that avoid specific
user-labeled semantic coherence problems.
The contributions of this paper are threefold: (1)
To identify distinct classes of low-quality topics,
some of which are not flagged by existing evalua-
tion methods; (2) to introduce a new topic ?coher-
ence? score that corresponds well with human co-
herence judgments and makes it possible to identify
262
specific semantic problems in topic models without
human evaluations or external reference corpora; (3)
to present an example of a new topic model that
learns latent topics by directly optimizing a metric
of topic coherence. With little additional computa-
tional cost beyond that of LDA, this model exhibits
significant gains in average topic coherence score.
Although the model does not result in a statistically-
significant reduction in the number of topics marked
?bad?, the model consistently improves the topic co-
herence score of the ten lowest-scoring topics (i.e.,
results in bad topics that are ?less bad? than those
found using LDA) while retaining the ability to iden-
tify low-quality topics without human interaction.
2 Latent Dirichlet Allocation
LDA is a generative probabilistic model for docu-
mentsW = {w(1),w(2), . . . ,w(D)}. To generate a
word token w(d)n in document d, we draw a discrete
topic assignment z(d)n from a document-specific dis-
tribution over the T topics ?d (which is itself drawn
from a Dirichlet prior with hyperparameter ?), and
then draw a word type for that token from the topic-
specific distribution over the vocabulary ?z(d)n . Theinference task in topic models is generally cast as in-
ferring the document?topic proportions {?1, ...,?D}
and the topic-specific distributions {?1 . . . ,?T }.
The multinomial topic distributions are usually
drawn from a shared symmetric Dirichlet prior with
hyperparameter ?, such that conditioned on {?t}Tt=1
and the topic assignments {z(1), z(2), . . . ,z(D)},
the word tokens are independent. In practice, how-
ever, it is common to deal directly with the ?col-
lapsed? distributions that result from integrating
over the topic-specific multinomial parameters. The
resulting distribution over words for a topic t is then
a function of the hyperparameter ? and the number
of words of each type assigned to that topic, Nw|t.
This distribution, known as the Dirichlet compound
multinomial (DCM) or Po?lya distribution (Doyle
and Elkan, 2009), breaks the assumption of condi-
tional independence between word tokens given top-
ics, but is useful during inference because the con-
ditional probability of a word w given topic t takes
a very simple form: P (w | t, ?) = Nw|t+?Nt+|V|? , where
Nt =
?
w? Nw?|t and |V| is the vocabulary size.
The process for generating a sequence of words
from such a model is known as the simple Po?lya urn
model (Mahmoud, 2008), in which the initial prob-
ability of word type w in topic t is proportional to
?, while the probability of each subsequent occur-
rence of w in topic t is proportional to the number
of times w has been drawn in that topic plus ?. Note
that this unnormalized weight for each word type de-
pends only on the count of that word type, and is in-
dependent of the count of any other word type w?.
Thus, in the DCM/Po?lya distribution, drawing word
type w must decrease the probability of seeing all
other word types w? 6= w. In a later section, we will
introduce a topic model that substitutes a general-
ized Po?lya urn model for the DCM/Po?lya distribu-
tion, allowing a draw of word type w to increase the
probability of seeing certain other word types.
For real-world data, documents W are observed,
while the corresponding topic assignments Z are
unobserved and may be inferred using either vari-
ational methods (Blei et al, 2003; Teh et al, 2006)
or MCMC methods (Griffiths and Steyvers, 2004).
Here, we use MCMC methods?specifically Gibbs
sampling (Geman and Geman, 1984), which in-
volves sequentially resampling each topic assign-
ment z(d)n from its conditional posterior given the
documents W , the hyperparameters ? and ?, and
Z\d,n (the current topic assignments for all tokens
other than the token at position n in document d).
3 Expert Opinions of Topic Quality
Concentrating on 300,000 grant and related jour-
nal paper abstracts from the National Institutes of
Health (NIH), we worked with two experts from
the National Institute of Neurological Disorders and
Stroke (NINDS) to collaboratively design an expert-
driven topic annotation study. The goal of this study
was to develop an annotated set of baseline topics,
along with their salient characteristics, as a first step
towards automatically identifying and inferring the
kinds of topics desired by domain experts.1
3.1 Expert-Driven Annotation Protocol
In order to ensure that the topics selected for anno-
tation were within the NINDS experts? area of ex-
pertise, they selected 148 topics (out of 500), all as-
sociated with areas funded by NINDS. Each topic
1All evaluated models will be released publicly.
263
t was presented to the experts as a list of the thirty
most probable words for that topic, in descending or-
der of their topic-specific ?collapsed? probabilities,
Nw|t+?
Nt+|V|? . In addition to the most probable words,the experts were also given metadata for each topic:
The most common sequences of two or more con-
secutive words assigned to that topic, the four topics
that most often co-occurred with that topic, the most
common IDF-weighted words from titles of grants,
thesaurus terms, NIH institutes, journal titles, and
finally a list of the highest probability grants and
PubMed papers for that topic.
The experts first categorized each topic as one
of three types: ?research?, ?grant mechanisms and
publication types? or ?general?.2 The quality of
each topic (?good?, ?intermediate?, or ?bad?) was
then evaluated using criteria specific to the type
of topic. In general, topics were only annotated
as ?good? if they contained words that could be
grouped together as a single coherent concept. Addi-
tionally, each ?research? topic was only considered
to be ?good? if, in addition to representing a sin-
gle coherent concept, the aggregate content of the
set of documents with appreciable allocations to that
topic clearly contained text referring to the concept
inferred from the topic words. Finally, for each topic
marked as being either ?intermediate? or ?bad?, one
or more of the following problems (defined by the
domain experts) was identified, as appropriate:
? Chained: every word is connected to every
other word through some pairwise word chain,
but not all word pairs make sense. For exam-
ple, a topic whose top three words are ?acids?,
?fatty? and ?nucleic? consists of two distinct
concepts (i.e., acids produced when fats are
broken down versus the building blocks of
DNA and RNA) chained via the word ?acids?.
? Intruded: either two or more unrelated sets
of related words, joined arbitrarily, or an oth-
erwise good topic with a few ?intruder? words.
? Random: no clear, sensical connections be-
tween more than a few pairs of words.
? Unbalanced: the top words are all logically
connected to each other, but the topic combines
very general and specific terms (e.g., ?signal
2Equivalent to ?vacuous topics? of AlSumait et al (2009).
transduction? versus ?notch signaling?).
Examples of a good general topic, a good research
topic, and a chained research topic are in Table 1.
3.2 Annotation Results
The experts annotated the topics independently and
then aggregated their results. Interestingly, no top-
ics were ever considered ?good? by one expert and
?bad? by the other?when there was disagreement
between the experts, one expert always believed the
topic to be ?intermediate.? In such cases, the ex-
perts discussed the reasons for their decisions and
came to a consensus. Of the 148 topics selected for
annotation, 90 were labeled as ?good,? 21 as ?inter-
mediate,? and 37 as ?bad.? Of the topics labeled as
?bad? or ?intermediate,? 23 were ?chained,? 21 were
?intruded,? 3 were ?random,? and 15 were ?unbal-
anced?. (The annotators were permitted to assign
more than one problem to any given topic.)
4 Automated Metrics for Predicting
Expert Annotations
The ultimate goal of this paper is to develop meth-
ods for building models with large numbers of spe-
cific, high-quality topics from domain-specific cor-
pora. We therefore explore the extent to which in-
formation already contained in the documents being
modeled can be used to assess topic quality.
In this section we evaluate several methods for
ranking the quality of topics and compare these
rankings to human annotations. No method is likely
to perfectly predict human judgments, as individual
annotators may disagree on particular topics. For
an application involving removing low quality top-
ics we recommend using a weighted combination of
metrics, with a threshold determined by users.
4.1 Topic Size
As a simple baseline, we considered the extent to
which topic ?size? (as measured by the number of
tokens assigned to each topic via Gibbs sampling) is
a good metric for assessing topic quality. Figure 1
(top) displays the topic size (number of tokens as-
signed to that topic) and expert annotations (?good?,
?intermediate?, ?bad?) for the 148 topics manually
labeled by annotators as described above. This fig-
ure suggests that topic size is a reasonable predic-
264
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll
40000 60000 80000 120000 160000
Tokens
goo
d
inte
r
bad
lllllllllllllllllllllllllllllllllllllllllllllllllll
?600 ?500 ?400 ?300 ?200
Coherence
goo
d
inte
r
bad
Figure 1: Topic size is a good indicator of quality; the
new coherence metric is better. Top shows expert-rated
topics ranked by topic size (AP 0.89, AUC 0.79), bottom
shows same topics ranked by coherence (AP 0.94, AUC
0.87). Random jitter is added to the y-axis for clarity.
tor of topic quality. Although there is some overlap,
?bad? topics are generally smaller than ?good? top-
ics. Unfortunately, this observation conflicts with
the goal of building highly specialized, domain-
specific topic models with many high-quality, fine-
grained topics?in such models the majority of top-
ics will have relatively few tokens assigned to them.
4.2 Topic Coherence
When displaying topics to users, each topic t is gen-
erally represented as a list of theM=5, . . . , 20 most
probable words for that topic, in descending order
of their topic-specific ?collapsed? probabilities. Al-
though there has been previous work on automated
generation of labels or headings for topics (Mei et
al., 2007), we choose to work only with the ordered
list representation. Labels may obscure or detract
from fundamental problems with topic coherence,
and better labels don?t make bad topics good.
The expert-driven annotation study described in
section 3 suggests that three of the four types of
poor-quality topics (?chained,? ?intruded? and ?ran-
dom?) could be detected using a metric based on
the co-occurrence of words within the documents
being modeled. For ?chained? and ?intruded? top-
ics, it is likely that although pairs of words belong-
ing to a single concept will co-occur within a single
document (e.g., ?nucleic? and ?acids? in documents
about DNA), word pairs belonging to different con-
cepts (e.g., ?fatty? and ?nucleic?) will not. For ran-
dom topics, it is likely that few words will co-occur.
This insight can be used to design a new metric
for assessing topic quality. Letting D(v) be the doc-
ument frequency of word type v (i.e., the number
of documents with least one token of type v) and
D(v, v?) be co-document frequency of word types v
and v? (i.e., the number of documents containing one
or more tokens of type v and at least one token of
type v?), we define topic coherence as
C(t;V (t)) =
M?
m=2
m?1?
l=1
log D(v
(t)
m , v(t)l ) + 1
D(v(t)l )
, (1)
where V (t) =(v(t)1 , . . . , v(t)M ) is a list of the M most
probable words in topic t. A smoothing count of 1
is included to avoid taking the logarithm of zero.
Figure 1 shows the association between the expert
annotations and both topic size (top) and our coher-
ence metric (bottom). We evaluate these results us-
ing standard ranking metrics, average precision and
the area under the ROC curve. Treating ?good? top-
ics as positive and ?intermediate? or ?bad? topics as
negative, we get average precision values of 0.89 for
topic size vs. 0.94 for coherence and AUC 0.79 for
topic size vs. 0.87 for coherence. We performed a
logistic regression analysis on the binary variable ?is
this topic bad?. Using topic size alone as a predic-
tor gives AIC (a measure of model fit) 152.5. Co-
herence alone has AIC 113.8 (substantially better).
Both predictors combined have AIC 115.8: the sim-
pler coherence alone model provides the best perfor-
mance. We tried weighting the terms in equation 1
by their corresponding topic?word probabilities and
and by their position in the sorted list of the M most
probable words for that topic, but we found that a
uniform weighting better predicted topic quality.
Our topic coherence metric also exhibits good
qualitative behavior: of the 20 best-scoring topics,
18 are labeled as ?good,? one is ?intermediate? (?un-
balanced?), and one is ?bad? (combining ?cortex?
and ?fmri?, words that commonly co-occur, but are
conceptually distinct). Of the 20 worst scoring top-
ics, 15 are ?bad,? 4 are ?intermediate,? and only one
(with the 19th worst coherence score) is ?good.?
265
Our coherence metric relies only upon word co-
occurrence statistics gathered from the corpus being
modeled, and does not depend on an external ref-
erence corpus. Ideally, all such co-occurrence infor-
mation would already be accounted for in the model.
We believe that one of the main contributions of our
work is demonstrating that standard topic models
do not fully utilize available co-occurrence informa-
tion, and that a held-out reference corpus is therefore
not required for purposes of topic evaluation.
Equation 1 is very similar to pointwise mutual in-
formation (PMI), but is more closely associated with
our expert annotations than PMI (which achieves
AUC 0.64 and AIC 170.51). PMI has a long history
in language technology (Church and Hanks, 1990),
and was recently used by Newman et al (2010) to
evaluate topic models. When expressed in terms of
count variables as in equation 1, PMI includes an
additional term for D(v(t)m ). The improved perfor-
mance of our metric over PMI implies that what mat-
ters is not the difference between the joint probabil-
ity of words m and l and the product of marginals,
but the conditional probability of each word given
the each of the higher-ranked words in the topic.
In order to provide intuition for the behavior of
our topic coherence metric, table 1 shows three
example topics and their topic coherence scores.
The first topic, related to grant-funded training pro-
grams, is one of the best-scoring topics. All pairs
of words have high co-document frequencies. The
second topic, on neurons, is more typical of qual-
ity ?research? topics. Overall, these words occur
less frequently, but generally occur moderately in-
terchangeably: there is little structure to their co-
variance. The last topic is one of the lowest-scoring
topics. Its co-document frequency matrix is shown
in table 2. The top two words are closely related:
487 documents include ?aging? at least once, 122
include ?lifespan?, and 55 include both. Meanwhile,
the third word ?globin? occurs with only one of the
top seven words?the common word ?human?.
4.3 Comparison to word intrusion
As an additional check for both our expert annota-
tions and our automated metric, we replicated the
?word intrusion? evaluation originally introduced by
Chang et al (2009). In this task, one of the top ten
most probable words in a topic is replaced with a
l lll l lll ll l lll ll l lll lll l lll ll lll l l ll l llll
l
ll l lll l lll ll l lll
l
l ll ll lll ll
l
l l
l
l
ll l
40000 60000 80000 120000 160000
0
4
8
Comparison of Topic Size to Intrusion Detection
Tokens assigned to topic
Cor
rect
 Gu
ess
es
ll ll ll ll l ll ll ll l ll ll llll l lll llll ll ll ll lll l
l
l lll lllll ll l ll
l
lll ll ll ll
l
l l
l
l
ll l
?600 ?500 ?400 ?300 ?200
0
4
8
Comparison of Coherence to Intrusion Detection
Coherence
Cor
rect
 Gu
ess
es
Good Topics
Correct Guesses
Fre
que
ncy
0 2 4 6 8 10
0
15
35
Bad Topics
Correct Guesses
Fre
que
ncy
0 2 4 6 8 10
0
15
35
Figure 2: Top: results of the intruder selection task rel-
ative to two topic quality metrics. Bottom: marginal in-
truder accuracy frequencies of good and bad topics.
another word, selected at random from the corpus.
The resulting set of words is presented, in a random
order, to users, who are asked to identify the ?in-
truder? word. It is very unlikely that a randomly-
chosen word will be semantically related to any of
the original words in the topic, so if a topic is a
high quality representation of a semantically coher-
ent concept, it should be easy for users to select the
intruder word. If the topic is not coherent, there may
be words in the topic that are also not semantically
related to any other word, thus causing users to se-
lect ?correct? words instead of the real intruder.
We recruited ten additional expert annotators
from NINDS, not including our original annotators,
and presented them with the intruder selection task,
using the set of previously evaluated topics. Re-
sults are shown in figure 2. In the first two plots,
the x-axis is one of our two automated quality met-
266
Table 1: Example topics (good/general, good/research, chained/research) with different coherence scores (numbers
closer to zero indicate higher coherence). The chained topic combines words related to aging (indicated in plain text)
and words describing blood and blood-related diseases (bold). The only connection is the common word human.
-167.1 students, program, summer, biomedical, training, experience, undergraduate, career, minority, student, ca-
reers, underrepresented, medical students, week, science
-252.1 neurons, neuronal, brain, axon, neuron, guidance, nervous system, cns, axons, neural, axonal, cortical,
survival, disorders, motor
-357.2 aging, lifespan, globin, age related, longevity, human, age, erythroid, sickle cell, beta globin, hb, senes-
cence, adult, older, lcr
Table 2: Co-document frequency matrix for the top words in a low-quality topic (according to our coherence metric),
shaded to highlight zeros. The diagonal (light gray) shows the overall document frequency for each word w. The
column on the right is Nw|t. Note that ?globin? and ?erythroid? do not co-occur with any of the aging-related words.
aging 487 53 0 65 42 0 51 0 138 0 914
lifespan 53 122 0 15 28 0 15 0 44 0 205
globin 0 0 39 0 0 19 0 15 27 3 200
age related 65 15 0 119 12 0 25 0 37 0 160
longevity 42 28 0 12 73 0 6 0 20 1 159
erythroid 0 0 19 0 0 69 0 8 23 1 110
age 51 15 0 25 6 0 245 1 82 0 103
sickle cell 0 0 15 0 0 8 1 43 16 2 93
human 138 44 27 37 20 23 82 16 4347 157 91
hb 0 0 3 0 1 1 0 2 5 15 73
267
rics (topic size and coherence) and the y-axis is the
number of annotators that correctly identified the
true intruder word (accuracy). The histograms be-
low these plots show the number of topics with each
level of annotator accuracy for good and bad top-
ics. For good topics (green circles), the annotators
were generally able to detect the intruder word with
high accuracy. Bad topics (red diamonds) had more
uniform accuracies. These results suggest that top-
ics with low intruder detection accuracy tend to be
bad, but some bad topics can have a high accuracy.
For example, spotting an intruder word in a chained
topic can be easy. The low-quality topic recep-
tors, cannabinoid, cannabinoids, ligands, cannabis,
endocannabinoid, cxcr4, [virus], receptor, sdf1, is
a typical ?chained? topic, with CXCR4 linked to
cannabinoids only through receptors, and otherwise
unrelated. Eight out of ten annotators correctly iden-
tified ?virus? as the correct intruder. Repeating the
logistic regression experiment using intruder detec-
tion accuracy as input, the AIC value is 163.18?
much worse than either topic size or coherence.
5 Generalized Po?lya Urn Models
Although the topic coherence metric defined above
provides an accurate way of assessing topic quality,
preventing poor quality topics from occurring in the
first place is preferable. Our results in the previous
section show that we can identify low-quality top-
ics without making use of external supervision; the
training data by itself contains sufficient information
at least to reject poor combinations of words.
In this section, we describe a new topic model that
incorporates the corpus-specific word co-occurrence
information used in our coherence metric directly
into the statistical topic modeling framework. It
is important to note that simply disallowing words
that never co-occur from being assigned to the same
topic is not sufficient. Due to the power-law charac-
teristics of language, most words are rare and will
not co-occur with most other words regardless of
their semantic similarity. It is rather the degree
to which the most prominent words in a topic do
not co-occur with the other most prominent words
in that topic that is an indicator of topic incoher-
ence. We therefore desire models that guide topics
towards semantic similarity without imposing hard
constraints.
As an example of such a model, we present a new
topic model in which the occurrence of word type w
in topic t increases not only the probability of seeing
that word type again, but also increases the probabil-
ity of seeing other related words (as determined by
co-document frequencies for the corpus being mod-
eled). This new topic model retains the document?
topic component of standard LDA, but replaces the
usual Po?lya urn topic?word component with a gen-
eralized Po?lya urn framework (Mahmoud, 2008).
A sequence of i.i.d. samples from a discrete dis-
tribution can be imagined as arising by repeatedly
drawing a random ball from an urn, where the num-
ber of balls of each color is proportional to the prob-
ability of that color, replacing the selected ball af-
ter each draw. In a Po?lya urn, each ball is replaced
along with another ball of the same color. Samples
from this model exhibit the ?burstiness? property:
the probability of drawing a ball of colorw increases
each time a ball of that color is drawn. This process
represents the marginal distribution of a hierarchical
model with a Dirichlet prior and a multinomial like-
lihood, and is used as the distribution over words
for each topic in almost all previous topic models.
In a generalized Po?lya urn model, having drawn a
ball of color w, Avw additional balls of each color
v ? {1, . . . ,W} are returned to the urn. Given W
and Z , the conditional posterior probability of word
w in topic t implied by this generalized model is
P (w | t,W,Z, ?,A) =
?
vNv|tAvw + ?
Nt + |V|?
, (2)
where A is a W ? W real-valued matrix, known
as the addition matrix or schema. The simple Po?lya
urn model (and hence the conditional posterior prob-
ability of word w in topic t under LDA) can be re-
covered by setting the schema A to the identity ma-
trix. Unlike the simple Po?lya distribution, we do not
know of a representation of the generalized Po?lya
urn distribution that can be expressed using a con-
cise set of conditional independence assumptions. A
standard graphical model with plate notation would
therefore not be helpful in highlighting the differ-
ences between the two models, and is not shown.
Algorithm 1 shows pseudocode for a single Gibbs
sweep over the latent variables Z in standard LDA.
Algorithm 2 shows the modifications necessary to
268
1: for d ? D do
2: for wn ? w(d) do
3: Nzi|di ? Nzi|di ? 1
4: Nwi|zi ? Nwi|zi ? 1
5: sample zi ? (Nz|di + ?z)
Nwi|z+??
z? (Nwi|z?+?)6: Nzi|di ? Nzi|di + 1
7: Nwi|zi ? Nwi|zi + 1
8: end for
9: end for
Algorithm 1: One sweep of LDA Gibbs sampling.
1: for d ? D do
2: for wn ? w(d) do
3: Nzi|di ? Nzi|di ? 1
4: for all v do
5: Nv|zi ? Nv|zi ?Avwi
6: end for
7: sample zi ? (Nz|di + ?z)
Nwi|z+??
z? (Nwi|z?+?)8: Nzi|di ? Nzi|di + 1
9: for all v do
10: Nv|zi ? Nv|zi +Avwi
11: end for
12: end for
13: end for
Algorithm 2: One sweep of gen. Po?lya Gibbs sam-
pling, with differences from LDA highlighted in red.
support a generalized Po?lya urn model: rather than
subtracting exactly one from the count of the word
given the old topic, sampling, and then adding one
to the count of the word given the new topic, we sub-
tract a column of the schema matrix from the entire
count vector over words for the old topic, sample,
and add the same column to the count vector for the
new topic. As long as A is sparse, this operation
adds only a constant factor to the computation.
Another property of the generalized Po?lya urn
model is that it is nonexchangeable?the joint prob-
ability of the tokens in any given topic is not invari-
ant to permutation of those tokens. Inference of Z
givenW via Gibbs sampling involves repeatedly cy-
cling through the tokens in W and, for each one,
resampling its topic assignment conditioned on W
and the current topic assignments for all tokens other
than the token of interest. For LDA, the sampling
distribution for each topic assignment is simply the
product of two predictive probabilities, obtained by
treating the token of interest as if it were the last.
For a topic model with a generalized Po?lya urn for
the topic?word component, the sampling distribu-
tion is more complicated. Specifically, the topic?
word component of the sampling distribution is no
longer a simple predictive distribution?when sam-
pling a new value for z(d)n , the implication of each
possible value for subsequent tokens and their topic
assignments must be considered. Unfortunately, this
can be very computationally expensive, particularly
for large corpora. There are several ways around this
problem. The first is to use sequential Monte Carlo
methods, which have been successfully applied to
topic models previously (Canini et al, 2009). The
second approach is to approximate the true Gibbs
sampling distribution by treating each token as if it
were the last, ignoring implications for subsequent
tokens and their topic assignments. We find that
this approximate method performs well empirically.
5.1 Setting the Schema A
Inspired by our evaluation metric, we define A as
Avv ? ?vD(v) (3)
Avw ? ?vD(w, v)
where each element is scaled by a row-specific
weight ?v and each column is normalized to sum
to 1. Normalizing columns makes comparison to
standard LDA simpler, because the relative effect of
smoothing parameter ?=0.01 is equivalent. We set
?v = log (D/D(v)), the standard IDF weight used
in information retrieval, which is larger for less fre-
quent words. The column for word type w can be
interpreted as word types with significant associa-
tion with w. The IDF weighting therefore has the
effect of increasing the strength of association for
rare word types. We also found empirically that it is
helpful to remove off-diagonal elements for the most
common types, such as those that occur in more than
5% of documents (IDF < 3.0). Including nonzero
off-diagonal values in A for very frequent types
causes the model to disperse those types over many
topics, which leads to large numbers of extremely
similar topics. To measure this effect, we calcu-
lated the Jensen-Shannon divergence between all
pairs of topic?word distributions in a given model.
For a model using off-diagonal weights for all word
269
?
29
0
?
26
0
100 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
200 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
300 Topics
Co
he
ren
ce
50 300 550 800
?
29
0
?
26
0
400 Topics
Co
he
ren
ce
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
40
0
?
34
0
10
 W
or
st 
Co
he
r
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
?
17
00
?
16
60
Iteration
HO
LP
50 300 550 800
Figure 3: Po?lya urn topics (blue) have higher average coherence and converge much faster than LDA topics
(red). The top plots show topic coherence (averaged over 15 runs) over 1000 iterations of Gibbs sampling. Error bars
are not visible in this plot. The middle plot shows the average coherence of the 10 lowest scoring topics. The bottom
plots show held-out log probability (in thousands) for the same models (three runs each of 5-fold cross-validation).
Name Docs Avg. Tok. Tokens Vocab
NIH 18756 114.64 ? 30.41 2150172 28702
Table 3: Data set statistics.
types, the mean of the 100 lowest divergences was
0.29 ? .05 (a divergence of 1.0 represents distribu-
tions with no shared support) at T =200. The aver-
age divergence of the 100 most similar pairs of top-
ics for standard LDA (i.e.,A = I) is 0.67?.05. The
same statistic for the generalized Po?lya urn model
without off-diagonal elements for word types with
high document frequency is 0.822? 0.09.
Setting the off-diagonal elements of the schema
A to zero for the most common word types also has
the fortunate effect of substantially reducing prepro-
cessing time. We find that Gibbs sampling for the
generalized Po?lya model takes roughly two to three
times longer than for standard LDA, depending on
the sparsity of the schema, due to additional book-
keeping needed before and after sampling topics.
5.2 Experimental Results
We evaluated the new model on a corpus of NIH
grant abstracts. Details are given in table 3. Figure 3
shows the performance of the generalized Po?lya urn
model relative to LDA. Two metrics?our new topic
coherence metric and the log probability of held-out
documents?are shown over 1000 iterations at 50 it-
eration intervals. Each model was run over five folds
of cross validation, each with three random initial-
izations. For each model we calculated an overall
coherence score by calculating the topic coherence
for each topic individually and then averaging these
values. We report the average over all 15 models in
each plot. Held-out probabilities were calculated us-
ing the left-to-right method of Wallach et al (2009),
with each cross-validation fold using its own schema
A. The generalized Po?lya model performs very well
in average topic coherence, reaching levels within
the first 50 iterations that match the final score. This
model has an early advantage for held-out proba-
bility as well, but is eventually overtaken by LDA.
This trend is consistent with Chang et al?s observa-
tion that held-out probabilities are not always good
predictors of human judgments (Chang et al, 2009).
Results are consistent over T ? {100, 200, 300}.
In section 4.2, we demonstrated that our topic co-
herence metric correlates with expert opinions of
topic quality for standard LDA. The generalized
270
Po?lya urn model was therefore designed with the
goal of directly optimizing that metric. It is pos-
sible, however, that optimizing for coherence di-
rectly could break the association between coher-
ence metric and topic quality. We therefore repeated
the expert-driven evaluation protocol described in
section 3.1. We trained one standard LDA model
and one generalized Po?lya urn model, each with
T = 200, and randomly shuffled the 400 resulting
topics. The topics were then presented to the experts
from NINDS, with no indication as to the identity of
the model from which each topic came. As these
evaluations are time consuming, the experts evalu-
ated the only the first 200 topics, which consisted of
103 generalized Po?lya urn topics and 97 LDA top-
ics. AUC values predicting bad topics given coher-
ence were 0.83 and 0.80, respectively. Coherence
effectively predicts topic quality in both models.
Although we were able to improve the average
overall quality of topics and the average quality of
the ten lowest-scoring topics, we found that the gen-
eralized Po?lya urn model was less successful reduc-
ing the overall number of bad topics. Ignoring one
?unbalanced? topic from each model, 16.5% of the
LDA topics and 13.5% from the generalized Po?lya
urn model were marked as ?bad.? While this result
is an improvement, it is not significant at p = 0.05.
6 Discussion
We have demonstrated the following:
? There is a class of low-quality topics that can-
not be detected using existing word-intrusion
tests, but that can be identified reliably using a
metric based on word co-occurrence statistics.
? It is possible to improve the coherence score
of topics, both overall and for the ten worst,
while retaining the ability to flag bad topics, all
without requiring semi-supervised data or ad-
ditional reference corpora. Although additional
information may be useful, it is not necessary.
? Such models achieve better performance with
substantially fewer Gibbs iterations than LDA.
We believe that the most important challenges in fu-
ture topic modeling research are improving the se-
mantic quality of topics, particularly at the low end,
and scaling to ever-larger data sets while ensuring
high-quality topics. Our results provide critical in-
sight into these problems. We found that it should be
possible to construct unsupervised topic models that
do not produce bad topics. We also found that Gibbs
sampling mixes faster for models that use word co-
occurrence information, suggesting that such meth-
ods may also be useful in guiding online stochastic
variational inference (Hoffman et al, 2010).
Acknowledgements
This work was supported in part by the Center
for Intelligent Information Retrieval, in part by the
CIA, the NSA and the NSF under NSF grant # IIS-
0326249, in part by NIH:HHSN271200900640P,
and in part by NSF # number SBE-0965436. Any
opinions, findings and conclusions or recommenda-
tions expressed in this material are the authors? and
do not necessarily reflect those of the sponsor.
References
Loulwah AlSumait, Daniel Barbara, James Gentle, and
Carlotta Domeniconi. 2009. Topic significance rank-
ing of LDA generative models. In ECML.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via Dirichlet forest priors. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 25?32.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
K.R. Canini, L. Shi, and T.L. Griffiths. 2009. Online
inference of topics with latent Dirichlet alocation. In
Proceedings of the 12th International Conference on
Artificial Intelligence and Statistics.
Jonathan Chang, Jordan Boyd-Graber, Chong Wang,
Sean Gerrish, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Ad-
vances in Neural Information Processing Systems 22,
pages 288?296.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 6(1):22?29.
Gabriel Doyle and Charles Elkan. 2009. Accounting for
burstiness in topic models. In ICML.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transaction on Pattern Analysis and
Machine Intelligence 6, pages 721?741.
271
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101(suppl. 1):5228?5235.
Matthew Hoffman, David Blei, and Francis Bach. 2010.
Online learning for latent dirichlet alocation. In NIPS.
Hosan Mahmoud. 2008. Po?lya Urn Models. Chapman
& Hall/CRC Texts in Statistical Science.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
David Newman, Jey Han Lau, Karl Grieser, and Timothy
Baldwin. 2010. Automatic evaluation of topic coher-
ence. In Human Language Technologies: The Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Yee Whye Teh, Dave Newman, and Max Welling. 2006.
A collapsed variational Bayesian inference algorithm
for lat ent Dirichlet alocation. In Advances in Neural
Information Processing Systems 18.
Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, and
David Mimno. 2009. Evaluation methods for topic
models. In Proceedings of the 26th Interational Con-
ference on Machine Learning.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrival. In Proceedings of the 29th
Annual International SIGIR Conference.
272
