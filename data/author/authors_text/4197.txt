Evaluation of Automatically Identified Index Terms 
for Browsing Electronic Documents I 
Nina Wacholder, Judith L. Klavans and David K. Evans 
Columbia University 
Department of Computer Science and 
Center for Research on Information Access 
1. Abstract 
We present an evaluation of domain- 
independent atural anguage tools for use in 
the identification of significant concepts in 
documents. Using qualitative evaluation, we 
compare three shallow processing methods for 
extracting index terms, i.e., terms that can be 
used to model the content of documents. We 
focus on two criteria: quality and coverage. In 
terms of quality alone, our results show that 
technical term (TT) extraction \[Justeson and 
Katz 1995\] receives the highest rating. How- 
ever, in terms of a combined quality and cover- 
age metric, the Head Sorting (HS) method, 
described in \[Wacholder 1998\], outperforms 
both other methods, keyword (KW) and TT. 
2. Introduction 
In this paper, we consider the problem of how 
to evaluate the automatic identification of index 
terms that have been derived without recourse 
to lexicons or to other kinds of domain-specific 
information. By index terms, we mean natural 
language xpressions that constitute a meaning- 
ful representation f a document for humans. 
The premise of this research is that if signifi- 
cant topics coherently represent information in 
a document, hese topics can be used as index 
terms that approximate the content of individ- 
ual documents in large collections of electronic 
documents. 
We compare three shallow processing 
methods for identifying index terms: 
? Keywords (KW) are terms identified by 
counting frequency of stemmed words in a 
document; 
Technical terms (TT) are noun phrases 
(NPs) or subparts of NPs repeated more 
than twice in a document \[Justeson and 
Katz 1995\]; 
Head sorted terms (HS) are identified by 
a method in which simplex noun phrases 
(as defined below) are sorted by head and 
then ranked in decreasing order of fre- 
quency \[Wacholder 1998\]. 
The three methods that we evaluated are do- 
main-independent i  that they use statistical 
and/or linguistic properties that apply to any 
natural anguage document in any field. These 
methods are also corpus-independent, i  that 
the ranking of terms for an individual document 
is not dependent on properties of the corpus. 
2.1 Overview of methods and results 
Subjects were drawn from two groups: 
professionals and students. Professionals in- 
cluded librarians and publishing professionals 
familiar with both manual and automatic text 
indexing. Students included undergraduate and 
graduate students with a variety of academic 
interests. 
To assess terms, we used a standard 
qualitative ranking technique. We presented 
subjects with an article and a list of terms 
identified by one of the three methods. Subjects 
were asked to answer the following general 
question: "Would this term be useful in an 
electronic index for this article?" Terms were 
rated on a scale of 1 to 5, where 1 indicates a
high quality term that should definitely be in- 
cluded in the index and 5 indicates a junk term 
that definitely should not be included. For ex- 
1 This research was partly funded by NSF IRI 97-12069, "Automatic identification ofsignificant topics in do- 
main independent full text documents" and NSF IRI 97-53054, "Computationally tractable methods for docu- 
ment analysis". _'tO~ 
ample, the phrase court-approved affirmative 
action plans received an average rating of 1 
from the professionals, meaning that it was 
ranked as useful for the article; the KW af- 
firmative received an average rating of 3.75, 
meaning that it was less useful; and the KW 
action received an average ranking of 4.5, 
meaning that it was not useful. 
The goal of our research is to determine 
which method, or combination of methods, 
provides the best results. We measure results 
in terms of two criteria: quality and coverage. 
By quality, we mean that evaluators ranked 
terms high on the 1 to 5 scale from highest o 
lowest. By coverage, we mean the thoroughness 
with which the terms cover the significant op- 
ics in the document. Our methodology permits 
us to measure both criteria, as shown in Figure 
4. 
Our results from both the professionals and 
students how that TTs are superior with re- 
spect to quality; however, there are only a 
small number of TTs per document, so they do 
not provide adequate coverage in that they are 
not fully representative of the document as a 
whole. In contrast, KWs provide good cover- 
age but relatively poor quality in that KWs are 
vague, and not well filtered. SNPs, which have 
been sorted using HS and filtered, provide a 
better balance of quality and coverage. 
From our study, we draw the following 
conclusions: 
? The KW approach identifies some useful 
index terms, but they are mixed in with a 
large number of low-ranked terms. 
? The TT approach identifies high quality 
terms, but with low coverage, i.e., rela- 
tively few indexing terms. 
? The HS approach achieves a balance be- 
tween quality and coverage. 
3. Domain-independent metrics for identi- 
fying significant opics 
In order to identify significant opics in 
a document, a significance measure is needed, 
i.e., a method for determining which concepts 
in the document are relatively important for a 
given task. The need to determine the impor- 
tance of a particular concept within a document 
is motivated by a range of applications, in- 
cluding information retrieval \[Salton 1989\], 
303 
automatic determination of authorship 
\[Mosteller and Wallace 1963\], similarity met- 
rics for cross-document clustering \[Hatzivas- 
siloglou et al 1999\], automatic indexing 
\[Hodges et al 1996\] and input to summariza- 
tion \[Paice 1990\]. 
For example, one of the earlier appli- 
cations using frequency for identifying signifi- 
cant topics in a document was proposed by 
\[Luhn 1958\] for use in creating automatic ab- 
stracts. For each document, a list of stop- 
listed stems was created, and ranked by fre- 
quency; the most frequent keywords were used 
to identify significant sentences in the original 
document. Luhn's premise was that emphasis, 
as indicated by repetition of words and collo- 
cation is an indicator of significance. Namely, 
"the more often certain words are found in each 
other's company within a sentence, the more 
significance may be attributed to each of these 
words." This basic observation, although re- 
fined extensively by later summarization tech- 
niques (as reviewed in \[Paice 1990\]), relies on 
the capability of identifying significant con- 
cepts. 
The standard IR technique known as 
tf*idf \[Salton 1989\] seeks to identify docu- 
ments relevant o a particular query by relativ- 
izing keyword frequency in a document as 
compared to frequency in a corpus. This 
method can be used to locate at least some im- 
portant concepts in full text. Although it has 
been effective for information retrieval, for 
other applications, such as human-oriented in- 
dexing, this technique is impractical. Ambigu- 
ity of stems (trad might refer to trader or 
tradition) and of isolated words (state might be 
a political entity or a mode of being) means 
that lists of keywords have not usually been 
used to represent the content of a document to 
human beings. Furthermore, humans have a 
difficult time processing stems and parts of 
words out of phrasal context. 
The technical term (TT) method, an- 
other technique for identification of significant 
terms in text that can be used as index terms 
was introduced by \[Justeson and Katz 1995\], 
who developed an algorithm for identifying 
repeated multi-word phrases such as central 
processing unit in the computer domain or 
word sense in the lexical semantic domain. 
This algorithm identifies candidate TTs in a 
corpus by locating NPs consisting of nouns, 
adjectives, and sometimes prepositional 
phrases. TTs are defined as those NPs, or their 
subparts, which occur above some frequency 
threshold in a corpus. However, as \[Boguraev 
and Kennedy 1998\] observe, the TT technique 
may not characterize the full content of docu- 
ments. Indeed, even in a technical document, 
TTs do not provide adequate coverage of the 
NPs in a document that contribute to its con- 
tent, especially since TTs are by definition 
multi-word. A truly domain-general method 
should apply to both technical and non- 
technical documents. The relevant difference 
between technical and non-technical documents 
is that in technical documents, many of the 
topics which are significant to the document as 
a whole may be also TTs. 
\[Wacholder 1998\] proposed the 
method of Head Sorting for identifying signifi- 
cant topics that can be used to represent a
source document. HS also uses a frequency 
measure to provide an approximation of topic 
significance. However, instead of counting fre- 
quency of stems or repetition of word se- 
quences, this method counts frequency of a 
relatively easily identified grammatical e ement, 
heads of simplex noun phrases (SNPs). For 
common NPs (NPs whose head is a common 
noun), an SNP is a maximal NP that includes 
premodifiers uch as determiners and posses- 
sives but not post-nominal constituents such as 
prepositions or relativizers. For example, the 
well-known book is an SNP but the well-known 
book on asteroids includes two SNPs, well- 
known book and asteroids. For proper names, 
an SNP is a name that refers to a single entity. 
For example, Museum of the City of New York, 
the name of an organization, is an SNP even 
though the organizational name incorporates a 
city name. Others, such as \[Church 1988\], 
have discussed a similar concept, sometimes 
called simple or base NPs. 
The HS approach is based on the as- 
sumption that nominal elements can be used to 
convey the gist of a document. SNPs, which 
are semantically and syntactically coherent, 
appear to be at a good level of detail for con- 
tent representation f the document. ' 
304 
SNPs are identified by a system \[Evans 
1998; Evans et al 2000\] which sequentially 
parses text that has been tagged with part of 
speech using a finite state machine. Next, the 
complete list of SNPs identified in a document 
is sorted by the head of the phrase, which, at 
least for English-language common SNPs, is 
almost always the last word. The intuitive justi- 
fication for sorting SNPs by head is based on 
the fundamental linguistic distinction between 
head and modifier: in general, a head makes a 
greater contribution to the syntax and seman- 
tics of a phrase than does a modifier. This lin- 
guistic insight can be extended to the document 
level. If, as a practical matter, it is necessary to 
rank the contribution to a whole document 
made by the sequence of words constituting an 
NP, the head should be ranked more highly 
than other words in the phrase. This distinction 
is important in linguistic theory; for example, 
\[Jackendoff 1977\] discusses the relationship of 
heads and modifiers in phrase structure. It is 
also important in NLP, where, for example, 
\[Strzalkowski 1997\] and \[Evans and Zhai 
1996\] have used the distinction between heads 
and modifiers to add query terms to informa- 
tion retrieval systems. 
Powerful corpus processing techniques 
have been developed to measure deviance from 
an average occurrence or co-occurrence in the 
corpus. In this paper we chose to evaluate 
methods that depend only on document-internal 
data, independent of corpus, domain or genre. 
We therefore did not use, for example, tf*idf, 
the purely statistical technique that is the used 
by most information retrieval systems, or 
\[Smadja 1993\], a hybrid statistical and sym- 
bolic technique for identifying collocations. 
4. Experimental Method 
To evaluate techniques, we performed a quali- 
tative user evaluation i  which the terms identi- 
fied by each method were compared for 
usefulness as index terms. 
4.1 Subjects 
We performed our study with librari- 
ans, publishing professionals and undergradu- 
ate and graduate students at our university. 29 
subjects participated in the study: 7 librarians 
and publishing professionals and 22 students. 
4.2 Data 
For this experiment, we selected three 
articles from the 1990 Wall Street Journal 
contained in the Tipster collection of docu- 
ments. The articles were about 500 words in 
length. 
To compare methods, each article was 
processed three times: 1) with SMART to 
identify stemmed keywords \[Salton 1989\]; 2) 
with an implementation of the TT algorithm 
based on \[Justeson and Katz 1995\]; and 3) with 
our implementation of the HS method. Output 
for one article is shown in Appendix A. Figure 
1 shows the articles selected, their length in 
words and the number of index terms from 
each method for each article presented to the 
subjects. 
DOC words KW TT HS 
415-0109 509 63 4 49 
516-0043 594 51 9 54 
517-0062 514 52 8 57 
Figure 1: Word and term count, by type, per 
article 
The number of TTs is much lower than the 
number of KWs or HSs. This presented us with 
a problem: on the one hand, we were concerned 
about preserving the integrity of the three 
methods, each of which has their own logic, 
and at the same time, we were concerned to 
present lists that were balanced relative to each 
other. Toward this end, we made several deci- 
sions about presentation of the data: 
1. Threshold: So that no bias would be un- 
intentionally introduced, we presented 
subjects with all terms output by each 
method, up to a specified cut-off poin- 
However, using lists of equal length for 
each method would have necessitated ither 
omitting HSs and KWs or changing the 
definition of TTs. Therefore we made the 
following decisions: 
? For TTs, we included all identified 
terms; 
? For HSs, we included all terms whose 
head occurred more than once in the 
document; 
305 
. 
. 
? For KWs, we included all terms in or- 
der of decreasing frequency, up to the 
point where we observed diminishing 
quality and where the number of KWs 
approximated the number of HSs. 
Order: For the KW and TT approach, 
order is not significant. However, for the 
HS approach, the grouping together of 
phrases with common heads is, we claim, 
one of the advantages of the method. We 
therefore alphabetized the KWs and TTs in 
standard left to right order and alphabet- 
ized the HSs by head, e.g., trust account 
precedes money market fund. 
Morphological expansion: The KW ap- 
proach identifies stems which represent a
set of one or more morphological variants 
of the stem. Since in some cases the stem 
is not an English word, we expanded each 
stem to include the morphological variants 
that actually occurred in the article. For 
example, for the stem reject, we listed re- 
jected and rejecting but did not list rejects, 
which did not occur in the article. 
4.3 Presentation to subjects 
Each subject was presented with three articles. 
For one article, the subject received a head 
sorted list of HSs; for another article, the sub- 
ject received a list of technical terms, and for 
the third article, the subject saw a list of key- 
words. No time limit was placed on the task. 
5. Results 
Our results for the three types of terms, by 
document, are shown in Figure 2. Although we 
asked subjects to rate three articles, some vol- 
unteers rated only two. All results were in- 
cluded. 
Doc 
900405-0109 
900516-0043 3.73 
900517-0062 2.98 
3.27 Avg of Avgs 
Figure 2: Average 
index terms 
Avg 
KW 
rating 
3.08 
Avg Avg 
TT HS 
rating rating 
1.45 2.71 
2.19 2.71 
1.7 3.25 
1.79 2.89 
ratings of 3 types of 
5.1 Quality 
For the three lists of index terms, TTs received 
the highest ratings for all three documents--an 
average of 1.79 on the scale of 1 to 5, with 1 
being the best rating. HS came in second, with 
an average of 2.89, and KW came in last with 
an average of 3.27. It should be noted that av- 
eraging the average conceals the fact that the 
number of TTs is much lower than the other 
two types of terms, as shown in Figure 1. 
Figure 3 (included before Appendix A) 
shows cumulative rankings of terms by method. 
The X axis represents ratings awarded by sub- 
jects. The Y axis reflects the percentage of 
terms receiving a given rank or better. All data 
series must reach 100% since every term has 
been assigned a rating by the evaluators. At 
any given data point, a larger value indicates 
that a larger percentage of that series' data has 
that particular ating or better. For example, 
100% of the TTs have a rating of 3 or better; 
while only about 30% of the terms of the low- 
est-scoring KW document received a score of 3 
or better. In two out of the three documents, 
HS terms fall between TTs and KWs. 
5.2 Coverage 
The graph in Figure 3 shows results 
for quality, not coverage. In contrast, Figure 4, 
which shows the total number of terms rated at 
or below specified rankings, allows us to meas- 
ure quality and coverage. (1 is the highest rat- 
ing; 5 is the lowest.) This figure shows that the 
HS method identifies more high quality terms 
than the TT method oes. 
~ od HS 
Number of terms ranked 
at or better than 
2 3 4 5 
27 75 124 166 
41 96 132 160 
15 21 21 21 
Figure 4: Running total of terms identified at 
or below a specified rank 
TT clearly identifies the highest quality terms: 
100% of TTs receive a rating of 2 or better. 
However, only 8 TTs received a rating of 2 or 
better (38% of the total), while 41 HSs re- 
306 
ceived a rating of 2 or better (26% of the total). 
This indicates that the TT method misses many 
high quality terms. KW, the least discriminat- 
ing method in terms of quality, also provides 
better coverage than does TT. 
This result is consistent with our observa- 
tion that TT identifies the highest quality terms, 
but there are very few of them: an average of 7 
per 500 words compared to over 50 for HS and 
KW. Therefore there is a need for additional 
high quality terms. The list of HSs received a
higher average rating than did the list of KWs, 
as shown in Figure 2. This is consistent with 
our expectation that phrases containing more 
content-bearing modifiers would be perceived 
as more useful index terms than would single 
word phrases consisting only of heads. 
5.3 Ranking variability 
The difference in the average ratings for 
the list of KWs and the list of head-sorted 
SNPs was less than expected. The small differ- 
ence in average ratings for the HS list and the 
KW list can be explained, at least in part, by 
two factors: 1) Differences among profession- 
als and students in inter-subject agreement and 
reliability; 2) A discrepancy in the rating of 
single word terms across term types. 
22 students and 7 professionals par- 
ticipated in the study. Figure 5 shows differ- 
ences in the ratings of professionals and of 
students. 
KW 
HS 
TT 
Professionals Students 
2.64 3.30 
2.3 3.03 
1.49 2.1 
Figure 5: Average ratings, by term type, of 
professionals and students 
When variation in the scores for terms was cal- 
culated using standard eviation, the standard 
deviation for the professionals was 0.78, while 
for the students it was 1.02. Because of the 
relatively low number of professionals, the 
standard deviation was calculated only over 
terms that were rated by more than one profes- 
sional. A review of the students' results showed 
that they appeared not to be as careful as the 
professionals. For example, the phrase 'Wall 
Street Journal' was included on the HS list only 
because it is specified as the document source. 
However, four of the eight students assigned 
this term a high rating (1 or 2); this is puzzling 
because the document is about asbestos-related 
disease. The other four students assigned a 4 
or 5 to 'Wall Street Journal', as we expected. 
But the average score for this term was 3, due 
to the anomalous ratings. We therefore have 
more confidence in the reliability of the profes- 
sional ratings, even though there are relatively 
few of them. 
We examined some of the differences in 
rating for term types. Single word index terms 
are rated more highly by professionals when 
they appear in the context of other single word 
index terms, but are downrated in the context 
of phrasal expansions that make the meaning of 
the one-word term more specific. The KW list 
and HS list overlap when the SNP consists only 
of a single word (the head) or only of a head 
modified by determiners. When the same word 
appears in both lists in identical form, the token 
in the KW list tends to receive a better ating 
than the token does when it appears in the HS 
list, where it is often followed by expansions of 
the head. For example, the word exposure re- 
ceived an average rating of 2.2 when it ap- 
peared on the KW list, but a rating of only 2.75 
on the HS list. However, the more specific 
phrase racial quotas, which immediately fol- 
lowed quota on the HS list received a rating of 
1. 
To better understand these differences, we 
selected 40 multi-word phrases and examined 
the average score that the phrase received in the 
TT and HS lists, and compared it to the aver- 
age ratings that individual words received in 
the KW list. We found that in about half of the 
cases (21 of 40), the phrase as a whole and the 
individual words in the phrase received similar 
scores, as in Example 1 in Figure 6. In just 
over one-fourth of the cases (12 of 40), the 
phrase scored well, but scores from the indi- 
vidual words were rated from good to poor, as 
in Example 2. In about one-eighth of the cases 
(6 of 40), the phrase scored well, but the indi- 
vidual words scored poorly, as in Example 3. 
Finally, in only one case, shown in Example 4 
of Figure 6, the phrase scored poorly but the 
individual words scored well. 
307 
Phrase 
Supreme Court 
(1.5) 
reverse discrimi- 
I nation 
(1) 
lymph system 
employment 
decisions 
(2.75) 
Word 1 
Supreme 
(1) 
reverse 
(3.25) 
lymph 
(1) 
employ- 
ment 
(1.25) 
Word 2 
Court 
(1.25) 
discrimi- 
nation 
(3.25) 
system 
(5) 
decisions 
(1.25) 
Figure 6: Comparison of scores of phrases 
and single words 
This shows that single words in isolation are 
judged differently than the same word when 
presented in the context of a larger phrase. 
These results have important implications in 
the design of indexing tools. 
6. Conclusion 
Our results show that the head sorting 
technique outperforms two other indexing 
methods, technical terms and keywords, as 
measured by balance of quality and coverage. 
We have performed a qualitative valuation of 
three techniques for identifying significant 
terms in a document, driven by an indexing 
task. Such an applicati;on can be used to create 
a profile or thumbnail of a document by pre- 
senting to users a set of terms which can be 
considered to be a representation f the content 
of the document. We have used human judges 
to evaluate the effectiveness of each method. 
This research is a contribution to the overall 
evaluation of computational linguistic tools in 
terms of their usefulness for human-oriented 
computational applications. 
8. References 
Boguraev, Branimir and Kennedy, Christopher 
(1998) "Applications of term identification 
terminology: domain description and content 
characterisation", Natural Language Engi- 
neering 1(1): 1-28. 
Church, Kenneth Ward (1988) "A stochastic parts 
program and noun phrase parser for unre- 
stricted text", in Proceedings of the Second 
Conference on Applied Natural Language 
Processing, pp. 136-143. 
Evans, David A. and Chengxiang Zhai (1996) 
"Noun-phrase analysis in unrestricted text for 
information retrieval", Proceedings of the 
34th Annual Meeting of the Association for 
Computational Linguistics, pp. 17-24.24-27 
June 1996, University of California, Santa 
Cruz, California, Morgan Kaufmann Pub- 
lishers. 
Evans, David K. (1998) LinklT Documentation, 
Columbia University Department ofCom- 
puter Science Report. 
Evans, David K., Klavans, Judith, and Wacholder, 
Nina (2000) "Document processing with 
LinklT", RIAO Conference, Paris, France, to 
appear. 
Hatzivassiloglou, Vasileios, Judith L. Klavans and 
Eleazar Eskin (1999) "Detecting text simi- 
larity over short passages: exploring linguis- 
tic feature combinations via machine 
learning", Proceedings of the EMNLP/VLC- 
99 Joint SIGDAT Conference on Empirical 
Methods in NLP and Very Large Corpora, 
June 21-22, 1999, University of Maryland, 
College Park, MD. 
Hedges, Julia, Shiyun Yie, Ray Reighart and Lois 
Boggess (1996) "An automated system that 
assists in the generation ofdocument in- 
dexes", Natural Language Engineering 
2(2): 137-160. 
Jackendoff, Ray (1977) X-bar Syntax: A Study of 
Phrase Structure, MIT Press, Cambridge, 
MA. 
Justeson, John S. and Slava M. Katz (1995) 
"Technical terminology: some linguistic 
properties and an algorithm for identification 
in text", Natural Language Engineering 
1(1):9-27. 
Luhn, Hans P. (1958) "The automatic creation of 
literature abstracts", IBM Journal, 159-165. 
Mosteller, Frederick and David L. Wallace (1963) 
"Inference in an authorship problem", Jour- 
nal of the American Statistical Association 
58(302):275-309. Available at 
http://www.jstor.org/. 
Paice, Chris D. (1990) "Constructing literature 
abstracts by computer: techniques and pros- 
pects". Information Processing & Manage- 
ment 26(1): 171-186. 
Salton, Gerald (1989) Automatic Text Processing: 
The Transformation, Analysis and Retrieval 
of lnformation by Computer. Addison- 
Wesley, Reading, MA. 
Smadja, Frank (1993) "Retrieving collocations 
from text", Computational Linguistics 
19(1):143-177. 
Strzalkowski, Thomas (1997) "Building effective 
queries in natural language information re- 
trieval", Proceedings of the ANLP, ACL, 
Washington, DC., pp.299-306. 
Wacholder, Nina (1998) "Simplex NPS sorted by 
head: a method for identifying significant 
topics within a document", Proceedings of 
the Workshop on the Computational Treat- 
ment of Nominals, pp.70-79. COLING-ACL 
'98, Montreal, Canada, August 16, 1998. 
Figure 3: Cumulative ranking of terms, by method 
0.9 
0.8 
~ 0.7 
i. 0.6 0.5 
0.4 
~ o.a 
0.2 
?~ 0.1 
0 
0.5 1 1.5 2 2.5 3 
Rating 
3.5 4 4.5 5 
308 
Appendix A: Terms identified in WSJ900405-0109 
HSs 
amendments 
Hatch amendment 
other amendments 
attempts 
bias 
job bias 
intentional bias 
bill 
committee 
Senate labor Committee 
court 
Supreme Court 
co-workers 
decisions 
Supreme Court decisions 
employment decisions 
Democrats 
discrimination 
reverse discrimination 
employees 
women employees 
employers 
groups 
civil-rights groups 
conservative policy 
groups 
Orrin Hatch 
health 
discriminatory impact 
Job-Bias Measure 
basic employment anti- 
discrimination law 
1866 civil-rights law 
lawsuits 
lawmakers 
legislation 
comprehensive legislation 
more modest measure 
minority/minorities 
panel 
plans 
court-approved affirmative 
action plans 
discriminatory seniority plans 
practices 
employment practices 
quotas 
racial quotas 
fight/rights 
equal rights 
year 
Keywords 
action 
address/addressing 
adopt/adopted 
affirmative 
agree 
aimed 
alleged/alleging 
amend 
approved 
attempt/attempts 
bias 
bill 
Bush 
challenge 
circumstances 
civil 
clears 
committee 
court/Court 
decision 
Democrats 
discrimination 
employment/employers/employees 
force/Force 
give/giving 
GOP 
groups 
Hatch 
health 
~gh 
impact 
job 
justify 
labor/Labor 
law 
lawmakers 
lawsuits 
legislative/legislation 
make 
measure 
minority/minorities 
Mr. 
overturning 
panel 
plans 
policy 
practices 
quotas 
racial 
rejected/rejecting 
reverse 
rights 
rules/ruling 
safety 
Sen./Sens. 
Senate 
shown 
street 
Supreme; vote/voted 
309 
women 
workers 
year 
Technical terms 
discriminatory impact 
employment practice 
Senator Hatch 
Supreme Court 
Columbia?s Newsblaster: New Features and Future Directions
Kathleen McKeown, Regina Barzilay, John Chen, David Elson, David Evans,
Judith Klavans, Ani Nenkova, Barry Schiffman and Sergey Sigelman
Department of Computer Science
Columbia University
1214 Amsterdam Avenue, New York, N.Y. 10027
kathy@cs.columbia.edu
Abstract
Columbia?s Newsblaster tracking and summa-
rization system is a robust system that clus-
ters news into events, categorizes events into
broad topics and summarizes multiple articles
on each event. Here we outline our most cur-
rent work on tracking events over days, produc-
ing summaries that update a user on new infor-
mation about an event, outlining the perspec-
tives of news coming from different countries
and clustering and summarizing non-English
sources.
1 Introduction
Columbia?s Newsblaster1 provide news updates on a
daily basis from news published on the Internet; it crawls
news sites, categorizes stories into six broad areas, groups
news into stories on the same event, and generates a sum-
mary of the multiple articles describing each event. In ad-
dition to demonstrating the robustness of current summa-
rization and tracking technology, Newsblaster also serves
as a research environment in which we explore new di-
rections and problems. Currently, we are exploring the
tasks of multilingual summarization where input sources
are drawn frommultiple languages and a summary is gen-
erated in English on the same event (Figure 1), tracking
events across days and generating summaries that update
the user on what is new, and editing generated summaries
to improve fluency and accuracy. Our focus here is on
editing references to people, improving coherency of the
summary and ensuring that references are accurate. Edit-
ing is particularly important as we add multilingual capa-
bilities, given the errors inherent in machine translation.
1http://newsblaster.cs.columbia.edu
2 Multilingual Tracking and
Summarization
The multilingual version of Columbia Newsblaster is
built upon the English version of Columbia Newsblaster,
sharing the same structure and components. To add mul-
tilingual capability, the system first crawls web sites in
foreign languages, and stores both the language and en-
coding for the files. To extract the article text from the
HTML pages, we use a new article extraction component
using language-independent statistical features computed
over text blocks along with a machine learning compo-
nent to classify text blocks as one of ?Article Text?, ?Ti-
tle?, ?Image?, ?Image Caption?, or ?Other?. The article
extraction component has been trained and tested on En-
glish, Japanese, and Russian data, but is also being suc-
cessfully applied to French, Spanish, German, and Ital-
ian data. We plan to train the article extractor on other
languages (Chinese, Arabic, Korean, Spanish, German,
French, etc.) in the near future.
To cluster multilingual documents with English doc-
uments, we use the existing Newsblaster English doc-
ument clustering module. Non-English documents are
translated for clustering after the article extraction phase.
We use simple and fast document translation techniques
for clustering if available, since we potentially process
thousands of documents for a language for each run. We
have developed simple dictionary lookup techniques for
translation for clustering for Japanese and Russian; for
other languages we use an interface to the Systran trans-
lation system via Babelfish. We plan on adding Arabic
translation to the system in the near future.
Summarization is performed using the same summa-
rization strategies in Newsblaster. We are experimenting
with different methods for improving summary quality
when translation of text is noisy. For example, when an
input cluster contains both English and foreign sources,
we weight the English higher in cases where we deter-
mine it is representative of both the English and foreign
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 15-16
                                                         Proceedings of HLT-NAACL 2003
Figure 1: Multilingual Version
input documents. We are also experimenting with meth-
ods for determining similarity across documents using
different levels of translation.
3 Different Perspectives
When news media report on international issues, they re-
flect the perspectives of their own countries. In the past,
Newsblaster has included all international sources as in-
put to its summaries. Recently, we have added a feature
of ?international perspectives? to the system. In addition
to the universal summary for a particular event, which
includes all international sources, Newsblaster now gen-
erates separate summaries for each country, which may
illustrate unique biases or disagree on facts. The News-
blaster interface allows users to view any pair of sum-
maries side by side to compare different perspectives.
4 Summary Rewrite
Newsblaster also currently includes a module for rewrit-
ing summaries to achieve better readability. References
to people are rewritten so that the first mention includes
the person?s full name and a selected description and later
mentions are restricted to last name only. In addition to
improving readability, the rewritten version of the sum-
mary is usually shorter than the version before rewrite,
since multiple verbose descriptions of the same entity are
discarded. These changes can be seen when comparing
the summary sentence with the original document via a
link from the summary using a proxy.
5 Event Tracking and Updates
Newsblaster currently identifies events within a single
day; a new set of clusters is generated each day. We have
designed a new module for tracking events across days,
allowing the system to relate stories published on one day
to closely related stories on other days. In this way, the
user can more easily track events of interest as they un-
fold. The typical approach for tracking events across days
represents each event as one monolithic set of stories. We
have focused instead on a model where events on one day
can divide into related sub-events on the next day. For ex-
ample, a set of stories about the start of the Iraq war is an
event that can branch into multiple sets of stories, each set
representing a different facet of the war. We are currently
determining an appropriate evaluation of this approach as
well as investigating different possible interfaces.
If a user is tracking events across days, it is more useful
to have a summary that provides updates on what is new
as opposed to a summary of similarities across all days.
We have built a prototype update summarizer that scans
new articles extracted by the system and compares these
new articles with a background cluster on the same event.
The summarizer will provide the user with a summary of
only important new developments. As the tracking mod-
ule locates new articles, it will pass these to the update
summarizer, which will determine what, if anything, has
changed. This summarizer uses more syntactic and se-
mantic information about the articles to determine nov-
elty than is used in our other summarization strategies and
thus, efficiency is a challenge. We will demo these com-
ponents in a separately fromNewsblaster as they have not
yet been integrated in the development version.
Columbia Newsblaster: Multilingual News Summarization on the Web
David Kirk Evans Judith L. Klavans
Department of Computer Science
Columbia University, NY, NY 10027
{devans, klavans, kathy}@cs.columbia.edu
Kathleen R. McKeown
Abstract
We present the new multilingual version of
the Columbia Newsblaster news summariza-
tion system. The system addresses the problem
of user access to browsing news from multiple
languages from multiple sites on the internet.
The system automatically collects, organizes,
and summarizes news in multiple source lan-
guages, allowing the user to browse news top-
ics with English summaries, and compare per-
spectives from different countries on the topics.
1 Introduction
The Columbia Newsblaster1 system has been online and
providing summaries of topically clustered news daily
since late 2001 (McKeown et al, 2002). The goal of
the system is to aid daily news browsing by providing
an automatic, user-friendly access to important news top-
ics, along with summaries and links to the original arti-
cles for further information. The system has six major
phases: crawling, article extraction, clustering, sum-
marization, classification, and web page generation.
The focus of this paper is to present the entire mul-
tilingual Columbia Newsblaster system as a platform
for multilingual multi-document summarization exper-
iments. The phases in the multilingual version of
Columbia Newsblaster have been modified to take lan-
guage and character encoding into account, and a new
phase, translation, has been added. Figure 1 depicts the
multilingual Columbia Newsblaster architecture. We will
describe the system, in particular a method using machine
learning to extract article text from web pages that is ap-
plicable to different languages, and a baseline approach
to multilingual multi-document summarization.
1.1 Related Research
Previous work in multilingual document summarization,
such as the SUMMARIST system (Hovy and Lin, 1999)
1http://newsblaster.cs.columbia.edu/
Figure 1: Architecture of the multilingual Columbia
Newsblaster system.
extracts sentences from documents in a variety of lan-
guages, and translates the resulting summary. This sys-
tem has been applied to Information Retrieval in the
MuST System (Lin, 1999) which uses query translation
to allow a user to search for documents in a variety of lan-
guages, summarize the documents using SUMMARIST,
and translate the summary. The Keizei system (Ogden et
al., 1999) uses query translation to allow users to search
Japanese and Korean documents in English, and displays
query-specific summaries focusing on passages contain-
ing query terms. Our work differs in the document clus-
tering component ? we cluster news to provide emergent
topic structure from the data, instead of using an informa-
tion retrieval model. This is useful in analysis, monitor-
ing, and browsing settings, where a user does not have an
a priori topic in mind. Our summarization strategy also
differs from the approach taken by MuST in that we focus
our effort on the summarization system, but only target a
single language, shifting the majority of the multilingual
knowledge burden to specialized machine translation sys-
tems. The Keizei system has the advantage of being able
to generate query-specific summaries.
Chen and Lin (Chen and Lin, 2000) describe a sys-
tem that combines multiple monolingual news clustering
components, a multilingual news clustering component,
and a news summarization component. Their system
clusters news in each language into topics, then the mul-
tilingual clustering component relates the clusters that
are similar across languages. A summary is generated
by linking sentences that are similar from the two lan-
guages. The system has been implemented for Chinese
and English, and an evaluation over six topics is pre-
sented. Our clustering strategy differs here, as we trans-
late documents before clustering, and cluster documents
from all languages at the same time. This makes it easy
to add support for additional languages by incorporating a
new translation system for the language; no other changes
need to be made. Our summarization model also provides
summaries for documents from each language, allowing
comparisons between them.
2 Extracting article data
2.1 Extracting article text
To move Columbia Newsblaster into a multilingual ca-
pable environment, we must be able to extract the ?ar-
ticle text? from web pages in multiple languages. The
article text is the portion of a web page that contains the
actual news content of the page, as opposed to site navi-
gation links, ads, layout information, etc. Our previous
approach to extracting article text in Columbia News-
blaster used regular expressions that were hand-tailored
to specific web sites. Adapting this approach to new web
sites is difficult, and it is also difficult to adapt to for-
eign languages sites. We solved this problem by incor-
porating a new article extraction module using machine
learning techniques. The new article extraction module
parses HTML into blocks of text based on HTML markup
and computes a set of 34 features based on simple sur-
face characteristics of the text. We use features such as
the percentage of text that is punctuation, the number
of HTML links in the block, the percentage of question
marks, the number of characters in the text block, and so
on. Since the features are relatively language independent
they can be computed for and applied to any language.
Training data for the system is generated using a GUI
that allows a human to annotate text candidates with one
of fives labels: ?ArticleText?, ?Title?, ?Caption?, ?Im-
age?, or ?Other?. The ?ArticleText? label is associated
with the actual text of the article which we wish to ex-
tract. At the same time, we try to determine document
titles, image caption text, and image blocks in the same
framework. ?Other? is a catch-all category for all other
text blocks, such as links to related articles, navigation
links, ads, and so on. The training data is used with the
Language Training set Precision Recall
English 353 89.10% 90.70%
Russian 112 90.59% 95.06%
Russian English Rules 37.66% 73.05%
Japanese 67 89.66% 100.00%
Japanese English Rules 100.00% 20.00%
Table 1: Article extractor performance for detecting arti-
cle text in three languages.
machine learning program Ripper (Cohen, 1996) to in-
duce a hypothesis for categorizing text candidates accord-
ing to the features. This approach has been trained on
web pages from sites in English, Russian, and Japanese
as shown in Table 1, but has been used with sites in En-
glish, Russian, Japanese, Chinese, French, Spanish, Ger-
man, Italian, Portuguese, and Korean.
The English training set was composed of 353 arti-
cles, collected from 19 web sites. Using 10-fold cross-
validation, the induced hypothesis classify into the article
text category with a precision of 89.1% and a recall of
90.7%. Performance over Russian data was similar, with
a precision of 90.59% and recall of 95.06%. We evalu-
ated the English hypothesis against the Russian data to
observe whether the languages behave differently. As ex-
pected, the English hypothesis resulted in poor perfor-
mance over the Russian data, and we saw comparable
results for Japanese. The same English hypothesis per-
forms adequately on other English sites not in the train-
ing set, so the differences between languages seem to be
significant.
2.2 Title and date extraction
The article extraction component also determines a title
for each document, and attempts to locate a publishing
date for the articles. Title identification is important since
in a cluster, sometimes with as many as 60 articles, the
only information the user sees are the titles for the arti-
cles; if our system chooses poor titles, they will have a
difficult time discriminating between the articles. If the
article extraction component finds a title it is used. Un-
fortunately, this process is not always successful, so we
have a variety of fall-back methods, including taking the
title from the HTML TITLE tag, using heuristics to de-
tect the title from the first text block, and using a portion
of the first sentence. These approaches led to many un-
informative titles extracted from the non-English sites,
since they were developed for English news. We imple-
mented a system to identify titles that are clearly non-
descriptive, such as ?Stock Market News?, that would
apply to non-English text as well. We record the titles
seen and rejected over time and use the list to reject ti-
tles with high frequency. A title with high frequency is
assumed to be not descriptive enough to give a clear idea
of the content of an article in a cluster of similar articles.
To correctly extract dates for articles, we use heuristics
to identify sequences of possible dates, weigh them, and
choose the most likely date as the publication date. Reg-
ular expressions for Japanese date extraction were added
to the system.
3 Multilingual Clustering
The document clustering system that we use (Hatzivas-
siloglou et al, 2000) has been trained on, and extensively
tested with English. While it can cluster documents in
other languages, our goal is to generate clusters with doc-
uments from multiple languages, so a baseline approach
is to translate all non-English documents into English,
and then cluster the translated documents. We take this
approach, and further experimented with using simple
and fast techniques for glossing the input articles for clus-
tering. We developed simple dictionary lookup glossing
systems for Japanese and Russian. Our experimentation
showed that full translation using Systran outperformed
our glossing-based techniques, so the glossing techniques
are not used in the current system.
4 Multilingual Summarization Baseline
Our baseline approach to multilingual multi-document
summarization is to apply our English-based summa-
rization system, the Columbia Summarizer (McKeown
et al, 2001), to document clusters containing machine-
translated versions of non-English documents. The
Columbia Summarizer routes to one of two multi-
document summarization systems based on the similar-
ity of the documents in the cluster. If the documents
are highly similar, the Multigen summarization system
(McKeown et al, 1999) is used. Multigen clusters sen-
tences based on similarity, and then parses and fuses in-
formation from similar sentences to form a summary.
The second summarization system used is DEMS, the
Dissimilarity Engine for Multi-document Summarization
(Schiffman et al, 2002), which uses a sentence extraction
approach to summarization. The resulting summary is
then run through a named entity recovery tool (Nenkova
and McKeown, 2003), which repairs named entity refer-
ences in the summary by making the first reference de-
scriptive, and shortening subsequent reference mentions
in the summary. Using an unmodified version of DEMS,
summaries might contain sentences from translated doc-
uments which are not grammatically correct. The DEMS
summarization system was modified to prefer choosing
a sentence from an English article if there are sentences
that express similar content in multiple languages. By
setting different weight penalties we can take the quality
of the translation system for a given language pair into
Figure 2: A screen shot comparing a summary from En-
glish documents to a summary from German documents.
account.
4.1 Similarity-based Summarization
As part of our multilingual summarization work, we
are investigating approaches to summarization that use
sentence-level similarity computation across languages to
cluster sentences by similarity, and then generate a sum-
mary sentence using translated portions of the relevant
sentences. The multilingual version of Columbia News-
blaster provides us with a platform to frame future ex-
periments for this summarization technique. We are in-
vestigating translation at different levels - sentence level,
clause level, and phrase level. Our initial similarity-based
summarization system works at the sentence level. Start-
ing with machine-translated sentences, we compute their
similarity to English sentences that have been simpli-
fied(Siddharthan, 2002). Foreign-language sentences that
have a high enough similarity to English text are replaced
(or augmented with) the similar English sentence.
This first system using full machine translation over
the sentences and English similarity detection will be ex-
tended using simple features for multilingual similarity
detection in SimFinder MultiLingual (SimFinderML), a
multilingual version of SimFinder (Hatzivassiloglou et
al., 2001). We also plan an experiment evaluating the use-
fulness of noun phrase detection and noun phrase variant
detection as a primitive for multilingual similarity detec-
tion, using tools such as Christian Jacquemin?s FASTR
(Jacquemin, 1994; Jacquemin, 1999).
4.2 Summary presentation
Multilingual Newsblaster presents multiple views of a
cluster of documents to the user, broken down by lan-
guage and by country. Summaries are generated for the
entire cluster, as well as sub-sets of the articles based on
the country of origin and language of the original arti-
cles. Users are first presented with a summary of the en-
tire cluster using all documents, and then have the ability
to focus on countries or languages of their choosing. We
also allow the user to view two summaries side-by-side so
they can easily compare differences between summaries
from different countries. For example, figure 4.2 shows a
summary of articles about talks between America, Japan,
and Korea over nuclear arms, comparing the summaries
from articles in English and German.
5 Evaluation
Evaluation of multi-document summarization is a dif-
ficult task; the Document Understanding Conference
(DUC)2 is designed as an evaluation for multi-document
summarization systems. We participated in the DUC
2004 conference submitting the results of the summariza-
tion system used in Newsblaster, as well as an in-progress
system described in Section 4.1 for multilingual cluster
summarization. The results of the DUC evaluation will
provide us with valuable feedback on the multi-document
multi-lingual summarization components in Newsblaster.
6 Conclusions
In this paper we have described a multilingual version
of Columbia Newsblaster, a system that runs daily offer-
ing users an accessible interface to online news brows-
ing. The multilingual version of the system incorporates
two varieties of machine translation, one for clustering,
and one for translation of documents for summarization.
Existing summarization methods have been applied to
translated text, with plans for an evaluation of the current
method, and incorporation of summarization techniques
specific to translated documents. The system presents a
platform for further multilingual summarization experi-
ments and user-oriented studies.
References
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. A multilin-
gual news summarizer. In Proceedings of the 18th In-
ternational Conference on Computational Linguistics,
pages 159?165.
William W. Cohen. 1996. Learning trees and rules with
set-valued features. In AAAI/IAAI, Vol. 1, pages 709?
716.
Vasileois Hatzivassiloglou, Luis Gravano, and Ankineedu
Maganti. 2000. An investigation of linguistic features
and clustering algorithms for topical document clus-
tering. In Proceedings of the 23rd ACM SIGIR Con-
ference on Research and Development in Information
Retrieval.
2http://duc.nist.gov/
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa
Holcombe, Regina Barzilay, Min-Yen Kan, and Kathy
McKeown. 2001. Simfinder: A flexible clustering tool
for summarization. In Proceedings of the North Amer-
ican Association for Computational Linguistics Auto-
matic Summarization Workshop.
E.H. Hovy and Chin-Yew Lin. 1999. Automated text
summarization in summarist. In I. Mani and M. May-
bury, editors, Advances in Automated Text Summariza-
tion, chapter 8. MIT Press.
Christian Jacquemin. 1994. Fastr: a unification-based
front-end to automatic indexing. In In Proceedings,
Intelligent Multimedia Information Retrieval Systems
and Management (RIAO?94), pages p. 34?47.
Christian Jacquemin. 1999. Syntagmatic and paradig-
matic representations of term variation. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics (ACL?99), pages 341?348.
Chin-Yew Lin. 1999. Machine translation for informa-
tion access across the language barrier: the must sys-
tem. In Machine Translation Summit VII, September.
Kathleen McKeown, Judith Klavans, Vasileios Hatzivas-
siloglou, Regina Barzilay, and Eleazar Eskin. 1999.
Towards multidocument summarization by reformula-
tion: Progress and prospects. In AAAI, pages 453?460.
Kathleen R. McKeown, Regina Barzilay, David Kirk
Evans, Vasileios Hatzivassiloglou, Min-Yen Kan,
Barry Schiffman, and Simone Teufel. 2001. Columbia
multi-document summarization: Approach and evalu-
ation. In Proceedings of the Document Understanding
Conference.
Kathleen R. McKeown, Regina Barzilay, David Kirk
Evans, Vasileios Hatzivassiloglou, Judith L. Klavans,
Ani Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with columbia?s newsblaster. In Proceed-
ings of the Human Language Technology Conference.
Ani Nenkova and Kathy McKeown. 2003. References to
named entities: A corpus study. In Short Paper Pro-
ceedings of NAACL-HLT.
William Ogden, James Cowie, Mark Davis, Eugene Lu-
dovik, Hugo Molina-Salgado, and Hyopil Shin. 1999.
Getting information from documents you cannot read:
An interactive cross-language text retrieval and sum-
marization system. In SIGIR/DL Workshop on Mul-
tilingual Information Discovery and Access (MIDAS),
August.
Barry Schiffman, Ani Nenkova, and Kathleen McKeown.
2002. Experiments in multidocument summarization.
In Proceedings of the Human Language Technology
Conference, March.
Advaith Siddharthan. 2002. Resolving attachment and
clause boundary ambiguities for simplifying relative
clause constructs. In Proceedings of the Student Work-
shop, 40th Meeting of the Association for Computa-
tional Linguistics (ACL?02), pages 60?65, Philadel-
phia, USA.
Combining Linguistic and Machine Learning Techniques for Email
Summarization
Smaranda Muresan
Dept. of Computer Science
Columbia University
500 West 120 Street
New York, NY, 10027
smara@cs.columbia.edu
Evelyne Tzoukermann
Bell Laboratories
Lucent Technologies
700 Mountain Avenue
Murray Hill, NJ, 07974
evelyne@lucent.com
Judith L. Klavans
Columbia University
Center for Research on
Information Access
535 West 114th Street
New York, NY 10027
klavans@cs.columbia.edu
Abstract
This paper shows that linguistic tech-
niques along with machine learning
can extract high quality noun phrases
for the purpose of providing the gist
or summary of email messages. We
describe a set of comparative experi-
ments using several machine learning
algorithms for the task of salient noun
phrase extraction. Three main conclu-
sions can be drawn from this study: (i)
the modifiers of a noun phrase can be
semantically as important as the head,
for the task of gisting, (ii) linguistic fil-
tering improves the performance of ma-
chine learning algorithms, (iii) a combi-
nation of classifiers improves accuracy.
1 Introduction
In this paper we present a comparative study of
symbolic machine learning models applied to nat-
ural language task of summarizing email mes-
sages through topic phrase extraction.
Email messages are domain-general text, they
are unstructured and not always syntactically well
formed. These characteristics raise challenges for
automatic text processing, especially for the sum-
marization task. Our approach to email summa-
rization, implemented in the GIST-IT system, is
to identify topic phrases, by first extracting noun
phrases as candidate units for representing doc-
ument meaning and then using machine learning
algorithms to select the most salient ones.
The comparative evaluation of several machine
learning models in the settings of our experiments
indicates that : (i) for the task of gisting the mod-
ifiers of the noun phrase are equally as important
as the head, (ii) noun phrases are better than n-
grams for the phrase-level representation of the
document, (iii) linguistic filtering enhances ma-
chine learning techniques, (iv) a combination of
classifiers improves accuracy.
Section 2 of the paper outlines the machine
learning aspect of extracting salient noun phrases,
emphasizing the features used for classifica-
tion and the symbolic machine learning models
used in the comparative experiments. Section
3 presents the linguistic filtering steps that im-
prove the accuracy of the machine learning algo-
rithms. Section 4 discusses in detail our conclu-
sions stated above.
2 Machine Learning for Content
Extraction
Symbolic machine learning has been applied suc-
cessfully in conjunction with many NLP applica-
tions (syntactic and semantic parsing, POS tag-
ging, text categorization, word sense disambigua-
tion) as reviewed by Mooney and Cardie (1999).
We used machine learning techniques for finding
salient noun phrases that can represent the sum-
mary of an email message. This section describes
the three steps involved in this classification task:
1) what representation is appropriate for the infor-
mation to be classified as relevant or non-relevant
(candidate phrases), 2) which features should be
associated with each candidate, 3) which classifi-
cation models should be used.
Case 1
CNP: scientific/JJ and/CC technical/JJ articles/NNS
SNP1: scientific/JJ articles/NNS
SNP2: technical/JJ articles/NNS
Case 2
CNP: scientific/JJ thesauri/NNS and databases/NNS
SNP1: scientific/JJ thesauri/NNS
SNP2: scientific/JJ databases/NNS
Case 3
CNP: physics/NN and/CC biology/NN skilled/JJ researchers/NNS
SNP1: physics/NN skilled/JJ researchers/NNS
SNP2: biology/NN skilled/JJ researchers/NNS
Table 1: Resolving Coordination of NPs
2.1 Candidate Phrases
Of the major syntactic constituents of a sentence,
e.g. noun phrases, verb phrases, and prepositional
phrases, we assume that noun phrases (NPs) carry
the most contentful information about the doc-
ument, a well-supported hypothesis (Smeaton,
1999; Wacholder, 1998).
As considered by Wacholder (1998), the sim-
ple NPs are the maximal NPs that contain pre-
modifiers but not post-nominal constituents such
as prepositions or clauses. We chose simple NPs
for content representation because they are se-
mantically and syntactically coherent and they are
less ambiguous than complex NPs. For extracting
simple noun phrases we first used Ramshaw and
Marcus?s base NP chunker (Ramshaw and Mar-
cus, 1995). The base NP is either a simple NP or
a coordination of simple NPs. We used heuristics
based on POS tags to automatically split the co-
ordinate NPs into simple ones, properly assigning
the premodifiers. Table 1 presents some coordi-
nate NPs (CNP) encountered in our data collec-
tion and the results of our algorithm which split
them into simple NPs (SNP1 and SNP2).
2.2 Features used for Classification
The choice of features used to represent the can-
didate phrases has a strong impact on the accu-
racy of the classifiers (e.g. the number of exam-
ples needed to obtain a given accuracy on the test
data, the cost of classification). For our classifica-
tion task of determining if a noun phrase is salient
or not to the document meaning, we chose a set of
nine features.
Several studies rely on the linguistic intuition
that the head of the noun phrase makes a greater
contribution to the semantics of the nominal
group than the modifiers. However, for some
specific tasks in NLP , the head is not necessar-
ily the most semantically important part of the
noun phrase. In analyzing email messages from
the perspective of finding salient NPs, we claim
that the modifier(s) of the noun phrase - usually
nominal modifiers(s), often have as much seman-
tic content as the head. This opinion is also sup-
ported in the work of Strzalkowski et al (1999),
where syntactic NPs are captured for the goal
of extracting their semantic content but are pro-
cessed as an ?ordered? string of words rather than
a syntactic unit. Thus we introduce as a sepa-
rate feature in the feature vector, a new TF*IDF
measure which consider the NP as a sequence of
equally weighted elements, counting individually
the modifier(s) and the head.
Consider the following list of simple NPs se-
lected as candidates:
1. conference workshop announcement
2. international conference
3. workshop description
4. conference deadline
In the case of the first noun phrase, for exam-
ple, its importance is found in the two noun mod-
ifiers: conference and workshop as much as in
the head announcement, due to their presence as
heads or modifiers in the candidate NPs 2-4. Our
new feature will be: TF  IDF
conference
+TF 
IDF
workshop
+ TF  IDF
announcement
. Giving
these linguistic observations we divided the set of
features into three groups, as we mentioned also
in (Tzoukermann et al, 2001): 1) one associated
with the head of the noun phrase; 2) one associ-
ated with the whole NP and 3) one that represents
the new TF*IDF measure discussed above.
2.2.1 Features associated with the Head
We choose two features to characterize the
head of the noun phrases:
 head tfidf: the TF*IDF measure of the
head of the candidate NP. For the NP in
example (1) this feature will be TF 
IDF
announcement
.
 head focc: The position of the first occur-
rence of the head in text (the number of
words that precede the first occurrence of the
head divided by the total number of words in
the document).
2.2.2 Features associated with the whole NP
We select six features that we consider relevant
in determining the relative importance of the noun
phrase:
 np tfidf: the TF*IDF measure of
the whole NP. For the NP in the
example (1) this feature will be
TF  IDF
conference workshop announcement
.
 np focc: The position of the first occurrence
of the noun phrase in the document.
 np length words: Noun phrase length mea-
sured in number of words, normalized by di-
viding it with the total number of words in
the candidate NP list.
 np length chars: Noun phrase length mea-
sured in number of characters, normalized
by dividing it with the total number of char-
acters in the candidate NPs list.
 sent pos: Position of the noun phrase in the
sentence: the number of words that precede
the noun phrase, divided by sentence length.
For noun phrases in the subject line (which
are usually short and will be affected by this
measure), we consider the maximum length
of sentence in document as the normalization
factor.
 par pos: Position of noun phrase in para-
graph, same as sent pos, but at the paragraph
level.
2.2.3 Feature that considers all constituents
of the NP equally weighted
One of the important hypotheses we tested in
this work is that both the modifiers and the head
of NP contribute equally to its salience. Thus we
consider mh tfidf as an additional feature in the
feature vector.
 mh tfidf: the new TF*IDF measure that
takes also into consideration the importance
of the modifiers. In our example the value of
this feature will be : TF  IDF
conference
+
TF IDF
workshop
+TF IDF
announcement
In computing the TF*IDF measures (head tfidf,
np tfidf, mh tfidf), specific weights, w
i
, were as-
signed to account for the presence in the email
subject line and/or headlines in the email body.
 w
i1
: presence in the subject line and head-
line
 w
i2
: presence in the subject line
 w
i3
: presence in headlines where w
i1
> w
i2
> w
i3
.
These weights were manually chosen after a set
of experiments, but we plan to use a regression
method to automatically learn them.
2.3 Symbolic Machine Learning Models
We compared three symbolic machine learning
paradigms (decision trees, rule induction and de-
cision forests) applied to the task of salient NP
extraction, evaluating five classifiers.
2.3.1 Decision Tree Classifiers
Decision trees classify instances represented as
feature vectors, where internal nodes of the tree
test one or several attributes of the instance and
where the leaves represent categories. Depending
on how the test is performed at each node, there
exists two types of decision tree classifiers: axis
parallel and oblique. The axis-parallel decision
trees check at each node the value of a single at-
tribute. If the attributes are numeric, the test has
the form x
i
> t, where x
i
is one of the attribute
of an instance and t is the threshold. Oblique de-
cision trees test a linear combination of attributes
at each internal node:
n
X
i=1
a
i
x
i
+ a
n+1
> 0
where a
i
; :::; a
n+1
are real-valued coefficients.
We compared the performance of C4.5, an axis-
parallel decision tree classifier (Quinlan, 1993)
and OC1, an oblique decision tree classifier
(Murthy et al, 1993).
2.3.2 Rule Induction Classifiers
In rule induction, the goal is to learn the small-
est set of rules that capture all the generalisable
knowledge within the data. Rule induction clas-
sification is based on firing rules on a new in-
stance, triggered by the matching feature values
to the left-hand side of the rules. Rules can be of
various normal forms and can be ordered. How-
ever, the appropriate ordering can be hard to find
and the key point of many rule induction algo-
rithms is to minimize the search strategy through
the space of possible rule sets and orderings. For
our task, we test the effectiveness of two rule
induction algorithms : C4.5rules that form pro-
duction rules from unpruned decision tree, and
a fast top-down propositional rule learning sys-
tem, RIPPER (Cohen, 1995). Both algorithms
first construct an initial model and then iteratively
improve it. C4.5rules improvement strategy is a
greedy search, thus potentially missing the best
rule set. Furthermore, as discussed in (Cohen,
1995), for large noisy datasets RIPPER starts with
an initial model of small size, while C4.5rules
starts with an over-large initial model. This means
that RIPPER?s search is more efficient for noisy
datasets and thus is more appropriate for our data
collection. It also allows the user to specify the
loss ratio, which indicates the ratio of the cost of
false positives to the cost of false negatives, thus
allowing a trade off between precision and recall.
This is crucial for our analysis since we deal with
sparse data due to the fact that in a document the
number of salient NPs is much smaller than the
number of irrelevant NPs.
2.3.3 Decision Forest Classifier
Decision forests are a collection of decision
trees together with a combination function. We
test the performance of DFC (Ho, 1998), a deci-
sion forest classifier that systematically constructs
decision trees by pseudo-randomly selecting sub-
sets of components of feature vectors. The advan-
tage of this classifier is that it combines a set of
different classifiers in order to improve accuracy.
It implements different splitting functions. In the
setting of our evaluation we tested the informa-
tion gain ratio (similar to the one used by Quinlan
in C4.5). An augmented feature vector (pairwise
sums, differences, and products of features) was
used for this classifier.
3 Linguistic Knowledge Enhances
Machine Learning
Not all simple noun phrases are equally
important to reflect document meaning.
Boguraev and Kennedy (1999) discuss the
issue that for the task of document gisting, topical
noun phrases are usually noun-noun compounds.
In our work, we rely on ML techniques to decide
which are the salient NPs, but we claim that a
shallow linguistic filtering applied before the
learning process improves the accuracy of the
classifiers. We performed four filtering steps:
1. Inflectional morphological processing:
Grouping inflectional variants together can
help especially in case of short documents
(which is sometimes the case for email
messages). English nouns have only two
kinds of regular inflection: a suffix for
the plural mark and another suffix for the
possessive one.
2. Removing unimportant modifiers: In this
second step we remove the determiners that
accompany the nouns and also the auxil-
iary words most and more that form the pe-
riphrastic forms of comparative and superla-
tive adjectives modifying the nouns (e.g.
?the most complex morphology? will be fil-
tered to ?complex morphology?).
3. Removing common words: We used a list
of 571 common words used in IR systems
in order to further filter the list of candi-
date NPs. Thus, words like even, following,
every, are eliminated from the noun phrase
structure.
4. Removing empty nouns: Words like lot,
group, set, bunch are considered empty
heads. For example the primary concept of
the noun phrases like ?group of students?,
?lots of students? or ?bunch of students?
is given by the noun ?students?. We ex-
tracted all the nouns that appear in front of
the preposition ?of? and then sorted them by
frequency of appearance. A threshold was
then used to select the final list (Klavans et
al., 1990). Three different data collections
were used: the Brown corpus, the Wall Street
Journal, and a set of 4000 email messages
(most of them related to a conference orga-
nization). We generated a set of 141 empty
nouns that we used in this forth step of the
filtering process.
4 Results and Discussion
One important step in summarization is the dis-
covery of the relevant information from the source
text. Our approach was to extract the salient NPs
using linguistic knowledge and machine learning
techniques. Our evaluation corpus consists of a
collection of email messages which is heteroge-
neous in genre, length, and topic. We used 2,500
NPs extracted from 51 email messages as a train-
ing set and 324 NPs from 8 messages for testing.
Each NP was manually tagged for saliency by one
human judge. We are planning to add more judges
in the future and measure the interuser agreement.
This section outlines a comparative evaluation
of five classifiers using two feature settings on the
task of extracting salient NPs from email mes-
sages. The evaluation shows the following im-
portant results:
Result 1. In the context of gisting, the head-
modifier relationship is an ordered relation be-
tween semantically equal elements.
We evaluate the impact of adding mh tfidf (see
section 2.2), as an additional feature in the feature
vector. This is shown in Table 2 in the different
feature vectors fv1 and fv2. The first feature vec-
tor, fv1, contains the features in sections 2.2.1 and
2.2.2, while fv2 includes as an additional feature
mh tfidf.
As can be seen from Table 3, the results of eval-
uating these two feature settings using five differ-
ent classifiers, show that fv2 performed better than
fv1. For example, the DFC classifier shows an in-
crease both in precision and recall. This allows us
to claim that in the context of gisting, the syntactic
head of the noun phrase is not always the seman-
tic head, and modifiers can have also an important
role.
One advantage of the rule-induction algorithms
is that their output is easily interpretable by hu-
mans. Analyzing C4.5rules output, we gain an
insight on the features that contribute most in the
classification process. In case of fv1, the most im-
portant features are: the first appearance of the
NP and its head (np focc, head focc), the length
of NP in number of words (np length words) and
the tf*idf measure of the whole NP and its head
(np tfidf, head tfidf ). For example:
 IF head focc <= 0.0262172 AND np tfidf
> 0.0435465 THEN Relevant
 IF np focc <= 0.912409 AND
np length words > 0.0242424 THEN
Relevant
 IF head tfidf <= 0.0243452 AND np tfidf
<= 0.0435465 AND np length words <=
0.0242424 then Not relevant
In case of fv2, the new feature m tfidf impacts
the rules for both Relevant and Not relevant cat-
egories. It supercedes the need for np tfidf and
head tfidf, as can be seen also from the rules be-
low:
 IF mh tfidf > 0.0502262 AND np focc <=
0.892585 THEN Relevant
 IF mh tfidf > 0.0180134 AND
np length words > 0.0260708 THEN
Relevant
 IF mh tfidf <= 0.0223546 AND
np length words <= 0.0260708 THEN
Not relevant
 IF mh tfidf <= 0.191205 AND np focc >
0.892585 THEN Not relevant
Feature vector 1 (fv1)
head focc head tfidf np focc np tfidf np length chars np length words par pos sent pos
Feature vector 2 (fv2)
head focc head tfidf mh tfidf np focc np tfidf np length chars np length words par pos sent pos
Table 2: Two feature settings to evaluate the impact of mh tfidf
C4.5 OC1 C4.5 rules Ripper DFC
p r p r p r p r p r
fv1 73.3% 78.6% 73.7% 93% 73.7% 88.5% 83.6% 71.4% 80.3% 83.5%
fv2 70% 88.9% 82.3% 88% 73.7% 95% 85.7% 78.8% 85.7% 87.9%
Table 3: Evaluation of two feature vectors using five classifiers
Result 2. Classifiers? performance depends
on the characteristics of the corpus, and com-
bining classifiers improves accuracy
This result was postulated by evaluating the
performance of five different classifiers in the task
of extracting salient noun phrases. As measures
of performance we use precision and recall . The
evaluation was performed according to what de-
gree the output of the classifiers corresponds to
the user judgments and the results are presented
in Table 3.
We first compare two decision tree classifiers:
one which uses as the splitting function only a sin-
gle feature (C4.5) and the other, the oblique tree
classifier (OC1) which at each internal node tests
a linear combination of features. Table 3 shows
that OC1 outperforms C4.5.
Columns 4 and 5 from Table 3 show the rela-
tive performance of RIPPER and C4.5rules. As
discussed in (Cohen, 1995), RIPPER is more ap-
propriate for noisy and sparse data collection than
C4.5rules. Table 3 shows that RIPPER performs
better than C4.5rules in terms of precision.
Finally, we investigate whether a combination
of classifiers will improve performance. Thus we
choose the Decision Forest Classifier, DFC, to
perform our test. DFC obtains the best results,
as can be seen from column 6 of Table 3.
Result 3. Linguistic filtering is an important
step in extracting salient NPs
As seen from Result 2, the DFC performed best
in our task, so we chose only this classifier to
present the impact of linguistic filtering. Table
4 shows that linguistic filtering improves preci-
sion and recall, having an important role espe-
cially on fv2, where the new feature, mh tfidf was
used (from 69.2% precision and 56.25% recall to
85.7% precision and 87.9% recall).
without filtering with filtering
precision recall precision recall
fv1 75% 75% 80.3% 83.5%
fv2 69.2% 56.25% 85.7% 87.9%
Table 4: Evaluation of linguistic filtering
This is explained by the fact that the filter-
ing presented in section 3 removed the noise in-
troduced by unimportant modifiers, common and
empty nouns.
Result 4. Noun phrases are better candi-
dates than n-grams
Presenting the gist of an email message by
phrase extraction addresses one obvious question:
are noun-phrases better than n-grams for repre-
senting the document content? To answer this
question we compared the results of our system,
GIST-IT, that extracts linguistically well moti-
vated phrasal units, with KEA output, that ex-
tracts bigrams and trigrams as key phrases using
a Na?ive Bayes model (Witten et al, 1999). Table
5 shows the results on one email message. The
n-gram approach of KEA system extracts phrases
like sort of batch, extracting lots, wn, and even
URLs that are unlikely to represent the gist of a
document. This is an indication that the linguis-
tically motivated GIST-IT phrases are more use-
ful for document gisting. In future work we will
perform also a task-based evaluation of these two
GIST-IT KEA
perl module wordnet interface module
?wn? command line program sort of batch
simple easy perl interface WordNet data
wordnet.pm module accesses the WordNet
wordnet system lots of WordNet
query perl module WordNet perl
wordnet QueryData
wordnet package wn
wordnet relation perl module
command line extracting
wordnet data use this module
included man page extracting lots
free software WordNet system
querydata www.cogsci.princeton.edu
Table 5: Salient phrase extraction with GIST-IT vs. KEA on one email message
approaches, to test usability.
5 Related Work
Machine learning has been successfully applied
to different natural language tasks, including text
summarization. A document summary is seen
as a succinct and coherent prose that captures
the meaning of the text. Prior work in docu-
ment summarization has been mostly based on
sentence extraction. Kupiec et al (1995) use ma-
chine learning for extracting the most impor-
tant sentences of the document. But extrac-
tive summarization relies on the properties of
source text that emails typically do not have:
coherence, grammaticality, well defined struc-
ture. Berger and Mittal (2000) present a summa-
rization system, named OCELOT that provides
the gist of the web documents based on proba-
bilistic models. Their approach is closed related
with statistical machine translation.
As discussed in (Boguraev and Kennedy,
1999), the meaning of ?summary? should be ad-
justed depending on the information management
task for which it is used. Key phrases, for ex-
ample, can be seen as semantic metadata that
summarize and characterize documents (Witten
et al, 1999; Turney, 2000). These approaches
select a set of candidate phrases (bigrams or tri-
grams) and then apply Na?ive Bayes learning to
classify them as key phrases or not. But deal-
ing only with n-grams does not always provide
good output in terms of a summary. In (Bogu-
raev and Kennedy, 1999) the ?gist? of a document
is seen as a sequence of salient objects, usually
topical noun phrases, presented in a highlighted
context. Their approach is similar to extracting
technical terms (Justeson and Katz, 1995). Noun
phrases are used also in IR task (Strzalkowski et
al., 1999; Smeaton, 1999; Sparck Jones, 1999).
The work of Strzalkowski et al (1999) supports
our hypothesis that for some NLP tasks (gisting,
IR) the head+modifier relation of a noun phrase is
in fact an ordered relation between semantically
equally important elements.
6 Conclusions and Future Work
In this paper we presented a novel technique for
document gisting suitable for domain and genre
independent collections such as email messages.
The method extracts simple noun phrases using
linguistic techniques and then uses machine learn-
ing to classify them as salient for the document
content. The contributions of this work are:
1. From a linguistic standpoint, we demon-
strated that the modifiers of a noun phrase
can be as semantically important as the head
for the task of gisting.
2. From a machine learning standpoint, we
evaluated the power and limitation of sev-
eral classifiers: decision trees, rule induc-
tion, and decision forests classifiers.
3. We proved that linguistic knowledge can en-
hance machine learning by evaluating the
impact of linguistic filtering before applying
the learning scheme.
The study, the evaluation, and the results pro-
vide experimental grounds for research not only
in summarization, but also in information extrac-
tion and topic detection.
References
A.L. Berger and V.O. Mittal. 2000. OCELOT:A sys-
tem for summarizing web pages. In Proceedings of
the 23rd Anual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 144?151, Athens, Greece.
B. Boguraev and C. Kennedy. 1999. Salience-based
content characterisation of text documents. In In-
terjit Mani and T. Maybury, Mark, editors, Ad-
vances in Automatic Text Summarization, pages 99?
111. The MIT Press.
W. Cohen. 1995. Fast effective rule induction. In
Machine-Learning: Proceedings of the Twelfth In-
ternational Conference.
T.K. Ho. 1998. The random subspace method
for constructing decision forests. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(8):832?844.
J. Justeson and S. Katz. 1995. Technical terminol-
ogy: Some linguistic properties and an algorithm
for identification in text. Natural Language Engi-
neering, (1):9?27.
J.L. Klavans, M.S. Chodorow, and N. Wacholder.
1990. From dictionary to knowledge base via
taxonomy. In Proceedings of the Sixth Confer-
ence of the University of Waterloo Centre for the
New Oxford English Dictionary and Text Research:
Electronic Text Research, University of Waterloo,
Canada.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A train-
able document summarizer. In Proceedings on the
18th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 68?73, Seattle,WA.
R.J Mooney and C. Cardie. 1999. Symbolic ma-
chine learning for natural language processing. In
ACL?99 Tutorial.
S.K. Murthy, S. Kasif, S. Salzberg, and R. Beigel.
1993. OC1: Randomized induction of oblique de-
cision trees. In Proceedings of the Eleventh Na-
tional Conference on Artificial Intelligence, pages
322?327, Washington, D.C.
J.R Quinlan. 1993. C4.5: Program for Machine
Learning. Morgan Kaufmann Publisher, San Ma-
teo, California.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Pro-
ceedings of Third ACL Workshop on Very Large
Corpora.
A. Smeaton. 1999. Using NLP or NLP resources
for information retrieval tasks. In Tomek Strza-
lkowski, editor, Natural Language Information Re-
trieval. Kluwer, Boston, MA.
K. Sparck Jones. 1999. What is the role for NLP in
text retrieval. In Tomek Strzalkowski, editor, Nat-
ural Language Information Retrieval, pages 1?12.
Kluwer, Boston, MA.
T. Strzalkowski, F. Lin, J. Wang, and J. Perez-
Carballo. 1999. Evaluating natural language pro-
cessing techniques in information retrieval. In
Tomek Strzalkowski, editor, Natural Language In-
formation Retrieval. Kluwer, Boston, MA.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336,
May.
E Tzoukermann, S Muresan, and J.L. Klavans.
2001. GIST-IT: Summarizing email using linguis-
tic knowledge and machine learning. In Proceeding
of the HLT and KM Workshop, EACL/ACL 2001.
N. Wacholder. 1998. Simplex NPS sorted by head:
A method for identifying significant topics within
a document. In Proceedings of the COLING-ACL
Workshop on the Computational Treatment of Nom-
inals, Montreal, Canada.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. KEA: Practical au-
tomatic keyphrase extraction. In Proceedings of
DL?99, pages 254?256.
Applying Natural Language Generation to Indicative Summarization
Min-Yen Kan and Kathleen R. McKeown
Department of Computer Science
Columbia University
New York, NY 10027, USA
 
min,kathy  @cs.columbia.edu
Judith L. Klavans
Columbia University
Center for Research on Information Access
New York, NY, 10027
klavans@cs.columbia.edu
Abstract
The task of creating indicative sum-
maries that help a searcher decide
whether to read a particular document
is a difficult task. This paper exam-
ines the indicative summarization task
from a generation perspective, by first
analyzing its required content via pub-
lished guidelines and corpus analysis.
We show how these summaries can be
factored into a set of document features,
and how an implemented content plan-
ner uses the topicality document fea-
ture to create indicative multidocument
query-based summaries.
1 Introduction
Automatic summarization techniques have
mostly neglected the indicative summary, which
characterizes what the documents are about. This
is in contrast to the informative summary, which
serves as a surrogate for the document. Indicative
multidocument summaries are an important way
of helping a user discriminate between several
documents returned by a search engine.
Traditional summarization systems are primar-
ily based on text extraction techniques. For an in-
dicative summary, which typically describes the
topics and structural features of the summarized
documents, these approaches can produce sum-
maries that are too specific. In this paper, we pro-
pose a natural language generation (NLG) model
for the automatic creation of indicative multidoc-
ument summaries. Our model is based on the val-
ues of high-level document features, such as its
distribution of topics and media types.
Highlighted differences between the documents:
The topics include "definition" and "what are the risks?"
More information on additional topics which are not
(The American Medical Assocation family medical 
Physicians and Surgeons complete home medical guide).
This file (5 minute emergency medicine consult) is 
close in content to the extract.
included in the extract is available in these files
The Merck manual of medical information contains
extensive information on the topic.
guide and The Columbia University College of
We found 4 documents on Angina:
Summary of the Disease: Angina
Get information on: [ variant angina | treatment? | diag ... ]
N
av
ig
at
io
na
l A
id
s
Treatment is designed to prevent or reduce ischemia andExtract:
minimize symptoms.  Angina that cannot be controlled by drugs ...
Ex
tra
ct
ed
 S
um
m
ar
y
G
en
er
at
ed
  S
um
m
ar
y
Figure 1: A CENTRIFUSER summary on the
healthcare topic of ?Angina?. The generated in-
dicative summary in the bottom half categorizes
documents by their difference in topic distribu-
tion.
Specifically, we focus on the problem of con-
tent planning in indicative multidocument sum-
mary generation. We address the problem of
?what to say? in Section 2, by examining what
document features are important for indicative
summaries, starting from a single document con-
text and generalizing to a multidocument, query-
based context. This yields two rules-of-thumb for
guiding content calculation: 1) reporting differ-
ences from the norm and 2) reporting information
relevent to the query.
We have implemented these rules as part of the
content planning module of our CENTRIFUSER
summarization system. The summarizer?s archi-
tecture follows the consensus NLG architecture
(Reiter, 1994), including the stages of content cal-
culation and content planning. We follow the
generation of a sample indicative multidocument
query-based summary, shown in the bottom half
of Figure 1, focusing on these two stages in the
remainder of the paper.
2 Document features as potential
summary content
Information about topics and structure of the doc-
ument may be based on higher-level document
features. Such information typically does not
occur as strings in the document text. Our ap-
proach, therefore, is to identify and extract the
document features that are relevant for indica-
tive summaries. These features form the poten-
tial content for the generated summary and can
be represented at a semantic level in much the
same way as input to a typical language gener-
ator is represented. In this section, we discuss the
analysis we did to identify features of individual
and sets of multiple documents that are relevant
to indicative summaries and show how feature se-
lection is influenced by the user query.
2.1 Features of individual documents
Document features can be divided into two sim-
ple categories: a) those which can be calculated
from the document body (e.g. topical struc-
ture (Hearst, 1993) or readability using Flesch-
Kincaid or SMOG (McLaughlin, 1969) scores),
and b) ?metadata? features that may not be con-
tained in the source article at all (e.g. author
name, media format, or intended audience). To
decide which of these document features are im-
portant for indicative summarization, we exam-
ined the problem from two points of view. From
a top-down perspective, we examined prescriptive
guidelines for summarization and indexing. We
analyzed a corpus of indicative summaries for the
alternative bottom-up perspective.
Prescriptive Guidelines. Book catalogues in-
dex a number of different document features in
order to provide enhanced search access. The
United States MARC format (2000), provides in-
dex codes for document-derived features, such as
for a document?s table of contents. It provides a
larger amount of index codes for metadata docu-
ment features such as fields for unusual format,
size, and special media. ANSI?s standard on de-
scriptions for book jackets (1979) asks that pub-
lishers mention unusual formats, binding styles,
or whether a book targets a specific audience.
Descriptive Analysis. Naturally indicative
summaries can also be found in library catalogs,
since the goal is to help the user find what they
need. We extracted a corpus of single document
summaries of publications in the domain of con-
sumer healthcare, from a local library. The corpus
contained 82 summaries, averaging a short 2.4
sentences per summary. We manually identified
several document features used in the summaries
and characterized their percentage appearance in
the corpus, presented in Table 1.
Document Feature % appearance
in corpus
Document-derived features
Topicality 100%
(e.g. ?Topics include symptoms, ...?)
Content Types 37%
(e.g. ?figures and tables?)
Internal Structure 17%
(e.g. ?is organized into three parts?)
Readability 18%
(e.g. ?in plain English?)
Special Content 7%
(e.g. ?Offers 12 credit hours?)
Conclusions 3%
Metadata features
Title 32%
Revised/Edition 28%
Author/Editor 21%
Purpose 18%
Audience 17%
Background/Lead 11%
Source 8%
(e.g. ?based on a report?)
Media Type 5%
(e.g. ?Spans 2 CDROMs?)
Table 1: Distribution of document features in li-
brary catalog summaries of consumer healthcare
publications.
Our study reports results for a specific domain,
but we feel that some general conclusions can be
drawn. Document-derived features are most im-
portant (i.e., most frequently occuring) in these
single document summaries, with direct assess-
ment of the topics being the most salient. Meta-
data features such as the intended audience, and
the publication information (e.g. edition) infor-
mation are also often provided (91% of sum-
maries have at least one metadata feature when
they are independently distributed).
2.2 Generalizing to multiple documents
We could not find a corpus of indicative multi-
document summaries to analyze, so we only ex-
amine prescriptive guidelines for multidocument
summarization.
The Open Directory Project?s (an open source
Yahoo!-like directory) editor?s guidelines (2000)
states that category pages that list many different
websites should ?make clear what makes a site
different from the rest?. ?the rest? here can mean
several things, such as ?rest of the documents in
the set to be summarized? or ?the rest of the doc-
uments in the collection?. We render this as the
following rule-of-thumb 1:
1. for a multidocument summary, a content
planner should report differences in the doc-
ument that deviate from the norm for the
document?s type.
This suggests that the content planner has an
idea of what values of a document feature are
considered normal. Values that are significantly
different from the norm could be evidence for
a user to select or avoid the document; hence,
they should be reported. For example, consider
the document-derived feature, length: if a doc-
ument in the set to be summarized is of signifi-
cantly short length, this fact should be brought to
the user?s attention.
We determine a document feature?s norm
value(s) based on all similar documents in the cor-
pus collection. For example, if all the documents
in the summary set are shorter than normal, this is
also a fact that may be significant to report to the
user. The norms need to be calculated from only
documents of similar type (i.e. documents of the
same domain and genre) so that we can model dif-
ferent value thresholds for different kinds of doc-
uments. In this way, we can discriminate between
?long? for consumer healthcare articles (over 10
pages) versus ?long? for mystery novels (over 800
pages).
2.3 Generalizing to interactive queries
If we want to augment a search engine?s ranked
list with an indicative multidocument summary,
we must also handle queries. The search engine
ranked list does this often by highlighting query
terms and/or by providing the context around a
query term. Generalizing this behavior to han-
dling multiple documents, we arrive at rule-of-
thumb 2.
2. for a query-based summary, a content plan-
ner should highlight differences that are rel-
evant to the query.
This suggests that the query can be used to
prioritize which differences are salient enough
to report to the user. The query may be rele-
vant only to a portion of a document; differences
outside of that portion are not relevant. This
mostly affects document-derived document fea-
tures, such as topicality. For example, in the con-
sumer healthcare domain, a summary in response
to a query on treatments of a particular disease
may not want to highlight differences in the doc-
uments if they occur in the symptoms section.
3 Introduction to CENTRIFUSER
CENTRIFUSER is the indicative multi-document
summarization system that we have developed
to operate on domain- and genre-specific doc-
uments. We are currently studying consumer
healthcare articles using it. The system produces
a summary of multiple documents based on a
query, producing both an extract of similar sen-
tences (see Hatzivassiliglou et al (2001)) as well
as generating text to represent differences. We fo-
cus here only on the content planning engine for
the indicative, difference reporting portion. Fig-
ure 2 shows the architecture of the system.
We designed CENTRIFUSER?s input based on
the requirements from our analysis; document
features are extracted from the input texts and
serve as the potential content for the generated
summary. CENTRIFUSER uses a plan to select
summary content, which was developed based on
our analysis and the resulting previous rules.
Our current work focuses on the document fea-
ture which most influences summary content and
form, topicality. It is also the most significant and
useful document feature. We have found that dis-
cussion of topics is the most important part of the
indicative summary. Thus, the text plan is built
around the topicality document feature and other
features are embedded as needed. Our discussion
Query as 
Individual
Features
Indicative Summary
(differences; generated)
Document Features
IR Engine
Navigation Aids
Document set
Query
documents in collection
All domain? / genre?
Centrifuser System
Extracted Synopsis
(similarities; extracted)
to be summarized
Composite
Document Document
Features
Figure 2: CENTRIFUSER architecture.
now focuses on how the topicality document fea-
ture is used in the system.
In the next sections we detail the three stages
that CENTRIFUSER follows to generate the sum-
mary: content calculation, planning and realiza-
tion. In the first, potential summary content is
computed by determining input topics present in
the document set. For each topic, the system as-
sesses its relevance to the query and its prototyp-
icality given knowledge about the topics covered
in the domain. More specifically, each document
is converted to a tree of topics and each of the
topics is assigned a topic type according to its re-
lationship to the query and to its normative value.
In the second stage, our content planner uses a
text plan to select information for inclusion in
the summary. In this stage, CENTRIFUSER deter-
mines which of seven document types each doc-
ument belongs to, based on the relevance of its
topics to the query and their prototypicality. The
plan generates a separate description for the doc-
uments in each document type, as in the sample
summary in Figure 1, where three document cat-
egories was instantiated. In the final stage, the
resulting description is lexicalized to produce the
summary.
4 Computing potential content:
topicality as topic trees
In CENTRIFUSER, the topicality document fea-
ture for individual documents is represented by
a tree data structure. Figure 3 gives an example
document topic tree for a single consumer health-
Coronary Artery Disease
Angina
Uns..Sta...Con...Cor...Ex...Rad...Var..
Causes Symptoms Diagnosis Prognosis Treatment
Uns.. Ex...
Document:  Merck.xml
. . .       . . .      . . .
Figure 3: A topic tree for an article about coro-
nary artery disease from The Merck manual of
medical information, constructed automatically
from its section headers.
care article. Each document in the collection is
represented by such a tree, which breaks each
document?s topic into subtopics.
We build these document topic trees automati-
cally for structured documents using a simple ap-
proach that utilizes section headers, which suf-
fices for our current domain and genre. Other
methods such as layout identification (Hu et al,
1999) and text segmentation / rhetorical parsing
(Yaari, 1999; Kan et al, 1998; Marcu, 1997) can
serve as the basis for constructing such trees in
both structured and unstructured documents, re-
spectively.
4.1 Normative topicality as composite topic
trees
As stated in rule 1, the summarizer needs norma-
tive values calculated for each document feature
to properly compute differences between docu-
ments.
The composite topic tree embodies this
paradigm. It is a data structure that compiles
knowledge about all possible topics and their
structure in articles of the same intersection of
domain and genre, (i.e., rule 1?s notion of ?doc-
ument type?). Figure 4 shows a partial view of
such a tree constructed for consumer healthcare
articles.
The composite topic tree carries topic infor-
mation for all articles of a particular domain and
genre combination. It encodes each topic?s rela-
tive typicality, its prototypical position within an
article, as well as variant lexical forms that it may
be expressed as (e.g. alternate headers). For in-
stance, in the composite topic tree in Figure 4, the
topic ?Symptoms? is very typical (.95 out of 1),
*Definition* . . .*Symptoms*
. . .
Variants:
"CHF", "CAD",
"Atherosclerosis",
Typicality: 1.00
"Angina Pectoris",
"Angina",
Ordering: 1 of 1
Level: 1
"Arterio...", ...
   "How did I get *X*?"
"What is *X*?"
Variants: "Definition",
Typicality: .75
Ordering: 1 of 7
Level: 2
Level: 2
Ordering: 2 of 7
Typicality: .22
Variants: "Causes",
   "What causes *X*?",
Composite Topic Tree
Typicality: .95
Level: 2
Ordering: 3 of 7
Variants: "Symptoms",
    "Signs", "Signs and 
    Symptoms", ...
Genre: Patient Information
Domain : Disease
*Causes*
*Disease*
Figure 4: A sample composite topic tree for con-
sumer health information for diseases.
may be expressed as the variant ?Signs? and usu-
ally comes after other its sibling topics (?Defini-
tion? and ?Cause?).
Compiling composite topic trees from sample
documents is a non-trivial task which can be done
automatically given document topic trees. Within
our project, we developed techniques that align
multiple document topic trees using similarity
metrics, and then merge the similar topics (Kan
et al, 2001), resulting in a composite topic tree.
5 Content Planning
NLG systems traditionally have three compo-
nents: content planning, sentence planning and
linguistic realization. We will examine how the
system generates the summary shown earlier in
Figure 1 by stepping through each of these three
steps.
During content planning, the system decides
what information to convey based on the calcu-
lated information from the previous stage. Within
the context of indicative multidocument summa-
rization, it is important to show the differences
between the documents (rule 1) and their relation-
ship to the query (rule 2). One way to do so is to
classify documents according to their topics? pro-
totypicality and relevance to the query. Figure 5
gives the different document categories we use to
capture these notions and the order in which in-
formation about a category should be presented
in a summary.
e e
e
e
e
ee
e
e
GenericIrrelevant
Deep
Start
End
Prototypical
Comprehensive
Specialized
Atypical
Figure 5: Indicative summary content plan, solid
edges indicate moves in the sample summary.
5.1 Document categories
Each of the document categories in the content
plan in Figure 5 describes documents that are sim-
ilar in their distribution of information with re-
spect to the topical norm (rule 1) and to the query
(rule 2). We explain these document categories
found in the text plan below. The examples in the
list below pertain to a general query of ?Angina?
(a heart disorder) in the same domain of consumer
healthcare.
1. Prototypical - contains information that
one would typically expect to find in an on-topic
document of the domain and genre. An exam-
ple would be a reference work, such as The AMA
Guide to Angina.
2. Comprehensive - covers most of the typical
content but may also contain other added topics.
An example could be a chapter of a medical text
on angina.
3. Specialized - are more narrow in scope than
the previous two categories, treating only a few
normal topics relevant to the query. A specialized
example might be a drug therapy guide for angina.
4. Atypical - contains high amounts of rare
topics, such as documents that relate to other gen-
res or domains, or which discuss special topics.
If the topic ?Prognosis? is rare, then a document
about life expectancy of angina patients would be
an example.
5. Deep - are often barely connected with the
query topic but have much underlying informa-
tion about a particular subtopic of the query. An
example is a document on ?Surgical treatments of
Angina?.
6. Irrelevant - contains mostly information not
relevant to the query. The document may be very
broad, covering mostly unrelated materials. A
document about all cardiovascular diseases may
be considered irrelevant.
7. Generic - don?t display tendencies towards
any particular distribution of information.
5.2 Topic types
Each of these document categories is different
because they have an underlying difference in
their distribution of information. CENTRIFUSER
achieves this classification by examining the dis-
tribution of topic types within a document. CEN-
TRIFUSER types each individual topic in the in-
dividual document topic trees as one of four pos-
sibilities: typical, rare, irrelevant and intricate.
Assigning topic types to each topic is done by op-
erationalizing our two content planning rules.
To apply rule 2, we map the text query to the
single most similar topic in each document topic
tree (currently done by string similarity between
the query text and the topic?s possible lexical
forms). This single topic node ? the query node
? establishes a relevant scope of topics. The rele-
vant scope defines three regions in the individual
topic tree, shown in Figure 6: topics that are rel-
evant to the query, ones that are too intricate, and
ones that are irrelevant with respect to the query.
Irrelevant topics are not subordinate to the query
node, representing topics that are too broad or be-
yond the scope of the query. Intricate topics are
too detailed; they are topics beyond  hops down
from the query node.
Each individual document?s ratio of topics in
these three regions thus defines its relationship
to the query: a document with mostly informa-
tion on treatment would have a high ratio of rele-
vant to other topics if given a treatment query; but
the same document given a query on symptoms
would have a much lower ratio.
To apply rule 1, we need to know whether a
particular topic ?deviates from the norm? or not.
We interpret this as whether or not the topic nor-
mally occurs in similar documents ? exactly the
information encoded in the composite topic tree?s
typicality score. As each topic in the document
topic trees is an instance of a node in the compos-
ite topic tree, each topic can inherit its composite
node?s typicality score. We assign nodes in the
relevant region (as defined by rule 2), with labels
based on their typicality. For convenience, we set
Irrelevant
Tree BTree A
Relevant
Relevant
Intricate
	 

  	Verification and Validation of Language Processing Systems: Is It
Evaluation?
Valerie B. Barr
Department of Computer Science
Hofstra University
Hempstead, NY 11549-1030  USA
vbarr@hofstra.edu
Judith L. Klavans
Center for Research on Information Access
Columbia University
535 West 114th Street, MC 1103
New York, NY  10027 USA
klavans@cs.columbia.edu
Abstract
If Natural Language Processing
(NLP) systems are viewed as
intelligent systems then we should be
able to make use of verification and
validation (V&V) approaches and
methods that have been developed in
the intelligent systems community.
This paper addresses language
engineering infrastructure issues by
considering whether standard V&V
methods are fundamentally different
than the evaluation practices
commonly used for NLP systems, and
proposes practical approaches for
applying V&V in the context of
language processing systems.  We
argue that evaluation, as it is
performed in the NL community, can
be improved by supplementing it with
methods from the V&V community.
1 NLP Systems as Intelligent
Systems
Language engineering research is carried out in
areas such as speech recognition, natural
language understanding, natural language
generation, speech synthesis, information
retrieval, information extraction, and inference
(Jurafsky  & Martin, 2000).   In practice this
means building systems which model human
activities in various language processing tasks.
Therefore, we can quite clearly view language
processing systems as forms of intelligent
systems.  This view allows us to draw on work
that has been done within the intelligent systems
community within computer science on
verification and validation of systems.  It also
allows us to consider V&V in the context of NL
systems, and evaluation, as carried out on NL
systems, in the context of software engineering
methodologies.  This research extends the first
author?s earlier work on software testing
methodologies in the context of expert systems
(Barr, 1995; Barr, 1999).
2 Verification and Validation of
Intelligent Systems
The area of verification and validation of
software systems has suffered from a
multiplicity of definitions (Barr, 2001; Gonzalez
and Barr, 2000).  However, the most commonly
used definitions are :
? Verification ? ensuring that software
correctly implements specific functions,
that it satisfies its specification.
? Validation ? determining that the system
satisfies customer requirements.
These definitions have been re-examined in
order to account for the differences between
?conventional? software and intelligent systems.
An intelligent system is built based on an
interpretation of the problem domain, with the
expectation that the system will behave in a
fashion that is equivalent to the behavior of an
expert in the domain.  It follows that human
performance is often the benchmark we use to
evaluate an intelligent system.
The usual definitions of verification and
validation can be applied to intelligent systems
with slight modifications to take into account
the presence of a knowledge base and the
necessity of comparing system performance to
that of humans in the problem domain.  The core
issue in validation and verification of an
intelligent system boils down to one simple
objective: ensuring that the resulting system will
provide an answer, solution or behavior
equivalent to what an expert in the field would
say if given the same inputs.
Therefore the definitions of verification and
validation have been refined (Gonzalez and
Barr, 2000) in order to account for the differing
aspects of intelligent systems :
? Verification ? the process of ensuring 1)
that the intelligent system conforms to
specifications, and 2) its knowledge
base is consistent and complete within
itself.
? Validation ? the process of ensuring that
the output of the intelligent system is
equivalent to those of human experts
when given the same inputs.
Consistency of the knowledge base means that
there are no redundancies, conflicts or cycles.
Completeness means that all facts are used, there
are no unreachable conclusions, missing rules
(in the rule-based expert systems context), there
are no dangling conditions. These definitions of
verification and validation retain the standard
definitions used in software engineering, while
also requiring that the knowledge base be free of
internal errors, and letting human performance
be the standard for ?customer requirements?.
In the realm of language processing, the
?expert? can often be any user of language in the
context for which the system has been
developed.
3 Evaluation of NLP Systems
The previous section presented definitions for
V&V.  In this section the general paradigms for
evaluation of NLP system is presented.
3.1 Taxonomies of Evaluation within
NLP
Our review of  the evaluation literature indicates
that NLP systems have largely been evaluated
using a black-box, functional, approach.
Evaluation is often subdivided into formative
evaluation and summative evaluation (Sparck
Jones & Galliers, 1996).  The former determines
if the system meets the objectives that were set
for it.  It can be diagnostic, indicating areas in
which the system does not perform well, or
predictive, indicating the performance that can
be expected in actual use.  Summative
evaluation is a comparison of different systems
or approaches for solving a single problem.
In a somewhat different taxonomy
(Hirschman and Thompson, 1998), evaluation is
subdivided into
? Adequacy evaluation ? determination
of the fitness of a system for its intended
purpose.  Will it do what is required by
the user, how well, and at what cost?
? Diagnostic evaluation ? exposure of
system failures and production of a
system performance profile.
? Performance evaluation ?
measurement of system performance in
one or more specific areas.  Can be used
to compare alternative implementations
or successive generations of a system.
We can see that performance evaluation
overlaps with summative evaluation, while
adequacy evaluation corresponds to formative
evaluation.
While the evaluation process must consider
the results generated by an NLP system, it also
considers the usability of the system
(Hirschman and Thompson, 1998; White and
Taylor, 1998), its features, and how easily it can
be enhanced.  For example, a translation system
may appear to work well in a testbed situation,
but may not function well when embedded into
a larger system.  Or it may perform acceptably
when its output is intended for a general
audience, but not when an expert uses the
output.   
Sparck Jones and Galliers (1996) discuss
how the evaluation process should take into
account whether the NLP task is part of a larger
system with both linguistic and non-linguistic
components, and determine the impact on
overall performance of each of the subparts.  We
call this component performance evaluation.
Additional complexity arises in the evaluation of
component performance within multi-faceted
systems, such as embodied conversational
agents, where  assessment of how well the
system works is based on more than strict
language aspects, considering also more subtle
features such as gesture and tone (Cassell et al,
2000). Furthermore, whether or not a system
response is considered to be correct or
acceptable may depend on who is judging it.
In general, NLP systems for various kinds of
tasks require differing views of the evaluation
process, with different criteria, measures, and
methods.  For example, consider the ways in
which evaluation of machine translation (MT)
systems is carried out.  Notice that not all
aspects of  validation and verification, as
discussed in section 2, are represented.
Evaluation of machine translation (MT) systems
has to consider the pre-processing of input and
the post-editing of output.  Black-box evaluation
of MT systems can measure the percentage of
words that are incorrect in the entire output text
(based on how post-editing changes raw output
text to fix it).  But whether or not a word is
considered incorrect in the output may depend
on the task of the system.  So functional
evaluation of an MT system may have to be
augmented by a subjective determination of
whether the output text carries the same
information as the input text, and whether the
output is intelligible (Sparck Jones and Galliers
1996).  Another example is the case of speech
interfaces and spoken dialogue systems.   the
evaluation process typically focuses on the
accuracy, coverage, and speed of the system,
with increasing attention paid to user
satisfaction (James et al 2000, Walker and
Hirschman 1999).  Notice that in just these two
examples, various kinds of evaluation are called
into play.  We will argue in section 4 that V&V
techniques extend these evaluation methods,
providing system coverage analysis that assesses
completeness and consistency.
3.2 Factors which impact evaluation
Evaluation of NLP systems must also take
into account the kinds of inputs we expect a
system to work on after its testing phase is
complete.  Wacholder (1997)  demonstrates the
extent to which the linguistic complexity of
documents is one of the factors responsible for
the weakness of applications that process natural
language texts.  The ability to categorize test
data by complexity will help distinguish
between a failure of an NLP system that results
from extraordinary document complexity
(beyond that of the data on which the system
was tested) and a failure that results from
inadequate testing of the NLP tool.  The former
should be predictable, while the latter should
rarely happen if a system has been adequately
tested.  It is certainly possible that a tool may be
very well tested, functionally and with regard to
consistency and completeness, on text of certain
degree of complexity, but still fail on text that is
more complex or from a different domain.
3.3 Comparative evaluation efforts
There are NLP evaluation methods that,
although in a different problem domain, closely
mirror the approach typically used with expert
systems, comparing machine results to human
results.  For example, the TAUM-AVIATION
machine translation system was evaluated in
1980, in part by comparing the raw translation
produced by the system to several human
translations.  Then revised and post-edited
translations (human and machine) were rated
and ranked by a number of potential users
(Sparck Jones and Galliers, 1996).  This is
essentially the same testing method that was
used for the MYCIN expert system (Yu, 1985)
and many additional systems.  However, within
the expert systems area several methods have
been developed in subsequent years that address
the weaknesses of strictly functional evaluation
approaches (e.g. Barr, 1999; Grossner, 1993).
 There are also well-known evaluation
efforts such as EAGLES (Sparck Jones and
Galliers, 1996) and the Paradise evaluation
framework (Walker et al, 1997).  In addition,
many researchers have participated in the
comparative evaluation efforts characterized by
the Text Retrieval Conferences (TREC)1, the
Message Understanding Conferences (MUC)2
and Document Understanding Conferences
(DUC)3, the Cross-Language Evaluation Forum
(CLEF)4, and the summarization evaluation
effort (SUMMAC) (for a very comprehensive
list of evaluation related links, see
http://www.limsi.fr/TLP/CLASS/prj.eval.links.h
tml).
Evaluation of NLP systems is aided by the
fact that there is considerable test data available.
There are substantial repositories of data, such
as the TREC collection that includes, among
other data, Associated Press wire feeds;
Department of Energy documents; Federal
Register documents; Wall Street Journal full
texts; and sources from Ziff-Davis Publishing.
                                                     
1
 http://trec.nist.gov/
2
http://www.itl.nist.gov/iaui/894.02/related_projects/ti
pster/muc.htm
3
 http://www-nlpir.nist.gov/projects/duc/index.html
4
 http://www.iei.i.cnr.it/DELOS/CLEF/
It is important to note that the DARPA/ARPA
sponsored conferences (MUC, TIPSTER, and
TREC, for example), while making considerable
data available, promote functional testing by
stressing black-box performance of a system.
The metrics used in the MUC program are
oriented toward functional testing, focusing on
the number of spots in a template that are
correctly filled in by a particular MUC system,
along with various error-based measures.  For
database query systems the emphasis has been
on functional testing, supplemented with
evaluations of the system by users, given the
desire to create marketable systems.
An issue that arises in comparative
evaluation efforts, particularly because there is
so much test data available, is what it means to
compare the behavior of two systems designed
to carry out the same task, based on their
performance on a common set of test data.
Allen (1995) argues that evaluation results for
individual systems, and any comparison of
results across systems, should not be given
much credence until they reach ?some
reasonably high level of performance.?
Certainly the MUC and TREC programs are
based on comparing performance of multiple
systems on a common task.  One of the purposes
of our research is to show that without
assessment of consistency and completeness, the
quality of the functional testing alone may not
be sufficient for predicting reliability of an NLP
system and V&V methods will improve the
situation.
3.4 Additional comments on functional
testing
We have referred to functional testing in
prior paragraphs in the context of various
aspects of evaluation.  Recent literature (Declerk
et al, 1998; Rubio et al, 1998; Klavans et al,
1998; Jing et al, 1998) shows that functional
testing is still very much in use for evaluation of
NLP systems and larger systems of which NLP
components are a part.  Where other evaluation
mechanisms are in use, they are still based on
the behavior of the system under test, not based
on an analysis of how test case execution
exercises the system.  For example, White and
Taylor (1998) propose evaluating machine
translation (MT) systems based on what kind of
text handling tasks could be supported by the
output of the MT system.  They examine the text
handling tasks (publishing, gisting, extraction,
triage, detection, filtering) to determine how
good a translation has to be in order for it to be
useful for each task.  They then rank the text
handling tasks in such a way that if an MT
system?s output can facilitate a task, it can also
facilitate tasks lower on the scale, but is unlikely
to facilitate tasks higher on the scale.  This kind
of evaluation is functional in nature, though the
assessment of the quality of the MT system?s
output is based not on an examination of the
output but on a subsequent use of the output.
The notion of functional glass-box testing
does not assess coverage of the system itself, but
is essentially an assessment of how well a
system carries out its task.  It relies on the
programmer or tester?s idea of how a component
should carry out its task for a particular test
input (Sparck Jones and Galliers 1996).  At the
same time, black-box evaluation is a very
important and powerful testing approach,
particularly because it works from the
perspective of the user, without concern for
implementation.
4 Applying V&V to NLP ? Is it
Evaluation?
In the previous section we outlined many
different types of evaluation that are performed
on NL systems.  Our claim at the beginning of
the paper was that evaluation, as it is performed
in the NL community, can be improved by
adopting V&V approaches.  In this section we
show specifically what the relationship is
between V&V, as it is typically applied in
software development, and evaluation as it is
carreid out in the context of NLP systems.
In considering whether V&V and evaluation
are equivalent, we need to consider whether the
evaluation process achieves the goals of
verification and validation.  That is, does the
evaluation process demonstrate that
? the system is correct and conforms to its
specification
? the knowledge inherent in the system is
consistent and complete
? the output is equivalent to that of human
?experts?.
It is apparent that summative, adequacy and
diagnostic evaluation are all in some way
equivalent to validation.  The evaluation steps
involve black-box exercise of test data through
the system, which then allows for a comparison
of actual results to expected results.  This
facilitates an assessment of whether the output is
equivalent to that of human experts (who
provide the expected results).
The usual evaluation processes, through
formative evaluation, also facilitate one aspect
of verification, in that they allow us to determine
if a system conforms to its specification.  That
is, based on the specification for a system, a
domain-based test set can be constructed for
evaluation which will then demonstrate whether
or not a system meets the specification.
It is the second aspect of verification,
determining whether the knowledge represented
within the system is consistent and complete,
that seems not to be taken into account by the
evaluation processes in NLP.  The difficulty lies
in the fact that a domain based test set can never
completely test the actual system as built.
Rather, it tests the linguistic assumptions that
motivated construction of the system.  A domain
based test set can determine if the system
behaves correctly over the test data, but may not
adequately test the full system.  In particular,
any inconsistencies in the knowledge
represented within the system, or missing
knowledge, may not be identified by an
evaluation process that relies on domain-based
test data.
To address this issue, we need to apply
additional testing techniques, based on coverage
of the actual system, in order to achieve the full
breadth of verification activities on a language
processing system.  Furthermore, we may not
need larger test sets, but we may need different
test cases in the test set.
5 Applying V&V to NLP ? How Do
We Do It?
In our research we are in the early stages of
experiments wherein we apply existing V&V
tools to a number of NL systems for indexing
and significant topics detection.  We expect the
results of these experiments will support our
claim that V&V techniques will positively
enhance the evaluation process.
The actual software testing tools we
have chosen are based on the implementation
paradigms that are used in the specific NL
systems.  For example, for a C based system for
automatic indexing (Wacholder et al, 2001), we
have selected the Panorama C/C++ package5.
Various features of this tool facilitate testing of
the system as built, based on code coverage
rather than domain coverage.  This approach
effectively tests the knowledge base for
consistency and completeness, which we cannot
do as successfully with a domain based test set.
Regular expressions are frequently used
to implement components of NL systems.   We
are studying a component (Evans et al, 2000) of
a significant topics identification system that
uses regular expressions.  The software testing
community has not yet developed tools for
addressing coverage (completeness and
consistency) testing of regular expressions.  In
this case we will construct a tool, building on
theoretical work (Yannakakis and Lee, 1995)
that has been carried out in the network protocol
research community for testing finite-state
machines.
Clearly we propose adding additional
steps to the testing process.  However, this does
not necessarily imply that huge amounts of
additional test data will be necessary.  In a
typical testing protocol, a developer can start the
V&V phase with a domain based test set.
Additional test cases are then added
incrementally as needed until the test set is
adequate for coverage of the system as built, and
for assessment of the consistency and
completeness  of the system?s knowledge.
6 Open Questions
The question we intend to address in future
research is whether different natural language
application areas can profitably benefit from
different techniques utilized in the software
testing/V&V world.  For example, the issues
involved with grammars and parsers are
undoubtedly quite different from those that
come into play in machine translation systems.
With grammars and parsers it is quite tempting
to test the grammar by running the parser and
vice versa.  Yet the grammar and parser are
essentially built concurrently, and an error in
one would easily carry over as an error in the
other.  Typical testing strategies make it quite
                                                     
5
 http://www.softwareautomation.com
difficult to expose these sorts of errors.  A
testing approach is necessary which will help
expose errors or incompletenesses that exist in
both a grammar and its parser.  An effort by
Br?ker (2000) applies code instrumentation
techniques to grammar analysis.  However, the
kinds of errors that may occur in a translation
system or a language generation system are of a
different nature and will require different testing
strategies to expose.
7 Acknowledgements
This research was partially supported by the
National Science Foundation under NSF
POWRE grant #9973855.  We also thank
Bonnie Webber, of the University of Edinburgh,
and the Columbia University Natural Language
Processing Group, particularly Kathy McKeown
and Nina Wacholder, for their helpful
discussions.
References
Allen, James (1995).  Natural Language
Understanding.  Benjamin/Cummings, Redwood
City, CA.
Barr, Valerie (1995).  TRUBAC:  A Tool for
Testing Expert Systems with Rule-Base Coverage
Measures.  Proceedings of the 13th Annual Pacific
Northwest Software Quality Conference, Portland,
OR.
Barr, Valerie  (1999).  Applications of Rule-Base
Coverage Measures to Expert System Evaluation.
Journal of Knowledge Based Systems, Volume 12
(1999), pp. 27-35.
Barr, Valerie (2001). A quagmire of terminology.
Proceedings of Florida Artificial Intelligence
Research Symposium 2001.
Br?ker, Norbert (2000). The use of instrumentation
in grammar engineering.  Proceedings of COLING
2000.
Cassell, Justine, Joseph Sullivan, Scott Prevost,
and Elizabeth Churchill (2000).  Embodied
Conversational Agents.  MIT Press, Cambridge, MA.
Declerk, Thierry et al (1998).  Evaluation of the
NLP Components of an Information Extraction
System for German.  Proceedings of the 1st
International Conference on Language Resources
and Evaluation, Granada, Spain, May 1998, pgs.
293-297.
Evans, David K. et al (2000).  Document
Processing with LinkIT,  Proceedings of RIAO 2000
(Recherche d?Informations Assistee par Ordinateur),
Paris.
Gonzalez, Avelino and Valerie Barr (2000).
Validation and verification of intelligent systems ?
what are they and how are they different? Journal of
Experimental and Theoretical Artificial Intelligence,
12(4).
Grossner, C. et al (1993). Exploring the structure
of rule based systems.  Proceedings, AAAI-93,
Washington, D.C., pp. 704-709.
Hirschman, Lynette and Henry S. Thompson
(1998).  Overview of Evaluation in Speech and
Natural Language Processing in Survey of the State
of the Art in Human Language Technology, Giovanni
Varile and Antonio Zampolli, eds., Cambridge
University Press, New York.
James, Frankie et al (2000).  Accuracy, Coverage,
and Speed:  What Do They Mean to Users?  See
http://www.riacs.edu/doc/2000/htmlo/chi_nl_worksh
op.html
Jing, Hongyan et al (1998).  Summarization
Evaluation Methods: Experiments and Analysis.
AAAI Symposium on Intelligent Summarization,
March 1998, Stanford University.
Jurafsky, Daniel and James Martin (2000).  Speech
and Language Processing. Prentice-Hall, NJ.
Klavans, Judith L., Kathleen McKeown, Min-Yen
Kan, and Susan Lee (1998).  Resources for
Evaluation of Summarization Techniques.
Proceedings of the First International Conference on
Language Resources and Evaluation, Granada,
Spain, 1998, pgs. 899-902.
Rubio, A. et.al. (1998).  On the Comparison of
Speech Recognition Tasks. Proceedings of the First
International Conference on Language Resources
and Evaluation, Granada, Spain, 1998.
Sparck Jones, Karen and Julia Galliers (1996).
Evaluating Natural Language Processing Systems.
Springer-Verlag, Berlin.
Wacholder, Nina. (1997).  POWRE:
Computationally Tractable Methods for Document
Analysis.  CS Report, Dept. of Computer Science,
Columbia University (NSF funded project).
Wacholder, Nina,  et al (2001). Automatic
Generation of Indexes for Digital Libraries. IEEE-
ACM Joint Conference on Digital Libraries.
Walker, M., et.al. (1997).  PARADISE:  A
Framework for evaluating spoken dialogue agents.
Proceedings of Association of Computational
Linguists 35th Annual Meeting.
Walker, M. and L. Hirschman (1999).  DARPA
Communicator Evaluation Proposal.
www.research.att.com/~walker/eval/evalplan6.rtf
White, John S. and Kathryn B. Taylor (1998).  A
Task-Oriented Evaluation Metric for Machine
Translation.  Proceedings of the First International
Conference on Language Resources and Evaluation,
Granada, Spain, May 1998, pgs. 21-25.
Yannakakis, Mihalis and David Lee (1995).
Testing Finite State Machines : Fault Detection .
Journal of Computer and Systems Sciences, Volume
50, pages 209-227.
Yu, V.L. et.al. (1985). An evaluation of MYCIN?s
advice.  In Rule-Based Expert Systems, Bruce
Buchanan and Edward Shortliffe (Eds.), Addison-
Wesley, Reading, MA.                                                                                                                                      
GIST-IT: Summarizing Email Using Linguistic Knowledge and Machine 
Learning  
 
Evelyne Tzoukermann 
Bell Labs, Lucent 
Technologies 
700 Mountain Avenue 
Murray Hill, NJ, 07974, USA 
evelyne@lucent.com 
Smaranda Muresan 
Columbia University  
500 W 120th Street 
New York, NY, 10027, USA 
smara@cs.columbia.edu 
Judith L. Klavans 
Columbia University 
Center for Research on 
Information Access 
535 W 114th Street 
New York, NY, 10027, USA 
klavans@cs.columbia.edu 
 
Abstract  
We present a system for the automatic 
extraction of salient information from 
email messages, thus providing the gist of 
their meaning.   Dealing with email raises 
several challenges that we address in this 
paper:  heterogeneous data in terms of 
length and topic. Our method combines 
shallow linguistic processing with 
machine learning to extract phrasal units 
that are representative of email content. 
The GIST-IT application is fully 
implemented and embedded in an active 
mailbox platform.  Evaluation was 
performed over three machine learning 
paradigms.  
Introduction 
The volume of email messages is huge and 
growing.  A qualitative and quantitative study of 
email overload [Whittaker and Sidner (1996)] 
shows that people receive a large number of 
email messages each day (~ 49) and that 21% of 
their   inboxes (about 334 messages) are long 
messages (over 10 Kbytes).  Therefore 
summarization techniques adequate for real-
world applications are of great interest and need 
[Berger and Mittal (2000), McKeown and Radev 
(1995), Kupiec et al(1995), McKeown et al
(1999), Hovy (2000)].  
  
In this paper we present GIST-IT, an 
automatic email message summarizer that will 
convey to the user the gist of the document 
through topic phrase extraction, by combining 
linguistic and machine learning techniques. 
 Email messages and web documents raise 
several challenges to automatic text 
processing, and the summarization task 
addresses most of them: they are free-style 
text, not always syntactically or 
grammatically well-formed, domain and 
genre independent, of variable length and on 
multiple topics.  Furthermore, due to the lack 
of well-formed syntactic and grammatical 
structures, the granularity of document 
extracts presents another level of complexity.  
In our work, we address the extraction 
problem at phrase-level [Ueda et al(2000), 
Wacholder et al(2000)], identifying salient 
information that is spread across multiple 
sentences and paragraphs.           
Our novel approach first extracts simple 
noun phrases as candidate units for 
representing document meaning and then 
uses machine learning algorithms to select 
the most prominent ones.  This combined 
method allows us to generate an informative, 
generic, ?at-a-glance? summary.  
In this paper, we show: (a) the efficiency 
of the linguistic approach for phrase 
extraction in comparing results with and 
without filtering techniques,  (b) the 
usefulness of vector representation in 
determining proper features to identify 
contentful information, (c) the benefit of 
using a new measure of TF*IDF for the noun 
phrase and its constituents, (d) the power of 
machine learning systems in evaluating 
several classifiers in order to select the one 
performing the best for this task. 
1 Related work  
Traditionally a document summary is seen as a 
small, coherent prose that renders to the user the 
important meaning of the text. In this framework 
most of the research has focused on extractive 
summaries at sentence level. However, as 
discussed in [Boguraev and Kennedy (1999)], 
the meaning of ?summary? should be adjusted 
depending on the information management task 
for which it is used. Key phrases, for example, 
can be seen as semantic metadata that 
summarize and characterize documents [Witten 
et al(1999), Turney (1999)]. These approaches 
select a set of candidate phrases (sequence of 
one, two or three consecutive stemmed, non-stop 
words) and then apply machine learning 
techniques to classify them as key phrases or 
not. But dealing only with n-grams does not 
always provide good output in terms of a 
summary (see discussion in Section 5.4).      
 Wacholder (1998) proposes a linguistically-
motivated method for the representation of the 
document aboutness: ?head clustering?. A list of 
simple noun phrases is first extracted, clustered 
by head and then ranked by the frequency of the 
head. Klavans et al(2000) report on the 
evaluation of ?usefulness? of head clustering in 
the context of browsing applications, in terms of 
quality and coverage.  
  Other researchers have used noun-phrases 
quite successfully for information retrieval task  
[Strzalkowski et al(1999), Sparck-Jones 
(1999)]. Strzalkowski et al(1999) uses head + 
modifier pairs as part of a larger system 
which constitutes the ?stream model? that is 
used for information retrieval. They treat the 
head-modifier relationship as an ?ordered 
relation between otherwise equal elements?, 
emphasizing that for some tasks, the syntactic 
head of the NP is not necessarily a semantic 
head, and the modifier is not either 
necessarily a semantic modifier and that the 
opposite is often true. Using a machine 
learning approach, we proved this hypothesis 
for the task of gisting.  
 Berger and Mittal (2000) present a 
summarization system named OCELOT, 
based on probabilistic models, which 
provides the gist of web documents. Like 
email messages, web documents are also very 
heterogeneous and their unstructured nature 
pose equal difficulties.  
In this paper, we propose a novel 
technique for summarization that combines 
the linguistic approach of extracting simple 
noun phrases as possible candidates for 
document extracts, and the use of machine 
learning algorithms to automatically select 
the most salient ones. 
2 System architecture 
The input to GIST-IT is a single email 
message. The architecture, presented in 
Figure 1 consists of four distinct functional 
components.  The first module is an email 
preprocessor developed for Text-To-Speech 
 
HPDLO 
PHVVDJH 
(  0DLO 3UHS 
7RNHQL]DWLRQ 
6LPSOH 13  
([WUDFWLRQ  
13 ILOWHULQJ 
13 ([WUDFWLRQ DQG )LOWHULQJ 8QLW 
)HDWXUH  
VHOHFWLRQ 
)HDWXUH  
VHOHFWLRQ 
&ODVVLILFDWLRQ  
0RGHO 
13  
FODVVLILFDWLRQ 
JLVW RI HPDLO  
PHVVDJH 
SUHVHQWDWLRQ 
0/ 8QLW 
 
Figure 1 System Architecture 
applications. The second component is a shallow 
text processing unit, which is actually a pipeline 
of modules for extraction and filtering of simple 
NP candidates.  The third functional component 
is a machine learning unit, which consists of a 
feature selection module and a text classifier. 
This module uses a training set and a testing set 
that were devided from our email corpus.  In 
order to test the performance of GIST-IT on the 
task of summarization, we use a heterogeneous 
collection of email messages in genre, length, 
and topic.  We represent each email as a set of 
NP feature vectors.  We used 2,500 NPs 
extracted from 51 email messages as a training 
set and 324 NPs from 8 messages for testing. 
Each NP was manually tagged for saliency by 
one of the authors and we are planning to add 
more judges in the future. The final module 
deals with presentation of the gisted email 
message. 
2.1 The Email Preprocessor 
This module uses finite-state transducer 
technology in order to identify message content.  
Information at the top of the message related to 
?From/To/Date'' as well as the signature block 
are separated from the message content. 
2.2 Candidate Simple Noun Phrase Extraction and 
Filtering Unit 
This module performs shallow text processing 
for extraction and filtering of simple NP 
candidates, consisting of a pipeline of three 
modules: text tokenization, NP extraction, and 
NP filtering. Since the tool was created to 
preprocess email for speech output, some of the 
text tokenization suitable for speech is not 
accurate for text processing and some 
modifications needed to be implemented (e.g. 
email preprocessor splits acronyms like DLI2 
into DLI 2).  The noun phrase extraction module 
uses Brill's POS tagger [Brill (1992)]and a base 
NP chunker [Ramshaw and Marcus (1995)]. 
After analyzing some of these errors, we 
augmented the tagger lexicon from our training 
data and we added lexical and contextual rules 
to deal mainly with incorrect tagging of gerund 
endings. In order to improve the accuracy of 
classifiers we perform linguistic filtering, as 
discussed in detail in Section 3.1.2. 
2.3 Machine Learning Unit 
The first component of the ML unit is the 
feature selection module to compute NP 
vectors.  In the training phase, a model for 
identifying salient simple NPs is created.  
The training data consist of a list of feature 
vectors already classified as salient/non-
salient by the user.  Thus we rely on user-
relevance judgments to train the ML unit. In 
the extraction phase this unit will classify 
relevant NPs using the model generated 
during training.  We applied three machine 
learning paradigms (decision trees, rule 
induction algorithms, and decision forest) 
evaluating three different classifiers.  
2.4 Presentation 
The presentation of the message gist is a 
complex user interface issue with its 
independent set of problems.   Depending on 
the application and its use, one can think of 
different presentation techniques.  The gist of 
the message could be the set of NPs or the set 
of sentences in which these NPs occur so that 
the added context would make it more 
understandable to the user. We do not address 
in this work the disfluency that could occur in 
listing a set of extracted sentences, since the 
aim is to deliver to the user the very content 
of the message even in a raw fashion.   GIST-
IT is to be used in an application where the 
output is synthesized speech.   The focus of 
this paper is on extracting content with GIST-
IT, although presentation is a topic for future 
research.  
3 Combining Linguistic Knowledge and 
Machine Learning for Email Gisting 
We combine symbolic machine learning and 
linguistic processing in order to extract the 
salient phrases of a document.   Out of the 
large syntactic constituents of a sentence, e.g. 
noun phrases, verb phrases, and prepositional 
phrases, we assume that noun phrases (NPs) 
carry the most contentful information about 
the document, even if sometimes the verbs 
are important too, as reported in the work by 
[Klavans and Kan (1998)]. The problem is 
that no matter the size of a document, the 
number of informative noun phrases is very 
small comparing with the number of all noun 
phrases, making selection a necessity. Indeed, in 
the context of gisting, generating and presenting 
the list of all noun phrases, even with adequate 
linguistic filtering, may be overwhelming. Thus, 
we define the extraction of important noun 
phrases as a classification task, applying 
machine learning techniques to determine which 
features associated with the candidate NPs 
classify them as salient vs. non-salient.  We 
represent the document -- in this case an email 
message -- as a set of candidate NPs, each of 
them associated with a feature vector used in the 
classification model.  We use a number of 
linguistic methods both in the extraction and in 
the filtering of candidate noun phrases, and in 
the selection of the features.  
   
3.1 Candidate NPs 
Noun phrases were extracted using Ramshaw 
and Marcus's base NP chunker [Ramshaw and 
Marcus (1995)].  The base NP is either a simple 
NP as defined by Wacholder (1998) or a 
conjunction of two simple NPs.  Since the 
feature vectors used in the classifier scheme are 
simple NPs we used different heuristics to 
automatically split the conjoined NPs (CNP) 
into simple ones (SNP), properly assigning the 
premodifiers. Table 1 presents such an example: 
 
CNP: physics/NN and/CC biology/NN skilled/JJ  
researchers/NNS 
SNP1:  physics/NN skilled/JJ researchers/NNS 
SNP2: biology/NN skilled/JJ researchers/NNS 
Table 1 Splitting Complex NPs into Simple NPs  
3.1.2 Filtering simple NPs   
Since not all simple noun phrases are equally 
important to reflect the document meaning, we 
use well-defined linguistic properties to extract 
only those NPs (or parts of NPs) that have a 
greater chance to render the salient information. 
By introducing this level of linguistic filtering 
before applying the learning scheme, we 
improve the accuracy of the classifiers, thus 
obtaining better results (see discussion in 
sections 4.1.3 and 5.3). We performed four 
filtering steps: 
1. Inflectional morphological processing. 
English nouns have only two kinds of inflection: 
an affix that marks plural and an affix that 
marks possessive.
 
2. Removing unimportant modifiers. In this 
second step we remove the determiners that 
accompany the nouns and also the auxiliary 
words most and more that form the 
periphrastic forms of comparative and 
superlative adjectives modifying the nouns.
 
3. Remove common words. We used a list of 
571 common words used in IR systems in 
order to further filter the list of candidate 
NPs. Thus, words like even, following, every, 
are eliminated from the noun phrase 
structure. (i.e. ?even more detailed 
information? and ?detailed information? will 
also be grouped together). 
 
4. Remove ?empty? nouns. Words like lot, 
group, set, bunch are considered ?empty? 
nouns in the sense that they have no 
contribution to the noun phrase meaning. For 
example the meaning of the noun phrases like 
?group of students?,  ?lots of students? or 
?bunch of students? is given by the noun 
?students?. In order not to bias the extraction 
of empty nouns we used three different data 
collections: Brown corpus, Wall Street 
Journal, and a set of 4000 email messages 
(most of which were collected during a 
conference organization). Our algorithm was 
a simple one: we extracted all the nouns that 
appear in front of the preposition ?of? and 
then sorted them by frequency of appearance 
in all three corpora and used a threshold to 
select the final list. We generated a set of 141 
empty nouns that we used in this forth step of 
filtering process.   
3.2 Feature Selection 
We select a set of nine features that fall into 
three categories: linguistic, statistical 
(frequency-based) and positional. These 
features capture information about the 
relative importance of NPs to the document 
meaning.  
Several studies rely on linguistic intuition 
that the head of the noun phrase makes a 
greater contribution to the semantics of the 
nominal group than the modifiers. For some 
NLP tasks, the head is not necessarily the 
most important item of the noun phrase.  In 
analyzing email messages from the 
perspective of finding salient NPs, we claim 
that the constituents of the NP have often as 
much semantic content as the head.  This 
opinion is also supported in the work of 
[Strzalkowski et al(1999)]. In many cases, the 
meaning of the NP is given equally by 
modifier(s) -- usually nominal modifiers(s) -- 
and head.  Consider the following list of simple 
NPs selected as candidates: 
(1) ?conference workshop announcement? 
(2) ?international conference? 
(3) ?workshop description? 
(4) ?conference deadline? 
In the case of noun phrase (1) the importance of 
the noun phrase is found in the two noun 
modifiers: conference and   workshop as much 
as in the head announcement. We test this 
empirical observation by introducing as a 
separate feature in the feature vector, a new 
TF*IDF measure that counts for both the 
modifiers and the head of the noun phrase, thus 
seeing the NP as a sequence of equally weighted 
elements.  For the example above the new 
feature will be: 
TF*IDFconference + TF*IDFworkshop + TF*IDFannouncement 
We divided the set of features into three 
groups: one associated with the head of the noun 
phrase, one associated with the whole NP and 
one that represents the new TF*IDF measure 
discussed above.  Since we want to use this 
technique on other types of documents, all 
features are independent of the text type or 
genre.  For example, in the initial selection of 
our attributes we introduced as separate features 
the presence or the absence of NPs in the subject 
line of the email and in the headline of the body. 
Kilander (1996) pointed out that users estimate 
that ?subject lines can be useful, but also 
devastating if their importance is overly 
emphasized?.  Based on this study and also on 
our goal to provide a method that is domain and 
genre independent we decided not to consider 
the subject line and the headlines as separate 
features, but rather as weights included in the 
TF*IDF measures as presented below.  Another 
motivation for this decision is that in email 
processing the correct identification of headlines 
is not always clear. 
3.2.1 Features associated with the Head 
We choose two features to characterize the head 
of the noun phrases: 
head_tfidf ? the TF*IDF measure of the 
head of the candidate NP. 
head_focc - The first occurrence of the head 
in text (the numbers of words that precede the 
head divided by the total number of words in 
the document).  
3.2.2 Features associated with the whole 
NP 
We select six features that we consider 
relevant in association with the whole NP:  
np_tfidf ? the TF*IDF measure associated 
with the whole NP.  
np_focc - The first occurrence of the noun 
phrase in the document.  
np_length_words - Noun phrase length 
measured in number of words, normalized by 
dividing it with the total numbers of words in 
the candidate NPs list. 
np_length_chars - Noun phrase length 
measured in number of characters, 
normalized by dividing it with the total 
numbers of characters in the candidate NPs 
list. 
sent_pos - Position of the noun phrase in 
sentence: the number of words that precede 
the noun phrase, divided by the sentence 
length. For noun phrases in the subject line 
and headlines (which are usually short and 
will be affected by this measure), we consider 
the maximum length of sentence in document 
as the normalization factor.  
par_pos - Position of noun phrase in 
paragraph, same as sent_pos, but at the 
paragraph level. 
3.2.3 Feature that considers all constituents 
of the NP equally weighted 
m_htfidf - the new TF*IDF measure that 
take into consideration the importance of the 
modifiers.  
In computing the TF*IDF measures 
(head_tfidf, np_tfidf, m_tfidf), weights wi, 
were assigned to account for the presence in 
the subject line and/or headline.  
wi1 ? if the head appears both in the subject 
line and headline; 
wi2 ? if the head appears only in the subject 
line; 
wi3 ? if the head appears only in headlines 
 where wi1 > wi2 > wi3. 
These weights were manually chosen after 
a set of experiments, but we plan to use either 
a regression method or explore with genetic 
algorithms to automatically learn them. 
3.3 Three Paradigms of Supervised Machine 
Learning  
Symbolic machine learning is used in 
conjunction with many NLP applications 
(syntactic and semantic parsing, POS tagging, 
text categorization, word sense disambiguation).     
In this paper we compare three symbolic 
learning techniques applied to the task of salient 
NP extraction: decision tree, rule induction 
learning and decision forests.   
We tested the performance of an axis-parallel 
decision tree, C4.5 [Quinlan (1993)]; a rule 
learning system RIPPER [Cohen (1995)] and a 
decision forest classifier (DFC) [Ho (1998)]. 
RIPPER allows the user to specify the loss ratio, 
which indicates the ratio of the cost of a false 
positive to the cost of a false negative, thus 
allowing the trade off between precision and 
recall. This is crucial for our analysis since we 
deal with sparse data set (in a document the 
number of salient NPs is much smaller than the 
number of irrelevant NPs). Finally we tried to 
prove that a combination of classifiers might 
improve accuracy, increasing both precision and 
recall. The Decision Forest Classifier (DFC) 
uses an algorithm for systematically 
constructing decision trees by pseudo-randomly 
selecting subsets of components of feature 
vectors. It implements different splitting 
functions.  In the setting of our evaluation we 
tested the information gain ratio (similar to the 
one used by Quinlan in C4.5). An augmented 
feature vector (pairwise sums, differences, and 
products of features) was used for this classifier. 
4 Evaluation and Experimental Results 
Since there are many different summaries for 
each document, evaluating summaries is a 
difficult problem. Extracting the salient noun 
phrases is the first key step in the summarization 
method that we adopt in this paper. Thus, we 
focus on evaluating the performance of GIST-IT 
on this task, using three classification schemes 
and two different feature settings. 
4.1 Evaluation Scheme 
There are several questions that we address in 
this paper: 
4.1.1 What features or combination of 
features are important in determining the 
degree of salience of an NP?   
Following our assumption that each 
constituent of the noun phrase is equally 
meaningful, we evaluate the impact of adding 
m_htfidf
 (see section 3.2.3), as an additional 
feature in the feature vector.  This is shown in 
Table 2 in the different feature vectors fv1 
and fv2. 
 
fv1-  head_focc  head_tfidf np_focc np_tfidf   
        np_length_words  np_length_chars par_pos sent_pos 
fv2 - head_focc  head_tfidf  m_htfidf  np_focc np_tfidf  
         np_length_words np_length_chars par_pos sent_pos 
Table 2 Two feature settings to evaluate the 
impact of m_htfidf 
4.1.2 What classification scheme is more 
adequate to our task? 
We evaluate the performance of three 
different classifiers in the task of extracting 
salient noun phrases.  As measures of 
performance we use precision (p) and recall 
(r).  The evaluation was performed according 
to what degree the output of the classifiers 
corresponds to the user judgments.  
 
C4.5 Ripper     DFC  Feature 
vectors p  r p r p r 
fv1 73.3 78.6 83.6 71.4 80.3 83.5 
fv2 70 88.9 85.7 78.8 85.7 87.9 
Table 3 Evaluation of two feature vectors using 
three classifiers 
 
Table 3 shows our results that answer 
these two questions. The table rows represent 
the two feature vectors we are comparing, 
and the columns correspond to the three 
classifiers chosen for the evaluation.   
4.1.3 Is linguistic filtering an important step 
in extracting salient NPs? 
In the third evaluation we analyse the impact 
of linguistic filtering on the classifier?s 
performance. It turns out that results show 
major improvements, from 69.2% to 85.7% 
for precision of fv2, and from 56.25% to 
87.9% for recall of fv2.  For detailed results, 
see [Muresan et al (2001)]. 
4.1.4 After the filtering and classification, are 
noun phrases good candidates for representing 
the gist of an email message? 
In order to answer this question, we compare 
the output of GIST-IT on one email with the 
results of KEA system [Witten et al(1999)] that 
uses a 'bag-of-words' approach to key phrase 
extraction (see Table 4). 
 
module     
sort of batch 
WordNet data 
 accesses   
the WordNet     
lots of WordNet       
WordNet perl          
QueryData 
wn 
perl module 
extracting      
use this module 
extracting lots      
WordNet system  
www.cogsci.princeton.e
du 
Perl module wordne 
interface  
'wn' command line program   
simple easy perl interface      
included man page 
wordnet                 
wordnet.pm module 
wordnet system      
wordnet package 
query perl module     
command line  
wordnet relation    
wordnet data   
free software        
querydata        
Table 4 KEA (left)  vs GIST-IT output (right) 
5  Discussion of results 
The results shown indicate that best system 
performance reached 87.9% recall and 85.7% 
precision.  Although these results are very high, 
judging NP relevance is a complex and highly 
variable task.  In the future, we will extend the 
gold standard with more judges, more data, and 
thus a more precise standard for measurement. 
5.1 The right selection of features 
Feature selection has a decisive impact on 
overall performance. As seen in Table 2, fv2 has 
m_htfidf
 as an additional feature, and its 
performance shown in Table 3 is superior to fv1; 
the DFC classifier shows an increase both in 
precision and recall. These results support the 
original hypothesis that in the context of gisting, 
the syntactic head of the noun phrase is not 
always the semantic head, and modifiers can 
also have an important role.  
5.2 Different classification models 
The effectiveness of different classification 
schemes in the context of our task is discussed 
here. As shown in Table 3, C4.5 performs well 
especially in terms of recall. RIPPER, as 
discussed in [Cohen (1995)], is more appropriate 
for noisy and sparse data collection than 
C4.5, showing an improvement in precision. 
Finally, DFC which is a combination of 
classifiers, shows  improved performance. 
The classifier was run with an augumented 
feature vector that included pairwise sums, 
differences and products of the features.  
5.3 Impact of linguistic knowledge 
As shown in previous section, DFC 
performed best in our task, so we chose only 
this classifier to present the impact of 
linguistic knowledge. Linguistic filtering 
improved precision and recall, having an 
important role especially on fv2, where the 
new feature m_tfidf was used. This is 
explained by the fact that the filtering 
presented in section 3.1.2 removed the noise 
introduced by unimportant modifiers, 
common and empty nouns, thus giving this 
new feature a larger impact.   
5.4 Noun phrases are better than n-grams   
Presenting the gist of an email message by 
phrase extraction addresses one obvious 
question: can any phrasal extract represent 
the content of a document, or must a well 
defined linguistic phrasal structure be used? 
To answer this question we compare the 
results of our system that extract 
linguistically principled phrasal units, with 
KEA output, that extracts bigrams and 
trigrams as key phrases [Witten et al(1999)].   
Table 4 shows the results of the KEA system. 
Due to the n-gram approach, KEA output 
contains phrases like sort of batch, extracting 
lots, wn, and even urls that are unlikely to 
represent the gist of a document. 
 
Conclusion and future work 
In this paper we presented a novel technique 
for document gisting suitable for domain and 
genre independent collections such as email 
messages.  The method extracts simple noun 
phrases using linguistic techniques and then 
use machine learning to classify them as 
salient for the document content.  We 
evaluated the system in different 
experimental settings using three 
classification models. In analyzing the 
structure of NPs, we demonstrated that the 
modifiers of a noun phrase can be 
semantically as important as the head for the 
task of gisting. GIST-IT is fully implemented, 
evaluated, and embedded in an application, 
which allows user to access a set of information 
including email, finances, etc.  
  We plan to extend our work by taking 
advantage of structured email, by classifying 
messages into folders, and then by applying 
information extraction techniques.  Since NPs 
and machine learning techniques are domain and 
genre independent, we plan to test GIST-IT on 
different data collections (e.g. web pages), and 
for other knowledge management tasks, such as   
document indexing or query refinement. 
Additionally, we plan to test the significance of 
the output for the user, i.e. whether the system 
provide informative content and adequate gist of 
the message. 
References  
Berger, A.L and Mittal, V.O (2000). OCELOT:A system for 
summarizing web pages. In Proceedings of the 23rd 
Annual International ACM SIGIR, Athens, Greece, pp 
144-151.  
Brill, E. (1992).  A Simple Rule-based Part of Speech 
Tagger. In Proceedings of the Third Conference on 
ANLP. Trento, Italy; 1992 
Boguraev, B. and Kennedy, C. (1999). Salience-based 
content characterisation of text documents. In I. Mani 
and T. Maybury, M., editors, Advances in Automatic 
Text Summarization, pp 99-111. The MIT Press.  
Cohen, W. (1995). Fast Effective Rule Induction. Machine-
Learning: Proceedings of the Twelfth International 
Conference.  
Ho, T.K (1998). The random subspace method for 
constructing decision forests. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 20(8). 
Hovy, E.H (2000). Automated Text Summarization. In R. 
Mitkov, editor,  Oxford University Handbook of 
Computational Linguistics. Oxford Univ. Press. 
Kilander, F. (1996). Properties of electronic texts for 
classification purposes as suggested by users. 
Klavans, J.L., Wacholder, N. and Evans, D.K. (2000) 
Evaluation of computational linguistic techniques for 
identifying significant topics for browsing applications. 
In Proceedings (LREC-2000), Athens. Greece. 
Klavans, J.L. and Kan, M-Y. (1998).Role of verbs in 
document analysis. In proceedings of COLING/ACL  98. 
Kupiec, J., Pedersen, J. and Chen, F. (1995). A trainable 
document summarizer. In Proceedings of the 18th 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp 
68-73, Seattle, WA. 
McKeown, K.R,  Klavans, J.L, Hatzivassiloglou, V.,  
Barzilay, R. and Eskin, E. (1999). Towards 
multidocument summarization by reformulation: 
Progress and prospects. In Proceedings of AAAI'99. 
McKeown, K.R and Radev, D.R (1995). Generating 
summaries of multiple news articles. In Proceedings 
of the 18th Annual International ACM SIGIR 
Conference on Research and Development in 
Information Retrieval, pp 74-82, Seattle, WA. 
Muresan, S., Tzoukermann, E. and Klavans, J.L. 
(2001). Email Summarization Using Linguistic and 
Machine Learning Techniques. In Proceedings of 
CoNLL 2001 ACL Workshop, Toulouse, France. 
Murthy, S.K., Kasif, S., Salzberg, S. and Beigel, R. 
(1993). OC1: Randomized Induction of Oblique 
Decision Trees. Proceedings of the Eleventh National 
Conference on Artificial Intelligence, pp. 322--327, 
Washington, D.C. 
Quinlan, J.R (1993). C4.5: Program for Machine 
Learning. Morgan Kaufmann. 
Ramshaw, L.A. and Marcus, M.P. (1995). Text 
Chunking Using Transformation-Based Learning. In 
Proceedings of Third ACL Workshop on Very Large 
Corpora, MIT. 
Sparck-Jones, K. (1999). What Is The Role of NLP in 
Text Retrieval. In T. Strzalkowski, editor, Natural 
Language Information Retrieval. Kluwer, Boston, 
MA.    
Strzalkowski, T., Lin, F., Wang, J., and Perez-Carballo, 
J. (1999). Evaluating natural language  processing 
techniques for information retrieval. In T. 
Strzalkowski, editor, Natural Language Information 
Retrieval. Kluwer, Boston, MA.    
Turney, P.D. (2000). Learning algorithms for 
keyphrase exraction. Information Retrieval, 2(4): pp 
303-336. 
Ueda, Y., Oka M., Koyama T. and Miyauchi T (2000). 
Toward the "at-a-glance" summary: Phrase-
representation summarization method. In 
Proceedings of COLING 2000.   
Wacholder, N. (1998). Simplex NPS sorted by head: a 
method for identifying significant topics within a 
document, In Proceedings of the COLING-ACL 
Workshop on the Computational Treatment of 
Nominals. 
Whittaker, S. and Sidner, C. Email overload: Exploring 
personal information management of email. In 
Proceedings of CHI?96. p. 276-283. NY:ACM Press 
Witten, I.H, Paynter, G.W., Frank E., Gutwin C. and 
Nevill-Manning, C.G (1999). KEA: Practical 
automatic keyphrase extraction. In Proceedings of 
DL'99, pp 254-256. 
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 25?32,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Concept Disambiguation for Improved Subject Access  
Using Multiple Knowledge Sources 
 
Tandeep Sidhu, Judith Klavans, and Jimmy Lin 
College of Information Studies 
University of Maryland 
College Park, MD 20742 
tsidhu@umiacs.umd.edu, {jklavans, jimmylin}@umd.edu 
 
 
Abstract 
 
We address the problem of mining text for 
relevant image metadata.  Our work is situ-
ated in the art and architecture domain, 
where highly specialized technical vocabu-
lary presents challenges for NLP tech-
niques.  To extract high quality metadata, 
the problem of word sense disambiguation 
must be addressed in order to avoid leading 
the searcher to the wrong image as a result 
of ambiguous ? and thus faulty ? meta-
data.  In this paper, we present a disam-
biguation algorithm that attempts to select 
the correct sense of nouns in textual de-
scriptions of art objects, with respect to a 
rich domain-specific thesaurus, the Art and 
Architecture Thesaurus (AAT).  We per-
formed a series of intrinsic evaluations us-
ing a data set of 600 subject terms ex-
tracted from an online National Gallery of 
Art (NGA) collection of images and text.  
Our results showed that the use of external 
knowledge sources shows an improvement 
over a baseline. 
      
1. Introduction 
We describe an algorithm that takes noun phrases 
and assigns a sense to the head noun or phrase, 
given a large domain-specific thesaurus, the Art 
and Architecture Thesaurus1 (published by the 
Getty Research Institute).  This research is part of 
the Computational Linguistics for Metadata 
                                                                 
1http://www.getty.edu/research/conducting_research/vocabul
aries/aat/ 
Building (CLiMB) project (Klavans 2006, Kla-
vans in preparation), which aims to improve im-
age access by automatically extracting metadata 
from text associated with images.  We present 
here a component of an overall architecture that 
automatically mines scholarly text for metadata 
terms.  In order to filter and associate a term with 
a related concept, ambiguous terms must be clari-
fied.  The disambiguation of terms is a basic chal-
lenge in computational linguistics (Ide and Vero-
nis 1990, Agirre and Edmonds 2006). 
As more non-specialists in digital libraries 
search for images, the need for subject term ac-
cess has increased.  Subject terms enrich catalog 
records with valuable broad-reaching metadata 
and help improve image access (Layne 1994).  
Image seekers will receive more relevant results 
if image records contain terms that reflect con-
ceptual, semantic, and ontological relationships.  
Furthermore, subject terms associated with hier-
archical and faceted thesaural senses promise to 
further improve precision in image access.  Such 
terms map to standardized thesaurus records that 
include the term?s preferred, variant, and related 
names, including both broader and specific con-
cepts, and other related concepts.  This informa-
tion can then be filtered, linked, and subsequently 
tested for usefulness in performing richer image 
access.  As with other research on disambigua-
tion, our hypothesis is that accurate assignment of 
senses to metadata index terms will results in 
higher precision for searchers.  This hypothesis 
will be fully tested as we incorporate the disam-
biguation module in our end-to-end CLiMB 
Toolkit, and as we perform user studies. 
Finding subject terms and mapping them to a 
thesaurus is a time-intensive task for catalogers 
25
(Rasmussen 1997, Ferguson and Intner 1998).  
Doing so typically involves reading image-related 
text or other sources to find subject terms.  Even 
so, the lack of standard vocabulary in extensive 
subject indexing means that the enriched number 
of subject terms could be inadvertently offset by 
the vocabulary naming problem (Baca 2002).   
This paper reports on our results using the 
subject terms in the AAT; the CLiMB project is 
also using the Thesaurus of Geographic Names 
(TGN) and the Union List of Artist Names 
(ULAN).  Since the focus of this paper is on dis-
ambiguation of common nouns rather than proper 
nouns, the AAT is our primary resource. 
2. Resources 
2.1 Art and Architecture Thesaurus (AAT)  
The AAT is a widely-used multi-faceted thesau-
rus of terms for the cataloging and indexing of 
art, architecture, artifactual, and archival materi-
als. Since the AAT offers a controlled vocabulary 
for recording and retrieval of data in object, bib-
liographic, and visual databases, it is of interest to 
a wide community. 
In the AAT, each concept is described 
through a record which has a unique ID, preferred 
name, record description, variant names, broader, 
narrower, and related terms.  In total, AAT has 
31,000 such records.  For the purpose of this arti-
cle, a record can be viewed as synonymous with 
sense. Within the AAT, there are 1,400 homo-
nyms, i.e., records with same preferred name.  
For example, the term wings has five senses in 
the AAT (see Figure 1 below).   
Wings (5 senses): 
? Sense#1: Used for accessories that project outward 
from the shoulder of a garment and are made of cloth 
or metal.   
? Sense#2: Lateral parts or appendages of a work of 
art, such as those found on a triptych.  
? Sense#3: The areas offstage and to the side of the 
acting area. 
? Sense#4: The two forward extensions to the sides of 
the back on an easy chair.  
? Sense#5: Subsidiary parts of buildings extending out 
from the main portion. 
Figure 1:  Selection of AAT records for term ?wings? 
Table 1 shows the breakdown of the AAT vo-
cabulary by number of senses with a sample lexi-
cal item for each frequency. 
# of 
Senses 
# of  
Homonyms 
Example 
2 1097 bells 
3 215 painting 
4 50 alabaster 
5 39 wings 
6 9 boards 
7 5 amber 
8 2 emerald 
9 1 plum 
10 1 emerald green 
11 1 magenta 
12 1 ocher 
13 1 carmine 
14 2 slate 
Table 1:  Scope of the disambiguation problem in AAT 
Note that there are potentially three tasks that 
could be addressed with our algorithm: (i) map-
ping a term to the correct sense in the AAT, (ii) 
selecting amongst closely related terms in the 
AAT, and (iii) mapping synonyms onto a single 
AAT entry.  In this paper, our primary focus is on 
task (i); we handle task (ii) with a simple ranking 
approach; we do not address task (iii).  
Table 1 shows that multiple senses per term 
makes mapping subject terms to AAT very chal-
lenging.  Manual disambiguation would be slow, 
tedious, and unrealistic.  Thus we explore auto-
matic methods since, in order to identify the cor-
rect sense of a term in running text, each of these 
senses needs to be viewed in context. 
2.2 The Test Collection 
The data set of terms that we use for evaluation 
comes from the National Gallery of Art (NGA) 
online archive2.  This collection covers paintings, 
sculpture, decorative arts, and works from the 
Middle Ages to the present.  We randomly se-
lected 20 images with corresponding text from 
this collection and extracted noun phrases to form 
the data set.  The data set was divided into two 
categories: the training set and the test set.  The 
training set consisted of 326 terms and was used 
                                                                 
2 http://www.nga.gov/home.htm 
26
to develop the algorithm.  The test set consisted 
of 275 terms and was used to evaluate. 
Following standard procedure in word sense 
disambiguation tasks (Palmer et al 2006), 
groundtruth for the data set was created manually 
by two labelers (referred to as Labeler 1 and La-
beler 2 in Section 4 below).  These labelers were 
part of the larger CLiMB project but they were 
not involved in the development of the disam-
biguation algorithm.  The process of creating the 
groundtruth involved picking the correct AAT 
record for each of the terms in the data set.  
Terms not appearing in the AAT (as determined 
by the labelers) were given an AAT record value 
of zero.  Each labeler worked independently on 
this task and had access to the online version of 
the AAT and the text where each term appeared. 
Interannotator agreement for the task was encour-
agingly high, at 85% providing a notional upper 
bound for automatic system performance (Gale et 
al.  1992).  
Not all terms in this dataset required disam-
biguation; 128 terms (out of 326) under the train-
ing set and 96 terms (out of 275) under the test 
set required disambiguation, since they matched 
more than one AAT record.  The dataset we se-
lected was adequate to test our different ap-
proaches and to refine our techniques.  We intend 
to run over more data as we collect and annotate 
more resources for evaluation. 
2.3 SenseRelate AllWords3 and WordNet4 
SenseRelate AllWords (Banerjee and Pederson 
2003, Patwardhan et al 2003) is a Perl program 
that our algorithm employs to perform basic dis-
ambiguation of words. We have adapted Sen-
seRelate for the purpose of disambiguating AAT 
senses.  
Given a sentence, SenseRelate AllWords dis-
ambiguates all the words in that sentence.  It uses 
word sense definitions from WordNet (in this 
case WordNet 2.1), a large lexical database of 
English nouns, verbs, adjectives, and adverbs.  As 
an example, consider the text below: 
                                                                 
3 http://sourceforge.net/projects/senserelate 
4 http://wordnet.princeton.edu/ 
With more than fifty individual scenes, the al-
tarpiece was about fourteen feet wide. 
 
The SenseRelate result is: 
With more#a#2 than fifty#n#1 individual#n#1 
scene#n#10 the altarpiece#n#1 be#v#1 about#r#1 
fourteen#n#1 foot#n#2 wide#a#1 
 
In the above example, more#a#2 means SenseRe-
late labeled more as an adjective and mapped it to 
second meaning of more (found in WordNet). 
fifty#n#1 means SenseRelate labeled fifty as a 
noun and mapped it to first meaning of fifty 
(found in WordNet).  Note, that fifty#n#1 maps to 
a sense in WordNet, whereas in our algorithm it 
needs to map to an AAT sense.  In Section 3, we 
show how we translate a WordNet sense to an 
AAT sense for use in our algorithm. 
To perform disambiguation, SenseRelate re-
quires that certain parameters be set:  (1) the 
number of words around the target word (also 
known as the context window), and  (2) the simi-
larity measure.  We used a value of 20 for the 
context window, which means that SenseRelate 
will use 10 words to the left and 10 words to the 
right of the target word to determine the correct 
sense.  We used lesk as the similarity measure in 
our algorithm which is based on Lesk (1986).  
This decision was based on several experiments 
we did with various context window sizes and 
various similarity measures on a data set of 60 
terms.   
27
3. Methodology 
3.1 Disambiguation Algorithm  
 
Figure 2:  Disambiguation Algorithm 
Figure 2 above shows that first we identify the 
noun phrases from the input document.  Then we 
disambiguate each noun phrase independently by 
first looking it up in the AAT.  If a record is 
found, we move on to the next step; otherwise we 
look up the head noun (as the noun phrase) in the 
AAT.  
Second, we filter out any AAT records where 
the noun phrase (or the head noun) is used as an 
adjective (for a term like painting this would be 
painting techniques, painting knives, painting 
equipment, etc). Third, if zero records are found 
in the AAT, we label the term as ?not found in 
AAT.?  If only one matching record is found, we 
label the term with the ID of this record.  Fourth, 
if more than one record is found, we use the dis-
ambiguation techniques outlined in the next sec-
tion to find the correct record.  
3.2 Techniques for Disambiguation 
For each of the terms, the following techniques 
were applied in the order they are given in this 
section. If a technique failed to disambiguate a 
term, we applied the next technique. If none of 
these techniques was able to disambiguate, we 
selected the first AAT record as the correct re-
cord.  Findings for each technique are provided in 
the Results section below. 
First, we used all modifiers that are in the 
noun phrase to find the correct AAT record.  We 
searched for the modifiers in the record descrip-
tion, variant names, and the parent hierarchy 
names of all the matching AAT senses.  If this 
technique narrowed down the option set to one 
record, then we found our correct record.  For 
example, consider the term ceiling coffers.  For 
this term we found two records: coffers (coffered 
ceiling components) and coffers (chests).  The 
first record has the modifier ceiling in its record 
description, so we were able to determine that 
this was the correct record. 
Second, we used SenseRelate AllWords and 
WordNet.  This gave us the WordNet sense of our 
noun phrase (or its head noun).  Using that sense 
definition from WordNet, we next examined 
which of the AAT senses best matches with the 
WordNet sense definition.  For this, we used the 
word overlapping technique where we awarded a 
score of N to an AAT record where N words 
overlap with the sense that SenseRelate picked.  
The AAT record with the highest score was se-
lected as the correct record.  If none of the AAT 
records received any positive score (above a cer-
tain threshold), then it was decided that this tech-
nique could not find the one correct match.  
As an example, consider finding the correct 
sense for the single word noun bells using Sen-
seRelate: 
1. Given the input sentence: 
?? city officials, and citizens were followed by 
women and children ringing bells for joy.? 
2. Search for AAT records.  There are two records 
for the bells in AAT: 
a. bells: ?Flared or bulbous terminals found on 
many open-ended aerophone tubes?. 
b. bells: ?Percussion vessels consisting of a hollow 
object, usually of metal but in some cultures of 
hard clay, wood, or glass, which when struck emits 
a sound by the vibration of most of its mass;?? 
3. Submit the input sentence to SenseRelate, which 
provides a best guess for the corresponding 
WordNet senses for each word. 
4. Get SenseRelate output, which indicates that the 
WordNet definition for bells is WordNet-Sense1, 
i.e., ?a hollow device made of metal that makes a 
ringing sound when struck? 
28
SenseRelate output: 
city#n#1 official#n#1 and citizen#n#1 be#v#1 
follow#v#20 by#r#1 woman#n#1 and child#n#1 
ringing#a#1 bell#n#1 for joy#n#1 
5. Find the correct AAT match using word overlap of 
the WordNet definition and the two AAT defini-
tions for bells: 
 
WordNet:  ?a hollow device made of metal that 
makes a ringing sound when struck? 
compared with: 
AAT: ?Flared or bulbous terminals found on many 
open-ended aerophone tubes? 
and compared with: 
AAT:  ?Percussion vessels consisting of a hollow 
object, usually of metal but in some cultures of 
hard clay, wood, or glass, which when struck 
emits a sound by the vibration of most of its 
mass;?? 
  
6. The second AAT sense is the correct sense accord-
ing to the word overlap (see Table 2 below): 
 
Comparison Score Word Overlap 
AAT ? Definition 1 and 
WordNet Sense1 
0 None 
AAT ? Definition 2 and 
WordNet Sense1 
4 hollow, metal, 
sound, struck 
Table 2: Word Overlap to Select AAT Definition 
Notice that we only used the AAT record descrip-
tion for performing the word overlap.  We ex-
perimented by including other information pre-
sent in the AAT record (like variant names, par-
ent AAT record names) also, but simply using the 
record description yielded the best results.   
Third, we used AAT record names (preferred 
and variant) to find the one correct match.  If one 
of the record names matched better than the other 
record names to the noun phrase name, that re-
cord was deemed to be the correct record.  For 
example, the term altar more appropriately 
matches altars (religious building fixtures) than 
altarpieces (religious visual works).  Another 
example is children, which better matches chil-
dren (youth) than offspring (people by family re-
lationship).   
Fourth, if none of the above techniques 
succeeded in selecting one record, we used the 
most common sense definition for a term (taken 
from WordNet) in conjunction with the AAT re-
sults and word overlapping mentioned above to 
find the one correct record.  
4. Results and Evaluation 
4.1 Methodologies 
We used three different evaluation methods to 
assess the performance of our algorithm.  The 
first evaluation method computes whether our 
algorithm picked the correct AAT record (i.e., the 
AAT sense picked is in agreement with the 
groundtruth).  The second method computes 
whether the correct record is among the top three 
records picked by our algorithm.  In Table 3 be-
low, this is referred to as Top3.  The third evalua-
tion method computes whether the correct record 
is in top five records picked by our algorithm, 
Top5.  The last two evaluations helped us deter-
mine the usability of our algorithm in situations 
where it does not pick the correct record but it 
still narrows down to top three or top five results.  
We ranked the AAT records according to 
their preferred name for the baseline, given the 
absence of any other disambiguation algorithm. 
Thus, AAT records that exactly matched the term 
in question appear on top, followed by records 
that partially matched the term.  For example, for 
term feet, the top three records were feet (terminal 
elements of objects), French feet (bracket feet), 
and Spanish feet (furniture components).  For the 
noun wings, the top three records were wings 
(shoulder accessories), wings (visual works com-
ponents), and wings (backstage spaces). 
4.2 Overall Results 
In this section, we present evaluation results for 
all the terms.  In the next section, we present re-
sults for only those terms that required disam-
biguation. 
Overall results for the training set (326 terms) 
are shown in Table 3. This table shows that over-
all accuracy of our algorithm is 76% and 68% for 
Labeler 1 and Labeler 2, respectively.  The base-
line accuracy is 69% for Labeler 1 and 62% for 
Labeler 2. The other two evaluations show much 
better results.  The Top 3 and Top5 evaluations 
have accuracy of 84% and 88% for Labeler 1 and 
accuracy of 78% and 79% for Labeler 2. This 
argues for bringing in additional techniques to 
29
enhance the SenseRelate approach in order to 
select from Top3 or Top5. 
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 76% 68% 
Baseline Accuracy 69% 62% 
Top3 84% 78% 
Top5 88% 79% 
Table 3: Results for Training Set (n=326 terms) 
In contrast to Table 3 for the training set, Table 4 
shows results for the test set.  Labeler 1 shows an 
accuracy of 74% on the algorithm and 72% on the 
baseline; Labeler 2 has an accuracy of 73% on 
the algorithm and 69% on the baseline.  
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 74% 73% 
Baseline Accuracy 72% 69% 
Top3 79% 79% 
Top5 81% 80% 
Table 4: Results for Test Set (n=275 terms) 
4.3 Results for Ambiguous Terms 
This section shows the results for the terms from 
the training set and the test set that required dis-
ambiguation.  Table 5 below shows that our algo-
rithm?s accuracy for Labeler 1 is 55% compared 
to the baseline accuracy of 35%. For Labeler 2, 
the algorithm accuracy is 48% compared to base-
line accuracy of 32%. This is significantly less 
than the overall accuracy of our algorithm.  Top3 
and Top5 evaluations have accuracy of 71% and 
82% for Labeler 1 and 71% and 75% for Labeler 
2.  
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 55% 48% 
Baseline Accuracy 35% 32% 
Top3 71% 71% 
Top5 82% 75% 
Table 5: Ambiguous Terms for Training (n=128 terms) 
Similar results can be seen for the test set (96 
terms) in Table 6 below.  Labeler 1 shows an ac-
curacy of 50% on the algorithm and 42% on the 
baseline; Labeler 2 has an accuracy of 53% on 
the algorithm and 39% on the baseline.   
Evaluation Labeler 1 Labeler 2 
Algorithm Accuracy 50% 53% 
Baseline Accuracy 42% 39% 
Top3 63% 68% 
Top5 68% 71% 
Table 6: Results for Ambiguous Terms  
under the Test Set (n=96 terms) 
4.4 Analysis 
Table 7 shows that SenseRelate is used for most 
of the AAT mappings, and provides a breakdown 
based upon the disambiguation technique used.   
Row One in Table 7 shows how few terms were 
disambiguated using the lookup modifier tech-
nique, just 1 in the training set and 3 in the test 
set. 
Row Technique Training 
Set(n=128) 
Test  Set 
(n=96) 
One Lookup  
Modifier 
1 3 
Two SenseRelate 108 63 
Three Best Record 
Match 
14 12 
Four Most Common 
Sense 
5 18 
Table 7: Breakdown of AAT mappings  
by Disambiguation Technique 
Rows Two and Three show that most of the terms 
were disambiguated using the SenseRelate tech-
nique followed by the Best Record Match tech-
nique. The Most Common Sense technique (Row 
Four) accounted for the rest of the labelings.  
Table 8 gives insight into the errors of our algo-
rithm for the training set terms: 
Technique Reason for Error Error 
Count 
SenseRelate picked wrong 
WordNet sense 
16 
WordNet does not have the 
sense 
8 
Definitions did not overlap 11 
SenseRelate 
Other reasons 10 
Best Record 
Match 
 10 
Lookup 
Modifier 
 0 
Most Com-
mon Sense 
 3 
Table 8: Breakdown of the errors in our algorithm  
under training set (58 total errors) 
Table 8 shows the following: 
(1) Out of the total of 58 errors, 16 errors were 
caused because SenseRelate picked the wrong 
WordNet sense.  
(2) 8 errors were caused because WordNet did 
not  contain the sense of the word in which it was 
30
being used.  For example, consider the term work-
shop.  WordNet has two definitions of workshop: 
i. ?small workplace where handcrafts or manufac-
turing are done? and 
ii. ?a brief intensive course for a small group; em-
phasizes problem solving? 
but AAT has an additional definition that was 
referred by term workshop in the NGA text: 
?In the context of visual and decorative arts, refers 
to groups of artists or craftsmen collaborating to 
produce works, usually under a master's name? 
(3) 11 errors occurred because the AAT record 
definition and the WordNet sense definition did 
not overlap.  Consider the term figures in the sen-
tence, ?As with The Holy Family, the style of the 
figures offers no clear distinguishing characteris-
tic.?  Then examine the AAT and WordNet sense 
definitions below for figures: 
AAT sense: ?Representations of humans or ani-
mals? 
WordNet sense: ?a model of a bodily form (espe-
cially of a person)? 
These definitions do not have any words in com-
mon, but they discuss the same concept. 
(4) 10 errors occurred in the Best Record Match 
technique, 0 errors occurred under the Lookup 
Modifier Technique, and 3 errors occurred under 
the Most Common Sense technique. 
5. Conclusion  
We have shown that it is possible to create an 
automated program to perform word sense dis-
ambiguation in a field with specialized vocabu-
lary.   Such an application could have great poten-
tial in rapid development of metadata for digital 
collections.   Still, much work must be done in 
order to integrate our disambiguation program 
into the CLiMB Toolkit, including the following: 
(1) Our algorithm?s disambiguation accuracy is 
between 48-55% (Table 5 and Table 6), and so 
there is room for improvement in the algorithm.  
Currently we depend on an external program 
(SenseRelate) to perform much of the disam-
biguation (Table 7).  Furthermore, SenseRelate 
maps terms to WordNet and we then map the 
WordNet sense to an AAT sense.  This extra step 
is overhead, and it causes errors in our algorithm.  
We can either explore the option of re-
implementing concepts behind SenseRelate to 
directly map terms to the AAT, or we may need 
to find additional approaches to employ hybrid 
techniques (including machine learning) for dis-
ambiguation.  At the same time, we may benefit 
from the fact that WordNet, as a general resource, 
is domain independent and thus offers wider cov-
erage.  We will need to explore the trade-off in 
precision between different configurations using 
these different resources. 
(2) We need more and better groundtruth.  Our 
current data set of noun phrases includes term 
like favor, kind, and certain aspects.  These terms 
are unlikely to be used as meaningful subject 
terms by a cataloger and will never be mapped to 
AAT.  Thus, we need to develop reliable heuris-
tics to determine which noun phrases are poten-
tially high value subject index terms.  A simple 
frequency count does not achieve this purpose.  
Currently we are evaluating based on ground-
truth that our project members created.  Instead, 
we would like to extend the study to a wider set 
of image catalogers as labelers, since they will be 
the primary users of the CLiMB tool.  Image 
catalogers have experience in finding subject 
terms and mapping subject terms to the AAT.  
They can also help determine which terms are 
high quality subject terms.   
In contrast to working with the highly experi-
enced image cataloger, we also want to extend the 
study to include various groups with different 
user needs.  For example, journalists have ongo-
ing needs for images, and they tend to search by 
subject.  Using participants like these for markup 
and evaluation promises to provide comparative 
results, ones which will enable us to effectively 
reach a broad audience. 
We also would like to test our algorithm on 
more collections.  This will help us ascertain 
what kind of improvements or additions would 
make CLiMB a more general tool. 
6. Acknowledgements 
We thank Rachel Wadsworth and Carolyn Shef-
field.  We also acknowledge Philip Resnik for 
valuable discussion. 
31
7. References 
Baca, Murtha, ed. 2002. Introduction to art image 
access: issues, tools, standards, strategies. Getty 
Research Institute. 
Banerjee, S., and T. Pedersen. 2003. Extended 
gloss overlaps as a measure of semantic relat-
edness. In Proceedings of the Eighteenth Inter-
national Joint Conference on ArtificialIntelli-
gence, 805?810. 
Ferguson, Bobby and Sheila Intner. 1998. Subject 
Analysis: Blitz Cataloging Workbook. West-
port, CT:Libraries Unlimited Inc.  
Gale, W. A., K. W. Church, and D. Yarowsky. 
1992. Using bilingual materials to develop 
word sense disambiguation methods. In Pro-
ceedings of the Fourth International Confer-
ence on Theoretical and Methodological Issues 
in Machine Translation, 101-112, Montreal, 
Canada. 
Ide, Nancy M. and Jean Veronis. 1990.  Mapping 
Dictionaries: A Spreading Activation Ap-
proach. In Proceedings of the 6th Annual Con-
ference of the UW Centre for the New OED and 
Text Research, 52-64 Waterloo, Ontario. 
Lesk, Michael. 1986. Automatic Sense Disam-
biguation Using Machine Readable Dictionar-
ies: How to Tell a Pine Cone from an Ice 
Cream Cone. In Proceedings of ACM SIGDOC 
Conference, 24-26, Toronto, Canada. 
Klavans, Judith L. 2006. Computational Linguis-
tics for Metadata Building (CLiMB). In Pro-
cedings of the OntoImage Workshop, G. Gref-
fenstette, ed.  Language Resources and Evalua-
tion Conference (LREC), Genova, Italy. 
Klavans, Judith L. (in preparation). Using Com-
putational Linguistic Techniques and Thesauri 
for Enhancing Metadata Records in Image 
Search:  The CLiMB Project.  
Layne, Sara Shatford. 1994. Some issues in the 
indexing of images. Journal of the American 
Society for Information Science, 583-588. 
Palmer, Martha, Hwee Tou Ng, & Hoa Trang 
Dang. 2006. Evaluation of WSD Systems. 
Word Sense Disambiguation: Algorithms and 
Applications. Eneko Agirre and Philip Ed-
monds, ed. 75-106. Dordrecht, The Nether-
lands:Springer. 
Patwardhan, S., S. Banerjee, S. and T. Pedersen. 
2003. Using measures of semantic relatedness 
for word sense disambiguation. Proceedings of 
the Fourth International Conference on Intelli-
gent Text Processing and Computational Lin-
guistics, 241?257. 
Rasmussen, Edie. M. 1997. Indexing images. An-
nual Review of Information Science and Tech-
nology (ARIST), 32, 169-196. 
32
