Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 890?895,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A New Set of Norms for Semantic Relatedness Measures
Sean Szumlanski
Department of EECS
University of Central Florida
seansz@cs.ucf.edu
Fernando Gomez
Department of EECS
University of Central Florida
gomez@eecs.ucf.edu
Valerie K. Sims
Department of Psychology
University of Central Florida
Valerie.Sims@ucf.edu
Abstract
We have elicited human quantitative judg-
ments of semantic relatedness for 122
pairs of nouns and compiled them into a
new set of relatedness norms that we call
Rel-122. Judgments from individual sub-
jects in our study exhibit high average cor-
relation to the resulting relatedness means
(r = 0.77, ? = 0.09, N = 73), although not
as high as Resnik?s (1995) upper bound
for expected average human correlation to
similarity means (r = 0.90). This suggests
that human perceptions of relatedness are
less strictly constrained than perceptions
of similarity and establishes a clearer ex-
pectation for what constitutes human-like
performance by a computational measure
of semantic relatedness.
We compare the results of several
WordNet-based similarity and relatedness
measures to our Rel-122 norms and
demonstrate the limitations of WordNet
for discovering general indications of
semantic relatedness. We also offer a cri-
tique of the field?s reliance upon similarity
norms to evaluate relatedness measures.
1 Introduction
Despite the well-established technical distinc-
tion between semantic similarity and relatedness
(Agirre et al, 2009; Budanitsky and Hirst, 2006;
Resnik, 1995), comparison to established similar-
ity norms from psychology remains part of the
standard evaluative procedure for assessing com-
putational measures of semantic relatedness. Be-
cause similarity is only one particular type of re-
latedness, comparison to similarity norms fails to
give a complete view of a relatedness measure?s
efficacy.
In keeping with Budanitsky and Hirst?s (2006)
observation that ?comparison with human judg-
ments is the ideal way to evaluate a measure of
similarity or relatedness,? we have undertaken the
creation of a new set of relatedness norms.
2 Background
The similarity norms of Rubenstein and Goode-
nough (1965; henceforth R&G) and Miller and
Charles (1991; henceforth M&C) have seen ubiq-
uitous use in evaluation of computational mea-
sures of semantic similarity and relatedness.
R&G established their similarity norms by pre-
senting subjects with 65 slips of paper, each of
which contained a pair of nouns. Subjects were
directed to read through all 65 noun pairs, then
sort the pairs ?according to amount of ?similarity
of meaning.?? Subjects then assigned similarity
scores to each pair on a scale of 0.0 (completely
dissimilar) to 4.0 (strongly synonymous).
The R&G results have proven to be highly repli-
cable. M&C repeated R&G?s study using a subset
of 30 of the original word pairs, and their resulting
similarity norms correlated to the R&G norms at
r = 0.97. Resnik?s (1995) subsequent replication
of M&C?s study similarly yielded a correlation of
r = 0.96. The M&C pairs were also included in a
similarity study by Finkelstein et al (2002), which
yielded correlation of r = 0.95 to the M&C norms.
2.1 WordSim353
WordSim353 (Finkelstein et al, 2002) has re-
cently emerged as a potential surrogate dataset for
evaluating relatedness measures. Several studies
have reported correlation to WordSim353 norms
as part of their evaluation procedures, with some
studies explicitly referring to it as a collection of
human-assigned relatedness scores (Gabrilovich
and Markovitch, 2007; Hughes and Ramage,
2007; Milne and Witten, 2008).
890
Yet, the instructions presented to Finkelstein et
al.?s subjects give us pause to reconsider Word-
Sim353?s classification as a set of relatedness
norms. They repeatedly framed the task as one in
which subjects were expected to assign word simi-
larity scores, although participants were instructed
to extend their definition of similarity to include
antonymy, which perhaps explains why the au-
thors later referred to their data as ?relatedness?
norms rather than merely ?similarity? norms.
Jarmasz and Szpakowicz (2003) have raised fur-
ther methodological concerns about the construc-
tion of WordSim353, including: (a) similarity was
rated on a scale of 0.0 to 10.0, which is intrin-
sically more difficult for humans to manage than
the scale of 0.0 to 4.0 used by R&G and M&C,
and (b) the inclusion of proper nouns introduced
an element of cultural bias into the dataset (e.g.,
the evaluation of the pair Arafat?terror).
Cognizant of the problematic conflation of sim-
ilarity and relatedness in WordSim353, Agirre et
al. (2009) partitioned the data into two sets:
one containing noun pairs exhibiting similarity,
and one containing pairs of related but dissimilar
nouns. However, pairs in the latter set were not
assessed for scoring distribution validity to ensure
that strongly related word pairs were not penalized
by human subjects for being dissimilar.1
3 Methodology
In our experiments, we elicited human ratings of
semantic relatedness for 122 noun pairs. In doing
so, we followed the methodology of Rubenstein
and Goodenough (1965) as closely as possible:
participants were instructed to read through a set
of noun pairs, sort them by how strongly related
they were, and then assign each pair a relatedness
score on a scale of 0.0 (?completely unrelated?) to
4.0 (?very strongly related?).
We made two notable modifications to the ex-
perimental procedure of Rubenstein and Goode-
nough. First, instead of asking participants to
judge ?amount of ?similarity of meaning,?? we
asked them to judge ?how closely related in mean-
ing? each pair of nouns was. Second, we used a
Web interface to collect data in our study; instead
of reordering a deck of cards, participants were
presented with a grid of cards that they were able
1Perhaps not surprisingly, the highest scores in Word-
Sim353 (all ratings from 9.0 to 10.0) were assigned to pairs
that Agirre et al placed in their similarity partition.
to rearrange interactively with the use of a mouse
or any touch-enabled device, such as a tablet PC.2
3.1 Experimental Conditions
Each participant in our study was randomly as-
signed to one of four conditions. Each condition
contained 32 noun pairs for evaluation.
Of those pairs, 10 were randomly selected
from from WordNet++ (Ponzetto and Navigli,
2010) and 10 from SGN (Szumlanski and Gomez,
2010)?two semantic networks that categori-
cally indicate strong relatedness between Word-
Net noun senses. 10 additional pairs were gen-
erated by randomly pairing words from a list of
all nouns occurring in Wikipedia. The nouns
in the pairs we used from each of these three
sources were matched for frequency of occurrence
in Wikipedia.
We manually selected two additional pairs that
appeared across all four conditions: leaves?rake
and lion?cage. These control pairs were included
to ensure that each condition contained examples
of strong semantic relatedness, and potentially to
help identify and eliminate data from participants
who assigned random relatedness scores. Within
each condition, the 32 word pairs were presented
to all subjects in the same random order. Across
conditions, the two control pairs were always pre-
sented in the same positions in the word pair grid.
Each word pair was subjected to additional
scrutiny before being included in our dataset. We
eliminated any pairs falling into one or more
of the following categories: (a) pairs containing
proper nouns, (b) pairs in which one or both nouns
might easily be mistaken for adjectives or verbs,
(c) pairs with advanced vocabulary or words that
might require domain-specific knowledge in or-
der to be properly evaluated, and (d) pairs with
shared stems or common head nouns (e.g., first
cousin?second cousin and sinner?sinning). The
latter were eliminated to prevent subjects from
latching onto superficial lexical commonalities as
indicators of strong semantic relatedness without
reflecting upon meaning.
3.2 Participants
Participants in our study were recruited from in-
troductory undergraduate courses in psychology
and computer science at the University of Cen-
tral Florida. Students from the psychology courses
2Online demo: http://www.cs.ucf.edu/?seansz/rel-122
891
participated for course credit and accounted for
89% of respondents.
92 participants provided data for our study. Of
these, we identified 19 as outliers, and their data
were excluded from our norms to prevent interfer-
ence from individuals who appeared to be assign-
ing random scores to noun pairs. We considered
an outlier to be any individual whose numeric rat-
ings fell outside two standard deviations from the
means for more than 10% of the word pairs they
evaluated (i.e., at least four word pairs, since each
condition contained 32 word pairs).
For outlier detection, means and standard de-
viations were computed using leave-one-out sam-
pling. That is, data from individual J were not in-
corporated into means or standard deviations when
considering whether to eliminate J as an outlier.3
Of the 73 participants remaining after outlier
elimination, there was a near-even split between
males (37) and females (35), with one individual
declining to provide any demographic data. The
average age of participants was 20.32 (? = 4.08,
N = 72). Most students were freshmen (49), fol-
lowed in frequency by sophomores (16), seniors
(4), and juniors (3). Participants earned an average
score of 42% on a standardized test of advanced
vocabulary (? = 16%, N = 72) (Test I ? V-4 from
Ekstrom et al (1976)).
4 Results
Each word pair in Rel-122 was evaluated by at
least 20 human subjects. After outlier removal
(described above), each word pair retained eval-
uations from 14 to 22 individuals. The resulting
relatedness means are available online.4
An excerpt of the Rel-122 norms is shown in
Table 1. We note that the highest rated pairs in our
dataset are not strictly similar entities; exactly half
of the 10 most strongly related nouns in Table 1 are
dissimilar (e.g., digital camera?photographer).
Judgments from individual subjects in our study
exhibited high average correlation to the elicited
relatedness means (r = 0.769, ? = 0.09, N =
73). Resnik (1995), in his replication of the
3We used this sampling method to prevent extreme out-
liers from masking their own aberration during outlier de-
tection, which is potentially problematic when dealing with
small populations. Without leave-one-out-sampling, we
would have identified fewer outliers (14 instead of 19), but
the resulting means would still have correlated strongly to
our final relatedness norms (r = 0.991, p < 0.01).
4http://www.cs.ucf.edu/?seansz/rel-122
# Word Pair ?
1. underwear lingerie 3.94
2. digital camera photographer 3.85
3. tuition fee 3.85
4. leaves rake 3.82
5. symptom fever 3.79
6. fertility ovary 3.78
7. beef slaughterhouse 3.78
8. broadcast commentator 3.75
9. apparel jewellery 3.72
10. arrest detention 3.69
. . .
122. gladiator plastic bag 0.13
Table 1: Excerpt of Rel-122 norms.
M&C study, reported average individual correla-
tion of r = 0.90 (? = 0.07, N = 10) to similar-
ity means elicited from a population of 10 gradu-
ate students and postdoctoral researchers. Presum-
ably Resnik?s subjects had advanced knowledge of
what constitutes semantic similarity, as he estab-
lished r = 0.90 as an upper bound for expected
human correlation on that task.
The fact that average human correlation in our
study is weaker than in previous studies suggests
that human perceptions of relatedness are less
strictly constrained than perceptions of similarity,
and that a reasonable computational measure of re-
latedness might only approach a correlation of r =
0.769 to relatedness norms.
In Table 2, we present the performance of a va-
riety of relatedness and similarity measures on our
new set of relatedness means.5 Coefficients of cor-
relation are given for Pearson?s product-moment
correlation (r), as well as Spearman?s rank corre-
lation (?). For comparison, we include results for
the correlation of these measures to the M&C and
R&G similarity means.
The generally weak performance of the
WordNet-based measures on this task is not
surprising, given WordNet?s strong disposition
toward codifying semantic similarity, which
makes it an impoverished resource for discovering
general semantic relatedness. We note that the
three WordNet-based measures from Table 2
that are regarded in the literature as relatedness
measures (Banerjee and Pedersen, 2003; Hirst and
St-Onge, 1998; Patwardhan and Pedersen, 2006)
5Results based on standard implementations in the Word-
Net::Similarity Perl module of Pedersen et al (2004) (v2.05).
892
Rel-122 M&C R&G
Measure r ? r ? r ?
* Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841
* Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795
Path Length 0.225 0.183 0.755 0.715 0.784 0.783
* Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718
Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757
Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592
Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783
Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768
Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636
* Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797
Table 2: Correlation of similarity and relatedness measures to Rel-122, M&C, and R&G. Starred rows
(*) are considered relatedness measures. All measures are WordNet-based, except for the scoring metric
of Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia.
# Noun Pair Sim. Rel. # Noun Pair Sim. Rel.
1. car automobile 3.92 4.00 16. lad brother 1.66 2.68
2. gem jewel 3.84 3.98 17. journey car 1.16 3.00
3. journey voyage 3.84 3.97 18. monk oracle 1.10 2.54
4. boy lad 3.76 3.97 19. cemetery woodland 0.95 1.69
5. coast shore 3.70 3.97 20. food rooster 0.89 2.59
6. asylum madhouse 3.61 3.91 21. coast hill 0.87 1.59
7. magician wizard 3.50 3.58 22. forest graveyard 0.84 2.01
8. midday noon 3.42 4.00 23. shore woodland 0.63 1.63
9. furnace stove 3.11 3.67 24. monk slave 0.55 1.31
10. food fruit 3.08 3.91 25. coast forest 0.42 1.89
11. bird cock 3.05 3.71 26. lad wizard 0.42 2.12
12. bird crane 2.97 3.96 27. chord smile 0.13 0.68
13. tool implement 2.95 2.86 28. glass magician 0.11 1.30
14. brother monk 2.82 2.89 29. rooster voyage 0.08 0.63
15. crane implement 1.68 0.90 30. noon string 0.08 0.14
Table 3: Comparison of relatedness means to M&C similarity means. Correlation is r = 0.91.
have been hampered by their reliance upon Word-
Net. The disparity between their performance on
Rel-122 and the M&C and R&G norms suggests
the shortcomings of using similarity norms for
evaluating measures of relatedness.
5 (Re-)Evaluating Similarity Norms
After establishing our relatedness norms, we cre-
ated two additional experimental conditions in
which subjects evaluated the relatedness of noun
pairs from the M&C study. Each condition again
had 32 noun pairs: 15 from M&C and 17 from
Rel-122. Pairs from M&C and Rel-122 were uni-
formly distributed between these two new condi-
tions based on matched normative similarity or re-
latedness scores from their respective datasets.
Results from this second phase of our study are
shown in Table 3. The correlation of our relat-
edness means on this set to the similarity means
of M&C was strong (r = 0.91), but not as strong
as in replications of the study that asked subjects
to evaluate similarity (e.g. r = 0.96 in Resnik?s
(1995) replication and r = 0.95 in Finkelstein et
al.?s (2002) M&C subset).
That the synonymous M&C pairs garner high
relatedness ratings in our study is not surprising;
strong similarity is, after all, one type of strong
relatedness. The more interesting result from
893
our study, shown in Table 3, is that relatedness
norms for pairs that are related but dissimilar (e.g.,
journey?car and forest?graveyard) deviate signif-
icantly from established similarity norms. This in-
dicates that asking subjects to evaluate ?similar-
ity? instead of ?relatedness? can significantly im-
pact the norms established in such studies.
6 Conclusions
We have established a new set of relatedness
norms, Rel-122, that is offered as a supplementary
evaluative standard for assessing semantic related-
ness measures.
We have also demonstrated the shortcomings
of using similarity norms to evaluate such mea-
sures. Namely, since similarity is only one type of
relatedness, comparison to similarity norms fails
to provide a complete view of a measure?s abil-
ity to capture more general types of relatedness.
This is particularly problematic when evaluating
WordNet-based measures, which naturally excel at
capturing similarity, given the nature of the Word-
Net ontology.
Furthermore, we have found that asking judges
to evaluate ?relatedness? of terms, rather than
?similarity,? has a substantive impact on resulting
norms, particularly with respect to the M&C sim-
ilarity dataset. Correlation of individual judges?
ratings to resulting means was also significantly
lower on average in our study than in previous
studies that focused on similarity (e.g., Resnik,
1995). These results suggest that human percep-
tions of relatedness are less strictly constrained
than perceptions of similarity and validate the
need for new relatedness norms to supplement ex-
isting gold standard similarity norms in the evalu-
ation of relatedness measures.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
19?27.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the 18th International
Joint Conference on Artificial Intelligence (IJCAI),
pages 805?810.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Ruth B. Ekstrom, John W. French, Harry H. Harman,
and Diran Dermen. 1976. Manual for Kit of Factor-
Referenced Cognitive Tests. Educational Testing
Service, Princeton, NJ.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems (TOIS), 20(1):116?131.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using Wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 1606?1611.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detec-
tion and correction of malapropisms. In Christiane
Fellbaum, editor, WordNet: An Electronic Lexical
Database, pages 305?332. MIT Press.
Thad Hughes and Daniel Ramage. 2007. Lexi-
cal semantic relatedness with random graph walks.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 581?589, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s
thesaurus and semantic similarity. In Proceedings of
the International Conference on Recent Advances in
Natural Language Processing (RANLP), pages 212?
219.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of the International Confer-
ence on Research in Computational Linguistics (RO-
CLING), pages 19?33.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 265?283. MIT Press.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th Inter-
national Conference on Machine Learning (ICML),
pages 296?304.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
894
David Milne and Ian H. Witten. 2008. An effective,
low-cost measure of semantic relatedness obtained
from Wikipedia links. In Proceedings of the First
AAAI Workshop on Wikipedia and Artificial Intelli-
gence (WIKIAI), pages 25?30.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet-based context vectors to estimate the
semantic relatedness of concepts. In Proceedings
of the 11th Conference of the European Chapter of
the Association for Computational Linguistics Work-
shop on Making Sense of Sense, pages 1?8.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity ? Measuring
the relatedness of concepts. In Proceedings of the
5th Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL), pages 38?11.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1522?1531.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI), pages 448?453.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Sean Szumlanski and Fernando Gomez. 2010. Au-
tomatically acquiring a semantic network of related
concepts. In Proceedings of the 19th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 19?28.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 133?139.
895
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 190?199,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Evaluating a Semantic Network Automatically Constructed from Lexical
Co-occurrence on a Word Sense Disambiguation Task
Sean Szumlanski
Department of EECS
University of Central Florida
seansz@cs.ucf.edu
Fernando Gomez
Department of EECS
University of Central Florida
gomez@eecs.ucf.edu
Abstract
We describe the extension and objective eval-
uation of a network1 of semantically related
noun senses (or concepts) that has been au-
tomatically acquired by analyzing lexical co-
occurrence in Wikipedia. The acquisition pro-
cess makes no use of the metadata or links
that have been manually built into the ency-
clopedia, and nouns in the network are auto-
matically disambiguated to their correspond-
ing noun senses without supervision. For
this task, we use the noun sense inventory of
WordNet 3.0. Thus, this work can be con-
ceived of as augmenting the WordNet noun
ontologywith unweighted, undirected related-
to edges between synsets. Our network con-
tains 208,832 such edges.
We evaluate our network?s performance on a
word sense disambiguation (WSD) task and
show: a) the network is competitive with
WordNet when used as a stand-alone knowl-
edge source for two WSD algorithms; b) com-
bining our network with WordNet achieves
disambiguation results that exceed the perfor-
mance of either resource individually; and c)
our network outperforms a similar resource
that has been automatically derived from se-
mantic annotations in the Wikipedia corpus.
1 Introduction
A growing interest in using semantic relatedness in
word sense disambiguation (WSD) tasks has spurred
investigations into the limitations of the WordNet
ontology (Fellbaum, 1998) for this purpose. Al-
though WordNet comprises a rich set of semantic
1http://www.cs.ucf.edu/? seansz/sem
links between word senses (or concepts), indicat-
ing semantic similarity through subsumptive hyper-
nymic and hyponymic relations (among others), it
lacks a general indication of semantic relatedness.
We present a semantic network that is automat-
ically acquired from lexical co-occurrence in Wi-
kipedia, and indicates general semantic relatedness
between noun senses in WordNet 3.0. In this work,
the discovery of relatedness is a context-sparse affair
that takes place in absentia of the semantic annota-
tions of Wikipedia, such as inter-article links, entries
in disambiguation pages, the title of the article from
which a sentence is extracted, and so on.
We released an earlier version of such a network
that was limited by the fact that only relationships
involving at least one monosemous noun had been
included, and it was not evaluated on a WSD task
(Szumlanski and Gomez, 2010).
In contrast, the network we present here has relat-
edness data for over 4,500 polysemous noun targets
and 3,000 monosemous noun targets, each of which
are related to an average of 27.5 distinct noun senses.
It consists of 208,832 undirected edges ? a 181% in-
crease in size over the previous network. The result
is a semantic network that has reached maturity and,
as we will show, can be successfully applied to a
WSD task.
This paper proceeds as follows. In the next sec-
tion (Section 2), we discuss related work. We then
give an overview of the method we use to con-
struct our network (Sections 3 and 4). The network
is evaluated through its application to a WSD task
(Sections 5?7), where we compare its performance
to WordNet and another automatically acquired se-
mantic network called WordNet++ (Ponzetto and
Navigli, 2010). A discussion follows (Section 8),
190
and we present our conclusions in Section 9.
2 Related Work
Our work bears strong relation to WordNet++
(henceforth WN++), which is constructed automat-
ically from the semantic annotations in Wikipedia
(Ponzetto and Navigli, 2010).2 Links in WN++ are
established between words whose articles link to one
another. For example, the article on astronomy in
Wikipedia links to the article on celestial naviga-
tion, so we find an edge from astronomy#n#1 to
celestial navigation#n#1 in WN++.3 The nouns re-
lated in WN++ are disambiguated automatically us-
ing further semantic annotation data from Wikipe-
dia, including sense labels, the titles of other pages
linked to by any two related nouns, and the folk-
sonomic categories to which articles belong. These
serve as context words that are compared with con-
text words from various WordNet relations in or-
der to map the nouns to their appropriate WordNet
senses. The resulting resource contains 1,902,859
unique edges between noun senses.
Augmenting the structure of Wikipedia itself has
been the subject of research as well, and involves
the discovery of relations between articles. Mihal-
cea and Csomai (2007), for example, added links
between Wikipedia pages after automatically iden-
tifying keywords in each article and disambiguating
those words to their appropriate Wikipedia concepts
(article titles), while Ponzetto and Navigli (2009)
used graph theoretic approaches to augment the tax-
onomic organization of Wikipedia articles.
In terms of automatically discovering semantic re-
lations, many pattern-based approaches have been
used to extract specific types of relations from large
corpora, e.g., hyponymy, meronymy, and synonymy
(Hearst, 1992; Pantel and Pennacchiotti, 2006).
Approaches based on distributional similarity have
been applied toward the same end (Harris, 1985;
Gorman and Curran, 2006), and there are sev-
eral approaches that rely on the underlying struc-
ture of WordNet or Wikipedia to measure the re-
latedness between two concepts or nouns quantita-
tively (Hughes and Ramage, 2007; Gabrilovich and
2http://lcl.uniroma1.it/wordnetplusplus
3The notation astronomy#n#1 refers to sense 1 (#1) of the
noun (#n) ?astronomy? in WordNet. Other parts of speech are
denoted by #v (verbs), #a (adjectives), or #r (adverbs).
Markovitch, 2007; Zaragoza et al, 2007; Patward-
han and Pedersen, 2006; Strube and Ponzetto, 2006;
Budanitsky and Hirst, 2006; Resnik, 1995).
Other quantitative approaches have leveraged the
large amounts of data available on the Web to dis-
cover relatedness. Notably, Agirre and de Lacalle
(2004) employed web queries to associate WordNet
synsets with representative context words, known as
topic signatures. Cuadros and Rigau (2008) have
used these data to construct four KnowNets, seman-
tic knowledge bases derived by disambiguating the
top 5, 10, 15, and 20 nouns, respectively, from the
topic signatures of Agirre and de Lacalle.
3 Automatic Acquisition of the Semantic
Network
The semantic network is automatically acquired in
three distinct stages (Szumlanski and Gomez, 2010):
(1) quantitative measurement of relatedness between
nouns that co-occur in a large corpus; (2) categori-
cal determination of whether the quantitative mea-
sure indicates strong and mutual semantic related-
ness between a given pair of nouns; and (3) unsuper-
vised disambiguation of all the nouns that are found
to be semantically related. We provide an overview
of each of these steps below (Sections 3.1?3.3), and
then discuss how we have expanded this method-
ology to create a more complete semantic network
(Section 4).
3.1 Quantitatively measuring relatedness from
lexical co-occurrence
We first measure the semantic relatedness, or re-
lational strength, of a target, t, to one of its co-
occurring nouns, or co-targets, c, with the following
asymmetric function:
Srel(t, c) = P (t|c)P (c|t)log
P (c|t)
P (c)
where P (c|t) is the relative frequency of c among all
nouns co-occurring with t, and vice versa for P (t|c).
P (c) is the relative frequency of c among all nouns
occurring in the corpus. For these values, we rely on
lexical co-occurrence data extracted from Wikipe-
dia. Co-occurrence is considered intra-sententially
(as opposed to co-occurrence in entire articles or
paragraphs, or co-occurrence within variable-sized
windows of context).
191
This function essentially measures the degree to
which an occurrence of t in a sentence predicts the
co-occurrence of c. It is an adaptation of Resnik?s
(1999) selectional association measure.
Table 1 shows the results of applying this function
to the co-targets of ?yoga? and ?meditation.?
Target (t): yoga Target (t): meditation
Co-target (c) Srel Co-target (c) Srel
hatha yoga .1801 yoga .0707
asana .0761 mindfulness .0415
meditation .0673 contemplation .0165
bhakti .0508 prayer .0139
raja .0410 practice .0068
tantra .0148 technique .0060
yogi .0132 mantra .0053
karma .0125 relaxation .0048
posture .0104 retreat .0047
aerobics .0093 enlightenment .0031
tai chi .0089 monk .0025
exercise .0036 posture .0024
practice .0032 breathing .0017
instructor .0031 - - - - - - - - - - - - -
- - - - - - - - - - - - - exercise .0015
guru .0027 teaching .0014
massage .0026 practitioner .0014
exercise .0019 ascetic .0014
...
...
...
...
Table 1: The most strongly related co-targets of ?yoga?
and ?meditation,? sorted by decreasing value of relational
strength (Srel). Nouns above dashed lines are the top 5%
of the target?s most strongly related co-targets.
3.2 Establishing categorical relatedness
We then use a mutual relatedness algorithm to as-
certain whether two nouns are semantically related
by determining whether the nouns under considera-
tion reciprocate a high degree of relatedness to one
another. It proceeds as follows:
For some target noun of interest, t, let Cx(t) be
the set of the top x% of t?s co-targets as sorted by
Srel(t, c). For each c ? Cx(t), if we have t ? Cx(c),
then we say that t and c are categorically related and
add the noun pair (t, c) to our semantic network. We
then increment x by one and repeat the process: for
every c ? Cx+1(t) such that (t, c) is not already in
our network, we look for t in Cx+1(c), and add (t, c)
to our network if we find it. This process continues
until we have incremented x some number of times
without adding any new relations to the semantic
network. We then take the symmetric closure of the
network, so that if (t, c) is in the network, (c, t) is, as
well. (That is, the relation is considered undirected.)
Consider, for example, the nouns in Table 1.
Given the target ?yoga,? we might first examine the
top 5% of its most strongly related co-targets (an ar-
bitrary initial threshold chosen simply for illustra-
tive purposes). In this case, we have all the nouns
above the dashed line: C5(yoga) = {hatha yoga,
asana, meditation, bhakti, raja, tantra, yogi, karma,
posture, aerobics, tai chi, exercise, practice, instruc-
tor}. The algorithm then searches C5(hatha yoga),
C5(asana), and so on, for ?yoga,? adding a new re-
lation to the network every time ?yoga? is found.
Thus, we can see by the inclusion of ?yoga? in
C5(meditation) (all nouns above the dashed line in
the second column of Table 1), that the pair (yoga,
meditation) will be included in the network.
This reliance on mutual relatedness ensures that
only noun pairs exhibiting strong semantic related-
ness are admitted to the network.
3.3 Disambiguation
Disambiguation of the resulting noun-noun pairs is
the product of majority-rules voting by the following
three algorithms.
Subsumption. The most frequently occurring
immediate hypernyms of all nouns related to our
target are permitted to disambiguate the polyse-
mous nouns. This is useful because of the semantic
clustering that tends to occur among related nouns.
(E.g., ?astronomer? is related to several terms cat-
egorized as celestial bodies in WordNet, such as
?planet,? ?star,? ?minor planet,? and ?quasar.?)
Glosses. Senses of polysemous co-targets with
occurrences of monosemous co-targets in their
glosses are preferentially taken as the intended
meanings of the polysemous nouns. Monosemous
co-targets are matched directly, or by suffix replace-
ment. (E.g., ?biology? can be matched by the oc-
currence of ?biologist? in a gloss, ?engineering? by
?engineers,? and so on.)
Selectional Preferences. This method associates
a numerical score with all superordinate synsets
from the WordNet noun ontology that categorize
192
the monosemous nouns related to a target. For
example, the noun ?unicorn? strongly predicts re-
lated nouns categorized as monsters (monster#1)4
and mythical beings (mythical being#1) in Word-
Net. These selectional preferences are applied to
polysemous co-targets in decreasing order of their
relational strength to the target noun. A polysemous
noun is disambiguated to the first sense or senses
subsumed by one of these selectional preferences.
For example, ?phoenix,? as it relates to ?unicorn,? is
disambiguated to phoenix#3 in WordNet (the fiery
bird that is reborn from its own ashes) by virtue of
its subsumption by mythical being#1.
4 Creating a More Complete Network
A shortcoming of our previously released network is
that it lacked concept-level relations between pairs
of polysemous nouns.
When humans encounter a pair of ambiguous but
closely related words, like bus?horn, we automat-
ically disambiguate to the automobile and the car
horn, as opposed to a computer?s front-side bus or
a rhinoceros?s horn. The human ability to perform
this disambiguation stems from the fact that human
semantic memory relates not just individual words,
but specific concepts denoted by those words. But if
our goal is to establish such a link in our computa-
tional model of semantic relatedness, then we can-
not rely on the link to perform that disambiguation
for us; another approach is called for.
One reasonable approach (the one taken in our
previous work) is to go where the problem no
longer exists ? to relationships that involve at
least one monosemous noun. Monosemous-to-
monosemous noun relationships require no disam-
biguation. Monosemous-to-polysemous noun rela-
tionships, on the other hand, require that only one
noun be disambiguated. This ameliorates our prob-
lem tremendously, because the monosemous noun
in the pair anchors the polysemous noun in an un-
ambiguous context where disambiguation can more
readily take place. That context includes all the
nouns related to our monosemous noun, which,
through their transitive relatedness to the polyse-
mous noun in question, can assist in the act of disam-
4We sometimes drop the part of speech from our word sense
notation for brevity, but only in the case of noun senses.
biguation vis-a`-vis the algorithms described in Sec-
tion 3.3.
Consider, in contrast, the polysemous ?batter,?
which can refer to the baseball player or the cake
batter. The algorithm for discovering semantic relat-
edness yields several nouns related to each of these
senses of ?batter? (see Table 2). If we wish to dis-
ambiguate the pair (batter, cake), we are left with the
question: which of the nouns in Table 2 should we
take as contextual anchors for the disambiguation?
baking fastball inning strike
ball flour outfielder strikeout
base glove pancake swing
baseball hitter pitch tempura
bat home plate pudding umpire
cake home run runner waffle
dugout infielder shortstop
Table 2: An excerpt of some of the nouns related to ?bat-
ter? by the algorithm for automatic acquisition.
In considering this question, it is important to note
that although the ontological categories that sub-
sume the nouns related to ?batter? exhibit greater
entropy than we usually observe among the terms
related to a monosemous noun, clear delineations
still exist. For example, Figure 1 shows the clusters
that form as we consider shared hypernymic rela-
tionships between all senses of the nouns related to
?batter? (gray nodes in the graph). We see that many
of the nouns related to ?batter? have senses catego-
rized by food#1, cake#3, pitch#2, ballplayer#1, or
equipment#1 ? the heads of five distinct clusters by
semantic similarity.
It is worth noting that some nouns related to ?bat-
ter? (such as ?baking,? ?swing,? and ?umpire?) do
not fall into any of these semantic clusters. In these
cases, the WordNet glosses serve as our primary
tool for disambiguation. (For example, the glosses
of both swing#8 and umpire#1 include mention of
?baseball,? which is also related to ?batter.?)
Conversely, some of the polysemous nouns in our
example have senses that join semantic clusters un-
intendedly. For instance, cake#2 (?[a] small flat
mass of chopped food,? according to WordNet) falls
under the cluster headed by food#1. Although this is
potentially problematic, cake#2 is discarded in this
particular case in favor of cake#3 (the baked good),
193
which has a greater mass because of its subsump-
tion of waffle#1 and pancake#1, and is indeed the
intended meaning of ?cake? as it relates to ?batter.?
Another example of unintended cluster member-
ship comes from bat#4 (the cricket bat), which is
categorized by sports equipment#1. In contrast, the
baseball bat does not have its own entry in WordNet,
and the most reasonable sense choice, bat#5 (?a club
used for hitting a ball in various games?), is cate-
gorized as a stick (stick#1), and not as equipment,
sports equipment, or game equipment.
These unintended cluster memberships are bound
to cause minor errors in our disambiguation efforts.
However, our analysis reveals that we do not find
such high entropy among the relatives of a polyse-
mous noun that the semantic clustering effect (which
is necessary for the success of the disambiguation
algorithms described above in Section 3.3) is dimin-
ished. Thus, to construct our network, we apply the
disambiguation algorithms described above, with
the following modification: when confronted with
a pair of semantically related polysemous nouns,
we apply the disambiguation mechanism described
above in both directions, and then fuse the results to-
gether. So, in one direction, the various baked goods
related to ?batter? help us to properly disambiguate
?cake? to cake#3 in WordNet, yielding the pair (bat-
ter, cake#3). A similar scenario yields the pair (cake,
batter#2) when disambiguating in the other direc-
tion, and we fuse the results together into the prop-
erly disambiguated pair (batter#2, cake#3).
Using this method, we have automatically created
a semantic network that has 208,832 pairs of related
noun senses ? the most extensive semantic network
between WordNet noun senses to be derived auto-
matically from a simple lexical co-occurrence mea-
sure. For the remainder of this paper, we will refer
to our network as the Szumlanski-Gomez network
(SGN).
5 Coarse-Grained WSD Experiments
To evaluate our semantic network, and to provide
fair comparison to related work, we take our cue
from Ponzetto and Navigli (2010), who evaluated
the performance of WN++ on the SemEval-2007
(Navigli et al, 2007) coarse-grained all-words WSD
task using extended gloss overlaps (Banerjee and
entity#1
food#1
cake#2
equipment#1
ballplayer#1
pitch#2
cake#3
dessert#1
tempura#1
game_equipment#1
sports_equipment#1
runner#4
fielder#1
hitter#1
fastball#1
strike#5
waffle#1
pancake#1
pudding#2
pudding#3
ball#1
infielder#1
outfielder#1
baseball#2
bat#4
base#3
glove#1
glove#3
shortstop#1
centerfielder#1
Figure 1: A partial view of the WordNet graph, showing
senses of nouns related to ?batter? (gray nodes) and inter-
mediary concepts (white nodes) that connect them to the
root of the taxonomy through hypernymic relationships.
Pedersen, 2003) and the graph-based degree central-
ity algorithm (Navigli and Lapata, 2010).
In this particular SemEval task, we are presented
with 237 sentences in which lemmatized target
words have been flagged for disambiguation. In our
experiments, we disambiguate nouns only (as did
Ponzetto and Navigli), since both SGN (our net-
work) and WN++ relate only concepts denoted by
nouns, and no other parts of speech. In our exper-
imental setup, each sentence is considered in isola-
tion from the rest, and all lemmatized content words
in a sentence are provided to the disambiguation
algorithms; the verbs, adjectives, and adverbs, al-
though we do not resolve their senses, lend addi-
tional context to the disambiguation algorithms.
The coarse-grained nature of the SemEval-2007
task provides that there may be more than one ac-
ceptable sense assignment for many of the targets. In
the coarse-grained setting, an algorithm?s sense as-
signment is considered correct when it appears in the
list of acceptable senses for the given target word.
The algorithms below both allow for multiple dis-
ambiguation results to be returned in the event of a
tie. In these cases (although they are rare), we adopt
the approach of Banerjee and Pedersen (2003), who
award partial credit and discredit proportionally for
all the senses returned by the algorithm.
194
6 Extended Gloss Overlaps (ExtLesk)
The first disambiguation algorithm we employ is
the extended gloss overlaps measure (henceforth
ExtLesk) of Banerjee and Pedersen (2003), which
is an extension of the Lesk (1986) gloss overlap
measure. Loosely speaking, the algorithm disam-
biguates a target noun by maximizing the overlap
(number of words in common) between the glosses
of word senses related5 to the target?s noun senses
and those related to all context words (all lemma-
tized targets in the sentence under consideration
other than the target itself). The sense with the great-
est overlap is selected as the intended meaning of a
target noun.
In the event of a tie, multiple senses may be se-
lected. ExtLesk does not attempt to perform sense
assignment if the score for every sense of a target
noun is zero, except when dealing with a monose-
mous noun, in which case we default to the only
sense possible.
6.1 Results
We have run ExtLesk on the SemEval-2007 task us-
ing five combinations of semantic resources: Word-
Net only, SGN (our semantic network) only, SGN
and WordNet combined (that is, the union of all
links contained in both networks), WN++ only, and
WN++ combined withWordNet. We include the tra-
ditional baselines of most frequent sense (MFS) as-
signment and random sense assignment for compari-
son, and measure precision (number of correct sense
assignments divided by the number of attempted
sense assignments), recall (number of correct sense
assignments divided by the number of target nouns
to be disambiguated), and the harmonic mean of the
two, F1, defined as:
F1 =
2 ? precision ? recall
precision + recall
We present our results in Table 3, and offer the
following observations. Firstly, SGN as a stand-
alone network rivals the performance of WordNet.
This is particularly impressive given the fact that
5We use all relations available in WordNet, as well as a
related-to relation derived from the links in our semantic net-
work.
Resource P R F1
WordNet 78.80 74.82 76.76
SGN 78.64 72.82 75.62
SGN and WordNet 82.35 78.11 80.18
WN++ 74.67 61.87 67.67
WN++ and WordNet 77.35 73.38 75.31
MFS Baseline 77.40 77.40 77.40
Random Baseline 63.50 63.50 63.50
Table 3: ExtLesk disambiguation results on the SemEval-
2007 all-words coarse-grained WSD task (nouns only).
the edges in SGN were derived automatically from
a simple lexical co-occurrence measure.
Equally impressive is the ability of SGN and
WordNet, when used in combination, to achieve re-
sults that exceed what either network is able to ac-
complish as a stand-alone knowledge source. When
combined, we see improvements of 3.42% and
4.56% over WordNet and SGN as stand-alone re-
sources, respectively. It is also only with these re-
sources combined that we are able to outperform the
MFS baseline, and we do so by 2.78%.6
In contrast, WN++ fails to perform as a stand-
alone resource, falling behind the MFS baseline by
9.73%.7 Of all the resources tested, WN++ yields
the lowest results. When combined with WordNet,
WN++ actually diminishes the ability of WordNet to
perform on this WSD task by 1.45%. We defer our
discussion of factors impacting the performance of
WN++ to Section 8 (Discussion).
7 WSD with Degree Centrality
Degree centrality is a graph-based measure of se-
mantic relatedness (Navigli and Lapata, 2010) in
which we search through a semantic network for
paths of length l ? maxLength between all sense
nodes for all lemmas in our context. The edges along
all such paths are added to a new graph, G?, and for
each target noun to be disambiguated, the sense node
with the greatest number of incident edges (highest
vertex degree) in G? is taken as its intended sense.
6Other systems have obtained better results on the same
dataset, but we focus only on SGN and WN++ because our aim
is to compare the resources themselves.
7Ponzetto and Navigli (2010) report results of F1 = 68.3 and
72.0 for WN and WN++ as stand-alone resources. Space con-
siderations prevent us from discussing this disparity in detail.
195
In these graphs, nodes represent synsets, as op-
posed to instantiating separate nodes for different
members of the same synset and allowing edges to
be constructed between them. We include all lem-
mas from a sentence in our context, but only return
disambiguation results for the nouns.
With SGN and WN++, the implementation of this
algorithm is straightforward. We initiate a breadth-
first search (BFS)8 at each target sense node in the
network, and proceed through ?maxLength+12 ? itera-
tions of spreading activation. Whenever the tendrils
of this spreading activation from one target sense
node in the graph connect to those of another,9 we
add the path between the nodes to our new graph, G?,
potentially incrementing the degree of the involved
target sense nodes in G? as we do so.
Because BFS is an admissible algorithm (guaran-
teed to find the shortest path from an initial state to
a goal), it provides a computationally efficient ap-
proach to finding all paths between all target nodes.
Also, because any node on a path of length l ?
maxLength between two target nodes is at most
? l2? nodes removed from at least one of those tar-
get sense nodes, we only need to perform a BFS of
depth ?maxLength+12 ? from every target sense node
in order to guarantee that every such path between
them will be discovered. Since the time complexity
of BFS is exponential with respect to the depth of
the search, cutting this depth in half (in comparison
to performing a BFS of depth maxLength) greatly
reduces the running time of our algorithm.
We take the same approach in traversing the
WordNet noun graph, using all possible sense re-
lations as edges. In keeping with the approach of
Navigli and Lapata (2010), an edge is also induced
between synsets if the gloss of one synset contains a
monosemous content word. For example, the gloss
for leprechaun#n#1, ?a mischievous elf in Irish folk-
lore,? contains the monosemous noun ?folklore;?
thus, we have an edge between leprechaun#n#1 and
8This is in contrast to the DFS implementation of Navigli
and Lapata (2010), so for the sake of posterity, we expound
upon our approach in this section.
9When maxLength is odd, this requires an additional
check to ensure that the intersection is not taking place at a node
that is exactly ?maxLength+12 ? degrees removed from each of
the two target nodes it is connecting, as this would result in a
path with overall length maxLength + 1 between the target
nodes.
folklore#n#1 in the WordNet graph.
Once we have our new graph, G?, constructed in
this manner, the vertex degree is considered an in-
dication of the semantic relatedness of a particular
synset to all other lemmas in our context. For each
target noun, we use its sense node with the highest
degree in G? for sense assignment.
7.1 Results
We have tested the degree centrality algorithm with
the following combinations of semantic resources:
WordNet, SGN, WN++, Refined WN++, SGN and
WordNet combined, and Refined WN++ and Word-
Net combined. (Refined WN++ consists of 79,422
of WN++?s strongest relations, and was created in an
unsupervised setting by Ponzetto and Navigli specif-
ically for use with degree centrality when they dis-
covered that WN++ had too many weak relations to
perform well with the algorithm.)
We have observed that the performance of de-
gree centrality rapidly levels off as maxLength
increases. Ponzetto and Lapata (2010) also re-
port this so-called ?plateau? effect, and employ a
maxLength of 6 in their experiments, despite find-
ing that results level off around maxLength = 4.
We, too, find that performance levels off around
maxLength = 4 in almost all cases, and so only
continue up to maxLength = 5.
We find that, in all cases tested, degree centrality
is unable to outperform the MFS baseline (with re-
spect to F1) (see Table 4). SGN and WN++ exhibit
comparable performance with this algorithm, with
maximum F1 values of 68.4% (maxLength = 2)
and 67.3% (maxLength = 3?5), respectively. Nei-
ther achieves the performance of WordNet with de-
gree centrality (F1 = 74.0%), which underperforms
the MFS baseline (F1 = 77.4%) by 3.4%.10 Ponzetto
and Navigli (2010) reported that only performing
sense assignment when the max degree exceeded an
empirically derived but non-disclosed threshold im-
proved performance, but we have found that imple-
menting such a threshold universally lowers results
for all resources we tested with degree centrality.
10Although Ponzetto and Navigli (2010) reported similar re-
sults with WordNet (F1 = 74.5), we have been unable to repro-
duce their results using Refined WN++, either combined with
WordNet (F1 = 79.4) or as a stand-alone resource (F1 = 57.4).
196
The lowest performance using degree central-
ity comes from Refined WN++ as a stand-alone
resource. We attribute this to the fact that Re-
fined WN++ is so semantically sparse. On average,
noun senses in Refined WN++ are related to only
3.42 other noun senses, while those in WN++ and
SGN relate to an average of 44.59 and 10.92 noun
senses, respectively. Accordingly, the success of Re-
fined WN++ and WordNet combined is attributable
mostly to the success of WordNet as a stand-alone
resource; as maxLength increases, the contribu-
tions made by the sparse Refined WN++ network
rapidly become negligible in comparison to those
provided by the WordNet ontology.
l P R F1 P R F1
WordNet SGN
1 96.9 16.8 28.6 79.7 32.9 46.6
2 77.6 45.1 57.0 72.0 64.6 68.4
3 76.7 65.6 70.7 68.7 63.5 66.0
4 76.9 71.0 73.9 68.0 63.9 65.9
5 76.6 71.6 74.0 68.0 64.2 66.1
SGN & WN WN++
1 77.4 52.4 62.5 87.2 23.5 37.1
2 74.7 70.7 72.7 71.6 60.2 65.4
3 70.3 67.1 68.7 70.7 64.3 67.3
4 70.5 67.4 68.9 70.4 64.5 67.3
5 70.1 67.0 68.5 70.4 64.5 67.3
WN++refined WN
++
refined& WN
1 98.3 15.3 26.5 83.3 31.2 45.4
2 91.4 23.4 37.3 77.5 66.6 71.6
3 88.7 29.9 44.7 77.6 73.6 75.5
4 83.7 32.3 46.7 74.7 71.4 73.0
5 80.2 35.3 49.0 74.7 71.4 73.0
MFS Baseline Random Baseline
77.4 77.4 77.4 63.5 63.5 63.5
Table 4: Degree centrality disambiguation results on
the SemEval-2007 all-words coarse-grained WSD task
(nouns only). l is maximum path length.
8 Discussion
The fact that the performance of degree centrality
quickly plateaus hints at the root cause of its weak
performance compared to ExtLesk and the MFS
baseline. As the maximum path length is increased
in a dense semantic network, all possible edges from
our target sense nodes rapidly find themselves in-
volved with paths to other target sense nodes. This is
particularly true of WN++ (notice its rapid and sta-
ble convergence), where certain ?sticky? nodes form
bridges between seemingly unrelated concepts. For
example, the frequent appearance of ?United States?
in Wikipedia articles, and its tendency to be linked
to the United States Wikipage when it occurs, causes
the term to serve as a bridge between such diverse
concepts as automaton#2 and burrito#1, which one
would typically expect to be far removed from one
another in a model of semantic relatedness.
Nonetheless, the degree centrality algorithm has
no difficulty finding short paths between target sense
nodes when traversing any of the semantic networks
we tested. In fact, we have discovered that as the
results of degree centrality converge, they approach
the performance obtained by foregoing the algo-
rithm altogether and simply disambiguating each
noun to the sense with the most edges in the net-
work (regardless of whether those edges ultimately
connect two word senses from the disambiguation
context). The expected values of convergence at-
tained by defaulting to the most semantically well-
connected sense of each target noun in each network
are F1 = 66.3%, 67.5%, and 74.6% for SGN,WN++,
and WordNet, respectively ? remarkably close to the
experimentally derived degree centrality results of
F1 = 66.1%, 67.3%, and 74.0%.
9 Conclusion
We have constructed a semantic network of related
noun senses automatically from intra-sentential lex-
ical co-occurrence data, and shown that on a WSD
task, it outperforms a similar resource, WN++,
which is derived from the rich set of semantic anno-
tations available in the Wikipedia corpus. Our net-
work has also shown competitive performance with
the WordNet ontology onWSD, and when combined
with WordNet, improves disambiguation results in
a coarse-grained setting using the ExtLesk disam-
biguation algorithm.
Acknowledgments
This research was supported in part by the
NASA Engineering and Safety Center under
Grant/Cooperative Agreement NNX08AJ98A.
197
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Pub-
licly available topic signatures for all WordNet nom-
inal senses. In Proceedings of the 4th International
Conference on Language Resources and Evaluations
(LREC ?04), pages 1123?1126, Lisbon, Portugal.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Confer-
ence on Artificial Intelligence (IJCAI ?03), pages 805?
810, Acapulco, Mexico.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13?47.
Montse Cuadros and German Rigau. 2008. KnowNet:
building a large net of knowledge from the web. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING ?08), pages 161?
168, Manchester, UK. Association for Computational
Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness usingWikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI ?07), pages 1606?1611, Hyderabad, India.
James Gorman and James R. Curran. 2006. Scaling dis-
tributional similarity to large corpora. In Proceedings
of the 21st InternationalConference on Computational
Linguistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-ACL
?06), pages 361?368, Sydney, Australia. Association
for Computational Linguistics.
Zellig S. Harris. 1985. Distributional structure. In J. J.
Katz, editor, The Philosophy of Linguistics, pages 26?
47. Oxford University Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING ?92), pages 539?545, Nantes,
France.
Thad Hughes and Daniel Ramage. 2007. Lexical se-
mantic relatedness with random graph walks. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
?07), pages 581?589, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th Annual International Conference on Systems Doc-
umentation (SIGDOC ?86), pages 24?26, Toronto, On-
tario, Canada. ACM.
RadaMihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Pro-
ceedings of the 16th ACM Conference on Information
and Knowledge Management (CIKM ?07), pages 233?
242, Lisbon, Portugal. ACM.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study of graph connectivity for unsupervised
word sense disambiguation. IEEE Transactions on
Pattern Analysis andMachine Intelligence, 32(4):678?
692.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: coarse-grained
English all-words task. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations (Sem-
Eval ?07), pages 30?35, Prague, Czech Republic. As-
sociation for Computational Linguistics.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL ?06), pages
113?120, Sydney, Australia. Association for Compu-
tational Linguistics.
Siddharth Patwardhan and Ted Pedersen. 2006. Using
WordNet-based context vectors to estimate the seman-
tic relatedness of concepts. In Proceedings of the 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics Workshop on Mak-
ing Sense of Sense, pages 1?8, Trento, Italy.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring and
integrating Wikipedia. In Proceedings of the 21st In-
ternational Joint Conference on Artifical Intelligence
(IJCAI ?09), pages 2083?2088, Pasadena, CA.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL ?10), pages 1522?1531, Uppsala, Sweden.
Association for Computational Linguistics.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence (IJCAI ?95), pages 448?453, Mon-
treal, QC.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
an information-based measure and its application to
problems of ambiguity in natural language. Journal
of Artificial Intelligence Research, 11:95?130.
198
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using wi-
kipedia. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence (AAAI ?06), pages 1419?
1424, Boston, MA. AAAI Press.
Sean Szumlanski and Fernando Gomez. 2010. Auto-
matically acquiring a semantic network of related con-
cepts. In Proceedings of the 19th ACM Conference on
Information and KnowledgeManagement (CIKM ?10),
pages 19?28, Toronto, Ontario, Canada. ACM.
Hugo Zaragoza, Henning Rode, Peter Mika, Jordi Atse-
rias, Massimiliano Ciaramita, and Giuseppe Attardi.
2007. Ranking very many typed entities on Wikipe-
dia. In Proceedings of the 16th ACM Conference on
Information and KnowledgeManagement (CIKM ?07),
pages 1015?1018, Lisbon, Portugal. ACM.
199
