Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 186?191,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Multimodal Annotation of Conversational Data
P. Blache1, R. Bertrand1, B. Bigi1, E. Bruno3, E. Cela6, R. Espesser1, G. Ferr?4, M. Guardiola1, D. Hirst1,
E.-P. Magro6, J.-C. Martin2, C. Meunier1, M.-A. Morel6, E. Murisasco3, I Nesterenko1, P. Nocera5,
B. Pallaud1, L. Pr?vot1, B. Priego-Valverde1, J. Seinturier3, N. Tan2, M. Tellier1, S. Rauzy1
(1) LPL-CNRS-Universit? de Provence (2) LIMSI-CNRS-Universit? Paris Sud
(3) LSIS-CNRS-Universit? de Toulon (4) LLING-Universit? de Nantes
(5) LIA-Universit? d?Avignon (6) RFC-Universit? Paris 3
blache@lpl-aix.fr
Abstract
We propose in this paper a broad-coverage
approach for multimodal annotation of
conversational data. Large annotation pro-
jects addressing the question of multimo-
dal annotation bring together many dif-
ferent kinds of information from different
domains, with different levels of granula-
rity. We present in this paper the first re-
sults of the OTIM project aiming at deve-
loping conventions and tools for multimo-
dal annotation.
1 Introduction
We present in this paper the first results of the
OTIM1 project aiming at developing conventions
and tools for multimodal annotation. We show
here how such an approach can be applied in the
annotation of a large conversational speech cor-
pus.
Before entering into more details, let us men-
tion that our data, tools and conventions are des-
cribed and freely downlodable from our website
(http ://www.lpl-aix.fr/ otim/).
The annotation process relies on several tools
and conventions, most of them elaborated within
the framework of the project. In particular, we pro-
pose a generic transcription convention, called En-
riched Orthographic Trancription, making it pos-
sible to annotate all specific pronunciation and
speech event, facilitating signal alignment. Dif-
ferent tools have been used in order to prepare
or directly annotate the transcription : grapheme-
phoneme converter, signal alignment, syllabifica-
tion, prosodic analysis, morpho-syntactic analysis,
chunking, etc. Our ambition is to propose a large
corpus, providing rich annotations in all the dif-
1OTIM stands for Outils pour le Traitement de l?Informa-
tion Multimodale (Tools for Multimodal Annotation). This
project in funded by the French ANR agency.
ferent linguistic domains, from prosody to gesture.
We describe in the following our first results.
2 Annotations
We present in this section some of the annota-
tions of a large conversational corpus, called CID
(Corpus of Interactional Data, see (Bertrand08)),
consisting in 8 dialogues, with audio and video si-
gnal, each lasting 1 hour.
Transcription : The transcription process is
done following specific conventions derived from
that of the GARS (Blanche-Benveniste87). The
result is what we call an enriched orthographic
construction, from which two derived transcrip-
tions are generated automatically : the standard or-
thographic transcription (the list of orthographic
tokens) and a specific transcription from which
the phonetic tokens are obtained to be used by the
grapheme-phoneme converter.
From the phoneme sequence and the audio si-
gnal, the aligner outputs for each phoneme its
time localization. This aligner (Brun04) is HMM-
based, it uses a set of 10 macro-classes of vowel
(7 oral and 3 nasal), 2 semi-vowels and 15 conso-
nants. Finally, from the time aligned phoneme se-
quence plus the EOT, the orthographic tokens is
time-aligned.
Syllables : The corpus was automatically seg-
mented in syllables. Sub-syllabic constituents (on-
set, nucleus and coda) are then identified as well
as the syllable structure (V, CV, CCV, etc.). Sylla-
bic position is specified in the case of polysyllabic
words.
Prosodic phrasing : Prosodic phrasing refers
to the structuring of speech material in terms of
boundaries and groupings. Our annotation scheme
supposes the distinction between two levels of
phrasing : the level of accentual phrases (AP, (Jun,
2002)) and the higher level of intonational phrases
186
(IP). Mean annotation time for IPs and APs was
30 minutes per minute.
Prominence : The prominence status of a syl-
lable distinguishes between accentuability (the
possibility for syllable to be prominent) and pro-
minence (at the perception level). In French the
first and last full syllables (not containing a
schwa) of a polysyllabic word can be prominent,
though this actual realization depends on spea-
kers choices. Accentuability annotation is auto-
matic while prominence annotation is manual and
perceptually based.
Tonal layer : Given a lack of consensus on the
inventory of tonal accents in French, we choose to
integrate in our annotation scheme three types of
tonal events : a/ underlying tones (for an eventual
FrenchToBI annotation) ; b/ surface tones (anno-
tated in terms of MOMel-Intsint protocol Hirst et
al 2000) ; c/ melodic contours (perceptually anno-
tated pitch movements in terms of their form and
function). The interest to have both manual and
automatic INTSINT annotations is that it allows
the study of their links.
Hand gestures : The formal model we use for
the annotation of hand gestures is adapted from
the specification files created by Kipp (2004) and
from the MUMIN coding scheme (Allwood et al,
2005). Among the main gesture types, we anno-
tate iconics, metaphoric, deictics, beats, emblems,
butterworths or adaptors.
We used the Anvil tool (Kipp, 2004) for the ma-
nual annotations. We created a specification files
taking into account the different information types
and the addition of new values adapted to the
CID corpus description (e.g. we added a separate
track Symmetry). For each hand, the scheme has 10
tracks. We allowed the possibility of a gesture per-
taining to several semiotic types using a boolean
notation. A gesture phrase (i.e. the whole gesture)
can be decomposed into several gesture phases i.e.
the different parts of a gesture such as the prepara-
tion, the stroke (the climax of the gesture), the hold
and the retraction (when the hands return to their
rest position) (McNeill, 1992). The scheme also
enables to annotate gesture lemmas (Kipp, 2004),
the shape and orientation of the hand during the
stroke, the gesture space, and contact. We added
the three tracks to code the hand trajectory, ges-
ture velocity and gesture amplitude.
Discourse and Interaction : Our discourse an-
notation scheme relies on multidimensional fra-
meworks such as DIT++ (Bunt, 2009) and is com-
patible with the guidelines defined by the Semantic
Annotation Framework (Dialogue Act) working
group of ISO TC37/4.
Discourse units include information about their
producer, have a form (clause, fragment, dis-
fluency, non-verbal), a content and a communi-
cative function. The same span of raw data may
be covered by several discourse units playing dif-
ferent communicative functions. Two discourse
units may even have exactly the same temporal ex-
tension, due to the multifonctionality that cannot
be avoided (Bunt, 2009).
Compared to standard dialogue act annotation
frameworks, three main additions are proposed :
rhetorical function, reported speech and humor.
Our rhetorical layer is an adaptation of an exis-
ting schema developed for monologic written data
in the context of the ANNODIS project.
Disfluencies : Disfluencies are organized
around an interruption point, which can occur al-
most anywhere in the production. Disfluencies can
be prosodic (lenghtenings, silent and filled pauses,
etc.), or lexicalized. In this case, they appear as a
word or a phrase truncation, that can be comple-
ted. We distinguish three parts in a disfluency (see
(Shriberg, 1994), (Blanche-Benveniste87)) :
? Reparandum : what precedes the interruption
point. This part is mandatory in all disfluen-
cies. We indicate there the nature of the inter-
rupted unit (word or phrase), and the type of
the truncated word (lexical or grammatical) ;
? Break interval. It is optional, some disfluen-
cies do not bear any specific event there.
? Reparans : the part following the break, repai-
ring the reparandum. We indicate there type
of the repair (no restart, word restart, determi-
ner restart, phrase restart, etc.), and its func-
tion (continuation, repair without change, re-
pair with change, etc.).
3 Quantitative information
We give in this section some indication about
the state of development of the CID annotation.
Hand gestures : 75 minutes involving 6 spea-
kers have been annotated, yielding a total number
of 1477 gestures. The onset and offset of gestures
correspond to the video frames, starting from and
187
going back to a rest position.
Face and gaze : At the present time, head move-
ments, gaze directions and facial expressions have
been coded in 15 minutes of speech yielding a to-
tal number of 1144 movements, directions and ex-
pressions, to the exclusion of gesture phases. The
onset and offset of each tag are determined in the
way as for hand gestures.
Body Posture : Our annotation scheme consi-
ders, on top of chest movements at trunk level,
attributes relevant to sitting positions (due to the
specificity of our corpus). It is based on the Pos-
ture Scoring System (Bull, 1987) and the Annota-
tion Scheme for Conversational Gestures (Kipp et
al., 2007). Our scheme covers four body parts :
arms, shoulders, trunk and legs. Seven dimensions
at arm level and six dimensions at leg level, as well
as their related reference points we take in fixing
the spatial location, are encoded.
Moreover, we added two dimensions to describe
respectively the arm posture in the sagittal plane
and the palm orientation of the forearm and the
hand. Finally, we added three dimensions for leg
posture : height, orientation and the way in which
the legs are crossed in sitting position.
We annotated postures on 15 minutes of the cor-
pus involving one pair of speakers, leading to 855
tags with respect to 15 different spatial location
dimensions of arms, shoulder, trunk and legs.
Annotation Time (min.) Units
Transcript 480 -
Hands 75 1477
Face 15 634
Gaze 15 510
Posture 15 855
R. Speech 180
Com. Function 6 229
Disfluencies At the moment, this annotation is
fully manual (we just developed a tool helping the
process in identifying disfluencies, but it has not
yet been evaluated). Annotating this phenomenon
requires 15mns for 1 minute of the corpus. The
following table illustrates the fact that disfluen-
cies are speaker-dependent in terms of quantity
and type. These figures also shows that disfluen-
cies affect lexicalized words as well as grammati-
cal ones.
Speaker_1 Speaker_1
Total number of words 1,434 1,304
Disfluent grammatical words 17 54
Disfluent lexicalized words 18 92
Truncated words 7 12
Truncated phrases 26 134
Transcription and phonemes The following
table recaps the main figures about the different
specific phenomena annotated in the EOT. To the
best of our knowledge, these data are the first of
this type obtained on a large corpus. This informa-
tion is still to be analyzed.
Phenomenon Number
Elision 11,058
Word truncation 1,732
Standard liaison missing 160
Unusual liaison 49
Non-standard phonetic realization 2,812
Laugh seq. 2,111
Laughing speech seq. 367
Single laugh IPU 844
Overlaps > 150 ms 4,150
Syntax We used the stochastic parser developed
at the LPL (Blache&Rauzy, 2008) to automaticaly
generate morppho-syntactic and syntactic annota-
tions. The parser has been adapted it in order to ac-
count for the specificities of speech analysis. First,
the system implements a segmentation technique,
identifying large syntactic units that can be consi-
dered as the equivalent of sentences in written
texts. This technique distinguishes between strong
and weak or soft punctuation marks. A second mo-
dification concerns the lexical frequencies used by
the parser model in order to capture phenomena
proper to conversational data.
The categories and chunks counts for the whole
corpus are summarized in the following figure :
Category Count Group Count
adverb 15123 AP 3634
adjective 4585 NP 13107
auxiliary 3057 PP 7041
determiner 9427 AdvP 15040
conjunction 9390 VPn 22925
interjection 5068 VP 1323
preposition 8693 Total 63070
pronoun 25199
noun 13419 Soft Pct 9689
verb 20436 Strong Pct 14459
Total 114397 Total 24148
4 Evaluations
Prosodic annotation : Prosodic annotation of
1 dialogue has been done by 2 experts. The
annotators worked separately using Praat. Inter-
transcriber agreement studies were done for the
annotation of higher prosodic units. First anno-
tator marked 3,159 and second annotator 2,855
188
Intonational Phrases. Mean percentage of inter-
transcriber agreement was 91.4% and mean
kappa-statistics 0.79, which stands for a quite sub-
stantial agreement.
Gesture : We performed a measure of inter-
reliability for three independent coders for Gesture
Space. The measure is based on Cohen?s correc-
ted kappa coefficient for the validation of coding
schemes (Carletta96).
Three coders have annotated three minutes for
GestureSpace including GestureRegion and Ges-
tureCoordinates. The kappa values indicated that
the agreement is high for GestureRegion of right
hand (kappa = 0.649) and left hand (kappa =
0.674). However it is low for GestureCoordinates
of right hand (k= 0.257) and left hand (k= 0.592).
Such low agreement of GestureCoordinates might
be due to several factors. First, the number of ca-
tegorical values is important.
Second, three minutes might be limited in terms
of data to run a kappa measure. Third, GestureRe-
gion affects GestureCoordinates : if the coders di-
sagree about GestureRegion, they are likely to also
annotate GestureCoordinates in a different way.
For instance, it was decided that no coordinate
would be selected for a gesture in the center-center
region, whereas there is a coordinate value for ges-
tures occurring in other parts of the GestureRe-
gion. This means that whenever coders disagree
between the center-center or center region, the an-
notation of the coordinates cannot be congruent.
5 Information representation
5.1 XML encoding
Our approach consists in first precisely define
the organization of annotations in terms of typed-
feature structures. We obtain an abstract descrip-
tion from which we automatically generate a for-
mal schema in XML. All the annotations are then
encoded following this schema.
Our XML schema, besides a basic encoding of
data following AIF, encode all information concer-
ning the organization as well as the constraints on
the structures. In the same way as TFS are used
as a tree description language in theories such as
HPSG, the XML schema generated from our TFS
representation also plays the same role with res-
pect to the XML annotation data file. On the one
hand, basic data are encoded with AIF, on the
other hand, the XML schema encode all higher
level information. Both components (basic data +
structural constraints) guarantee against informa-
tion loss that otherwise occurs when translating
from one coding format to another (for example
from Anvil to Praat).
5.2 Querying
To ease the multimodal exploitation of the data,
our objective is to provide a set of operators dedi-
cated to concurrent querying on hierarchical an-
notation. Concurrent querying consists in que-
rying annotations belonging to two or more mo-
dalities or even in querying the relationships bet-
ween modalities. For instance, we want to be able
to express queries over gestures and intonation
contours (what kind of intonational contour does
the speaker use when he looks at the listener ?).
We also want to be able to query temporal relation-
ships (in terms of anticipation, synchronization or
delay) between both gesture strokes and lexical af-
filiates.
Our proposal is to define these operators as an
extension of XQuery. From the XML encoding
and the temporal alignment of annotated data, it
will possible to express queries to find patterns and
to navigate in the structure. We also want to en-
able a user to check predicates on parts of the cor-
pus using classical criteria on values, annotations
and existing relationships (temporal or structural
ones corresponding to inclusions or overlaps bet-
ween annotations). First, we shall rely on one of
our previous proposal called MSXD (MultiStruc-
tured XML Document). It is a XML-compatible
model designed to describe and query concurrent
hierarchical structures defined over the same tex-
tual data which supports Allen?s relations.
6 Conclusion
Multimodal annotation is often reduced to
the encoding of gesture, eventually accompa-
nied with another level of linguistic information
(e.g. morpho-syntax). We reported in this paper a
broad-coverage approach, aiming at encoding all
the linguistic domains into a unique framework.
We developed for this a set of conventions and
tools making it possible to bring together and align
all these different pieces of information. The result
is the CID (Corpus of Interactional Data), the first
large corpus of conversational data bearing rich
annotations on all the linguistic domains.
189
References
Allen J. (1999) Time and time again : The many way to re-
present time. International Journal of Intelligent Systems,
6(4)
Allwood, J., Cerrato, L., Dybkjaer, L., Jokinen, K., Navar-
retta, C., Paggio, P. (2005) The MUMIN Multimodal Co-
ding Scheme, NorFA yearbook 2005.
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P.
F. Patel-Schneider (2003) The Description Logic Hand-
book : Theory, Implementation, Applications. Cambridge
University Press.
Bertrand, R., Blache, P., Espesser, R., Ferr?, G., Meunier, C.,
Priego-Valverde, B., Rauzy, S. (2008) ?Le CID - Corpus
of Interactional Data - Annotation et Exploitation Multi-
modale de Parole Conversationnelle?, in revue Traitement
Automatique des Langues, 49 :3.
Bigi, C. Meunier, I. Nesterenko, R. Bertrand 2010. ?Syllable
Boundaries Automatic Detection in Spontaneous Speech?,
in proceedings of LREC 2010.
Blache P. and Rauzy S. 2008. ?Influence de la qualit? de
l??tiquetage sur le chunking : une corr?lation d?pendant de
la taille des chunks?. in proceedings of TALN 2008 (Avi-
gnon, France), pp. 290-299.
Blache P., R. Bertrand, and G. Ferr? 2009. ?Creating and
Exploiting Multimodal Annotated Corpora : The ToMA
Project?. In Multimodal Corpora : From Models of Natu-
ral Interaction to Systems and Applications, Springer.
Blanche-Benveniste C. & C. Jeanjean (1987) Le fran?ais
parl?. Transcription et ?dition, Didier Erudition.
Blanche-Benveniste C. 1987. ?Syntaxe, choix du lexique et
lieux de bafouillage?, in DRLAV 36-37
Browman C. P. and L. Goldstein. 1989. ?Articulatory ges-
tures as phonological units?. In Phonology 6, 201-252
Brun A., Cerisara C., Fohr D., Illina I., Langlois D., Mella O.
& Smaili K. (2004- ?Ants : Le syst?l?me de transcription
automatique du Loria?, Actes des XXV Journ?es d?Etudes
sur la Parole, F?s.
E. Bruno, E. Murisasco (2006) Describing and Querying hie-
rarchical structures defined over the same textual data, in
Proceedings of the ACM Symposium on Document Engi-
neering (DocEng 2006).
Bull, P. (1987) Posture and Gesture, Pergamon Press.
Bunt H. 2009. ?Multifunctionality and multidimensional
dialogue semantics.? In Proceedings of DiaHolmia?09,
SEMDIAL.
B?rki A., C. Gendrot, G. Gravier & al.(2008) ?Alignement
automatique et analyse phon?tique : comparaison de dif-
f?rents syst?mes pour l?analyse du schwa?, in revue TAL
,49 :3
Carletta, J. (1996) ?Assessing agreement on classification
tasks : The kappa statistic?, in Computational Linguistics
22.
Corlett, E. N., Wilson,John R. Manenica. I. (1986) ?Influence
Parameters and Assessment Methods for Evaluating Body
Postures?, in Ergonomics of Working Postures : Models,
Methods and Cases , Proceedings of the First International
Occupational Ergonomics Symposium.
Di Cristo & Hirst D. (1996) ?Vers une typologie des unites in-
tonatives du fran?ais?, XXI?me JEP, 219-222, 1996, Avi-
gnon, France
Di Cristo A. & Di Cristo P. (2001) ?Syntaix, une approche
m?trique-autosegmentale de la prosodie?, in revue Traite-
ment Automatique des Langues, 42 :1.
Dipper S., M. Goetze and S. Skopeteas (eds.) 2007. Informa-
tion Structure in Cross-Linguistic Corpora : Annotation
Guidelines, Working Papers of the SFB 632, 7 :07
FGNet Second Foresight Report (2004) Face
and Gesture Recognition Working Group.
http ://www.mmk.ei.tum.de/ waf/fgnet-intern/3rd-
fgnet-foresight-workshop.pdf
Gendner V. et al 2003. ?PEAS, the first instantiation of a
comparative framework for evaluating parsers of French?.
in Research Notes of EACL 2003 (Budapest, Hungaria).
Hawkins S. and N. Nguyen 2003. ?Effects on word re-
cognition of syllable-onset cues to syllable-coda voicing?,
in Papers in Laboratory Phonology VI. Cambridge Univ.
Press.
Hirst, D., Di Cristo, A., Espesser, R. 2000. ?Levels of des-
cription and levels of representation in the analysis of in-
tonation?, in Prosody : Theory and Experiment, Kluwer.
Hirst, D.J. (2005) ?Form and function in the representation
of speech prosody?, in K.Hirose, D.J.Hirst & Y.Sagisaka
(eds) Quantitative prosody modeling for natural speech
description and generation (Speech Communication 46 :3-
4.
Hirst, D.J. (2007) ?A Praat plugin for Momel and INTSINT
with improved algorithms for modelling and coding into-
nation?, in Proceedings of the XVIth International Confe-
rence of Phonetic Sciences.
Hirst, D. (2007), Plugin Momel-Intsint. Inter-
net : http ://uk.groups.yahoo.com/group/praat-
users/files/Daniel_Hirst/plugin_momel-intsint.zip,
Boersma, Weenink, 2007.
Jun, S.-A., Fougeron, C. 2002. ?Realizations of accentual
phrase in French intonation?, in Probus 14.
Kendon, A. (1980) ?Gesticulation and Speech : Two Aspects
of the Porcess of Utterance?, in M.R. Key (ed.), The Re-
lationship of Verbal and Nonverbal Communication, The
Hague : Mouton.
Kita, S., Ozyurek, A. (2003) ?What does cross-linguistic va-
riation in semantic coordination of speech and gesture re-
veal ? Evidence for an interface representation of spatial
thinking and speaking?, in Journal of Memory and Lan-
guage, 48.
Kipp, M. (2004). Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation. Boca
Raton, Florida, Dissertation.com.
Kipp, M., Neff, M., Albrecht, I. (2007). An annotation
scheme for conversational gestures : how to economically
capture timing and form. Language Resources and Eva-
luation, 41(3).
Koiso H., Horiuchi Y., Ichikawa A. & Den Y.(1998) ?An ana-
lysis of turn-taking and backchannels based on prosodic
and syntactic features in Japanese map task dialogs?, in
Language and Speech, 41.
McNeill, D. (1992). Hand and Mind. What Gestures Re-
veal about Thought, Chicago : The University of Chicago
Press.
McNeill, D. (2005). Gesture and Thought, Chicago, London :
The University of Chicago Press.
Milborrow S., F. Nicolls. (2008). Locating Facial Features
with an Extended Active Shape Model. ECCV (4).
Nesterenko I. (2006) ?Corpus du parler russe spontan? : an-
notations et observations sur la distribution des fronti?res
prosodiques?, in revue TIPA, 25.
190
Paroubek P. et al 2006. ?Data Annotations and Measures in
EASY the Evaluation Campaign for Parsers in French?. in
proceedings of the 5th international Conference on Lan-
guage Resources and Evaluation 2006 (Genoa, Italy), pp.
314-320.
Pierrehumbert & Beckman (1988) Japanese Tone Structure.
Coll. Linguistic Inquiry Monographs, 15. Cambridge,
MA, USA : The MIT Press.
Platzer, W., Kahle W. (2004) Color Atlas and Textbook of
Human Anatomy, Thieme. Project MuDis. Technische
Universitat Munchen. http ://www9.cs.tum.edu/research
Scherer, K.R., Ekman, P. (1982) Handbook of methods in
nonverbal behavior research. Cambridge University Press.
Shriberg E. 1994. Preliminaries to a theory of speech dis-
fluencies. PhD Thesis, University of California, Berkeley
Wallhoff F., M. Ablassmeier, and G. Rigoll. (2006) ?Mul-
timodal Face Detection, Head Orientation and Eye Gaze
Tracking?, in proceedings of International Conference on
Multisensor Fusion and Integration (MFI).
White, T. D., Folkens, P. A. (1991) Human Osteology. San
Diego : Academic Press, Inc.
191
Proceedings of the SIGDIAL 2013 Conference, pages 87?91,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A quantitative view of feedback lexical markers in conversational French
Laurent Pre?vot Brigitte Bigi
Aix Marseille Universite? & CNRS
Laboratoire Parole et Langage
Aix-en-Provence (France)
firstname.lastname@lpl-aix.fr
Roxane Bertrand
Abstract
This paper presents a quantitative descrip-
tion of the lexical items used for linguis-
tic feedback in the Corpus of Interactional
Data (CID). The paper includes the raw
figures for feedback lexical item as well
as more detailed figures concerning inter-
individual variability. This effort is a first
step before a broader analysis including
more discourse situations and featuring
communicative function annotation.
Index Terms: Feedback, Backchannel, Corpus,
French Language
1 Objectives
Conversational feedback is mostly performed
through short utterances such as yeah, mh, okay
not produced by the main speaker but by one of
the other participants of a conversation. Such ut-
terances are among the most frequent in conver-
sational data (Stolcke et al, 2000). They also
have been described in psycho-linguistic models
of communication as a crucial communicative tool
for achieving coordination or alignment in dia-
logue (Clark, 1996).
The general objective of the project (ANR
CoFee: Conversational Feedback)1(Pre?vot and
Bertrand, 2012) in which this work takes place
is to propose a fine grained model of the
form/function relationship concerning feedback
behaviors in conversation. The present study is
first exploration aiming at knowing better the dis-
tribution of these items in one of our corpus. More
precisely, we would to verify how much inter-
individual variability we will face in further study
and whether we can identify a structure in this
variability (e.g speaker profiles). Second, we tried
1See the project website: http://cofee.hypotheses.org
to check there some strong trends in terms of evo-
lution of use of these items in the course of the
conversation. This later point was not conclusive
and is not developed in this paper.
Some data-intensive works exist for English
(Gravano et al, 2012), Japanese (Kamiya et al,
2010; Misu et al, 2011) or Swedish (Allwood et
al., 1992; Cerrato, 2007; Neiberg et al, 2013) but
not on many other languages such as French for
example. On French, the work of (Muller and
Pre?vot, 2003; Muller and Pre?vot, 2009) concerned
a smaller scale (A hour corpus) and very specific
task. (Bertrand et al, 2007) was focussed on the
feedback inviting cues and also on a smaller scale
(2 ? 15 minutes). They showed that particular
pitch contours and discursive markers play a sys-
tematic role as inviting-cues both for vocal and
gestural back-channels.
The paper is structured as follow. Section 2
presents the conversational corpus used for this
study, then section 3 presents how this corpus has
been processed. Section 4 is related to general fig-
ures for the feedback lexical items, followed by
more detailed information about inter-individual
variability (section 5).
2 The corpus
The Corpus of Interactional Data (CID) (Bertrand
et al, 2008; Blache et al, 2009)2 is an audio-video
recording of 8 hours of spontaneous French dia-
logues, 1 hour of recording per session. Each di-
alogue involved two participants of the same gen-
der. One of the following two topics of conver-
sation was suggested to participants: conflicts in
their professional environment or unusual situa-
tions in which participants may have found them-
selves. It features a nearly free conversational
style with only a single theme proposed to the par-
ticipants at the beginning of the experiment. This
2http://www.sldr.org/sldr000027/en
87
corpus is fully transcribed and forced-aligned at
phone level. Moreover, it has been annotated with
various linguistic information (Prosodic Phrasing,
Discourse units, Syntactic tags, ...) (Blache et al,
2010) which will allow us later to take advantage
of these levels of analysis.
Numerous studies have been carried out in pre-
pared speech. However, conversational speech
refers to a more informal activity, in which par-
ticipants have constantly to manage and negotiate
turn-taking, topic changes (among other things)
without any preparation. As a consequence, nu-
merous phenomena appear such as hesitations, re-
peats, backchannels, etc. Phonetic phenomena
such as non-standard elision, reduction phenom-
ena, truncated words, and more generally, non-
standard pronunciations are also very frequent.
All these phenomena can impact on the phoneti-
zation, then on alignment.
3 Processing the corpus
The transcription process is done following spe-
cific conventions derived from that of the GARS
(Blanche-Benveniste and Jeanjean, 1987). The
result is what we call an enriched orthographic
transcription (EOT), from which two derived tran-
scriptions are generated automatically : the stan-
dard orthographic transcription (the list of ortho-
graphic tokens) and a specific transcription from
which the phonetic tokens are obtained to be
used by the grapheme-phoneme converter. From
the phoneme sequence and the audio signal, the
aligner outputs for each phoneme its time localiza-
tion. This corpus has been processed with several
aligners. The first and main one (Brun et al, 2004)
is HMM-based, it uses a set of 10 macro-classes
of vowel (7 oral and 3 nasal), 2 semi-vowels and
15 consonants. Finally, from the time aligned
phoneme sequence plus the EOT, the orthographic
tokens is time-aligned.
The alignment for this paper is another ver-
sion that has been carried out using SPPAS3 (Bigi,
2012). SPPAS is a tool to produce automatic anno-
tations which include utterance, word, syllabic and
phonemic segmentations from a recorded speech
sound and its transcription.
Alignment of items of the list given in (1) were
then manually verified. Largest errors were cor-
rected to obtain reliable alignments.
DM prononciations are the standard ones except
3http://www.lpl-aix.fr/?bigi/sppas/
for a few cases. There are only two items with
non standard cases that are over 2 occurrences:
sampa: m.w.e.) that is an hybrid between mh
and ouais, and sampa w.a.l.a, a reduction of
v.w.a.l.a voila`.
The extraction themselves have been realized
by the authors with a Python script and all the
statistical analyses and plots have been produced
with R statistical analysis tool.
4 Descriptive statistics for the lexical
markers used in feedback
All the lexical items of the list given in (1) were
automatically extracted and categorized into two
categories: (i) Isolated items are items or sequence
of items surrounded by pauses of at least 200 ms
and not including any extra material than the items
of this list ; (ii) Initial items (or sequence items)
are located in front of some other items (but there
is no other material within the sequence). Most
of these items also occur in final or even sur-
rounded positions but we did not consider these
cases since they do are not clearly related to feed-
back. More precisely surrounded items are mostly
consisting in breaks of disfluencies or genuinely
integrated construction (e.g j?e?tais d?accord avec
lui / I agreed with him). Final ones can play a
role in eliciting feedback or sometimes bring some
kind of closure at the end of the utterance (what
has been described as Pivot Ending in (Gravano et
al., 2012)).
(1) ah (ah), bon (well), ben (well), euh (err,
uh), mh (mh), ouais (yeah), oui (yes), non
(no), d?accord (agreed), OK (okay), voila`
(that?s it, right)
Strictly speaking, the list (1) is not exhaustive.
However, other items are already in the thin part
of the distribution?s tail. Moreover, some of the
items such as euh / err are not necessarily related
to feedback. However, by crossing lexical values
with position we expect to get close enough the
full set of tokens involved in feedback. For exam-
ple, initial euh not followed by a feedback related
item will not be included in the final dataset. This
is also an objective of the present work to identify
these situations.
The different markers exhibit very different fig-
ures with regard to their location as it can be seen
in 1. While some are specialized in isolated feed-
back such as the continuer mh which is most of the
88
time backchanneled, others are found at the begin-
ning of utterances such as euh, ah. The later makes
sense since euh is also a filled pause.
ah 
ah_
oua
is 
ah_
oui ben bon 
dacc
ord euh mh 
mh_
mh non
 
oua
is 
oua
is_o
uais
 
oui voila
 0
100
200
300
400
500
initialisolated
Figure 1: Distribution of isolated vs. initial posi-
tion for the most frequent lexical items
In total 197 different combinations of the ba-
sic markers were identified. The most frequent
are the simple repetitions of items such as ouais
(up to nine times) or mh. There are also more
complex structures as exhibited in (2) that seem
to mix two kinds of items: base ones and mod-
ifiers (ah, euh). The base ones seem by default
to carry general purpose communicative functions
as described in (Bunt, 2009; Bunt, 2012) while the
others can also be produced alone but are generally
dealing specific dimension such as turn-taking, at-
titude expression or time management.
(2) a. ah ouais d?accord ok (ah yeah right
okay)
b. voila` oui non (that?s it yes no)
With regard to duration, the data is rather messy
concerning the very long items. There are extreme
lengthening on these units. Aside that and the filler
uh that exhibit a wide spread, the other items are
not produced with huge variations. Monosyllabic
remain well centered around 150-250 ms while di-
syllabic and repeated items are distributed in the
250-500 ms range. This is important for our next
step in which automatic acoustic analysis of these
items will be performed.
l
l
ll
l
l
lll
l
l
ll
l
l
l
l
l l
l
l l
l
l
l
l
ll
l
ll
l
l
l
ll
l
l
l
llll
l
l
l
ll lll
l
ll
l
l
l
l
ll ll
llll
l
ll
l
l
ll
l
l
ll
l
l
ll
l
l
l l
ah 
ah_
oua
is 
ah_
oui ben bon 
dacc
ord euh mh 
mh_
mh non
 
oua
is 
oua
is_o
uais
 
oui voila
 
0.0
0.5
1.0
1.5
2.0
Figure 2: Duration (in seconds) of each lexical
type
5 Inter-individual variability
Inter-individual variation is a big issue on the way
to the generalizability. We would like to under-
stand some of the feedback producing profiles.
Our intuitions coming from familiarity of the data
is that there are strong variation but they corre-
spond to a few different speaking styles. In fu-
ture work, we would like to see in a second step
whether we can identify and characterize these
styles.
l
150
200
250
300
350
400
450
Figure 3: Number of feedback items per speaker
Figure 3 illustrates the total figures of feedback
per speaker. As expected variation is huge, from
132 to 425 but with in fact with few outliers with
a nice batch of speaker in the 200 ? 300 range.
The wider spread of the distribution in the high
range comes from two factors. First of all, there
are participants producing a high quantity of feed-
89
back items. They produce a massive amount of
light backchannels (mh, ouais) compared to low-
quantity feedback producers. The later also pro-
duce feedback during the long pauses of the main
speaker but they produce much less overlapping
backchannels. This should be double checked
with a specific measure (adding overlapping as a
factor). However, a second effect seems important
for at least one speaker (the outlier): the amount
to time holding the floor. In fact the speaker pro-
ducing the most feedback did so because she was
rarely the main speaker.
In order to get a global idea of the different uses
of these items, Figure 5 represents the proportion
of each item per speaker. As expected, the varia-
tion is important but one can spot some tendencies.
For examples for the most frequent items, the rank
seems to preserved across speakers.
CID_
AB 
CID_
AC 
CID_
AG 
CID_
AP 
CID_
BX 
CID_
CM 
CID_
EB 
CID_
IM 
CID_
LJ 
CID_
LL 
CID_
MB 
CID_
MG 
CID_
ML 
CID_
NH 
CID_
SR 
CID_
YM 
voilaouiouais_ouaisouaisnonmh_mhmheuhdaccordbonbenah_ouiah_ouaisah
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Distribution of the lexical items
Based on their feedback profile (proportion of
use of each items as illustrated in Figure 5), we
attempted to cluster the participants as showed in
5. While the lower parts of the dendrogram are
hard to interpret the higher part matches well with
the impression acquired by listening to the corpus
(no backchannels and rather formal feedback vs.
lots of backchannels and very colloquial style).
6 Current and Future Work
About this first batch of analyses, we will com-
plete the analysis of the evolution during the con-
versation. More precisely, we will go at the
individual level looking for time-based changes
AB
AG
IM ML
YM
BX
AP EB LJ NH
CM
SR
AC MB
LL MG0.1
0.2
0.3
0.4
0.5
Dendrogram of  diana(x = distSpForm)
diana (*, "NA")distSpForm
Heig
ht
Figure 5: Dendrogram of the participants cluster
based on their feedback profile
in their profiles as well as looking at the pairs
for tracking potential convergence effect either in
terms of distribution of lexical marker types or in
their duration.
In parallel to this work, we are launching in-
dependent prosodic and kinesic analyses of the
forms, as well as a discourse analysis of the func-
tions. Moreover the work is being extended by
adding two corpora in the study in order to allow
for a better situation generalisability: A French
MapTask; and a third corpus consisting in a less
cooperative situation. The idea is later to bring
together the observations from the different levels
in order to propose a multidimensional model for
feedback in French dialogues.
Those are steps toward more extensive studies
in the spirit of (Gravano et al, 2012) or (Neiberg
et al, 2013) on French language and in which we
hope to address more directly the issue of dis-
course situation generalisability.
Acknowledgment
This work has been realized with the support of
the ANR (Grant Number: ANR-12-JCJC-JSH2-
006-01) and exploited aligned data produced in
the framework of the ANR project (Grant Number
ANR-08-BLAN-0239). We would like to thank all
the members of these two projects.
90
References
J. Allwood, J. Nivre, and E. Ahlsen. 1992. On the se-
mantics and pragmatics of linguistic feedback. Jour-
nal of Semantics, 9.
R. Bertrand, G. Ferre?, P. Blache, R. Espesser, and
S. Rauzy. 2007. Backchannels revisited from a mul-
timodal perspective. In Proceedings of Auditory-
visual Speech Processing. Citeseer.
R. Bertrand, P. Blache, R. Espesser, G. Ferre?, C. Me-
unier, B. Priego-Valverde, and S. Rauzy. 2008.
Le cid-corpus of interactional data-annotation et ex-
ploitation multimodale de parole conversationnelle.
Traitement Automatique des Langues, 49(3):1?30.
B. Bigi. 2012. SPPAS: a tool for the phonetic segmen-
tation of speech. In Language Resource and Evalu-
ation Conference, pages 1748?1755, ISBN 978?2?
9517408?7?7, Istanbul (Turkey).
P. Blache, R. Bertrand, and G. Ferre?. 2009. Creating
and exploiting multimodal annotated corpora: the
toma project. Multimodal corpora, pages 38?53.
P. Blache, R. Bertrand, B. Bigi, E. Bruno, E. Cela,
R. Espesser, G. Ferre?, M. Guardiola, D. Hirst,
E. Muriasco, J.-C. Martin, C. Meunier, M.-A. Morel,
I. Nesterenko, P. Nocera, B. Palaud, L. Pre?vot,
B. Priego-Valverde, J. Seinturier, N. Tan, M. Tel-
lier, and S. Rauzy. 2010. Multimodal annotation
of conversational data. In Proceedings of Linguistic
Annotation Workshop.
C. Blanche-Benveniste and C. Jeanjean. 1987. Le
franc?ais parle?. Edition et transcription. Paris, Di-
dier Erudition.
A. Brun, C. Cerisara, D. Fohr, I. Illina, D. Langlois,
O. Mella, and K. Sma??li. 2004. Ants: le syste`me de
transcription automatique du loria. In Actes des XXV
Journe?es d?Etudes sur la Parole, Fe`s, Morocco.
H. Bunt. 2009. Multifunctionality and multidimen-
sional dialogue act annotation. In Proceedings of
DiaHolmia, SEMDIAL.
H. Bunt. 2012. The semantics of feedback. In
16th Workshop on the Semantics and Pragmatics of
Dialogue (SEMDIAL 2012), pages 118?127, Paris
(France).
L. Cerrato. 2007. Investigating Communicative Feed-
back Phenomena across Languages and Modalities.
Ph.D. thesis.
H.H. Clark. 1996. Using language. Cambridge: Cam-
bridge University Press.
A. Gravano, J. Hirschberg, and S?. Ben?us?. 2012. Af-
firmative cue words in task-oriented dialogue. Com-
putational Linguistics, 38(1):1?39.
Y. Kamiya, T. Ohno, and S. Matsubara. 2010. Coher-
ent back-channel feedback tagging of in-car spoken
dialogue corpus. In Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 205?208. Association for Com-
putational Linguistics.
T. Misu, E. Mizukami, Y. Shiga, S. Kawamoto,
H. Kawai, and S. Nakamura. 2011. Toward con-
struction of spoken dialogue system that evokes
users? spontaneous backchannels. In Proceedings
of the SIGDIAL 2011 Conference, pages 259?265.
Association for Computational Linguistics.
P. Muller and L. Pre?vot. 2003. An empirical study
of acknowledgement structures. In Proceedings od
Diabruck, 7th workshop on semantics and pragmat-
ics of dialogue, Saarbrucken.
P. Muller and L. Pre?vot. 2009. Grounding information
in route explanation dialogues. In Spatial Language
and Dialogue. Oxford University Press.
D. Neiberg, G. Salvi, and J. Gustafson. 2013. Semi-
supervised methods for exploring the acoustics of
simple productive feedback. Speech Communica-
tion.
L. Pre?vot and R. Bertrand. 2012. Cofee-toward a mul-
tidimensional analysis of conversational feedback,
the case of french language. In Proceedings of the
Workshop on Feedback Behaviors. (poster).
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
91
